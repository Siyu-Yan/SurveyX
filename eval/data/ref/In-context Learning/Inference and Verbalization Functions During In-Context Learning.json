{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.09349",
    "title": "Inference and Verbalization Functions During In-Context Learning",
    "abstract": "Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., \u201ctrue\u201d/\u201cfalse\u201d to \u201ccat\u201d/\u201cdog\u201d), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-227B, and LLAMA-3.1-70B.",
    "bib_name": "tao2024inferenceverbalizationfunctionsincontext",
    "md_text": "Junyi Tao* Stanford University junyitao@stanford.edu Xiaoyin Chen* Mila, University of Montreal xiaoyin.chen@mila.quebec Nelson F. Liu Stanford University nfliu@cs.stanford.edu\nJunyi Tao* Stanford University junyitao@stanford.edu Xiaoyin Chen* Mila, University of Montreal xiaoyin.chen@mila.quebec\n# Abstract\nLarge language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., \u201ctrue\u201d/\u201cfalse\u201d to \u201ccat\u201d/\u201cdog\u201d), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-227B, and LLAMA-3.1-70B.\narXiv:2410.09349v1\n# 1 Introduction\nLarge language models (LMs) are capable of incontext learning (ICL)\u2014the ability to solve novel tasks from solely a handful of demonstration examples provided in-context during inference (Brown et al., 2020). Previous work has found that, in certain settings, ICL performance is minimally affected by using demonstrations with irrelevant label words (Min et al., 2022). How do LMs manage to perform in-context learning with irrelevant and even misleading label words? This research seeks to offer a causal explanation for such model\n*Equal contribution.\nNelson F. Liu Stanford University nfliu@cs.stanford.edu\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d34f/d34f95fd-1b20-4e13-bf42-a1db434a8bd2.png\" style=\"width: 50%;\"></div>\nFigure 1: Summary of our hypotheses, method, and findings. We hypothesize that LMs perform ICL via two sequential functions: (1) an inference function that uses an input ICL context and returns a representation of the answer and 2) a verbalization function that maps the aforementioned answer representation to the output label space specified by the demonstrations. Furthermore, the inference function is invariant to remappings of the output label space. We empirically test our hypotheses by conducting interchange intervention experiments (i.e., fixing one of the functions while modifying the other).\nbehaviors, going beyond mere summaries of inputoutput patterns. We hypothesize that the model develops and utilizes certain abstractions to consistently work well on ICL tasks despite remappings of the label spaces. Specifically, we propose and test the following hypotheses:\nH1 Causal mechanism: LMs sequentially apply two functions when performing ICL: First, an inference function that constructs a representation of the answer to the ICL input; and second, a verbalization function that maps this answer representation to the output label space specified by the demonstrations.\nTo learn about the model\u2019s low-level implementations of these high-level abstractions (distributed across neural activations), we train probes to detect representations of label words. Its results suggest that the two functions are separately located in the similar layers across different runs (Appendix A). This motivates an additional hypothesis:\nH3 Separate and consistent localization: The two functions are located (largely) separately in different sets of sequential layers, and the locations of these layers are consistent across settings with different remapped label spaces. This is a working hypothesis that serves as a \u201cladder\u201d, providing protocols for us to intervene on the high-level abstractions through modifying the low-level internal representations of the model.\nMethods. Our hypotheses are validated if we can define and realize a counterfactual scenario that occurs only if our hypotheses are true. For example, let\u2019s imagine a scenario where we \u201cconcatenate\u201d an inference function and a verbalization function, with one induced from a run where the label words are \u201ctrue\u201d/\u201cfalse\u201d, and another from a run where the label words are \u201ccat\u201d/\u201cdog\u201d. Suppose the output of the former run is \u201ctrue\u201d, and the latter run is \u201cfalse\u201d. This concatenation can make the model generate the hypothetical counterfactual output \u201ccat\u201d only if all of our hypotheses hold true (see more discussion on this in Section 2.1). We realize this hypothetical concatenation by swapping the last token representations created for one input into the model which is processing another input with remapped label words. This operation is known as interchange intervention or activation patching (Geiger et al., 2020; Vig et al., 2020; Finlayson et al., 2021; Meng et al., 2022). Findings. By experimenting with remapped label spaces that will induce changes in the ver balization function (illustrated in Figure 2), we find that our intervention on certain layers can achieve a \u201cconcatenated\u2019 model that produces the counterfactual output. This validates H2. We also conduct a complementary experiment to induce changes in the inference function with constructed alternative tasks on MultiNLI, where the example-label relations are changed and the input and label spaces are fixed (illustrated in Figure 7). This is aimed for further validating the localization of the inference function. The results from these two experiments together validate H1. Ad-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a05b/a05b5881-15c6-4ca2-bd1d-7adb58fcd3ad.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/56d8/56d8ad59-19a6-48e8-b9e5-6f2223d6c40f.png\" style=\"width: 50%;\"></div>\nFigure 2: Intervention experiments with remapped label spaces. (1, top): First, we induce changes in the verbalization function by remapping the output label space in the demonstrations (e.g., \u201ctrue\u201d/\u201cfalse\u201d to \u201ccat\u201d/\u201cdog\u201d). (2): Then, we intervene on the verbalization function by replacing the representation of the intervened model (prompted with the remapped label space \u201ccat\u201d/\u201cdog\u201d) at layer l with the representation from the source model (prompted with the default label space \u201ctrue\u201d/\u201cfalse\u201d) at the same layer. (3): We evaluate the intervention effects by measuring how often the intervened output Y \u2032\u2032 matches the hypothetical counterfactual output Yc. (4, bottom): Distribution of outputs predicted by GEMMA-7B on MultiNLI when intervening with the remapped label space \u201ctrue\u201d/\u201cfalse\u201d \u2192\u201ccat\u201d/\u201cdog\u201d. For clarity, we visualize the subset examples where the hypothetical counterfactual output is \u201ccat\u201d. We observe that the rate at which the intervened output matches the counterfactual output peaks around layers 18-20, localizing the verbalization function and suggesting the inference function is invariant to label remapping. ditionally, we observe that the two functions are separately and consistently located in similar layers of each model, across various tasks and datasets, including MultiNLI, RTE, ANLI, IMDb, and AGNews. This is aligned with H3. These findings apply to models of various sizes and families, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-227B, and LLAMA-3.1-70B.1\n1Our code is publicly available at https://github.com/ JunyiTao/infer-then-verbalize-during-icl.\n# 2 Methods\nWe aim for an explanation that reveals the causal structure employed by the model. Successful causal explanation is marked by its ability to allow us to make counterfactual predictions, that is, answering the \u201cwhat-if-things-had-been-different questions\u201d should we perform certain interventions (Woodward, 2003). Typically, direct observation of outcomes from distinct interventions on the same unit is not possible (Holland, 1986). But, it is not an issue for us, because we can let the model generate outputs under any conceivable intervention on the units. Moreover, we can make counterfactual statements with mere interventional information with deterministic neural network models (Bareinboim et al., 2022), which is also typically not possible (Pearl and Mackenzie, 2018). These enable a concrete operationalization of our hypothesis testing.\n# 2.1 Testing Hypotheses with Counterfactual Scenarios\nFramework for testing our hypotheses. Our approach is to (1) define a counterfactual scenario aligned with our hypotheses, wherein certain model behaviors emerge only if our hypotheses hold true, (2) employ appropriate intervention methods to realize this scenario, and (3) conduct the interventionbased experiments and interpret their results.\nDesign a counterfactual scenario. Imagine a scenario corroborating our hypotheses, where we \u201cconcatenate\u201d the verbalization function and inference function induced from different model runs with distinct label words. Assuming H2 (invariant to label remapping) is true, this concatenation should function effectively, enabling the verbalization function to successfully decode the answer representation produced by the inference function and correctly verbalize it, leading to a hypothetical counterfactual output where the answer from the first run appears in the label words of the second run. Their functionalities must remain unaffected by the concatenation for inference function to transfer the answer representation to verbalization function, collectively producing the counterfactual output.\nOperationalize the setting. With our \u201cladder\u201d hypotheses, we can modify the model\u2019s low-level internal representation as a protocol for intervening on the high-level inference function and verbalization function. Specifically, the two\nfunctions being localized separately (H3.1 separate localization) enables us to isolate their causal effects, and H3.2 (consistent localization) makes it possible to conduct the intervention with the counterfactual value of the representation\u2014the value it would take under alternative input scenarios in the same position\u2014rather than a value the neuron might never actually assume. With these, our objective becomes to find a layer (or sequence of layers), lmid, such that taking the last token representation from one run and plugging it into another would change the model\u2019s output in a way that reflects the hypothetical counterfactual condition. Interpreting results. Our hypotheses are supported if the model consistently generates a sufficient proportion of counterfactual outputs. Additionally, this will also provide information about the specific (though not exact) locations of the two functions within the model\u2014lmid should occur after the layer where the answer representation is sufficiently developed. This, however, does not tell us the exact starts and ends of the two functions. It is possible that only a subset of layers before lmid is actively involved in implementing the inference function and after lmid is actively involved in implementing the verbalization function, while some others may be redundant. Our hypotheses can be weakened by the absence of evidence, i.e., no specific layer found to allow effective concatenation of the functions; and they can falsified if our interventions yield consistent results that contradict our predictions, which may indicate that the model systematically employs mechanisms different than those hypothesized.\n# 2.2 Using Interchange Interventions to Realize Counterfactual Scenarios\nInterchange intervention. We will first introduce the formulation of interchange intervention (Geiger et al., 2020, 2021, 2022, 2024a,b; Huang et al., 2024) and then apply it to our context. Consider a model M that takes an input string x and generates an output string y. We denote the entire set of internal representations of the model M created during this inference as M(x) and the predicted token y = \u03c4(M(x)). Let\u2019s conduct interchange intervention on a set of intermediate representations Z of the model M by replacing them with the value of z and denote the post-intervention model as MZ\u2190z. The difference between \u03c4(M(x)) and \u03c4(MZ\u2190z(x)) manifests\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e42/2e4299c6-f3a7-4921-8e98-e43e7a7a67ad.png\" style=\"width: 50%;\"></div>\nFigure 3: NLI experiments with remapped label spaces. The y-axis is the rate of the output label flipped into the hypothetical counterfactual output label (\u201cflip rate\u201d) and the x-axis is the intervened layer. Each curve represents the flip rates observed when the intervened model is prompted with the corresponding label space. The background bars represent the flip rates of the default \u2190default baseline, where the intervened model is also prompted with the default label space (\u201ctrue\u201d/\u201cfalse\u201d). Due to their unsatisfactory ICL performance, we do not conduct intervention experiments with the two 7-billion models on ANLI (see more discussion on this in Section 2.2).\nthe causal contribution of Z to the model behavior. To get the intended value of z, we use GetVals to retrieve the representation values that the variables Z would have taken on if M processes another input x\u2032, denoted as GetVals(M, x\u2032, Z). With the two operations, we obtain the post-intervention version of the model, MZ\u2190GetVals(M,x\u2032,Z), where the values of Z set as those obtained by processing x\u2032. We refer to M(x\u2032) as the source model since it provides the desired values of z and M(x) as the intervened model whose internal representations are to be modified. Thus, we get the output of the intervened model after the interchange intervention: IntInv(M, x, x\u2032, Z) := \u03c4 \ufffd MZ\u2190GetVals(M,x\u2032,Z)(x) \ufffd . In our study, we want the intervention to reroute the computational graph of the source model to the intervened model during the last l layers, to achieve a \u201cconcatenation\u201d of functions from different runs (illustrated in Figure 2). We achieve this by setting the variables Z to be the representations of the last token of the input context. The causal attention mask of the decoder-only architecture of the LMs we study ensures that no intermediate representation of a token is influenced by its successors, allowing interventions on the last token without altering the computational graph of preceding tokens. Thus, the intervened model will apply the same function during the last l layers for both its original representation and the plugged-in representation.\nFormulate counterfactual outputs. An ICL prompt x is composed of (1) a set of demonstrations {x(j) demo, y(j) demo}k j=1, where the example-label relations are intended to determine the task to be inferred, such as NLI, and y(j) demo in the output label space Y, such as {\"true\",\"false\"} where each element corresponds to the positive and negative label respectively, (2) a test example xtest, and (3) a prompt template (see Figure 1, Top). In Section 2.1, we imagine a counterfactual scenario where we concatenate two functions, each induced by M(x) and M(x\u2032). The hypothetical counterfactual output here reflects that the answer comes from the model M inferring x\u2032 and appears in the label words specified in x. Note that when experimenting with remapped label spaces (Figure 2 and Section 3.1), we take the upstream inference function from the source model (with default label words) to pass information to the downstream verbalization function of the intervened model, not the reverse. To make the causal effects of our intervention observable on the behavioral level, we set up (x, x\u2032) pairs to ensure that the hypothetical counterfactual output always differs from the original output of the intervened model. For example (illustrated in Figure 2), consider an input x\u2032 with the default label words \u201ctrue\u201d/\u201cfalse\u201d and an input x with the irrelevant label words \u201ccat\u201d/\u201cdog\u201d. If feeding x\u2032 to the source model yields an output\n\u03c4(M(x\u2032)) = true, we will sample the corresponding x to construct our intervention experiment from the subset {x | \u03c4(M(x)) = dog} where the intervened model outputs \u201cdog\u201d. This enables us to observe whether the output of the intervened model changes from \u201cdog\u201d to the counterfactual output \u201ccat\u201d. This construction requires the test example in x and x\u2032 to differ from each other, which we will justify later in this section. One interesting property from the \u201cconcatenation\u201d view is that we can interpret the \u201cflip\u201d from two perspectives: (1) the output of the intervened model flips from \u201cdog\u201d to \u201ccat\u201d, manifesting the causal effects of inference function, or (2) the output of the source model flips from \u201ctrue\u201d to \u201ccat\u201d, manifesting the causal effects of verbalization function.\nEvaluate causal effects. We measure the effectiveness of our interchange intervention based on the flip rate, which is straightforwardly defined as the percentage of cases in which the intervention succeeds in making the intervened model\u2019s output flipped to the hypothetical counterfactual output. The strength of the evidence supporting our hypotheses is proportional to the flip rate: \u226570% is considered adequate, and \u226590% is considered very strong. We further construct a default \u2190default baseline where the intervention is conducted on the intervened model and the source model that are both prompted with the default label words. The counterfactual output is simply a flipped label word: if the intervened model\u2019s original output is \u201ctrue\u201d, then the counterfactual output is \u201cfalse\u201d. We consider the processes the model implements at certain layers to be shared across settings with remapped label spaces if the corresponding flip rates are close to this baseline; conversely, if the flip rates diverge from this baseline, we conclude that the model uses different functions during these layers. In this case, the diverging processes are very likely to perform causal roles related to the label words.\nUse different test examples. Another implication of differentiating the test examples in x and x\u2032 is it prevents the intervened model from ignoring the plugged-in representations and just solving the task on its own by attending to the test example in its input contexts. We control that only the source model \u201cknows\u201d the test example, ensuring that the concatenated model produces the\ncounterfactual output only if it correctly verbalizes the answer passed on by the inference function.\nFiltering out cases with low ICL accuracy. We generally focus on a subset of cases where the model achieves adequate ICL accuracy (above 0.7). This serves dual purposes: first, to verify if these results are reproducible on base models, and second, to ensure that the model\u2019s successes are not merely due to chance, but rather a result of systematic causal mechanisms developed to address the task. We discuss how we adjust the prompting strategy to improve the ICL performance without explicit instructions in Appendix D and explore the low ICL cases in Appendix C.\n# 3 Experiments\n# 3.1 Experiment with Remapped Label Spaces\nModulate the label words. For each task, we experiment with a diverse set of label words. For example, in addition to the default label words\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/54bd/54bd2864-84e4-4aa4-bf42-98f66454499f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Experiment with remapped label spaces on IMDb and AGNews. We perform the same experiment with remapped label spaces on tasks other than NLI. Different sets of label spaces are used for IMDb and AGNew to accommodate the semantic variations of the label words.</div>\nused for NLI (\u201ctrue\u201d/\u201cfalse\u201d), we select (1) the task-related label words \u201centailment\u201d/\u201cnot entailment\u201d and (2) a set of task-irrelevant label words, including those generally conceived and used as binary pairs (\u201cyes\u201d/\u201cno\u201d and \u201cgood\u201d/\u201cbad\u201d) and those that are less obviously binary or used in contrasting contexts (\u201ccat\u201d/\u201cdog\u201d, \u201cfoo\u201d/\u201cbar\u201d, and \u201cblack\u201d/\u201cwhite\u201d). We list all label words in Table 1. Findings of counterfactual outputs. We will examine both the patterns of flip rates (Figure 3) and the distribution of output tokens (Figure 2, bottom; Appendix E). Below, we discuss these results based on a representative setting: the experiments on GEMMA-7B, as illustrated in Figure 2. During the early layers (1-17), flip rates are generally very low. Such ineffective intervention suggests that the answer representation is not yet sufficiently developed to be decoded by the other verbalization function. This can be validated by the distribution of output tokens: the intervened model mostly generates \u201cdog\u201d, which is the original output of the intervened model. During the middle layers (18-20), we observe that nearly all settings achieve a flip rate of at least 70%. This means our interventions on these layers can effectively achieve the counterfactual scenario, that is, create a \u201cconcatenated\u201d model that outputs the counterfactual output \u201ccat\u201d. This success depends on the verbalization function of the intervened model correctly decoding representations from the inference function of the source model and maintaining a correct mapping function to the label space. During the late layers (21-27), the model outputs\nare gradually dominated by the original outputs of the source model (\u201ctrue\u201d instead of \u201ccat\u201d). It is possible that the intervention may have bypassed the starting point of the verbalization function, which means the representations can no longer be successfully transformed into the intended label space.\nFindings of generalizability. We experiment with three groups of tasks and datasets, including: the primary setting MultiNLI (Figure 3, Top), other NLI tasks such as RTE and ANLI (Figure 3, Mid and bottom), and other tasks such as sentiment analysis on IMDb and topic classification on AGNews (Figure 4). The patterns\u2019 generalization to other NLI datasets means they are not just specific to some constructs in MultiNLI; the generalization to IMDb means that they are not specific to NLI but perhaps to classification tasks in general; and the generalization to AGNews means that they are not specific to binary classification tasks but are shared even in multi-class classifications. Caveat. An alternative explanation for the results is that the intervened model ignores the pluggedin representations entirely and just re-solves the task on its own, given that the plugged-in representation may fully represent the test example. In this case, what has been transferred in the intervened representations is merely a task-agnostic representation of the test example. To address this concern, we conduct a complementary experiment to induce changes in the inference function with constructed alternative tasks on MultiNLI, where the example-label relations are changed and the input and label spaces are fixed (see Section 3.2). So\nFindings of generalizability. We experiment with three groups of tasks and datasets, including: the primary setting MultiNLI (Figure 3, Top), other NLI tasks such as RTE and ANLI (Figure 3, Mid and bottom), and other tasks such as sentiment analysis on IMDb and topic classification on AGNews (Figure 4). The patterns\u2019 generalization to other NLI datasets means they are not just specific to some constructs in MultiNLI; the generalization to IMDb means that they are not specific to NLI but perhaps to classification tasks in general; and the generalization to AGNews means that they are not specific to binary classification tasks but are shared even in multi-class classifications.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1eed/1eed8875-5e85-49f3-8cbb-bcc32a9456b5.png\" style=\"width: 50%;\"></div>\nFigure 5: Experiment with reconstructed tasks on MultiNLI. Curves represent the flip rates of different settings where the intervened model is prompted with alternative tasks. The intervened model always sees the default task. The background bars represent the flip rates of default \u2190default baseline, where both the intervened model and the base model are prompted with the default task (NLI).\nfar we can only conclude that the computation after the middle layers, e.g., after layer 18 for GEMMA7B, at least performs verbalization.\nFurther interpretation of the results of multiclass classification. Flip rates sometimes decrease earlier on AGNews than in binary classification. Why? We analyze the distribution of output tokens (Appendix E). In binary classification datasets, the output tokens are typically dominated by the counterfactual output during middle layers, which we labeled as \u201cflipped\u201d tokens; subsequently, these tokens are increasingly replaced by those from the source model, which we refer to as \u201coverwritten\u201d tokens. However, on AGNews, we observe that during the middle layers: (1) a significant proportion of \u201cflipped\u201d tokens become \u201coverwritten\u201d tokens, and (2) the remainder are tokens not predefined as default or irrelevant, referred to here as \u201cother tokens\u201d (Figure 13 and Figure 14). This behavior does not contradict our hypotheses. It is possible that the inference function is implemented in the same layer as in binary settings, evidenced by the peak in flip rates around similar layers in both binary and AGNews classification; while the verbalization function starts in earlier layers in multi-class classification, whose initialization is marked by the increasing dominance of \u201coverwritten\u201d tokens. Meanwhile, a further investigation into the \u201cother tokens\u201d reveals that they seem to still encode the correct information of the answer and be decoded into tokens that are semantically similar to the default ones. For example, \u201cother tokens\u201d includes \u201cscience\u201d and \u201ctechnology\u201d,\nboth are synonymous with the default label word \u201csci/tech\u201d, and the token \u201cpolitics\u201d will pick up examples similar to those labeled by \u201cworld\u201d in the default label space. Together, the findings suggest that the intervened model can still correctly decode the answer representation transmitted by the inference function of the source model, but its implementation of the verbalization function is more sensitive to our interventions.\n# 3.2 Experiment with Reconstructed Tasks on MultiNLI\nPurpose. To address the potential concern mentioned earlier, we design a complementary experiment to test if the plugged-in representations are task-agnostic. We want to control that the intervened model can only know about the intended tasks (that can lead to the counterfactual output) by decoding a task-relevant representation from the source model, i.e., the representations we take from the source model and plug into the intervened model. We conclude that the plugged-in representations are not task-agnostic if the model can still achieve high flip rates; and this conclusion can be strengthened if the high flip rates occur in a similar location as in the previous experiments with remapped label spaces, that is, the results validate the location of the inference function.\n# Set desiderata for alternative tasks.\nthe alternative tasks that (1) maximally preserve the input space to allow the model to continue employing similar mechanisms, (2) secure a balanced dataset with an adequate number of examples for both positive and negative labels, and (3) allow\nthe model to attain sufficient ICL accuracy, which means the model has been redirected to the new tasks and developed corresponding abstractions. The criterion for ICL accuracy is less stringent than the experiment with remapped label spaces given the challenges of re-purposing a dataset designed for another task. Construct alternative tasks. To achieve these conditions, we want to find suitable tasks that are typically simple, deterministic functions, especially those based on shallow linguistic features or those that capture the ambiguity or heuristics exploited by the model. Following Si et al. (2023), beyond the default task NLI, we construct two domain classification tasks using existing metadata of the MultiNLI and two lexical classification tasks reflecting the simple syntactic heuristics the model may exploit to achieve high performance on NLI tasks (McCoy et al., 2019). We list them in Table 7. Apply interchange Intervention. In experiments, we always feed the NLI task to the intervened model and use \u201ctrue\u201d/\u201cfalse\u201d for the label space (illustrated in Figure 7). We define different tasks with the same sets of demonstration examples, but use different example-label relations for each task. Specifically, each data point is assigned with two labels, with one for the NLI task and another for the alternative task. In addition, we prepend instructions to each example, adopted from (Si et al., 2023) (summarized in Table 8), since none of the prompting templates without explicit instructions we have tried can lead to adequate ICL performance. The default \u2190default baseline is created by using the NLI task for both the intervened and the source model.\nFindings of consistent locations. As shown in Figure 5, the flip rates surge at the same layer in both the experiment with remapped label spaces and with reconstructed tasks. Given that the intervention design ensures only the source model has access to the correct task\u2014which leads to the hypothetical counterfactual output\u2014a high flip rate is possible only if the plugged-in representation has already encoded information about this counterfactual output. This means that the early-stage layers of the upstream source model have already performed the task, i.e., implemented the inference function. Thus, the results validate the location of the inference function and thus help us address the aforementioned concern.\nCaveat. The evidence coming from this experiment will only serve complementary roles, and its strength is weaker than the experiment with remapped label words. First, it is hard to control what has been changed when intervening on the upstream process; and second, the patterns here are less consistent (alternative tasks can have flip rates varying in values and locations where they peak). However, these do not contradict our hypotheses, as they do not require high and consistent flip rates in this setting. The goal of this experiment is to verify the early-stage layers do perform inference function, and for this purpose, it is sufficient to observe that in all settings (except one for MISTRAL-7B-V0.3), the flip rates with reconstructed tasks rise significantly above the random baseline (0.5) before the flip rates in the experiment with remapped label spaces drop to 0. The inconsistencies of the locations where the flip rates peak are expected, since changing the task likely causes significant changes in the model\u2019s internal circuits and states used to solve the tasks (even though these tasks might share some sub-processes), making the representations less transferable.\n# 4 Discussion\nValidation of H1: Causal mechanism. This hypothesis is confirmed if we can \u201cconcatenate\u201d an inference function with another verbalization function and observe the generation of the counterfactual output that reflects the answer obtained by the inference function and the corresponding label space of the verbalization function. We achieve such a scenario with the experiment with remapped label spaces, as manifested by the high flip rates we observe during the middle layers of the model. Concern may be raised that the intervened model just re-solves the task on its own during the downstream processes, that is, both inference function and verbalization function are performed in the middle and the late layers. Our experiment with alternative tasks shows that this is not true. This experiment ensures that only the source model knows the answer that leads to the hypothetical counterfactual output, which means high flip rates can only be achieved if the plugged-in representation involves a sufficiently developed representation of the answer. Therefore, the high flip rates we observe during and after the middle layers support the hypothesis that the inference function\nis implemented by the upstream processes (before the layers with effective interventions).\nValidation of H2: Invariant to label remapping. From the results of the experiment with remapped label spaces, we know that intervening on a sequence of middle layers in the model can successfully achieve the counterfactual scenario, where the inference function can be concatenated with another verbalization function to produce the hypothetical counterfactual output. This indicates that the inference function can play its causal role invariant to the downstream verbalization function.\nValidation of H3: Separate localization and consistent localization. This hypothesis is a necessary condition for us to use interchange intervention to realize the counterfactual scenario, and thus it gets validated if our interventions are effective. It can be further supported by the observation that for each model, the trend of flip rates is similar across different datasets and tasks. For example, flip rates surge at the same or very close layers (Figure 3 and 4). We summarize the start and the end of the layers enabling effective interventions in Table 5.\n# 5 Related Work\nUnderstanding ICL. A variety of work has sought to better understand how language models perform in-context learning. Min et al. (2022) show that ICL performance on a variety of text classification tasks is minimally affected by randomly changing the demonstration labels, hypothesizing that demonstrations may primarily help with determining the label space (i.e., the verbalization function), the distribution of the input examples, and how the output text should be formatted. (Xie et al., 2022) hypothesize that LMs learn latent tasks during pre-training, primarily using ICL demonstrations to identify which latent task is most pertinent to the provided ICL input. Other work (Aky\u00fcrek et al., 2023; von Oswald et al., 2022; Dai et al., 2022) hypothesizes that LMs perform implicit gradient descent on a latent model during ICL. Wei et al. (2023) and Pan et al. (2023) confirmed these observations and further indicated that this consistency depends on the size of the model. Our study offers a deeper analysis of these behaviors at the representational level. Wei et al. (2023) study how model size affects whether LMs prefer to rely on semantic priors about a task, as opposed to the\nexample-label mappings provided in the demonstrations. They find that smaller models tend to rely on semantic priors during pre-training, since they ignore flipped labels presented in the ICL demonstrations. In contrast, larger models make use these flipped labels, indicating that they rely less on their semantic task priors. Pan et al. (2023) disentangle task learning and task recognition in in-context learning; intuitively, LMs use their priors to perform novel tasks during ICL (since a few demonstrations are unlikely to provide complete information about a complex task), but they also do not completely rely on their task priors (since they can handle ICL settings with arbitrary label mappings). Implicit functions in language models. Another line of prior work has sought to characterize what functions might be implicitly implemented by LMs. Olsson et al. (2022) identify \u201cinduction heads\u201d, a unique type of attention head that replicates repeating patterns from prior contexts. Hendel et al. (2023) argue ICL compresses the demonstrations into a task vector, which is then used to generate the output for the ICL input. Merullo et al. (2023) provide evidence that Transformer LMs perform ICL in three layer-wise stages: argument formulation, function application, and saturation. Todd et al. (2024) show that ICL creates function vectors of the demonstrated task, and these function vectors can trigger execution of the task in other settings (e.g., zero-shot prediction or natural text).\n# 6 Conclusion\nWe hypothesize that when LMs perform ICL with irrelevant or misleading label words, they first apply an inference function to obtain a representation of the answer and then apply a verbaliza tion function to verbalize the answer as one of the label words specified in the demonstrations (H1). In addition, the inference function is invariant to remappings of label words (H2), and both the functions can be localized separately and consistently within certain layers of the model (H3). To validate our hypotheses, we design experiments based on interchange intervention to realize counterfactual scenarios that would only occur if all of our hypotheses hold true. Our experiments across a variety of tasks, datasets and models indicate that our hypotheses hold across a variety of settings. Our findings contribute to a growing body of work on mechanistically understanding how language models perform in-context learning.\n# 7 Limitations\nCoverage of models and tasks. Since our intervention study requires access to the model\u2019s intermediate representations, we can only experiment with open-source models. There is no guarantee that similar findings can be observed in the stateof-the-art close models, and how the MoE architectures will affect our findings remains unclear. In addition, we acknowledged that we only focus on classification tasks on which the notion of \u201cverbalization\u201d or remapping of label spaces can be more naturally defined. Future studies could extend to more complex tasks, such as those that involve answering open-ended questions.\nDoes the model truly carry out natural language inference when processing datasets like MultiNLI, RTE, and ANLI? Our current evidence does not allow us to definitively answer this question, and the notion of \u201ctextual entailment\u201d itself is not without its controversies. We cannot claim about what the model is actually doing during the layers of the inference function. Moreover, when experimenting with reconstructed tasks on MultiNLI, we do not know the true relationships between the alternative tasks (lexical and domain classification) and NLI. Nonetheless, these ambiguities do not undermine our argument\u2014we do not assert that the model performs an \u201cNLI-inference\u201d function on NLI datasets. By inference function we refer to the (general) processes the model uses to derive the answer representation.\nPredicting ICL performance. Prior studies have shown that the model\u2019s ICL performance can be (sometimes significantly) harmed by the irrelevant labels (Min et al., 2022; Pan et al., 2023; Wei et al., 2023). We do not conduct a systematic analysis and explanation of how different factors\u2014such as prompt templates, label word choices, and the number of shots\u2014causally contribute to the model\u2019s performance on ICL tasks with irrelevant labels. A nuanced understanding of the underlying mechanism would require analysis tailored to each model and setting. In particular, our current research does not address which specific features of label words influence ICL performance positively or negatively, which could be an interesting topic to explore.\nUnknown training data. Concern arises whether the model\u2019s good performance on ICL tasks with irrelevant label words stems from its prior exposure\nto similar cases. We cannot rule out the possibility of data leakage during pre-training, primarily because we do not have access to the pretraining data of the models we study. We are hopeful, however, that our use of a diverse range of irrelevant labels in our prompts reduces the likelihood that these specific cases were present in the pre-training dataset, thus alleviating potential concerns.\n# 8 Acknowledgements\nWe would like to thank Atticus Geiger, Jing Huang, Jacqueline Harding, Zhengxuan Wu, Kara Schechtman, Junlin Wang, Jiarui Lu, Zhihan Li, and the anonymous reviewers for feedback and discussions that helped improve this work. This research was enabled in part by the computational resources provided by Mila (https://mila.quebec) and NVIDIA.\n# References\nEkin Aky\u00fcrek, D. Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations with linear models. In Proc. of ICLR. Guillaume Alain and Yoshua Bengio. 2016. Understanding intermediate layers using linear classifier probes. ArXiv:1610.01644. Elias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard. 2022. On Pearl\u2019s Hierarchy and the Foundations of Causal Inference, 1 edition, page 507\u2013556. Association for Computing Machinery, New York, NY, USA.\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207\u2013219.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv: 2005.14165.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? an analysis of BERT\u2019s attention. In Proc. of BlackboxNLP.\ncan GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers. ArXiv:2212.10559.\nbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur \u00c7elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong\nXiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm\u00e1n, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir\nPatel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, V\u00edtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The Llama 3 herd of models. ArXiv:2407.21783.\nAtticus Geiger, Kyle Richardson, and Christopher Potts. 2020. Neural natural language inference models partially embed theories of lexical entailment and negation. In Proc. of BlackboxNLP.\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D. Goodman, and Christopher Potts. 2022. Inducing causal structure for interpretable neural networks. ArXiv:2112.00826. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. 2024b. Finding alignments between interpretable causal variables and distributed neural representations. ArXiv:2303.02536. Jacqueline Harding. 2023. Operationalising representation in natural language processing. The British Journal for the Philosophy of Science. Roee Hendel, Mor Geva, and Amir Globerson. 2023. In-context learning creates task vectors. ArXiv:2310.15916. Paul W. Holland. 1986. Statistics and causal inference. Journal of the American Statistical Association, 81(396):945\u2013960. Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. 2024. RAVEL: Evaluating interpretability methods on disentangling language model representations. ArXiv:2402.17700. Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. 2018. Visualisation and \u2018diagnostic classifiers\u2019 reveal how recurrent and recursive neural networks process hierarchical structure. Journal of Artificial Intelligence Research, 61(1):907\u2013926. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv:2310.06825. Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503\u2013528. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proc. of ACL. Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proc. of ACL. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Proc. of NeurIPS. Curran Associates, Inc. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023. A mechanism for solving relational tasks in transformer language models. ArXiv:2305.16130.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proc. of EMNLP.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proc. of ACL.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and induction heads. arXiv:2209.11895.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. 2023. What in-context learning \"learns\" in-context: Disentangling task recognition and task learning. In Findings of ACL.\nChenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. 2023. Measuring inductive biases of in-context learning with underspecified demonstrations. In Proc. of ACL.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee,\nKathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv: 2403.08295. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proc. of ACL. Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. 2024. Function vectors in large language models. In Proc. of ICLR. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. In Proc. of NeurIPS. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2022. Transformers learn in-context by gradient descent. In Proc. of ICML. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of BlackboxNLP. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. ArXiv:2303.03846. Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proc. of NAACL. James F. Woodward. 2003. Making Things Happen: A Theory of Causal Explanation. Oxford University Press, New York. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In Proc. of ICLR.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Proc. of NeurPS.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ef9d/ef9dd41e-536e-43dd-b672-763084132bba.png\" style=\"width: 50%;\"></div>\nFigure 6: Probing setting and results. Top: an illustration of the probing experiment, where we train a probe on the last token presentation to predict the NLI label. Bottom: results of the probing study. The first graph shows the probing accuracy when the probe is trained and evaluated on the default setting; while the second graph displays the probing accuracy when the probe is trained on the default setting and evaluated on other settings with different label words. By comparing the out-of-distribution curves (in the second graph) with the in-distribution curve (in the first graph), we can see an obvious bifurcation starting around the 18th-20th layer. This indicates (1) that the representation of the answer has been fully developed around these layers, and (2) the first process (before the bifurcation point) is roughly invariant to the changes in label space, while the second process (after the bifurcation point) is heavily dependent on the label space.\nTo identify the low-level implementations that align with our hypothesized high-level abstractions, we start with identifying processes shared across different model runs and then verify if they satisfy all properties we hypothesize. As a proxy, we conduct a pilot study where we use probing to examine how the intermediate representations, i.e., the neural activations, correlate with the high-level concept of the answer to the ICL task and the label\nProbe. A probe (Alain and Bengio, 2016; Peters et al., 2018; Tenney et al., 2019; Clark et al., 2019; Hupkes et al., 2018) is usually a linear classifier or shallow MLP that takes the intermediate representation as the input and output labels for some property, aimed at testing how easily the representations can be linearly separated. A high probing accuracy on a hold-out test set indicates that the information about the property is encoded in the intermediate representation. Experimental details. We implement probes as logistic regressions with Scikit-learn (Pedregosa et al., 2011) and the L-BFGS optimization algorithm (Liu and Nocedal, 1989). We train one probe for each layer with representations generated on the RTE training set by GEMMA-7B. Probes are then applied to the RTE validation set with different label words. Results are averaged over three trials with different sets of demonstrations. We repeat this process for each layer. Experiments. We train probes to predict the output labels on the last token representations of one run with default label words (\u201ctrue\u201d/\u201cfalse\u201d) in the NLI task. We first apply to probe to the representations produced by runs with default label words to observe the development of the representation of the output throughout the forward pass. Then, we test if this probe can generalize to the representations produced by runs with remapped label words (e.g., when the default label space \u201ctrue\u201d/\u201cfalse\u201d is remapped to \u201ccat\u201d/\u201cdog\u201d). This aims to identify if there are shared processes in runs with different label words. Specifically, the process on certain layers is interpreted as being shared if the probing accuracy on these layers are similar across different runs; on the contrary, if the probing accuracy on certain layers of one run diverges from the probing accuracy on another run, we conclude that the model implements different processes on these layers in each run. Findings. For probes trained and tested on the runs with default label words (Figure 6, Middle), the probing accuracy is monotonically increasing, suggesting the development of the representation of the output label. For probes trained on the runs with default label words and tested on runs with different label words, the probes achieve high accuracy on other label words in layers 15-20 and then drop to a random baseline in the last 8 layers. In other\nFindings. For probes trained and tested on the runs with default label words (Figure 6, Middle), the probing accuracy is monotonically increasing, suggesting the development of the representation of the output label. For probes trained on the runs with default label words and tested on runs with different label words, the probes achieve high accuracy on other label words in layers 15-20 and then drop to a random baseline in the last 8 layers. In other\nwords, the probing accuracy on different runs first converges and then diverges. These results indicate that the information about the NLI labels is encoded at least in two different patterns across the model\u2019s forward pass. In the middle layer, where the probe can generalize, the representations share a similar structure in relation to the NLI label, enabling the probe trained on the default label words to still correctly classify in the other label words cases. In the late layers, the representation structures start to vary based on the label words, causing a decline in generalization performance. From these results, we can categorize the model\u2019s intermediate representations into two groups: one that is sensitive to label words and another that is not. Given these findings, it is natural to assume that the processes that generate the representations share the same characteristics. That is, the first process implemented by the middle layers is inference function related since it produces unified representations of the task target. The second process implemented by the late layers is verbalization function related, as its intermediate representation structure varies across different label words. Caveat. We note that probing results only tell us whether the representations encode certain information without guaranteeing that these representations indeed play causal roles in the generation of certain model\u2019s behaviors, i.e., used by the model (Belinkov, 2022), or genuinely represented in the layers where they are detected (Harding, 2023). Claiming about the causal effects we want to study will require further hypotheses and experiments introduced later in the paper. Nonetheless, we take the probing results as an initial point, and the three key messages delivered: (1) the two functions may be located separately, (2) if located separately, the inference function comes first then the verbalization function, and (3) the location of the two functions may be consistent across settings. Clarification of \u201clocalization\u201d. We do not commit to the notion of \u201cverbalization layer\u201d, i.e., layers that are active exclusively in response to label words in ICL, resembling the classic example \u201cgrandmother cell\u201d in neuroscience. We do not follow the traditional view of located representation positing that a specific vehicle of the representation\nfind and we will claim is that the model perform or implement the high-level verbalization function consistently in certain layers under the background conditions we test (prompting strategies, etc).\n# B Illustrations of the Reconstructed Task Intervention\nFigure 7 illustrates and summarizes our reconstructed task intervention outlined in Section 3.2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5887/5887963c-7d4d-4d42-a4cd-0868765ef420.png\" style=\"width: 50%;\"></div>\nFigure 7: Experiment with reconstructed tasks on MultiNLI. (1) We induce changes in the inference function by prompting the model to perform a different task on the same input space. (2) We perform intervention by taking the representation from the sourced model (prompted by the alternative task) and replacing the representation of the intervened model (prompted by the NLI task) at layer l. (3) Intervention effects are evaluated by calculating the matching rate between the intervened output Y \u2032\u2032 and the hypothetical counterfactual output Yc.\n# C Implications of Poor ICL Performance\nWe do not expect to observe effective intervention results on cases where the ICL performance is inadequate, and such cases will not weaken our argument. This is because the poor ICL performance is very likely to come from the model\u2019s failure to develop the necessary abstractions for solving the task, which would include recognizing the task, inferring the correct answer, recognizing the label space, and verbalizing the answer correctly. If the model achieves high ICL performance with some\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e969/e9695414-feb3-4968-808f-e857ec531e96.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Exploration of negative cases where the ICL performance is low. Top: flip rates of intervention with remapped label spaces. Bottom: Distribution of predicted token from the intervened model with the underperforming label words.</div>\nlabel spaces while not with others, it may employ different sets of abstractions to solve these two ICL tasks. Nonetheless, we intentionally include some cases where the model fails to solve the ICL task, as complementary results (ICL performance is summarized in Table 5 with the low accuracy colored in orange or red). In our experiments, we find that (1) if the ICL accuracy is good enough, e.g., \u22650.7, most settings will show a strong pattern, as summarized before; while some inconsistencies may exist, though rarely. One example is the case where MISTRAL-7B-V0.3 is used on AGNews with the label words with the label words \u201ccat\u201d/\u201cdog\u201d, as shown in Figure 8, second column); (2) if the ICL is not good enough (around 0.6 for binary classification and around 0.4 for four-class classification), we find the flip rates still generally follow the same trend, though the patterns are weaker than settings with high ICL accuracy due to the lack of necessary abstractions developed. Examples include: cases with the label space \u201cfoo\u201d/\u201cbar\u201d and \u201cblack\u201d/\u201cwhite\u201d for GEMMA-7B and MISTRAL-7B-V0.3 on RTE (Figure 3, second row) and \u201ccat\u201d/\u201cdog\u201d/\u201cdeer\u201d/\u201cbird\u201d for MISTRAL7B-V0.3 on AGNews (Figure 8, first and third column).\n# D Prompting Strategy to Improve ICL Performance\nWe slightly adjust the selection of irrelevant label words and prompting strategies for each setting, to facilitate the model in achieving adequate ICL performance. We observe that reproducing the phenomena of\ninterest on base models can be challenging with some settings. For instance, 7-8 models on ANLI are difficult to handle with irrelevant label words despite performing adequately with default label words. And AGNews is generally hard for all models because it is a four-class classification and naturally requires more shots. We adjust our template selection based on the corresponding ICL performance. We always start with a 16-shot demonstration with the \u201csentence\u201d template, aiming for ICL performance above 0.7. If this benchmark is not met, we increase the demonstration to 32 shots. Should challenges persist, we modify the template to \u201csent_label\u201d and consider adding specific keywords in front of the answer to implicitly hint for the intended tasks, such as \u201ctopic:\u201d for AGNews and \u201csentiment:\u201d for IMDb. Note on the notion of generalizability. By the generalizability of our findings, we do not mean that a single set of irrelevant labels should work universally across all models in all settings. Again, our study of causal mechanisms focuses on cases where the model achieves high ICL performance, which means it develops the necessary causal abstractions required to solve the task we want to study. Since LMs are sensitive to contexts and to different prompting strategies, the required settings unsurprisingly vary model by model.\n# Address concerns about cherry-picking. C\ncerns may arise that our selection of successful cases constitutes cherry-picking. However, we select cases based on their ICL performance, not their intervention results, and we test a variety of irrelevant label words that are representative of broader cases. We also discuss cases where ICL perfor-\nmance is not good and find that they still generally follow the patterns, though they do so to a lesser extent. (See Appendix C).\n# E Predicted Tokens from the experiment with remapped label spaces\nWe present the full results of the intervened model\u2019s prediction distributions on all five datasets used in our intervention with remapped label spaces: MultiNLI (Figure 9), RTE (Figure 10), ANLI (Figure 11), IMDb (Figure 12), AGNews (Figure 13 & 14).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5d33/5d33cf27-ac3f-4aa2-a652-63415c43e364.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Full results of intervened model\u2019s prediction distributions on the MultiNLI dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48ec/48ec5c25-118d-4997-a7b3-6ee4350275e6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">10: Full results of intervened model\u2019s prediction distributions on the RT</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb5b/cb5b2245-9a31-4a21-8921-b5fa961d5a1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/96d4/96d4c411-728f-40d5-af4d-36a5c6afb5b7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Full results of intervened model\u2019s prediction distributions on the IMDb dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fac6/fac63619-1168-419f-8608-1abce014998e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Model = Mistral-7B-v0.3 not flipped</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45a0/45a06949-6cd5-481c-928d-e538dcbd09b4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Label words (single-token) = \"foo\"/\"bar\"/\"baz\"/\"qu\" Label words (single-token) = \"foo\"/\"bar\"/\"baz\"/\"qu\"</div>\nFigure 14: Full results of intervened model\u2019s prediction distributions on the AGNews dataset. Part 2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c01/0c018b5b-92ca-47d1-9e2c-e4a458ac7339.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Label words (single-token) = \"foo\"/\"bar\"/\"baz\"/\"qu\" Label words (single-token) = \"foo\"/\"bar\"/\"baz\"/\"qu\"</div>\nPredicted Single Tokens\nDataset\nVerbalizers\nGEMMA-7B\nMISTRAL-7B-V0.3\nGEMMA-2-27B\nLLAMA-3.1-70B\nMultiNLI/\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"true\"/\"false\"\nRTE/\n\"en\"/\"not\"\n\"en\"/\"not\"\n\"ent\"/\"not\"\n\"en\"/\"not\"\n\"ent\"/\"not\"\nANLI\n\"yes\"/\"no\"\n\"yes\"/\"no\"\n\"yes\"/\"no\"\n\"yes\"/\"no\"\n\"yes\"/\"no\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"foo\"/\"bar\n\"foo\"/\"bar\n\"foo\"/\"bar\n\"foo\"/\"bar\n\"foo\"/\"bar\n\"black\"/\"white\"\n\"black\"/\"white\"\n\"black\"/\"white\"\n\"black\"/\"white\"\n\"black\"/\"white\"\nIMDb\n\"positive\"/\"negative\"\n\"positive\"/\"negative\"\n\"pos\"/\"negative\"\n\"positive\"/\"negative\"\n\"positive\"/\"negative\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"good\"/\"bad\"\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"true\"/\"false\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"cat\"/\"dog\"\n\"foo\"/\"bar\"\n\"foo\"/\"bar\"\n\"foo\"/\"bar\"\n\"foo\"/\"bar\"\n\"foo\"/\"bar\"\n\"water\"/\"fire\"\n\"water\"/\"fire\"\n\"water\"/\"fire\"\n\"water\"/\"fire\"\n\"water\"/\"fire\"\nAGNews\n\"world\"/\"sports\"/\n\"business\"/\"sci/tech\"\n\"world\"/\"sports\"/\n\"business\"/\"sci\"\n\"world\"/\"s\"/\n\"business\"/\"sc\"\n\"world\"/\"sports\"/\n\"business\"/\"sci\"\n\"world\"/\"sports\"/\n\"business\"/\"sci\"\n\"international\"/\"athletics\"/\n\"commerce\"/\"innovation\"\n\"international\"/\"ath\"/\n\"commerce\"/\"innovation\"\n\"intern\"/\"ath\"/\n\"commerce\"/\"inn\"\n\"international\"/\"ath\"/\n\"commerce\"/\"innovation\"\n\"international\"/\"ath\"/\n\"commerce\"/\"inn\"\n\"a\"/\"b\"/\"c\"/\"d\"\n\"a\"/\"b\"/\"c\"/\"d\"\n\"a\"/\"b\"/\"c\"/\"d\"\n\"a\"/\"b\"/\"c\"/\"d\"\n\"a\"/\"b\"/\"c\"/\"d\"\n\"1\"/\"2\"/\"3\"/\"4\"\n\"1\"/\"2\"/\"3\"/\"4\"\n\"1\"/\"2\"/\"3\"/\"4\"\n\"1\"/\"2\"/\"3\"/\"4\"\n\"1\"/\"2\"/\"3\"/\"4\"\n\"foo\"/\"bar\"/\"baz\"/\"qux\"\n\"foo\"/\"bar\"/\"baz\"/\"qu\"\n\"foo\"/\"bar\"/\"b\"/\"qu\"\n\"foo\"/\"bar\"/\"baz\"/\"qu\"\n\"foo\"/\"bar\"/\"baz\"/\"qu\"\nTable 1: Tokenization of verbalizers: LM tokenizers may break a word into sub-tokens. Our intervention metho requires verbalizers to be differentiable by the first sub-token since it only evaluates the first generated token. He we present how each model\u2019s tokenizer break-down the verbalizers.\nIntervention\nTask\nDataset\n# of Test Examples\nOriginal\nExperiments (7-8b models)\nExperiments (27-70b models)\nChange Verbalizer\nNatural\nRTE\n277\n277\n277\nLanguage\nMultiNLI\n9820\n300\n300\nInference\nANLI\n1000\n300\n300\nSentiment Analysis\nIMDb\n25000\n300\n300\nTopic Classification\nAGNews\n7600\n300\n300\nChange Task\nMultiple Tasks\nMultiNLI\n9820\n300\n300\n<div style=\"text-align: center;\">et # of Test Examples Original Experiments (7-8b models) Experiments (27-70b models)</div>\nTable 2: Statistic of datasets: Due to the computational budget, we reduced the amounts of test examples for all datasets other than the RTE to around 300 examples which we believe is sufficient to test the generalizability of our hypothesized functions. Subsets are sampled randomly using a fix random seed 42 from each corresponding task\nGEMMA-7B\nMISTRAL-7B-V0.3\nGEMMA-2-27B\nLLAMA-3.1-70B\nIntervention\nTask\nDataset\nShot\nTemplate\nShot\nTemplate\nShot\nTemplate\nShot\nTemplate\nChange Verbalizer\nNatural\nMultiNLI\n16\nsentence\n16\nsentence\n16\nsentence\n16\nsentence\nLanguage\nRTE\n16\nsent_label\n16\nsent_label\n16\nsentence\n16\nsentence\nInference\nANLI\n/\n/\n/\n/\n16\nsentence\n16\nsentence\nSentiment Analysis\nIMDb\n16\npassage_label\n16\npassage_label\n8\npassage_label\n16\npassage_sentiment\nTopic Classification\nAGNews\n32\ntext_topic\n32\ntext_topic\n32\ntext_topic\n32\ntext_linebreak\nChange Task\nMultiple Tasks\nMultiNLI\n32\nambi_instruct\n32\nambi_instruct\n16\nambi_instruct\n16\nambi_instruct\nTable 3: ICL settings details: we select the combinations of the prompt template and number of demonstration based on the ICL performance. We search the optimal number of shots in {8, 16, 24, 32}. See Table 6 for detail of prompt templates and Table 8 for instructions used in the Change Task setting.\n<div style=\"text-align: center;\">Predicted Single Tokens</div>\nGEMMA-7B\nMISTRAL-7B-V0.3\nDataset\nIntervention\non Label Words\nLabel Words\nsentence 16shots\nsentence 32shots\nsent_label 16shots\nsentence 16shots\nsentence 32shots\nsent_label 16shots\nRTE\ndefault \u2192default\n\"true\"/\"false\"\n0.807\n0.786\n0.819\n0.827\n0.838\n0.813\ndefault \u2192relevant\n\"en\"/\"not\"\n0.798\n0.787\n0.788\n0.755\n0.795\n0.773\ndefault \u2192irrelevant\n\"yes\"/\"no\"\n0.838\n0.795\n0.745\n0.742\n0.826\n0.771\n\"good\"/\"bad\"\n0.780\n0.767\n0.810\n0.721\n0.767\n0.724\n\"cat\"/\"dog\"\n0.574\n0.621\n0.759\n0.558\n0.573\n0.637\n\"foo\"/\"bar\n0.777\n0.761\n0.633\n0.536\n0.539\n0.567\n\"black\"/\"white\"\n0.611\n0.537\n0.620\n0.607\n0.639\n0.609\nGEMMA-7B\nMISTRAL-7B-V0.3\nset\nIntervention\non Label Words\nLabel Words\nsentence 16shots\nsentence 32shots\nsent_label 16shots\nsentence 16shots\nsentence 32shots\nsent_label 16shot\ndefault \u2192default\n\"true\"/\"false\"\n0.807\n0.786\n0.819\n0.827\n0.838\n0.813\ndefault \u2192relevant\n\"en\"/\"not\"\n0.798\n0.787\n0.788\n0.755\n0.795\n0.773\ndefault \u2192irrelevant\n\"yes\"/\"no\"\n0.838\n0.795\n0.745\n0.742\n0.826\n0.771\n\"good\"/\"bad\"\n0.780\n0.767\n0.810\n0.721\n0.767\n0.724\n\"cat\"/\"dog\"\n0.574\n0.621\n0.759\n0.558\n0.573\n0.637\n\"foo\"/\"bar\n0.777\n0.761\n0.633\n0.536\n0.539\n0.567\n\"black\"/\"white\"\n0.611\n0.537\n0.620\n0.607\n0.639\n0.609\nGEMMA-2-27B\nLLAMA-3.1-70B\nDataset\nIntervention\non Label Words\nsentence 8shots\nsentence 16shots\nsentence 8shots\nsentence 16shots\nANLI\ndefault \u2192default\n\"true\"/\"false\"\n0.824\n0.851\n0.854\ndefault \u2192relevant\n\"en\"/\"not\"\n0.801\n0.816\n0.841\ndefault \u2192irrelevant\n\"yes\"/\"no\"\n0.811\n0.843\n0.821\n\"good\"/\"bad\"\n0.758\n0.790\n0.802\n\"cat\"/\"dog\"\n0.628\n0.643\n0.766\n\"foo\"/\"bar\n0.633\n0.728\n0.808\n\"black\"/\"white\"\n0.667\n0.710\n0.794\nTable 4: An (representative) example (RTE for GEMMA-7B and MISTRAL-7B-V0.3; ANLI for GEMMA-2-27B and LLAMA-3.1-70B). Additional information we want to provide; for informing the justification of we choose the setting we are using instead of others we start from \"sentence\" template and 16 shot (for small models) and 32 for large models chosen setting are bold/colored in blue LLAMA-3.1-70B in general does not work well with \"label\" and so we create some\nGEMMA-7B\nMISTRAL-7B-V0.3\nGEMMA-2-27B\nLLAMA-3.1-70B\nDataset\nIntervention\non Label Words\nLabel Words\nICL\nStart\nEnd\nICL\nStart\nEnd\nICL\nStart\nEnd\nICL\nStart\nEnd\nMultiNLI\ndefault \u2192default\n\"true\"/\"false\"\n0.889\n18\n27\n0.842\n15\n31\n0.939\n24\n45\n0.956\n34\n79\ndefault \u2192relevant\n\"en\"/\"not\"\n0.921\n18\n21\n0.870\n15\n18\n0.943\n24\n29\n0.958\n34\n52\n\"yes\"/\"no\"\n0.856\n18\n21\n0.820\n15\n20\n0.944\n24\n31\n0.959\n34\n53\n\"good\"/\"bad\"\n0.834\n18\n21\n0.747\n15\n20\n0.913\n24\n31\n0.950\n34\n52\ndefault \u2192irrelevant\n\"cat\"/\"dog\"\n0.702\n18\n20\n0.691\n15\n19\n0.912\n24\n29\n0.949\n34\n52\n\"foo\"/\"bar\n0.770\n18\n21\n0.616\n15\n17\n0.908\n24\n29\n0.950\n34\n52\n\"black\"/\"white\"\n0.701\n18\n20\n0.772\n15\n19\n0.906\n24\n28\n0.944\n34\n52\nRTE\ndefault \u2192default\n\"true\"/\"false\"\n0.819\n18\n27\n0.813\n17\n31\n0.857\n24\n45\n0.854\n34\n79\ndefault \u2192relevant\n\"en\"/\"not\"\n0.788\n18\n21\n0.773\n17\n18\n0.868\n24\n28\n0.839\n34\n52\n\"yes\"/\"no\"\n0.745\n18\n21\n0.771\n17\n20\n0.863\n24\n31\n0.854\n34\n52\n\"good\"/\"bad\"\n0.810\n18\n21\n0.724\n17\n20\n0.846\n24\n30\n0.829\n34\n52\ndefault \u2192irrelevant\n\"cat\"/\"dog\"\n0.759\n18\n20\n0.637\n17\n19\n0.805\n24\n31\n0.835\n34\n39\n\"foo\"/\"bar\n0.633\n/\n/\n0.567\n/\n/\n0.841\n24\n31\n0.830\n34\n52\n\"black\"/\"white\"\n0.620\n18\n20\n0.609\n17\n19\n0.833\n24\n28\n0.838\n34\n39\nANLI\ndefault \u2192default\n\"true\"/\"false\"\n0.723\n-\n-\n0.679\n-\n-\n0.851\n24\n45\n0.854\n34\n79\ndefault \u2192relevant\n\"en\"/\"not\"\n0.693\n-\n-\n0.589\n-\n-\n0.816\n24\n28\n0.841\n34\n52\n\"yes\"/\"no\"\n0.721\n-\n-\n0.659\n-\n-\n0.843\n24\n30\n0.821\n34\n52\n\"good\"/\"bad\"\n0.676\n-\n-\n0.606\n-\n-\n0.790\n24\n29\n0.802\n34\n52\ndefault \u2192irrelevant\n\"cat\"/\"dog\"\n0.522\n-\n-\n0.519\n-\n-\n0.643\n24\n28\n0.766\n34\n52\n\"foo\"/\"bar\n0.601\n-\n-\n0.521\n-\n-\n0.728\n24\n29\n0.808\n34\n52\n\"black\"/\"white\"\n0.511\n-\n-\n0.542\n-\n-\n0.710\n24\n28\n0.794\n34\n52\nIMDb\ndefault \u2192default\n\"positive\"/\"negative\"\n0.844\n18\n27\n0.873\n17\n31\n0.889\n24\n45\n0.913\n39\n79\ndefault \u2192relevant\n\"good\"/\"bad\"\n0.887\n18\n21\n0.904\n17\n24\n0.919\n24\n30\n0.923\n39\n52\n\"true\"/\"false\"\n0.733\n18\n21\n0.772\n17\n21\n0.912\n24\n28\n0.918\n39\n52\ndefault \u2192irrelevant\n\"cat\"/\"dog\"\n0.709\n18\n20\n0.778\n17\n17\n0.880\n24\n27\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in large language models (LMs), particularly focusing on their ability to perform tasks using demonstrations that may contain irrelevant label words. Previous studies have shown that the ICL performance is minimally affected by such irrelevant labels, which raises questions about the underlying mechanisms at play. The necessity for a new approach arises from the need to understand how LMs can effectively utilize these irrelevant labels without significant performance degradation.",
        "problem": {
            "definition": "The problem this paper aims to solve is understanding the mechanisms by which LMs perform ICL with irrelevant or misleading label words, specifically how they manage to generate correct outputs despite potentially confusing inputs.",
            "key obstacle": "The main challenge is identifying the causal processes that allow LMs to achieve effective ICL performance when faced with irrelevant label mappings, as existing methods do not adequately explain this phenomenon."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that LMs seem to use two distinct functions\u2014an inference function and a verbalization function\u2014that operate sequentially when processing ICL tasks. This leads to the hypothesis that these functions can work independently of the specific label mappings.",
            "opinion": "The proposed idea suggests that LMs first infer the answer representation from the input context and then verbalize this representation into the appropriate label space specified by the demonstrations. This two-step process is thought to be robust to variations in label words.",
            "innovation": "The innovation lies in the hypothesis that the inference function is invariant to remappings of the label space, which contrasts with existing approaches that do not account for such flexibility in LMs."
        },
        "method": {
            "method name": "Interchange Intervention",
            "method abbreviation": "II",
            "method definition": "Interchange Intervention is a method that involves modifying the internal representations of LMs to create counterfactual scenarios, allowing researchers to isolate and analyze the distinct functions involved in ICL.",
            "method description": "The core of the method involves swapping representations between different runs of the model to examine how changes in the label space affect the output.",
            "method steps": [
                "Define a counterfactual scenario based on the hypotheses.",
                "Conduct an interchange intervention by replacing specific internal representations.",
                "Evaluate the effects of the intervention on model outputs."
            ],
            "principle": "This method is effective because it allows for the isolation of the inference and verbalization functions, enabling a clearer understanding of how LMs process irrelevant label mappings."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted across multiple datasets and tasks, including natural language inference (MultiNLI), sentiment analysis (IMDb), and topic classification (AGNews). Baseline methods were also employed for comparison.",
            "evaluation method": "The evaluation involved measuring the flip rates of outputs generated by the model after applying interchange interventions, with a focus on determining the effectiveness of the inference and verbalization functions."
        },
        "conclusion": "The experiments confirmed the hypotheses regarding the sequential application of inference and verbalization functions in LMs. The findings indicate that LMs can effectively perform ICL with irrelevant labels through a robust two-step process, contributing valuable insights into the mechanisms of ICL.",
        "discussion": {
            "advantage": "The proposed approach offers a novel understanding of how LMs maintain performance in ICL tasks despite variations in label mappings, shedding light on their operational flexibility.",
            "limitation": "One limitation is that the study focuses on open-source models, and the findings may not generalize to proprietary or state-of-the-art models. Additionally, the complexity of tasks beyond classification remains unexplored.",
            "future work": "Future research could investigate the application of these findings to more complex tasks and explore the effects of different prompting strategies on ICL performance."
        },
        "other info": {
            "code_link": "https://github.com/JunyiTao/infer-then-verbalize-during-icl",
            "dataset_info": {
                "MultiNLI": {
                    "examples": 9820,
                    "label_words": [
                        "true",
                        "false"
                    ]
                },
                "IMDb": {
                    "examples": 25000,
                    "label_words": [
                        "positive",
                        "negative"
                    ]
                },
                "AGNews": {
                    "examples": 7600,
                    "label_words": [
                        "world",
                        "sports",
                        "business",
                        "sci/tech"
                    ]
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of in-context learning (ICL) in large language models (LMs), particularly focusing on their ability to perform tasks using demonstrations that may contain irrelevant label words."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea suggests that LMs first infer the answer representation from the input context and then verbalize this representation into the appropriate label space specified by the demonstrations."
        },
        {
            "section number": "3.2",
            "key information": "The innovation lies in the hypothesis that the inference function is invariant to remappings of the label space, which contrasts with existing approaches that do not account for such flexibility in LMs."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge is identifying the causal processes that allow LMs to achieve effective ICL performance when faced with irrelevant label mappings."
        },
        {
            "section number": "4.1",
            "key information": "The experiments confirmed the hypotheses regarding the sequential application of inference and verbalization functions in LMs."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that the study focuses on open-source models, and the findings may not generalize to proprietary or state-of-the-art models."
        },
        {
            "section number": "6.4",
            "key information": "Future research could investigate the application of these findings to more complex tasks and explore the effects of different prompting strategies on ICL performance."
        }
    ],
    "similarity_score": 0.7252950602924744,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Inference and Verbalization Functions During In-Context Learning.json"
}