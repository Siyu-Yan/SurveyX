{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.14936",
    "title": "On the growth of the parameters of approximating ReLU neural networks",
    "abstract": "This work focuses on the analysis of fully connected feed forward ReLU neural networks as they approximate a given, smooth function. In contrast to conventionally studied universal approximation properties under increasing architectures, e.g., in terms of width or depth of the networks, we are concerned with the asymptotic growth of the parameters of approximating networks. Such results are of interest, e.g., for error analysis or consistency results for neural network training. The main result of our work is that, for a ReLU architecture with state of the art approximation error, the realizing parameters grow at most polynomially. The obtained rate with respect to a normalized network size is compared to existing results and is shown to be superior in most cases, in particular for high dimensional input.",
    "bib_name": "morina2024growthparametersapproximatingrelu",
    "md_text": "# On the growth of the parameters of approximating ReLU neural networks\nMartin Holler \u2217 Erion Morina \u2020\nJune 24, 2024\nAbstract\n# Abstract\nThis work focuses on the analysis of fully connected feed forward ReLU neural networks as they approximate a given, smooth function. In contrast to conventionally studied universal approximation properties under increasing architectures, e.g., in terms of width or depth of the networks, we are concerned with the asymptotic growth of the parameters of approximating networks. Such results are of interest, e.g., for error analysis or consistency results for neural network training. The main result of our work is that, for a ReLU architecture with state of the art approximation error, the realizing parameters grow at most polynomially. The obtained rate with respect to a normalized network size is compared to existing results and is shown to be superior in most cases, in particular for high dimensional input.\nKeywords: Neural networks, approximation, complexity, growth of parameter MSC Codes: 41A25, 41A65\n# 1 Introduction\nIt is well known that certain neural network architectures have a universal ap proximation property, i.e., functions of certain regularity may be approximated arbitrarily well with respect to appropriate norms. This can be achieved by in creasing the complexity of the underlying neural networks, where complexity i\n\u2217Department of Mathematics and Scientific Computing, University of Graz. MH further is a member of NAWI Graz (www.nawigraz.at) and of BioTechMed Graz (biotechmedgraz.at) (martin.holler@uni-graz.at) \u2020Department of Mathematics and Scientific Computing, University of Graz. (erion.morina@uni-graz.at).\nusually described by the network size in terms of width, depth, or number of weights and neurons when it comes to fully connected feed forward neural networks. In the classical work [13], shallow approximations of Sobolev functions with respect to Lp-norms are studied. See also [6] for deep ReLU approximation of Sobolev functions with respect to general Sobolev norms and the references therein. For the (nearly) optimal approximation of (piecewise) smooth functions by ReLU networks see [12, 14]. The works in [3, 4, 5] together with their references give a comprehensive overview of the approximation theory based on neural networks. A question that has not been given much attention in the current literature is how the parameters realizing the approximating networks behave asymptotically, see Section 2 for an overview of related works. This question is of particular interest, e.g., in view of a full error analysis or consistency results for neural network training. As an example for the former, let us consider the results of [8, 9], where a full error analysis of deep learning for empirical risk minimization is provided. There, the underlying networks are trained based on Stochastic Gradient Descent (SGD) with random initializations and the results hold in the probabilistic sense. A simplification of the result in [8, Theorem 1.1] is given as follows. For some d \u2208N let f : [0, 1]d \u2192[0, 1] be Lipschitz continuous. Assume that (fN,L)N,L\u2208N is a sequence of networks such that fN,L is a network of width N and depth L with parameters bounded by c(N, L), minimizing the empirical risk over M given i.i.d. training samples. Let further A(N, L) denote some upper bound on the approximation error of f in terms of N and L, approaching zero for increasing N and L (Note that A depends on the approximating architecture and in general stronger regularity assumptions of f than Lipschitz-continuity are required). Furthermore, let O(N, L, K)c(N, L)L+1 be some upper bound on the optimization error, where K is the number of random initializations of SGD, and let G(N, L, M)c(N, L) be some upper bound on the generalization error. Explicit characterizations of G and O in terms of N, L, M, K, respectively, are given in [8, Theorem 1.1]. The error functions G and O approach zero for fixed N, L \u2208N as K and M increase. Under these assumptions it holds true by [8, Theorem 1.1] that E(\u2225fN,L \u2212f\u2225L1([0,1]d,P)) \u2264A(N, L) + O(N, L, K)c(N, L)L+1 + G(N, L, M)c(N, L). This shows that in order to bound the left hand side, one does not only require estimates on A(N, L), O(N, L, K), and G(N, L, M), but also on the bound c(N, L) of the parameters of the approximating neural network. It is clear that bounds on the parameters of approximating neural networks must always be analyzed jointly with the network depth and width. Indeed, networks with large parameter bounds may be expanded in width and depth such that they\ndescribe the same function, but such that the modified parameters are considerably smaller. On the other hand. fixing the architecture in terms of width and depth can obviously not allow the approximation of arbitrary complex but smooth functions. In order to account for this, our strategy is to consider networks that achieve optimal or nearly optimal approximation results with respect to width and depth, and analyze the asymptotic behavior of parameters realizing those networks. Specifically, we analyze the asymptotic behavior of the parameters realizing the approximating network architectures studied in [12] and [13]. An advantage with these works is that they are based on explicit constructions which allow for successive estimation of the occurring parameters. The approximation scheme in [13] provides an optimal approximation error for single-hidden-layer neural networks whereas [12] establishes the optimal order of approximation in terms of width and depth. Note that we do not make any statement on optimality of the asymptotical behavior of the realizing parameters. The goal is to get an insight in the asymptotic behavior of the parameters realizing the approximating schemes in [12, 13], compared to the current state in literature. For the shallow approximation with smooth activation functions introduced in [13] based on trigonometric polynomials, we show exemplarily the negative result that, for the Gaussian and logistic activation function, the parameters realizing the approximating neural networks grow at least exponentially under mild assumptions. For the deep approximation introduced in [12] based on the ReLU activation function we provide a small modification with slightly increased depth, such that the realizing parameters of the modified approximating networks grow at most polynomially. A simplified version of our main result in this context, which is based on [12, Theorem 1.1], is given as follows.\n# Theorem (Simplification of Theorem 4). Let d, q \u2208N and f \u2208Cq([0, 1]d). Then for any N, L \u2208N there exists a ReLU feed forward neural network fN,L with width of order N log N and depth of order L2 log L such that\n\u2225f \u2212fN,L\u2225L\u221e([0,1]d) = O(N\u22122q/dL\u22122q/d).\nThe parameters of the fN,L grow asymptotically as O(max(N(6q\u22123)/dL(6q\u22122)/d, N2L3)).\nScope of the paper. In Section 2 we compare the result above to existing results in the literature. In Section 3 we provide our main results, in particular the analysis on the asymptotical behavior of the parameters of the approximation with deep networks introduced in [12]. In Appendix A we provide the proof of our negative result on the asymptotic behavior of the parameters for the approximation with single-hidden-layer introduced in [13].\n# 2 Comparison to existing literature\nThis section provides works that also deal with the asymptotic behavior of realizing parameters of approximating networks and compares their result with ours. The work [2] considers the approximation of certain Sobolev-regular functions by shallow feed-forward tanh-type neural networks. In particular, the approximation result in [2, Theorem 5.1] is provided under parameters that grow at most polynomially in terms of the width. The work [1] deals with the approximation of H\u00a8older-smooth functions by feed forward neural networks with piecewise polynomial activation functions amongst others. In [1, Theorem 2] the approximation result is achieved with uniformly bounded weights. For so-called (p, C)\u2212smooth functions (see e.g. [11, Definition 1]) an approximation result with sigmoidal activation functions is provided in [11, Theorem 1] for at most polynomially growing parameters in terms of the width. In [7, Proposition 4.8] an approximation result for Sobolev-regular functions under activation functions enabling the construction of exact/exponential/polynomial partitions of unity (see [7, Definition 4.1]) is established with parameters bounded polynomially in terms of the number of non-zero weights. For comparing the above approximation result to ours, it is important that the functions that are approximated attain the same regularity, that the norm which measures the approximation error is the same or at least comparable, and that the hyperparameter of the approximating architecture with respect to which the approximation error decreases (e.g. width) is transformed to the same order of complexity. Consequently, we compare the approximation results for functions f \u2208Cq([0, 1]d), since they meet the regularity requirements of [12] (smoothness), [2, 7] (Sobolevregularity) and [1, 11] (H\u00a8older-regularity since the derivative of order q \u22121 is Lipschitz-continuous). A comparable norm for the approximation error in [1, 2, 7, 11, 12] is the supremum norm. A summary of the approximation results of the different works and of Theorem 4 is provided in Table 1 and Table 2. Note that the asymptotical bound for the approximation error of Theorem 4 in Table 1 holds for any \u03b4 \u2208(0, 1) (as the logarithm of N grows slower than any positive power of N) and that the result in [1] is valid for q \u22653. Also note that, in Theorem 4, it is possible to vary the depth of the approximating architecture as opposed to the results in [1, 2, 11], but for the sake of comparison we consider a constant depth. The approximation error in Table 1 relative to the width of Theorem 4 is better than in [1, 2] but slightly worse than in [11]. Regarding the growth of parameters, in case 6q < 2d + 3, one can observe that the bound for Theorem 4 grows slower than of [2] except for the case q = 1, d = 2. In case 6q \u22652d + 3, the bound for Theorem 4 grows slower than of [2] except for d = 2 with q \u2208{2, 3, 4} and d = 1\nFor comparing the above approximation result to ours, it is important that the functions that are approximated attain the same regularity, that the norm which measures the approximation error is the same or at least comparable, and that the hyperparameter of the approximating architecture with respect to which the approximation error decreases (e.g. width) is transformed to the same order of complexity. Consequently, we compare the approximation results for functions f \u2208Cq([0, 1]d), since they meet the regularity requirements of [12] (smoothness), [2, 7] (Sobolevregularity) and [1, 11] (H\u00a8older-regularity since the derivative of order q \u22121 is Lipschitz-continuous). A comparable norm for the approximation error in [1, 2, 7, 11, 12] is the supremum norm. A summary of the approximation results of the different works and of Theorem 4 is provided in Table 1 and Table 2. Note that the asymptotical bound for the approximation error of Theorem 4 in Table 1 holds for any \u03b4 \u2208(0, 1) (as the logarithm of N grows slower than any positive power of N) and that the result in [1] is valid for q \u22653. Also note that, in Theorem 4, it is possible to vary the depth of the approximating architecture as opposed to the results in [1, 2, 11], but for the sake of comparison we consider a constant depth. The approximation error in Table 1 relative to the width of Theorem 4 is better than in [1, 2] but slightly worse than in [11]. Regarding the growth of parameters, in case 6q < 2d + 3, one can observe that the bound for Theorem 4 grows slower than of [2] except for the case q = 1, d = 2. In case 6q \u22652d + 3, the bound for Theorem 4 grows slower than of [2] except for d = 2 with q \u2208{2, 3, 4} and d = 1\nResult\nWidth\nDepth\nApproximation\nGrowth of parameters\nActivation\nTh. 4\nO(N)\nO(L)\nO(N\n\u22122q\nd(1+\u03b4)L\n\u2212q\nd(1+\u03b4))\nO(N\n6q\u22123\nd L\n3q\u22121\nd\n\u2228N2L\n3\n2)\nReLU\n[1]\nO(N)\nO(1)\nO(N\u2212q/d)\nO(1)\nReQU\n[2]\nO(N)\n3\nO(N\u2212q/d)\nO(N(d+q2)/2)\ntanh\n[11]\nO(N)\nO(1)\nO(N\u22122q/d)\nO(N(16q+2d+9)/d)\n1\n1+exp(\u2212x)\n<div style=\"text-align: center;\">Table 1: Comparison of state of the art results on growth of parameters realizing approximations to f \u2208Cq([0, 1]d) with normalized width.</div>\nResult\nNonzero weights\nApproximation\nGrowth of parameters\nActivation\nTh. 4\nO(W)\nO(W \u2212q/d)\nO(W\n9q\u22124\n2d \u22287\n4)\nReLU\n[7]\nO(W)\nO(W \u2212q/d)\nO(W 4+2q/d)\nRePU, soft+\nTable 2: Comparison of state of the art result on growth of parameters realizing approximations to f \u2208Cq([0, 1]d) with normalized number of nonzero weights.\nTable 2: Comparison of state of the art result on growth of parameters realizing approximations to f \u2208Cq([0, 1]d) with normalized number of nonzero weights. with q \u226411. Regarding [11], one can observe that the growth of parameters in Theorem 4 is always slower than the one of [11]. Regarding a comparison to [1], it is interesting to see that [1] even yields uniformly bounded parameters in [\u22121, 1] (though with slightly worse approximation error compared to Theorem 4). In the following, we detail the main differences between the result of [1] and Theorem 4. One difference is that Theorem 4 allows for adjusting the depth of the approximating architecture, yielding a better approximation error. Furthermore, different regularity assumptions are required, in [1] a form H\u00a8older-regularity whereas Theorem 4 requires smoothness of certain degree. Moreover, a key difference is that the result in [1] is based on the ReQU activation function \u03c3ReQU(x) = (x \u22280)2 which is capable of approximating several higher order derivatives simultaneously. The approximation in [1] is essentially based on tensor-product splines, which are certain piecewise polynomials. The crucial point here is that the coefficients of corresponding normalized basis splines are a priori uniformly bounded in terms of the approximated function. The choice of the ReQU in [1] is therefore crucial as it can represent piecewise polynomials exactly, thus, in particular also the identity mapping and products. Expanding the approximating architecture yields that the parameters, which are contained in a compact set, can be restricted to the interval [\u22121, 1], respectively. The usage of ReQU in [1] is essential, as for the ReLU activation function, used in [12], approximability is only achievable in spaces W q,p([0, 1]d) for 0 \u2264q \u22641 and 1 \u2264p \u2264\u221e(see [6]) due to the first order irregularity of the ReLU in zero. In ad-\nition, the ReLU is only capable of approximating multiplications with decreasing rror and increasing architectures (see [12, Lemma 4.2]) as opposed to the ReQU.\nFinally, we compare the result in Theorem 4 to [7, Proposition 4.8] summarized in Table 2. Again we consider the approximation of some f \u2208Cq([0, 1]d). As a consequence, the result in [7] yields that for \u01eb > 0 and the number of nonzero weights being of order O(\u01eb\u2212d/q), the approximation error equals \u01eb and the parameters are of order O(\u01eb\u22122(1+2d/q)). Thus, if the number of nonzero weights is of order O(W) then the approximation error is of complexity O(W \u2212q/d) and the parameters of order O(W 4+2q/d). The approximation result in Theorem 4 is formulated in terms of width N and depth L. Thus, the number of nonzero weights is of order O(NL). Identifying the product NL by W under N \u2248L (and hence N, L \u2248W 1/2) we derive that the growth of parameters in Theorem 4 given in Table 1 is of order O(W 9q\u22124 2d \u22287 4). As a consequence, for 18q \u22647d + 8 the upper bound of the parameters in Theorem 4 grows slower than that of [7]. In case 18q > 7d + 8 this applies only if 5q \u22648d + 4.\nIn summary, compared to state of the art results, except for [1] where uniform boundedness is achieved by using ReQU activations, the bound on the parameter of Theorem 4 grows slower in most cases, depending on the input dimension an regularity of the approximated function.\n# 3 Growth of parameters of approximating neural networks\n# 3 Growth of parameters of approximating neural\nIn this section we provide analytical results on the asymptotic behavior of the supremum norm of the parameters of two approximating fully connected feed forward neural network architectures based on [12]. For completeness, we first provide the definition of a fully connected feed forward neural network.\nDefinition 1. Given L \u2208N and nl \u2208N for 0 \u2264l \u2264L, a fully connected feed forward neural network N\u03b8 with activation function \u03c3 is defined as N\u03b8 = L\u03b8L \u25e6 \u00b7 \u00b7 \u00b7 \u25e6L\u03b81 for L\u03b8l : Rnl\u22121 \u2192Rnl with L\u03b8l(z) := \u03c3(wlz + \u03b2l) for 1 \u2264l \u2264L \u22121 and L\u03b8L(z) := wLz + \u03b2L where \u03b8l = (wl, \u03b2l) with wl \u2208L(Rnl\u22121, Rnl) \u2243Rnl\u00d7nl\u22121, \u03b2l \u2208Rnl for 1 \u2264l \u2264L. Further we define the depth of the network by D(N ) = L and width W(N ) = N = maxl nl. Denoting by FN N the class of fully connected feed forward neural networks and by \u0398 the class of parameter configurations \u03b8 =\nwhere \u2225\u03b8\u2225\u221eis the corresponding supremum norm for \u03b8 \u2208\ufffdL l=1 Rnl\u00d7nl\u22121 \u00d7 Rnl.\n\ufffd Note that it follows by standard arguments that the minimum in (1) is attained and hence the map P is well-defined. Furthermore, given N \u2208FN N and a \u02dc\u03b8 \u2208\u0398 with R(\u02dc\u03b8) = N it holds true that P(N ) \u2264\u2225\u02dc\u03b8\u2225\u221e.\nThe main goal of this work is to study the following problem: Given f \u2208X , e.g. X = Cq([0, 1]d), assume that there exists c > 0 such that for all N, L \u2208N there exists \u03c6N,L \u2208FN N with width W(\u03c6N,L) = w(N) and depth D(\u03c6N,L) = d(L) fulfilling\n\u2225\u03c6N,L \u2212f\u2225Y \u2264c\u2225f\u2225X\u03b1X(N, L)\nwith X \u0592\u2192Y, \u03b1 : N2 \u2192[0, \u221e) monotonically decreasing in both components, and decreasing to zero in at least one component. In case the constant c is independent of f, N and L we write \u2225\u03c6N,L \u2212f\u2225Y \u2272\u2225f\u2225X\u03b1X(N, L) for (2). Given this kind of approximation result, the question is how P(\u03c6N,L) behaves asymptotically for \u03c6N,L approximating f via the width N and/or the depth L going to infinity. Note that, in contrast to most works that deal with approximability in terms of width and depth of neural networks, we take such results as given and rather focus on the worst-case growth of the supremum of the parameters of the approximating neural network. As first example, we consider the classical approximation result of [13, Theorem 2.1] using single-hidden-layer neural networks. As the following (negative) result shows, the asymptotic growth of the network parameters highly depends on the way the neural-network approximation is constructed, and may even be exponential in some cases.\nTheorem 2. Let 1 \u2264q \u2264\u221eand f \u2208W 1,\u221e([\u22121, 1]) be given as in (25). Then the single hidden layer feed forward neural networks (fN)N\u2208N as constructed in [13, Theorem 2.1] (see also (23)) with width of order O(N) and activation function given by either \u03c6(x) = exp(\u2212x2) or \u03c6(x) = (1 + exp(\u2212x))\u22121 fulfill\nFurthermore, there exists some c > 1 such that the realizing parameters of the (fN)N grow asymptotically as \u2126(cN).\n(1)\n(2)\nNow we move to the case of deep-neural-network approximation and the main result of this paper. An important result in this context is provided in [12], which shows via an explicit construction, that fully connected feed forward neural networks with ReLU activation functions, with width of order N log(8N) and depth of order L log(4L), can approximate functions f \u2208Cq([0, 1]d) with an error of order \u2225f\u2225Cq([0,1]d)(NL)\u22122q/d. That is in (2) it holds X = Cq([0, 1]d), Y = L\u221e([0, 1]d) and \u03b1X(N, L) = (NL)\u22122q/d. The main result in [12] reads as follows Theorem 3. [12, Theorem 1.1] For f \u2208Cq([0, 1]d) with q \u2208N+ there exists some ReLU generated neural network \u03c6 with width W(\u03c6) = O(N log N) and depth D(\u03c6) = O(L log L) such that\n\u2225 \u2212\u2225\u2225\u2225 Based on this approximation, the main result of this section is as follows. Theorem 4. Let d, q \u2208N and f \u2208Cq([0, 1]d). Then for any N, L \u2208N there exists a ReLU feed forward neural network fN,L with width C1N log(8N) and depth C2L2 log(4L) such that\n\u2225 \u2212\u2225\u2225\u2225 with C1, C2 > 0 independent of f, N and L. The parameters of the fN,L grow asymptotically as\nO(max(N(6q\u22123)/dL(6q\u22122)/d, NL(N + L2))).\nO In order to prove Theorem 4, we follow the construction for the proof of Theorem 3, which is based on the two main auxiliary results [12, Theorem 2.1 and Theorem 2.2] as follows. 1. The result in [12, Theorem 2.2] gives a constructive proof for approximating a given, sufficiently regular function on [0, 1]d with an approximation error of order (NL)\u22122q/d with a ReLU-neural-network with width O(N log N) and depth O(L log L) outside a trifling region\n2. The result [12, Theorem 2.1] then shows how such an approximation can be extended to approximate the function on all of [0, 1]d.\n3. Finally, in the main result [12, Theorem 1.1], the trifling region is chosen small enough, which infers the final asymptotic behavior of the network parameters in terms of N and L. Accordingly, our proof of Theorem 4 is divided into three subsections corresponding to the steps 1) - 3) above, where the main effort lies in the first step.\n(3)\n# 3.1 Estimation of P(\u03c6) in [12, Theorem 2.2]: The approximating neural network of [12, Theorem 2.2] is given by\n P The approximating neural network of [12, Theorem 2.2] is given by\n\u03c6(x) := \ufffd \u2225\u03b1\u22251\u2264q\u22121 \u03d5( 1 \u03b1!\u03c6\u03b1(\u03a8(x)), P\u03b1(x \u2212\u03a8(x)))\nfor x \u2208Rd, where the role of the subnetworks \u03a8, P\u03b1, \u03c6\u03b1, \u03d5 is as follows:\n\u2022 The ReLU FNN \u03a8 realizes projections of subcubes of [0, 1]d to exactly one corner of the subcube based on one-dimensional step functions \u03c8 (see considerations on [12, Proposition 4.3] for \u03c8) \u2022 The ReLU FNN P\u03b1 achieves an approximation of multinomials of order at most q \u22121 (see [12, Proposition 4.1]). \u2022 The ReLU FNN \u03c6\u03b1 achieves fitting partial derivatives of f of order at most q \u22121 at the corners of the subcubes to which \u03a8 projects to (see [12, Proposition 4.4]).\nThe approximation error for \u03c6 as above is estimated in [12, Step 3, p. 25 ff], which essentially relies on the triangle inequality and the approximation properties of the single components. Figure 1 provides an overview of the relevant subresults derived in [12, 15, 16] for constructing \u03c6 as above. In the following we will not give and explain each of these subresults in detail, but rather refer to the original references [12, 15, 16]. Now we estimate the growth of the parameters in each of these subresults to finally estimate P(\u03c6). In accordance with the chain of dependencies depicted in Figure 1, we start by analyzing [15, Lemma 2.1].\nComplexity estimation in [15, Lemma 2.1]: The result in [15, Lemma 2.1] shows that the set of continuous piecewise linear functions with N pieces mapping an interval to R is expressible by a single-hidden-layer ReLU network \u03c6 of width W(\u03c6) = O(N). We restrict ourselves to two cases that occur in the subsequent results in [12, 15, 16], i.e., the general estimation of the growth of the parameters of the interpolating networks is not necessary. These two cases are covered by the following two lemmata.\nLemma 5. Let \u02dcR \u2208N, R > 0 and yk \u2208R for 0 \u2264k \u2264\u02dcR be given. Then there exists a single-hidden-layer ReLU network \u03c6 with width W(\u03c6) = \u02dcR such that\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c72e/c72edd3a-b766-4ef7-95d5-a8501aed7945.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Overview of structure of results in [12]</div>\n<div style=\"text-align: center;\">\u03c6(xk) = yk where xk = k/R for 0 \u2264k \u2264\u02dcR, i.e., the xk are equidistantly distributed in the interval [0, \u02dcR/R]. Furthermore, the network \u03c6 fulfills</div>\nProof. The network \u03c6 may be realized by \u03c6(x) = W2 ReLU(W1x + b1) + b2 where by [15, Lemma 2.1] the parameters W1, b1, b2 can be chosen as W1 = (1, . . . , 1)T \u2208 R \u02dcR\u00d71, b1 = (\u2212x0, . . . , \u2212x \u02dcR\u22121)T. It remains to determine b2 and W2 and finally the asymptotical behavior of the parameters of \u03c6. It is straightforward to show that, with b2 = y0 and for W2 = (w0, . . . , w \u02dcR\u22121) with\nand, for 1 \u2264j \u2264\u02dcR \u22121,\nwj = 1 xj+1 \u2212xj (yj+1\u2212y0\u2212 j\u22121 \ufffd l=0 wl(xj+1\u2212xl)) = R(yj+1\u2212y0)\u2212 j\u22121 \ufffd l=0 wl(j\u2212l+1) (4\n(4)\nwe obtain in the previous Lemma that\nP(\u03c6) \u2272max(X, RY ).\nNext we consider a similar result generalized to certain inequidistant grids {xk}k of the interval [0, \u02dcR/R]. Note that the following Lemma is general enough to cover the instances in [12] used by [15, Lemma 2.1]. Lemma 6. Let \u02dcR \u2208N, R > 0 and \u03b4 = 1 (c+1)R for some c \u2208N be such that there exist m, n \u2208N with 2 \u02dcR = m(n + 1) where n + 1 = 2p for some p \u2208N. Let further the grid points {xk}k be given as x2k = k R for 0 \u2264k \u2264\u02dcR and x2k\u22121 = k R \u2212\u03b4 for 1 \u2264k \u2264\u02dcR.\nNext we consider a similar result generalized to certain inequidistant grids {xk}k of the interval [0, \u02dcR/R]. Note that the following Lemma is general enough to cover the instances in [12] used by [15, Lemma 2.1].\nLemma 6. Let \u02dcR \u2208N, R > 0 and \u03b4 = 1 (c+1)R for some c \u2208N be such that the exist m, n \u2208N with 2 \u02dcR = m(n + 1) where n + 1 = 2p for some p \u2208N. Let furth the grid points {xk}k be given as\n(5)\n\n(6)\nAssume that yk \u2208R for 0 \u2264k \u22642 \u02dcR are given. Then there exists a single-hidden layer ReLU network \u03c6 with width W(\u03c6) = 2m such that \u03c6(xj(n+1)) = yj(n+1) fo j = 0, . . . , m and \u03c6(xj(n+1)+n) = yj(n+1)+n for j = 0, . . . , m \u22121. Furthermore, th network \u03c6 fulfills\nwhere X := max 0\u2264k\u22642 \u02dcR |xk| and Y := |y0| + max 0\u2264k\u22642 \u02dcR\u22121 |yk+1 \u2212yk|.\n    Proof. We recall that interpolation is considered only in the points xj(n+1) fo j = 0, . . . , m and xj(n+1)+n for j = 0, . . . , m \u22121. Define\nz2j = xj(n+1), v2j = yj(n+1), for j = 0, . . . , m z2j\u22121 = xj(n+1)\u22121, v2j\u22121 = yj(n+1)\u22121, for j = 1, . . . , m.\nThen the network \u03c6, realized in form of \u03c6(x) = W2 ReLU(W1x + b1) + b2, has to fulfill the conditions \u03c6(zk) = vk for 0 \u2264k \u22642m. The parameters W1, b1, b2 may be chosen similarly as in the proof of Lemma 5 by W1 = (1, . . . , 1)T \u2208R2m, b1 = (\u2212z0, . . . , \u2212z2m\u22121)T and b2 = y0. Then W2 = (w0, . . . , w2m\u22121)T is the unique solution of the linear system\nwhere Zij = zi \u2212zj\u22121 if 1 \u2264j \u2264i \u22642m and 0 else, and Vi = vi \u2212v0 fo i, j = 1, . . . , 2m. As it holds true that\nNoting that z2j+1\u2212z2j = p R \u2212\u03b4 = p(c+1)\u22121 R(c+1) and z2j \u2212z2j\u22121 = \u03b4 under the assumption that \u03b4 = 1 (c+1)R for some c \u2208N (yielding in particular, that c\u03b4 = 1 R \u2212\u03b4) the system may be extended to an equidistant system with linearly interpolated target data such that the considerations in the proof of Lemma 5 are applicable yielding\nNoting that z2j+1\u2212z2j = p R \u2212\u03b4 = p(c+1)\u22121 R(c+1) and z2j \u2212z2j\u22121 = \u03b4 under the assumption that \u03b4 = 1 (c+1)R for some c \u2208N (yielding in particular, that c\u03b4 = 1 R \u2212\u03b4) the system may be extended to an equidistant system with linearly interpolated target data such that the considerations in the proof of Lemma 5 are applicable yielding w2j+1 = (c + 1)R(v2j+2 \u2212(1 + 1 p(c + 1) \u22121)v2j+1 + 1 p(c + 1) \u22121v2j), w2j = (c + 1)R( 1 (c + 1)p \u22121v2j+1 \u2212(1 + 1 (c + 1)p \u22121)v2j + v2j\u22121). (11)\n(7)\nfor\n(8)\n(9)\n(10)\n(11)\nIndeed, it is straightforward to verify that the explicit representations in (11) resolve the recursion in (10) and hence, solve (8). Consequently, we derive that\nThis follows as w2j+1 and w2j for j = 0, . . . , m \u22121 given in (11) are bounded by (c+1)R( 1 p(c + 1) \u22121 max 0\u2264j\u2264m\u22121 |yj(n+1)+n\u2212yj(n+1)|+ max 0\u2264i\u22642 \u02dcR\u22121 |yi+1\u2212yi|) \u22642(c+1)RY. For the last inequality note that maxj |yj(n+1)+n \u2212yj(n+1)| \u2264n maxi |yi+1 \u2212yi| and\nThus, using (12) together with the definition of X, Y in (7) we derive that P(\u03c6) \u2272max(X, cRY ).\n<div style=\"text-align: center;\">Thus, using (12) together with the definition of X, Y in (7) we derive that P(\u03c6) \u2272max(X, cRY ).</div>\nComplexity estimation in [12, Lemma 5.4]: The result in [12, Lemma 5.4] is the same as [15, Lemma 2.2]. It shows that the set of continuous piecewise linear functions with m(n + 1) pieces mapping an interval (with increasing breakpoints xi and corresponding values yi \u22650 for 0 \u2264i \u2264m(n + 1)) to R is expressible by a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively. The underlying neural network \u03c6 may be realized in the form \u03c6(x) = W3 ReLU(W2 ReLU(W1x+b1)+b2)+b3 where W1 \u2208R2m\u00d71, W2 \u2208R(2n+1)\u00d72m, W3 \u2208 R1\u00d7(2n+1), b1 \u2208R2m, b2 \u2208R2n+1, b3 \u2208R fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Our result on the growth of parameters of this network is as follows. Lemma 7. In the situation of Lemma 6 with yi \u22650 there exists a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively, fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Furthermore, it holds true that\nComplexity estimation in [12, Lemma 5.4]: The result in [12, Lemma 5.4] is the same as [15, Lemma 2.2]. It shows that the set of continuous piecewise linear functions with m(n + 1) pieces mapping an interval (with increasing breakpoints xi and corresponding values yi \u22650 for 0 \u2264i \u2264m(n + 1)) to R is expressible by a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively. The underlying neural network \u03c6 may be realized in the form \u03c6(x) = W3 ReLU(W2 ReLU(W1x+b1)+b2)+b3 where W1 \u2208R2m\u00d71, W2 \u2208R(2n+1)\u00d72m, W3 \u2208 R1\u00d7(2n+1), b1 \u2208R2m, b2 \u2208R2n+1, b3 \u2208R fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Our result on the growth of parameters of this network is as follows. Lemma 7. In the situation of Lemma 6 with yi \u22650 there exists a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively, fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Furthermore, it holds true that P(\u03c6) \u2272max(X, c3nRY ) with X and Y defined as in (7). Inspecting the proof of [15, Lemma 2.2] one sees that the asymptotics of the parameter supremum of the parameters of \u03c6 is governed by the parameters W2 and b2. The reason is that W1, W3, b1, b3 can be chosen as W1 = (1, . . . , 1)T \u2208R2m\u00d71, b1 = (\u2212x0, \u2212xn, \u2212xn+1, \u2212x2n+1, . . . , \u2212xm(n+1)\u22121) \u2208R2m, W3 = (1, 1, \u22121, 1, \u22121, . . ., 1, \u22121) \u2208R1\u00d7(2n+1),\nLemma 7. In the situation of Lemma 6 with yi \u22650 there exists a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively, fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Furthermore, it holds true that\nwith X and Y defined as in (7). Inspecting the proof of [15, Lemma 2.2] one sees that the asymptotics of the parameter supremum of the parameters of \u03c6 is governed by the parameters W2 and b2. The reason is that W1, W3, b1, b3 can be chosen as W1 = (1, . . . , 1)T \u2208R2m\u00d71,\nwith X and Y defined as in (7). Inspecting the proof of [15, Lemma 2.2] one sees that the asymptotics of the parameter supremum of the parameters of \u03c6 is governed by the parameters W2 and b2. The reason is that W1, W3, b1, b3 can be chosen as W1 = (1, . . . , 1)T \u2208R2m\u00d71, b1 = (\u2212x0, \u2212xn, \u2212xn+1, \u2212x2n+1, . . . , \u2212xm(n+1)\u22121) \u2208R2m, R1\u00d7(2n+1)\nInspecting the proof of [15, Lemma 2.2] one sees that the asymptotics of the parameter supremum of the parameters of \u03c6 is governed by the parameters W2 and b2. The reason is that W1, W3, b1, b3 can be chosen as W1 = (1, . . . , 1)T \u2208R2m\u00d71,\nb1 = (\u2212x0, \u2212xn, \u2212xn+1, \u2212x2n+1, . . . , \u2212xm(n+1)\u22121) \u2208R2m, W3 = (1, 1, \u22121, 1, \u22121, . . ., 1, \u22121) \u2208R1\u00d7(2n+1),\n(12)\n# \nb3 = 0 and contribute to the asymptotical behavior of P(\u03c6) only in terms of X defined as in (6). Hence, in order to prove Lemma 7, we need to consider the construction of the\nb3 = 0 and contribute to the asymptotical behavior of P(\u03c6) only in terms of X defined as in (6). Hence, in order to prove Lemma 7, we need to consider the construction of the parameters W2 and b2 in detail. These parameters are given in terms of the parameters of inductively constructed single-hidden-layer ReLU-networks g+ k , g\u2212 k for 1 \u2264k \u2264n and g0 via\ng+ k (x) = (W2)2k ReLU(W1x+b1)+(b2)2k, g\u2212 k (x) = (W2)2k+1 ReLU(W1x+b1)+(b2)2k+ for 1 \u2264k \u2264n, where (W2)j denotes the j-th row of W2. The single-hidden-layer ReLU networks are constructed to be linear on the intervals [xj(n+1), xj(n+1)+n] for 0 \u2264j \u2264m \u22121 and [xj(n+1)\u22121, xj(n+1)] for 1 \u2264j \u2264m such that, in the breakpoints xj(n+1) for 0 \u2264j \u2264m and xj(n+1)\u22121 for 1 \u2264j \u2264m, certain, in the following described, interpolation conditions are met. Let f0 be the piecewise linear continuous function fitting the data yi at xi for 0 \u2264i \u2264m(n + 1), which is linear on each of the subintervals [xi\u22121, xi] for 1 \u2264 i \u2264m(n + 1). The network g0 is constructed such that g0(xj(n+1)) = yj(n+1) for 0 \u2264j \u2264m and g0(xj(n+1)+n) = yj(n+1)+n for 0 \u2264j \u2264m \u22121. Given f0 and g0 the network f1 is defined by f1 := f0 \u2212g0. Following [15, Lemma 2.2] the networks g+ k and g\u2212 k are constructed inductively for 1 \u2264k \u2264n as follows. Assuming fk to be given, the networks g+ k , g\u2212 k and fk+1 are determined by the following conditions. For each 0 \u2264j \u2264m if fk(xj(n+1)+k) \u22650 then g+ k , g\u2212 k must attain values at xj(n+1), xj(n+1)+n such that g+ k (xj(n+1)+k) = fk(xj(n+1)+k) and g+ k (xj(n+1)+k\u22121) = 0 and g\u2212 k \u22610 on [xj(n+1), xj(n+1)+n]. Else it holds true that fk(xj(n+1)+k) < 0. Then g+ k , g\u2212 k must attain values at xj(n+1), xj(n+1)+n such that g\u2212 k (xj(n+1)+k) = \u2212fk(xj(n+1)+k) and g\u2212 k (xj(n+1)+k\u22121) = 0 and g+ k \u22610 on [xj(n+1), xj(n+1)+n]. Also, g+ k and g\u2212 k must be linear on [xj(n+1), xj(n+1)+n]. Finally, the function fk+1 is defined by\nfk+1 := fk \u2212ReLU(g+ k ) + ReLU(g\u2212 k ).\nWe proceed by considering the following auxiliary results which provide a more explicit form of fk and will be useful later for estimating fk(xj(n+1)+k) for 0 \u2264k, l \u2264 n. Recall that these values essentially describe the data which is interpolated in the construction of the g+ k and g\u2212 k , hence they determine W2 and b2. Let us introduce the shorthand notation f j k,l = fk(xj(n+1)+l) for 0 \u2264k, l \u2264n, 0 \u2264 j \u2264m. Then the f j k,l fulfill the following recursion. Lemma 8. In the situation of Lemma 6 let 2 \u2264k \u2264l \u2264n and \u03b1 = R\u03b4. If \u2022 k is even and l is even then f j  = f j  + k\u2212l\u22122 f j .\nWe proceed by considering the following auxiliary results which provide a more explicit form of fk and will be useful later for estimating fk(xj(n+1)+k) for 0 \u2264k, l \u2264 n. Recall that these values essentially describe the data which is interpolated in the construction of the g+ k and g\u2212 k , hence they determine W2 and b2. Let us introduce the shorthand notation f j k,l = fk(xj(n+1)+l) for 0 \u2264k, l \u2264n, 0 \u2264 j \u2264m. Then the f j k,l fulfill the following recursion.\nLemma 8. In the situation of Lemma 6 let 2 \u2264k \u2264l \u2264n and \u03b1 = R\u03b4. If \u2022 k is even and l is even then f j k,l = f j k\u22121,l + k\u2212l\u22122 2(1\u2212\u03b1)f j k\u22121,k\u22121.\n\u2022 k is even and l is odd then f j k,l = f j k\u22121,l + k\u2212l\u22123+2\u03b1 2(1\u2212\u03b1) f j k\u22121,k\u22121. \u2022 k is odd and l is even then f j k,l = f j k\u22121,l + k\u2212l\u22121\u22122\u03b1 2\u03b1 f j k\u22121,k\u22121. \u2022 k is odd and l is odd then f j k,l = f j k\u22121,l + k\u2212l\u22122 2\u03b1 f j k\u22121,k\u22121. Proof. It holds true by the construction in [15, Lemma 2.2] that\nWe determine ReLU(g+ k\u22121(xj(n+1)+l)) and ReLU(g\u2212 k\u22121(xj(n+1)+l)) in terms of fk\u22121 based on the setup of Lemma 6. For that, we recall that for k \u22651 if fk(xj(n+1)+k) \u22650 then g+ k , g\u2212 k must attain values at xj(n+1), xj(n+1)+n such that g+ k (xj(n+1)+k) = fk(xj(n+1)+k) and g+ k (xj(n+1)+k\u22121) = 0 and g\u2212 k \u22610 on [xj(n+1), xj(n+1)+n]. Note that g+ k and g\u2212 k are linear on [xj(n+1), xj(n+1) In case k is even, as then\nNote that then independently of the parity of k it holds g+ k (xj(n+1)+k) = fk(xj(n+1)+k and g+ k (xj(n+1)+k\u22121) = 0 as g+ k is linear on [xj(n+1), xj(n+1)+n]. If fk(xj(n+1)+k) < 0 then g+ k , g\u2212 k must attain values at xj(n+1), xj(n+1)+n such that g\u2212 k (xj(n+1)+k) = \u2212fk(xj(n+1)+k) and g\u2212 k (xj(n+1)+k\u22121) = 0 and g+ k \u22610 on\nNote that then independently of the parity of k it holds g+ k (xj(n+1)+k) = fk(xj(n+1)+k and g+ k (xj(n+1)+k\u22121) = 0 as g+ k is linear on [xj(n+1), xj(n+1)+n].\nIf fk(xj(n+1)+k) < 0 then g+ k , g\u2212 k must attain values at xj(n+1), xj(n+1)+n suc that g\u2212 k (xj(n+1)+k) = \u2212fk(xj(n+1)+k) and g\u2212 k (xj(n+1)+k\u22121) = 0 and g+ k \u22610 o\n(13)\n[xj(n+1), xj(n+1)+n]. Note that g+ k and g\u2212 k are linear on [xj(n+1), xj(n+1)+n]. The concrete values are similar to those for the case fk(xj(n+1)+k) \u22650 with exchanged roles of g+ k , g\u2212 k and opposite signs.\nReturning to identity (13) without loss of generality it suffices to consider the case fk\u22121(xj(n+1)+k\u22121) \u22650. The reason is that both g+ k\u22121(xj(n+1)+l), g\u2212 k\u22121(xj(n+1)+l) \u22650 as l \u2265k, the terms g+ k\u22121, g\u2212 k\u22121 occur in (13) with different signs and g\u2212 k\u22121(xj(n+1)+l) corresponds to g+ k\u22121(xj(n+1)+l) when applying the interpolation formula of g+ k\u22121 to \u2212f j k\u22121,k\u22121 instead of f j k\u22121,k\u22121.\nAs a consequence, we may reduce (13) to\nThe next step is to write g+ k\u22121(xj(n+1)+l) as the linear interpolation between g+ k\u22121(xj( and g+ k\u22121(xj(n+1)+n). By the previous considerations in case of even k (odd k \u22121) it holds true that\nThe next step is to write g+ k\u22121(xj(n+1)+l) as the linear interpolation between g+ k\u22121(xj(n+1) and g+ k\u22121(xj(n+1)+n). By the previous considerations in case of even k (odd k \u22121)\nInserting even l it yields\nwhereas odd l imply\nIn case of odd k (even k \u22121) it holds true that\nInserting even l it yields\nwhereas odd l imply\nTogether with equation (14) this concludes the result.\n(14)\n\nWe proceed by showing how f j k,l can be reduced to an expression containing o evaluations of the form f j 1,i. Lemma 9. In the situation of Lemma 8 let 2 \u2264k \u2264l \u2264n. If \u2022 k is even and l is even then f j k,l = f j 1,l \u2212l\u2212k+2 2(1\u2212\u03b1)f j 1,k\u22121 + l\u2212k+2\u03b1 2(1\u2212\u03b1) f j 1,k\u22122. \u2022 k is even and l is odd then f j k,l = f j 1,l \u2212l\u2212k+3\u22122\u03b1 2(1\u2212\u03b1) f j 1,k\u22121 + l\u2212k+1 2(1\u2212\u03b1)f j 1,k\u22122. \u2022 k is odd and l is even then f j k,l = f j 1,l \u2212l\u2212k+1+2\u03b1 2\u03b1 f j 1,k\u22121 + l\u2212k+1 2\u03b1 f j 1,k\u22122. \u2022 k is odd and l is odd then f j k,l = f j 1,l \u2212l\u2212k+2 2\u03b1 f j 1,k\u22121 + l\u2212k+2\u22122\u03b1 2\u03b1 f j 1,k\u22122. Proof. We prove the assertion via bilevel induction. Induction Start: For k = 2 and even l it follows by Lemma 8 and f j 1,0 = 0 that f j 2,l = f j 1,l \u2212 l 2(1 \u2212\u03b1)f j 1,1 = f j 1,l \u2212l \u22122 + 2 2(1 \u2212\u03b1) f j 1,1 + l \u22122 + 2\u03b1 2(1 \u2212\u03b1) f j 1,0. Similarly for odd l it follows\nSimilarly for odd l it follows\nFor k = 3 and even l we derive\nSimilarly for odd l it follows\nInduction Step: If k + 1 is even and l is even we derive by Lemma 8 and th induction hypothesis that\n \u2212 If k + 1 is even and l is odd we derive\n  Thus, the assertion holds also for k + 1 which finishes the proof.\n\nThe case k = l yields the following identities. Corollary 10. In the situation of Lemma 8 for even k it holds true that\nCorollary 10. In the situation of Lemma 8 for even k it holds true that\nand for odd k that\nHaving the above auxiliary results, we are now ready to prove Lemma 7.\nProof of Lemma 7: It suffices to estimate \u2225W2\u2225\u221eand \u2225b2\u2225\u221e. Recall that the rows (W2)2k and entries (b2)2k correspond to the interpolation problem of g+ k and that the rows (W2)2k+1 and entries (b2)2k+1 correspond to the interpolation problem of g\u2212 k for 1 \u2264k \u2264n specified in the proof of Lemma 8 in the setup of Lemma 7. As a consequence, we may apply the result in Lemma 6. Here we have to be careful regarding the term Y . It is the absolute value of the first interpolated point plus the maximal distance between two consecutive targets for g+ k and g\u2212 k , respectively. In either case f j k,k < 0 or f j k,k \u22650 for 0 \u2264k, l \u2264n, 0 \u2264j \u2264m, it holds true that the distance between two consecutive targets (for both g+ k and g\u2212 k ) corresponding to (12) is bounded by\n \u2212 The absolute value of the first interpolated point is bounded for both g+ k , g\u2212 k by\nAs 0 < \u03b1 = 1 c+1 \u22641 2 we obtain the estimation\n0 < \u03b1 = 1 c+1 \u22641 2 we obtain the estimation\nBy Lemma 6 the entries (b2)k of the parameter b2 are bounded by the absolute value of the first interpolated point given in (15). Again, as a consequence of Lemma 6, we obtain that\nfor 1 \u2264k \u2264n. The next step is to describe the asymptotical behavior of the rows of W2 independently of the row-index. By (12) we have that |(W2)1|\u221e\u2264cR max j (|yj(n+1)+n \u2212yj(n+1)|, |yj(n+1) \u2212yj(n+1)\u22121|) \u2272cnRY.\n(15)\n(16)\nAs (b2)1 = y0 the same asymptotical upper bound applies to |(b2)1|\u221etoo. F k = 1 it holds true that f j 1,1 = yj(n+1)+1 \u2212g0(xj(n+1)+1). Using that\nwe obtain\n|(W2)2|\u221e, |(W2)3|\u221e, |(b2)2|\u221e, |(b2)3|\u221e\u2272c2nR max j |f j 1,1| \u2272c2nRY.\nNext we show this asymptotical behavior for general k \u22652. More concretely, we verify that for k \u22652 it holds that maxj |f j k,k| \u2272cY and thu by (16) that\nThe estimations max( 1\u2212\u03b1 \u03b1 , \u03b1 1\u2212\u03b1) \u2264c and\ninfer that\nmax j |f j k,k| \u22643(c + 1)Y.\n<div style=\"text-align: center;\">max j |f j k,k| \u22643(c + 1)Y.</div>\nRecalling that for k \u22652\n|(W2)2k|\u221e, |(W2)2k+1|\u221e, |(b2)2k|\u221e, |(b2)2k+1|\u221e\u2272c2nR max j |f j k\n(17)\nf j k,k|\nwe conclude that\n# |(W2)2k|\u221e, |(W2)2k+1|\u221e, |(b2)2k|\u221e, |(b2)2k+1|\u221e\u2272c3nRY and finally, the growth of the parameters of \u03c6 is given by P(\u03c6) \u2272max(X, c3nRY ).\nSimilar arguments yield the growth of parameters of the network in [15, Lemma 2.2] for an equidistant grid. Corollary 11. In the situation of Lemma 5 with yi \u22650 assume that there exist m, n \u2208N with \u02dcR = m(n + 1). Then there exists a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively, fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Furthermore, it holds true that\nSimilar arguments yield the growth of parameters of the network in [15, Lemma 2.2] for an equidistant grid.\nCorollary 11. In the situation of Lemma 5 with yi \u22650 assume that there exist m, n \u2208N with \u02dcR = m(n + 1). Then there exists a two hidden layer ReLU network \u03c6 whose layers have widths 2m and 2n + 1, respectively, fulfilling \u03c6(xi) = yi for 0 \u2264i \u2264m(n + 1). Furthermore, it holds true that\nP(\u03c6) \u2272max(X, nRY )\nwith X and Y defined as in (6).\nProof. The arguments are essentially the same as for the proof of Lemma 7. For that reason we omit an explicit proof since the more complicated inequidistant case is dealt with in Lemma 7. Nevertheless, we give the main steps.\nThe main observation is that an inequidistant grid of the form given in Lemma 6 transforms into an equidistant grid of the form given in Lemma 5 for c = 1, though scaled. As a consequence, it holds true that \u03b1 = R\u03b4 = R/(2R) = 1/2. Following the proofs of Lemma 8, Lemma 9 and Corollary 10, one obtains that f j 0,0 = yj(n+1), f j 1,1 = (yj(n+1)+1 \u2212yj(n+1)) \u22121 n(yj(n+1)+n \u2212yj(n+1)) and for k \u22652 f j k,k = f j 1,k \u22122f j 1,k\u22121 + f j 1,k\u22122 for 0 \u2264j \u2264m. By (17) it follows that maxj |f j k,k| \u22646Y . Thus, as by Lemma 5 it holds true that |(W2)2k|\u221e, |(W2)2k+1|\u221e, |(b2)2k|\u221e, |(b2)2k+1|\u221e\u2272nR max j |f j k,k| we conclude that |(W2)2k|\u221e, |(W2)2k+1|\u221e, |(b2)2k|\u221e, |(b2)2k+1|\u221e\u2272nRY and finally, the growth of the parameters of \u03c6 is given by P(\u03c6) \u2272max(X, nRY ).\nThe main observation is that an inequidistant grid of the form given in Lemma 6 transforms into an equidistant grid of the form given in Lemma 5 for c = 1, though scaled. As a consequence, it holds true that \u03b1 = R\u03b4 = R/(2R) = 1/2. Following the proofs of Lemma 8, Lemma 9 and Corollary 10, one obtains that f j 0,0 = yj(n+1), f j 1,1 = (yj(n+1)+1 \u2212yj(n+1)) \u22121 n(yj(n+1)+n \u2212yj(n+1)) and for k \u22652\nfor 0 \u2264j \u2264m. By (17) it follows that maxj |f j k,k| \u22646Y . Thus, as by Lemma 5 it holds true that |(W2)2k|\u221e, |(W2)2k+1|\u221e, |(b2)2k|\u221e, |(b2)2k+1|\u221e\u2272nR max j |f j k,k|\nwe conclude that\n|(W2)2k|\u221e, |(W2)2k+1|\u221e, |(b2)2k|\u221e, |(b2)2k+1|\u221e\u2272nRY and finally, the growth of the parameters of \u03c6 is given by P(\u03c6) \u2272max(X, nRY ).\n\n# We conclude that for the equidistant grid in Corollary 11 the growth of parameters\n# We conclude that for the equidistant grid in Corollary 11 the growth of parameters amounts to\n# We conclude that for the equidistant grid in Corollary 11 the growth of parameters amounts to\nP(\u03c6) \u2272nRY\nP(\u03c6) \u2272nRY and for the special inequidistant grid in Lemma 7 to P(\u03c6) \u2272c3nRY.\nand for the special inequidistant grid in Lemma 7 to P(\u03c6) \u2272c3nRY.\nComplexity estimation in [12, Lemma 5.5]: The result in [12, Lemma 5.5] is the same as [16, Lemma 3.4]. It shows that a two hidden layer ReLU network with d\u2212dimensional input and layers of width N and NL, respectively, of the form \u02dc\u03c6(x) = W3 ReLU(W2 ReLU(W1x + b1) + b2) + b3, can be expressed by a ReLU network \u03c6 with d\u2212dimensional input, width W(\u03c6) = O(N) and depth D(\u03c6) = O(L). More concretely, for g = ReLU(W1x+b1) and h = ReLU(W2g+b2) defining the outputs of the intermediate layers of \u02dc\u03c6, the L + 1 intermediate layers of the proposed deep ReLU network \u03c6 of width O(N) and depth O(L) consist of expressions of the form g, hi, ReLU(si) and ReLU(\u2212si) for 1 \u2264i \u2264L. Dividing W2 \u2208RNL\u00d7N and b2 \u2208RNL evenly into L parts W2,i \u2208RN\u00d7N and b2,i \u2208RN, respectively, the hi are obtained by hi = ReLU(W2,ig + b2,i) for 1 \u2264i \u2264L. Similarly dividing W3 \u2208R1\u00d7NL evenly into parts of length N denoted by W3,i, the si are recursively defined by s0 := 0 and si := si\u22121+W3,ihi for 1 \u2264i \u2264L. We argue that the necessary parameters for constructing these expressions, determining the intermediate layers of \u03c6, consist of entries of the parameters of \u02dc\u03c6 given by Wl, bl for l = 1, 2, 3. The parameters resulting in the output of the first intermediate layer of \u03c6 given by g are W1 and b1 as g = ReLU(W1x+b1). The output of the second layer is given by h1 and g and obtained by W2,1 and b2,1 due to the definition of h1 and constants given by \u00b11 as g = ReLU(g)\u2212ReLU(\u2212g). For 3 \u2264i \u2264L+1 the output of the i\u2212th intermediate layer of \u03c6 is formed by ReLU(si\u22122), ReLU(\u2212si\u22122), hi\u22121 and g. As a consequence of the previous considerations and recursion formula of the si, the parameters necessary to obtain the output of the i\u2212th layer by the previous layer are W2,i\u22121, b2,i\u22121, W3,i\u22122 and constants given by \u00b11 (to map g to g and ReLU(si) with ReLU(\u2212si) to si). Finally, the necessary parameters mapping the penultimate layer of \u03c6 to the output layer consisting of \u03c6(x) = W3h + b3 = sL + b3 are b3, W3,L and again constants given by \u00b11. As a consequence, the asymptotical behavior of the parameters in [16, Lemma 3.4] is given by\n# P(\u03c6) \u2272P(\u02dc\u03c6) \u2272max((|bi|\u221e)i=1,2,3, (|Wi|\u221e)i=1,2,3).\nComplexity estimation in [12, Proposition 4.3]: The result in [12, Proposition 4.3] shows that there exists a ReLU FNN \u03c6 with width W(\u03c6) = O(N1/d) and depth D(\u03c6) = O(L) realizing the step function fulfilling \u03c6(x) = k if x \u2208\n[ k K, k+1 K \u2212\u03b41{k\u2264K\u22122}] for 0 \u2264k \u2264K \u22121 with K = \u230aN1/d\u230b2\u230aL2/d\u230b. The cases d = 1 and d \u22652 are considered separately. Case d = 1: The network \u03c6 is given by \u03c6(x) = \u03c61(x)L + \u03c62(x \u2212M\u22121\u03c61(x)) where M = N2L and \u03c61, \u03c62 are defined as follows. The \u03c61 is a ReLU network with \u03c61(x) = m if x \u2208[ m M , m+1 M \u2212\u03b41{m\u2264M\u22122}] for 0 \u2264m \u2264M \u22121. The \u03c62 is a ReLU network with \u03c62(x) = l if x \u2208[ l ML, l+1 ML \u2212\u03b41{l\u2264L\u22122}] for 0 \u2264l \u2264L \u22122. In view of the considerations on [15, Lemma 2.2] we may apply Lemma 7 to \u03c61 with R = N2L, m = N, n = 2NL \u22121, Y = 1, X = 2, \u03b4 = 1 (c+1)R and thus, obtain\nSimilarly for \u03c62 the result in Lemma 7 is applicable with R = N2L2, m = 1, n = 2L \u22121, Y = 1, X = 2, \u03b4 = 1 (c+1)R and hence,\nP(\u03c62) \u2272c3N2L3.\nAs we have discussed in the paragraph above, the application of [16, Lemma 3.4] to \u03c61, \u03c62 resulting in modified networks with W(\u03c61) = O(N), D(\u03c61) = O(L), W(\u03c62) = O(1) and D(\u03c62) = O(L), does not increase the complexity of the parameters such that by definition of \u03c6 (see also [12, Figure 13]) it holds true that\nP(\u03c6) \u2272c3N2L2(N + L)\nwith W(\u03c6) = O(N) and D(\u03c6) = O(L).\nCase d \u22652: The result in Lemma 7 is applicable to \u03c6 with R = \u230aN1/d\u230b2\u230aL2/d m = \u230aN1/d\u230b, n = 2\u230aN1/d\u230b\u230aL2/d\u230b\u22121, Y = 1, X = 2 and as a consequence,\nP(\u03c6) \u2272c3N3/dL4/d.\nAgain the application of [16, Lemma 3.4] to \u03c6 resulting in a modified network with W(\u03c6) = O(N1/d) and D(\u03c6) = O(L) fulfilling the properties above does not increase the complexity of the parameters. Note that the asymptotical bound for d \u22652 applies also in the case d = 1 but is worse.\nComplexity estimation in [12, Lemma 5.6]: The result in [12, Lemma 5.6] is the same as [16, Lemma 3.6] and corresponds to a certain bit-extraction technique. More concretely, it shows that for given \u03b8m,l \u2208{0, 1} for 0 \u2264m \u2264M \u22121, 0 \u2264l \u2264 L \u22121 with M = N2L there exists a ReLU network \u03c6 with W(\u03c6) = O(N) and D(\u03c6) = O(L) such that \u03c6(m, l) = \ufffdl j=0 \u03b8m,j for 0 \u2264m \u2264M \u22121, 0 \u2264l \u2264L \u22121. The \u03c6 is given by \u03c6(m, l) = \u03c62(\u03c61(m), l + 1) where \u03c61 is a ReLU network mapping m to the unique real number ym which has the coefficients (\u03b8m,l)0\u2264l\u2264L\u22121 in its\nbinary representation and \u03c62 is a ReLU network mapping x, l to the sum of the first l coefficients of the binary representation of x. In view of the considerations on [15, Lemma 2.2] applying Lemma 7 to \u03c61 with R = 1, m = N, n = NL\u22121, Y \u22641, X = N2L yields P(\u03c61) \u2272N2L. The application of [16, Lemma 3.4] to \u03c61 resulting in a modified network with W(\u03c61) = O(N) and D(\u03c61) = O(L) does not increase the complexity of the parameters. To analyze the parameters of the network \u03c62 with W(\u03c62) = O(1) and D(\u03c62) = O(L) we consider the transformations between the layers of \u03c62. For given (\u03b8l)1\u2264l\u2264L and \u03bej the real number attaining binary coefficients (\u03b8l)j\u2264l\u2264L the recursion formulas\nare shown to hold true in [16, Lemma 3.5]. Furthermore, for zl,j = ReLU(\u03b8j + ReLU(l \u2212j + 1) \u2212\u03c3(l \u2212j) \u22121\nzl,j = ReLU(\u03b8j + ReLU(l \u2212j + 1) \u2212\u03c3(l \u2212j) \u22121)\nzl,j = ReLU(\u03b8j + ReLU(l \u2212j + 1) \u2212\u03c3(l \u2212\nit is argued that \ufffdl j=1 \u03b8j = \ufffdL j=1 zl,j. The formulas (18)-(20) are employed in the intermediate layers of \u03c62 to generate \u03bej, \u03b8j, zl,j recursively for 1 \u2264l \u2264L and finally output \ufffdl j=1 \u03b8j = \ufffdL j=1 zl,j in the last layer. As a consequence, it holds true that P(\u03c62) \u22722L due to the multiplication by 2L occurring in the formulas (18) and 19. The fact that the complexity behaves asymptotically exponential in the depth L is undesirable. One can circumvent this by adapting the constructive proof of [16, Lemma 3.5] as follows. One can introduce a network \u03c63 of width equal to 2 and L layers with P(\u03c63) = 2 realizing the multiplication by 2L. Then the network \u03c62 can be modified by applying \u03c63 to \u03bej \u22121/2 (compare to formulas (18) and (19)) right before those intermediate layers of \u03c62 where \u03b8j and \u03bej+1 are generated for 1 \u2264j \u2264L. As a consequence in [16, Lemma 3.5] we obtain P(\u03c62) \u2272L with modified \u03c62. Note that the depth of the modified network is no more linear in L but quadratic and given by L2 + L + 1 = O(L2) and similarly also for the network in [16, Lemma 3.6] which, with the modification above, fulfills\n# P(\u03c6) \u2272N2L.\nComplexity estimation in [12, Lemma 5.7]: The result in [12, Lemma 5.7] corresponds to a modified bit-extraction technique. More concretely, for N, L \u2208N and \u03b8i \u2208{0, 1} for 0 \u2264i \u2264N2L2 \u22121 the network \u03c6 realizes \u03c6(i) = \u03b8i for 0 \u2264i \u2264N2L2 \u22121 with W(\u03c6) = O(N) and D(\u03c6) = O(L). The network \u03c6 fulfills \u03c6(i) = \u03c61(\u03c8(i), i \u2212L\u03c8(i)) \u2212\u03c62(\u03c8(i), i \u2212L\u03c8(i)) where \u03c61, \u03c62, \u03c8 are defined as follows. Defining am,l := \u03b8i if i = mL + l for 0 \u2264m \u2264N2L \u22121, 0 \u2264l \u2264L \u22121, bm,0 = 0 and bm,l = am,l\u22121 for 0 \u2264m \u2264\n(18) (19)\n(20)\nN2L \u22121, 1 \u2264l \u2264L \u22121, the networks \u03c61, \u03c62 fulfill that \u03c61(m, l) = \ufffdl j=0 am,j and \u03c62(m, l) = \ufffdl j=0 bm,j for 0 \u2264m \u2264N2L \u22121, 0 \u2264l \u2264L \u22121. The asymptotics of the parameters of \u03c61, \u03c62 are governed under the previously discussed modifications in [12, Lemma 5.6] by the complexity P(\u03c6i) \u2272N2L where W(\u03c6i) = O(N) and D(\u03c6i) = O(L2) for i = 1, 2. Finally, the \u03c8 is a ReLU network with \u03c8(x) = m if x \u2208[mL, (m + 1)L \u22121] for 0 \u2264m \u2264M \u22121. We may apply Lemma 7 to \u03c8 with R = 1/L, m = N, n = 2NL \u22121, Y = 1, X = N2L2, \u03b4 = 1 = 1 (c+1)R, i.e., c = L \u22121, that P(\u03c8) \u2272NL3. Again the application of [16, Lemma 3.4] resulting in a modified network \u03c8 with W(\u03c8) = O(N) and D(\u03c8) = O(L) does not increase the complexity of the parameters. Thus, the growth of parameters of the realizing network in [12, Lemma 5.7] is given by P(\u03c6) \u2272NL(N + L2).\nComplexity estimation in [12, Proposition 4.4]: The result in [12, Proposition 4.4] shows that fitting partial derivatives of order at most q \u22121 at the corners of the subcubes to which \u03a8 projects to (see [12, Proposition 4.4]) is realizable by a ReLU FNN. More concretely, for N, L, s \u2208N and 0 \u2264\u03bei \u22641 for 0 \u2264i \u2264N2L2 \u22121 there exists a ReLU FNN \u03c6 with W(\u03c6) = O(sN log N) and D(\u03c6) = O(L2 log L) such that |\u03c6(i) \u2212\u03bei| \u2264N\u22122sL\u22122s for 0 \u2264i \u2264N2L2 \u22121 and 0 \u2264\u03c6 \u22641. For that, the ReLU FNN \u03c6j realizing \u03c6j(i) = \u03bei,j for 0 \u2264i \u2264N2L2 \u22121 are introduced for 1 \u2264j \u2264J := \u23082s log(NL + 1)\u2309where \u03bei,j \u2208{0, 1} are such that the real number with binary coefficients given by (\u03bei,j)0\u2264j\u2264J approximates \u03bei with error bounded by 2\u2212J. For \u02dc\u03c6(x) := \ufffdJ j=1 2\u2212j\u03c6j(x) it follows by the considerations in the previous paragraph that P(\u02dc\u03c6) \u2272NL(N + L2). Finally, the network \u03c6 is defined by \u03c6(x) = min(ReLU( \u02dc \u03c6(x)), 1) which, as the minimum is expressable by ReLU FNN with parameters \u00b11 and constant architecture, fulfills the growth of parameters P(\u03c6) \u2272NL(N + L2).\nComplexity estimation in [12, Lemma 5.1]: The result in [12, Lemma 5.1] shows that the function x \ufffd\u2192x2 may be approximated with error N\u2212L on the unit interval by a ReLU FNN \u03c6 with width W(\u03c6) = 3N and depth D(\u03c6) = L. The network \u03c6 is given by \u03c6(x) = x \u2212\ufffdLk i=1 2\u22122iTi(x) with k \u2208N uniquely given such that (k \u22121)2k\u22121 + 1 \u2264N \u2264k2k and Ti sawtooth functions fulfilling Ti(l2\u2212i) = 1 for odd 0 \u2264l \u22642i and Ti(l2\u2212i) = 0 for even 0 \u2264l \u22642i. The complexity of \u03c6 is governed by the growth of parameters of the sawtooth function Tk. The reason is that T1, . . . , Tk which are generated in the first intermediate layer of \u03c6, are transformed to the higher order Ti by application of Tk. By Lemma 5 as Tk(l2\u2212k) = 1 for odd 0 \u2264l \u22642k and Tk(l2\u2212k) = 0 for even 0 \u2264l \u22642k it follows that\nP(\u03c6) \u22722k. By the choice of k it holds 2k \u2264N and consequently the asymptotica behavior of the parameters of \u03c6 fulfills\n# P(\u03c6) \u2272N.\nComplexity estimation in [12, Lemma 5.2]: The result in [12, Lemma 5.2] shows that the function (x, y) \ufffd\u2192xy may be approximated with error of order N\u2212L on the unit square by a ReLU FNN \u03c6 with width W(\u03c6) = 9N and depth D(\u03c6) = L. For \u03c8 denoting the network providing the result of the previous paragraph, the network \u03c6 is defined by \u03c6(x, y) = 2(\u03c8( x+y 2 ) \u2212\u03c8( x 2) \u2212\u03c8( y 2)). As a consequence, we derive that the complexity of \u03c6 follows immediately by the considerations on [12, Lemma 5.1] and fulfills\n# P(\u03c6) \u2272N.\nComplexity estimation in [12, Lemma 4.2]: The result in [12, Lemma 4.2 shows that the function (x, y) \ufffd\u2192xy may be approximated with error of orde N\u2212L on a general square [a, b]2 by a ReLU FNN \u03c6 with width W(\u03c6) = 9N +1 an depth D(\u03c6) = L. For \u03c8 denoting the network providing the result of the previou paragraph, the network \u03c6 is defined by\n\u03c6(x, y) := (b \u2212a)2\u03c8(x \u2212a b \u2212a , y \u2212a b \u2212a) + a ReLU(x + y + 2|a|) \u2212a2 \u22122a|a|.\nThus, we obtain that the complexity of \u03c6 follows immediately by the consideration on [12, Lemma 5.2] and fulfills\nThus, we obtain that the complexity of \u03c6 follows immediately by the considerations on [12, Lemma 5.2] and fulfills P(\u03c6) \u2272N.\nComplexity estimation in [12, Lemma 5.3]: The result in [12, Lemma 5.3] shows that multivariable functions of the form (x1, . . . , xk) \ufffd\u2192x1x2 . . . xk on the k\u2212unit cube may be approximated with error of order N\u22127kL by a ReLU FNN \u03c6 with width W(\u03c6) = O(N) and depth D(\u03c6) = O(L). For \u03c61 denoting the the network providing the result of the previous paragraph, the networks \u03c6i are recursively defined by\n\u03c6i+1(x1, . . . , xi+2) := \u03c61(\u03c6i(x1, . . . , xi+1), ReLU(xi+2))\nfor x1, . . . , xi+2 \u2208R and \u03c6 is defined by \u03c6 := \u03c6k\u22121. Hence, the complexity of P(\u03c6) is a direct corollary of the previous paragraph as the network in [12, Lemma 4.2] is self-composed k \u22121 times to obtain the network in [12, Lemma 5.3]. As a consequence, we derive that P(\u03c6) \u2272N.\nComplexity estimation in [12, Proposition 4.1]: The result in [12, Proposition 4.1] shows that multivariable polynomials P(x) = x\u03b1 of d variables and degree \u02dck := |\u03b1|1 \u2264k can be approximated on the d\u2212unit cube with error of order N\u22127kL by a ReLU FNN \u03c6 with width W(\u03c6) = O(N + k) and depth D(\u03c6) = O(k2L). Denoting by \u03c8 the network providing the result of the previous paragraph and by L : Rd \u2192Rk the affine linear map which will be recalled from [12, Lemma 5.3] in the following, the network \u03c6 is defined by \u03c6 = \u03c8 \u25e6L. Given x \u2208Rd, z \u2208R \u02dck is defined by zl = xj if \ufffdj\u22121 i=1 \u03b1i < l \u2264\ufffdj i=1 \u03b1i for 1 \u2264j \u2264d, i.e., z is the entrywise replication of x with respect to \u03b1. Then x \u2208Rd is mapped to (z, 1, . . . , 1)T \u2208Rk by L. The affine linear map L can be expressed by a ReLU FNN where each nonzero scalar parameter is equal to 1. Thus, the complexity of P(\u03c6) is a direct corollary of the considerations of the previous paragraph and fulfills\nP(\u03c6) \u2272N.\n# P(\u03c6) \u2272N.\n# Complexity estimation in [12, Theorem 2.2]: We recall that the approximating neural network in [12, Theorem 2.2] on \u2126([0, 1]d, R, \u03b4) with R = \u230aN1/d\u230b2\u230aL2/d\u230b is given by\n\u03c6(x) := \ufffd \u2225\u03b1\u22251\u2264q\u22121 \u03d5( 1 \u03b1!\u03c6\u03b1(\u03a8(x)), P\u03b1(x \u2212\u03a8(x)))\nfor x \u2208Rd, where the role of the subnetworks \u03a8, P\u03b1, \u03c6\u03b1, \u03d5 is as follows:\n\u2022 The ReLU FNN \u03a8 realizes projections of subcubes of [0, 1]d to exactly one corner of the subcube based on one-dimensional step functions \u03c8 with \u03a8(x) = (\u03c8(x1), . . . , \u03c8(xd))T/R for x \u2208[0, 1]d (see considerations on [12, Proposition 4.3] for construction of \u03c8). The realized analysis revealed that the growth of parameters of \u03a8 fulfills\nP(\u03a8) \u2272c3N3/dL4/d.\n# P(\u03a8) \u2272c3N3/dL4/d.\n\u2022 The ReLU FNN P\u03b1 achieves an approximation of multinomials of order at most q \u22121 (see considerations on [12, Proposition 4.1]) and is shown in the complexity estimations above to fulfill\n# P(P\u03b1) \u2272N.\n\u2022 The ReLU FNN \u03c6\u03b1 achieves fitting partial derivatives of f of order at most q \u22121 at the corners of the subcubes to which \u03a8 projects to (see considerations on [12, Proposition 4.4]). For the growth of parameters we derive P(\u03c6\u03b1) \u2272NL(N + L2).\nThe width of the realizing network fulfills W(\u03c6) = O(qd+1N log(8N)) and the modified depth (see the considerations on the complexity estimation in [12, Lemma 5.6]) D(\u03c6) = O(q2L2 log(4L)).\nThis concludes the the considerations on [12, Theorem 2.2]. Next we analyze the growth of parameters of the network achieving an extension of the approximation to the whole domain based on [12, Theorem 2.1].\n# 3.2 Estimation of P(\u03c6) in [12, Theorem 2.1]:\nThe result shows that given f \u2208C([0, 1]d) and a ReLU FNN \u02dc\u03c6 approximating f uniformly with error \u01eb > 0 outside some trifling region (3) with respect to \u03b4 > 0 then there exists some ReLU FNN \u03c6 approximating f uniformly with error given by \u01eb + d\u03c9f(\u03b4) where the modulus of continuity \u03c9f is defined as\n\u03c9f(r) = sup{|f(x) \u2212f(y)| : \u2225x \u2212y\u2225\u2264r, x, y \u2208[0, 1]d}\nfor r > 0. The approximating ReLU FNN \u03c6 is constructed as follows. Given \u02dc\u03c6 approximating f outside a trifling region (3) as in [12, Theorem 2.2], the networks \u03c6i for 0 \u2264i \u2264d are set as \u03c60 = \u02dc\u03c6 and inductively\n\u03c6i+1(x) = mid(\u03c6i(x \u2212\u03b4ei+1), \u03c6i(x), \u03c6i(x + \u03b4ei+1))\nfor 0 \u2264i \u2264d \u22121. Here, the median function mid is constructed by a ReLU FNN as mid(x1, x2, x3) = ReLU(x1+x2+x3)\u2212ReLU(\u2212x1\u2212x2\u2212x3)\u2212max(x1, x2, x3)\u2212min(x1, where max(x, y) = 1 2(ReLU(x+y)\u2212ReLU(\u2212x\u2212y)+ReLU(x\u2212y)+ReLU(\u2212x+y)). Finally, the approximating ReLU FNN, including the trifling region, is given by \u03c6 = \u03c6d. As a consequence of the construction we derive immediately that P(\u03c6i+1) \u2264max(P(\u03c6i), 1, \u03b4).\nmid(x1, x2, x3) = ReLU(x1+x2+x3)\u2212ReLU(\u2212x1\u2212x2\u2212x3)\u2212max(x1, x2, x3)\u2212min(x1, x2, x3) where max(x, y) = 1 2(ReLU(x+y)\u2212ReLU(\u2212x\u2212y)+ReLU(x\u2212y)+ReLU(\u2212x+y)). Finally, the approximating ReLU FNN, including the trifling region, is given by \u03c6 = \u03c6d. As a consequence of the construction we derive immediately that P(\u03c6i+1) \u2264max(P(\u03c6i), 1, \u03b4).\nP(\u03c6) \u2264max(P(\u02dc\u03c6), 1, \u03b4) \u2264P(\u02dc\u03c6)\nwhere in the last inequality we have used that 0 < \u03b4 < 1 and 1 \u2264P(\u02dc\u03c6). Hence, the asymptotical behavior of the parameters of \u03c6 is governed by the suprema of the parameters of the reduced approximation \u02dc\u03c6.\nThis concludes the considerations on [12, Theorem 2.1] and shows that the approximating network \u03c6 constructed in [12] under the previously discussed modification attains an error of order N\u22122q/dL\u22122q/d + d\u03c9f(\u03b4) for \u01eb = N\u22122q/dL\u22122q/d and growth of parameters\nP(\u03c6) \u2272max(NL(N + L2), c3N3/dL4/d)\nP(\u03c6) \u2272max(NL(N + L2), c3N3/dL4/d) with width W(\u03c6) = O(qd+13dN log(8N)) and depth D(\u03c6) = O(q2L2 log(4L) + d).\n13dN log(8N)) and depth D(\u03c6) = O(q2L2 log(4L) + d).\n# 3.3 Estimation of P(\u03c6) in [12, Theorem 1.1]:\n P Finally, in the main result [12, Theorem 1.1], the trifling region is chosen small enough (i.e., the parameter \u03b4 > 0 is chosen sufficiently small), to recover the approximation rate N\u22122q/dL\u22122q/d on the whole domain [0, 1]d in terms of the width N and depth L. Following the proof of [12, Theorem",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the analysis of fully connected feed forward ReLU neural networks in the context of approximating smooth functions. It emphasizes the asymptotic growth of the parameters of these networks as opposed to traditional studies focusing on the universal approximation properties achieved through increasing network complexity, such as width and depth.",
        "problem": {
            "definition": "The problem under investigation is how the parameters of neural networks that approximate smooth functions behave asymptotically, particularly in the context of error analysis and consistency results for neural network training.",
            "key obstacle": "A significant challenge is understanding the growth behavior of the parameters in relation to the network's architecture, especially given that existing literature has not extensively addressed this aspect."
        },
        "idea": {
            "intuition": "The idea was inspired by the need for a better understanding of the relationship between network parameters and their capacity to approximate complex functions.",
            "opinion": "The authors propose that analyzing the asymptotic growth of parameters is crucial for error analysis and consistency in neural network training.",
            "innovation": "The primary innovation lies in demonstrating that for a ReLU architecture achieving state-of-the-art approximation error, the parameters grow at most polynomially, which is a significant improvement over previous exponential growth results."
        },
        "Theory": {
            "perspective": "The theoretical perspective is based on approximation theory, particularly focusing on the relationship between the parameters of neural networks and their approximation capabilities.",
            "opinion": "The authors assume that the growth of parameters is fundamentally linked to the network's ability to approximate smooth functions effectively.",
            "proof": "The proof involves deriving bounds on the growth of parameters through explicit constructions and comparisons with existing results in the literature."
        },
        "experiments": {
            "evaluation setting": "The evaluation involves comparing the asymptotic growth of parameters of ReLU networks with existing results, particularly focusing on the approximation of functions in Sobolev spaces.",
            "evaluation method": "The evaluation method includes theoretical comparisons and derivations based on the architecture of the networks and their corresponding parameter growth."
        },
        "conclusion": "The paper concludes that the parameters of ReLU neural networks can be controlled to grow at most polynomially, which enhances our understanding of their approximation capabilities and has implications for training consistency.",
        "discussion": {
            "advantage": "One advantage of this paper is its focus on the asymptotic behavior of parameters, which provides new insights into neural network training and error analysis.",
            "limitation": "A limitation is that the results are primarily theoretical and may require empirical validation in practical scenarios.",
            "future work": "Future work could explore empirical validations of the theoretical findings and investigate the implications of parameter growth in different types of neural network architectures."
        },
        "other info": [
            {
                "info1": "The paper provides a comprehensive comparison of its results with existing literature, highlighting the improvements in parameter growth rates.",
                "info2": {
                    "info2.1": "The results are particularly relevant for high-dimensional input scenarios.",
                    "info2.2": "The authors do not claim optimality of the asymptotic behavior but aim to provide insights into the dynamics of parameter growth."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper addresses the analysis of fully connected feed forward ReLU neural networks in the context of approximating smooth functions, emphasizing the relationship between network parameters and their capacity to approximate complex functions."
        },
        {
            "section number": "6.3",
            "key information": "A significant challenge identified in the paper is understanding the growth behavior of the parameters in relation to the network's architecture, which relates to the computational demands and scalability of neural network technologies."
        },
        {
            "section number": "6.4",
            "key information": "The paper discusses the need for error analysis and consistency results for neural network training, which connects to the limitations of neural networks in terms of generalization and reasoning capabilities."
        },
        {
            "section number": "7.1",
            "key information": "Future work suggested in the paper includes exploring empirical validations of theoretical findings and investigating the implications of parameter growth in different types of neural network architectures, highlighting future opportunities and challenges."
        }
    ],
    "similarity_score": 0.5301752984639768,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0746_artif/papers/On the growth of the parameters of approximating ReLU neural networks.json"
}