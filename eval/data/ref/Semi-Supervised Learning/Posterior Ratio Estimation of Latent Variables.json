{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2002.06410",
    "title": "Posterior Ratio Estimation of Latent Variables",
    "abstract": "Density Ratio Estimation has attracted attention from the machine learning community due to its ability to compare the underlying distributions of two datasets. However, in some applications, we want to compare distributions of random variables that are \\emph{inferred} from observations. In this paper, we study the problem of estimating the ratio between two posterior probability density functions of a latent variable. Particularly, we assume the posterior ratio function can be well-approximated by a parametric model, which is then estimated using observed information and prior samples. We prove the consistency of our estimator and the asymptotic normality of the estimated parameters as the number of prior samples tending to infinity. Finally, we validate our theories using numerical experiments and demonstrate the usefulness of the proposed method through some real-world applications.",
    "bib_name": "liu2020posteriorratioestimationlatent",
    "md_text": "# Posterior Ratio Estimation of Latent Variables\nSong Liu, Yulong Zhang, Mingxuan Yi\u2217, Mladen Kolar\u2020\nAbstract\nDensity Ratio Estimation has attracted attention from machine learning community due to its ability of comparing the underlying distributions of two datasets. However, in some applications, we want to compare distributions of random variables that are inferred from observations. In this paper, we study the problem of estimating the ratio between two posterior probability density functions of a latent variable. Particularly, we assume the posterior ratio function can be well-approximated by a parametric model, which is then estimated using observed information and prior samples. We prove consistency of our estimator and the asymptotic normality of the estimated parameters as the number of prior samples tending to infinity. Finally, we validate our theories using numerical experiments and demonstrate the usefulness of the proposed method through some real-world applications.\n# 1 Introduction\nComparing the underlying distributions of two given datasets has been an important task and has a wide range of applications. For example, change detection algorithms [8] compare datasets collected at different time points and report how the underlying distribution has shifted over time; transfer learning [14] uses the estimated differences between two datasets to efficiently share information between different tasks. Generative Adversarial Net (GAN) [4] learns an implicit generative model whose output minimizes the differences between an artificial dataset and a real dataset. Various computational methods have been proposed for comparing underlying distributions given two datasets. For example, Maximum Mean Discrepancy (MMD) [5] computes the distance between the kernel mean embeddings of two datasets in Reproducing Kernel Hilbert Space (RKHS). Density ratio estimation (DRE) [18] estimates the ratio function between two probability densities. The estimated ratio can be used to approximate various statistical divergences [12, 7]. Recently, Wasserstein distance (and Optimal Transport distance) as an alternative to statistical divergence has been explored for many learning tasks [2, 1] and efficient algorithms for computing such a distance from two sets of samples have been proposed [3].\nSometimes we are not interested in comparing distributions of observable random variables, but distributions of latent random variables given observations. For example, we want to monitor the engine vibration pattern via readings from a sensor placed on the engine. However, our readings may be noisy as the sensor is also susceptible to vibrations of other nearby components. Thus, instead of directly monitoring the observed sensor readings, it makes sense to compare the latent vibration patterns given such readings. In the classic Bayesian setting, a latent pattern is described by a posterior probability, which can be inferred from observations, a prior, and a likelihood. Thus, comparing distributions of latent variables can be framed as a comparison between two posteriors. In this paper, we consider the problem of estimating the ratio between two posteriors probabilities. One straightforward approach to obtain the posterior ratio is to represent two posterior probabilities in the form of likelihood functions and priors using Bayes\u2019 rule. However, in many cases, we do not have explicit expressions of priors, but only some simulated samples from the priors. In the above example, coming up with an explicit prior for the engine vibration is almost impossible as it is governed by a complex physics process. However, it is possible to simulate samples from the latent variable under different settings (e.g., \u201cnormal condition\u201d or \u201cdamaged engine\u201d), either by conducting experiments or running computer simulations. One simple solution to the \u201cunknown prior\u201d problem above is to estimate prior densities from prior samples and then plug the estimates into the posterior ratio formula. However, it is unlikely that we know how to model the prior distribution, while non-parametric estimators such as kernel density estimators (KDE) [16, 13] become infeasible when the dimensionality is high. Alternatively, one can estimate the ratio of priors directly using DRE. To obtain the posterior ratio, we only need the ratio of priors, thus we should not try to solve two separate and more generic estimation problems individually. However, this likelihood-agnostic approach may not be optimal as it cannot guarantee that the estimated ratio would behave well with respect to the given likelihood functions and observations which are simply ignored during the estimation. Following this rationale, we develop a novel algorithm which estimates the ratio between two posterior probabilities directly. The algorithm is referred as Posterior Ratio Estimation (PRE) and can be regarded as an analogue of DRE for posterior probabilities. We assume two likelihood functions and two sets of prior samples are given. Our algorithm approximates the true posterior ratio by minimizing the KL divergence from one posterior to the other posterior reweighed by a ratio model. We prove that the estimated model parameters eventually converge to the true minimizer of the KL divergence, as the number of the prior samples increases. The estimated parameters are asymptotically normal, which is useful for statistical inference. We evaluate the performance of PRE in two applications: Latent signal detection and local linear classifier extraction. Promising results are obtained.\n# 2 Posterior Ratio Estimation\n# 2.1 Problem Setting\nFormally, P and Q are two joint probability distributions of two random variables Y and X. We shorten the marginal/conditional probability of P and Q as p(x) := P(X = x), p(x|y) := P(X = x|Y = y), etc. Suppose we have \u2022 yp, yq: two observations from P(Y ) and Q(Y ) respectively; \u2022 p(yp|x), q(yq|x): two likelihood functions; \u2022 Xp := {x(i) p }np i=1, Xq := {x(i) q }nq i=1: two sets of samples from P(X) and Q(X) respectively. We want to estimate the posterior density ratio p(x|yp) q(x|yq) up to a constant that is independent of x.\n\u2022 Xp := {x(i) p }np i=1, Xq := {x(i) q }nq i=1: two sets of samples from P(X) and Q(X) respectively We want to estimate the posterior density ratio p(x|yp) q(x|yq) up to a constant that is independent of x.\n# 2.2 Posterior Ratio Model\nWe model the posterior ratio p(x|yp) q(x|yq) using a parametric function\nand f is a feature function chosen beforehand. For example, the polynomial feature, f(x) = [x, x2]\u22a4(where the power is applied over all elements of x), is shown to have good performance in our experiments. Z(\u03b4) ensures that the ratio model is properly normalized: for any \u03b4, \ufffd q(x|yq)r(x; \u03b4)dx = 1. Note that if both p(x|yp) and q(x|yq) are from the exponential family with the sufficient statistic f, then this density ratio model is well-specified.\n# 2.3 Posterior Ratio Estimation\nOur estimation strategy is similar to the one used in a density ratio estimator called KullbackLeibler Importance Estimation Procedure (KLIEP) [19, 20]. To estimate \u03b4 in the ratio model r(x; \u03b4), we minimize the KL divergence from p(x|yp) to r(x; \u03b4)q(x|yq):\nwhere C and p(yp) do not depend on \u03b4 and the last equality is due to Bayes\u2019 rule. Ignoring constants that are not dependent on \u03b4, we obtain the following minimization problem: min \u03b4 \u2212 \ufffd p(yp|x)p(x) log r(x; \u03b4)dx. (2)\nwhere C and p(yp) do not depend on \u03b4 and the last equality is due to Bayes\u2019 rule. Ignoring constants that are not dependent on \u03b4, we obtain the following minimization problem:\n(1)\n(2)\nmin \u03b4 Ep(x) [lp(x) \u00b7 \u27e8\u2212\u03b4, f(x)\u27e9] + Ep(x) [lp(x)] \u00b7 log Eq(x)\nwhere C\u2032 = \u2212\u02c6p(yp) log \u02c6q(yq) and is independent of \u03b4. Note that if lp = 1 and lq = 1, then \u02c6\u03b4 coincides with the KLIEP estimator of p(x) q(x). The above optimization problem is convex with respect to \u03b4 and the gradient descent algorithm can be employed to find \u02c6\u03b4.\n# 2.4 Relationship with Density Ratio Est\nDRE [18, 11] approximates the ratio function between two probability densities given two sets of samples, which are drawn from these probability distributions. As we previously mentioned, one of the DRE algorithms, KLIEP shares the same KL divergence minimization criterion with PRE. However, DRE learns a ratio of probability densities of two observed random variables. If one wants to estimate p(x|yp) q(x|yq) using KLIEP, one must directly draw samples from p(x|yp) and q(x|yq), which themselves are difficult problems. In comparison, PRE avoids this problem by using Bayes\u2019 rule in the objective, and estimates the ratio of two latent probability densities given likelihood functions and samples from their priors without needing to sample from the posterior distributions. However, there is a \u201cplugin\u201d method to obtain posterior ratio by using DRE rather than PRE: We can obtain p(x) q(x) by performing the regular DRE using datasets Xp and Xq, then\n(3)\n(4)\n(5)\nmultiply it by the ratio of likelihood p(yp|x) q(yq|x). Due to Bayes\u2019 rule, this estimator approximates the true posterior ratio up to a constant that is independent of x. We refer this estimator as the \u201cplugin\u201d estimator since rather than estimating the posterior ratio, we \u201cplugin\u201d the estimate of prior ratio and likelihood to obtain a new estimator. Its performance is compared in Section 5.2.\n# 3 Consistency and Asymptotic Normality of \u02c6\u03b4\nFirst we establish the consistency of \u02c6\u03b4 as an estimator of the posterior ratio parameters and analyze its sufficient conditions. Under the assumption \u221e> \u02c6p(yp) > 0, let us denote the optimization problem (5) as argmin \u03b4 \u2113(\u03b4) \u00b7 \u02c6p(yp), where \u2113(\u03b4) is simply the objective in (5) rescaled by 1/\u02c6p(yp).\nFirst we establish the consistency of \u02c6\u03b4 as an estimator of the posterior ratio parameters and analyze its sufficient conditions. Under the assumption \u221e> \u02c6p(yp) > 0, let us denote the optimization problem (5) as argmin \u03b4 \u2113(\u03b4) \u00b7 \u02c6p(yp), where \u2113(\u03b4) is simply the objective in (5)\nNotations. \u2225a\u2225is the \u21132 norm of a vector a. \u2225A\u2225is the spectral norm of a matrix A. Ball(a, R) is a \u21132 ball centered at a with radius R. \u2207af(a0) is the gradient of f(a) evaluated at a0, while \u22072 af(a) is the Hessian of f. \u03bbmin(A) is the minimum eigenvalue of a matrix A. P\u2192and \u21dddenote convergence in probability and in distribution, respectively.\n# 3.1 Consistency of \u02c6\u03b4 and Its Sufficient Conditions\nWe first state a main theorem specifying a list of sufficient conditions under which our estimator \u02c6\u03b4 converges to \u03b4\u2217in probability as np \u2227nq \u2192\u221e. Then we show these conditions are satisfied with high probability when similar conditions are imposed on some population\nWe first state a main theorem specifying a list of sufficient conditions under which our estimator \u02c6\u03b4 converges to \u03b4\u2217in probability as np \u2227nq \u2192\u221e. Then we show these conditions are satisfied with high probability when similar conditions are imposed on some population quantities. Assumption 1. \u03b4\u2217, which is the minimizer of (3), is unique. Since (3) is convex and twice continuously differentiable with respect to \u03b4, a sufficient condition of Assumption 1 is that the Hessian of (3) is positive definite for all \u03b4. Theorem 1. Suppose Assumption 1 holds,\nAssumption 1. \u03b4\u2217, which is the minimizer of (3), is unique. Since (3) is convex and twice continuously differentiable with respect to \u03b4, a sufficient condition of Assumption 1 is that the Hessian of (3) is positive definite for all \u03b4. Theorem 1. Suppose Assumption 1 holds,\nwith probability at least \u03f5R1 and \u2203R2 > 0, Cr > 1,\n\ufffd \ufffd with probability at least 1 \u2212\u03f5R2. Then \u2203N such that np \u2227nq > N, we have \u02c6\u03b4 P\u2192\u03b4\u2217with probability 1 \u2212\u03f5R1 \u2212\u03f5R2. The proof can be found in the supplementary materials. Although Theorem 1 is established on a set of inequalities of random variables, we show these sample conditions hold with high probability when their population counterparts hold.\n(6)\n(7)\n# 3.2 Eigenvalue Lowerbound of \u22072 \u03b4\u2113(\u03b4)\nLet us define a few new notations:\n\ufffd \ufffd where rnq can be seen as the empirical version of our posterior ratio model. Now we analyze sufficient conditions (7) in Theorem (1), which states that the minimum eigenvalue of \u22072 \u03b4\u2113(\u03b4) is lower-bounded within the neighbourhood of \u03b4\u2217. On one hand, it is easy to see that if there exists a \u03a3 \u2208Rd\u00d7d, \u03a3 \u2248\u22072 \u03b4\u2113(\u03b4), \u2200\u03b4 \u2208\u2206n, then inf\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)] \u2248 \u03bbmin [\u03a3]. More formally, Wely\u2019s inequality [6] allows us to lower-bound inf\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)]:\n\ufffd \ufffd On the other hand, it can be seen that\n\u22072 \u03b4\u2113(\u03b4) = \u02c6Eq(x) \ufffd rnq(x; \u03b4)f \u2032 q(x)f \u2032 q(x)\u22a4\ufffd \u2212\u02c6Eq(x) \ufffd rnq(x; \u03b4)f \u2032 q(x) \ufffd Eq(x) \ufffd rnq(x; \u03b4)f \u2032 q(x) \ufffd\u22a4.\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd By using the newly defined feature function f \u2032 q, we are able to write the Hessian in the form of a sample covariance matrix of f \u2032 q weighted by the ratio model rnq. The convergence of such a density ratio reweighted sample covariance matrix has been studied for classic KLIEP algorithm [9]. We state the following proposition:\nProposition 2. (Lemma 4 and 5 in [9]) Suppose M \u22121 \u2264r(x; \u03b4) \u2264M, then \u2200\u03b4  \u2206n, P (\u2225\u22072 \u03b4\u2113(\u03b4) \u2212\u03a3\u2225\u221e\u2264\u03f5) \u2264C \u00b7 exp(\u2212C\u2032\u03f52n), where C, C\u2032 > 0 are constants and\n\u03a3 := Eq(x) \ufffd r(x; \u03b4)f \u2032 q(x)f \u2032 q(x)\u22a4\ufffd \u2212Eq(x) \ufffd r(x; \u03b4)f \u2032 q(x) \ufffd Eq(x) \ufffd r(x; \u03b4)f \u2032 q(x) \ufffd\u22a4.\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd Notice that \u03a3 is the Hessian of the population objective (3). Considering \u2206n \u2282Rd is a compact Euclidean space, we can see that sup\u03b4\u2208\u2206n \u2225\u03a3 \u2212\u22072 \u03b4\u2113(\u03b4)\u2225= op(1). Combining this with (8), we can see that inf\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)] \u2265\u03bbmin(\u03a3) \u2212op(1). Therefore, our analysis shows the gap of the lowest eigenvalue between \u22072 \u03b4\u2113(\u03b4) and \u03a3 should be shrinking as nq \u2192\u221e. Therefore, as long as \u03a3 is bounded away from 0, when nq is large enough, the \u03bbmin(\u03a3) is also bounded away from 0. This claim will be verified later via numerical experiments in supplementary materials, Section 10.1.\n# 3.3 Convergence of \u2207\u03b4\u2113(\u03b4\u2217)\n \u2207 Now we analyze another condition required for the consistency: (6) states \u2207\u03b4\u2113(\u03b4\u2217) should converge to zero in terms of \u21132 norm at the rate 1 \u221anp\u2227nq . This is guaranteed with high probability:\n \u2207 Now we analyze another condition required for the consistency: (6) states \u2207\u03b4\u2113(\u03b4\u2217) should converge to zero in terms of \u21132 norm at the rate 1 \u221anp\u2227nq . This is guaranteed with high probability: Proposition 3. Suppose \u2203M > 0, M \u22121 \u2264r(x; \u03b4) \u2264M, \u2203K < \u221e, \u2225Ep(x) \ufffd f \u2032 p(x) \ufffd \u2225\u2264K, \u2203L > 0, L\u22121 \u2264lp(x) \u2264L, \u2200x. Then \u2203C, C\u2032 > 0, P (\u2225\u2207\u03b4\u2113(\u03b4\u2217)\u2225\u221e\u2265\u03f5) \u2264C exp [\u2212C\u2032\u03f52(np \u2227nq)]\n(8)\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd Moreover, we can see \u2225a\u2225\u2264|p(yp)\u2212\u02c6p(yp)| p(yp)\u00b7\u02c6p(yp) \u00b7 \u2225Ep(x) \ufffd f \u2032 p(x) \ufffd \u2225due to Holder\u2019s inequality, thus knowing lp are bounded, Hoeffding inequality shows P [\u2225a\u2225\u221e\u2265\u03f5] \u2264C\u2032 1 exp(\u2212C\u2032\u2032 1\u03f5np) where C\u2032 1, C\u2032\u2032 1 > 0 are constants. It can be seen that b is the difference between a simple sample average over p and another sample average over q reweighted by a ratio model rnq. The convergence of such a term has also been studied in the classic KLIEP algorithm. Here we directly invoke Lemma C.2 in [9]: P [\u2225b\u2225\u221e\u2265\u03f5] \u2264C\u2032 2 exp(\u2212C\u2032\u2032 2\u03f52 min(np, nq)) where C\u2032 2, C\u2032\u2032 2 > 0 are constants. . Combining the boundedness of a and b using triangle inequality yields desired results.\n# 3.4 Asymptotic Normality of \u02c6\u03b4\nGiven the consistency results above, we now establish the normality of the estimator (5). Let us define \u03a3p := Varp(x) \ufffd f \u2032 p(x) \ufffd , \u03a3q := Varq(x) \ufffd r(x; \u03b4\u2217)f \u2032 q(x) \ufffd , where Varp(x) [g(x)] is the covariance operator of a random vector g(x) with respect to p(x). Theorem 2. If \u02c6\u03b4 P\u2192\u03b4\u2217, sup\u03b4\u2208\u2206n \u2225\u22072 \u03b4\u2113(\u03b4) \u2212\u03a3\u2225 P\u21920, then as np \u2192\u221e, nq \u2192\u221eand 0 < np nq < \u221e,\nGiven the consistency results above, we now establish the normality of the estimator (5) Let us define \u03a3p := Varp(x) \ufffd f \u2032 p(x) \ufffd , \u03a3q := Varq(x) \ufffd r(x; \u03b4\u2217)f \u2032 q(x) \ufffd , where Varp(x) [g(x)] is the covariance operator of a random vector g(x) with respect to p(x).\nTheorem 2. If \u02c6\u03b4 P\u2192\u03b4\u2217, sup\u03b4\u2208\u2206n \u2225\u22072 \u03b4\u2113(\u03b4) \u2212\u03a3\u2225 P\u21920, then as np \u2192\u221e, nq \u2192\u221ean 0 < np nq < \u221e,\nThe proof can be found in supplementary materials. The conditions under which the uniform convergence holds have been studied in Section 3.2. In practice, we can use \u02c6\u03b4 to replace \u03b4\u2217and use sample average to replace expectations when calculating the above asymptotic covariance.\n# 4 Lagrangian Dual of (5)\nIn some cases, the direct computation of (5) is infeasible: For example, when the feature function f is in an RKHS [17], f(x) may not be evaluated explicitly. Therefore, we study an alternative to (5). Denote f \u2032 pn := lp \u02c6p(yp). We can consider the following bi-level optimization problem:\n\u02c6\u03b4dual := argmin \u03b4 Eqn(\u03b4) = argmin \u03b4 \u2225\u02c6Eq(x) \ufffd rnq(x; \u03b4)f \u2032 q(x) \ufffd \u2212\ufffdnq i=1 \u02c6\u00b5if(x(i) q )\u2225,\n(9)\n\ufffd and Rn is a constant dependent on np \u2227nq. If Rn = 0, (10) is the Lagrangian dual of (5). The proof can be found in the supplementary material. We show \u02c6\u03b4dual is also consistent. Theorem 3. Suppose Assumption 1 holds, Eqn(\u02c6\u03b4dual) \u2264C0, (6) holds with probability at least 1 \u2212\u03f5R1 and \u2203R2 > 0, R3 \u22650, Cr > 1,\nwith probability at least \u03f5R2, then \u2203Rn \u2264 R3 \u221anp\u2227nq such that \ufffd\ufffd\ufffd\u03b4\u2217\u2212\u02c6\u03b4dual \ufffd\ufffd\ufffd\u2264R1+R3 R2 1 \u221anp\u2227nq + C R with probability at least 1 \u2212\u03f5R1 \u2212\u03f5R2.\n\ufffd \ufffd The proof can be found in Section 10 in the supplementary material. In our experiments, we find Eqn(\u02c6\u03b4dual) is usually around 10\u22127, close to the the tolerance of optimization software. Therefore, in practice, we can regard C0 \u22480. Moreover, in this formulation, f only occurs in the inner product thus the evaluation of (10) can be evaluated using kernel trick when f is in RKHS. However, an RKHS version of PRE will be a future work.\n# 5 Numerical Experiments and Applications\n# 5.1 Asymptotic Normality of \u02c6\u03b4\nIn this experiment, we validate the asymptotic distribution discussed in Section 3.4 under a finite-sample setting. We generate two sets of 100 i.i.d. samples from N(0.5, 0.12) as yp \u2208R100 and yq \u2208R100 and set the likelihood function lp(x) = lq(x) := N([x1 \u00b7 1, x2 \u00b7 1], 102 \u00b7 I), x \u2208R2, where 1 is a 50-dimensional vector filled with ones and I is an identity matrix. 500 prior samples Xp and Xq are drawn from p(x) = q(x) = N([0, 0], I). np = nq = n, f(x) = x. Then \u02c6\u03b4 is calculated using (5) given Xp, Xq, yp, yq, lp and lq. We run the estimation of \u02c6\u03b4 5000 times with different random seeds and scatter-plot \u221an\u02c6\u03b4 in Figure 1. We also plot 95% confidence interval (CI) predicted by Theorem 2 in black on the same plot. It can be seen that the overall distribution of \u221an\u02c6\u03b4 fits the shape of CI well. 249 (4.98%) simulations are outside of the confidence interval. We also compare the empirical distribution of \u02c6\u03b41 and \u02c6\u03b42 with their theoretical pred plots (qq-plot) in Figure 1.\n(10)\n(11)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4dc4/4dc4ce57-637b-4092-9388-6de16e07ff3a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Simulation vs. Prediction. dictions on the quantile quantile</div>\nStrictly speaking, in this experiment, p(x|yp) \u0338= q(x|yq) since yp \u0338= yq. However, as we take 100 samples from the same distribution as yp and yq, given this strong evidence, \u2113p(z) \u2248\u2113q(z). Thus \u03b4\u2217\u22480. Both plots show that the prediction made by Theorem 2 matches the simulations well.\n# 5.2 Application: Latent Signal Detectio\nGiven a reference time-sequence yp \u2208Rd that is drawn from a \u201cbackground\u201d distribution p(y), we would like to know whether a testing sequence yq \u2208Rd is standing out from the background or not. Moreover, y is usually a noisy version of a latent signal x and we are more interested in detecting the latent signal. In change detection literature, a common strategy is checking if these two sequences are significantly different in terms of some distance measure. For example, let yp or yq be the vibration of a car engine at a certain time. Though it is difficult to study yp or yq directly, its latent vibration pattern x can be understood by analyzing the mechanics of the engine. We can prepare simulations in two possible scenarios: \u201cnormal state\u201d, p(x) and \u201cdamage state\u201d, q(x). Our assumption is, if yq is not a background signal, the posterior ratio p(x|yp)/q(x|yq) must deviate from 1. Therefore, we propose to use \u2113(\u02c6\u03b4) as the detection distance measure. We first demonstrate this application using a numerical experiment. We introduce a simple random process xt = \u03b1xt\u22121 + \u03f5, \u03f5 \u223cN(0, .1), yt = xt + \u03f5\u2032, \u03f5 \u223cN(0, .02). yp \u2208R50 is obtained by running this process fixing \u03b1 = 0.5 for 100 time steps then take yp = [y51 . . . y100]. We generate yq \u2208R50 in the same way by fixing \u03b1 = \u22120.2. 100,000 sequences from p(x) are simulated using the above process but with a random draw of \u03b1 \u223cN(0.5, 0.12). 100,000 sequences from q(x) are generated using \u03b1 \u223cN(0, .52). It means that we anticipate the latent signal p will have a different auto-correlation coefficient compared to the background q. Note since we introduced an additional prior on \u03b1, both p(x) and q(x) do not have tractable expressions. We generate 100 background yp and signal yq, then use \u2113(\u02c6\u03b4) for signal detection. f(x) = AR(x, 20), which is the auto-correlation coefficient calculated from x. Considering our time-sequences are generated by state-space models, we also compare with several other distance measures: auto-corr: \u21132 norm of the difference between AR(20) model parameters fitted on yp and yq by MLE, State-Space Transformation (SST) [10]: a method specifically for detecting changes of state-space models. The ROC curves of different distance measures are shown in Figure 2. The result shows, in terms of AUC, the PRE metric has a significant lead among all methods. Next, we show how this signal detection technique can be used in a time-series event detection problem. Suppose we have a time-series yt, t = 1 \u00b7 \u00b7 \u00b7 T. We can create y(t) p and y(t) q using two consecutive time windows (as shown in Figure 2). Using the above technique we can compute a metric \u2113t(\u02c6\u03b8). By sliding two windows forward, we can compute \u2113t+1, \u2113t+2, etc. Now we can use \u2113t as a running event detection score, to detect possible signals in time-series. We record a speech audio time-series in a noisy background, which says \u201cI go to bank today.\u201d\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c3e7/c3e74b43-6fd1-4134-94cc-e6d6a6fa4486.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb2f/cb2f7d4a-d087-45a3-8a64-3e41242344e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Left: ROC curves of latent signals detection from artificial time-sequences. Center: Illustration of a sliding window based time-series change detection. Right: Speech keyword detection.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a5d3/a5d3ba4a-4267-49f5-923b-abef57507d39.png\" style=\"width: 50%;\"></div>\nFigure 3: Left: Linear classification boundary extracted by PRE and LIME. Center: ROC curve of extracted classifiers on Xloc, Right: Interpretable features identified by PRE for a few random x .\n<div style=\"text-align: center;\">Figure 3: Left: Linear classification boundary extracted by PRE and LIME. Center: ROC curve of extracted classifiers on Xloc, Right: Interpretable features identified by PRE for a few random x</div>\nWe also record the speaker repeating the word \u201cbank\u201d in a quiet background for 10 times, then cut the the second speech into small sliding windows sized 500 to create xp \u2208R500 \u223cp(x). We let xq \u2208R500 \u223cN(0, 0.012). The detection result is plotted in Figure 2 where we can see the event-detection score clearly peaks at the word \u201cbank\u201d. In comparison, the plugin estimator gives a much less clear detection score and does not seem to peak at the desired signal word \u201cbank\u201d.\n# 5.3 Application: Extracting Locally Linear Logistic Classifier\nPRE can also be used to extract a locally linear classifier from a black-box non-linear probabilistic classifier. Formally speaking, given a black-box, non-linear classifier p(y|x), we would like to obtain a linear logistic classifier, i.e., p(y|x; \u03b4, \u03b40) := 1 1+exp(\u03b4\u22a4x+\u03b40), such that within a local neighbourhood specified by a user, the linear classifier works similarly comparing to the original black-box classifier. This technique is crucial when the user demands the interpretability of a black-box decision. For simplicity, we focus on a binary classification\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31ec/31ec77d8-a414-4e3a-889e-5be0b02358b3.png\" style=\"width: 50%;\"></div>\nproblem in this section, i.e., y \u2208{\u22121, 1}. Using Bayes rule and some algebra, we can see p(y|x) = 1 1+ p(x|\u22121) p(x|1) \u00b7c, where c is a constant that does not depend on x. User usually specifies a local region by constructing a dataset Xloc = {x(i)}n i=1 \u223cploc(x) which contains perturbations of a data-point x0. Thus, extracting parameters of a linear logistic classifier becomes fitting the log p(x|\u22121) p(x|1) using a linear model, given p(y|x) and X \u223cploc(x) as inputs. This can be efficiently solved using PRE by letting p(yp|x) = p(\u2212|x), q(yp|x) = p(+|x), p(x) = q(x) = ploc(x). Although estimating p(x|\u22121) p(x|1) using PRE seems to require the samples from the marginal p(x), not ploc(x), as p(x|\u22121) p(x|1) \u221dp(\u22121|x)p(x) p(1|x)p(x) = p(\u22121|x)ploc(x) p(1|x)ploc(x) , using samples from ploc(x) in PRE would also yield a consistent estimator of p(x|\u22121) p(x|1) . Traditionally, such a local linear classifier is obtained by minimizing the discrepancy of the predictions between the pre-trained classifier p(y|x) and the local classifier p(y|x; \u03b4, \u03b40) over Xloc (See, e.g., LIME, [15]). However, we show that such a process is not necessarily robust. See Figure 3 for an example: An outlier (blue diamond) is included in Xloc. Linear classifier obtained by PRE (black dotted line) nicely recovers an accurate linear boundary around the majority samples of Xloc. However, as the pretrained classifier labels the outlier as a negative example, LIME (green dotted line) tries to mimic such a prediction, resulting in a skewed classification boundary. Intuitively, this issue can be understood as that LIME estimates a sigmoid transform of the class density ratio rather than the ratio itself. This may magnify the effect of outlier samples. The extracted local classifier must also perform similarly to the original blackbox classifier on Xloc. Figure 3 shows the ROC curves computed on the same dataset for two extracted classifiers. The \u201ctrue\u201d label in this case is defined as the predicted label by the black-box classifier. PRE classifier (red) achieves a significant lead comparing to LIME (blue) when n = 20 and 40. Finally, we train a two layer neural network binary calssifier on MNIST dataset using digits \u201c6\u201d and \u201c9\u201d then extract a linear classifier. We generate Xloc by first picking an arbitrary image, then randomly setting some of its super-pixels\u2019 value to 0. One image is divided into 7 \u00d7 7 super-pixels. The probability of each super-pixel turning black is a random coin flip. In this way, we obtain Xloc of size 1000. We use f(x) = x for PRE and the magnitude of \u02c6\u03b4 for each picture is plotted in Figure 3. It can be seen that the selected pixels (on which |\u03b4i| > 0) are indeed parts of the written digits thus should be important features of the black-box classifier.\n# 6 Conclusions\nWe studied the problem of learning latent distribution changes via posterior ratio estimation. We propose a novel method that directly estimates the posterior ratio from two observed datasets, two likelihood functions and two sets of synthetic samples from priors. The proposed method is consistent and the estimated parameters have a limiting normal distribution. We consider two practical applications: Latent signal detection and local linear classifier extraction. Both experiments report promising results.\n# 7 The Dual Formulation of (5\nmin\u03b4 \u2113(\u03b4) can be re-written as\nmin \u03b4,ti \u2212\u02c6Ep(x) \ufffdlp(x) \u02c6p(yp) \u00b7 \u27e8\u03b4, f(x)\u27e9 \ufffd + log \u02c6Eq(x) [lq(x) exp(t)] , subject to \u2200i \u2208{1 . . . nq}, ti = \u27e8\u03b4, f(x(i) q )\u27e9.\nWrite the Lagrangian of the above convex optimization,\nmax \u00b5i min \u03b4,ti \u2212\u02c6Ep(x) \ufffdlp(x) \u02c6p(yp) \u00b7 \u27e8\u03b4, f(x)\u27e9 \ufffd + log \u02c6Eq(x) [lq(x) exp(t)] \u2212 nq \ufffd i=1 \u00b5i \ufffd ti \u2212\u27e8\u03b4, f(x(i) q )\u27e9 \ufffd ,\nwhere \u00b5i are Lagrangian multipliers. First, we solve above minimization with respect to t (by setting the derivative to zero), and it can be seen that the optimum is attained when \u00b5i = lq \ufffd x(i) q \ufffd exp(ti) \u02c6Eqx[lq(x) exp(t)], \ufffdnq i=1 \u00b5i = 1, \u00b5i \u22650. Substituting above optimality condition to the objective function, we obtain\n\ufffd Second, we solve the minimization with respect to \u03b4, one can derive the optimality condition \u02c6Ep(x) \ufffd lp(x) \u02c6p(yp) \u00b7 f(x) \ufffd = \ufffdnq i=1 \u00b5if(x(i) q ). Substituting the above optimality condition, we obtain the objective function\nThe above objective function is equivalent to (10) when Rn = 0.\n# 8 Proof of Theorem 1\nProof. Let us begin by considering the following constrainted optimization problem:\nThe Lagrangian of (12) is: Lag(\u00b5, \u03b4) := \u2113(\u03b4)+\u00b5\u00b7\u2225\u03b4\u2217\u2212\u03b4\u2225\u2212\u00b5\u00b7 Cr \u221anp\u2227nq . The KKT condition states that the minimizer \u02c7\u03b4 must satisfy 0 = \u2207\u03b4\u2113(\u02c7\u03b4) + \u02c6\u00b5 \u00b7 \u03b4\u2217\u2212\u02c7\u03b4 \u2225\u03b4\u2217\u2212\u02c7\u03b4\u2225for some \u02c6\u00b5 \u22650. Rewrite this equality by expanding \u2207\u03b4\u2113(\u02c7\u03b4) at \u03b4\u2217using mean value theorem in a coordinate-wise fashion:\nwhere \u00af\u03b4i := h\u02c6\u03b4i+(1\u2212h)\u03b4\u2217 i for some h \u2208[0, 1] which implies (\u00af\u03b4i\u2212\u03b4\u2217 i )2 = h2(\u02c6\u03b4i\u2212\u03b4\u2217 i )2 \u2264(\u02c6\u03b4i\u2212\u03b4\u2217 i )2 Therefore, \u00af\u03b4 \u2208\u2206n. After some algebra and using the fact that \u00af\u03b4 \u2208\u2206n we get\n\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd where the first inequality is due to Holder\u2019s inequality and the second inequality first uses an equality \u2225A\u22121\u2225= 1/\u03bbmin(A), if A is p.s.d., then applies inequality (7). The last inequality is due to (6). (6) holds with probability at least 1-\u03f5R1 and (7) holds with probability at least 1-\u03f5R2. A union bound shows the above inequality holds with probability at least 1 \u2212\u03f5R1 \u2212\u03f5R2. Since the ball constraint has radius R1 R2 \u00b7 Cr \u221anp\u2227nq and Cr > 1, we conclude the minimizer \u02c7\u03b4 is in the interior of \u2206n with probability at least 1 \u2212\u03f5R1 \u2212\u03f5R2. The slackness condition states, \u2203\u02c6\u00b5 \u22650, \u02c6\u00b5 \u00b7 \ufffd \u2225\u03b4\u2217\u2212\u02c7\u03b4\u2225\u2212R1 R2 \u00b7 Cr \u221anp\u2227nq \ufffd = 0. Knowing \u02c7\u03b4 is in the interior of \u2206n, we can deduce \u02c6\u00b5 = 0, which means the constraint \u03b4 \u2208\u2206n is inactive, so \u02c6\u03b4 = \u02c7\u03b4 with probability at least 1 \u2212\u03f5R1 \u2212\u03f5R2. The fact that \u02c7\u03b4 is in the interior of a \u03b4\u2217-centered ball whose radius shrinks at the rate 1 \u221anp\u2227nq allows us to conclude that \u02c7\u03b4 P\u2192\u03b4\u2217. Finally \u02c6\u03b4 P\u2192\u03b4\u2217due to \u02c6\u03b4 = \u02c7\u03b4 with probability at least 1 \u2212\u03f5R1 \u2212\u03f5R2 as discussed above.\n(12)\n# 9 Proof of Theorem 2\n\ufffd \ufffd \ufffd \ufffd where the last equality is due to the uniform convergence of the Hessian assumed in ou theorem and Continuous Mapping Theorem. Therefore \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd where the last equality is due to the uniform convergence of the Hessian assumed in our theorem and Continuous Mapping Theorem.\n\ufffd \ufffd Expanding \u2207\u03b4\u2113(\u03b4\u2217) and multiply \u221anq,\n\uf8f3 \ufffd \ufffd\ufffd \ufffd \uf8fe The first equality is due to Ep(x) \ufffd f \u2032 p(x) \ufffd = Eq(x) \ufffd r(x; \u03b4\u2217)f \u2032 q(x) \ufffd , for the optimal parameter \u03b4\u2217. Since yp, yq and xp, xq are independent of each other, a and b are two independent zero-mean random variables with covariance \u03a3p and np nq \u03a3q respectively. Applying CLT on (14) yields \u2212\u221anp\u2207\u03b4\u2113(\u03b4\u2217) \u21ddN(0, \u03a3p + np nq \u03a3q). Finally, (13) indicates that we should left and right multiply the covariance by \u03a3\u22121 to obtain covariance of \u221anp \u00b7 (\u03b4\u2217\u2212\u02c6\u03b4). This yields desired results in (9).\n(13)\n(14)\n# 0 Proof of Theorem 3\nSimilar to Theorem 1, we consider optimizing \u03b4 with a constraint \u03b4 \u2208\u2206n. First, due to the optimization constraint in (10) we know:\n\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd where rnq(x; \u03b4\u2217) is shortened as r\u2217 n. As \u2225\u2207\u03b4\u2113(\u03b4\u2217)\u2225\u2264 R1 \u221anp\u2227nq is assumed with probability a least, we can see\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd where rnq(x; \u03b4\u2217) is shortened as r\u2217 n. As \u2225\u2207\u03b4\u2113(\u03b4\u2217)\u2225\u2264 R1 \u221anp\u2227nq is assumed with probability a least 1 \u2212\u03f5R1, we can see\n\ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd where \u00af\u03b4 is a parameter that is in between \u02c6\u03b4 and \u03b4\u2217in a coordinate-wise fashion. Combining the above inequality with (15) we obtain: \ufffd\ufffd\ufffd\u03b4\u2217\u2212\u02c6\u03b4dual \ufffd\ufffd\ufffd\u2264R1+R3 R2 1 \u221anp\u2227nq + C0 R2 with probability at least 1\u2212\u03f5R1 \u2212\u03f5R2 (union bound). Since the \u2206n has a radius R1+R3 R2 Cr \u221anp\u2227nq +\n(15)\nC0 R2 where Cr > 1, \u02c6\u03b4dual is at the interior of \u2206n. This means that even without the constraint \u03b4 \u2208\u2206n, \ufffd\ufffd\ufffd\u03b4\u2217\u2212\u02c6\u03b4dual \ufffd\ufffd\ufffd\u2264R1+R3 R2 1 \u221anp\u2227nq + C0 R2 with probability at least 1 \u2212\u03f5R1 \u2212\u03f5R2.\n# 10.1 Conditions for Consistency\nIn this experiment, we numerically evaluate the conditions required by Theorem 1 and explore possible settings of the constants in this theorem. We let yp = yq = [0, 0], lp = lq = NX(x, I), x \u2208R2 and Xp, Xq be independently samples from a 2 dimensional standard normal distribution. np = nq = n. We use f(x) = x in our posterior ratio model and it can be seen that \u03b4\u2217= 0 by our construction of the dataset. We first investigate the minimum eigenvalue condition on \u22072 \u03b4\u2113(\u03b4). In the left plot of Figure 4, we plot min\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)] (which is obtained by numerical optimization) as a red solid line. Here \u2206n := Ball \ufffd 0, 10 \u221an \ufffd and the error bar indicates the standard deviation of 50 runs. It can be seen that when n \u2265400, min\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)] minus two standard deviation is safely bounded by 0.18 from below. We also investigate the tightness a lower-bound obtained from Wyel\u2019s inequality (see (8)) which is marked by a blue dash line. As we can see, when n \u2265400, Wyel\u2019s inequality becomes very close to min\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)] and consequently \u03bbmin [\u22072 \u03b4\u2113(\u03b4\u2217)] (black dash line) becomes a good approximation to min\u03b4\u2208\u2206n \u03bbmin [\u22072 \u03b4\u2113(\u03b4)]. Now we explore some possible settings of the constants in Theorem 1. It can be seen from Theorem 1 that R1 and R2 determine \u03f5R1 and \u03f5R2. To make probability 1 - \u03f5R1 \u2212\u03f5R2 as large as possible, we should make R1 large but R2 small. However, if the ratio R1/R2 is large, \u2206n would be large too (Cr > 1) and condition (7) may struggle to hold. To investigate the possible settings of R1 and R2, we plot \u2225\u221an\u2207\u03b4\u2113(\u03b4\u2217)\u2225 inf\u03b4\u2208\u2206n \u03bbmin[\u22072 \u03b4\u2113(\u03b4)] in the right plot of Figure 4, where \u2206n has a radius 10/\u221an. The plot is generated from 500 repetitions with different draws of samples from p(x), q(x). Error bars indicate standard deviations. We choose Cr = 1.5. Since the ratio \u2225\u221an\u2207\u03b4\u2113(\u03b4\u2217)\u2225 inf\u03b4\u2208\u2206n \u03bbmin[\u22072 \u03b4\u2113(\u03b4)] \u2264R1 R2 = 10/Cr, 6.66 is the upper limit of \u2225\u221an\u2207\u03b4\u2113(\u03b4\u2217)\u2225 inf\u03b4\u2208\u2206n . The plot shows, this requirement is comfortably met with some room to spare. We can see that the averaged ratio plus two standard deviations is nicely bounded by 10/Cr from above. We can also see from this plot that \u2225\u221an\u2207\u03b4\u2113(\u03b4\u2217)\u2225 \u03bbmin[\u22072 \u03b4\u2113(\u03b4\u2217)] becomes a nice approximation to \u2225\u221an\u2207\u03b4\u2113(\u03b4\u2217)\u2225 inf\u03b4\u2208\u2206n \u03bbmin[\u22072 \u03b4\u2113(\u03b4)] when n is larger than 400, which aligns with our expectation in Section 3.2 that \u03a3 \u2248\u22072 \u03b4\u2113(\u03b4\u2217) \u2248\u22072 \u03b4\u2113(\u03b4), \u2200\u03b4 \u2208\u2206n when n is large.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e06/4e068dd8-f9a7-4f9f-8a8c-1a5ab1f663fb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Numerical evaluation of Theorem 1\u2019s conditions</div>\n# 11 Additional Information on Latent Signal Detection\nIn Section 5.2, the state-space data experiment, we chose to use normal distribution as our likelihood function: lp(x) := Nyp(x, I) and lq(x) := Nyq(x, I). f(x) := AR(x, 20), where AR stands for the MALTAB \u201cautocorr\u201d function. In the speech keyword detection experiment, we used likelihood functions lp(x) := Nyp(x, 0.18\u00b7I) and lq(x) := Nyq(x, 0.18\u00b7I). f(x) := [x1x2, x2x3, . . . , xd\u22121xd], i.e. the Lag-1 autocorrelation feature. The State-Space Transformation (SST) method [10] produces a discrepancy measure by evaluating two trajectory matrices of two time-series signals using singular spectrum analysis. We use the first 20 eigenvectors to compare the difference between two spectrum. The number \u201c20\u201d was confirmed to be a reasonable choice in our preliminary experiments.\n# 12 Additional Information on Locally Linear Classifier\n# 12 Additional Information on Locally Linear Classifier Extraction\nIn Section 5.3, we estimated p(x|\u22121) p(x|1) . However, if one wanted to produce a decision boundary that is plotted on the left, Figure 3, one would need to know the constant c. In our experiment, we estimated such a constant using 400 data points in the original labelled dataset near Xloc. In practice, this is not possible as we cannot possibly know any labelled data points near Xloc. However, we argue that knowing c is not relevant to our task, which is identifying useful features of a local linear classifier. Such features can be efficiently identified by looking at the magnitude of |\u02c6\u03b4i|, which was what we did in the MNIST digits experiment.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the problem of estimating the ratio between two posterior probability density functions of a latent variable, particularly when direct comparisons of observable random variables are not feasible.",
        "problem": {
            "definition": "The paper aims to solve the issue of estimating the posterior density ratio p(x|yp) q(x|yq) where yp and yq are observations from two different distributions.",
            "key obstacle": "The main challenge is the lack of explicit prior distributions, which complicates the comparison of latent variables inferred from observations."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to effectively compare latent distributions through posterior probabilities, particularly when prior distributions are unknown.",
            "opinion": "The proposed method, Posterior Ratio Estimation (PRE), estimates the posterior ratio directly from likelihood functions and prior samples, bypassing the need for explicit prior density modeling.",
            "innovation": "PRE differs from existing methods by directly addressing the posterior ratio estimation without requiring samples from the posterior distributions, which is often difficult."
        },
        "method": {
            "method name": "Posterior Ratio Estimation",
            "method abbreviation": "PRE",
            "method definition": "PRE is defined as a method that estimates the ratio of two posterior probabilities using observed data and prior samples.",
            "method description": "The core of PRE involves minimizing the KL divergence between two posterior distributions reweighted by a ratio model.",
            "method steps": [
                "Define the likelihood functions and prior samples.",
                "Set up the optimization problem to minimize the KL divergence.",
                "Use gradient descent to find the optimal parameters that converge to the true posterior ratio."
            ],
            "principle": "The effectiveness of PRE lies in its ability to leverage likelihood functions and prior samples to accurately estimate posterior ratios, ensuring robustness in high-dimensional settings."
        },
        "experiments": {
            "evaluation setting": "The experiments involved numerical simulations comparing the performance of PRE against baseline methods in latent signal detection and classifier extraction.",
            "evaluation method": "Performance was assessed using statistical measures such as confidence intervals and ROC curves to validate the accuracy of the estimated posterior ratios."
        },
        "conclusion": "The experiments demonstrated that PRE effectively estimates posterior ratios, providing reliable results in both latent signal detection and classifier extraction applications.",
        "discussion": {
            "advantage": "PRE offers a significant advantage by directly estimating posterior ratios without requiring samples from the posterior, making it applicable in complex scenarios where prior distributions are unknown.",
            "limitation": "One limitation of PRE is its dependency on the quality of the prior samples and likelihood functions, which can affect the accuracy of the estimates.",
            "future work": "Future research could explore refining the estimation process in high-dimensional settings and extending the method to other applications in machine learning."
        },
        "other info": {
            "info1": "The method assumes that the posterior ratio function can be well-approximated by a parametric model.",
            "info2": {
                "info2.1": "The estimator is consistent and asymptotically normal as the number of prior samples increases.",
                "info2.2": "The method has potential applications in various fields such as signal processing and machine learning."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational idea behind semi-supervised algorithms is to leverage both labeled and unlabeled data for enhanced learning, which parallels the paper's focus on estimating posterior ratios without explicit prior distributions."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of the Posterior Ratio Estimation (PRE) method is to effectively compare latent distributions through posterior probabilities when prior distributions are unknown."
        },
        {
            "section number": "1.3",
            "key information": "The main objective of the paper is to solve the problem of estimating the posterior density ratio p(x|yp) q(x|yq) where yp and yq are observations from two different distributions."
        },
        {
            "section number": "3.5",
            "key information": "The PRE method represents a novel approach to semi-supervised learning by estimating posterior ratios directly from likelihood functions and prior samples, bypassing the need for explicit prior density modeling."
        },
        {
            "section number": "7.1",
            "key information": "One of the challenges identified in the paper is the dependency on the quality of prior samples and likelihood functions, which can affect the accuracy of the posterior ratio estimates."
        },
        {
            "section number": "7.4",
            "key information": "Future research directions suggested in the paper include refining the estimation process in high-dimensional settings and extending the PRE method to other applications in machine learning."
        },
        {
            "section number": "6.6",
            "key information": "The method has potential applications in various fields such as signal processing and machine learning, illustrating the practical benefits of using semi-supervised algorithms."
        }
    ],
    "similarity_score": 0.5885534315671942,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Posterior Ratio Estimation of Latent Variables.json"
}