{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1705.09684",
    "title": "Multiple Source Domain Adaptation with Adversarial Training of Neural Networks",
    "abstract": "While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. As a step toward bridging the gap, we propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting.",
    "bib_name": "zhao2017multiplesourcedomainadaptation",
    "md_text": "# Multiple Source Domain Adaptation with Adversarial Learning\nHan Zhao\u2020\u2217 Shanghang Zhang\u2021\u2217 Guanhang Wu\u266e Jo\u00e3o P. Costeira\u266d Jos\u00e9 M. F. Moura\u2021 Geoffrey J. Gordon\u2020\nHAN.ZHAO@CS.CMU.EDU SHANGHAZ@ANDREW.CMU.EDU GUANHANW@ANDREW.CMU.EDU JPC@ISR.IST.UTL.PT MOURA@ANDREW.CMU.EDU GGORDON@CS.CMU.EDU\no\u2020\u2217 HAN.ZHAO@CS.CMU.EDU g Zhang\u2021\u2217 SHANGHAZ@ANDREW.CMU.EDU g Wu\u266e GUANHANW@ANDREW.CMU.EDU osteira\u266d JPC@ISR.IST.UTL.PT  Moura\u2021 MOURA@ANDREW.CMU.EDU J. Gordon\u2020 GGORDON@CS.CMU.EDU\n\u2020Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA \u2021Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, US \u266eRobotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA \u266dDepartment of Electrical and Computer Engineering, Instituto Superior T\u00e9cnico, Lisbon, Portugal\n# Abstract\nWhile domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. As a step toward bridging the gap, we propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting.\narXiv:1705.09684v2\n# 1. Introduction\nThe success of machine learning algorithms has been partially attributed to rich datasets with abundant annotations (Krizhevsky et al., 2012; Hinton et al., 2012; Russakovsky et al., 2015). Unfortunately, collecting and annotating such large-scale training data is prohibitively expensive and time-consuming. To solve these limitations, different labeled datasets can be combined to build a larger one, or synthetic training data can be generated with explicit yet inexpensive annotations (Shrivastava et al., 2016). However, due to the possible shift between training and test samples, learning algorithms based on these cheaper datasets still suffer from high generalization error. Domain adaptation (DA) focuses on such problems by establishing knowledge transfer from a labeled source domain to an unlabeled target domain, and by exploring domain-invariant structures and representations to bridge the gap (Pan and Yang, 2010). Both theoretical results (Ben-David et al., 2010;\n\u2217. The first two authors contributed equally to this work.\nHAN.ZHAO@CS.CMU.EDU SHANGHAZ@ANDREW.CMU.EDU GUANHANW@ANDREW.CMU.EDU JPC@ISR.IST.UTL.PT MOURA@ANDREW.CMU.EDU GGORDON@CS.CMU.EDU\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\nMansour et al., 2009a; Mansour and Schain, 2012; Xu and Mannor, 2012) and algorithms (Becker et al., 2013; Hoffman et al., 2012; Ajakan et al., 2014) for DA have been proposed. Recently, DA algorithms based on deep neural networks produce breakthrough performance by learning more transferable features (Glorot et al., 2011; Donahue et al., 2014; Yosinski et al., 2014; Bousmalis et al., 2016; Long et al., 2015). Most theoretical results and algorithms with respect to DA focus on the single-source-single-target adaptation setting (Ganin et al., 2016). However, in many application scenarios, the labeled data available may come from multiple domains with different distributions. As a result, naive application of the single-source-single-target DA algorithms may lead to suboptimal solutions. Such problem calls for an efficient technique for multiple source domain adaptation. In this paper, we theoretically analyze the multiple source domain adaptation problem and propose an adversarial learning strategy based on our theoretical results. Specifically, we prove a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Our theoretical results build on the seminal theoretical model for domain adaptation introduced by Ben-David et al. (2010), where a divergence measure, known as the H-divergence, was proposed to measure the distance between two distributions based on a given hypothesis space H. Our new result generalizes the bound (Ben-David et al., 2010, Thm. 2) to the case when there are multiple source domains. The new bound has an interesting interpretation and reduces to (Ben-David et al., 2010, Thm. 2) when there is only one source domain. Technically, we derive our bound by first proposing a generalized H-divergence measure between two sets of distributions from multi-domains. We then prove a PAC bound (Valiant, 1984) for the target risk by bounding it from empirical source risks, using tools from concentration inequalities and the VC theory (Vapnik, 1998). Compared with existing bounds, the new bound does not require expert knowledge about the target domain distribution (Mansour et al., 2009b), nor the optimal combination rule for multiple source domains (Ben-David et al., 2010). Our results also imply that it is not always beneficial to naively incorporate more source domains into training, which we verify to be true in our experiments. Interestingly, our bound also leads to an efficient implementation using adversarial neural networks. This implementation learns both domain invariant and task discriminative feature representations under multiple domains. Specifically, we propose two models (both named MDANs) by using neural networks as rich function approximators to instantiate the generalization bound we derive (Fig. 1). After proper transformations, both models can be viewed as computationally efficient approximations of our generalization bound, so that the goal is to optimize the parameters of the networks in order to minimize the bound. The first model optimizes directly our generalization bound, while the second is a smoothed approximation of the first, leading to a more data-efficient and task-adaptive model. The optimization problem for each model is a minimax saddle point problem, which can be interpreted as a zero-sum game with two participants competing against each other to learn invariant features. Both models combine feature extraction, domain classification, and task learning in one training process. MDANs is generalization of the popular domain adversarial neural network (DANN) (Ganin et al., 2016) and reduce to it when there is only one source domain. We propose to use stochastic optimization with simultaneous updates to optimize the parameters in each iteration. To demonstrate the effectiveness of MDANs as well as the relevance of our theoretical results, we conduct extensive experiments on real-world datasets, including both natural language and vision tasks. We achieve superior adaptation performances on all the tasks, validating the effectiveness of our models.\nMansour et al., 2009a; Mansour and Schain, 2012; Xu and Mannor, 2012) and algorithms (Becker et al., 2013; Hoffman et al., 2012; Ajakan et al., 2014) for DA have been proposed. Recently, DA algorithms based on deep neural networks produce breakthrough performance by learning more transferable features (Glorot et al., 2011; Donahue et al., 2014; Yosinski et al., 2014; Bousmalis et al., 2016; Long et al., 2015). Most theoretical results and algorithms with respect to DA focus on the single-source-single-target adaptation setting (Ganin et al., 2016). However, in many application scenarios, the labeled data available may come from multiple domains with different distributions. As a result, naive application of the single-source-single-target DA algorithms may lead to suboptimal solutions. Such problem calls for an efficient technique for multiple source domain adaptation. In this paper, we theoretically analyze the multiple source domain adaptation problem and propose an adversarial learning strategy based on our theoretical results. Specifically, we prove a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Our theoretical results build on the seminal theoretical model for domain adaptation introduced by Ben-David et al. (2010), where a divergence measure, known as the H-divergence, was proposed to measure the distance between two distributions based on a given hypothesis space H. Our new result generalizes the bound (Ben-David et al., 2010, Thm. 2) to the case when there are multiple source domains. The new bound has an interesting interpretation and reduces to (Ben-David et al., 2010, Thm. 2) when there is only one source domain. Technically, we derive our bound by first proposing a generalized H-divergence measure between two sets of distributions from multi-domains. We then prove a PAC bound (Valiant, 1984) for the target risk by bounding it from empirical source risks, using tools from concentration inequalities and the VC theory (Vapnik, 1998). Compared with existing bounds, the new bound does not require expert knowledge about the target domain distribution (Mansour et al., 2009b), nor the optimal combination rule for multiple source domains (Ben-David et al., 2010). Our results also imply that it is not always beneficial to naively incorporate more source domains into training, which we verify to be true in our experiments. Interestingly, our bound also leads to an efficient implementation using adversarial neural networks.\nInterestingly, our bound also leads to an efficient implementation using adversarial neural networks. This implementation learns both domain invariant and task discriminative feature representations under multiple domains. Specifically, we propose two models (both named MDANs) by using neural networks as rich function approximators to instantiate the generalization bound we derive (Fig. 1). After proper transformations, both models can be viewed as computationally efficient approximations of our generalization bound, so that the goal is to optimize the parameters of the networks in order to minimize the bound. The first model optimizes directly our generalization bound, while the second is a smoothed approximation of the first, leading to a more data-efficient and task-adaptive model. The optimization problem for each model is a minimax saddle point problem, which can be interpreted as a zero-sum game with two participants competing against each other to learn invariant features. Both models combine feature extraction, domain classification, and task learning in one training process. MDANs is generalization of the popular domain adversarial neural network (DANN) (Ganin et al., 2016) and reduce to it when there is only one source domain. We propose to use stochastic optimization with simultaneous updates to optimize the parameters in each iteration. To demonstrate the effectiveness of MDANs as well as the relevance of our theoretical results, we conduct extensive experiments on real-world datasets, including both natural language and vision tasks. We achieve superior adaptation performances on all the tasks, validating the effectiveness of our models.\n# 2. Preliminary\nWe first introduce the notation used in this paper and review a theoretical model for domain adaptation when there is only one source and one target domain (Kifer et al., 2004; Ben-David et al., 2007; Blitzer et al., 2008; Ben-David et al., 2010). The key idea is the H-divergence to measure the discrepancy between two distributions. Other theoretical models for DA exist (Cortes et al., 2008; Mansour et al., 2009a,c; Cortes and Mohri, 2014); we choose to work with the above model because this distance measure has a particularly natural interpretation and can be well approximated using samples from both domains.\nWe first introduce the notation used in this paper and review a theoretical model for domain adaptation when there is only one source and one target domain (Kifer et al., 2004; Ben-David et al., 2007; Blitzer et al., 2008; Ben-David et al., 2010). The key idea is the H-divergence to measure the discrepancy between two distributions. Other theoretical models for DA exist (Cortes et al., 2008; Mansour et al., 2009a,c; Cortes and Mohri, 2014); we choose to work with the above model because this distance measure has a particularly natural interpretation and can be well approximated using samples from both domains. Notations We use domain to represent a distribution D on input space X and a labeling function f : X \u2192[0, 1]. In the setting of one source one target domain adaptation, we use \u27e8DS, fS\u27e9and \u27e8DT , fT \u27e9to denote the source and target domain, respectively. A hypothesis is a binary classification function h : X \u2192{0, 1}. The error of a hypothesis h w.r.t. a labeling function f under distribution DS is defined as: \u03b5S(h, f) := Ex\u223cDS[|h(x) \u2212f(x)|]. When f is also a hypothesis, then this definition reduces to the probability that h disagrees with h under DS: Ex\u223cDS[|h(x) \u2212f(x)|] = Ex\u223cDS[I(f(x) \u0338= h(x))] = Prx\u223cDS(f(x) \u0338= h(x)). We define the risk of hypothesis h as the error of h w.r.t. a true labeling function under domain DS, i.e., \u03b5S(h) := \u03b5S(h, fS). As common notation in computational learning theory, we use \ufffd\u03b5S(h) to denote the empirical risk of h on the source domain. Similarly, we use \u03b5T (h) and \ufffd\u03b5T (h) to mean the true risk and the empirical risk on the target domain. H-divergence is defined as follows: Definition 2.1. Let H be a hypothesis class for instance space X, and AH be the collection of subsets of X that are the support of some hypothesis in H, i.e., AH := {h\u22121({1}) | h \u2208H}. The distance between two distributions D and D\u2032 based on H is:\nNotations We use domain to represent a distribution D on input space X and a labeling function f : X \u2192[0, 1]. In the setting of one source one target domain adaptation, we use \u27e8DS, fS\u27e9and \u27e8DT , fT \u27e9to denote the source and target domain, respectively. A hypothesis is a binary classification function h : X \u2192{0, 1}. The error of a hypothesis h w.r.t. a labeling function f under distribution DS is defined as: \u03b5S(h, f) := Ex\u223cDS[|h(x) \u2212f(x)|]. When f is also a hypothesis, then this definition reduces to the probability that h disagrees with h under DS: Ex\u223cDS[|h(x) \u2212f(x)|] = Ex\u223cDS[I(f(x) \u0338= h(x))] = Prx\u223cDS(f(x) \u0338= h(x)). We define the risk of hypothesis h as the error of h w.r.t. a true labeling function under domain DS, i.e., \u03b5S(h) := \u03b5S(h, fS). As common notation in computational learning theory, we use \ufffd\u03b5S(h) to denote the empirical risk of h on the source domain. Similarly, we use \u03b5T (h) and \ufffd\u03b5T (h) to mean the true risk and the empirical risk on the target domain. H-divergence is defined as follows: Definition 2.1. Let H be a hypothesis class for instance space X, and AH be the collection of subsets of X that are the support of some hypothesis in H, i.e., AH := {h\u22121({1}) | h \u2208H}. The distance between two distributions D and D\u2032 based on H is:\ndH(D, D\u2032) := 2 sup A\u2208AH | Pr D (A) \u2212Pr D\u2032(A)|\nWhen the hypothesis class H contains all the possible measurable functions over X, dH(D, D\u2032) reduces to the familiar total variation. Given a hypothesis class H, we define its symmetric difference w.r.t. itself as: H\u2206H = {h(x) \u2295h\u2032(x) | h, h\u2032 \u2208H}, where \u2295is the xor operation. Let h\u2217be the optimal hypothesis that achieves the minimum combined risk on both the source and the target domains:\n\u2208H and use \u03bb to denote the combined risk of the optimal hypothesis h\u2217: \u03bb := \u03b5S(h\u2217) + \u03b5T (h\u2217)\n\u2208H and use \u03bb to denote the combined risk of the optimal hypothesis h\u2217:\nBen-David et al. (2007) and Blitzer et al. (2008) proved the following generalization bound on th target risk in terms of the source risk and the discrepancy between the source domain and the targe domain:\nBen-David et al. (2007) and Blitzer et al. (2008) proved the following generalization bound on the target risk in terms of the source risk and the discrepancy between the source domain and the target domain: Theorem 2.1 ((Blitzer et al., 2008)). Let H be a hypothesis space of V C-dimension d and US, UT be unlabeled samples of size m each, drawn from DS and DT , respectively. Let \ufffddH\u2206H be the empirical distance on US and UT ; then with probability at least 1 \u2212\u03b4 over the choice of samples, for each h \u2208H,  1 \ufffd 2d log(2m) + log(4/\u03b4)\n(1)\nThe generalization bound depends on \u03bb, the optimal combined risk that can be achieved by hypothesis in H. The intuition is that if \u03bb is large, then we cannot hope for a successful domain adaptation. One notable feature of this bound is that the empirical discrepancy distance between two samples US and UT can usually be approximated by a discriminator to distinguish instances from these two domains.\n# 3. A New Generalization Bound for Multiple Source Domain Adaptation\nIn this section we first generalize the definition of the discrepancy function dH(\u00b7, \u00b7) that is only appropriate when we have two domains. We will then use the generalized discrepancy function to derive a generalization bound for multisource domain adaptation. We conclude this section with a discussion and comparison of our bound and existing generalization bounds for multisource domain adaptation (Mansour et al., 2009c; Ben-David et al., 2010). We refer readers to appendix for proof details and we mainly focus on discussing the interpretations and implications of the theorems. Let {DSi}k i=1 and DT be k source domains and the target domain, respectively. We define the discrepancy function dH(DT ; {DSi}k i=1) induced by H to measure the distance between DT and a set of domains {DSi}k i=1 as follows:\n# Definition 3.1.\ndH(DT ; {DSi}k i=1) := max i\u2208[k] dH(DT ; DSi) = 2 max i\u2208[k] sup A\u2208AH | Pr DT(A) \u2212Pr DSi (A)|\nAgain, let h\u2217be the optimal hypothesis that achieves the minimum combined risk:\n# Again, let h\u2217be the optimal hypothesis that achieves the minimum combined risk:\n# h\u2217:= arg min h\u2208H \ufffd \u03b5T (h) + max i\u2208[k] \u03b5Si(h) \ufffd\nand define\n\u03bb := \u03b5T (h\u2217) + max i\u2208[k] \u03b5Si(h\u2217)\n\u03bb := \u03b5T (h\u2217) + max i\u2208[k] \u03b5Si(h\u2217)\ni.e., the minimum risk that is achieved by h\u2217. The following lemma holds for \u2200h \u2208H:\nem 3.1. \u03b5T (h) \u2264maxi\u2208[k] \u03b5Si(h) + \u03bb + 1 2dH\u2206H(DT ; {DSi}k i=1).\nRemark. Let us take a closer look at the generalization bound: to make it small, the discrepancy measure between the target domain and the multiple source domains need to be small. Otherwise we cannot hope for successful adaptation by only using labeled instances from the source domains. In this case there will be no hypothesis that performs well on both the source domains and the target domain. It is worth pointing out here that the second term and the third term together introduce a tradeoff (regularization) on the complexity of our hypothesis class H. Namely, if H is too restricted, then the second term \u03bb can be large while the discrepancy term can be small. On the other hand, if H is very rich, then we expect the optimal error, \u03bb, to be small, while the discrepancy measure dH\u2206H(DT ; {DSi}k i=1) to be large. The first term is a standard source risk term that usually appears in generalization bounds under the PAC-learning framework (Valiant, 1984; Vapnik, 1998). Later we shall upper bound this term by its corresponding empirical risk. The discrepancy distance dH\u2206H(DT ; {DSi}k i=1) is usually unknown. However, we can bound dH\u2206H(DT ; {DSi}k i=1) from its empirical estimation using i.i.d. samples from DT and {DSi}k i=1:\nTheorem 3.2. Let DT and {DSi}k i=1 be the target distribution and k source distributions over X. Let H be a hypothesis class where V Cdim(H) = d. If \ufffdDT and { \ufffdDSi}k i=1 are the empirical distributions of DT and {DSi}k i=1 generated with m i.i.d. samples from each domain, then for \u03f5 > 0, we have:\n\ufffd\ufffd\ufffd\ufffd  \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd The main idea of the proof is to use VC theory (Vapnik, 1998) to reduce the infinite hypothesis space to a finite space when acting on finite samples. The theorem then follows from standard union bound and concentration inequalities. Equivalently, the following corollary holds: Corollary 3.1. Let DT and {DSi}k i=1 be the target distribution and k source distributions over X. Let H be a hypothesis class where V Cdim(H) = d. If \ufffdDT and { \ufffdDSi}k i=1 are the empirical distributions of DT and {DSi}k i=1 generated with m i.i.d. samples from each domain, then, for 0 < \u03b4 < 1, with probability at least 1 \u2212\u03b4 (over the choice of samples), we have:\n\ufffd\ufffd\ufffd\ufffd  \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd The main idea of the proof is to use VC theory (Vapnik, 1998) to reduce the infinite hypothesis spa to a finite space when acting on finite samples. The theorem then follows from standard union boun and concentration inequalities. Equivalently, the following corollary holds:\nCorollary 3.1. Let DT and {DSi}k i=1 be the target distribution and k source distributions over X. Let H be a hypothesis class where V Cdim(H) = d. If \ufffdDT and { \ufffdDSi}k i=1 are the empirical distributions of DT and {DSi}k i=1 generated with m i.i.d. samples from each domain, then, for 0 < \u03b4 < 1, with probability at least 1 \u2212\u03b4 (over the choice of samples), we have:\n\ufffd\ufffd\ufffddH(DT ; {DSi}k i=1) \u2212dH( \ufffdDT ; { \ufffdDSi}k i=1) \ufffd\ufffd\ufffd\u22642 \ufffd 2 m \ufffd log 4k \u03b4 + d log em d \ufffd\n\ufffd\ufffd\ufffddH(DT ; {DSi}k i=1) \u2212dH( \ufffdDT ; { \ufffdDSi}k i=1) \ufffd\ufffd\ufffd\u22642 \ufffd 2 m \ufffd log 4k \u03b4 + d log em d \ufffd\n\ufffd\ufffd  \ufffd \ufffd \ufffd\ufffd Note that multiple source domains do not increase the sample complexity too drastically: it is only the square root of a log term in Corollary. 3.1 where k appears. Similarly, we do not usually have access to the true error maxi\u2208[k] \u03b5Si(h) on the source domains, but we can often have an estimate (maxi\u2208[k] \ufffd\u03b5Si(h)) from training samples. We now provide a probabilistic guarantee to bound the difference between maxi\u2208[k] \u03b5Si(h) and maxi\u2208[k] \ufffd\u03b5Si(h) uniformly for all h \u2208H:\n \ufffd Theorem 3.3. Let {DSi}k i=1 be k source distributions over X. Let H be a hypothesis class whe V Cdim(H) = d. If { \ufffdDSi}k i=1 are the empirical distributions of {DSi}k i=1 generated with m i.i. samples from each domain, then, for \u03f5 > 0, we have:\n \ufffd Pr \ufffd sup h\u2208H \ufffd\ufffd\ufffd\ufffdmax i\u2208[k] \u03b5Si(h) \u2212max i\u2208[k] \ufffd\u03b5Si(h) \ufffd\ufffd\ufffd\ufffd\u2265\u03f5 \ufffd \u22642k \ufffdme d \ufffdd exp(\u22122m\u03f52\n\ufffd\ufffd  \ufffd \ufffd\ufffd Again, Thm. 3.3 can be proved by a combination of concentration inequalities and a reduction from infinite space to finite space, along with the subadditivity of the max function. Equivalently, we hav the following corollary hold:\nCorollary 3.2. Let {DSi}k i=1 be k source distributions over X. Let H be a hypothesis class where V Cdim(H) = d. If { \ufffdDSi}k i=1 are the empirical distributions of {DSi}k i=1 generated with m i.i.d. samples from each domain, then, for 0 < \u03b4 < 1, with probability at least 1 \u2212\u03b4 (over the choice of samples), we have:\n\ufffd\ufffd  \ufffd \ufffd\ufffd Combining Thm. 3.1 and Corollaries. 3.1, 3.2 and realizing that V Cdim(H\u2206H) \u22642V Cdim(H) (Anthony and Bartlett, 2009), we have the following theorem:\nTheorem 3.4. Let DT and {DSi}k i=1 be the target distribution and k source distributions over X. Let H be a hypothesis class where V Cdim(H) = d. If \ufffdDT and { \ufffdDSi}k i=1 are the empirical distributions of DT and {DSi}k i=1 generated with m i.i.d. samples from each domain, then, for 0 < \u03b4 < 1, with probability at least 1 \u2212\u03b4 (over the choice of samples), we have:\n \ufffd  \ufffd \ufffd Remark. Thm. 3.4 has a nice interpretation for each term: the first term measures the worst case accuracy of hypothesis h on the k source domains, and the second term measures the discrepancy between the target domain and the k source domains. For domain adaptation to succeed in the multiple sources setting, we have to expect these two terms to be small: we pick our hypothesis h based on its source training errors, and it will generalize only if the discrepancy between sources and target is small. The third term \u03bb is the optimal error we can hope to achieve. Hence, if \u03bb is large, one should not hope the generalization error to be small by training on the source domains. 1 The last term bounds the additional error we may incur because of the possible bias from finite samples. It is also worth pointing out that these four terms appearing in the generalization bound also capture the tradeoff between using a rich hypothesis class H and a limited one as we discussed above: when using a richer hypothesis class, the first and the third terms in the bound will decrease, while the value of the second term will increase; on the other hand, choosing a limited hypothesis class can decrease the value of the second term, but we may incur additional source training errors and a large \u03bb due to the simplicity of H. One interesting prediction implied by Thm. 3.4 is that the performance on the target domain depends on the worst empirical error among multiple source domains, i.e., it is not always beneficial to naively incorporate more source domains into training. As we will see in the experiment, this is indeed the case in many real-world problems. Comparison with Existing Bounds First, it is easy to see that, upto a multiplicative constant, our bound in (2) reduces to the one in Thm. 2.1 when there is only one source domain (k = 1). Hence Thm. 3.4 can be treated as a generalization of Thm. 2.1. Blitzer et al. (2008) give a generalization bound for semi-supervised multisource domain adaptation where, besides labeled instances from multiple source domains, the algorithm also has access to a fraction of labeled instances from the target domain. Although in general our bound and the one in (Blitzer et al., 2008, Thm. 3) are incomparable, it is instructive to see the connections and differences between them: on one hand, the multiplicative constants of the discrepancy measure and the optimal error in our bound are half of those in Blitzer et al. (2008)\u2019s bound, leading to a tighter bound; on the other hand, because of the access to labeled instances from the target domain, their bound is expressed relative to the optimal error rate on the target domain, while ours is in terms of the empirical error on the source domain. Finally, thanks to our generalized definition of dH(DT ; {DSi}k i=1), we do not need to manually specify the optimal combination vector \u03b1 in (Blitzer et al., 2008, Thm. 3), which is unknown in practice. Mansour et al. (2009b) also give a generalization bound for multisource domain adaptation under the assumption that the target distribution is a mixture of the k sources and the target hypothesis\n1. Of course it is still possible that \u03b5T (h) is small while \u03bb is large, but in domain adaptation we do not have ac labeled samples from DT .\ncan be represented as a convex combination of the source hypotheses. While the distance measure we use assumes 0-1 loss function, their generalized discrepancy measure can also be applied for other losses functions (Mansour et al., 2009a,c,b).\n# ultisource Domain Adaptation with Adversarial Neural N\nIn this section we shall describe a neural network based implementation to minimize the generalization bound we derive in Thm. 3.4. The key idea is to reformulate the generalization bound by a minimax saddle point problem and optimize it via adversarial training.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba65/ba65061e-754b-4587-8d24-9e0dfe122254.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: MDANs Network architecture. Feature extractor, domain classifier, and task learning are combined in one training process. Hard version: the source that achieves the minimum domain classification error is backpropagated with gradient reversal; Smooth version: all the domain classification risks over k source domains are combined and backpropagated adaptively with gradient reversal.</div>\nSuppose we are given samples drawn from k source domains {DSi}, each of which contains m instance-label pairs. Additionally, we also have access to unlabeled instances sampled from the target domain DT . Once we fix our hypothesis class H, the last two terms in the generalization bound (2) will be fixed; hence we can only hope to minimize the bound by minimizing the first two terms, i.e., the maximum source training error and the discrepancy between source domains and target domain. The idea is to train a neural network to learn a representation with the following two properties: 1). indistinguishable between the k source domains and the target domain; 2). informative enough for our desired task to succeed. Note that both requirements are necessary: without the second property, a neural network can learn trivial random noise representations for all the domains, and such representations cannot be distinguished by any discriminator; without the first property, the learned representation does not necessarily generalize to the unseen target domain. Taking these two properties into consideration, we propose the following optimization problem:\n# minimize max i\u2208[k] \ufffd \ufffd\u03b5Si(h) + 1 2dH\u2206H( \ufffdDT ; { \ufffdDSi}k i=1) \ufffd\nOne key observation that leads to a practical approximation of dH\u2206H( \ufffdDT ; { \ufffdDSi}k i=1) from BenDavid et al. (2007) is that computing the discrepancy measure is closely related to learning a classifier\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\n   \uf8ed \uf8ed \ufffd \ufffd \uf8f8 Let \ufffd\u03b5T,Si(h) be the empirical risk of hypothesis h in the domain discriminating task. Ignoring the constant terms that do not affect the optimization formulation, moving the max operator out, we can reformulate (3) as: \ufffd \ufffd\n   \uf8ed \uf8ed \ufffd \ufffd \uf8f8 Let \ufffd\u03b5T,Si(h) be the empirical risk of hypothesis h in the domain discriminating task. Ignoring the constant terms that do not affect the optimization formulation, moving the max operator out, we can\n\ufffd  \ufffd The two terms in (4) exactly correspond to the two criteria we just proposed: the first term asks for an informative feature representation for our desired task to succeed, while the second term captures the notion of invariant feature representations between different domains.\nAlgorithm 1 Multiple Source Domain Adaptation via Adversarial Training\n1: for t = 1 to \u221edo\n2:\nSample {S(t)\ni }k\ni=1 and T (t) from { \ufffdDSi}k\ni=1 and \ufffdDT , each of size m\n3:\nfor i = 1 to k do\n4:\nCompute \ufffd\u03b5(t)\ni\n:= \ufffd\u03b5S(t)\ni (h) \u2212minh\u2032\u2208H\u2206H \ufffd\u03b5T (t),S(t)\ni (h\u2032)\n5:\nCompute w(t)\ni\n:= exp(\ufffd\u03b5(t)\ni )\n6:\nend for\n7:\n# Hard version\n8:\nSelect i(t) := arg maxi\u2208[k] \ufffd\u03b5(t)\ni\n9:\nUpdate parameters via backpropagating gradient of \ufffd\u03b5(t)\ni(t)\n10:\n# Smoothed version\n11:\nfor i = 1 to k do\n12:\nNormalize w(t)\ni\n\u2190w(t)\ni / \ufffd\ni\u2032\u2208[k] w(t)\ni\u2032\n13:\nend for\n14:\nUpdate parameters via backpropagating gradient of \ufffd\ni\u2208[k] w(t)\ni \ufffd\u03b5(t)\ni\n15: end for\n   \ufffd Inspired by Ganin et al. (2016), we use the gradient reversal layer to effectively implement (4) by backpropagation. The network architecture is shown in Figure. 1. The pseudo-code is listed in Alg. 1 (the hard version). One notable drawback of the hard version in Alg. 1 is that in each iteration the algorithm only updates its parameter based on the gradient from one of the k domains. This is data inefficient and can waste our computational resources in the forward process. To improve this, we approximate the max function in (4) by the log-sum-exp function, which is a frequently used smooth approximation of the max function. Define \ufffd\u03b5i(h) := \ufffd\u03b5Si(h) \u2212minh\u2032\u2208H\u2206H \ufffd\u03b5T,Si(h\u2032):\n \ufffd \ufffd where \u03b3 > 0 is a parameter that controls the accuracy of this approximation. As \u03b3 \u2192\u221e, 1 \u03b3 log \ufffd i\u2208[k] exp(\u03b3\ufffd\u03b5i(h)) \u2192maxi\u2208[k] \ufffd\u03b5i(h). Correspondingly, we can formulate a smoothed version\n\uf8f6 \uf8f7 \uf8f8 \uf8f6 \uf8f7 \uf8f8\n(4)\n\ufffd  \ufffd During the optimization, (5) naturally provides an adaptive weighting scheme for the k sour domains depending on their relative error. Use \u03b8 to denote all the model parameters, then:\n\u2202 \u2202\u03b8 1 \u03b3 log \ufffd i\u2208[k] exp \ufffd \u03b3(\ufffd\u03b5Si(h) \u2212 min h\u2032\u2208H\u2206H \ufffd\u03b5T,Si(h\u2032)) \ufffd = \ufffd i\u2208[k] exp \u03b3\ufffd\u03b5i(h) \ufffd i\u2032\u2208[k] exp \u03b3\ufffd\u03b5i\u2032(h) \u2202\ufffd\u03b5i(h) \u2202\u03b8\n\ufffd  \ufffd \ufffd \ufffd The approximation trick not only smooths the objective, but also provides a principled and adaptive way to combine all the gradients from the k source domains. In words, (6) says that the gradient of MDAN is a convex combination of the gradients from all the domains. The larger the error from one domain, the larger the combination weight in the ensemble. We summarize this algorithm in the smoothed version of Alg. 1. Note that both algorithms, including the hard version and the smoothed version, reduce to the DANN algorithm (Ganin et al., 2016) when there is only one source domain.\n# 5. Experiments\nWe evaluate both hard and soft MDANs and compare them with state-of-the-art methods on three real-world datasets: the Amazon benchmark dataset (Chen et al., 2012) for sentiment analysis, a digit classification task that includes 4 datasets: MNIST (LeCun et al., 1998), MNIST-M (Ganin et al., 2016), SVHN (Netzer et al., 2011), and SynthDigits (Ganin et al., 2016), and a public, largescale image dataset on vehicle counting from city cameras (Zhang et al., 2017). Details about network architecture and training parameters of proposed and baseline methods, and detailed dataset description will be introduced in the appendix.\n# 5.1 Amazon Reviews\nDomains within the dataset consist of reviews on a specific kind of product (Books, DVDs, Electronics, and Kitchen appliances). Reviews are encoded as 5000 dimensional feature vectors of unigrams and bigrams, with binary labels indicating sentiment. We conduct 4 experiments: for each of them, we pick one product as target domain and the rest as source domains. Each source domain has 2000 labeled examples, and the target test set has 3000 to 6000 examples. During training, we randomly sample the same number of unlabeled target examples as the source examples in each mini-batch. We implement the Hard-Max and Soft-Max methods according to Alg. 1, and compare them with three baselines: MLPNet, marginalized stacked denoising autoencoders (mSDA) (Chen et al., 2012), and DANN (Ganin et al., 2016). DANN cannot be directly applied in multiple source domains setting. In order to make a comparison, we use two protocols. The first one is to combine all the source domains into a single one and train it using DANN, which we denote as (cDANN). The second protocol is to train multiple DANNs separately, where each one corresponds to a source-target pair. Among all the DANNs, we report the one achieving the best performance on the target domain. We denote this experiment as (sDANN). For fair comparison, all these models are built on the same basic network structure with one input layer (5000 units) and three hidden layers (1000, 500, 100 units). Results and Analysis We show the accuracy of different methods in Table 1. Clearly, Soft-Max significantly outperforms all other methods in most settings. When Kitchen is the target domain,\n(6)\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDO\ncDANN performs slightly better than Soft-Max, and all the methods perform close to each other. Hard-Max is typically slightly worse than Soft-Max. This is mainly due to the low data-efficiency of the Hard-Max model (Section 4, Eq. 4, Eq. 5). We argue that with more training iterations, the performance of Hard-Max can be further improved. These results verify the effectiveness of MDANs for multisource domain adaptation. To validate the statistical significance of the results, we run a non-parametric Wilcoxon signed-ranked test for each task to compare Soft-Max with the other competitors, as shown in Table 2. Each cell corresponds to the p-value of a Wilcoxon test between Soft-Max and one of the other methods, under the null hypothesis that the two paired samples have the same mean. From these p-values, we see Soft-Max is convincingly better than other methods.\n<div style=\"text-align: center;\">Table 1: Sentiment classification accuracy.</div>\nTrain/Test\nMLPNet\nmSDA\nsDANN\ncDANN\nMDANs\nH-Max\nS-Max\nD+E+K/B\n0.7655\n0.7698\n0.7650\n0.7789\n0.7845\n0.7863\nB+E+K/D\n0.7588\n0.7861\n0.7732\n0.7886\n0.7797\n0.8065\nB+D+K/E\n0.8460\n0.8198\n0.8381\n0.8491\n0.8483\n0.8534\nB+D+E/K\n0.8545\n0.8426\n0.8433\n0.8639\n0.8580\n0.8626\n<div style=\"text-align: center;\">Table 2: p-values under Wilcoxon test.</div>\nMLPNet\nmSDA\nsDANN\ncDANN\nH-Max\nS-Max\nS-Max\nS-Max\nS-Max\nS-Max\nB\n0.550\n0.101\n0.521\n0.013\n0.946\nD\n0.000\n0.072\n0.000\n0.051\n0.000\nE\n0.066\n0.000\n0.097\n0.150\n0.022\nK\n0.306\n0.001\n0.001\n0.239\n0.008\n# 5.2 Digits Datasets\nFollowing the setting in (Ganin et al., 2016), we combine four popular digits datasets (MNIST, MNIST-M, SVHN, and SynthDigits) to build the multisource domain dataset. We take each of MNIST-M, SVHN, and MNIST as target domain in turn, and the rest as sources. Each source domain has 20, 000 labeled images and the target test set has 9, 000 examples. We compare Hard-Max and Soft-Max of MDANs with five baselines: i). best-Single-Source. A basic network trained on each source domain (20, 000 images) without domain adaptation and tested on the target domain. Among the three models, we report the one achieves the best performance on the test set. ii). Combine-Source. A basic network trained on a combination of three source domains (20, 000 images for each) without domain adaptation and tested on the target domain. iii). best-Single-DANN. We train DANNs (Ganin et al., 2016) on each source-target domain pair (20, 000 images) and test it on target. Again, we report the best score among the three. iv). Combine-DANN. We train a single DANN on a combination of three source domains (20, 000 images for each). v). Target-only. It is the basic network trained and tested on the target data. It serves as an upper bound of DA algorithms. All the MDANs and baseline methods are built on the same basic network structure to put them on a equal footing.\nResults and Analysis The classification accuracy is shown in Table 3. The results show that a naive combination of different training datasets can sometimes even decrease the performance. Furthermore, we observe that adaptation to the SVHN dataset (the third experiment) is hard. In this case, increasing the number of source domains does not help. We conjecture this is due to the large dissimilarity between the SVHN data to the others. For the combined sources, MDANs always perform better than the source-only baseline (MDANs vs. Combine-Source). However, directly training DANN on a combination of multiple sources leads to worse performance when compared with our approach (Combine-DANN vs. MDANs). In fact, this strategy may even lead to worse results than the source-only baseline (Combine-DANN vs. Combine-Source). Surprisingly, using a single domain (best-Single DANN) can sometimes achieve the best result. This means that in domain adaptation the quality of data (how close to the target data) is much more important than the quantity (how many source domains). As a conclusion, this experiment further demonstrates the effectiveness of MDANs when there are multiple source domains available, where a naive combination of multiple sources using DANN may hurt generalization.\nble 3: Accuracy on digit classification. Mt: MNIST; Mm: MNIST-M, Sv: SVHN, Sy: SynthDigits.\n<div style=\"text-align: center;\">Table 3: Accuracy on digit classification. Mt: MNIST; Mm: MNIST-M, Sv: SVHN, Sy: SynthDigits.</div>\nTrain/Test\nbest-Single\nSource\nbest-Single\nDANN\nCombine\nSource\nCombine\nDANN\nMDAN\nTarget\nOnly\nHard-Max\nSoft-Max\nSv+Mm+Sy/Mt\n0.964\n0.967\n0.938\n0.925\n0.976\n0.979\n0.987\nMt+Sv+Sy/Mm\n0.519\n0.591\n0.561\n0.651\n0.663\n0.687\n0.901\nMm+Mt+Sy/Sv\n0.814\n0.818\n0.771\n0.776\n0.802\n0.816\n0.898\nS\nT\nMDANs\nDANN\nFCN\nT\nMDANs\nDANN\nFCN\nHard-Max\nSoft-Max\nHard-Max\nSoft-Max\n2\nA\n1.8101\n1.7140\n1.9490\n1.9094\nB\n2.5059\n2.3438\n2.5218\n2.6528\n3\nA\n1.3276\n1.2363\n1.3683\n1.5545\nB\n1.9092\n1.8680\n2.0122\n2.4319\n4\nA\n1.3868\n1.1965\n1.5520\n1.5499\nB\n1.7375\n1.8487\n2.1856\n2.2351\n5\nA\n1.4021\n1.1942\n1.4156\n1.7925\nB\n1.7758\n1.6016\n1.7228\n2.0504\n6\nA\n1.4359\n1.2877\n2.0298\n1.7505\nB\n1.5912\n1.4644\n1.5484\n2.2832\n7\nA\n1.4381\n1.2984\n1.5426\n1.7646\nB\n1.5989\n1.5126\n1.5397\n1.7324\n# 5.3 WebCamT Vehicle Counting Dataset\nWebCamT is a public dataset for vehicle counting from large-scale city camera videos, which has low resolution (352 \u00d7 240), low frame rate (1 frame/second), and high occlusion. It has 60, 000 frames annotated with vehicle bounding box and count, divided into training and testing sets, with 42, 200 and 17, 800 frames, respectively. Here we demonstrate the effectiveness of MDANs to count vehicles from an unlabeled target camera by adapting from multiple labeled source cameras: we select 8 cameras that each has more than 2, 000 labeled images for our evaluations. As shown in Fig. 3, they are located in different intersections of the city with different scenes. Among these 8 cameras, we randomly pick two cameras and take each camera as the target camera, with the other 7 cameras as sources. We compute the proxy A-distance (PAD) (Ben-David et al., 2007) between each source camera and the target camera to approximate the divergence between them. We then rank the source cameras by the PAD from low to high and choose the first k cameras to form the k source\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f7ae/f7aec5ce-13f8-49a3-9263-7d29001b9277.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Counting results for target camera A (first row) and B (second row). X-frames; Y-Counts.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1a3/e1a3bc76-6a9d-41a5-a538-7047113adc5e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">mera map. Figure 4: Counting error over different source numb</div>\ndomains. Thus the proposed methods and baselines can be evaluated on different numbers of sources (from 2 to 7). We implement the Hard-Max and Soft-Max MDANs according to Alg. 1, based on the basic vehicle counting network FCN (Zhang et al., 2017). We compare our method with two baselines: FCN (Zhang et al., 2017), a basic network without domain adaptation, and DANN (Ganin et al., 2016), implemented on top of the same basic network. We record mean absolute error (MAE) between true count and estimated count. Results and Analysis The counting error of different methods is compared in Table 4. The HardMax version achieves lower error than DANN and FCN in most settings for both target cameras. The Soft-Max approximation outperforms all the baselines and the Hard-Max in most settings, demonstrating the effectiveness of the smooth and adaptative approximation. The lowest MAE achieved by Soft-Max is 1.1942. Such MAE means that there is only around one vehicle miscount for each frame (the average number of vehicles in one frame is around 20). Fig. 2 shows the counting results of Soft-Max for the two target cameras under the 5 source cameras setting. We can see that the proposed method accurately counts the vehicles of each target camera for long time sequences. Does adding more source cameras always help improve the performance on the target camera? To answer this question, we analyze the counting error when we vary the number of source cameras as shown in Fig. 4. From the curves, we see the counting error goes down with more source cameras at the beginning, while it goes up when more sources are added at the end. This phenomenon corresponds to the prediction implied by Thm. 3.4 (the last remark in Section 3): the performance on the target domain depends on the worst empirical error among multiple source domains, i.e., it is not always\ndomains. Thus the proposed methods and baselines can be evaluated on different numbers of sources (from 2 to 7). We implement the Hard-Max and Soft-Max MDANs according to Alg. 1, based on the basic vehicle counting network FCN (Zhang et al., 2017). We compare our method with two baselines: FCN (Zhang et al., 2017), a basic network without domain adaptation, and DANN (Ganin et al., 2016), implemented on top of the same basic network. We record mean absolute error (MAE) between true count and estimated count.\nResults and Analysis The counting error of different methods is compared in Table 4. The HardMax version achieves lower error than DANN and FCN in most settings for both target cameras. The Soft-Max approximation outperforms all the baselines and the Hard-Max in most settings, demonstrating the effectiveness of the smooth and adaptative approximation. The lowest MAE achieved by Soft-Max is 1.1942. Such MAE means that there is only around one vehicle miscount for each frame (the average number of vehicles in one frame is around 20). Fig. 2 shows the counting results of Soft-Max for the two target cameras under the 5 source cameras setting. We can see that the proposed method accurately counts the vehicles of each target camera for long time sequences. Does adding more source cameras always help improve the performance on the target camera? To answer this question, we analyze the counting error when we vary the number of source cameras as shown in Fig. 4. From the curves, we see the counting error goes down with more source cameras at the beginning, while it goes up when more sources are added at the end. This phenomenon corresponds to the prediction implied by Thm. 3.4 (the last remark in Section 3): the performance on the target domain depends on the worst empirical error among multiple source domains, i.e., it is not always\nbeneficial to naively incorporate more source domains into training. To illustrate this prediction better, we show the PAD of the newly added camera (when the source number increases by one) in Fig. 4. By observing the PAD and the counting error, we see the performance on the target can degrade when the newly added source camera has large divergence from the target camera.\n# 6. Related Work\nA number of adaptation approaches have been studied in recent years. From the theoretical aspect, several theoretical results have been derived in the form of upper bounds on the generalization target error by learning from the source data. A keypoint of the theoretical frameworks is estimating the distribution shift between source and target. Kifer et al. (2004) proposed the H-divergence to measure the similarity between two domains and derived a generalization bound on the target domain using empirical error on the source domain and the H-divergence between the source and the target. This idea has later been extended to multisource domain adaptation (Blitzer et al., 2008) and the corresponding generalization bound has been developed as well. Ben-David et al. (2010) provide a generalization bound for domain adaptation on the target risk which generalizes the standard bound on the source risk. This work formalizes a natural intuition of DA: reducing the two distributions while ensuring a low error on the source domain and justifies many DA algorithms. Based on this work, Mansour et al. (2009a) introduce a new divergence measure: discrepancy distance, whose empirical estimate is based on the Rademacher complexity (Koltchinskii, 2001) (rather than the VC-dim). Other theoretical works have also been studied such as (Mansour and Schain, 2012) that derives the generalization bounds on the target error by taking use of the robustness properties introduced in (Xu and Mannor, 2012). See (Cortes et al., 2008; Mansour et al., 2009a,c) for more details. Following the theoretical developments, many DA algorithms have been proposed, such as instancebased methods (Tsuboi et al., 2009); feature-based methods (Becker et al., 2013); and parameterbased methods (Evgeniou and Pontil, 2004). The general approach for domain adaptation starts from algorithms that focus on linear hypothesis class (Blitzer et al., 2006; Germain et al., 2013; Cortes and Mohri, 2014). The linear assumption can be relaxed and extended to the non-linear setting using the kernel trick, leading to a reweighting scheme that can be efficiently solved via quadratic programming (Huang et al., 2006; Gong et al., 2013). Recently, due to the availability of rich data and powerful computational resources, non-linear representations and hypothesis classes have been increasingly explored (Glorot et al., 2011; Baktashmotlagh et al., 2013; Chen et al., 2012; Ajakan et al., 2014; Ganin et al., 2016). This line of work focuses on building common and robust feature representations among multiple domains using either supervised neural networks (Glorot et al., 2011), or unsupervised pretraining using denoising auto-encoders (Vincent et al., 2008, 2010). Recent studies have shown that deep neural networks can learn more transferable features for DA (Glorot et al., 2011; Donahue et al., 2014; Yosinski et al., 2014). Bousmalis et al. (2016) develop domain separation networks to extract image representations that are partitioned into two subspaces: domain private component and cross-domain shared component. The partitioned representation is utilized to reconstruct the images from both domains, improving the DA performance. Reference (Long et al., 2015) enables classifier adaptation by learning the residual function with reference to the target classifier. The main-task of this work is limited to the classification problem. Ganin et al. (2016) propose a domain-adversarial neural network to learn the domain indiscriminate but main-task discriminative features. Although these works generally outperform non-deep learning\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\nbased methods, they only focus on the single-source-single-target DA problem, and much work is rather empirical design without statistical guarantees. Hoffman et al. (2012) present a domain transform mixture model for multisource DA, which is based on non-deep architectures and is difficult to scale up. Adversarial training techniques that aim to build feature representations that are indistinguishable between source and target domains have been proposed in the last few years (Ajakan et al., 2014; Ganin et al., 2016). Specifically, one of the central ideas is to use neural networks, which are powerful function approximators, to approximate a distance measure known as the H-divergence between two domains (Kifer et al., 2004; Ben-David et al., 2007, 2010). The overall algorithm can be viewed as a zero-sum two-player game: one network tries to learn feature representations that can fool the other network, whose goal is to distinguish representations generated from the source domain between those generated from the target domain. The goal of the algorithm is to find a Nash-equilibrium of the game, or the stationary point of the min-max saddle point problem. Ideally, at such equilibrium state, feature representations from the source domain will share the same distributions as those from the target domain, and, as a result, better generalization on the target domain can be expected by training models using only labeled instances from the source domain.\n# 7. Conclusion\nWe derive a new generalization bound for DA under the setting of multiple source domains with labeled instances and one target domain with unlabeled instances. The new bound has interesting interpretation and reduces to an existing bound when there is only one source domain. Following our theoretical results, we propose MDANs to learn feature representations that are invariant under multiple domain shifts while at the same time being discriminative for the learning task. Both hard and soft versions of MDANs are generalizations of the popular DANN to the case when multiple source domains are available. Empirically, MDANs outperform the state-of-the-art DA methods on three real-world datasets, including a sentiment analysis task, a digit classification task, and a visual vehicle counting task, demonstrating its effectiveness for multisource domain adaptation.\n# References\nH. Ajakan, P. Germain, H. Larochelle, F. Laviolette, and M. Marchand. Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009. P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE transactions on pattern analysis and machine intelligence, 33(5):898\u2013916, 2011. M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann. Unsupervised domain adaptation by domain invariant projection. In Proceedings of the IEEE International Conference on Computer Vision, pages 769\u2013776, 2013. C. J. Becker, C. M. Christoudias, and P. Fua. Non-linear domain adaptation with boosting. In Advances in Neural Information Processing Systems, pages 485\u2013493, 2013.\nS. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19:137, 2007. S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151\u2013175, 2010. J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128. Association for Computational Linguistics, 2006. J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman. Learning bounds for domain adaptation. In Advances in neural information processing systems, pages 129\u2013136, 2008. K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. In Advances in Neural Information Processing Systems, pages 343\u2013351, 2016. M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, 2012. C. Cortes and M. Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103\u2013126, 2014. C. Cortes, M. Mohri, M. Riley, and A. Rostamizadeh. Sample selection bias correction theory. In International Conference on Algorithmic Learning Theory, pages 38\u201353. Springer, 2008. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Icml, volume 32, pages 647\u2013655, 2014. T. Evgeniou and M. Pontil. Regularized multi\u2013task learning. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 109\u2013117. ACM, 2004. Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1\u201335, 2016. P. Germain, A. Habrard, F. Laviolette, and E. Morvant. A pac-bayesian approach for domain adaptation with specialization to linear classifiers. In ICML (3), pages 738\u2013746, 2013. X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 513\u2013520, 2011. B. Gong, K. Grauman, and F. Sha. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In ICML (1), pages 222\u2013230, 2013. G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.\nJ. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In Computer Vision\u2013ECCV 2012, pages 702\u2013715. Springer, 2012. J. Huang, A. Gretton, K. M. Borgwardt, B. Sch\u00f6lkopf, and A. J. Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pages 601\u2013608, 2006. D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in data streams. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 180\u2013191. VLDB Endowment, 2004. V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Information Theory, 47(5):1902\u20131914, 2001. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97\u2013105, 2015. Y. Mansour and M. Schain. Robust domain adaptation. In ISAIM, 2012. Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009a. Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In Advances in neural information processing systems, pages 1041\u20131048, 2009b. Y. Mansour, M. Mohri, and A. Rostamizadeh. Multiple source adaptation and the r\u00e9nyi divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 367\u2013374. AUAI Press, 2009c. Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011. S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828, 2016. Y. Tsuboi, H. Kashima, S. Hido, S. Bickel, and M. Sugiyama. Direct density ratio estimation for large-scale covariate shift adaptation. Journal of Information Processing, 17:138\u2013155, 2009.\nL. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142, 1984. V. Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998. P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM, 2008. P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371\u20133408, 2010. H. Xu and S. Mannor. Robustness and generalization. Machine learning, 86(3):391\u2013423, 2012. J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328, 2014. S. Zhang, G. Wu, J. P. Costeira, and J. M. Moura. Understanding traffic density from large-scale web camera data. arXiv preprint arXiv:1703.05868, 2017.\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\nAppendix A. Outline\n# Appendix A. Outline\nOrganization of the appendix: 1). For the convenience of exposition in showing our technical proofs, we first introduce the technical tools that will be used during our proofs in Sec. B. 2). We provide detailed proofs for all the claims, lemmas and theorems presented in the main paper in Sec. C. 3). We describe more experiment details in Sec. D, including dataset description, network architecture and training parameters of the proposed and baseline methods, and more analysis of the experimental results.\n# Appendix B. Technical Tools\n\u2200m \u2208N, \u03a0H(m) = max Xm\u2286X |{(h(x1), . . . , h(xm)) | h \u2208H}|\n  where Xm = {x1, . . . , xm} is a subset of X with size m.\nRoughly, the growth function \u03a0H(m) computes the maximum number of distinct ways in which m points can be classified using hypothesis in H. A closely related concept is the Vapnik\u2013Chervonenkis dimension (VC dimension) (Vapnik, 1998): Definition B.2 (VC dimension). The VC-dimension of a hypothesis class H is defined as: V Cdim(H) = max{m : \u03a0H(m) = 2m} A well-known result relating V Cdim(H) and the growth function \u03a0H(m) is the Sauer\u2019s lemma: Lemma B.1 (Sauer\u2019s lemma). Let H be a hypothesis class with V Cdim(H) = d. Then, for m \u2265d, the following inequality holds:\nRoughly, the growth function \u03a0H(m) computes the maximum number of distinct ways in which m points can be classified using hypothesis in H. A closely related concept is the Vapnik\u2013Chervonenkis dimension (VC dimension) (Vapnik, 1998): Definition B.2 (VC dimension). The VC-dimension of a hypothesis class is defined as:\nRoughly, the growth function \u03a0H(m) computes the maximum number of distinct ways in which m points can be classified using hypothesis in H. A closely related concept is the Vapnik\u2013Chervonenkis dimension (VC dimension) (Vapnik, 1998): Definition B.2 (VC dimension). The VC-dimension of a hypothesis class H is defined as: V Cdim(H) = max{m : \u03a0(m) = 2m}\nA well-known result relating V Cdim(H) and the growth function \u03a0H(m) is the Sauer\u2019s lemma: Lemma B.1 (Sauer\u2019s lemma). Let H be a hypothesis class with V Cdim(H) = d. Then, for m \u2265d, the following inequality holds:\nfollowing concentration inequality will be used:\nTheorem B.1 (Hoeffding\u2019s inequality). Let X1, . . . , Xn be independent random variables where each Xi is bounded by the interval [ai, bi]. Define the empirical mean of these random variables by \u00afX := 1 n \ufffdn i=1 Xi, then \u2200\u03b5 > 0:\n\ufffd\ufffd\ufffd \ufffd\ufffd \ufffd \ufffd   The VC inequality allows us to give a uniform bound on the binary classification error of a hypothesis class H using growth function:\n\ufffd\ufffd\ufffd \ufffd\ufffd \ufffd \ufffd   The VC inequality allows us to give a uniform bound on the binary classification error of a hypothesis class H using growth function: Theorem B.2 (VC inequality). Let \u03a0H be the growth function of hypothesis class H. For h \u2208H, let \u03b5(h) be the true risk of h w.r.t. the generation distribution D and the true labeling function h\u2217. Similarly, let \u02c6\u03b5n(h) be the empirical risk on a random i.i.d. sample containing n instances from D, then, for \u2200\u03b5 > 0, the following inequality hold: \ufffd \ufffd\nTheorem B.2 (VC inequality). Let \u03a0H be the growth function of hypothesis class H. For h \u2208H, let \u03b5(h) be the true risk of h w.r.t. the generation distribution D and the true labeling function h\u2217 Similarly, let \u02c6\u03b5n(h) be the empirical risk on a random i.i.d. sample containing n instances from D, then, for \u2200\u03b5 > 0, the following inequality hold:\nPr \ufffd sup h\u2208H |\u03b5(h) \u2212\u02c6\u03b5n(h)| \u2265\u03b5 \ufffd \u22648\u03a0H(n) exp \ufffd \u2212n\u03b52/32 \ufffd\nAlthough the above theorem is stated for binary classification error, we can extend it to any bounded error. This will only change the multiplicative constant of the bound.\n# Appendix C. Proofs\nFor all the proofs presented here, the following lemma shown by Blitzer et al. (2008) will b repeatedly used: Lemma C.1 ((Blitzer et al., 2008)). \u2200h, h\u2032 \u2208H, |\u03b5S(h, h\u2032) \u2212\u03b5T (h, h\u2032)| \u22641 2dH\u2206H(DS, DT ).\n# C.1 Proof of Thm. 3.1\nOne technical lemma we will frequently use to prove Thm. 3.1 is the triangular inequality w.r.t. \u03b5D(h), \u2200h \u2208H: Lemma C.2. For any hypothesis class H and any distribution D on X, the following triangular inequality holds: \u2200h, h\u2032, f \u2208H, \u03b5D(h, h\u2032) \u2264\u03b5D(h, f) + \u03b5D(f, h\u2032)\nProof.\nNow we are ready to prove Thm. 3.1: Theorem 3.1. \u03b5T (h) \u2264maxi\u2208[k] \u03b5Si(h) + \u03bb + 1 2dH\u2206H(DT ; {DSi}k i=1). Proof. \u2200h \u2208H, define ih := arg maxi\u2208[k] \u03b5Si(h, h\u2217):\nNow we are ready to prove Thm. 3.1:\nThe first and the fifth inequalities are due to the triangle inequality, and the third inequality is based on Lemma C.1. The second holds due to the property of | \u00b7 | and the others follow by the definition of H-divergence. \u25a0\n# C.2 Proof of Thm. 3.2\nTheorem 3.2. Let DT and {DSi}k i=1 be the target distribution and k source distributions over X. Le H be a hypothesis class where V Cdim(H) = d. If \ufffdDT and { \ufffdDSi}k i=1 are the empirical distribution of DT and {DSi}k i=1 generated with m i.i.d. samples from each domain, then for \u03f5 > 0, we have:\nPr \ufffd\ufffd\ufffd\ufffddH(DT ; {DSi}k i=1) \u2212dH( \ufffdDT ; { \ufffdDSi}k i=1) \ufffd\ufffd\ufffd\u2265\u03f5 \ufffd \u22644k \ufffdem d \ufffdd exp \ufffd \u2212m\u03f52/8 \ufffd\nProof.\nThe first inequality holds due to the sub-additivity of the max function, and the second inequality is due to the union bound. The third inequality holds because of the triangle inequality, and we use the averaging argument to establish the fourth inequality. The fifth inequality is an application of the VC-inequality, and the sixth is by the Hoeffding\u2019s inequality. Finally, we use the Sauer\u2019s lemma to prove the last inequality. \u25a0\n# C.3 Proof of Thm. 3.3\nWe now show the detailed proof of Thm. 3.3.\nProof.\n\ufffd \ufffd Again, the first inequality is due to the subadditivity of the max function, and the second inequality holds due to the union bound. We apply the VC-inequality to bound the third inequality, and Hoeffding\u2019s inequality to bound the fourth. Again, the last one is due to Sauer\u2019s lemma. \u25a0\n# C.4 Derivation of the Discrepancy Distance as Classification Error\nWe show that the H-divergence is equivalent to a binary classification accuracy in discriminating instances from different domains. Suppose AH is symmetric, i.e., A \u2208AH \u21d4X\\A \u2208AH, and we have samples {Si}k i=1 and T from {DSi}k i=1 and DT respectively, each of which is of size m, then:\n= max i\u2208[k] sup h\u2208H\u2206H | Pr x\u223c\u02c6DT (h(x) = 1) \u2212 Pr x\u223c\u02c6DSi (h(x = 1))| \ufffd\n= max i\u2208[k] \uf8eb \uf8ec \uf8ed1 \u22122 min h\u2208H\u2206H \uf8eb \uf8ec \uf8ed1 2m \ufffd x\u223c\u02c6DT I(h(x) = 1) + 1 2m \ufffd x\u223c\u02c6DSi I(h(x = 0)) \uf8f6 \uf8f7 \uf8f8 \uf8f6 \uf8f7 \uf8f8\n# Appendix D. Details about Experiments\nIn this section, we describe more details about the datasets and the experimental settings. We extensively evaluate the proposed methods on three datasets: 1). We first evaluate our methods on Amazon Reviews dataset (Chen et al., 2012) for sentiment analysis. 2). We evaluate the proposed methods on the digits classification datasets including MNIST (LeCun et al., 1998), MNIST-M (Ganin et al., 2016), SVHN (Netzer et al., 2011), and SynthDigits (Ganin et al., 2016). 3). We further evaluate the proposed methods on the public dataset WebCamT (Zhang et al., 2017) for vehicle counting. It contains 60,000 labeled images from 12 city cameras with different distributions. Due to the substantial difference between these datasets and their corresponding learning tasks, we will introduce more detailed dataset description, network architecture, and training parameters for each dataset respectively in the following subsections.\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\n<div style=\"text-align: center;\">Table 5: Network parameters for proposed and baseline methods</div>\nMethod\nInput layer\nHidden layers\nEpochs\nDropout\nDomains\nDomain adaptation\nweight\nGamma\nMLPNet\n5000\n(1000, 500, 100)\n50\n0.01\nN/A\nN/A\nN/A\nDANN\n5000\n(1000, 500, 100)\n50\n0.01\n1\n0.01\nN/A\nMDAN\n5000\n(1000, 500, 100)\n50\n0.7\n3\n0.1\n10\n# D.1 Details on Amazon Reviews evaluation\nAmazon reviews dataset includes four domains, each one composed of reviews on a specific kind of product (Books, DVDs, Electronics, and Kitchen appliances). Reviews are encoded as 5000 dimensional feature vectors of unigrams and bigrams. The labels are binary: 0 if the product is ranked up to 3 stars, and 1 if the product is ranked 4 or 5 stars.\ndimensional feature vectors of unigrams and bigrams. The labels are binary: 0 if the product is ranked up to 3 stars, and 1 if the product is ranked 4 or 5 stars. We take one product domain as target and the other three as source domains. Each source domain has 2000 labeled examples and the target test set has 3000 to 6000 examples. We implement the Hard-Max and Soft-Max methods according to Alg. 1, based on a basic network with one input layer (5000 units) and three hidden layers (1000, 500, 100 units). The network is trained for 50 epochs with dropout rate 0.7. We compare Hard-Max and Soft-Max with three baselines: Baseline 1: MLPNet. It is the basic network of our methods (one input layer and three hidden layers), trained for 50 epochs with dropout rate 0.01. Baseline 2: Marginalized Stacked Denoising Autoencoders (mSDA) (Chen et al., 2012). It takes the unlabeled parts of both source and target samples to learn a feature map from input space to a new representation space. As a denoising autoencoder algorithm, it finds a feature representation from which one can (approximately) reconstruct the original features of an example from its noisy counterpart. Baseline 3: DANN. We implement DANN based on the algorithm described in (Ganin et al., 2016) with the same basic network as our methods. Hyper parameters of the proposed and baseline methods are selected by cross validation. Table 5 summarizes the network architecture and some hyper parameters.\n# D.2 Details on Digit Datasets evaluation\nWe evaluate the proposed methods on the digits classification problem. Following the experiments in (Ganin et al., 2016), we combine four popular digits datasets-MNIST, MNIST-M, SVHN, and SynthDigits to build the multi-source domain dataset. MNIST is a handwritten digits database with 60, 000 training examples, and 10, 000 testing examples. The digits have been size-normalized and centered in a 28 \u00d7 28 image. MNIST-M is generated by blending digits from the original MNIST set over patches randomly extracted from color photos from BSDS500 (Arbelaez et al., 2011; Ganin et al., 2016). It has 59, 001 training images and 9, 001 testing images with 32 \u00d7 32 resolution. An output sample is produced by taking a patch from a photo and inverting its pixels at positions corresponding to the pixels of a digit. For DA problems, this domain is quite distinct from MNIST, for the background and the strokes are no longer constant. SVHN is a real-world house number dataset with 73, 257 training images and 26, 032 testing images. It can be seen as similar to MNIST, but comes from a significantly harder, unsolved, real world problem. SynthDigits consists of 500; 000 digit images generated by Ganin et al. (2016) from WindowsTM fonts by varying the text, positioning, orientation, background and stroke colors, and the amount of blur. The degrees\nof variation were chosen to simulate SVHN, but the two datasets are still rather distinct, with the biggest difference being the structured clutter in the background of SVHN images. We take MNIST-M, SVHN, and MNIST as target domain in turn, and the remaining three as sources. We implement the Hard-Max and Soft-Max versions according to Alg. 1 based on a basic network, as shown in Fig. 5. The baseline methods are also built on the same basic network structure to put them on a equal footing. The network structure and parameters of MDANs are illustrated in Fig. 5. The learning rate is initialized by 0.01 and adjusted by the first and second order momentum in the training process. The domain adaptation parameter of MDANs is selected by cross validation. In each mini-batch of MDANs training process, we randomly sample the same number of unlabeled target images as the number of the source images.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ad67/ad67b2d7-98de-4ec8-aa22-9ce004899418.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: MDANs network architecture for digit classification</div>\n# D.3 Details on WebCamT Vehicle Counting\nWebCamT is a public dataset for large-scale city camera videos, which have low resolution (352 \u00d7 240), low frame rate (1 frame/second), and high occlusion. WebCamT has 60, 000 frames annotated with rich information: bounding box, vehicle type, vehicle orientation, vehicle count, vehicle reidentification, and weather condition. The dataset is divided into training and testing sets, with 42,200 and 17,800 frames, respectively, covering multiple cameras and different weather conditions. WebCamT is an appropriate dataset to evaluate domain adaptation methods, for it covers multiple city cameras and each camera is located in different intersection of the city with different perspectives and scenes. Thus, each camera data has different distribution from others. The dataset is quite challenging and in high demand of domain adaptation solutions, as it has 6, 000, 000 unlabeled images from 200 cameras with only 60, 000 labeled images from 12 cameras. The experiments on WebCamT provide an interesting application of our proposed MDANs: when dealing with spatially and temporally large-scale dataset with much variations, it is prohibitively expensive and time-consuming to label large amount of instances covering all the variations. As a result, only a limited portion of the dataset can be annotated, which can not cover all the data domains in the dataset. MDAN provide an effective\n<div style=\"text-align: center;\"></div>\nHAN ZHAO, SHANGHANG ZHANG, GUANHANG WU, JOAO COSTEIRA, JOSE MOURA, GEOFFREY GORDON\nsolution for this kind of application by adapting the deep model from multiple source domains to the\nsolution for this kind of application by adapting the deep model from multiple source domains to the unlabeled target domain.\nWe evaluate the proposed methods on different numbers of source cameras. Each source camera provides 2000 labeled images for training and the test set has 2000 images from the target camera. In each mini-batch, we randomly sample the same number of unlabeled target images as the source images. We implement the Hard-Max and Soft-Max version of MDANs according to Alg. 1, based on the basic vehicle counting network FCN described in (Zhang et al., 2017). Please refer to (Zhang et al., 2017) for detailed network architecture and parameters. The learning rate is initialized by 0.01 and adjusted by the first and second order momentum in the training process. The domain adaptation parameter is selected by cross validation. We compare our method with two baselines: Baseline 1: FCN. It is our basic network without domain adaptation as introduced in work (Zhang et al., 2017). Baseline 2: DANN. We implement DANN on top of the same basic network following the algorithm introduced in work (Ganin et al., 2016).\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of multiple source domain adaptation, which is crucial due to the limitations of existing algorithms that primarily focus on single-source-single-target settings. The need for effective techniques in scenarios where labeled data comes from multiple domains with different distributions is highlighted.",
        "problem": {
            "definition": "The problem is the adaptation of a model trained on multiple labeled source domains to a single unlabeled target domain, where naive application of single-source algorithms may lead to suboptimal results.",
            "key obstacle": "The main challenge is the potential discrepancy between multiple source domains and the target domain, which can hinder effective adaptation."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to bridge the gap between multiple source domains and the target domain without requiring expert knowledge about the target distribution.",
            "opinion": "The proposed approach involves a new generalization bound that facilitates learning feature representations invariant to domain shifts while maintaining task discriminative properties.",
            "innovation": "The main innovation lies in deriving a new generalization bound that does not necessitate the optimal combination rule for multiple sources, thus simplifying the adaptation process."
        },
        "Theory": {
            "perspective": "The theoretical perspective is based on a generalized H-divergence measure that quantifies the distance between the target domain and multiple source domains.",
            "opinion": "The theory assumes that successful adaptation depends on minimizing both the maximum source training error and the discrepancy between the source and target domains.",
            "proof": "The proof involves bounding the target risk using empirical source risks and concentration inequalities, leading to a PAC bound for the target risk."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three real-world datasets: sentiment analysis (Amazon reviews), digit classification (MNIST, MNIST-M, SVHN, SynthDigits), and vehicle counting from city cameras.",
            "evaluation method": "The evaluation involved comparing the proposed MDANs (both hard and soft versions) against state-of-the-art methods using accuracy and mean absolute error metrics."
        },
        "conclusion": "The experiments demonstrate that MDANs outperform existing methods in multiple source domain adaptation, validating the effectiveness of the proposed theoretical framework.",
        "discussion": {
            "advantage": "The advantages of this paper include a novel theoretical framework that simplifies the adaptation process and empirical validation of the proposed models across diverse tasks.",
            "limitation": "A limitation is that the performance may degrade with an increasing number of source domains if they exhibit significant divergence from the target domain.",
            "future work": "Future work could focus on improving the robustness of the models against domain divergence and exploring more adaptive strategies for selecting source domains."
        },
        "other info": [
            {
                "info1": "The paper introduces two models, both termed MDANs: one optimizing the generalization bound directly and the other as a smoothed approximation."
            },
            {
                "info2": {
                    "info2.1": "The optimization tasks of both models are framed as minimax saddle point problems.",
                    "info2.2": "The architecture combines feature extraction, domain classification, and task learning in a unified training process."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper addresses the issue of multiple source domain adaptation, highlighting the limitations of existing algorithms that primarily focus on single-source-single-target settings."
        },
        {
            "section number": "2.3",
            "key information": "Key terminologies include 'multiple source domain adaptation' and 'generalization bound' which are crucial for understanding the theoretical framework proposed in the paper."
        },
        {
            "section number": "3.1",
            "key information": "The proposed models, termed MDANs, optimize a new generalization bound that facilitates learning feature representations invariant to domain shifts while maintaining task discriminative properties."
        },
        {
            "section number": "5.1",
            "key information": "The paper discusses a new generalization bound that simplifies the adaptation process, representing an advancement in pre-training strategies for models dealing with multiple source domains."
        },
        {
            "section number": "7.1",
            "key information": "The main challenge identified is the potential discrepancy between multiple source domains and the target domain, which can hinder effective adaptation."
        },
        {
            "section number": "7.3",
            "key information": "Future work could focus on improving the robustness of the models against domain divergence and exploring more adaptive strategies for selecting source domains."
        }
    ],
    "similarity_score": 0.6244612895003454,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Multiple Source Domain Adaptation with Adversarial Training of Neural Networks.json"
}