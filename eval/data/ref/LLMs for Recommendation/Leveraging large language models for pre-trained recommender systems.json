{
    "from": "google",
    "scholar_id": "fnGVIZI-byYJ",
    "detail_id": null,
    "title": "Leveraging large language models for pre-trained recommender systems",
    "abstract": "\n\nAbstract\n\nRecent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However, effectively integrating LLM\u2019s commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference. This allows RecSysLLM to leverage LLMs\u2019 capabilities for recommendation tasks in an efficient, unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.\n\n# Introduction\n\nThe realm of recommendation has gained considerable attention in recent years due to its ability to drive business growth and enhance user engagement. Recent advancements in recommender systems have shifted towards incorporating diverse information and catering to a broader range of application scenarios, rather than focusing on task-specific architectures. This shift has been driven by the need for more comprehensive and personalized recommendations, as well as the availability of new data sources and knowledge (Geng et al. 2022; Chu et al. 2022; Hui et al. 2022; Sheu et al. 2021; Li and Zhao 2021; Jiang et al. 2022; Xue et al. 2021). In addition, with the advent of the Large Language Model (LLM) (Radford et al., 2019; Brown et al. 2020; Ouyang et al. 2022), we have witnessed an unprecedented surge in the capabilities of natural language processing. The power of LLM lies in its ability to understand and generate humanlike language. LLM has also enabled the extraction of implicit knowledge from text data (Gu et al. 2023; Yoneda et al. 2023; Zhao et al. 2023",
    "bib_name": "chu2023leveraging",
    "md_text": "# Leveraging Large Language Models for Pre-trained Reco\n\n# Leveraging Large Language Models for Pre-trained Recommender Syst\n\n# Zhixuan Chu *1, Hongyan Hao *1, Xin Ouyang 1, Simeng Wang 1, Yan Wang 1, Yue Shen 1, Jin Qing Cui 1, Longfei Li 1, Siqiao Xue 1, James Y Zhang 1, Sheng Li 2\n\n1 Ant Group 2 University of Virginia {chuzhixuan.czx, hongyanhao.hhy, xin.oyx, simeng.wsm, luli.wy, zhanying, jinjie.gujj, cuiqing.cq, longy james.z} @antgroup.com, shengli@virginia.edu\n\n1 Ant Group 2 University of Virginia\n\n1 Ant Group 2 University of Virginia {chuzhixuan.czx, hongyanhao.hhy, xin.oyx, simeng.wsm, luli.wy, zhanying, jinjie.gujj, cuiqing.cq, longyao.llf, siqiao.xsq, james.z} @antgroup.com, shengli@virginia.edu\n\n{chuzhixuan.czx, hongyanhao.hhy, xin.oyx, simeng.wsm, luli.wy, zhanying, jinjie.gujj, cuiqing.cq james.z} @antgroup.com, shengli@virginia.edu\n\nAbstract\n\nRecent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However, effectively integrating LLM\u2019s commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference. This allows RecSysLLM to leverage LLMs\u2019 capabilities for recommendation tasks in an efficient, unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.\n\n# Introduction\n\nThe realm of recommendation has gained considerable attention in recent years due to its ability to drive business growth and enhance user engagement. Recent advancements in recommender systems have shifted towards incorporating diverse information and catering to a broader range of application scenarios, rather than focusing on task-specific architectures. This shift has been driven by the need for more comprehensive and personalized recommendations, as well as the availability of new data sources and knowledge (Geng et al. 2022; Chu et al. 2022; Hui et al. 2022; Sheu et al. 2021; Li and Zhao 2021; Jiang et al. 2022; Xue et al. 2021). In addition, with the advent of the Large Language Model (LLM) (Radford et al., 2019; Brown et al. 2020; Ouyang et al. 2022), we have witnessed an unprecedented surge in the capabilities of natural language processing. The power of LLM lies in its ability to understand and generate humanlike language. LLM has also enabled the extraction of implicit knowledge from text data (Gu et al. 2023; Yoneda et al. 2023; Zhao et al. 2023). This newfound capability of LLM has opened up exciting avenues for the integration of semantic information into recommender systems and provides a wealth of insights into user preferences and behaviors (Shi\n\n* These authors contributed equally.\n\net al. 2023; Zhao, Tan, and Mei 2022). As a result, incorporating LLM into recommender systems has become a crucial step toward providing a powerful and comprehensive paradigm for recommendation tasks. In the following, we will discuss the new generation of recommendation model paradigms from two directions, i.e., the unified pre-trained recommendation model and the combination of LLM and recommendation model. On the one hand, training a pre-trained recommendation model can help overcome the limitations of existing recommendation approaches that require designing task-specific architectures and training objectives. Traditional recommendation methods have focused on a single task, such as personalized product recommendations, contextual advertising, customer segmentation, and so on, making them less adaptable to new tasks and limiting their ability to generalize to new domains. By training a pre-trained recommendation model, we can leverage the power of pre-trained models to learn generalizable representations of user behavior and product characteristics (Tsai et al. 2023; Zhao, Tan, and Mei 2022) that can be applied to a variety of recommendation tasks. Overall, a pre-trained recommendation model provides a flexible and scalable solution that can be adapted to a variety of recommendation tasks. Since recommendation tasks usually share a common user\u2013item pool, features, behavioral sequences, and other contextual information, we believe it is promising to merge even more recommendation tasks into a unified framework so that they can implicitly transfer knowledge to benefit each other and enable generalization to other unseen tasks (Xie et al. 2022). On the other hand, integrating LLMs into recommendation systems has several significant advantages. These advantages are linked to the LLM\u2019s capabilities in thinking, reasoning, and discovering implicit relationships within textual data based on the entailment of wealthy background knowledge and logical chains. (1) By leveraging the semantic information in natural language data, LLMs can help the recommendation system understand and infer the relationship between user features and behavioral sequences and among entities in behavioral sequences. This allows the recommendation system to understand the user\u2019s needs and preferences in a more comprehensive way. (2) Another benefit of integrating LLMs into recommendation systems is the ability to leverage the implicit knowledge that is hidden in\n\nthe models. LLMs are trained on vast amounts of textual data and can help to understand the relationships between different concepts and ideas. By incorporating LLMs into recommendation systems, this implicit knowledge can be used to generate more divergent and logical recommendations. This can lead to more creative and unexpected recommendations that the user may not have considered otherwise. (3) By leveraging the natural language processing capabilities of LLMs, recommendation tasks that previously required separate specialized systems can now be integrated into a unified framework. The pretrained knowledge and few-shot learning abilities of LLMs allow recommendation models to be rapidly adapted to new domains with limited data. Overall, the natural language processing power and versatility of LLMs can help merge more recommendation tasks into a unified framework. Furthermore, a comprehensive survey on recommendations and LLMs is provided in the Appendix. This survey covers the motivation behind them, current development, and challenges. However, constructing a robust and integrated recommendation system that fully utilizes large language models\u2019 immense knowledge and reasoning capacities poses several key challenges. Directly training a pre-trained recommendation model from scratch is not only a waste of time and data collection efforts but also lacks general common sense and reasoning capabilities that underpin modern large language models. Meanwhile, directly fine-tuning a pre-trained LLM model on recommendation data also has drawbacks. Recommendation data has distinct characteristics - such as fixed entities and sequential user behaviors - that differ from the raw text corpora used to train language models. As such, fine-tuning may erase much of the capabilities specific to recommendation tasks. Therefore, we propose a novel pretrained recommendation paradigm (RecSysLLM) based on the pre-trained large language model through unique designs for recommendation in three phases, i.e., data phase, training phase, and inference phase. Our model retains the reasoning ability and rich knowledge contained in large language models while integrating the recommendation-specific knowledge. It directly inherits the parameters and framework of the original large language model but also designs and extends some mechanisms in the data phase (textualization and sampling), training phase (mask, position, and ordering), and inference phase (dynamic position infilling). These modifications do not discard the tokenization, parameters, structure, or previously learned knowledge in the LLM. On this basis, recommendation data is used to fine-tune it. The significant advantage of this pre-trained recommendation model is that it can utilize the reasoning capabilities and rich knowledge of large language models while incorporating domain-specific knowledge of the recommendation system through parameter-efficient fine-tuning of userprofiles and behavioral sequences data. Another crucial benefit of this model is that it can be easily adapted to different downstream recommendation sub-tasks. We evaluate the proposed model on extensive benchmark datasets and realworld scenarios. The experimental results demonstrate its effectiveness in improving the quality of recommendations. Overall, our proposed pre-trained recommendation model\n\nprovides a promising approach for building recommendation systems that are efficient, effective, and unified.\n\n# RecSysLLM Pretraining Mechanism\n\nTo fully take advantage of LLM and domain knowledge in recommendation tasks, we need to modify the LLM and fine-tune the existing LLM to get a pre-trained recommendation model. However, the conventional large language models are trained on general knowledge and coherent corpus, and the framework of the model is not designed for behavioral sequence data and recommendation tasks. To address these two points, we make modifications from three phases, i.e., data, training, and inference phases, to transform a conventional pre-trained language model into a pre-trained recommendation model. The whole framework is illustrated in Figure 1. This pre-trained recommendation model has been employed in real-world applications in Chinese scenarios, so we take the GLM (Du et al. 2021) as an example to introduce the RecSysLLM pretraining mechanism, which is bilingual in Chinese and English. Our model can also be adapted to other large language models with minor modifications.\n\n# Data Phase\n\nIn the data phase, textualizing tabular data is often the easiest and most straightforward approach for implementing large language models. For the pre-training of RecSysLLM, we first textualize conventional tabular data, such as user features stored in a table with rows and columns into text. Since large language models are originally trained on textual data, text-based features can be easily combined with text-based behavioral sequences and other text information, which helps our model better capture the relationship between features and behavioral sequences. In addition, textualizing tabular data allows for greater flexibility in how they are used in the following tasks. Compared with ordinary language texts, the training texts in the recommendation system should take into account the interests and preferences of users from different periods (Yu et al. 2019). Long-term preferences are usually stable and reflect the general preferences of a user. These preferences do not change frequently over time, but they lack timeliness and may not reflect current interests. On the other hand, short-term preferences tend to change frequently over time and are more reflective of a user\u2019s current interests. We aim to use different periods of preferences to provide accurate and relevant recommendations to users, which can balance the user\u2019s general interests with their current needs. Therefore, we sample behavioral sequences in long-term preferences (10%), medium-term preferences (30%), and short-term preferences (60%). Long-term preferences capture the user\u2019s preferences that have remained consistent for an extended period of time, typically spanning over several months or years. Medium-term preferences capture the user\u2019s preferences that have developed and changed over a shorter period of time, typically spanning over several weeks or months. Short-term preferences can improve recommendation accuracy by providing the system with the user\u2019s most recent preferences, spanning over several days or hours.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c772/c7729eb6-83da-472c-b48c-b04e14dca1cd.png\" style=\"width: 50%;\"></div>\nFigure 1: This is the framework of RecSysLLM based on a pre-trained generative language model (GLM). To transform the GLM into a specialized model for recommendation systems, several modifications are made while preserving the core knowledge and capabilities of the original language model architecture, such as the new mask mechanism, span order, positional encoding, dynamic position mechanism, and so on.\n\n# Training Phase\n\nTo be consistent with the architecture of GLM, our model is still trained by optimizing an autoregressive blank infilling objective based on an input text x = [x 1, \u00b7 \u00b7 \u00b7, x n]. Different from the general language text in GLM, our input text is composed of user features and behavioral sequences. Although textualized user features and behavioral sequences are also composed of multiple tokens, they often represent a complete meaning as a whole. If they are split into different parts, like regular text, they will lose their unique meaning. In addition, the LLM\u2019s power comes from the way it tokenizes and processes text. It has been trained on a vast amount of data and has learned to recognize patterns and relationships between tokens, enabling it to identify entities accurately and extract information. If we were to create a new tokenization method, we would lose the LLM\u2019s power. Therefore, to maintain the LLM\u2019s power and supplement the new knowledge in the recommendation data, it is best to leverage the existing tokenization and enhance it with additional information and capabilities rather than create a new tokenization. In the following, we name the attributes in user features and items in the behavioral sequences as entities, which means that they are complete units and have fixed meanings. Therefore, as shown in the \u201cEntities\u201d of Figure 1, our data are composed of plain language text and entities, where (x 1, x 2, and x 3) have merged to form e 1 and (x 6 and x 7) to form e 2. x 4 and x 5 are separate tokens.\nMask Mechanism. To inject the new knowledge of recommendation tasks based on the original LLM, we follow the principle in the LLM and design the new mask mechanism and position strategies. Similar to the GLM (Du et al. 2021), multiple text spans {s 1, \u00b7 \u00b7 \u00b7, s m} are sampled, where each span s i corresponds to a series of consecutive tokens [s i, 1, \u00b7 \u00b7 \u00b7, s i,l i] in x. Each span is replaced with a single [MASK] token. The remaining text and [MASK] s form a corrupted text x corrupt. In the GLM, since there is no existence of entity, the tokens can be randomly sampled into spans. However, in our model, the multiple and consecutive tokens composing an entity should not be split into different parts. In other words, the tokens of an entity are treated as\n\na whole. The [MASK] mechanism will not break the  complete entities, which will highlight the whole structure of entities and help to capture the interrelationship between entities. For example, as shown in the \u201cMasks\u201d of Figure 1, x 1, x 2, and x 3 composing the e 1  are blocked as a whole and single token x 5 is also blocked. Therefore, we form the x corrupt with [M], x 4, [M], x 6, and x 7 in the \u201cDivision\u201d of Figure 1. Compatible with different natural language processing tasks, we adopt the multi-task pretraining setup (Du et al. 2021) with entity-level [M], sentence-level [sM], and document-level [gM]. Specifically, entity-level refers to the randomly blanking out continuous spans of tokens from the input text, following the idea of autoencoding, which captures the interdependencies between entities. Sentence level restricts that the masked spans must be full sentences. Document-level is to sample a single span whose length is sampled from a uniform distribution over 50%\u2013100% of the original length. The objective aims for long text generation.\nSpan Order. We implement the autoregressive blank infilling objective with the following techniques. The input x is divided into two parts: one part is the corrupted text x corrupt, and the other consists of the masked spans. Our model automatically learns a bidirectional encoder for the first part and a unidirectional decoder for the second part in a unified model. The model predicts the missing tokens in the spans from the corrupted text in an autoregressive manner, which means when predicting the missing tokens in a span, the model has access to the corrupted text and the previously predicted spans. Instead of randomly permuting the order of the spans in the original GLM (Du et al. 2021), we keep all spans in chronological order to keep the interrelationship among different entities. Formally, we define the pretraining objective of a lengthm index sequence [1, 2, ..., m] as\n\n(1)\n\n\ufffd\nPositional Encoding. To enable autoregressive generation, each span is padded with special tokens [START] and [END], for input and output, respectively. To be consistent with the original LLM, we cannot arbitrarily modify, add, or\n\nreduce the original positional strategies. Therefore, we extend 2D positional encodings (Du et al. 2021) based on entities. Specifically, each token is encoded with two positional ids, i.e., inter-position and intra-position ids. The inter-position id represents the position in the corrupted text x corrupt. For the masked spans, it is the position of the corresponding [MASK] token. For the intra-position id, we follow the essential meaning in the original LLM, which still refers to the intra-position. Instead of the scope of the whole span, we extend it into a finer granularity. For the entities, it represents the intra-relationship among entities. As shown in Figure 1, for separate tokens (not in the entities) in the encoder part ([M], x 4, [M]), their intra-position ids are 0. For consecutive tokens in the entities (x 6 and x 7), they are numbered in chronological order. For tokens in the autoregressive blank infilling part, they range from 1 to the length of the entities including [S], such as (entities: [S], x 1, x 2, x 3 \u2192 1, 2, 3, 4) and (independent token: [S], x 5 \u2192 1, 2). The two positional ids are projected into two vectors via learnable embedding tables, which are both added to the input token embeddings.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff0a/ff0a0c41-17aa-4d95-b78f-4c3b1858d611.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: This is the dynamic position mechanism. When one token is generated, it will be judged as one part of an entity or not. If it and the previous token belong to one entity, the intra-position id will continue to grow. Otherwise, it will start at 1 again.\n</div>\nFigure 2: This is the dynamic position mechanism. When one token is generated, it will be judged as one part of an entity or not. If it and the previous token belong to one entity, the intra-position id will continue to grow. Otherwise, it will start at 1 again.\n\n# Inference phase\n\nBecause our pre-trained model is designed to fit different downstream tasks, the length of the generated text should be unknown beforehand and flexible for the different tasks. Further, due to the existence of entities, the intra-position ids represent the relative position of the entity. As shown in the \u201cInference Phase\u201d of Figure 1, we cannot specify the intraposition ids in advance when autoregressive blank infilling. Hence, we designed a dynamic position mechanism for the mask and position modifications made during the inference phase. It can conduct the autoregressive judgment to determine and complement the intra-position ids one by one as each token is generated in the autoregressive generation procedure. Specifically, we establish an entity pool beforehand, which stores all the tokens of the entities that existed in our recommendation task. When one token is generated, it will be judged as one part of an entity or not. We utilize the Trie\n\nalgorithm (Bodon and R\u00b4onyai 2003) to check whether the generated token and previous token belong to the same entity, which is a tree data structure used for locating specific keys from within a set. If they belong to one entity, the intraposition id will continue to grow. Otherwise, it will start at 1 again. The detailed procedure is illustrated in Figure 2.\n\n# Experiments\n\n# Experimental Setup\n\nDatasets. We evaluate our method on three real-world ecommerce datasets from Amazon.com, spanning the categories of Sports & Outdoors, Beauty, and Toys & Games. The datasets contain user ratings and reviews from 2019, along with transaction records between January 1 and December 31 (Zhou et al. 2020; Xue et al. 2022, 2023). Key statistics of the resulting datasets are provided in Table 1.\nMetrics. Following the experiments in (Geng et al. 2022), we cover five different task families \u2013 rating, sequential recommendation, explanation, review, and direct recommendation to facilitate the multitask pretraining for the recommendation. For rating prediction, we adopt Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) as evaluation metrics. For sequential recommendation and direct recommendation tasks, we employ topk Hit Ratio (HR@ k) and Normalized Discounted Cumulative Gain (NDCG@ k) to evaluate the performance and report HR@1, 5, 10 and NGCG@5, 10. For explanation generation and review summarization, we evaluate different methods with BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L. Lower values of RMSE and MAE indicate better performance, while higher values are preferred for all other metrics. In all result tables, bold numbers represent the best performance, while underlined numbers refer to the second-best performance.\nBaselines for Multiple Tasks To demonstrate competence on a wide range of recommendation-related tasks, we adopt the same representative approaches as (Geng et al. 2022) for different tasks, such as Rating Prediction (MF (Koren, Bell, and Volinsky 2009) and MLP (Cheng et al. 2016)), Direct Recommendation (BPR-MF  (Rendle et al. 2009), BPR-MLP (Cheng et al. 2016), and SimpleX (Mao et al. 2021)), Sequential Recommendation (Caser (Tang and Wang 2018), HGN (Ma, Kang, and Liu 2019), GRU4Rec (Hidasi et al. 2016), BERT4Rec (Sun et al. 2019), FDSA (Zhang et al. 2019), SASRec (Kang and McAuley 2018), and S 3-Rec  (Zhou et al. 2020)), Explanation Generation (Attn2Seq (Dong et al. 2017), NRT (Li et al. 2017), PETER (Li, Zhang, and Chen 2021), and PETER+), and review summarization (T0 (Sanh et al. 2022) and  GPT2 (Radford et al. 2019)). The detailed baselines are provided in the Appendix.\n\n# Implementation\n\nTo facilitate the multitask prompt-based pretraining for the recommendation, Geng et al. (2022) created a collection of personalized prompt templates. The collection covers five different task families \u2013 rating, sequential recommendation, explanation, review, and direct recommendation. The\n\nTable 1: Basic statistics of the experimental datasets.\n\nDataset\nSports\nBeauty\nToys\n#Users\n35,598\n22,363\n19,412\n#Items\n18,357\n12,101\n11,924\n#Reviews\n296,337\n198,502\n167,597\n#Sparsity (%)\n0.0453\n0.0734\n0.0724\nprompts include personalized fields for users and items to help the model discover user-item preferences. For rating prediction, prompts ask to predict a user\u2019s rating or preference for an item. For sequential recommendation, prompts ask to predict the next item a user will interact with. For explanation, prompts ask to generate text explaining a user\u2019s preferences. For review, prompts summarize or predict ratings from reviews. For direct recommendation, prompts ask whether to recommend an item to a user. The complete collection of personalized prompts with examples is provided in the Appendix of (Geng et al. 2022). These prompts enable the building of diverse training examples from raw data for multitask pertaining. We pretrain our RecSysLLM with diverse training examples with different prompt templates from all five task families to verify its multitask learning ability. Besides, we adopt a part of prompts in each task family for zero-shot evaluation while all remaining prompts are utilized for multitasking prompted pretraining. As a result, we are able to not only compare the performance across various recommendation tasks but also evaluate the zero-shot generalization capability on unseen prompts. Our RecSysLLM model for these English language tasks leverages the powerful GLM-10B for English (Du et al. 2021) model as a foundation. GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks. Our approach builds on this pre-trained GLM-10B foundation by utilizing a parameterefficient fine-tuning method called LoRA (Low-Rank Adaptation) (Hu et al. 2021) to adapt the model to our specific recommendation tasks. LoRA enables efficiently customizing the enormous GLM-10B model to specialized domains by learning a low-dimensional decomposition of the model update. This allows us to tap into GLM-10B\u2019s broad language knowledge while calibrating it to our RecSysLLM objectives. We inject trainable rank decomposition matrices into each query key value, dense, dense h to 4 h and dense 4 h to h  layer of Transformer architecture in GLM10B. We pretrain our RecSysLLM for eight epochs with AdamW optimization (Loshchilov and Hutter 2017) on four NVIDIA RTX A100 GPUs. In order to achieve efficient use of memory and distributed training, we use the DeepSpeed (Rasley et al. 2020) module. The batch size is set to 32 per GPU. We set the peak learning rate as 1 \u00d7 10 \u2212 5 and use a warmup strategy to adjust the learning rate. In addition, we set the maximum length of input tokens to 1024.\n\n# Performance.\n\nWe pretrain our RecSysLLM on a diverse set of training examples utilizing different prompt templates across all five\n\nTable 2: Performance on rating prediction. The shadow refers to the test on unseen prompts in a zero-shot manner.\n\n<div style=\"text-align: center;\">Table 2: Performance on rating prediction. The shadow refers to the test on unseen prompts in a zero-shot manner.\n</div>\nMethods\nSports\nBeauty\nToys\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nMF\n1.0234\n0.7935\n1.1973\n0.9461\n1.0123\n0.7984\nMLP\n1.1277\n0.7626\n1.3078\n0.9597\n1.1215\n0.8097\nP5\n1.0357\n0.6813\n1.2843\n0.8534\n1.0544\n0.7177\nRecSysLLM\n1.0410\n0.7012\n1.2721\n0.8431\n1.0246\n0.7012\nP5\n1.0292\n0.6864\n1.2870\n0.8531\n1.0245\n0.6931\nRecSysLLM\n1.0278\n0.6631\n1.2671\n0.8235\n1.0112\n0.6014\ntask families. This is to thoroughly verify its multitask learning capabilities. The results in Tables 2-7 demonstrate that for tasks with seen prompt templates, our model reaches the same conclusions as the P5 model and achieves comparable or superior performance. However, we were pleasantly surprised to discover that for unseen prompt templates in a zero-shot manner, our model significantly surpasses P5. (1) From Table 2, for rating prediction, our RecSysLLM gets similar performance on prompt in the train data set, but it has better RMSE and MAE on all three datasets compared with P5 on zero-shot setting. It reflects that our RecSysLLM inherits the semantic understanding capacity of LLM on unseen prompts, which meets our expectations for the LLM. (2) In Table 4, for the sequential recommendation, our RecSysLLM surpasses P5 on Beauty and Toys. It gets better performance than P5 on unseen prompts in a zero-shot manner. The results show that our RecSysLLM gains inter- and intraentity knowledge and make more reasonable predictions. (3) As shown in Table 5, our RecSysLLM demonstrates superior performance on the task of explanation generation, both with and without feature-based hints. The large improvements in natural language processing abilities of LLMs underlie this strong performance. Moreover, the considerable increase in scores when hints are provided highlights the critical role prompt engineering plays in eliciting the full capabilities of large language models. Through prompt design and the generative power of LLMs, our system achieves state-of-the-art results on this challenging task. (4) The review summarization results further demonstrate the superiority of our RecSysLLM, as shown in Table 6. Despite having fewer parameters than T0 (7 billion vs 11 billion), our model attains higher performance across all evaluation metrics. These gains over strong baselines like T0 underscore the efficiency and effectiveness of our approach. The capability to produce high-quality summaries with fewer parameters highlights the strength of our method, delivering strong performance without the need for extremely large models. (5) For the task of direct recommendation, we make an evaluation on open question prompts to test the ability of generative recommendation. The results are illustrated in Table 7. Our RecSysLLM outperforms P5 on most evaluation metrics for this task. The simpleX model is a strong collaborative filtering baseline, but RecSysLLM achieves better top-1 item ranking compared to simpleX. To further analyze the performance gap between the P5 model and our proposed method, we conducted an in-depth examination of the training data. Table 3 illustrates that in the P5 model, the items are simply represented by numeric\n\nTable 3: The training sequences in Amazon Toys dataset for P5 and our RecSysLLM model.\n\nMethods\nSports\nBeauty\nToys\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nCaser\n0.0116\n0.0072\n0.0194\n0.0097\n0.0205\n0.0131\n0.0347\n0.0176\n0.0166\n0.0107\n0.0270\n0.0141\nHGN\n0.0189\n0.0120\n0.0313\n0.0159\n0.0325\n0.0206\n0.0512\n0.0266\n0.0321\n0.0221\n0.0497\n0.0277\nGRU4Rec\n0.0129\n0.0086\n0.0204\n0.0110\n0.0164\n0.0099\n0.0283\n0.0137\n0.0097\n0.0059\n0.0176\n0.0084\nBERT4Rec\n0.0115\n0.0075\n0.0191\n0.0099\n0.0203\n0.0124\n0.0347\n0.0170\n0.0116\n0.0071\n0.0203\n0.0099\nFDSA\n0.0182\n0.0122\n0.0288\n0.0156\n0.0267\n0.0163\n0.0407\n0.0208\n0.0228\n0.0140\n0.0381\n0.0189\nSASRec\n0.0233\n0.0154\n0.0350\n0.0192\n0.0387\n0.0249\n0.0605\n0.0318\n0.0463\n0.0306\n0.0675\n0.0374\nS3-Rec\n0.0251\n0.0161\n0.0385\n0.0204\n0.0387\n0.0244\n0.0647\n0.0327\n0.0443\n0.0294\n0.0700\n0.0376\nP5\n0.0364\n0.0296\n0.0431\n0.0318\n0.0508\n0.0379\n0.0664\n0.0429\n0.0608\n0.0507\n0.0688\n0.0534\nRecSysLLM\n0.0360\n0.0291\n0.0417\n0.0302\n0.0508\n0.0381\n0.0667\n0.0446\n0.0676\n0.0583\n0.0712\n0.0596\nP5\n0.0387\n0.0312\n0.0460\n0.0336\n0.0493\n0.0367\n0.0645\n0.0416\n0.0587\n0.0486\n0.0675\n0.0536\nRecSysLLM\n0.0392\n0.0330\n0.0512\n0.0375\n0.0501\n0.0361\n0.0650\n0.0407\n0.0630\n0.0523\n0.0691\n0.0540\nMethods\nSports\nBeauty\nToys\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nw/o hints\nAttn2Seq\n0.5305\n12.2800\n1.2107\n9.1312\n0.7889\n12.6590\n1.6820\n9.7481\n1.6238\n13.2245\n2.9942\n10.7398\nNRT\n0.4793\n11.0723\n1.1304\n7.6674\n0.8295\n12.7815\n1.8543\n9.9477\n1.9084\n13.5231\n3.6708\n11.1867\nPETER\n0.7112\n12.8944\n1.3283\n9.8635\n1.1541\n14.8497\n2.1413\n11.4143\n1.9861\n14.2716\n3.6718\n11.7010\nP5\n1.0407\n14.1589\n2.1220\n10.6096\n0.9742\n16.4530\n1.8858\n11.8765\n2.3185\n15.3474\n3.7209\n12.1312\nRecSysLLM\n1.2673\n16.7132\n2.8980\n13.0104\n1.5230\n19.0032\n3.0422\n14.7471\n2.9923\n16.7823\n4.8372\n15.0231\nw/ hints\nPETER+\n2.4627\n24.1181\n5.1937\n18.4105\n3.2606\n25.5541\n5.9668\n19.7168\n4.7919\n28.3083\n9.4520\n22.7017\nP5\n1.4689\n23.5476\n5.3926\n17.5852\n1.8765\n25.1183\n6.0764\n19.4488\n3.8933\n27.9916\n9.5896\n22.2178\nRecSysLLM\n3.7232\n30.1129\n5.0232\n20.0020\n4.8232\n26.9832\n6.2382\n21.4842\n5.9323\n29.3232\n9.4234\n23.9843\nP5\n1.4303\n23.3810\n5.3239\n17.4913\n1.9031\n25.1763\n6.1980\n19.5188\n3.5861\n28.1369\n9.7562\n22.3056\nRecSysLLM\n3.9842\n30.2913\n5.8923\n20.3821\n5.0021\n27.3854\n6.7281\n22.7439\n6.2912\n30.2948\n10.0329\n24.9932\n<div style=\"text-align: center;\">Table 6: Performance on review summarization (%). The shadow refers to the test on unseen prompts in a zero-sho\n</div>\n<div style=\"text-align: center;\">on review summarization (%). The shadow refers to the test on unseen prompts in a zero-shot manner.\n</div>\nMethods\nSports\nBeauty\nToys\nBLUE2\nROUGE1\nROUGE2\nROUGEL\nBLUE2\nROUGE1\nROUGE2\nROUGEL\nBLUE2\nROUGE1\nROUGE2\nROUGEL\nT0\n2.1581\n2.2695\n0.5694\n1.6221\n1.2871\n1.2750\n0.3904\n0.9592\n2.2296\n2.4671\n0.6482\n1.8424\nGPT-2\n0.7779\n4.4534\n1.0033\n1.9236\n0.5879\n3.3844\n0.6756\n1.3956\n0.6221\n3.7149\n0.6629\n1.4813\nP5\n2.6910\n12.0314\n3.2921\n10.7274\n1.9325\n8.2909\n1.4321\n7.4000\n1.7833\n8.7222\n1.3210\n7.6134\nRecSysLLM\n4.2823\n14.8343\n4.3984\n12.4833\n3.3821\n9.8103\n2.8543\n10.4003\n4.0320\n12.2932\n3.2943\n10.4092\nMethods\nSports\nBeauty\nToys\nHR@1\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@1\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@1\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nBPR-MF\n0.0314\n0.1404\n0.0848\n0.2563\n0.1220\n0.0311\n0.1426\n0.0857\n0.2573\n0.1224\n0.0233\n0.1066\n0.0641\n0.2003\n0.0940\nBPR-MLP\n0.0351\n0.1520\n0.0927\n0.2671\n0.1296\n0.0317\n0.1392\n0.0848\n0.2542\n0.1215\n0.0252\n0.1142\n0.0688\n0.2077\n0.0988\nSimpleX\n0.0331\n0.2362\n0.1505\n0.3290\n0.1800\n0.0325\n0.2247\n0.1441\n0.3090\n0.1711\n0.0268\n0.1958\n0.1244\n0.2662\n0.1469\nP5\n0.0641\n0.1794\n0.1229\n0.2598\n0.1488\n0.0588\n0.1573\n0.1089\n0.2325\n0.1330\n0.0386\n0.1122\n0.0756\n0.1807\n0.0975\nRecSysLLM\n0.0654\n0.2008\n0.1438\n0.2984\n0.1692\n0.0618\n0.1612\n0.1110\n0.2209\n0.1302\n0.0370\n0.1301\n0.0808\n0.1902\n0.0998\nP5\n0.0726\n0.1955\n0.1355\n0.2802\n0.1627\n0.0608\n0.1564\n0.1096\n0.2300\n0.1332\n0.0389\n0.1147\n0.0767\n0.1863\n0.0997\nRecSysLLM\n0.0892\n0.2029\n0.1502\n0.3001\n0.1703\n0.6072\n0.1502\n0.1097\n0.2317\n0.1302\n0.0327\n0.1423\n0.0825\n0.1926\n0.1028\nIDs based on their order of occurrence in the dataset. This type of simplistic representation cannot capture semantic information about the items. In contrast, our RecSysLLM model represents all items as text strings. The textual representation enables our large language model to understand and capture nuanced interrelationships between items much more effectively. We believe this is the primary reason why\n\nour model outperformed P5 across most cases. The textual representation in our model empowers it to ingest semantic details and identify meaningful connections that cannot be derived from IDs alone.\n\n# Applications in real-world dataset\n\n# Dataset\n\nThe data used in this work was collected from Alipay, a mobile payment platform in China. We extracted user behavior logs, including bills, search queries, and page visits for several recommendation tasks. Each user sequence consists of the user\u2019s 500 most recent interactions, spanning over one year of history for some users. The user sequences are used to model evolving user interests and capture both long- and short-term preferences. The training set contains 200, 000 sequences, and the test set contains 10, 000 sequences. The large-scale real-world dataset enables the modeling of complex user behavior and preferences for various recommendation tasks. The hierarchical categories and sequential interactions provide rich signals for understanding user interests.\n\n# Implementation Details\n\nOur RecSysLLM model for Chinese language tasks leverages the powerful ChatGLM-6B (Du et al. 2021) model as a foundation. ChatGLM-6B is an open-source bilingual language model with 6.2 billion parameters, trained on a trillion-token corpus comprised primarily of Chinese text with some English. The model architecture is based on the General Language Model (GLM) framework. Similarly, our approach builds on this pre-trained ChatGLM-6B foundation by utilizing LoRA to adapt the model to our specific recommender system tasks. We set the rank of Lora to 8, which is a proper coefficient chosen by the ablation study.\n\n# Sequential Recommendation.\n\nTask Description. In this section, we conduct two sequential recommendation tasks to evaluate the performance of our model, i.e., next-item prediction and candidate recommendation. For next-item prediction, the model directly predicts the next item a user will interact with based on their historical interactions and profiles. For candidate recommendation, given a user\u2019s interaction history, profiles, and a list of candidate items where only one is positive, the model chooses the correct next item. We have benchmarked our model on the Amazon Sports, Beauty, and Toys datasets and demonstrated superior recommendation capabilities compared to other baseline recommender systems. Here, we compare our RecSysLLM to the powerful generative models ChatGPT and the recently announced GPT4. We also compare our method against a basic fine-tuning approach of ChatGLM on our recommendation tasks. This allows us to analyze the improvements gained by our specialized techniques that are tailored for the recommendation systems based on LLM. By evaluating against a simple finetuning baseline, we can quantify the benefits of our proposed approach and demonstrate that our architectural choices and training methodology confer meaningful advantages on recommendation performance compared to just fine-tuning a large language model out-of-the-box.\nNext Item Prediction. The results in Table 8 demonstrate that for next-item prediction, our RecSysLLM achieves performance on par with ChatGPT, with both significantly outperforming the naive ChatGLM fine-tuning and GPT-4. This\n\nis a surprising result, as we expected the larger GPT-4 model to achieve superior performance compared to ChatGPT on this recommendation task due to its greater parameter size and pretraining scale. However, GPT-4 did not exhibit particularly strong results and was not decisively superior to ChatGPT. There are several potential explanations for why GPT-4 underperformed expectations on the next item prediction. First, the dataset and evaluation methodology used for this task may not have fully exercised GPT-4\u2019s strengths in areas like few-shot learning and knowledge recall. Second, GPT-4\u2019s more powerful generative capabilities may have caused it to diverge too far from the tight distributions of the recommendation data. There could be a mismatch between GPT-4\u2019s broad natural language generation skills and the specialized prediction required by the recommender system task. In summary, our specialized RecSysLLM demonstrates that simply utilizing a larger pre-trained language model is not the only path to improved recommendation performance. The model architecture and pretraining objectives also play a vital role. By designing a model specifically for the recommendation, focusing the pretraining on recommendation data, and tightly bounding the final fine-tuning, our RecSysLLM is able to match or exceed the performance of even much larger general language models like GPT-4 for nextitem prediction. These results highlight the importance of specialized model design in addition to scale for advancing recommendation systems.\nCandidate Recommendation. For candidate recommendation in Table 9, our RecSysLLM consistently outperforms both ChatGPT and the naive ChatGLM fine-tuning across metrics. This demonstrates the effectiveness of our specialized approach for this task. In contrast to the next item results, this time, GPT-4 achieves the overall best performance on candidate recommendation. In candidate recommendation, given a user\u2019s interaction history, profile, and a list of candidate items where only one is the ground truth next interaction, the model must choose the correct item from the candidates. With a constrained set of options provided, GPT4 is able to give full play to its powerful reasoning and deduction capabilities. The limited choice set prevents GPT4\u2019s generative tendencies from leading it astray. As a result, GPT-4 is able to leverage its scale and pretraining to achieve the best overall performance on candidate recommendation. In summary, by providing GPT-4 a focused set of candidates, we can elicit its strengths in logical reasoning while avoiding over-generation. This allows GPT-4 to achieve state-of-theart results on candidate recommendation, showcasing the benefits of its scale and pretraining. Our specialized RecSysLLM still exceeds the general language models on this task, demonstrating the value of recommendation-specific modeling. But these results highlight how large generative LMs like GPT-4 can excel given the right setup.\n\nis a surprising result, as we expected the larger GPT-4 model to achieve superior performance compared to ChatGPT on this recommendation task due to its greater parameter size and pretraining scale. However, GPT-4 did not exhibit particularly strong results and was not decisively superior to ChatGPT. There are several potential explanations for why GPT-4 underperformed expectations on the next item prediction. First, the dataset and evaluation methodology used for this task may not have fully exercised GPT-4\u2019s strengths in areas like few-shot learning and knowledge recall. Second, GPT-4\u2019s more powerful generative capabilities may have caused it to diverge too far from the tight distributions of the recommendation data. There could be a mismatch between GPT-4\u2019s broad natural language generation skills and the specialized prediction required by the recommender system task. In summary, our specialized RecSysLLM demonstrates that simply utilizing a larger pre-trained language model is not the only path to improved recommendation performance. The model architecture and pretraining objectives also play a vital role. By designing a model specifically for the recommendation, focusing the pretraining on recommendation data, and tightly bounding the final fine-tuning, our RecSysLLM is able to match or exceed the performance of even much larger general language models like GPT-4 for nextitem prediction. These results highlight the importance of specialized model design in addition to scale for advancing recommendation systems.\n\n# Conclusion\n\nThe focus of this paper is to design a novel paradigm of pretraining recommendation models based on large language models. We introduce a novel mask mechanism, span order, and positional encoding to inject inter- and intra-entity\n\nTable 8: Performance on next item recommendation.\n\nMethods\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nChatGPT\n0.4326\n0.3208\n0.5110\n0.3465\nGPT-4\n0.3846\n0.2890\n0.4674\n0.3159\nChatGLM+SFT\n0.2654\n0.2091\n0.3729\n0.2513\nRecSysLLM\n0.3805\n0.3072\n0.4756\n0.4091\nTable 9: Performance on candidate recommendation task.\n\nMethods\nHR@1\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nChatGPT\n0.3786\n0.5550\n0.4715\n0.6424\n0.5001\nGPT-4\n0.7079\n0.8154\n0.7671\n0.8560\n0.7804\nChatGLM+SFT\n0.2984\n0.7012\n0.6826\n0.7621\n0.7038\nRecSysLLM\n0.4965\n0.7435\n0.7032\n0.7728\n0.7237\nknowledge into the LLM. Although our method follows the architecture of generative language models (GLM) to some extent, the core ideas of special designs for entities in recommendation tasks can be extended to other large language models. The experiments conducted on public and industrial datasets demonstrate the effectiveness and potential of our proposed model on recommendation systems and related applications. The results show improvements over strong baselines, indicating that encoding entity relationships during pretraining can meaningfully improve downstream performance. While we validate our approach on a select set of datasets, further experiments on a wider range of tasks would better reveal the strengths and limitations of the method. In particular, evaluating the approach across a more diverse set of domains could shed light on how robust the learned representations are. Additionally, from the perspective of causal inference (Yao et al. 2021; Chu et al. 2023), there are likely further improvements to be made in terms of how semantic connections between entities are captured and injected into the model.\n\n# References\n\nAndreas, J. 2022. Language models as agent models. arXiv preprint arXiv:2212.01681. Bao, K.; Zhang, J.; Zhang, Y.; Wang, W.; Feng, F.; and He, X. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447. Bodon, F.; and R\u00b4onyai, L. 2003. Trie: an alternative data structure for data mining algorithms. Mathematical and Computer Modelling, 38(7-9): 739\u2013751. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners.  Advances in neural information processing systems, 33: 1877\u2013 1901. Chen, Z. 2023. PALR: Personalization Aware LLMs for Recommendation. arXiv preprint arXiv:2305.07622. Cheng, H.-T.; Koc, L.; Harmsen, J.; Shaked, T.; Chandra, T.; Aradhye, H.; Anderson, G.; Corrado, G.; Chai, W.; Ispir, M.; et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, 7\u201310.\n\nCho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1724\u20131734. Chu, Z.; Ding, H.; Zeng, G.; Huang, Y.; Yan, T.; Kang, Y.; and Li, S. 2022. Hierarchical capsule prediction network for marketing campaigns effect. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, 3043\u20133051. Chu, Z.; Huang, J.; Li, R.; Chu, W.; and Li, S. 2023. Causal effect estimation: Recent advances, challenges, and opportunities. arXiv preprint arXiv:2302.00848. Dai, S.; Shao, N.; Zhao, H.; Yu, W.; Si, Z.; Xu, C.; Sun, Z.; Zhang, X.; and Xu, J. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Dong, L.; Huang, S.; Wei, F.; Lapata, M.; Zhou, M.; and Xu, K. 2017. Learning to generate product reviews from attributes. In EACL. Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2021. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360. Friedman, L.; Ahuja, S.; Allen, D.; Tan, T.; Sidahmed, H.; Long, C.; Xie, J.; Schubiner, G.; Patel, A.; Lara, H.; et al. 2023. Leveraging Large Language Models in Conversational Recommender Systems. arXiv preprint arXiv:2305.07961. Gao, Y.; Sheng, T.; Xiang, Y.; Xiong, Y.; Wang, H.; and Zhang, J. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524. Geng, S.; Liu, S.; Fu, Z.; Ge, Y.; and Zhang, Y. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In  Proceedings of the 16th ACM Conference on Recommender Systems, 299\u2013315. Gu, J.; Zhao, H.; Xu, H.; Nie, L.; Mei, H.; and Yin, W. 2023. Robustness of Learning from Task Instructions. In Findings of ACL. Hidasi, B.; Karatzoglou, A.; Baltrunas, L.; and Tikk, D. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939. Hidasi, B.; Karatzoglou, A.; Baltrunas, L.; and Tikk, D. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR. Hou, Y.; Zhang, J.; Lin, Z.; Lu, H.; Xie, R.; McAuley, J.; and Zhao, W. X. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845.\n\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Hui, B.; Zhang, L.; Zhou, X.; Wen, X.; and Nian, Y. 2022. Personalized recommendation system based on knowledge embedding and historical behavior. Applied Intelligence, 1\u2013 13. Jiang, C.; Xue, S.; Zhang, J.; Liu, L.; Zhu, Z.; and Hao, H. 2022. Learning Large-scale Universal User Representation with Sparse Mixture of Experts. Kang, W.-C.; and McAuley, J. 2018. Self-attentive sequential recommendation. In  2018 IEEE international conference on data mining (ICDM), 197\u2013206. IEEE. Kang, W.-C.; Ni, J.; Mehta, N.; Sathiamoorthy, M.; Hong, L.; Chi, E.; and Cheng, D. Z. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint arXiv:2305.06474. Koren, Y.; Bell, R.; and Volinsky, C. 2009. Matrix factorization techniques for recommender systems. Computer, 42(8): 30\u201337. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25. Li, L.; Zhang, Y.; and Chen, L. 2021. Personalized Transformer for Explainable Recommendation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 4947\u20134957. Li, P.; Wang, Z.; Ren, Z.; Bing, L.; and Lam, W. 2017. Neural rating regression with abstractive tips generation for recommendation. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, 345\u2013354. Li, S.; and Zhao, H. 2021. A survey on representation learning for user modeling. In  Proceedings of the TwentyNinth International Conference on International Joint Conferences on Artificial Intelligence, 4997\u20135003. Lin, J.; Dai, X.; Xi, Y.; Liu, W.; Chen, B.; Li, X.; Zhu, C.; Guo, H.; Yu, Y.; Tang, R.; et al. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817. Liu, J.; Liu, C.; Lv, R.; Zhou, K.; and Zhang, Y. 2023a. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149. Liu, Q.; Chen, N.; Sakai, T.; and Wu, X.-M. 2023b. A First Look at LLM-Powered Generative News Recommendation. arXiv preprint arXiv:2305.06566. Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Ma, C.; Kang, P.; and Liu, X. 2019. Hierarchical gating networks for sequential recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 825\u2013833.\n\nMao, K.; Zhu, J.; Wang, J.; Dai, Q.; Dong, Z.; Xiao, X.; and He, X. 2021. SimpleX: A Simple and Strong Baseline for Collaborative Filtering. In  Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 1243\u20131252. Muhamed, A.; Keivanloo, I.; Perera, S.; Mracek, J.; Xu, Y.; Cui, Q.; Rajagopalan, S.; Zeng, B.; and Chilimbi, T. 2021. CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models. In  NeurIPS Efficient Natural Language and Speech Processing Workshop. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback.  Advances in Neural Information Processing Systems, 35: 27730\u201327744. Qiu, Z.; Wu, X.; Gao, J.; and Fan, W. 2021. U-BERT: Pretraining user representations for improved recommendation. In  Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 4320\u20134327. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. ???? Improving language understanding by generative pre-training. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1): 5485\u20135551. Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In  Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 3505\u20133506. Rendle, S.; Freudenthaler, C.; Gantner, Z.; and SchmidtThieme, L. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, 452\u2013461. Arlington, Virginia, USA: AUAI Press. ISBN 9780974903958. Sanh, V.; Webson, A.; Raffel, C.; Bach, S.; Sutawika, L.; Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Raja, A.; Dey, M.; Bari, M. S.; Xu, C.; Thakker, U.; Sharma, S. S.; Szczechla, E.; Kim, T.; Chhablani, G.; Nayak, N.; Datta, D.; Chang, J.; Jiang, M. T.-J.; Wang, H.; Manica, M.; Shen, S.; Yong, Z. X.; Pandey, H.; Bawden, R.; Wang, T.; Neeraj, T.; Rozen, J.; Sharma, A.; Santilli, A.; Fevry, T.; Fries, J. A.; Teehan, R.; Scao, T. L.; Biderman, S.; Gao, L.; Wolf, T.; and Rush, A. M. 2022. Multitask Prompted Training Enables ZeroShot Task Generalization. In International Conference on Learning Representations. Schuster, M.; and Paliwal, K. K. 1997. Bidirectional recurrent neural networks.  IEEE transactions on Signal Processing, 45(11): 2673\u20132681. Sheu, H.-S.; Chu, Z.; Qi, D.; and Li, S. 2021. Knowledgeguided article embedding refinement for session-based news\n\nrecommendation. IEEE Transactions on Neural Networks and Learning Systems, 33(12): 7921\u20137927. Shi, X.; Xue, S.; Wang, K.; Zhou, F.; Zhang, J. Y.; Zhou, J.; Tan, C.; and Mei, H. 2023. Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. arXiv preprint arXiv:2305.16646. Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In  Proceedings of the 28th ACM international conference on information and knowledge management, 1441\u20131450. Tang, J.; and Wang, K. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, 565\u2013573. Tsai, C. F.; Zhou, X.; Liu, S. S.; Li, J.; Yu, M.; and Mei, H. 2023. Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. arXiv preprint. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need.  Advances in neural information processing systems, 30. Wang, W.; Lin, X.; Feng, F.; He, X.; and Chua, T.-S. 2023. Generative recommendation: Towards next-generation recommender paradigm. arXiv preprint arXiv:2304.03516. Wang, X.; Zhou, K.; Wen, J.-R.; and Zhao, W. X. 2022. Towards unified conversational recommender systems via knowledge-enhanced prompt learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1929\u20131937. Wu, C.; Wu, F.; Qi, T.; and Huang, Y. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1652\u20131656. Wu, L.; Zheng, Z.; Qiu, Z.; Wang, H.; Gu, H.; Shen, T.; Qin, C.; Zhu, C.; Zhu, H.; Liu, Q.; et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860. Xiao, S.; Liu, Z.; Shao, Y.; Di, T.; Middha, B.; Wu, F.; and Xie, X. 2022. Training large-scale news recommenders with pretrained language models in the loop. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 4215\u20134225. Xie, S.; Qiu, J.; Pasad, A.; Du, L.; Qu, Q.; and Mei, H. 2022. Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning. In Findings of EMNLP. Xue, S.; Shi, X.; Chu, Z.; Wang, Y.; Zhou, F.; Hao, H.; Jiang, C.; Pan, C.; Xu, Y.; Zhang, J. Y.; Wen, Q.; Zhou, J.; and Mei, H. 2023. EasyTPP: Towards Open Benchmarking the Temporal Point Processes. Xue, S.; Shi, X.; Hao, H.; Ma, L.; Zhang, J.; Wang, S.; and Wang, S. 2021. A Graph Regularized Point Process Model For Event Propagation Sequence. In 2021 International Joint Conference on Neural Networks (IJCNN), 1\u20137.\n\nrecommendation. IEEE Transactions on Neural Networks and Learning Systems, 33(12): 7921\u20137927. Shi, X.; Xue, S.; Wang, K.; Zhou, F.; Zhang, J. Y.; Zhou, J.; Tan, C.; and Mei, H. 2023. Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. arXiv preprint arXiv:2305.16646. Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In  Proceedings of the 28th ACM international conference on information and knowledge management, 1441\u20131450. Tang, J.; and Wang, K. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, 565\u2013573. Tsai, C. F.; Zhou, X.; Liu, S. S.; Li, J.; Yu, M.; and Mei, H. 2023. Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. arXiv preprint. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need.  Advances in neural information processing systems, 30. Wang, W.; Lin, X.; Feng, F.; He, X.; and Chua, T.-S. 2023. Generative recommendation: Towards next-generation recommender paradigm. arXiv preprint arXiv:2304.03516. Wang, X.; Zhou, K.; Wen, J.-R.; and Zhao, W. X. 2022. Towards unified conversational recommender systems via knowledge-enhanced prompt learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1929\u20131937. Wu, C.; Wu, F.; Qi, T.; and Huang, Y. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1652\u20131656. Wu, L.; Zheng, Z.; Qiu, Z.; Wang, H.; Gu, H.; Shen, T.; Qin, C.; Zhu, C.; Zhu, H.; Liu, Q.; et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860. Xiao, S.; Liu, Z.; Shao, Y.; Di, T.; Middha, B.; Wu, F.; and Xie, X. 2022. Training large-scale news recommenders with pretrained language models in the loop. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 4215\u20134225. Xie, S.; Qiu, J.; Pasad, A.; Du, L.; Qu, Q.; and Mei, H. 2022. Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning. In Findings of EMNLP. Xue, S.; Shi, X.; Chu, Z.; Wang, Y.; Zhou, F.; Hao, H.; Jiang, C.; Pan, C.; Xu, Y.; Zhang, J. Y.; Wen, Q.; Zhou, J.; and Mei, H. 2023. EasyTPP: Towards Open Benchmarking the Temporal Point Processes. Xue, S.; Shi, X.; Hao, H.; Ma, L.; Zhang, J.; Wang, S.; and Wang, S. 2021. A Graph Regularized Point Process Model For Event Propagation Sequence. In 2021 International Joint Conference on Neural Networks (IJCNN), 1\u20137.\n\nXue, S.; Shi, X.; Zhang, Y. J.; and Mei, H. 2022. HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences. In  Advances in Neural Information Processing Systems. Yao, L.; Chu, Z.; Li, S.; Li, Y.; Gao, J.; and Zhang, A. 2021. A survey on causal inference.  ACM Transactions on Knowledge Discovery from Data (TKDD), 15(5): 1\u201346. Yao, S.; Tan, J.; Chen, X.; Zhang, J.; Zeng, X.; and Yang, K. 2022. ReprBERT: Distilling BERT to an Efficient Representation-Based Relevance Model for E-Commerce. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 4363\u20134371. Yoneda, T.; Fang, J.; Li, P.; Zhang, H.; Jiang, T.; Lin, S.; Picker, B.; Yunis, D.; Mei, H.; and Walter, M. R. 2023. Statler: State-Maintaining Language Models for Embodied Reasoning. arXiv preprint. Yu, Z.; Lian, J.; Mahmoody, A.; Liu, G.; and Xie, X. 2019. Adaptive User Modeling with Long and Short-Term Preferences for Personalized Recommendation. In IJCAI, 4213\u2013 4219. Zhang, J.; Xie, R.; Hou, Y.; Zhao, W. X.; Lin, L.; and Wen, J.-R. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001. Zhang, T.; Zhao, P.; Liu, Y.; Sheng, V. S.; Xu, J.; Wang, D.; Liu, G.; and Zhou, X. 2019. Feature-level Deeper SelfAttention Network for Sequential Recommendation. In  IJCAI, 4320\u20134326. Zhao, H.; Tan, H.; and Mei, H. 2022. Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters. In EMNLP. Zhao, H.; Wang, K.; Yu, M.; and Mei, H. 2023. Explicit Planning Helps Language Models in Logical Reasoning. arXiv preprint. Zhou, K.; Wang, H.; Zhao, W. X.; Zhu, Y.; Wang, S.; Zhang, F.; Wang, Z.; and Wen, J.-R. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In  Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 1893\u20131902.\n\nXue, S.; Shi, X.; Zhang, Y. J.; and Mei, H. 2022. HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences. In  Advances in Neural Information Processing Systems. Yao, L.; Chu, Z.; Li, S.; Li, Y.; Gao, J.; and Zhang, A. 2021. A survey on causal inference.  ACM Transactions on Knowledge Discovery from Data (TKDD), 15(5): 1\u201346. Yao, S.; Tan, J.; Chen, X.; Zhang, J.; Zeng, X.; and Yang, K. 2022. ReprBERT: Distilling BERT to an Efficient Representation-Based Relevance Model for E-Commerce. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 4363\u20134371. Yoneda, T.; Fang, J.; Li, P.; Zhang, H.; Jiang, T.; Lin, S.; Picker, B.; Yunis, D.; Mei, H.; and Walter, M. R. 2023. Statler: State-Maintaining Language Models for Embodied Reasoning. arXiv preprint. Yu, Z.; Lian, J.; Mahmoody, A.; Liu, G.; and Xie, X. 2019. Adaptive User Modeling with Long and Short-Term Preferences for Personalized Recommendation. In IJCAI, 4213\u2013 4219. Zhang, J.; Xie, R.; Hou, Y.; Zhao, W. X.; Lin, L.; and Wen, J.-R. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001. Zhang, T.; Zhao, P.; Liu, Y.; Sheng, V. S.; Xu, J.; Wang, D.; Liu, G.; and Zhou, X. 2019. Feature-level Deeper SelfAttention Network for Sequential Recommendation. In  IJCAI, 4320\u20134326. Zhao, H.; Tan, H.; and Mei, H. 2022. Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters. In EMNLP. Zhao, H.; Wang, K.; Yu, M.; and Mei, H. 2023. Explicit Planning Helps Language Models in Logical Reasoning. arXiv preprint. Zhou, K.; Wang, H.; Zhao, W. X.; Zhu, Y.; Wang, S.; Zhang, F.; Wang, Z.; and Wen, J.-R. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In  Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 1893\u20131902.\n\n# Recommendations and LLM\n\n# Motivation\n\nCompared with recommendation models based on large language models (LLMs), conventional recommendation models (Hidasi et al. 2015; Tang and Wang 2018; Kang and McAuley 2018; Sun et al. 2019; Geng et al. 2022) trained from scratch using architectures like Transformer (Vaswani et al. 2017), Bert (Devlin et al. 2018), RNN (Schuster and Paliwal 1997), CNN (Krizhevsky, Sutskever, and Hinton 2012) have several key limitations. First, they lack a deep understanding of context and semantics that comes from pretraining a large model on diverse corpora. As a result, they struggle to truly comprehend user preferences and behavioral sequences. Second, they have minimal ability to generate novel, high-quality recommendations since they are not optimized for free-form text generation. LLMs, in contrast, can produce human-like recommendations by leveraging their generative capabilities. Third, conventional models have difficulty effectively leveraging multiple data modalities like text, images, audio, etc. LLMs are adept at multimodal processing due to pretraining objectives that learn connections between modalities. Finally, LLMs can seamlessly adapt to new downstream recommendation tasks through simple fine-tuning, whereas conventional models require extensive retraining. For example, BERT4Rec (Sun et al. 2019) employs deep bidirectional self-attention to model user behavior sequences. They are trained solely based on the recommendation data without the general knowledge corpus, resulting in a limited understanding and reasoning of behavior sequence data and an inability to empower downstream tasks better. In summary, recommendation models based on pretrained LLMs are more contextual, creative, versatile, and adaptable compared to conventional models trained from scratch.\n\n# Current Development\n\nAlthough the application of LLMs like ChatGPT in recommendation has not been widely explored yet, some novel investigations have emerged recently that show their promising potential in this domain. There are mainly three categories. (1) LLM as a recommendation system.  First, Unlike traditional recommendation methods, they do not retrain a new model, relying only on the prompts of LLM (Liu et al. 2023a; Gao et al. 2023; Dai et al. 2023; Chen 2023) or slight fine-tuning (Zhang et al. 2023; Kang et al. 2023; Bao et al. 2023) to convert recommendation tasks into natural language tasks. They always design a set of prompts on recommendation scenarios, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. They explore the use of few-shot prompting to inject interaction information that contains user potential interest to help LLM better understand user needs and interests. (2) LLM as supplementary information via embeddings or tokens. This modeling paradigm (Wu et al. 2021; Qiu et al. 2021; Yao et al. 2022; Muhamed et al. 2021; Xiao et al. 2022) views the language model as a feature extractor,\n\nwhich feeds the features of items and users into LLMs and outputs corresponding embeddings. A traditional RS model can utilize knowledge-aware embeddings for various recommendation tasks. This approach (Liu et al. 2023b; Wang et al. 2022, 2023) generates tokens based on the inputted items\u2019 and users\u2019 features. The generated tokens capture potential preferences through semantic mining, which can be integrated into the decision-making process of a recommendation system. (3) LLM as Agent. As an agent, the large model assists in scheduling the entire recommendation model for recommendations and is responsible for pipeline control. Specifically, these models (Andreas 2022; Bao et al. 2023; Hou et al. 2023; Lin et al. 2023; Gao et al. 2023; Friedman et al. 2023) help to adapt LLM to the recommendation domain, coordinate user data collection, feature engineering, feature encoder, scoring/ranking function.\n\n# Challenges\n\nCompared to superficially leveraging large language models, our purpose is built on the large language model, maximizing the preservation of knowledge and logical reasoning abilities from the original large language model to ensure the inference for the behavioral sequences and fluent generation of downstream sub-tasks, while also achieving the recommendation function by learning user profile features and user behavior sequences. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. (Wu et al. 2023). Therefore, we need to preserve the tokenization, parameters, and architecture of the large language model as much as possible. For example, Pretrain, Personalized Prompt, and Predict Paradigm (P5) (Geng et al. 2022) is established upon a basic encoder\u2013decoder framework with Transformer blocks to build both the encoder and decoder. Although it is built on T5 (Raffel et al. 2020), it modified the structure of the model by adding additional positional encodings and whole-word embeddings, which will partially destroy the original knowledge in the language model. Notably, there is a difference in the format of the data. Large language models are trained on vast amounts of logically structured text, with consistent reasoning, logical thought processes, and proper grammar. In contrast, recommendation systems analyze digital user features, fixed item entities, and incoherent behavioral sequences. Additionally, The purpose of training data for large language models is to teach the model how to understand language and generate new text that is similar to the training data. Conversely, the purpose of user behavioral sequence data in recommendation systems is to dig a deeper understanding of user preferences, behavior sequences, and relationships between them so that to provide personalized recommendations. Therefore, building a recommendation system on top of a large language model that retains the LLM\u2019s knowledge and logical reasoning abilities, while also achieving the recommendation function by learning user profile features and\n\n# user behavior sequences poses significant challenges.\n\nuser behavior sequences poses significant challenges.\n\n# Baselines in Benchmark Experiments\n\nTo showcase our competence in a wide range of recommendation-related tasks, we employ representative approaches for different tasks, including Rating Prediction, Direct Recommendation, Sequential Recommendation, Explanation Generation, and Review Summarization, that have been previously used by (Geng et al. 2022). The summary of baseline methods for five different task families is provided in Table 10. Rating Prediction.  This task involves incorporating useritem rating data as part of the training set, where item ratings are represented numerically. The model is asked questions with prompts, and it outputs corresponding rating values. The baselines for this task are MF (Koren, Bell, and Volinsky 2009) and MLP (Cheng et al. 2016). Direct Recommendation. For direct recommendation, we employ classic algorithms BPR-MF (Rendle et al. 2009), BPR-MLP (Cheng et al. 2016) and SimpleX (Mao et al. 2021) as baselines. They showcase the effectiveness of direct recommendation tasks when utilizing non-semantic information as features. This allows us to gain a more comprehensive understanding of the potential of recommendations given by LLM-based models. Sequential Recommendation.  The sequential recommendation task utilizes the user\u2019s historical interaction sequences as input to predict the next item. We compare our proposed approaches with representative baselines in the field. Among that, some models aim to model the Markov Chain of user interactions by way of neural network architectures like convolutional neural networks, recurrent neural networks, and attention-based modules. Caser  (Tang and Wang 2018) employs convolutional neural networks to model user interests. HGN (Ma, Kang, and Liu 2019) adopts hierarchical gating networks to capture user behaviors from both long and short-term perspectives. GRU4Rec (Hidasi et al. 2016) utilizes recurrent neural network to model the user click history sequence. SASRec (Kang and McAuley 2018) and FDSA (Zhang et al. 2019) use self-attention modules to model feature transition patterns for sequential recommendation and the former combine RNN-based approaches to retain the sequential properties of items. BERT4Rec (Sun et al. 2019) adopts the BERT-style masked language modeling to learn the relations among items from the perspective of bidirectional representations in the recommendation. It started to use methods in neural language processing, but BERT did not have a strong semantic understanding capacity at that time. S 3-Rec  (Zhou et al. 2020) leverages selfsupervised objectives to enhance the discovery of correlations among different items and their attributes. Explanation Generation.  We evaluate the task of explanation generation by comparing the performance of several baseline models. Attn2Seq (Dong et al. 2017) and NRT (Li et al. 2017) utilizes the neural network to encode attributes of user and item into vectors and then invokes an attention mechanism or GRU (Cho et al. 2014) to generate reviews conditioned on the attribute vector. PETER (Li, Zhang, and Chen 2021) use Transformer architecture and designa a\n\ne summary of baseline methods for five differlies.\n\n<div style=\"text-align: center;\">Table 10: The summary of baseline methods for five different task families.\n</div>\nTable 10: The summary of baseline methods for five different task families.\n\nRating Pre\nMF (Koren, Bell, and Volinsky 2009)\nMLP (Cheng et al. 2016)\nDirect Rec\nBPR-MF (Rendle et al. 2009)\nBPR-MLP (Cheng et al. 2016)\nSimpleX (Mao et al. 2021)\nSequential Rec\nCaser (Tang and Wang 2018)\nHGN (Ma, Kang, and Liu 2019)\nGRU4Rec (Hidasi et al. 2016)\nBERT4Rec (Sun et al. 2019)\nFDSA (Zhang et al. 2019)\nSASRec (Kang and McAuley 2018)\nS3-Rec (Zhou et al. 2020)\nBERT4Rec (Sun et al. 2019)\nExplanation Gen\nAttn2Seq (Dong et al. 2017)\nNRT (Li et al. 2017)\nPETER (Li, Zhang, and Chen 2021)\nPETER+\nReview Sum\nT0 (Sanh et al. 2022)\nGPT-2 (Radford et al. 2019)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45aa/45aaf143-b984-452a-8416-cf85eda7807b.png\" style=\"width: 50%;\"></div>\nmodified attention mask. The variant PETER+ takes a hint feature word to augment the process of generating explanations. Review Related.  For review summarization, we adopt pretrained T0 (Sanh et al. 2022) and GPT-2 (Radford et al. 2019) as baselines. The latter model parameters were obtained from Hugging Face 1, which is a big platform to share models, datasets, and applications.\n\n# Further Analysis in the real-world dataset\n\nIn addition to optimizing the recommendation performance, it is also important to understand why large language models like ChatGPT and GPT-4 are able to effectively conduct recommendation tasks in the first place. To explore this further, we provide several real-world case studies in Figure 4, where we systematically probe and dissect the reasoning process of these models when making recommendations, using carefully designed prompt-based queries. This analysis sheds light on the strengths and limitations of relying solely on the knowledge and reasoning capabilities embedded in large pre-trained language models for recommendation tasks, and points towards potential areas for improvement. Our experiments also analyze the impact of the rank r of Low-Rank Adaptation on model performance. We evaluate five different rank values 2, 4, 8, 16, and 32  - to determine the optimal balance between model capacity and pre\n\n1 https://huggingface.co/\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f3e/2f3e678a-df0f-4f0e-b136-72c929eae385.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\">Figure 4: The case studies of ChatGPT and GPT-4 for next item recommendation in the real-world dataset.\n</div>\ndictive ability. As shown in Figure 3, we find that a rank of 8 provides sufficient learning capacity, with minimal improvements from increasing to 16. This indicates that capturing inter- and intra-entity relationships requires only a small number of additional trainable parameters beyond the base LLM, without the need for substantial model expansion. Rank 8 strikes the right balance, enabling Low-Rank Adaptation to boost performance through targeted parameterization rather than sheer scale. Overall, our results demonstrate that Low-Rank Adaptation offers an efficient approach to entity-aware language modeling.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However, effectively integrating LLM\u2019s commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.",
        "problem": {
            "definition": "The paper aims to solve the integration of LLMs into recommendation systems to leverage their reasoning and knowledge capabilities while adapting to the specific characteristics of recommendation data.",
            "key obstacle": "Existing methods either waste resources by training from scratch or lose the unique capabilities of LLMs when fine-tuning them on recommendation data."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to combine the reasoning abilities of LLMs with the domain-specific knowledge of recommendation systems.",
            "opinion": "The proposed idea is RecSysLLM, a novel pre-trained recommendation model that integrates LLMs with recommendation domain knowledge through tailored data, training, and inference designs.",
            "innovation": "RecSysLLM differs from existing approaches by retaining LLM capabilities while incorporating recommendation-specific knowledge through innovative training mechanisms."
        },
        "method": {
            "method name": "RecSysLLM",
            "method abbreviation": "RLLM",
            "method definition": "RecSysLLM is a pre-trained recommendation model that utilizes large language models, integrating domain-specific knowledge through unique designs in data, training, and inference phases.",
            "method description": "RecSysLLM leverages LLMs to enhance recommendation tasks within an efficient, unified framework.",
            "method steps": [
                "Data phase: Textualize tabular data and sample behavioral sequences from different time preferences.",
                "Training phase: Optimize an autoregressive blank infilling objective with a new mask mechanism and positional encoding.",
                "Inference phase: Implement a dynamic position mechanism to adaptively generate recommendations."
            ],
            "principle": "The method is effective because it preserves the reasoning and knowledge capabilities of LLMs while adapting to the unique characteristics of recommendation data."
        },
        "experiments": {
            "evaluation setting": "The model was evaluated on three real-world e-commerce datasets from Amazon.com, covering various categories and user interaction data.",
            "evaluation method": "Performance was assessed through multiple metrics including RMSE, MAE, HR@k, and NDCG@k, comparing against baseline methods for various recommendation tasks."
        },
        "conclusion": "The experiments demonstrate that RecSysLLM effectively improves the quality of recommendations, showcasing its potential as a unified approach for recommendation systems leveraging LLMs.",
        "discussion": {
            "advantage": "RecSysLLM combines the reasoning capabilities of LLMs with recommendation-specific knowledge, leading to better performance across various tasks.",
            "limitation": "The method may still face challenges in capturing all nuances of user preferences and behaviors due to the inherent complexity of recommendation tasks.",
            "future work": "Future research could explore broader applications of RecSysLLM across different domains and improve its adaptability to various recommendation scenarios."
        },
        "other info": {
            "additional details": {
                "authors": "Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jin Qing Cui, Longfei Li, Siqiao Xue, James Y Zhang, Sheng Li",
                "institution": "Ant Group and University of Virginia",
                "dataset": "Real-world e-commerce datasets from Amazon.com, including user ratings and reviews."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommendation algorithms have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLMs)."
        },
        {
            "section number": "2.3",
            "key information": "RecSysLLM is a pre-trained recommendation model that integrates LLMs with recommendation domain knowledge through tailored data, training, and inference designs."
        },
        {
            "section number": "4.2",
            "key information": "RecSysLLM leverages LLMs to enhance recommendation tasks within an efficient, unified framework."
        },
        {
            "section number": "6.1",
            "key information": "The proposed idea is RecSysLLM, which retains LLM capabilities while incorporating recommendation-specific knowledge through innovative training mechanisms."
        },
        {
            "section number": "10.1",
            "key information": "The integration of LLMs into recommendation systems remains a challenging problem, particularly in capturing the nuances of user preferences and behaviors."
        },
        {
            "section number": "11",
            "key information": "RecSysLLM effectively improves the quality of recommendations, showcasing its potential as a unified approach for recommendation systems leveraging LLMs."
        }
    ],
    "similarity_score": 0.8140723703500301,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c772/c7729eb6-83da-472c-b48c-b04e14dca1cd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff0a/ff0a0c41-17aa-4d95-b78f-4c3b1858d611.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45aa/45aaf143-b984-452a-8416-cf85eda7807b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f3e/2f3e678a-df0f-4f0e-b136-72c929eae385.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Leveraging large language models for pre-trained recommender systems.json"
}