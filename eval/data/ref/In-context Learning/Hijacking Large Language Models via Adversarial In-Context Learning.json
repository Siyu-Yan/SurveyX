{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.09948",
    "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
    "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. This work introduces a novel transferable attack against ICL to address these issues, aiming to hijack LLMs to generate the target response or jailbreak. Our hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos without directly contaminating the user queries. Comprehensive experimental results across different generation and jailbreaking tasks highlight the effectiveness of our hijacking attack, resulting in distracted attention towards adversarial tokens and consequently leading to unwanted target outputs. We also propose a defense strategy against hijacking attacks through the use of extra clean demos, which enhances the robustness of LLMs during ICL. Broadly, this work reveals the significant security vulnerabilities of LLMs and emphasizes the necessity for indepth studies on their robustness.",
    "bib_name": "qiang2024hijackinglargelanguagemodels",
    "md_text": "# Yao Qiang\u2217and Xiangyu Zhou\u2217and Saleh Zare Zade Prashant Khanduri and Dongxiao Zhu\nDepartment of Computer Science, Wayne State University {yao, xiangyu, salehz, khanduri.prashant, dzhu}@wayne.edu\n# Abstract\nIn-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. This work introduces a novel transferable attack against ICL to address these issues, aiming to hijack LLMs to generate the target response or jailbreak. Our hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos without directly contaminating the user queries. Comprehensive experimental results across different generation and jailbreaking tasks highlight the effectiveness of our hijacking attack, resulting in distracted attention towards adversarial tokens and consequently leading to unwanted target outputs. We also propose a defense strategy against hijacking attacks through the use of extra clean demos, which enhances the robustness of LLMs during ICL. Broadly, this work reveals the significant security vulnerabilities of LLMs and emphasizes the necessity for indepth studies on their robustness.\n# Introduction\nIn-context learning (ICL) is an emerging technique for rapidly adapting large language models (LLMs), i.e., GPT-4 (Achiam et al., 2023) and LLaMA2 (Touvron et al., 2023), to new tasks without finetuning the pre-trained models (Brown et al., 2020). The key idea behind ICL is to provide LLMs with labeled examples as in-context demonstrations (demos) within the prompt context before a test query. LLMs are able to generate responses to queries via learning from the in-context demos (Dong et al., 2022; Min et al., 2022).\nSeveral existing works, however, have demonstrated the highly unstable nature of ICL (Zhao et al., 2021; Chen et al., 2022). Specifically, performance on target tasks using ICL can vary wildly based on the selection and order of demos, giving rise to highly volatile outcomes ranging from random to near state-of-the-art (Qiang et al., 2020; Lu et al., 2021; Min et al., 2022; Pezeshkpour and Hruschka, 2023; Qiang et al., 2024). Correspondingly, several approaches (Liu et al., 2021; Wu et al., 2022; Nguyen and Wong, 2023) have been proposed to address the unstable issue of ICL. Further research has examined how adversarial examples can undermine the performance of ICL (Zhu et al., 2023a; Wang et al., 2023c,b; Shayegani et al., 2023). These studies show that maliciously designed examples injected into the prompt instructions (Zhu et al., 2023a; Zou et al., 2023; Xu et al., 2023), demos (Wang et al., 2023c; Mo et al., 2023a), or queries (Wang et al., 2023b; Kandpal et al., 2023) can successfully attack LLMs to degrade their performance, revealing the significant vulnerabilities of ICL against adversarial inputs. While existing adversarial attacks have been applied to evaluate LLM robustness, they have some limitations in practice. Most character-level attacks, e.g., TextAttack (Morris et al., 2020) and TextBugger (Li et al., 2018), can be easily detected and evaded through grammar checks, limiting realworld effectiveness (Qiang et al., 2022; Jain et al., 2023). Some other attacks like BERTAttack (Li et al., 2020) even require an extra model to generate adversarial examples. Crucially, existing attacks are not specifically crafted for ICL. As such, the inherent security risks of ICL remain largely unexplored. There is an urgent need for red teaming specifically designed for ICL to expose significant risks and improve the robustness of LLMs against potential real-world threats. This work proposes a novel adversarial attack specifically targeting ICL. We develop a gradient-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a72c/a72c4c65-0d0b-48ed-b733-23b52f703f04.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Illustrations of hijacking attack during ICL. First, our proposed GGI algorithm learns and appends adversarial suffixes like \u2018For\u2019 and \u2018Location\u2019 to the system or the user-provided in-context demos for hijacking LLMs to generate the target response, e.g., the \u2018negative\u2019 sentiment, regardless of the user queries. Second, GGI can accomplish jailbreaking by adding adversarial suffixes to in-context demos, eliciting harmful responses while bypassing the safeguards in LLMs. More detailed examples are provided in the Appendix.</div>\nbased prompt search algorithm to learn adversarial suffixes in order to efficiently and effectively hijack LLMs via adversarial ICL, as illustrated in Figure 1. (Wang et al., 2023b) is the closest work to ours where they \u2018search\u2019 adversarial examples to simply manipulate model outputs. Yet, our attack method \u2018learns\u2019 adversarial tokens that directly hijack LLMs to generate the unwanted target that disrupts alignment with the desired output. This enables our attack to be used in more complex generation tasks, such as jailbreaking, as illustrated in Figure 1. Furthermore, instead of manipulating the prompt instructions (Zhu et al., 2023a), demos (Wang et al., 2023c), or queries (Wang et al., 2023b) leveraging standard adversarial examples, e.g., character-level attacks (Morris et al., 2020; Li et al., 2018), which are detectable easily, our hijacking attack is imperceptible in that it adds only 1-2 suffixes to the demos. Specifically, these suffixes are semantically incongruous but not easily identified as typos or gibberish compared to the existing ICL attack (Wang et al., 2023c). Finally, direct attacks on user queries, such as backdoors (Kandpal et al., 2023), which require a trigger, are easily detectable and may not be practical for realworld applications. In contrast, our attack hijacks the LLM to generate the unwanted target without triggering or compromising the user\u2019s queries directly. Our adversary attacker only needs to append the adversarial tokens to system-provided demos. Our extensive experiments validate the efficacy and scalability of the proposed hijacking attacks. First, the attacks reliably induce LLMs to generate the targeted and misaligned output from the de-\nsired ones. Second, the learned adversarial tokens are transferable, remaining effective on different demo sets. Third, the adversarial transferability holds even across different datasets for the same task. Finally, our analysis shows that the adversarial suffixes distract LLMs\u2019 attention away from the task-relevant concepts. Our hijacking attacks pose a considerable threat to practical LLM applications during ICL due to their robust transferability, imperceptibility, and scalability. As this work represents one of the first efficient adversarial demo attacks during ICL, strategies for defending against such attacks have yet to be thoroughly investigated. Recently, (Mo et al., 2023b) introduced a method for defending against backdoor attacks at test time, leveraging few-shot demos to correct the inference behavior of poisoned LLMs. Similarly, (Wei et al., 2023b) explored the power of in-context demos in manipulating the alignment ability of LLMs and proposed in-context attack and in-context defense methods for jailbreaking and guarding the aligned LLMs. Consequently, we explore the potential of using in-context demos exclusively to rectify the behavior of LLMs subjected to our hijacking attacks. Our defense strategy employs additional clean in-context demos at test time to safeguard LLMs from being hijacked by adversarial in-context demos. The experimental results demonstrate the efficacy of our proposed defense method against adversarial demo attacks. This work makes the following contributions: (1) We propose a novel stealthy adversarial attack targeting in-context demos to hijack LLMs to generate unwanted target output during ICL. (2) We\ndesign a novel and efficient gradient-based prompt search algorithm to learn adversarial suffixes to demos. (3) Comprehensive experimental results across various generation tasks demonstrate the effectiveness of our hijacking attack. (4) Our extensive experiments reveal the transferability of the proposed attack across demo sets and datasets. (5) The proposed defense strategy effectively protects LLMs from being compromised by our attacks.\n# 2 Preliminaries 2.1 ICL Formulation\nFormally, ICL is characterized as a problem involving the conditional generation of text (Liu et al., 2021), where an LLM M is employed to generate response yQ given an optimal task instruction I, a demo set C, and an input query xQ. I specifies the downstream task that M should perform, e.g., \u201cChoose sentiment from positive or negative\u201d used in the sentiment generation task. C consists of N (e.g., 8) concatenated data-label pairs following a specific template S, formally: C = [S(x1, y1); \u00b7 \u00b7 \u00b7 ; S(xN, yN)], \u2018;\u2019 here denotes the concatenation operator. Thus, given the input prompt as p = [I; C; S(xQ, _)], M generates the response as \u02c6yQ = M(p). S(xQ, _) here means using the same template as the demos but with the label empty.\n# 2.2 Adversarial Attack on LLMs\nIn text-based adversarial attacks, the attackers manipulate the input x with the goal of misleading the model to generate inaccurate or malicious outputs (Zou et al., 2023; Maus et al., 2023). Specifically, given the input-output pair (x, y), the attackers aim to learn the adversarial perturbation \u03b4 adding to x by maximizing the model\u2019s objective function but without misleading humans by bounding the perturbation within the \u201cperceptual\u201d region \u2206. The objective function of the attacking process thus can be formulated as:\n(1)\n\u2208 L here denotes the task-specific loss function, for instance, cross-entropy loss for classification tasks. 3 The Threat Model\nL here denotes the task-specific loss function, for instance, cross-entropy loss for classification tasks.\n# 3 The Threat Model 3.1 ICL Hijacking Attack\nICL consists of an instruction I, a demo set C, and an input query xQ, providing more potential attack vectors than conventional text-based adversarial attacks. This work focuses on manipulating C without changing I and xQ.\nSpecifically, our hijacking attack learns the adversarial suffix tokens to the in-context demos to manipulate LLMs\u2019 output via a new greedy gradient-based prompt injection algorithm. Given a clean demo set C = [S(x1, y1); \u00b7 \u00b7 \u00b7 ; S(xN, yN)], our hijacking attack automatically produces an adversarial suffix for each demo in c, formally:\nC\u2032 = [S(x1+\u03b41, y1); \u00b7 \u00b7 \u00b7 ; S(xN+\u03b4N, yN)], (2)\n(2)\nwhere C\u2032 denotes the perturbed demo set. To make it clear, the adversarial suffixes appended to each demo as perturbations are different. In this case, the attack or perturbation budget refers to the number of tokens in each adversarial suffix. As a result, our hijacking attack induces M to generate an unwanted target output yT via appending adversarial suffix tokens on the in-context demos as yT = M(p\u2032). In other words, M generates the same or different responses for the clean and perturbed prompts depending on the True or False of M(p) = yT :\nwhere p = [I; C; S(xQ, _)] and p\u2032 = [I; C\u2032; S(xQ, _)], respectively.\n# 3.2 Hijacking Attack Objective\nWe express the goal of the hijacking attack as a formal objective function. Let us consider the LLM M as a function that maps a sequence of tokens x1:n, with x \u2208{1, \u00b7 \u00b7 \u00b7 , V } where V denote the vocabulary size, namely, the number of tokens, to a probability distribution over the next token xn+1. Specifically, P(xn+1|x1:n) denotes the probability that xn+1 is the next token given the previous tokens x1:n. Using the notations defined earlier, the hijacking attack objective we want to optimize is simply the negative log probability of the target token xn+1. The generated target output yT differs from the ground truth label yQ for the training query (xQ, yQ). Formally:\n(3)\nwhere yT neqyQ, demonstrating the attack hijacks mathcalM to generate the target output. For instance, the target output for the sentiment analysis task can be set as \u2018positive\u2019 or \u2018negative\u2019. For the jailbreaking task, we set the target token as \u2018Sure\u2019\naiming to elicit the following harmful responses. In summary, the problem of optimizing the adversarial suffix tokens can be formulated as the following optimization objective:\n(4)\nwhere i and N denote the indices and the number of the demos, respectively.\n# 3.3 Greedy Gradient-guided Injection\nA primary challenge in optimizing Eq. 4 is optimizing over a discrete set of possible token values. Motivated by prior works (Shin et al., 2020; Zou et al., 2023; Wen et al., 2024), we propose a simple yet effective algorithm for LLMs hijacking attacks, called greedy gradient-guided injection (GGI) algorithm (Algorithm 1 in the Appendix). The key idea comes from greedy coordinate descent: if we could evaluate all possible suffix token injections, we could substitute the tokens that maximize the adversarial loss reduction. Since exhaustively evaluating all tokens is infeasible due to the large candidate vocabulary size, we instead leverage gradients with respect to the suffix indicators to find promising candidate tokens for each position. We then evaluate all of these candidate injections with explicit forward passes to find the one that decreases the loss the most. This allows an efficient approximation of the true greedy selection. We can optimize the discrete adversarial suffixes by iteratively injecting the best tokens. We compute the linearized approximation of replacing the demo xi in C by evaluating the gradient \u2207exj i L(xQ) \u2208R|V |, where exj i denotes the vector representing the current value of the j-th adversarial suffix token. Note that because LLMs typically form embeddings for each token, they can be written as functions of exj i , and thus we can immediately take the gradient with respect to this quantity (Ebrahimi et al., 2017; Shin et al., 2020). The key aspects of our GGI algorithm are: firstly, it uses gradients of the selected token candidates to calculate the top candidates; secondly, it evaluates the top candidates explicitly to identify the most suitable one; and lastly, it iteratively injects the best token at each position to optimize the suffixes. This approximates an extensive greedy search in a computationally efficient manner.\n# 4 The Defense Method\nHaving developed the hijacking attack by incorporating adversarial tokens into the in-context demos,\nwe now present a straightforward yet potent defense strategy to counter this attack. Initially, we assume that defenders treat LLMs as black-box, lacking any insight into their training processes or underlying parameters. The defenders apply defense on the input prompt p directly during testtime evaluation. Their goal is to rectify the behavior of LLMs and induce LLMs to generate desired responses to user queries. Given an input prompt p\u2032 that includes adversarial tokens within the demos C\u2032, we assume that LLMs, when presented with demos containing clean data for the same tasks, can understand the genuine intent of the user\u2019s query through ICL, rather than being misled by the adversarial demos. In this context, \u2018clean data\u2019 refers to data without any adversarial tokens and is randomly selected from the training set. More precisely, the defenders modify the input prompt p\u2032 into \u02dcp by appending or inserting more clean demos into the demo set C\u2032, as follows: \u02dcp = [I; C\u2032; \u02dcC; S(xQ, _)]. \u02dcC = [S(\u02dcx1, \u02dcy1); \u00b7 \u00b7 \u00b7 ; S(\u02dcxN, \u02dcyN)] here denotes the clean demos. Through this approach, the defender guarantees that the in-context demos align with the user\u2019s query and possess resilience against adversarial attacks. In our experiments, we maintained an equal number of demos in C\u2032 and \u02dcC and observed that this method resulted in effective defense across various datasets and tasks.\n# 5 Experiment Setup\nDatasets: We evaluate the performance of our LLM hijacking algorithm and other baseline algorithms on several text generation benchmarks. SST-2 (Socher et al., 2013) and Rotten Tomatoes (RT) (Pang and Lee, 2005) are binary sentiment analysis datasets of movie reviews. AG\u2019s News (Zhang et al., 2015) is a multi-class news topic generation dataset. AdvBench (Zou et al., 2023) is a new adversarial benchmark to evaluate jailbreak attacks for circumventing the specified guardrails of LLMs to generate harmful or objectionable content. These datasets enable us to evaluate the proposed hijacking attacks across a variety of text generation tasks, including both single token and long sequential text generation. More details of the dataset statistics are provided in Table 5 of the Appendix. Large Language Models: The experiments are conducted using various LLMs covering a diverse set of architectures and model sizes, i.e., GPT2-XL (Radford et al., 2019), LLaMA-7b/13b (Touvron et al., 2023), OPT-2.7b/6.7b (Zhang et al., 2022),\nTable 1: The performance on sentiment analysis task with and without attacks on ICL. The \u2018Clean\u2019 row in gray color represents the accuracy with clean in-context demos. Other rows illustrate the accuracies with adversarial in-context demos. The details of the baselines in green color are present in Section B of the Appendix. Specifically, we employ TextAttack (TA) (Morris et al., 2020) following the attack in (Wang et al., 2023c) as the most closely related baseline for our attack (GGI). The accuracies of positive (P) and negative (N) sentiments are reported separately to highlight the effectiveness of our hijacking attack.\nectiveness of our hijacking attack.\nModel\nMethod\nSST-2\nRT\n2-shots\n4-shots\n8-shots\n2-shots\n4-shots\n8-shots\nP\nN\nP\nN\nP\nN\nP\nN\nP\nN\nP\nN\nGPT2-XL\nClean\n94.7\n52.2\n88.6\n49.4\n91.6\n69.0\n93.3\n54.7\n88.6\n76.9\n90.2\n80.5\nSquare\n99.4\n2.0\n99.8\n4.2\n99.4\n11.0\n99.8\n1.5\n100\n4.1\n99.3\n7.5\nGreedy\n100\n10.8\n100\n6.2\n100\n0.2\n100\n5.3\n100\n2.8\n100\n0.0\nTA\n95.0\n2.2\n99.8\n17.8\n99.6\n21.6\n95.9\n8.1\n96.3\n41.3\n96.4\n47.3\nGGI\n100\n1.2\n100\n0.0\n100\n0.0\n100\n2.8\n100\n0.0\n100\n0.0\nOPT-6.7b\nClean\n69.4\n87.8\n70.2\n93.8\n77.8\n93.0\n84.4\n91.4\n84.4\n93.1\n88.6\n92.8\nSquare\n99.2\n31.4\n93.8\n72.2\n99.6\n29.0\n98.1\n42.2\n97.0\n68.7\n99.4\n33.2\nGreedy\n100\n25.0\n97.8\n39.0\n100\n2.0\n99.4\n31.7\n99.8\n4.7\n100\n0.8\nTA\n94.8\n80.8\n54.8\n98.6\n91.6\n89.4\n92.5\n86.1\n77.6\n96.4\n94.0\n86.3\nGGI\n100\n0.0\n98.4\n2.0\n100\n0.2\n100\n2.6\n99.8\n0.0\n100\n0.2\nVicuna-7b\nClean\n91.4\n81.2\n88.2\n81.4\n94.6\n82.6\n84.8\n78.4\n85.9\n80.5\n90.4\n85.4\nSquare\n89.2\n84.4\n86.6\n85.8\n94.0\n83.8\n85.9\n85.4\n84.6\n88.6\n91.6\n88.4\nGreedy\n93.0\n83.4\n88.4\n87.0\n94.6\n80.0\n91.2\n82.8\n86.9\n88.7\n91.9\n85.9\nTA\n87.0\n85.2\n76.2\n88.2\n94.2\n80.6\n83.3\n84.2\n79.6\n88.6\n92.1\n84.4\nGGI\n90.6\n42.2\n96.4\n23.2\n100\n0.8\n87.6\n36.4\n95.1\n35.7\n100\n0.2\nLLaMA-7b\nClean\n81.4\n86.3\n74.4\n91.9\n82.7\n92.4\n86.0\n83.6\n81.9\n91.6\n89.3\n97.8\nSquare\n86.8\n80.0\n96.8\n58.6\n98.0\n56.4\n86.9\n57.4\n97.4\n50.1\n97.8\n57.4\nGreedy\n95.0\n47.6\n100\n0.0\n100\n0.0\n88.9\n2.8\n99.8\n0.0\n100\n0.0\nTA\n87.2\n77.8\n93.8\n69.0\n99.8\n8.8\n83.1\n57.4\n94.2\n68.9\n99.6\n3.80\nGGI\n100\n0.4\n100\n0.0\n100\n0.0\n96.8\n0.0\n100\n0.0\n100\n0.0\nLLaMA-13b\nClean\n97.8\n76.4\n95.6\n88.0\n95.8\n90.0\n94.2\n84.8\n92.7\n92.1\n91.4\n91.9\nSquare\n98.4\n72.8\n98.2\n78.4\n97.8\n85.4\n93.6\n87.4\n94.4\n84.1\n94.2\n87.6\nGreedy\n98.0\n41.4\n100\n3.0\n100\n0.0\n55.9\n11.3\n92.9\n0.0\n100\n0.4\nTA\n98.2\n72.2\n92.8\n92.8\n97.5\n87.6\n94.8\n81.8\n88.0\n94.0\n92.5\n89.3\nGGI\n99.2\n37.8\n100\n7.2\n100\n0.0\n99.1\n3.8\n86.1\n3.6\n100\n0.0\n<div style=\"text-align: center;\">Table 2: The performance of AG\u2019s News topic generation task with and without attacks on ICL. The clean and atta accuracies are reported separately for the four topics. These results highlight the effectiveness of our hijacki attacks to induce LLMs to generate the target token, i.e., \u201ctech\u201d, regardless of the query content.</div>\ninduce LLMs to generate the target token, i.e., \u201ctech\u201d, regardless of the query content.\nModel\nMethod\n4-shots\n8-shots\nword\nsports\nbusiness\ntech\nword\nsports\nbusiness\ntech\nGPT2-XL\nClean\n48.5\n87.0\n64.9\n71.9\n48.2\n50.6\n71.0\n83.6\nSquare\n2.0\n66.0\n26.8\n96.0\n19.6\n65.6\n28.0\n97.2\nGreedy\n12.8\n60.4\n29.2\n96.4\n8.0\n21.2\n10.0\n98.8\nTA\n54.8\n84.0\n73.2\n82.4\n82.0\n82.4\n91.2\n57.6\nGGI\n0.0\n2.0\n0.4\n100\n0.0\n0.0\n0.0\n100\nLLaMA-7b\nClean\n68.2\n96.8\n66.6\n49.0\n88.6\n97.4\n78.2\n61.0\nSquare\n78.4\n98.0\n76.0\n36.8\n94.4\n98.0\n60.0\n57.6\nGreedy\n69.6\n98.8\n75.2\n51.6\n89.6\n100\n68.4\n73.6\nTA\n42.4\n94.8\n67.6\n32.4\n95.2\n96.0\n39.2\n24.8\nGGI\n0.0\n20.0\n0.00\n98.0\n29.6\n56.0\n0.0\n100\nand Vicuna-7b (Chiang et al., 2023). This enables us to comprehensively evaluate attack effectiveness on both established and SOTA LLMs.\n# 6 Result and Discussion 6.1 ICL Performance\nThe rows identified as \u2018Clean\u2019 in Table 1 and Table 2 show the ICL performance on the respective tasks when using clean in-context demos. In particular, Table 1 presents the accuracies for the generation of positive (P) and negative (N) sentiments in the SST-2 and RT datasets. All the tested LLMs perform well, achieving an average accuracy of 83.6% on SST-2 and 86.7% on RT across various in-context few-shot settings. Table 2 indicates that LLMs with ICL also perform well in the context of multi-class generation on AG\u2019s News dataset. The average accuracies stand at 69.1% for 4-shot settings and 72.3% for 8-shot settings across vari-\nous LLMs. Additionally, LLMs with ICL exhibit improved performance with an increased number of in-context demos, particularly achieving best results with 8-shot settings.\nWhile LLMs utilizing ICL show strong performance with clean in-context demos, Tables 1 and 2 reveal that hijacking attacks significantly undermine their effectiveness. While the baseline methods, i.e., Square, Greedy, and TA, deteriorate model performance on the smaller LLM, e.g., GPT2-XL, they fail to effectively manipulate the larger LLMs, e.g., LLaMA-7/13 b. Additionally, these methods become inefficient as the number of in-context demonstrations increases. Compared to the baselines, our hijacking attacks successfully induce LLMs to generate the targeted positive sentiment through a few shots of adversarially perturbed\nTable 3: The performance of the defenses using ASRs across various LLMs and datasets. Adv denotes our hijacking attack using the adversarial demos. Adv+Clean, i.e., Pre and Pro, represents the proposed defense method, leveraging extra clean demos with adversarial demos. Onion (Qi et al., 2020) is the defense method based on outlier word\nn and filtering.\nModel\nSST-2\nRT\nAG\u2019s News\nAdv\nAdv+Clean\nOnion\nAdv\nAdv+Clean\nOnion\nAdv\nAdv+Clean\nOnion\nPre\nPro\nPre\nPro\nPre\nPro\nGPT2-XL\n100\n100\n99.6\n100\n100\n100\n97.4\n100\n99.1\n75.5\n80.5\n83.7\nOPT-6.7b\n98.2\n44.9\n52.5\n59.3\n99.9\n50.2\n57.8\n74.2\n65.6\n23.5\n22.5\n14.1\nLLaMA-7b\n100\n49.1\n98.3\n99.6\n100\n53.1\n99.8\n99.9\n82.8\n42.2\n88.2\n9.8\n<div style=\"text-align: center;\">Table 4: Jailbreaking performance on 200 randomly selected harmful queries from AdvBench.</div>\nselected harmful queries from AdvBench.\nModel\nMethod\nASR\n2-shots\n4-shots\nLLaMA2-7b-chat\nClean Query Only\n1.5\nICA (Wei et al., 2023b)\n3.5\n4.0\nGGI (ours)\n39.5\n54.5\nVicuna-7b\nClean Query Only\n65.0\nICA (Wei et al., 2023b)\n4.0\n67.5\nGGI (ours)\n80.0\n91.5\nLLaMA3-8b-chat\nClean Query Only\n21.0\nICA (Wei et al., 2023b)\n20.0\n61.0\nGGI (ours)\n63.5\n83.5\ndemos, resulting in predominantly higher positive accuracies than the negative ones, as shown in Tables 1. The positive test samples achieve almost 100% accuracy. On the contrary, the negative ones get nearly 0% accuracy in most settings. For the more complex multi-class AG\u2019s News topic generation task, the effectiveness of those baseline attacks decreases significantly. Only our GGI attack successfully hijacks the LLMs to generate the target topic \u2018tech\u2019, as shown in Table 2.\ndemos, resulting in predominantly higher positive accuracies than the negative ones, as shown in Tables 1. The positive test samples achieve almost 100% accuracy. On the contrary, the negative ones get nearly 0% accuracy in most settings. For the more complex multi-class AG\u2019s News topic generation task, the effectiveness of those baseline attacks decreases significantly. Only our GGI attack successfully hijacks the LLMs to generate the target topic \u2018tech\u2019, as shown in Table 2. 6.3 Jailbreaking Performance We randomly select 200 samples from AdvBench (Zou et al., 2023) as harmful queries to evaluate whether our GGI can learn adversarial tokens that generate harmful or objectionable responses. As long as LLMs generate harmful responses instead of refusal answers, as illustrated in Figure 12 of the Appendix, we consider it as a successful attack. When we input clean queries directly into the tested LLMs, i.e., LLaMA2-7b-chat, Vicuna-7b, and LLaMA3-8b-chat, their safeguards generally prevent the generation of harmful content, resulting in only a few harmful responses, as evidenced by the low ASRs in Table 4. Recently, (Wei et al., 2023b) proposed In-Context Attack (ICA), which employs harmful demos to subvert LLMs for jailbreaking, which achieves slightly higher ASRs as illustrated in Table 4. Furthermore, we utilize GGI to efficiently learn adversarial tokens from harmful demos and then append them to the demos during ICL. Our attack achieves the highest ASRs compared to the baselines, demonstrating the effectiveness of our hijacking attack in inducing harmful responses for jailbreaking, as shown in Figure 12 of the Appendix. The jailbreaking results further illus-\ntrate the applicability of our GGI method to more complex generative tasks, effectively hijacking the model to generate malicious responses.\n# 6.4 Defense Method Performance\nTable 3 presents ASRs of our hijacking attack when countered with the proposed defense mechanism that uses additional clean demos and the baseline defense Onion (Qi et al., 2020). Our proposed defense method is tested in two different settings. The preceding (Pre) setting places the clean demos before the adversarial demos in the sequence \u02dcp = [I; \u02dcC; C\u2032; S(xQ, _)]. Conversely, the proceeding (Pro) setting adds the clean demos after the adversarial demos as \u02dcp = [I; C\u2032; \u02dcC; S(xQ, _)]. The decreases in ASRs of our hijacking attack affirm the effectiveness of these defense methods. Notably, the results of Pre in considerably lower ASRs compared to Pro, which relates to the mechanism through which our hijacking attack induces LLMs to generate target outputs, as discussed in Appendix Sec G. Although the Onion method is ineffective at defending against hijacking attacks in sentiment analysis tasks, it successfully protects LLMs from hijacking attacks in more complex topic generation tasks. Furthermore, the results indicate that all the defense methods are ineffective on small-sized LLMs, such as the GPT2-XL used in our experiments, due to their limited emergent abilities.\n# 6.5 Transferability of GGI\nOur GGI exhibits two advanced transferabilities: across different demo sets and across different datasets of the same task. Firstly, the adversarial tokens derived from any demo can be used in any ICL demo set. Once selected, these adversarial tokens consistently hijack LLMs regardless of the demos employed by developers or users, demonstrating their robustness and effectiveness. As illustrated in Figure 2, we evaluated the same adversarial tokens on three distinct demo sets from SST-2 and RT, respectively. Both sets resulted in high ASRs on both SST-2 and RT datasets, highlighting their transferability across different demo sets. Furthermore, the adversarial tokens, such as \u2018NULL\u2019 and \u2018Remove,\u2019 as illustrated in Figure 10 of the Appendix, used in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6844/6844c657-7c08-4f9d-ba85-26444d1e00c3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Transferability of GGI across different demo sets and different datasets of the same task. The normal and striped bars indicate the demos are from SST-2 and RT, respectively. Different colors represent test queries from different datasets.</div>\nFigure 2: Transferability of GGI across different demo sets and different datasets of the same task. The normal and striped bars indicate the demos are from SST-2 and RT, respectively. Different colors represent test queries from different datasets. sentiment analysis tasks were learned from the RT dataset and effectively applied to the SST-2 dataset. Our attack GGI achieves promising adversarial attack success rates on both SST-2 and RT datasets, as demonstrated by Figure 2.\nsentiment analysis tasks were learned from the RT dataset and effectively applied to the SST-2 dataset. Our attack GGI achieves promising adversarial attack success rates on both SST-2 and RT datasets, as demonstrated by Figure 2.\n# 6.6 Stealthiness of GGI\nFigure 3 presents the perplexity scores for the input prompts from different attack methods. The perplexity scores for the word-level adversarial attacks, i.e., Greedy, Square, and Ours, exhibit nonsignificant increases compared to the clean samples, highlighting their stealthiness. This demonstrates that using a perplexity-based filter, e.g., Onion (Qi et al., 2020), would be challenging to defend against our attacks. However, the character-level attack TA, used in (Wang et al., 2023c), results in significantly higher perplexity scores than others. This makes it more easily detected or corrected by basic grammar checks, as illustrated in Figure 10 and Figure 11 in the Appendix.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/841b/841b9863-904f-4bee-87e2-f140f436891b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Average perplexity scores from LLaMA-7b on 100 random samples under 4-shots setting of RT derived from three separate runs under various attacks.</div>\nFigure 3: Average perplexity scores from LLaMA-7b on 100 random samples under 4-shots setting of RT derived from three separate runs under various attacks.\n# 7 Related Work 7.1 In-Context Learning\nLLMs have shown impressive performance on numerous NLP tasks (Devlin et al., 2018; Lewis et al., 2019; Radford et al., 2019). Although fine-tuning has been a common method for adapting models\nto new tasks, it is often less feasible to fine-tune extremely large models with over 10 billion parameters. As an alternative, recent work has proposed ICL, where the model adapts to new tasks solely via inference conditioned on the provided in-context demos, without any gradient updates (Brown et al., 2020). By learning from the prompt context, ICL allows leveraging massive LLMs\u2019 knowledge without the costly fine-tuning process, showcasing an exemplar of the LLMs\u2019 emergent abilities (Schaeffer et al., 2023; Wei et al., 2022). Intensive research has been dedicated to ICL. Initial works attempt to find better ways to select labeled examples for the demos (Liu et al., 2021; Rubin et al., 2021). For instance, (Liu et al., 2021) presents a simple yet effective retrievalbased method that selects the most semantically similar examples as demos, leading to improved accuracy and higher stability. Follow-up works have been done to understand why ICL works (Xie et al., 2021; Razeghi et al., 2022; Min et al., 2022; Wei et al., 2023a; Kossen et al., 2023). (Xie et al., 2021) provides theoretical analysis that ICL can be formalized as Bayesian inference that uses the demos to recover latent concepts. Another line of research reveals the brittleness and instability of ICL approaches: small changes to the demo examples, labels, or order can significantly impact performance (Lu et al., 2021; Zhao et al., 2021; Min et al., 2022; Nguyen and Wong, 2023).\n# 7.2 Adversarial Attacks on LLMs\nEarly adversarial attacks on LLMs apply simple character or token operations to trigger the LLMs to generate incorrect predictions, such as TextAttack (Morris et al., 2020) and BERT-Attack (Li et al., 2020). Since these attacks usually generate misspelled and/or gibberish prompts that can be detected using spell checker and perplexitybased filters, they are easy to block in real-world applications. Some other attacks struggled with optimizing over discrete text, leading to the manual or semi-automated discovery of vulnerabilities through trial-and-error (Li et al., 2021; Perez and Ribeiro, 2022; Li et al., 2023c; Qiang et al., 2023; Casper et al., 2023; Kang et al., 2023; Li et al., 2023a; Shen et al., 2023). For example, jailbreaking prompts are intentionally designed to bypass an LLM\u2019s built-in safeguard, eliciting it to generate harmful content that violates the usage policy set by the LLM vendor (Shen et al., 2023; Zhu et al., 2023b; Chao et al., 2023; Mehrotra et al., 2023;\nJeong, 2023; Guo et al., 2024; Yu et al., 2024). These red teaming efforts craft malicious prompts in order to understand LLM\u2019s attack surface (Ganguli et al., 2022). However, the discrete nature of text has significantly impeded learning more effective adversarial attacks against LLMs. Recent work has developed gradient-based optimizers for efficient text modality attacks. For example, (Wen et al., 2023) presented a gradientbased discrete optimizer that is suitable for attacking the text pipeline of CLIP, efficiently bypassing the safeguards in the commercial platform. (Zou et al., 2023), building on (Shin et al., 2020), described an optimizer that combines gradient guidance with random search to craft adversarial strings that induce LLMs to respond to the questions that would otherwise be banned. More recently, (Zhao et al., 2024) proposed poisoning demo examples and prompts to make LLMs behave in alignment with pre-defined intentions. Our hijacking attack algorithm falls into this stream of work, yet we target few-shot ICL instead of zero-shot queries. We use gradient-based prompt search to automatically learn effective adversarial suffixes rather than manually engineered prompts. Importantly, we show that LLMs can be hijacked to output the targeted unwanted output by appending optimized adversarial tokens to the ICL demos, which reveals a new lens of LLM vulnerabilities that prior approaches may have missed.\n# 7.3 Defense Against Attacks on LLMs\nThe existing literature on the robustness of LLMs includes various strategies for defense (Liu et al., 2023; Xu et al., 2024; Wu et al., 2024). However, most of these defenses, such as those involving adversarial training (Liu et al., 2020; Li et al., 2023b; Formento et al., 2024; Wang et al., 2024) or data augmentation (Qiang et al., 2024; Yuan et al., 2024), need to re-train or fine-tune the models, which is computationally infeasible for LLM users. Moreover, restricting many closed-source LLMs to only permit query access for candidate defenses introduces new challenges. Recent studies focus on developing defenses against attacks on LLMs that utilize adversarial prompting. (Jain et al., 2023) and (Alon and Kamfonas, 2023) have suggested using perplexity filters to detect adversarial prompts. While the filters are effective at catching the attack strings that contain gibberish words or character-level adversarial tokens with high perplexity scores, they fall short\nin detecting more subtle adversarial prompts, like the ones used in our adversarial demo attacks with as low perplexity as clean samples shown in Figure 3. Recently, (Mo et al., 2023b) introduced a method to mitigate backdoor attacks at test time by identifying the task and retrieving relevant defensive demos. These demos are combined with user queries to counteract the adverse effects of triggers present in backdoor attacks. This defense strategy eliminates the need for modifications or tuning of LLMs. Its objective is to re-calibrate and correct the behavior of LLMs during test-time evaluations. Similarly, (Wei et al., 2023b) investigated the role of in-context demos in enhancing the robustness of LLMs and highlighted their effectiveness in defending against jailbreaking attacks. The authors developed an in-context defense strategy that constructs a safe context to caution the model against generating any harmful content. So far, defense mechanisms against adversarial demo attacks have not been extensively explored. Our approach introduces a test-time defense strategy that uses additional clean in-context demos to safeguard LLMs from adversarial in-context manipulations. In line with prior works (Mo et al., 2023b; Wei et al., 2023b; Wang et al., 2024), this defense strategy avoids the necessity for retraining or fine-tuning LLMs. Instead, it focuses on re-calibrating and correcting the behavior of LLMs during evaluations at test time.\n# 8 Conclusion\nThis work reveals the vulnerability of ICL via crafted hijacking attacks. By appending imperceptible adversarial suffixes to the in-context demos using a greedy gradient-based algorithm, our attack GGI effectively hijacks the LLMs to generate the unwanted target outputs by diverting their attention from the relevant context to the adversarial suffixes. Furthermore, GGI can accomplish jailbreaking by adding adversarial suffixes to in-context demos, eliciting harmful responses while bypassing the safeguards in LLMs. The advanced transferability of GGI makes it significantly more efficient and scalable for real-world applications. GGI\u2019s imperceptibility and stealthiness highlight the difficulty of defending against it with simple grammar checks and perplexity-based filters. We propose a test-time defense strategy that effectively protects LLMs from being compromised by our attack. We will continue studying novel attack and defense techniques for more robust ICL approaches.\n# 9 Limitations and Risks\nThis work uncovers a potential vulnerability of LLMs during in-context learning. By inserting adversarial tokens, which our algorithm has learned, into in-context demos, we can make the LLM generate undesired target outputs without the need for a trigger in the query nor contaminating the user\u2019s queries. This work represents a purple teaming effort to discover LLM\u2019s vulnerabilities during in-context learning and defend against attacks. It offers a unified platform that enables both the red team and blue team to collaborate more effectively. Moreover, it facilitates a seamless knowledge transfer between the teams. As such, it will not pose risks for natural users or LLM vendors. Rather, our findings can be utilized by these stakeholders to guard against malicious uses and enhance the robustness of LLMs to such threats.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132. Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. 2020. Square attack: a query-efficient black-box adversarial attack via random search. In European conference on computer vision, pages 484\u2013501. Springer. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. 2023. Explore, establish, exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442. Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. 2023. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419. Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. 2022. On the relation between sensitivity and accuracy in in-context learning. arXiv preprint arXiv:2209.07661. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2017. Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751. Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, and See-Kiong Ng. 2024. Semrode: Macro adversarial training to learn representations that are robust to word-level attacks. arXiv preprint arXiv:2403.18423.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858.\nXingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu. 2024. Cold-attack: Jailbreaking llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679.\nNeel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614.\nNikhil Kandpal, Matthew Jagielski, Florian Tram\u00e8r, and Nicholas Carlini. 2023. Backdoor attacks for in-context learning with language models. arXiv preprint arXiv:2307.14692.\nXin Li, Xiangrui Li, Deng Pan, Yao Qiang, and Dongxiao Zhu. 2023c. Learning compact features via intraining representation alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 8675\u20138683. Xin Li, Xiangrui Li, Deng Pan, and Dongxiao Zhu. 2021. Improving adversarial robustness via probabilistically compact loss with logit constraints. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 8482\u20138490. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804. Qin Liu, Fei Wang, Chaowei Xiao, and Muhao Chen. 2023. From shortcuts to triggers: Backdoor defense with denoised poe. arXiv preprint arXiv:2305.14910. Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. 2020. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786. Natalie Maus, Patrick Chao, Eric Wong, and Jacob R Gardner. 2023. Black box adversarial prompting for foundation models. In The Second Workshop on New Frontiers in Adversarial Machine Learning. Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. 2023. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837. Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun. 2023a. How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities. arXiv preprint arXiv:2311.09447. Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, and Muhao Chen. 2023b. Testtime backdoor mitigation for black-box large language models with defensive demonstrations. arXiv preprint arXiv:2311.09763. John X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909.\nTai Nguyen and Eric Wong. 2023. In-context example selection with influences. arXiv preprint arXiv:2302.11042. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL. F\u00e1bio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527. Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483. Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2020. Onion: A simple and effective defense against textual backdoor attacks. arXiv preprint arXiv:2011.10369. Yao Qiang, Supriya Tumkur Suresh Kumar, Marco Brocanelli, and Dongxiao Zhu. 2022. Tiny rnn model with certified robustness for text classification. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE. Yao Qiang, Chengyin Li, Prashant Khanduri, and Dongxiao Zhu. 2023. Interpretability-aware vision transformer. arXiv preprint arXiv:2309.08035. Yao Qiang, Xin Li, and Dongxiao Zhu. 2020. Toward tag-free aspect based sentiment analysis: A multiple attention network approach. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE. Yao Qiang, Subhrangshu Nandi, Ninareh Mehrabi, Greg Ver Steeg, Anoop Kumar, Anna Rumshisky, and Aram Galstyan. 2024. Prompt perturbation consistency learning for robust language models. arXiv preprint arXiv:2402.15833. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004. Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844.\nXinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. \" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825. Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, et al. 2023a. Are large language models really robust to word-level perturbations? arXiv preprint arXiv:2309.11166. Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. 2023b. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095. Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie Hu, Yixuan Li, Bo Li, and Chaowei Xiao. 2024. Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment. arXiv preprint arXiv:2402.14968. Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. 2023c. Adversarial demonstration attacks on large language models. arXiv preprint arXiv:2305.14950. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023a. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846. Zeming Wei, Yifei Wang, and Yisen Wang. 2023b. Jailbreak and guard aligned language models with only few in-context demonstrations. arXiv preprint arXiv:2310.06387.\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36.\nblum, Jonas Geiping, and Tom Goldstein. 2023. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36. Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. 2024. A new era in llm security: Exploring security concerns in real-world llmbased systems. arXiv preprint arXiv:2402.18649. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2022. Self-adaptive in-context learning. arXiv preprint arXiv:2212.10375. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. 2023. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. arXiv preprint arXiv:2305.14710. Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. 2024. Llm jailbreak attack versus defense techniques\u2013a comprehensive study. arXiv preprint arXiv:2402.13457. Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, and Ning Zhang. 2024. Don\u2019t listen to me: Understanding and exploring jailbreak prompts of large language models. arXiv preprint arXiv:2403.17336. Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo Li. 2024. Rigorllm: Resilient guardrails for large language models against undesired content. arXiv preprint arXiv:2403.13031. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In NIPS. Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, and Jinming Wen. 2024. Universal vulnerabilities in large language models: In-context learning backdoor attacks. arXiv preprint arXiv:2401.05949.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023a. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528. Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. 2023b. Autodan: Automatic and interpretable adversarial attacks on large language models. arXiv preprint arXiv:2310.15140. Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.\n# A Experiments Details\nDataset Statistics: We show the dataset statistics in Table 5. Specifically for the SST-2 and RT sentiment analysis tasks, we employ only 2 training queries to train adversarial suffixes using our GGI method. We use 4 training queries for the more complex multi-class topic generation tasks, i.e., AG\u2019s News. We randomly select 1,000 samples as user queries for testing. Similarly, we utilize 4 training queries from Advbench (Zou et al., 2023) for the jailbreaking task and evaluate the attack success rate on 200 randomly selected harmful queries.\n<div style=\"text-align: center;\">Table 5: Statistics of the training queries used in Algorithm 1 and test queries for the three datasets.</div>\nDatasets\nTraining Queries\nTest Queries\nSST-2\n2\n1,000\nRT\n2\n1,000\nAG\u2019s News\n4\n1,000\nAdvBench\n4\n200\nICL Settings: For ICL, we follow the setting in (Wang et al., 2023c) and use their template to incorporate the demos for prediction. The detailed template is provided in Figure 9. We evaluate the 2-shot, 4-shot, and 8-shot settings for the number of demos. Specifically, for each test example, we randomly select the demos from the training set and repeat this process 5 times, reporting the average accuracy over the repetitions. Evaluation Metrics: Several different metrics evaluate the performance of ICL and hijacking attacks. Clean accuracy evaluates the accuracy of ICL on downstream tasks using clean demos. Attack accuracy evaluates the accuracy of ICL given the perturbed demos. Defense accuracy demonstrates the accuracy of ICL with the defense method against the hijacking attack. We further evaluate the effectiveness of hijacking attacks using attack success rate (ASR). Given a test sample (x, y) from a test set D, the clean and perturbed prompts are denoted as p = [I; C; x] and p\u2032 = [I; C\u2032; x], respectively. For the general generation tasks, such as sentiment analysis and news topic generation, ASR is calculated as\n(5)\nwhere 1 denotes the indicator function and yT \u0338= y. For the jailbreaking task, ASR is calculated as:\n(6)\n# where y represents a refusal response by safeguards and yH here denotes the harmful response.\nwhere y represents a refusal response by safeguards and yH here denotes the harmful response.\n# B Baseline Attacks\nGreedy Search: We consider a heuristics-based perturbation strategy, which conducts a greedy search over the vocabulary to select tokens, maximizing the reduction in the adversarial loss from Eq. 3. Specifically, it iteratively picks the token that decreases the loss the most at each step. Square Attack: The square attack (Andriushchenko et al., 2020) is an iterative algorithm for optimizing high-dimensional black-box functions using only function evaluations. To find an input x + \u03b4 in the demo set C that minimizes the loss in Eq. 3, the square attack has three steps: Step 1: Select a subset of inputs to update; Step 2: Sample candidate values to substitute for those inputs; Step 3: Update x + \u03b4 with the candidate values that achieve the lowest loss. The square attack can optimize the hijacking attack objective function without requiring gradient information by iteratively selecting and updating a subset of inputs. Text Attack: We also utilize TextAttack (TA) (Morris et al., 2020), adopting a similar approach to the attack described by (Wang et al., 2023c), which serves as the most closely related baseline for our hijacking attack. Unlike our word-level attack, the use of TA at the character level includes minor modifications to some words in the in-context demos and simply flips the labels of user queries, as depicted in Figure 8. In our experiments, we employ a transformation where characters are swapped with those on adjacent QWERTY keyboard keys, mimicking errors typical of fast typing, as done in TextAttack (Morris et al., 2020). Specifically, we use the adversarial examples for the same demos in our hijacking attack during the application of TA.\n# C Attack Performance\nIn addition to the attack accuracy performance provided in Table 1 and 2, we present ASRs for various attacks across the three datasets. As outlined in Table 6, our GGI attack achieves the highest ASRs, substantiating its highest effectiveness in hijacking the LLM to generate the targeted output. In sentiment analysis tasks like SST-2 and RT, some attacks exhibit high ASRs. Meanwhile, for the more complex multi-class topic generation task, such as AG\u2019s News, only our GGI attack achieves\n<div style=\"text-align: center;\">Table 6: ASR among different datasets, models, and attack methods. Best scores are in bold.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e57/4e57168d-4a58-4fe6-a191-3e07aafcb6ed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/27dd/27ddc9e0-83aa-42a8-b767-7de63d9aaea5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Impact of LLM size on adversarial robustness. ASRs on the AG\u2019s News topic generation task using different sizes of OPT models, i.e., OPT-2.7b and OPT6.7b, with two different few-shot settings.</div>\nFigure 4: Impact of LLM size on adversarial robustness. ASRs on the AG\u2019s News topic generation task using different sizes of OPT models, i.e., OPT-2.7b and OPT6.7b, with two different few-shot settings.\nhigh ASRs. This further emphasizes the potential effectiveness of our hijacking attack on more complex generative tasks, such as question answering.\n# D Impact of Number of In-context Demos\nWe extend our investigation to explore the impact of in-context demos on adversarial ICL attacks. We observe a substantial impact on the attack performance in ICL based on the number of demos employed. As indicated in Tables 1 and 2, an increase in the number of in-context demos correlates with a higher susceptibility of the attack to hijack LLMs, resulting in the generation of target outputs with greater ease. Specifically, in the 8-shot setting, LLMs consistently exhibit significantly lower accuracies in negative sentiment generation, demonstrating a higher rate of successful attacks compared to the 2-shot and 4-shot settings. Moreover, the attacks demonstrate higher ASRs as the number of in-context demos used in ICL increases, as shown in Table 6.\n# E Impact of Sizes of LLMs\nResults in Table 6 reveal that the ASRs on GPT2XL are significantly higher than those on LLaMA7b, suggesting that hijacking the larger LLM is more challenging. Here, we continue examining how the size of LLMs influences the performance of hijacking attacks. Table 7 illustrates the performance of sentiment analysis tasks with and without attacks on ICL using different sizes of OPT, i.e., OPT-2.7b and OPT-6.7b. These results further highlight that the smaller LLM, i.e., OPT-2.7b, is much easier to attack and induce to generate unwanted target outputs, such as \u2018positive\u2019, in the sentiment analysis tasks. Figure 4 illustrates our proposed hijacking attack performance using ASR on two OPT models of varying sizes in AG\u2019s News topic generation task. It clearly shows that attacking the smaller OPT2-2.7b model achieves a much higher ASR in both settings, confirming our finding and others (Wang et al., 2023a) that larger models are more resistant to adversarial attacks.\n# F Comparison of Hijacking Attacks\nIn contrast to baseline hijacking attacks, i.e., Square and Greedy, our GGI exhibits superior performance in generating targeted outputs, as evidenced by the results in Table 1 and 2, along with the highest ASRs highlighted in Table 6. This underscores the effectiveness of GGI as a more potent method of attack. To further illustrate the efficiency of our GGI, we present the objective function values of Eq. 3 in Figure 5 for various attack methods. Since our GGI attack enjoys the advantages of both greedy and gradient-based search strategies as depicted in Algorithm 1, the values of the object function decrease steadily and rapidly, ultimately reaching\n<div style=\"text-align: center;\">Table 7: The performance of sentiment analysis task with and without attacks on ICL using different sizes of OP</div>\nModel\nMethod\nSST-2\nRT\n2-shots\n4-shots\n8-shots\n2-shots\n4-shots\n8-shots\nP\nN\nP\nN\nP\nN\nP\nN\nP\nN\nP\nN\nOPT-2.7b\nClean\n98.5\n38.6\n85.6\n62.8\n58.4\n76.4\n98.1\n36.6\n81.2\n68.4\n57.8\n89.6\nSquare\n100\n0.0\n100\n0.0\n100\n1.8\n100\n1.3\n100\n0.0\n99.6\n7.5\nGreedy\n100\n0.0\n100\n0.0\n100\n0.0\n100\n0.4\n100\n0.2\n100\n0.0\nTA\n99.6\n13.8\n99.8\n26.8\n99.0\n7.2\n97.6\n52.9\n97.2\n59.7\n99.4\n6.8\nGGI\n100\n0.0\n100\n0.0\n100\n0.0\n100\n0.0\n100\n0.0\n100\n0.0\nOPT-6.7b\nClean\n69.4\n87.8\n70.2\n93.8\n77.8\n93.0\n84.4\n91.4\n84.4\n93.1\n88.6\n92.8\nSquare\n99.2\n31.4\n93.8\n72.2\n99.6\n29.0\n98.1\n42.2\n97.0\n68.7\n99.4\n33.2\nGreedy\n100\n25.0\n97.8\n39.0\n100\n2.0\n99.4\n31.7\n99.8\n4.7\n100\n0.8\nTA\n94.8\n80.8\n54.8\n98.6\n91.6\n89.4\n92.5\n86.1\n77.6\n96.4\n94.0\n86.3\nGGI\n100\n0.0\n98.4\n2.0\n100\n0.2\n100\n2.6\n99.8\n0.0\n100\n0.2\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c2e9/c2e901c5-4296-4fe4-b050-02a9cf6f10de.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: An illustration of the learning objective values during iterations among different attacks on SST2 using GPT2-XL with 8-shots.</div>\nthe minimum loss value. On the other hand, both the Square and Greedy attacks use a greedy search strategy, with fluctuating results that increase and decrease the loss value, unable to converge to the minimum loss value corresponding to the optimal adversarial suffixes.\n# G Diverting LLM Attention\nAttempting to interpret the possible mechanism of our hijacking attacks, we show an illustrative example using attention weights from LLaMA-7b on the SST2 task with both clean and perturbed prompts. As depicted in Figure 6b, the model\u2019s attention for generating the sentiment token of the test query has been diverted towards the adversarial suffix tokens \u2018NULL\u2019 and \u2018Remove\u2019. Compared to the attention maps using the clean prompt (Figure 6a), these two suffixes attain the largest attention weights represented by the darkest green color. This example illuminates a possible mechanism for why our hijacking attack can induce the LLM to generate the targeted outputs - the adversarial suffixes divert the LLMs\u2019 attention away from the original query. Additionally, Figure 7 illustrates the attention\ndistribution for the perturbed prompts after applying the preceding and proceeding defense methods. Notably, in the demos, the model primarily focuses on the front segments of demos, which are indicated by a darker green color. Therefore, the model converts its attention to the front segments, which are the extra clean samples, in the preceding method. These clean samples effectively re-calibrate and rectify the model\u2019s behavior, leading to a significant reduction in ASRs, as shown in Table 3. In contrast, the first few demos remain adversarial in the proceeding method, rendering it ineffective in defending against the adversarial demo attack. Overall, these attention maps visualize how the adversarial suffixes distract LLMs from focusing on the relevant context to generate the unwanted target output and how our proposed defense methods rectify the behavior of LLMs given the extra clean demos.\n# H More Results\nFigure 9 illustrates the prompt template employed in ICL for various tasks. For the SST2/RT dataset, the template is structured to include an instruction, a demo set composed of reviews and sentiment labels, and the user query. Similarly, the AG\u2019s News dataset template comprises the instruction, the demo set with articles and topic labels, and the user query. The AdvBench template includes instructions, a demo set of harmful queries and responses, and a user\u2019s harmful query. Additionally, examples are provided in Figure 10, Figure 11, and Figure 12 to enhance understanding.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f42/6f42c8ad-d1b7-447e-8e21-808de9672696.png\" style=\"width: 50%;\"></div>\nFigure 6: Attentions maps generated using (a) clean and (b) adversarial perturbed prompts. In (b), the adversarial suffix tokens, i.e., \u2018NULL\u2019 and \u2018Remove\u2019, are underlined in red. Darker green colors represent larger attention weights. The prompts are tokenized to mimic the actual inputs to the LLMs. Best viewed in color.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f406/f406b696-25de-4267-9847-9fc0236af134.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/11aa/11aa661f-542f-45c4-85b1-1225e7125a31.png\" style=\"width: 50%;\"></div>\nFigure 8: Illustrations of ICL using clean prompt and adversarial prompt. Given the clean in-context demos, LLMs can correctly generate the sentiment of the test queries. The previous attacks (Wang et al., 2023c) at the character level involve minor edits in some words, such as altering \u2018so\u2019 to \u2018s0\u2019 and \u2018film\u2019 to \u2018fi1m\u2019, of these in-context demos, leading to incorrect sentiment generated for the test queries. However, ours learns to append adversarial suffixes like \u2018For\u2019 and \u2018Location\u2019 to the in-context demos to efficiently and effectively hijack LLMs to generate the unwanted target, e.g., the \u2018negative\u2019 sentiment, regardless of the test query content. It is important to highlight that the adversary attacker only needs to append the adversarial tokens to either the system or the user-provided demos without compromising the user\u2019s queries directly.\nAlgorithm 1: Greedy Gradient-guided Injection (GGI)\nInput\n: Model: M, Iterations: T, Batch Size: b, Instruction: I, Demos: C, Query: (xQ, yQ)\nTarget: yT\nInitialization: p\u2032\n0 = [I; [S(x1 + \u03b41, y1); \u00b7 \u00b7 \u00b7 ; S(xN + \u03b4N, yN)]; S(xQ, yT )]\nrepeat\nfor i \u2208N do\n[\u03b4i1; ...; \u03b4ik] = Top\u2212k(\u2212\u2207p\u2032L(M(\u02c6y|p\u2032\nt\u22121), yT ))\n/* Compute top-k substitutions */\nK = {[\u03b4i1; ...; \u03b4ik] | i = 1, ..., N}\nB = {(\u03b4i1, . . . , \u03b4ib) | (\u03b4i1, . . . , \u03b4ik) \u2208K}\n/* Introducing variability by selecting different\nsubsets of substitutions in each iteration\nhelps avoid local minima */\nfor i \u2208N do\n\u03b4\u22c6\ni = \u03b4ij, where j = argmin\u03b4ibL(M(\u02c6y|p\u2032\nt\u22121), yT )\n/* Compute best replacement */\n\u2206= [\u03b4\u22c6\n1; ...; \u03b4\u22c6\nN]\np\u2032\nt = [I; [S(x1 + \u03b4\u22c6\n1, y1); \u00b7 \u00b7 \u00b7 ; S(xN + \u03b4\u22c6\nN, yN)]; S(xQ, yT )]\n/* Update prompt */\nuntil T times;\nOutput :Optimized prompt suffixes [\u03b4\u22c6\n1, \u00b7 \u00b7 \u00b7 , \u03b4\u22c6\nN]\n\nFigure 9: Template designs for all the datasets used in our experiments. We also provide examples for these datasets to ensure a better understanding.\n\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f09/1f09a921-a634-4329-974d-44e5550325c3.png\" style=\"width: 50%;\"></div>\n\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. This work introduces a novel transferable attack against ICL to address these issues, aiming to hijack LLMs to generate the target response or jailbreak.",
        "problem": {
            "definition": "The problem addressed in this paper is the vulnerability of ICL to adversarial attacks, specifically the instability and susceptibility of LLMs when using in-context demonstrations.",
            "key obstacle": "The main challenge is that existing adversarial attacks are not tailored for ICL, making it difficult to effectively evaluate and improve the robustness of LLMs against such attacks."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that adversarial examples can significantly undermine the performance of ICL, revealing substantial vulnerabilities.",
            "opinion": "The proposed idea involves a novel hijacking attack that appends imperceptible adversarial suffixes to in-context demos to manipulate LLM outputs without altering user queries.",
            "innovation": "The key innovation lies in the gradient-based prompt search method that learns to append these adversarial suffixes, making the attack stealthy and effective across various tasks."
        },
        "method": {
            "method name": "Greedy Gradient-guided Injection (GGI)",
            "method abbreviation": "GGI",
            "method definition": "GGI is a gradient-based algorithm that learns adversarial suffixes to hijack LLMs during ICL by modifying the in-context demos.",
            "method description": "GGI efficiently appends adversarial suffixes to the in-context demos to induce LLMs to generate targeted outputs.",
            "method steps": [
                "Initialize the prompt with clean demos and the user query.",
                "Compute the top-k substitutions for adversarial suffixes based on gradients.",
                "Select the best replacement for each demo.",
                "Update the prompt with the optimized suffixes."
            ],
            "principle": "The effectiveness of this method is based on the ability to divert the attention of LLMs from relevant context to the adversarial suffixes, leading to misaligned outputs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using various LLMs on datasets such as SST-2, Rotten Tomatoes, AG\u2019s News, and AdvBench, evaluating the performance of the GGI attack against baseline methods.",
            "evaluation method": "Performance was measured through accuracy metrics on clean vs. adversarial demos, as well as attack success rates (ASR) across different tasks."
        },
        "conclusion": "The experimental results demonstrate that the GGI attack effectively hijacks LLMs to generate unwanted outputs, highlighting significant vulnerabilities in ICL and the need for improved defense strategies.",
        "discussion": {
            "advantage": "The proposed GGI attack is stealthy, imperceptible, and exhibits strong transferability across different demo sets and tasks.",
            "limitation": "While effective, the method may encounter challenges with larger LLMs, which are generally more robust against such attacks.",
            "future work": "Future research should focus on enhancing defense mechanisms against hijacking attacks and further exploring the vulnerabilities of LLMs in ICL."
        },
        "other info": {
            "additional details": {
                "contribution1": "Introduction of a novel stealthy adversarial attack targeting in-context demos.",
                "contribution2": "Development of a gradient-based prompt search algorithm for learning adversarial suffixes.",
                "contribution3": "Demonstration of the transferability and effectiveness of the hijacking attack across various datasets."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) leverages large language models (LLMs) for specific downstream tasks by utilizing labeled examples as demonstrations in the precondition prompts."
        },
        {
            "section number": "1.2",
            "key information": "Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples, making it crucial to address these issues for effective natural language processing."
        },
        {
            "section number": "3.1",
            "key information": "The paper discusses the vulnerability of ICL to adversarial attacks, highlighting the instability and susceptibility of LLMs when using in-context demonstrations."
        },
        {
            "section number": "3.3",
            "key information": "The proposed Greedy Gradient-guided Injection (GGI) method efficiently appends adversarial suffixes to in-context demos to induce LLMs to generate targeted outputs."
        },
        {
            "section number": "6.1",
            "key information": "The paper identifies challenges related to model bias and context sensitivity, particularly in how adversarial examples can undermine the performance of ICL."
        },
        {
            "section number": "6.2",
            "key information": "The GGI method may encounter challenges with larger LLMs, which are generally more robust against adversarial attacks, raising concerns about computational efficiency."
        },
        {
            "section number": "7",
            "key information": "The experimental results demonstrate significant vulnerabilities in ICL, emphasizing the need for improved defense strategies against hijacking attacks."
        }
    ],
    "similarity_score": 0.6999422266599858,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Hijacking Large Language Models via Adversarial In-Context Learning.json"
}