{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.20646",
    "title": "Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item",
    "abstract": "Sequential recommender systems (SRS) aim to predict users\u2019 subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load from LLMs. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results show that our method surpasses existing baselines consistently, and benefits long-tail users and items especially. The implementation code is available at https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.",
    "bib_name": "liu2024largelanguagemodelsenhanced",
    "md_text": "# LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation\n# Qidong Liu1,2, Xian Wu3 \u2217, Yejing Wang2, Zijian Zhang2, 4, Feng Tian5 \u2217, Yefeng Zheng3, 6, Xiangyu Zhao2 \u2217\n3 Jarvis Research Center, Tencent YouTu Lab, 4 Jilin University  School of Comp. Science & Technology, MOEKLINNS Lab, Xi\u2019an Jiaotong Univers 6 Medical Artificial Intelligence Lab, Westlake University liuqidong@stu.xjtu.edu.cn, {kevinxwu, yefengzheng}@tencent.com, yejing.wang@my.cityu.edu.hk, zhangzijian@jlu.edu.cn, fengtian@mail.xjtu.edu.cn, xianzhao@cityu.edu.hk\n# Abstract\nSequential recommender systems (SRS) aim to predict users\u2019 subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load from LLMs. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results show that our method surpasses existing baselines consistently, and benefits long-tail users and items especially. The implementation code is available at https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.\n# 1 Introduction\nThe objective of sequential recommendation is to predict the next likely item for users based o their historical records [7, 53]. Owing to its wide-ranging applicability in various domains such as commerce [47] and social media [5], sequential recommendation has garnered considerable attentio in recent years. Given that the essence of sequential recommendation revolves around extracting us preferences from their interaction records, several innovative architectures have been proposed. F\n\u2217Corresponding authors: Xian Wu, Feng Tian and Xiangyu Zhao\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/070c/070cedb6-fdd5-40fa-8083-be5102d27a60.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">a) Long-tail User Challenge (b) Long-tail Item Challeng Figure 1: The preliminary experiments of SASRec on Beauty dataset.</div>\nwhile FMLPRec [24] introduces a pure MLP architecture to identify dynamics in users\u2019 preference. Despite significant advancements in sequential recommendation, the long-tail challenges continue to undermine its practical utility. Generally, these challenges can be categorized into two types, affecting either the user or the item side. To illustrate, we present the performance of a well-known SRS model, SASRec [18], on the Amazon Beauty dataset, along with its statistics in Figure 1. i) Long-tail User Challenge: In Figure 1 (a), we note that above 80% users have interacted with fewer than 10 items (i.e., long-tail users), and SASRec\u2019s performance is subpar for these users compared to those with more interaction records. This suggests that the majority of users receive less than optimal recommendation services. ii) Long-tail Item Challenge: Figure 1 (b) demonstrates that SASRec performs significantly better on more popular items. However, the histogram indicates that around 71.4% items own no more than 30 interaction records, meaning they are less frequently consumed. Addressing these long-tail challenges is crucial for elevating user experience and seller benefits. To tackle the long-tail item challenge, existing studies [17, 20] examine the co-occurrence pattern between popular and long-tail items, aiming to enrich the representation of long-tail items with that of popular ones. Nevertheless, ignorance of the true relationship between items may cause a seesaw problem [35]. As for the long-tail user challenge, existing research [36, 34] explores the interaction history of all users, attempting to augment pseudo items for tail users. However, these approaches still only rely on collaborative information, which inclines to generate noisy items due to inaccurate similarity between users [34]. At this time, superb semantic relations between users or items can make an effect, which indicates the potential of utilizing semantics to face long-tail challenges. Recent advancements in large language models (LLMs) offer promise for alleviating long-tail challenges from a semantic perspective. However, LLMs are initially designed for natural language processing tasks but not for recommendation ones. Some works [62, 42] have made efforts to adapt, but two problems still exist. i) Inefficient Integration: Recent research has explored deriving informative prompts to activate ChatGPT [54, 10] or modifying the tokenization method of LLaMA [25, 27, 58] for sequential recommendation. Despite their impressive performance, these approaches are challenging to apply in industrial settings. This is because recommender systems typically require low latency for online deployment, whereas LLMs often entail high inference costs [11]. ii) Deficiency of Semantic Information: Several recent works [13, 16] propose utilizing embeddings derived from LLMs to initialize the item embedding layer of sequential recommendation models, thereby integrating semantic information. However, the fine-tuning process, if not done without freezing the embedding layer, may erode the original semantic relationships between items. Additionally, these approaches focus solely on the item side, neglecting the potential benefits of incorporating semantic information on the user side which could aid the sequence encoder of an SRS. In this paper, to better integrate LLMs into SRS for addressing long-tail challenges, we design a Large Langauge Models Enhancement framework for Sequential Recommendation (LLM-ESR). Firstly, we derive the semantic embeddings of items and users by encoding prompt texts from LLMs. Since these embeddings can be cached in advance, our integration does not impose any extra inference burden from LLMs. To tackle the long-tail item challenge, we devise a dual-view modeling framework that combines semantic and collaborative information. Specifically, the embeddings derived from LLMs are frozen to avoid deficiency of semantics. Next, we propose a retrieval augmented selfdistillation method to enhance the sequence encoder of an SRS model using similar users. The similarity between users is measured by the user representations from LLMs. Finally, it is important\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3cfb/3cfb9a26-06ba-4399-97d2-5b85c126b08c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The overview of the proposed LLM-ESR framework.</div>\nto note that the proposed framework is model-agnostic, allowing it to be adapted to any sequential recommendation model. The contributions of this paper are as follows: \u2022 We propose a large language models enhancement framework, which can alleviate both long-tail user and item challenges for SRS by introducing semantic information from LLMs. \u2022 To avoid the inference burden of LLMs, we design an embedding-based enhancement method. Besides, the derived embeddings are utilized directly to retain the original semantic relations. \u2022 We conduct extensive experiments on three real-world datasets with three backbone SRS models to validate the effectiveness and flexibility of LLM-ESR.\n# 2 Problem Definition\nThe goal of the sequential recommendation is to give out the next item that users are possible to interact with based on their interaction records. The set of users and items are denoted as U = {u1, . . . , ui, . . . , u|U|} and V = {v1, . . . , vi, . . . , v|V|}, respectively, where |U| and |V| are the number of users and items. Each user has an interaction sequence, which arranges the interacted items by timeline, denoted as Su = {v(u) 1 , . . . , v(u) i , . . . , v(u) nu }. nu represents the interaction number of user u. For simplicity, we omit the superscript (u) in the following sections. Then, the problem of sequential recommendation can be defined as follows:\nFollowing the existing works related to long-tailed SRS [17, 20], we can split the users and items into tail and head groups. Let nu and pv denote the length of the user\u2019s interaction sequence and the popularity of the item v (i.e., the total interaction number). Firstly, we sort the users and items by the values of nu and pv in descending order. Then, take out the top 20% users and items as head user and head item according to Pareto principle [4], denoted as Uhead and Vhead. The rest of the users and items are the tail user and tail item, i.e., Utail = U \\ Uhead and Vtail = V \\ Vhead. To alleviate the long-tail challenges, we aim to elevate the recommending performance for Utail and Vtail.\n(1)\n# 3 LLM-ESR\n# 3.1 Overview\nThe overview of the proposed LLM-ESR is shown in Figure 2. To acquire the semantic information, we adopt LLMs to encode textual users\u2019 historical interactions and items\u2019 attributes into LLMs user embedding and LLMs item embedding. Then, two modules are proposed to augment long-tail items and long-tail users, respectively, i.e., Dual-view Modeling and Retrieval Augmented SelfDistillation. i) Dual-view Modeling: This module consists of two branches. One is semantic-view modeling, which aims to extract the semantic information from the user\u2019s interaction sequence. It first utilizes the semantic embedding layer, derived from LLMs item embedding, to encode the items. Then, an adapter is designed for dimension adaptation and space transformation. The output item embedding sequence will be fed into cross-attention for fusion and then sequence encoder to get the user representation in semantic view. The other branch is collaborative-view modeling, which transforms the interaction sequence into an embedding one by a collaborative embedding layer. Next, followed by a cross-attention and the sequence encoder, the collaborative user preference is obtained. At the end of this module, the user representations in the two views will be fused for the final recommendations. ii) Retrieval Augmented Self-Distillation: This module expects to enhance long-tail users through informative interactions of similar users. First, the derived LLMs user embedding is considered as a semantic user base for retrieving similar users. Then, similar users are fed into dual-view modeling to get their user representations, which are the guide signal for self-distillation. Finally, the derived distillation loss will be utilized as an auxiliary loss for training.\n# 3.2 Dual-view Modeling\nThe traditional SRS models are skilled in capturing collaborative signals, which can recommend for popular items well [20, 17]. However, they compromise on long-tail items due to the lack of semantics [2]. Therefore, we model the preferences of users from the dual views to cover all items simultaneously. Besides, we propose a two-level fusion to better combine the benefits from both two. Semantic-view Modeling. In general, the attributes and descriptions of items contain abundant semantics. To utilize the powerful semantic understanding abilities of LLMs, we organize the attributes and descriptions into textual prompts (the template of prompts can be found in Appendix A.1). Then, in avoid of possible inference burden brought by LLMs, we cache the embeddings derived from LLMs for usage. In specific, the embeddings can be obtained by taking out the last hidden state of open-sourced LLMs, such as LLaMA [50], or the public API, such as text-embedding-ada-0022. We adopt the latter one in this paper. Let Ese \u2208R|V|\u00d7dllm denotes the LLMs embedding of all items, where dllm is dimension of LLMs embedding. Then, the semantic embedding layer Ese from LLMs can be used for semantic-view modeling to enhance long-tail items. However, previous works [13, 16] often adapt it as the initialization of the item embedding layer, which may ruin the original semantic relations during fine-tuning. In order to retain the semantics, we freeze the Ese and propose an adapter to transform the raw semantic space into the recommending space. For each item i, we can get its LLMs embedding ellm i by taking the i-th row of Ese. Then, it will be fed into the tunable adapter to get the semantic embedding:\nwhere Wa 1 \u2208R dllm 2 \u00d7dllm, Wa 2 \u2208Rd\u00d7 dllm 2 and ba 1 \u2208R dllm 2 \u00d71, ba 2 \u2208Rd\u00d71 are the weight matrices and bias of adapter. Following this process, we can obtain the item embedding sequence of the user\u2019s interaction records, denoted as Sse = [ese 1 , . . . , ese nu]. Similar to a general SRS model, we employ a sequence encoder f\u03b8 (e.g., self-attention layers [51] for SASRec [18]) to get the representation of user preference in semantic view as follows:\nwhere use \u2208Rd\u00d71 is the user preference representation in semantic view and \u03b8 denotes the parameters of sequence encoder in an SRS model.\nwhere use \u2208Rd\u00d71 is the user preference representation in semantic view and \u03b8 denotes the parameters of sequence encoder in an SRS model. Collaborative-view Modeling. To utilize the collaborative information, we adopt a trainable item embedding layer and supervised update it by interaction data. Let Eco \u2208R|V|\u00d7d denotes the 2https://platform.openai.com/docs/guides/embeddings\n2https://platform.openai.com/docs/guides/embeddings\n(2)\n(3)\ncollaborative embedding layer of the item. Then, the item embedding sequence Sco = [eco 1 , . . . , eco nu] is acquired by extracting the corresponding rows from Eco. To get the user preference uco in the collaborative view, we input embedding sequence to sequence encoder, i.e., uco = f\u03b8(Sco). It is worth noting that, the sequence encoder f\u03b8 is the same one in both semantic and collaborative views for the shared sequential pattern and higher efficiency [45]. Besides, the embedding layers in the two views are in unbalanced training stages (one is pretrained, while the other is from scratch), which may lead to optimization difficulty [1]. To handle such a problem, we initialize the Eco by dimension-reduced Ese. The Principal Component Analysis (PCA) [43] is used as the dimension reduction method in this paper. Two-level Fusion. The effective integration of both semantic-view and collaborative-view is essential to absorb the benefits of these two. However, the direct merge of the user representations in dual views may overlook the nuanced inter-relationships between item sequences. Thus, we design a two-level fusion method for the dual-view modeling module, i.e., sequence-level and logit-level. The former aims to implicitly capture the mutual relationships between the item sequences of dual views, while the latter explicitly targets the combination of recommending abilities. In specific, we propose a cross-attention mechanism for sequence-level fusion. To simplify the description, we only take the semantic view interacting with the collaborative view for illustration, and the other view is the same. Specifically, Sse is considered as the query, and Sco as the key and value in attention mechanism. Let Q = SseWQ, K = ScoWK, V = ScoWV , where WQ, WK, WV \u2208Rd\u00d7d are weight matrices. Then, the interacted collaborative embedding sequence can be formulated as follows:\nFollowing the same process of cross-attention, we can also get the corresponding semantic embedding sequence \u02c6Sse. Finally, Sse, Sco are substituted by \u02c6Sse, \u02c6Sse to be fed into f\u03b8(\u00b7). As for logit-level fusion, we concatenate the two-view user and item embeddings for recommendation. The probability score of recommending item j for the user u is therefore calculated as:\nwhere \u201c:\u201d denotes the concatenation operation of two vectors. Based on the probability score, we adopt the pairwise ranking loss to train the framework:\nwhere v+ k+1 and v\u2212 k+1 are the ground-truth item and paired negative item. It is worth noting that the ranking loss may differ a little according to different backbone SRS models, e.g., sequence-to-one pairwise loss for GRU4Rec [14].\n# 3.3 Retrieval Augmented Self-Distillation\nThe long-tail user problem originates from the lack of enough interactions for the sequence encoder in an SRS to capture users\u2019 preferences. Thus, we propose a self-distillation method to augment the extraction capacity of the sequence encoder. Self-distillation [12, 60] is a type of knowledge distillation that considers one model as both the student and teacher for model enhancement. As for the SRS, since multiple similar users have more informative interactions, it is promising to transfer their knowledge to the target user for strengthening. Thereafter, there are two key challenges for such knowledge transfer, i.e., how to retrieve similar users and how to transfer the knowledge. Retrieve Similar Users. Previous works have confirmed that LLMs can understand the semantic meanings of textual user interaction records for recommendation [25, 10]. Based on their observation, we organize the item\u2019s title that interacted by users into the textual prompts (the template of prompts can be found in Appendix A.1). Then, similar to the derivation of LLMs item embedding Ese, we can obtain and save the LLMs user embedding, denoted as Ullm \u2208R|U|\u00d7dllm. It is also dubbed as the semantic user base in this paper, because the semantic relations are encoded in it. For each target user k, we can retrieve the similar user set Uk as follows:\nUk = Top({cos(ullm k , ullm j )}|U| j=1, N)\n(5)\n(6)\n(7)\nwhere cos(\u00b7, \u00b7) is the cosine similarity function to measure the distance between two vectors. N represents the size of similar user sets, which is a hyper-parameter. Self-Distillation. As mentioned before, we design the self-distillation to transfer the knowledge from several similar users to the target user. Since the representation of user preference, i.e., use and uco, encode the comprehensive knowledge of the user, we configure such representation as the mediator for the distillation. To get the teacher mediator, we first utilize the dual-view modeling framework (Section 3.2) to get the user representation for each similar user, denoted as {use j , uco j }|Uk| j=1. Then, the teacher mediator is calculated by mean pooling, as the following formula:\nwhere cos(\u00b7, \u00b7) is the cosine similarity function to measure the distance between two vectors. N represents the size of similar user sets, which is a hyper-parameter.\nThe student mediator is the representation of target user k, i.e., [use k : uco k ]. Based on the teacher and student mediators, the self-distillation loss can be formulated as:\n\ufffd Note that the gradients of use Tk and uco Tk are stopped, because they only provide the guidance signal instead of optimizing the model.\n# 3.4 Train and Inference\nTrain. Based on the illustration in Section 3.2 and Section 3.3, we only update the collaborative embedding layer, adapter, cross-attention and sequence encoder during the training, while freezing the semantic embedding layer and semantic user base. Since the original LLMs embeddings Ese and Ullm are frozen, the original semantic relations get preserved well. The training loss for optimization is the combination of pairwise ranking loss and self-distillation loss, which can be written as follows\nInference. During the inference process of the LLM-ESR, the retrieval augmented self-distillation module is exempted due to no need for the auxiliary loss. Thus, we follow the dual-view modeling process for the final recommendation by Equation (5). Besides, since the semantic embedding layer can be cached in advance, the call for LLMs is avoided, which prevents the extra inference costs. Due to the limited space, the algorithm lies in Appendix A.2 for more clarity.\n# 4 Experiment\n# 4.1 Experimental Settings\nDataset. There are three real-world datasets applied for evaluation, i.e., Yelp, Amazon Fashion and Amazon Beauty. We follow the previous SRS works [18, 49] for preprocessing and data split. More details about the datasets and preprocessing can be seen in Appendix B.1. Baselines. To validate the flexibility, we combine the competing baselines and LLM-ESR with three well-known backbone SRS models: GRU4Rec [14], Bert4Rec [48] and SASRec [18]. Then, two groups of baselines are compared in the experiments. One group is the traditional enhancement framework for the long-tailed sequential recommendation, including CITIES [17] and MELT [20]. The other group is the LLM-based enhancement framework, which contains RLMRec [44] and LLMInit [13, 16]. The more details about baselines are put into Appendix B.2. Implementation Details. The hardware used in all experiments is an Intel Xeon Gold 6133 platform with Tesla V100 32G GPUs, while the basic software requirements are Python 3.9.5 and PyTorch 1.12.0. The hyper-parameters N and \u03b1 are searched from {2, 6, 10, 14, 18} and {1, 0.5, 0.1, 0.05, 0.01}. More details about the implementation details are in Appendix B.3. The implementation code is available at https://github.com/Applied-Machine-Learning-Lab/LLM-ESR. Evaluation Metrics. In the experiments, we adopt the metrics of Top-10 list for evaluation. Specifically, the Hit Rate (H@10) and Normalized Discounted Cumulative Gain (N@10) are used. Following [18], we randomly sample 100 items that the user has not interacted with as the negatives paired\n(8)\n(10)\n<div style=\"text-align: center;\">Table 1: The overall results of competing baselines and our LLM-ESR. The boldface refers to the highest score and the underline indicates the next best result of the models. \u201c*\u201d indicates the statistically significant improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.</div>\nstatistically significant improvements (i.e., two-sided t-test with p < 0.05) over the best baseline.\nDataset\nModel\nOverall\nTail Item\nHead Item\nTail User\nHead User\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nYelp\nGRU4Rec\n0.4879\n0.2751\n0.0171\n0.0059\n0.6265\n0.3544\n0.4919\n0.2777\n0.4726\n0.2653\n- CITIES\n0.4898\n0.2749\n0.0134\n0.0051\n0.6301\n0.3543\n0.4936\n0.2783\n0.4756\n0.2618\n- MELT\n0.4985\n0.2825\n0.0201\n0.0079\n0.6393\n0.3633\n0.5046\n0.2865\n0.4750\n0.2671\n- RLMRec\n0.4886\n0.2777\n0.0188\n0.0067\n0.6269\n0.3574\n0.4920\n0.2804\n0.4756\n0.2671\n- LLMInit\n0.4872\n0.2749\n0.0201\n0.0072\n0.6246\n0.3537\n0.4908\n0.2775\n0.4732\n0.2647\n- LLM-ESR\n0.5724*\n0.3413*\n0.0763*\n0.0318*\n0.7184*\n0.4324*\n0.5782*\n0.3456*\n0.5501*\n0.3247*\nBert4Rec\n0.5307\n0.3035\n0.0115\n0.0044\n0.6836\n0.3916\n0.5325\n0.3047\n0.5241\n0.2988\n- CITIES\n0.5249\n0.3015\n0.0041\n0.0014\n0.6783\n0.3899\n0.5274\n0.3032\n0.5155\n0.2954\n- MELT\n0.6206\n0.3770\n0.0429\n0.0149\n0.7907\n0.4836\n0.6210\n0.3780\n0.6191\n0.3733\n- RLMRec\n0.5306\n0.3039\n0.0104\n0.0040\n0.6938\n0.3922\n0.5351\n0.3065\n0.5137\n0.2936\n- LLMInit\n0.6199\n0.3781\n0.0874\n0.0330\n0.7766\n0.4797\n0.6204\n0.3796\n0.6178\n0.3723\n- LLM-ESR\n0.6623*\n0.4222*\n0.1227*\n0.0500*\n0.8212*\n0.5318*\n0.6637*\n0.4247*\n0.6571*\n0.4127*\nSASRec\n0.5940\n0.3597\n0.1142\n0.0495\n0.7353\n0.4511\n0.5893\n0.3578\n0.6122\n0.3672\n- CITIES\n0.5828\n0.3540\n0.1532\n0.0700\n0.7093\n0.4376\n0.5785\n0.3511\n0.5994\n0.3649\n- MELT\n0.6257\n0.3791\n0.1015\n0.0371\n0.7801\n0.4799\n0.6246\n0.3804\n0.6299\n0.3744\n- RLMRec\n0.5990\n0.3623\n0.0953\n0.0412\n0.7474\n0.4568\n0.5966\n0.3613\n0.6084\n0.3658\n- LLMInit\n0.6415\n0.3997\n0.1760\n0.0789\n0.7785\n0.4941\n0.6403\n0.4010\n0.6462\n0.3948\n- LLM-ESR\n0.6673*\n0.4208*\n0.1893*\n0.0845*\n0.8080*\n0.5199*\n0.6685*\n0.4229*\n0.6627*\n0.4128*\nFashion\nGRU4Rec\n0.4798\n0.3809\n0.0257\n0.0101\n0.6606\n0.5285\n0.3781\n0.2577\n0.6118\n0.5408\n- CITIES\n0.4762\n0.3743\n0.0252\n0.0103\n0.6557\n0.5191\n0.3729\n0.2501\n0.6103\n0.5354\n- MELT\n0.4884\n0.3975\n0.0291\n0.0112\n0.6712\n0.5513\n0.3890\n0.2770\n0.6173\n0.5538\n- RLMRec\n0.4795\n0.3808\n0.0253\n0.0105\n0.6603\n0.5282\n0.3773\n0.2577\n0.6120\n0.5405\n- LLMInit\n0.4864\n0.4095\n0.0250\n0.0104\n0.6702\n0.5684\n0.3852\n0.2973\n0.6177\n0.5550\n- LLM-ESR\n0.5409*\n0.4567*\n0.0807*\n0.0384*\n0.7242*\n0.6233*\n0.4560*\n0.3568*\n0.6512*\n0.5864*\nBert4Rec\n0.4668\n0.3613\n0.0142\n0.0067\n0.6470\n0.5024\n0.3500\n0.2344\n0.6183\n0.5258\n- CITIES\n0.4926\n0.4090\n0.0223\n0.0099\n0.6799\n0.5679\n0.3952\n0.2975\n0.6190\n0.5535\n- MELT\n0.4897\n0.3810\n0.0059\n0.0019\n0.6823\n0.5319\n0.3842\n0.2514\n0.6266\n0.5491\n- RLMRec\n0.4744\n0.3567\n0.0044\n0.0015\n0.6615\n0.4981\n0.3626\n0.2268\n0.6194\n0.5251\n- LLMInit\n0.4854\n0.4035\n0.0328\n0.0161\n0.6655\n0.5577\n0.3773\n0.2846\n0.6255\n0.5578\n- LLM-ESR\n0.5487*\n0.4529*\n0.0525*\n0.0225*\n0.7462*\n0.6243*\n0.4629*\n0.3460*\n0.6599*\n0.5916*\nSASRec\n0.4956\n0.4429\n0.0454\n0.0235\n0.6748\n0.6099\n0.3967\n0.3390\n0.6239\n0.5777\n- CITIES\n0.4923\n0.4423\n0.0407\n0.0214\n0.6721\n0.6098\n0.3936\n0.3392\n0.6203\n0.5760\n- MELT\n0.4875\n0.4150\n0.0368\n0.0144\n0.6670\n0.5745\n0.3792\n0.2933\n0.6280\n0.5729\n- RLMRec\n0.4982\n0.4457\n0.0410\n0.0223\n0.6803\n0.6143\n0.3990\n0.3415\n0.6270\n0.5808\n- LLMInit\n0.5119\n0.4492\n0.0596\n0.0305\n0.6920\n0.6159\n0.4184\n0.3501\n0.6332\n0.5777\n- LLM-ESR\n0.5619*\n0.4743*\n0.1095*\n0.0520*\n0.7420*\n0.6424*\n0.4811*\n0.3769*\n0.6668*\n0.6005*\nBeauty\nGRU4Rec\n0.3683\n0.2276\n0.0796\n0.0567\n0.4371\n0.2683\n0.3584\n0.2191\n0.4135\n0.2663\n- CITIES\n0.2456\n0.1400\n0.1122\n0.0760\n0.2774\n0.1552\n0.2382\n0.1346\n0.2795\n0.1645\n- MELT\n0.3702\n0.2161\n0.0009\n0.0003\n0.4582\n0.2675\n0.3637\n0.2116\n0.3997\n0.2365\n- RLMRec\n0.3668\n0.2278\n0.0780\n0.0560\n0.4357\n0.2688\n0.3576\n0.2202\n0.4089\n0.2626\n- LLMInit\n0.4151\n0.2713\n0.0896\n0.0637\n0.4928\n0.3208\n0.4059\n0.2621\n0.4571\n0.3133\n- LLM-ESR\n0.4917*\n0.3140*\n0.1547*\n0.0801*\n0.5721*\n0.3698*\n0.4851*\n0.3079*\n0.5220*\n0.3420*\nBert4Rec\n0.3984\n0.2367\n0.0101\n0.0038\n0.4910\n0.2922\n0.3851\n0.2272\n0.4593\n0.2801\n- CITIES\n0.3961\n0.2339\n0.0023\n0.0008\n0.4900\n0.2895\n0.3832\n0.2250\n0.4551\n0.2746\n- MELT\n0.4716\n0.2965\n0.0709\n0.0291\n0.5671\n0.3603\n0.4596\n0.2865\n0.5263\n0.3423\n- RLMRec\n0.3977\n0.2365\n0.0090\n0.0032\n0.4903\n0.2921\n0.3853\n0.2277\n0.4539\n0.2765\n- LLMInit\n0.5029\n0.3209\n0.0927\n0.0451\n0.6007\n0.3867\n0.4919\n0.3117\n0.5530\n0.3632\n- LLM-ESR\n0.5393*\n0.3590*\n0.1379*\n0.0745*\n0.6350*\n0.4269*\n0.5295*\n0.3507*\n0.5839*\n0.3972*\nSASRec\n0.4388\n0.3030\n0.0870\n0.0649\n0.5227\n0.3598\n0.4270\n0.2941\n0.4926\n0.3438\n- CITIES\n0.2256\n0.1413\n0.1363\n0.0897\n0.2468\n0.1536\n0.2215\n0.1406\n0.2441\n0.1444\n- MELT\n0.4334\n0.2775\n0.0460\n0.0172\n0.5258\n0.3995\n0.4233\n0.2673\n0.4796\n0.3241\n- RLMRec\n0.4460\n0.3075\n0.0924\n0.0658\n0.5303\n0.3652\n0.4365\n0.3016\n0.4892\n0.3345\n- LLMInit\n0.5455\n0.3656\n0.1714\n0.0965\n0.6347\n0.4298\n0.5359\n0.3592\n0.5893\n0.3948\n- LLM-ESR\n0.5672*\n0.3713*\n0.2257*\n0.1108*\n0.6486*\n0.4334*\n0.5581*\n0.3643*\n0.6087*\n0.4032*\nwith the ground truth for calculation of the metrics. To guarantee the robustness of the experimental results, we report the average results of the triplicate test with random seeds {42, 43, 44}.\n# 4.2 Overall Performance\nTo validate the effectiveness and flexibility of the proposed LLM-ESR, we show the overall, tail and head performance on three datasets in Table 1. At a glance, we find that the proposed LLM-ESR can outperform all competing baselines with all SRS models across all user or item groups, which verifies the usefulness of our framework. Then, we probe more conclusions by the following analysis.\n<div style=\"text-align: center;\">Table 2: The ablation study on the Yelp dataset with SASRec as the backbone SRS model. The boldface refers to the highest score and the underline indicates the next best result of the models.</div>\nboldface refers to the highest score and the underline indicates the next best result of the models.\nModel\nOverall\nTail Item\nHead Item\nTail User\nHead User\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\n- LLM-ESR\n0.6673\n0.4208\n0.1893\n0.0845\n0.8080\n0.5199\n0.6685\n0.4229\n0.6627\n0.4128\n- w/o Co-view\n0.6320\n0.3816\n0.1898\n0.0856\n0.7621\n0.4687\n0.6318\n0.3823\n0.6325\n0.3787\n- w/o Se-view\n0.6468\n0.4038\n0.1105\n0.0460\n0.8047\n0.5091\n0.6459\n0.4043\n0.6501\n0.4018\n- w/o SD\n0.6572\n0.4121\n0.2003\n0.0898\n0.7911\n0.5071\n0.6566\n0.4130\n0.6574\n0.4091\n- w/o Share\n0.6595\n0.4158\n0.1728\n0.0783\n0.8027\n0.5152\n0.6606\n0.4186\n0.6552\n0.4055\n- w/o CA\n0.6644\n0.4160\n0.1850\n0.0803\n0.8004\n0.5119\n0.6652\n0.4175\n0.6616\n0.4105\n1-layer Adapter\n0.6108\n0.3713\n0.1107\n0.0469\n0.7580\n0.4668\n0.6065\n0.3702\n0.6269\n0.3754\nRandom Init\n0.6440\n0.3984\n0.1899\n0.0839\n0.7777\n0.4910\n0.6454\n0.4018\n0.6388\n0.3853\nOverall Comparison. From the results, we observe that the proposed LLM-ESR leads the overall performance under both two metrics, which indicates better-enhancing effects. LLMInit is often the secondary. This phenomenon shows that the injection of semantics from LLMs actually augments the SRS. However, RLMRec often underperforms compared with other LLM-based methods, because it is devised for collaborative filtering algorithms, incompatible with SRS. As for the traditional baselines, MELT stays ahead in most cases. The reason lies in that it addresses the long-tail user and long-tail item challenges simultaneously. By comparison, CITIES is even sometimes inferior to the backbone SRS model due to the seesaw problem, i.e., drastic drops for popular items. Long-tail Item and User Comparison. According to the split method illustrated in Section 2, the items are grouped into Tail Item and Head Item. From Table 1, we observe that our LLM-ESR not only achieves the best on the tail item group but also gets the first place on the head item group. Such performance comparison highlights the combination of semantics and collaborative signals by our dual-view modeling. LLMInit leads the tail group across all baselines, which suggests that semantic information can benefit long-tail items. It is worth noting that CITIES sometimes perform better for the tail group but harm those popular items, which means it has a seesaw problem. Additionally, the results illustrate that MELT, LLMInit and LLM-ESR can augment the tail user group markedly. MELT is devised to enhance tail user, but underperforms our method because of its limitations to collaborative perspective. Though LLMInit can also benefit tail users by introducing semantics, it ignores the utilization of LLMs from the user side. Flexibility. Table 1 shows that the proposed framework can get the largest performance improvements on all three backbone SRS models, which indicates the flexibility of LLM-ESR. By comparison, the other baselines incline to depend on the type of SRS. The traditional method, i.e., CITIES and MELT, tend to perform better for GRU4Rec, while LLMInit is more beneficial to Bert4Rec and SASRec.\n# 4.3 Ablation Study\nThe results of the ablation study are shown in Table 2. Firstly, we remove the collaborative view or semantic view to investigate the dual-view modeling, denoted as w/o Co-view and w/o Se-view. The results show that w/o Co-view downgrades performance dramatically on the head group, while w/o Se-view harms tail items evidently. Such changes indicate the distinct specialty of collaborative and semantic information, highlighting the combination of both. w/o SD means dropping self-distillation, which shows performance drops for long-tail users. It suggests the effects of the proposed retrieval augmented self-distillation. The results of these three variants validate the motivation for designing each component for LLM-ESR. w/o Share and w/o CA represent using split sequence encoder and removing cross-attention. The decrease in performance of these two illustrates the effectiveness of the sharing design and sequence-level fusion. More results can be seen in Appendix C.1. Furthermore, we have two designs to ease the optimization of the entire LLM-ESR framework. One is that we use dimension-reduced LLM item embeddings to initialize the collaborative embedding layer instead of random initialization. On the other hand, we propose a two-layer adapter to fill the large dimension gap between LLM embeddings and item embeddings. To illustrate the effectiveness of these two designs, we compare 1-layer Adapter and Random Init variants of LLM-ESR. The results, shown in Table 2, indicate that both variants underperform the original LLM-ESR, verifying the success of our special designs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a898/a8988f67-f883-4e9b-892d-cf49e764eb8e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The hyper-parameter experiments on the weight of self-distillation loss \u03b1 and the number of retrieved similar users N. The results are based on the Yelp dataset with the SASRec model.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c228/c228c4e0-6bea-4880-a90b-12f385a1f249.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) User Group</div>\n<div style=\"text-align: center;\">Figure 4: The results of the proposed LLM-ESR and competing baselines in meticulous user and item groups. The results are based on the Beauty dataset with the SASRec model.</div>\n# 4.4 Hyper-parameter Analysis\nTo investigate the effects of the hyper-parameters in LLM-ESR, we show the performance trend along with their changes in Figure 3. The hyper-parameter \u03b1 controls to what extent the designed selfdistillation affects the optimization. With \u03b1 ranging from 1 to 0.01, the recommending accuracy rises first and drops then. The reason for the compromised performance of large \u03b1 lies in that overemphasis on self-distillation will affect the convergence of ranking loss. Smaller \u03b1 also downgrades the performance, which indicates the usefulness of the designed self-distillation. As for the number of retrieved users N, the best is 10. The reason is that more users can provide more informative interactions. However, too large N may decrease the relatedness of the retrieved users.\n# 4.5 Group Analysis\nFor more meticulous analysis, we split the users and items into 5 groups according to sequence length nu and popularity pv, and show the performance of each group in Figure 4. From the results, we observe that LLM-based frameworks derive increases in every user and item group, while MELT has a positive effect on some specific groups. It reflects the seesaw problem of MLET and reveals the benefit of making use of semantic embeddings from LLMs. Comparing LLMInit with LLM-ESR, LLM-ESR can get more increments on the long-tail groups (e.g., 1-4 user group and 1-9 item group), which proves the better reservation of semantic information from LLMs by our framework. The group analysis of Bert4Rec and GRU4Rec as backbones are shown in Appendix C.3.\n# 5 Related Works\n# 5.1 Sequential Recommendation\nThe core of sequential recommendation refers to capturing the sequence pattern for the next likely item [29, 38, 31, 37, 59, 24, 23, 26, 39]. Thus, at the early stage, researchers focus on fabricating the\n<div style=\"text-align: center;\">(b) Item Group</div>\narchitecture to improve model capacity. GRU4Rec [14] and Caser [49] apply RNNs and CNNs [21] for sequence modeling. Later, inspired by the great success of self-attention [51] in natural language processing, SASRec [18] and Bert4Rec [48] verify its potential in SRS. Also, Zhou et al. [65] proposes a pure MLP architecture, achieving similar accuracy but higher efficiency compared with SASRec. Despite the great progress in SRS, long-tail problems are still underexplored. As for the long-tail item problem, CITIES [17] designs an embedding inference function for those long-tail items specially. In terms of the long-tail user problem, data augmentation is the main way [36, 34]. Only one work, MELT [20], addresses both two problems simultaneously but still sticks to a collaborative perspective. By comparison, the proposed LLM-ESR handles both the two long-tail problems better from a semantic view by introducing LLMs.\n# 5.2 LLMs for Recommendation\nLarge language models [62, 42] have attracted widespread attention due to their powerful abilities in semantic understanding. Recently, There emerge several works to explore how to utilize LLMs in recommender systems (RS) [63, 28, 22, 40, 56, 57, 64, 32, 30], which can be categorized into two lines, i.e., LLMs as RS and LLMs enhancing RS. The first line of research aims to complete recommendation tasks by LLMs directly. At the early stage, researchers tend to fabricate the prompt templates to stimulate the recommending ability of LLMs by dialogues. For example, ChatRec [10] proposes a dialogue process to complete recommendation tasks step by step. DRDT [54] integrates a retrieval-based dynamic reflection process for SRS by in-context learning [6]. LLMRerank [9] and UniLLMRec [61] fabricate the chain-of-thought prompts to target the reranking stage and whole recommendation process, respectively. Besides, some other researchers explore fine-tuning open-sourced LLMs for RS. TALLRec [2] is the first one, which fine-tunes a LLaMA-7B by parameter-efficient fine-tuning techniques [15, 33]. Some following works, including E4SRec [25], LLaRA [27] and RecInterpreter [58], target combining collaborative signals into LLMs by modifying the tokenization. However, this line of work faces the challenge of high inference costs. Another line, LLMs enhancing RS, is more practical, because they avoid the use of LLMs while recommending. For instance, RLMRec [44] aligns with LLMs by an auxiliary loss. AlphaRec [46] adopts LLMs embedding to enhance the collaborative filtering models. On the other hand, LLM4MSR [55] and Uni-CTR [8] propose to utilize LLMs to augment the multi-domain recommendation models. As for LLMs enhancing sequential recommendation, Harte et al. [13] and Hu et al. [16] adopt LLMs embedding as the initialization for the traditional models. The proposed LLM-ESR belongs to the latter category but further alleviates the problem of defect of semantic information.\n# 6 Conclusion\nIn this paper, we propose a large language model enhancement framework for sequential recommendation (LLM-ESR) to handle the long-tail user and long-tail item challenges. Firstly, we acquire and cache the semantic embeddings derived from LLMs, which is for inference efficiency. Then, a dual-view modeling framework is proposed to combine the semantics from LLMs and collaborative signals contained in the traditional model. It can help augment the long-tail items in SRS. Next, we design the retrieval augmented self-distillation to alleviate the long-tail user challenge. Through the comprehensive experiments, we verify the effectiveness and flexibility of our LLM-ESR.\n# 7 Acknowledgements\nThis research was partially supported by National Key Research and Development Program of China (2022YFC3303600), National Natural Science Foundation of China (No.62192781, No.62177038, No.62293551, No.62277042, No.62137002, No.61721002, No.61937001, No.62377038), Project of China Knowledge Centre for Engineering Science and Technology, \u201cLENOVO-XJTU\u201d Intelligent Industry Joint Laboratory Project, Research Impact Fund (No.R1015-23), APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of CityU), CityU - HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No.ITS/034/22MS), Hong Kong Environmental and Conservation Fund (No. 88/2022), SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046), and Tencent (CCF-Tencent Open Fund, Tencent Rhino-Bird Focused Research Program).\n# References\n[1] I. Amos, J. Berant, and A. Gupta. Never train from scratch: Fair comparison of long-sequence models requires data-driven priors. In The Twelfth International Conference on Learning Representations, 2023. [2] K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 1007\u20131014, 2023. [3] P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. [4] G. E. Box and R. D. Meyer. An analysis for unreplicated fractional factorials. Technometrics, 28(1):11\u201318, 1986. [5] J. Chang, C. Gao, Y. Zheng, Y. Hui, Y. Niu, Y. Song, D. Jin, and Y. Li. Sequential recommendation with graph neural networks. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 378\u2013387, 2021. [6] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [7] H. Fang, D. Zhang, Y. Shu, and G. Guo. Deep learning for sequential recommendation: Algorithms, influential factors, and evaluations. ACM Transactions on Information Systems (TOIS), 39(1):1\u201342, 2020. [8] Z. Fu, X. Li, C. Wu, Y. Wang, K. Dong, X. Zhao, M. Zhao, H. Guo, and R. Tang. A unified framework for multi-domain ctr prediction via large language models. ACM Transactions on Information Systems, 2023. [9] J. Gao, B. Chen, X. Zhao, W. Liu, X. Li, Y. Wang, Z. Zhang, W. Wang, Y. Ye, S. Lin, et al. Llm-enhanced reranking in recommender systems. arXiv preprint arXiv:2406.12433, 2024. 10] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524, 2023. 11] B. Geng, Z. Huan, X. Zhang, Y. He, L. Zhang, F. Yuan, J. Zhou, and L. Mo. Breaking the length barrier: Llm-enhanced ctr prediction in long textual user behaviors. arXiv preprint arXiv:2403.19347, 2024. 12] J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789\u20131819, 2021. 13] J. Harte, W. Zorgdrager, P. Louridas, A. Katsifodimos, D. Jannach, and M. Fragkoulis. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 1096\u20131102, 2023. 14] B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk. Session-based recommendations with recurrent neural networks. In The International Conference on Learning Representations, 2016. 15] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 16] J. Hu, W. Xia, X. Zhang, C. Fu, W. Wu, Z. Huan, A. Li, Z. Tang, and J. Zhou. Enhancing sequential recommendation via llm-based semantic embedding learning. In Companion Proceedings of the ACM on Web Conference 2024, pages 103\u2013111, 2024. 17] S. Jang, H. Lee, H. Cho, and S. Chung. Cities: Contextual inference of tail-item embeddings for sequential recommendation. In 2020 IEEE International Conference on Data Mining (ICDM), pages 202\u2013211. IEEE, 2020.\n[18] W.-C. Kang and J. McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), pages 197\u2013206. IEEE, 2018. [19] J. D. M.-W. C. Kenton and L. K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186, 2019. [20] K. Kim, D. Hyun, S. Yun, and C. Park. Melt: Mutual enhancement of long-tailed user and item for sequential recommendation. In Proceedings of the 46th international ACM SIGIR conference on Research and development in information retrieval, pages 68\u201377, 2023. [21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. [22] L. Li, Y. Zhang, D. Liu, and L. Chen. Large language models for generative recommendation: A survey and visionary discussions. arXiv preprint arXiv:2309.01157, 2023. [23] M. Li, Z. Zhang, X. Zhao, W. Wang, M. Zhao, R. Wu, and R. Guo. Automlp: Automated mlp for sequential recommendations. In Proceedings of the ACM Web Conference 2023, pages 1190\u20131198, 2023. [24] M. Li, X. Zhao, C. Lyu, M. Zhao, R. Wu, and R. Guo. Mlp4rec: A pure mlp architecture for sequential recommendations. In 31st International Joint Conference on Artificial Intelligence and the 25th European Conference on Artificial Intelligence (IJCAI-ECAI 2022), pages 2138\u2013 2144. International Joint Conferences on Artificial Intelligence, 2022. [25] X. Li, C. Chen, X. Zhao, Y. Zhang, and C. Xing. E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation. arXiv preprint arXiv:2312.02443, 2023. [26] J. Liang, X. Zhao, M. Li, Z. Zhang, W. Wang, H. Liu, and Z. Liu. Mmmlp: Multi-modal multilayer perceptron for sequential recommendations. In Proceedings of the ACM Web Conference 2023, pages 1109\u20131117, 2023. [27] J. Liao, S. Li, Z. Yang, J. Wu, Y. Yuan, X. Wang, and X. He. Llara: Aligning large language models with sequential recommenders. arXiv preprint arXiv:2312.02445, 2023. [28] J. Lin, X. Dai, Y. Xi, W. Liu, B. Chen, X. Li, C. Zhu, H. Guo, Y. Yu, R. Tang, et al. How can recommender systems benefit from large language models: A survey. arXiv preprint arXiv:2306.05817, 2023. [29] L. Liu, L. Cai, C. Zhang, X. Zhao, J. Gao, W. Wang, Y. Lv, W. Fan, Y. Wang, M. He, et al. Linrec: Linear attention mechanism for long-term sequential recommender systems. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 289\u2013299, 2023. [30] Q. Liu, J. Hu, Y. Xiao, X. Zhao, J. Gao, W. Wang, Q. Li, and J. Tang. Multimodal recommender systems: A survey. ACM Computing Surveys, 57(2):1\u201317, 2024. [31] Q. Liu, F. Tian, Q. Zheng, and Q. Wang. Disentangling interest and conformity for eliminating popularity bias in session-based recommendation. Knowledge and Information Systems, 65(6):2645\u20132664, 2023. [32] Q. Liu, X. Wu, W. Wang, Y. Wang, Y. Zhu, X. Zhao, F. Tian, and Y. Zheng. Large language model empowered embedding generator for sequential recommendation. arXiv preprint arXiv:2409.19925, 2024. [33] Q. Liu, X. Wu, X. Zhao, Y. Zhu, D. Xu, F. Tian, and Y. Zheng. When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1104\u20131114, 2024. [34] Q. Liu, F. Yan, X. Zhao, Z. Du, H. Guo, R. Tang, and F. Tian. Diffusion augmentation for sequential recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 1576\u20131586, 2023.\n[35] S. Liu and Y. Zheng. Long-tail session-based recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems, pages 509\u2013514, 2020. [36] Z. Liu, Z. Fan, Y. Wang, and P. S. Yu. Augmenting sequential recommendation with pseudoprior items via reversely pre-training transformer. In Proceedings of the 44th international ACM SIGIR conference on Research and development in information retrieval, pages 1608\u20131612, 2021. [37] Z. Liu, Q. Liu, Y. Wang, W. Wang, P. Jia, M. Wang, Z. Liu, Y. Chang, and X. Zhao. Bidirectional gated mamba for sequential recommendation. arXiv preprint arXiv:2408.11451, 2024. [38] Z. Liu, S. Liu, Z. Zhang, Q. Cai, X. Zhao, K. Zhao, L. Hu, P. Jiang, and K. Gai. Sequential recommendation for optimizing both immediate feedback and long-term retention. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1872\u20131882, 2024. [39] Z. Liu, J. Tian, Q. Cai, X. Zhao, J. Gao, S. Liu, D. Chen, T. He, D. Zheng, P. Jiang, et al. Multi-task recommendations with reinforcement learning. In Proceedings of the ACM Web Conference 2023, pages 1273\u20131282, 2023. [40] S. Luo, Y. Yao, B. He, Y. Huang, A. Zhou, X. Zhang, Y. Xiao, M. Zhan, and L. Song. Integrating large language models into recommendation via mutual augmentation and adaptive aggregation. arXiv preprint arXiv:2401.13870, 2024. [41] J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 43\u201352, 2015. [42] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1\u201340, 2023. [43] K. Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559\u2013572, 1901. [44] X. Ren, W. Wei, L. Xia, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024, page 3464\u20133475, 2024. [45] J. Shang, T. Ma, C. Xiao, and J. Sun. Pre-training of graph augmented transformers for medication recommendation. In 28th International Joint Conference on Artificial Intelligence, IJCAI 2019, pages 5953\u20135959. International Joint Conferences on Artificial Intelligence, 2019. [46] L. Sheng, A. Zhang, Y. Zhang, Y. Chen, X. Wang, and T.-S. Chua. Language models encode collaborative signals in recommendation. arXiv preprint arXiv:2407.05441, 2024. [47] U. Singer, H. Roitman, Y. Eshel, A. Nus, I. Guy, O. Levi, I. Hasson, and E. Kiperwasser. Sequential modeling with multiple attributes for watchlist recommendation in e-commerce. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 937\u2013946, 2022. [48] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1441\u20131450, 2019. [49] J. Tang and K. Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 565\u2013573, 2018. [50] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [52] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. [53] S. Wang, L. Hu, Y. Wang, L. Cao, Q. Z. Sheng, and M. Orgun. Sequential recommender systems: challenges, progress and prospects. In 28th International Joint Conference on Artificial Intelligence, IJCAI 2019, pages 6332\u20136338. International Joint Conferences on Artificial Intelligence, 2019. [54] Y. Wang, Z. Liu, J. Zhang, W. Yao, S. Heinecke, and P. S. Yu. Drdt: Dynamic reflection with divergent thinking for llm-based sequential recommendation. arXiv preprint arXiv:2312.11336, 2023. [55] Y. Wang, Y. Wang, Z. Fu, X. Li, X. Zhao, H. Guo, and R. Tang. Llm4msr: An llm-enhanced paradigm for multi-scenario recommendation. arXiv preprint arXiv:2406.12529, 2024. [56] L. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen. Exploring large language model for graph data understanding in online job recommendations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 9178\u20139186, 2024. [57] L. Wu, Z. Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu, et al. A survey on large language models for recommendation. World Wide Web, 27(5):60, 2024. [58] Z. Yang, J. Wu, Y. Luo, J. Zhang, Y. Yuan, A. Zhang, X. Wang, and X. He. Large language model can interpret latent space of sequential recommender. arXiv preprint arXiv:2310.20487, 2023. [59] C. Zhang, Q. Han, R. Chen, X. Zhao, P. Tang, and H. Song. Ssdrec: Self-augmented sequence denoising for sequential recommendation. arXiv preprint arXiv:2403.04278, 2024. [60] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3713\u20133722, 2019. [61] W. Zhang, X. Li, Y. Wang, K. Dong, Y. Wang, X. Dai, X. Zhao, H. Guo, R. Tang, et al. Tired of plugins? large language models can be end-to-end recommenders. arXiv preprint arXiv:2404.00702, 2024. [62] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [63] Z. Zhao, W. Fan, J. Li, Y. Liu, X. Mei, Y. Wang, Z. Wen, F. Wang, X. Zhao, J. Tang, et al. Recommender systems in the era of large language models (llms). IEEE Transactions on Knowledge and Data Engineering, 2024. [64] Z. Zheng, W. Chao, Z. Qiu, H. Zhu, and H. Xiong. Harnessing large language models for text-rich sequential recommendation. In Proceedings of the ACM on Web Conference 2024, pages 3207\u20133216, 2024. [65] K. Zhou, H. Yu, W. X. Zhao, and J.-R. Wen. Filter-enhanced mlp is all you need for sequential recommendation. In Proceedings of the ACM web conference 2022, pages 2388\u20132399, 2022.\n# A Supplement to Method\nIn this section, the details of prompt design and the procedures of LLM-ESR are addressed.\n# A.1 Prompt Design\nIn Section 3.2 and Section 3.3, we format the attributes of items and historical interactions of users into textual prompts, for their semantic embeddings by LLMs. During the process of constructing prompts, the templates play a vital role. Here, the templates are listed as follows. Item Prompt Template. The templates mainly organize the attributes and descriptions of items, which vary across distinct datasets due to different recorded attributes. In the following templates, the words underlined are the corresponding attributes that will be filled in.\nItem Prompt Template (Yelp)\nThe point of interest has the following attributes:\nname is <NAME>; category is <CATEGORY>; type is <TYPE>; open status is <OPEN>;\nreview count is <COUNT>; city is <CITY>; average score is <STARS>.\nItem Prompt Template (Fashion)\nThe fashion item has the following attributes:\nname is <TITLE>; brand is <BRAND>; score is <DATE>; price is <PRICE>.\nThe item has the following features: <FEATURE>.\nThe item has the following descriptions: <DESCRIPTION>.\nItem Prompt Template (Beauty)\nThe beauty item has the following attributes:\nname is <TITLE>; brand is <BRAND>; price is <PRICE>.\nThe item has the following features: <CATEGORIES>.\nThe item has the following descriptions: <DESCRIPTION>.\nUser Prompt Template. This template mainly organizes the items that the user has interacted with.\nTo utilize the semantic information and avoid excess of the limitation of input length, the item in the\nprompt is represented by its title. Besides, the three datasets share a unique template.\nItem Prompt Template (Yelp) The point of interest has the following attributes: name is <NAME>; category is <CATEGORY>; type is <TYPE>; open status is <OPEN>; review count is <COUNT>; city is <CITY>; average score is <STARS>.\nUser Prompt Template. This template mainly organizes the items that the user has interacted with. To utilize the semantic information and avoid excess of the limitation of input length, the item in the prompt is represented by its title. Besides, the three datasets share a unique template.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2fe7/2fe76691-ce3c-48e0-85ac-69991a6570ce.png\" style=\"width: 50%;\"></div>\n# User Prompt Template\n# A.2 Train and Inference Process\nFor a clearer illustration of the training and inference process, we conclude them in Algorithm 1. First, the hyper-parameters and backbone SRS model are specified (lines 1-3). Then, organize the attributes of items and historical interactions into textual prompts to get their semantic embeddings (line 4). At the beginning of the training, we initialize the embedding layers in the dual-view framework (line 5). Next, calculate the ranking loss by dual-view modeling (lines 7-9) and auxiliary loss by retrieval augmented self-distillation (lines 10-11). Through the sum of these two losses (line 12), we can optimize the whole LLM-ESR. During the inference, only the dual-view modeling process is conducted to get the final recommendations (lines 16-17).\nAlgorithm 1 Train and inference process of LLM-ESR\n1: Indicate the backbone sequential recommendation model f\u03b8.\n2: Indicate the number of retrieved similar users N.\n3: Indicate the weight of self-distillation loss \u03b1.\n4: Get the semantic embeddings Ese and Ullm by LLMs.\nTrain Process\n5: Initialize the embedding layers in the dual-view framework by the raw and dimension-reduced\nEse. Freeze the raw Ese.\n6: for a batch of users UB in U do\n7:\nGet the user preference representation in semantic and collaborative views, i.e., use and uco,\nrespectively.\n8:\nCalculate the probability score of ground-truth and negative items by Equation (5).\n9:\nCalculate the ranking loss by Equation (6).\n10:\nRetrieve the similar users for each user in UB by Equation (7).\n11:\nCalculate the self-distillation loss by Equation (9).\n12:\nSum the ranking loss and self-distillation loss. Then, update the parameters.\n13: end for\nInference Process\n14: Load Ese for item embedding layers and other trained parameters.\n15: for each user uk in U do\n16:\nGet the user preference representation in semantic and collaborative views, i.e., use and uco.\n17:\nCalculate the probability score of each candidate item by Equation (5) and give out the final\nrecommended list.\n18: end for\nAlgorithm 1 Train and inference process of LLM-ESR 1: Indicate the backbone sequential recommendation model f\u03b8. 2: Indicate the number of retrieved similar users N. 3: Indicate the weight of self-distillation loss \u03b1. 4: Get the semantic embeddings Ese and Ullm by LLMs. Train Process 5: Initialize the embedding layers in the dual-view framework by the raw and dimension-reduced Ese. Freeze the raw Ese. 6: for a batch of users UB in U do 7: Get the user preference representation in semantic and collaborative views, i.e., use and uco, respectively. 8: Calculate the probability score of ground-truth and negative items by Equation (5). 9: Calculate the ranking loss by Equation (6). 10: Retrieve the similar users for each user in UB by Equation (7). 11: Calculate the self-distillation loss by Equation (9). 12: Sum the ranking loss and self-distillation loss. Then, update the parameters. 13: end for Inference Process 14: Load Ese for item embedding layers and other trained parameters. 15: for each user uk in U do 16: Get the user preference representation in semantic and collaborative views, i.e., use and uco. 17: Calculate the probability score of each candidate item by Equation (5) and give out the final recommended list. 18: end for\n# Train Process\n# B Experimental Settings\nIn this section, we will refer to more details about the experimental settings.\nIn this section, we will refer to more details about the experimental settings.\n# B.1 Dataset and Preprocessing\nThe comprehensive experiments in this paper are conducted on three common-used datasets, i.e., Yelp, Fashion and Beauty. Yelp3 is the dataset that records the check-in histories and corresponding reviews of users. We only adopt the check-in data and the attribute information of the point-of-interests. Amazon4 [41] is a large e-commerce dataset, which includes user\u2019s reviews on commodities. There are several sub-categories in this dataset and we use two of them, i.e., Fashion and Beauty. For preprocessing, we refer to the procedures in SASRec [18]. Since the sequential recommendation is often utilized for implicit interactions, we consider all review or rate records as interactions. Then, the users with fewer than three interacted items are dropped, because we do not explore the problem of cold-start users in this paper. As for the data split, the last item vnu and the penultimate item vnu\u22121 of each interaction sequence are taken out as the test and validation, respectively. The statistics of the three preprocessed datasets are shown in Table 3.\n<div style=\"text-align: center;\">Table 3: The statistics of the preprocessed datasets</div>\nTable 3: The statistics of the preprocessed datasets\nDataset\n# Users\n# Items\nSparsity\nAvg.length\nYelp\n15,720\n11,383\n99.89%\n12.23\nFashion\n9,049\n4,722\n99.92%\n3.82\nBeauty\n52,204\n57,289\n99.99%\n7.57\n3https://www.yelp.com/dataset 4https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_revie\nBackbone Models. To show the flexibility of our enhancement method, we test three popula sequential recommendation models in the experiments. The main distinction between these model refers to the sequence encoder f\u03b8 and ranking loss LRank.\nrefers to the sequence encoder f\u03b8 and ranking loss LRank. \u2022 GRU4Rec [14]. It adopts the GRU as the sequence encoder, and sequence-to-one pairwise loss as the final ranking loss. \u2022 Bert4Rec [48]. Inspired by the training pattern of Bert [19], this backbone proposes a combination between pairwise ranking loss and cloze task, which mask a proportion of items in one sequence. The sequence encoder of Bert4Rec is the stack of bi-directional self-attention layers. \u2022 SASRec [18]. Compared with Bert4rec, SASRec adopts the causal self-attention layer as the basic unit of its sequence encoder. Besides, the sequence-to-sequence pairwise ranking loss is applied for optimization during the training. There are two groups of up-to-date baselines that are compared within this paper, i.e., traditional baselines and LLM-based baselines. Traditional Baselines. This category split the users and items into long-tail and head groups at first. Then, they enhance the long-tail users or items by fabricated training procedures. Note that they only utilize the collaborative signals essentially and do not introduce any semantics. \u2022 CITIES [17]. This work devises an embedding-inference function to refine the embeddings of long-tail items specially. Such embedding-inference function is trained by head items and used for long-tail items during inference. We follow the hyper-parameters in the original paper and code5. \u2022 MELT [20]. MELT proposes a bilateral-branch framework to enhance the long-tail users and items. One branch is trained to generate the head user representations and enhance the tail users while inference. The other branch is to recover the embeddings of head items during training and update embeddings of tail items during inference. We refer to the implementation and the hyper-parameter settings in official code6.\n\u2022 CITIES [17]. This work devises an embedding-inference function to refine the embeddings of long-tail items specially. Such embedding-inference function is trained by head items and used for long-tail items during inference. We follow the hyper-parameters in the original paper and code5. \u2022 MELT [20]. MELT proposes a bilateral-branch framework to enhance the long-tail users and items One branch is trained to generate the head user representations and enhance the tail users while inference. The other branch is to recover the embeddings of head items during training and update embeddings of tail items during inference. We refer to the implementation and the hyper-parameter settings in official code6.\n\u2022 RLMRec [44]. This baseline is one of the pioneers in utilizing the semantic embeddings derived from LLMs. However, it is designed for collaborative filtering but not sequential recommendation. For a fair comparison, we eliminate the process of profile generation during the implementation. We refer to the source code7 of RLMRec to adapt it to sequential recommendation models. \u2022 LLMInit [13, 16]. More recent works, i.e., LLM2Bert4Rec [13] and SAID [16], both utilize the LLMs embedding to initialize the item embedding layer in SRS models and then fine-tune it by interaction data. In this paper, we dub this way as LLMInit.\n# B.3 Implementation Details\nWe conduct all experiments on an Intel Xeon Gold 6133 platform with Tesla V100 32G GPUs. Besides, the implementation is based on Python 3.9.5 and PyTorch 1.12.0. In terms of the hyperparameter search, the criterion is N@10 on the validation set. To avoid overfitting, we adopt the early stop strategy with 20-epoch patience. For the backbone SRS models, the number of GRU layers is set to 1 for GRU4Rec, while the number of self-attention layers is fixed at 2 for SASRec and Bert4Rec. Also, the dropout rate is 0.6 for Bert4Rec. In terms of the training, the batch size and learning rate are set as 128 and 0.001 for all datasets. The embedding size is 128 for all baselines, while 64 for LLM-ESR. The reason is that there are two branches in LLM-ESR, and the half size of the other unique-branch baseline is a fair setting. Then, we choose the Adam as the optimizer. The hyperparameters N and \u03b1 for LLM-ESR are searched from {2, 6, 10, 14, 18} and {1, 0.5, 0.1, 0.05, 0.01}. We find that the best choice is 10 for N and 0.1 for \u03b1 for all three datasets used in this paper.\n5https://github.com/swonj90/CITIES 6https://github.com/rlqja1107/MELT 7https://github.com/HKUDS/RLMRec\n<div style=\"text-align: center;\">Table 4: The ablation study on the Yelp dataset with Bert4Rec as the backbone SRS model. The boldface refers to the highest score and the underline indicates the next best result of the models.</div>\nboldface refers to the highest score and the underline indicates the next best result of the models.\nModel\nOverall\nTail Item\nHead Item\nTail User\nHead User\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\n- LLM-ESR\n0.6623\n0.4222\n0.1227\n0.0500\n0.8212\n0.5318\n0.6637\n0.4247\n0.6571\n0.4127\n- w/o Co-view\n0.6273\n0.3737\n0.1272\n0.0520\n0.7745\n0.4684\n0.6296\n0.3760\n0.6184\n0.3647\n- w/o Se-view\n0.6521\n0.4125\n0.0981\n0.0395\n0.8153\n0.5224\n0.6533\n0.4150\n0.6477\n0.4031\n- w/o SD\n0.6539\n0.4114\n0.1299\n0.0534\n0.8081\n0.5168\n0.6539\n0.4129\n0.6538\n0.4055\n- w/o Share\n0.6592\n0.4193\n0.1182\n0.0480\n0.8187\n0.5276\n0.6619\n0.4229\n0.6482\n0.4100\n- w/o CA\n0.6368\n0.3924\n0.0940\n0.0369\n0.7966\n0.4971\n0.6369\n0.3940\n0.6367\n0.3862\n<div style=\"text-align: center;\">Table 5: The ablation study on the Yelp dataset with GRU4Rec as the backbone SRS model. The boldface refers to the highest score and the underline indicates the next best result of the models.</div>\nboldface refers to the highest score and the underline indicates the next best result of the models.\nModel\nOverall\nTail Item\nHead Item\nTail User\nHead User\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\nH@10\nN@10\n- LLM-ESR\n0.5724\n0.3413\n0.0763\n0.0318\n0.7184\n0.4324\n0.5782\n0.3456\n0.5501\n0.3247\n- w/o Co-view\n0.5660\n0.3263\n0.0831\n0.0331\n0.7022\n0.4097\n0.5720\n0.3310\n0.5530\n0.3188\n- w/o Se-view\n0.5273\n0.3091\n0.0441\n0.0187\n0.6695\n0.3945\n0.5316\n0.3122\n0.5107\n0.2971\n- w/o SD\n0.5562\n0.3236\n0.0720\n0.0309\n0.7028\n0.4053\n0.5605\n0.3371\n0.5498\n0.3203\n- w/o Share\n0.5661\n0.3353\n0.0789\n0.0325\n0.7124\n0.4274\n0.5689\n0.3297\n0.5536\n0.3285\n- w/o CA\n0.5657\n0.3327\n0.0736\n0.0304\n0.7126\n0.4128\n0.5755\n0.3426\n0.5493\n0.3227\nFurthermore, the embeddings of LLMs are derived from the API8 named \u201ctext-ada-embedding-002 provided by OpenAI.\n# C More Experimental Results\nIn this section, we will show more experimental results to further analyze the flexibility and effectiveness of our LLM-ESR.\nC.1 Ablation Study\nFor further analysis, we conduct the ablation study on the proposed LLM-ESR with Bert4Rec and GRU4Rec as the backbone SRS models. The results are shown in Table 4 and Table 5. At first, we probe the effects of dual-view modeling by removing one of the views, denoted as w/o Co-view and w/o Se-view. From the overall performance, these two variants both underperform, which indicates the essence of the dual-view. Besides, w/o Co-view downgrades the accuracy of the head item group more, while w/o So-view harms the long-tail item group compared with LLM-ESR. This phenomenon highlights the advantages of the collaborative view and semantic view, respectively. As for distinct SRS backbone models, we find that Bert4Rec benefits more from collaborative information, because removing the collaborative view causes a more severe performance drop. By comparison, GRU4Rec can get more enhancement from the semantic view. Then, w/o SD means eliminating self-distillation. It downgrades the performance of the tail user group consistently, which indicates the proposed retrieval augmented self-distillation can actually help alleviate the long-tail user challenge. w/o Share represents using separate sequence encoders for the dual views. This variant is a little worse than applying a shared encoder, illustrating the common pattern for both views. Another advantage of the shared encoder is higher parameter efficiency. Besides, LLM-ESR without cross-attention (w/o CA) is inferior to LLM-ESR totally, which indicates the effectiveness of the sequence-level fusion. At the same time, it is risky to overfit with semantic embeddings when the textual data is scarce. To validate the robustness of our LLM-ESR, we conduct additional experiments in scenarios with limited textual data. To simulate this situation, we removed all attributes from the item descriptions except for \u201cname\u201d and \u201ccategories\u201d when constructing the textual prompts for the Yelp dataset (originally using 8 attributes). This reduced the average word count of the textual prompts from 38.38 to 20.33. We used SASRec as the backbone model in these supplementary experiments, with results presented\n8https://api.openai.com/v1/embeddings\n<div style=\"text-align: center;\">Table 6: The experiments for limited text and the design of freezing semantic embedding. All the experiments are conducted on the Yelp dataset and for LLM-ESR. \u201cFull\u201d and \u201cCrop\u201d mean that we use the completed item prompt and attribute-cropped prompt to get the LLM embeddings, respectively \u201cw/o F\u201d means that we train the LLM-ESR without freezing the semantic embedding layer.</div>\nTable 6: The experiments for limited text and the design of freezing semantic embedding. All the experiments are conducted on the Yelp dataset and for LLM-ESR. \u201cFull\u201d and \u201cCrop\u201d mean that we use the completed item prompt and attribute-cropped",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the long-tail challenges in sequential recommender systems (SRS), where most users interact with a limited number of items while many items remain seldom consumed. Existing methods struggle with these challenges, leading to suboptimal user experiences. The advancements in large language models (LLMs) offer a promising solution, motivating the development of a new framework, LLM-ESR, which integrates semantic embeddings from LLMs to enhance SRS without incurring additional inference costs.",
        "problem": {
            "definition": "The problem of sequential recommendation involves predicting the next likely item for users based on their historical interactions. This paper focuses on enhancing performance for long-tail users and items, where the majority of users have limited interactions and many items receive few interactions.",
            "key obstacle": "The main challenge lies in the intrinsic scarcity of interactions for long-tail users and items, which existing methods fail to address effectively, often resulting in noisy recommendations and a seesaw effect."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of large language models to capture semantic relationships between users and items, which can be leveraged to improve recommendations for long-tail users and items.",
            "opinion": "The proposed method, LLM-ESR, combines semantic information from LLMs with collaborative signals from traditional SRS to address the long-tail challenges effectively.",
            "innovation": "The key innovation of LLM-ESR is its dual-view modeling framework, which integrates semantic embeddings and collaborative signals, and a retrieval augmented self-distillation method that enhances user representation through informative interactions from similar users."
        },
        "method": {
            "method name": "Large Language Models Enhancement for Sequential Recommendation",
            "method abbreviation": "LLM-ESR",
            "method definition": "LLM-ESR is a framework designed to enhance sequential recommendation systems by utilizing semantic embeddings from large language models to improve recommendations for long-tail users and items.",
            "method description": "The method combines dual-view modeling and retrieval augmented self-distillation to leverage both semantic and collaborative information.",
            "method steps": [
                "1. Derive semantic embeddings for users and items using LLMs.",
                "2. Implement dual-view modeling to fuse semantic and collaborative information.",
                "3. Apply retrieval augmented self-distillation to enhance user preference representation.",
                "4. Train the model using a combination of ranking loss and self-distillation loss."
            ],
            "principle": "The effectiveness of LLM-ESR stems from its ability to capture rich semantic relationships and enhance collaborative signals, thereby improving the representation of long-tail users and items."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three real-world datasets: Yelp, Amazon Fashion, and Amazon Beauty, comparing LLM-ESR against traditional and LLM-based baselines using three popular SRS models (GRU4Rec, Bert4Rec, SASRec).",
            "evaluation method": "The performance was assessed using metrics such as Hit Rate (H@10) and Normalized Discounted Cumulative Gain (N@10), with extensive experiments to validate the effectiveness and flexibility of LLM-ESR."
        },
        "conclusion": "The results demonstrate that LLM-ESR consistently outperforms existing baselines across all datasets and user/item groups, effectively addressing the long-tail challenges in sequential recommendation systems.",
        "discussion": {
            "advantage": "LLM-ESR stands out by effectively combining semantic and collaborative signals, significantly enhancing the recommendation performance for long-tail users and items.",
            "limitation": "While LLM-ESR improves performance, it may still face challenges in scenarios with extremely limited user interactions or items with very few historical data points.",
            "future work": "Future research could explore further optimizations of the framework, including refining the integration of LLMs and evaluating the method's performance in different domains."
        },
        "other info": {
            "implementation code": "The implementation code for LLM-ESR is available at https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.",
            "datasets": [
                {
                    "name": "Yelp",
                    "description": "Dataset containing check-in histories and reviews of users."
                },
                {
                    "name": "Amazon Fashion",
                    "description": "E-commerce dataset with user reviews on fashion items."
                },
                {
                    "name": "Amazon Beauty",
                    "description": "E-commerce dataset with user reviews on beauty products."
                }
            ],
            "hyperparameters": {
                "N": "10",
                "alpha": "0.1"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "The problem of sequential recommendation involves predicting the next likely item for users based on their historical interactions, focusing on enhancing performance for long-tail users and items."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method, LLM-ESR, integrates semantic embeddings from large language models to enhance sequential recommendation systems, addressing long-tail challenges."
        },
        {
            "section number": "4.1",
            "key information": "LLM-ESR utilizes semantic embeddings derived from large language models to improve recommendations for long-tail users and items."
        },
        {
            "section number": "6.2",
            "key information": "LLM-ESR combines semantic information from LLMs with collaborative signals from traditional sequential recommendation systems to effectively address long-tail challenges."
        },
        {
            "section number": "10.1",
            "key information": "Existing methods struggle with long-tail challenges in sequential recommendation systems, often resulting in noisy recommendations and a seesaw effect."
        }
    ],
    "similarity_score": 0.7880882494196578,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item.json"
}