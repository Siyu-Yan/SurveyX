{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.10135",
    "title": "The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation",
    "abstract": "Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.",
    "bib_name": "wang2024bettersumusingaggregated",
    "md_text": "# The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation\nLei Wang Ee-Peng Lim\u2217 Singapore Management University {lei.wang.2019, eplim}@smu.edu.sg\nAbstract\nLarge language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRecSyn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https: //github.com/demoleiwang/LLMSRec_Syn.\n# 1 Introduction\nMotivation. Large language models (LLMs) are known to perform well as a zero-shot solution for many natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2022; Qin et al., 2023). Recently, there are some works that focus on using LLMs to perform recommendation with promising accuracies (Hou et al., 2023; Wang and Lim, 2023; Liu et al., 2023a; Bao et al., 2023; Gao et al., 2023) and to provide explanations (Yang et al., 2023; Wang et al., 2023b). Most of these works developed LLM prompts for zeroshot sequential recommendation. To investigate whether LLM can serve as a strong zero-shot sequential recommender, Hou et al. (2023) devised a prompt that is filled with historical items in chronological order, candidate items, and instruction to rank the candidate\n\u2217Corresponding author.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c59/9c5941bd-923b-44fa-94e2-585ca7f1d7e2.png\" style=\"width: 50%;\"></div>\nFigure 1: The illustrative comparison of (a) supervised sequential recommendation method and (b) in-context learning based sequential recommendation method.\n<div style=\"text-align: center;\">Figure 1: The illustrative comparison of (a) supervised sequential recommendation method and (b) in-context learning based sequential recommendation method.</div>\nitems. Wang and Lim (2023) proposed a threestep prompting method, where LLMs first summarizes the user preference based on the user\u2019s past interacted items. It then identifies representative items from the past interacted items that capture the user preference, and finally recommends items among the candidate items which are aligned with the representative items. Among the very few oneshot sequential recommendation works, Liu et al. (2023a) and Hou et al. (2023) explored in-context learning using the test user\u2019s second last item as the ground truth next-item and all earlier interacted items as input to create self-demonstrations. Nevertheless, previous experiments have shown that in-context learning (ICL) based sequential recommendation methods perform poorly compared with\nthe supervised learning-based methods (e.g., SASRec) due to the complex recommendation task definition (Liu et al., 2023a; Hou et al., 2023; Wang and Lim, 2023). The illustrative comparison of these two methods is shown in Figure 1. To develop an effective in-context learning approach for LLMs to perform sequential recommendation, we first define the sequential recommendation problem as follows. Problem definition. We denote each input user instance ui to be a (xi, ci, yi) tuple where xi denotes the sequence of past interacted items (excluding yi) by ui, ci denotes the candidate items to be recommended (|ci| = M), and yi denotes the ground truth next-item which is also the last item interacted by ui. Note that yi appears in ci (yi \u2208ci). A LLM-based sequential recommendation method is required to assign a rank rank(d) \u2208[1, M] to each item d in ci. Our objective is to ensure that the method ranks yi, i.e., rank(yi), as high as possible for all. The above definition includes ci as input as it is usually infeasible for LLMs to take all items as input due to limited prompt length. Moreover, having ci does not introduce bias in the evaluation. The above definition is also adopted in Hou et al. (2023). We also assume a dataset of users\u2019 interacted item sequences from which we can construct demonstrations for ICL, and a LLM which is too large for pretraining or finetuning. Overview of our study. Past works has shown that the effectiveness of ICL in adapting LLMs to new tasks is significantly influenced by instruction wording (Madaan and Yazdanbakhsh, 2022; Yang et al., 2023), label design (Yoo et al., 2022; Wei et al., 2023), selection of demonstrations (Liu et al., 2021; Shi et al., 2022; Zhang et al., 2023b), and number of demonstrations Chen et al. (2023); Zhao et al. (2023). Our study thus begins by systematically investigating how the instruction format, task consistency (between test and demonstration), demonstration selection, and the number of demonstrations affect ICL-based sequential recommendation. Through our preliminary experiments, we obtain four findings including the one that observes degradation of recommendation accuracy when the number of demonstrations increases. As each demonstration takes up significant length, it is also easy for multiple demonstrations to exceed the prompt limit of LLMs. Moreover, as LLMs are known to miss out relevant information in a long input prompt (Liu et al., 2023b), we thus embark\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1107/11075e72-50b6-4855-ab14-375b4c6efe8a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The overall framework of zero-shot, fewshot, and aggregated one-shot LLM-based sequential recommender systems.</div>\non a follow-up study on designing a more efficient ICL scheme based on aggregated demonstration. Figure 2 shows a comparison of the frameworks for zero-shot, few-shot, and aggregated one-shot LLM-based sequential recommender systems. The key idea in aggregated demonstration is to combine multiple training users into one demonstration. This reduces the repetition of instruction text in the ICL prompt. It also seeks to summarize multiple training users relevant to the test instance in a compact manner. We also develop a novel ICL method using aggregated demonstration for sequential recommendation known as LLMSRec-Syn. The length of LLMSRec-Syn prompt increases only gradually with number of demonstration users, LLMSRec-Syn can cope with more relevant information from the demonstration users within a concise input context. We finally show LLMSRecSyn outperforms other zero-shot and one-shot ICL methods in an extensive set of experiments. Contribution. Our contributions can be summarized as follows: (1) We systematically explore the ICL approach to sequential recommendation by empirically investigating the effect of instruction format, task consistency, demonstration selection, and number of demonstrations; (2) We propose a new in-context learning method for sequential recommendation called LLMSRec-Syn which leverages on a novel concept of aggregated demonstration; (3) We experiment on three popular recommendation datasets and show that LLMSRec-Syn outperforms previous LLM-based sequential recommendation methods.\n# 2 Related Work\nIn-Context Learning. Several works show that LLMs can effectively adapt to different NLP and multimodal tasks, including machine trans-\nlation (Agrawal et al., 2022), visual question answering (Yang et al., 2022), and foreground segmentation (Zhang et al., 2023b). This adaptation is achieved by learning from a few task-relevant demonstrations, commonly known as in-context learning (ICL) (Brown et al., 2020). Despite the above successes, ICL\u2019s performance is still significantly affected by the wording of instructions (Madaan and Yazdanbakhsh, 2022; Yang et al., 2023), label design (Yoo et al., 2022; Wei et al., 2023), demonstration selection (Liu et al., 2021; Shi et al., 2022; Zhang et al., 2023b), and number of demonstrations Chen et al. (2023); Zhao et al. (2023). ICL is much less studied in LLMbased sequential recommendation. As sequential recommendation is distinct from the pretraining tasks of LLMs and also different from the abovementioned tasks, new designs of demonstration(s) and ICL prompt is necessary. LLMs for Sequential Recommendation. Early sequential recommendation works adopt techniques such as Markov Chains (Rendle et al., 2010; He and McAuley, 2016) and neural networks (e.g., RNN (Hidasi et al., 2015), CNN (Tang and Wang, 2018), Self-Attention (Kang and McAuley, 2018), and GNN (Chang et al., 2021)). To investigate if LLMs can be used as effective sequential recommenders without training, Hou et al. (2023) formulated sequential recommendation as conditional ranking, employing zero-shot LLM methods to reflect user preferences from past interactions and recency. Wang and Lim (2023) developed a threestep LLM prompting to summarize user preferences, while Hou et al. (2023) and Liu et al. (2023a) introduced a one-shot ICL method that utilizes the previous item interactions of the target user as a demonstration. To address position bias, Hou et al. (2023) proposed to randomize the candidate item order. In this work, we explore using training data demonstrations, not just user own history, and introduce aggregated demonstration for combining relevant users.\n# 3 What Makes In-Context Learning Work for Sequential Recommendation\nIn this section, we conduct a preliminary empirical study to investigate the role of various aspects of demonstrations. These aspects include the wording of prompts, task consistency between demonstrations and test instances, selection of demonstrations, and number of demonstrations. While pre-\n<div style=\"text-align: center;\">Table 1: Dataset statistics after removing duplicate interactions and users or items with fewer than 5 interactions.</div>\nDatasets\nML-1M LastFM-2K Games\n# Users\n6,040\n1,143\n50,547\n# Items\n3,706\n11,854\n16,859\n# User-item Interactions\n1,000,209\n68,436\n389,718\nAvg. interacted items per user\n165.59\n59.92\n7.71\nAvg. interacted users per item\n269.88\n5.77\n23.11\nvious studies have explored the use of LLM as sequential recommenders in a zero-shot manner (Hou et al., 2023; Wang and Lim, 2023), this is the first study to comprehensively discuss how in-context learning can improve sequential recommendation.\n# 3.1 Experiment Setup\nWe implement zero-shot, one-shot, and few-shot methods in this study, using three widely used recommendation datasets: the movie rating dataset MovieLens-1M (ML-1M) dataset, the category of Games from the Amazon Review dataset (McAuley et al., 2015), and the music artist listening dataset LastFM-2K (Cantador et al., 2011). The data statistics are summarized in Table 1. Taking into account cost-effectiveness of LLMs, we select 50 data examples from each of the three datasets to carry out all experiments for analysis in Section 3. Following the previous works (Hou et al., 2023; Wang and Lim, 2023), we use a leave-one-out strategy for evaluation, i.e., predicting the last interacted item of each user sequence and using the earlier interacted items as input. For each user sequence, we remove the last item, keeping it aside for testing. The rest of the sequence is used for training and validation. To evaluate the ranking results for each user ui over a set of candidate items ci, we adopt the widely used NDCG@N (N = 10, 20) as the evaluation metric. For MovieLens-1M and Games, we directly use the candidate sets utilized in an earlier work (Hou et al., 2023). For LastFM, we follow (Hou et al., 2023) and randomly select candidate items from the item universal set for each user sequence. We then insert the ground truth next item into the candidate item set. We use ChatGPT (GPT-3.5-Turbo) as the default LLM due to its excellent performance and cost-effectiveness. To ensure the reliability of findings, we repeat each experiment 9 times and report the average results. Without exception, we use ML-1M as an example for discussion.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a09e/a09ecf24-782c-4aa7-9232-2b66f1d91ef9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Instruction Format options: (A) Full, (B) w/o preference alignment, (C) w/o watched movie focus, (D) w/o rank result format</div>\n# 3.2 In-Context Learning for Sequential Recommendation\nIn ICL for sequential recommendation, one or a few training users are used as demonstrations that are included in the LLM prompt. Each demonstration thus includes a training user i\u2019s historical item interactions xi, a set of candidates ci, and ground truth next-item yi. We denote the prompt capturing the demonstration user i by T (xi, ci, yi). The following shows the concatenation of n demonstrations C which is appended by the instruction prompt for the test user T (xtest, ytest) for prediction.\n(1) (2)\n# 3.3 Wording of Instructions\nLLMs have been found to be sensitive to wording of the prompt (Madaan and Yazdanbakhsh, 2022; Yang et al., 2023). For example, prompts (or instructions) that are semantically similar may yield significantly different results (Kojima et al., 2022; Zhou et al., 2022; Wang et al., 2023a; Zhang et al., 2023a). To examine the impact of instruction wording and exclude the influence of other factors such as demonstration labels and selection, we employ LLM as a zero-shot solver for sequential recommendation. We discuss four different options for the instruction format to investigate the sensitivity of the LLM to the wording of the instruction. Considering the\nprompts used in LLM-based zero-shot recommendation models (Hou et al., 2023; Wang and Lim, 2023), we derive instructions with four possible mention components: (a) candidate item ranking, (b) user preference alignment, (c) historical interacted items, and (d) ranked result format. As recommendation is formulated as a ranking task, component (b) is mandatory. The full instruction covers all four components. To explore better instructions, we derive other instruction options by leaving out one of the remaining components. We thus have four instruction options: (A) full instruction T A, (B) full instruction without (b) T B, (C) full instruction without (c) T C, and (D) full instruction with (d) replaced by textual result table description T D as shown in Figure 3. As shown in Figure 3, we observe that ChatGPT\u2019s performance degrades when the instruction does not make reference to interacted items or user preferences across three datasets. This suggests that explicit inclusion of watched movies or user preferences can improve its ability to leverage the user\u2019s historical items effectively. While Instruction (A) shows similar average performance as Instruction (D) on ML-1M and LastFM, the former enjoys a smaller variance and outperforms the latter on LastFm. This suggests that LLM prefers explicit output formats over textual description of output format.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/37c9/37c93efc-76a8-403c-8cab-9ff70ef9a196.png\" style=\"width: 50%;\"></div>\n# 3.4 Task Consistency\nLLMs are capable of learning new tasks at test time by understanding the relationship between the input of a demonstration and its corresponding output label (Yoo et al., 2022; Wei et al., 2023). In sequential recommendation, LLM is required to rank the ground truth target item at the top followed by other candidate items. However, in a demonstration example from the training set, we observe only one labeled next item but not the ranking of other candidate items. Hence, when constructing demonstrations for in-context learning, we have to answer the important questions: How to prepare the inputlabel correspondence for a demonstration to be consistent with the sequential recommendation task?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/38bf/38bf3b9e-dc65-44ad-9407-df2a6799db7c.png\" style=\"width: 50%;\"></div>\nFigure 4: Impact of task consistency between demonstrations and test instances. CIR: Candidate Inclusion Ratio of Demonstration Templates: (T1) Next-Item option; (T2) Contrasting Item Pair option; (T3) Ranked Items option.\nTo eliminate other factors that may influence the results, such as the number of demonstrations and instructions, we employ instruction (A) as it has proven to be the most effective and robust across three datasets in our previous experiments. We randomly select only one demonstration example for all experiments in this study. In traditional sequential recommendation, nextitem prediction (Song et al., 2021; Petrov and Macdonald, 2022), positive and negative item comparison (Rendle et al., 2012; Kang and McAuley, 2018; Xie et al., 2020), and reranking (Xu et al., 2023) are commonly utilized objectives to train models. Hence, we develop three different prediction tasks for demonstrations for in-context learning. These tasks include: (T1) predicting the next item, (T2) contrasting item pairs, and (T3) ranking candidate items. The prompts corresponding to these prediction tasks are shown in Figure 4. T1 uses the ground truth next-item directly in the demonstration. T2 uses the ground truth next item and another randomly selected item as the positive and negative items respectively. T3 ranks the ground truth next item at the first position and randomly shuffles the remaining candidate items to fill the other positions. Among the task prediction task options, T3 is the only one that aligns closely with the instruction for the test user, i.e., (A). Figure 4 shows the results of these three tasks across three datasets. T3 consistently outperforms T1 and T2 on all three datasets, suggesting that task consistency between demonstration and test\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ae0/3ae01006-788a-4635-9a4c-66a0e70755d5.png\" style=\"width: 50%;\"></div>\nFigure 5: Demonstration selection: (1) random selection; (2) overlapping interacted items; (3) cosine similarity between the SBERT embeddings of interacted item sequences; (4) cosine similarity between the LLM (OpenAI embeddings) of interacted item sequences; (5) cosine similarity using CL embeddings of interacted item sequences.\nuser benefits in-context learning for sequential recommendation. Additionally, As the recommended items may not be found among the provided candidates, we also report candidate inclusion ratio (CIR) which measures the proportion of the candidate items that appear in the ranked item results. As shown in Figure 4, we observe that the CIR generally correlates with the NDCG results. The inconsistent demonstration task options (e.g., T1 and T2 coupled with test instruction option (A)) are more likely to cause the LLM to generate non-candidate items in the results. This helps to understand why T3 achieves the best performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5dc/e5dc12b1-954c-4284-8a75-bc4eceb10c48.png\" style=\"width: 50%;\"></div>\n# 3.5 Selection of Demonstrations\nIt has been observed that the performance of incontext learning greatly depends on selecting suitable demonstrations (Liu et al., 2021). Utilizing examples that are semantically similar to the test sample can provide more informative and taskrelevant knowledge to LLMs. Following Liu et al. (2021), there are several follow-up works (Rubin et al., 2021; Shi et al., 2022; Zhang et al., 2023b; Li et al., 2023) to develop methods for selecting better demonstrations. In this work, we evaluate five different demonstration selection methods to determine their impact to in-context learning for sequential recommendation. These methods include: (1) random selection; (2) overlapping historical items of demonstration user and test user; (3) text similarity scores using Sentence-BERT embedding (Reimers and Gurevych, 2019) (SBERT); (4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2372/2372434f-5fe1-4eb2-8cc9-97ab3140de12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Varying number of demonstrations.</div>\ntext similarity scores using LLM OpenAI embedding1 (LLM); and (5) trained retriever using contrastive learning (Xie et al., 2020; Li et al., 2023) (CL). In Option (5), positive examples are obtained by data augmentation applied to the anchor user sequence, while negative examples are randomly selected user item-interaction sequences. Figure 5 compares the five selection methods on ML-1M and LastFM as they are used in one-shot sequential recommendation. The results show that selection methods (4) and (5) generally outperform the rest. As method (4) appears to be more robust than (5) and it does not require additional training, we thus use that as the default retriever model in the subsequent experiments.\nFinding 3. Retrieval-based methods are better than random selection, and stronger LLMs can serve as stronger retrievers without any training.\nWhen training a model, having more training data examples usually leads to better model performance. However, it is interesting to note that Zhao et al. (2023) discover that increasing the number of demonstrations for in-context learning does not necessarily result in improved performance. Similarly, Chen et al. (2023) finds that using only one demonstration may not perform worse than using more demonstrations. In our case, we evaluate the impact of the number of demonstrations on ML1M and LastFM using random selection, LLM, and CL demonstration selection methods. We conduct experiments with the number of demonstrations ranging from 1 to 4, as exceeding 4 demonstrations would exceed the input limit of ChatGPT (GPT-3.5-Turbo). Figure 6 demonstrates a clear trend of performance decreasing with number of\n1text-embedding-ada-002 (https://platform.openai.com/docs/models/moderation)\nFinding 4. Increasing demonstrations for incontext learning for sequential recommendation would result in performance degradation and exceed the input limit of LLMs.\n# 4 In-Context Learning with Aggregated Demonstrations\nFinding 4 suggests LLMs have difficulties coping with multiple demonstrations in sequential recommendation. A similar finding by Liu et al. (2023b) also suggests that the current language models often struggle to utilize information in long input contexts. In particular, their performance tends to significantly degrade when the relevant information is located in the middle of long contexts, also known as the \u201clost in the middle\u201d phenomenon. The in-context learning prompts for sequential recommendation can easily exceed the prompt length limit of LLM when more than 4 demonstrations are to be accommodated. Such prompts not only suffer from \u201clost in the Middle\u201d, but also incur additional costs of calling LLM APIs. To address the above challenge, we propose aggregated demonstration which combines K (K > 1) demonstration users into one for in-context learning. This simple yet effective in-context learning method for sequential recommendation is called LLMSRec-Syn. As the prompt length of aggregated demonstration only increases marginally when we increase K, LLMSRec-Syn can accommodate more member demonstration users. Based on Finding 3, LLMSRec-Syn begins with selecting K demonstration users that are similar to the test user. We use similarity between the LLM embeddings of demonstration and test users. We also follow Finding 1 and adopt instruction template (A) for the test user. Based on Finding 2, we also adopt demonstrate template (T3) for the aggregated demonstration to maintain consistency with the task for test user. Next, we construct the aggregated demonstration\u2019s historical item-interactions, candidate items, and the desired ranking of the candidate items from its member demonstrations, as shown in Figure 10 in the Appendix. Historical item-interactions. Let H denote the historical item-interactions and H is empty initially. We first rank the K selected demonstration users by similarity score. We then add the most recent interacted item from the most similar demonstration\nto H. We repeat the same step for the remaining demonstrations in their similarity order. When we run out of most recent interacted items from K selected demonstrations, we continue to add the next recent interacted items of these demonstrations to H until the number of historical items reaches MAXH. Candidate Items. Let C denote the candidate items of the aggregated demonstration and C is empty initially. We first gather all the ground truth next items from the K selected demonstrations and add them to C. Next, we randomly add other items from the item pool to C so as to meet the required number of candidate items. Ranking of Candidate Items. To rank the candidate items in C, we place the ground truth next item of the most similar demonstration at rank 1, followed by that of next similar demonstration until we run out of the ground truth next items of all K selected demonstrations. Next, we assign random ranks to the remaining items in C. Once the aggregated demonstration is constructed, it is added to the prompt the same way a training user is added as a demonstration. we add it to the corresponding test user and use them as input for the LLM.\n(3)\n (4)\nytest \u223cPLLM \ufffd \u00b7 | C \u2295T  (xtest , ctest ) \ufffd , (4) where \u03c3i represents the ith ranked selected users returned by the retrieval model. Finally, the LLM generates a ranked list of candidate items as the recommendation result. There are several advantages of the proposed LLMSRec-Syn: 1) Standard demonstration only has one ground truth next item in the ranking list. In contrast, the aggregated demonstration includes more next items at high positions in the ranking list. This approach can avoid sparse signals and provide more guidance to LLMs for recommending to the test user; 2) LLMSRec-Syn is less sensitive to the number of demonstrations; 3) Cost of LLMSRecSyn does not increase much with the number of demonstrations; and 4) LLMSRec-Syn keeps to the prompt length limit of LLMs.\n# 5 Experiments and Results 5.1 Methods for Comparison\n# 5.1 Methods for Comparison\nTo evaluate the performance of LLMSRec-Syn, we conduct an extensive set of experiments on ML-1M,\nGames, and LastFM-2K datasets. Following Hou et al. (2023), we select 200 data examples from each of the three datasets to carry out all experiments. We use an experiment setup similar to that mentioned in Section 3.1 except that we now uses more LLMs and reports the NDCG@N results where N=5,10, and 20. We compare LLMSRecSyn with 10 methods categorized into 3 types: Supervised methods: Most Popular (Recommending items based on their overall popularity among all users in the training data), GRU4Rec (Hidasi et al., 2015) (using GRUs to model user\u2019s item sequences), and SASRec (Kang and McAuley, 2018) (employing a self-attention mechanism to learn user preferences from their item sequences). Zero-shot methods: BM25 (Robertson et al., 2009) (ranking candidate items based on their textual similarity with the test user\u2019s interacted items), LLMSeqSim (Harte et al., 2023) (ranking candidate items by semantically similarity using OpenAI embeddings (text-embedding-ada-002)), LLMRank-Seq (Hou et al., 2023) (using ChatGPT to rank candidate items with crafted prompts), and LLMSRec (a zero-shot version of the proposed LLMSRec-Syn using the instruction prompt T A). One-shot methods: LLMRank-His (Hou et al., 2023) (using historical items of the test user to form a demonstration), LLMSRec-Fixed (using a randomly selected demonstration for all test users), and LLMSRec-Nearest (finding the most similar training user as the demonstration). As Section 3.6 shows that more than one demonstration in in-context learning for sequential recommendation does not yield better performance, we do not include few-shot methods in this set of experiments. We however will study how many member demonstrations K is ideal for aggregated demonstration (see Section 5.2). We implement LLMSRec-Syn using three different LLMs, LLaMa2 (Touvron et al., 2023), ChatGPT (OpenAI, 2022) (LLMSRec-Syn), and GPT4 (OpenAI, 2023) (LLMSRec-Syn-4). For the LLMSRec-Syn-4 experiment, which is shown in the last row of Table 2, we used GPT-4 as the base LLM. For all other experiments, including preliminary studies, in-depth analysis, and method comparisons presented in Table 2, we used the same ChatGPT (GPT-3.5-Turbo). To ensure the reliability of our findings, each experiment is conducted 9 times, and the average results are reported. However, we found LLaMa2 unable to follow recommendation instructions and is prone to generating historical\ninteracted items or in-context examples. As a result, we exclude the LLaMa2 results In LLMSRecSyn, we set the number of member users in the aggregated demonstration as {1,2,3,4,5,6,7} and conduct a brute force search to determine the optimal number for each dataset. We set the number of historical items MAXH = 50 and number of candidate items to 20. We analyse some specific test cases of LLMSRec-Syn-4 in the Appendix 8.2.\n# 5.2 Main Results\nThe main experiment results are shown in Table 2, from which we obtain the following findings: ICL one-shot methods with appropriate demonstrations out-perform zero-shot methods. As shown in Table 2, LLMRank-His, LLMSRecFixed, and LLMSRec-Nearest using one training user as demonstration outperform LLMRankSeq on three datasets, except for LLMSRec-Fixed which performs slightly worse than LLMRank-Seq on LastFM-2K. This result suggests that ICL can enhance the LLM\u2019s ability to perform a complex task such as sequential recommendation. Aggregated demonstration, combining multiple member users, allows LLM to effectively gather useful task specific information about the test user within a concise context. Compared to other ICL baselines (i.e., LLMRank-His, LLMSRecFixed, and LLMSRec-Nearest), LLMSRec-Syn achieves the superior one-shot performance across all datasets as shown in Table 2. While Figure 6 shows that having more demonstrations may hurt ICL for sequential recommendation, the idea of incorporating multiple demonstration users into an aggregated demonstration enhances the performance of LLMSRec-Syn. These results illustrate the advantage of aggregated demonstration in accommodating multiple training users within a limited prompt length. LLMSRec-Syn is competitive against supervised methods when the amount of training data is limited. LLMSRec-Syn easily outperforms the simple supervised baseline, Most Popular. While it does not outperform GRU4Rec and SASRec on ML-1M and Games, LLMSRec-Syn surprisingly outperforms all supervised baselines on LastFM2K. One possible reason is that LastFM-2K has sparse information about items after removing duplicate user-item interactions and users/items with less than 5 interactions, making it challenging to train a good supervised model. LLMSRec-Syn using more powerful LLMs\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c91f/c91fb7fd-fd92-43c2-b6cf-aabda9a02243.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Varying number of users (K) in aggregated demonstration.</div>\nmay outperform supervised methods in the future. With rapid advancement of LLM research, LLMSRec-Syn can be further enhanced when more powerful LLM is used. Our results in Table 2 shows that LLMSRec-Syn-4 significantly outperforms LLMSRec-Syn on all the 3 datasets.\n# 5.3 Analysis of Aggregated Demonstrations\nIn this section, we study the recommendation performance when varying the settings of aggregated demonstrations. Analysis of ordering of users and label in the aggregated demonstration can be found in the Appendix 8.1. Impact of number of users in the aggregated demonstration. We evaluate the impact of K (the number of member users) in the aggregated demonstration on LLMSRec-Syn\u2019s performance. We empirically vary K from 2 to 7. As shown in Figure 7, an approximate inverted U-shaped relationship exists between K and NDCG@10/20 performance. Initially, as K increases, there is a noticeable performance increase, suggesting that LLMSRec-Syn benefits from aggregated demonstration. However, beyond some K value, more member users in aggregated demonstration leads to lower performance. This can be explained by more irrelevant training users being incorporated into the aggregated demonstration. Impact of number of aggregated demonstrations. We evaluate the impact of the number of aggregated demonstrations to LLMSRec-Syn by varying the number of aggregated demonstrations from 1 to 4 such that each demonstration involves 2 users (see Figure 8(a)) and 3 users (see Figure 8(b)). For the Games dataset, experimentation with 3 aggregated demonstrations was not possible due to GPT-3.5-Turbo\u2019s input limit. The results show that a single aggregated demonstration outperforms multiple ones, except in the LastFM-2K dataset, where two demonstrations slightly excel.\n<div style=\"text-align: center;\">Table 2: Main results. We report NDCG@5, NDCG@10 and NDCG@20 on ML-1M, LastFM-2K and Games. (Best results in each group of methods are boldfaced and overall best results are underlined).</div>\nSetting\nMethod\nML-1M\nLastFM-2K\nGames\nNDCG@5 NDCG@10 NDCG@20 NDCG@5 NDCG@10 NDCG@20 NDCG@5 NDCG@10 NDCG@20\nSupervised\nMost Popular\n0.3673\n0.4623\n0.4748\n0.4055\n0.4205\n0.4803\n0.2746\n0.3905\n0.4496\nGRU4Rec\n0.7205\n0.7494\n0.7610\n0.3382\n0.3971\n0.4784\n0.6747\n0.7002\n0.7278\nSASRec\n0.7322\n0.7595\n0.7702\n0.4081\n0.4680\n0.5303\n0.6828\n0.7189\n0.7311\nZero-shot\nBM25\n0.1314\n0.2053\n0.3370\n0.1215\n0.1393\n0.3354\n0.2285\n0.3108\n0.4055\nLLMSeqSim\n0.3250\n0.4037\n0.4723\n0.4090\n0.4662\n0.5293\n0.4269\n0.4830\n0.5360\nLLMRank-Seq\n0.3344\n0.3882\n0.4612\n0.5084\n0.5545\n0.6070\n0.3063\n0.3607\n0.4074\nLLMSRec\n0.3339\n0.4087\n0.4723\n0.5126\n0.5602\n0.6057\n0.4070\n0.4555\n0.5103\nOne-shot\nLLMRank-His\n0.3919\n0.4444\n0.5074\n0.5318\n0.5725\n0.6212\n0.4191\n0.4667\n0.5206\nLLMSRec-Fixed\n0.3590\n0.4193\n0.4793\n0.4961\n0.5425\n0.5984\n0.3744\n0.4400\n0.4899\nLLMSRec-Nearest\n0.3842\n0.4382\n0.5017\n0.5249\n0.5697\n0.6197\n0.3975\n0.4388\n0.4994\nLLMSRec-Syn\n0.4267\n0.4813\n0.5334\n0.5554\n0.5918\n0.6371\n0.4989\n0.5334\n0.5869\nLLMSRec-Syn-4\n0.5112\n0.5685\n0.5936\n0.6544\n0.6799\n0.7017\n0.5647\n0.6019\n0.6277\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba8c/ba8c34b0-4ffe-4fa7-8369-bc1f6dec07de.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Varying number of aggregated demonstrations each with: (a) 2 member users, and (b) 3 member users.</div>\nFigure 8: Varying number of aggregated demonstrations each with: (a) 2 member users, and (b) 3 member users.\n# 6 Conclusion\nThis paper investigates in-context learning (ICL) for LLM-based sequential recommendation. Our study identifies key factors such as instruction format and demonstration selection that influence ICL\u2019s effectiveness. We further introduce the LLMSRec-Syn method which utilizes our proposed aggregated demonstration to efficiently incorporate relevant information from multiple training users. Tested on three datasets, LLMSRec-Syn consistently outperforms existing LLM-based sequential recommendation methods. Future work includes a detailed analysis of LLMSRec-Syn\u2019s unexpected success compared to some supervised methods and the optimization of aggregated demonstration strategies.\n# 7 Limitations\nWhile this paper considers several factors in applying LLMs to sequential recommendation and proposes a new demonstration concept known as aggregated demonstration, there are still some limitations yet to be addressed. Firstly, the wording of LLMSRec-Syn prompt is manually handcrafted and may not be optimal. This concern\nis also mentioned in works on prompt optimization (Yang et al., 2023; Deng et al., 2022; Pryzant et al., 2023). However, determining the optimal prompt wording typically requires feedback (such as validation set results (Yang et al., 2023)), carefully designed reward function (Deng et al., 2022), or textual feedback from large language models to iteratively update the initial prompt (Pryzant et al., 2023). Moreover, when a user\u2019s historical items are too many, LLMSRec-Syn may still suffer from the issue of long text. Furthermore, the aggregated demonstration method, while mitigating input length constraints, might oversimplify the user preferences, potentially resulting in less personalized recommendations. Moreover, the nonutilization of existing user datasets for pretraining or fine-tuning, due to LLM size constraints, limits the adaptability and fine-tuning of the model to specific recommendation contexts. These limitations highlight the need for further research in optimizing LLMs for complex, dynamic tasks such as sequential recommendation, where user context and historical data play crucial roles.\n# References\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022. Incontext examples selection for machine translation. arXiv preprint arXiv:2212.02437. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nlearners. Advances in neural information processing systems, 33:1877\u20131901. Iv\u00e1n Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. 2nd workshop on information heterogeneity and fusion in recommender systems (hetrec 2011). In Proceedings of the 5th ACM conference on Recommender systems, RecSys 2011, New York, NY, USA. ACM. Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2021. Sequential recommendation with graph neural networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 378\u2013387. Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023. How many demonstrations do you need for in-context learning? In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11149\u201311159. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548. Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chatrec: Towards interactive and explainable llmsaugmented recommender system. arXiv preprint arXiv:2303.14524. Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 1096\u20131102. Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pages 191\u2013200. IEEE. Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845.\nWang-Cheng Kang and Julian McAuley. 2018. Selfattentive sequential recommendation. In 2018 IEEE International Conference on Data Mining (ICDM), pages 197\u2013206. IEEE. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for incontext learning. arXiv preprint arXiv:2305.04320. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804. Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023a. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 43\u201352. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837. OpenAI. 2022. Introducing chatgpt. https://openai. com/blog/chatgpt. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Aleksandr Petrov and Craig Macdonald. 2022. Effective and efficient training for sequential recommendation using recency sampling. In Proceedings of the 16th ACM Conference on Recommender Systems, pages 81\u201391. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. Bpr: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618. Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web, pages 811\u2013820. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633. Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing. arXiv preprint arXiv:2210.13693. Wenzhuo Song, Shoujin Wang, Yan Wang, and Shengsheng Wang. 2021. Next-item recommendations in short sessions. In Proceedings of the 15th ACM Conference on Recommender Systems, pages 282\u2013291. Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 565\u2013573. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091. Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, and Yong Wang. 2023b. Llm4vis: Explainable visualization recommendation using chatgpt. arXiv preprint arXiv:2310.07652. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846.\nXu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020. Contrastive learning for sequential recommendation. arXiv preprint arXiv:2010.14395. Yue Xu, Hao Chen, Zefan Wang, Jianwen Yin, Qijie Shen, Dimin Wang, Feiran Huang, Lixiang Lai, Tao Zhuang, Junfeng Ge, et al. 2023. Multi-factor sequential re-ranking with perception-aware diversification. arXiv preprint arXiv:2305.12420. Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and Xiangnan He. 2023. Large language model can interpret latent space of sequential recommender. arXiv preprint arXiv:2310.20487. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An empirical study of gpt-3 for few-shot knowledgebased vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3081\u2013 3089. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. arXiv preprint arXiv:2205.12685. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. 2023a. Tempera: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations. Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. 2023b. What makes good examples for visual in-context learning? arXiv preprint arXiv:2301.13670. Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, and Xinyu Dai. 2023. Dynamic demonstrations controller for in-context learning. arXiv preprint arXiv:2310.00385. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3986/398672b6-31fb-46c4-b3de-8f2a9b8eed0a.png\" style=\"width: 50%;\"></div>\nTable 3: Further comparison of different tasks with candidates. We have developed two methods with the same information as T3: (1) T1 w/ Candidate (adding candidate items in the T1 prompt) and (2) T2 w/ Candidate (adding candidate items in the T2 prompt).\nML-1M\nT1\nT1 w/ Cand.\nT2\nT2 w/ Cand.\nT3\nNDCG@10 0.3640\n0.3766\n0.3776\n0.3972\n0.4584\nNDCG@20 0.4193\n0.4420\n0.4384\n0.4510\n0.5077\n# 8 Appendix\n# 8.1 More In-Depth Analysis\nImpact of user order in aggregated demonstrations. We experiment with 3 possible orders of member users: (i) Random (randomly selects historical items and next-items from the selected users to construct the aggregated demonstration), (ii) Right2Left (the reverse order of demonstration users in constructing an aggregated demonstration in LLMSRec-Syn in Section 4), and (iii) Left2Right (the user order used in the LLMSRecSyn). Figure 9(a) illustrates that Left2Right and Right2Left are the most and least ideal orders respectively. The performance of Random is naturally sandwiched in between. Impact of labeled next-items in the aggregated demonstration. According to Min et al. (2022), ground truth labels are not important for in-context learning. To investigate this claim for ICL-based sequential recommendation, we compare LLMSRecSyn using ground truth next-items in the aggregated demonstration (referred to as \u201cGold Labels\u201d) with that using random non-ground truth next-items (referred to as \u201cRandom Labels\u201d). Our results in Figure 9(b) clearly indicate that ground truth nextitems are required to yield better performance contradicting the claim by Min et al. (2022). This could possibly be explained by the complexity of sequential recommendation task. Further comparison of different tasks with can-\nTable 4: Results of fine-tuned LLaMa2 with LoRA for in-context sequential recommendation. Regular means LLaMa2-LoRA-Regular. Aggregated means LLaMa2LoRA-Aggregated.\nML-1M\nRegular\nAggregated\nLLMSRec-Syn\nNDCG@10\n0.3640\n0.3766\n0.3776\nNDCG@20\n0.4193\n0.4420\n0.4384\ndidates. As shown in Table 3, we observed that T1(T2) with candidate items in the prompt performs better than T1(T2). These results support the reviewer\u2019s comment that including more information in the prompt will enhance the performance. However, in this more fair comparison, T3 still outperforms T1 with candidate items in the prompt and T2 with candidate items in the prompt. Could fine-tuned LLaMa2 improve the performance of in-context sequential recommendation? We initially used a training dataset of 150 data examples to train LLaMa2 with LoRA, which we referred to as LLaMa2-LoRA-Regular. For each training data example in this training dataset, the target output is the ranking of the candidate items for a training user. The input consists of a regular demonstration example, as well as historical items and candidate items from the training user. After training, we evaluated the performance of LLaMa2-LoRA-Regular using the same 50 test users as ChatGPT-based LLMSRec-Syn (0.5283 NDCG@10). As shown in Table 4, the results showed that LLaMa2-LoRA-Regular achieved a NDCG@10 score of 0.2344. To investigate whether aggregated demonstration helps to train a better model compared to regular demonstrations, we prepared a training dataset using aggregated demonstrations instead of regular demonstrations. We trained LLaMa2 with LoRA using this dataset, which we call LLaMa2-LoRA-Aggregated. LLaMa2-LoRAAggregated achieved a NDCG@10 score of 0.3432 on the same test set. Although the initial study indicates that LLaMa-LoRA performs worse than ChatGPT, the fine-tuned LLaMa2-LoRA appears to have the potential to enable in-context learningbased sequential recommendation and aggregated demonstration can help to train a better model.\n# 8.2 Case Study Examples\nIn this section, we provide comparative examples of one-shot LLMSRec-Syn (Table 5), one-shot LLMSRec-Nearest (Table 6), one-shot LLMSRec-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5135/5135d282-00e6-4213-9eb0-97b905fd4c56.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Construction of in-context learning prompt with aggregated demonstration for sequential recommenda tion.</div>\nFixed (Table 7), and zero-shot LLMSRec (Table 8). Observations show that LLMSRec-Syn ranks the ground truth movie higher than the other methods. Compared to Nearest and Fixed demonstrations, the aggregated demonstration allows the LLM to better identify a user\u2019s interests and align the ranking with those interests. Without demonstration, zero-shot LLMSRec relies solely on the LLM\u2019s knowledge and performs poorly. This suggests that LLMs can learn from demonstrations to improve in areas where they might not originally be good at.\nTable 5: Example of the one-shot LLMSRec-Syn on the ML-1M dataset. Ground truth recommendation is highlighted in Maroon.\nAggregated Demonstration Example: The User\u2019s Movie Profile: - Watched Movies: [\u20180. Caddyshack\u2019, \u20181. Glory\u2019, \u20182. A Bug\u2019s Life\u2019, \u20183. Star Trek VI: The Undiscovered Country\u2019, \u20184. Indiana Jones and the Last Crusade\u2019, \u20185. The Color of Money\u2019, \u20186. Raging Bull\u2019, \u20187. Edward Scissorhands\u2019, \u20188. Kramer Vs. Kramer\u2019, \u20189. Roger & Me\u2019, \u201810. Romancing the Stone\u2019, \u201811. Full Metal Jacket\u2019, \u201812. The Shining\u2019, \u201813. Easy Rider\u2019, \u201814. Glory\u2019, \u201815. The Color Purple\u2019, \u201816. Die Hard\u2019, \u201817. Who Framed Roger Rabbit?\u2019, \u201818. Ghostbusters\u2019, \u201819. The Right Stuff\u2019, \u201820. No Way Out\u2019, \u201821. The Breakfast Club\u2019, \u201822. Dead Poets Society\u2019, \u201823. One True Thing\u2019, \u201824. Full Metal Jacket\u2019, \u201825. U2: Rattle and Hum\u2019, \u201826. Caddyshack\u2019, \u201827. Jaws\u2019, \u201828. Get Shorty\u2019, \u201829. A Fish Called Wanda\u2019, \u201830. Star Trek IV: The Voyage Home\u2019, \u201831. Brazil\u2019, \u201832. The Fugitive\u2019, \u201833. Willy Wonka and the Chocolate Factory\u2019, \u201834. Ghostbusters\u2019, \u201835. The Blues Brothers\u2019, \u201836. Stripes\u2019, \u201837. Stand by Me\u2019, \u201838. Pleasantville\u2019, \u201839. Bull Durham\u2019, \u201840. Risky Business\u2019, \u201841. Indiana Jones and the Last Crusade\u2019, \u201842. Star Wars: Episode VI - Return of the Jedi\u2019, \u201843. Witness\u2019, \u201844. Brazil\u2019, \u201845. Predator\u2019, \u201846. The Terminator\u2019, \u201847. Grosse Pointe Blank\u2019, \u201848. Contact\u2019, \u201849. The Princess Bride\u2019] The User\u2019s Potential Matches: - Candidate Movies: [\u20180. Excalibur\u2019, \u20181. The Fox and the Hound\u2019, \u20182. The Cable Guy\u2019, \u20183. Cat on a Hot Tin Roof\u2019, \u20184. Slingshot, The (K\u00e5disbellan )\u2019, \u20185. Institute Benjamenta, or This Dream People Call Human Life\u2019, \u20186. Prefontaine\u2019, \u20187. Star Wars: Episode IV - A New Hope\u2019, \u20188. 8 Seconds\u2019, \u20189. Two Family House\u2019, \u201810. American Graffiti\u2019, \u201811. Seven Days in May\u2019, \u201812. The English Patient\u2019, \u201813. Jurassic Park\u2019, \u201814. Meet the Parents\u2019, \u201815. Weekend at Bernie\"s\u2019, \u201816. Kurt & Courtney\u2019, \u201817. Children of a Lesser God\u2019, \u201818. The Haunting\u2019, \u201819. Love and Basketball\u2019] Based on the user\u2019s watched movies, please rank the candidate movies that align closely with the user\u2019s preferences. - You ONLY rank the given Candidate Movies. - You DO NOT generate movies from Watched Movies. Present your response in the format below: 1. [Top Recommendation (Candidate Movie)] 2. [2nd Recommendation (Candidate Movie)] ... 20. [20th Recommendation (Candidate Movie)] Answer: 1. Children of a Lesser God 2. Excalibur 3. Jurassic Park 4. American Graffiti 5. Meet the Parents 6. The Haunting 7. Institute Benjamenta, or This Dream People Call Human Life 8. Cat on a Hot Tin Roof 9. Slingshot, The (K\u00e5disbellan ) 10. Prefontaine 11. The English Patient 12. Love and Basketball 13. The Fox and the Hound 14. The Cable Guy 15. Two Family House 16. Star Wars: Episode IV - A New Hope 17. Weekend at Bernie\"s 18. Seven Days in May 19. 8 Seconds 20. Kurt & Courtney\nTest User:\n- Watched Movies: [\u20180. Nikita (La Femme Nikita)\u2019, \u20181. Wallace & Gromit: The Best of Aardman Animation\u2019, \u20182. Unforgiven\u2019, \u20183. Batman: Mask of the Phantasm\u2019, \u20184. The Fugitive\u2019, \u20185. Out of Sight\u2019, \u20186. Twelve Monkeys\u2019, \u20187. GoodFellas\u2019, \u20188. Fight Club\u2019, \u20189. Groundhog Day\u2019, \u201810. A Bug\"s Life\u2019, \u201811. Tombstone\u2019, \u201812. Aladdin\u2019, \u201813. Beauty and the Beast\u2019, \u201814. Grosse Pointe Blank\u2019, \u201815. Election\u2019, \u201816. Leaving Las Vegas\u2019, \u201817. Total Recall\u2019, \u201818. A Few Good Men\u2019, \u201819. Pleasantville\u2019, \u201820. Jerry Maguire\u2019, \u201821. Pretty Woman\u2019, \u201822. Contact\u2019, \u201823. True Lies\u2019, \u201824. Waking Ned Devine\u2019, \u201825. Romeo Must Die\u2019, \u201826. Mission: Impossible 2\u2019, \u201827. Mission to Mars\u2019, \u201828. Killer, The (Die xue shuang xiong)\u2019, \u201829. Blade Runner\u2019, \u201830. The Princess Bride\u2019, \u201831. Brazil\u2019, \u201832. Henry V\u2019, \u201833. Amadeus\u2019, \u201834. The Right Stuff\u2019, \u201835. The Terminator\u2019, \u201836. Stand by Me\u2019, \u201837. Back to the Future\u2019, \u201838. This Is Spinal Tap\u2019, \u201839. Gandhi\u2019, \u201840. Star Trek: The Wrath of Khan\u2019, \u201841. Ghostbusters\u2019, \u201842. Mad Max 2 (a.k.a. The Road Warrior)\u2019, \u201843. A Fish Called Wanda\u2019, \u201844. Trading Places\u2019, \u201845. Chariots of Fire\u2019, \u201846. Time Bandits\u2019, \u201847. Who Framed Roger Rabbit?\u2019, \u201848. Indiana Jones and the Last Crusade\u2019, \u201849. Die Hard\u2019] The User\u2019s Potential Matches: - Candidate Movies: [\u20180. Sorority House Massacre\u2019, \u20181. Arguing the World\u2019, \u20182. Cop Land\u2019, \u20183. The Inkwell\u2019, \u20184. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar)\u2019, \u20185. Airplane!\u2019, \u20186. Pok\u00e9mon the Movie 2000\u2019, \u20187. Johnny 100 Pesos\u2019, \u20188. Happy, Texas\u2019, \u20189. Arlington Road\u2019, \u201810. The Prophecy II\u2019, \u201811. Prom Night\u2019, \u201812. The Commitments\u2019, \u201813. Federal Hill\u2019, \u201814. Another Stakeout\u2019, \u201815. The Postman Always Rings Twice\u2019, \u201816. The Crow\u2019, \u201817. Shanghai Surprise\u2019, \u201818. Patriot Games\u2019, \u201819. Modulations\u2019] Based on the user\u2019s watched movies, please rank the candidate movies that align closely with John\u2019s preferences. - You ONLY rank the given Candidate Movies. - You DO NOT generate movies from Watched Movies. Present your response in the format below: 1. [Top Recommendation (Candidate Movie)] 2. [2nd Recommendation (Candidate Movie)] ... 20. [20th Recommendation (Candidate Movie)] Answer: 1. The Commitments 2. Cop Land 3. Airplane! 4. The Crow 5. Patriot Games 6. The Postman Always Rings Twice 7. Shanghai Surprise 8. Arlington Road 9. Happy, Texas 10. Another Stakeout 11. The Inkwell 12. Modulations 13. Johnny 100 Pesos 14. Sorority House Massacre 15. Pok\u00e9mon the Movie 2000 16. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar) 17. The Prophecy II 18. Federal Hill 19. Prom Night 20. Arguing the World\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ec5/0ec59756-53e7-494f-a7c9-c5d4ee23b894.png\" style=\"width: 50%;\"></div>\nTest User:\nLearn from the above demonstration examples to solve the following test example. The User\u2019s Movie Profile:\n- Watched Movies: [\u20180. Nikita (La Femme Nikita)\u2019, \u20181. Wallace & Gromit: The Best of Aardman Animation\u2019, \u20182. Unforgiven\u2019, \u20183. Batman: Mask of the Phantasm\u2019, \u20184. The Fugitive\u2019, \u20185. Out of Sight\u2019, \u20186. Twelve Monkeys\u2019, \u20187. GoodFellas\u2019, \u20188. Fight Club\u2019, \u20189. Groundhog Day\u2019, \u201810. A Bug\"s Life\u2019, \u201811. Tombstone\u2019, \u201812. Aladdin\u2019, \u201813. Beauty and the Beast\u2019, \u201814. Grosse Pointe Blank\u2019, \u201815. Election\u2019, \u201816. Leaving Las Vegas\u2019, \u201817. Total Recall\u2019, \u201818. A Few Good Men\u2019, \u201819. Pleasantville\u2019, \u201820. Jerry Maguire\u2019, \u201821. Pretty Woman\u2019, \u201822. Contact\u2019, \u201823. True Lies\u2019, \u201824. Waking Ned Devine\u2019, \u201825. Romeo Must Die\u2019, \u201826. Mission: Impossible 2\u2019, \u201827. Mission to Mars\u2019, \u201828. Killer, The (Die xue shuang xiong)\u2019, \u201829. Blade Runner\u2019, \u201830. The Princess Bride\u2019, \u201831. Brazil\u2019, \u201832. Henry V\u2019, \u201833. Amadeus\u2019, \u201834. The Right Stuff\u2019, \u201835. The Terminator\u2019, \u201836. Stand by Me\u2019, \u201837. Back to the Future\u2019, \u201838. This Is Spinal Tap\u2019, \u201839. Gandhi\u2019, \u201840. Star Trek: The Wrath of Khan\u2019, \u201841. Ghostbusters\u2019, \u201842. Mad Max 2 (a.k.a. The Road Warrior)\u2019, \u201843. A Fish Called Wanda\u2019, \u201844. Trading Places\u2019, \u201845. Chariots of Fire\u2019, \u201846. Time Bandits\u2019, \u201847. Who Framed Roger Rabbit?\u2019, \u201848. Indiana Jones and the Last Crusade\u2019, \u201849. Die Hard\u2019] The User\u2019s Potential Matches: - Candidate Movies: [\u20180. Sorority House Massacre\u2019, \u20181. Arguing the World\u2019, \u20182. Cop Land\u2019, \u20183. The Inkwell\u2019, \u20184. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar)\u2019, \u20185. Airplane!\u2019, \u20186. Pok\u00e9mon the Movie 2000\u2019, \u20187. Johnny 100 Pesos\u2019, \u20188. Happy, Texas\u2019, \u20189. Arlington Road\u2019, \u201810. The Prophecy II\u2019, \u201811. Prom Night\u2019, \u201812. The Commitments\u2019, \u201813. Federal Hill\u2019, \u201814. Another Stakeout\u2019, \u201815. The Postman Always Rings Twice\u2019, \u201816. The Crow\u2019, \u201817. Shanghai Surprise\u2019, \u201818. Patriot Games\u2019, \u201819. Modulations\u2019] Based on the user\u2019s watched movies, please rank the candidate movies that align closely with John\u2019s preferences. - You ONLY rank the given Candidate Movies. - You DO NOT generate movies from Watched Movies. Present your response in the format below: 1. [Top Recommendation (Candidate Movie)] 2. [2nd Recommendation (Candidate Movie)] ... 20. [20th Recommendation (Candidate Movie)] Answer: 1. Arlington Road 2. Cop Land 3. The Crow 4. Patriot Games 5. The Postman Always Rings Twice 6. The Commitments 7. Airplane! 8. Another Stakeout 9. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar) 10. Shanghai Surprise 11. Happy, Texas 12. Modulations 13. The Inkwell 14. Johnny 100 Pesos 15. Sorority House Massacre 16. Arguing the World 17. Prom Night 18. Federal Hill 19. Pok\u00e9mon the Movie 2000 20. The Prophecy II\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8008/800865be-98d8-488b-bc50-52c6ecab4db9.png\" style=\"width: 50%;\"></div>\nTest User:\nLearn from the above demonstration examples to solve the following test example. The User\u2019s Movie Profile:\n- Watched Movies: [\u20180. Nikita (La Femme Nikita)\u2019, \u20181. Wallace & Gromit: The Best of Aardman Animation\u2019, \u20182. Unforgiven\u2019, \u20183. Batman: Mask of the Phantasm\u2019, \u20184. The Fugitive\u2019, \u20185. Out of Sight\u2019, \u20186. Twelve Monkeys\u2019, \u20187. GoodFellas\u2019, \u20188. Fight Club\u2019, \u20189. Groundhog Day\u2019, \u201810. A Bug\"s Life\u2019, \u201811. Tombstone\u2019, \u201812. Aladdin\u2019, \u201813. Beauty and the Beast\u2019, \u201814. Grosse Pointe Blank\u2019, \u201815. Election\u2019, \u201816. Leaving Las Vegas\u2019, \u201817. Total Recall\u2019, \u201818. A Few Good Men\u2019, \u201819. Pleasantville\u2019, \u201820. Jerry Maguire\u2019, \u201821. Pretty Woman\u2019, \u201822. Contact\u2019, \u201823. True Lies\u2019, \u201824. Waking Ned Devine\u2019, \u201825. Romeo Must Die\u2019, \u201826. Mission: Impossible 2\u2019, \u201827. Mission to Mars\u2019, \u201828. Killer, The (Die xue shuang xiong)\u2019, \u201829. Blade Runner\u2019, \u201830. The Princess Bride\u2019, \u201831. Brazil\u2019, \u201832. Henry V\u2019, \u201833. Amadeus\u2019, \u201834. The Right Stuff\u2019, \u201835. The Terminator\u2019, \u201836. Stand by Me\u2019, \u201837. Back to the Future\u2019, \u201838. This Is Spinal Tap\u2019, \u201839. Gandhi\u2019, \u201840. Star Trek: The Wrath of Khan\u2019, \u201841. Ghostbusters\u2019, \u201842. Mad Max 2 (a.k.a. The Road Warrior)\u2019, \u201843. A Fish Called Wanda\u2019, \u201844. Trading Places\u2019, \u201845. Chariots of Fire\u2019, \u201846. Time Bandits\u2019, \u201847. Who Framed Roger Rabbit?\u2019, \u201848. Indiana Jones and the Last Crusade\u2019, \u201849. Die Hard\u2019] The User\u2019s Potential Matches: - Candidate Movies: [\u20180. Sorority House Massacre\u2019, \u20181. Arguing the World\u2019, \u20182. Cop Land\u2019, \u20183. The Inkwell\u2019, \u20184. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar)\u2019, \u20185. Airplane!\u2019, \u20186. Pok\u00e9mon the Movie 2000\u2019, \u20187. Johnny 100 Pesos\u2019, \u20188. Happy, Texas\u2019, \u20189. Arlington Road\u2019, \u201810. The Prophecy II\u2019, \u201811. Prom Night\u2019, \u201812. The Commitments\u2019, \u201813. Federal Hill\u2019, \u201814. Another Stakeout\u2019, \u201815. The Postman Always Rings Twice\u2019, \u201816. The Crow\u2019, \u201817. Shanghai Surprise\u2019, \u201818. Patriot Games\u2019, \u201819. Modulations\u2019] Based on the user\u2019s watched movies, please rank the candidate movies that align closely with John\u2019s preferences. - You ONLY rank the given Candidate Movies. - You DO NOT generate movies from Watched Movies. Present your response in the format below: 1. [Top Recommendation (Candidate Movie)] 2. [2nd Recommendation (Candidate Movie)] ... 20. [20th Recommendation (Candidate Movie)] Answer: 1. The Crow 2. Shanghai Surprise 3. The Commitments 4. Arlington Road 5. The Postman Always Rings Twice 6. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar) 7. Cop Land 8. Happy, Texas 9. Another Stakeout 10. Airplane! 11. Federal Hill 12. Johnny 100 Pesos 13. The Prophecy II 14. Modulations 15. Patriot Games 16. Pok\u00e9mon the Movie 2000 17. Sorority House Massacre 18. Prom Night 19. Arguing the World 20. The Inkwell\nLearn from the above demonstration examples to solve the following test example. The User\u2019s Movie Profile: - Watched Movies: [\u20180. Nikita (La Femme Nikita)\u2019, \u20181. Wallace & Gromit: The Best of Aardman Animation\u2019, \u20182. Unforgiven\u2019, \u20183. Batman: Mask of the Phantasm\u2019, \u20184. The Fugitive\u2019, \u20185. Out of Sight\u2019, \u20186. Twelve Monkeys\u2019, \u20187. GoodFellas\u2019, \u20188. Fight Club\u2019, \u20189. Groundhog Day\u2019, \u201810. A Bug\"s Life\u2019, \u201811. Tombstone\u2019, \u201812. Aladdin\u2019, \u201813. Beauty and the Beast\u2019, \u201814. Grosse Pointe Blank\u2019, \u201815. Election\u2019, \u201816. Leaving Las Vegas\u2019, \u201817. Total Recall\u2019, \u201818. A Few Good Men\u2019, \u201819. Pleasantville\u2019, \u201820. Jerry Maguire\u2019, \u201821. Pretty Woman\u2019, \u201822. Contact\u2019, \u201823. True Lies\u2019, \u201824. Waking Ned Devine\u2019, \u201825. Romeo Must Die\u2019, \u201826. Mission: Impossible 2\u2019, \u201827. Mission to Mars\u2019, \u201828. Killer, The (Die xue shuang xiong)\u2019, \u201829. Blade Runner\u2019, \u201830. The Princess Bride\u2019, \u201831. Brazil\u2019, \u201832. Henry V\u2019, \u201833. Amadeus\u2019, \u201834. The Right Stuff\u2019, \u201835. The Terminator\u2019, \u201836. Stand by Me\u2019, \u201837. Back to the Future\u2019, \u201838. This Is Spinal Tap\u2019, \u201839. Gandhi\u2019, \u201840. Star Trek: The Wrath of Khan\u2019, \u201841. Ghostbusters\u2019, \u201842. Mad Max 2 (a.k.a. The Road Warrior)\u2019, \u201843. A Fish Called Wanda\u2019, \u201844. Trading Places\u2019, \u201845. Chariots of Fire\u2019, \u201846. Time Bandits\u2019, \u201847. Who Framed Roger Rabbit?\u2019, \u201848. Indiana Jones and the Last Crusade\u2019, \u201849. Die Hard\u2019] The User\u2019s Potential Matches: - Candidate Movies: [\u20180. Sorority House Massacre\u2019, \u20181. Arguing the World\u2019, \u20182. Cop Land\u2019, \u20183. The Inkwell\u2019, \u20184. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar)\u2019, \u20185. Airplane!\u2019, \u20186. Pok\u00e9mon the Movie 2000\u2019, \u20187. Johnny 100 Pesos\u2019, \u20188. Happy, Texas\u2019, \u20189. Arlington Road\u2019, \u201810. The Prophecy II\u2019, \u201811. Prom Night\u2019, \u201812. The Commitments\u2019, \u201813. Federal Hill\u2019, \u201814. Another Stakeout\u2019, \u201815. The Postman Always Rings Twice\u2019, \u201816. The Crow\u2019, \u201817. Shanghai Surprise\u2019, \u201818. Patriot Games\u2019, \u201819. Modulations\u2019] Based on the user\u2019s watched movies, please rank the candidate movies that align closely with John\u2019s preferences. - You ONLY rank the given Candidate Movies. - You DO NOT generate movies from Watched Movies. Present your response in the format below: 1. [Top Recommendation (Candidate Movie)] 2. [2nd Recommendation (Candidate Movie)] ... 20. [20th Recommendation (Candidate Movie)] Answer: 1. The Commitments 2. Patriot Games 3. Arlington Road 4. Cop Land 5. The Crow 6. The Postman Always Rings Twice 7. Shanghai Surprise 8. Another Stakeout 9. Modulations 10. Pok\u00e9mon the Movie 2000 11. The Inkwell 12. Happy, Texas 13. The Prophecy II 14. Johnny 100 Pesos 15. Lovers of the Arctic Circle, The (Los Amantes del C\u00edrculo Polar) 16. Arguing the World 17. Federal Hill 18. Prom Night 19. Sorority House Massacre 20. Airplane!\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of leveraging large language models (LLMs) for sequential recommendation tasks through in-context learning (ICL). Previous methods have shown limitations in accuracy and effectiveness, necessitating a new approach to improve performance in this area.",
        "problem": {
            "definition": "The problem is defined as ranking the next item a user is likely to interact with based on their past interactions and a set of candidate items, with the aim of maximizing the rank of the ground truth next item.",
            "key obstacle": "A key obstacle is that existing ICL methods tend to perform poorly compared to supervised learning methods, particularly due to the complex nature of sequential recommendation tasks."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that combining multiple demonstration users into one aggregated demonstration can provide a more effective learning context for LLMs, reducing redundancy and improving information retention.",
            "opinion": "The proposed idea, LLMSRec-Syn, involves using aggregated demonstrations to enhance the sequential recommendation process by integrating relevant information from multiple users into a single, concise prompt.",
            "innovation": "The primary innovation of LLMSRec-Syn lies in its method of aggregating multiple user demonstrations, which contrasts with existing approaches that typically rely on individual demonstrations, thus improving the efficiency and effectiveness of the learning process."
        },
        "method": {
            "method name": "LLMSRec-Syn",
            "method abbreviation": "LLMSRec-Syn",
            "method definition": "LLMSRec-Syn is a method that utilizes aggregated demonstrations from multiple users to inform the ranking of candidate items in sequential recommendations.",
            "method description": "The method aggregates user interactions and candidate items to create a compact demonstration that captures essential user preferences.",
            "method steps": [
                "Select K demonstration users similar to the test user based on similarity scores.",
                "Construct the aggregated demonstration by compiling historical interactions and candidate items from the selected users.",
                "Use the aggregated demonstration in the prompt for the LLM to generate recommendations."
            ],
            "principle": "The effectiveness of LLMSRec-Syn is based on the principle that providing a richer, more contextually relevant input through aggregated demonstrations enhances the LLM's ability to make accurate recommendations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using three datasets: MovieLens-1M, LastFM-2K, and Games, with a focus on evaluating the performance of LLMSRec-Syn against various baseline methods.",
            "evaluation method": "Performance was assessed using the NDCG@N metric (N=5, 10, 20) across multiple runs to ensure reliability, comparing LLMSRec-Syn with other supervised and zero-shot methods."
        },
        "conclusion": "The results demonstrate that LLMSRec-Syn consistently outperforms existing LLM-based sequential recommendation methods, highlighting the advantages of using aggregated demonstrations for improved performance.",
        "discussion": {
            "advantage": "The key advantages of LLMSRec-Syn include its ability to effectively utilize multiple user demonstrations, thereby enhancing the contextual relevance of recommendations without significantly increasing prompt length.",
            "limitation": "One limitation is that the method may oversimplify user preferences due to aggregation, potentially leading to less personalized recommendations.",
            "future work": "Future research directions include optimizing the aggregated demonstration strategies and exploring ways to fine-tune LLMs for specific recommendation contexts."
        },
        "other info": {
            "code availability": "The code for LLMSRec-Syn is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.",
            "dataset statistics": {
                "ML-1M": {
                    "users": 6040,
                    "items": 3706,
                    "interactions": 1000209
                },
                "LastFM-2K": {
                    "users": 1143,
                    "items": 11854,
                    "interactions": 68436
                },
                "Games": {
                    "users": 50547,
                    "items": 16859,
                    "interactions": 389718
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses leveraging large language models (LLMs) for sequential recommendation tasks through in-context learning (ICL)."
        },
        {
            "section number": "1.2",
            "key information": "Existing ICL methods have shown limitations in accuracy and effectiveness, necessitating a new approach to improve performance in sequential recommendations."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea, LLMSRec-Syn, enhances the sequential recommendation process by integrating relevant information from multiple users into a single, concise prompt."
        },
        {
            "section number": "3.1",
            "key information": "The method LLMSRec-Syn aggregates user interactions and candidate items to create a compact demonstration that captures essential user preferences."
        },
        {
            "section number": "3.2",
            "key information": "The principle behind LLMSRec-Syn is that providing a richer, more contextually relevant input through aggregated demonstrations enhances the LLM's ability to make accurate recommendations."
        },
        {
            "section number": "5.1",
            "key information": "The experiments evaluate LLMSRec-Syn's performance in sequential recommendation tasks using datasets like MovieLens-1M, LastFM-2K, and Games."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of LLMSRec-Syn is that the method may oversimplify user preferences due to aggregation, potentially leading to less personalized recommendations."
        },
        {
            "section number": "6.4",
            "key information": "Future research directions include optimizing the aggregated demonstration strategies and exploring ways to fine-tune LLMs for specific recommendation contexts."
        }
    ],
    "similarity_score": 0.7096954273093594,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/The Whole is Better than the Sum_ Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation.json"
}