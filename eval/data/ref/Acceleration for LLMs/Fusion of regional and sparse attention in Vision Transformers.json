{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.08859",
    "title": "Fusion of regional and sparse attention in Vision Transformers",
    "abstract": "Modern vision transformers leverage visually inspired local interaction between pixels through attention computed within window or grid regions, in contrast to the global attention employed in the original ViT. Regional attention restricts pixel interactions within specific regions, while sparse attention disperses them across sparse grids. These differing approaches pose a challenge between maintaining hierarchical relationships vs. capturing a global context. In this study, drawing inspiration from atrous convolution, we propose Atrous Attention, a blend of regional and sparse attention that dynamically integrates both local and global information while preserving hierarchical structures. Based on this, we introduce a versatile, hybrid vision transformer backbone called ACC-ViT, tailored for standard vision tasks. Our compact model achieves approximately 84% accuracy on ImageNet-1K with fewer than 28.5 million parameters, outperforming the state-of-the-art MaxViT by 0.42% while requiring 8.4% fewer parameters.",
    "bib_name": "ibtehaz2024fusionregionalsparseattention",
    "md_text": "# Fusion of regional and sparse attention in Vision Transformers\nNabil Ibtehaz1,\u2217 Ning Yan2 Masood Mortazavi2 Daisuke Kihara1\n13 Jun 2024\n# Abstract\nModern vision transformers leverage visually inspired local interaction between pixels through attention computed within window or grid regions, in contrast to the global attention employed in the original ViT. Regional attention restricts pixel interactions within specific regions, while sparse attention disperses them across sparse grids. These differing approaches pose a challenge between maintaining hierarchical relationships vs. capturing a global context. In this study, drawing inspiration from atrous convolution, we propose Atrous Attention, a blend of regional and sparse attention that dynamically integrates both local and global information while preserving hierarchical structures. Based on this, we introduce a versatile, hybrid vision transformer backbone called ACC-ViT, tailored for standard vision tasks. Our compact model achieves approximately 84% accuracy on ImageNet-1K with fewer than 28.5 million parameters, outperforming the state-of-the-art MaxViT by 0.42% while requiring 8.4% fewer parameters.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b59e/b59e818e-b995-4f39-bf98-6f7c5f411fc0.png\" style=\"width: 50%;\"></div>\narXiv:2406.08859v1\n# 1. Introduction\nThe early Vision Transformers (ViT) explored the feasibility of leveraging the seemingly infinite scalability [9] of text transformers for processing images but lacked sufficient inductive bias and were devoid of any vision-specific adjustments [27]. Hence, they fell short to state-of-the-art CNN models [10]. One particular aspect those former ViTs overlooked was local patterns, which usually carry strong contextual information [17]. This led to the development of windowed attention, proposed by Swin transformer [18], the first truly competent vision transformer model.This concept went through further innovations and eventually deviated towards two directions, namely, regional and sparse attention. In regional attention, windows of different sizes are considered to compute local attention at different scales [3, 6, 34], whereas sparse attention computes a simplified global attention among pixels spread across a sparse grid\n* Work done as an intern at Futurewei Technologies Inc.\n[29\u201331]. Regional attention contains a sense of hierarchy [3], but the global view is harshly limited due to only considering a couple of regions to account for computational complexity [34]. On the contrary, sparse attention can compute a better approximate global context with reasonable computational cost, but sacrifices nested interaction across hierarchies [30]. Although hierarchical information is immensely valuable [35], owing to the access to a richer global context, sparse attention is used in state-of-the-art models and a relatively small-sized window attention can compensate for the limited local interaction [29].\n<div style=\"text-align: center;\">Figure 1. Simplified illustrations of different types of windowed attention mechanisms.</div>\nIn Fig. 1, we have presented simplified examples of regional and sparse attention. In regional attention (Fig. 1a), the cat is identified by two patches. The blue and the green patches inspect the head and most of the cat\u2019s body, respectively. By combining the information hierarchy, we can deduce that it is a cat. However, the legs and the tail have been missed by regional attention due to the high cost of computing attention over large regions. In the case of sparse attention (Fig. 1b), we can observe that the entire image is being analyzed by sets of grids marked by four different colors. Although seemingly, we can compute attention over the entire cat\u2019s body, there hardly exists information hierarchy and a good deal of irrelevant pixels are also considered. For instance, computing attention over the duck pixels probably does not contribute much to classifying the cat. Based on the above intuitive analysis, we can make the following two observations. Firstly, it is preferable to cover more regions, with reasonable computational expense. Secondly, while global information is beneficial, we may prob-\nably benefit by limiting our receptive field up to a certain extent. Combining these two observations, we can thus induce sparsity in inspected regions or make the grids bounded. These reduce to dilated regions, which have been analyzed in atrous convolution [4]. Therefore, taking inspiration from atrous convolution, we propose Atrous Attention. We can consider multiple windows, dilated at varying degrees, to compute attention and then combine them together. This solves the two issues concurrently, maintaining object hierarchy and capturing global context. For example, in Fig. 1c, it can be observed that 3 smaller patches with different levels of dilation are capable of covering all of the cat. The concept of atrous or dilated convolution from traditional signal and image processing [12] was properly introduced in computer vision by deeplab [4], which quickly gained popularity. By ignoring some consecutive neighboring pixels, atrous convolution increases the receptive field in deep networks [20]. Moreover, Atrous spatial pyramid pooling can extract hierarchical features [5], which can also be employed adaptively based on conditions [22]. However, in the ViT era, the use of atrous convolution has severely declined, limited only up to patch computation [14]. Very recently the use of dilation in attention computation have been investigated. DiNat [11] involves computing attention maps with a fixed rate of dilation, whereas DilateFormer [16] leverages multiple dilation rates with smaller kernels. We propose a novel attention mechanism for vision transformers, design a hybrid vision transformer based on that attention mechanism. We turn back to almost obscured atrous (or dilated) convolution, in the vision transformer era, and discover that the attributes of atrous convolution are quite beneficial for vision transformers. We thus design both our attention mechanism and convolution blocks based on the atrous convolution. In addition, taking inspiration from atrous spatial pyramid pooling (ASPP) [4], we experiment with parallel design, deviating from the existing style of stacking different types of transformer layers [29].\n# 2. Methods\n# 2.1. Atrous Attention\nAs a fusion between sparse and regional attention, we propose atrous attention (Fig. 1). We take motivation from atrous convolution [4], which drops some rows and columns from an image to effectively increase the receptive field, using similar number of parameters. This enables us to inspect over a global context with reasonable computational complexity, while also retaining the hierarchical information by considering windows at multiple dilation. For an input featuremap x \u2208Rc,h,w, where c, h, w refers to the number of channels, height, and width, respectively, we compute windows at different levels of dilation as,\nxatr\u2212k[c, i, j] = x[c, i + 2k \u22121, j + 2k \u22121]\n(1)\nHere, we only consider dilation rates as powers of 2, as they can be efficiently computed on GPU using einops operation [23]. Similar to atrous convolution, all the values can be captured by shifting or sliding the dilated patterns. We apply windowed multi-head self-attention with relative positional embedding [25], on each of the windows computed at different levels of dilation (xatr\u2212k) along with the undilated input (x).\n# 2.2. Gating Operation for Adaptive Fusion of Different Branches\nProper aggregation of hierarchical context can substantially improve the visual representation of vision models [35]. Therefore, we have developed a simple, lightweight, and adaptive gating operation to merge the features extracted from different levels of dilation. Based on the input featuremap, x, g \u2208[0 \u223c1] gating weight factors are computed for each output branch using a single linear layer coupled with softmax operation. We hypothesize that based on the input, the gating operation understands which portions of the different computed features should be emphasized or discarded. For the different output braches yi, corresponding weight factors gi are computed and the fused featuremap yfused can be computed from a Hadamard product.\n(2)\n# 2.3. Shared MLP Layer across Parallel Attentions\nIn our formulation, we apply a shared MLP layer on the fused attention map, unlike conventional transformer layers, where MLP layers are used after each attention operation. This simultaneously reduces the computational complexity, and makes the learning easier for the model (as seen in ablation). We conjecture that having a shared MLP reduces the degree of variation the model needs to reconcile.\n# 2.4. Parallel Atrous Inverted Residual Convolution\nInverted residual convolution is the de facto choice of convolution in vision transformers [29, 33]. However, in order to exploit sparsity and hierarchy throughout the model, we replace the intermediate depthwise separable convolution with 3 parallel atrous, depthwise separable convolution, with dilation rates of 1, 2, and 3, respectively. The outputs of the three convolutional operations are merged through gating. Thus, formally our proposed Atrous Inverted Residual Convolution block can be presented as follows:\ny = x+SE(Convc\u00d70.25 1\u00d71 (G(DConvdil=1,2,3 3\u00d73 (Convc\u00d74 1\u00d71(x))))) (3) Here, SE and G refers to squeeze-and-excitation block [13] and gating operation, respectively. The subscripts of Conv\ny = x+SE(Convc\u00d70.25 1\u00d71 (G(DConvdil=1,2,3 3\u00d73 (Convc\u00d74 1\u00d71(x))))) (3)\nHere, SE and G refers to squeeze-and-excitation block [13] and gating operation, respectively. The subscripts of Conv\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8056/8056d6e8-338c-442f-a027-973b8b0609d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Figure 2. ACC-ViT model architecture and its components.</div>\nand DConv denote the kernel size, and the superscripts refer to the expansion ratio or dilation rates, where applicable.\n2.5. ACC-ViT\n# 2.5. ACC-ViT\nUsing the proposed attention and convolution blocks, we have designed ACC-ViT, a hybrid, hierarchical vision transformer architecture. Following conventions, the model comprises a convolution stem at the input level, 4 ACCViT blocks, and a classification top [29]. The stem downsamples the input image, making the computation of selfattention feasible. The stem is followed by 4 ACC-ViT blocks, which are built by stacking Atrous Convolution and Attention layers, and the images are downsampled after each block. Since the image size gets reduced, we use 3,2,1 and 0 (no dilation) levels of Atrous Attention in the 4 blocks, respectively. Finally, the classifier top is based on global average pooling and fully connected layers, as used in contemporary architecture [18, 19, 29]. A diagram of our proposed architecture is presented in Fig. 2. Following the conventional practices, we have designed several variants of ACC-ViT, having different levels of complexity, the configurations are presented in the appendix.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f6a/1f6aa82f-3cc0-4197-af05-7c74b8746d0a.png\" style=\"width: 50%;\"></div>\n# 3. Results\n# 3.1. ImageNet-1k Image Classification\nWe have trained the different variants of ACC-ViT on the standard image classification task, using the ImageNet-1K dataset [8]. The models were trained using a combination\nof the hyperparameter values recommended by Torchvision [26] and as adopted in recent works [29, 33, 35]. Under the conventional supervised learning paradigm, the models were trained on 224 \u00d7 224 resolution images, without any extra data. The top-1 validation accuracy achieved by ACCViT is presented in Fig. 3, along with a comparison against popular state-of-the-art models.\n<div style=\"text-align: center;\">Figure 3. ACC-ViT performs competitively against state-of-theart models on ImageNet-1K.</div>\nACC-ViT outperforms the state-of-the-art MaxViT and MOAT models within similar ranges of parameters and FLOPs. In tiny and small models, using a similar amount of flops, ACC-ViT achieves 0.35% and 0.15% higher accuracy than MaxViT, despite having 9.15% and 9.7% less parameters. For all the other models, the performance of ACC-ViT is even more impressive. The tiny and small versions of ACC-ViT are more accurate than the small and base variants of the other models, respectively.\nHAM10000\nEyePACS\nBUSI\nPr\nRe\nF1\nAcc\nPr\nRe\nF1\nAcc\nPr\nRe\nF1\nAcc\nSwin-T\n82.34\n75.93\n78.37\n88.32\n67.37\n48.94\n52.85\n83.05\n75.18\n75.70\n75.07\n77.56\nConvNeXt-T\n83.69\n76.84\n79.48\n89.57\n66.82\n52.30\n56.20\n83.90\n79.75\n79.65\n79.42\n82.05\nMaxVit-T\n64.45\n57.87\n59.71\n84.62\n64.63\n51.43\n53.99\n83.67\n72.30\n63.75\n66.46\n73.08\nACC-ViT-T\n90.02\n84.26\n86.77\n92.06\n66.42\n57.00\n60.12\n85.07\n80.10\n76.30\n77.80\n80.77\n# 3.2. Transfer Learning Experiment on Medical Image Datasets\nOne of the most useful applications of general vision backbone models is transfer learning, where ViT models have demonstrated remarkable efficacy [38]. In order to evaluate the transfer learning capability of ACC-ViT, we selected medical image datasets for the diversity and associated challenges [21]. 3 different medical image datasets of different modalities and size categories were selected, as a means to critically assess how transferable the visual representations are under different configurations. The images from HAM10000 [28] (skin melanoma, 10,015 images), EyePACS [7] (diabetic retinopathy, 5,000 images) and BUSI [2] (breast ultrasound, 1,578 images) datasets were split into 60:20:20 train-validation-test splits randomly, in a stratified manner. The tiny versions of Swin, ConvNext, MaxViT and ACC-ViT, pretrained on ImageNet-1K were finetuned for 100 epochs and the macro precision, recall, F1 scores along with the accuracy on the test data were computed (Table 1). It can be observed that ACC-ViT outperformed the other models in most of the metrics, which was more noticeable on the larger datasets. For the small dataset, BUSI, ConvNeXt turned out to be the best model, ACC-ViT becoming the second best. This is expected as it has been shown in medical imaging studies that convolutional networks perform better than transformers on smaller datasets [15]. Out of all the models, MaxViT seemed to perform the worst, particularly for the rarer classes, which probably implies that the small-scale dataset was not sufficient to tune the model\u2019s parameters sufficiently, (please refer to the appendix for class-specific metrics).\n# 3.3. Model Interpretation\nIn order to interpret and analyze what the model learns, we applied Grad-CAM [24] on Swin, MaxViT and ACC-ViT (Fig. 4). The class activation maps revealed that MaxViT tends to focus on irrelevant portions of the image, probably due to grid attention. On the contrary, Swin seemed to often focus on a small local region. ACC-ViT apparently managed to inspect and distinguish the entire goldfish and eraser of (Fig. 4a and b). Moreover, when classifying the flamingo from the example of Fig. 4c, ACC-ViT focused on the entire flock of flamingos, whereas Swin and MaxViT\nfocused on a subset and irrelevant pixels, respectively. Interestingly, when classifying Fig. 4d as hammerhead, ACCViT only put focus on the hammerhead fish, whereas the other transformers put emphasis on both the fishes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1538/1538f540-1c95-4656-983b-d2c71972aeab.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4. Model Interpretation using Grad-CAM.</div>\n# 4. Conclusion\nIn this work, we have developed a novel attention mechanism for vision transformers, by fusing the concepts of regional and sparse attention. Seeking inspiration from atrous convolution, we have designed a sparse regional attention mechanism, named Atrous Attention. Our proposed hybrid ViT architecture, ACC-ViT, maintains a balance between local and global, sparse and regional information throughout the model.\n[1] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in Brief, 28:104863, 2020. 10 [2] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in Brief, 28:104863, 2020. 4 [3] Chun-Fu Chen, Rameswar Panda, and Quanfu Fan. RegionViT: Regional-to-Local Attention for Vision Transformers. 2021. 1 [4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. 2016. 2 [5] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking Atrous Convolution for Semantic Image Segmentation. 2017. 2 [6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the Design of Spatial Attention in Vision Transformers. 2021. 1 [7] Jorge Cuadros and George Bresnick. EyePACS: An Adaptable Telemedicine System for Diabetic Retinopathy Screening. Journal of Diabetes Science and Technology, 3(3):509\u2013 516, 2009. 4, 10 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009. 3, 8 [9] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. 2021. 1 10] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00b4e J\u00b4egou, and Matthijs Douze. LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference. 2021. 1 11] Ali Hassani and Humphrey Shi. Dilated neighborhood attention transformer. 2022. 2 12] M. Holschneider, R. Kronland-Martinet, J. Morlet, and Ph. Tchamitchian. A Real-Time Algorithm for Signal Analysis with the Help of the Wavelet Transform. pages 286\u2013297. 1990. 2 13] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation Networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7132\u20137141. IEEE, 2018. 2 14] Muqi Huang and Lefei Zhang. Atrous Pyramid Transformer with Spectral Convolution for Image Inpainting. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4674\u20134683, New York, NY, USA, 2022. ACM. 2 15] Nabil Ibtehaz and Daisuke Kihara. ACC-UNet: A Completely Convolutional UNet Model for the 2020s. pages 692\u2013 702. 2023. 4 16] Jiayu Jiao, Yu-Ming Tang, Kun-Yu Lin, Yipeng Gao, Jinhua Ma, Yaowei Wang, and Wei-Shi Zheng. Dilateformer: Multi-scale dilated transformer for visual recognition. IEEE Transaction on Multimedia, 2023. 2\n[17] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. LocalViT: Bringing Locality to Vision Transformers. 2021. 1 [18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. 2021. 1, 3 [19] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the 2020s. 2022. 3 [20] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the Effective Receptive Field in Deep Convolutional Neural Networks. 2017. 2 [21] Andreas S. Panayides, Amir Amini, Nenad D. Filipovic, Ashish Sharma, Sotirios A. Tsaftaris, Alistair Young, David Foran, Nhan Do, Spyretta Golemati, Tahsin Kurc, Kun Huang, Konstantina S. Nikita, Ben P. Veasey, Michalis Zervakis, Joel H. Saltz, and Constantinos S. Pattichis. AI in Medical Imaging Informatics: Current Challenges and Future Directions. IEEE Journal of Biomedical and Health Informatics, 24(7):1837\u20131857, 2020. 4 [22] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution. 2020. 2 [23] Alex Rogozhnikov. Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation. In International Conference on Learning Representations, 2022. 2, 7 [24] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. 2016. 4 [25] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. SelfAttention with Relative Position Representations. 2018. 2 [26] TorchVision maintainers and contributors. TorchVision: PyTorch\u2019s Computer Vision library. https://github.com/pytorch/vision, 2016. 3, 8 [27] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training data-efficient image transformers & distillation through attention. 2020. 1 [28] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific Data, 5(1):180161, 2018. 4, 10 [29] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. MaxViT: Multi-Axis Vision Transformer. 2022. 1, 2, 3, 10 [30] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu. CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention. 2021. 1 [31] Wenxiao Wang, Wei Chen, Qibo Qiu, Long Chen, Boxi Wu, Binbin Lin, Xiaofei He, and Wei Liu. CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention. 2023. 1 [32] Ross Wightman. PyTorch Image Models. https://github.com/rwightman/pytorch-image-models, 2019. 8\n[33] Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, Alan Yuille, Hartwig Adam, and Liang-Chieh Chen. MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models. 2022. 2, 3 [34] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal Self-attention for Local-Global Interactions in Vision Transformers. 2021. 1 [35] Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Focal Modulation Networks. 2022. 1, 2, 3, 8 [36] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. 2019. 8 [37] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond Empirical Risk Minimization. 2017. 8 [38] Hong-Yu Zhou, Chixiang Lu, Sibei Yang, and Yizhou Yu. ConvNets vs. Transformers: Whose Visual Representations are More Transferable? In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 2230\u20132238. IEEE, 2021. 4\n# Fusion of regional and sparse attention in Vision Transformers Supplementary Material\n# A. Architectural Details\n# A.1. Efficient dilated window generation for Atrous Attention using einops\nOur proposed Atrous Attention depends on computing dilated windows efficiently. We have devised a compact einops operation [23] to partition the images or featuremaps into dilated windows. x \u2212attr1, x \u2212attr2, and x \u2212attr3, which are dilated versions of input x with dilation rates of 2, 4, and 8, respectively, can\nx-attr1 = rearrange(x, \u2019b c (h hs) (w ws) -> (b hs ws) c h w\u2019, hs=2, ws=2)\nx-attr2 = rearrange(x, \u2019b c (h hs) (w ws) -> (b hs ws) c h w\u2019, hs=4, ws=4)\nx-attr3 = rearrange(x, \u2019b c (h hs) (w ws) -> (b hs ws) c h w\u2019, hs=8, ws=8)\nIn our ACC-ViT we consider dilation rates up to 8, therefore it suffices to compute up to x \u2212attr3. This operation effectively extracts k2 dilated windows, partitioned at a dilation rate of k. The extracted windows thus can be used for computing windowed attention without any modification to the attention mechanism. The original image or featuremap can be retrieved from the partitioned windows through a departition operation implemented using the inverse einops operation.\nx = rearrange(x-attr1, \u2019(b hs ws) c h w -> b c (h hs) (w ws)\u2019, hs=2, ws=2) x = rearrange(x-attr2, \u2019(b hs ws) c h w -> b c (h hs) (w ws)\u2019, hs=4, ws=4) x = rearrange(x-attr3, \u2019(b hs ws) c h w -> b c (h hs) (w ws)\u2019, hs=8, ws=8)\n# A.2. Overview of the different ACC-ViT layers\nThe ACC-ViT layers are based on our proposed Atrous Attention, Atrous Inverted Residual Convolution, and Gating. Th convolution operations are identical in all the stages. However. the different stages of ACC-ViT layers use different degree of Atrous Attention, due to the change in resolution of featuremaps. (S1) Block 1 : Atrous Attention with dilation rates of 2, 4, and 8 are used, along with undilated windowed attention. (S2) Block 2 : Atrous Attention with dilation rates of 2, and 4 are used, along with undilated windowed attention. (S3) Block 3 : Atrous Attention with a dilation rate of 2 is used, along with undilated windowed attention. (S4) Block 4 : Only undilated windowed attention is used.\nThe ACC-ViT layers are based on our proposed Atrous Attention, Atrous Inverted Residual Convolution, and Gating. The convolution operations are identical in all the stages. However. the different stages of ACC-ViT layers use different degrees of Atrous Attention, due to the change in resolution of featuremaps. (S1) Block 1 : Atrous Attention with dilation rates of 2, 4, and 8 are used, along with undilated windowed attention. (S2) Block 2 : Atrous Attention with dilation rates of 2, and 4 are used, along with undilated windowed attention. (S3) Block 3 : Atrous Attention with a dilation rate of 2 is used, along with undilated windowed attention. (S4) Block 4 : Only undilated windowed attention is used.\n# A.3. ACC-ViT configurations\n<div style=\"text-align: center;\">Table 2. Different Configurations of ACC-ViT model.</div>\nStage\nSize\ntiny\nsmall\nbase\nnano\npico\nfemto\nS0: Stem\n1/2\nB=2, C=64\nB=2, C=64\nB=2, C=64\nB=2, C=64\nB=2, C=48\nB=2, C=32\nS1: Block 1\n1/4\nB=2, C=64\nB=2, C=96\nB=4, C=96\nB=1, C=64\nB=1, C=48\nB=1, C=32\nS2: Block 2\n1/8\nB=3, C=128\nB=3, C=192\nB=6, C=192\nB=2, C=128\nB=2, C=96\nB=2, C=64\nS3: Block 3\n1/16\nB=6, C=256\nB=6, C=384\nB=14, C=384\nB=4, C=256\nB=4, C=192\nB=4, C=128\nS4: Block 4\n1/32\nB=2, C=512\nB=2, C=768\nB=2, C=768\nB=1, C=512\nB=1, C=384\nB=1, C=256\n#params (M)\n28.367\n62.886\n103.576\n16.649\n9.55\n4.4\nFLOPs (G)\n5.694\n11.59\n22.316\n3.812\n2.217\n1.049\nRecent state-of-the-art models make the implementation and trained-weights public. However, the training pipeline script is hardly accessible, which prevents the identical and consistent training of new models. As a result, there have been community efforts to retrain the models using standardized protocols, maintaining reproducibility. TorchVision [26] and timm [32] are the two most notable open-source projects in this regard. Our ACC-ViT implementation and ImageNet training pipeline are based on TorchVision v0.15.2 (https://pypi.org/ project/torchvision/0.15.2/). Furthermore, we used the ImageNet-1K trained weights of the baseline models, released by TorchVision. The baseline model weights are summarized in Table 3.\n<div style=\"text-align: center;\">Table 3. TorchVision weights of baseline models</div>\nModel\nTorchVision weight\nIN-1K Acc\n(reproduced)\nIN-1K\n(published)\nLink\nSwin\nSwin T Weights.IMAGENET1K V1\n81.474%\n81.3%\nhttps://download.pytorch.org/models/\nswin t-704ceda3.pth\nConvNeXt\nConvNeXt Tiny Weights.IMAGENET1K V1\n82.520%\n82.1%\nhttps://download.pytorch.org/models/\nconvnext tiny-983f1562.pth\nMaxViT\nMaxVit T Weights.IMAGENET1K V1\n83.7%\n83.6%\nhttps://download.pytorch.org/models/\nmaxvit t-bc5ab103.pth\n# C. Experimental Details\nHyperparameter\nfemto\npico\nnano\ntiny\nsmall\nimage resolution\n224 \u00d7 224\nbatch size\n3072\n2560\n2048\n1024\n768\nstochastic depth\n0.0\n0.0\n0.1\n0.2\n0.3\nlearning rate\n1e-3\n1e-3\n1e-3\n1e-3\n5e-4\nmin learning rate\n1e-5\n1e-5\n1e-5\n1e-5\n5e-6\ntraining epochs\n400\nwarm-up epochs\n32\nOptimizer\nadamw\nschedular\ncosine annealing\nloss function\nsoftmax\nrandaugment policy\nta-wide\nrandaugment magnitude\n15\nrandaugment sampler\n3\ncutmix-alpha\n0.8\nmixup-alpha\n0.8\nlabel-smoothing\n0.1\ngradient clip\n1.0\nema steps\n32\n# C.1. ImageNet-1K Training\nThe ImageNet-1K dataset [8] is a benchmark dataset containing images from 1000 classes. It provides 1.2 million and 50,000 images for training and validation, respectively. We trained the ACC-ViT models mostly based on the TorchVision defaults and the recipe for MaxViT https : / / github . com / pytorch / vision / tree / main / references / classification#maxvit and trained the models for 400 epochs with cosine annealing learning rate scheduler and exponential moving average (EMA). The batch size and learning rate were adjusted based on the computational limitations set by GPU memory. Taking inspiration from FocalNet [35] we considered both cutmix [36] and mixup [37] augmentations. The training hyperparameters are summarized in table 4.\n<div style=\"text-align: center;\">Table 4. Hyperparameters for ImageNet training</div>\nCUBLAS_WORKSPACE_CONFIG=:16:8 torchrun --nproc-per-node=16 --nnodes=1 train.py --model accvit_f --epochs 400 --batch-size 192 --opt adamw --lr 1e-3 --weight-decay 0.05 --lr-scheduler cosineannealinglr --lr-min 1e-5 --lr-warmup-method linear --lr-warmup-epochs 32 --label-smoothing 0.1 --mixup-alpha 0.8 --cutmix-alpha 0.8 --clip-grad-norm 1.0 --interpolation bicubic --auto-augment ta_wide --ra-magnitude 15 --ra-sampler --model-ema --val-resize-size 224 --val-crop-size 224 --train-crop-size 224 --model-ema-steps 32 --transformer-embedding-decay 0 --amp --use-deterministic-algorithms --sync-bn --data-path ../../../imagenet_full/ILSVRC/Data/CLS-LOC/ --output-dir accvit_femto_imgnet_1k\n# \u2022 ACC-ViT pico\nCUBLAS_WORKSPACE_CONFIG=:16:8 torchrun --nproc-per-node=16 --nnodes=1 train.py --model accvit_p --epochs 400 --batch-size 160 --opt adamw --lr 1e-3 --weight-decay 0.05 --lr-scheduler cosineannealinglr --lr-min 1e-5 --lr-warmup-method linear --lr-warmup-epochs 32 --label-smoothing 0.1 --mixup-alpha 0.8 --cutmix-alpha 0.8 --clip-grad-norm 1.0 --interpolation bicubic --auto-augment ta_wide --ra-magnitude 15 --ra-sampler --model-ema --val-resize-size 224 --val-crop-size 224 --train-crop-size 224 --model-ema-steps 32 --transformer-embedding-decay 0 --amp --use-deterministic-algorithms --sync-bn --data-path ../../../imagenet_full/ILSVRC/Data/CLS-LOC/ --output-dir accvit_pico_imgnet_1k\n# \u2022 ACC-ViT nano\nCUBLAS_WORKSPACE_CONFIG=:16:8 torchrun --nproc-per-node=16 --nnodes=1 train.py --model accvit_n --epochs 400 --batch-size 128 --opt adamw --lr 1e-3 --weight-decay 0.05 --lr-scheduler cosineannealinglr --lr-min 1e-5 --lr-warmup-method linear --lr-warmup-epochs 32 --label-smoothing 0.1 --mixup-alpha 0.8 --cutmix-alpha 0.8 --clip-grad-norm 1.0 --interpolation bicubic --auto-augment ta_wide --ra-magnitude 15 --ra-sampler --model-ema --val-resize-size 224 --val-crop-size 224 --train-crop-size 224 --model-ema-steps 32 --transformer-embedding-decay 0 --amp --use-deterministic-algorithms --sync-bn --data-path ../../../imagenet_full/ILSVRC/Data/CLS-LOC/ --output-dir accvit_nano_imgnet_1k\n# \u2022 ACC-ViT tiny\nCUBLAS_WORKSPACE_CONFIG=:16:8 torchrun --nproc-per-node=16 --nnodes=1 train.py --model accvit_t --epochs 400 --batch-size 64 --opt adamw --lr 1e-3 --weight-decay 0.05 --lr-scheduler cosineannealinglr --lr-min 1e-5 --lr-warmup-method linear --lr-warmup-epochs 32 --label-smoothing 0.1 --mixup-alpha 0.8 --cutmix-alpha 0.8 --clip-grad-norm 1.0 --interpolation bicubic --auto-augment ta_wide --ra-magnitude 15 --ra-sampler --model-ema --val-resize-size 224 --val-crop-size 224 --train-crop-size 224 --model-ema-steps 32 --transformer-embedding-decay 0 --amp\nCUBLAS_WORKSPACE_CONFIG=:16:8 torchrun --nproc-per-node=16 --nnodes=1 train.py --model accvit_t --epochs 400 --batch-size 64 --opt adamw --lr 1e-3 --weight-decay 0.05 --lr-scheduler cosineannealinglr --lr-min 1e-5 --lr-warmup-method linear --lr-warmup-epochs 32 --label-smoothing 0.1 --mixup-alpha 0.8 --cutmix-alpha 0.8 --clip-grad-norm 1.0 --interpolation bicubic --auto-augment ta_wide --ra-magnitude 15 --ra-sampler --model-ema --val-resize-size 224 --val-crop-size 224 --train-crop-size 224 --model-ema-steps 32 --transformer-embedding-decay 0 --amp\n--use-deterministic-algorithms --sync-bn --data-path ../../../imagenet_full/ILSVRC/Data/CLS-LOC/ --output-dir accvit_tiny_imgnet_1k\n# C.2. Finetuning on Medical Image Datasets\nIn order to assess the transferability of the learned visual representations, we experimented on 3 medical image datasets of different modalities and sizes.\n# 1. HAM10000 : The HAM10000 [28] dataset \n1. HAM10000 : The HAM10000 [28] dataset contains 10,015 dermatoscopic images from 7 different classes of pigmented skin lesions. The 7 classes are akiec (actinic keratoses and intraepithelial carcinomae), bcc (basal cell carcinoma), bkl (benign keratosis-like lesions), df (dermatofibroma), nv (melanocytic nevi), vasc (pyogenic granulomas and hemorrhage) and mel (melanoma). This dataset is quite imbalanced, nearly 2/3rd of the images belong to the nv class, whereas akiec, df and vasc comprises 3.25%, 1.15%, and 1.4%, of the dataset respectively. 2. EyePACS : The EyePACS dataset [7] is a source of highresolution retina images taken under a variety of imaging conditions. In our experiments, we considered the 5,000 images of this dataset used for training in the Diabetic Retinopathy Detection competition, held at Kaggle (https://www.kaggle.com/competitions/ diabetic- retinopathy- detection/data). The retinal images in this dataset are annotated into 5 grades of diabetic retinopathy, starting from none, spread to mild, moderate, severe, and up to proliferative level of diabetic retinopathy. This dataset is quite imbalanced as well, as almost 3/4th of the retinal images have no diabetic retinopathy. 3. BUSI : The BUSI dataset [1] is a dataset of 780 breast ultrasound images from 600 patients. The images are categorized into 3 classes, namely, normal, benign, and malignant. We randomly partitioned the datasets into 60-20-20 train-validation-test splits. The models were fine-tuned using the training split and based on the validation loss the best checkpoints were obtained. Finally, the performance on the test split was assessed. We adopted the finetuning strategy of MaxViT [29] to finetune ACC-ViT and the baseline models on the medical image datasets. The learning rate was set to 5e\u22125 without any scheduler or warm-up. Other than RandAugment no other augmentation strategy, i.e., cutmix or mixup, was used. The values of label smoothing and gradient were set to 0.1 and 1.0, respectively. We finetuned the models for 100 epochs, longer than the usual 30 epochs. ACC-ViT managed to finetune rather quickly whereas MaxViT took longer on the medical images.\n1. HAM10000 : The HAM10000 [28] dataset contains 10,015 dermatoscopic images from 7 different classes of pigmented skin lesions. The 7 classes are akiec (actinic keratoses and intraepithelial carcinomae), bcc (basal cell carcinoma), bkl (benign keratosis-like lesions), df (dermatofibroma), nv (melanocytic nevi), vasc (pyogenic granulomas and hemorrhage) and mel (melanoma). This dataset is quite imbalanced, nearly 2/3rd of the images belong to the nv class, whereas akiec, df and vasc comprises 3.25%, 1.15%, and 1.4%, of the dataset respectively. 2. EyePACS : The EyePACS dataset [7] is a source of highresolution retina images taken under a variety of imaging conditions. In our experiments, we considered the 5,000 images of this dataset used for training in the Diabetic Retinopathy Detection competition, held at Kaggle (https://www.kaggle.com/competitions/ diabetic- retinopathy- detection/data). The retinal images in this dataset are annotated into 5 grades of diabetic retinopathy, starting from none, spread to mild, moderate, severe, and up to proliferative level of diabetic retinopathy. This dataset is quite imbalanced as well, as almost 3/4th of the retinal images have no diabetic retinopathy. 3. BUSI : The BUSI dataset [1] is a dataset of 780 breast ultrasound images from 600 patients. The images are categorized into 3 classes, namely, normal, benign, and malignant. We randomly partitioned the datasets into 60-20-20 train-validation-test splits. The models were fine-tuned using the training split and based on the validation loss the best checkpoints were obtained. Finally, the performance on the test split was assessed. We adopted the finetuning strategy of MaxViT [29] to finetune ACC-ViT and the baseline models on the medical image datasets. The learning rate was set to 5e\u22125 without any scheduler or warm-up. Other than RandAugment no other augmentation strategy, i.e., cutmix or mixup, was used. The values of label smoothing and gradient were set to 0.1 and 1.0, respectively. We finetuned the models for 100 epochs, longer than the usual 30 epochs. ACC-ViT managed to finetune rather quickly whereas MaxViT took longer on the medical images.\n# D. Detailed Results\n# D.1. Finetuning on Medical Images\nHere, we present the detailed class-wise metrics and confusion matrices of the different models on the three different datasets. From the overall results, a few points can be observed. 1. Across all the datasets, ACC-ViT predictions were consistently precise, which is evident from the highest precision scores. 2. ACC-ViT has demonstrated a poor recall on the normal class of the BUSI dataset, which has affected the overall score on that dataset. However, for the most important class of that dataset, i.e., malignent, ACC-ViT actually managed to score the highest recall, 4.76% higher than the best-performing ConvNeXt model. 3. Notably, for the difficult mild class of the EyePACS dataset ACC-ViT resulted in the highest recall, which is twice the second best method. 4. MaxViT struggled quite a bit in predicting the rare classes. For example, it failed to classify any images of the df class from the HAM10000 dataset correctly.\n# E. Ablation Study\nTable 8 presents an ablation study focusing on the contributions of the different design choices in our development roadmap. We conducted our design discovery and ablation study on the nano variant, i.e., \u223c17 M parameter model. We started with Atrous Attention in conjunction with MBConv, which resulted in 79.5% accuracy. Later, we used parallel atrous convolutions, increasing the accuracy only by 0.5% with a considerable increase in parameters. Sharing the MLP layer across attentions, turned out quite beneficial, substantially reducing the FLOPs and increasing the accuracy. Up to this moment, the model, merely averaging the different regional information, would apparently learn the parameters quickly and reach a plateau. Implementing the gating function improved this scenario, as the model learned to focus on the key regional information dynamically, based on the input. We also attempted to replace the MLP layer with ConvNext layer, similar to MOAT, but that did not yield a satisfactory outcome and thus was discarded.\n<div style=\"text-align: center;\">Table 5. Detailed results on the HAM10000 dataset</div>\nACC-ViT\nConvNeXt\nSwin\nMaxViT\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\ncount\nakiec\n0.8235\n0.6462\n0.7241\n0.7069\n0.6308\n0.6667\n0.7358\n0.6000\n0.6610\n0.6857\n0.3692\n0.4800\n65\nbcc\n0.8785\n0.9126\n0.8952\n0.8034\n0.9126\n0.8545\n0.7851\n0.9223\n0.8482\n0.6791\n0.8835\n0.7679\n103\nbkl\n0.8545\n0.8545\n0.8545\n0.8586\n0.7455\n0.7981\n0.8503\n0.7227\n0.7813\n0.7026\n0.7409\n0.7212\n220\ndf\n1.0000\n0.7826\n0.8780\n0.8571\n0.5217\n0.6486\n0.8667\n0.5652\n0.6842\n0.0000\n0.0000\n0.0000\n23\nmel\n0.7860\n0.8072\n0.7965\n0.7708\n0.6637\n0.7133\n0.7031\n0.6054\n0.6506\n0.6378\n0.5291\n0.5784\n223\nnv\n0.9586\n0.9664\n0.9625\n0.9330\n0.9761\n0.9541\n0.9260\n0.9709\n0.9479\n0.9171\n0.9567\n0.9365\n1341\nvasc\n1.0000\n0.9286\n0.9630\n0.9286\n0.9286\n0.9286\n0.8966\n0.9286\n0.9123\n0.8889\n0.5714\n0.6957\n28\nmacro\navg\n0.9002\n0.8426\n0.8677\n0.8369\n0.7684\n0.7948\n0.8234\n0.7593\n0.7837\n0.6445\n0.5787\n0.5971\nweighted\navg\n0.9205\n0.9206\n0.9200\n0.8918\n0.8957\n0.8918\n0.8784\n0.8832\n0.8786\n0.8318\n0.8462\n0.8354\naccuracy\n0.9206\n0.8957\n0.8832\n0.8462\n<div style=\"text-align: center;\">Table 6. Detailed results on the EyePACS dataset</div>\nACC-ViT\nConvNeXt\nSwin\nMaxViT\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\ncount\nnone\n0.8970\n0.9698\n0.9320\n0.8753\n0.9766\n0.9232\n0.8626\n0.9781\n0.9167\n0.8733\n0.9804\n0.9238\n5162\nmild\n0.4577\n0.1881\n0.2667\n0.4839\n0.0920\n0.1546\n0.5000\n0.0348\n0.0650\n0.4894\n0.0470\n0.0858\n489\nmoderate\n0.7269\n0.6843\n0.7050\n0.7048\n0.6295\n0.6650\n0.6812\n0.6059\n0.6413\n0.6974\n0.6144\n0.6533\n1058\nsevere\n0.5526\n0.3600\n0.4360\n0.5271\n0.3886\n0.4474\n0.5046\n0.3143\n0.3873\n0.4929\n0.3943\n0.4381\n175\nproliferative\n0.6866\n0.6479\n0.6667\n0.7500\n0.5282\n0.6198\n0.8202\n0.5141\n0.6320\n0.6786\n0.5352\n0.5984\n142\nmacro\navg\n0.6642\n0.5700\n0.6012\n0.6682\n0.5230\n0.5620\n0.6737\n0.4894\n0.5285\n0.6463\n0.5143\n0.5399\nweighted\navg\n0.8280\n0.8507\n0.8338\n0.8112\n0.8390\n0.8128\n0.8003\n0.8305\n0.7971\n0.8067\n0.8367\n0.8061\naccuracy\n0.8507\n0.8390\n0.8305\n0.8367\n<div style=\"text-align: center;\">Table 7. Detailed results on the BUSI dataset</div>\nACC-ViT\nConvNeXt\nSwin\nMaxViT\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\ncount\nbenign\n0.8261\n0.8736\n0.8492\n0.8539\n0.8736\n0.8636\n0.8161\n0.8161\n0.8161\n0.7358\n0.8966\n0.8083\n87\nmalignent\n0.7674\n0.7857\n0.7765\n0.8611\n0.7381\n0.7949\n0.8333\n0.7143\n0.7692\n0.7273\n0.5714\n0.6400\n42\nnormal\n0.8095\n0.6296\n0.7083\n0.6774\n0.7778\n0.7241\n0.6061\n0.7407\n0.6667\n0.7059\n0.4444\n0.5455\n27\nmacro\navg\n0.8010\n0.7630\n0.7780\n0.7975\n0.7965\n0.7942\n0.7518\n0.7570\n0.7507\n0.7230\n0.6375\n0.6646\nweighted\navg\n0.8074\n0.8077\n0.8052\n0.8253\n0.8205\n0.8210\n0.7844\n0.7756\n0.7776\n0.7284\n0.7308\n0.7175\naccuracy\n0.8077\n0.8205\n0.7756\n0.7308\n<div style=\"text-align: center;\">Table 8. Ablation Study</div>\n#params(M)\nFLOPs(G)\nAcc\nAtrous Attention with MBConv\n16.907\n4.862\n79.498\nInroducing Atrous Convolution\n19.33\n4.939\n80.017\nShared MLP across attentions\n14.585\n3.473\n81.523\nAdaptive Gating in Parallel branches\n16.649\n3.812\n82.412\nReplacing MLP with ConvNext\n17.025\n3.844\n81.850\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c9b9/c9b9b42b-b82e-4907-abe9-fcfd55a7ba63.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5. Confusion Matrix of the different models on the three datasets.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of effectively combining local and global context in vision transformers, as previous methods either focused on regional attention, which maintains hierarchical relationships but limits global context, or sparse attention, which captures global context but sacrifices hierarchical information. A new method is necessary to balance these two approaches.",
        "problem": {
            "definition": "The problem addressed is the inability of existing attention mechanisms in vision transformers to simultaneously capture local hierarchical information and global context effectively.",
            "key obstacle": "The main difficulty is the trade-off between maintaining hierarchical relationships through regional attention and capturing a global context with sparse attention, leading to limitations in performance."
        },
        "idea": {
            "intuition": "The inspiration for the proposed idea comes from atrous convolution, which allows for increasing the receptive field without a significant increase in computational complexity.",
            "opinion": "The proposed idea, named Atrous Attention, fuses regional and sparse attention mechanisms to dynamically integrate local and global information while preserving hierarchical structures.",
            "innovation": "The key innovation lies in the use of dilated windows to compute attention, allowing for a more flexible and efficient approach that combines the strengths of both regional and sparse attention."
        },
        "method": {
            "method name": "Atrous Attention",
            "method abbreviation": "AA",
            "method definition": "Atrous Attention is defined as an attention mechanism that computes attention over dilated windows of varying sizes to capture both local and global contexts effectively.",
            "method description": "Atrous Attention combines local and global attention by using multiple dilated windows to compute attention dynamically.",
            "method steps": [
                "Compute windows at different levels of dilation from the input feature map.",
                "Apply windowed multi-head self-attention on each dilated window and the undilated input.",
                "Merge the attention outputs using a gating operation for adaptive fusion."
            ],
            "principle": "The effectiveness of this method is based on its ability to maintain hierarchical relationships while capturing a broader global context through the use of dilated attention windows."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the ImageNet-1K dataset, comparing the performance of ACC-ViT against state-of-the-art models like MaxViT and ConvNeXt.",
            "evaluation method": "Performance was assessed using top-1 validation accuracy, with models trained under conventional supervised learning paradigms."
        },
        "conclusion": "The proposed ACC-ViT model, utilizing Atrous Attention, demonstrates improved performance on standard vision tasks, achieving 84% accuracy on ImageNet-1K with fewer parameters than existing state-of-the-art models.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to effectively integrate local and global information while maintaining computational efficiency.",
            "limitation": "A limitation of the method is that while it improves performance on larger datasets, it may not perform as well on smaller datasets where convolutional networks traditionally excel.",
            "future work": "Future work should explore enhancing the model's performance on smaller datasets and investigating further optimizations for the attention mechanism."
        },
        "other info": {
            "info1": "The ACC-ViT model has several configurations with varying complexities, allowing for flexibility in application.",
            "info2": {
                "info2.1": "The model architecture includes a convolutional stem, multiple ACC-ViT blocks, and a classification top.",
                "info2.2": "The model has been trained using standard protocols and is designed for transfer learning applications."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "5",
            "key information": "The paper introduces a new attention mechanism called Atrous Attention, which effectively combines local and global context in vision transformers."
        },
        {
            "section number": "5.1",
            "key information": "Atrous Attention computes attention over dilated windows of varying sizes to capture both local and global contexts effectively."
        },
        {
            "section number": "5.2",
            "key information": "The proposed method maintains hierarchical relationships while capturing a broader global context through dilated attention windows."
        },
        {
            "section number": "6",
            "key information": "The main advantage of the proposed approach is its ability to effectively integrate local and global information while maintaining computational efficiency."
        },
        {
            "section number": "8",
            "key information": "The proposed ACC-ViT model demonstrates improved performance on standard vision tasks, achieving 84% accuracy on ImageNet-1K with fewer parameters than existing state-of-the-art models."
        }
    ],
    "similarity_score": 0.591497564685269,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Fusion of regional and sparse attention in Vision Transformers.json"
}