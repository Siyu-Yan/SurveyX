{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.16127",
    "title": "Finetuning Large Language Model for Personalized Ranking",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various domains, motivating researchers to investigate their potential use in recommendation systems. However, directly applying LLMs to recommendation tasks has proven challenging due to the significant disparity between the data used for pre-training LLMs and the specific requirements of recommendation tasks. In this study, we introduce Direct Multi-Preference Optimization (DMPO), a streamlined framework designed to bridge the gap and enhance the alignment of LLMs for recommendation tasks. DMPO enhances the performance of LLM-based recommenders by simultaneously maximizing the probability of positive samples and minimizing the probability of multiple negative samples. We conducted experimental evaluations to compare DMPO against traditional recommendation methods and other LLM-based recommendation approaches. The results demonstrate that DMPO significantly improves the recommendation capabilities of LLMs across three real-world public datasets in few-shot scenarios. Additionally, the experiments indicate that DMPO exhibits superior generalization ability in cross-domain recommendations. A case study elucidates the reasons behind these consistent improvements and also underscores DMPO\u2019s potential as an explainable recommendation system. Our code and data are available at https://github.com/BZX667/DMPO",
    "bib_name": "bai2024finetuninglargelanguagemodel",
    "md_text": "# Finetuning Large Language Model for Personalized Ranking\nNing Wu* wuning@buaa.edu.cn Beihang University China Fengyu Cai fengyu.cai@tu-darmstadt.de Technical University of Darmstad Germany\nNing Wu* wuning@buaa.edu.cn Beihang University China Tec\nZhuoxi Bai* bzxleon844@gmail.com Cashcat China\nZhuoxi Bai* bzxleon844@gmail.com Cashcat China Ning Wu* wuning@buaa.edu.cn Beihang University China\nXinyi Zhu helvieluke534@gmail.com Shanghai University China Yun Xiong\u2020 yunx@fudan.edu.cn Fudan University China\nXinyi Zhu helvieluke534@gmail.com Shanghai University China\n# ABSTRACT\nLarge Language Models (LLMs) have demonstrated remarkable performance across various domains, motivating researchers to investigate their potential use in recommendation systems. However, directly applying LLMs to recommendation tasks has proven challenging due to the significant disparity between the data used for pre-training LLMs and the specific requirements of recommendation tasks. In this study, we introduce Direct Multi-Preference Optimization (DMPO), a streamlined framework designed to bridge the gap and enhance the alignment of LLMs for recommendation tasks. DMPO enhances the performance of LLM-based recommenders by simultaneously maximizing the probability of positive samples and minimizing the probability of multiple negative samples. We conducted experimental evaluations to compare DMPO against traditional recommendation methods and other LLM-based recommendation approaches. The results demonstrate that DMPO significantly improves the recommendation capabilities of LLMs across three real-world public datasets in few-shot scenarios. Additionally, the experiments indicate that DMPO exhibits superior generalization ability in cross-domain recommendations. A case study elucidates the reasons behind these consistent improvements and also underscores DMPO\u2019s potential as an explainable recommendation system. Our code and data are available at https://github.com/BZX667/DMPO\narXiv:2405.16127v2\narXiv:2405.1\n# \u2022 Information systems \u2192Recommender systems.\n# KEYWORDS\nLarge Language Model, Recommender Systems, Direct Preference Optimization\n*Both authors contributed equally to this research. \u2020The corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY \u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX\nFengyu Cai cai@tu-darms\nYun Xiong\u2020 yunx@fudan.edu.cn Fudan University China\nACM Reference Format: Zhuoxi Bai*, Ning Wu*, Fengyu Cai, Xinyi Zhu, and Yun Xiong\u2020. 2018. Finetuning Large Language Model for Personalized Ranking. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym \u2019XX). ACM, New York, NY, USA, 11 pages. https: //doi.org/XXXXXXX.XXXXXXX\n# 1 INTRODUCTION\nRecommendation systems are essential for addressing information overload and fulfilling the information needs of users [58\u201360]. Large Language Models (LLMs) have emerged as powerful tools in Natural Language Processing (NLP), demonstrating impressive capacity in natural language understanding and text generation [61\u201364]. Previous studies have highlighted the rich knowledge and generalization abilities of LLMs [41, 74, 75]. Given these strengths, LLMs are poised to revolutionize recommendation systems, particularly in few-shot or zero-shot scenarios. Consequently, researchers have begun exploring the application of LLMs in recommendation systems. However, directly applying LLMs to recommendation tasks may not yield satisfactory performance, as the pre-training data of LLMs is primarily tailored for NLP tasks, which significantly differ in purpose and requirements from recommendation tasks [2]. Previous work has employed in-context learning [61] to align LLMs with recommendation problems [49, 55]. These methods typically use closed-source models such as ChatGPT and GPT4 1 to rerank candidates selected by traditional recommendation models such as Matrix Factorization (MF) [56] and LightGCN [4]. However, these methods only superficially incorporate LLMs, constrained by the limitations of previous methods. This is a pressing need to explore strategies for effective and efficient utilization of LLMs in recommendation tasks. Other studies, such as TALLrec [2], have proposed Supervised Fine-Tuning (SFT) to bridge this gap by introducing data samples specific to recommendation tasks. TALLrec leverages the fine-tuning method used in general NLP to train on recommendation samples, aiming to achieve improved results compared to the in-context learning method. However, SFT only maximizes the probability of generating the correct answer during training. When faced with unseen incorrect answers, the model may overestimate their probability and make incorrect predictions due to the lack of negative sample learning. This approach overlooks the benefits of learning and establishing distinctions and connections between multiple pairs of positive and negative samples, as illustrated in Fig 1.\n1https://chat.openai.com/\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e860/e860ebb3-4b76-40b5-8ed4-4a0693893b86.png\" style=\"width: 50%;\"></div>\nFigure 1: When aligning the LLMs with recommendation tasks, applying Supervised Fine-Tuning (SFT) to LLM initially helps maximize the probability of generating each token in positive items. However, this method overlooks the potential benefits of comparing positive and multiple negative samples. As a result, although the model trained solely with SFT may improve the probability of positive items during training, it may overestimate the probability of unseen negative items during testing, eading to incorrect prediction.\nTo address this limitation, we propose Direct Multi-Preference Optimization (DMPO), a streamlined framework designed to maximize the probability of generating correct answers while simultaneously minimizing the probability of generating multiple incorrect answers under a dynamic margin. The motivation for introducing DMPO arises from the constraints of the original Direct Preference Optimization (DPO) [3], which considered only one positive sample and one negative sample from a user\u2019s user-item interaction list, thereby restricting the diversity of training data and potential performance improvements. Drawing inspiration from contrastive learning methods that employ negative sampling strategies [67\u201370], DMPO incorporates sampling multiple negative samples to expand the range of negative samples. This allows the model to establish more comprehensive and balanced relationships between positive and negative samples, enhancing overall performance. To evaluate the effectiveness of DMPO, we conducted extensive experiments on three real-world public datasets: \"Movielens-1M\", \"Amazon Movies and TV\", and \"Amazon Video Games\". DMPO aims to perform binary classification by distinguishing positive items from negative items, and then rank the likelihood of a user giving a high rating to these items. When generating the answer, the model is expected to prioritize the positive items over the negative items. More details of the dataset and task can be found in Section 5.1 and Section 4.1. Since DMPO utilizes user-item interaction lists, it is appropriate to compare it with LLM-based methods [2] and traditional sequential methods[12\u201317, 77]. The results demonstrate that DMPO significantly improves AUC compared to other methods in a few-shot scenario. Furthermore, in order to validate the superior generalization ability of DMPO, we conducted experiments to compare it with other LLM-based cross-domain methods [2] and traditional cross-domain methods [6, 9\u201311, 76]. The results highlight DMPO\u2019s breakthrough performance in cross-domain recommendation tasks. We also conducted ablation studies to investigate the factors that may affect the performance of DMPO, including the\nnumber of negative samples, the number of few-shot samples, and different base models. Additionally, we performed a case study to demonstrate the reasons why DMPO can bring improvements, and we highlighted that DMPO is an explainable recommendation system. In total, our contributions are summarized as follows:\n\u2022 In order to narrow the gap between LLMs and the recommendation tasks, we propose Direct Multi-Preference Optimization (DMPO). This streamlined framework enhances LLMs\u2019 ability to model the comparative relationships between positive and negative samples for recommendation tasks. \u2022 Through the comprehensive experiments, we showcase that DMPO significantly outperforms the previous LLM-based and traditional methods; \u2022 Moreover, DMPO\u2019s consistent improvements across multiple domains and various types of LLMs empirically underscore its strong generalization and robustness.\n# 2 RELATED WORK\n# 2.1 LLM-based recommendation systems\nThere have been various attempts to incorporate LLMs into recommendation systems. Despite the integration of LLMs [49, 50], some efforts focus on enhancing the recommendation capability of LLMs through in-context learning methods and combining them with traditional recommendation models like MF [56] and LightGCN [4], including Chat-rec [54] and NIR [55]. However, certain approaches only depend on UserIDs and ItemIDs to represent users and items, disregarding the benefits of incorporating language information and leveraging the extensive knowledge of Language Models (LLMs) about the items. These methods might also encounter challenges in generalization due to limited training data for certain User/Item\nIDs [51]. Furthermore, some other works employ undisclosed models that possess initial recommendation capabilities [52], or small models trained on large-scale downstream task data [53]. However, the capabilities of these methods are demonstrated to be limited by the smaller-scale models. We have also observed some methods that utilize the SFT approach to bridge the gap between natural language processing tasks and recommendations, such as TALLRec [2]. TALLRec is a tuning framework for aligning LLMs with recommendations. However, solely relying on the SFT may only maximize the probability of generating the correct answer. To further enhance performance, we can introduce multiple incorrect answers into the system and establish pair-wise relationships between the correct and incorrect answers.\n# 2.2 Sequential Recommendation\nThe user-item interaction list is utilized in DMPO, which is similar to the sequential recommendation. The sequential recommendation aims to predict the next item for a user based on their historical interactions with items [27, 28]. Initially, Markov chain models were prevalent in sequential recommendation [29\u201332]. In recent times, deep learning approaches have gained popularity, including those based on RNNs [17, 33, 34], CNNs [35\u201337], and attention structures [12, 38\u201340]. However, these models typically focus solely on UserID and ItemID, limiting their ability to generalize across different domains. Some recent studies have emphasized improving the generalization capability of sequential recommendation models through techniques like pre-training [41, 42], data augmentation [43\u201345], debiasing [45\u201348], and robust optimization [40, 57]. However, these methods typically use UserIDs and ItemIDs to represent users and items, which can lead to insufficient training when the training data is limited. As a result, the potential of leveraging the strong generalization ability of LLMs, trained on extensive data and likely possessing rich knowledge about common items, remains largely unexplored.\n# 2.3 Cross-domain Recommendation\nTransfer learning aims to utilize knowledge from a source domain to enhance learning performance in the target domain or reduce the number of labeled examples needed in the target domain when achieving the same performance. [19, 20]. Cross-Domain Recommendation (CDR), inspired by transfer learning, is a promising approach to address data sparsity and the cold-start issue in the target domain by leveraging the auxiliary (source) domain. Initially, CMF [21] assumes a shared global user embedding matrix for all domains and factorizes matrices from multiple domains simultaneously. CoNet [8] transfers and integrates knowledge through cross-connections in feed-forward neural networks. Another set of CDR methods focuses on connecting user preferences across different domains, as demonstrated by studies such as [22\u201326]. These methods either utilize the user embedding learned in the source domain to initialize the user embedding in the target domain and restrict them to being closed or explicitly model the preference bridge. All the above methods are involved in exacting information from the source domain and transferring it to the target domain; however, the limitation persists because all the user/item embeddings are\nbuilt on certain user/item IDs. For LLMs, using natural language provides an excellent and natural medium to transfer the features learned across domains. Consequently, the potential of using LLMbased models to accomplish the cross-domain recommendation task still remains largely unexplored.\n# 3 PRELIMINARY\nIn this section, we introduce preliminary knowledge of DMPO. In previous sections, we mentioned conceiving DMPO as a pairwise ranking loss, which we consider as an important concept. We introduce what pair-wise ranking loss is and explain why DMPO shares a similar mathematical foundation with pair-wise ranking loss.\n# 3.1 Pair-Wise Ranking loss\nPair-wise ranking loss is a type of loss function used in supervised learning specifically for sorting problems. It calculates the loss by comparing the relative rankings of a pair of input elements. Commonly used options for this type of loss function include Hinge loss [66] and Bayesian Personalized Ranking (BPR) loss [65], among others. In recommendation systems, pair-wise ranking loss is typically employed to establish the pair-wise relationship between positive items and negative items. To further illustrate this concept, the mathematical equation of BPR loss is presented in Equation 1. \u2211\ufe01\n(1)\n\u2211\ufe01 ()\u2208D where the D refers to the dataset,\ud835\udc62refers to the user,\ud835\udc56and \ud835\udc57denotes the positive and negative sample pair. \ud835\udc53\ud835\udc62(\ud835\udc56) denotes the preference score for the user \ud835\udc62of the item \ud835\udc56. Commonly used preference score functions include cosine similarity and Pearson correlation coefficient. By utilizing the BPR loss, the \ud835\udc53\ud835\udc62(\ud835\udc56) is increased, while the \ud835\udc53\ud835\udc62(\ud835\udc57) is decreased. Compare to the DMPO mathematical equations, which are shown in Equation 3. We found that the fundamental idea behind it is similar, except the DMPO increases the likelihood of the preferred completions \ud835\udc66\ud835\udc64and decreases the likelihood of dispreferred completions \ud835\udc66\ud835\udc59. The likelihood in the DMPO refers to the joint generating probability of multiple tokens.\n# 4 METHODOLOGY\nIn this section, we introduce the DMPO, a streamlined framework that aims to align LLMs with recommendation tasks.\n# 4.1 Task Introduction\nThe DMPO is expected to distinguish whether the user enjoys the given items or not. The task is essentially a binary classification task. The user\u2019s item preferences history is inputted into the prompt. A pair of items from the user-item interaction list is then offered to make the prediction. One of the items, denoted as \ud835\udc56, has been rated highly by the user with a score greater than 3 out of 5, making it a positive sample. The other item, denoted as \ud835\udc57, has received a low rating from the user, scoring lower than 3. The model is expected to determine that the user is more likely to enjoy the positive sample item \ud835\udc56than the negative sample item \ud835\udc57by generating a recommendation that ranks the likelihood of the user giving a high rating to the items, and placing the item \ud835\udc56before the item \ud835\udc57.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/442c/442c98f5-e1a5-4c93-a5f0-69be41e96529.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: We first performed SFT on the base LLM model and then proceeded with DMPO. SFT samples and DMPO samples were constructed as inputs for training. Both SFT and DMPO were trained using LoRA. DMPO aims to maximize the probability of positive samples while minimizing the probability of multiple negative samples simultaneously.</div>\nTable 1: An instruction tuning sample for DMPO. \"Roman Holiday\" is the positive item, and the other movies serve as negative items. The order of positive and negative samples placed in the instruction input is randomly altered across different samples to prevent the LLMs from making predictions based on the input sequence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f748/f7484011-0462-465e-a118-583630a33137.png\" style=\"width: 50%;\"></div>\n# 4.2 Instruction Prompts\nWe provide an instruction sample for DMPO, as shown in Table 1. Firstly, construct the Task input, which includes the model\u2019s role settings, user-item interaction list, and positive and negative candidates. The order of positive and negative candidates in the Task input should be randomly arranged to prevent the model from generating answers based on the input. In the user-item interaction list, only the item title is used and separated by \"<>\" for easier identification by the model. In the Task output, for SFT, we only need to provide the correct answer as \"<positive sample, negative sample>\", with the positive sample listed before the negative sample. For DMPO, both the correct and incorrect answers should be provided in a list format \"[<positive sample, negative sample>, <negative sample, positive sample>]\". In the case of multiple negative sampling, multiple negative samples should be provided.\n# 4.3 Supervised Fine Tuning (SFT)\nSFT involves fine-tuning Language Models (LLMs) for the recommendation task using positive samples. The main objective of it is to\nmaximize the probability of each token in the correct answers. Additionally, SFT ensures that the answers generated by LLMs adhere to the correct format, avoiding ambiguous or evasive responses. The mathematical formula is shown in equation 2.\n\u2211\ufe01 () \u2208D \u2211\ufe01 where \ud835\udc65and \ud835\udc66represent the \"Task Input\" and \"Task Output\" in the instruction tuning data, respectively, \ud835\udc66\ud835\udc61is the \ud835\udc61-th token of the \ud835\udc66, \ud835\udc66<\ud835\udc61represents the tokens before \ud835\udc66\ud835\udc61, \u03a6 is the original parameters of the model, and D is the training set.\n# 4.4 Direct Multiple Preference Optimization (DMPO)\n# 4.4 Direct Multiple Preference Optimization (DMPO)\nDMPO builds upon SFT by not only maximizing the probability of generating correct answers but also minimizing the probability of generating multiple negative samples. This helps the model learn and establish the subtle and complex relationships between positive and negative samples, enabling it to capture their differences and connections. Additionally, DMPO helps suppress the generation probability of key tokens in negative samples. The introduction of multiple negative sampling in DMPO expands the range of sampled negative samples compared to the original DPO[3], promoting a more diverse and uniform learning of negative samples. This enhancement ultimately improves the performance of the model. The mathematical formula of DMPO is shown in the equation below: ;\n(3)\n\u2211\ufe01 where \ud835\udc58represents the number of negative samples, \ud835\udf0b\ud835\udf03represents the language model policy, \ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53denotes the reference policy, \ud835\udc65denotes the instruction inputs, \ud835\udf0erefers to the sigmoid activation function, \ud835\udc66\ud835\udc64and \ud835\udc66\ud835\udc59indicate the correct answers and incorrect answers that constructed as shown in Table 1, D denotes the instruction dataset. The gradient of the DMPO can be written as follows:\n\u2207\ud835\udf03LDMPO(\ud835\udf0b\ud835\udf03; \ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53) = \ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ee3/2ee3175f-342c-45b1-ae19-c26e8db86d69.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd decrease likelihood of \ud835\udc66\ud835\udc59</div>\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd where \ud835\udc58also refers to the number of negative samples. \u02c6\ud835\udc5f\ud835\udf03(\ud835\udc65,\ud835\udc66) = \ud835\udefdlog \ud835\udf0b\ud835\udf03(\ud835\udc66|\ud835\udc65) \ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53(\ud835\udc66|\ud835\udc65) denotes the reward implicitly defined by the language model \ud835\udf0b\ud835\udf03and reference model \ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53. The mathematical formulation of DMPO shows that instead of computing the probability of generating negative samples \u02c6\ud835\udc5f\ud835\udf03(\ud835\udc65,\ud835\udc66\ud835\udc59) using a single negative item, it is now computed as the average value of \ud835\udc58negative samples. It promotes a more diverse and uniform learning of negative samples, which enhances the performance in recommendation tasks.\n# 4.5 Prediction\nTo predict the generation probabilities of positive and negative candidates, we calculate the probability of each token in each candidate and obtain their average, denoted as \ud835\udc43. We then use this average probability \ud835\udc43to calculate the evaluation metric AUC. The mathematical formula of \ud835\udc43is shown in the equation below:\n(4)\nwhere \ud835\udc5b\ud835\udc66represents the number of tokens in the candidate \ud835\udc66that needs to be predicted, \ud835\udf0b\ud835\udf03represents the model we have established, \ud835\udc65denotes the instruction inputs.\n# 5.1 Dataset\nWe conduct experiments on three real-world public datasets, which are MovieLens 1M, Amazon Movies and TV, and Amazon Video Games. All three datasets contain a list of user-item interactions, with ratings of products. In our experiment, following the prior studies [4, 5], we categorize the ratings into two classes: ratings <= 3 indicate users who do not prefer the movie (negative item), while ratings > 3 indicate users who prefer the movie (positive item). Data filtering is performed based on the number of user-item interactions, ensuring there are a minimum of 5 positive and 5 negative samples, with the length of the interaction list limited to 40 items. The experiment is conducted in a few-shot learning scenario. For training, validation, and testing, we randomly selected 100, 100, and 1000 samples, respectively. Statistics and more details can be found in our released data.\n5.1.1 MovieLens 1M. It is a movie dataset commonly utilized in recommendation tasks, collected by the GroupLens Research Project at the University of Minnesota 2. The dataset comprises 1, 000, 209\n2https://grouplens.org/datasets/movielens/1m/\nanonymous ratings for around 3, 900 movies provided by 6, 040 MovieLens users who joined the platform in 2000. Each user has rated a minimum of 20 movies. Data collection took place on the MovieLens website over seven months from September 19th, 1997, to April 22nd, 1998. User ratings range from (1 \u22125). 5.1.2 Amazon Datasets. This dataset contains 142.8 million reviews and metadata of Amazon products from May 1996 to July 2014 3. It includes subsets like Books, Movies and TV, Video Games, Electronics, and more. For our study, we chose the \"Amazon Movies and TV\" and \"Amazon Video Games\" datasets, which include reviewer ID, item ID, item title, item brand, user-item interactions, ratings, and user reviews. In our experiment, we only use the item title and the user-item interaction history for DMPO to make the prediction.\n5.1.2\n# 5.2 Baseline\nWe compared DMPO with both LLM-based and traditional sequential recommendation methods. To ensure a fair comparison, we followed the same setup as DMPO for the LLM-based method, including tuning instructions and training, validation, and test data. For the traditional sequential recommendation methods, we generated train, valid, and test datasets using the same data as DMPO, which include item ID, user ID, and labels derived from user ratings. 5.2.1 LLM-based methods. We conducted an experiment to compare DMPO with previous work that employed LLM for the recommendation task. (i) TALLRec [2] is an LLM-based approach that aims to bridge the gap between training tasks for LLMs and recommendation tasks. It incorporates recommendation data into LLMs through rec-tuning, which involves constructing instructiontuning samples using recommendation data and conducting SFT. We use the implementation provided by the authors.4\nWe compared DMPO with both LLM-based and traditional sequential recommendation methods. To ensure a fair comparison, we followed the same setup as DMPO for the LLM-based method, including tuning instructions and training, validation, and test data. For the traditional sequential recommendation methods, we generated train, valid, and test datasets using the same data as DMPO, which include item ID, user ID, and labels derived from user ratings. 5.2.1 LLM-based methods. We conducted an experiment to compare DMPO with previous work that employed LLM for the recommendation task. (i) TALLRec [2] is an LLM-based approach that aims to bridge the gap between training tasks for LLMs and recommendation tasks. It incorporates recommendation data into LLMs through rec-tuning, which involves constructing instructiontuning samples using recommendation data and conducting SFT. We use the implementation provided by the authors.4 5.2.2 Traditional methods. Since DMPO utilizes the user\u2019s preference item list to construct the prompt, it bears similarities to sequential recommendation systems. Therefore, we experimented to compare DMPO with the traditional sequential methods. (i) GRU4Rec [17] is an RNN-based sequential recommender, which utilizes GRU to encode historical interactions. (ii) GCSAN [13] focuses on addressing session recommendation problems by utilizing Graph Neural Networks (GNNs) and self-attention networks. (iii) SASRec [77] is a classic transformer-based sequential recommender. (iv) STAMP [16] is a short-term attention/memory priority model. It combines long-term memory to capture general interests and short-term memory to account for current user interests (v) BERT4Rec[12] is a sequential recommendation model that utilizes deep bidirectional self-attention to model user behavior sequences. It addresses the limitations of unidirectional models by allowing each item in the sequence to fuse information from both left and right sides. (vi) SHAN[14] is a two-layer hierarchical attention network that considers user long-term preferences and dynamic characteristics. It incorporates both user-item and item-item interactions in a non-linear way. (vii) CORE[15] is a recommender that addresses the inconsistent prediction issue in the session-based recommendation by unifying the representation space for session embedding and item embeddings. It achieves this 3https://nijianmo.github.io/amazon/index.html\n5.2.2 Traditional methods. Since DMPO utilizes the user\u2019s preference item list to construct the prompt, it bears similarities to sequential recommendation systems. Therefore, we experimented to compare DMPO with the traditional sequential methods. (i) GRU4Rec [17] is an RNN-based sequential recommender, which utilizes GRU to encode historical interactions. (ii) GCSAN [13] focuses on addressing session recommendation problems by utilizing Graph Neural Networks (GNNs) and self-attention networks. (iii) SASRec [77] is a classic transformer-based sequential recommender. (iv) STAMP [16] is a short-term attention/memory priority model. It combines long-term memory to capture general interests and short-term memory to account for current user interests (v) BERT4Rec[12] is a sequential recommendation model that utilizes deep bidirectional self-attention to model user behavior sequences. It addresses the limitations of unidirectional models by allowing each item in the sequence to fuse information from both left and right sides. (vi) SHAN[14] is a two-layer hierarchical attention network that considers user long-term preferences and dynamic characteristics. It incorporates both user-item and item-item interactions in a non-linear way. (vii) CORE[15] is a recommender that addresses the inconsistent prediction issue in the session-based recommendation by unifying the representation space for session embedding and item embeddings. It achieves this\n3https://nijianmo.github.io/amazon/index.html 4https://github.com/SAI990323/TALLRec.\nbetter than all baselines with a t-test \ud835\udc5d<0.01.\nDataset\nGCSAN\nSTAMP\nSASRec\nGRU4Rec\nBERT4Rec\nSHAN\nCORE\nTALLrec\nOurMethod\nMovieLens 1M\n52.77\n53.75\n54.77\n52.75\n47.72\n51.18\n50.32\n67.79\n70.20\u2021\nAmazon Movie\n51.07\n53.04\n49.05\n47.51\n50.17\n52.88\n49.55\n57.62\n66.60\u2021\nAmazon Game\n52.11\n51.27\n50.23\n51.26\n50.81\n50.35\n49.96\n53.17\n64.51\u2021\nthrough a representation-consistent encoder and a robust distancemeasuring method.\n# 5.3 Evaluation Metric\nAlthough the task\u2019s output format involves ranking the likelihood of user preference for items, the task itself is fundamentally a binary classification problem. The model is tasked with distinguishing between the positive and negative samples and placing the positive sample ahead of the negative sample. The objective is to differentiate between positive and negative samples and establish pair-wise relationships between correct and incorrect answers. Therefore, we utilize a widely used evaluation metric in recommendation systems: the Area Under the Receiver Operating Characteristic (AUC).\n# 5.4 Implementation Details\nTo ensure sequence lengths, we truncated the user-item interaction list to 40, while ensuring a minimum of 5 positive and 5 negative items. For training, validation, and testing, we randomly selected 100, 100, and 1000 samples, respectively. For most experiments, we utilized Llama-2-7b-chat-hf 5 as the base model. The training of LoRA [1] was implemented using Llama Factory [71], which is an efficient LLM tuning framework. We examined the learning rates for all methods in the set 1e-3, 1e-4, 1e-5, 1e-6 to identify the optimal choice and adopted a cosine decay policy for the learning rate. For LoRA rank, we conducted experiments with 8, 16, 32 to determine the optimal choice, using q, k, and v as LoRA targets. In the case of the traditional method, we utilized the implementation provided by RecBole6, a project that replicates and enhances recommendation algorithms within a unified framework. Subsequently, we conducted five runs using different random seeds and presented the averaged results. Further details are provided in the code.\n# 6 ANALYSIS\nIn this section, we conduct analysis of our experiments to address the following research questions:\n- RQ1: How does DMPO perform compared to current LLM-based and traditional sequential recommendation models? - RQ2: What factors affect the performance of DMPO? - RQ3: What about the generalization ability of DMPO? How does it perform under cross-domain recommendation? - RQ4: Why does DMPO lead to improvements?\n- RQ1: How does DMPO perform compared to current LLM-based and traditional sequential recommendation models? - RQ2: What factors affect the performance of DMPO? - RQ3: What about the generalization ability of DMPO? How does it perform under cross-domain recommendation? - RQ4: Why does DMPO lead to improvements?\n5https://huggingface.co/meta-llama/Llama-2-7b-chat-hf 6https://github.com/RUCAIBox/RecBole\n# 6.1 Performance Comparison (RQ1)\nWe conducted the experiment in a few-shot learning scenario to compare the recommendation performance of different methods. The results are presented in Table 2. From the table, we draw the following observations: 1) Our method significantly outperforms both traditional sequential recommendation methods and LLM-based methods, demonstrating the superiority of the DMPO framework. 2) The LLM-based method shows improvements compared to the traditional sequential recommendation method, aligning with expectations but still lower than the DMPO. 3) The traditional sequential recommendation method performs similarly to random guessing (\ud835\udc34\ud835\udc48\ud835\udc36\u22480.5) in this scenario, indicating that its effectiveness is limited by the amount of training data and its inability to quickly learn recommendation capabilities with a small number of training samples.\n# 6.2 Ablation Study (RQ2)\n6.2.1 Number of Negative samples in DMPO. In the DMPO, multiple negative sampling is introduced into the framework, as discussed in section 4.2. One important question that arises is the optimal number of negative samples to include in DMPO and how this quantity affects performance. To address this question, we conducted ablation studies with varying numbers of negative samples denoted as \ud835\udc58. The results are shown in Figure 3. Based on the results, we make the following observations: 1) Increasing the number of negative samples in DMPO improves the performance compared to using only a single negative sample. 2) The performance tends to stabilize when the number of negative samples ranges from three to five. This improvement is derived from the introduction of more diverse negative samples for comparison. The multiple negative sampling enhances the diversity and uniformity of negative samples, enabling the model to establish more comprehensive and rich comparisons between positive and negative samples. However, the extent of this improvement is not infinite, and adding more negative samples beyond five may lead to additional computational overhead, reducing the efficiency of the model, and providing limited additional benefits. Moreover, the number of negative samples is also limited by the total length of the user-item interaction list. Therefore, we did not further increase the number of negative samples.\n6.2.2 DMPO and SFT with Varying Few-Shot Samples. We conducted experiments on the Amazon Movies and TV dataset to investigate two questions. The first question was about the difference in performance between using DMPO alone and using SFT+DMPO, and to clarify the roles of SFT and DPO. The second question was about the impact of different numbers of few-shot samples on the model. The experimental results are presented in Figure 4. The\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c459/c459ca88-92cd-45d3-ae53-9b14711cd1e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Performance for different numbers of negative samples in DMPO. The x-axis label represents the number of nega samples.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3958/395873c7-284d-4161-8e51-076b522c6c6d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance of using SFT+DMPO and using only DMPO is compared under different few-shot sample numbers.</div>\n<div style=\"text-align: center;\">Figure 4: Performance of using SFT+DMPO and using only DMPO is compared under different few-shot sample numbers.</div>\nobservation can be drawn as follows: 1) When the number of fewshot samples is less than or equal to 50, using DMPO alone yields lower results than using SFT+DMPO. However, when the number of few-shot samples increases to 100 or more, the performance gap between DMPO and SFT+DMPO is reduced. The performance of both approaches is similar, with a difference of only about 1.3 percent. This suggests that DMPO plays an important role in improving model performance. 2) As the number of few-shot samples increases, the performance of both DMPO and SFT+DMPO shows a clear upward trend. However, with only 20 few-shot samples, the AUC can reach around 87 percent of the AUC achieved with 200 few-shot samples. This demonstrates the high efficiency of DMPO.\nobservation can be drawn as follows: 1) When the number of fewshot samples is less than or equal to 50, using DMPO alone yields lower results than using SFT+DMPO. However, when the number of few-shot samples increases to 100 or more, the performance gap between DMPO and SFT+DMPO is reduced. The performance of both approaches is similar, with a difference of only about 1.3 percent. This suggests that DMPO plays an important role in improving model performance. 2) As the number of few-shot samples increases, the performance of both DMPO and SFT+DMPO shows a clear upward trend. However, with only 20 few-shot samples, the AUC can reach around 87 percent of the AUC achieved with 200 few-shot samples. This demonstrates the high efficiency of DMPO. 6.2.3 Generalization across Base Models. Recently, many researchers in the field of LLMs have publicly released their models and model parameters, including Llama-3-8B 7, gemma-7b 8, Phi-3-mini-128kinstruct 9 and Mistral-7B-Instruct-v0.2 10. We have a strong interest in evaluating the performance of our method on various opensource language model-based recommendation systems (LLMs), particularly those models that claim to have SOTA performance. As a result, we conducted experiments on multiple recently released\n7https://huggingface.co/meta-llama/Meta-Llama-3-8B 8https://huggingface.co/google/gemma-7b 9https://huggingface.co/microsoft/Phi-3-mini-128k-instruct 10https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e53/4e53de84-7751-4406-a10c-558a7f39b438.png\" style=\"width: 50%;\"></div>\nFigure 5: Performance for different base models of DMPO. In the label \"SFT+DMPO(1)\" and the label \"SFT+DMPO(3)\", the number in the brackets refers to the number of negative samples used in DMPO.\nLLMs using the Amazon Movies and TV. To validate the effectiveness of DMPO, we conducted experiments under three different conditions: (1) SFT; (2) SFT and DMPO using only one negative sample; (3) SFT and DMPO using three negative samples. In Section 6.2.1, we found that employing three negative samples in the DMPO yielded relatively stable improvements. The results, shown in Figure 5, demonstrate the following conclusions: 1) Firstly, on various base models of LLMs, performing DMPO can lead to an increase in AUC cross different base models; 2) The increasing number of negative sample brings further performance improvements; 3) Among all models, the Mistral-7B-Instruct-v0.2 shows the smallest improvement after utilizing DMPO, while the Llama-3-8B model shows the largest improvement, which may be related to the structures, train methods, and parameter choices of the different base models.\n# 6.3 Cross-Domain Analysis (RQ3)\nTo investigate the generalization ability of DMPO, we conducted cross-domain experiments between the \"Amazon Movies and TV\" and the \"Amazon Video Games\". Initially, we used \"Amazon Movies and TV\" as the source domain and \"Amazon Video Games\" as the target domain. Subsequently, we swapped the roles of the source and target domains for further analysis. In the cross-domain evaluation, the data from the source domain was used for training,\nTable 3: Comparison of cross-domain generalization performance between DMPO, traditional cross-domain recommendation models, and LLM-based models under few-shot scenarios. The reported results are presented as the AUC multiplied by 100, with boldface indicating the highest score, \u2021, which is significantly better than all baselines according to a t-test with \ud835\udc5d<0.01. \"Movie to Game\" means the \"Amazon Movies and TV\" is the source domain and the \"Amazon Video Games\" is the target domain. \"Game to Movie\" means that \"Amazon Video Games\" is the source domain and \"Amazon Movies and TV\" is the target domain.\nTable 3: Comparison of cross-domain generalization performance between DMPO, traditional cross-domain recommendation models, and LLM-based models under few-shot scenarios. The reported results are presented as the AUC multiplied by 100, with boldface indicating the highest score, \u2021, which is significantly better than all baselines according to a t-test with \ud835\udc5d<0.01. \"Movie to Game\" means the \"Amazon Movies and TV\" is the source domain and the \"Amazon Video Games\" is the target domain. \"Game to Movie\" means that \"Amazon Video Games\" is the source domain and \"Amazon Movies and TV\" is the target domain. Dataset BiTGCF CMF DTCDR CoNet CLFM DeepAPF TALLRec DMPO Movie to Game 52.57 49.16 48.29 49.7 51.25 50.49 54.61 62.73\u2021 Game to Movie 52.4 52.70 48.73 47.91 53.33 49.13 52.59 64.55\u2021\n\"Game to Movie\" means that \"Amazon Video Games\" is the source domain and \"Amazon Movies and TV\" is the target domain.\nDataset\nBiTGCF\nCMF\nDTCDR\nCoNet\nCLFM\nDeepAPF\nTALLRec\nDMPO\nMovie to Game\n52.57\n49.16\n48.29\n49.7\n51.25\n50.49\n54.61\n62.73\u2021\nGame to Movie\n52.4\n52.70\n48.73\n47.91\n53.33\n49.13\n52.59\n64.55\u2021\nwhile the data from the target domain was used for validation and testing. The datasets remained consistent with the original datasets described in Section 5.1.\n6.3.1 Baselines. We compared DMPO with the traditional crossdomain method and LLM-based cross-domain method. For the traditional cross-domain approach, we considered the following methods: (i) CMF [6], a well-known cross-domain recommender using Collective Matrix Factorization. (ii) CLFM [10], a cross-domain recommender that identifies shared and unique rating patterns between domains, focusing solely on shared patterns for knowledge transfer. (iii) DTCDR [76], a cross-domain model employing an adaptive embedding-sharing strategy based on multi-task learning to enhance recommendation performance on both richer and sparser domains simultaneously. (iv) DeepAPF [11], a crossdomain recommender that captures cross-domain common interests and domain-specific interests using an attention network. (v) BiTGCF [9], a cross-domain recommender based on a graph collaborative filtering network, integrating common user features with target domain-specific features.\n6.3.2 Performance Comparsion. The results presented in Table 3 indicate the following findings, 1) DMPO demonstrates a significant improvement compared to both LLM-based and traditional cross-domain methods. 2) The cross-domain recommendation result for DMPO was slightly lower compared to training on the target domain data, which is aligned with the exception. We also conduct the experiment to investigate the influence of varying numbers of negative samples in DMPO in cross-domain recommendation. The results present in Figure 6 indicate the following observations. 1) The increasing number of negative samples enhances the performance in the cross-domain context. 2) The performance tends to stabilize when the number of negative samples reaches three to five.\n# 6.4 Case Study (RQ4)\nTo investigate why DMPO leads to improvements, we conducted a case study. By inputting a user\u2019s historical sequence, positive and negative candidates, we obtained the conditional probability of each token of candidates. Through a specific example, as shown in Figure 8, we made the following observations: 1) the gap between positive and negative candidates increased after applying DMPO; 2) the conditional probability of key tokens in negative candidates was suppressed after DMPO, while this was not observed in the model that only underwent SFT. This suggests that through DMPO,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/640f/640f9e66-ae1e-419f-8e49-6b9990e3fbb7.png\" style=\"width: 50%;\"></div>\nFigure 6: Performance of the DMPO in cross-domain recommendation varies with different numbers of negative samples. The x-axis represents the number of negative samples\n<div style=\"text-align: center;\">Figure 6: Performance of the DMPO in cross-domain recommendation varies with different numbers of negative samples. The x-axis represents the number of negative samples</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba74/ba74e3fe-8d5a-4595-a437-d5210b68befe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Average conditional probabilities for positive and negative candidates of \"Movielens-1M\" test dataset. Predictions of the model using only SFT, the model using only DMPO, and the model using SFT+DMPO and the gap of probabilities between candidates are presented.</div>\nFigure 7: Average conditional probabilities for positive and negative candidates of \"Movielens-1M\" test dataset. Predictions of the model using only SFT, the model using only DMPO, and the model using SFT+DMPO and the gap of probabilities between candidates are presented.\nthe model achieved better discrimination between positive and negative candidates. The average conditional probability data for the \"Movielens-1M\" test dataset also supports this conclusion, as shown in Figure 7. The gap between positive and negative candidates\u2019 conditional probabilities increased from 0.4 percent in the model using only SFT to 2.0 percent in the model using SFT+DMPO, and the average conditional probabilities of negative candidates decreased. This further validates our previous findings. It is worth noting that the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5714/57143d38-9030-4869-af3e-7c77ae95c8e2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3aa3/3aa3ca21-fac4-4744-8247-218d887b3ee1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6550/65502cbc-fc75-4865-a92b-be7dfba7a3a3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: A case study sample illustrates the reason why DMPO can improve the performance. The user\u2019s historical sequence positive and negative candidates, and the conditional probabilities for generating each token are provided. model using only SFT made an incorrect prediction, while model using SFT+DMPO made the correct prediction.</div>\nabove conclusion can also be obtained by using DMPO alone, as shown in the label DMPO, which means that DMPO can provide performance benefits even when used independently.\n# .5 Explainable Recommendation System\nThe explainable recommendation system can improve the acceptance of recommended products, persuade users to purchase them, and even enhance the overall trust of the system [73]. The DMPO can provide interpretability for recommendation tasks [72]. As shown in Section 4.5, the model calculates the probability of generating each token in the candidates and assigns higher probabilities to the important tokens, indicating that the recommendation is made based on those crucial tokens. This explains why certain candidates are chosen for item recommendations.\n# 7 CONCLUSION\nIn this study, we introduced Direct Multiple Preference Optimization (DMPO), a streamlined framework that effectively aligns LLMs with recommendation tasks. DMPO can be seen as a pair-wise ranking loss that distinguishes between positive and negative samples. Through multiple negative sampling, DMPO enhances the performance of LLM-based recommenders by maximizing the probability of positive samples and minimizing the probability of multiple negative samples simultaneously. This enhances the diversity and uniformity of negative samples, enabling the model to learn and establish more comprehensive and accurate positive-negative\nsample relationships. Through extensive experiments, we compared the performance of DMPO with other methods, including LLM-based and traditional sequential recommendation approaches. Significant improvements were observed with DMPO in few-shot scenarios. Additionally, we evaluated the generalization ability of DMPO in cross-domain recommendation tasks, comparing it with traditional cross-domain methods and LLM-based cross-domain methods. DMPO exhibited superior performance in cross-domain recommendation. Ablation studies were conducted to analyze the factors contributing to the enhancements in DMPO, and case studies explored the reasons for the improvements. Moreover, DMPO is also highlighted as an explainable recommendation system. Moving forward, our future research will focus on exploring more efficient methods to leverage the recommendation capabilities of LLMs.\n# REFERENCES\n[1] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLora: Low-rank adaptation of large language models,\u201d arXiv preprint arXiv:2106.09685, 2021. [2] K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He, \u201cTallrec: An effective and efficient tuning framework to align large language model with recommendation,\u201d in Proceedings of the 17th ACM Conference on Recommender Systems, 2023, pp. 1007\u20131014. [3] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, \u201cDirect preference optimization: Your language model is secretly a reward model,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024. [4] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang, \u201cLightgcn: Simplifying and powering graph convolution network for recommendation,\u201d in SIGIR\u201920, 2020, pp. 639\u2013648. [5] Y. Zhang, T. Shi, F. Feng, W. Wang, D. Wang, X. He, and Y. Zhang, \u201cReformulating ctr prediction: Learning invariant feature interactions for recommendation,\u201d in Proceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, 2023, pp. 1386\u20131395. [6] A. P. Singh and G. J. Gordon, \u201cRelational learning via collective matrix factorization,\u201d in Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 2008, pp. 650\u2013658. [7] F. Zhu, C. Chen, Y. Wang, G. Liu, and X. Zheng, \u201cDtcdr: A framework for dualtarget cross-domain recommendation,\u201d in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 1533\u2013 1542. [8] G. Hu, Y. Zhang, and Q. Yang, \u201cConet: Collaborative cross networks for crossdomain recommendation,\u201d in Proceedings of the 27th ACM international conference on information and knowledge management, 2018, pp. 667\u2013676. [9] M. Liu, J. Li, G. Li, and P. Pan, \u201cCross domain recommendation via bi-directional transfer graph collaborative filtering networks,\u201d in Proceedings of the 29th ACM international conference on information & knowledge management, 2020, pp. 885\u2013 894. [10] \u2014\u2014, \u201cCross domain recommendation via bi-directional transfer graph collaborative filtering networks,\u201d in Proceedings of the 29th ACM international conference on information & knowledge management, 2020, pp. 885\u2013894. [11] H. Yan, X. Chen, C. Gao, Y. Li, and D. Jin, \u201cDeepapf: Deep attentive probabilistic factorization for multi-site video recommendation,\u201d TC, vol. 2, no. 130, pp. 17\u2013883, 2019. [12] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang, \u201cBert4rec: Sequential recommendation with bidirectional encoder representations from transformer,\u201d in Proceedings of the 28th ACM international conference on information and knowledge management, 2019, pp. 1441\u20131450. [13] Z. Fu, C. Wang, and J. Xu, \u201cGraph contextualized self-attention network for software service sequential recommendation,\u201d Future Generation Computer Systems, vol. 149, pp. 509\u2013517, 2023. [14] H. Ying, F. Zhuang, F. Zhang, Y. Liu, G. Xu, X. Xie, H. Xiong, and J. Wu, \u201cSequential recommender system based on hierarchical attention network,\u201d in IJCAI international joint conference on artificial intelligence, 2018. [15] Y. Hou, B. Hu, Z. Zhang, and W. X. Zhao, \u201cCore: simple and effective sessionbased recommendation within consistent representation space,\u201d in Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval, 2022, pp. 1796\u20131801. [16] Q. Liu, Y. Zeng, R. Mokhosi, and H. Zhang, \u201cStamp: short-term attention/memory priority model for session-based recommendation,\u201d in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, 2018, pp. 1831\u20131839. [17] Y. K. Tan, X. Xu, and Y. Liu, \u201cImproved recurrent neural networks for sessionbased recommendations,\u201d in Proceedings of the 1st workshop on deep learning for recommender systems, 2016, pp. 17\u201322. [18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [19] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and S. Y. Philip, \u201cGeneralizing to unseen domains: A survey on domain generalization,\u201d IEEE transactions on knowledge and data engineering, vol. 35, no. 8, pp. 8052\u20138072, 2022. [20] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, \u201cA comprehensive survey on transfer learning,\u201d Proceedings of the IEEE, vol. 109, no. 1, pp. 43\u201376, 2020. [21] A. P. Singh and G. J. Gordon, \u201cRelational learning via collective matrix factorization,\u201d in Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 2008, pp. 650\u2013658. [22] S. Kang, J. Hwang, D. Lee, and H. Yu, \u201cSemi-supervised learning for cross-domain recommendation to cold-start users,\u201d in Proceedings of the 28th ACM international conference on information and knowledge management, 2019, pp. 1563\u20131572. [23] T. Man, H. Shen, X. Jin, and X. Cheng, \u201cCross-domain recommendation: An embedding and mapping approach.\u201d in IJCAI, vol. 17, 2017, pp. 2464\u20132470. [24] W. Pan, E. Xiang, N. Liu, and Q. Yang, \u201cTransfer learning in collaborative filtering for sparsity reduction,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 24, no. 1, 2010, pp. 230\u2013235. [25] Y. Zhang, Y. Liu, P. Han, C. Miao, L. Cui, B. Li, and H. Tang, \u201cLearning personalized itemset mapping for cross-domain recommendation,\u201d 2020. [26] F. Zhu, Y. Wang, C. Chen, G. Liu, M. Orgun, and J. Wu, \u201cA deep framework for cross-domain and cross-system recommendations,\u201d arXiv preprint arXiv:2009.06215, 2020. [27] H. Fang, D. Zhang, Y. Shu, and G. Guo, \u201cDeep learning for sequential recommendation: Algorithms, influential factors, and evaluations,\u201d ACM Transactions on Information Systems (TOIS), vol. 39, no. 1, pp. 1\u201342, 2020. [28] S. Wang, L. Hu, Y. Wang, L. Cao, Q. Z. Sheng, and M. Orgun, \u201cSequential recommender systems: challenges, progress and prospects,\u201d arXiv preprint arXiv:2001.04830, 2019. [29] R. He and J. McAuley, \u201cFusing similarity models with markov chains for sparse sequential recommendation,\u201d in 2016 IEEE 16th international conference on data mining (ICDM). IEEE, 2016, pp. 191\u2013200.\n[30] T. Mahmood and F. Ricci, \u201cLearning and adaptivity in interactive recommender systems,\u201d in Proceedings of the ninth international conference on Electronic commerce, 2007, pp. 75\u201384. [31] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme, \u201cFactorizing personalized markov chains for next-basket recommendation,\u201d in Proceedings of the 19th international conference on World wide web, 2010, pp. 811\u2013820. [32] P. Wang, J. Guo, Y. Lan, J. Xu, S. Wan, and X. Cheng, \u201cLearning hierarchical representation model for nextbasket recommendation,\u201d in Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval, 2015, pp. 403\u2013412. [33] Q. Cui, S. Wu, Q. Liu, W. Zhong, and L. Wang, \u201cMv-rnn: A multi-view recurrent neural network for sequential recommendation,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 32, no. 2, pp. 317\u2013331, 2018. [34] T. Donkers, B. Loepp, and J. Ziegler, \u201cSequential user-based recurrent neural network recommendations,\u201d in Proceedings of the eleventh ACM conference on recommender systems, 2017, pp. 152\u2013160. [35] J. Tang and K. Wang, \u201cPersonalized top-n sequential recommendation via convolutional sequence embedding,\u201d in Proceedings of the eleventh ACM international conference on web search and data mining, 2018, pp. 565\u2013573. [36] A. Yan, S. Cheng, W.-C. Kang, M. Wan, and J. McAuley, \u201cCosrec: 2d convolutional neural networks for sequential recommendation,\u201d in Proceedings of the 28th ACM international conference on information and knowledge management, 2019, pp. 2173\u20132176. [37] F. Yuan, A. Karatzoglou, I. Arapakis, J. M. Jose, and X. He, \u201cA simple convolutional generative network for next item recommendation,\u201d in Proceedings of the twelfth ACM international conference on web search and data mining, 2019, pp. 582\u2013590. [38] W.-C. Kang and J. McAuley, \u201cSelf-attentive sequential recommendation,\u201d in 2018 IEEE international conference on data mining (ICDM). IEEE, 2018, pp. 197\u2013206. [39] C. Xu, J. Feng, P. Zhao, F. Zhuang, D. Wang, Y. Liu, and V. S. Sheng, \u201cLong-and short-term self-attention network for sequential recommendation,\u201d Neurocomputing, vol. 423, pp. 580\u2013589, 2021. [40] T. Zhang, P. Zhao, Y. Liu, V. S. Sheng, J. Xu, D. Wang, G. Liu, X. Zhou et al., \u201cFeature-level deeper self-attention network for sequential recommendation.\u201d in IJCAI, 2019, pp. 4320\u20134326. [41] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., \u201cTraining language models to follow instructions with human feedback,\u201d Advances in neural information processing systems, vol. 35, pp. 27 730\u201327 744, 2022. [42] F. Yuan, X. He, A. Karatzoglou, and L. Zhang, \u201cParameter-efficient transfer from sequential behaviors for user modeling and recommendation,\u201d in Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, 2020, pp. 1469\u20131478. [43] R. Qiu, Z. Huang, H. Yin, and Z. Wang, \u201cContrastive learning for representation degeneration problem in sequential recommendation,\u201d in Proceedings of the fifteenth ACM international conference on web search and data mining, 2022, pp. 813\u2013823. [44] Z. Wang, H. Liu, W. Wei, Y. Hu, X.-L. Mao, S. He, R. Fang, and D. Chen, \u201cMulti-level contrastive learning framework for sequential recommendation,\u201d in Proceedings of the 31st ACM International Conference on Information & Knowledge Management, 2022, pp. 2098\u20132107. [45] X. Xie, F. Sun, Z. Liu, S. Wu, J. Gao, J. Zhang, B. Ding, and B. Cui, \u201cContrastive learning for sequential recommendation,\u201d in 2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 2022, pp. 1259\u20131273. [46] Z. Wang, S. Shen, Z. Wang, B. Chen, X. Chen, and J.-R. Wen, \u201cUnbiased sequential recommendation with latent confounders,\u201d in Proceedings of the ACM Web Conference 2022, 2022, pp. 2195\u20132204. [47] Y. Zhang, F. Feng, X. He, T. Wei, C. Song, G. Ling, and Y. Zhang, \u201cCausal intervention for leveraging popularity bias in recommendation,\u201d in Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021, pp. 11\u201320. [48] Y. Zheng, C. Gao, X. Li, X. He, Y. Li, and D. Jin, \u201cDisentangling user interest and conformity for recommendation with causal embedding,\u201d in Proceedings of the Web Conference 2021, 2021, pp. 2980\u20132991. [49] S. Geng, S. Liu, Z. Fu, Y. Ge, and Y. Zhang, \u201cRecommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5),\u201d in Proceedings of the 16th ACM Conference on Recommender Systems, 2022, pp. 299\u2013315. [50] L. Li, Y. Zhang, and L. Chen, \u201cPersonalized prompt learning for explainable recommendation,\u201d ACM Transactions on Information Systems, vol. 41, no. 4, pp. 1\u201326, 2023. [51] Y. Hou, S. Mu, W. X. Zhao, Y. Li, B. Ding, and J.-R. Wen, \u201cTowards universal sequence representation learning for recommender systems,\u201d in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 585\u2013593. [52] Z. Cui, J. Ma, C. Zhou, J. Zhou, and H. Yang, \u201cM6-rec: Generative pretrained language models are open-ended recommender systems,\u201d arXiv preprint arXiv:2205.08084, 2022.\n[53] Z. Zhang and B. Wang, \u201cPrompt learning for news recommendation,\u201d in Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023, pp. 227\u2013237. [54] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, \u201cChat-rec: Towards interactive and explainable llms-augmented recommender system,\u201d arXiv preprint arXiv:2303.14524, 2023. [55] L. Wang and E.-P. Lim, \u201cZero-shot next-item recommendation using large pretrained language models,\u201d arXiv preprint arXiv:2304.03153, 2023. [56] Y. Koren, R. Bell, and C. Volinsky, \u201cMatrix factorization techniques for recommender systems,\u201d Computer, vol. 42, no. 8, pp. 30\u201337, 2009. [57] H. Wen, X. Yi, T. Yao, J. Tang, L. Hong, and E. H. Chi, \u201cDistributionally-robust recommendations for improving worst-case user experience,\u201d in Proceedings of the ACM Web Conference 2022, 2022, pp. 3606\u20133610. [58] H. Guo, R. Tang, Y. Ye, Z. Li, and X. He, \u201cDeepfm: a factorization-machine based neural network for ctr prediction,\u201d arXiv preprint arXiv:1703.04247, 2017. [59] Y. Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and Y. Yu, \u201cA bird\u2019s-eye view of reranking: from list level to page level,\u201d in Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, 2023, pp. 1075\u20131083. [60] Y. Xi, W. Liu, J. Lin, J. Zhu, B. Chen, R. Tang, W. Zhang, R. Zhang, and Y. Yu, \u201cTowards open-world recommendation with knowledge augmentation from large language models,\u201d arXiv preprint arXiv:2306.10933, 2023. [61] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020. [62] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023. [63] Y. Wang, Z. Jiang, Z. Chen, F. Yang, Y. Zhou, E. Cho, X. Fan, X. Huang, Y. Lu, and Y. Yang, \u201cRecmind: Large language model powered agent for recommendation,\u201d arXiv preprint arXiv:2308.14296, 2023. [64] K. Zhang, F. Zhao, Y. Kang, and X. Liu, \u201cMemory-augmented llm personalization with short-and long-term memory coordination,\u201d arXiv preprint arXiv:2309.11696, 2023. [65] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, \u201cBpr: Bayesian personalized ranking from implicit feedback,\u201d arXiv preprint arXiv:1205.2618, 2012.\n[66] C. Cortes and V. Vapnik, \u201cSupport-vector networks,\u201d Machine learning, vol. 20, pp. 273\u2013297, 1995. [67] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 9729\u20139738. [68] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in International conference on machine learning. PMLR, 2020, pp. 1597\u20131607. [69] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \u201cUnsupervised feature learning via nonparametric instance discrimination,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3733\u20133742. [70] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, \u201cUnsupervised embedding learning via invariant and spreading instance feature,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6210\u20136219. [71] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, and Y. Ma, \u201cLlamafactory: Unified efficient fine-tuning of 100+ language models,\u201d arXiv preprint arXiv:2403.13372, 2024. [Online]. Available: http://arxiv.org/abs/2403.13372 [72] R. Rafailov, J. Hejna, R. Park, and C. Finn, \u201cFrom \ud835\udc5fto \ud835\udc5e\u2217: Your language model is secretly a q-function,\u201d arXiv preprint arXiv:2404.12358, 2024. [73] J. L. Herlocker, J. A. Konstan, and J. Riedl, \u201cExplaining collaborative filtering recommendations,\u201d in Proceedings of the 2000 ACM conference on Computer supported cooperative work, 2000, pp. 241\u2013250. [74] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., \u201cMultitask prompted training enables zeroshot task generalization,\u201d arXiv preprint arXiv:2110.08207, 2021. [75] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, \u201cFinetuned language models are zero-shot learners,\u201d arXiv preprint arXiv:2109.01652, 2021. [76] F. Zhu, C. Chen, Y. Wang, G. Liu, and X. Zheng, \u201cDtcdr: A framework for dualtarget cross-domain recommendation,\u201d in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 1533\u2013 1542. [77] W.-C. Kang and J. McAuley, \u201cSelf-attentive sequential recommendation,\u201d in 2018 IEEE international conference on data mining (ICDM). IEEE, 2018, pp. 197\u2013206.\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenges of integrating Large Language Models (LLMs) into recommendation systems, highlighting the gap between the pre-training data of LLMs and the specific requirements of recommendation tasks. Previous methods have attempted to use LLMs for recommendations but have been limited in their effectiveness, necessitating a new approach to fully utilize LLMs in this domain.",
        "problem": {
            "definition": "The problem this paper aims to solve is the inefficacy of directly applying LLMs to recommendation tasks due to the mismatch between the data used for pre-training LLMs and the requirements of recommendation systems.",
            "key obstacle": "The main difficulty lies in the lack of effective negative sample learning in existing methods, which leads to overestimation of unseen incorrect answers and incorrect predictions."
        },
        "idea": {
            "intuition": "The idea for Direct Multi-Preference Optimization (DMPO) stems from recognizing the need to not only maximize the probability of positive samples but also to minimize the probability of multiple negative samples, thereby enhancing the model's ability to distinguish between them.",
            "opinion": "DMPO is proposed as a systematic approach to optimize the alignment of LLMs with recommendation tasks by incorporating multiple negative samples into the training process.",
            "innovation": "The key innovation of DMPO lies in its ability to utilize multiple negative samples, expanding the training data diversity and improving the model's performance compared to traditional methods that only consider single positive and negative samples."
        },
        "method": {
            "method name": "Direct Multi-Preference Optimization",
            "method abbreviation": "DMPO",
            "method definition": "DMPO is a framework that aims to align LLMs with recommendation tasks by maximizing the probability of generating correct answers while minimizing the probability of generating multiple incorrect answers.",
            "method description": "DMPO enhances LLM-based recommenders by leveraging a pair-wise ranking loss approach that incorporates multiple negative sampling.",
            "method steps": [
                "Perform Supervised Fine-Tuning (SFT) on the base LLM model.",
                "Introduce DMPO by constructing instruction tuning samples that include multiple negative samples.",
                "Train the model using the DMPO framework to optimize the generation of positive samples while suppressing negative samples."
            ],
            "principle": "The effectiveness of DMPO is underpinned by its use of multiple negative samples, which allows the model to learn more nuanced relationships between positive and negative samples, thereby improving recommendation accuracy."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three real-world public datasets: 'Movielens-1M', 'Amazon Movies and TV', and 'Amazon Video Games'. The experiments focused on few-shot scenarios with a structured user-item interaction list.",
            "evaluation method": "The performance of DMPO was assessed through metrics such as the Area Under the Receiver Operating Characteristic (AUC), comparing it against traditional recommendation methods and other LLM-based approaches."
        },
        "conclusion": "The study demonstrates that DMPO significantly enhances the recommendation capabilities of LLMs, outperforming traditional and other LLM-based methods in few-shot scenarios. The framework also exhibits strong generalization ability in cross-domain recommendation tasks, highlighting its potential as an effective approach for personalized ranking.",
        "discussion": {
            "advantage": "DMPO stands out due to its ability to effectively model the comparative relationships between positive and negative samples, leading to superior performance in recommendation tasks.",
            "limitation": "While DMPO improves upon existing methods, it may still encounter challenges related to computational efficiency when increasing the number of negative samples beyond a certain limit.",
            "future work": "Future research will focus on further optimizing the efficiency of DMPO and exploring additional strategies to leverage the capabilities of LLMs in recommendation systems."
        },
        "other info": {
            "code_link": "https://github.com/BZX667/DMPO",
            "datasets": [
                {
                    "name": "Movielens-1M",
                    "description": "A dataset with over 1 million movie ratings from users."
                },
                {
                    "name": "Amazon Movies and TV",
                    "description": "A dataset containing reviews and metadata for Amazon's Movies and TV products."
                },
                {
                    "name": "Amazon Video Games",
                    "description": "A dataset with reviews and metadata for Amazon's Video Games."
                }
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "8",
            "key information": "This paper addresses the challenges of integrating Large Language Models (LLMs) into recommendation systems, highlighting the gap between the pre-training data of LLMs and the specific requirements of recommendation tasks."
        },
        {
            "section number": "8.2",
            "key information": "The main difficulty lies in the lack of effective negative sample learning in existing methods, which leads to overestimation of unseen incorrect answers and incorrect predictions."
        },
        {
            "section number": "4.2",
            "key information": "DMPO is proposed as a systematic approach to optimize the alignment of LLMs with recommendation tasks by incorporating multiple negative samples into the training process."
        },
        {
            "section number": "7",
            "key information": "The study demonstrates that DMPO significantly enhances the recommendation capabilities of LLMs, outperforming traditional and other LLM-based methods in few-shot scenarios."
        },
        {
            "section number": "10.1",
            "key information": "While DMPO improves upon existing methods, it may still encounter challenges related to computational efficiency when increasing the number of negative samples beyond a certain limit."
        }
    ],
    "similarity_score": 0.7604593414541461,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Finetuning Large Language Model for Personalized Ranking.json"
}