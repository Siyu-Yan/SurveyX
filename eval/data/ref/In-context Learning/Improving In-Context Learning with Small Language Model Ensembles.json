{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.21868",
    "title": "Improving In-Context Learning with Small Language Model Ensembles",
    "abstract": "Large language models (LLMs) have shown impressive capabilities across various tasks, but their performance on domain-specific tasks remains limited. While methods like retrieval augmented generation and fine-tuning can help to address this, they require significant resources. In-context learning (ICL) is a cheap and efficient alternative but cannot match the accuracies of advanced methods. We present Ensemble SuperICL, a novel approach that enhances ICL by leveraging the expertise of multiple fine-tuned small language models (SLMs). Ensemble SuperICL achieves state of the art (SoTA) results on several natural language understanding benchmarks. Additionally, we test it on a medical-domain labelling task and showcase its practicality by using off-the-shelf SLMs fine-tuned on a general language task, achieving superior accuracy in large-scale data labelling compared to all baselines. Finally, we conduct an ablation study and sensitivity analyses to elucidate the underlying mechanism of Ensemble SuperICL. Our research contributes to the growing demand for efficient domain specialisation methods in LLMs, offering a cheap and effective method for practitioners.",
    "bib_name": "mojarradi2024improvingincontextlearningsmall",
    "md_text": "# Improving In-Context Learning with Small Language Model Ensembles\n# Abstract\nLarge language models (LLMs) have shown impressive capabilities across various tasks, but their performance on domain-specific tasks remains limited. While methods like retrieval augmented generation and fine-tuning can help to address this, they require significant resources. In-context learning (ICL) is a cheap and efficient alternative but cannot match the accuracies of advanced methods. We present Ensemble SuperICL, a novel approach that enhances ICL by leveraging the expertise of multiple fine-tuned small language models (SLMs). Ensemble SuperICL achieves state of the art (SoTA) results on several natural language understanding benchmarks. Additionally, we test it on a medical-domain labelling task and showcase its practicality by using off-the-shelf SLMs fine-tuned on a general language task, achieving superior accuracy in large-scale data labelling compared to all baselines. Finally, we conduct an ablation study and sensitivity analyses to elucidate the underlying mechanism of Ensemble SuperICL. Our research contributes to the growing demand for efficient domain specialisation methods in LLMs, offering a cheap and effective method for practitioners.1\narXiv:2410.21868v\n# 1 Introduction\nIn-context learning (ICL) is an effective method for adapting large language models (LLMs) to perform specific tasks without the need of updating model parameters through fine-tuning [Brown et al., 2020]. It involves prompting an LLM with few-shot training examples and a test input, allowing the LLM to infer the correct output from the provided context. While ICL is a time and cost-efficient method, it lacks the accuracy of more expensive methods such as fine-tuning and retrieval-augmented generation (RAG). Addressing this limitation is significant in light of two trends. First, there is a growing need for adapting LLMs to specific tasks or specialised domains, such as question answering on specific corpora (e.g. medical or legal documents) [Ling et al., 2023]. Second, increasing computational and data bottlenecks are creating a growing demand for methods that do not require large compute and data resources, unlike RAG and fine-tuning [Thompson et al., 2020]. Improving the performance of ICL can provide practitioners with cheap but highly accurate domain specialisation in LLMs.\n\u2217Correspondence to: Mehdi Mojarradi <m.mehdi.mojarradi@gmail.com>. 1Code: https://github.com/mehdimojarradi/Ensemble-SuperICL\n38th Conference on Neural Information Processing Systems (NeurIPS 2024). Workshop on Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024). Workshop on Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/caed/caed50f4-68d5-44be-839e-082109770315.png\" style=\"width: 50%;\"></div>\nFigure 1: Three stages in Ensemble SuperICL. In-context examples: k input-true label pairs are sampled from a dataset, along with a test input. Ensemble super context: the examples are passed to p small language models (SLMs) which each produce predicted labels and confidence scores, for the examples and the test input. Inference: a large language model (LLM) is given an instruction prompt along with the SLM-enhanced examples and test input, and must predict the label of the test input. The need for improved ICL performance has led to a line of studies dedicated to constructing prompts that achieve high and stable ICL performance [Gao et al., 2020, Liu et al., 2021, Su et al., 2022, Levy et al., 2022, Ye et al., 2023]. In Xu et al. [2023] and Yang et al. [2023], the authors proposed SuperICL, which appends the predictions of a fine-tuned small language model (SLM)2 to ICL prompts, combining the specialised knowledge of an expert model with the general language capabilities of an LLM. However, SuperICL requires fine-tuning the SLM on the target dataset, which assumes heavy compute resources and a large amount of high-quality data. This cancels the main benefit of ICL which is its low cost. Additionally fine-tuning may lead to data leakage issues. Contributions. We propose Ensemble SuperICL, an ICL method that enables an LLM to leverage the predictions and confidence scores of several off-the-shelf SLMs. We test Ensemble SuperICL on several natural language understanding benchmarks and show that it outperforms ICL, SLM, and SuperICL baselines. To demonstrate the transferability of expertise across tasks, we test Ensemble SuperICL on a medical labelling task, and show that our method can label large-scale domain-specific data more accurately than all baselines. We conduct an ablation study and sensitivity analyses to validate results.\n# 2 Ensemble Super In-Context Learning\nAs their parameter counts scale to billions, transformer-based language models begin to exhibit in-context learning abilities which allow them to perform tasks they were not trained to do [Brown et al., 2020]. Few-shot prompting leverages an LLM\u2019s ICL ability by providing the LLM with a few carefully chosen demonstrations (in-context examples) [Dong et al., 2022].3 Past works have explored both ensembling in-context demonstrations and incorporating SLMs in ICL. Khalifa et al. [2023] proposed demonstration ensembling, where an LLM splits demonstrations into subsets, produces output probabilities for each subset, and combines the probabilities for a final prediction. Sun et al. [2023] used a SLM to perform k-Nearest Neighbours demonstration search for ICL. Shen et al. [2024] used an LLM as a controller to access many expert models, small and large, for complicated tasks. Wang et al. [2024] developed an algorithm that uses a small language model to select optimal demonstrations from a set of annotated data.\nAs their parameter counts scale to billions, transformer-based language models begin to exhibit in-context learning abilities which allow them to perform tasks they were not trained to do [Brown et al., 2020]. Few-shot prompting leverages an LLM\u2019s ICL ability by providing the LLM with a few carefully chosen demonstrations (in-context examples) [Dong et al., 2022].3\nPast works have explored both ensembling in-context demonstrations and incorporating SLMs in ICL. Khalifa et al. [2023] proposed demonstration ensembling, where an LLM splits demonstrations into subsets, produces output probabilities for each subset, and combines the probabilities for a final prediction. Sun et al. [2023] used a SLM to perform k-Nearest Neighbours demonstration search for ICL. Shen et al. [2024] used an LLM as a controller to access many expert models, small and large, for complicated tasks. Wang et al. [2024] developed an algorithm that uses a small language model to select optimal demonstrations from a set of annotated data.\n2Henceforth in this paper, small language model (SLM) refers to a language model with less than 1B parameters that is fine-tuned on a task-specific dataset. 3As the dominant form of in-context learning (ICL), few-shot prompting is often used synonymously with ICL, as is the case in this work.\n2Henceforth in this paper, small language model (SLM) refers to a language model with less than 1B parameters that is fine-tuned on a task-specific dataset. 3As the dominant form of in-context learning (ICL), few-shot prompting is often used synonymously with ICL, as is the case in this work.\nTable 1: The seven small language models (SLMs) considered in our analysis, their sizes (parameter count), and their accuracy on four benchmark datasets and one case study dataset (MedMCQA). For the benchmarks, a version of the SLM fine-tuned on the benchmark tasks was used for evaluation (e.g., to evaluate MRPC, we used a version of RoBERTa-large fine-tuned on the MRPC dataset). For MedMCQA, all SLMs used were fine-tuned on one of the benchmark tasks, MNLI. Dashes indicate where fine-tuned SLMs were unavailable for a dataset or not considered.\nSLM\nSize\nSST-2\nMRPC\nMNLI\nCoLA\nMedMCQA\nMobileBERT\n25M\n-\n-\n-\n52.78\n-\nflan-t5-base\n248M\n-\n-\n88.68\n-\n70.43\nELECTRA-large\n335M\n96.56\n89.95\n90.28\n67.43\n29.86\nDeBERTa-large\n350M\n94.95\n89.71\n90.39\n64.06\n71.43\nRoBERTa-large\n356M\n96.44\n89.71\n88.68\n65.65\n61.57\nBART-large\n407M\n95.30\n87.50\n88.85\n-\n68.71\nT5-large\n770M\n-\n-\n-\n53.51\n-\nMost recently, Xu et al. [2023] and Yang et al. [2023] proposed SuperICL and SuperContext, both methods that enrich the in-context demonstrations typical of ICL with the predictions of a SLM as well as its confidence scores, represented as the sigmoid applied to the logit probabilities produced by the SLM for each prediction. The smaller models provide task-specific knowledge, acting as plug-in models to the LLM that possesses general language capabilities. Building on these past works, we propose Ensemble SuperICL, a method which leverages ICL to enable an LLM to draw on the expert knowledge of several SLMs. A flowchart of the Ensemble SuperICL process is shown in Figure 1. The first step in Ensemble SuperICL is selecting in-context examples where each example is a pair of input and true label. Next, two or more fine-tuned SLMs (which are fine-tuned directly on the task or on a general task) produce ensemble super context: one demonstration in Ensemble SuperICL consists of an input, the predicted labels and confidence scores of two to five SLMs on this in-context example, and the true label. Finally, a test question is concatenated with its predicted labels and confidence scores from SLMs and fed to the LLM. To summarise, the LLM uses the constructed context and test question, both enhanced with the predictions and confidence scores of several SLMs, to formulate a response. An example of Ensemble SuperICL on the SST-2 dataset in a 1-shot setting with two SLMs is shown in Appendix B. The motivation behind such context construction is that the LLM will be able to use the SLM predictions, their confidence scores, and the true label to both triangulate the correct response and learn how reliable a given SLM\u2019s predictions (and confidence) are.\nMost recently, Xu et al. [2023] and Yang et al. [2023] proposed SuperICL and SuperContext, both methods that enrich the in-context demonstrations typical of ICL with the predictions of a SLM as well as its confidence scores, represented as the sigmoid applied to the logit probabilities produced by the SLM for each prediction. The smaller models provide task-specific knowledge, acting as plug-in models to the LLM that possesses general language capabilities.\n# 3 Methods\nDatasets. We used five datasets for our experiments: four natural language understanding (NLU) benchmarks and one domain-specific dataset. The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for assessing NLU systems [Wang et al., 2019]. We used four of the eleven GLUE datasets to evaluate a range of NLU abilities: the Multi-Genre Natural Language Inference corpus (MNLI), the Stanford Sentiment Treebank (SST-2), the Microsoft Research Paraphrase Corpus (MRPC), and the Corpus of Linguistic Acceptability (CoLA) (Appendix A) [Dolan and Brockett, 2005, Socher et al., 2013, Williams et al., 2018, Warstadt et al., 2018]. In addition, the Medical Multiple-Choice Question Answering dataset (MedMCQA) contains over 183k medical entrance exam questions. Each question is assigned one of 21 medical subjects such as surgery, dental, and pathology. We task our models with inferring the subject of a given question. Ensemble SuperICL Models. We used Llama3-8b-Instruct as the LLM, and considered seven SLMs: MobileBERT, flan-t5-large, ELECTRA-large, DeBERTa-large, RoBERTa-large, BART-large, and T5-large (Table 1). We used fine-tuned versions of the SLMs from Hugging Face for each dataset, except for MedMCQA where we reused the MNLI fine-tuned SLMs. MNLI is one of the most popular dataset choices for fine-tuning SLMs, providing them with potentially transferable general language understanding abilities. We ran experiments with 2, 3, 4, and 5 choices of SLMs, and considered all possible combinations of SLMs over 0, 8, 16, 24, and 32-shot settings. Baselines. We consider three baselines. The first is traditional in-context learning with 8, 16, 24, and 32-shot examples, and the second is SuperICL, the original method that uses a single SLM. Since our\nTable 2: Ensemble SuperICL (E-SuperICL) outperforms all baselines on three natural language understanding benchmarks and a domain-specific labelling task (MedMCQA). We present accuracies of the best performing versions of each order of Ensemble SuperICL (2, 3, 4, and 5 SLMs) on four GLUE benchmark datasets and a medical domain labelling task based on the MedMCQA dataset. The baselines considered are ICL with Llama3-8b-Instruct, the original SuperICL which uses a single SLM [Xu et al., 2023, Yang et al., 2023], and a majority vote of the SLM predictions. Note: only four SLMs were considered in SST-2 and MRPC due to a lack of available fine-tuned versions.\nSST-2\nMRPC\nMNLI\nCoLA\nMedMCQA\nICL (Llama3-8b-Instruct)\n94.15\n75.25\n71.24\n55.43\n79.43\nSuperICL\n96.56\n89.22\n87.45\n67.21\n82.71\nSLM Majority vote\n96.67\n90.69\n91.39\n65.64\n68.14\nE-SuperICL 2\n97.13\n90.69\n88.41\n70.36\n84.29\nE-SuperICL 3\n97.02\n91.42\n90.25\n70.36\n83.71\nE-SuperICL 4\n96.79\n91.42\n90.47\n70.32\n82.43\nE-SuperICL 5\n-\n-\n91.27\n68.17\n80.57\nmethod demands the use of several small language models (SLMs), we construct a third baseline predictor: a simple majority vote of the SLM predictions. If there is no majority vote, as is possible in cases where only four SLMs were considered, the last appearing prediction is used. After considering several versions of majority vote predictors, including one weighted by the SLM confidence scores, we found that this approach yielded the highest accuracy across datasets, on average. This is because underperforming SLMs can be confidently wrong. Constructing the context. For each dataset, we randomly sample 8, 16, 24, or 32 question-answer pairs from the training set to use for context construction. For fair comparison and reproducibility, we use the same in-context demonstrations for a dataset across all experiments. Inference time. The ensemble super context, test input, and an instruction prompt are given to the LLM to predict the label of the test input (see Appendix C for specific prompt designs). For fair comparison and reproducibility, the LLM in our analyses predicts the token with the highest probability (greedy decoding) rather than sampling from the output distribution. All experiments were run on two NVIDIA A100 80GB GPUs.\n# 4 Results and Discussion\n# 4.1 Ensemble SuperICL outperforms nearly all baselines on each d\nA summary of the results can be seen in Table 2. Ensemble SuperICL boosted ICL performance by 3 to 20 percentage points across datasets, with greater gains on more challenging tasks, and outperforms all baselines on three out of four benchmark datasets: SST-2, MRPC, and CoLA. On MNLI, Ensemble SuperICL outperforms the ICL and SuperICL baselines, but the SLM majority vote baseline is marginally better. Extensive results from over 500 experiments containing all few-shot settings and SLM combinations are reported in Appendices D and E. Ensemble SuperICL outperforms all baselines on the MedMCQA labelling task. Note that the SLMs used were not fine-tuned on the target task. This suggests that ensembling SLMs (tuned on general reasoning tasks) can outperform ICL on a domain-specific task while preserving its low data and compute requirements.\nA summary of the results can be seen in Table 2. Ensemble SuperICL boosted ICL performance by 3 to 20 percentage points across datasets, with greater gains on more challenging tasks, and outperforms all baselines on three out of four benchmark datasets: SST-2, MRPC, and CoLA. On MNLI, Ensemble SuperICL outperforms the ICL and SuperICL baselines, but the SLM majority vote baseline is marginally better. Extensive results from over 500 experiments containing all few-shot settings and SLM combinations are reported in Appendices D and E.\nEnsemble SuperICL outperforms all baselines on the MedMCQA labelling task. Note that the SLMs used were not fine-tuned on the target task. This suggests that ensembling SLMs (tuned on general reasoning tasks) can outperform ICL on a domain-specific task while preserving its low data and compute requirements.\nSurprisingly, even low-performing SLMs were useful in boosting ICL performance. In the experiments on CoLA, we included two \u2018weak\u2019 SLMs that, despite being fine-tuned on the target task, performed worse than the LLM in ICL: MobileBERT and T5 performed with 52.78% and 53.51% accuracy, and Llama3-8b-Instruct ICL with 55.43% accuracy, yet the combination of MobileBERT and T5 through Ensemble SuperICL outperformed ICL under all few-shot regimes, with 62.21% accuracy at its highest (Table 6 in Appendix D). In MedMCQA, ELECTRA-large performed very poorly with 29.86% accuracy, compared to Llama3-8b-Instruct ICL with 79.43% accuracy, yet Ensemble SuperICL with ELECTRA-large outperformed ICL in three out of five few-shot regimes, with 82.00% accuracy at its highest (Table 7 in Appendix E). These results suggests that Ensemble\nTable 3: Ablation study on Ensemble SuperICL. We run ablation experiments on the best performing versions of Ensemble SuperICL for each dataset, focusing on three components. (a) Ctxt is the SLM predictions for the in-context examples; (b) Conf is the SLM confidence scores in both the in-context examples and the test input; (c) Test represents the SLM predictions for the test input. Note: since the best-performing Ensemble SuperICL on MNLI is zero-shot, we use the second best version (24-shot) in this table to enable ablations on the in-context examples.\nComponents\n(a) Ctxt\n(b) Conf\n(c) Test\nSST-2\nMRPC\nMNLI\nCoLA\nMedMCQA\n(1)\n\u00d7\n\u00d7\n\u2713\n96.90\n89.71\n84.76\n67.94\n78.57\n(2)\n\u2713\n\u00d7\n\u2713\n96.79\n89.71\n89.41\n67.25\n83.00\n(3)\n\u00d7\n\u2713\n\u2713\n97.02\n91.42\n86.94\n69.85\n78.86\n(4)\n\u2713\n\u2713\n\u2713\n97.13\n91.42\n91.14\n70.36\n84.29\nSuperICL with \u2018weak\u2019 SLMs may be sufficient to surpass ICL performance. We also demonstra that our results remain valid across random seeds (Appendix G).\n# 2 Ablation study: all components of Ensemble SuperICL are signif\nWe investigated the effects of three components on the best-performing versions of Ensemble SuperICL: (a) the SLM predictions for the in-context examples; (b) the confidence scores of the SLMs in both the in-context examples and test input; (c) the SLM predictions for the test input. The results of the ablation study are presented in Table 3, suggesting all components are essential for optimal performance. First (1), we retained only the SLM predictions in the test input, removing SLM predictions from in-context examples and the SLM confidence scores from the test input. On average, this has the most significant impact on performance, most notably in MNLI and MedMCQA where performance drops about 6 percentage points. Second (2), we retained SLM predictions in both the in-context examples and test input, but removed SLM confidence scores in both. This has the most significant impact on performance in SST-2, MRPC, and CoLA, but has the least impact on MNLI and MedMCQA. Third (3), we retained SLM predictions and confidence scores in the test input, but removed both from the in-context examples. On average, this has the least significant impact on performance.\n# 4.3 Limitations and future works\nFirst, the biggest limitation of Ensemble SuperICL is that the optimal configuration of hyperparameters, such as number of in-context examples, number of SLMs, and choice of SLMs, varies considerably across datasets (Appendix F). However, Ensemble SuperICL outperforms ICL regardless of hyperparameters, on average, suggesting it may still be useful without a judicious hyperparameter selection process. Further, the scope of this work was limited to text classification tasks. Yang et al. [2023] showed positive results for SuperICL on text generation tasks, which suggests Ensemble SuperICL should generalise to text generation as well. A follow-up study may therefore test Ensemble SuperICL on question answering datasets such as SQuAD 2.0, QNLI, and QQP [Rajpurkar et al., 2018, Wang et al., 2019, Chen et al., 2018]. In the same vein, a future work may expand beyond the medical domain to better evaluate the domain specialisation capabilities of Ensemble SuperICL.\n# 5 Conclusion\nWe present Ensemble SuperICL, a method that ensembles small language models (SLMs) to improve ICL classification accuracy while preserving its low time, compute, and data requirements. Ensemble SuperICL outperforms all baselines on three natural language understanding (NLU) benchmarks, as well as a medical classification task, showing promise in generalising to domain-specific classification tasks. Two results further support its case for real-world applications: (1) the \u2018harder\u2019 the task for an LLM (as measured by ICL accuracy), the more Ensemble SuperICL boosts ICL performance, and (2) even the combination of weak SLMs may be sufficient to obtain high performance. Result (2) maintains the \u2018cheap\u2019 nature of ICL while benefiting from the gains of using small language models, and result (1) makes the method attractive for labelling tasks that require domain expertise.\n# References\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs, 2018. Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third international workshop on paraphrasing (IWP2005), 2005. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv:2301.00234, 2022. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv:2012.15723, 2020. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Exploring demonstration ensembling for in-context learning. arXiv:2308.08780, 2023. Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. arXiv:2212.06800, 2022. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv:2305.18703, 2023. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv:2101.06804, 2021. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Gerardo Flores, George H Chen, Tom Pollard, Joyce C Ho, and Tristan Naumann, editors, Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248\u2013260. PMLR, 07\u201308 Apr 2022. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad. arXiv:1806.03822, 2018. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv:2209.01975, 2022. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. Text classification via large language models. arXiv:2305.08377, 2023. Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. The computational limits of deep learning. arXiv:2007.05558, 10, 2020.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36, 2024. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. arXiv preprint 1805.12471, 2018. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122. Association for Computational Linguistics, 2018. Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. Small models are valuable plug-ins for large language models. arXiv:2305.08848, 2023. Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, et al. Supervised knowledge makes large language models better in-context learners. arXiv:2312.15918, 2023. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. In International Conference on Machine Learning, pages 39818\u201339833. PMLR, 2023.\n# A Datasets\nWe used five datasets for our experiments, four of which are popular benchmark sets from the General Language Understanding Evaluation benchmark (GLUE).\nTable 4: Datasets, tasks, number of classes, metrics, and the number of evaluation examples for each dataset. All are classification tasks. Matthew\u2019s correlation coefficient is a popular metric for unbalanced datasets; it produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives).\nDataset\nTask\n# Classes\nMetric\n# Eval\nGlue-SST2 [Socher et al., 2013]\nsentiment\nprediction\n2\naccuracy\n872\nGlue-MRPC [Dolan and Brockett, 2005]\nparaphrase\ndetection\n2\naccuracy\n408\nGlue-MNLI [Williams et al., 2018]\nnatural language\ninference\n3\naccuracy\n9815\nGlue-CoLA [Warstadt et al., 2018]\ngrammatical\nacceptability\n2\nmatthew\u2019s\ncorrelation\ncoefficient\n1043\nMedMCQA [Pal et al., 2022]\nmedical subject\nclassification\n2\naccuracy\n700\nFor the domain-specific case study, we used the Medical Multiple-Choice Question Answering dataset (MedMCQA), a corpus of over 183k medical entrance exam questions [Pal et al., 2022]. Each question in MedMCQA is assigned one of 21 medical subjects such as Surgery, Dental, and Pathology. We created a randomly sampled, balanced dataset of 700 questions labelled as either Dental or Surgery (to maximise amount of test data). One example of the questions is given below:\nQuestion: Till what age tetracycline should not be given to prevent discoloration? Label: Dental\nB Example of Ensemble SuperICL procedure on the SST-2 datase\n# B Example of Ensemble SuperICL procedure on the SST-2 dataset\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98d1/98d14948-02c0-456c-8675-ecca112dc0ee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Example of Ensemble SuperICL procedure on the SST-2 dataset.</div>\n# C Prompt designs for each dataset\nThis section presents the detailed prompt designs for each dataset:\n# MNLI:\n\u2018role\u2019: \u2018system\u2019, \u2018content\u2019: You are tasked with determining the relationship between a pair of sentences as entailment (the hypothesis is a true conclusion from the premise), contradiction (the hypothesis contradicts the premise), or neutral (the hypothesis neither necessarily follows from nor contradicts the premise). RoBERTa-Large is a language model finetuned on this task, and you may use its output as an aid to your judgement. Fill in your answer after \u2018Label: \u2019 at the end of the prompt. [in-context examples]\u2019 \u2018role\u2019: \u2018user\u2019, \u2018content\u2019: \u2018[test example] Label: \u2019,\nSST-2:\n\u2018role\u2019: \u2018system\u2019, \u2018content\u2019: You are tasked with predicting the sentiment of a given sentence (positive or negative). RoBERTa-Large is a language model fine-tuned on this task, and you may use its output as an aid to your judgement. Fill in your answer after \u2018Label: \u2019 at the end of the prompt. [in-context examples]\u2019 \u2018role\u2019: \u2018user\u2019, \u2018content\u2019: \u2018[test example] Label: \u2019,\nMRPC:\n\u2018role\u2019: \u2018system\u2019, \u2018content\u2019: You are tasked with determining whether two sentences are semantically equivalent (equivalent or not_equivalent). RoBERTa-Large is a language model fine-tuned on this task, and you may use its output as an aid to your judgement. Fill in your answer after \u2018Label: \u2019 at the end of the prompt. [in-context examples]\u2019 \u2018role\u2019: \u2018user\u2019, \u2018content\u2019: \u2018[test example] Label: \u2019,\nCoLA:\n\u2018role\u2019: \u2018system\u2019, \u2018content\u2019: You are tasked with determining whether the grammar of a given sentence is correct (acceptable or unacceptable). RoBERTa-Large is a language model fine-tuned on this task, and you may use its output as an aid to your judgement. Fill in your answer after \u2018Label: \u2019 at the end of the prompt. [in-context examples]\u2019 \u2018role\u2019: \u2018user\u2019, \u2018content\u2019: \u2018[test example] Label: \u2019,\n\u2018role\u2019: \u2018system\u2019, \u2018content\u2019: You are tasked with determining whether the grammar of a given sentence is correct (acceptable or unacceptable). RoBERTa-Large is a language model fine-tuned on this task, and you may use its output as an aid to your judgement. Fill in your answer after \u2018Label: \u2019 at the end of the prompt. [in-context examples]\u2019 \u2018role\u2019: \u2018user\u2019, \u2018content\u2019: \u2018[test example] Label: \u2019,\n\u2018role\u2019: \u2018system\u2019, \u2018content\u2019: You are tasked with determining the medical subject that a given question belongs to (Dental or Surgery). RoBERTa-Large is a language model fine-tuned on this task, and you may use its output as an aid to your judgement. Fill in your answer after \u2018Label: \u2019 at the end of the prompt. [in-context examples]\u2019 \u2018role\u2019: \u2018user\u2019, \u2018content\u2019: \u2018[test example] Label: \u2019,\n# D Full results of experiments on benchmark datasets\nThis section reports the results for all experiments conducted on the benchmark datasets where the small language models (SLMs) used were fine-tuned on the dataset being tested.\nTable 5: Ensemble SuperICL outperforms all baselines, except SLM majority vote on the MNLI-m dataset. Results of all orders of Ensemble SuperICL with Llama3-8b-Instruct and up to 5 SLMs on three datasets from the General Language Understanding Evaluation benchmark (GLUE). The first two row groups contain baseline comparisons: in-context learning (ICL) where the LLM can only reference demonstrations, the performance of each fine-tuned small language model (SLM) in a zero-shot setting, and the performance of a majority vote classifier with all SLMs. The majority vote classifier is defined as the mode of the set of SLM predictions.\nMNLI-m\nSST-2\nMRPC\nModel\n0-\nshot\n8-\nshot\n16-\nshot\n24-\nshot\n32-\nshot\n0-\nshot\n8-\nshot\n16-\nshot\n24-\nshot\n32-\nshot\n0-\nshot\n8-\nshot\n16-\nshot\n24-\nshot\n32-\nshot\nICL (Llama3-8b-Instruct)\n63.62\n68.63\n69.48\n71.24\n67.90\n89.45\n90.83\n93.81\n94.04\n94.15\n67.16\n66.91\n66.18\n71.57\n75.25\nRoBERTa\n88.68\n-\n-\n-\n-\n96.44\n-\n-\n-\n-\n89.71\n-\n-\n-\n-\nDeBERTa\n90.39\n-\n-\n-\n-\n94.95\n-\n-\n-\n-\n89.71\n-\n-\n-\n-\nBART\n88.85\n-\n-\n-\n-\n95.30\n-\n-\n-\n-\n87.50\n-\n-\n-\n-\nflan-t5\n88.68\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nELECTRA\n90.28\n-\n-\n-\n-\n96.56\n-\n-\n-\n-\n89.95\n-\n-\n-\n-\nMajority vote\n91.39\n-\n-\n-\n-\n96.67\n-\n-\n-\n-\n90.69\n-\n-\n-\n-\nR\n86.71\n83.48\n80.48\n81.78\n78.59\n94.95\n96.10\n96.45\n96.22\n96.22\n89.71\n87.50\n88.48\n87.01\n87.26\nD\n88.26\n84.70\n83.50\n83.74\n79.53\n94.04\n95.07\n95.30\n95.76\n95.76\n89.71\n87.50\n88.24\n86.28\n86.52\nB\n87.75\n85.23\n82.23\n82.69\n80.26\n94.73\n95.41\n95.76\n96.10\n96.22\n87.75\n86.28\n86.52\n85.78\n84.56\nf\n85.60\n83.50\n80.79\n81.48\n78.15\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nE\n88.42\n87.45\n85.09\n86.38\n81.38\n95.41\n96.45\n96.33\n96.56\n96.45\n89.95\n88.73\n89.22\n88.48\n87.01\nR+D\n90.70\n84.66\n82.61\n85.81\n83.78\n95.64\n95.64\n95.99\n96.56\n96.67\n89.71\n89.95\n89.71\n89.22\n87.50\nR+B\n89.96\n84.86\n83.79\n86.07\n83.93\n95.76\n96.10\n96.10\n96.67\n96.45\n90.44\n89.71\n88.97\n88.97\n86.03\nR+f\n89.15\n86.00\n84.20\n85.62\n83.40\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+E\n90.67\n85.20\n84.74\n87.33\n84.98\n96.33\n96.33\n96.67\n97.02\n97.13\n90.20\n90.44\n89.95\n89.95\n88.73\nD+B\n90.60\n85.79\n84.30\n86.38\n84.14\n95.76\n95.99\n95.87\n95.87\n95.87\n90.20\n89.95\n88.97\n86.77\n83.82\nD+f\n90.25\n87.40\n85.78\n87.12\n84.60\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nD+E\n91.01\n86.59\n86.02\n88.09\n85.85\n96.33\n96.10\n96.33\n96.67\n96.79\n90.20\n90.69\n90.20\n89.22\n88.97\nB+f\n89.32\n86.72\n83.75\n86.12\n82.07\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nB+E\n90.59\n87.68\n87.13\n88.41\n86.48\n96.22\n95.87\n96.10\n96.90\n96.79\n90.93\n90.44\n90.44\n89.71\n88.73\nf+E\n89.94\n87.83\n86.93\n88.15\n85.90\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+D+B\n90.78\n85.79\n86.29\n89.36\n87.63\n96.22\n95.76\n95.76\n95.99\n96.10\n91.18\n90.44\n90.93\n89.46\n89.46\nR+D+f\n90.36\n87.83\n87.56\n89.81\n88.24\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+D+E\n91.14\n86.69\n86.64\n89.29\n87.55\n96.67\n95.99\n96.67\n97.02\n96.90\n90.93\n91.42\n90.93\n90.93\n90.20\nR+B+f\n89.89\n87.23\n87.44\n88.88\n87.29\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+B+E\n90.82\n86.75\n87.09\n89.26\n86.79\n96.33\n96.01\n96.56\n96.79\n96.56\n91.18\n90.93\n91.18\n90.44\n89.46\nR+f+E\n90.69\n88.67\n87.34\n89.30\n88.30\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nD+B+f\n90.69\n87.63\n87.79\n89.36\n87.56\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nD+B+E\n91.21\n88.15\n87.87\n89.32\n87.87\n95.87\n95.99\n96.45\n96.67\n96.79\n90.44\n90.69\n90.44\n88.24\n89.22\nD+f+E\n90.97\n89.60\n88.24\n89.37\n88.97\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nB+f+E\n90.82\n89.58\n89.05\n90.25\n89.48\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+D+B+f\n90.54\n88.47\n90.02\n89.65\n89.80\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+D+B+E\n91.10\n88.60\n89.66\n89.79\n89.90\n96.33\n95.99\n96.45\n96.79\n96.56\n91.42\n91.42\n90.69\n90.93\n90.20\nR+D+f+E\n91.03\n88.74\n89.99\n90.23\n89.77\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nD+B+f+E\n91.04\n89.52\n90.47\n90.34\n90.04\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+B+f+E\n90.76\n89.55\n90.39\n90.39\n89.95\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR+D+B+f+E\n91.27\n90.40\n90.96\n91.14\n90.89\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nTable 6: Ensemble SuperICL outperforms all baselines on the Corpus of Linguistic Acceptability dataset (CoLA). Results of all orders of Ensemble SuperICL with Llama3-8b-Instruct and up to 5 SLMs on CoLA. The first two row groups contain baseline comparisons: in-context learning (ICL) where the LLM can only use demonstrations as reference, the performance of each fine-tuned small language model (SLM) in a zero-shot setting, and the performance of a majority vote classifier with all SLMs. The majority vote classifier is defined as the mode of the set of SLM predictions. The metric reported in this table is Matthew\u2019s Correlation.\nCoLA\nModel\n0-\nshot\n8-\nshot\n16-\nshot\n24-\nshot\n32-\nshot\nICL (Llama3-8b-Instruct)\n52.16\n55.19\n55.43\n52.52\n54.90\nRoBERTa (R)\n65.65\n-\n-\n-\n-\nDeBERTa (D)\n64.06\n-\n-\n-\n-\nMobileBERT (M)\n52.78\n-\n-\n-\n-\nELECTRA (E)\n67.43\n-\n-\n-\n-\nT5 (T)\n53.51\n-\n-\n-\n-\nMajority vote\n65.64\n-\n-\n-\n-\nR\n65.92\n65.67\n65.67\n65.05\n65.00\nD\n64.56\n63.83\n63.60\n64.61\n64.58\nM\n54.70\n60.33\n57.39\n57.67\n56.76\nE\n67.70\n67.21\n67.21\n66.76\n67.01\nT\n55.99\n57.84\n56.18\n57.00\n55.29\nR+D\n68.96\n68.47\n67.71\n69.25\n67.44\nR+M\n66.06\n64.49\n63.35\n64.82\n64.31\nR+E\n69.76\n70.26\n70.36\n69.34\n69.87\nR+T\n66.19\n66.29\n64.56\n67.46\n66.03\nD+M\n64.13\n64.47\n66.14\n66.36\n65.92\nD+E\n68.05\n68.25\n69.09\n68.80\n69.57\nD+T\n65.32\n65.17\n67.10\n68.25\n67.14\nM+E\n66.17\n69.20\n67.88\n67.04\n68.18\nM+T\n58.85\n62.21\n59.24\n59.95\n59.00\nE+T\n67.80\n65.77\n66.11\n66.67\n69.28\nR+D+M\n67.51\n66.67\n64.78\n65.28\n64.52\nR+D+E\n69.98\n68.47\n68.74\n69.24\n69.83\nR+D+T\n68.37\n64.67\n65.11\n66.45\n67.31\nR+M+E\n67.44\n66.67\n68.63\n70.36\n69.78\nR+M+T\n67.34\n66.14\n65.83\n66.45\n67.17\nR+E+T\n68.78\n64.61\n65.33\n66.94\n67.42\nD+M+E\n68.33\n66.93\n67.82\n68.41\n68.59\nD+M+T\n65.80\n62.52\n65.67\n66.70\n67.21\nD+E+T\n68.28\n64.30\n66.57\n67.16\n69.80\nM+E+T\n64.11\n64.73\n65.56\n67.20\n66.95\nR+D+M+E\n69.52\n69.02\n68.20\n68.86\n67.50\nR+D+M+T\n68.59\n64.78\n66.16\n66.82\n66.52\nR+D+E+T\n68.88\n68.41\n70.32\n67.37\n67.27\nD+M+E+T\n69.45\n65.58\n65.57\n68.28\n67.73\nR+M+E+T\n68.17\n65.67\n66.91\n67.91\n65.54\nR+D+M+E+T\n69.44\n63.92\n68.17\n67.49\n66.27\n# Full results of experiments from MedMCQA case study\nRecall we randomly sampled 350 questions labelled as both surgery and dental for a total of 700 questions for our experiments. Given a question from these 700, the LLM is tasked with assigning the correct subject (dental or surgery). To show the practicality of our method, we deploy Ensemble SuperICL using small language models (SLMs) fine-tuned on MNLI, a general natural language understanding task.\nTable 7: Ensemble SuperICL outperforms all baselines on the MedMCQA labelling task, without the need for the small language models to be fine-tuned on medical domain tasks. Results of all orders of Ensemble SuperICL on the MedMCQA labelling task with Llama3-8b-Instruct and up to 5 SLMs, all fine-tuned on MNLI. The first two row groups contain baseline comparisons: in-context learning (ICL) where the LLM can only reference demonstrations, the performance of each fine-tuned small language model (SLM) in a zero-shot setting, and the performance of a majority vote classifier with all SLMs. The majority vote classifier is defined as the mode of the set of SLM predictions.\nMedMCQA\nModel\n0-\nshot\n8-\nshot\n16-\nshot\n24-\nshot\n32-\nshot\nICL (Llama3-8b-Instruct)\n70.29\n75.29\n74.14\n78.00\n79.43\nRoBERTa (MNLI-tuned)\n61.57\n-\n-\n-\n-\nDeBERTa (MNLI-tuned)\n71.43\n-\n-\n-\n-\nBART (MNLI-tuned)\n68.71\n-\n-\n-\n-\nflan-t5 (MNLI-tuned)\n70.43\n-\n-\n-\n-\nELECTRA (MNLI-tuned)\n29.86\n-\n-\n-\n-\nMajority vote\n68.14\n-\n-\n-\n-\nR\n71.43\n82.29\n78.57\n82.71\n80.00\nD\n74.43\n82.00\n82.71\n82.71\n82.29\nB\n72.00\n80.29\n80.43\n80.00\n81.14\nf\n75.43\n78.71\n81.71\n79.71\n82.00\nE\n57.00\n78.57\n79.43\n82.00\n80.00\nR+D\n77.57\n79.29\n81.71\n82.57\n82.71\nR+B\n76.57\n82.14\n80.29\n82.43\n80.57\nR+f\n76.86\n83.29\n83.00\n82.14\n82.00\nR+E\n70.14\n78.86\n82.29\n81.57\n82.57\nD+B\n78.29\n80.00\n80.71\n81.57\n80.14\nD+f\n79.71\n80.71\n84.29\n82.29\n82.57\nD+E\n72.57\n81.57\n81.71\n81.29\n79.71\nB+f\n78.00\n81.57\n82.86\n81.43\n82.14\nB+E\n71.29\n78.29\n77.43\n79.14\n79.71\nf+E\n74.29\n79.00\n81.43\n81.57\n81.71\nR+D+B\n78.00\n82.71\n81.43\n81.43\n80.29\nR+D+f\n78.86\n80.57\n82.86\n81.00\n80.00\nR+D+E\n75.57\n82.00\n79.71\n83.00\n80.14\nR+B+f\n77.86\n80.14\n83.14\n81.57\n78.86\nR+B+E\n74.86\n78.86\n79.57\n82.29\n80.00\nR+f+E\n74.86\n82.29\n82.29\n83.71\n80.71\nD+B+f\n79.00\n79.71\n81.00\n82.00\n79.29\nD+B+E\n74.14\n79.14\n79.43\n80.14\n79.00\nD+f+E\n77.00\n81.29\n80.29\n82.57\n80.14\nB+f+E\n75.00\n80.57\n80.57\n80.86\n79.00\nR+D+B+f\n78.71\n82.43\n81.43\n81.57\n79.14\nR+D+B+E\n74.43\n81.71\n80.14\n81.43\n79.43\nR+D+f+E\n77.43\n81.57\n79.86\n82.00\n80.29\nD+B+f+E\n77.00\n80.00\n80.43\n81.43\n78.57\nR+B+f+E\n76.43\n81.29\n79.43\n81.71\n80.57\nR+D+B+f+E\n76.43\n79.29\n80.29\n80.57\n79.43\n# F Best hyperparameters of Ensemble SuperICL\n<div style=\"text-align: center;\">able 8 gives the optimal set of Ensemble SuperICL hyperparameters for each dataset. Table 8: The optimal hyperparameters for Ensemble SuperICL can vary across dataset</div>\n# of few-shot\nexamples\n# of SLMs\nchoice of\nbase SLMs\nSLMs\nfine-tuned on\nSST-2\n32\n2\nRoBERTa-Large,\nELECTRA-Large\nSST-2\nMRPC\n8\n3,4\nRoBERTa-Large,\nDeBERTa-Large,\nBART-Large,\nELECTRA-Large\nMRPC\nMNLI\n0\n5\nRoBERTa-Large,\nDeBERTa-Large,\nBART-Large,\nflan-t5-base,\nELECTRA-Large\nMNLI\nCoLA\n16,24\n2,3\nRoBERTA-Large,\nMobileBERT,\nELECTRA-Large\nCoLA\nMedMCQA\n16\n2\nDeBERTa-Large,\nflan-t5-base\nMNLI\n# G Variance in Ensemble SuperICL is generally low\nTable 9 shows that our results are robust to choosing different in-context examples.\nTable 9: Sensitivity of Ensemble SuperICL to varying in-context examples. The best performing versions of Ensemble SuperICL for each dataset with different seeds for in-context examples. Var = variance. Note: since the best-performing Ensemble SuperICL on MNLI is zero-shot, the second best version (24-shot) was used in this table to enable ablations on the in-context examples.\nRandom seed\nVar\n42\n0\n1\n2\n3\nSST-2\nICL\n94.15\n93.81\n94.27\n93.69\n95.07\n0.29\nSLM MV\n96.67\n96.67\n96.67\n96.67\n96.67\n-\nE-SuperICL\n97.13\n97.02\n96.79\n97.02\n96.56\n0.05\nMRPC\nICL\n75.25\n75.49\n74.51\n74.27\n73.78\n0.50\nSLM MV\n90.69\n90.69\n90.69\n90.69\n90.69\n-\nE-SuperICL\n91.42\n89.95\n91.18\n88.73\n90.69\n1.18\nMNLI\nICL\n71.24\n62.28\n71.37\n70.10\n70.13\n14.57\nSLM MV\n91.39\n91.39\n91.39\n91.39\n91.39\n-\nE-SuperICL\n91.14\n91.35\n91.21\n91.13\n91.50\n0.02\nCoLA\nICL\n55.43\n54.50\n54.58\n53.71\n55.07\n0.42\nSLM MV\n65.64\n65.64\n65.64\n65.64\n65.64\n-\nE-SuperICL\n70.36\n70.25\n69.73\n69.00\n68.20\n0.82\nMed-\nMCQA\nICL\n79.43\n73.86\n80.00\n77.57\n75.57\n6.68\nSLM MV\n68.14\n68.14\n68.14\n68.14\n68.14\n-\nE-SuperICL\n84.29\n81.43\n82.00\n80.00\n82.43\n2.44\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of in-context learning (ICL) in large language models (LLMs) for domain-specific tasks, highlighting the need for efficient methods that do not require extensive computational resources or large datasets.",
        "problem": {
            "definition": "The main issue is that while ICL is a cost-effective method for adapting LLMs to specific tasks, it often fails to achieve the accuracy levels of more resource-intensive methods like fine-tuning.",
            "key obstacle": "Existing methods require significant compute resources and high-quality data, which are not always available, thus limiting their applicability in many practical scenarios."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that combining the strengths of multiple small language models (SLMs) can enhance the performance of ICL without the need for extensive fine-tuning.",
            "opinion": "The proposed Ensemble SuperICL method leverages the predictions and confidence scores of several fine-tuned SLMs to improve the accuracy of ICL.",
            "innovation": "The primary innovation lies in the ensemble approach, which utilizes multiple SLMs to provide a richer context for the LLM, thus improving its performance on domain-specific tasks."
        },
        "method": {
            "method name": "Ensemble SuperICL",
            "method abbreviation": "E-SuperICL",
            "method definition": "Ensemble SuperICL is a method that combines the predictions and confidence scores of multiple SLMs to enhance the in-context learning capabilities of an LLM.",
            "method description": "The method constructs an enhanced context by integrating outputs from several SLMs, allowing the LLM to make more informed predictions.",
            "method steps": [
                "Select in-context examples from a dataset.",
                "Obtain predictions and confidence scores from multiple fine-tuned SLMs.",
                "Concatenate the predictions and confidence scores with the test input for the LLM.",
                "Use the enhanced context to generate a prediction for the test input."
            ],
            "principle": "The effectiveness of this method stems from the ability of the LLM to leverage diverse predictions and confidence levels from multiple models, which helps in triangulating the correct response."
        },
        "experiments": {
            "evaluation setting": "The evaluation involved several natural language understanding benchmarks and a medical-domain labelling task using datasets such as GLUE and MedMCQA.",
            "evaluation method": "Performance was assessed by comparing the accuracy of Ensemble SuperICL against traditional ICL, SuperICL, and a majority vote baseline across multiple datasets."
        },
        "conclusion": "Ensemble SuperICL significantly enhances ICL performance, achieving state-of-the-art results across various benchmarks and demonstrating its effectiveness in domain-specific tasks, while maintaining low computational and data requirements.",
        "discussion": {
            "advantage": "The key advantages of Ensemble SuperICL include its ability to improve accuracy without the need for extensive fine-tuning and its robustness across different datasets.",
            "limitation": "A notable limitation is the dependency on optimal hyperparameter selection, which can vary significantly between datasets.",
            "future work": "Future research should explore the application of Ensemble SuperICL to text generation tasks and other domains beyond the medical field to further validate its versatility."
        },
        "other info": {
            "code": "https://github.com/mehdimojarradi/Ensemble-SuperICL",
            "datasets used": [
                "GLUE benchmark datasets (MNLI, SST-2, MRPC, CoLA)",
                "Medical Multiple-Choice Question Answering (MedMCQA)"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "In-context learning (ICL) is a cost-effective method for adapting large language models (LLMs) to specific tasks, but it often fails to achieve the accuracy of more resource-intensive methods like fine-tuning."
        },
        {
            "section number": "1.3",
            "key information": "The proposed Ensemble SuperICL method leverages the predictions and confidence scores of several fine-tuned small language models (SLMs) to improve the accuracy of in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "Ensemble SuperICL enhances the performance of ICL by combining the strengths of multiple SLMs, allowing LLMs to adapt to various contexts more effectively."
        },
        {
            "section number": "3.4",
            "key information": "The method constructs an enhanced context by integrating outputs from several SLMs, allowing the LLM to make more informed predictions."
        },
        {
            "section number": "6.1",
            "key information": "A notable limitation of Ensemble SuperICL is its dependency on optimal hyperparameter selection, which can vary significantly between datasets."
        },
        {
            "section number": "7",
            "key information": "Ensemble SuperICL significantly enhances ICL performance, achieving state-of-the-art results across various benchmarks and demonstrating its effectiveness in domain-specific tasks, while maintaining low computational and data requirements."
        }
    ],
    "similarity_score": 0.726811607755522,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Improving In-Context Learning with Small Language Model Ensembles.json"
}