{
    "from": "google",
    "scholar_id": "AVJ3jlb6JHYJ",
    "detail_id": null,
    "title": "Preliminary study on incremental learning for large language model-based recommender systems",
    "abstract": " Abstract\n\nAdapting Large Language Models for Recommendation (LLM4Rec) has shown promising results. However, the challenges of deploying LLM4Rec in real-world scenarios remain largely unexplored. In particular, recommender models need incremental adaptation to evolving user preferences, while the suitability of traditional incremental learning methods within LLM4Rec remains ambiguous due to the unique characteristics of Large Language Models (LLMs). In this study, we empirically evaluate two commonly employed incremental learning strategies (full retraining and fine-tuning) for LLM4Rec. Surprisingly, neither approach shows significant improvements in the performance of LLM4Rec. Instead of dismissing the role of incremental learning, we attribute the lack of anticipated performance enhancement to a mismatch between the LLM4Rec architecture and incremental learning: LLM4Rec employs a single adaptation module for learning recommendations, limiting its ability to simultaneously capture long-term and short-term user preferences in the incremental learning context. To test this speculation, we introduce a Long- and Short-term Adaptation-aware Tuning (LSAT) framework for incremental learning in LLM4Rec. Unlike the single adaptation module approach, LSAT utilizes two distinct adaptation modules to independently learn long-term and\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM \u201924, October 21\u201325, 2024, Boise, ID, USA \u00a9 2024 Copyright held ",
    "bib_name": "shi2024preliminary",
    "md_text": "# liminary Study on Incremental Learning for Large Languag Model-based Recommender Systems\n\nYang Zhang \u2217\nNational University of Singapore Singapore, Singapore zyang1580@gmail.com\nTianhao Shi University of Science and Technology of China Hefei, China sth@mail.ustc.edu.cn\nUniversity zjan\n\nYang Zhang \u2217\nNational University of Singapore Singapore, Singapore zyang1580@gmail.com\nhao Shi nce and Technology China i, China l.ustc.edu.cn\nZhijian Xu University of Science and Technology of China Hefei, China zjane@mail.ustc.edu.cn\n\nChong Chen Huawei Cloud BU Shenzhen, China chenchong55@huawei.com\n\nFuli Feng University of Science and Technology of China Hefei, China fulifeng93@gmail.com\nXiangnan He University of Science and Technology of China Hefei, China xiangnanhe@gmail.com\n\nQi Tian Huawei Cloud BU Shenzhen, China tian.qi1@huawei.com\n\n# Abstract\n\nAdapting Large Language Models for Recommendation (LLM4Rec) has shown promising results. However, the challenges of deploying LLM4Rec in real-world scenarios remain largely unexplored. In particular, recommender models need incremental adaptation to evolving user preferences, while the suitability of traditional incremental learning methods within LLM4Rec remains ambiguous due to the unique characteristics of Large Language Models (LLMs). In this study, we empirically evaluate two commonly employed incremental learning strategies (full retraining and fine-tuning) for LLM4Rec. Surprisingly, neither approach shows significant improvements in the performance of LLM4Rec. Instead of dismissing the role of incremental learning, we attribute the lack of anticipated performance enhancement to a mismatch between the LLM4Rec architecture and incremental learning: LLM4Rec employs a single adaptation module for learning recommendations, limiting its ability to simultaneously capture long-term and short-term user preferences in the incremental learning context. To test this speculation, we introduce a Long- and Short-term Adaptation-aware Tuning (LSAT) framework for incremental learning in LLM4Rec. Unlike the single adaptation module approach, LSAT utilizes two distinct adaptation modules to independently learn long-term and\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM \u201924, October 21\u201325, 2024, Boise, ID, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0436-9/24/10 https://doi.org/10.1145/3627673.3679922\n\nZhijian Xu\n\nXiangnan He University of Science and Technology of China Hefei, China xiangnanhe@gmail.com\n\nshort-term user preferences. Empirical results verify that LSAT enhances performance, thereby validating our speculation. We release our code at: https://github.com/TianhaoShi2001/LSAT.\n\n# CCS Concepts\n\u2022 Information systems \u2192 Recommender systems.\n\n# \u2022 Information systems \u2192 Recommender systems.\n\nKeywords\nLarge Language Models, Model Retraining, Incremental Lear\n\nLarge Language Models, Model Retraining, Incremental Lear\n\nACM Reference Format: Tianhao Shi, Yang Zhang, Zhijian Xu, Chong Chen, Fuli Feng, Xiangnan He, and Qi Tian. 2024. Preliminary Study on Incremental Learning for Large Language Model-based Recommender Systems. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM \u201924), October 21\u201325, 2024, Boise, ID, USA. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3627673.3679922\n\n# 1 Introduction\n\nThe emergence of Large Language Models [11], equipped with extraordinary capabilities in text comprehension and generation, has been successfully applied in various domains like Robotics [9, 30] and Computer Vision [4, 38]. Inspired by this success, there is growing interest in using LLMs for recommendations in both academia [1, 10, 37] and industry [22]. Among current efforts, tuning LLMs with recommendation-specific data using LoRA [15] (a well-known efficient fine-tuning method) has yielded promising results [2, 3, 23, 43, 45], underscoring the substantial potential of LLM4Rec in real-world applications. However, the challenges associated with the practical deployment of LLM4Rec remain explored, particularly considering the unique characteristics of LLMs. When deploying a recommender system in real-world scenarios, one of the primary challenges is ensuring the recommender models can adapt incrementally to evolving user preferences and\n\nenvironments [20, 42, 44]. This adaptation is crucial as user feedback streamingly flows in, requiring the recommender model to be incrementally updated with the latest data to achieve timely personalization. For traditional recommendation models, the critical role of incremental learning and associated challenges have been extensively researched [20, 42, 44]. However, when it comes to LLM4Rec, the issues related to incremental learning lack adequate attention. The unique characteristics of LLM4Rec, such as its massive parameters and its high tuning cost [24], may introduce new challenges or insights that require thorough examination.\nIn this study, we first empirically examine how incremental learning impacts the performance of LLM4Rec. Considering the broad adoption of LoRA in developing LLM4Rec models [2, 3, 23, 45] , and acknowledging LoRA\u2019s efficiency and effectiveness [8], our research focuses on this specific type of LLM4Rec. We examine two commonly used incremental learning strategies: 1) full retraining [20], which involves periodic retraining using complete historical data and new data, and 2) fine-tuning [28, 35], which updates the model based solely on new data. Based on our empirical results, we find that both full retraining and fine-tuning have a minimal impact on the performance of LLM4Rec. These results emphasize that LLM4Rec exhibits good generalization capabilities even under delayed updates; however, they also suggest that incremental learning might not lead to performance improvements for LLM4Rec.\nBased on our empirical results, executing incremental learning appears to be unnecessary for LLM4Rec. This is somewhat surprising, as user preferences do change over time, and a recommender system should adapt to these changes [20, 39, 44]. We speculate that the lack of anticipated performance improvements may be attributed to a mismatch between the LoRA architecture and incremental learning: LoRA avoids training the entire model and instead tunes a low-rank adaptation module [8] with recommendation data, while a single LoRA module may have the inability to simultaneously emphasize long-term and short-term user preferences under incremental learning. Specifically, for full retraining, the LoRA module might emphasize long-term preferences but overlook short-term ones, given the substantial volume of historical data compared to the new data [16]. For fine-tuning, the LoRA module may forget previous knowledge due to catastrophic forgetting [25], leading to a decline in performance.\nTo test our speculation, we develop a modified updating method called Long- and Short-term Adaptation-aware Tuning (LSAT). This method utilizes two LoRA modules to separately learn long-term and short-term user preferences and then integrates them to merge the different types of preferences. During each update, the shortterm LoRA module is temporarily retrained using solely new data to focus on the latest evolving preferences. In contrast, given the robust generalization capabilities exhibited by the long-term LoRA even with delayed updates, it remains fixed once it has been sufficiently trained or retrained at a relatively gradual frequency to conserve training costs. We conduct a comparison between LSAT, full retraining, and fine-tuning methods. Extensive results demonstrate that LSAT brings performance enhancements, confirming the validity of our speculation. Nevertheless, at present, LSAT only explores incremental learning from the perspective of LoRA capacity. To comprehensively understand and address the issue, further investigation in various directions is still necessary.\n\nThe main contributions are summarized as follows:\n\u2022 New Problem: This work marks the inaugural investigation into incremental learning for LLM4Rec, furnishing practical insights for the real-world deployment of LLM4Rec.\n\u2022 New Finding: Our empirical results underscore that the common incremental learning methods (full retraining and fine-tuning) do not clearly enhance the performance of LoRA-based LLM4Rec.\n\u2022  Proposal: We propose that using separate LoRA modules to capture long-term and short-term preferences can enhance the performance of LLM4Rec in incremental learning, offering valuable insights from the perspective of the capacity of the LoRA module.\n\nThe main contributions are summarized as follows:\n\u2022 New Problem: This work marks the inaugural investigation into incremental learning for LLM4Rec, furnishing practical insights for the real-world deployment of LLM4Rec.\n\u2022 New Finding: Our empirical results underscore that the common incremental learning methods (full retraining and fine-tuning) do not clearly enhance the performance of LoRA-based LLM4Rec.\n\u2022  Proposal: We propose that using separate LoRA modules to capture long-term and short-term preferences can enhance the performance of LLM4Rec in incremental learning, offering valuable insights from the perspective of the capacity of the LoRA module.\n\n# 2 RELATED WORKS\n\nLLM-based recommender. Adapting LLMs as a recommender has gained substantial attention. In-context learning enables LLMs to provide recommendations without explicit training [5, 34]. Nevertheless, due to the lack of recommendation-specific knowledge during pre-training, applying instruction tuning [27] with recommendation data helps LLMs achieve much better recommendation performance [2, 3]. Among tuning methods, InstructRec [41] fine-tunes all LLM\u2019s parameters, while most approaches employ parameter-efficient fine-tuning (PEFT) [8] to avoid adjusting the extensive parameters of LLMs [2, 3, 23, 45]. Within PEFT, LoRA is adopted by the majority of LLM4Rec models [2, 3, 23, 45] due to its good convergence and accuracy [8]. Considering the broad adoption of LoRA in developing LLM4Rec models, this work explores the issues of incremental learning in LoRA-based LLM4Rec. Incremental learning in recommendation.  Incremental learning is crucial for recommender models due to new users/items and changing user preferences[20, 29]. Representative methods for incremental updates include 1) full retraining [20], which retrains models with both new and historical data, achieving high accuracy with extensive training cost; 2) fine-tuning [28, 35], which updates models exclusively with the latest data, offering efficiency but facing potential forgetting; 3) sample-based methods [7, 36], which update models with new data and a sampled subset of historical data, where the sampled data is expected to retain long-term preference signals; and 4) meta-learning-based methods [39, 44], utilizing meta-learning to optimize the model for better future serving. Unlike prior works focusing on traditional recommender models, this work explores incremental learning within LLM4Rec.\n\n# 3 Preliminaries\n\nIn this study, we explore incremental learning in LLM4Rec. Our investigation is centered around a representative LLM4Rec model known as TALLRec [3], chosen due to the widespread adoption of its tuning paradigm [2, 45]. Next, we briefly introduce TALLRec and incremental learning in the recommendation. \u2022 TALLRec.  To align LLMs with recommendations, TALLRec utilizes instruction tuning [27]. This involves organizing historical interaction data into textual instructions and responses and then fine-tuning LLMs using this structured data to improve recommendation performance. Notably, TALLRec adopts LoRA [15] for efficient tuning, which freezes pre-trained LLM parameters and integrates lightweight trainable matrices. Specifically, LoRA introduces a pair of rank-decomposed weight matrices to each pre-trained\n\nary Study on Incremental Learning for Large Language Model-based Recommend\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05a7/05a75d4b-8853-48a7-892e-03edf7cc60be.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Incremental learning process in recommendation\n</div>\nweight matrix \ud835\udc4a \u2208R \ud835\udc51 \u00d7 \ud835\udc58, formally \ud835\udc4a + \ud835\udc34\ud835\udc35, where \ud835\udc34 \u2208R \ud835\udc51 \u00d7 \ud835\udc5f and \ud835\udc35 \u2208R \ud835\udc5f \u00d7 \ud835\udc58 are added learnable LoRA matrices (\ud835\udc5f \u226a min (\ud835\udc51,\ud835\udc58)). \u2022 Incremental learning in recommendation. To signify the incremental learning process, following [20, 44], we represent the data stream as {D 1, D 2, . . . , D \ud835\udc61, . . .}, where D \ud835\udc61 represents the collected data at time period \ud835\udc61. The length of a period may vary (e.g., daily, weekly) based on system requirements. At each period \ud835\udc61, the model has access to new data D \ud835\udc61 and all previous data for updating, and the updated model needs to serve for the near future D \ud835\udc61 + 1. This paper uses two representative incremental learning strategies: full retraining [20], updating the model with both new data D \ud835\udc61 and entire historical data {D 1, D 2, . . . , D \ud835\udc61}; and fine-tuning [28, 35], utilizing only the latest data D \ud835\udc61 for the update. Figure 1 illustrates the process of incremental learning in recommender systems.\n\n# 4 Empirical Explorations\n\nIn this section, we conduct experiments to explore the impact of the commonly employed incremental learning methods on TALLRec.\n\n# 4.1 Experiemental Settings\n\nDatasets. We conduct experiments on two representative datasets: MovieLens-1M (ML-1M) [13], which is a movie rating dataset collected by GroupLens Research, and Amazon-Book [26], which includes user reviews of books in Amazon. In ML-1M and AmazonBook, ratings range from 1 to 5. Following [3, 45], interactions with ratings \u2265 4 are positive; others are negative. To pre-process the data, we adopt the approach from TALLRec [3], converting ratings to binary labels and excluding users with less than 10 interactions. To assess incremental learning\u2019s impact, we divided the data chronologically based on interaction timestamps. For ML-1M, we use data from Dec. 2000 to Feb. 2003, creating 20 periods of 10,000 samples. Similarly, for Amazon-Book, we keep the data from Mar. 2014 to May. 2018, and sampled 20% of users, resulting in 328,168 samples, which were then divided into 20 two-month periods. For each period, we train models on the initial 90% of the data and validate them on the remaining 10%. Table 1 presents the dataset statistics. Models. Due to the diversity of incremental learning algorithms, we select the two most representative incremental learning strategies, full retraining, and fine-tuning for updating TALLRec. We also evaluate how these two update methods affect five traditional recommendation models: 1) MF [19], which is a latent factor-based collaborative filtering, 2)DeepMusic [33], which is a content-based recommendation model, 3) GRU4Rec [14], which is an RNN-based sequential recommender, 4) Caser [31], which uses CNN to model sequence patterns, and 5) SASRec [17], which employs a self-attention\n\n<div style=\"text-align: center;\">Table 1: Statistics of the evaluation datasets.\n</div>\nDataset\n# Users # Items # Instances Density\nML-1M\n1,813\n3,503\n200,000\n3.1491%\nAmazon-Book\n28,427\n12,680\n328,168\n0.0869%\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f07d/f07d9037-28a8-4d2e-bbf6-27a979f7ccea.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Performance of TALLRec, BookGPT and traditional models obtained at different update periods on D 20.\n</div>\nmechanism to grasp sequential patterns. Additionally, we evaluate BookGPT [21], an in-context learning-based method, which is tuning-free and always incorporates the latest data into prompts. Evaluation metrics and hyper-parameters.  Following TALLRec [3], we use AUC [12] to evaluate recommendation performance. TALLRec is deployed based on LLaMA-7B [32], and BookGPT on GPT-3.5-turbo, with settings aligned with the original papers. We optimize all traditional models using the Adam optimizer [18] with the MSE loss, employing a learning rate of 1e-3, batch size of 256, embedding size of 64, and weight decay of 1e-5 (tuned results).\n\n# 2 Incremental Learning\u2019s Impact on LLM4Rec\n\n# 4.2 Incremental Learning\u2019s Impact on\n\nOverall results. To evaluate the effect of incremental learning, the model undergoes continuous updates until the 19th period. Then, we evaluate the performance of the model obtained at each update period on the data D 20 of the 20th period, and plot the performance curve against the update periods in Figure 2, where we can find: \u2022  For traditional models, timely updates could enhance their performance, particularly with full retraining. Fine-tuning is usually less effective than full retraining, and its effectiveness may diminish over time, possibly due to the forgetting issue.\n\u2022 TALLRec significantly outperforms BookGPT, suggesting that while in-context learning can utilize the latest data without additional training, its performance is limited.\n\u2022 Unlike traditional models, the performance of TALLRec remains relatively unaffected by both full retraining and fine-tuning. This underscores its ability to excel in generalizing to new data. However, it also indicates that timely incremental learning does not enhance LLM4Rec\u2019s performance. Our initial findings suggest that incremental learning has a limited impact on the performance of LLM4Rec. Subsequently, we conduct a more detailed analysis to explore the effects of incremental learning on LLM4Rec, focusing on two key aspects as highlighted\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4198/4198ecc5-c313-4167-b515-f0b20ba7f6d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Performance comparison between TALLRec and baselines on warm items and cold items. All models are trained on D 1 \u2212D 15 and tested on D 16 \u2212D 20.\n</div>\nin [20]: 1) timely incorporation of new items and users, and 2) adaptation to changing user preferences. Cold-start items evaluation. In examining the first aspect, we compare the performance of TALLRec and traditional models on warm and cold items (trained on D 1\u2013 D 15, tested on D 16\u2013 D 20) in Figure 3. From the figure, we can find collaborative filtering methods exhibit near-random guessing (AUC=0.5) for cold items, indicating they would suffer performance deterioration without updates due to an increased number of cold items [20]. In contrast, TALLRec\u2019s proficiency in general language understanding enables accurate recommendations for cold items. Hence, from the perspective of cold items, incremental learning has a smaller impact on LLM4Recs compared to traditional models. Dynamic preference on warm items. We then explore whether incremental learning improves the performance of LLM4Rec by adapting to the latest user preferences. Toward this, we filter cold items (items do not appear in D 1\u2013 D 10 but appear in D 20) and evaluate the performance of warm items on D 20 under different periods in Figure 4. We find that incremental learning always improves the performance of traditional models on warm items, except for fine-tuning on ML-1M due to forgetting issues. However, both full retraining and fine-tuning cannot enhance TALLRec\u2019s performance on warm items. We suggest this could be due to the inability of a single LoRA to capture both long-term and short-term user preferences simultaneously. Full retraining may focus more on long-term preferences due to the larger quantity of historical data [16]. Fine-tuning might prioritize short-term preferences in new data while forgetting previous knowledge [25]. Consequently, both full-retraining and fine-tuning fail to achieve performance improvements by adapting to the latest preferences.\n\n# 5 LSAT\n\nWe have observed that both full retraining and fine-tuning do not effectively improve LLM4Rec\u2019s performance. We posit that a single LoRA module may struggle to simultaneously capture both long-term and short-term user preferences. This insight draws inspiration from advancements in leveraging multiple LoRA modules to handle distinct tasks or domain knowledge [6, 40]. Considering the divergence between long-term and short-term user preferences, it may be necessary to employ separate LoRA modules to capture them individually. To validate this speculation, we develop a new method called LSAT, employing two dedicated LoRA modules \u2014 one for capturing long-term user preferences and another for capturing short-term user preferences. During each update, the short-term\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/49b7/49b77513-7146-4dc0-949f-4124ccc1f6f5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance of TALLRec and traditional models obtained at different update periods for warm items on D 20.\n</div>\n# Figure 4: Performance of TALLRec and traditional models obtained at different update periods for warm items on D 20.\n\nLoRA module is temporarily introduced and trained on new data, while the long-term LoRA module remains fixed once trained on sufficient previous data. In the inference phase, the long-term LoRA module collaborates with the current short-term LoRA module to provide personalized recommendations. Next, we elaborate on the building of the two LoRA modules and the details of the inference: Short-term LoRA.  This LoRA module targets short-term preferences. Toward this, at each period \ud835\udc61, we train a new module with parameters \u0398 \ud835\udc61 using the newly collected data D \ud835\udc61 as:\n\n# min \u0398 \ud835\udc61 \ud835\udc3f (D \ud835\udc61; \u03a6, \u0398 \ud835\udc61),\n\n(1)\n\nwhere \u03a6 represents the frozen pre-trained parameters of the LLM, and \ud835\udc3f (D \ud835\udc61; \u03a6, \u0398 \ud835\udc61) is the recommendation loss on D \ud835\udc61. Notably, this new LoRA training approach is trained from scratch instead of being fine-tuned from the previous period, as fine-tuning has shown relatively poor performance in adapting to new preferences. Long-term LoRA. This LoRA module aims to capture aggregated long-term preferences by fitting sufficient historical data. To achieve this, we train the long-term LoRA with ample historical data, denoted as H = {D 1, D 2, . . . , D \ud835\udc5a} as follows:\n\n# min \u0398 \u210e \ud835\udc3f (H; \u03a6, \u0398 \u210e),\n\n(2)\n\nwhere \u0398 \u210e denotes long-term LoRA parameters. Once long-term LoRA is sufficiently trained after the \ud835\udc5a-th period, \u0398 \u210e can be updated at a slower pace, making LSAT training costs similar to fine-tuning. Before the \ud835\udc5a-th period, retraining \u0398 \u210e is needed. Inference. During inference, we explore two methods to merge long- and short-term preferences from two LoRA modules: 1) Output ensemble: This approach involves directly averaging the predictions with two LoRA modules. For a given sample \ud835\udc65 at the \ud835\udc61 +1-th period, the final prediction is formulated as follows:\n\n# \ud835\udefc\ud835\udc53 (\ud835\udc65; \u03a6, \u0398 \u210e) + (1 \u2212 \ud835\udefc) \ud835\udc53 (\ud835\udc65; \u03a6, \u0398 \ud835\udc61),\n\n(3)\n\nwhere \ud835\udefc is a hyper-parameter chosen on the validation set, and \ud835\udc53 (\ud835\udc65; \u03a6, \u0398 \ud835\udc61) and \ud835\udc53 (\ud835\udc65; \u03a6, \u0398 \u210e) are predictions of LLM4Rec using the \ud835\udc61-th period short-term LoRA and th long-term LoRA, respectively.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3cf6/3cf6e168-03a2-495e-80f2-aec2a5da5098.png\" style=\"width: 50%;\"></div>\nFigure 5: Performance comparison of full retraining, finetuning, and LSAT. All models are updated promptly with newly collected data D \ud835\udc61 and tested on D \ud835\udc61 + 1. LSAT (10) means it utilizes H = {D 1, D 2, . . . , D 10} to train the long-term LoRA.\n2) LoRA fusion:  As the output ensemble involves two LLM inferences, we explore merging the two LoRA modules for a single-pass inference, adopting a common fusion strategy\u2014task arithmetic [40]. Formally, task arithmetic fuses the parameters of the long-term LoRA (\u0398 \u210e) and the short-term LoRA at the \ud835\udc61-th period (\u0398 \ud835\udc61), and generates the final prediction for a sample \ud835\udc65 as:\n\n\ud835\udc53 (\ud835\udc65; \u03a6, \ud835\udf06 \u0398 \u210e + (1 \u2212 \ud835\udf06) \u0398 \ud835\udc61),\n\nwhere \ud835\udf06 is a hyper-parameter chosen on the validation set.\n\n# 6 Experiments\n\nWe conduct experiments to verify the effectiveness of our proposal. Experimental settings. We compare the performance of LSAT with full retraining and fine-tuning on ML-1M and Amazon-Book. Details about datasets and TALLRec are presented in Section 4.1. To assess which approach yields superior results after updates, following [44], the model is updated with D \ud835\udc61 and evaluated on D \ud835\udc61 + 1. For LSAT, the long-term LoRA module relies on a historical dataset H = {D 1, D 2, . . . , D \ud835\udc5a} (Equation (2)), with \ud835\udc5a set to 10 (LSAT (10)). We study two methods for model merging defined in the Inference part of Section 5: ensemble (LSAT-EN), and task arithmetic (LSAT-TA). The coefficient of LSAT \ud835\udefc (Equation 3) and \ud835\udf06 (Equation 4) are searched within {0, 0. 1, . . . , 1}. Overall results.  Figure 5 illustrates the overall performance comparison between full retraining, fine-tuning, and LSAT. From the figure, we can find that LSAT-EN and LSAT-TA outperform full retraining and fine-tuning on two datasets, emphasizing the effectiveness of using separate LoRAs for long-term and short-term interests. This supports our initial hypothesis that employing two adapters leads to improved performance in modeling both longterm and short-term interests. Nevertheless, LSAT-TA\u2019s relatively modest improvement suggests merging adapter parameters into a single adapter may be less effective, implying the need for exploring a parameter-level LoRA merging method tailored for incremental learning in recommendation systems. Analyses. We then study LSAT\u2019s two integral components: the short-term LoRA and the long-term LoRA, comparing their performance with LSAT-EN in Table 2, where we can find: \u2022 Short-term LoRA surpasses fine-tuning, as fine-tuning may suffer catastrophic forgetting [25, 42], where newly acquired knowledge conflicts with old knowledge, resulting in performance decline. This suggests the rationale of using a new LoRA to learn the latest preferences, rather than fine-tuning from the previous stage.\n\nTable 2: Average AUC across D 11 \u2212D 20. All models are updated promptly with newly collected data D \ud835\udc61 and tested on D \ud835\udc61 + 1. LSAT-EN (full) means its long-term LoRA is retrained during each period with all historical data.\n\nML-1M Amazon-Book\nFull Retraining\n0.7650\n0.7780\nFine-tuning\n0.7594\n0.7696\nShort-term LoRA\n0.7638\n0.7806\nLong-term LoRA\n0.7617\n0.7789\nLSAT-TA (10)\n0.7665\n0.7813\nLSAT-EN (full)\n0.7691\n0.7822\nLSAT-EN (10)\n0.7720\n0.7830\n\u2022 LSAT-EN (full), consistently updating the long-term LoRA, does not bring additional improvements compared to LSAT-EN (10), using a fixed long-term LoRA. These results suggest that the long-term LoRA can be updated at a slower pace once adequately trained, as long-term preferences tend to remain relatively stable.\n\u2022 Using only a single short-term LoRA or long-term LoRA leads to a performance decrease, highlighting the importance of merging both the long-term and short-term LoRAs for LSAT.\n\n# 7 Conclusion\n\nThis study studies the impact of incremental learning on LLM4Rec. Empirical results and analysis reveal that both full retraining and fine-tuning fail to deliver the anticipated performance improvement for LLM4Rec. We posit that a singular LoRA may encounter challenges in simultaneously capturing long-term and short-term preferences. To validate our hypothesis, we introduce LSAT and conduct experimental validation. Nevertheless, our current research is limited to the TALLRec backbone, which uses solely textual information (title) for recommendations. Future investigations will extend to other backbones. Additionally, as LSAT studies incremental learning methods solely from the perspective of LoRA capacity, we plan to explore other dimensions for more effective methods.\n\n# Acknowledgments\n\nThis work is supported by the National Key Research and Development Program of China (2022YFB3104701), the National Natural Science Foundation of China (62272437), and the CCCD Key Lab of Ministry of Culture and Tourism.\n\n# References\n\n[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. AI Open 4 (2023), 80\u201390.\n[2] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnan He, and Qi Tian. 2023. A Bi-step Grounding Paradigm for Large Language Models in Recommendation Systems. arXiv preprint arXiv:2308.08434 (2023).\n[3] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 1007\u20131014.\n[4] William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, and Amanpreet Singh. 2023. Towards Language Models that can See: Computer Vision through the Lens of Natural Language. arXiv preprint arXiv:2306.16410 (2023).\n[5] Zheng Chen. 2023. PALR: Personalization Aware LLMs for Recommendation. arXiv preprint arXiv:2305.07622 (2023).\n\n[6] Alexandra Chronopoulou, Matthew E Peters, Alexander Fraser, and Jesse Dodge. 2023. Adaptersoup: Weight Averaging to Improve Generalization of Pretrained Language Models. arXiv preprint arXiv:2302.07027 (2023).\n[7] Ernesto Diaz-Aviles, Lucas Drumond, Lars Schmidt-Thieme, and Wolfgang Nejdl. 2012. Real-time Top-n Recommendation in Social Streams. In Proceedings of the sixth ACM Conference on Recommender Systems. 59\u201366.\n[8] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al.  2023. Parameterefficient Fine-tuning of Large-scale Pre-trained Language Models. Nature Machine Intelligence 5, 3 (2023), 220\u2013235.\n[9]  Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An Embodied Multimodal Language Model. arXiv preprint arXiv:2303.03378 (2023).\n[10] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender Systems in the Era of Large Language Models. arXiv preprint arXiv:2307.02046 (2023).\n[11] Muhammad Usman Hadi, R Qureshi, A Shah, M Irfan, A Zafar, MB Shaikh, N Akhtar, J Wu, and S Mirjalili. 2023. A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage. TechRxiv (2023).\n[12] James A Hanley and Barbara J McNeil. 1982. The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve. Radiology 143, 1 (1982), 29\u201336.\n[13] F Maxwell Harper and Joseph A Konstan. 2015. The Movielens Datasets: History and Context. Acm Transactions on Interactive Intelligent Systems 5, 4 (2015), 1\u201319.\n[14] Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016.\n[15] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.\n[16] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, KIM Gyeonghun, Stanley Jungkyu Choi, and Minjoon Seo. 2021. Towards Continual Knowledge Learning of Language Models. In International Conference on Learning Representations.\n[17]  Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive Sequential Recommendation. In 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 197\u2013206.\n[18]  Diederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980 (2014).\n[19]  Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. Computer 42, 8 (2009), 30\u201337.\n[20] Hyunsung Lee, Sungwook Yoo, Dongjun Lee, and Jaekwang Kim. 2023. How Important is Periodic Model Update in Recommender System?. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2661\u20132668.\n[21] Zhiyu Li, Yanfang Chen, Xuan Zhang, and Xun Liang. 2023. BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model. Electronics 12, 22 (2023), 4654.\n[22] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023).\n[23] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. arXiv preprint arXiv:2308.11131 (2023).\n[24] Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et al.  2023. LLMRec: Benchmarking Large Language Models on Recommendation Task. arXiv preprint arXiv:2308.12241 (2023).\n[25] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An Empirical Study of Catastrophic Forgetting in Large Language Models during Continual Fine-tuning. arXiv preprint arXiv:2308.08747 (2023).\n[26] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying Recommendations Using Distantly-labeled Reviews and Fine-grained Aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 188\u2013197.\n[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training Language Models to Follow Instructions with Human Feedback.  Advances in Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[28] Steffen Rendle and Lars Schmidt-Thieme. 2008. Online-updating Regularized Kernel Matrix Factorization Models for Large-scale Recommender systems. In Proceedings of the 2008 ACM conference on Recommender systems. 251\u2013258.\n\n[29] Chijun Sima, Yao Fu, Man-Kit Sit, Liyi Guo, Xuri Gong, Feng Lin, Junyu Wu, Yongsheng Li, Haidong Rong, Pierre-Louis Aublin, et al. 2022. Ekko: A {LargeScale} Deep Learning Recommender System with {Low-Latency} Model Update. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 821\u2013839.\n[30] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating Situated Robot Task Plans Using Large Language Models. In 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 11523\u201311530.\n[31] Jiaxi Tang and Ke Wang. 2018. Personalized Top-n Sequential Recommendation via Convolutional Sequence Embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. 565\u2013573.\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971 (2023).\n[33] Aaron Van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep Content-based Music Recommendation.  Advances in Neural Information Processing Systems 26 (2013).\n[34] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153 (2023).\n[35] Qinyong Wang, Hongzhi Yin, Zhiting Hu, Defu Lian, Hao Wang, and Zi Huang. 2018. Neural Memory Streaming Recommender Networks with Adversarial Training. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2467\u20132475.\n[36] Weiqing Wang, Hongzhi Yin, Zi Huang, Qinyong Wang, Xingzhong Du, and Quoc Viet Hung Nguyen. 2018. Streaming Ranking Based Recommender Systems. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 525\u2013534.\n[37] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023).\n[38] Lingxi Xie, Longhui Wei, Xiaopeng Zhang, Kaifeng Bi, Xiaotao Gu, Jianlong Chang, and Qi Tian. 2023. Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models. arXiv preprint arXiv:2306.08641 (2023).\n[39] Ruobing Xie, Yalong Wang, Rui Wang, Yuanfu Lu, Yuanhang Zou, Feng Xia, and Leyu Lin. 2022. Long Short-term Temporal Meta-learning in Online Recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. 1168\u20131176.\n[40] Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. 2023. Composing Parameter-efficient Modules with Arithmetic Operations. arXiv preprint arXiv:2306.14870 (2023).\n[41] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. arXiv preprint arXiv:2305.07001 (2023).\n[42] Peiyan Zhang and Sunghun Kim. 2023. A Survey on Incremental Update for Neural Recommender Systems. arXiv preprint arXiv:2303.02851 (2023).\n[43] Yang Zhang, Keqin Bao, Ming Yan, Wenjie Wang, Fuli Feng, and Xiangnan He. 2024. Text-like Encoding of Collaborative Information in Large Language Models for Recommendation. arXiv preprint arXiv:2406.03210 (2024).\n[44] Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li, and Yongdong Zhang. 2020. How to Retrain Recommender System? A Sequential Meta-learning Method. In  Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 1479\u20131488.\n[45] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023. CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation. arXiv preprint arXiv:2310.19488 (2023).\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenges of deploying Large Language Model-based recommender systems (LLM4Rec) in real-world scenarios, particularly the need for incremental adaptation to evolving user preferences. Previous methods like full retraining and fine-tuning have shown limited success, necessitating a new approach to effectively capture user preferences over time.",
        "problem": {
            "definition": "The problem is the inability of existing incremental learning methods to enhance the performance of LLM4Rec, specifically in adapting to changing user preferences without significant performance degradation.",
            "key obstacle": "The main challenge is the mismatch between the architecture of LLM4Rec, which employs a single adaptation module, and the requirements of incremental learning that necessitate capturing both long-term and short-term user preferences."
        },
        "idea": {
            "intuition": "The idea originated from observing that a single adaptation module in LLM4Rec struggles to balance long-term and short-term user preferences, leading to suboptimal performance during incremental updates.",
            "opinion": "The proposed solution is the Long- and Short-term Adaptation-aware Tuning (LSAT) framework, which utilizes two distinct adaptation modules to more effectively learn and integrate long-term and short-term user preferences.",
            "innovation": "The key innovation lies in using two separate LoRA modules for long-term and short-term preferences, contrasting with existing methods that rely on a single module, thereby enhancing the adaptability of LLM4Rec."
        },
        "method": {
            "method name": "Long- and Short-term Adaptation-aware Tuning",
            "method abbreviation": "LSAT",
            "method definition": "LSAT is a framework designed to improve incremental learning in LLM4Rec by employing two LoRA modules that independently capture long-term and short-term user preferences.",
            "method description": "LSAT enhances performance by integrating two distinct LoRA modules for long-term and short-term user preferences, allowing for more nuanced adaptation to user feedback.",
            "method steps": [
                "Train a short-term LoRA module on newly collected data to focus on the latest preferences.",
                "Keep the long-term LoRA module fixed after sufficient training on historical data.",
                "During inference, merge the outputs of both LoRA modules to provide personalized recommendations."
            ],
            "principle": "The effectiveness of LSAT is based on the principle that separate modules can better capture the distinct dynamics of long-term and short-term user preferences, leading to improved performance in recommendations."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on two datasets: MovieLens-1M and Amazon-Book, with models evaluated using AUC metrics. The datasets were divided chronologically to assess incremental learning's impact over multiple periods.",
            "evaluation method": "The performance of LSAT was compared against full retraining and fine-tuning methods, with models updated using newly collected data and evaluated on subsequent datasets to measure improvements."
        },
        "conclusion": "The study concludes that both full retraining and fine-tuning do not effectively enhance the performance of LLM4Rec. LSAT demonstrates that using separate LoRA modules for long-term and short-term preferences can significantly improve performance, validating the initial hypothesis regarding the limitations of a single module approach.",
        "discussion": {
            "advantage": "The primary advantage of LSAT is its ability to simultaneously capture and integrate long-term and short-term user preferences, leading to better adaptability and performance in LLM4Rec.",
            "limitation": "A limitation of the current study is that LSAT has only been tested with the TALLRec backbone, which may not generalize to other recommendation frameworks or data types.",
            "future work": "Future research should explore the application of LSAT to other model architectures and investigate additional dimensions of incremental learning beyond the capacity of LoRA modules."
        },
        "other info": {
            "acknowledgments": "This work is supported by the National Key Research and Development Program of China and the National Natural Science Foundation of China.",
            "repository": "The code for LSAT is available at https://github.com/TianhaoShi2001/LSAT."
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "LSAT is a framework designed to improve incremental learning in LLM4Rec by employing two LoRA modules that independently capture long-term and short-term user preferences."
        },
        {
            "section number": "4.2",
            "key information": "The primary advantage of LSAT is its ability to simultaneously capture and integrate long-term and short-term user preferences, leading to better adaptability and performance in LLM4Rec."
        },
        {
            "section number": "5.3",
            "key information": "The problem is the inability of existing incremental learning methods to enhance the performance of LLM4Rec, specifically in adapting to changing user preferences without significant performance degradation."
        },
        {
            "section number": "6.2",
            "key information": "The main challenge is the mismatch between the architecture of LLM4Rec, which employs a single adaptation module, and the requirements of incremental learning that necessitate capturing both long-term and short-term user preferences."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore the application of LSAT to other model architectures and investigate additional dimensions of incremental learning beyond the capacity of LoRA modules."
        }
    ],
    "similarity_score": 0.7676032998774547,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05a7/05a75d4b-8853-48a7-892e-03edf7cc60be.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f07d/f07d9037-28a8-4d2e-bbf6-27a979f7ccea.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4198/4198ecc5-c313-4167-b515-f0b20ba7f6d9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/49b7/49b77513-7146-4dc0-949f-4124ccc1f6f5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3cf6/3cf6e168-03a2-495e-80f2-aec2a5da5098.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Preliminary study on incremental learning for large language model-based recommender systems.json"
}