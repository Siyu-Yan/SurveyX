{
    "from": "google",
    "scholar_id": "FXT72nVLADcJ",
    "detail_id": null,
    "title": "Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation",
    "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4\u2019s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multipath prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CRITIQUELLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT1.",
    "bib_name": "ke2023critiquellm",
    "md_text": "# CRITIQUELLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\nPei Ke1,\u2217, Bosi Wen1,2,\u2217,\u2020, Zhuoer Feng1,2,\u2217,\u2020, Xiao Liu3,2,\u2217, Xuanyu Lei3,2,\u2020, Jiale Cheng1,2,\u2020, Shengyuan Wang3,2,\u2020, Aohan Zeng3,2,\u2020, Yuxiao Dong3, Hongning Wang1, Jie Tang3, Minlie Huang1,\u2021 The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University\n3The Knowledge Engineering Group (KEG), Tsinghua University kepei1106@outlook.com, {wbs23,fze22,liuxiao21}@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.c\n# Abstract\nSince the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4\u2019s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multipath prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CRITIQUELLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT1.\n# 1 Introduction\nRecently, large language models (LLMs) (OpenAI, 2022, 2023; Touvron et al., 2023a) have been improved rapidly and approached human-level performance on various natural language processing\n\u2217Equal contribution \u2020Work done when these authors interned at Zhipu AI. \u2021Corresponding author 1The codes are available at https://github.com/ thu-coai/CritiqueLLM.\n(NLP) tasks, such as question answering, text summarization, dialogue generation, and code generation (Laskar et al., 2023). How to automatically measure the performance of LLMs has now become an essential research problem and attracted extensive attention (Chang et al., 2023; Zhang et al., 2023; Liu et al., 2024). Strong evaluation methods are expected to provide high-quality critiques (including not only rating scores but also explanations) that act as scalable feedback and guide LLMs to improve persistently (Cui et al., 2023). Traditional evaluation metrics, usually based on n-gram overlap between generated texts and reference texts (such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)), have limited effectiveness. Recent works mostly resort to model-based evaluation metrics, especially LLM-based ones (Wang et al., 2023a; Liu et al., 2023b; Zheng et al., 2023). Since most of the best-performing LLMs such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) can only be accessed via OpenAI APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models, aiming to avoid potential risks of commerical APIs, such as high cost, unstable usage, and data leakage (Zheng et al., 2023; Wang et al., 2024; Li et al., 2024). However, we argue that these evaluation models are still struggling to generate informative critiques in different evaluation tasks including pointwise grading and pairwise comparison. Especially in the challenging reference-free setting, these models tend to generate general critiques without finegrained distinguishability on generated texts, causing unsatisfactory evaluation performance (Zheng et al., 2023). In this work, we propose a simple yet effective method called Eval-Instruct, which can automatically construct informative instruction-tuning data for different evaluation tasks and settings, including pointwise grading and pairwise comparison\nwith / without references. Our main idea is to fully utilize referenced pointwise grading critiques, which are shown to possess rich information with the assistance of references and elaborate prompt design (Zheng et al., 2023; Liu et al., 2023a), to construct evaluation data for other tasks and settings. Specifically, after acquiring pointwise grading critiques with pseudo references via GPT-4, we devise a multi-path prompting method including two strategies: 1) Pointwise-to-Pairwise Prompting aims to inject pointwise grading critiques into pairwise critiques and enrich them with more information about the respective quality of text pairs. 2) Referenced-to-Reference-Free Prompting is targeted at removing direct comparison with references in referenced critiques, while keeping other details to improve the specificity of reference-free critiques. The evaluation data in different tasks and settings can be acquired via different paths consisting of these two strategies. And we also design a cross validation mechanism to improve the data quality of reference-free pairwise comparison because both of the two paths reach this task. After fine-tuning on the data of all the tasks and settings, the resulting model CRITIQUELLM is empirically shown to outperform all the opensource baselines and even achieve comparable performance with GPT-4 in system-level correlations of pointwise grading. We also show the potential of CRITIQUELLM to act as effective feedback to enhance the performance of LLMs like ChatGPT. Our main contributions are as follows:\n\u2022 We propose an evaluation data construction method called Eval-Instruct to automatically acquire informative evaluation data in both pointwise grading and pairwise comparison with / without references.\n\u2022 We conduct extensive experiments on CRITIQUELLM, which is fine-tuned on the data constructed by Eval-Instruct. Experimental results on three instruction following benchmark datasets show that our model can outperform all the open-source baselines and even perform comparably with GPT-4 in system-level correlations of pointwise grading.\n We reveal the potential of CRITIQUELLM to guide LLMs to improve persistently by showing the positive impact of our generated critiques as scalable feedback on the generation quality of LLMs.\n# 2 Related Work\nEvaluation is a long-standing task in NLP, which becomes more challenging with the rapid development of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are mainly two lines of work on LLM evaluation, including NLU-style and NLG-style evaluations. NLU-style evaluation methods utilize natural language understanding (NLU) tasks such as multi-choice QA to measure the performance of LLMs via simple objective metrics (such as accuracy and F1 score) (Hendrycks et al., 2021; Zhong et al., 2023; Huang et al., 2023b), which may deviate from the common usage of LLMs and may not exactly reflect the ability of LLMs in generating responses for user queries. NLG-style evaluation methods extend metrics for natural language generation (NLG) tasks and expect to apply them to the measurement of LLM\u2019s performance, which are the main focus of this paper. Compared with early metrics that depend on the n-gram overlap between generated texts and reference texts (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004), recently proposed metrics based on state-of-the-art LLMs like GPT-4 (OpenAI, 2023) are shown to be strong evaluators due to the encouraging effectiveness of LLMs and the simplicity of formulating evaluation tasks as instruction-following tasks (Wang et al., 2023a; Chen et al., 2023; Liu et al., 2023b; Zheng et al., 2023; Ke et al., 2023; Fu et al., 2023). Since most of the state-of-the-art LLMs can only be accessed via APIs, researchers start to automatically collect evaluation data by directly prompting GPT-4 and train their own evaluation models to provide stable and effective evaluations at a lower cost (Wang et al., 2024; Li et al., 2024; Kim et al., 2024). The concurrent works similar to ours are the LLMs specially trained for evaluation tasks like PandaLM (Wang et al., 2024), JudgeLM (Zhu et al., 2023), and AUTO-J (Li et al., 2024). For comparison, our work is the first attempt to deal with the challenge of uninformative critique generation which commonly appears in recent LLMbased evaluation models especially without references. Instead of prompting GPT-4 directly, our proposed Eval-Instruct can fully utilize the connection among different evaluation tasks and settings to construct informative evaluation data, which are empirically shown to improve the quality of generated critiques.\n# 3 Method\n# 3.1 Task Definition and Method Overview\nThis paper mainly involves two typical evaluation tasks: 1) Pointwise Grading: Given a user query q, a LLM-generated text x, and a reference text r (omitted in the reference-free setting), the goal is to obtain a critique c including a rating score and an explanation to support this score. 2) Pairwise Comparison: Given a user query q, two LLMgenerated texts x1 and x2, and a reference text r (omitted in the reference-free setting), our purpose is to acquire a critique c including a comparison label (i.e., win / tie / lose) and an explanation to support this label. Our method consists of the following steps. We first construct an informative instruction-tuning dataset for different evaluation tasks and settings, including pointwise grading and pairwise comparison with / without references (\u00a73.2). Specifically, after collecting user queries, LLM-generated texts, and pseudo references (\u00a73.2.1), we can acquire high-quality referenced pointwise grading critiques via elaborately prompting GPT-4. Then, we devise a multi-path prompting method to construct informative evaluation data in other tasks and settings, which covers pointwise-to-pairwise and referenced-to-reference-free prompting strategies (\u00a73.2.2). Since there are two paths to obtain reference-free pairwise comparison data, we design a cross validation mechanism to filter out the contradictory data and improve the quality (\u00a73.2.3). Finally, we perform supervised fine-tuning on the automatically constructed evaluation data in a multitask manner to train a unified critique generation model for different evaluation tasks and settings (\u00a73.3).\n# 3.2 Evaluation-Oriented Instruction Data Construction (Eval-Instruct)\n# 3.2 Evaluation-Oriented Instruction Data Construction (Eval-Instruct) 3.2.1 Pseudo Reference Collection\nTo construct instruction-tuning data for evaluation, it is imperative to first obtain the evaluation input, including user queries, LLM-generated texts, and references. We refer to recent works on instruction following (Liu et al., 2023a; Li et al., 2024; Zhang et al., 2024) and merge their task taxonomy to consider ten instruction following tasks covering diverse NLP applications in real-world scenarios2.\n2Our task taxonomy contains fundamental language ability, advanced Chinese understanding, open-ended question answering, writing ability, logical reasoning, mathematics,\nWe utilize self-instruct (Wang et al., 2023d) to augment seed queries of these tasks which are publicly available and conduct strictly filtering to improve the data quality. The details are provided in Appendix A. Then, we collect LLM-generated texts from 10 representative models, which cover different levels of generation qualities, including GPT-4 (OpenAI, 2023), ChatGPT (OpenAI, 2022), two versions of ChatGLM (Du et al., 2022; Zeng et al., 2023), MOSS (Sun et al., 2023), Minimax3, Sparkdesk4, Chinese-Llama2-7B-Chat5, Baichuan2-13B-Chat (Yang et al., 2023), and Ernie Bot6. We further filter out the generated results by removing a small number of failure cases, such as empty responses. Finally, we select the best-performing LLM (i.e., GPT-4) and manually check its generated texts for each user query, while revising them if necessary to improve the quality. Thus, these generated texts after manual check and revise can act as pseudo references to assist the evaluation data construction.\n# 3.2.2 Multi-Path Prompting\nTo acquire high-quality evaluation data in different evaluation tasks and settings, we first construct referenced pointwise grading critiques by prompting GPT-4 with the assistance of pseudo references and well-designed prompts like Liu et al. (2023a), which are empirically shown to be informative (Zheng et al., 2023). Then, regarding this setting as a beginning, we devise a multi-path prompting method to obtain evaluation data in other tasks and settings. As shown in Figure 1, there are two main prompting strategies: (1) Pointwise-to-Pairwise Prompting (fP2P ): This prompting strategy injects pointwise grading critiques of generated texts into pairwise comparison critiques, enriching them with information about the respective text quality. Meanwhile, it requires self-reflection on the pointwise critiques generated by GPT-4 before obtaining the final pairwise comparison results. (2) Referenced-to-Reference-Free Prompting (fR2RF ): This prompting strategy aims to remove direct comparison with references while keeping informative contents from references. It also re-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ce2/6ce20c11-e5c1-46cf-af8f-921e889fdf9e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40ba/40baa1b3-51c4-4eb2-8879-2f418d509a1c.png\" style=\"width: 50%;\"></div>\nFigure 1: Overview of Eval-Instruct. Starting from referenced pointwise grading data, our proposed multi-path prompting method can apply pointwise-to-pairwise and referenced-to-reference-free prompting strategies to acquire evaluation data in other tasks and settings via two different paths. Cross validation is adopted to filter out the contradictory data from these two paths and further improve the data quality.\nquires GPT-4 to self-reflect7 whether the evaluation results including scores / labels and revised explanations are consistent, and modify the results if necessary. Equipped with the above prompting strategies, we have two paths to construct evaluation data in different tasks and settings. Assume that Dpoint,r = {(qi, ri, xi, cpoint,r i )}N i=1 indicates the referenced pointwise grading dataset constructed above and cpoint,r i represents the critique in the corresponding setting, our purpose is to acquire the datasets Dpair,r, Dpoint,rf, Dpair,rf via different paths, where point/pair means pointwise / pairwise evaluation and r/rf indicates referenced / reference-free evaluation, respectively. The two paths are devised as follows. Path#1: Dpoint,r fP 2P \u2212\u2212\u2212\u2192Dpair,r fR2RF \u2212\u2212\u2212\u2212\u2192Dpair,rf As shown in Path#1 of Figure 1, we firstly conduct pointwise-to-pairwise prompting to acquire the referenced pairwise comparison dataset Dpair,r = {(qi, ri, xi,1, xi,2, cpair,r i )}M i=1:\n(1)\nwhere qi, ri, xi,1, xi,2 indicate the user query, the reference, and two generated texts of the i-th data,\nwhere qi, ri, xi,1, xi,2 indicate the user query, the reference, and two generated texts of the i-th data, 7The purpose of self-reflection in the two strategies is to alleviate the inconsistency problem in the output critiques, reducing error propagation during the data construction process.\n7The purpose of self-reflection in the two strategies is to alleviate the inconsistency problem in the output critiques, reducing error propagation during the data construction process.\nrespectively. cpoint,r i,1 , cpoint,r i,2 , cpair,r i are the referenced pointwise and pairwise evaluation results of xi,1, xi,2, respectively8. Then, we can apply referenced-to-reference-free prompting to obtain Dpair,rf = {(qi, xi,1, xi,2, cpair,rf i )}:\n(2)\nwhere cpair,rf,1 i means the reference-free pairwise comparison critique of the i-th data from Path#1. Path#2: Dpoint,r fR2RF \u2212\u2212\u2212\u2212\u2192Dpoint,rf fP 2P \u2192Dpair,rf Similarly, as shown in Path#2 of Figure 1, we can exchange the order of two prompting strategies applied to Dpoint,r accordingly. In this way, we can in turn acquire Dpoint,rf and Dpair,rf:\n(3)\n(4)\nwhere cpair,rf,2 i denotes the reference-free pairwise comparison critique of the i-th data from Path#2.\n8We conduct strictly rule-based filtering after each prompting step to remove low-quality data with errors in format and other aspects, which is omitted in this subsection.\n# 3.2.3 Cross Validation\n3.2.3 Cross Validation Since both of the two paths finally reach Dpair,rf, we design a cross validation mechanism to further improve the data quality. Specifically, Dpair,rf only contains the data whose comparison labels from two paths are consistent. In this case, the critiques from both of the two paths are added to Dpair,rf. The other data with contradictory comparison labels are strictly filtered. In our experiment, the proportion of the evaluation data which are filtered out is 7.7%, demonstrating that most of our constructed data from the two paths have consistent comparison labels, indicating acceptable data quality.\n# 3.3 Supervised Fine-Tuning\nWe perform supervised fine-tuning on the LLM P\u03b8 using all the constructed training data in a multitask manner to obtain CRITIQUELLM:\nwhere M \u2032 indicates the data amount of Dpair,rf after cross validation. During fine-tuning, we follow Bai et al. (2022) to add simplified prompts to distinguish different parts of inputs. We also follow Li et al. (2024) to augment pairwise training data via swapping the order of two generated texts and exchanging the corresponding contents in critiques.\n# 4 Experiment\n# 4.1 Dataset\nWe adopt three benchmark datasets on open-ended instruction following, which involve various NLP tasks in LLM\u2019s real-world scenarios9. The datasets also cover all the evaluation tasks and settings in this paper. The statistics are shown in Table 1. AlignBench (Liu et al., 2023a): This benchmark includes 8 categories of instruction following tasks\n9We have conducted string matching to show that there is no overlap between the queries in the training and test sets.\nDataset\nTask\nSetting #Models #Samples / #Pairs Length\nAlignBench\nPointwise R / R-F\n8\n3,200\n274\nPairwise\nR / R-F\n8\n1,600\n293\nAUTO-J (Eval-P)\nPairwise\nR-F\n6\n1,392\n372\nLLMEval\nPairwise\nR-F\n11\n1,530\n283\nTable 1: Statistics of the benchmark datasets, including the evaluation task / setting, the number of models / samples / pairs, and the average length of generated texts. R / R-F indicates referenced / reference-free evaluation, respectively.\nand 8 LLMs for generation. It provides an evaluation dataset with human-annotated scores on the quality of generated texts. In addition to using human-annotated scores for measuring pointwise grading performance, we also follow the original paper to sample text pairs of the same query for pairwise comparison10, whose label is automatically determined by their pointwise scores. AUTO-J (Eval-P) (Li et al., 2024): This benchmark provides 1,392 pairwise comparison data, each of which contains a user query, two LLMgenerated texts, and a human-annotated preference label. These data involve 58 real-world scenarios and 6 model families for generation. LLMEval (Zhang et al., 2024): This benchmark designs 17 types of user queries covering representative NLP tasks in real-world scenarios, and provides \u223c100,000 pairwise comparison data with human-annotated labels. Due to the limitation of computational resources and API costs for LLMbased evaluation methods, we randomly sample a subset (\u223c1,000) to measure the performance of our method and all the baselines for a fair comparison. As for the relationship between our training dataset in \u00a73.2 and these benchmark datasets, our training dataset has similar task categories with AlignBench because our task taxonomy is built mainly based on AlignBench (Liu et al., 2023a) with other tasks in recent works (Li et al., 2024; Zhang et al., 2024) as supplementary, as described in \u00a73.2.1. Also, our training dataset includes the training data of AUTO-J (Eval-P) (Li et al., 2024) while excluding its test set. Compared with AlignBench and AUTO-J (Eval-P), LLMEval (Zhang et al., 2024) does not have a similar task or data\n10The authors in the original paper of AlignBench (Liu et al., 2023a) collect all the pairs of generated texts for each query (\u223c10,000 pairwise comparison data), causing high demand of computational resources and API costs for LLM-based evaluation methods. Thus, we randomly sample a subset (\u223c1,000 pairwise comparison data) to test our method and all the baselines for a fair comparison.\nLevel\nText-Level\nSystem-Level\nSetting\nReferenced\nReference-Free\nReferenced\nReference-Free\nMetric\nr\n\u03c1\n\u03c4\nr\n\u03c1\n\u03c4\nr\n\u03c1\n\u03c4\nr\n\u03c1\n\u03c4\nClosed-Source Evaluation Models\nChatGPT\n0.443\n0.421\n0.379\n0.292\n0.287\n0.266\n0.955\n0.976\n0.929\n0.778\n0.833\n0.643\nGPT-4\n0.629\n0.583\n0.532\n0.523\n0.494\n0.447\n0.995\n1.000\n1.000\n0.997\n0.976\n0.929\nOpen-Source Evaluation Models\nChatGLM3-6B\n0.223\n0.222\n0.207\n0.159\n0.150\n0.140\n0.790\n0.833\n0.643\n0.544\n0.548\n0.429\nBaichuan2-13B-Chat\n0.199\n0.200\n0.187\n0.125\n0.117\n0.110\n0.854\n0.929\n0.786\n0.663\n0.527\n0.400\nQwen-14B-Chat\n0.373\n0.379\n0.358\n0.255\n0.254\n0.239\n0.901\n0.929\n0.786\n0.772\n0.833\n0.643\nMixtral-8x7B\n0.474\n0.471\n0.426\n0.302\n0.306\n0.282\n0.972\n0.976\n0.929\n0.863\n0.929\n0.786\nLlama-2-70B-Chat\n0.152\n0.162\n0.109\n0.123\n0.122\n0.113\n0.663\n0.667\n0.500\n0.547\n0.429\n0.286\nJudgeLM-13B\n0.450\n0.430\n0.391\n0.170\n0.162\n0.155\n0.984\n0.976\n0.929\n0.717\n0.905\n0.786\nAUTO-J-Bilingual-6B\n-\n-\n-\n0.044\n0.045\n0.041\n-\n-\n-\n0.558\n0.571\n0.500\nCRITIQUELLM (Ours)\n0.555\n0.523\n0.477\n0.366\n0.352\n0.319\n0.995\n1.000\n1.000\n0.954\n0.976\n0.929\nTable 2: Text-level and system-level Pearson (r), Spearman (\u03c1), and Kendall (\u03c4) correlations in referenced and reference-free settings of pointwise grading on AlignBench. The highest correlation among the methods based on local models is bold, while the highest correlation overall is underlined. - means that AUTO-J-Bilingual-6B cannot support referenced pointwise grading.\ndistribution with our training dataset, which can act as a benchmark to test the generalization ability.\n# 4.2 Baselines\nWe choose state-of-the-art general LLMs and evaluation-specific LLMs as our baselines. General LLMs: We adopt ChatGPT (gpt-3.5-turbo-1106) (OpenAI, 2022), GPT4 (gpt-4-1106-preview) (OpenAI, 2023), ChatGLM3-6B (Du et al., 2022; Zeng et al., 2023), Baichuan2-13B-Chat (Yang et al., 2023), Qwen14B-Chat (Bai et al., 2023), Llama-2-70B-Chat (Touvron et al., 2023b), and Mixtral-8x7B (Jiang et al., 2024) as our general baselines. These general LLMs can perform as an evaluator for pointwise grading and pairwise comparison via elaborate prompts without further training. We directly prompt these LLM to obtain evaluation results in single-turn interaction. Evaluation-Specific LLMs: We select AUTO-JBilingual-6B (Li et al., 2024) and JudgeLM-13B (Zhu et al., 2023) as our task-specific baselines. These two baselines are designed for specific evaluation tasks and settings.\n# 4.3 Implementation Details\nWe choose ChatGLM3-6B (Du et al., 2022; Zeng et al., 2023) as our base model and use Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) stage 2 framework from the Deepspeed (Rasley et al., 2020) library. CRITIQUELLM is trained on 8 A800 GPUs. The number of training samples\nfor Dpoint,r/Dpoint,rf/Dpair,r/Dpair,rf is 12,102 / 12,095 / 6,190 / 5,428, respectively. We use AdamW (Kingma and Ba, 2015) optimizer with the weight decay of 0.1. The peak learning rate is 6e-5 with 10% warmup ratio. We set the maximum sequence length to 8,192 and the batch size to 64. The number of training epochs is 5. We use greedy decoding in the main result and investigate the effect of different decoding methods on our model in \u00a74.7. For beam search, we set the beam size to 4. For the sampling-based decoding method, we adopt Nucleus Sampling (i.e., Top-p Sampling) (Holtzman et al., 2020) and set both the temperature and p to 0.9. For self-consistency decoding (Wang et al., 2023c), the number of candidate critiques is 5.\n# 4.4 Main Results 4.4.1 Pointwise Grading\nFollowing Colombo et al. (2022), we adopt textlevel and system-level Pearson (r), Spearman (\u03c1), and Kendall (\u03c4) correlation coefficients between human judgments and automatic metrics to measure the pointwise grading performance. Text-level correlation is computed by the average score over the correlation coefficients between human judgments and automatic metrics for all the generated texts of each instruction. For comparison, system-level correlation is obtained by the correlation coefficients between human judgments and automatic metrics of each LLM\u2019s score, which is the average value over all the scores of the corresponding model on\nDataset\nAlignBench\nAUTO-J (Eval-P)\nLLMEval\nSetting\nReferenced\nReference-Free\nReference-Free\nReference-Free\nMetric\nAgr.\nCons.\nAgr.\nCons.\nAgr.\nCons.\nAgr.\nCons.\nClosed-Source Evaluation Models\nChatGPT\n32.50\n38.56\n39.56\n53.94\n42.74\n62.43\n40.07\n64.58\nGPT-4\n74.69\n86.75\n70.25\n84.88\n62.28\n86.28\n50.98\n84.71\nOpen-Source Evaluation Models\nChatGLM3-6B\n17.75\n31.84\n24.75\n42.88\n14.15\n26.22\n28.56\n51.70\nBaichuan2-13B-Chat\n35.81\n50.06\n27.06\n40.82\n19.40\n32.33\n23.53\n43.27\nQwen-14B-Chat\n33.81\n43.25\n42.06\n58.75\n31.68\n52.08\n42.81\n69.61\nMixtral-8x7B\n61.69\n74.06\n53.88\n72.25\n35.20\n52.66\n48.04\n79.02\nLlama-2-70B-Chat\n40.56\n57.13\n41.38\n64.19\n33.62\n56.90\n40.00\n68.50\nJudgeLM-13B\n-\n-\n42.50\n66.00\n35.13\n58.19\n44.77\n75.82\nAUTO-J-Bilingual-6B\n-\n-\n26.00\n45.38\n49.43\n77.23\n27.58\n55.56\nCRITIQUELLM (Ours)\n70.56\n89.25\n58.81\n83.06\n50.93\n82.76\n50.72\n85.95\nTable 3: Agreement (Agr.) and consistency (Cons.) rates in pairwise comparison evaluation. The highest correlati among the methods based on local models is bold, while the highest correlation overall is underlined. - means t JudgeLM-13B and AUTO-J-Bilingual-6B cannot support referenced pairwise comparison.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2aa2/2aa26e80-be79-46ab-98b2-ead6c5f9b0fc.png\" style=\"width: 50%;\"></div>\nFigure 2: Critique quality evaluation results. The percentages indicate the preference results between CRITIQUELLM and other models via GPT-4\u2019s evaluation and human verification.\n<div style=\"text-align: center;\">Figure 2: Critique quality evaluation results. The percentages indicate the preference results between CRITIQUELLM and other models via GPT-4\u2019s evaluation and human verification.</div>\n# the dataset.\nThe results in Table 2 show that CRITIQUELLM can achieve comparable performance with GPT-4 especially in system-level correlations, while outperforming ChatGPT and all the open-source baselines. This indicates that our proposed method can successfully improve the quality of generated critiques. We can observe that system-level correlations of CRITIQUELLM are almost the same as those of GPT-4, which even approach 1,0. This demonstrate that our model is nearly able to distinguish the overall performance of all the eight LLMs.\n# 4.4.2 Pairwise Comparison\nFollowing Li et al. (2024), we adopt agreement and consistency rates to test the pairwise comparison\nperformance. Specifically, we conduct two comparisons for each data sample via swapping the order of two generated texts. We consider the model\u2019s evaluation result to agree with humans only when the two comparison results are consistent and align with the human preference label. The results in Table 3 show that CRITIQUELLM can beat ChatGPT and all the open-source baselines in both agreement and consistency rates. Compared with GPT-4, CRITIQUELLM achieves comparable performance especially in the consistency rate. This indicates that CRITIQUELLM equipped with high-quality evaluation data in different tasks and settings not only performs well in pointwise grading, but also has a strong evaluation ability in pairwise comparison.\n# 4.5 Analysis on Critique Quality\nTo further measure the quality of generated critiques, we follow Chen et al. (2024) to combine automatic and human evaluations. Specifically, we follow existing works (Wang et al., 2023b; Sun et al., 2024) to devise an evaluation prompt for GPT-4 to judge the quality of generated critiques. After GPT-4\u2019s evaluation, we manually verify the results and modify them if necessary. We randomly select 100 evaluation data in the setting of pairwise comparison, which are from the mix of three datasets. And we collect generated critiques from CRITIQUELLM, state-of-the-art evaluators (i.e., ChatGPT and GPT-4), and an alternative model CRITIQUELLM (DP) whose training data in differ-\nCritique Model\nOverall\nLogical\nOpen-ended QA\nProfessional\nFundamental\nMathematics\nRole Play\nWriting\nChinese Understanding\nNone\n6.385\n5.318\n7.000\n5.824\n6.310\n6.160\n7.260\n7.154\n6.000\nChatGPT\n6.300\n5.045\n6.762\n6.353\n6.276\n5.760\n7.000\n6.885\n6.063\nGPT-4\n6.545\n4.455\n7.190\n6.588\n6.897\n6.200\n7.111\n7.077\n6.563\nCRITIQUELLM\n6.530\n5.136\n7.381\n6.765\n6.414\n6.000\n7.407\n7.192\n5.315\nTable 4: GPT-4\u2019s referenced pointwise scores on AlignBench for original generated texts from ChatGPT (i.e., None and modified texts based on each critique generation model, respectively.\nent tasks and settings are acquired from GPT-4\u2019s direct prompting. For each pair of critiques (one from CRITIQUELLM and the other from a baseline / an alternative model, given the same evaluation input), GPT-4 are required to label which critique is better (i.e. win, lose or tie) in terms of correctness, helpfulness, and informativeness. The priority of these three aspects is set to follow the above order. Then, human verification is conducted to check GPT-4\u2019s evaluation on critiques. The results are shown in Figure 2. We can observe that CRITIQUELLM can achieve superior performance over ChatGPT and CritiqueLLM (DP), and even perform comparably with GPT-4. This demonstrates that our proposed evaluation data construction method can successfully improve the overall quality of generated critiques and enhance their informativeness.\n# 4.6 Analysis of Critique as Feedback\nTo investigate whether the critiques generated by our model can serve as feedback to improve the quality of LLM-generated texts, we employ ChatGPT, GPT-4, and CRITIQUELLM to provide critiques for the generated texts of ChatGPT in the reference-free setting. Then, we instruct ChatGPT to modify its original generation based on the critiques. Finally, we use GPT-4 to perform referenced evaluations on the original texts and the modified texts generated by ChatGPT, respectively. The results in Table 4 show that the critiques from CRITIQUELLM can serve as positive feedback whose contributed improvement on the overall score is close to that from the GPT-4\u2019s critiques. This further verifies the utility of CRITIQUELLM to provide informative critiques as scalable feedback that can guide LLMs towards better generation. We also notice that the critiques from ChatGPT itself have a negative impact on the overall quality of its generated texts. This phenomenon is consistent with recent works that doubt the selfcorrection ability of LLMs without external inputs (Huang et al., 2023a; Stechly et al., 2023;\nValmeekam et al., 2023). We also report the evaluation scores before and after the critique-based modification across different tasks in Table 4. It is notable that the critiques from CRITIQUELLM can help enhance the quality of generated texts in a majority of tasks. However, in the tasks of logical reasoning, mathematics, and advanced Chinese understanding which are mostly hard tasks involving reasoning, the critiques from CRITIQUELLM seem to degrade the performance. We manually checked error cases and found that our model obtained misleading critiques on the reasoning process of generated texts. Since the evaluation of reasoning chains remains a challenging task (Golovneva et al., 2023) even for GPT-4, we leave further investigation in these tasks as future work. Since our experiment is a preliminary step towards utilizing critiques as feedback, we additionally have some findings which may inspire future research. First, while incorporating human critiques can provide the comparison results between the generation performance assisted by the critiques from humans and LLMs, we notice that it is not trivial to collect high-quality critiques from human annotators for AlignBench especially in the reference-free setting. It is because AlignBench is designed to be difficult and covers a wide range of tasks (Liu et al., 2023a). Thus, how to collect highquality human critiques to improve the generation quality of LLMs is worth further exploring. Then, since we choose ChatGPT as the generation model, we find that stronger LLMs which can already generate high-quality responses struggle to be further improved via generated critiques. While weaker LLMs have a lot of room for improvement, they also have the weak ability to follow instructions. Thus, how to make weaker LLMs follow critiques to generate texts of a higher quality should be left as important future work.\n# 4.7 Ablation Study\nTo further investigate the impact of each part on CRITIQUELLM, we conduct additional ablation\nSetting\nPointwise\nPairwise\nR\nR-F\nR\nR-F\nMetric\nr\nr\nAgr.\nAgr.\nCRITIQUELLM\n0.555\n0.366\n70.56\n58.81\nFine-Tuning Data\nw/o Cross Validation\n0.566\n0.361\n66.13\n57.44\nDecoding Strategy\nw/ Beam Search\n0.554\n0.374\n70.31\n57.75\nw/ Sampling\n0.547\n0.353\n68.69\n57.31\nw/ Self-Consistency\n0.573\n0.384\n69.13\n58.44\nExplanation\nw/o Explanation\n0.509\n0.332\n60.19\n51.56\nTable 5: Text-level Pearson (r) correlations and agreement rates (Agr.) of ablation models in reference (R) and reference-free (R-F) settings of AlignBench.\nstudies. For fine-tuning data, we remove the cross validation module (\u00a73.2.3) to explore its impact on the evaluation performance. Table 5 shows that the performance of CRITIQUELLM degrades especially in pairwise comparison, demonstrating that cross validation can filter out low-quality evaluation data and contribute to the final performance. As for decoding strategies, we show the evaluation performance of three decoding strategies in addition to greedy decoding in the main result, including beam search, Nucleus Sampling (Holtzman et al., 2020), and self-consistency decoding (Wang et al., 2023c). The results in Table 5 show that the self-consistency decoding method can enhance the performance of our model especially in pointwise grading. Meanwhile, greedy decoding performs best in pairwise comparison, while achieving comparable performance with other methods in pointwise grading at a smaller computational cost. For evaluation explanations, we remove the explanations in the critiques of training data. The results in Table 5 show that the performance of CRITIQUELLM largely degrades in both pointwise and pairwise evaluations without explanations. This verifies the positive impact of explanations on the final performance, which play a similar role to chain-of-thought reasoning (Wei et al., 2022).\n# 5 Conclusion\nWe present an evaluation data construction method called Eval-Instruct, which can automatically construct informative evaluation data in both pointwise grading and pairwise comparison with / without references. After fine-tuning on the data from Eval-\nInstruct, the resulting model CRITIQUELLM can beat ChatGPT and all the open-source baselines, and perform comparably with GPT-4 in systemlevel correlations of pointwise grading. CRITIQUELLM can also provide scalable feedback which can improve the generation quality of LLMs.\n# Limitations\nThe limitations of our work are summarized as follows:\n(1) In our method of multi-path prompting, we devise two prompting strategies to enrich the information in the resulting critiques, which can improve the critique quality. However, this method also increases the length of input prompts and lead to higher API costs when constructing evaluation data in different tasks and settings. We believe that it is not a severe problem because data acquisition is single-round and we do not repeatedly acquire critiques for the same evaluation input. Also, our proposed critique generation model based on opensource LLMs (i.e., ChatGLM3-6B) can achieve comparable performance with GPT-4 in some aspects, which may save the cost for LLM evaluation via APIs and avoid the risks such as unstable usage and data leakage. (2) Similar to other model-based evaluation methods, our evaluation model suffers from the selfevaluation bias (He et al., 2023) (also known as self-enhancement bias (Zheng et al., 2023)), which indicates the preference on the generated texts from the same base model. This bias is commonly recognized even in state-of-the-art LLM-based evaluators like GPT-4. We argue that researchers and developers can use multiple LLM-based evaluators with different base models including CRITIQUELLM to avoid self-evaluation bias towards specific generation models. Since there does not exist a satisfactory solution to the self-evaluation bias currently, we leave the further investigation as important future work.\n# Acknowledgements\nThis work was supported by the NSFC projects (with No. 62306160) and the National Science Foundation for Distinguished Young Scholars (with No. 62125604). This work was also supported by China National Postdoctoral Program for Innovative Talents (No. BX20230194) and China Postdoctoral Science Foundation (No. 2023M731952). We would also like to thank Zhipu AI for sponsoring\nthe computation resources and annotation cost used in this work.\n# References\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2023. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109. Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, and Qun Liu. 2024. Gaining wisdom from setbacks: Aligning large language models via mistake analysis. In The Twelfth International Conference on Learning Representations. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. arXiv preprint arXiv:2304.00723. Pierre Jean A Colombo, Chlo\u00e9 Clavel, and Pablo Piantanida. 2022. Infolm: A new metric to evaluate summarization & data2text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10554\u201310562. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023. Roscoe: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations. Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, and Yulia Tsvetkov. 2023. On the blind spots of model-based evaluation metrics for text generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12067\u201312097. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023a. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023b. Ceval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu, and Minlie Huang. 2023. DecompEval: Evaluating generated texts as unsupervised decomposed question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9676\u20139691. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al.\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al.\ncapability in language models. In The Twelfth International Conference on Learning Representations. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Findings of the Association for Computational Linguistics: ACL 2023, pages 431\u2013469. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2024. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023a. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2024. Agentbench: Evaluating llms as agents. In The Twelfth International Conference on Learning Representations. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511\u20132522.\n# OpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, page 20.\neff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3505\u20133506.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3505\u20133506.\nShichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, and Pengfei Liu. 2024. The critique of critique. arXiv preprint arXiv:2401.04518.\nTianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023. Moss: Training conversational language models from synthetic data. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O\u2019Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023b. Shepherd: A critic for language model generation. arXiv preprint arXiv:2308.04592. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2024. Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2024. Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023d. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 13484\u201313508. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130B: an open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations. Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Llmeval: A preliminary study on how to evaluate large language models. In The 38th Annual AAAI Conference on Artificial Intelligence. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2023. Safetybench: Evaluating the safety of large language models with multiple choice questions. arXiv preprint arXiv:2309.07045. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631. A Query Augmentation and Scoring\nLianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631.\n# A Query Augmentation and Scoring Prompts\nWe provide the prompt for query augmentation and scoring in Table 6. First, in the stage of generation, we give some in-context examples and devise\ndetailed requirements to help ChatGPT (OpenAI, 2022) generate augmented user queries and assign the category label to them. Then, during evaluation, we instruct ChatGPT to provide a difficulty score to each query for difficulty balance in the whole augmentation dataset.\n# B Prompt Design for Eval-Instruct\nWe provide original prompts for pointwise-topairwise and referenced-to-reference-free strategies in Table 7 and Table 9, respectively. We also translate these prompts into English and show them in Table 8 and Table 10.\n# C Case Study on Critique Generation\nTo intuitively show the effectiveness of our critique generation model, we provide two generated cases of pointwise and pairwise settings, respectively, in Table 11 and 13. We also translate these cases into English and show them in Table 12 and 14.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb6a/bb6ae4b6-2d70-4982-a546-8545ded37aab.png\" style=\"width: 50%;\"></div>\nStage Prompt Generation You are asked to provide 10 diverse prompts. These task prompts will be provided to a GPT model and we will evaluate the ability of the GPT model to reply to these prompts. The following are some examples: 1.{example prompt 1} 2.{example prompt 2} 3.{example prompt 3} Here are the requirements you need to follow to provide prompts: 1. The prompts need to be complete sentences, not phrases or fragments. 2. The prompts need to be varied, do not use similar prompts. 3. the prompts need to be meaningful, do not use meaningless prompts. 4. The prompts need to have a variety of tones, e.g., combining interrogative and imperative sentences. 5. The prompts need to be challenging, do not use simple directions. 6. The prompts need to be something that the Large Language Model can accomplish. For example, don\u2019t ask the assistant to create any visual or audio output. For example, don\u2019t ask the assistant to wake you up at 5pm or set a reminder because it can\u2019t perform any action. For example, prompts should not be related to audio, video, images, hyperlinks. 7. The prompts are in Simplified Chinese, except for translation-related questions or math-related questions. 8. Some prompts can provide contextual information, should involve realistic data, and should not contain simple placeholders. Not all prompts require input. For example, when an prompts asks for general knowledge information, such as \"What is the tallest mountain in the world?\", it does not need to provide specific context. After you have provided the prompts, please add the category of the prompts in a pair of && sign after the prompt and surround the prompt with in a pair of @@ sign. For example, if the prompt is \"@@What is the tallest mountain in the world?@@&& \u57fa\u672c\u4efb\u52a1&&\", then the category is \u57fa\u672c\u4efb\u52a1. The category must be one of the following 10 categories. 1. \u57fa\u672c\u4efb\u52a12. \u4e2d\u6587\u7406\u89e33. \u7efc\u5408\u95ee\u7b544. \u6587\u672c\u5199\u4f5c5. \u6570\u5b66\u8ba1\u7b976. \u903b\u8f91\u63a8\u74067. \u89d2\u8272\u626e\u6f148. \u4e13\u4e1a \u80fd\u529b9. \u4ee3\u7801\u751f\u621010. \u591a\u8bed\u8a00\u80fd\u529b Here are some examples of prompts you provide: @@example prompt1@@ &&category1&& @@example prompt2@@ &&category2&& \u00b7 \u00b7 \u00b7 @@example prompt9@@ &&category9&& @@example prompt10@@ &&category10&&\n<div style=\"text-align: center;\">The following is a list of 10 good task prompts with serial numbers and categories:</div>\nEvaluation\n\u5df2\u77e5\u4e0a\u9762\u4e09\u4e2a\u95ee\u9898\u548c\u5b83\u4eec\u7684\u7c7b\u522b\uff0c\u73b0\u5728\u8bf7\u4f60\u6839\u636e\u4ee5\u4e0b\u8981\u6c42\uff0c\u5bf9\u8fd9\u4e09\u4e2a\u95ee\u9898\u7684\u9898\u76ee\u7684\u96be\u5ea6\u57281-3\u5206\u7684\u91cf\u8868\u4e0a\u5206\u522b\u8bc4\u5206:\n(1) 1\u5206\uff1a\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\uff0c\u8fd9\u7c7b\u95ee\u9898\u662f\u5bb9\u6613\u7684\n(2) 2\u5206\uff1a\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\uff0c\u8fd9\u7c7b\u95ee\u9898\u662f\u4e2d\u7b49\u96be\u5ea6\u7684\n(3) 3\u5206\uff1a\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\uff0c\u8fd9\u7c7b\u95ee\u9898\u662f\u56f0\u96be\u7684\n\u6700\u540e\uff1a\u8bf7\u5c06\u8fd9\u4e09\u4e2a\u95ee\u9898\uff0c\u9898\u76ee\u7528\u4e00\u5bf9@@\u7b26\u53f7\u5305\u56f4\uff0c\u5bf9\u5e94\u7684\u7c7b\u522b\u7528\u4e00\u5bf9&&\u7b26\u53f7\u5305\u56f4\uff0c\u5206\u6570\u7528\u4e00\u5bf9##\u5305\u56f4\uff0c\u5206\u522b\u5e26\u6709\u5e8f\u53f7\u5730\u8f93\u51fa\u51fa\u6765\uff1a\n\u4f8b\u5982\uff1a\u5982\u679c\u95ee\u98981\u7684\u9898\u76ee\u662f\u9898\u76ee1\uff0c\u7c7b\u522b\u662f\u7efc\u5408\u95ee\u7b54\u7c7b\u522b\uff0c\u5206\u6570\u662f1\u5206\uff0c\u95ee\u98982\u7684\u9898\u76ee\u662f\u9898\u76ee2\uff0c\u7c7b\u522b\u662f\u57fa\u672c\u4efb\u52a1\u7c7b\u522b\uff0c\u5206\u6570\u662f2\u5206\uff0c\u95ee\u98983\u7684\u9898\u76ee\u662f\u9898\u76ee3\uff0c\n\u7c7b\u522b\u662f\u6587\u672c\u5199\u4f5c\u7c7b\u522b\uff0c\u5206\u6570\u662f3\u5206\uff0c\u90a3\u4e48\u8f93\u51fa\u5982\u4e0b\uff1a\n1.@@\u9898\u76ee1@@&&\u7efc\u5408\u95ee\u7b54&&##1##\n2.@@\u9898\u76ee2@@&&\u57fa\u672c\u4efb\u52a1&&##2##\n3.@@\u9898\u76ee3@@&&\u6587\u672c\u5199\u4f5c&&##3##\n\u4e0b\u9762\u662f\u6309\u7167\u4e0a\u8ff0\u8981\u6c42\u751f\u6210\u7684\u793a\u4f8b\uff1a\nEvaluation\n(English)\nGiven the above three questions and their categories, please rate the difficulty of each question on a scale of 1-3 based on the following requirements:\n(1) Score 1: For large language models, this type of question is easy.\n(2) Score 2: For large language models, this type of question is of medium difficulty.\n(3) Score 3: For large language models, this type of question is difficult.\nFinally, please output the three questions with their titles enclosed in a pair of @@ symbols, the corresponding categories enclosed in a pair of && symbols, and the\nscores enclosed in a pair of ## symbols, each with an serial number.\nFor example, if question 1 is titled \u201cTitle 1\u201d, the category is \u201cOpen-ended Questions\u201d, and the score is 1, question 2 is titled \u201cTitle 2\u201d, the category is \u201cFundamental\nLanguage Ability\u201d, and the score is 2 points, question 3 is titled \u201cTitle 3\u201d, the category is \u201cWriting Ability\u201d, and the score is 3 points, then the output is as follows:\n1.@@Title 1@@&&Open-ended Questions&&##1##\n2.@@Title 2@@&&Fundamental Language Ability&&##2##\n3.@@Title 3@@&&Writing Ability&&##3##\nThe following parts are generated examples based on the above requirements:\nTable 6: Prompts for instructing ChatGPT to generate, categorize and evaluate user queries. Examples and corresponding categories are randomly sampled from the set of seed queries.\nTable 6: Prompts for instructing ChatGPT to generate, categorize and evaluate user queries. Examples and corresponding categories are randomly sampled from the set of seed queries.\nd to d n \u4f60\u662f\u4e00\u4e2a\u64c5\u957f\u8bc4\u4ef7\u6587\u672c\u8d28\u91cf\u7684\u52a9\u624b\u3002\u8bf7\u4f60\u4ee5\u516c\u6b63\u7684\u8bc4\u5224\u8005\u7684\u8eab\u4efd\uff0c\u6bd4\u8f83\u4e24\u4e2aAI\u52a9\u624b\u5bf9\u4e8e\u7528\u6237\u63d0\u95ee\u7684\u56de\u7b54\u7684\u8d28\u91cf\u4f18\u52a3\u3002\u6211\u4eec\u4f1a\u7ed9\u4f60\u63d0\u4f9b\u7528\u6237\u7684\u63d0 \u95ee\uff0c\u9ad8\u8d28\u91cf\u7684\u53c2\u8003\u7b54\u6848\uff0c\u9700\u8981\u4f60\u6bd4\u8f83\u7684\u4e24\u4e2aAI\u52a9\u624b\u7684\u7b54\u6848\uff0c\u4ee5\u53ca\u4e24\u4e2a\u7b54\u6848\u5404\u81ea\u7684\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u3002\u5f53\u4f60\u5f00\u59cb\u4f60\u7684\u8bc4\u4f30\u65f6\uff0c\u4f60\u9700\u8981\u9075\u5b88\u4ee5\u4e0b\u7684\u6d41\u7a0b\uff1a 1. \u7ed3\u5408\u53c2\u8003\u7b54\u6848\u3001\u4e24\u4e2aAI\u52a9\u624b\u7684\u7b54\u6848\u4ee5\u53ca\u5176\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\uff0c\u6839\u636e\u4e0a\u8ff0\u6307\u5b9a\u7684\u7ef4\u5ea6\u5bf9\u4ed6\u4eec\u7684\u7b54\u6848\u8fdb\u884c\u7ec6\u81f4\u7684\u6bd4\u8f83\uff0c\u7ed9\u51fa\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u6587\u672c\u3002\u6bd4\u8f83 \u5206\u6790\u6587\u672c\u8981\u6c42\u8986\u76d6\u4e24\u4e2a\u7b54\u6848\u7684\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u4e2d\u53ef\u7528\u4e8e\u6bd4\u8f83\u7684\u6240\u6709\u91cd\u8981\u7ec6\u8282\uff0c\u5e76\u5305\u542b\u5bf9\u7b54\u6848\u4e2d\u5177\u4f53\u5185\u5bb9\u7684\u5206\u6790\u3002 2. \u7ed3\u5408\u53c2\u8003\u7b54\u6848\u548c\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u4ece\u4e24\u4e2aAI\u52a9\u624b\u7684\u7b54\u6848\u4e2d\u9009\u51fa\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u90a3\u4e2a\uff0c\u6216\u8005\u5224\u5b9a\u4ed6\u4eec\u8d28\u91cf\u76f8\u5f53\uff0c\u5e76\u7ed9\u51fa\u8be6\u5c3d\u7684\u9009\u62e9\u7406\u7531\u3002\u4f60 \u7684\u6bd4\u8f83\u9700\u8981\u5c3d\u53ef\u80fd\u4e25\u8c28\u7ec6\u81f4\uff0c\u4e0d\u53d7\u4e24\u4e2aAI\u52a9\u624b\u7b54\u6848\u5148\u540e\u987a\u5e8f\u7684\u5f71\u54cd\u3002 \u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u4e2d\u7684\u5404\u7ef4\u5ea6\u5206\u6570\u548c\u7efc\u5408\u5f97\u5206\u4ec5\u4f9b\u53c2\u8003\uff0c\u5728\u5404\u7ef4\u5ea6\u548c\u7efc\u5408\u7684\u6bd4\u8f83\u5206\u6790\u6587\u672c\u4e2d\u4e0d\u80fd\u76f4\u63a5\u63d0\u53ca\u5404\u7ef4\u5ea6\u5206\u6570\u548c\u7efc\u5408\u5f97\u5206\u3002\u9488\u5bf9\u7efc\u5408\u5f97\u5206\u5dee\u8ddd \u8f83\u5927\u7684\u6837\u672c\u5bf9\uff0c\u5e94\u5c3d\u53ef\u80fd\u6309\u7167\u5206\u6570\u9ad8\u4f4e\u5f97\u51fa\u6bd4\u8f83\u7ed3\u679c\uff0c\u9664\u975e\u53d1\u73b0\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u4e2d\u5b58\u5728\u660e\u663e\u9519\u8bef\u3002\u800c\u9488\u5bf9\u7efc\u5408\u5f97\u5206\u5dee\u8ddd\u8f83\u5c0f\u7684\u6837\u672c\u5bf9\uff0c\u5219\u5141\u8bb8\u6bd4\u8f83 \u7ed3\u679c\u548c\u5206\u6570\u9ad8\u4f4e\u4e0d\u4e00\u81f4\uff0c\u4f46\u4ecd\u9700\u8981\u8be6\u7ec6\u8bf4\u660e\u6bd4\u8f83\u8bc4\u4ef7\u7684\u7406\u7531\u3002 \u8bf7\u8bb0\u4f4f\uff0c\u4f60\u5fc5\u987b\u9996\u5148\u6309\u7167\u7ed9\u5b9a\u7684\u8bc4\u4ef7\u7ef4\u5ea6\uff0c\u8f93\u51fa\u76f8\u5e94\u7ef4\u5ea6\u7684\u540d\u79f0\u548c\u6bd4\u8f83\u5206\u6790\u7684\u6587\u672c\u3002\u7136\u540e\u518d\u7ed9\u51fa\u7efc\u5408\u8d28\u91cf\u6bd4\u8f83\u7ed3\u679c\uff0c\u5e76\u7ed9\u51fa\u6bd4\u8f83\u7ed3\u679c\u7684\u5206\u6790\u548c\u89e3 \u91ca\u3002\u4e4b\u540e\uff0c\u5728\u4f60\u56de\u7b54\u7684\u672b\u5c3e\uff0c\u6309\u7167\u4ee5\u4e0b\u5b57\u5178\u683c\u5f0f\uff08\u5305\u62ec\u62ec\u53f7\uff09\u8fd4\u56de\u4f60\u7684\u7efc\u5408\u8d28\u91cf\u9009\u62e9\u7ed3\u679c\uff0c\u5373\u4f60\u9009\u62e9\u7684\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u90a3\u4e2aAI\u52a9\u624b\uff08\u6216\u8005\u8ba4\u4e3a\u8d28 \u91cf\u76f8\u5f53\uff09\uff0c\u5e76\u786e\u4fdd\u4f60\u8fd4\u56de\u7684\u7ed3\u679c\u548c\u4e0a\u8ff0\u751f\u6210\u6587\u672c\u4e2d\u7684\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\uff1a {{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u56de\u7b54\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u52a9\u624b\u5e8f\u53f7\u6216\u8d28\u91cf\u76f8\u5f53}}\uff0c\u4f8b\u5982\uff1a{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u2019\u52a9\u624b1\u2019}}\u6216{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u2019\u52a9\u624b2\u2019}}\u6216{{\u2019\u7efc\u5408\u6bd4\u8f83 \u7ed3\u679c\u2019: \u2019\u8d28\u91cf\u76f8\u5f53\u2019}}\u3002\n<div style=\"text-align: center;\">[\u52a9\u624b2\u7684\u7b54\u6848\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u5f00\u59cb] {Referenced Pointwise Grading Critique for Generated Text 2} [\u52a9\u624b2\u7684\u7b54\u6848\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u7ed3\u675f]</div>\nReferenceree Pointise Grading o Referenceree Pairwise Comparison \u4f60\u662f\u4e00\u4e2a\u64c5\u957f\u8bc4\u4ef7\u6587\u672c\u8d28\u91cf\u7684\u52a9\u624b\u3002\u8bf7\u4f60\u4ee5\u516c\u6b63\u7684\u8bc4\u5224\u8005\u7684\u8eab\u4efd\uff0c\u6bd4\u8f83\u4e24\u4e2aAI\u52a9\u624b\u5bf9\u4e8e\u7528\u6237\u63d0\u95ee\u7684\u56de\u7b54\u7684\u8d28\u91cf\u4f18\u52a3\u3002\u6211\u4eec\u4f1a\u7ed9\u4f60\u63d0\u4f9b\u7528\u6237\u7684\u63d0 \u95ee\uff0c\u9700\u8981\u4f60\u6bd4\u8f83\u7684\u4e24\u4e2aAI\u52a9\u624b\u7684\u7b54\u6848\uff0c\u4ee5\u53ca\u4e24\u4e2a\u7b54\u6848\u5404\u81ea\u7684\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u3002\u5f53\u4f60\u5f00\u59cb\u4f60\u7684\u8bc4\u4f30\u65f6\uff0c\u4f60\u9700\u8981\u9075\u5b88\u4ee5\u4e0b\u7684\u6d41\u7a0b\uff1a 1. \u7ed3\u5408\u4e24\u4e2aAI\u52a9\u624b\u7684\u7b54\u6848\u4ee5\u53ca\u5176\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\uff0c\u6839\u636e\u4e0a\u8ff0\u6307\u5b9a\u7684\u7ef4\u5ea6\u5bf9\u4ed6\u4eec\u7684\u7b54\u6848\u8fdb\u884c\u7ec6\u81f4\u7684\u6bd4\u8f83\uff0c\u7ed9\u51fa\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u6587\u672c\u3002\u6bd4\u8f83\u5206\u6790\u6587\u672c\u8981 \u6c42\u8986\u76d6\u4e24\u4e2a\u7b54\u6848\u7684\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u4e2d\u53ef\u7528\u4e8e\u6bd4\u8f83\u7684\u6240\u6709\u91cd\u8981\u7ec6\u8282\uff0c\u5e76\u5305\u542b\u5bf9\u7b54\u6848\u4e2d\u5177\u4f53\u5185\u5bb9\u7684\u5206\u6790\u3002 2. \u7ed3\u5408\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u4ece\u4e24\u4e2aAI\u52a9\u624b\u7684\u7b54\u6848\u4e2d\u9009\u51fa\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u90a3\u4e2a\uff0c\u6216\u8005\u5224\u5b9a\u4ed6\u4eec\u8d28\u91cf\u76f8\u5f53\uff0c\u5e76\u7ed9\u51fa\u8be6\u5c3d\u7684\u9009\u62e9\u7406\u7531\u3002\u4f60\u7684\u6bd4\u8f83\u9700\u8981 \u5c3d\u53ef\u80fd\u4e25\u8c28\u7ec6\u81f4\uff0c\u4e0d\u53d7\u4e24\u4e2aAI\u52a9\u624b\u7b54\u6848\u5148\u540e\u987a\u5e8f\u7684\u5f71\u54cd\u3002 \u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u4e2d\u7684\u5404\u7ef4\u5ea6\u5206\u6570\u548c\u7efc\u5408\u5f97\u5206\u4ec5\u4f9b\u53c2\u8003\uff0c\u5728\u5404\u7ef4\u5ea6\u548c\u7efc\u5408\u7684\u6bd4\u8f83\u5206\u6790\u6587\u672c\u4e2d\u4e0d\u80fd\u76f4\u63a5\u63d0\u53ca\u5404\u7ef4\u5ea6\u5206\u6570\u548c\u7efc\u5408\u5f97\u5206\u3002\u9488\u5bf9\u7efc\u5408\u5f97\u5206\u5dee\u8ddd \u8f83\u5927\u7684\u6837\u672c\u5bf9\uff0c\u5e94\u5c3d\u53ef\u80fd\u6309\u7167\u5206\u6570\u9ad8\u4f4e\u5f97\u51fa\u6bd4\u8f83\u7ed3\u679c\uff0c\u9664\u975e\u53d1\u73b0\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u4e2d\u5b58\u5728\u660e\u663e\u9519\u8bef\u3002\u800c\u9488\u5bf9\u7efc\u5408\u5f97\u5206\u5dee\u8ddd\u8f83\u5c0f\u7684\u6837\u672c\u5bf9\uff0c\u5219\u5141\u8bb8\u6bd4\u8f83 \u7ed3\u679c\u548c\u5206\u6570\u9ad8\u4f4e\u4e0d\u4e00\u81f4\uff0c\u4f46\u4ecd\u9700\u8981\u8be6\u7ec6\u8bf4\u660e\u6bd4\u8f83\u8bc4\u4ef7\u7684\u7406\u7531\u3002 \u8bf7\u8bb0\u4f4f\uff0c\u4f60\u5fc5\u987b\u9996\u5148\u6309\u7167\u7ed9\u5b9a\u7684\u8bc4\u4ef7\u7ef4\u5ea6\uff0c\u8f93\u51fa\u76f8\u5e94\u7ef4\u5ea6\u7684\u540d\u79f0\u548c\u6bd4\u8f83\u5206\u6790\u7684\u6587\u672c\u3002\u7136\u540e\u518d\u7ed9\u51fa\u7efc\u5408\u8d28\u91cf\u6bd4\u8f83\u7ed3\u679c\uff0c\u5e76\u7ed9\u51fa\u6bd4\u8f83\u7ed3\u679c\u7684\u5206\u6790\u548c\u89e3 \u91ca\u3002\u4e4b\u540e\uff0c\u5728\u4f60\u56de\u7b54\u7684\u672b\u5c3e\uff0c\u6309\u7167\u4ee5\u4e0b\u5b57\u5178\u683c\u5f0f\uff08\u5305\u62ec\u62ec\u53f7\uff09\u8fd4\u56de\u4f60\u7684\u7efc\u5408\u8d28\u91cf\u9009\u62e9\u7ed3\u679c\uff0c\u5373\u4f60\u9009\u62e9\u7684\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u90a3\u4e2aAI\u52a9\u624b\uff08\u6216\u8005\u8ba4\u4e3a\u8d28 \u91cf\u76f8\u5f53\uff09\uff0c\u5e76\u786e\u4fdd\u4f60\u8fd4\u56de\u7684\u7ed3\u679c\u548c\u4e0a\u8ff0\u751f\u6210\u6587\u672c\u4e2d\u7684\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\uff1a {{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u56de\u7b54\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u52a9\u624b\u5e8f\u53f7\u6216\u8d28\u91cf\u76f8\u5f53}}\uff0c\u4f8b\u5982\uff1a{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u2019\u52a9\u624b1\u2019}}\u6216{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u2019\u52a9\u624b2\u2019}}\u6216{{\u2019\u7efc\u5408\u6bd4\u8f83 \u7ed3\u679c\u2019: \u2019\u8d28\u91cf\u76f8\u5f53\u2019}}\u3002\n\u7528\u6237\u7684\u63d0\u95ee\uff1a{Question}\n[\u52a9\u624b1\u7684\u7b54\u6848\u5f00\u59cb]\n{Generated Text 1}\n[\u52a9\u624b1\u7684\u7b54\u6848\u7ed3\u675f]\n[\u52a9\u624b1\u7684\u7b54\u6848\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u5f00\u59cb]\n{Reference-Free Pointwise Grading Critique for Generated Text 1}\n[\u52a9\u624b1\u7684\u7b54\u6848\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u7ed3\u675f]\n[\u52a9\u624b2\u7684\u7b54\u6848\u5f00\u59cb]\n{Generated Text 2}\n[\u52a9\u624b2\u7684\u7b54\u6848\u7ed3\u675f]\n[\u52a9\u624b2\u7684\u7b54\u6848\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u5f00\u59cb]\n{Reference-Free Pointwise Grading Critique for Generated Text 2}\n[\u52a9\u624b2\u7684\u7b54\u6848\u8d28\u91cf\u8bc4\u4ef7\u5206\u6790\u7ed3\u675f]\nable 7: Pointwise-to-Pairwise prompt design in multi-path prompting.\nReferenced\nPointwise\nGrading\nto\nReferenced\nPairwise\nComparison\nYou are an expert at text quality evaluation. Please act as a fair judge, and compare the quality between two AI assistants\u2019 answers to a user query. We will provide you with a user query, a high-quality reference answer, two AI assistants\u2019 responses to the query, and the corresponding critiques to the two responses, respectively. When you start your evaluation, you need to follow the procedures below: 1. Considering the reference answers, along with two AI assistants\u2019 answers and the corresponding critiques to them, conduct detailed comparison between two AI assistants\u2019 answers based on the evaluation dimensions Dimension. Provide a detailed comparison result. The comparison result should cover all the important details from the pointwise critiques that can be used for the comparison, and it should include an analysis of the specific content in the answers. 2. Based on the reference answer and the comparison result of each dimension, choose the answer from the two AI assistants that has the higher overall quality, or judge that their qualities are equivalent. Provide a detailed rationale for your choice. Your comparison needs to be as rigorous and detailed as possible, and not be affected by the order in which the two AI assistants\u2019 answers were given. The scores of each dimension and the overall score in the pointwise critique are for reference only, neither of which can be directly referred to in the comparison result. For the text pairs with a large difference in overall scores, the comparison result should be determined largely according to the scores, unless there are obvious errors in the pointwise critique. For text pairs with a small difference in overall scores, the comparison result is allowed to be inconsistent with the score ranking, but the reason for the comparison result needs to be detailed. Please remember that you must first output the names and comparison results of each given evaluation dimensions, respectively. Then, give the comparison result of overall quality and provide an analysis and explanation of the comparison result. Afterwards, at the end of your answer, return your choice of overall quality result in the following dictionary format (including brackets), that is, the AI assistant you chose as having higher overall quality (or considered to have equivalent quality), and be sure that the result you return is consistent with the result in the generated text above. {{\u2019Overall Comparison Result\u2019: the assistant number with higher overall quality or tie}}\uff0cfor example: {{\u2019Overall Comparison Result\u2019: \u2019Assistant 1\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Assistant 2\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Tie\u2019}}\u3002\nYou are an expert at text quality evaluation. Please act as a fair judge, and compare the quality between two AI assistants\u2019 answers to a user query. We will provide you with a user query, a high-quality reference answer, two AI assistants\u2019 responses to the query, and the corresponding critiques to the two responses, respectively. When you start your evaluation, you need to follow the procedures below: 1. Considering the reference answers, along with two AI assistants\u2019 answers and the corresponding critiques to them, conduct detailed comparison between two AI assistants\u2019 answers based on the evaluation dimensions Dimension. Provide a detailed comparison result. The comparison result should cover all the important details from the pointwise critiques that can be used for the comparison, and it should include an analysis of the specific content in the answers. 2. Based on the reference answer and the comparison result of each dimension, choose the answer from the two AI assistants that has the higher overall quality, or judge that their qualities are equivalent. Provide a detailed rationale for your choice. Your comparison needs to be as rigorous and detailed as possible, and not be affected by the order in which the two AI assistants\u2019 answers were given. The scores of each dimension and the overall score in the pointwise critique are for reference only, neither of which can be directly referred to in the comparison result. For the text pairs with a large difference in overall scores, the comparison result should be determined largely according to the scores, unless there are obvious errors in the pointwise critique. For text pairs with a small difference in overall scores, the comparison result is allowed to be inconsistent with the score ranking, but the reason for the comparison result needs to be detailed. Please remember that you must first output the names and comparison results of each given evaluation dimensions, respectively. Then, give the comparison result of overall quality and provide an analysis and explanation of the comparison result. Afterwards, at the end of your answer, return your choice of overall quality result in the following dictionary format (including brackets), that is, the AI assistant you chose as having higher overall quality (or considered to have equivalent quality), and be sure that the result you return is consistent with the result in the generated text above. {{\u2019Overall Comparison Result\u2019: the assistant number with higher overall quality or tie}}\uff0cfor example: {{\u2019Overall Comparison Result\u2019: \u2019Assistant 1\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Assistant 2\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Tie\u2019}}\u3002\n[Reference Answer Begin] {Reference} [Reference Answer End]\n[Assistant 1\u2019s Answer Begin] {Generated Text 1} [Assistant 1\u2019s Answer End]\n[Critique for Assistant 2\u2019s Answer Begin] {Referenced Pointwise Grading Critique for Generated Text 2} [Critique for Assistant 2\u2019s Answer End]\nYou are an expert at text quality evaluation. Please act as a fair judge, and compare the quality between two AI assistants\u2019 answers to a user query. We will provide you with a user query, two AI assistants\u2019 responses to the query, and the corresponding critiques to the two responses, respectively. When you start your evaluation, you need to follow the procedures below: 1. Considering two AI assistants\u2019 answers and the corresponding critiques to them, conduct detailed comparison between two AI assistants\u2019 answers based on the evaluation dimensions Dimension. Provide a detailed comparison result. The comparison result should cover all the important details from the pointwise critiques that can be used for the comparison, and it should include an analysis of the specific content in the answers. 2. Based on the comparison result of each dimension, choose the answer from the two AI assistants that has the higher overall quality, or judge that their qualities are equivalent. Provide a detailed rationale for your choice. Your comparison needs to be as rigorous and detailed as possible, and not be affected by the order in which the two AI assistants\u2019 answers were given. The scores of each dimension and the overall score in the pointwise critique are for reference only, neither of which can be directly referred to in the comparison result. For the text pairs with a large difference in overall scores, the comparison result should be determined largely according to the scores, unless there are obvious errors in the pointwise critique. For text pairs with a small difference in overall scores, the comparison result is allowed to be inconsistent with the score ranking, but the reason for the comparison result needs to be detailed. Please remember that you must first output the names and comparison results of each given evaluation dimensions, respectively. Then, give the comparison result of overall quality and provide an analysis and explanation of the comparison result. Afterwards, at the end of your answer, return your choice of overall quality result in the following dictionary format (including brackets), that is, the AI assistant you chose as having higher overall quality (or considered to have equivalent quality), and be sure that the result you return is consistent with the result in the generated text above. {{\u2019Overall Comparison Result\u2019: the assistant number with higher overall quality or tie}}, for example: {{\u2019Overall Comparison Result\u2019: \u2019Assistant 1\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Assistant 2\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Tie\u2019}}.\nYou are an expert at text quality evaluation. Please act as a fair judge, and compare the quality between two AI assistants\u2019 answers to a user query. We will provide you with a user query, two AI assistants\u2019 responses to the query, and the corresponding critiques to the two responses, respectively. When you start your evaluation, you need to follow the procedures below: 1. Considering two AI assistants\u2019 answers and the corresponding critiques to them, conduct detailed comparison between two AI assistants\u2019 answers based on the evaluation dimensions Dimension. Provide a detailed comparison result. The comparison result should cover all the important details from the pointwise critiques that can be used for the comparison, and it should include an analysis of the specific content in the answers. 2. Based on the comparison result of each dimension, choose the answer from the two AI assistants that has the higher overall quality, or judge that their qualities are equivalent. Provide a detailed rationale for your choice. Your comparison needs to be as rigorous and detailed as possible, and not be affected by the order in which the two AI assistants\u2019 answers were given. The scores of each dimension and the overall score in the pointwise critique are for reference only, neither of which can be directly referred to in the comparison result. For the text pairs with a large difference in overall scores, the comparison result should be determined largely according to the scores, unless there are obvious errors in the pointwise critique. For text pairs with a small difference in overall scores, the comparison result is allowed to be inconsistent with the score ranking, but the reason for the comparison result needs to be detailed. Please remember that you must first output the names and comparison results of each given evaluation dimensions, respectively. Then, give the comparison result of overall quality and provide an analysis and explanation of the comparison result. Afterwards, at the end of your answer, return your choice of overall quality result in the following dictionary format (including brackets), that is, the AI assistant you chose as having higher overall quality (or considered to have equivalent quality), and be sure that the result you return is consistent with the result in the generated text above. {{\u2019Overall Comparison Result\u2019: the assistant number with higher overall quality or tie}}, for example: {{\u2019Overall Comparison Result\u2019: \u2019Assistant 1\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Assistant 2\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Tie\u2019}}.\n[Assistant 1\u2019s Answer Begin] {Generated Text 1} [Assistant 1\u2019s Answer End]\nTable 8: Pointwise-to-Pairwise prompt design in multi-path prompting (translated into English)\nSetting\nPrompt\nReferenced\nPointwise\nGrading\nto\nReference-\nFree\nPoint-\nwise Grading\n\u4f60\u662f\u4e00\u4e2a\u64c5\u957f\u8bc4\u4ef7\u6587\u672c\u8d28\u91cf\u7684\u52a9\u624b\u3002\u8bf7\u4f60\u6839\u636e\u4ee5\u4e0b\u8981\u6c42\u4fee\u6539\u8bc4\u4ef7\u6587\u672c\u3002\n1. \u5728\u4fee\u6539\u540e\u7684\u8bc4\u4ef7\u6587\u672c\u4e2d\uff0c\u4e0d\u8981\u76f4\u63a5\u63d0\u53ca\u53c2\u8003\u7b54\u6848\u3002\u53ef\u4ee5\u5728\u8bc4\u4ef7\u6587\u672c\u4e2d\u9002\u5f53\u5229\u7528\u53c2\u8003\u7b54\u6848\u4e2d\u7684\u5177\u4f53\u5185\u5bb9\u8f85\u52a9\u5206\u6790\uff0c\u4f46\u4e0d\u8981\u8ba9\u8bfb\u8005\u611f\u53d7\u5230\u53c2\u8003\u7b54\u6848\n\u7684\u5b58\u5728\u3002\u4fee\u6539\u540e\u7684\u8bc4\u4ef7\u6587\u672c\u9700\u8981\u8bed\u8a00\u4e0a\u901a\u987a\uff0c\u903b\u8f91\u4e0a\u5408\u7406\uff0c\u5206\u6790\u5185\u5bb9\u4e0e\u6bd4\u8f83\u7ed3\u679c\u547c\u5e94\u3002\n2. \u5728\u4fee\u6539\u5404\u4e2a\u7ef4\u5ea6\u7684\u5206\u6790\u65f6\uff0c\u5206\u6790\u7684\u5185\u5bb9\u9700\u8981\u548c\u5f53\u524d\u8bc4\u4ef7\u6587\u672c\u57fa\u672c\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u4e0d\u8981\u76f4\u63a5\u63d0\u53ca\u53c2\u8003\u7b54\u6848\u3002\n3. \u5728\u4fee\u6539\u7efc\u5408\u5f97\u5206\u7684\u5206\u6790\u6587\u672c\u65f6\uff0c\u4e0d\u8981\u76f4\u63a5\u63d0\u53ca\u53c2\u8003\u7b54\u6848\uff0c\u5c3d\u91cf\u4fdd\u7559\u5f53\u524d\u8bc4\u4ef7\u6587\u672c\u4e2d\u7684\u5176\u4ed6\u7ec6\u8282\uff0c\u5e76\u5145\u5206\u5229\u7528\u4fee\u6539\u540e\u7684\u5206\u7ef4\u5ea6\u5206\u6790\u3002\u4fee\u6539\u540e\u7684\u7efc\n\u5408\u5206\u6790\u6587\u672c\u5e94\u901a\u987a\u3001\u6d41\u7545\u3001\u81ea\u6d3d\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\u5e94\u4e0e\u7efc\u5408\u5f97\u5206\u4fdd\u6301\u4e00\u81f4\u3002\u5982\u679c\u53d1\u73b0\u5f53\u524d\u7efc\u5408\u5206\u6790\u6587\u672c\u4e2d\u5b58\u5728\u91cd\u8981\u9519\u8bef\uff0c\u5e94\u4fee\u6539\u76f8\u5e94\u7684\u5206\u6790\u6587\u672c\u3002\u4ec5\n\u5f53\u8be5\u9519\u8bef\u4e25\u91cd\u5f71\u54cd\u5230\u7efc\u5408\u5f97\u5206\u65f6\uff0c\u624d\u614e\u91cd\u4fee\u6539\u7efc\u5408\u5f97\u5206\u3002\n4. \u4fee\u6539\u540e\u6240\u6709\u8f93\u51fa\u683c\u5f0f\u9700\u8981\u548c\u5f53\u524d\u8bc4\u4ef7\u6587\u672c\u4e25\u683c\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u4f60\u56de\u7b54\u7684\u672b\u5c3e\uff0c\u4ecd\u9700\u8981\u6309\u7167\u4ee5\u4e0b\u5b57\u5178\u683c\u5f0f\uff08\u5305\u62ec\u62ec\u53f7\uff09\u8fd4\u56de\u4f60\u7684\u7efc\u5408\u8d28\u91cf\u5f97\u5206\uff0c\u5e76\n\u786e\u4fdd\u4f60\u8fd4\u56de\u7684\u7ed3\u679c\u548c\u4e0a\u8ff0\u751f\u6210\u6587\u672c\u4e2d\u7684\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\uff1a\n{{\u2019\u7efc\u5408\u5f97\u5206\u2019: \u56de\u7b54\u7684\u7efc\u5408\u8d28\u91cf\u5f97\u5206}}\uff0c\u4f8b\u5982\uff1a{{\u2019\u7efc\u5408\u5f97\u5206\u2019: \u20195\u2019}}\u3002\n\u7528\u6237\u7684\u63d0\u95ee\uff1a{Question}\n[\u53c2\u8003\u7b54\u6848\u5f00\u59cb]\n{Reference}\n[\u53c2\u8003\u7b54\u6848\u7ed3\u675f]\n[\u52a9\u624b\u7684\u7b54\u6848\u5f00\u59cb]\n{Generated Text}\n[\u52a9\u624b\u7684\u7b54\u6848\u7ed3\u675f]\n[\u8bc4\u4ef7\u6587\u672c\u5f00\u59cb]\n{Referenced Pointwise Grading Critique for Generated Text}\n[\u8bc4\u4ef7\u6587\u672c\u7ed3\u675f]\nReferenced\nPairwise\nComparison\nto Reference-\nFree Pairwise\nComparison\n\u4f60\u662f\u4e00\u4e2a\u64c5\u957f\u8bc4\u4ef7\u6587\u672c\u8d28\u91cf\u7684\u52a9\u624b\u3002\u8bf7\u4f60\u6839\u636e\u4ee5\u4e0b\u8981\u6c42\u4fee\u6539\u6bd4\u8f83\u5f0f\u8bc4\u4ef7\u6587\u672c\u3002\n1. \u5728\u4fee\u6539\u540e\u7684\u8bc4\u4ef7\u6587\u672c\u4e2d\uff0c\u4e0d\u8981\u76f4\u63a5\u63d0\u53ca\u53c2\u8003\u7b54\u6848\u3002\u53ef\u4ee5\u5728\u8bc4\u4ef7\u6587\u672c\u4e2d\u9002\u5f53\u5229\u7528\u53c2\u8003\u7b54\u6848\u4e2d\u7684\u5177\u4f53\u5185\u5bb9\u8f85\u52a9\u5206\u6790\uff0c\u4f46\u4e0d\u8981\u8ba9\u8bfb\u8005\u611f\u53d7\u5230\u53c2\u8003\u7b54\u6848\n\u7684\u5b58\u5728\u3002\u4fee\u6539\u540e\u7684\u8bc4\u4ef7\u6587\u672c\u9700\u8981\u8bed\u8a00\u4e0a\u901a\u987a\uff0c\u903b\u8f91\u4e0a\u5408\u7406\uff0c\u5206\u6790\u5185\u5bb9\u4e0e\u6bd4\u8f83\u7ed3\u679c\u547c\u5e94\u3002\n2. \u5728\u4fee\u6539\u5404\u4e2a\u7ef4\u5ea6\u7684\u6bd4\u8f83\u5206\u6790\u65f6\uff0c\u5206\u6790\u7684\u5185\u5bb9\u9700\u8981\u548c\u5f53\u524d\u8bc4\u4ef7\u6587\u672c\u57fa\u672c\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u4e0d\u8981\u76f4\u63a5\u63d0\u53ca\u53c2\u8003\u7b54\u6848\u3002\n3. \u5728\u4fee\u6539\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u7684\u5206\u6790\u6587\u672c\u65f6\uff0c\u4e0d\u8981\u76f4\u63a5\u63d0\u53ca\u53c2\u8003\u7b54\u6848\uff0c\u5c3d\u91cf\u4fdd\u7559\u5f53\u524d\u8bc4\u4ef7\u6587\u672c\u4e2d\u7684\u5176\u4ed6\u7ec6\u8282\uff0c\u5e76\u5145\u5206\u5229\u7528\u4fee\u6539\u540e\u7684\u5206\u7ef4\u5ea6\u5206\u6790\u3002\u4fee\u6539\u540e\n\u7684\u7efc\u5408\u5206\u6790\u6587\u672c\u5e94\u901a\u987a\u3001\u6d41\u7545\u3001\u81ea\u6d3d\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\u5e94\u4e0e\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\u3002\u5982\u679c\u53d1\u73b0\u5f53\u524d\u7efc\u5408\u5206\u6790\u6587\u672c\u4e2d\u5b58\u5728\u91cd\u8981\u9519\u8bef\uff0c\u5e94\u4fee\u6539\u76f8\u5e94\u7684\u5206\u6790\n\u6587\u672c\u3002\u4ec5\u5f53\u8be5\u9519\u8bef\u4e25\u91cd\u5f71\u54cd\u5230\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u65f6\uff0c\u624d\u614e\u91cd\u4fee\u6539\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u3002\n4. \u4fee\u6539\u540e\u6240\u6709\u8f93\u51fa\u683c\u5f0f\u9700\u8981\u548c\u5f53\u524d\u8bc4\u4ef7\u6587\u672c\u4e25\u683c\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u4f60\u56de\u7b54\u7684\u672b\u5c3e\uff0c\u4ecd\u9700\u8981\u6309\u7167\u4ee5\u4e0b\u5b57\u5178\u683c\u5f0f\uff08\u5305\u62ec\u62ec\u53f7\uff09\u8fd4\u56de\u4f60\u7684\u7efc\u5408\u8d28\u91cf\u9009\u62e9\u7ed3\n\u679c\uff0c\u5373\u4f60\u9009\u62e9\u7684\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u90a3\u4e2aAI\u52a9\u624b\uff08\u6216\u8005\u8ba4\u4e3a\u8d28\u91cf\u76f8\u5f53\uff09\uff0c\u5e76\u786e\u4fdd\u4f60\u8fd4\u56de\u7684\u7ed3\u679c\u548c\u4e0a\u8ff0\u751f\u6210\u6587\u672c\u4e2d\u7684\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\uff1a\n{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u56de\u7b54\u7efc\u5408\u8d28\u91cf\u66f4\u9ad8\u7684\u52a9\u624b\u5e8f\u53f7\u6216\u8d28\u91cf\u76f8\u5f53}}\uff0c\u4f8b\u5982\uff1a{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u2019\u52a9\u624b1\u2019}}\u6216{{\u2019\u7efc\u5408\u6bd4\u8f83\u7ed3\u679c\u2019: \u2019\u52a9\u624b2\u2019}}\u6216{{\u2019\u7efc\u5408\u6bd4\u8f83\n\u7ed3\u679c\u2019: \u2019\u8d28\u91cf\u76f8\u5f53\u2019}}\u3002\n\u7528\u6237\u7684\u63d0\u95ee\uff1a{Question}\n[\u53c2\u8003\u7b54\u6848\u5f00\u59cb]\n{Reference}\n[\u53c2\u8003\u7b54\u6848\u7ed3\u675f]\n[\u52a9\u624b1\u7684\u7b54\u6848\u5f00\u59cb]\n{Generated Text 1}\n[\u52a9\u624b1\u7684\u7b54\u6848\u7ed3\u675f]\n[\u52a9\u624b2\u7684\u7b54\u6848\u5f00\u59cb]\n{Generated Text 2}\n[\u52a9\u624b2\u7684\u7b54\u6848\u7ed3\u675f]\n[\u8bc4\u4ef7\u6587\u672c\u5f00\u59cb]\n{Referenced Pairwise Comparison Critique for Generated Text 1&2}\n[\u8bc4\u4ef7\u6587\u672c\u7ed3\u675f]\nTable 9: Referenced-to-Reference-Free prompt design in multi-path prompting.\nSetting\nPrompt\nReferenced\nPointwise\nGrading\nto\nReference-\nFree\nPoint-\nwise Grading\nYou are an expert at text quality evaluation. Please revise the critique following the instructions below:\n1. In the revised critique, do not directly refer to the reference answer. You can use the specific content in the reference answer to assist your analysis in the\ncritique, but do not make the readers feel the presence of the reference answer. The revised critique should be fluent, logically reasonable. The explanation\nshould be consistent with the score.\n2. When revising the explanation of each dimension, the content should basically be consistent with the corresponding score. But do not directly mention the\nreference answer.\n3. When revising the explanation of the final score, do not directly mention the reference answer. Try to retain other details in the current critique and fully utilize\nthe modified critique of each dimension. The revised explanation of the final score should be smooth, fluent, and self-consistent, and it should commonly be\nconsistent with the final score. If an important error is found in the current critique, the error in the critique should be revised. Only when this error severely\naffects the final score, you may carefully revise the final score.\n4. The output format of all the revised results needs to strictly adhere to the current critique. At the end of your output, you still need to return your overall\nquality score in the following dictionary format (including brackets), and ensure that the result you return is consistent with the result in the above generated text.\n{{\u2019Overall Score\u2019: Score for Overall Quality}}\uff0cfor instance: {{\u2019Overall Score\u2019: \u20195\u2019}}\u3002\nThe user\u2019s query: {Question}\n[Reference Answer Begin]\n{Reference}\n[Reference Answer End]\n[AI Assistant\u2019s Answer Begin]\n{Generated Text}\n[AI Assistant\u2019s Answer End]\n[Critique Begin]\n{Referenced Pointwise Grading Critique for Generated Text}\n[Critique End]\nReferenced\nPairwise\nComparison\nto Reference-\nFree Pairwise\nComparison\nYou are an expert at text quality evaluation. Please revise the critique following the instructions below:\n1. In the revised critique, do not directly refer to the reference answer. You can use the specific content in the reference answer to assist your analysis in the\ncritique, but do not make the readers feel the presence of the reference answer. The revised critique should be fluent, logically reasonable. The explanation\nshould be consistent with the comparison result.\n2. When revising the explanation of each dimension, the content should basically be consistent with the current critiques. But do not directly mention the\nreference answer.\n3. When revising the explanation of the overall comparison result, do not directly mention the reference answer. Try to retain other details in the current critique\nand fully utilize the modified critique of each dimension. The revised explanation of the overall comparison result should be smooth, fluent, and self-consistent,\nand it should commonly be consistent with the overall comparison result. If an important error is found in the current critique, the error in the critique should be\nrevised. Only when this error severely affects the overall comparison result, you may carefully revise the overall comparison result.\n4. The output format of all the revised results needs to strictly adhere to the current critique. At the end of your output, you still need to return your overall\ncomparison result in the following dictionary format (including brackets), and ensure that the result you return is consistent with the result in the above generated\ntext.\n{{\u2019Overall Comparison Result\u2019: the assistant number with higher overall quality or tie }}, for example: {{\u2019Overall Comparison Result\u2019: \u2019Assistant 1\u2019}} or\n{{\u2019Overall Comparison Result\u2019: \u2019Assistant 2\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Tie\u2019}}.\nThe user\u2019s query: {Question}\n[Reference Answer Begin]\n{Reference}\n[Reference Answer End]\n[Assistant 1\u2019s Answer Begin]\n{Generated Text 1}\n[Assistant 1\u2019s Answer End]\n[Assistant 2\u2019s Answer Begin]\n{Generated Text 2}\n[Assistant 2\u2019s Answer End]\n[Critique Begin]\n{Referenced Pairwise Comparison Critique for Generated Text 1&2 }\n[Critique End]\nTable 10: Referenced-to-Reference-Free prompt design in multi-path prompting (translated into English).\nReferenced Pairwise Comparison to ReferenceFree Pairwise Comparison You are an expert at text quality evaluation. Please revise the critique following the instructions below: 1. In the revised critique, do not directly refer to the reference answer. You can use the specific content in the reference answer to assist your analysis in the critique, but do not make the readers feel the presence of the reference answer. The revised critique should be fluent, logically reasonable. The explanation should be consistent with the comparison result. 2. When revising the explanation of each dimension, the content should basically be consistent with the current critiques. But do not directly mention the reference answer. 3. When revising the explanation of the overall comparison result, do not directly mention the reference answer. Try to retain other details in the current critique and fully utilize the modified critique of each dimension. The revised explanation of the overall comparison result should be smooth, fluent, and self-consistent, and it should commonly be consistent with the overall comparison result. If an important error is found in the current critique, the error in the critique should be revised. Only when this error severely affects the overall comparison result, you may carefully revise the overall comparison result. 4. The output format of all the revised results needs to strictly adhere to the current critique. At the end of your output, you still need to return your overall comparison result in the following dictionary format (including brackets), and ensure that the result you return is consistent with the result in the above generated text. {{\u2019Overall Comparison Result\u2019: the assistant number with higher overall quality or tie }}, for example: {{\u2019Overall Comparison Result\u2019: \u2019Assistant 1\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Assistant 2\u2019}} or {{\u2019Overall Comparison Result\u2019: \u2019Tie\u2019}}. The user\u2019s query: {Question} [Reference Answer Begin] {Reference} [Reference Answer End] [Assistant 1\u2019s Answer Begin] {Generated Text 1} [Assistant 1\u2019s Answer End] [Assistant 2\u2019s Answer Begin] {Generated Text 2} [Assistant 2\u2019s Answer End] [Critique Begin] {Referenced Pairwise Comparison Critique for Generated Text 1&2 } [Critique End]\nHuman Score\n(1-5)\n3\nChatGPT\n(1-10)\n\u8bc4\u4f30\u7ed3\u679c\u5982\u4e0b\uff1a\n1. \u4e8b\u5b9e\u6b63\u786e\u6027: 9. \u8fd9\u6bb5\u56de\u7b54\u5e76\u6ca1\u6709\u6d89\u53ca\u5177\u4f53\u7684\u4e8b\u5b9e\u6570\u636e\uff0c\u800c\u662f\u8868\u8fbe\u4e86\u4e00\u79cd\u60c5\u611f\u548c\u5fc3\u5883\uff0c\u56e0\u6b64\u65e0\u6cd5\u5bf9\u4e8b\u5b9e\u6b63\u786e\u6027\u8fdb\u884c\u8bc4\u4f30\u3002\n2. \u6ee1\u8db3\u7528\u6237\u9700\u6c42: 8. \u56de\u7b54\u5168\u9762\u800c\u6070\u5f53\u5730\u8868\u8fbe\u4e86\u5b64\u72ec\u5bc2\u5bde\u7684\u5fc3\u5883\uff0c\u5bf9\u7528\u6237\u63d0\u51fa\u7684\u95ee\u9898\u8fdb\u884c\u4e86\u5168\u9762\u56de\u5e94\u3002\n3. \u903b\u8f91\u8fde\u8d2f\u6027: 9. \u56de\u7b54\u5728\u6574\u4f53\u4e0a\u4fdd\u6301\u4e86\u903b\u8f91\u8fde\u8d2f\u6027\uff0c\u60c5\u611f\u63cf\u5199\u6d41\u7545\u81ea\u7136\uff0c\u6ca1\u6709\u51fa\u73b0\u81ea\u76f8\u77db\u76fe\u7684\u60c5\u51b5\u3002\n4. \u521b\u9020\u6027: 9. \u56de\u7b54\u5177\u6709\u521b\u65b0\u6027\u548c\u72ec\u7279\u6027\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5b64\u72ec\u5bc2\u5bde\u5fc3\u5883\u7684\u6df1\u523b\u89c1\u89e3\u548c\u60c5\u611f\u63cf\u5199\u3002\n5. \u4e30\u5bcc\u5ea6: 9. \u56de\u7b54\u5305\u542b",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of evaluating large language models (LLMs) and the limitations of existing critique generation models that rely heavily on direct prompting from models like GPT-4. It highlights the necessity for a new method to generate more informative critiques that can effectively assess the quality of generated texts in various tasks.",
        "problem": {
            "definition": "The problem is the inability of current critique generation models to provide fine-grained and informative critiques for evaluating generated texts, particularly in reference-free settings.",
            "key obstacle": "The main challenge is that existing models tend to produce general critiques lacking specific details, which undermines their effectiveness in distinguishing the quality of different generated texts."
        },
        "idea": {
            "intuition": "The idea stems from the observation that referenced critiques provide richer information, and thus, leveraging these critiques could enhance the quality of evaluations in other settings.",
            "opinion": "The proposed method, Eval-Instruct, aims to construct informative evaluation data through multi-path prompting strategies, thereby improving the critique generation process.",
            "innovation": "The innovation lies in the dual prompting strategies that enrich critiques by combining pointwise grading with pairwise comparisons and adapting referenced critiques for reference-free scenarios."
        },
        "method": {
            "method name": "CRITIQUELLM",
            "method abbreviation": "CRL",
            "method definition": "CRITIQUELLM is a critique generation model designed to provide informative evaluations of generated texts through a systematic approach that includes multi-path prompting and fine-tuning.",
            "method description": "The method utilizes a two-step process involving the acquisition of pointwise grading critiques and their revision through multipath prompting to enhance the informativeness of the critiques.",
            "method steps": [
                "Construct an informative instruction-tuning dataset for evaluation tasks.",
                "Acquire referenced pointwise grading critiques using GPT-4.",
                "Employ multi-path prompting strategies to generate critiques for various tasks.",
                "Implement cross-validation to ensure data quality.",
                "Fine-tune the model on the constructed datasets."
            ],
            "principle": "The effectiveness of CRITIQUELLM is based on the principle that richer, more detailed critiques can significantly improve the evaluation of generated texts, leading to better performance in distinguishing text quality."
        },
        "experiments": {
            "evaluation setting": "The evaluation was conducted using three benchmark datasets that encompass various NLP tasks, including pointwise grading and pairwise comparisons, both with and without references.",
            "evaluation method": "Performance was assessed through correlation metrics such as Pearson, Spearman, and Kendall coefficients, as well as agreement and consistency rates in pairwise comparisons."
        },
        "conclusion": "The experiments demonstrate that CRITIQUELLM outperforms existing models, including ChatGPT and other open-source baselines, achieving comparable results to GPT-4 in system-level correlations of pointwise grading.",
        "discussion": {
            "advantage": "The key advantage of CRITIQUELLM is its ability to generate detailed and informative critiques that significantly improve the evaluation of generated texts, making it a valuable tool for enhancing LLM performance.",
            "limitation": "One limitation is the potential increase in input length and API costs due to the multi-path prompting approach, which may complicate data acquisition.",
            "future work": "Future research could focus on addressing the self-evaluation bias observed in model-based evaluations and exploring methods to enhance critique generation for reasoning-intensive tasks."
        },
        "other info": {
            "acknowledgements": "This work was supported by various funding sources, including the NSFC projects and the National Science Foundation for Distinguished Young Scholars.",
            "data availability": "The codes and datasets used in this study are publicly available at the provided GitHub repository."
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The method CRITIQUELLM is designed to provide informative evaluations of generated texts through a systematic approach that includes multi-path prompting and fine-tuning."
        },
        {
            "section number": "4.2",
            "key information": "The proposed method, Eval-Instruct, aims to construct informative evaluation data through multi-path prompting strategies, thereby improving the critique generation process."
        },
        {
            "section number": "5.1",
            "key information": "The evaluation was conducted using three benchmark datasets that encompass various NLP tasks, including pointwise grading and pairwise comparisons, both with and without references."
        },
        {
            "section number": "10.2",
            "key information": "Future research could focus on addressing the self-evaluation bias observed in model-based evaluations and exploring methods to enhance critique generation for reasoning-intensive tasks."
        },
        {
            "section number": "10.3",
            "key information": "One limitation is the potential increase in input length and API costs due to the multi-path prompting approach, which may complicate data acquisition."
        }
    ],
    "similarity_score": 0.7241520453977959,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ce2/6ce20c11-e5c1-46cf-af8f-921e889fdf9e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40ba/40baa1b3-51c4-4eb2-8879-2f418d509a1c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2aa2/2aa26e80-be79-46ab-98b2-ead6c5f9b0fc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb6a/bb6ae4b6-2d70-4982-a546-8545ded37aab.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d25/4d258677-30ab-4d8f-a946-d6b34f5b2b27.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Critiquellm_ Scaling llm-as-critic for effective and explainable evaluation of large language model generation.json"
}