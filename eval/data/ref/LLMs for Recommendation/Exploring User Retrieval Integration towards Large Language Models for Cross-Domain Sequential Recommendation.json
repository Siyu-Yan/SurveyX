{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.03085",
    "title": "Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation",
    "abstract": "Cross-Domain Sequential Recommendation (CDSR) aims to mine and transfer users\u2019 sequential preferences across different domains to alleviate the long-standing cold-start issue. Traditional CDSR models capture collaborative information through user and item modeling while overlooking valuable semantic information. Recently, Large Language Model (LLM) has demonstrated powerful semantic reasoning capabilities, motivating us to introduce them to better capture semantic information. However, introducing LLMs to CDSR is non-trivial due to two crucial issues: seamless information integration and domain-specific generation. To this end, we propose a novel framework named URLLM, which aims to improve the CDSR performance by exploring the User Retrieval approach and domain grounding on LLM simultaneously. Specifically, we first present a novel dual-graph sequential model to capture the diverse information, along with an alignment and contrastive learning method to facilitate domain knowledge transfer. Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability. Furthermore, we propose a domainspecific strategy and a refinement module to prevent out-of-domain generation. Extensive experiments on Amazon demonstrated the information integration and domain-specific generation ability of URLLM in comparison to state-of-the-art baselines. Our code is available at https://github.com/TingJShen/URLLM",
    "bib_name": "shen2024exploringuserretrievalintegration",
    "md_text": "# Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation\n# ABSTRACT\nCross-Domain Sequential Recommendation (CDSR) aims to mine and transfer users\u2019 sequential preferences across different domains to alleviate the long-standing cold-start issue. Traditional CDSR models capture collaborative information through user and item modeling while overlooking valuable semantic information. Recently, Large Language Model (LLM) has demonstrated powerful semantic reasoning capabilities, motivating us to introduce them to better capture semantic information. However, introducing LLMs to CDSR is non-trivial due to two crucial issues: seamless information integration and domain-specific generation. To this end, we propose a novel framework named URLLM, which aims to improve the CDSR performance by exploring the User Retrieval approach and domain grounding on LLM simultaneously. Specifically, we first present a novel dual-graph sequential model to capture the diverse information, along with an alignment and contrastive learning method to facilitate domain knowledge transfer. Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability. Furthermore, we propose a domainspecific strategy and a refinement module to prevent out-of-domain generation. Extensive experiments on Amazon demonstrated the information integration and domain-specific generation ability of URLLM in comparison to state-of-the-art baselines. Our code is available at https://github.com/TingJShen/URLLM\n# CCS CONCEPTS\nCCS CONCEPTS \u2022 Information systems \u2192Information retrieval; \u2022 Computing methodologies \u2192Natural language generation.\n\u2022 Information systems \u2192Information retrieval; \u2022 Computing methodologies \u2192Natural language generation.\n# KEYWORDS\nCross-Domain Sequential Recommendation, Large Language Model, Cold-Start Recommendation\nCross-Domain Sequential Recommendation, Large Language Model, Cold-Start Recommendation\n# 1 INTRODUCTION\nSequential Recommendation (SR), focused on suggesting the next item for a user based on their past sequential interactions to capture dynamic user preferences, has gained significant attention in commercial, social, and diverse scenarios [19, 50, 64]. However, SR methods within a single domain usually encounter the long-standing cold-start issue [41], i.e., it is challenging to perform personalized recommendations for users with few interaction records. To address the issue, Cross-Domain Sequential Recommendation (CDSR) has garnered considerable attention in the field of recommendation systems, aiming to mine and transfer users\u2019 sequential preferences across different domains [12, 58]. Pioneer works like \ud835\udf0b-net [39] and PSJNet [44] focused on designing knowledge transfer modules to capture cross-domain user preferences. Followup works like MIFN [38] and DA-GCN [13] further borrowed the powerful strength of Graph Neural Networks (GNNs) to model the high-order relationship across domains. These methods have been demonstrated to be effective for the CDSR problem. Despite the achieved results, most previous works overlook the valuable semantic information buried in item features [1, 13, 63], leading to skewed user preferences. Recently, the powerful emergent capabilities [5] of Large Language Models (LLMs) have revolutionized the field of recommendation systems [8], which can absorb item text features and inject pre-trained common knowledge into recommendation systems. Meanwhile, they can generate recommendations based on user preferences and historical data, enabling interactive and explainable recommendations [9]. LLMs also offer the flexibility to design tuning strategies for specific subtasks, such as determining whether to recommend an item or integrating collaborative information. This integration aligns the sequential behaviors of users with the language space, creating new modalities for recommendation [30, 61]. Therefore, we are motivated by these encouraging capabilities to propose an LLM paradigm for the CDSR scenario.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d48f/d48f0a41-4fb6-425c-aa85-5cccea14ddc9.png\" style=\"width: 50%;\"></div>\nFigure 1: An illustration of cold-start CDSA task along with the various forms of information, the domain-specific demand on information and generation. The line linking attributes represent the structural-semantic information.\nHowever, harnessing the explicit capabilities of LLMs in the CDSR scenario is non-trivial. Two essential issues arise: (1) Seamless Information Integration: As shown in Figure 1, the CDSR task involves diverse formats of domain information, including collaborative and semantic information. Items exhibit intricate intrinsic structures respectively specified by the diverse information. To fully leverage the emergent capabilities of LLMs, it is crucial to integrate the structured information into LLMs in a seamless manner. (2) Domain-Specific Generation. CDSR requires equipping the model with domain-specific information and constraining the generation process to remain confined within a specific domain. However, despite instructing the model to generate items within a particular domain, the uncontrollable nature of LLMs leads to 2% to 20% of generated content belonging to other domains. Figure 1 illustrates an optimal pipeline for domain-specific generation, wherein distinct information aligns with specific generation processes. Regarding issue (1), some efforts have been made to integrate collaborative and semantic information with LLMs. Pioneer works like UniSRec [21] and CTRL [34] proposed to integrate semantic information through a discriminative LLM like BERT. As for collaborative information, BIGRec [3] adopts a post-processing approach, integrating through statistical results. RecInterpreter [57], LLaRA [35], and CoLLM [65] tend to capture collaborative information using an external traditional model and map it into the input token embedding space of LLM. However, these methods fail to seamlessly incorporate the emergent language reasoning capabilities of LLMs [33, 54], as evidenced by the misalignment between the pre-trained knowledge representation of LLMs and\ntheir embedding representations. This oversight is particularly evident in the absence of a seamless integration of collaborative information and structural-semantic information. Consequently, the integration of diverse information continues to pose a substantial challenge. In addressing issue (2), existing retrieval methodologies for LLMs, such as ICL [33] and Agent-based LLM [23, 62], primarily yield common sense retrievals rather than domain-specific information. Furthermore, while the existing LLM-based recommendation model [4] [3] [47] acknowledge the cross-domain capabilities of LLMs, they fail to impose constraints on the generation process of LLMs. This lack of restriction results in out-of-domain generations, significantly undermining the performance of CDSR. Towards these challenges, in this paper, we propose a novel framework named URLLM, addressing the cold-start issue by exploring a novel paradigm of User-Retrieval approach and domain grounding on LLM simultaneously. Firstly, we propose a dual graph sequence modeling model combining alignment and contrastive learning on an item-attribute graph and three item-item domain sequence graphs, aiming to model collaborative and structuralsemantic information. Subsequently, we present a KNN user retriever to retrieve relevant user information for LLM. By leveraging LLM\u2019s extensive reasoning and few-shot learning capabilities in the integration of collaborative user information, we can achieve a seamless integration of diverse information. Finally, in order to preserve the domain-specific nature of both the input and generated responses in the LLM, we propose a domain differentiation strategy for user retrieval modules. Additionally, we introduce a refining mechanism to enhance the outputs of the LLM. Extensive experiments on two datasets, including movie-game and art-office on Amazon following [21] have demonstrated the effectiveness of the proposed framework. Moreover, through analysis of experimental results, we (1) recognize that the improvement brought by the types of information integrated into components of URLLM positively correlated with the most crucial information in the dataset. (2) find there existing positive relation between the hit rate of retrieved users and the performance of the model. The main contributions could be summarized as follows:\n\u2022 To our best knowledge, we are the first to study CDSR from a new perspective on the user retrieval paradigm with seamless information integration and domain-specific generation. \u2022 We develop a user retrieval bounded interaction paradigm between dual graph sequence modeling models and LLM. With the aid of the module, we can integrate structural-semantic and collaborative information into LLM in a seamless manner. \u2022 We introduce a domain differentiation strategy for user retrieval modules and a refinement module for the generated items of the LLM. The proposed module ensures that the integrated user information and generation are tailored to specific domains, aiming for domain-specific generation. \u2022 Extensive experiments on two public datasets and ablation analysis validate that our URLLM framework unequivocally affirms the information integration and domain-specific generation ability of our proposed framework.\nring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recom\n# 2 RELATED WORK 2.1 Sequential Recom\n# 2 RELATED WORK 2.1 Sequential Recom\n# 2 RELATED WORK\n# 2.1 Sequential Recommendation\nSequential recommendation is a technique that aims to delve into and understand users\u2019 interest patterns by analyzing their historical interactions. Initially, techniques such as markov chain and matrix factorization were employed [17]. However, with the emergence of neural networks, deep learning approaches like GRU4Rec [18], Hypersorec [49] and Caser [46] were introduced to improve recommendation accuracy, while some efforts are made with clustering [15], denoising [14] or data regeneration [59]. Another notable technique in sequential recommendation is the attention mechanism. SASRec [27] and APGL4SR [60], for instance, utilize selfattention to independently learn the impact of each interaction on the target behavior. On the other hand, BERT4Rec [42] incorporates bi-directional transformer layers after conducting pre-training tasks. In recent years, graph neural networks (GNNs) have gained attention for their ability to capture higher-order relationships among items. GCE-GNN [52] constructs local session graphs and leverages information from other sessions to create a dense global graph for modeling the current session. SR-GNN [? ] employs gated GNNs in session graphs to capture complex item transitions. To address the issue of data sparsity, contrastive mechanisms have been adopted in some works. CL4SRec [56] and CoSeRec [37] propose data augmentation approaches to construct contrastive tasks, which help alleviate the sparsity problem.\n# 2.2 Cross-Domain Sequential Recommendation\nCross-Domain Sequential Recommendation (CDSR) aims to improve recommendation performance for tasks involving items from different domains. Pioneering works in this field include \ud835\udf0b-Net [39] and PSJNet [44], which employ sophisticated gating mechanisms to transfer single-domain information. CD-SASRec [1] extends SASRec to the cross-domain setting by integrating the source-domain aggregated vector into the target-domain item embedding. DAGCN [13], a GNN-based model, constructs a domain-aware graph to capture associations among items from different domains. Hybrid models that combine various techniques to capture item dependencies within sequences and complex associations between domains have also been proposed. RecGURU [31] introduces adversarial learning to unify user representations from different domains into a generalized representation. UniSRec [21] leverages item texts to learn more transferable representations for sequential recommendation. In comparison, C2DSR [6] employs a graphical and attentional encoder to capture item relationships. It utilizes two sequential objectives, in conjunction with a contrastive objective, to facilitate the joint learning of single-domain and cross-domain user representations, achieving significant progress.\n# 2.3 LLM-based Recommendation System\nLarge Language Models (LLMs) have been widely adopted as recommender systems to leverage item text features and enhance recommendation performance [11, 36, 55]. The majority of existing LLM-based recommenders operate in a tuning-free manner, utilizing pretrained knowledge to generate recommendations for the next item [45, 51]. For instance, CHAT-REC [10] employs ChatGPT\nto grasp user preferences and enhance interactive and explainable recommendations. GPT4Rec [32] utilizes GPT-2 to generate hypothetical \"search queries\" based on a user\u2019s historical data, which are then queried using the BM25 search engine to retrieve recommended items. Another research direction in LLM-based recommendation focuses on designing tuning strategies for specific subtasks. TALLRec [4], for example, employs instruction-tuning to determine whether an item should be recommended. BIGRec [3] adopts a post-processing approach where recommendations are initially generated using LLM and then integrated with collaborative information through an ensemble method. RecInterpreter [57] and LLaRA [35] proposes a novel perspective by considering the \"sequential behaviors of users\" as a new modality for LLMs in recommendation, aligning it with the language space. CoLLM [65] captures collaborative information using an external traditional model and maps it into the input token embedding space of LLM, creating collaborative embeddings for LLM utilization. Despite the significant progress achieved in the field of SR through traditional or LLM-based methods, these approaches tend to focus on a limited perspective of information formats. Consequently, there is an underutilization of information pertaining to the coldstart feature of CDSR, which is the primary focus of this paper. Moreover, although some previous works such as Tallrec [4], BIGRec [3], and LLM-Rec [47] recognize the cross-domain capabilities of LLMs, there is currently a lack of an LLM-based structure specifically optimized for CDSR.\n# 3 PRELIMINARY\n# 3.1 Problem Definition\nIn this work, we consider a general CDSR scenario, where users in the user set \ud835\udc48= {\ud835\udc621,\ud835\udc622, ...,\ud835\udc62|\ud835\udc48|} have interactions with two product sets \ud835\udc4b= {\ud835\udc651,\ud835\udc652, ...,\ud835\udc65|\ud835\udc4b|} and \ud835\udc4c= {\ud835\udc661,\ud835\udc662, ...,\ud835\udc66|\ud835\udc4c|}. Each user \ud835\udc62\u2208\ud835\udc48has an interaction sequence \ud835\udc46\ud835\udc62= [\ud835\udc561,\ud835\udc562, ...,\ud835\udc56\ud835\udc61, ...,\ud835\udc56\ud835\udc58] representing the chronological order of items in two domains. The primary objective of CDSR is to train the model \ud835\udc40(\ud835\udc4b,\ud835\udc4c,\ud835\udc48,\ud835\udc46) to predict the subsequent product\ud835\udc56\ud835\udc58+1 \u2208\ud835\udc4b\u222a\ud835\udc4c, where\ud835\udc46= {\ud835\udc461,\ud835\udc462, ...,\ud835\udc46|\ud835\udc48|} denotes the cross-domain interaction sequence.\nIn this paper, we use LLMs as the recommendation model \ud835\udc40for answer formulation. The LLMs should be fine-tuned to adeptly adapt to the data distribution and domain knowledge relevant to specific downstream tasks. The fine-tuning process involves meticulously crafting instruction data to guide the model\u2019s output scope and format, as detailed below:\n(1)\nwhere \u03a6 is the parameters of LLM to be optimized,\ud835\udc47is the training set, \ud835\udc4e\ud835\udc56,\ud835\udc61is the \ud835\udc61-th token of the generated answer word, and \ud835\udc65is the input context which contains an instruction and a query question.\n# 4 METHODOLOGY\nTo harness the inferential capabilities of Large Language Models (LLMs) for blending LLMs with traditional models, we present\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d013/d013a5d8-0bf2-4959-9efe-b20c2da1711f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The overall framework of URLLM. The component on the left showcases exemplary prompts employed in graph construction and the similar user-augmented LLM module. On the right, the process is delineated, wherein the expansiv reasoning and few-shot analogy capabilities of the LLM are harnessed, concomitantly integrating structured knowledge.</div>\nURLLM illustrated in Figure 2. Initially, we introduce a Dual Graph Sequence-Modeling Model. This model employs an LLM-enhanced item-attribute graph and an item-item sequence graph to encapsulate collaborative and structural-semantic information. This type of information is challenging to model with an isolated LLM. Then, we adopt a User Retrieve-Generation Model to retrieve the most similar users from the target domain and fuse structured text with collaborative information seamlessly into LLM. Finally, to make the output of our model directly match the real item in the correct domain, we combine BM25 [40] and the Dual Graph SequenceModeling Model to refine the generated prediction.\n# 4.1 Dual Graph Sequence Modeling Model\nAiming to model collaborative and semantic information, the construction of this model contains a graph construction module, a graph alignment module, and a contrastive self-attention module.\n4.1.1 Graph Construction Module. In order to capture valuable collaborative and semantic information, including their structural relationships, which can be effectively represented as a graph, we construct a graph based on rules due to the absence of such data in our dataset. The graph construction process comprises two modules: item-attribute graph construction and item-item graph construction. These modules establish collaborative relationships among items and structural-semantic relationships, respectively. For item-attribute graph construction, the Chain-of-Thought (COT) method [54] is employed to utilize an LLM for item description.\nSubsequently, the model summarizes this description to generate output attributes for the product. For instance, in Figure 2, for the product I=Tinker Bell, the model provides an introduction denoted as \ud835\udc3f\ud835\udc3f\ud835\udc40(\ud835\udc3c) = \ud835\udc3c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5cand generates the attribute reusing \ud835\udc3cas \ud835\udc34\ud835\udc3c= \ud835\udc3f\ud835\udc3f\ud835\udc40(\ud835\udc3c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc5c, \ud835\udc3c). The undirected attribute graph, denoted as \ud835\udc3a\ud835\udc34, is constructed by linking items with attributes formulated below: \ud835\udc3a\ud835\udc34= (\ud835\udc49, \ud835\udc38),\ud835\udc49= \ud835\udc4b\u222a\ud835\udc4c\u222a\ud835\udc34, \ud835\udc38= {(\ud835\udc63,\ud835\udc61)|\ud835\udc63\u2208\ud835\udc4b\u222a\ud835\udc4c,\ud835\udc61\u2208\ud835\udc34}. (2) where \ud835\udc49and \ud835\udc38denotes the vector and edges of graph \ud835\udc3a\ud835\udc34. For item-item graph construction, we construct \ud835\udc3a\ud835\udc46= \ud835\udc3a\ud835\udc4b+\ud835\udc4c, \ud835\udc3a\ud835\udc4b and \ud835\udc3a\ud835\udc4crespectively inspired by [7]. Firstly, given user interaction sequence\ud835\udc46\ud835\udc62= [\ud835\udc561, ...,\ud835\udc56\ud835\udc58], we split the interaction sequence into \ud835\udc46\ud835\udc62= \ud835\udc46\ud835\udc62,\ud835\udc46\ud835\udc4b\ud835\udc62= \ud835\udc46\ud835\udc62\u2229\ud835\udc4b,\ud835\udc46\ud835\udc4c\ud835\udc62= \ud835\udc46\ud835\udc62\u2229\ud835\udc4c. We then construct the directed item-item graph \ud835\udc3a\ud835\udc46,\ud835\udc3a\ud835\udc4b,\ud835\udc3a\ud835\udc4cby linking items before and after using the formula below: \ud835\udc3a\ud835\udc46= (\ud835\udc49, \ud835\udc38),\ud835\udc49= \ud835\udc4b\u222a\ud835\udc4c, \ud835\udc38= {(\ud835\udc63,\ud835\udc61)|\u2203\ud835\udc57, \ud835\udc63= \ud835\udc56\ud835\udc57,\ud835\udc61= \ud835\udc56\ud835\udc57+1}, (3) \ud835\udc3a\ud835\udc4b= (\ud835\udc49, \ud835\udc38),\ud835\udc49= \ud835\udc4b, \ud835\udc38= {(\ud835\udc63,\ud835\udc61)|\u2203\ud835\udc57, \ud835\udc63= \ud835\udc56\ud835\udc4b \ud835\udc57,\ud835\udc61= \ud835\udc56\ud835\udc4b \ud835\udc57+1}, (4) \ud835\udc3a\ud835\udc4c= (\ud835\udc49, \ud835\udc38),\ud835\udc49= \ud835\udc4c, \ud835\udc38= {(\ud835\udc63,\ud835\udc61)|\u2203\ud835\udc57, \ud835\udc63= \ud835\udc56\ud835\udc4c \ud835\udc57,\ud835\udc61= \ud835\udc56\ud835\udc4c \ud835\udc57+1}, (5) where \ud835\udc49denotes the vector set, \ud835\udc38denotes the edge set, \ud835\udc56\ud835\udc4band \ud835\udc56\ud835\udc4c\n(2)\n(4)\n(5)\n+ where \ud835\udc49denotes the vector set, \ud835\udc38denotes the edge set, \ud835\udc56\ud835\udc4band \ud835\udc56\ud835\udc4c denotes item in \ud835\udc46\ud835\udc4b\ud835\udc62and \ud835\udc46\ud835\udc4c\ud835\udc62.\n4.1.2 Graph Alignment Module. Recognizing the deficiency of differential knowledge across distinct domains in the existing itemattribute graph, it becomes imperative to investigate the acquisition of domain transfer information to enhance the item-attribute graphs. Domain transfer, in turn, enables the extraction of intricate\nintrinsic structures that are specifically specified by diverse information sources. However, the division of the item-attribute graph alone does not provide distinct information. Therefore, in this section, we propose a Graph Neural Network (GNN) that incorporates an alignment loss function to align and integrate these fragmented pieces of information. Given four graphs \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc46,\ud835\udc3a\ud835\udc4b,\ud835\udc3a\ud835\udc4c, We first initialize the graph embedding with \ud835\udc380 \ud835\udc34\u2208R|\ud835\udc380 \ud835\udc34|\u00d7\ud835\udc51\ud835\udc4e, \ud835\udc380 \ud835\udc46\u2208R|\ud835\udc380 \ud835\udc46|\u00d7\ud835\udc51, \ud835\udc380 \ud835\udc4b\u2208R|\ud835\udc380 \ud835\udc4b|\u00d7\ud835\udc51, \ud835\udc380 \ud835\udc4c\u2208 R|\ud835\udc380 \ud835\udc4c|\u00d7\ud835\udc51. Then, given these adjacency matrix \ud835\udc34\ud835\udc34,\ud835\udc34\ud835\udc46,\ud835\udc34\ud835\udc4b,\ud835\udc34\ud835\udc4c, we construct \ud835\udc59-layer GNN with output denoted as \ud835\udc38\ud835\udc59as below:\n(6)\nwhere \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(\u00b7) represent the row-normalized function, \ud835\udc38\ud835\udc56\u22121 represents the current graph convolutional layer and \ud835\udc38\ud835\udc56represents the next layer. Then, to fully encapsulate the graphical information across layers, we average the graph embeddings \ud835\udc38gaining \u02c6\ud835\udc3athat yield item representations as:\n(7)\nWe apply this procedure to \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc46,\ud835\udc3a\ud835\udc4b,\ud835\udc3a\ud835\udc4cseparately gaining \u02c6 \ud835\udc3a\ud835\udc34, \u02c6 \ud835\udc3a\ud835\udc46, \u02c6 \ud835\udc3a\ud835\udc4b, \u02c6 \ud835\udc3a\ud835\udc4c, each with its corresponding adjacency matrix \ud835\udc34and initialized graph embedding \ud835\udc380. Then, To resolve differences in input and output dimensions in alignment, we design linear projection models \ud835\udc3f, \ud835\udc3f\ud835\udc4b, \ud835\udc3f\ud835\udc4cwith \ud835\udc51\ud835\udc4eas input dim and \ud835\udc51as output dim to reform the hidden representation. Finally, the alignment loss function is designed below to transfer item-attribute graph knowledge of domains \ud835\udc4band \ud835\udc4c:\nWe then concat them together as the final representation \ud835\udc45of the item, gaining\ud835\udc45\ud835\udc46= \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc3f( \u02c6 \ud835\udc3a\ud835\udc34), \u02c6 \ud835\udc3a\ud835\udc46), \ud835\udc45\ud835\udc4b= \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc3f\ud835\udc4b( \u02c6 \ud835\udc3a\ud835\udc34), \u02c6 \ud835\udc3a\ud835\udc4b), \ud835\udc45\ud835\udc4c= \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc3f\ud835\udc4c( \u02c6 \ud835\udc3a\ud835\udc34), \u02c6 \ud835\udc3a\ud835\udc4c). With the alignment above the itemattribute and item-item graphs are modelized symmetrical and are successfully aligned to specific domains.\n4.1.3 Self-attention Contrastive Sequential Module. Having successfully obtained dual graph item representations from the aforementioned models, our objective is to capture users\u2019 sequential preferences based on their interaction sequences. This endeavor is crucial to promote the retrieval of analogous user behaviors, thereby ensuring the optimal utilization of the few-shot capabilities of LLMs. Similar to SASRec [26], we employ a multi-head self-attention layer and point-wise feed-forward layer to distinctly capture user preferences and enhance retrieval precision. The formal definition of sequence modeling is provided below:\n\ud835\udc3b\ud835\udc46= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc46(\ud835\udc46\ud835\udc62, \ud835\udc45\ud835\udc46), \ud835\udc3b\ud835\udc4b= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4b(\ud835\udc46\ud835\udc4b \ud835\udc62, \ud835\udc45\ud835\udc4b),\n\ud835\udc3b\ud835\udc46= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc46(\ud835\udc46\ud835\udc62, \ud835\udc45\ud835\udc46), \ud835\udc3b\ud835\udc4b= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4b(\ud835\udc46\ud835\udc4b \ud835\udc62, \ud835\udc45\ud835\udc4b), \ud835\udc3b\ud835\udc4c= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4c(\ud835\udc46\ud835\udc4c \ud835\udc62, \ud835\udc45\ud835\udc4c),\nDenoting user preferences in the full domain, domain X, and domain Y as \ud835\udc3b\ud835\udc46, \ud835\udc3b\ud835\udc4b, \ud835\udc3b\ud835\udc4crespectively, the training target of the sequential module focuses on predicting the next model. We incorporate a discriminator \ud835\udc37\ud835\udc4b, \ud835\udc37\ud835\udc4cwith a softmax function to assign\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/81a4/81a45dca-9577-4d41-96d6-a4629413970c.png\" style=\"width: 50%;\"></div>\nFigure 3: The example case illustrates the importance of LLM inferencing and similar user retrieval.\n<div style=\"text-align: center;\">Figure 3: The example case illustrates the importance of LLM inferencing and similar user retrieval.</div>\n# scores to each item and then maximize the score for the groundtruth answer. The training loss is defined as follows: \ufffd\nL\ud835\udc61= \ufffd\u2212\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc43(\ud835\udc56\ud835\udc58|\ud835\udc37\ud835\udc4b(\ud835\udc3b\ud835\udc46+ \ud835\udc3b\ud835\udc4b))) \ud835\udc56\ud835\udc58\u2208\ud835\udc4b \u2212\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc43(\ud835\udc56\ud835\udc58|\ud835\udc37\ud835\udc4c(\ud835\udc3b\ud835\udc46+ \ud835\udc3b\ud835\udc4c))) \ud835\udc56\ud835\udc58\u2208\ud835\udc4c.\n(10)\nwhere \ud835\udc56\ud835\udc58denotes the item to be recommended. To handle the negative transfer challenge on Cross-Domain Sequential Recommendation (CDSR), we adopt sequence corruption and random noise integration to gain negative sequence samples. The formalized definition is demonstrated below:\n\u02c6 \ud835\udc3b\ud835\udc4b= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4b( \u02c6\ud835\udc46\ud835\udc4b, \ud835\udc45\ud835\udc4b) + \u0394, \u02c6 \ud835\udc3b\ud835\udc4c= \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc35( \u02c6\ud835\udc46\ud835\udc4c, \ud835\udc45\ud835\udc4c) + \u0394, (1\n(11)  is a\nwhere \u02c6\ud835\udc46\ud835\udc4band \u02c6\ud835\udc46\ud835\udc4crandomly replaces X and Y domain items. \u0394 is a random noice with \u2225\u0394\u22252 = \ud835\udf16and \u0394 = \u00af\u0394 \u2299\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b(e\ud835\udc56), \u00af\u0394 \u2208R\ud835\udc51(0, 1) The contrastive training loss is denoted as below:\nL\ud835\udc50\ud835\udc61= \u03a3(\u2212\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc37\ud835\udc4b(\ud835\udc3b\ud835\udc46+ \ud835\udc3b\ud835\udc4b)) + (1 \u2212\ud835\udc37\ud835\udc4b(\ud835\udc3b\ud835\udc46+ \u02c6 \ud835\udc3b\ud835\udc4c))) + \u03a3(\u2212\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc37\ud835\udc4c(\ud835\udc3b\ud835\udc46+ \ud835\udc3b\ud835\udc4c)) + (1 \u2212\ud835\udc37\ud835\udc4c(\ud835\udc3b\ud835\udc46+ \u02c6 \ud835\udc3b\ud835\udc4b))) .\n(12)\nto maximize the infomax objective between domain preferences A and B. The final optimization loss is as follows:\nL = L\ud835\udc61+ \ud835\udf06L\ud835\udc50\ud835\udc61+ \ud835\udefeL\ud835\udc4e\ud835\udc59\n(13)\nBy combining contrastive alignment methodology for dual graph inherent knowledge preferences \ud835\udc3b\ud835\udc46, \ud835\udc3b\ud835\udc34, \ud835\udc3b\ud835\udc35, which is hard for LLM to model. Furthermore, the dual graph sequence-modeling model is also a recommendation model with answer \ud835\udc3c1 = [\ud835\udc561, ...,\ud835\udc56|\ud835\udc3c|], and the answer can be utilized to refine LLM\u2019s generation afterward.\n# 4.2 User Retrieve-Generation Model\nIn the preceding section, we obtained high-quality structured user preferences. However, LLM cannot directly leverage this representation information. In order to fully exploit the emergent capabilities of LLMs, such as few-shot learning and language inferencing, we need a model that translates the representation information into textual information, preferably in the form of similar example data. Therefore, the following retrieve-augmented-generation contains a KNN retriever, a LoRA-tuned LLM augmented by the target user and retrieved user interactions, and an answer refinement structure.\n4.2.1 KNN retriver. We employ a KNN retrieval model to query the training users using \ud835\udc3e\ud835\udc41\ud835\udc41(\ud835\udc62) to retrieve its k-nearest neighbors, denoted as N, based on a distance function \ud835\udc51(\u00b7, \u00b7), specifically using the inner product, as they are already normalized beforehand. The formal retrieval procedure is presented below:\n\ud835\udc38\u2032 \ud835\udc4b= \ud835\udc3b\ud835\udc46+ \ud835\udc3b\ud835\udc4b, \ud835\udc38\u2032\ud835\udc4c= \ud835\udc3b\ud835\udc46+ \ud835\udc3b\ud835\udc4c \ud835\udc48\ud835\udc5f= \ud835\udc3e\ud835\udc41\ud835\udc41(\ud835\udc62,\ud835\udc48, \ud835\udc38) \ufffd\n(14)\nwhere \ud835\udc48denotes the user in the training set, \ud835\udc48\ud835\udc5fdenotes the retrieved users.\n4.2.2 User Retrieval Augmented LLM. After gaining high-quality similar users, we then integrate this high-quality knowledge using few-shot learning, proven to be one of the most effective ways to enhance the emergent ability of LLM. Specifically, we form the prompt as \u2018There are similar users who bought these items:\u2019 with \ud835\udc46\ud835\udc48\ud835\udc5f= {\ud835\udc46\ud835\udc621, ...,\ud835\udc46\ud835\udc62\ud835\udc58},\ud835\udc621, ...,\ud835\udc62\ud835\udc58\u2208\ud835\udc48\ud835\udc5f, and \u2018The user has bought these items:\u2019 with \ud835\udc46\ud835\udc62. The actual prompt is illustrated in prompt II, Figure 2. However, there exists a distribution shift between original instruction tuning on the CDSR task and similar user integration CDSR task. We adopt the same KNN retrieval model on training users but choose from top-2 instead of top-1 (top-1 is the same user) to construct training prompts. Suppose the prompt is denoted as \ud835\udc43,\nAlgorithm 1 Overall pseudo code of URLLM\nInput: Product in two domain \ud835\udc4band \ud835\udc4c, suppose \ud835\udc4b\ud835\udc56\ud835\udc51< \ud835\udc4c\ud835\udc56\ud835\udc51. Train-\ning user set \ud835\udc48, user interaction sequence \ud835\udc46, user \ud835\udc62\nOutput: User\u2019s next possible item list \ud835\udc3c\n1: Train \ud835\udc40(\ud835\udc46\ud835\udc62,\ud835\udc4b,\ud835\udc4c) = \ud835\udc38\u2032\n\ud835\udc48, \ud835\udc38\u2032\ud835\udc62, \ud835\udc3c1\n\u22b2Prompt I is used here\n2: Retrieve \ud835\udc48\ud835\udc5f= \ud835\udc3e\ud835\udc41\ud835\udc41(\ud835\udc62,\ud835\udc48, \ud835\udc38), \ud835\udc48\ud835\udc61= \ud835\udc3e\ud835\udc41\ud835\udc41(\ud835\udc48,\ud835\udc48, \ud835\udc38)\n3: Train \ud835\udc3f\ud835\udc3f\ud835\udc40with \ud835\udc48\ud835\udc61\n\u22b2Prompt II is used here\n4: \ud835\udc3c2 = \ud835\udc3f\ud835\udc3f\ud835\udc40(\ud835\udc48\ud835\udc5f,\ud835\udc46\ud835\udc62)\n5: \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc3c= \ud835\udc5a\ud835\udc4e\ud835\udc65{\ud835\udc3c2[0 : \ud835\udc5a]},\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc3c= \ud835\udc5a\ud835\udc56\ud835\udc5b{\ud835\udc3c2[0 : \ud835\udc5a]}\n6: \ud835\udc3c= \ud835\udc3c2\n7: if \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc3c> \ud835\udc4b\ud835\udc56\ud835\udc51in recommend domain A then\n8:\n\ud835\udc3c= \ud835\udc3c1\n9: end if\n10: if \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc3c> \ud835\udc4c\ud835\udc56\ud835\udc51in recommend domain B then\n11:\n\ud835\udc3c= \ud835\udc3c1\n12: end if\n<div style=\"text-align: center;\">Table 1: The detailed description and statistics of datasets.</div>\nScenarios\n#Items\n#Train\n#Valid\n#Test\nAvg.length\nMovie\n71067\n35941\n1775\n3601\n4.095\nGame\n112233\n3.277\nArt\n18639\n16000\n1154\n2000\n6.386\nOffice\n19757\n8.263\n# the optimization of user-integration tuning loss is as follows:\n# the optimization of user-integration tuning loss is as follows:\nL\ud835\udc5f= \ud835\udc5a\ud835\udc4e\ud835\udc65\u03a6\ud835\udc3f \u2211\ufe01 \ud835\udc5d\u2208\ud835\udc43 |\ud835\udc5d| \u2211\ufe01 \ud835\udc61=1 \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc43\u03a6+\u03a6\ud835\udc3f(\ud835\udc5d\ud835\udc61|\ud835\udc5d<\ud835\udc61)).\n(15)\n\u03a6\ud835\udc3fis the LoRA parameters and we only update LoRA parameters during the training process. However, due to the uncontrollability of LLM outputs, even when prompting and training explicitly for the generation of products in domain \ud835\udc4b, there is still about 2% to 20% output of products from domain \ud835\udc4cduring actual inference. Therefore, a subsequent answer refinement module is required to further optimize the outputs of the LLM.\n4.2.3 Answer Refinement Structure. We first adopt a BM25 retrieval model to ground the space of recommendation language space to actual item space with \ud835\udc3c2. Subsequently, to stably identify whether LLM has influenced out-of-domain items, we consider the top-m grounding items. If one of the answers runs out of domain, we will consider adopting \ud835\udc3c1 from the dual graph sequence-modeling model. In our experiment, we set m=5 to gain an answer stably. The overall algorithm is delineated in algorithm 1. Finally, by combining a dual graph sequence-modeling model and a controllable answer selection model, we leverage the expansive reasoning and few-shot analogy capabilities of the LLM on integrating collaborative user information. This approach not only addresses the cold-start problem but also tackles the challenges of aligning cross-domain information in collaborative recommendation systems. To provide a clearer illustration of how different components of URLLM contribute to its performance, we present example cases in Figure 3.\n# 5 EXPERIMENTAL SETTINGS\n# 5.1 Datasets\nTo demonstrate the performance of our proposed model, we conduct experiments on two publicly available datasets from the Amazon platform [25]. This forms two distinct cross-domain scenarios: Movie-Game and Art-Office. The Movie-Game dataset, preprocessed by [2], is sparser and contains many cold-start users. The Art-Office dataset exhibits a more standardized distribution and is preprocessed by [20]. We further refine the latter by filtering users with fewer than 3 item interactions. Detailed descriptions and statistics for both datasets1 are provided in Table 1.\n<div style=\"text-align: center;\">Table 2: The overall performance of all baselines on the Movie-Game dataset.</div>\nBaseline Type\nMethod\nHR@1\nHR@5\nHR@10\nHR@20\nMRR\nNG@1\nNG@5\nNG@10\nNG@20\nTraditional\nBaselines\nLightGCN\n0.0033\n0.0064\n0.0099\n0.0147\n0.0054\n0.0033\n0.0063\n0.0087\n0.0105\nSASRec\n0.0013\n0.0038\n0.0049\n0.0075\n0.0029\n0.0013\n0.0025\n0.0029\n0.0035\nCoSeRec\n0.0036\n0.0112\n0.0174\n0.0251\n0.0078\n0.0036\n0.0081\n0.0090\n0.0116\n\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.0047\n0.0124\n0.0181\n0.0268\n0.0089\n0.0047\n0.0085\n0.0104\n0.0125\nLLM-based\nBaselines\nUniSRec\n0.0090\n0.0220\n0.0259\n0.0287\n0.0148\n0.0090\n0.0160\n0.0173\n0.0181\nBIGRec\n0.0067\n0.0194\n0.0305\n0.0444\n0.0168\n0.0067\n0.0148\n0.0183\n0.0233\nGPT4Rec\n0.0088\n0.0322\n0.0410\n0.0515\n0.0198\n0.0088\n0.0208\n0.0234\n0.0266\nCoLLM\n0.0101\n0.0202\n0.0354\n0.0455\n0.0171\n0.0101\n0.0146\n0.0193\n0.0219\nOur Work\nURLLM\n0.0105\n0.0333\n0.0416\n0.0522\n0.0211\n0.0105\n0.0221\n0.0248\n0.0298\n<div style=\"text-align: center;\">Table 3: The overall performance of all baselines on the Art-Office datas</div>\nBaseline Type\nMethod\nHR@1\nHR@5\nHR@10\nHR@20\nMRR\nNG@1\nNG@5\nNG@10\nNG@20\nTraditional\nBaselines\nLightGCN\n0.0105\n0.0225\n0.0255\n0.0325\n0.0198\n0.0104\n0.0133\n0.0163\n0.0184\nSASRec\n0.0055\n0.0120\n0.0175\n0.0235\n0.0100\n0.0055\n0.0069\n0.0084\n0.0108\nCoSeRec\n0.0099\n0.0217\n0.0284\n0.0357\n0.0154\n0.0099\n0.0159\n0.0181\n0.0199\n\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.0155\n0.0275\n0.0320\n0.0415\n0.0229\n0.0155\n0.0219\n0.0234\n0.0257\nLLM-based\nBaselines\nUniSRec\n0.0145\n0.0245\n0.0375\n0.0525\n0.0237\n0.0145\n0.0201\n0.0242\n0.0256\nBIGRec\n0.0220\n0.0315\n0.0370\n0.0480\n0.0277\n0.0220\n0.0268\n0.0286\n0.0313\nGPT4Rec\n0.0165\n0.0360\n0.0435\n0.0525\n0.0262\n0.0165\n0.0263\n0.0287\n0.0311\nCoLLM\n0.0170\n0.0347\n0.0438\n0.0493\n0.0286\n0.0170\n0.0263\n0.0295\n0.0309\nOur Work\nURLLM\n0.0270\n0.0400\n0.0485\n0.0595\n0.0355\n0.0270\n0.0353\n0.0371\n0.0399\n# 5.2 Baselines\nWe compare the performance of URLLM with state-of-the-art traditional and LLM-based CDSR methods to showcase its effectiveness. The baselines are as follows:\n\u2022 LightGCN [16] simplifies the design of Graph Convolution Networks for collaborative filtering by focusing on the essential component of neighborhood aggregation. \u2022 SASRec [26] uses a causal attention mechanism to model sequential patterns. \u2022 CoSeRec [37] introduces two new informative augmentation operators that leverage item correlations with contrastive sequence. \u2022 \ud835\udc6a2\ud835\udc6b\ud835\udc7a\ud835\udc79[7] captures user preferences by simultaneously leveraging intra- and inter-sequence item relationships throug contrasive item-item graphs.\n\u2022 LightGCN [16] simplifies the design of Graph Convolution Networks for collaborative filtering by focusing on the essential component of neighborhood aggregation. \u2022 SASRec [26] uses a causal attention mechanism to model sequential patterns. \u2022 CoSeRec [37] introduces two new informative augmentation operators that leverage item correlations with contrastive sequence. \u2022 \ud835\udc6a2\ud835\udc6b\ud835\udc7a\ud835\udc79[7] captures user preferences by simultaneously leveraging intra- and inter-sequence item relationships throug contrasive item-item graphs.\n# 5.2.2 LLM-based Baselines.\n\u2022 UniSRec [20] incorporates a lightweight item encoding architecture and employs contrastive pre-training tasks to learn transferable representations across domains. \u2022 GPT4Rec-LLaMA2 [32] uses BM25 grounding method to refine answer into actual item space. We replaced the GPT2 with LLaMA2 to ensure a fair comparison.\n1The processed dataset will be open once accepted.\n\u2022 BIGRec [3] grounds recommendation space of tuned LLM into real item space by incorporating statistical information. \u2022 CoLLM [65]2 captures collaborative information using an external traditional model and maps it into LLM by adapter.\n# 5.3 Evaluation Metrics\nFollowing previous works [26, 43, 66], we leverage the leave-oneout method to calculate the recommendation performance. Besides, we adopt the whole item set as the candidate item set during evaluation to avoid the sampling bias of the candidate selection [29]. Then, we evaluate the Top-K recommendation performance by Mean Reciprocal Rank (MRR) [48], Normalized Discounted Cumulative Gain (NDCG) [24] and Hit Rate (HR) [53].\n# 5.4 Implementation Details\nThe instruction-tuning and model inference are conducted on 4 Tesla A100 40G GPUs, which takes approximately 24 hours for training stage and 1 hour for inferencing. On our dual graph sequencemodeling model, we set \ud835\udc51\ud835\udc60= 128 and \ud835\udc51\ud835\udc4e= 32 according to their scale and sparsity. \ud835\udefeand \ud835\udf06are all set to 0.3 for balance. Across all generative LLMs, we finetune LLaMA2-7B-chat with LoRA[22] with LoRA-rank=8 and LoRA-alpha=16 using Adam[28] in a default learning rate of 1e-4. In the item attribute gaining part, in\n2We transformed the item rating model into an item recommendation model by providing 5000 candidates and ranking them based on scores.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6400/6400dead-1542-428b-907b-0175578cfdb8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance comparison is conducted in warm (left) and cold (right) scenarios for Movie-Game and Art-Office. \"UC\" denotes the substitution of the retrieval model of URLLM with \ud835\udc6a2\ud835\udc6b\ud835\udc7a\ud835\udc79, and \"w/o R\" designates URLLM without user retrieval.</div>\nMethod\nHR1\nHR5\nHR10\nHR20\nMRR\nNG1\nNG5\nNG10\nNG20\nw/o LLM\n0.0069\n0.0091\n0.0108\n0.0127\n0.0082\n0.0069\n0.0080\n0.0085\n0.0090\nw/o retrieve\n0.0088\n0.0322\n0.0410\n0.0515\n0.0197\n0.0088\n0.0208\n0.0234\n0.0266\nw/o graph alignment\n0.0102\n0.0331\n0.0414\n0.0522\n0.0210\n0.0102\n0.0208\n0.0243\n0.0268\nw/o answer refine\n0.0105\n0.0302\n0.0410\n0.0510\n0.0203\n0.0105\n0.0206\n0.0241\n0.0267\nw/o domain-specific retrieval\n0.0074\n0.0197\n0.0269\n0.0366\n0.0143\n0.0074\n0.0131\n0.0155\n0.0180\nURLLM-SASRec\n0.0099\n0.0308\n0.0409\n0.0505\n0.0201\n0.0099\n0.0208\n0.0244\n0.0266\nURLLM-\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.0089\n0.0313\n0.0402\n0.0489\n0.0190\n0.0089\n0.0201\n0.0229\n0.0251\nURLLM\n0.0105\n0.0333\n0.0416\n0.0522\n0.0211\n0.0105\n0.0221\n0.0248\n0.0298\norder to gain extra information, we use gpt-3.5-turbo-061 to gain introduction and attribute of the item.\n# 6 RESULTS AND ANALYSIS 6.1 Overall Performance\n# 6 RESULTS AND ANALYSIS\n# 6.1 Overall Performance\nIn this subsection, we compare the overall performance of all methods, which is presented in Table 2 and Table 3. We can draw the following conclusions from the results: 1). LLM-based CDSR methods generally surpass traditional methods, particularly in the sparser Movie-Game domain. This highlights the LLMs\u2019 ability to perform extensive reasoning and few-shot learning. 2). Discriminative LLMbased method UniSRec underperforms compared to other generative LLMs, suggesting emergent reasoning capabilities in generative models. 3). In the Art-Office domain, LLMs incorporating collaborative information (BIGRec, CoLLM) outperform methods without such information (GPT4Rec), underscoring its importance in LLMs. The opposite trend exists in the sparse domain Movie-Game, indicating that inappropriate information integration has a detrimental effect on performance. 4). URLLM consistently outperforms all baselines, demonstrating its effectiveness. Its superiority over LLM-based baselines confirms the value of seamless information integration and domain-specific generation.\n# 6.2 Analysis on warm/cold start scenario\nURLLM seeks to retrieve similar users into LLM, aiming to integrate additional knowledge tackling cold-start scenarios of CDSR.\nTo evaluate the achievement of this goal, we conduct a detailed examination of the methods\u2019 performance in both warm and cold-start scenarios. In particular, users are categorized into cold scenarios when the length of their domain-relevant interaction sequences is less than 3. Other users are categorized into warm scenarios. To showcase the efficacy of our model in cold-start situations, comparison among four models are compared in Figure 4. We can draw three pieces of information: 1). In comparison to the model devoid of the retrieval module (w/o R), URLLM exhibits an enhancement in cold-start scenarios across both datasets. This underscores the efficacy of the user-retrieval module in augmenting the model\u2019s performance. Notably, in cold-start scenarios on the Art-Office dataset, our model surpasses its warm-start performance (0.0364 versus 0.0355 in MRR). This suggests that URLLM has the potential to convert cold-start scenarios into warm-start ones. 2). We observe that, within the context of the movie-game dataset\u2019s warm-start setting, the performance of user retrieval based on \ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45is inferior to not performing any retrieval. This is attributed to\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\u2019s inability to model structural-semantic information, which consequently impairs its user retrieval capability. We hypothesize that this deficiency stems from the absence of semantic information modeling, introducing noise into LLMs, particularly under warm-start conditions. 3). URLLM\u2019s efficacy in cold-start scenarios is demonstrably superior compared to the traditional\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45model. On the Movie-Game dataset, URLLM achieves a remarkable 3.6-fold improvement, while on the Art-Office dataset, the performance gap widens to a factor of 13. These substantial gains highlight URLLM\u2019s effectiveness in\n<div style=\"text-align: center;\">Table 5: Ablation study on Art-Office dataset. URLLM-SASRec, URLLM-\ud835\udc6a2\ud835\udc6b\ud835\udc7a\ud835\udc79denotes the substitution of the retrieval model of URLLM with SASRec or \ud835\udc6a2\ud835\udc6b\ud835\udc7a\ud835\udc79, simultaneously represent URLLM w/o item graph or URLLM w/o attribute graph.</div>\nMethod\nHR1\nHR5\nHR10\nHR20\nMRR\nNG1\nNG5\nNG10\nNG20\nw/o LLM\n0.0265\n0.0345\n0.0430\n0.0505\n0.0322\n0.0265\n0.0308\n0.0336\n0.0355\nw/o retriever\n0.0165\n0.0360\n0.0435\n0.0525\n0.0262\n0.0165\n0.0263\n0.0287\n0.0311\nw/o graph alignment\n0.0235\n0.0415\n0.0475\n0.0530\n0.0322\n0.0235\n0.0324\n0.0344\n0.0357\nw/o answer refine\n0.0230\n0.0415\n0.0480\n0.0560\n0.0323\n0.0230\n0.0329\n0.0349\n0.0370\nw/o domain-specific retrieval\n0.0235\n0.0315\n0.0365\n0.0455\n0.0289\n0.0235\n0.0274\n0.0292\n0.0307\nURLLM-SASRec\n0.0175\n0.0350\n0.0420\n0.0490\n0.0263\n0.0175\n0.0267\n0.0290\n0.0307\nURLLM-\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.0215\n0.0375\n0.0455\n0.0550\n0.0302\n0.0215\n0.0309\n0.0328\n0.0349\nURLLM\n0.0270\n0.0430\n0.0485\n0.0595\n0.0355\n0.0270\n0.0353\n0.0371\n0.0399\n<div style=\"text-align: center;\">ble 6: Movie-Game CDSR performance replacing on user retrieval with candidates. The subscript model in the Method notes the application of the model for either user retrieval or the generation of top-k candidates.</div>\nTable 6: Movie-Game CDSR performance replacing on user retrieval with candidates. The subscrip denotes the application of the model for either user retrieval or the generation of top-k candidates.\nMethod\nUHR(10\u22123)\nHR@1\nHR@5\nHR@10\nHR@20\nMRR\nNG@1\nNG@5\nNG@10\nNG@20\n\ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.4700\n0.0083\n0.0277\n0.0369\n0.0450\n0.0180\n0.0083\n0.0183\n0.0213\n0.0232\n\ud835\udc45\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.3694\n0.0089\n0.0313\n0.0402\n0.0489\n0.0190\n0.0089\n0.0201\n0.0229\n0.0251\n\ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc48\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc40\n0.6350\n0.0089\n0.0275\n0.0378\n0.4580\n0.0184\n0.0089\n0.0185\n0.0219\n0.0240\n\ud835\udc45\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc48\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc40\n0.5651\n0.0105\n0.0333\n0.0416\n0.0522\n0.0211\n0.0105\n0.0221\n0.0248\n0.0298\naugmenting the model\u2019s capabilities for dealing with sparse data and limited user interaction.\n# 6.3 Ablation study\nTo demonstrate the effectiveness of each component, we conduct an ablation study to compare URLLM with seven variants: (1) w/o LLM: removing LLM to use only dual graph sequence modeling model to recommend; (2) w/o retrieve: removing retriever to adopt LLaMA2 itself; (3) w/o graph alignment: removing the alignment loss by setting \ud835\udefe= 0; (4) w/o answer refine: generating without domain-specific refine; (5) w/o domain-specific retrieval: retrieving users without domain differentiation strategy; (6) URLLM-SASRec: replacing dual graph sequence modeling model with SASRec; (7) URLLM-\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45: replacing dual graph sequence modeling model with \ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45. The results are presented in Table 4 and Table 5. The results demonstrate that each component within our framework plays a positive role in enhancing overall performance. Notably, for the Art-Office data, removing the retrieval and answer refinement module significantly decreased recommendation effectiveness. This highlights the critical role of collaborative information, employed in both retrieval and refining. Conversely, in the sparser Movie-Game dataset, the LLM and attribute graph exhibited the most significant contributions, suggesting that semantic information is paramount for dealing with cold-start scenarios. Furthermore, when we dissociate the domain-specific retrieval method or substitute the Dual Graph Sequence Modeling Model with SASRec and \ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\u2014which fail to generate user representation in specific domains\u2014the model\u2019s performance deteriorates due to the loss of crucial collaborative information and the integration of noise. The answer refinement model also affects the results of the model as shown in Table 4 and Table 5, which shows the importance of domain-specific generation.\n# 6.4 Analysis on integration of LLM\n6.4.1 Analysis on quality of user integration. In general, superiorperforming models tend to produce more precise user representations during training, which in turn leads to improved user retrieval results. Nevertheless, there appears to be an inconsistency between the model performance depicted in Table 2 and the retrieval outcomes presented in Table 4. While \ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45outperforms SASRec in terms of retrieval performance, its effectiveness diminishes when used as a replacement for the search model than SASRec. This observation prompts us to explore the impact of the retrieval result quality on recommendation performance. To quantitatively assess performance, we employ User Hit Rate (UHR) dividing the hit rate of the user interaction by the interaction length for retrieval quality and Mean Reciprocal Rank (MRR) for recommendation effectiveness. We further select a diverse subset of Art-Office examples, encompassing both higher-quality and lowerquality retrievals. Figure 5 illustrates the observed relationship between retrieval result quality and recommendation performance. Notably, this experiment is not conducted on the Movie-Game dataset due to its sparsity, which leads to unstable retrieval results and compromises the reliability of sampled outcomes. Figure 5 reveals that URLLM outperforms \ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45, suggesting its superior ability to model collaborative information. However, the close alignment of their curves indicates that model choice becomes less impactful. Notably, the positive correlation between MRR and UHR underscores the influence of retrieval quality on performance. Therefore, we can now address the initial question posed in this subsection: even though a stronger model leads to \ud835\udc3b\ud835\udc45\ud835\udc46\ud835\udc34\ud835\udc46\ud835\udc45\ud835\udc52\ud835\udc50= 0.0067 < \ud835\udc3b\ud835\udc45\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45= 0.0097, the length of retrieved user \ud835\udc59\ud835\udc46\ud835\udc34\ud835\udc46\ud835\udc45\ud835\udc52\ud835\udc50= 14.41 < \ud835\udc59\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45= 26.26 resulting in \ud835\udc48\ud835\udc3b\ud835\udc451/\ud835\udc48\ud835\udc3b\ud835\udc452 = 1.24 > 1, this causes a reduction of the \ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45retrieval model.\n<div style=\"text-align: center;\">Table 7: Art-Office CDSR performance replacing on user retrieval with candidates. The subscript model in the Method denotes the application of the model for either user retrieval or the generation of top-k candidates.</div>\nTable 7: Art-Office CDSR performance replacing on user retrieval with candidates. The subscript mode the application of the model for either user retrieval or the generation of top-k candidates.\nMethod\nUHR(10\u22123)\nHR@1\nHR@5\nHR@10\nHR@20\nMRR\nNG@1\nNG@5\nNG@10\nNG@20\n\ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n1.5992\n0.0185\n0.0325\n0.0370\n0.0440\n0.0257\n0.0185\n0.0260\n0.0274\n0.0292\n\ud835\udc45\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc362\ud835\udc37\ud835\udc46\ud835\udc45\n0.8783\n0.0215\n0.0375\n0.0455\n0.0550\n0.0302\n0.0215\n0.0309\n0.0328\n0.0349\n\ud835\udc36\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc48\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc40\n2.1511\n0.0200\n0.0340\n0.0495\n0.0455\n0.0274\n0.0200\n0.0277\n0.0295\n0.0310\n\ud835\udc45\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc48\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc40\n1.6876\n0.0270\n0.0430\n0.0485\n0.0595\n0.0355\n0.0270\n0.0353\n0.0371\n0.0399\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d897/d897c561-5eb6-49b9-b144-3b72304d8444.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: The positive correlation between quality of retrieved user and performance of model.</div>\n6.4.2 Analysis on other linguistic integration of user integration. We replaced user retrieval with candidate provision to explore more semantic integration approaches, ensuring that their lengths are equal. The model\u2019s performance using candidates can be observed in Table 6 and Table 7. It is noteworthy that, even though the UHR value for candidates is higher than our user retrieval under the same length, their overall final performance is inferior to our model. Additionally, in the experiments, we discovered some interesting phenomena \u2013 In a limited subset of instances even when the user retrieval UHR is 0, our model\u2019s performance still improves. This demonstrates the importance of collaborative information in user retrieval and validates the correctness of our approach.\n# 7 CONCLUSION\nIn conclusion, this paper introduced URLLM, a novel user retrieval CDSR framework to seamlessly incorporate diverse information into LLM. Initially, we developed a dual-graph sequence-modeling framework to capture collaborative and structural-semantic information derived from user interactions. Subsequently, we devised a novel user retrieval model for LLMs, aimed at infusing this knowledge into LLMs by exploiting their robust language reasoning and ensemble capabilities. Furthermore, the domain-specific retrieval strategy and answer refinement module were proposed for domainspecific information integration and generation. Compared to traditional approaches and other LLM-based methods, URLLM exhibited improved performance on the CDSA task. This research not only explored the relationship between provided user retrieval results and model performance but also propelled LLM-based CDSR toward the desired format. In further work, we\nwill attempt to retrench the retrieved user length and evaluations on larger-scale models.\n[1] Nawaf Alharbi and Doina Caragea. 2022. Cross-domain Self-attentive Sequential Recommendations. In Proceedings of International Conference on Data Science and Applications, Mukesh Saraswat, Sarbani Roy, Chandreyee Chowdhury, and Amir H. Gandomi (Eds.). Springer Singapore, Singapore, 601\u2013614. [2] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Chong Chen, Fuli Feng, and Qi Tian. 2023. A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems. arXiv:2308.08434 [cs.IR] [3] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnan He, and Qi Tian. 2023. A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems. CoRR abs/2308.08434 (2023). https://doi.org/10.48550/ARXIV.2308.08434 arXiv:2308.08434 [4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems (Singapore, Singapore) (RecSys \u201923). Association for Computing Machinery, New York, NY, USA, 1007\u20131014. https: //doi.org/10.1145/3604915.3608857 [5] Daniil A. Boiko, Robert MacKnight, and Gabe Gomes. 2023. Emergent autonomous scientific research capabilities of large language models. CoRR abs/2304.05332 (2023). https://doi.org/10.48550/ARXIV.2304.05332 arXiv:2304.05332 [6] Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Contrastive Cross-Domain Sequential Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM \u201922). Association for Computing Machinery, New York, NY, USA, 138\u2013147. https://doi.org/10.1145/3511808.3557262 [7] Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Contrastive Cross-Domain Sequential Recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022, Mohammad Al Hasan and Li Xiong (Eds.). ACM, 138\u2013147. https://doi.org/10.1145/3511808.3557262 [8] Junyi Chen. 2023. A Survey on Large Language Models for Personalized and Explainable Recommendations. CoRR abs/2311.12338 (2023). https://doi.org/10. 48550/ARXIV.2311.12338 arXiv:2311.12338 [9] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender Systems in the Era of Large Language Models (LLMs). CoRR abs/2307.02046 (2023). https://doi.org/10.48550/ARXIV. 2307.02046 arXiv:2307.02046 [10] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. CoRR abs/2303.14524 (2023). https://doi.org/10.48550/ ARXIV.2303.14524 arXiv:2303.14524 [11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association for Computing Machinery, New York, NY, USA, 299\u2013315. https://doi.org/10.1145/3523227.3546767 [12] Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model for Cold-Start Scenario. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>) (CIKM \u201923). Association for Computing Machinery, New York, NY, USA, 4595\u20134601. https://doi.org/10.1145/ 3583780.3614657 [13] Lei Guo, Li Tang, Tong Chen, Lei Zhu, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2021. DA-GCN: A Domain-aware Attentive Graph Convolution Network for Shared-account Cross-domain Sequential Recommendation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21,\nZhi-Hua Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization, 2483\u20132489. https://doi.org/10.24963/ijcai.2021/342 Main Track. [14] Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, and Enhong Chen. 2024. END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation. arXiv preprint arXiv:2403.17603 (2024). [15] Yongqiang Han, Likang Wu, Hao Wang, Guifeng Wang, Mengdi Zhang, Zhi Li, Defu Lian, and Enhong Chen. 2023. Guesr: A global unsupervised dataenhancement with bucket-cluster sampling for sequential recommendation. In International Conference on Database Systems for Advanced Applications. Springer, 286\u2013296. [16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 639\u2013648. https://doi.org/10.1145/3397271.3401063 [17] Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast Matrix Factorization for Online Recommendation with Implicit Feedback. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (Pisa, Italy) (SIGIR \u201916). Association for Computing Machinery, New York, NY, USA, 549\u2013558. https://doi.org/10.1145/ 2911451.2911489 [18] Bal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (Torino, Italy) (CIKM \u201918). Association for Computing Machinery, New York, NY, USA, 843\u2013852. https://doi.org/10.1145/3269206.3271761 [19] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.06939 [20] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and JiRong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, Aidong Zhang and Huzefa Rangwala (Eds.). ACM, 585\u2013593. https: //doi.org/10.1145/3534678.3539381 [21] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and JiRong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD \u201922). Association for Computing Machinery, New York, NY, USA, 585\u2013593. https: //doi.org/10.1145/3534678.3539381 [22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [23] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the planning of LLM agents: A survey. arXiv preprint arXiv:2402.02716 (2024). [24] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422\u2013446. https://doi.org/10. 1145/582415.582418 [25] Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, et al. 2023. Amazon-m2: A multilingual multi-locale shopping session dataset for recommendation and text generation. arXiv preprint arXiv:2307.09688 (2023). [26] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Recommendation. In IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018. IEEE Computer Society, 197\u2013206. https: //doi.org/10.1109/ICDM.2018.00035 [27] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recommendation. In 2018 IEEE International Conference on Data Mining (ICDM). 197\u2013206. https://doi.org/10.1109/ICDM.2018.00035 [28] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 [cs.LG] [29] Walid Krichene and Steffen Rendle. 2020. On sampled metrics for item recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 1748\u20131757. [30] Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. 2023. RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability. CoRR abs/2311.10947 (2023). https://doi.org/10.48550/ARXIV. 2311.10947 arXiv:2311.10947 [31] Chenglin Li, Mingjun Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guoqiang Shu, BeiBei Kong, and Di Niu. 2022. RecGURU: Adversarial Learning of Generalized User Representations for Cross-Domain Recommendation. In Proceedings\nof the Fifteenth ACM International Conference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM \u201922). Association for Computing Machinery, New York, NY, USA, 571\u2013581. https://doi.org/10.1145/3488560.3498388 [32] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation. In Proceedings of the 2023 SIGIR Workshop on eCommerce co-located with the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2023), Taipei, Taiwan, July 27, 2023 (CEUR Workshop Proceedings, Vol. 3589), Surya Kallumadi, Yubin Kim, Tracy Holloway King, Shervin Malmasi, Maarten de Rijke, and Jacopo Tagliabue (Eds.). CEUR-WS.org. https://ceur-ws.org/Vol-3589/paper_2.pdf [33] Junlong Li, Zhuosheng Zhang, and Hai Zhao. 2023. Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. arXiv:2212.08635 [cs.CL] [34] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. CTRL: Connect Tabular and Language Model for CTR Prediction. CoRR abs/2306.02841 (2023). https://doi.org/10.48550/ARXIV.2306.02841 arXiv:2306.02841 [35] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, and Xiang Wang. 2023. LLaRA: Aligning Large Language Models with Sequential Recommenders. CoRR abs/2312.02445 (2023). https://doi.org/10.48550/ARXIV.2312. 02445 arXiv:2312.02445 [36] Weiwen Liu, Wei Guo, Yong Liu, Ruiming Tang, and Hao Wang. 2023. User Behavior Modeling with Deep Learning for Recommendation: Recent Advances. In Proceedings of the 17th ACM Conference on Recommender Systems. 1286\u20131287. [37] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S. Yu, Julian J. McAuley, and Caiming Xiong. 2021. Contrastive Self-supervised Sequential Recommendation with Robust Augmentation. CoRR abs/2108.06479 (2021). arXiv:2108.06479 https: //arxiv.org/abs/2108.06479 [38] Muyang Ma, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Lifan Zhao, Peiyu Liu, Jun Ma, and Maarten de Rijke. 2022. Mixed Information Flow for Cross-Domain Sequential Recommendations. ACM Trans. Knowl. Discov. Data 16, 4 (2022), 64:1\u201364:32. https://doi.org/10.1145/3487331 [39] Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Jun Ma, and Maarten de Rijke. 2019. \ud835\udf0b-Net: A Parallel Information-sharing Network for Shared-account Crossdomain Sequential Recommendations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Paris, France) (SIGIR\u201919). Association for Computing Machinery, New York, NY, USA, 685\u2013694. https://doi.org/10.1145/3331184.3331200 [40] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389. https://doi.org/10.1561/1500000019 [41] Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar, and David M. Pennock. 2002. Methods and metrics for cold-start recommendations. In SIGIR 2002: Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, August 11-15, 2002, Tampere, Finland, Kalervo J\u00e4rvelin, Micheline Beaulieu, Ricardo A. Baeza-Yates, and Sung-Hyon Myaeng (Eds.). ACM, 253\u2013260. https://doi.org/10.1145/564376.564421 [42] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (Beijing, China) (CIKM \u201919). Association for Computing Machinery, New York, NY, USA, 1441\u20131450. https://doi.org/10.1145/3357384.3357895 [43] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019, Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A. Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu (Eds.). ACM, 1441\u20131450. https://doi.org/10.1145/3357384.3357895 [44] Wenchao Sun, Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten de Rijke. 2023. Parallel Split-Join Networks for Shared Account Cross-Domain Sequential Recommendations. IEEE Transactions on Knowledge and Data Engineering 35, 4 (2023), 4106\u20134123. https://doi.org/10.1109/ TKDE.2021.3130927 [45] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 14918\u201314937. https://doi.org/10.18653/v1/2023.emnlp-main.923 [46] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (Marina Del Rey, CA, USA) (WSDM \u201918). Association for Computing Machinery, New York, NY, USA, 565\u2013573. https://doi.org/10.1145/3159652.3159656 [47] Zuoli Tang, Zhaoxin Huan, Zihao Li, Xiaolu Zhang, Jun Hu, Chilin Fu, Jun Zhou, and Chenliang Li. 2023. One Model for All: Large Language Models are Domain-Agnostic Recommendation Systems. CoRR abs/2310.14304 (2023).\nhttps://doi.org/10.48550/ARXIV.2310.14304 arXiv:2310.14304 [48] Ellen M. Voorhees. 1999. The TREC-8 Question Answering Track Report. In Proceedings of The Eighth Text REtrieval Conference, TREC 1999, Gaithersburg, Maryland, USA, November 17-19, 1999 (NIST Special Publication, Vol. 500-246), Ellen M. Voorhees and Donna K. Harman (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec8/papers/qa_report.pdf [49] Hao Wang, Defu Lian, Hanghang Tong, Qi Liu, Zhenya Huang, and Enhong Chen. 2021. Hypersorec: Exploiting hyperbolic user and item representations with multiple aspects for social-aware recommendation. ACM Transactions on Information Systems (TOIS) 40, 2 (2021), 1\u201328. [50] Hao Wang, Tong Xu, Qi Liu, Defu Lian, Enhong Chen, Dongfang Du, Han Wu, and Wen Su. 2019. MCNE: An end-to-end framework for learning multiple conditional network representations of social network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 1064\u20131072. [51] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. 2023. RecMind: Large Language Model Powered Agent For Recommendation. CoRR abs/2308.14296 (2023). https://doi.org/10.48550/ARXIV.2308.14296 arXiv:2308.14296 [52] Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui Qiu. 2020. Global Context Enhanced Graph Neural Networks for Session-based Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 169\u2013178. https://doi.org/10.1145/3397271.3401142 [53] SJ Waters. 1976. Hit ratios. Comput. J. 19, 1 (1976), 21\u201324. [54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html [55] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023). [56] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation. In 2022 IEEE 38th International Conference on Data Engineering (ICDE). 1259\u20131273. https://doi.org/10.1109/ICDE53745.2022.00099 [57] Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and Xiangnan He. 2023. Large Language Model Can Interpret Latent Space of Sequential Recommender. CoRR abs/2310.20487 (2023). https: //doi.org/10.48550/ARXIV.2310.20487 arXiv:2310.20487 [58] Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Zhi Li, Sirui Zhao, Defu Lian, and Enhong Chen. 2024. Learning Partially Aligned Item Representation for CrossDomain Sequential Recommendation. arXiv preprint arXiv:2405.12473 (2024). [59] Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen. 2024. Dataset Regeneration for Sequential Recommendation. arXiv preprint arXiv:2405.17795 (2024). [60] Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. 2023. APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 3009\u20133019. [61] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? IDvs. Modality-based Recommender Models Revisited. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.). ACM, 2639\u20132649. https://doi.org/10.1145/3539618.3591932 [62] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2023. On Generative Agents in Recommendation. arXiv:2310.10108 [cs.IR] [63] Luankang Zhang, Hao Wang, Suojuan Zhang, Mingjia Yin, Yongqiang Han, Jiaqing Zhang, Defu Lian, and Enhong Chen. 2024. A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation. arXiv preprint arXiv:2404.00268 (2024). [64] Yuren Zhang, Enhong Chen, Binbin Jin, Hao Wang, Min Hou, Wei Huang, and Runlong Yu. 2022. Clustering based behavior sampling with long sequential data for CTR prediction. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2195\u20132200. [65] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023. CoLLM: Integr",
    "paper_type": "method",
    "attri": {
        "background": "Cross-Domain Sequential Recommendation (CDSR) aims to mine and transfer users\u2019 sequential preferences across different domains to alleviate the long-standing cold-start issue. Traditional CDSR models capture collaborative information through user and item modeling while overlooking valuable semantic information. Recently, Large Language Model (LLM) has demonstrated powerful semantic reasoning capabilities, motivating us to introduce them to better capture semantic information.",
        "problem": {
            "definition": "The problem addressed in this paper is the challenge of performing personalized recommendations for users with few interaction records, particularly in the context of Cross-Domain Sequential Recommendation (CDSR).",
            "key obstacle": "The main challenge is the seamless integration of diverse information formats, including collaborative and semantic information, and ensuring domain-specific generation to avoid out-of-domain content."
        },
        "idea": {
            "intuition": "The idea is inspired by the emergent capabilities of Large Language Models (LLMs) and their potential to enhance recommendation systems by capturing semantic information.",
            "opinion": "The proposed idea involves a novel framework named URLLM that integrates User Retrieval and domain grounding on LLMs to improve CDSR performance.",
            "innovation": "The key innovation lies in the seamless integration of structural-semantic and collaborative information into LLMs, along with a domain differentiation strategy that ensures domain-specific generation."
        },
        "method": {
            "method name": "URLLM",
            "method abbreviation": "URLLM",
            "method definition": "URLLM is a framework designed to enhance Cross-Domain Sequential Recommendation by integrating user retrieval and domain-specific generation using Large Language Models.",
            "method description": "The core of URLLM combines a dual-graph sequential model with a user retrieval-generation model to capture and utilize diverse information effectively.",
            "method steps": [
                "Construct a dual-graph sequence model to capture collaborative and structural-semantic information.",
                "Implement a user retrieval model to fetch relevant user information.",
                "Integrate the retrieved user information into the LLM for generating recommendations.",
                "Apply a domain differentiation strategy to ensure outputs remain within the specified domain."
            ],
            "principle": "The effectiveness of URLLM in solving the problem stems from its ability to leverage the reasoning and inferencing capabilities of LLMs while ensuring that the information is relevant and domain-specific."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on two datasets from Amazon, focusing on Movie-Game and Art-Office scenarios, with comparisons against state-of-the-art baselines.",
            "evaluation method": "The performance of URLLM was assessed using metrics such as Hit Rate (HR), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG)."
        },
        "conclusion": "The results demonstrate that URLLM significantly outperforms traditional approaches and other LLM-based methods, confirming its effectiveness in integrating diverse information for Cross-Domain Sequential Recommendation.",
        "discussion": {
            "advantage": "The primary advantage of URLLM is its ability to seamlessly integrate collaborative and semantic information, leading to improved recommendation performance, especially in cold-start scenarios.",
            "limitation": "A limitation of the method is the uncontrollable nature of LLMs, which can sometimes produce out-of-domain generations despite domain-specific training.",
            "future work": "Future research will focus on optimizing the length of retrieved users and evaluating URLLM on larger-scale models to further enhance its capabilities."
        },
        "other info": {
            "code repository": "https://github.com/TingJShen/URLLM"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommendation algorithms are crucial in addressing the cold-start issue by mining and transferring users\u2019 sequential preferences across different domains."
        },
        {
            "section number": "1.2",
            "key information": "The paper highlights the role of Large Language Models (LLMs) in enhancing recommendation systems by capturing valuable semantic information."
        },
        {
            "section number": "2.1",
            "key information": "Cross-Domain Sequential Recommendation (CDSR) is defined as the challenge of performing personalized recommendations for users with few interaction records."
        },
        {
            "section number": "2.3",
            "key information": "The paper introduces the URLLM framework, which integrates user retrieval and domain-specific generation using LLMs to improve CDSR performance."
        },
        {
            "section number": "3.2",
            "key information": "The integration of structural-semantic and collaborative information into LLMs represents a significant advancement in AI-driven approaches to recommendations."
        },
        {
            "section number": "4.1",
            "key information": "URLLM leverages the reasoning and inferencing capabilities of LLMs to enhance recommendation systems."
        },
        {
            "section number": "4.2",
            "key information": "The integration of LLMs in the URLLM framework improves personalization and user interaction in Cross-Domain Sequential Recommendation."
        },
        {
            "section number": "6.1",
            "key information": "The URLLM framework captures collaborative information through a dual-graph sequential model, enhancing traditional collaborative filtering methods."
        },
        {
            "section number": "8.1",
            "key information": "The paper discusses how collaborative information is incorporated into LLMs to improve recommendations, addressing the integration of diverse information formats."
        },
        {
            "section number": "10.1",
            "key information": "The paper identifies the challenge of integrating diverse information formats, including collaborative and semantic information, as a current obstacle in recommendation systems."
        }
    ],
    "similarity_score": 0.7337337120939107,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation.json"
}