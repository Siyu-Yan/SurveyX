{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.08590",
    "title": "Neural Machine Translation Models Can Learn to be Few-shot Learners",
    "abstract": "The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant fewshot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",
    "bib_name": "reinauer2023neuralmachinetranslationmodels",
    "md_text": "# Raphael Reinauer\u2020 and Patrick Simianer\u2020 and Kaden Uhlig and Johannes E. M. Mosig and Joern Wuebker\nLilt\n{raphael.reinauer,patrick}@lilt.com\n# Abstract\nThe emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant fewshot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.\narXiv:2309.08590v1\n# 1 Introduction\nLarge Language Models (LLMs) have demonstrated few-shot learning capabilities on various natural language processing tasks, as highlighted by Brown et al. (2020) or Garcia et al. (2023). When prompted with suitable example translations, they can compete with neural machine translation (NMT) models, built and trained specifically for translating between languages (Vilar et al., 2023). Interestingly, one can adapt LLMs to specific domains merely by adding example translations to their prompt at inference time (Moslem et al., 2023). This ability to adapt to specific tasks and domains is known as in-context learning (ICL). In contrast to traditional fine-tuning methods, ICL does not require a separate set of customized parameters for each domain, which implies major efficiency gains through batched inference.\n\u2020 Equal contribution.\nIn this paper, we integrate ICL for domain adaptation into NMT systems in multiple steps. We compare our method for adapting NMT systems to traditional fine-tuning approaches, as well as to the domain adaptation abilities of an open-source LLM. Specifically, our main contributions are the following: 1. We evaluate an unmodified NMT system\u2019s ICL capacity for domain adaptation and demonstrate its limitations. 2. We propose a training scheme to improve an NMT model\u2019s ICL capability. 3. We show that ICL can be combined with more traditional adaptation methods to further improve domain adaptation performance. 4. We compare our method to the performance of the open-source LLM FALCON-40B (Penedo et al., 2023) on a machine translation task with ICL for domain adaptation.\nIn this paper, we integrate ICL for domain adaptation into NMT systems in multiple steps. We compare our method for adapting NMT systems to traditional fine-tuning approaches, as well as to the domain adaptation abilities of an open-source LLM. Specifically, our main contributions are the following:\n3. We show that ICL can be combined with more traditional adaptation methods to further improve domain adaptation performance.\n4. We compare our method to the performance of the open-source LLM FALCON-40B (Penedo et al., 2023) on a machine translation task with ICL for domain adaptation.\n# 2 Related Work\nBulte and Tezcan (2019) improve the translation performance of an NMT model by integrating translation fuzzy-matched pairs from a translation memory as input to an NMT model. This idea was further expanded by Pham et al. (2020) and Xu et al. (2020), who for a given source segment use sentence embeddings to retrieve similar examples and compared different schemes for integrating those as cues into the NMT network. Our approach differs in that we only train on the tokens belonging to the translation and not on the tokens provided as context, which we show to work better. In addition, Pham et al. (2020)\u2019s training procedure differs, as they train their model from scratch, using training data from multiple domains and evaluate on those same domains, while we train on general domain data and evaluate on a new domain that is not in the training data. Furthermore,\nwe focus on the multi-domain adaptation task using light-weight adapters. This approach not only allows us to extend to new domains without retraining the full model, but also offers a more practical and efficient strategy for real-world applications. The authors of (Moslem et al., 2023) investigated the capabilities of a proprietary LLM, specifically GPT-3.5, for adaptive machine translation using ICL. Their extensive experiments showed that GPT-3.5 can adapt well to in-domain sentence pairs and/or terminology.\n# 3 Experiments\nWe conduct a series of experiments to develop NMT systems that exceed at few-shot ICL domain adaptation. Here we present the experiments in a logical order, where we start with the baseline models described in Section 3.1 and subsequently introduce several stages of development. In stages 0 and 1 we attempt ICL with the unmodified and domain-fine-tuned baseline models, respectively. Then, in STAGE 2, we fine-tune the baseline model to the task of domain ICL, instead of a particular domain. Finally, we combine ICL and domain adaptation through fine-tuning in STAGE 3. Our experimental progression was guided by the metrics and datasets that we introduce in Sections 3.5 and 3.6, respectively.\n# 3.1 Models\nThroughout this paper, we work with an NMT system and the FALCON-40B LLM, which we both describe here.\n# 3.1.1 FALCON LLM\nTo provide a direct comparison with LLMs and their capacity for ICL, we conduct experiments with the decoder-only Transformer language model FALCON-40B (Penedo et al., 2023), specifically the non-instruction-tuned variant1. Inference is done with greedy decoding. Following previous work (Bawden and Yvon, 2023; Garcia et al., 2023; Hendy et al., 2023) (inter-alia) the model is prompted to perform translation without specific fine-tuning towards the machine translation task. A simple prompt template is used for all k-shot experiments with FALCON-40B, see Figure 1. In preliminary experiments we found that k = 0\n1The model is available from the huggingface platform: https://huggingface.co/tiiuae/falcon-40b\nEnglish: <source sentence>\\n German: <target sentence>\\n English: [...]\nFigure 1: Prompt template for LLM.\ndoes not work well with this specific model2 \u2013 the outputs tend to be entirely hallucinated.\n# 3.1.2 NMT Systems\nThe baseline model that we use as the starting point for all further experiments is a Transformer (Vaswani et al., 2017) model with 12 encoder layers and two decoder layers, implemented with the NVIDIA NeMo toolkit (Kuchaiev et al., 2019). The embedding size is 1,024 with a feed-forward network dimension of 4,096. The model has a joint vocabulary of 32,768 tokens, while embedding matrices are specific to the encoder, decoder, and output projection modules, i.e. parameters are not shared between them. The model was trained to support a maximum input size of 1,536 tokens by augmenting the training data with random concatenations of parallel sentences. We evaluate the model using greedy decoding. For the experiments presented here, the baseline model is either fine-tuned in full (STAGE 2A and STAGE 2B), or light-weight adapters (Bapna and Firat, 2019) are added to the model (STAGE 1 and STAGE 3). We choose full-model fine-tuning on out-of-domain data to adapt the NMT model to a new task \u2013 translating with an increased context of related examples \u2013 and adapter layers for learning from in-domain data. The adapters we use follow Bapna et al. (2019)\u2019s formulation, but with layer normalization applied after the bottleneck rather than before it. We use a bottleneck width of 256 and insert adapters in every layer of the decoder and every other layer of the encoder. We always fine-tune with the ADAM optimizer (Kingma and Ba, 2014) and early stopping based on validation loss.\n# 3.2 STAGE 0 & STAGE 1: ICL with a Standard NMT Model\nMotivated by the few-shot learning capabilities of LLMs, we examine the ability of a standard English-to-German NMT model to adapt to a domain given only similar and relevant translation\n2For k = 0 the prompt contains only the single source sentence as input and the target language followed by a colon.\npairs as additional context, i.e., without changing the model\u2019s parameters. To find similar source segments in the translation memory, we search for nearest neighbours in an embedding space. We use the multi-lingual sentence embedding model3 from the sentence transformer library (Reimers and Gurevych, 2020) to embed the source sides of all segment pairs. Then we employ hnswlib (Malkov and Yashunin, 2020) to find the approximate nearest neighbours: Each source sentence in the domain-specific datasets is first encoded with the sentence-embedding model and then added to an index. For the sake of simplicity in this paper, we will refer to the approximate nearest neighbors simply as nearest neighbors. To measure the similarity between a pair of segments s and s\u2032, we use the cosine distance of the corresponding embedding vectors vs and vs\u2032, i.e.,\nFor a given source s and target segment t, we identify its nearest neighbours s1, s2, ..., sk, using the the cosine distance above. Each source sentence si is paired with a reference translation ti for i = 1, ..., k. We sort the pairs by their distance to s in the embedding space, i.e.,\nOur assumption is that similar segments should have similar translations. For STAGE 0 of the experiments, we treat the context sentences and actual source text as one body of text, separated only by a single space, ordering the segments from least similar to most similar, with the current source segment s at the end. As a result, the input of the encoder is\nwhile for the decoder, we use the prefix:\n# while for the decoder, we use the prefix:\n<bos> tk tk\u22121 ... t1\nwhere <bos> and <eos> represent the beginning-ofsentence and end-of-sentence tokens, respectively. The model\u2019s task is then to continue from the target prefix by generating a translation of the source segment s. In our experiments, we evaluated the translation performance using a varying number k of nearest neighbors, specifically k \u2208{1, 2, 5}.\n3Model name on https://www.sbert.net/: all-MiniLM-L6-v2\nIn STAGE 1 we run additional experiments where we fine-tune the model for each domain, using the in-domain training data in the original format. This domain-specific fine-tuning is performed by injecting adapter layers (Bapna and Firat, 2019) into the network while freezing the rest of the model, and leveraging a standard negative log-likelihood (NLL) loss for training. For each domain, we then test the fine-tuned model directly (0-shot in Tables 3 and 4) as well as with ICL (k-shot with k \u0338= 0). Adapters are trained towards convergence, i.e. until there is no further improvement in terms of validation loss.\n# 3.3 STAGE 2A & STAGE 2B: Fine-Tuning towards ICL\nTo improve the model\u2019s capability to use nearest neighbor examples in the context, we further finetune the full model on out-of-domain data, namely News-Commentary4 (Kocmi et al., 2022), which contains roughly 450K parallel segments. For validation we use a sample of 2K parallel segments from EuroParl5 (Koehn, 2005). For this full model fine-tuning we do not train until convergence, but apply aggressive early stopping: Training is stopped when the validation loss does not decrease by at least 0.1 twice in a row, validating for every 1% of an epoch. This is to encourage the model to only learn the new task and data format, but not adapt to a new data distribution. Instead of directly concatenating the nearest neighbors to the training examples, we add a special separation token \u2013 <sep> \u2013 to separate the source and target segments. We then construct the training instances for the encoder as:\n<bos> sk <sep> sk\u22121 <sep> ... <sep> s1 <sep> s <eos>\nand for the decoder as:\n<bos> tk <sep> tk\u22121 <sep> ... <sep> t1 <sep> t <eos (1)\n(1)\nand compute the NLL loss on all tokens of (1). This training loss is identical to the one used in Pham et al. (2020). We denote this procedure as STAGE 2A. For STAGE 2B the idea is that the model should learn to predict the target segment from the source\n4From the WMT\u201923 evaluation campaign: https://data. statmt.org/news-commentary/v18.1/ 5Also from the WMT\u201923 evaluation campaign: https: //www.statmt.org/europarl/v10/\nsegment using the nearest neighbor translations but not learn to predict tk, ..., t1 as in (Pham et al., 2020). Hence we mask the NLL training loss such that it is computed only on the tokens that belong to the target segment t, excluding all context tokens, thus fully focusing the training signal on translating t in the context of its k nearest neighbors. We then use the same format as in STAGE 2A for training, while at inference time we provide the decoder with a prefix containing the ICL examples: <bos> tk <sep> tk\u22121 <sep> ... <sep> t1 <sep> Finally, we measure quality of the predicted translation \u02c6t by computing BLEU and COMET scores with the target segment t as reference. For both STAGE 2A and STAGE 2B, the k-nearest neighbors for each segment in the training data and validation data are extracted from the entire NewsCommentary dataset as described in Section 3.2.\n# 3.4 STAGE 3: Combining ICL and Domain Adaptation\n# 3.4 STAGE 3: Combining ICL and Domain Adaptation\n# 3.4 STAGE 3: Combining ICL and Domain\nTo combine STAGE 2B\u2019s ICL capacity with adapterbased domain adaptation, we add adapters to the model from STAGE 2B using the same configuration as for the STAGE 1 experiments. Again, we train separate adapter layers for each domain. Each example from the training set is annotated with its nearest neighbors from the same training set, excluding itself.\n# 3.5 Metrics\nFor evaluating translation quality, we used the SacreBLEU framework (Post, 2018) that implements the BLEU metric (Papineni et al., 2002). We also evaluate with reference-based COMET (Rei et al., 2022) to compare the model outputs to the reference translations in the test data.\n# 3.6 Datasets\nWe run our experiments with the English-German language pair on 8 domains from the ACED- and MDNS corpus collections, which we describe in this section. Statistics for all datasets are provided in Table 1.\n# 3.6.1 ACED corpus\nThe ACED corpus (Lin et al., 2022) is comprised of three distinct datasets, namely Asics, Emerson, and Digitalocean, each consisting of English-German sentences extracted from various domains. ACED is a real-world benchmark containing data derived from translations performed by humans.\nTraining\nValidation\nTest\nAsics\n1.4\n0.5\n0.6\nDigitalocean\n11.8\n2.0\n7.6\nEmerson\n4.3\n1.3\n1.7\nIT\n223\n2.0\n2.0\nKoran\n17.9\n2.0\n2.0\nLaw\n467\n2.0\n2.0\nMedical\n248\n2.0\n2.0\nSubtitles\n500\n2.0\n2.0\n<div style=\"text-align: center;\">Table 1: Segment counts for the domain-specific dataset splits used for experimentation, in thousands.</div>\nTable 1: Segment counts for the domain-specific dataset splits used for experimentation, in thousands.\n# 3.6.2 MDNS corpus\nThe MDNS corpus (Aharoni and Goldberg, 2020) is a multi-domain corpus containing EnglishGerman parallel text from five diverse domains (IT, Koran, Law, Medical, Subtitles). It was specifically created for evaluating domain-adaptation.\n# 4 Results\nHere we discuss the experimental results, progressing from STAGE 0 to STAGE 3. All results are depicted separately for ACED- and MDNS corpora in Tables 3 and 4 respectively.\nHere we discuss the experimental results, progressing from STAGE 0 to STAGE 3. All results are depicted separately for ACED- and MDNS corpora in Tables 3 and 4 respectively.\n# 4.1 STAGE 0: ICL with Baseline NMT Model\nWhen we add nearest neighbors to the inputs and target prefixes we first observe that the automated metrics are mostly improved across all datasets. Notably, the result with 1-shot nearest neighbors is the best in this group of experiments. Additionally we find that the 5-shot result often degrades below the baseline. Specifically for the Medical and Subtitles corpora of MDNS, we find that the model fails to improve over the baseline for all k. The cosine distance of the nearest neighbors seems to be a viable indicator of performance in this set of experiments, e.g. when comparing the results for ACED Emerson & Digitalocean, where the average cosine distance (see Table 2) for k = 1 is much lower for Emerson at 0.13, versus 0.3 for Digitalocean. We find a moderate, statistically insignificant, negative Pearson correlation (r = \u22120.43) between the average cosine distances for k = 1 and the difference in BLEU scores between the STAGE 0 1-shot experiment and the baseline.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ff5/2ff53d55-2a47-4465-9a6f-f382318358c3.png\" style=\"width: 50%;\"></div>\nTable 2: Average cosine distance in embedding space of test set sources to k-nearest neighb k \u2208{1, 2, 5}.\nAsics\nDigitalocean\nEmerson\nAverage\nBLEU COMET BLEU COMET BLEU COMET BLEU COMET\nBaseline 34.5\n0.8624\n53.3\n0.9043\n44.9\n0.9108\n44.2\n0.8925\nSTAGE 0\n1-shot\n43.7\n0.8578\n54.4\n0.8982\n72.1\n0.9213\n56.7\n0.8924\n2-shot\n44.5\n0.8525\n54.5\n0.8967\n67.2\n0.9137\n55.4\n0.8876\n5-shot\n41.0\n0.8420\n53.9\n0.8955\n28.7\n0.8705\n41.2\n0.8693\nSTAGE 1\n0-shot\n41.2\n0.8780\n60.1\n0.9152\n79.2\n0.944\n60.2\n0.9124\n1-shot\n46.4\n0.8657\n59.6\n0.9099\n78.1\n0.9378\n61.4\n0.9045\n2-shot\n46.2\n0.8628\n59.0\n0.9090\n66.3\n0.9275\n57.2\n0.8998\n5-shot\n44.2\n0.8500\n57.3\n0.9038\n32.2\n0.893\n44.6\n0.8823\nSTAGE 2A\n1-shot\n43.0\n0.8765\n55.0\n0.9073\n73.1\n0.9382\n57.0\n0.9073\n2-shot\n43.5\n0.8785\n54.4\n0.9072\n71.6\n0.9392\n56.5\n0.9083\n5-shot\n42.3\n0.8662\n54.4\n0.9066\n73.4\n0.9347\n56.7\n0.9025\nSTAGE 2B\n1-shot\n44.5\n0.8766\n54.9\n0.9046\n73.1\n0.9391\n57.5\n0.9068\n2-shot\n44.5\n0.8777\n55.4\n0.9080\n74.3\n0.939\n58.1\n0.9082\n5-shot\n44.7\n0.8734\n55.0\n0.9072\n70.0\n0.9363\n56.6\n0.9056\nSTAGE 3\n1-shot\n48.8\n0.8896\n60.5\n0.9141\n78.9\n0.9480\n62.7\n0.9172\n2-shot\n48.5\n0.8914\n60.1\n0.9132\n80.7\n0.9456\n63.1\n0.9167\n5-shot\n47.6\n0.8837\n59.0\n0.9095\n80.2\n0.9437\n62.3\n0.9123\nFalcon\n1-shot\n31.8\n0.8588\n40.0\n0.8677\n71.6\n0.9380\n47.8\n0.8882\n2-shot\n34.5\n0.8671\n44.8\n0.8876\n76.9\n0.9416\n52.1\n0.8988\n5-shot\n40.8\n0.8789\nX\nX\n78.5\n0.9434\nX\nX\nTable 3: Results for the ACED corpus of the multi-stage evaluation for various numbers of k-nearest-neighbors using BLEU and COMET metrics. The \"Baseline\" scores are for the English-to-German NMT system described in Section 3.1. We omit the Digitalocean dataset for the FALCON-40B 5-shot evaluation.\nWhile BLEU indicates improvement (COMET reduces only for k > 1), we find that the model\u2019s behavior is in fact degenerate. Specifically, the model often fails to produce any output after the given prefix and instead predicts <eos> immediately, which leads to empty translations. We find that the rates of empty translations are 8.5%, 8.1%, and 9.1% for k = 1, 2, and 5 respectively. In contrast, the baseline system has a 0% rate of empty outputs. This is despite the model being specifically trained to support inputs covering the full context-width in pre-training.\n# 4.2 STAGE 1: Combining ICL with Domain Fine-Tuning\nFor STAGE 1 we first observe that the model can be effectively adapted to each domain by training adapters (see the STAGE 1, 0-shot results in Tables 3 and 4). A notable exception is MDNS Subtitles, where adaptation only slightly improves over the baseline. This result is, however, consistent with other work (Aharoni and Goldberg, 2020). When we combine the trained adapters with ICL, we find no improvements over STAGE 1\u2019s 0-shot results, with the exception of ACED Asics. Performance drops catastrophically for the MDNS Medical & Subtitles corpora. The rate\nIT\nKoran\nLaw\nMedical\nSubtitles\nAverage\nBLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\nBaseline 34.3\n0.8153\n14.7\n0.7229\n44.7\n0.8696\n43.5\n0.8406\n27.7\n0.7891\n33.0\n0.8075\nSTAGE 0\n1-shot\n35.9\n0.7698\n17.2\n0.6580\n51.6\n0.853\n42.3\n0.7964\n17.5\n0.6358\n32.9\n0.7426\n2-shot\n35.9\n0.7433\n17.2\n0.6346\n49.9\n0.8467\n38.2\n0.7810\n22.4\n0.7024\n32.7\n0.7416\n5-shot\n31.9\n0.7196\n14.5\n0.6000\n42.3\n0.8287\n30.5\n0.7505\n24.4\n0.7400\n28.7\n0.7278\nSTAGE 1\n0-shot\n39.6\n0.8403\n22.6\n0.7274\n50.7\n0.8824\n47.8\n0.8429\n28.1\n0.7879\n37.8\n0.8162\n1-shot\n36.7\n0.7620\n21.1\n0.6434\n51.1\n0.8228\n7.1\n0.5078\n0.0\n0.4306\n23.2\n0.6333\n2-shot\n35.6\n0.7436\n20.5\n0.6152\n48.9\n0.8019\n15.9\n0.5441\n0.0\n0.4208\n24.2\n0.6251\n5-shot\n32.8\n0.7296\n18.4\n0.5980\n44.9\n0.7940\n23.4\n0.5854\n16.8\n0.6388\n27.3\n0.6692\nSTAGE 2A\n1-shot\n34.3\n0.8277\n15.5\n0.7222\n49.5\n0.8739\n43.6\n0.8380\n25.7\n0.7838\n33.7\n0.8091\n2-shot\n35.8\n0.8244\n16.4\n0.7154\n49.6\n0.8739\n44.6\n0.8362\n24.1\n0.7810\n34.1\n0.8062\n5-shot\n34.3\n0.8203\n15.9\n0.7083\n48.1\n0.8659\n40.7\n0.8220\n24.0\n0.7712\n32.6\n0.7975\nSTAGE 2B\n1-shot\n34.6\n0.8269\n16.0\n0.7217\n50.4\n0.8752\n44.2\n0.8405\n25.9\n0.7830\n34.2\n0.8095\n2-shot\n35.5\n0.8182\n16.5\n0.7150\n49.9\n0.8747\n43.4\n0.8349\n24.5\n0.7774\n34.0\n0.8040\n5-shot\n33.5\n0.8103\n16.6\n0.7070\n48.2\n0.8696\n37.5\n0.8274\n25.2\n0.7782\n32.2\n0.7985\nSTAGE 3\n1-shot\n41.4\n0.8423\n28.8\n0.7235\n58.1\n0.8862\n52.9\n0.8488\n27.0\n0.7846\n41.6\n0.8171\n2-shot\n41.7\n0.8401\n29.6\n0.7225\n57.3\n0.8850\n51.2\n0.8480\n27.6\n0.7850\n41.5\n0.8161\n5-shot\n40.9\n0.8296\n29.2\n0.7249\n55.8\n0.8804\n48.7\n0.8413\n27.5\n0.7876\n40.4\n0.8128\nFalcon\n1-shot\n31.5\n0.7985\n17.9\n0.7081\n45.4\n0.8538\n42.4\n0.8035\n21.7\n0.7586\n31.8\n0.7845\n2-shot\n35.5\n0.8202\n22.4\n0.7263\n49.5\n0.8680\n47.5\n0.8288\n21.4\n0.7605\n35.3\n0.8008\n5-shot\n40.1\n0.8377\n24.5\n0.7358\n50.5\n0.8749\n50.1\n0.8401\n22.6\n0.7776\n37.6\n0.8132\nTable 4: Results for the MDNS corpus of the multi-stage evaluation for various numbers of k-nearest-neigh using BLEU and COMET metrics. The \"Baseline\" scores are for the English-to-German NMT system describe Section 3.1.\nTable 4: Results for the MDNS corpus of the multi-stage evaluation for various numbers of k-nearest-neighbors using BLEU and COMET metrics. The \"Baseline\" scores are for the English-to-German NMT system described in\nof empty translations also increases dramatically6, with a rate of up to 63.1% for the 1-shot result on MDNS Medical (up from 8.0% at STAGE 0).\n# 4.3 STAGE 2A & STAGE 2B: Fine-Tuning towards ICL\nWhen we compare the STAGE 2B (fine-tuning with the masked loss as described in Section 3.3) to the STAGE 0 results, we find that adding the separator and fine-tuning the model leads to generally improved scores on the ACED corpora for all k. BLEU Results on MDNS corpora show slightly worse performance compared to the STAGE 0 results in 3 out of 5 corpora for k = 1, but the averages are still improved. COMET scores are however consistently improved for this comparison. We also find that the scores for k = 2 and k = 1 are very close, with 2-shot being ahead of 1-shot by 0.6% BLEU points on average on ACED data, and 1-shot being ahead of 2-shot by 0.2 BLEU points on MDNS. Which is in contrast to what we have observed in STAGE 0. k = 5 still performs worse,\n6Empty translation rates of STAGE 1 for each k over all corpora: 1-shot: 20.0%, 2-shot: 20.6%, 5-shot: 13.6%.\nbut we observe high relative gains compared to the 5-shot STAGE 0 result. When comparing STAGE 2A and STAGE 2B, i.e. the masked loss and the standard NLL loss the results are inconclusive. We further observe that STAGE 2B exhibits almost negligible rates of producing empty translations, at 0.3%, 0.8%, and 1.2% for k = 1, 2, 5 respectively.\n# 4.4 STAGE 3: Combining ICL and Domain Adaptation\nWhen combining ICL with adapters trained with nearest neighbor annotated data, we observe the globally best results for the NMT models. Compared to STAGE 1, which is also fine-tuned towards each domain, we observe greatly improved results on all automatic metrics. STAGE 3 2-shot delivers the best result on ACED, with an improvement of 2.5 BLEU points compared to the runner-up in terms of average BLEU STAGE 1 1-shot. On MDNS, STAGE 3 1-shot improves over the runnerup STAGE 1 0-shot by 3.8 points. Especially the scores for MDNS Koran improve\nwell above all previous models, with a relative improvement of 101% compared to the baseline. The models seem to be able to make better use of close nearest neighbors in this dataset, which are often substrings of one another. See Section 4.6 for a detailed analysis of the copying behavior on the ACED Asics dataset. The rate of empty translations is reduced to 0.0% for all k. We further notice that the results for 1- and 2shot ICL are very similar, and that the scores for 5-shot are also improved.\n# 4.5 FALCON: Adapting Both to a Task and a Domain at the Same Time\nThe FALCON-40B LLM proves to excel at ICL, learning a task and adapting to a domain at the same time. Notably, scores improve with higher values of k, which is the opposite behavior to what we have observed with NMT models. When nearest neighbors are close to the test data, as they are for the ACED Emerson and MDNS IT datasets, we find results that are close to the best STAGE 3 results. FALCON-40B\u2019s generation speed is however very slow at an average of 2.6 tokens per second in the 1-shot setting. Also note that we have no means at this time to check whether parts of the test data are contained in FALCON\u2019s training data.\n# 4.6 Qualitative Analysis\nMaintaining consistency in translations is an important quality criterion in the localization industry, and is a major motivator in the use of translation memories, which help ensure that marketing materials, for example, are uniform in the promised features and functions of the products being advertised (Emery et al., 2011). In NMT models, this consistency is traditionally increased by fine-tuning a translation model for a specific domain, which we denote by \"STAGE 1 with 0-shot\". In this section, we compare the fine-tuning approach with our ICL, specifically \"STAGE 3 with 1-shot\". We evaluate translation consistency on the Asics dataset. For that purpose we select segments s in the test data for which the source nearest neighbor s\u2032 in the Asics train data differs by exactly one word. These segments s are denoted as word-substitution segments. For each pair (s, s\u2032), we then use two sources and one target t\u2032 in the ICL prompt and the other target t as reference to compare the generated\ntranslation to. We define the fraction of pairs for which the generated translation exactly matches the reference as the word substitution accuracy (WSA). The results are in Table 6. The translation for STAGE 3 1-shot achieves a WSA score of 74.6%, compared to 57.14% for the fine-tuning approach (STAGE 1 0-shot), whereas the non-adapted model only produces the exact reference translation in 1.7% of cases.\n# 5 Conclusions\nWe have shown that a standard NMT system can be trained to be an effective in-context learner in domain adaptation tasks. We find that this is most effective when we combine generic fine-tuning towards the ICL task and training adapter layers for a specific domain with nearest neighbor annotated data. When the model is not fine-tuned towards the task, we find that ICL works to some extent, but shows degenerate behavior. While LLMs like FALCON-40B can adapt to the MT task with ICL, this comes at the cost of increased compute. Generally, the results with the LLM still underperform our dedicated MT models.\n# References\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. CoRR, abs/1909.08478. Ankur Bapna and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1538\u2013 1548, Hong Kong, China. Association for Computational Linguistics. Rachel Bawden and Fran\u00e7ois Yvon. 2023. Investigating the translation performance of a large multilingual language model: the case of BLOOM. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 157\u2013170, Tampere, Finland. European Association for Machine Translation.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nSource\nStrive for every point in the women\u2019s GEL-\nDEDICATE \u21226 CLAY tennis shoe by ASICS.\nReference Translation\nStrebe nach jedem Punkt in dem GEL-DEDICATE\n\u21226 CLAY Tennisschuh f\u00fcr Damen von ASICS.\nBASELINE\nMit dem\nGEL-DEDICATE \u2122\n6\nCLAY\nDamen-Tennisschuh von ASICS kannst du jeden\nPunkt erreichen.\nSTAGE 1 with 0-shot\nMit dem ASICS\nGEL-DEDICATE\n\u2122\n6\nCLAY\nTennisschuh\nf\u00fcr\nDamen\nkannst du jeden Punkt erreichen.\nSTAGE 3 with 1-shot\nStrebe nach jedem Punkt in dem GEL-DEDICATE\n\u21226 CLAY Tennisschuh f\u00fcr Damen von ASICS.\nTable 5: Comparison of example translation outputs from different models and the reference translation. Words that differ from the reference translation are highlighted in blue . The nearest source neighbor is \"Strive for every point in the men\u2019s GEL-DEDICATE \u21226 CLAY tennis shoe by ASICS.\" with the reference translation \"Strebe nach jedem Punkt in dem GEL-DEDICATE \u21226 CLAY Tennisschuh f\u00fcr Herren von ASICS.\". Notice that the nearest neighbor only differs by one word in each language.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cea3/cea3747e-12e1-4fda-83db-cee9075e5acd.png\" style=\"width: 50%;\"></div>\nSTAGE 3 with 1-shot\nSTAGE 1 with 0-shot\nNon-Adapted Model\nWord-substitution segments\n74.60%\n57.14%\n1.7%\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fa09/fa098528-39a8-46df-b218-32c1701c1f0c.png\" style=\"width: 50%;\"></div>\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901. Bram Bulte and Arda Tezcan. 2019. Neural fuzzy repair: Integrating fuzzy matches into neural machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1800\u20131809, Florence, Italy. Association for Computational Linguistics. Vince Emery, Karl Kadie, and Mary Laplante. 2011. Multilingual Marketing Content: Growing International Business with Global Content Value Chains. Outsell. Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of fewshot learning for machine translation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10867\u201310878. PMLR. Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are GPT models at machine translation? A comprehensive evaluation. arXiv preprint arXiv:2302.09210. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International\nDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations.\nom Kocmi, Rachel Bawden, Ond\u02c7rej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Nov\u00e1k, Martin Popel, and Maja Popovi\u00b4c. 2022. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1\u201345, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n# Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of machine translation summit x: papers, pages 79\u201386.\nJessy Lin, Geza Kovacs, Aditya Shastry, Joern Wuebker, and John DeNero. 2022. Automatic correction of human translations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 494\u2013507, Seattle, United States. Association for Computational Linguistics.\nYu A Malkov and Dmitry A Yashunin. 2020. Efficient and robust approximate nearest neighbor search us-\ning hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(4):824\u2013836. Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive machine translation with large language models. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227\u2013237, Tampere, Finland. European Association for Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only. M. Pham, Jitao Xu, Josep Maria Crego, Fran\u00e7ois Yvon, and Jean Senellart. 2020. Priming neural machine translation. In Conference on Machine Translation. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics. Ricardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578\u2013585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008. David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023. Prompting PaLM for translation: Assessing strategies and performance. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15406\u2013 15427, Toronto, Canada. Association for Computational Linguistics.\nJitao Xu, Josep Crego, and Jean Senellart. 2020. Boosting neural machine translation with similar translations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580\u20131590, Online. Association for Computational Linguistics.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of domain adaptation in neural machine translation (NMT) systems by leveraging in-context learning (ICL) capabilities of Large Language Models (LLMs). Previous methods required extensive fine-tuning for each domain, which is inefficient. This work proposes a new approach that allows smaller models to perform ICL effectively, thus enhancing translation quality and adaptation rates.",
        "problem": {
            "definition": "The problem is the limited ability of traditional NMT systems to adapt to new domains without extensive retraining, which is time-consuming and resource-intensive.",
            "key obstacle": "The core obstacle is the necessity for separate sets of parameters for each domain, which hinders the efficiency of translation systems."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that LLMs can utilize few-shot examples to adapt their outputs to specific domains, thus allowing for more efficient translations.",
            "opinion": "The idea entails training a smaller model to perform ICL for domain adaptation in NMT systems by fine-tuning it towards a specialized training objective.",
            "innovation": "The primary innovation is the integration of ICL into NMT systems without the need for extensive fine-tuning, enabling efficient batch inference and improved translation quality."
        },
        "method": {
            "method name": "Lilt",
            "method abbreviation": "Lilt",
            "method definition": "Lilt is a method that combines in-context learning with domain adaptation in neural machine translation by utilizing few-shot examples to enhance output quality.",
            "method description": "The method allows a smaller NMT model to adapt to various domains by leveraging few-shot examples during inference.",
            "method steps": [
                "Evaluate the ICL capacity of a baseline NMT model.",
                "Fine-tune the NMT model to improve ICL capability.",
                "Combine ICL with traditional adaptation methods for better performance."
            ],
            "principle": "The method is effective because it allows the model to learn from relevant examples without needing to retrain on extensive domain-specific data, thus improving adaptation speed and translation quality."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using an English-German language pair across eight domains from the ACED and MDNS corpus collections, comparing Lilt's performance against traditional methods and a 40B-parameter LLM.",
            "evaluation method": "The performance was assessed using BLEU and COMET scores to measure translation quality, with various configurations for the number of nearest neighbors used in the ICL process."
        },
        "conclusion": "The study demonstrates that the proposed method effectively enhances the ICL capabilities of NMT systems, outperforming traditional methods in translation quality and adaptation rates, particularly when combining ICL with domain-specific fine-tuning.",
        "discussion": {
            "advantage": "The key advantages include improved translation quality, faster adaptation to new domains, and reduced computational requirements compared to traditional fine-tuning methods.",
            "limitation": "One limitation is that while the method shows significant improvements, it may still not match the performance of larger LLMs in some scenarios.",
            "future work": "Future research could explore further optimizations of the training scheme and investigate the applicability of the method to other languages and translation tasks."
        },
        "other info": {
            "info1": "The method was tested on multiple datasets, including real-world benchmarks.",
            "info2": {
                "info2.1": "The model's performance was compared across different stages of development.",
                "info2.2": "Results indicated a significant reduction in empty translations with the proposed method."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of domain adaptation in neural machine translation (NMT) systems by leveraging in-context learning (ICL) capabilities of Large Language Models (LLMs)."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method allows smaller models to perform ICL effectively, thus enhancing translation quality and adaptation rates."
        },
        {
            "section number": "3.1",
            "key information": "The intuition behind the proposed idea is that LLMs can utilize few-shot examples to adapt their outputs to specific domains, thus allowing for more efficient translations."
        },
        {
            "section number": "3.2",
            "key information": "The primary innovation is the integration of ICL into NMT systems without the need for extensive fine-tuning, enabling efficient batch inference and improved translation quality."
        },
        {
            "section number": "4.1",
            "key information": "The method combines in-context learning with domain adaptation in neural machine translation by utilizing few-shot examples to enhance output quality."
        },
        {
            "section number": "5.2",
            "key information": "Experiments were conducted using an English-German language pair across eight domains, comparing the proposed method's performance against traditional methods."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that while the method shows significant improvements, it may still not match the performance of larger LLMs in some scenarios."
        }
    ],
    "similarity_score": 0.7065233143144336,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Neural Machine Translation Models Can Learn to be Few-shot Learners.json"
}