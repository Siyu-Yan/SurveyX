{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.11225",
    "title": "In-Context Learning State Vector with Inner and Momentum Optimization",
    "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introducing the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at https://github.com/HITsz-TMG/ICL-State-Vector",
    "bib_name": "li2024incontextlearningstatevector",
    "md_text": "# In-Context Learning State Vector with Inner and Momentum Optimization\n# Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun, Baotian Hu\u2217, Min Zhang Harbin Institute of Technology (Shenzhen), Shenzhen, China\n{crazyofapple, liuzhenyuhit}@gmail.com {hubaotian, zhangmin2021}@hit.edu.cn\n# Abstract\nLarge Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introducing the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at https://github.com/HITsz-TMG/ICL-State-Vector\n# 1 Introduction\nIn-Context Learning (ICL) has emerged as a powerful capability in tandem with the scaling of large language models (LLMs) [Brown et al., 2020]. By simply conditioning on a few input-label pairs as demonstrations, LLMs yield a significant improvement and deliver remarkable results in various downstream Natural Language Processing (NLP) tasks [Wei et al., 2022, Liu et al., 2023a]. For example, a model prompted with the input \u201cgaot \u2192goat, sakne \u2192snake, brid \u2192\u201d can produce the output \u201cbird\u201d. Given these successes, it is worthwhile to inquire about the exact internal working mechanisms of ICL. Considering the opaque operation of ICL within the auto-regressive transformer, it is plausible that ICL might function as a general mechanism that leverages both demonstrations and the query to yield the prediction [Dong et al., 2023]. Recently, some studies have found that the ICL mapping function exists in the outputs of the attention layers or attention heads [Liu et al., 2023b, Dai et al., 2023] when applying causal effects analysis on a different set of models and tasks, such as the task vector [Hendel et al., 2023] and the function vector [Todd et al., 2023]. These works show that the functionalities learned through ICL can be encapsulated in compressed vectors derived from transformers, which then can be used to intervene in the transformer to handle queries without demonstrations. This revelation suggests the potential mechanism of ICL that first utilises demonstrations to learn the mapping function from inputs to\n\u2217Corresponding author.\nPreprint. Under review.\nPreprint. Under review.\nlabels in shallow transformer layers, and then uses the ICL function in deeper transformer layers to make predictions [Hendel et al., 2023]. However, while these compressed vectors encapsulate learned information in a more condensed form and show significant promise in applying ICL, there still exists a considerable gap in understanding the operational mechanisms and optimization strategies of these vectors. This significant gap hinders the further grasping and utilization of ICL. In this paper, we aim to bridge the existing gap by presenting a comprehensive analysis of compressed vectors. Specifically, we investigate their similarities with parameters trained via gradient descent and introduce the formulation of state vector that encapsulates the processing state of ICL stored in the attention activations. Building on the concept of state vector, and drawing insights from the model soup [Wortsman et al., 2022] and momentum-based gradient optimization algorithms [Qian, 1999, Sutskever et al., 2013], we propose inner optimization and momentum optimization strategies which are progressively applied to enhance the state vector. Moreover, we further exploit the demonstration compression capabilities of the state vector to address the practical challenges encountered when applying ICL in settings with multiple examples, where demonstrations are typically too lengthy for standard ICL, such as in the 100-shot setting which is prevalent in practice. Specifically, we introduce a divide-and-conquer aggregation method that effectively aggregates the ICL functions of these extensive examples. This approach enables us to scale up for processing extended examples by compressing them into a single state vector. We conduct extensive experiments using Llama2 [Touvron et al., 2023] and GPT-J [Wang and Komatsuzaki, 2021] in both zero-shot and few-shot settings. The experimental results show that our method effectively enhances the state vector and achieves state-of-the-art performance on diverse tasks. This not only manifests the effectiveness of our approach but also paves the way for a more comprehensive understanding of ICL.\n\u2022 We delve into the working mechanism of compressed vectors in ICL and highlight their similarities with parameters trained via gradient descent. Building on this observation, we propose the formulation of the state vector. \u2022 We propose inner and momentum optimization to progressively refine the state vector as an efficient test-time adaptation. Additionally, we introduce a divide-and-conquer aggregation to effectively scale up to large numbers of examples. \u2022 We show the practicality of our proposed methods across a wide range of tasks through extensive experiments. Our results also offer insights for future research aiming to fully understand the functionalities of ICL.\n# 2 Related Work\nMechanistic Interpretability. Recent works have focused on the working mechanisms of ICL [Chan et al., 2022, Xie et al., 2022, Wang et al., 2023]. Olsson et al. [2022] argue that induction heads may be the mechanistic source of general ICL in transformers. Aky\u00fcrek et al. [2022] show that transformer-based in-context learners can implicitly implement standard optimization algorithms on linear models. A mainstream assumption posits that ICL has a similarity with the gradient descent. von Oswald et al. [2023] demonstrate how a linear attention-only transformer model can perform a gradient descent-like procedure implicitly. Dai et al. [2023] compare standard gradient descent based fine-tuning and ICL, and figure out that the transformer attention of ICL exhibits a dual form of gradient descent-based optimization. Moreover, some works revisit and modify this theory on the layer causality dependence [Natan et al., 2023] or training batch size [Shen et al., 2023]. In contrast, we focus on the application of the dual form of gradient descent and ICL and present optimization methods with inspiration from the dual form. Task Representation. Numerous studies have extensively explored the concept of compressing various tasks into task representations as a means of effectively manipulating tasks within ICL ability. Notably, Shao et al. [2023] and Mu et al. [2023] have successfully yielded compositional task representations by training a composition model. In a slightly different vein, some researchers have delved into the art of devising methodologies to compose minor parameter adjustments acquired through task fine-tuning [Ilharco et al., 2022, Panigrahi et al., 2023, Yu et al., 2023, Hu et al., 2024, Merullo et al., 2023]. An alternative line of research finds that the task representation could be extracted in ICL [Liu et al., 2023b, Hendel et al., 2023, Todd et al., 2023, Yang et al., 2023]. Different\nMechanistic Interpretability. Recent works have focused on the working mechanisms of ICL [Chan et al., 2022, Xie et al., 2022, Wang et al., 2023]. Olsson et al. [2022] argue that induction heads may be the mechanistic source of general ICL in transformers. Aky\u00fcrek et al. [2022] show that transformer-based in-context learners can implicitly implement standard optimization algorithms on linear models. A mainstream assumption posits that ICL has a similarity with the gradient descent. von Oswald et al. [2023] demonstrate how a linear attention-only transformer model can perform a gradient descent-like procedure implicitly. Dai et al. [2023] compare standard gradient descent based fine-tuning and ICL, and figure out that the transformer attention of ICL exhibits a dual form of gradient descent-based optimization. Moreover, some works revisit and modify this theory on the layer causality dependence [Natan et al., 2023] or training batch size [Shen et al., 2023]. In contrast, we focus on the application of the dual form of gradient descent and ICL and present optimization methods with inspiration from the dual form.\nTask Representation. Numerous studies have extensively explored the concept of compressing various tasks into task representations as a means of effectively manipulating tasks within ICL ability. Notably, Shao et al. [2023] and Mu et al. [2023] have successfully yielded compositional task representations by training a composition model. In a slightly different vein, some researchers have delved into the art of devising methodologies to compose minor parameter adjustments acquired through task fine-tuning [Ilharco et al., 2022, Panigrahi et al., 2023, Yu et al., 2023, Hu et al., 2024, Merullo et al., 2023]. An alternative line of research finds that the task representation could be extracted in ICL [Liu et al., 2023b, Hendel et al., 2023, Todd et al., 2023, Yang et al., 2023]. Different\nfrom these approaches, our work avoids the need for additional training and focuses more on analysing why these compressed vectors work and how to improve their performance.\n# 3 Formalization\nIn this section, we first provide a detailed examination of attention activation which is found to contain the compressed ICL function by previous works [Hendel et al., 2023, Todd et al., 2023]. Then, we highlight its inherent similarities with parameters trained through gradient descent. Finally, we introduce the concept of the state vector drawing inspiration from these observations. A classic template of ICL has the following necessary components: (1) N examples that are used to form the demonstrations and each example contains an input query X and its corresponding label Y. (2) Separate tokens S that separate the input query and the label for each example (e.g., \u2192). (3) A query Xq for prediction. With the above components, the contextual model input of ICL could be written as follows: X1, S, Y1, X2, S, Y2, \u00b7 \u00b7 \u00b7 , XN, S, YN, Xq, S.\nA classic template of ICL has the following necessary components: (1) N examples that are used to form the demonstrations and each example contains an input query X and its corresponding label Y (2) Separate tokens S that separate the input query and the label for each example (e.g., \u2192). (3) A query Xq for prediction. With the above components, the contextual model input of ICL could be written as follows:\nX S Y X S Y \u00b7 \u00b7 \u00b7 X S Y X S Here we analyse the attention activation of the last separate token. In the l-th transformer layer, the output activation al of the attention heads of the last separate token is: \ufffd \ufffd\nwhere X\u2032 denotes the hidden state of demonstrations, X denotes the hidden state of the query and the last separate token (called zero-shot input), q denotes the attention query vector of the last separate token, [X\u2032; X] denotes the matrix concatenation, \u221a d is the scaling factor, WK and WV are parameter weight matrix. Consistent with previous works [Dai et al., 2023, Natan et al., 2023], we omit the softmax operation and the scaling factor to approximate standard attention as relaxed linear attention for qualitative analysis. Consequently, the activation can be simplified as follows:\n\ufffd We define WZSL = WV X (WKX)T as the initialized parameters since it is the attention result in the Zero-Shot Learning (ZSL) setting. To draw a meaningful comparison between attention activation and parameters trained through gradient descent, we now shift our focus towards analyzing a simple linear transformation represented by yi = Wxi. Given a loss function L and the learning rate \u03b7, the gradient of linear weight is:\no draw a meaningful comparison between attention activation and parameters trained through radient descent, we now shift our focus towards analyzing a simple linear transformation represented y yi = Wxi. Given a loss function L and the learning rate \u03b7, the gradient of linear weight is:\nDenoting the back-propagated errors as ei = \u2212\u03b7\u2207yiL, we can get the full batch gradient with training examples:\nDenoting the back-propagated errors as ei = \u2212\u03b7\u2207yiL, we can get the full batch gradient with training examples: \ufffd\n\ufffd where x\u2032 i is the input training examples. Hence, in the previous Eqn. 2, if we substitute WKx\u2032 i as training examples, and take WV x\u2032 i \u2248ei corresponding to some meta gradients [Dai et al., 2023, Natan et al., 2023]. The activation can be written as: \ufffd \ufffd\n\ufffd Hence, it can be inferred that the output activation al can be regarded as parameters trained via gradient descent which utilizes the demonstrations as training instances.\n(1)\n(2)\n(3)\n(4)\n(5)\nWith the above dual form between activation and trained parameters, and in light of observations that transformers tend to learn the ICL function primarily in their first L layers [Wang et al., 2023], we have the following hypothesis: During the process of ICL, the first L layers progressively update the flow of information using each example in the demonstration through forward computation. The processing state of ICL is then stored within the activation of the attention head. The subsequent layers access and utilize the processing state to reinstate the ICL function, which is used implicitly for predicting the queries. Therefore we concatenate the activation in the initial L layers and introduce the notation of the state vector:\n\ufffd\ufffd where L is the number of layers and N is the number of examples in the demonstration. \u2225denotes the concatenation operation. Note that we have a completely different construction strategy and usage compared to the function vector [Todd et al., 2023]. Although the task vector [Hendel et al., 2023] may be functionally equivalent in the forward process, the proposed state vector differs significantly in its integration into the model, making it easier and more effective to analyse and interpret.\n# 4 Method\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7641/764136c1-5677-468d-9955-646c0e823cef.png\" style=\"width: 50%;\"></div>\nFigure 1: The overall framework of the proposed state vector. The state vectors are extracted from the output activations of attention heads. These state vectors are progressively optimized by inner optimization and momentum optimization, or be aggregated through a divide-and-conquer (D&C) aggregation. Finally, the processed state vector is utilized to intervene the inference forward pass.\n# 4.1 Overview\nAs illustrated in Figure 1, our approach initially extracts the state vector from the attention head that corresponds to the final separate token in the first L layers using a demonstration and a dummy query. Then, with the view of treating the state vector as trained parameters, coupled with drawing inspiration from the model soup and the momentum-based gradient optimization algorithm, we introduce two methods that progressively optimize the state vector as test-time adaptation [Liang et al., 2023]: (1) inner optimization (\u00a74.2) and (2) momentum optimization (\u00a74.3). Moreover, we propose a divide-and-conquer (D&C) state vector aggregation method for efficiently compressing the ICL function in the multiple example setting (\u00a74.4). After the state vector optimization or aggregation, we utilize the processed state vector to intervene the model during the forward inference pass. In particular, we first input a test query in the zero-shot setting or with the demonstration in the few-shot setting. During the forward pass in the first L layers, we replace the attention activation of the last separate token with the corresponding activation in the state vector. In other words, the state vector is leveraged to intervene in the output of the first L transformer layers, blocking the attention of the last separate token to the previous context. With state vector intervention, the transformer learns the ICL function from the processing state stored in the state vector, and continues to make the prediction on the test query.\nInspired by the works on the model soup [Wortsman et al., 2022, Chronopoulou et al., 2023] which show that weight-space averaging not only yields performance improvement but also often enhances robustness, we thus ask the following research question (RQ1): Is it possible to optimize our state vector using the model soup approach? To explore this question, we propose an inner optimization method to improve the effectiveness and robustness of state vector. Specifically, we not only extract the state vector in each separate token of the dummy query but also extract the state vector from each example. Formally, with a forward pass in an N shot ICL setting, we extract the N state vector VL i (1 \u2264i \u2264N) from last N separate token. Subsequently, we apply a uniform averaging process to these state vectors as follows:\nwhere V L N is the inner optimized state vector, which can be directly utilized for inference intervention or serves as the initial state vector for later momentum optimization.\n# 4.3 Momentum Optimization\nSince we view the state vector as parameters trained gradually through demonstration examples, the difference between two state vectors with adjacent corresponding separate tokens can also be regarded as the influence of the middle example, akin to the gradient. Motivated by this understanding, coupled with extensive studies of the gradient optimization algorithm [Sutskever et al., 2013, Duchi et al., 2010, Loshchilov and Hutter, 2019], we direct our focus toward a simple momentum-based gradient optimization algorithm, seeking to answer the following research question (RQ2): Can our state vector be optimized using momentum-based optimization algorithm? To answer this question, we propose a momentum optimization. Formally, we first extract the influence of each example by subtracting two adjacent state vectors:\nwhere EL i is the influence of i-th (1 < i \u2264N) example in the early L layer. Then, we apply th momentum gradient optimization algorithm to obtain optimized influence \ufffdEL i , and add it to the las state vector:\nwhere EL i is the influence of i-th (1 < i \u2264N) example in the early L layer. Then, we apply the momentum gradient optimization algorithm to obtain optimized influence \ufffdEL i , and add it to the last state vector:\n\ufffd  \ufffd where \ufffdVL N is the momentum optimized state vector and V L N is the inner optimized state vector. opt(\u00b7 denotes the momentum gradient optimization algorithm. We also explore various other gradien optimization algorithms in \u00a76.1.\n# 4.4 Divide-and-Conquer Aggregation\nIn addition to optimizing the state vector to more effectively represent the ICL function from a small number of examples, we also explore its capacity to encapsulate multiple examples within a single vector. However, regular ICL can not be directly used on multiple examples due to the context length limitation of current LLMs. This leads us to investigate the following question (RQ3): Can we use the state vector to represent multiple examples that are unmanageable for regular ICL? To address this question, we propose a divide-and-conquer method for state vector aggregation. As depicted in Figure 1, our approach involves distinct aggregation processes (i.e. the divide stage and the conquer stage). In the divide stage, examples are randomly divided into groups, termed grouped demonstrations. Within each group, a random example is selected to serve as a dummy query, which allows us to extract a group-specific state vector. In the conquer stage, these dummy queries are paired with their corresponding labels to form input-label pairs. From these input-label pairs, we form an aggregated demonstration, add an additional dummy query, and subsequently extract the aggregated state vector. It is worth noting that during the forward pass of aggregated state vector extraction, we utilise the group-specific state vector to intervene the attention activation of the separate tokens of their corresponding examples. The divide and conquer approach allows us to aggregate the ICL function of each grouped demonstration into its respective group-specific state vector, and subsequently aggregate the ICL function of each group-specific state vector into a single,\n(7)\n(8)\ncomprehensive aggregated state vector. This aggregated vector is then utilized for interventions during inference, similarly to the optimized state vector discussed in \u00a74.2 and \u00a74.3. Moreover, in the few-shot setting, the aggregated demonstrations are treated as inference demonstrations. The divide-and-conquer approach effectively circumvents the context-length constraints inherent in LLMs, thereby enabling a more effective and efficient aggregation of information across multiple examples.\n# 5 Experiment\n# 5.1 Setup\n\u2022 Linguistics includes Antonym [Nguyen et al., 2017], Capitalize, Present-Past, and SingularPlural [Todd et al., 2023], focusing on transformations in the form or meaning of words. \u2022 Translation is represented by the English-French [Lample et al., 2018] dataset, which involves translating English words into their French counterparts. \u2022 Knowledge comprises Country-Capital [Todd et al., 2023], AG News [Zhang et al., 2015], Person-Sport, Person-Instrument, Person-Occupation, Product-Company, and LandmarkCountry [Hernandez et al., 2023], which are centred around question-to-answer mappings for commonsense knowledge queries.\nWe employ Llama-2-7B and GPT-J-6B as our LLMs, chosen for their moderate model sizes, opensource and capability for ICL. We also provide the results with larger models (i.e., Llama-2-13B) in the Appendix H. We use Llama-2-7B as the default model unless otherwise specified. Our method is orthogonal to the choice of transformer-based decoder-only autoregressive LLMs. For simplicity evaluation, we restrict to single-token output and use first output token accuracy as the evaluation metric as in previous work [Hendel et al., 2023, Todd et al., 2023].\n# 5.2 Baseline\nIn the paper, we compare with the following methods:\n\u2022 Regular is the baseline for the zero-shot setting that uses only the given query as input, while ICL baseline [Wei et al., 2022] makes predictions on the label by taking both the demonstrations and the given query. \u2022 Function vector [Todd et al., 2023] is extracted from attention activation using the causal mediation method and is then added to the hidden state of certain transformer layers during inference. \u2022 Task vector [Hendel et al., 2023] is extracted from the hidden state of the separate token and is leveraged for blocking the layer when inference.\n# 5.3 Inner Optimization(RQ1)\nAs shown in Table 1, the performance of our inner optimized state vector has a significant improvement comparing the task vector and function vector in both zero-shot and few-shot settings. Our state vector with inner optimization. In the zero-shot setting, the inner optimization shows an average improvement of 10.2% on Llama-2 and 5.9% on GPT-J across six datasets. In the few-shot setting, the inner optimization also achieves a 1.2% improvement on Llama-2 and 1.7% on GPT-J. The improvement demonstrates the effectiveness of inner optimization. However, although state vector (inn.) outperforms task vector, its few-shot performance on some datasets is inferior to the ICL baseline. We attribute this primarily to the introduction of query information from examples. While inner optimization enhances task-relevant information for the state vector, it also introduces noise of other dummy queries, hindering the model\u2019s ability to focus on the current predictive query, thereby reducing performance. In addition to the performance improvements, our inner optimization approach also effectively alleviates the phenomenon of high variance in the original task vector in the zero-shot setting. In practical use, the performance of the task vector is influenced by demonstrations and dummy queries, leading to weaker robustness. Our proposed inner optimization approach effectively\nModel\nMethod\nAnym\nEng-Fr\nPers-Inst\nPers-Occ\nProd-Comp\nLand-Cout\nAverage\nLlama-2\nZero-shot\nRegular\n1.0\u00b1 0.2\n0.1\u00b10.1\n0.0\u00b10.0\n0.0\u00b10.0\n0.4\u00b10.2\n0.0\u00b10.0\n0.3\nFunction vector\n45.1\u00b12.0\n21.6\u00b12.0\n11.3\u00b110.7\n0.1\u00b10.1\n25.6\u00b14.3\n32.9\u00b121.6\n22.8\nTask vector\n56.2\u00b12.8\n63.2\u00b13.6\n61.8\u00b18.4\n27.9\u00b115.2\n55.5\u00b120.1\n57.8\u00b126.3\n53.7\nState vector (inn.)\n61.0\u00b11.0\n66.5\u00b12.2\n67.4\u00b12.6\n42.7\u00b14.2\n64.5\u00b110.6\n81.0\u00b11.7\n63.9\nState vector (mom.)\n60.4\u00b10.7\n67.5\u00b11.8\n68.7\u00b11.6\n45.6\u00b15.9\n71.3\u00b13.6\n77.7\u00b11.8\n65.2\nFew-shot\nICL baseline\n64.8\u00b14.8\n74.3\u00b10.8\n71.7\u00b13.7\n56.1\u00b12.7\n80.8\u00b10.8\n87.0\u00b10.3\n72.5\nFunction vector\n54.5\u00b10.9\n65.2\u00b11.4\n60.8\u00b15.6\n54.2\u00b12.2\n76.0\u00b11.3\n84.2\u00b12.9\n65.8\nTask vector\n65.7\u00b11.8\n73.8\u00b10.9\n66.6\u00b15.2\n56.4\u00b12.3\n81.9\u00b11.8\n86.7\u00b10.9\n71.8\nState vector (inn.)\n66.2\u00b11.6\n74.6\u00b10.9\n70.1\u00b14.3\n57.0\u00b12.2\n82.8\u00b11.6\n87.5\u00b10.9\n73.0\nState vector (mom.)\n65.8\u00b13.7\n74.3\u00b11.1\n74.9\u00b12.9\n58.2\u00b10.4\n82.0\u00b11.0\n87.6\u00b10.3\n73.8\nGPT-J\nZero-shot\nRegular\n8.1\u00b10.6\n7.2\u00b10.6\n0.0\u00b10.0\n0.0\u00b10.0\n1.9\u00b10.5\n0.9\u00b10.2\n3.0\nFunction vector\n33.1\u00b11.8\n29.1\u00b18.5\n4.1\u00b15.8\n11.1\u00b12.3\n46.3\u00b15.7\n22.5\u00b110.2\n24.4\nTask vector\n23.6\u00b13.8\n32.2\u00b15.1\n44.4\u00b15.0\n28.3\u00b118.6\n43.8\u00b15.7\n41.3\u00b112.3\n35.6\nState vector (inn.)\n33.4\u00b11.9\n31.7\u00b13.8\n49.3\u00b12.0\n30.0\u00b16.2\n42.8\u00b14.3\n61.9\u00b11.6\n41.5\nState vector (mom.)\n31.1\u00b11.0\n35.1\u00b12.4\n50.3\u00b13.0\n42.4\u00b11.5\n44.2\u00b11.5\n60.3\u00b10.9\n43.9\nFew-shot\nICL baseline\n59.2\u00b11.4\n69.9\u00b12.0\n44.7\u00b16.7\n29.3\u00b11.0\n62.5\u00b11.0\n69.3\u00b10.5\n55.8\nFunction vector\n56.4\u00b11.9\n65.8\u00b11.9\n49.1\u00b12.2\n30.3\u00b11.9\n58.5\u00b13.3\n69.2\u00b10.6\n54.9\nTask vector\n58.5\u00b11.6\n70.6\u00b11.2\n42.3\u00b16.4\n27.8\u00b13.3\n66.0\u00b12.6\n63.1\u00b15.3\n54.7\nState vector (inn.)\n58.7\u00b12.2\n70.9\u00b11.3\n46.5\u00b14.9\n29.4\u00b11.7\n66.3\u00b12.1\n66.4\u00b12.8\n56.4\nState vector (mom.)\n59.6\u00b11.4\n70.1\u00b12.2\n51.9\u00b12.4\n30.4\u00b11.1\n63.8\u00b10.8\n68.6\u00b10.3\n57.4\nTable 1: Performance of state vector optimization. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector. Note that we only present the results across six tasks here and leave the rest in the Appendix. We also report standard deviation and the results are passed with significance test (p < .05).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/62db/62db5c0b-4089-4099-b25b-5f38124b25c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) GPT-J Antonym</div>\n<div style=\"text-align: center;\">Figure 2: Performance of aggregation across number of examples. Avg. denotes the average aggregation base and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and Y axis represents the accuracy.</div>\nmitigates this issue, similarly motivated as the model averaging method, thereby enhancing th robustness of the state vector.\n# 5.4 Momentum Optimization (RQ2)\nAs depicted in Table 1, building upon the inner optimized state vector, our proposed momentum optimization algorithm further enhances the effectiveness of the state vector, achieving the best performance on average in all settings. In the zero-shot setting, the momentum optimization boosts the performance of the inner-optimized state vector with an average increase of 1.3% on Llama-2 and 2.4% on GPT-J. In the few-shot setting, state vector with momentum optimization achieves a 0.8% average increase on Llama-2 and 1.0% on GPT-J. This reveals the effectiveness of our momentum optimization. With the combination of inner optimization and momentum optimization, our state\nMethod\nZero-shot\nFew-shot\nICL baseline\n0.2\u00b1 0.4\n71.0\u00b1 10.8\nTask vector\n52.9\u00b1 9.4\n68.5\u00b1 10.5\nState vector (mom.)\n65.2\u00b1 10.2\n72.2\u00b1 10.6\nState vector (adag.)\n11.7\u00b1 12.0\n16.1\u00b1 10.2\nState vector (rms.)\n0.8\u00b1 0.9\n1.5\u00b1 1.0\nState vector (adam.)\n6.7\u00b1 6.1\n10.6\u00b1 8.5\nTable 2: Performance comparison of gradient optimization algorithms. The method means the optimization algorithm applied to the opt(\u00b7) in Eqn. 9.\nvector (mom.) surpasses the original variant, showcasing a remarkable improvement of 11.5% for Llama-2 and 8.3% for GPT-J in the zero-shot setting. In the few-shot setting, our state vector (mom.) still outperforms the task vector with a 2.0% improvement for Llama-2 and 2.7% for GPT-J. Furthermore, without inputting demonstration during inference, the state vector (mom.) achieves an impressive 90% ICL performance on Llama-2 and 78% ICL performance on GPT-J. When compared to ICL with the same examples as the demonstration, state vector (mom.) outperforms ICL in both Llama-2 and GPT-J. These improvements verify the effectiveness of our progressive optimization strategy. Note that applying momentum optimization directly to task vectors does not yield average improvements across tasks in our preliminary experiment. We speculate that this inconsistency stems from the poor robustness of the task vectors, which hinders the stable optimization by momentum optimization and leads to poor performance in some tasks.\n# 5.5 Divide-and-Conquer Aggregation (RQ3)\nIn this experiment, we explore the performance of D&C state vector aggregation across varying numbers of examples. Besides the regular and ICL baseline mentioned, we introduce average aggregation as a strong baseline. This approach first extracts state vectors from the example group and subsequently employs their mathematical average for aggregation. We compare our D&C aggregation method with the baseline ranging from 10 to 100 examples across two models. Due to limited computational resources, we were not able to do an exhaustive search over all datasets. Thus, we only present the results for four tasks. As illustrated in the Figure 2, both the D&C aggregation and average aggregation exhibit similar trends in both few-shot and zero-shot settings. The performance of both aggregation methods initially falls short of the ICL baseline. However, their performance boosts when examples increase. The initial poor performance can be attributed to the limited number of state vectors. Additionally, although the performance of the D&C aggregation initially falls behind that of the average aggregation, it exhibits a more substantial performance improvement when examples increase, ultimately outperforming average aggregation in the multiple example setting, highlighting the efficiency of D&C aggregation.\n# 6 Analysis\n# 6.1 Ablation with Other Optimization Methods\nWe present an ablation study to investigate various classical gradient optimization algorithms, aiming to delve deeper into the inner state vector optimization. We compare the momentum-based gradient optimization algorithm with following additional first-order gradient optimization algorithms: Adagrad (adag.) [Duchi et al., 2010], RMSprop (rms.) [Graves, 2013] and Adam(adam.) [Kingma and Ba, 2015]. As shown in Table 2, we observe a significant decrease in state vector performance with first-order gradient optimization algorithms, unlike with momentum-based optimization. This outcome indicates a discrepancy between the state vector and updated parameters with gradient\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d090/d090aae9-8c68-424f-b677-71d616a66745.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Layers</div>\n<div style=\"text-align: center;\">Figure 3: Average zero-shot performance across six datasets for each choice of the intermediate layer L. The solid line means the average value, while the shaded area indicates the standard deviation.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d027/d0272fb2-6cb2-4de6-8ffe-b48c626f2df9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The 2D PCA visualization of the state vector in the Antonym ,English-French and Product-Company task, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration and the outlier is the first order.</div>\ndescent. It suggests that the current first-order gradient optimization algorithms may not be optimally effective for state vector optimization. Due to computational constraints, we were not able to do an exhaustive search over all hyper-parameters.\n# 6.2 Layer Selection\nWe investigate the impact of layer selection on the extraction of state vectors in transformer models. We evaluate the average performance across different datasets in the zero-shot setting, as illustrated in Figure 3. Our results reveal a dual-phase trend: initially, increasing the number of layers for state vector extraction improves performance, but this improvement reverses beyond the 14th layer. We correlate this with the dynamics of ICL function processing in transformers in line with previous works [Voita et al., 2019, Wang et al., 2023]. In the initial layers, transformers are primarily engaged in learning and encapsulating the ICL function within state vector, where additional layers enhance the richness of the functional information in the state vector. In contrast, the later layers prioritize applying this learned information for prediction tasks. Here, additional layers tend to introduce noise, especially from predicted labels of dummy queries, which may negatively impact performance.\n# 6.3 Qualitative Study\nWe provide the visualization by Principal Component Analysis (PCA) of the original state vector in the Antonym, English-French and Product-Company task. As depicted in Figure 4, we have three observations: (1) State vectors corresponding to the examples occupying the same position tend to form distinct clusters. This clustering pattern suggests a high degree of similarity among state vectors within each example position, despite different contexts. (2) A notable separation is evident between the state vectors originating from the first example and other position examples. This demarcation implies that ICL may begin to effectively function with a few examples. (3) An interesting trend is observable in the movement of these clusters as the example position increases. This trend may be indicative of an accumulation of task-specific information, where each additional example contributes to a more nuanced understanding of the model. These findings suggest a progressive enhancement in the ability of model to internalize and reflect the subtleties of the task at hand. Moreover, these observations reflect the efficacy of momentum optimization to leverage the observed clustering trend.\n# 7 Conclusion\nIn this paper, we reveal that ICL compressed vector can be viewed as parameters trained through gradient descent on the demonstrations. Then, we introduce the concept of state vector coupled with two optimization methods to enhance the capability of ICL and conduct comprehensive experiments across two popular LLMs and multiple tasks to support our claim. Furthermore, our approach demonstrates the ability to compress context while maintaining lower variance. In the future, we aim to extend our methods to more complex ICL scenarios and apply them to larger LLMs and call for more nuanced and realistic studies of ICL.\n<div style=\"text-align: center;\">(c) Product-Company</div>\n# References\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learnin algorithm is in-context learning? investigations with linear models. ArXiv preprint, abs/2211.1566 2022.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Stephanie Chan, Adam Santoro, Andrew K. Lampinen, Jane Wang, Aaditya Singh, Pierre H. Richemond, James L. McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022. Alexandra Chronopoulou, Matthew Peters, Alexander Fraser, and Jesse Dodge. AdapterSoup: Weight averaging to improve generalization of pretrained language models. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2054\u20132063, 2023. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4005\u20134019, 2023. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. ArXiv preprint, abs/2301.00234, 2023. John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 257\u2013269, 2010. Alex Graves. Generating sequences with recurrent neural networks. ArXiv, abs/1308.0850, 2013. Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. ArXiv preprint, abs/2310.15916, 2023. Evan Hernandez, Arnab Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. Linearity of relation decoding in transformer language models. ArXiv preprint, abs/2308.09124, 2023. Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, and Min Zhang. Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation. In Proc. of AAAI, 2024. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. ArXiv preprint, abs/2212.04089, 2022. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Machine Learning, 2015.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611\u2013626. ACM, 2023. doi: 10.1145/ 3600006.3613165. Guillaume Lample, Alexis Conneau, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. Word translation without parallel data. In International Conference on Machine Learning, 2018. Jian Liang, Ran He, and Tieniu Tan. A comprehensive survey on test-time adaptation under distribution shifts. ArXiv preprint, abs/2303.15361, 2023. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023a. Sheng Liu, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. ArXiv preprint, abs/2311.06668, 2023b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Machine Learning, 2019. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vecstyle vector arithmetic. ArXiv preprint, abs/2305.16130, 2023. Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. Learning to compress prompts with gist tokens. ArXiv preprint, abs/2304.08467, 2023. Tomer Bar Natan, Gilad Deutch, Nadav Magar, and Guy Dar. In-context learning and gradient descent revisited. ArXiv preprint, abs/2311.07772, 2023. Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. Distinguishing antonyms and synonyms in a pattern-based neural network. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 76\u201385, 2017. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. ArXiv preprint, abs/2209.11895, 2022. Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill localization in fine-tuned language models. In International Conference on Machine Learning, 2023. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1): 145\u2013151, 1999. Nan Shao, Zefan Cai, Hanwei Xu, Chonghua Liao, Yanan Zheng, and Zhilin Yang. Compositional task representations for large language models. In International Conference on Learning Representations, 2023. Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? ArXiv preprint, abs/2310.08540, 2023. Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proc. of ICML, volume 28 of JMLR Workshop and Conference Proceedings, pages 1139\u20131147, 2013. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. Function vectors in large language models. ArXiv preprint, abs/2310.15213, 2023. Hugo Touvron, Louis Martin, and Kevin R. Stone et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023.\nElena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. In Proc. of EMNLP, pages 4396\u20134406, 2019. Johannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2023. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, 2021. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 9840\u20139855, 2023. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 23965\u201323998, 2022. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Machine Learning, 2022. Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Iterative forward tuning boosts in-context learning in language models. ArXiv preprint, abs/2305.13016, 2023. Le Yu, Yu Bowen, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. ArXiv preprint, abs/2311.03099, 2023. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649\u2013657, 2015.\nElena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. In Proc. of EMNLP, pages 4396\u20134406, 2019. Johannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2023. Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, 2021. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 9840\u20139855, 2023. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 23965\u201323998, 2022. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Machine Learning, 2022. Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Iterative forward tuning boosts in-context learning in language models. ArXiv preprint, abs/2305.13016, 2023. Le Yu, Yu Bowen, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. ArXiv preprint, abs/2311.03099, 2023. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649\u2013657, 2015.\n# A Implementation Details\nIn this paper, we use random sampling to create subsets for each dataset. Each subset consists of 10 instances for demonstrations and one instance for a dummy query since we employ a 10-shot as the default ICL setting. The remaining instances are split into test and development sets with a 7:3 ratio. For experiments with multiple examples, we sample 100 instances instead of 10. We use \u201c\u2192\u201d as the separate token similar to previous works. We tried other tokens but no significant difference. All the experiments are reported over 5 random seeds. The inference mechanism with state vector we describe in \u00a74.1 has a key hyper-parameter (i.e.the layer L). Previous studies [Hendel et al., 2023] have shown that the choice of L has an influence on performance. We find the best layer for different tasks via the accuracy of the development set. For the inner optimization in \u00a74.2, we choose the last seven state vectors to optimize. This is because the early state vectors yield subpar performance, primarily due to limitations in the available examples. For the momentum optimization, we choose 0.5 as the retention rate for historical momentum from the options of 0.25, 0.5 and 0.75. We run all the experiments on a single NVIDIA A100 80G GPUs. Each of our experiments consumes between 10 minutes to 8 hours of GPU time, depending on the dataset.\n# B More Details about Baseline\nIn this section, we present an in-depth and comprehensive analysis of two baselines (i.e. task vector [Hendel et al., 2023] and function vector [Todd et al., 2023]). Furthermore, we offer a more nuanced comparison with our proposed state vector, highlighting the distinct differences and advantages of our approach. The task vector is designed to extract the ICL function from a specific layer\u2019s hidden state within the transformer model. This is achieved by directly replacing the corresponding hidden state during inference for intervention. On the other hand, Todd et al. [2023] first extracts the ICL function from the output activations across all attention heads in all transformer layers. These activations are then prioritized based on their causal effect, quantified by the variance in the model\u2019s output space with or without individual activation interventions. The mathematical average of the top 10 causal effect activations is the function vector, which is subsequently added to the hidden state of a specific layer during the inference stage. In contrast to these methods, our approach for state vector extraction focuses on procuring the ICL procession state from the output activations of the attention heads within the first L layers. During inference, we replace the corresponding activations with optimized ones. While functionally equivalent to the forward process of the task vector when disregarding state vector optimization (i.e., the vanilla state vector), our approach offers enhanced mechanical explainability. This is attributable to its motivation from the dual form of in-context learning and gradient decay, as explicated in previous work [Dai et al., 2023, Natan et al., 2023]. Furthermore, inspired by the dual form, we focus on the further optimization process. On the other hand, unlike the function vector which extracts activations based on the causal effects resulting from individual interventions, our method is rooted in the underlying mechanisms of ICL. This strategy not only improves mechanical explainability but also demonstrates greater performance as evidenced by extensive experiments. Experiments also show notably poor performance of the function vector on certain knowledge-based datasets, such as Person-Occupation.\n# C More Details about Datasets\nHere, we describe in detail the tasks that we use to evaluate the state vectors\n\u2022 Antonym [Nguyen et al., 2017] contains 2398 word pairs that are antonyms of each other (e.g. \u201cmassive\u201d \u2192\u201ctiny\u201d). We apply the dataset processed version from the function vector [Todd et al., 2023]. They filter the word pairs where both words can be tokenized as a single token. \u2022 Capitalize [Todd et al., 2023] contains 813 word pairs that capitalize the first letter of the given input word (e.g. \u201cplan\u201d \u2192\u201cPlan\u201d). \u2022 Present-Past [Todd et al., 2023] contains 293 word pairs, where simple past tense verbs are output when given simple present tense verbs (e.g. \u201cadapt\u201d \u2192\u201cadapted\u201d).\n\u2022 Antonym [Nguyen et al., 2017] contains 2398 word pairs that are antonyms of each other (e.g. \u201cmassive\u201d \u2192\u201ctiny\u201d). We apply the dataset processed version from the function vector [Todd et al., 2023]. They filter the word pairs where both words can be tokenized as a single token. \u2022 Capitalize [Todd et al., 2023] contains 813 word pairs that capitalize the first letter of the given input word (e.g. \u201cplan\u201d \u2192\u201cPlan\u201d). \u2022 Present-Past [Todd et al., 2023] contains 293 word pairs, where simple past tense verbs are output when given simple present tense verbs (e.g. \u201cadapt\u201d \u2192\u201cadapted\u201d).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98ef/98ef3f6e-64b1-4093-8d2c-6cffc748cd89.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Time efficiency analysis of Llama-2-7B and GPT-J-6B. Inn denotes our state vector with inner optimization. Mom denotes our state vector with momentum optimization</div>\n\u2022 Singular-Plural [Todd et al., 2023] contains 205 word pairs, where the plural form of a given singular word (e.g., \u201cwallet\u201d \u2192\u201cwallets\u201d). \u2022 English-French [Lample et al., 2018] contains 4698 pairs of words, which consists of a word in English and its translation into French (e.g., \u201ccircle\u201d \u2192\u201ccercle\u201d). We apply the processed version from the function vector[Todd et al., 2023]. \u2022 Country-Capital [Todd et al., 2023] contains 197 instances, which output the name of the capital city of the given country (e.g. \u201cLuanda\u201d \u2192\u201cAngola\u201d). \u2022 AG News [Zhang et al., 2015] contains 7600 instances. Each instance contains the news headlines and the first few sentences of an article as input, and output corresponding labels include Business, Science, Sports, and World. \u2022 Person-Sport [Hernandez et al., 2023] contains 318 instances. Each instance contains the name of a professional athlete and the sport that they play (e.g. \u201cHank Aaron\u201d \u2192 \u201cbasketball\u201d). \u2022 Person-Instrument [Hernandez et al., 2023] contains 510 instances. Each instance contains the name of a professional musician and the instrument they play (e.g. \u201cTom Fletcher\u201d \u2192 \u201cguitar\u201d). \u2022 Person-Occupation [Hernandez et al., 2023] contains 821 instances. Each instance contains the name of a well-known individual and their occupation (e.g. \u201cTom Fletcher\u201d \u2192\u201cguitar\u201d). \u2022 Product-Company [Hernandez et al., 2023] contains 522 instances. Each instance contains the name of a commercial product and the company that sells the product (e.g. \u201cTom Fletcher\u201d \u2192\u201cguitar\u201d). \u2022 Landmark-Country [Hernandez et al., 2023] contains 836 instances. Each instance contains the name of a landmark and the country in which it is located.\n\u2022 Singular-Plural [Todd et al., 2023] contains 205 word pairs, where the plural form of a given singular word (e.g., \u201cwallet\u201d \u2192\u201cwallets\u201d). \u2022 English-French [Lample et al., 2018] contains 4698 pairs of words, which consists of a word in English and its translation into French (e.g., \u201ccircle\u201d \u2192\u201ccercle\u201d). We apply the processed version from the function vector[Todd et al., 2023]. \u2022 Country-Capital [Todd et al., 2023] contains 197 instances, which output the name of the capital city of the given country (e.g. \u201cLuanda\u201d \u2192\u201cAngola\u201d). \u2022 AG News [Zhang et al., 2015] contains 7600 instances. Each instance contains the news headlines and the first few sentences of an article as input, and output corresponding labels include Business, Science, Sports, and World. \u2022 Person-Sport [Hernandez et al., 2023] contains 318 instances. Each instance contains the name of a professional athlete and the sport that they play (e.g. \u201cHank Aaron\u201d \u2192 \u201cbasketball\u201d). \u2022 Person-Instrument [Hernandez et al., 2023] contains 510 instances. Each instance contains the name of a professional musician and the instrument they play (e.g. \u201cTom Fletcher\u201d \u2192 \u201cguitar\u201d). \u2022 Person-Occupation [Hernandez et al., 2023] contains 821 instances. Each instance contains the name of a well-known individual and their occupation (e.g. \u201cTom Fletcher\u201d \u2192\u201cguitar\u201d). \u2022 Product-Company [Hernandez et al., 2023] contains 522 instances. Each instance contains the name of a commercial product and the company that sells the product (e.g. \u201cTom Fletcher\u201d \u2192\u201cguitar\u201d). \u2022 Landmark-Country [Hernandez et al., 2023] contains 836 instances. Each instance contains the name of a landmark and the country in which it is located.\n# D Efficiency Analysis\nIn this section, we present an efficiency analysis of two proposed optimization methods. We evaluate the average inference time using 1000 test data on a single NVIDIA A100 (80G) GPU, covering six main datasets and 10 random seeds per dataset. The results are illustrated in Figure 5. In the zero-shot setting, we compress the ICL function into the state vector which eliminates the need to concatenate demonstrations during inference. As shown in the Figure 5, the proposed inner optimization and momentum optimization, which, while tripling the inference speed, achieve 89% of the regular ICL performance on Llama-2-7B and 78% on GPT-J-6B (see Table 1 in the paper). In the few-shot setting, the proposed inner optimization and momentum optimization achieve better results than standard ICL at the cost of a minimal loss in inference speed (e.g., 99% and 96%). Moreover, our method is orthogonal to attention speedup techniques, such as flash attention [Dao et al., 2022] and page attention [Kwon et al., 2023]. Therefore, our approach can also benefit from the achievements of these works and achieve further efficiency improvement. We leave the exploration of alternative enhancement as future work.\nPrompt\nLlama-2\n+SV\nWhat instrument did X play?\n8.7\u00b1 0.7\n67.3\u00b1 2.8\nCan you tell me which musical instrument was played by X?\n25.1\u00b1 0.7\n69.0\u00b1 4.3\nWhat was the primary instrument of X in their music career?\n17.3\u00b1 1.4\n70.3\u00b1 2.9\nEnglish-French\nPrompt\nWhat is the meaning of biography?\nLlama-2\nA written account of someone\u2019s life.\n+ state vector\nIt is biographie.\nAntonym\nPrompt\nWhen I think of upright, I think of\nLlama-2\nI think of a person who is standing up\nfor what they believe in.\n+ state vector\nI think of down.\nTable 4: Natural prompt cases with momentum optimized state vector on Antonym task and English-French task.\n# E Natural Text Completions\nIn this study, we evaluate the effectiveness of the momentum optimized state vector on natural text completions. Given a natural text template, we instruct the model to greedily generate 5 tokens with or without intervention in the zero-shot setting. We use exact match accuracy as the metric. Table 3 shows the result of natural text completions on Llama-2. The performance boosts observed with the momentum-optimized state vector on the separate tokens indicate that it can guide the model to generate answers correctly. We include more examples of natural text completions in the Appendix.\n# F Case Study\nIn this section, we present a case study shown in Table 4, to demonstrate the efficacy of the momentumoptimized state vector in natural text completions. Consider the query: \u201cWhat is the meaning of biography?\u201d, The vanilla Llama-2 model would directly answer this question. However, when influenced by an English-French state vector, Llama-2 changes its response, translating the question into French instead. Similarly, when presented with the sentence \u201cWhen I think of upright, I think of\u201d. Influenced by an Antonym state vector, Llama-2 completes the sentence with an anonymous pattern. These instances exemplify the model learning the ICL function stored in the momentum optimized state vector, enabling it to generate context relevant to the specified task.\n# G Full Result\nIn this section, we provide the additional result with llama-2-7B GPT-J model. We first present the main result of optimization on the other six tasks except the main result, and the average performance across all tasks. As shown in Table 5, our inner optimization and momentum optimization effectively enhance the state vector. Moreover, we provide the result of state vector aggregation on two additional datasets. As shown in Figure 6, the trends of both D$C and average aggregation follow a similar pattern to the main result shown in Figure 2 as the number of examples increases, illustrating the effectiveness of our aggregation methods.\nIn this section, we provide the additional result with llama-2-7B GPT-J model. We first present the main result of optimization on the other six tasks except the main result, and the average performance across all tasks. As shown in Table 5, our inner optimization and momentum optimization effectively enhance the state vector.\nModel\nMethod\nCapitalize\nCountry-Capital\nPresent-Past\nSingular-Plural\nPerson-Sport\nAG News\nAverage (All)\nLlama-2\nZero-shot\nRegular\n0.0\u00b1 0.0\n0.0\u00b1 0.0\n0.0\u00b1 0.0\n0.0\u00b1 0.0\n0.0\u00b1 0.0\n0.0\u00b1 0.0\n0.2\nFunction vector\n98.6\u00b1 0.4\n67.4\u00b1 20.7\n80.2\u00b1 4.5\n94.2\u00b1 0.6\n1.4\u00b1 0.5\n57.7\u00b1 0.9\n44.7\nTask vector\n92.9\u00b16.5\n92.8\u00b12.8\n95.2\u00b11.7\n95.3\u00b11.9\n86.9\u00b14.5\n47.8\u00b11.3\n69.4\nState vector (inn.)\n99.6\u00b10.4\n94.0\u00b11.3\n96.5\u00b11.2\n97.1\u00b11.0\n89.7\u00b13.2\n52.0\u00b15.5\n76.0\nState vector (mom.)\n99.1\u00b10.3\n94.5\u00b10.7\n96.5\u00b10.7\n96.6\u00b11.0\n88.1\u00b12.6\n50.0\u00b18.3\n76.3\nFew-shot\nICL baseline\n99.9\u00b10.1\n95.2\u00b11.0\n98.3\u00b10.6\n98.5\u00b10.1\n94.8\u00b10.2\n76.0\u00b15.7\n83.1\nFunction vector\n99.7\u00b1 0.1\n82.2\u00b1 3.8\n94.6\u00b1 1.7\n97.3\u00b1 0.7\n88.4\u00b1 1.9\n80.7\u00b14.6\n78.1\nTask vector\n98.0\u00b11.0\n92.9\u00b13.4\n98.2\u00b10.5\n98.5\u00b11.3\n95.4\u00b10.4\n64.3\u00b18.4\n81.5\nState vector (inn.)\n99.7\u00b10.1\n94.4\u00b11.3\n98.3\u00b10.6\n98.5\u00b10.4\n95.2\u00b10.2\n76.0\u00b18.5\n83.3\nState vector (mom.)\n99.3\u00b10.1\n94.9\u00b10.7\n98.3\u00b10.6\n98.8\u00b10.3\n95.7\u00b10.2\n76.3\u00b15.9\n83.8\nGPT-J\nZero-shot\nRegular\n0.3\u00b1 0.1\n1.8\u00b1 1.7\n19.4\u00b1 2.1\n22.7\u00b1 2.9\n0.0\u00b1 0.0\n0.0\u00b1 0.0\n5.2\nFunction vector\n66.3\u00b1 8.4\n57.0\u00b1 9.9\n63.1\u00b1 2.1\n69.3\u00b1 2.1\n0.8\u00b1 1.1\n46.4\u00b1 4.5\n37.4\nTask vector\n51.0\u00b14.7\n31.6\u00b14.8\n37.0\u00b15.3\n61.6\u00b11.2\n46.4\u00b14.0\n55.0\u00b13.7\n41.4\nState vector (inn.)\n58.2\u00b11.3\n45.5\u00b18.3\n47.3\u00b12.0\n61.9\u00b10.7\n51.7\u00b11.8\n59.7\u00b15.4\n47.8\nState vector (mom.)\n58.6\u00b10.8\n52.9\u00b16.1\n45.9\u00b10.2\n62.5\u00b10.7\n51.4\u00b11.4\n61.3\u00b14.8\n49.7\nFew-shot\nICL regular\n99.3\u00b10.3\n88.2\u00b13.4\n96.9\u00b10.9\n99.3\u00b10.5\n82.4\u00b13.5\n76.3\u00b11.7\n73.1\nFunction vector\n98.6\u00b1 0.6\n78.6\u00b1 5.1\n90.8\u00b1 1.3\n95.9\u00b1 0.9\n81.6\u00b1 1.4\n72.7\u00b13.2\n70.6\nTask vector\n99.3\u00b10.3\n89.8\u00b12.8\n97.3\u00b11.0\n99.3\u00b10.5\n83.3\u00b13.6\n63.3\u00b18.7\n71.7\nState vector (inn.)\n99.4\u00b10.3\n89.2\u00b13.6\n97.3\u00b10.8\n99.3\u00b10.5\n83.8\u00b13.5\n75.7\u00b11.2\n73.6\nState vector (mom.)\n99.4\u00b10.2\n90.1\u00b13.5\n97.6\u00b10.9\n99.4\u00b10.3\n83.7\u00b13.0\n78.0\u00b12.2\n74.4\nTable 5: Performance of state vector optimization across other six tasks and average performance of all task. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b077/b0779f44-489a-403a-b7ab-3c1dd8563c83.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ation (b) Llama-2 Product-Company</div>\n<div style=\"text-align: center;\">son-Occupation (b) Llama-2 Product-Company</div>\nFigure 6: Performance of aggregation across number of examples. Avg. denotes the average aggregation baseline and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and the Y axis represents the accuracy.\n# H Result on Larger Model\nIn this section, we provide the optimization and aggregation results on the larger model. Here we choose Llama-2-13B as its memory requirements suit our hardware conditions. We present the result of the optimization method on three representative datasets shown in Table 6, and the result of the aggregation method on four representative datasets shown in Figure 7. The result shows that our inner and momentum optimization and D&C aggregation method could also benefit the state vector on the larger model setting.\n# I Qualitative Study\nIn Figure 8, we present a Principal Component Analysis (PCA) visualization of the original state vector in GPT-J, applied to both the Antonym task and the English-French translation task. Note that the cluster distributions observed in GPT-J closely mirror those of Llama-2. This similarity indicates a consistent and progressive enhancement in the model capacity, as originally identified in Llama-2 in \u00a76.3, which is also shown on GPT-J. Such findings demonstrate the broad applicability and generalizability of our momentum optimization approach across different models.\n# J Robustness Analysis\nIn this appendix, we examine the robustness of the state vector with inner optimization. Specifically, we evaluate the task vector and the inner optimized state vector on the Llama-2 dataset, focusing on three tasks. We measure and report the performance standard deviation using 100\n<div style=\"text-align: center;\">(d) GPT-J Product-Company</div>\nModel\nMethod\nAntonym\nEnglish-French\nPerson-Instrument\nAverage\nLlama-2-13B\nZero-shot\nRegular\n1.2\u00b1 0.7\n0.2\u00b1 0.2\n0.0\u00b1 0.0\n0.5\nFunction vector\n47.1\u00b1 1.6\n23.2\u00b14.3\n0.1\u00b1 0.1\n23.5\nTask vector\n46.0\u00b1 2.4\n43.1\u00b17.2\n58.2\u00b16.3\n49.1\nState vector (inn.)\n47.0\u00b1 1.2\n50.5\u00b11.9\n66.6\u00b13.1\n54.7\nState vector (mom.)\n47.9\u00b1 1.1\n55.9\u00b13.4\n68.5\u00b12.0\n57.4\nFew-shot\nICL baseline\n67.0\u00b1 0.1\n74.5\u00b11.3\n75.0\u00b10.2\n72.2\nFunction vector\n65.7\u00b1 1.7\n75.2\u00b12.6\n72.2\u00b10.4\n71.3\nTask vector\n64.8\u00b1 1.2\n70.5\u00b13.5\n70.6\u00b13.1\n68.6\nState vector (inn.)\n65.5\u00b1 0.8\n75.8\u00b11.6\n77.0\u00b11.3\n72.8\nState vector (mom.)\n65.9\u00b1 0.7\n75.6\u00b10.4\n78.6\u00b11.1\n73.4\nTable 6: Performance of state vector optimization across three tasks on llama-2-13B. The best results in the zero shot setting are in underline and the best results in the few shot setting are in bold. The result of basic state vector is mathematically equivalent to task vector.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f216/f216657c-9db4-449f-b105-d06a58708c02.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Performance of aggregation on Llama-2-13B across number of examples. Avg. denotes the average aggregation baseline and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and the Y axis represents the accuracy.</div>\nFigure 7: Performance of aggregation on Llama-2-13B across number of examples. Avg. denotes the average aggregation baseline and D&C. denotes the divide-and-conquer aggregation. The X axis represents the number of examples, and the Y axis represents the accuracy.\ndiverse demonstrations or dummy queries. As illustrated in Figure 9, our analysis yields three ke observations:\n\u2022 The task vector and state vector exhibit greater sensitivity to dummy queries than to demonstrations. This finding suggests that dummy queries have a greater impact on performance compared to demonstrations, underscoring the importance of reducing the noise from dummy queries to enhance state vector performance. \u2022 In the few-shot setting, both the task vector and the state vector (inn.) indicate significantly greater robustness compared to their performance in the zero-shot setting. There is a noticeable reduction in the standard deviation across diverse demonstrations or dummy queries when applying demonstrations during ICL inference. This improvement may be attributed to the richer ICL function information provided by demonstrations, which in turn bolsters performance stability. \u2022 Compared to the task vector, our inner optimized state vector shows markedly enhanced robustness to the variations in demonstrations and dummy queries, in both zero-shot and few-shot settings. This highlights the effectiveness of our proposed inner optimization in improving the robustness of the state vector.\n# K Limitation\nThe definition of state vectors is contingent upon specific assumptions and lacks a rigorous theoretical foundation, which may impact its generalizability and reliability across different NLP tasks. Additionally, the experiments were conducted on a limited scale with moderate-sized models and datasets. These constraints may affect the applicability of the results to larger models or more complex datasets. Further research will explore these aspects to establish a more robust validation of the proposed methods.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f8e/6f8e59f6-7c0a-47d4-9616-33f849b2b26e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Antonym</div>\nFigure 8: The 2D PCA visualization of the state vector in the Antonym task and English-French task of GPT-J where each color represents the state vector corresponding to examples occupying specific positions in the demonstration and the outlier is of the first order.\nFigure 8: The 2D PCA visualization of the state vector in the Antonym task and English-French task of GPT-J, where each color represents the state vector corresponding to examples occupying specific positions in the demonstration and the outlier is of the first order.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd59/fd597e57-b697-416e-b6ca-1f1d61a0af28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Standard deviation of performance on Llama-2 across three datasets</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f84b/f84b3c29-8b06-4179-b074-1a5f9da317f6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) English-French</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the gap in understanding the operational mechanisms and optimization strategies of compressed vectors derived from In-Context Learning (ICL) in large language models (LLMs). Previous methods have not thoroughly explored how these vectors can be optimized, necessitating a new approach to enhance their effectiveness.",
        "problem": {
            "definition": "The core issue is the lack of understanding regarding how compressed vectors from ICL function and how they can be optimized to improve performance in various NLP tasks.",
            "key obstacle": "Existing methods do not effectively utilize the potential of these compressed vectors due to inadequate optimization strategies, which limits their application in practical scenarios."
        },
        "idea": {
            "intuition": "The idea stems from the observation that compressed vectors can be viewed as parameters trained through gradient descent, which inspired the formulation of a state vector that encapsulates the processing state of ICL.",
            "opinion": "The proposed method introduces inner and momentum optimization strategies to refine the state vector progressively, enhancing its performance during test-time adaptation.",
            "innovation": "The primary innovation is the introduction of the state vector and the novel optimization methods that leverage insights from gradient descent and model soup principles, differentiating this approach from existing methods."
        },
        "method": {
            "method name": "Inner and Momentum Optimization for State Vector",
            "method abbreviation": "IMOSV",
            "method definition": "This method involves extracting the state vector from attention activations and applying inner and momentum optimization techniques to enhance its performance in ICL tasks.",
            "method description": "The method optimizes the state vector through two distinct strategies, aiming to improve its efficacy in processing multiple examples during inference.",
            "method steps": [
                "Extract the state vector from the attention heads corresponding to the final separate token.",
                "Apply inner optimization to refine the state vector by averaging state vectors from multiple examples.",
                "Implement momentum optimization to progressively enhance the state vector based on influences from adjacent examples.",
                "Utilize the optimized state vector to intervene in the model during the forward inference pass."
            ],
            "principle": "The effectiveness of this method is grounded in the observation that the state vector can encapsulate learned information akin to parameters trained via gradient descent, allowing for improved performance in ICL tasks."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized Llama-2 and GPT-J models across various NLP tasks, including zero-shot and few-shot settings. The evaluation involved comparing the performance of the proposed state vector optimization against baseline methods.",
            "evaluation method": "Performance was measured through accuracy metrics on several datasets, with results analyzed to assess the impact of the proposed optimizations on the state vector."
        },
        "conclusion": "The experimental results demonstrate that the proposed inner and momentum optimization methods significantly enhance the efficacy of the state vector, achieving state-of-the-art performance on diverse NLP tasks and providing insights for future research in ICL.",
        "discussion": {
            "advantage": "The proposed approach offers improved robustness and efficiency in processing large numbers of examples, effectively circumventing context-length limitations of LLMs.",
            "limitation": "The method's definition relies on specific assumptions that may limit its generalizability across various NLP tasks, and further validation is needed for larger models.",
            "future work": "Future research will explore extending these methods to more complex ICL scenarios and larger models, as well as investigating the theoretical foundations of the state vector."
        },
        "other info": {
            "code repository": "https://github.com/HITsz-TMG/ICL-State-Vector",
            "datasets used": [
                "Antonym",
                "English-French",
                "Country-Capital",
                "AG News",
                "Person-Sport",
                "Product-Company"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the gap in understanding the operational mechanisms and optimization strategies of compressed vectors derived from In-Context Learning (ICL) in large language models (LLMs)."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method introduces inner and momentum optimization strategies to refine the state vector progressively, enhancing its performance during test-time adaptation."
        },
        {
            "section number": "3.1",
            "key information": "The method optimizes the state vector through two distinct strategies, aiming to improve its efficacy in processing multiple examples during inference."
        },
        {
            "section number": "3.2",
            "key information": "The effectiveness of the inner and momentum optimization methods is grounded in the observation that the state vector can encapsulate learned information akin to parameters trained via gradient descent."
        },
        {
            "section number": "6.1",
            "key information": "The proposed approach offers improved robustness and efficiency in processing large numbers of examples, effectively circumventing context-length limitations of LLMs."
        },
        {
            "section number": "6.2",
            "key information": "The method's definition relies on specific assumptions that may limit its generalizability across various NLP tasks, and further validation is needed for larger models."
        }
    ],
    "similarity_score": 0.6933091169601752,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning State Vector with Inner and Momentum Optimization.json"
}