{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.10375",
    "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering",
    "abstract": "Despite the impressive few-shot performance of in-context learning (ICL), it remains a common practice to randomly select examples to serve as the context. In this paper, we advocate self-adaptive in-context learning, a new principle for ICL, in which the self-adaption mechanism is introduced to help each input find an in-context example organization (i.e., selection and permutation) that can derive the correct output, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and a set of novel selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the great potential of selfadaptive ICL as a promising method to close the gap between ICL and finetuning. Our code will be released to facilitate future research.",
    "bib_name": "wu2023selfadaptiveincontextlearninginformation",
    "md_text": "# Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering\nZhiyong Wu\u2666\u2020, Yaoxiang Wang\u2663\u2020\u2217, Jiacheng Ye\u2660\u2020\u2217, Lingpeng Kong\u2660 \u2666Shanghai Artificial Intelligence Laboratory \u2663Xiamen University \u2660The University of Hong Kon {jcye2,lpk}@cs.hku.hk, {wuzhiyong,wangyaoxiang}@pjlab.org.cn,\n# Abstract\nDespite the impressive few-shot performance of in-context learning (ICL), it remains a common practice to randomly select examples to serve as the context. In this paper, we advocate self-adaptive in-context learning, a new principle for ICL, in which the self-adaption mechanism is introduced to help each input find an in-context example organization (i.e., selection and permutation) that can derive the correct output, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and a set of novel selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the great potential of selfadaptive ICL as a promising method to close the gap between ICL and finetuning. Our code will be released to facilitate future research.\n# 1 Introduction\nThe increasing scale of pre-trained language models (PLMs) has brought emergent abilities (Wei et al., 2022) via in-context learning (ICL), where the PLMs learn to do downstream tasks simply by conditioning on a prompt containing a few examples of their kinds (Brown et al., 2020a). Due to its impressive performance, ICL has now emerged as a popular and efficient way of using PLMs. However, ICL is inherently unstable: given different prompts, the performance of ICL on downstream tasks can vary from almost random to comparable with state-of-the-art systems (Zhao et al., 2021; Lu et al., 2022; Gao et al., 2021), depending on the quality of the prompts. The instability of ICL motivates researchers to explore methods that search for high-performing prompts. Note that a prompt within the context of ICL contains two ingredients: some input-output\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8fcd/8fcd8cf1-751d-4146-9c39-7041c90cd490.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Corpus-level method (red bar) is highly biased towards majority classes, given 8 in-context examples labeled as 2 5 4 4 4 1 2 3.</div>\npairs (i.e., in-context examples) and a template that wraps these examples into a natural language instruction. Extensive research has been carried out on searching for a better template (Gao et al., 2021; Shin et al., 2020; Sorensen et al., 2022; Deng et al., 2022). In contrast, very few efforts have been spent on searching for the best in-context example organization. 1 Recent work, however, has pointed out that the organization of in-context examples can have a significant influence on ICL\u2019s performance (Lu et al., 2022; Liu et al., 2022; Rubin et al., 2022). This paper fills this gap by proposing a framework for in-context example searching and ranking. While one can also trivially extend template searching methods to conduct in-context example searching, these methods operate at the corpuslevel. They first construct a small candidate template set using PLMs (Gao et al., 2021; Shin et al., 2020), data mining algorithms (Jiang et al., 2020), or by hands (Sorensen et al., 2022). After that, each candidate will be applied to the whole validation set for inference. According to validation performance,\nthe best template will be adapted for testing. However, existing solutions have the following problems: (i) Their performance relies heavily on the availability of a large-scale high-quality validation set; (ii) Corpus-level methods can be sub-optimal (see Figure 1) because finding a universal template that suits all testing samples perfectly is unlikely. Such majority bias (Zhao et al., 2021) will significantly hurt user experience in practice and make corpus-level methods less robust. To tackle these issues, we seek to construct a good-performing in-context example organization for each testing sample individually, without access to a validation dataset. This problem, namely self-adaptive in-context learning, is essentially an NP-hard combinatorial optimization problem that cannot be solved within polynomial time. We thus formulate it as a search problem and propose a general two-stage framework to cope with the issue of massive search space. In the first stage, we apply heuristic rules (e.g., nearest neighbors based on semantic similarity) to filter candidate examples. Given a much smaller candidate set, we then apply algorithms to rank different organizations and look for the best-performing one. Our ranking algorithms are theoretically supported by the Minimal Description Length (MDL) principle and can shed light on why certain permutations are better than others. Our contributions are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to formally define the problem of self-adaptive in-context learning and formulate it as a twostage search problem. We propose a general framework to address this problem.\n We achieve state-of-the-art performance using the proposed framework and outrun the previous best-performing methods by a large relative improvement. We also find that instancelevel ICL methods are generally more robust than corpus-level counterparts. Such empirical success shows a great promise of selfadaptive ICL.\n We conduct extensive analysis for selfadaptive ICL and make some exciting findings. For instance, in Section 6.3 we reveal that self-adaptive ICL still has much room for improvement. With better search methods, we might be able to close the gap between ICL and finetuning.\n\u2022 We will open-source the proposed framework to facilitate future research. This unified framework enables researchers to identify important design choices in previous methods and paves the way for further improvements.\n# 2 Related Work\nDespite the surprising zero-shot performance of PLMs, recent works show that ICL can bring the performance to the next level. Augmenting PLMs with ICL achieves SOTA results on a wide range of NLP tasks, ranging from question answering (Joshi et al., 2017), information retrieval (Tay et al., 2022), math word problem (Cobbe et al., 2021), commonsense reasoning (Geva et al., 2021), and fact checking (Rae et al., 2021) etc. The instability of ICL, however, has encouraged researchers to explore methods that search for robust and high-performing prompts. These methods can be categorized as follows based on the target of searching/optimization: Template search focuses on searching for the template that can guide PLM\u2019s behavior and steer its best performance. Great advances have been made in template searching using various methods: PLMs (Gao et al., 2021), heuristic rules (Jiang et al., 2020; Shin et al., 2020; Prasad et al., 2022; Xu et al., 2022), reinforcement learning (Deng et al., 2022), genetic algorithms (Kumar and Talukdar, 2021), or by hands (Sorensen et al., 2022; Zhao et al., 2021). Nonetheless, all these methods require a high-quality validation set to do prompt selection or optimization. Unlike them, our framework does not require a validation set. When the validation set is not available, researchers propose to search prompts using entropy (Lu et al., 2022) or mutual information (Sorensen et al., 2022). It\u2019s worth mentioning that these two works and all aforementioned methods search at the corpus-level: they pick the bestperforming template with or without a validation set and then equally apply this template to all test examples during inference. However, corpus-level methods might be sub-optimal. If we consider the No Free Lunch Theorem, finding one single template that works well for all testing examples is nearly impossible. In-context example search, unlike template search, is rarely explored in the literature despite that they also have a huge impact on ICL performance (Zhao et al., 2021; Lu et al., 2022). Lu et al. (2022) first propose a learning-free corpus-\nlevel method for in-context example search. However, they only consider an impractical setting with only 4 examples and their 24 permutations (4P4 = 4! = 24). Liu et al. (2022) find examples that are semantically similar to a test sample can serve as a good choice for its in-context examples. However, the reason why such a simple heuristic works is unclear. Su et al. (2022) extend this nearest neighbor search and further take the diversity of examples into consideration. Inspired by these methods, recent studies propose to learn to retrieve in-context examples (Rubin et al., 2022).\n# 3 Problem formulation\nGiven a test sample (x, y), the probability of generating the target y using a casual PLM P can be formulated as follows:\np(y|x) = P (V(y)|c, T (x)) ,\n(1)\nwhere T (\u00b7) is the template used to wrap up inputs and c = T (x1), \u00b7 \u00b7 \u00b7 , T (xk) is the context string concatenating k input-output examples. To deal with classification tasks, a verbalizer V(\u00b7) is introduced to map each label/class y to a word/words in P\u2019s vocabulary. Note that in a special scenario when k = 0, ICL degenerates to zero-shot prompting (Ye et al., 2022; Brown et al., 2020b). The goal of self-adaptive ICL is then to find an optimal organization of c \u2208C that can drive the correct y for each input x, and maximize the task performance. We formulate this as a combinatorial optimization problem.\n# 4 Method\nIn this section, we propose a two-stage framework to tackle the problem of self-adaptive ICL.\n# 4.1 Overview\nIn such a combinatorial optimization problem, an exhaustive search is not tractable. So we need specialized algorithms that can quickly rule out large parts of the search space. We present an overview of our selection-then-rank framework here: We first use a selection module to reduce the search space. One straightforward choice for pre-ranking would be to use nearest-neighbor algorithms to select examples that are semantically similar to test samples. The results are then fed into the ranking module, which picks the best combination and permutation according to information-theoretic-driven criteria.\n# 4.2 Selection\nThe goal of selection module is to filter out large parts of \u201cless useful\u201d examples and construct a small candidate set to reduce the search space. We present various selection methods below.\nTopK Liu et al. (2022) and Gao et al. (2021) observe that context examples that are closer to the test sample in the embedding space consistently give rise to stronger performance. This observation leads to the TopK method which uses the nearest neighbors of a given test sample as the corresponding in-context examples.\nVoteK Although ICL was originally proposed for few-shot settings, they often require a large example set to achieve good performance. VoteK (Su et al., 2022) proposes to alleviate this problem by selecting diverse yet representative examples. Intuitively, VoteK is built upon TopK, but it increases diversity by penalizing examples similar to those already selected.\nDPP Inspired by VoteK, we also experimented with the determinantal point process (DPP) based method, which is proposed for set selection problems where diversity is preferred. We refer readers to Kulesza and Taskar (2011) for details of DPP.\n# 4.3 Ranking\nWith the candidates returned by the selection module, the goal of the ranking module is to determine the best organization among candidates. Our ranking algorithm is inspired by the compression viewpoint of Solomonoff\u2019s general theory of inference (Solomonoff, 1964) and Minimum Description Length (MDL) principle (Gr\u00fcnwald, 2007) from information theory. Both Solomonoff\u2019s theory and the MDL formalize Occam\u2019s razor and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. These theories have led to advances in VAE (Kingma and Welling, 2013), and information bottleneck methods (Tishby and Zaslavsky, 2015). Inspired by the compression viewpoint of learning, we recast the problem of self-adaptive in-context learning into a similar paradigm. We assume that a good organization of in-context examples is the organization that is good at losslessly compressing testing samples. This allows us to give a clear optimization objective when searching for the best\n(2)\nwhere each c represents one possible organization of examples. L\u03b8(y|c, x) is the codelength required to compress and transmit testing label y given the organization c and testing input x. L(\u03b8) is the codelength required to describe the model, which can be ignored during ranking since all organizations use the same model without parameter updating. The codelength required for data transmission can be calculated using Shannon-Huffman code:\n(3)\nHowever, since we don\u2019t have access to testing label y when ranking, the exact computation of p(y|c, x) is impossible. To tackle this problem, we propose to compute the expectation of codelength as the surrogate:\n(4)\nwhere q(yi|Y ) is the prior of yi among all possible labels Y . A natural design choice of the prior is a uniform distribution, given that most datasets are label-balanced. However, since we focus on instance-level selection rather than corpus level, the likelihood p(yi|Y ) can vary significantly given different samples. We thus model this term using p(yi|c, x), leading to our final objective:\nc\u2217= arg min c\u2208C \u2212Ep(yi|c,x)log2 p(yi|c, x). (\n(5)\nNow that we have an interpretable metric for ranking, we can brute-force all possible permutations to obtain the optimal ranking result. Although we have significantly reduced the search space using the selection module, enumerating all organizations is still infeasible. For instance, if we want to search for the best organization that contains 8 examples, even a small candidate set of 10 examples can result in 1.8 million choices (A8 10). At the current stage, we randomly sample 10 permutations for ranking. We leave it as an interesting future work to investigate how to approximate the optimal ranking better.\n# 4.4 Interpretation of L\u03b8(y|c, x)\nExcept for the compression viewpoint, we offer some other interpretations of our method here.\nConnection to entropy When we use model confidence p(yi|c, x) as the estimation of q(yi|Y ), Eq 4 is basically calculating the entropy. Minimizing entropy is equivalent to searching for in-context examples that will lead to a skewed probability distribution. In other words, we are searching for in-context examples are will make PLMs very confident about its answer. This motivation is exactly opposite to the Local Entropy(LocalE) metric proposed by Lu et al. (2022), where they search by maximizing the entropy.\npaper, we focus on instance level ICL and assume no validation set is available. However, when we have a validation set to directly compute p(y|c, x), Eq 3 is exactly the categorical cross-entropy loss. Hence, trying to minimize the description length of the outputs is equivalent to minimizing the usual classification loss. This reveals why compression is another viewpoint of learning.\n# Connection to mutual information. Previou\neffort (Blier and Ollivier, 2018) has proved that the compression is limited by the mutual information between inputs and outputs:\nH(y)\u2212Eq[L(y | x)] \u2264H(y)\u2212H(y | x) = I(y; x),\nwhere we assume the inputs and outputs follow the joint distribution q. Based on this finding, any successful compression of the labels is, at the same time, a direct estimation of the mutual information between input and output. This connects our method to Sorensen et al. (2022) that selects templates by maximizing mutual information.\naforementioned connections and differences, our method significantly differs from Lu et al. (2022) and Sorensen et al. (2022) in that we perform instance-level selection without a validation set. Trivial extension of previous methods to our setting is impractical: Lu et al. (2022) requires a validation set to compute the Global Entropy, while the mutual information is always zero on instance-level setting according to Sorensen et al. (2022).\n# 5 Experiments\n# 5 Experiments 5.1 Evaluation details\nWe perform experiments across eight different NLP datasets. Unless otherwise stated, all experiments are conducted using GPT2-XL (1.5B) (Radford et al., 2019). Our method is denoted as\nTopK+MDL, in which we first use TopK to retrieve 30 candidates for each sample and then randomly sample 10 organizations (each with 8 examples) for ranking using MDL. All models and datasets are loaded from HuggingFace Hub. Templates are adopted from Ye et al. (2022); Gao et al. (2021) and detailed in Table 4. We ran all experiments three times with different random seeds and reported the average accuracies. Datasets We consider two sentiment classification datasets (Socher et al., 2013): SST-2 and SST-5, three natural language inference datasets: SNLI (Bowman et al., 2015), MNLI (Williams et al., 2017), and QNLI (Wang et al., 2018), one multi-choice question answering dataset: Commonsense QA (CMS QA) (Talmor et al., 2019), two topic classification datasets: TREC (Hovy et al., 2001) and AgNews (Zhang et al., 2015). Baselines We compare our framework with three groups of baselines: prompting, corpus-level methods, and instance-level methods. Prompting is a special case of ICL without in-context examples. For corpus-level methods, we consider two methods that require a validation set: GlobalIE (Lu et al., 2022) and Random & Validation, which picks 10 random organizations for each dataset and selects the best one according to the validation performance. We also consider validation-free baselines: Mutual Information (MI) (Sorensen et al., 2022) and a Random baseline that randomly initiates one organization for each dataset. For instancelevel methods, we consider TopK+LocalE (Lu et al., 2022), TopK (Liu et al., 2022) and a Random baseline that randomly selects 8 examples for each testing sample. We further add a Majority vote baseline that directly performs majority voting based on 8 examples retrieved by TopK. Evaluation Strategy Due to the restricted test set access of some datasets (MNLI, QNLI, and CMS QA), we hold out a small subset (i.e., 10%) of the training set for validation for corpus-level methods, and report results on the validation set. For PROMPTING and instance-level methods, we directly evaluate them on the original validation set when the test set is not available.\n# 5.2 Main Results\nFrom Table 1, we first observe that ICL methods outperform prompting in most cases. However, we also note that bad in-context organizations (e.g., the random baseline) can hurt performance and\nmake ICL performs even less well than prompting on SST-5. These results stress the importance of correct selection and permutation of in-context examples. We first compare our methods with corpus-level methods. As shown in Table 1, our method shows consistent and clear superiority over corpus-level baselines. This result also validates our conjecture that corpus-level methods can be sub-optimal and self-adaptive in-context examples can significantly improve ICL performance. Remarkably, our method demonstrates a 40% relative improvement against the common practice in ICL (i.e., the Random baseline). Such improvement is encouraging as it shows that despite the surprising performance of ICL in many tasks, there is still a large room for improvement with advanced in-context example searching techniques. Our method still registers decent improvements on most evaluated datasets even when compared with instance-level baselines. Compared with TopK+LocalE, our method makes a 17% relative improvement, this demonstrates the effectiveness of MDL as a ranking method. However, we also notice that TopK is a very competitive baseline to our method. Using semantic search to retrieve examples will result in incontext examples whose input distribution and label are quite similar, or even identical, to the testing sample. This phenomenon leads to our hypothesis about the surprising effectiveness of TopK. First, as pointed out by Xie et al. (2021), ICL can be cast as an implicit Bayesian inference process, where the PLMs implicitly infer a concept when making the prediction. Based on this theoretic finding, we deduce that semantically similar in-context examples improve prediction by providing more evidence for Bayesian inference, especially for topic classification tasks like TREC and AgNews. Second, we conjecture that providing a series of examples with the same label as the testing sample introduces a \u201clearning shortcut\u201d for PLMs and biases the results. We further examine this hypothesis below.\n# 5.3 Impact of label in ICL\nTo investigate the impact labels have on ICL, we calculate bias rate. Given a testing sample (x, y) and its in-context examples, the bias rate represents the percentage of in-context examples whose label is identical to y. As shown in Figure 2(a), the bias rate positively correlates with the performance. We conduct a more fine-grained exploration by corrupting the label space and breaking the input-label\nSST-2\nSST-5\nSNLI\nMNLI\nQNLI\nTrec\nAgNews\nCMS QA\nAVG\nPrompting\n71.38\n29.41\n41.23\n39.19\n50.44\n13.8\n29.75\n39.39\n39.32 (52.99%\u2191)\nCorpus-level\nRandom\n73.68\n23.88\n43.35\n39.43\n53.19\n19.66\n36.92\n52.66\n42.78 (40.41%\u2191)\nRandom & Validation\n87.86\n40.10\n49.27\n43.26\n51.12\n32.67\n52.01\n53.75\n51.25 (17.38%\u2191)\nMI (Sorensen et al., 2022)\n52.86\n35.35\n46.02\n41.32\n50.62\n16.00\n47.29\n52.78\n42.85 (40.63%\u2191)\nGlobalE (Lu et al., 2022)\n87.27\n33.21\n46.99\n40.46\n57.27\n28.53\n52.01\n22.42\n49.75 (20.92%\u2191)\nInstance-level\nRandom\n77.17\n25.65\n43.41\n41.17\n53.09\n18.33\n32.71\n52.93\n43.06 (39.72%\u2191)\nTopK (Liu et al., 2022)\n83.91\n37.01\n57.54\n45.72\n59.72\n40.80\n88.89\n51.51\n58.14 (3.48%\u2191)\nMajority vote\n85.34\n41.58\n52.06\n34.38\n58.02\n51.60\n60.91\n19.57\n50.43 (19.29%\u2191)\nTopK+LocalE (Lu et al., 2022)\n67.12\n31.65\n46.78\n41.51\n52.66\n36.20\n81.88\n53.07\n51.36 (17.17%\u2191)\nOurs (TopK+MDL)\n91.51\n40.27\n58.77\n46.56\n61.43\n42.47\n87.94\n53.15\n60.16\nTable 1: Evaluation results. Numbers in bold indicate the highest accuracy among all methods (except Majority vote). Numbers in the parenthesis represent the relative improvements our method achieved over baselines.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0eac/0eac53ec-6880-45d6-9847-f5eee6f05913.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: (a) Impact of the label in ICL. The bias rate reflects the percentage of in-context examples whose lab is identical to the testing sample. (b) Few-shot results on SST2. (c) Few-shot results on SNLI.</div>\nalignment. We corrupt the labels by exchanging label words between classes, e.g., exchanging label words between positive and negative classes in sentiment classification. As in Figure 2(a), we observe a clear performance drop with corrupted labels, which negatively correlates with the bias rate. These results suggest that in-context examples\u2019 labels could significantly impact ICL performance. Recent debates (Min et al., 2022; Kim et al., 2022) on the effect of label distribution focus on corpus-level ICL, and our findings complement their studies.\n# 6 Analysis\nThe observed benefits of our method raise the natural question of why and how it helps and whether the same performance improvements can be transferred to other PLMs or prompts. In this section, we conduct comprehensive experiments and analyses to understand the strength and weaknesses of our method.\n# 6.1 When a large set of annotated examples is not available\nDespite the surprising performance of ICL, a largescale training set is not always available for retrieval in practice. To address this concern, we conduct experiments under the few-shot setting. We randomly sample 16, 32, 64, 128, 256, 512, and 1024 examples as the candidates for searching. We select two representative tasks (SST2 and SNLI) for evaluation and run each experiment three times with different random seeds. As shown in Figure 2(b) and 2(c), our method consistently outperforms the strong baseline TopK as in the full-data setting. This demonstrated the general applicability of our method in both full-data and few-shot scenarios. We also observe that the performance steadily increases with the growing number of annotated examples.\n# 6.2 Impact of selection methods\nWe conduct most experiments using the popular TopK method for candidate example selection. Here we evaluate three other alternatives: random,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee34/ee34090e-91c1-436a-bc7e-c6f84122346a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: (a) impact of different selection methods. (b) the accuracy of our ranking method. (c) impact of window size(number of permutations to be ranked). (d) impact of the number of in-context examples. (e,f) impact of model scales on Commonsense QA and SST2.</div>\nDPP and VoteK. Figure 3(a) shows that using TopK for example selection outperforms all other alternatives on average. However, we also observe that the superiority of TopK is mainly in simple classification tasks with limited label space. On multi-choice tasks like Commonsense QA, all three alternatives outperform TopK (right side of Figure 3(a)). Note that although multi-choice tasks are also classification tasks, they have a huge label space like NLG tasks. The frustration of TopK on multi-choice tasks suggests that the popular TopK method does not work well for tasks with large label space and searching for better selection methods holds immense prospects, and therefore remains an interesting field of further research.\n# 6.3 Accuracy of ranking method\nIn our ranking module, we randomly select 10 different organizations for each testing sample and use MDL to select the best-performing one in an unsupervised manner. Despite the superior performance of MDL, the accuracy of using MDL for in-context example ranking has not been discussed. To understand the ranking accuracy of MDL, we assume a perfect ranking method oracle, which can always select the organization that leads to correct prediction if there is any. In the implementation, we first obtain predictions for all 10 organizations.\nDataset\nTopK\nTopK+MDL\nTopK+LocalE\nRandom\nSST-2\n0.6861(83.91)\n0.6810(91.51)\n0.6928(67.12)\n0.6918(77.17)\nSNLI\n1.0981(57.54)\n1.0929(58.77)\n1.0983(46.78)\n1.0974(43.41)\nCMS QA\n4.9883(51.51)\n4.9371(53.15)\n4.9692(53.07)\n4.9629(52.93)\nTrec\n5.5618(40.80)\n5.4496(42.47)\n5.7434(36.20)\n5.7859(18.33)\nIf at least one prediction matches the ground truth, we consider this testing example solvable by oracle. As shown in Figure 3(b), there are significant performance gaps between oracle and TopK+MDL. Although such oracle performance only exists theoretically, it\u2019s still encouraging to see the enormous promise of ICL: with better selection and ranking methods (e.g., supervised methods (Rubin et al., 2022)), we might be able to bridge the performance gap between ICL and finetuning. We investigate the correlation between MDL and accuracy by selecting four representative datasets and reporting the MDL of each method. As shown in Table 2, a smaller MDL generally indicates a higher accuracy (in the brackets). This validates the effectiveness of MDL as the criterion for incontext example searching. It\u2019s also interesting to see that tasks with lower MDL are generally easier to learn (as explained in \u00a7 4.3), thus ICL has a better performance.\n6.4 Impact of hyperparameter In this subsection, we investigate how different hyperparameters affect our performance.\nIn this subsection, we investigate how different hyperparameters affect our performance.\nIncreasing the window size of our method can steadily boost performance, by trading efficiency for better performance. We vary window size (i.e., number of organizations to be ranked per sample) from 2 to 50, and report the average accuracy. As shown in Figure 3(c), the performance steadily increases with the window size. We even observe gains when the window size is two. In particular, on tasks with short input lengths like SST2, using a window size of 2 already shows a clear gain (+3.19 in accuracy) over TopK. However, the improvement is achieved by sacrificing efficiency, i.e., window size hits 50 means performing forward passing for the test set 50 times. Together with findings above, we conclude that we must keep improving the accuracy of ranking methods to achieve a better efficiency-effectiveness trade-off.\nboosts accuracy for most tasks. We gradually increase the number of in-context examples (denoted as N) from 0 (prompting) to 32. From Figure 3(d), we see that increasing N consistently improves the performance on average. We also note that the random baseline reaches the performance plateau from N = 8. Such contradictions suggest that when analyzing the impact of N, the organization of examples is critical. Sometimes we find increasing N not helpful because we are not using the \u201cright\u201d organization. Our results raise an interesting question for future research: can we achieve finetuning-level performance by using thousands or even more examples as context?\nLarger model size does not guarantee better performance, but our method can bring consistent improvements over strong baselines. We use OPT and vary the model size from 350M to 175B. We have a mixed observation that blindly applying huge models does not always result in the best performance. For simple tasks like SST2 (see Figure 3(f)), we reach the performance plateau after 1.3B. And for SNLI, a 30B OPT even outperforms the 175B counterpart. Large models are powerful when dealing with complex tasks like Commonsense QA. From Figure 3(e), we can see steady and significant improvement whenever we scale up the model size. In addition, our method brings consistent improvements over baselines regardless of model sizes on all tasks evaluated.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/71d5/71d59409-7f97-4bc6-9db5-348677b15990.png\" style=\"width: 50%;\"></div>\nFigure 4: The average performance of TopK and our method on different PLMs.\n# 6.5 Robustness\nGenerability across different PLMs. We explore how our method generalizes between different PLMs. We average our results across datasets and present the results in Figure 4. On four different PLMs tested, our method consistently and significantly outperforms the strong TopK baseline. Overall, we have observed that our method is robust across various datasets and PLMs.\n# Generability across different prompts. As sen\nsitivity to prompt engineering is a key weakness of ICL, we evaluate the robustness of our method given different templates. We select two representative tasks (i.e., SST2 and SNLI) to conduct experiments, each with three different templates. As shown in Figure 5, our method is robust given different prompting templates. But still, the differences in prompting templates cause large variances in performance. The findings here motivate a line of research that simultaneously searches for the best template and in-context organization, which is rarely explored in the literature.\n# 7 Conclusion\nThis paper proposes a new paradigm for ICL: selfadaptive ICL. Unlike existing efforts that universally use one single example organization on all testing samples, we propose a general two-stage select-then-rank framework to search in-context examples at the instance-level. We instantiate this framework with an information-theory-driven ranking algorithm. Empirical results suggest that selfadaptive in-context learning can significantly outperform the common practice in ICL by a large margin. We reveal the great potential of self-adaptive in-context learning and point out several interesting research problems in method analysis.\nDespite the demonstrated effectiveness of selfadaptive ICL, this new paradigm suffers from the following limitations. (I) As we discussed in \u00a7 6.4, due to the large search space, we need to trade efficiency for effectiveness. So how to balance the efficiency-effectiveness trade-off is an important decision choice to make when deploying selfadaptive ICL methods. (II) As shown in \u00a7 6.1, the gains of our method shrink when the size of the retrieval set gets smaller. To maximize performance, we require a high-quality retrieval set, which might not always be available when dealing with unseen tasks in practice. We also note that both limitations can be alleviated with better selection and ranking algorithms. The remarkable performance of our method should partially attribute to the powerful TopK selection method, so we also discuss the limitation of TopK here. Despite its popularity, our analysis (\u00a7 6.2) reveals that TopK\u2019s effectiveness is limited to simple NLU tasks with limited label space, and it does not work well with tasks with large or even infinite label space (QA, multi-choice, and NLG). This limitation signals a new direction for ICL research: we need better selection methods to adapt ICL methods to more tasks.\n# 9 Acknowledgement\nYaoxiang, Zhiyong, and Jiacheng participate in coding and discussion. Yaoxiang and Zhiyong conduct the evaluation and analysis. Zhiyong leads the project and writes this manuscript. We want to thank members of Shark-NLP and reviewers for their valuable feedback. This work is partially supported by the National Key R&D Program of China(NO. 2022ZD0160100), and in part by Shanghai Committee of Science and Technology (Grant No. 21DZ1100100).\n# References\nAlex Kulesza and Ben Taskar. 2011. k-dpps: Fixedsize determinantal point processes. In ICML.\nSawan Kumar and Partha Talukdar. 2021. Reordering examples helps during priming-based few-shot learning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4507\u20134518.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222\u2013 4235.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1631\u20131642. ACL.\nRay J Solomonoff. 1964. A formal theory of inductive inference. part i. Information and control, 7(1):1\u2013 22.\nTaylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 819\u2013862.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975.\nDataset\nTask\nData Split\nSST-2\nSentiment Classification\n6920/872/1821/\nSST-5\nSentiment Classification\n8544/1101/2210\nSNLI\nNatural Language Inference\n550152/10000/10000\nMNLI\nNatural Language Inference\n392702/19647/19643\nQNLI\nNatural Language Inference\n104743/5463/5463\nTrec\nTopic Classification\n5452/0/500\nAgNews\nTopic Classification\n120000/0/7600\nCMS QA\nCommonsense Question Answering\n9471/1221/1140\nTable 3: Details of datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5b86/5b86f091-31e7-4866-af03-a8ea8b1df50d.png\" style=\"width: 50%;\"></div>\nFigure 5: Results of TopK and our method on SST2 and SNLI, using different prompts.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 2022. Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. arXiv preprint arXiv:2201.06910. Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. 2022. Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint arXiv:2202.07922. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR.\n# A Datasets\nDataset information is detailed in Table 3.\n# B Impact of hyperparameters\nThe results of adjusting the number of in-context examples and window size are shown in Figure 6 and 7, respectively.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c960/c960de42-c278-4d5c-82bc-e67de8564b69.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6278/6278f762-0e59-4c84-be41-c6ac67fce41d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Evaluation results with different window sizes (number of permutations to be ranked).</div>\n# C Templates\nThe templates used in this paper are detailed in Table 4.\nTask\nPrompt\nClass\nSST-2\nPositive Movie Review: \"<X>\"\nPositive\nNegative Movie Review: \"<X>\"\nNegative\nSST-5\n\"<X>\" It is terrible.\nVery Negative\n\"<X>\" It is bad.\nNegative\n\"<X>\" It is OK.\nNeutral\n\"<X>\" It is good.\nPositive\n\"<X>\" It is great.\nVery Positive\nSNLI & MNLI\n<X1>? Yes, <X2>\nEntailment\n<X1>? Maybe, <X2>\nNeutral\n<X1>? No, <X2>\nContradiction\nQNLI\n<C> Can we know <X>? Yes.\nEntailment\n<C> Can we know <X>? No.\nContradiction\nTREC\n\"<X>\" It is about abbreviation.\nABBR\n\"<X>\" It is about entity.\nENTY\n\"<X>\" It is about description and abstract concept.\nDESC\n\"<X>\" It is about human being.\nHUM\n\"<X>\" It is about location.\nLOC\n\"<X>\" It is about numeric value.\nNUM\nAgNews\n\"<X>\" It is about world.\nWorld\n\"<X>\" It is about sports.\nSports\n\"<X>\" It is about business.\nBusiness\n\"<X>\" It is about science and technology.\nSci/Tech\nCommonsense QA\nAnswer the following question: <X> Answer: <A>.\nA\nAnswer the following question: <X> Answer: <B>.\nB\nAnswer the following question: <X> Answer: <C>.\nC\nAnswer the following question: <X> Answer: <D>.\nD\nAnswer the following question: <X> Answer: <E>.\nE\nTable 4: Templates of tasks. Placeholders (e.g., <X> and <A>) will be replaced by real inputs or answers Commonsense QA).\n",
    "paper_type": "method",
    "attri": {
        "background": "Despite the impressive few-shot performance of in-context learning (ICL), it remains a common practice to randomly select examples to serve as the context. This paper advocates self-adaptive in-context learning, introducing a self-adaption mechanism to help each input find an in-context example organization that maximizes performance.",
        "problem": {
            "definition": "The problem is to find an optimal organization of in-context examples for each input to maximize the performance of in-context learning, which is essentially an NP-hard combinatorial optimization problem.",
            "key obstacle": "Existing methods rely heavily on the availability of a large-scale high-quality validation set and often result in sub-optimal performance due to majority bias."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that the organization of in-context examples significantly influences ICL\u2019s performance.",
            "opinion": "The proposed self-adaptive ICL method involves a two-stage framework that searches for the best organization of in-context examples tailored to each input.",
            "innovation": "The key innovation lies in the instance-level selection of in-context examples without requiring a validation set, contrasting with existing corpus-level methods."
        },
        "method": {
            "method name": "Self-Adaptive In-Context Learning",
            "method abbreviation": "SA-ICL",
            "method definition": "A two-stage framework that filters candidate examples and ranks their organizations to optimize in-context learning performance.",
            "method description": "The method employs heuristic selection followed by an information-theoretic ranking to determine the best organization of in-context examples.",
            "method steps": [
                "Filter candidate examples using nearest neighbor algorithms.",
                "Rank the filtered candidates using a ranking algorithm based on the Minimum Description Length principle."
            ],
            "principle": "The effectiveness of this method is rooted in the idea that a good organization of in-context examples can compress the information needed to predict outputs, thus enhancing performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on eight different NLP datasets using GPT2-XL. The method was evaluated against various baselines including prompting, corpus-level methods, and instance-level methods.",
            "evaluation method": "The performance was assessed by comparing accuracies across different methods, and results were averaged over multiple runs with different random seeds."
        },
        "conclusion": "The proposed self-adaptive ICL method significantly outperforms common practices in ICL, revealing its potential to enhance performance and suggesting directions for future research.",
        "discussion": {
            "advantage": "The method provides a more tailored approach to selecting in-context examples, leading to improved performance over traditional methods.",
            "limitation": "The method's performance can diminish with smaller retrieval sets, and the effectiveness of the selection method (TopK) is limited to simpler tasks.",
            "future work": "Future research should focus on developing better selection methods that can adapt to a wider range of tasks, as well as improving the efficiency of the current approach."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge the contributions of team members and the support from the National Key R&D Program of China and the Shanghai Committee of Science and Technology.",
            "datasets": {
                "SST-2": "Sentiment classification with 6920 training samples.",
                "SST-5": "Sentiment classification with 8544 training samples.",
                "SNLI": "Natural language inference with 550152 training samples.",
                "MNLI": "Natural language inference with 392702 training samples.",
                "QNLI": "Natural language inference with 104743 training samples.",
                "TREC": "Topic classification with 5452 training samples.",
                "AgNews": "Topic classification with 120000 training samples.",
                "CMS QA": "Commonsense question answering with 9471 training samples."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "3.1",
            "key information": "The proposed self-adaptive ICL method provides a more tailored approach to selecting in-context examples, leading to improved performance over traditional methods."
        },
        {
            "section number": "3.3",
            "key information": "The self-adaptive ICL method employs heuristic selection followed by an information-theoretic ranking to determine the best organization of in-context examples."
        },
        {
            "section number": "3.4",
            "key information": "The effectiveness of the self-adaptive ICL method is rooted in the idea that a good organization of in-context examples can compress the information needed to predict outputs, thus enhancing performance."
        },
        {
            "section number": "6.1",
            "key information": "The method's performance can diminish with smaller retrieval sets, indicating challenges related to context sensitivity in in-context learning."
        },
        {
            "section number": "6.2",
            "key information": "The self-adaptive ICL method's effectiveness is limited by the reliance on the availability of a large-scale high-quality validation set, which contributes to computational costs and efficiency concerns."
        },
        {
            "section number": "7",
            "key information": "The proposed self-adaptive ICL method significantly outperforms common practices in ICL, revealing its potential to enhance performance and suggesting directions for future research."
        }
    ],
    "similarity_score": 0.695884119479153,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Self-Adaptive In-Context Learning_ An Information Compression Perspective for In-Context Example Selection and Ordering.json"
}