{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.01860",
    "title": "Eliciting the Priors of Large Language Models using Iterated In-Context Learning",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical. One way to capture this knowledge is in the form of Bayesian prior distributions. We develop a prompt-based workflow for eliciting prior distributions from LLMs. Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution. We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants \u2013 causal learning, proportion estimation, and predicting everyday quantities. We found that priors elicited from GPT-4 qualitatively align with human priors in these settings. We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI.",
    "bib_name": "zhu2024elicitingpriorslargelanguage",
    "md_text": "# Eliciting the Priors of Large Language Models using Iterated In-Context Learning\nJian-Qiao Zhu Department of Computer Science Princeton University jz5204@princeton.edu Thomas L. Griffiths Department of Psychology and Computer Science Princeton University tomg@princeton.edu\n# Abstract\nAs Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical. One way to capture this knowledge is in the form of Bayesian prior distributions. We develop a prompt-based workflow for eliciting prior distributions from LLMs. Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution. We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants \u2013 causal learning, proportion estimation, and predicting everyday quantities. We found that priors elicited from GPT-4 qualitatively align with human priors in these settings. We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI.\narXiv:2406.01860v1\n# 1 Introduction\nAs Large Language Models (LLMs) become increasingly integrated into diverse real-world applications, there is a pressing need to understand their decision-making processes [3]. This is particularly crucial in scenarios where LLMs are granted agency to act and make decisions independently [38, 35]. Such agentic applications can significantly affect outcomes in sectors such as healthcare [e.g., 25], finance [e.g., 36], and legal services [e.g., 17], where the implications of errors or biased decisions can be profound. An essential component in understanding the decision-making processes of LLM is identifying the background knowledge they implicitly possess. For instance, consider a scenario where LLMs are asked to estimate a person\u2019s lifespan (i.e., hypotheses about the person\u2019s lifespan, h) based on a description of their current status (i.e., data about the person, d). Does the estimate produced by the LLM depend exclusively on the information provided in the description, or is it also shaped by background knowledge concerning the typical lifespan of individuals? To explore this question we adopt a Bayesian perspective, formalizing this background knowledge as a prior distribution over the hypothesis space (i.e., p(h)) [8, 32]. This approach enables us to assess, in probabilistic terms, how such prior knowledge affects the judgments and decisions made by LLMs, thereby enhancing our understanding of their underlying decision-making mechanisms. To elicit the priors of LLMs, we draw inspiration from cognitive psychology and develop an iterated learning procedure, a Markov chain Monte Carlo (MCMC) method. As illustrated in Figure 1 for the lifespan example, this method involves using successive inferences from LLMs in a sequential manner that supports direct sampling from the prior distribution, mirroring techniques used in psychological studies to elicit human priors [e.g., 9, 28, 16]. Given the established efficacy of iterated learning in eliciting human priors in cognitive psychology, we explored its applicability to LLMs. We conducted experiments using tasks from three distinct\nPreprint. Under review.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3bf3/3bf33eb9-4651-4367-89e7-799401c78958.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">h3 \u21e0p(h|d2)</div>\n<div style=\"text-align: center;\">Figure 1: Illustration of an iterated in-context learning procedure to elicit the implicit prior of an LLM regarding male lifespan. At each iteration, the LLM is given the current age of a random man and is prompted to predict the individual\u2019s remaining lifespan. This predicted lifespan is then used to generate a new current age for the next iteration. The new age is a random sample from the uniform distribution between 1 and the predicted lifespan. This implements a Markov chain Monte Carlo algorithm for sampling from the prior p(h).</div>\nFigure 1: Illustration of an iterated in-context learning procedure to elicit the implicit prior of an LLM regarding male lifespan. At each iteration, the LLM is given the current age of a random man and is prompted to predict the individual\u2019s remaining lifespan. This predicted lifespan is then used to generate a new current age for the next iteration. The new age is a random sample from the uniform distribution between 1 and the predicted lifespan. This implements a Markov chain Monte Carlo algorithm for sampling from the prior p(h).\ndomains \u2013 estimations of causal strengths, proportion, and everyday quantities \u2013 where human priors are well-documented [28, 39, 19]. These experiments successfully elicited priors from GPT-4. The priors recovered from GPT-4 not only align closely with human priors but can surpass the performance of generic priors, such as a uniform prior, in explaining decisions made by GPT-4 in these settings. Encouraged by the empirical evidence demonstrating shared priors between GPT-4 and humans across a broad range of tasks, we further investigated the potential of iterated learning to uncover priors in settings where priors are challenging to estimate directly from LLMs using standard prompting techniques. To illustrate, we applied iterated learning to elicit GPT-4\u2019s priors for three speculative events: the advent of superhuman AI, the achievement of zero carbon emissions, and the establishment of a Mars colony. The distributions recovered from GPT-4 suggest the model has plausible priors for these speculative events.\n# 2 Background\nIterated learning in cognitive science. Iterated learning was first introduced as a model for language evolution [18]. Language evolution can be conceptualized as the process through which languages are transmitted across successive generations of learners. In this model, an initial learner observes a corpus of linguistic data (for example, a collection of utterances), formulates a hypothesis about the underlying language that produced these utterances, and subsequently produces a new set of utterances. These are then used as data for the next learner in the sequence. Research has demonstrated that generational pressure on language transmission fosters the emergence of compositionality, realistic patterns of language dynamics, and several other observed properties of natural languages [18, 6]. Motivated by these results, an analysis of iterated learning with Bayesian learners showed that such a process will converge towards the prior distribution assumed by the learners [11].The analysis assumes that all learners share the same prior distribution over hypotheses p(h) and likelihood function p(d|h), which indicates how probable it would be to see data d if h were true. Each learner sees data d generated by the previous learner, samples a hypothesis h from p(h|d) \u221dp(d|h)p(h), and then generates data for the next learner from p(d|h). This process implements a Gibbs sampler for the joint distribution p(d, h) = p(d|h)p(h), a form of Markov chain Monte Carlo. The stationary distribution on hypotheses is thus the prior p(h), and samples from the prior can be drawn by running the iterated learning process long enough to converge to this distribution. These theoretical results suggest that implementing iterated learning in the laboratory may be an effective way to identify the prior distributions of human learners [16]. Based on the connection to Gibbs sampling, doing so involves implementing a form of Markov chain Monte Carlo with\npeople [30]. Researchers have indeed successfully used iterated learning to elicit human priors for various kinds of cognitive phenomena, such as concepts [10], categories [4], causal relationships [39], proportions [28], and everyday quantities [19].\nIterated learning in deep learning. The learning bottleneck in iterated learning limits the amount of information that can be transmitted and acquired by successive generations of learners, creating a selection pressure for communicative efficacy that promotes compositionality [18]. These advantages have led deep learning researchers to use iterated learning to encourage the development of structure and compositionality in neural networks [33, 27, 29]. However, neural networks are still far from being idealized Bayesian learners, as assumed in the limit of iterated learning presented above. To bridge the gap between neural networks and Bayesian learners, seeded iterated learning methods have been proposed [22, 23]. Instead of using a randomly initialized or pretrained neural network, seeded iterated learning methods use duplicated student and teacher networks at each iteration. Experimental results demonstrate that seeded iterated learning is effective in preserving the initial language structure acquired through pretraining while adapting the model to domain-specific tasks [22]. However, neglecting the gap between neural networks and Bayesian learners risks turning the iterated learning process into a degenerative one, where models gradually forget the true underlying data distribution \u2013 a phenomenon known as model collapse [31]. This issue has been frequently observed in machine learning, where training models on data produced by other models leads to forgetting improbable events, causing irreversible defects in the resultant models [2, 31].\n# 3 Eliciting Priors from GPT-4 using Iterated In-Context Learning\nBy contrast to previous work on iterated learning in machine learning, we use iterated learning applied to in-context learning rather than in-weight learning. That is, while previous work has focused on iterating the process of training a neural network\u2019s weights, we focus on generating predictions from a neural network that has already been pre-trained and has fixed weights. In this setting, we are relying on the network\u2019s ability to generate responses to prompts using that fixed set of weights. In doing so, we can capture the implicit knowledge encoded in those weights in the resulting prior distribution. This implementation of iterated learning also avoids the model collapse problem because there is no explicit training. This approach assumes that it is reasonable to interpret in-context learning as a form of Bayesian inference. Fortunately, a number of recent papers have provided support for this idea [37, 41]. We thus hypothesize that the theoretical results for iterated learning with Bayesian agents are applicable to LLMs. Specifically, in-context iterated learning with LLMs should converge to responses that support sampling from the prior distribution. To test the hypothesis that iterated in-context learning should reveal the prior distributions of LLMs, we incorporated GPT-4 [1] into a prompt-based iterated learning procedure. At each iteration t, GPT-4 undertakes a prediction task using the data dt\u22121. The model\u2019s prediction is recorded as ht. Subsequently, we employ generic likelihood functions that are a reasonable match for the sampling process producing the described data to randomly generate the data for the next iteration, dt \u223cp(d|ht). For instance, we applied the method depicted in Figure 1 to investigate GPT-4\u2019s prior beliefs about men\u2019s lifespans. In this procedure, the LLM is prompted to estimate the lifespan of a random man, given information about his current age. The age of the man encountered in the next prompt is then uniformly sampled from a range extending from 1 to the lifespan predicted in the previous iteration, matching the probability of randomly encountering the man at this point in his life. By iteratively applying this procedure, we expect the final prediction made by GPT-4 will converge on a stationary distribution that reflects the model\u2019s prior beliefs about human life expectancy. In the experiments presented in the remainder of the paper, 100 iterated learning chains were implemented with random seeds. We conducted 12 iterations for each chain. The temperature of GPT-4 was fixed at 1, consistent with the idea of sampling from the posterior.\n# Iterated Learning in Settings with Known Human Prior\nTo determine whether GPT-4\u2019s implicit priors resemble human priors, we first elicited GPT-4\u2019s implicit priors using a set of iterated learning tasks that have previously been used to infer human priors (see Table 1).\nTo determine whether GPT-4\u2019s implicit priors resemble human priors, we first elicited GPT-4\u2019s implicit priors using a set of iterated learning tasks that have previously been used to infer human priors (see Table 1).\n<div style=\"text-align: center;\">ble 1: Overview of human priors elicited using the iterated learning meth</div>\nChain\nSeeds\nLikelihood functions\nTrials\nGenerative causal strengths\nw0 = {0.3, 0.7}, w1 = {0.3, 0.7}\nnoisy-OR\n7\nPreventive causal strengths\nw0 = {0.3, 0.7}, w1 = {0.3, 0.7}\nnoisy-AND-NOT\n8\nCoin flips\np(Head) = {0.3, 0.5, 0.7}\nBin(10, ht\u22121)\n1\nLifespan (male)\ntmax = 150 years old\nU[1, ht\u22121]\n2\nMovie grosses\nxmax = 3000 million dollars\nU[0, ht\u22121]\n11\nLength of poems\nxmax = 200 lines\nU[1, ht\u22121]\n10\nReign of Pharaohs\ntmax = 100 years\nU[0, ht\u22121]\n8\nMovie run times\ntmax = 800 minutes\nU[0, ht\u22121]\n5\nCake baking times\ntmax = 120 minutes\nU[0, ht\u22121]\n3\nNote. Trials column indicates the estimated number of trials to convergence. Seeds determine the\n\u2212 Note. Trials column indicates the estimated number of trials to convergence. Seeds determine the generation of initial data (d0).\n# 4.1 Causal strength\nTo ensure accuracy and clarity in the use of LLMs for causal inference, it is crucial to understand the implicit priors about causal relationships embedded within these models. We examined an elemental problem of causal induction [12] involving two potential causes and one effect (see Figure 2a). In this model, the causal system is represented by three variables: the background cause (B), the candidate cause (C), and the effect (E). Both B and C can independently cause E, and this relationship is depicted by edges directed from both B and C to E. The causal strengths of B and C are represented by w0 and w1, respectively. We further assume that B is always present and generative, meaning it consistently increases the probability of E. However, C can be either generative or preventive. In the generative scenario, either B or C can cause E; in the preventive scenario, only B can cause E, while C may inhibit E. Additionally, E cannot occur unless it is caused by either B or C. Depending on the functional form of the causal relationships, the probability of observing an effect given two causes is expressed differently: a noisy-OR likelihood function is used for generative causes and a noisy-AND-NOT likelihood function for preventive causes [5, 12, 26]. The noisy-OR function gives the probability of observing E as:\np(e+|C+) = w0(1 \u2212w1), if C is present p(e+|C\u2212) = w0, if C is absent\nHere, we are particularly interested in the prior distribution on causal strengths implicitly used by LLMs: p(w0, w1). One potential prior is the uniform prior, arguably the simplest non-informative prior, which assigns equal probability to all possible values of w0 and w1 [15, 12]. Another prior that LLMs might employ is the sparse and strong prior, which is motivated by simplicity principles suggesting that people favor necessary and sufficient causes without complex interactions [21]. The sparse and strong prior is defined as follows:\nwhere \u03b1 is a free parameter representing the strength of belief in the sparsity and strength of causes. When \u03b1 = 0, the sparse and strong priors become identical to a uniform prior. Based on previous parameter estimation from human data, we also fixed \u03b1 = 5 [21]. Finally, LLMs could also employ an empirical prior that delineates a specific relationship between w0 and w1 but does not possess a precise mathematical description. To elicit the empirical prior on causal strengths from LLMs, we implemented an iterated learning procedure with GPT-4 based on an experiment conducted with human participants [39]. The prompts\n(1) (2)\n(5) (6)\nused a cover story involving the influence of various proteins on gene expression (see Appendix A for details). The iterated learning chain was initiated with four possible pairs of (w0, w1): (0.3, 0.3), (0.3, 0.7), (0.7, 0.3), (0.7, 0.7). The number of DNA fragments exposed and not exposed to the protein was fixed at 16 each (i.e., N(C+) = N(C\u2212) = 16). At each iteration t, we elicited GPT-4\u2019s estimates of the causal strengths: p(w0, w1|dt\u22121). The data presented at iteration t was a random sample drawn from the likelihood function based on GPT-4\u2019s estimates from the previous iteration. Each chain consisted of 12 iterations and was randomly initialized 25 times for each of the 4 seeds, resulting in a total of 100 chains. Using a Mann-Whitney U test with a significance level of p < .05, we found that the chains converged to a stationary distribution by iterations 7 and 8 for generative and preventive causal strengths, respectively. The empirical distributions of w0, w1 at iteration 12, smoothed with a Gaussian kernel, were then considered the empirical prior of causal strengths (see Figure 2b and Figure 2c). The empirical priors derived from GPT-4 closely resemble those observed in human experiments [39]. Since human data are only available in the form of visualizations provided in the paper, we rely on visual comparisons to analyze the priors of both humans and GPT-4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/223c/223c9a53-9ca5-4427-90d3-2d5d015af234.png\" style=\"width: 50%;\"></div>\nFigure 2: Priors on causal strengths. (a) The causal graphical model. (b) Smoothed empirical estimates of human (left) and GPT-4 (right) priors on causal strength produced by iterated learning for generative cases. (c) Smoothed empirical estimates of human (left) and GPT-4 (right) priors on causal strength produced by iterated learning for preventive cases. Human data in panel (b) and (c) were adapted from [39]. To further investigate which prior better captures GPT-4\u2019s decisions about causal relationships, we elicited an additional set of causal judgments from GPT-4. We used the same cover story as in iterated learning but varied the number of DNA fragments exposed and not exposed to the protein, which was previously fixed at 16. Now N(C+) and N(C\u2212) could take values from 8, 16, and 32, leading to a total of 3 \u00d7 3 = 9 possible combinations of sample sizes. When the sample size is 8, N(e+) takes all possible integer values from 0 to 9; when the sample size is 16, N(e+) takes integer values in increments of 2; and when the sample size is 32, N(e+) takes integer values in increments of 4. This results in each sample size contributing 9 data points to the causal judgments. To explain GPT-4\u2019s causal judgments, we developed three Bayesian models based on those used to model human causal judgments [12, 21, 39]. Each model assumed a different prior. The posterior distribution was obtained by multiplying the prior of causal strength with the appropriate likelihood for the causal direction (i.e., generative or preventive):\nFor all three Bayesian models, numerical methods were employed, discretizing the space of w0, w1 into a grid of 101 \u00d7 101 points. The mean of the posterior distribution was taken as the Bayesian model\u2019s prediction. We then compared the posterior means to the causal judgments produced by GPT-4. The results, summarized in Table 2, indicate that the empirical prior outperformed the uniform prior and the sparse and strong prior in all except the preventive case when measured by RMSD (see Appendix B for detail). These results suggest that we have successfully recovered the implicit prior of causal strengths using iterative learning with GPT-4. Since our primary interest is in comparing priors for causal directions (generative vs. preventive), we focused on the gene/protein cover story, as it is the only one that includes human priors for both generative and preventive causal directions. However, there are four other cover stories with human priors on the generative case [39].We conducted four additional iterated in-context learning experiments with GPT-4, using alternative cover stories along with the same seeds and likelihood\n(7)\n<div style=\"text-align: center;\">Table 2: Comparison of Bayesian models of causal induction with various priors and GPT-4\u2019s causal judgments using Pearson\u2019s r and root-mean-squared deviation (RMSD).</div>\nCausal direction\nMetric\nUniform prior\nSparse and strong prior\nEmpirical prior\nGenerative\nPearson\u2019s r (\u2191)\n0.85\n0.79\n0.86\nRMSD (\u2193)\n0.21\n0.25\n0.19\nPreventive\nPearson\u2019s r (\u2191)\n0.72\n0.68\n0.79\nRMSD (\u2193)\n0.26\n0.29\n0.27\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cec4/cec4616a-122a-4e62-8b2d-e3fbb3eb6012.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparison of causal generative priors using alternative cover stories between humans and GPT-4. Human data adapted from [39]. Detailed prompts are provided in Appendix A.</div>\nfunctions. As shown in Figure 3, the implicit priors recovered from GPT-4 also align with the human priors.\n# 4.2 Proportion estimation\nAnother setting with known human priors from iterated learning is proportion estimation [28, 42]. In these studies, human participants were asked at each iteration to judge the frequency of a binary event, such as a coin flip or a choice between two words [28]. We implemented an iterated learning chain with GPT-4 to replicate this process using the cover story of coin flips. At each iteration, GPT-4 received the outcomes of 10 random coin flips, generated based on the previous iteration\u2019s p(Head):\nThen, GPT-4 was asked to report a new p(Head) by imagining throwing the same coin another 100 times. This reported p(Head) was then used to generate coin flip data for the next iteration (see Appendix A for detailed prompts).\nThe evolution of the distribution of p(Head) is shown in Figure 4c. The distribution gradually shifted towards a U-shaped distribution, with most of its mass close to 0 or 1 (see Figure 4d for the final iteration\u2019s histogram and Figure 4e for a few example chains). The recovered prior of GPT-4 matches human priors (see Figure 4a). Moreover, the chain evolution from GPT-4 aligns with patterns observed in human data for iterated learning of the proportions of two words used as labels for an object (see Figure 4b). Due to the absence of human data beyond the visualizations presented in the papers, we depend on visual comparisons to examine the priors of humans and GPT-4. Given that in these settings the prior has only a weak effect on proportion estimation (which motivated the use of iterated learning to study human priors in this context), we did not further test different priors using Bayesian models.\n# 4.3 Everyday quantities.\nA third class of tasks with known human priors elicited by iterated learning methods concerns everyday quantities [19]. These tasks can be broadly summarized as future-prediction tasks, where participants repeatedly provided predictions for a quantity tfuture in response to a given value of tpresent. In our example of estimating a man\u2019s lifespan, tfuture would be the total lifespan and tpresent the age at which the man was encountered. Typically, the probe value of tpresent is randomly sampled from an interval ranging between 0 and the previous tfuture: tpresent \u223cU[0, tfuture). We implemented all six everyday quantities tested in [19] ranging from male lifespan, movie grosses, length of poems, reign of Pharaohs, movie runtimes, and cake baking times (see Figure 5a-f). Because the likelihood function is a uniform distribution, meaning that the posterior distribution depends solely on the prior, we did not compare Bayesian models with different priors. Instead, we focused on directly comparing the recovered priors from GPT-4 to those from human participants. As shown in Figures 5(a-f), the modes of the priors from humans and GPT-4 were matched. Because human data\n(8)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b035/b0357e63-ae85-4f0f-b640-d74246ae0920.png\" style=\"width: 50%;\"></div>\nFigure 4: Priors on proportion estimation. (a) The empirical distribution of probability-describing phrases from the British National Corpus. Figure adapted from [42]. (b) Example iterated learning chains for human participants estimating the proportion of binary events. Figure adapted from [28]. (c) The evolution of GPT-4\u2019s estimation of binary events using iterated learning. (d) The histogram of GPT-4\u2019s proportion estimation in the final (12th) iteration. (e) Example iterated learning chains for GPT-4 estimating the proportion of binary events, for comparison with human data.\n<div style=\"text-align: center;\">Figure 4: Priors on proportion estimation. (a) The empirical distribution of probability-describing phrases from the British National Corpus. Figure adapted from [42]. (b) Example iterated learning chains for human participants estimating the proportion of binary events. Figure adapted from [28]. (c) The evolution of GPT-4\u2019s estimation of binary events using iterated learning. (d) The histogram of GPT-4\u2019s proportion estimation in the final (12th) iteration. (e) Example iterated learning chains for GPT-4 estimating the proportion of binary events, for comparison with human data.</div>\nare limited to the figures included in the paper, we use visual comparisons to evaluate the priors of humans and GPT-4. However, the overall distribution differed sometimes, especially for the Pharaohs. This is actually a case where people\u2019s beliefs are systematically incorrect \u2013 by applying modern expectations about human lifespans to Ancient Egypt, people overestimate the length of the reigns of Pharaohs [13]. GPT-4 produces more appropriate predictions in this setting.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8293/82933d4e-ab65-4fd7-a53f-45a9a29d2ecd.png\" style=\"width: 50%;\"></div>\n# erated Learning as a Method for Estimating a Wider Ran\nApplying iterated learning to estimate the priors of GPT-4 on causal strengths, proportion estimation, and everyday quantities reveals qualitative similarities with human priors. This suggests that LLMs have successfully learned human-like priors when producing judgments and predictions. Motivated by these findings, we aimed to test some speculative events that (i) have no known human priors, (ii) are difficult to quantify directly through prompts to GPT-4, and (iii) lack explicit consensus among humans. Iterated learning might serve as a unique way to address these three challenges because the\npriors recovered from LLMs using iterated learning are likely to resemble the implicit priors that people assume but have not yet explicitly manifested.\nIn principle, iterated learning is broadly applicable to a wide range of speculative events. However, LLMs typically avoid speculating on future events involving sensitive or potentially harmful topics (see Appendix C for examples). These topics include political outcomes (e.g., predicting the winner of the U.S. presidential election in 2024), market forecasts (e.g., forecasting the price of Bitcoin in December of this year), personal futures (e.g., determining the likelihood of obtaining a recently interviewed job), legal outcomes (e.g., the outcome of ongoing investigations into public figures like Donald Trump), technological breakthroughs (e.g., the discovery of a cure for cancer next year), disasters (e.g., predicting the timing of the next earthquake in California), and specific dates for future events (e.g., when self-driving cars will become the primary mode of transportation worldwide). Generally, LLMs are restricted from making definitive predictions on sensitive and impactful issues related to speculative future events.\nChain\nSeeds\nLikelihood functions\nTrials\nSuperhuman AI\ntmax = 2200 year\nU[2024, ht\u22121]\n5\nZero carbon emission\ntmax = 2200 year\nU[2024, ht\u22121]\n5\nMars colony\ntmax = 2200 year\nU[2024, ht\u22121]\n9\note. Trials column indicates the estimated number of trials to convergence. Seeds determine th\n\u2212 Note. Trials column indicates the estimated number of trials to convergence. Seeds determine the generation of initial data (d0).\nTo illustrate the utility of eliciting priors from LLMs using iterated in-context learning rather than direct prompting, we focus on three technology and climate-related events: (i) the timing of the development of superhuman AI, (ii) the timing of achieving zero carbon emissions, and (iii) the timing of establishing a Mars colony. These events are particularly well-suited to our existing framework because they involve a clear two-stage completion process, similar to the future-prediction tasks illustrated in Figure 5. For example, superhuman AI can only be achieved if human-level AI has already been realized. Similarly, zero carbon emissions are possible only if the majority of energy use is renewable, and establishing a Mars colony is typically contingent upon the prior establishment of a Moon colony. An iterated learning design based on the everyday prediction task presented above can leverage the two-stage nature of these speculative events by prompting with a completion year for the first stage and then asking GPT-4 to predict the second-stage completion time based on the information about the first stage. To minimize assumptions about the relationship between the first and second stage completions, we chose a uniform distribution as the likelihood function (see Table 3). Each chain was also seeded with a maximum year of 2200. We found the iterated learning chains converged when asking GPT-4 about the three speculative events (see Figure 6). The median completion years for superhuman AI, zero carbon emission, and a Mars colony are 2042, 2045, and 2050, respectively.1\n# 6 Discussion\nBy adapting an iterated learning paradigm used to evaluating the priors of human participants, we were able to estimate implicit prior distributions used by GPT-4. We showed that these prior distributions correspond closely to those assumed by people in three settings where human priors have been established, and that they can also be used to predict the decisions that GPT-4 makes in response to related prompts. We were also able to estimate GPT-4\u2019s priors for three significant speculative events, where answers can be hard to elicit through direct prompting. These results have a wide range of implications about the potential uses of LLMs, although we also note some important limitations of our work. Implications for LLMs as cultural technologies. Cultural technologies are tools and systems created by humans that facilitate easy access to knowledge generated by others. Prime examples include language, writing, printing, Internet search, and Wikipedia. Recently, LLMs have been\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6b66/6b668d84-3f67-4700-99a6-b9b8c9bf39b5.png\" style=\"width: 50%;\"></div>\nFigure 6: Recovered GPT-4 priors on speculative events. (a) The timing of the development of superhuman AI. Median completion year: 2042. (b) The timing of achieving zero carbon emission. Median completion year: 2045. (c) The timing of establishing a Mars colony. Median completion year: 2050.\nargued to function as a cultural technology [40]. Our proposed method enriches this viewpoint by providing a procedure to robustly recover human-like priors curated by LLMs. Consider the example of recovering human prior beliefs about the arrival of superhuman AI. Traditionally, obtaining a quantitative description of such a prior would require researchers to perform extensive literature searches and gather relevant statistics, or conduct surveys with a representative sample of human participants. Now, with LLMs, we can envision a more effective approach by conducting iterated learning with LLMs to estimate human priors, using the priors of LLMs as surrogates. Given this information, research into human priors can become much more directed, as we have a promising null hypothesis of human priors recoverable from LLMs. Implications for automated science with LLMs. Recent proposals for using LLMs to automatically generate and test scientific hypotheses [e.g., 24] can also benefit from our findings. Given the humanlike behaviors produced by LLMs, researchers have started to simulate LLMs both as experimental participants and as scientists that generate and test scientific hypotheses [24, 14]. We believe the results derived from automated science with LLMs should be carefully interpreted in conjunction with the implicit priors encoded in these models. These implicit priors will inevitably shape, and perhaps proliferate, through the automated process of knowledge accumulation. Doing Bayesian inference with LLMs. Our work supports the emerging viewpoint that Bayesian inference can be conducted with the assistance of LLMs [20, 43, 34]. Two different approaches have been proposed. The first approach uses LLMs to translate inference problems described in natural language into probabilistic programs, which are then used to perform Bayesian inference [20, 34]. The second approach involves performing Bayesian inference directly using LLMs by constructing a Markov chain with LLMs, as demonstrated in our iterated learning method [43]. Both approaches have the potential to outperform standard prompting techniques. Limitations and Future Research. The key assumption of our proposed method is that LLMs function as approximate Bayesian agents, producing responses according to the posterior distribution p(h|d). While there is evidence that LLMs trained to predict the next word can encode latent generating distributions [41], and that in-context learning can be understood as implicit Bayesian inference [37], further investigations are needed to elucidate the exact relationship between autoregressive distributions and Bayesian inference. Moreover, although we have shown that GPT-4 can encode human-like priors, it remains unclear how LLMs learn to encode these priors from pretraining on human text. Future research could focus on developing a more precise theoretical framework to understand how autoregressive models perform Bayesian inference. Conclusion. LLM-based agents are poised to, if not already, make significant impacts on the world and interact at scale with both humans and other AI systems. In this paper, we proposed and empirically demonstrated a novel approach to gain deeper insights into the decision-making styles of LLMs by formalizing the prior knowledge they implicitly assume. Our method, iterated in-context learning, effectively extracts these priors through prompts and responses. This allows us to unravel the background knowledge that guides LLMs\u2019 decisions, providing a crucial step towards harnessing their full potential in real-world applications and ensuring more transparent and informed interactions between AI systems and humans.\nAcknowledgments. This work and related results were made possible with the support of the NOMIS Foundation, as well as Microsoft Azure credits supplied to Princeton and a Microsoft Foundation Models grant. We thank Haijiang Yan for helpful discussion.\n# References\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11254\u201311263, 2019. [3] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al. Managing AI risks in an era of rapid progress. arXiv preprint arXiv:2310.17688, 2023. [4] Kevin R Canini, Thomas L Griffiths, Wolf Vanpaemel, and Michael L Kalish. Revealing human inductive biases for category learning by simulating cultural transmission. Psychonomic Bulletin & Review, 21:785\u2013793, 2014. [5] Patricia W Cheng. From covariation to causation: A causal power theory. Psychological Review, 104(2):367, 1997. [6] Morten H Christiansen and Simon Kirby. Language evolution. Oxford University Press, 2003. [7] Katja Grace, Harlan Stewart, Julia Fabienne Sandk\u00fchler, Stephen Thomas, Ben Weinstein-Raun, and Jan Brauner. Thousands of ai authors on the future of ai. arXiv preprint arXiv:2401.02843, 2024. [8] Thomas L Griffiths, Nick Chater, Charles Kemp, Amy Perfors, and Joshua B Tenenbaum. Probabilistic models of cognition: Exploring representations and inductive biases. Trends in cognitive sciences, 14(8):357\u2013364, 2010. [9] Thomas L Griffiths, Brian R Christian, and Michael L Kalish. Revealing priors on category structures through iterated learning. In Proceedings of the 28th annual conference of the Cognitive Science Society, volume 199, 2006. 10] Thomas L Griffiths, Brian R Christian, and Michael L Kalish. Using category structures to test iterated learning as a method for identifying inductive biases. Cognitive Science, 32(1):68\u2013107, 2008. 11] Thomas L Griffiths and Michael L Kalish. Language evolution by iterated learning with bayesian agents. Cognitive science, 31(3):441\u2013480, 2007. 12] Thomas L Griffiths and Joshua B Tenenbaum. Structure and strength in causal induction. Cognitive Psychology, 51(4):334\u2013384, 2005. 13] Thomas L Griffiths and Joshua B Tenenbaum. Optimal predictions in everyday cognition. Psychological Science, 17(9):767\u2013773, 2006. 14] Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, et al. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679, 2024. 15] Edwin T Jaynes. Probability theory: The logic of science. Cambridge University Press, 2003. 16] Michael L Kalish, Thomas L Griffiths, and Stephan Lewandowsky. Iterated learning: Intergenerational knowledge transmission reveals inductive biases. Psychonomic Bulletin & Review, 14(2):288\u2013294, 2007.\n[17] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. GPT-4 passes the bar exam. Philosophical Transactions of the Royal Society A, 382(2270):20230254, 2024. [18] Simon Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, 5(2):102\u2013110, 2001. [19] Stephan Lewandowsky, Thomas L Griffiths, and Michael L Kalish. The wisdom of individuals: Exploring people\u2019s knowledge about everyday events using iterated learning. Cognitive Science, 33(6):969\u2013998, 2009. [20] Michael Y. Li, Emily B. Fox, and Noah D. Goodman. Automated statistical model discovery with language models. arXiv preprint arXiv:2402.17879, 2024. [21] Hongjing Lu, Alan L Yuille, Mimi Liljeholm, Patricia W Cheng, and Keith J Holyoak. Bayesian generic priors for causal learning. Psychological Review, 115(4):955, 2008. [22] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning, pages 6437\u20136447. PMLR, 2020. [23] Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, and Aaron Courville. Supervised seeded iterated learning for interactive language learning. arXiv preprint arXiv:2010.02975, 2020. [24] Benjamin S. Manning, Kehang Zhu, and John J. Horton. Automated social science: Language models as scientist and subjects. arXiv preprint arXiv:2404.11794, 2024. [25] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of GPT-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. [26] Judea Pearl. Causal inference in statistics: An overview. Statistics Surveys, pages 96\u2013146, 2009. [27] Sai Rajeswar, Pau Rodriguez, Soumye Singhal, David Vazquez, and Aaron Courville. Multilabel iterated learning for image classification with label ambiguity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4783\u20134793, 2022. [28] Florencia Reali and Thomas L Griffiths. The evolution of frequency distributions: Relating regularization to inductive biases through iterated learning. Cognition, 111(3):317\u2013328, 2009. [29] Yi Ren, Samuel Lavoie, Michael Galkin, Danica J Sutherland, and Aaron C Courville. Improving compositional generalization using iterated learning and simplicial embeddings. Advances in Neural Information Processing Systems, 36, 2024. [30] Adam Sanborn and Thomas Griffiths. Markov chain monte carlo with people. Advances in Neural Information Processing Systems, 20, 2007. [31] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023. [32] Joshua B Tenenbaum and Thomas L Griffiths. Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24(4):629\u2013640, 2001. [33] Ankit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, and Aaron Courville. Iterated learning for emergent systematicity in VQA. arXiv preprint arXiv:2105.01119, 2021. [34] Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. From word models to world models: Translating from natural language to the probabilistic language of thought. arXiv preprint arXiv:2306.12672, 2023.\n[35] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. [36] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023. [37] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. [38] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [39] Saiwing Yeung and Thomas L Griffiths. Identifying expectations about the strength of causal relationships. Cognitive Psychology, 76:1\u201329, 2015. [40] Eunice Yiu, Eliza Kosoy, and Alison Gopnik. Imitation versus innovation: What children can do that large language and language-and-vision models cannot (yet)? arXiv preprint arXiv:2305.07666, 2023. [41] Liyi Zhang, R Thomas McCoy, Theodore R Sumers, Jian-Qiao Zhu, and Thomas L Griffiths. Deep de finetti: Recovering topic distributions from large language models. arXiv preprint arXiv:2312.14226, 2023. [42] Jian-Qiao Zhu, Adam N Sanborn, and Nick Chater. The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments. Psychological Review, 127(5):719, 2020. [43] Jian-Qiao Zhu, Haijiang Yan, and Thomas L Griffiths. Recovering mental representations from large language models with Markov chain Monte Carlo. arXiv preprint arXiv:2401.16657, 2024.\n# A Prompts\n# Prompts for causal learning\nPrompts for causal learning\nSystem Please imagine that you are a researcher working for a bio-technology company and you are studying the relationship between genes and proteins concerning gene expression. This process may or may not be modulated by the presence of proteins. You will be given information about some past results involving this gene/protein pair and you will be asked to make some predictions based on these information. The past results consist of two samples: 1) a sample of DNA fragments that had not been exposed to the protein, and 2) a sample of DNA fragments that had been exposed to the protein. The number of DNA fragments that resulted in gene expression in each of these samples will be shown to you. Because there are many causes of gene expression, some background factors besides the presence or absence of the protein may play a role in whether the gene is expressed or not. Your job is to make predictions concerning the effect of these proteins on gene expression and answer the question based on this. User (generative causal inference) Within sample 1 that had not been exposed to the protein, N(e+, C\u2212) of N(C\u2212) DNA fragments were turned on; within sample 2 that had been exposed to the protein, N(e+, C+) of N(C+) DNA fragments were turned on. Suppose that there is a sample of 100 DNA fragments and these fragments were not exposed to the protein, in how many of them would the gene be turned on? Please limit your answer to a single value without outputing anything else. Within sample 1 that had not been exposed to the protein, N(e+, C\u2212) of N(C\u2212) DNA fragments were turned on; within sample 2 that had been exposed to the protein, N(e+, C+) of N(C+) DNA fragments were turned on. Suppose that there is a sample of 100 DNA fragments and that the gene is currently off in all those DNA fragments. If these 100 fragments were exposed to the protein, in how many of them would the gene be turned on? Please limit your answer to a single value without outputing anything else. User (preventive causal inference) Within sample 1 that had not been exposed to the protein, N(e+, C\u2212) of N(C\u2212) DNA fragments were turned on; within sample 2 that had been exposed to the protein, N(e+, C+) of N(C+) DNA fragments were turned on. Suppose that there is a sample of 100 DNA fragments and these fragments were not exposed to the protein, in how many of them would the gene be turned on? Please limit your answer to a single value without outputing anything else. Within sample 1 that had not been exposed to the protein, N(e+, C\u2212) of N(C\u2212) DNA fragments were turned on; within sample 2 that had been exposed to the protein, N(e+, C+) of N(C+) DNA fragments were turned on. Suppose that there is a sample of 100 DNA fragments and that the gene is currently on in all those DNA fragments. If these 100 fragments were exposed to the protein, in how many of them would the gene be turned off? Please limit your answer to a single value without outputing anything else.\n# Prompts for the physical condition\nSystem Please imagine that you are working for a pencil company and you are studying the relationship between a material called \u2019super lead\u2019 and machines called \u2019super lead detectors\u2019. Pencil lead is made of carbon. Your company recently discovered that a new production process was resulting in a new carbon structure in their pencils\u2014what they call \u2019super lead\u2019. Since they are not sure which pencils they previously manufactured contain super lead, they are building a set of machines in order to detect it. These machines are programmed with different parameters to detect\ndifferent types of carbon structures. You will be testing machines that are set up with different parameters. There are a number of trials in this experiment. Each trial involves a different type of super lead, and a super lead detector programmed with a different parameter set. You will see some information about how often the machine indicates the presence of super lead with a set of pencils that do not contain super lead, and how often with a set of pencils that do contain a particular type of super lead. You will be then asked to make some predictions based on these pieces of information.\nUser With 16 pencils that do not contain super lead, the super lead detecto indicated that N(e+, C\u2212) of them contain super lead; with 16 pencils that contain super lead, the super lead detector indicated that N(e+, C+) of them contain super lead. Question: Suppose that there are 100 pencils that do not contain super lead, how many of them would be detected to contain super lead by the detector? And if there are 100 pencils that do contain super lead, how many of them would be detected to contain super lead by the detector? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\nUser With 16 pencils that do not contain super lead, the super lead detector indicated that N(e+, C\u2212) of them contain super lead; with 16 pencils that contain super lead, the super lead detector indicated that N(e+, C+) of them contain super lead. Question: Suppose that there are 100 pencils that do not contain super lead, how many of them would be detected to contain super lead by the detector? And if there are 100 pencils that do contain super lead, how many of them would be detected to contain super lead by the detector? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\n# Prompts for the medical condition\nSystem Please imagine that you are a researcher working for a medical company and you are studying the relationship between some allergy medicines and hormonal imbalance as a side effect of these medicines. Your company recently discovered that a new production process was resulting in changes in the molecular structures in the allergy medicines, and these new medicines cause abnormal levels of hormones in people. Since they are not sure which medicines they previously manufactured might cause anomalies in which type of hormone, you are tasked with investigating this. There are a number of trials in this experiment and each trial involves a different type of medicine and a different hormone. You will see some information about how often people who don\u2019t take the medicine have a particular kind of hormonal imbalance, and how often people who take that medicine have the same kind of hormonal imbalance. You will be then asked to make some predictions based on these pieces of information. User Within 16 people who don\u2019t take the medicine, N(e+, C\u2212) of them have a particular kind of hormonal imbalance; within 16 people who take the medicine, N(e+, C+) of them have a particular kind of hormonal imbalance. Question: Suppose that there are 100 people who don\u2019t take the medicine, how many of them would have a particular kind of hormonal imbalance? And if there are 100 people who don\u2019t have a particular kind of hormonal imbalance currently and then take the medicine, how many of them would have a particular kind of hormonal imbalance after taking the medicine? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\nSystem Please imagine that you are a researcher working for a medical company and you are studying the relationship between some allergy medicines and hormonal imbalance as a side effect of these medicines. Your company recently discovered that a new production process was resulting in changes in the molecular structures in the allergy medicines, and these new medicines cause abnormal levels of hormones in people. Since they are not sure which medicines they previously manufactured might cause anomalies in which type of hormone, you are tasked with investigating this. There are a number of trials in this experiment and each trial involves a different type of medicine and a different hormone. You will see some information about how often people who don\u2019t take the medicine have a particular kind of hormonal imbalance, and how often people who take that medicine have the same kind of hormonal imbalance. You will be then asked to make some predictions based on these pieces of information. User Within 16 people who don\u2019t take the medicine, N(e+, C\u2212) of them have a particular kind of hormonal imbalance; within 16 people who take the medicine, N(e+, C+) of them have a particular kind of hormonal imbalance. Question: Suppose that there are 100 people who don\u2019t take the medicine, how many of them would have a particular kind of hormonal imbalance? And if there are 100 people who don\u2019t have a particular kind of hormonal imbalance currently and then take the medicine, how many of them would have a particular kind of hormonal imbalance after taking the medicine? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\n# Prompts for the social condition\nSystem Please imagine that you are an animal researcher and you are studying the relationship between music and the tail-wagging behavior of different dog breeds. You have found that some dogs would wag their tails after listening to some kinds of music. Since you are not sure what kind of music might cause which breed of dog to wag their tails, you have decided to investigate this. There are a number of trials in this experiment and each trial involves a different kind of music and a different breed of dogs. For each kind of music, you will see some information about how often dogs who were not played the music wagged their tails, and how often dogs who\nwere played the music wagged their tails. You will be then asked to make some predictions based on these pieces of information. User Within 16 dogs who were not played the music, N(e+, C\u2212) of them wagged their tails; within 16 dogs who were played the music, N(e+, C+) of them wagged their tails. Question: Suppose that there are 100 dogs who are not played the music, how many of them would wag their tails? And if there are 100 dogs who don\u2019t wag their tails currently, how many of them would wag their tails when they are played the music? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\n# Prompts for the psychic condition\nSystem Please imagine that you are a physics researcher and you are studying the relationship between psychic power and the behavior of molecules. All molecules that you are currently investigating share a characteristic in that they all emit photons at random intervals, but at different rates. A number of psychics have claimed that they can make these molecules emit photons within a minute of when they use their power. You are tasked with investigating this. There are a number of trials in this study and each trial involves a different psychic and a different type of molecule. For each psychic, you will see some information about how many molecules have emitted photons when a particular psychic was simply standing next to the molecules, and how many of them have emitted photons following when psychic used his/her power. You will be then asked to make some predictions based on these pieces of information.\nSystem Please imagine that you are a physics researcher and you are studying the relationship between psychic power and the behavior of molecules. All molecules that you are currently investigating share a characteristic in that they all emit photons at random intervals, but at different rates. A number of psychics have claimed that they can make these molecules emit photons within a minute of when they use their power. You are tasked with investigating this. There are a number of trials in this study and each trial involves a different psychic and a different type of molecule. For each psychic, you will see some information about how many molecules have emitted photons when a particular psychic was simply standing next to the molecules, and how many of them have emitted photons following when psychic used his/her power. You will be then asked to make some predictions based on these pieces of information.\nUser With 16 molecules when a particular psychic was simply standing next to the molecules, N(e+, C\u2212) of them emitted photons; with 16 molecules when psychic used his/her power, N(e+, C+) of them emitted photons. Question: Suppose that there are 100 molecules when a particular psychic is simply standing next to the molecules, how many of them would emit photons? And if there are 100 molecules that don\u2019t emit photons currently, how many of them would emit photons when psychic uses his/her power? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\nUser With 16 molecules when a particular psychic was simply standing next to the molecules, N(e+, C\u2212) of them emitted photons; with 16 molecules when psychic used his/her power, N(e+, C+) of them emitted photons. Question: Suppose that there are 100 molecules when a particular psychic is simply standing next to the molecules, how many of them would emit photons? And if there are 100 molecules that don\u2019t emit photons currently, how many of them would emit photons when psychic uses his/her power? Please limit your answer into the 2 numeric values for the 2 questions, for example, (50, 50), without outputing anything else.\n# Prompts for proportion estimation\nSystem Imagine that you are a participant in a psychology experiment. Your task is to evaluate the bias in a coin. User Here is a brief overview of the past coin flips: Out of Ncoinflips coin flips, Nhead resulted in heads and Ncoinflips \u2212 Nhead in tails. With this information, please predict the number of heads in a larger set of 100 coin flips. Please limit your answer to a single value without outputing anything else.\n# Prompts for male lifespan\nSystem You are an expert at predicting future events. User If you were to evaluate the lifespan of a random T-year-old man, what age would you predict he might reach? Please limit your answer to a single value without outputing anything else.\n# Prompts for movie grosses\nSystem You are an expert at forecasting movie revenue.\nUser Consider a movie that has already earned X million dollars at the box office, but you\u2019re unsure of how long it has been showing. Based on this information, what would be your prediction of the movie\u2019s total earnings in million dollars by the end of its run? Please limit your answer to a single value without outputing anything else.\n# Prompts for length of poems\nSystem You are an expert at predicting length of poems. User Imagine your friend shares her favorite line of poetry with you, which is line X from the poem. How many lines do you think the entire poem contains? Please limit your answer to a single value without outputing anything else.\n# Prompts for reign of pharaohs\nSystem You are an expert at estimating how long Egyptian pharaohs ruled. User If you found information in a book on ancient Egypt stating that a pharaoh had already been in power for X years, how long in years do you think his reign lasted? Please limit your answer to a single value without outputing anything else.\n# Prompts for movie runtimes\nSystem You are an expert at predicting the total run times of movies. User During a surprise visit to a friend\u2019s house, you discover they\u2019ve been watching a movie for T minutes. Based on this, how long do you think the movie will be in total, in minutes? Please limit your answer to a single value without outputing anything else.\n# Prompts for cake baking times\n# Prompts for superhuman AI\nSystem You are an expert at predicting future events. User If artificial intelligence reaches human-level intelligence by T, when might it surpass human capabilities in all areas? Please provide your prediction as a single year. Do not include any additional text or explanation in your response.\n# Prompts for zero carbon emission\nSystem You are an expert at predicting future events. User If humans manage to achieve 100% renewable energy sources by T, when might global carbon emissions reach zero? Please provide your prediction as a single year. Do not include any additional text or explanation in your response.\nSystem You are an expert at predicting future events. User If humans were able to colonize the Moon by T, when might they colonize Mars? Please provide your prediction as a single year. Do not include any additional text or explanation in your response.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2481/248187e5-2eee-4698-a54f-09c771e92a66.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Comparison of Bayesian models\u2019 predictions (x-axis) with GPT-4\u2019s actual responses (y-axis). (a) Causal generative case. The plot was window-binned along the x-axis into 13 bins for better visualization. Error bars denote \u00b1SE. (b) Causal preventive case. The plot was window-binned along the x-axis into 13 bins for better visualization. Error bars denote \u00b1SE.</div>\n# B Bayesian Models for GPT-4\u2019s Predictions\nAs shown in Figure 7, we present the pairwise relationships between the predicted responses from Bayesian models with different priors and GPT-4\u2019s responses.\n# Directly Prompting GPT-4 to Predict Speculative Events\nIn this section, we showcase typical responses from GPT-4 when asked to predict speculative events, including the developmental timelines for superhuman AI (Figure 8), zero carbon emissions (Figure 9), and a Mars colony (Figure 10). As expected, most of GPT-4\u2019s responses begin with an apologetic statement about being unable to provide precise predictions for each event. In instances where the responses remain relevant and reveal some genuine information, they are predominantly uncertain and provide broad date ranges (e.g., \u201cstarting in the 2030s or 2040s,\u201d \u201csomewhere between 2040 and 2100,\u201d or \u201caround the 2030-2050 timeframe\u201d).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7889/7889ac30-b0c7-4b89-a610-1e58f97c9c64.png\" style=\"width: 50%;\"></div>\nFigure 8: Example uncertain responses from GPT-4 when prompted directly about the timing of the development of superhuman AI.\n<div style=\"text-align: center;\">Figure 8: Example uncertain responses from GPT-4 when prompted directly about the timing of the development of superhuman AI.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e1d/1e1dd80d-81ef-453e-b561-1a48697af9be.png\" style=\"width: 50%;\"></div>\nFigure 9: Example uncertain responses from GPT-4 when prompted directly about the timing of achieving zero carbon emission.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b60c/b60ccff7-0ef7-4fff-ad93-5838a8856808.png\" style=\"width: 50%;\"></div>\nFigure 10: Example uncertain responses from GPT-4 when prompted directly about the timing of the establishment of a Mars colony.\n",
    "paper_type": "method",
    "attri": {
        "background": "As Large Language Models (LLMs) become increasingly integrated into diverse real-world applications, there is a pressing need to understand their decision-making processes. This is particularly crucial in scenarios where LLMs are granted agency to act and make decisions independently. An essential component in understanding the decision-making processes of LLM is identifying the background knowledge they implicitly possess.",
        "problem": {
            "definition": "The problem this paper addresses is the challenge of eliciting and understanding the prior distributions that Large Language Models (LLMs) use when making decisions, which is critical for their effective deployment in real-world applications.",
            "key obstacle": "The main difficulty lies in the fact that existing methods for eliciting priors from LLMs do not adequately capture the implicit knowledge these models encode, making it challenging to understand their decision-making processes."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea stems from cognitive psychology, particularly the concept of iterated learning, which has been shown to effectively elicit prior distributions in human participants.",
            "opinion": "The proposed idea involves using an iterated learning procedure to elicit prior distributions from LLMs, specifically through a prompt-based workflow that leverages the model's ability to generate responses based on fixed weights.",
            "innovation": "The key innovation of this method is the application of iterated learning to in-context learning rather than in-weight learning, allowing for the extraction of implicit knowledge without the risk of model collapse."
        },
        "method": {
            "method name": "Iterated In-Context Learning",
            "method abbreviation": "IICL",
            "method definition": "Iterated in-context learning is a prompt-based workflow that employs a Markov chain Monte Carlo approach to elicit prior distributions from LLMs by generating successive inferences.",
            "method description": "The method involves prompting the LLM to make predictions iteratively, where each prediction informs the next prompt, effectively sampling from the model's prior distribution.",
            "method steps": [
                "Initialize the model with a prompt.",
                "Generate a prediction based on the current data.",
                "Use the prediction to create new data for the next iteration.",
                "Repeat the process for a predetermined number of iterations."
            ],
            "principle": "This method is effective because it leverages the model's generative capabilities to produce responses that reflect its underlying prior distributions, thus enabling the extraction of implicit knowledge."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using tasks where human priors are well-documented, including causal learning, proportion estimation, and predicting everyday quantities.",
            "evaluation method": "The performance of the method was assessed by comparing the priors elicited from GPT-4 with established human priors, using statistical tests to determine convergence and alignment."
        },
        "conclusion": "The results demonstrate that the priors elicited from GPT-4 using iterated in-context learning closely align with human priors across various tasks, indicating the method's effectiveness in uncovering the implicit knowledge encoded in LLMs.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to recover human-like priors from LLMs, providing insights into their decision-making processes and enhancing their interpretability.",
            "limitation": "One limitation of the method is the assumption that LLMs function as approximate Bayesian agents, which may not fully capture the complexities of their decision-making mechanisms.",
            "future work": "Future research could explore the theoretical underpinnings of how LLMs learn to encode priors and investigate the applicability of iterated learning to a broader range of tasks and models."
        },
        "other info": [
            {
                "Acknowledgments": "This work was supported by the NOMIS Foundation and Microsoft Azure credits supplied to Princeton."
            },
            {
                "References": "The paper cites various studies and methodologies related to iterated learning, Bayesian inference, and the implications of LLMs in real-world applications."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper discusses the foundational concept of in-context learning as a method to elicit prior distributions from Large Language Models (LLMs) through a prompt-based workflow."
        },
        {
            "section number": "1.3",
            "key information": "The paper emphasizes the role of Large Language Models in decision-making processes, particularly how they can generate responses that reflect their underlying prior distributions."
        },
        {
            "section number": "3.1",
            "key information": "The paper presents the method of Iterated In-Context Learning (IICL) as a mechanism enabling LLMs to adaptively sample from their prior distributions through iterative prompting."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective in the paper is grounded in cognitive psychology, particularly the concept of iterated learning, which informs the understanding of how in-context learning functions in LLMs."
        },
        {
            "section number": "4.1",
            "key information": "The paper highlights that effective prompt design is critical, as the proposed IICL method relies on generating predictions iteratively based on previous responses."
        },
        {
            "section number": "6.1",
            "key information": "The paper discusses the limitation of assuming that LLMs function as approximate Bayesian agents, which raises concerns about model bias in their decision-making processes."
        },
        {
            "section number": "6.4",
            "key information": "The paper indicates that future research could explore the applicability of iterated learning to a broader range of tasks and models, addressing scalability challenges in in-context learning."
        }
    ],
    "similarity_score": 0.7035694192988399,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Eliciting the Priors of Large Language Models using Iterated In-Context Learning.json"
}