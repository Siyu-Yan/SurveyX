{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.15618",
    "title": "MLPs Learn In-Context on Regression and Classification Tasks",
    "abstract": "In-context learning (ICL), the remarkable ability to solve a task from only input exemplars, is often assumed to be a unique hallmark of Transformer models. By examining commonly employed synthetic ICL tasks, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context. Moreover, MLPs, and the closely related MLP-Mixer models, learn in-context competitively with Transformers given the same compute budget in this setting. We further show that MLPs outperform Transformers on a series of classical tasks from psychology designed to test relational reasoning, which are closely related to in-context classification. These results underscore a need for studying in-context learning beyond attention-based architectures, while also challenging strong prior arguments about MLPs\u2019 limited ability to solve relational tasks. Altogether, our results highlight the unexpected competence of MLPs, and support the growing interest in all-MLP alternatives to task-specific architectures.",
    "bib_name": "tong2024mlpslearnincontextregression",
    "md_text": "# MLPS LEARN IN-CONTEXT ON REGRESSION AND CLASSIFICATION TASKS\nWilliam L. Tong & Cengiz Pehlevan School of Engineering and Applied Sciences Center for Brain Sciences Kempner Institute for the Study of Artificial and Natural Intelligence Harvard University, Cambridge, MA 02138 {wtong@g,cpehlevan@seas}.harvard.edu\nWilliam L. Tong & Cengiz Pehlevan School of Engineering and Applied Sciences Center for Brain Sciences Kempner Institute for the Study of Artificial and Natural Intelligence Harvard University, Cambridge, MA 02138 {wtong@g,cpehlevan@seas}.harvard.edu\n# ABSTRACT\nIn-context learning (ICL), the remarkable ability to solve a task from only input exemplars, is often assumed to be a unique hallmark of Transformer models. By examining commonly employed synthetic ICL tasks, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context. Moreover, MLPs, and the closely related MLP-Mixer models, learn in-context competitively with Transformers given the same compute budget in this setting. We further show that MLPs outperform Transformers on a series of classical tasks from psychology designed to test relational reasoning, which are closely related to in-context classification. These results underscore a need for studying in-context learning beyond attention-based architectures, while also challenging strong prior arguments about MLPs\u2019 limited ability to solve relational tasks. Altogether, our results highlight the unexpected competence of MLPs, and support the growing interest in all-MLP alternatives to task-specific architectures.\n# INTRODUCTION\nThe last few years have witnessed meteoric progress in neural language models. Catalyzed by the Transformer architecture and driven by a steady increase in scale, these aptly-named Large Language Models (LLMs) demonstrate unprecedented competence in drafting grammatical text, answering questions, summarizing content, generating creative output, and even reasoning through non-trivial puzzles (Bubeck et al., 2023; Brown et al., 2020; Achiam et al., 2023). Crucial to an LLM\u2019s proficiency is its ability to learn in-context (Lu et al., 2023; Dong et al., 2022; Brown et al., 2020). In-context learning (ICL) refers to a task paradigm where exemplars from a novel task are presented during inference time rather than during training (Figure 1a). The model must then respond correctly to a query based only on these novel exemplars. No weight updates occur throughout this process; rather, the model infers the task from the input exemplars and, despite having fixed weights, produces the correct output. ICL is commonly assumed to be a unique ability of Transformers, and explanations of the phenomenon often ground their constructions in attention-based architectures (Aky\u00fcrek et al., 2024; Von Oswald et al., 2023; Zhang et al., 2023; Reddy, 2024; Lu et al., 2024). On controlled regression and classification tasks targeted specifically for evaluating ICL, we demonstrate that the simple multi-layer perceptron (MLP) can also learn in-context \u2014 and moreover, learn in-context competitively with the full Transformer given the same compute budget.1 These results suggest that ICL is not an exclusive feature of attention-based architectures, and highlights the need for studying the phenomenon in a broader setting.\nTasks. We focus on controlled tasks commonly studied in the ICL literature, where the specific capacity for in-context learning can be precisely characterized. These tasks are necessarily synthetic\n1Universal approximation by MLPs suggests that they may be able to learn in-context, though at uncertai cost. We demonstrate that MLPs learn in-context without significantly larger compute (and occasionally quite  bit smaller) than Transformers.\napproximations of natural language ICL prompting, but allow us to disambiguate a model\u2019s capacity for in-context learning from its ability to attain natural language fluency. In Section 2, we examine ICL versions of regression (Garg et al., 2022; Aky\u00fcrek et al., 2024; Ravent\u00f3s et al., 2024; Zhang et al., 2023) and classification (Reddy, 2024; Chan et al., 2022). As the two primary task paradigms of machine learning, regression and classification form a representative basis for measuring ICL competency. In Section 3, we consider a series of classical tasks used in the psychology literature to probe relational reasoning (Campbell et al., 2023; Skinner, 1950; Sabl\u00e9-Meyer et al., 2021), which are functionally in-context classification. On these tasks, we find that MLPs outperform Transformers, challenging common beliefs about MLPs\u2019 proficiency at relational reasoning (see Appendix A for an extended discussion). In focusing on controlled tasks, we avoid confounds irrelevant to ICL introduced by naturalistic settings like language and vision. Nonetheless, our findings remain consistent with existing results that do test MLPs in these more complex domains (Tolstikhin et al., 2021; Liu et al., 2021; Fusco et al., 2022; Bachmann et al., 2024).\napproximations of natural language ICL prompting, but allow us to disambiguate a model\u2019s capacity for in-context learning from its ability to attain natural language fluency. In Section 2, we examine ICL versions of regression (Garg et al., 2022; Aky\u00fcrek et al., 2024; Ravent\u00f3s et al., 2024; Zhang et al., 2023) and classification (Reddy, 2024; Chan et al., 2022). As the two primary task paradigms of machine learning, regression and classification form a representative basis for measuring ICL competency. In Section 3, we consider a series of classical tasks used in the psychology literature to probe relational reasoning (Campbell et al., 2023; Skinner, 1950; Sabl\u00e9-Meyer et al., 2021), which are functionally in-context classification. On these tasks, we find that MLPs outperform Transformers, challenging common beliefs about MLPs\u2019 proficiency at relational reasoning (see Appendix A for an extended discussion). In focusing on controlled tasks, we avoid confounds irrelevant to ICL introduced by naturalistic settings like language and vision. Nonetheless, our findings remain consistent with existing results that do test MLPs in these more complex domains (Tolstikhin et al., 2021; Liu et al., 2021; Fusco et al., 2022; Bachmann et al., 2024). Ground rules. To ensure different architectures are comparable across tasks, we observe the following ground rules. First, we compare models based on the total compute required for training (measured in peta-floating point operations, PFLOPs), which summarizes influences like parameter count, training iterations, and architectural efficiency. Details on how we compute this quantity are provided in Appendix C.13. Measuring by compute reflects the practical use of these models, fairly compares architectures by performance per floating-point cost, and is an established scale for defining neural scaling laws (Kaplan et al., 2020). Second, where a single model is required, we select the best\nGround rules. To ensure different architectures are comparable across tasks, we observe the following ground rules. First, we compare models based on the total compute required for training (measured in peta-floating point operations, PFLOPs), which summarizes influences like parameter count, training iterations, and architectural efficiency. Details on how we compute this quantity are provided in Appendix C.13. Measuring by compute reflects the practical use of these models, fairly compares architectures by performance per floating-point cost, and is an established scale for defining neural scaling laws (Kaplan et al., 2020). Second, where a single model is required, we select the best model configuration as measured by loss, keeping compute cost equal across architectures. Data are presented online, reflecting the \u201cone-pass\" setting common in training large language models (Brown et al., 2020). Specific model and task configurations are enumerated in Appendix C.\n# 1.1 RELATED WORK\nIn-context learning has been widely studied in a number of controlled settings. In particular, ICL has been reproduced for linear regression, where a Transformer trained to perform the task can extrapolate to novel input/label pairs provided in-context (Garg et al., 2022; Aky\u00fcrek et al., 2024; Ravent\u00f3s et al., 2024; Wu et al., 2024; Bai et al., 2024; Li et al., 2023; Lu et al., 2024). Proposed mechanisms whereby a Transformer accomplishes the feat include that the Transformer implements some form of gradient descent (Von Oswald et al., 2023; Aky\u00fcrek et al., 2024) or recapitulates least-squares or Ridge regression (Zhang et al., 2023; Aky\u00fcrek et al., 2024; Lu et al., 2024). It has also been observed that a Transformer interpolates between in-weight learning (IWL), the traditional paradigm where the model learns specific examples through training, to in-context learning, where the model uses only exemplars provided in the input context at inference time (Ravent\u00f3s et al., 2024; Wu et al., 2024). Such a transition occurs as a function of data diversity, where datasets with more distinct examples encourage the development of ICL competency. Analogous phenomena have been observed in in-context classification tasks (Chan et al., 2022; Reddy, 2024). Impressively, the ICL performance attained in these tasks by Transformers approaches Bayes optimality (Xie et al., 2021; Bai et al., 2024; Li et al., 2023; Ahuja et al., 2024; Lu et al., 2024). These studies nearly all ground their investigations in Transformer models, and explicitly assume that the model uses an attention mechanism to implement ICL. The exceptions include Chan et al. (2022), who discover that recurrent neural networks (both vanilla RNNs and LSTMs) are unable to learn an in-context classification task under the same conditions where a Transformer can, and Xie et al. (2021), who discover that LSTMs can in fact learn in-context on a synthetic language modeling task. Recently, Lee et al. (2024) found that a wide variety of causal sequence models can learn in-context on a broad array of toy tasks, with varying degrees of success. Park et al. (2024) support this finding by showing how state space models and their hybrids with Transformers can learn in-context competitively. To the best of our knowledge, no prior work has examined in-context learning in vanilla MLPs. The current resurgence of interest in applying MLPs to modern, complex tasks originates with Tolstikhin et al. (2021), which introduced the MLP-Mixer model. Mixers operate by alternating MLPs across the dimensions of the input, treating the remaining dimensions as batch dimensions. Despite their simplicity, Mixers attain state-of-the-art performance on image classification, recalling the broad observation that \u201cless inductive bias is better\" (Sutton, 2019; Bachmann et al., 2024). In\nthe ensuing years, \u201call-MLP\" models based primarily on MLP components have spread across many areas including vision (Bachmann et al., 2024) and natural language (Liu et al., 2021; Fusco et al., 2022). While strong performance has been documented on natural language, less is known about MLPs\u2019 specific proficiency for ICL, and how it compares with Transformer models. In this study, we select a series of controlled, representative tasks that clarify an MLP\u2019s surprising competence for ICL. Our findings underscore the ultimate utility of MLPs, uncovering avenues of both theoretic and practical interest.\n# 2 EXPERIMENT: IN-CONTEXT TASKS\nWe begin by exploring MLPs\u2019 behavior in a controlled ICL format, where their specific capacities and weaknesses can be precisely characterized. Specifically, we examine two tasks: in-context regression and in-context classification.\n# 2.1 ICL REGRESSION\nWe present in-context regression following its common formulation (Garg et al., 2022; Zhang et al., 2023). The input consists of a sequence of values (x1, y1), (x2, y2), . . . , (xL, yL), where xi \u2208Rn and yi \u2208R. The xi, yi pairs are linearly related through a set of weights \u03b2 \u2208Rn such that yi = xi \u00b7 \u03b2 + \u03b5, with noise \u03b5 \u223cN(0, \u03c32). Finally, the input includes a query xq. The model output is a single scalar regressed against the corresponding yq. Crucially, the weights \u03b2 vary between input sequences. The model cannot rely on learning any one \u03b2. Rather, it must infer from context exemplars (xi, yi) what the corresponding \u03b2 must be, and use this to predict the correct output yq. Figure 1b illustrates the task, with additional details in Appendix C. In the main text, our task fixes the number of context points at L. A common variation on this tasks allows the number of context points to vary, and trains the model autoregressively. Results on this autoregressive variation are presented in Figure 5, and are unchanged from the fixed-length case. Following Ravent\u00f3s et al. (2024), we consider two different task distributions: finite and unrestricted. For the finite distribution, we fix a finite pool of weights before training \u03b21, \u03b22, . . . , \u03b2k, where \u03b2i \u223cN(0, I/n). For each input, we sample a new \u03b2 by selecting uniformly at random one weight from the pool {\u03b2i}k i=1. Larger k corresponds to higher data diversity. For the unrestricted distribution, a new set of weights is sampled for each input \u03b2 \u223cN(0, I/n). The unrestricted distribution can be thought of as the k \u2192\u221ecase, and requires full ICL competency in order to infer the correct weights relating the context exemplars. Unless otherwise stated, we use n = 8 dimensional inputs. Results. We first consider how MLPs perform compared to Transformers on in-context regression. To do so, we train and test using online samples drawn from the unrestricted task distribution, requiring all models to learn an in-context solution. Figure 1c plots the MSE achieved by different architectures as a function of total compute. With sufficient compute, MLPs, Mixers, and Transformers all perform in-context regression with near optimal MSE, which is given by Ridge regression on context points using the Bayes optimal regularization parameter (Appendix C.6). For smaller compute, Transformers attain somewhat better MSE than their MLP counterparts, though the difference is modest and performance across all three architectures overlaps substantially. One domain in which a vanilla MLP is decisively worse than a Transformer is for long context length. Figure 1d plots the excess MSE obtained after training and testing on the unrestricted task distribution for varying number of points in the context, where {excess MSE} = {model MSE} - {Bayes optimal Ridge MSE}. The Transformer generally approaches the optimal MSE regardless of context length, though it performs with less stability for longer contexts. The vanilla MLP worsens quickly with larger contexts and approaches the performance of an estimator that returns zero for every input. Strikingly, the MLP mixer does not exhibit the same sensitivity to context length, and continues attaining the Bayes optimal MSE consistently even for very long contexts. One final observation: as data diversity increases, Transformers exhibit a transition from in-weight learning (IWL), the traditional paradigm where the model learns specific examples through training, to in-context learning, where the model uses only context exemplars presented at inference time (Ravent\u00f3s et al., 2024). We next show that MLPs exhibit a similar transition. Following Ravent\u00f3s et al.\nWe present in-context regression following its common formulation (Garg et al., 2022; Zhang et al., 2023). The input consists of a sequence of values (x1, y1), (x2, y2), . . . , (xL, yL), where xi \u2208Rn and yi \u2208R. The xi, yi pairs are linearly related through a set of weights \u03b2 \u2208Rn such that yi = xi \u00b7 \u03b2 + \u03b5, with noise \u03b5 \u223cN(0, \u03c32). Finally, the input includes a query xq. The model output is a single scalar regressed against the corresponding yq. Crucially, the weights \u03b2 vary between input sequences. The model cannot rely on learning any one \u03b2. Rather, it must infer from context exemplars (xi, yi) what the corresponding \u03b2 must be, and use this to predict the correct output yq. Figure 1b illustrates the task, with additional details in Appendix C. In the main text, our task fixes the number of context points at L. A common variation on this tasks allows the number of context points to vary, and trains the model autoregressively. Results on this autoregressive variation are presented in Figure 5, and are unchanged from the fixed-length case. Following Ravent\u00f3s et al. (2024), we consider two different task distributions: finite and unrestricted. For the finite distribution, we fix a finite pool of weights before training \u03b21, \u03b22, . . . , \u03b2k, where \u03b2i \u223cN(0, I/n). For each input, we sample a new \u03b2 by selecting uniformly at random one weight from the pool {\u03b2i}k i=1. Larger k corresponds to higher data diversity. For the unrestricted distribution, a new set of weights is sampled for each input \u03b2 \u223cN(0, I/n). The unrestricted distribution can be thought of as the k \u2192\u221ecase, and requires full ICL competency in order to infer the correct weights relating the context exemplars. Unless otherwise stated, we use n = 8 dimensional inputs. Results. We first consider how MLPs perform compared to Transformers on in-context regression. To do so, we train and test using online samples drawn from the unrestricted task distribution, requiring\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/235f/235fa0ba-bca1-4e14-8f6a-150fb201e7f2.png\" style=\"width: 50%;\"></div>\nFigure 1: ICL regression and classification results. (a) ICL presents context exemplars from a novel task (red), followed by a query input (blue). The model must infer the solution (green) based on the context. (b) ICL regression example. The model receives linearly-related input points, and must regress the query point. (c) Compute vs. MSE on the unrestricted task distribution. Each point represents a single model, with particular parameters and training iterations. At large compute, MSE is approximately equal across all architectures. The red line corresponds to the Bayes optimal Ridge MSE. (d) Excess MSE (MSE above Bayes optimal) for varying context length L on the unrestricted task distribution. Excess MSE remains flat for Mixers, but rises somewhat for Transformers. MLPs fail to learn in-context at all beyond 26 context exemplars. The grey line corresponds to the excess MSE incurred by always guessing zero. (e, f) IWL to ICL transition with increasing data diversity. We train on a finite distribution with k weights, then test on both the finite training distribution and the unrestricted distribution. All models exhibit a transition from IWL (represented by dMMSE) to ICL (represented by Ridge) as k increases. Note: it is possible to \u201coutperform\" Bayes optimal Ridge on the finite training distribution by learning in-weight the underlying \u03b2\u2019s. (g) ICL classification example, with burstiness B = 3. Multiple clusters may share the same label. (h) Compute vs. cross entropy loss on ICL classification, with k = 2048 clusters, B = 4, and L = 8, which pushes all models to learn in-context. At large compute, all architectures attain near-zero cross entropy loss. The gray line corresponds to loss obtained from placing equal probability on the 2 (of C = 32) labels present in context. (i) Cross entropy loss for varying context length L on the task configuration in (h). Loss is relatively flat for all architectures, though it increases a little for Mixers. (j) IWL to ICL transition with increasing data diversity, where L = 8 and B = 4. All models exhibit a transition from IWL to ICL as the number of clusters k increases. (all) We use n = 8 dimension inputs. All line plots feature 95 percent confidence intervals about the mean, estimated from 5 replications.\n(2024), we train each model on a finite distribution with k fixed regression weights. As we increase k, we record the MSE obtained by each model on both the finite distribution \u03b2 \u223cU \ufffd {\u03b2i}k i=1 \ufffd using the same \u03b2\u2019s from training (Figure 1e) and the unrestricted distribution \u03b2 \u223cN(0, I/n) where \u03b2\u2019s are sampled anew (Figure 1f). We determine whether a model has learned the in-weight solution by comparing its MSE to that of the discrete minimum mean squared error (dMMSE) estimator, which is a Bayesian estimator derived from a prior matched to the finite training distribution (see Appendix C.6 for details).2 We characterize the in-context solution by a Ridge estimator with the Bayes optimal choice of regularization. For small k, all models demonstrate in-weight learning by tracing the dMMSE curve. As k increases, we observe a swift transition to the Ridge curve, indicating a transition to in-context learning. The Transformer makes this transition at a somewhat smaller k than the MLP models. We consider additional plots and parameterizations in Appendix D.\n# 2.2 ICL CLASSIFICATION\nFollowing Reddy (2024), we present in-context classification as follows. The input consists of a sequence of context exemplars (x1, y1), (x2, y2), . . . , (xL, yL) followed by a query point xq, where xi, yi \u2208Rn. The x points are sampled from a Gaussian mixture model Mk consisting of k components. Each mixture component (i.e. cluster) is labeled by one among C labels, where k \u2265C, so multiple clusters may map to the same label. Labels are represented in the context by vectors \u03b11, \u03b12, . . . \u03b1C \u2208Rn. If xi belongs to cluster j, then yi = \u03b1j. The model must predict the correct label for xq, and outputs C logits corresponding to the C labels (not a vector of values \u03b1, which are used only to represent labels in the context). Figure 1g illustrates this task, with additional details in Appendix C.7. Importantly, the query point xq shares a cluster with at least one of the context points x1, x2, . . . , xL. Mixture components and cluster labels remain fixed throughout training. Hence, the model can learn either an in-weight solution by memorizing the label for each cluster, or an in-context solution by referencing the class label associated with xq among the context exemplars. We also consider the input\u2019s burstiness B, which is the number of repeats per cluster in the context (B must divide the context length L). For example, B = 3 means there are exactly three points from each cluster represented in the inputs. Data diversity corresponds to the number of clusters k, where larger k correspond to more diverse dataset. Unless otherwise stated, we use n = 8 dimensional inputs. Results. We first compare how different architectures perform at ICL across different compute in Figure 1h. The task is parameterized by burstiness B = 4 and k = 2048 with L = 8 points in the context, a setting in which all models develop an in-context solution (see Figure 7d for details). As before, with sufficient compute Transformers do not outperform vanilla MLPs or Mixers. Indeed, at small compute, vanilla MLPs attain a somewhat lower loss. Note: in this setting, there are L/B = 2 labels present in each context, out of C = 32 total possible labels. As a baseline, we plot in gray the loss obtained by placing equal probability on the 2 labels present in-context. MLPs in particular appear to plateau at this baseline before approaching zero loss with higher compute. We also measure how well each model handles increasingly long context lengths in Figure 1i. In a surprising reversal, vanilla MLPs attain a relatively flat loss across context lengths, as do Transformers. Mixers\u2019 loss increases somewhat for longer contexts, though this blip vanishes at higher dimensions (Figure 7b). Overall, MLPs continue to perform at a comparable level with Transformers on in-context classification. Finally, we observe a transition from IWL to ICL across the three architectures as data diversity increases. As in Reddy (2024), we measure IWL by constructing test examples where the query point does not appear in the context. The only way the model correctly classifies these points is if it memorizes the mapping from cluster to label. To measure ICL, we consider two different strategies: 1) sample points from an entirely different mixture M\u2032 k, producing novel clusters, or 2) swap cluster/label mappings so that clusters are labeled differently than they were during training. Test examples from either strategy can only be solved if the model identifies cluster labels incontext, since the underlying cluster label assignment is different from training.3 We plot accuracy 2The dMMSE estimator averages across the k weights in the finite training distribution based on their fit to\n2The dMMSE estimator averages across the k weights in the finite training distribution based on their fit t the current context exemplars. 3In practice, accuracy on these two ICL measures is virtually identical across all models and settings.\nacross all three types of test examples in Figure 1j for increasing k, and observe a transition from IWL to ICL across all three model architectures. The transition happens for somewhat lower data diversity in Transformers and Mixers, and somewhat higher in vanilla MLPs. Additional plots and parameterizations are explored in Appendix D.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/586f/586f3be8-4f0b-4881-a2cc-0191b2ae6b26.png\" style=\"width: 50%;\"></div>\nFigure 2: Relational reasoning results. Global legend is at the bottom right. (a) Match-to-sample task. (b) Compute vs. cross entropy loss on MTS task. Each point represents a single model, with particular parameters and training time. RB MLPs attain the best loss with the smallest compute, followed by MLPs and Transformers. (c) OOD generalization on MTS. In-distribution radii are highlighted in red. MLPs and RB MLPs generalize well on OOD radii. No model generalizes well on OOD test scrambling. (d) Sphere oddball task. (e) Same as in (b), for sphere oddball. (f) OOD generalization on sphere oddball. In-distribution distance is highlighted in red. Red dashed lines correspond to the accuracy obtained by guessing that the furthest point away from the cluster center is the oddball. (g) Logit of oddball point as its distance from the center increases. Dashed lines correspond to different polynomial scalings. Only the Transformer fails to increase its logit with distance. (h) Line oddball task. (i) Compute vs. loss on line oddball task. RB MLP no longer learns the task well, but appending additional MLP layers (\u201cRB MLP (deep)\") helps. (j) OOD generalization on line oddball. In-distribution distance is highlighted in red. Red lines indicate accuracy attained by a model guessing that the furthest point away from the center is the oddball. MLPs continue to generalize stronger than Transformers, and match the deep RB MLP. (all) Shaded regions and error bars correspond to 95 percent confidence intervals estimated from 5 replications.\n# 3 EXPERIMENT: RELATIONAL TASKS\nWe next consider classical tasks from psychology used to study relational reasoning in humans, animals, and neural networks (Campbell et al., 2023; Skinner, 1950; Sabl\u00e9-Meyer et al., 2021; Geiger et al., 2023). These tasks are functionally a subset of in-context classification, and rely on understanding similarity relationships between context exemplars.\nIn a surprising advance from the tasks explored in Section 2, we find that MLPs perform better with less compute than Transformers, and generalize more effectively on out-of-distribution test sets. As a benchmark for gold-standard performance using hand-crafted features, we implement a relationally bottlenecked MLP (RB MLP), an architecture demonstrated to solve many challenging relational tasks with competitive generalization and efficiency characteristics (Webb et al., 2023; Campbell et al., 2023). Relational bottlenecks are architectural components that prevent absolute information about the inputs from propagating downstream; rather, the RB computes a set of (hand-crafted) relations between inputs (often simple dot products), and allows only these relations to flow downstream, forcing the model to operate on abstractions. Our RB MLP operates by first computing dot product relations between inputs, then propagating optionally through several MLP layers before a final readout (see Appendix C.5 for details). We find that while relational bottlenecks are helpful when the model\u2019s relations align well with the task structure, they may fail on tasks with deviating structure. All in all, these relational tasks demonstrate that MLPs can quite surprisingly outperform Transformers on certain in-context tasks. The question of whether neural network models can reason relationally at all has been an enduring topic of heated debate (see Alhama and Zuidema (2019) for a recent review). Our results fall decisively in favor of the affirmative, and contrast a recent attempt at formally proving that MLPs cannot reason relationally (Boix-Adsera et al., 2023). In Appendix A, we discuss our relationship with the relational reasoning literature, comment on the proof by Boix-Adsera et al. (2023), and demonstrate empirically that a vanilla MLP solves a task posited by their arguments to be impossible.\n# 3.1 MATCH TO SAMPLE\nThe match-to-sample (MTS) task paradigm originates in Skinnerian behavioral experiments (Skinner, 1950). A test subject is first presented with a sample stimulus (e.g. an image). The subject is then shown a set of many stimuli, and must select the one that matches the original sample. Our MTS task proceeds as follows. The model is presented with L context points x1, x2, . . . , xL \u2208 R2 followed by a query point xq. The context points are evenly spaced along a circle with unit radius centered at the origin. The model must return the index of the context point closest to the query y = arg maxi (xi \u00b7 xq). The points can be thought of as idealized stimulus embeddings in neural representation space. A model must reason correctly about distances between points, and ignore their absolute location (which varies from example to example). Framed this way, the MTS setup is an in-context classification task with one context point per class. In the results that follow, we use L = 5 points in the context. Figure 2a presents an illustration of the task, with additional details in Appendix C.8. In addition to the vanilla MLP and Transformer models, we also consider a relationally bottlenecked MLP architecture (RB MLP) (Webb et al., 2023). The RB MLP uses dot-product relations r between the query point and each of the five context points r = (xq \u00b7 x1, xq \u00b7 x2, . . . , xq \u00b7 xL). The relations r are passed directly to a softmax readout, producing class predictions yRB = smax (W readout r). Note, a choice of weights W readout = I solves the task perfectly, though it remains to be seen whether the RB model discovers this solution. Further details on the RB MLP model are in Appendix C.5.\nResults. Figure 2b plots the loss obtained by each of the three models on the MTS task as a function of compute. The vanilla MLP outperforms the Transformer by a surprising margin. With relations that are well-aligned to the task, the RB MLP model achieves the best compute efficiency. We also consider how well each model performs in different kinds of out-of-distribution test examples. Results are plotted in Figure 2c. We first try perturbing the radius of the circle after training with unit radius. As we vary the radius during testing, both MLP models continue to perform well, but the Transformer quickly degenerates. We also try changing the order of context points. If the points are ordered, they are presented in clockwise order along the circle. If the points are scrambled, they are presented in random order. Curiously, if the models are trained first on ordered points, then no model generalizes well when subsequently tested with scrambled points (not even the relationally bottlenecked model).\nThe oddball tasks described in the next two sections are based on work from Sabl\u00e9-Meyer et al. (2021), who used it to measure geometric relational reasoning in humans and baboons. In an oddball task, the test subject is presented with six stimuli, originally geometric images. One image differs from the others. The subject must select this \u201coddball\u201d to succeed. Like the MTS task, the oddball tasks are a subset of ICL classification where all-but-one point belong to the same class. As before, our version of the oddball task simplify full visual stimuli into idealized stimulus representations. The model is presented with L context points x1, x2, . . . , xL \u2208R2. (There are no query points.) In the sphere oddball task, the context points are sampled as x \u223cN(\u00b5, I), for some nonzero center \u00b5. One point in the context is randomly selected and perturbed in a random direction by a distance d. The model must return the index of this oddball point. In the results that follow, we use L = 6 points in the context. Figure 2d presents an illustration of the task, with additional details in Appendix C.9. In addition to the vanilla MLP and Transformer models, we again use an RB MLP with dot-product relations. Given the average context point x = 1 L \ufffd i xi, the relations R form a matrix with entries Rij = (xi \u2212x) \u00b7 (xj \u2212x). These centered4 dot-product relations are flattened and passed directly to a softmax readout, forming class predictions yRB = smax(W readout flat(R)). Note, the sphere oddball task can be solved by finding the point that is furthest away from the cluster center. Hence, a choice of W readout that selects the diagonal relations Rii will solve the task, but it remains to be seen whether the model will learn such a readout. Additional details on the RB MLP are provided in Appendix C.5.\nResults. Figure 2e plots the loss obtained by each model on the sphere oddball task as a function of compute. Again, the vanilla MLP outperforms the Transformer by a wide margin. With well-suited relations, the RB MP achieves the best compute efficiency.\nWe also consider how each model performs on OOD test examples. Training examples consist of oddballs with fixed perturbation distance d = 5. As we vary towards longer distances, we observe in Figure 2f that both the vanilla and RB MLPs continue performing perfectly, while the Transformer\u2019s performance decays. We can also examine how the logit corresponding to the oddball index changes as we change the position of the oddball with respect to the cluster center (Figure 2g). Both the vanilla and RB MLPs learn strictly increasing relationships, suggesting they will correctly generalize to any d provided the other logits do not also increase. The Transformer seems to asymptote to a flat value, suggesting that it ultimately fails to distinguish the oddball logit for large d.\n# 3.3 LINE ODDBALL\nRather than sample context points from a spherical Gaussian, the line oddball task distributes context points along a line. For each training example, we select a line with random orientation that passes through the origin. Context points x1, x2, . . . , xL \u2208R2 are Gaussian distributed along this line with zero mean and unit variance. One context point is selected at random to be the oddball, and is perturbed by a distance d in the direction perpendicular to the line. The model must output the index of the oddball point. We use L = 6 points in the context. Figure 2h presents an illustration of the task, with additional details in Appendix C. We use the same models as for the spherical oddball, including an RB MLP using the same relations R. The line oddball task cannot be solved by simply selecting the point furthest away from all the others for small d. The relevant relations are more sophisticated, and must be sensitive to the linear structure between context points. The line oddball task also presents an alternative hypothesis for the structure of stimuli in representation space. Whereas the sphere oddball presumes that input stimuli are distributed isotropically in representation space, the line oddball task assumes that inputs fall along a favored direction. Neither is obviously more plausible than the other for a general task. However, as we see below, the RB MLP from the past two tasks fails quickly on this task, presenting a simple example in which well-aligned relations can be critical. We also experiment with a \u201cdeep\"\n4Centering was not required in the MTS task above, since the context points in centered.\nRB MLP, which features two additional MLP layers between the relations and readout, and now solves the task at a small compute cost.\nResults. Figure 2i plots the loss for each model as a function of compute. Vanilla MLPs perform just a little better than Transformers. A (shallow) RB MLP fails to solve the task altogether, and loss remains high. A deep RB MLP, which features two additional fully-connected layers after the relational bottleneck, can solve the task. We also compare how each model performs on different out-of-distribution test examples in Figure 2j. We vary the distance d between the oddball point and the line of context points on different training sets. At test time, we compare the accuracy of each model on the whole range of d. As we saw above, MLPs continue to outperform Transformers on almost all OOD test cases. Unless equipped with further layers, the shallow RB MLPs fail to learn the task at all for small d. Though a relationally bottlenecked model can succeed with great efficiency on well-aligned tasks, without relations that are tailored to the task\u2019s underlying structure, an RB model may be disadvantaged.\n# 4 DISCUSSION\nWe observed that MLPs can learn in-context and moreover, perform at a level comparable to Transformers on in-context tasks. We also examined relational reasoning tasks, closely related to ICL classification, which have historically been considered beyond the ability of simple neural architectures like MLPs (Alhama and Zuidema, 2019; Marcus, 1998; Boix-Adsera et al., 2023). Surprisingly, MLPs learn these relational tasks well \u2014 and exhibit both better compute efficiency and generalization performance than Transformers. This observation challenges earlier claims to the contrary (Boix-Adsera et al., 2023; Webb et al., 2023), but is consistent with the emerging realization that, given sufficient data diversity and compute, an MLP can indeed learn to reason relationally (Geiger et al., 2023). We discuss our relationship with prior work in relational reasoning further in Appendix A. Broadly, our work is consistent with the \u201cbitter lesson\u201d of AI research (Sutton, 2019): in the face of increasing compute and data resources, general methods with weaker inductive bias will outperform specialist methods endowed with stronger inductive bias. This heuristic speaks to the intuition that a strong inductive bias may be beneficial for particular tasks, but may misalign the model in different or more general settings. We see an extreme example of this in studying relationally bottlenecked MLPs, where hand-crafted relations strongly benefit the model in specific cases where they align with the task. However, departing even slightly from the ideal task structure prevents the shallow RB MLP from learning the task at all, while a vanilla MLP continues to exhibit strong performance. In the absence of hand-designed relations, Transformers are more general learners than RB MLPs, but less than vanilla MLPs. As a result, for certain well-suited tasks (like ICL regression), Transformers perform more efficiently for a fixed compute budget. But for other tasks (relational reasoning, simple regression and classification in Appendix B), MLPs have the upper hand. These results expand the range of possible tasks commonly thought solvable by MLP models. ICL is clearly not the exclusive domain of Transformers, and we encourage greater engagement with ICL beyond attention-based architectures. The surprising success of MLPs for relational reasoning also encourages a change in perspective about how simple architectures may be capable of solving relational tasks, and under what conditions they fail. The impressive performance of MLPs hints at potential real-world benefits, and we watch the future outcomes of all-MLP approaches with interest. Limitations and future directions. We consider only controlled, synthetic tasks designed to probe for specific characteristics. We never compare architectures on complex datasets like those found in language or vision, though there are other studies that do, and find that MLPs continue to perform competitively (Tolstikhin et al., 2021; Liu et al., 2021; Fusco et al., 2022). The advantage of our approach is that we avoid confounds irrelevant to ICL introduced by complex data, and clarify the specific competencies of each model to learn in-context across representative settings. Nonetheless, an important direction for future work is to study how MLPs perform on more complex tasks, and characterize failure modes if they occur. We also work exclusively in an online setting where models have access to a continuous stream of infinite data. As the bitter lesson heuristic predicts, this setup benefits the MLP, but we can certainly\nimagine that in data-limited scenarios, Transformers and other architectures with stronger inductive bias would dominate. Indeed, we have already observed that Transformers tend to learn in-context with comparatively less data diversity. Examining a data-limited setting represents another important future direction, and will potentially reveal important weaknesses in MLPs. Where possible, we test on a wide array of parameterizations and task settings. The main text figures represent only an illustrative subset of our total results, with supplementary figures provided in Appendix D. However, as with any empirical study, we cannot test every infinite variation on our models and tasks; there may be further unexpected results hiding behind a setting we have not tried. Overall, we hope our results encourage further work studying ICL beyond attention-based architectures, and the properties of simple architectures like MLPs that enable them to solve relational tasks. Important questions remain in quantifying how much data diversity is generally required to transition to ICL, how this threshold depends on architecture, varying sensitivity to context length across architectures, precisely characterizing differences in inductive bias for ICL, and more.\nWe thank Alex Atanasov, Hamza Chaudhry, Alex Meterez, Mo Osman, Sab Sainathan, Jacob Zavatone-Veth, and members of the Pehlevan Group for many helpful comments and discussions on our manuscript. WLT was supported by a Kempner Graduate Fellowship. CP and WLT were supported by NSF Award DMS-2134157 and NSF CAREER Award IIS-2239780. CP is further supported by a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. The computations in this paper were run on the FASRC cluster supported by the FAS Division of Science Research Computing Group at Harvard University.\n# REFERENCES\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809, 2023. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In International Conference on Learning Representations, 2024. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023. Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In International Conference on Learning Representations, 2024.\nYue M. Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic theory of in-context learning by linear attention. arXiv preprint arXiv:2405.11751, 2024. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Allan Ravent\u00f3s, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Advances in Neural Information Processing Systems, 36, 2024. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022. Declan Campbell, Sreejan Kumar, Tyler Giallanza, Jonathan D Cohen, and Thomas L Griffiths. Relational constraints on neural networks reproduce human biases towards abstract geometric regularity. arXiv preprint arXiv:2309.17363, 2023. Burrhus Frederic Skinner. Are theories of learning necessary? Psychological review, 57(4):193, 1950. Mathias Sabl\u00e9-Meyer, Jo\u00ebl Fagot, Serge Caparos, Timo van Kerkoerle, Marie Amalric, and Stanislas Dehaene. Sensitivity to geometric shape regularity in humans and baboons: A putative signature of human singularity. Proceedings of the National Academy of Sciences, 118(16):e2023123118, 2021. Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u2013 24272, 2021. Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. Pay attention to mlps. Advances in neural information processing systems, 34:9204\u20139215, 2021. Francesco Fusco, Damian Pascual, Peter Staar, and Diego Antognini. pnlp-mixer: An efficient all-mlp architecture for language. arXiv preprint arXiv:2202.04350, 2022. Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hofmann. Scaling mlps: A tale of inductive bias. Advances in Neural Information Processing Systems, 36, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In International Conference on Learning Representations, 2024. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594. PMLR, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2021. Kabir Ahuja, Madhur Panwar, and Navin Goyal. In-context learning through the bayesian prism. In International Conference on Learning Representations, 2024.\nIvan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Exploring the relationship between model architecture and in-context learning ability. In International Conference on Learning Representations, 2024. Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024. Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. Atticus Geiger, Alexandra Carstensen, Michael C Frank, and Christopher Potts. Relational reasoning and generalization using nonsymbolic neural networks. Psychological Review, 130(2):308, 2023. Taylor W Webb, Steven M Frankland, Awni Altabaa, Kamesh Krishnamurthy, Declan Campbell, Jacob Russin, Randall O\u2019Reilly, John Lafferty, and Jonathan D Cohen. The relational bottleneck as an inductive bias for efficient abstraction. arXiv preprint arXiv:2309.06629, 2023. Raquel G Alhama and Willem Zuidema. A review of computational models of basic rule learning: The neural-symbolic debate and beyond. Psychonomic bulletin & review, 26:1174\u20131194, 2019. Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, and Joshua Susskind. When can transformers reason with abstract symbols? arXiv preprint arXiv:2310.09753, 2023. Gary F Marcus. Rethinking eliminative connectionism. Cognitive psychology, 37(3):243\u2013282, 1998. Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988. Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. Andrew Y Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of the twenty-first international conference on Machine learning, page 78, 2004. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Grigory Khromov and Sidak Pal Singh. Some intriguing aspects about lipschitz continuity of neural networks. arXiv preprint arXiv:2302.10886, 2023. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021. Gary F Marcus, Sugumaran Vijayan, Shoba Bandi Rao, and Peter M Vishton. Rule learning by seven-month-old infants. Science, 283(5398):77\u201380, 1999. Junkyung Kim, Matthew Ricci, and Thomas Serre. Not-so-clevr: learning same\u2013different relations strains feedforward neural networks. Interface focus, 8(4):20180011, 2018. Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 2873\u20132882. PMLR, 2018. Thomas Serre. Deep learning: the good, the bad, and the ugly. Annual review of vision science, 5: 399\u2013426, 2019. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4334/433421b4-40c9-432c-a255-a58a1cdf7c2f.png\" style=\"width: 50%;\"></div>\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/google/flax. Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021. The pandas development team. pandas-dev/pandas: Pandas, February 2020. URL https://doi. org/10.5281/zenodo.3509134.\nRelational reasoning is closely related to in-context learning, as solving ICL tasks requires reasoning about the relationships between inputs while ignoring their absolute characteristics. Indeed, the relational tasks we explore in the main text are functional subsets of ICL classification. At the same time, MLPs are commonly presumed to fail at relational reasoning (Marcus, 1998; Boix-Adsera et al., 2023) or exhibit severe weaknesses (Fodor and Pylyshyn, 1988). While our primary focus remains on comparing in-context learning between Transformers and MLPs, we offer this digression to contextualize our results within the broader relational reasoning literature. The question of whether connectionist models are able to reason relationally at all has been an enduring topic of passionate debate (see Alhama and Zuidema (2019) for a recent review). Our empirics support the notion that classical architectures like vanilla MLPs can indeed reason relationally, consistent with recent findings in Geiger et al. (2023). However, many researchers presuppose that classical architectures cannot solve relational tasks, resulting in a zoo of alternatives aimed at endowing neural networks with relational capacities (Webb et al., 2023; Battaglia et al., 2018; Geiger et al., 2023; Alhama and Zuidema, 2019). One especially strong claim that MLPs cannot reason relationally was advanced by Boix-Adsera et al. (2023), who formally prove that MLPs will never generalize to unseen symbols on relational tasks. Their proof however relies on a pathological input scheme that hinders learning. Below, we discuss their analysis on MLPs and offer our own remarks on the learnability of relational tasks. We also demonstrate empirically that, under conventional settings, MLPs do generalize on a relational task claimed by Boix-Adsera et al. (2023) to be impossible.\nOne especially strong claim that MLPs cannot reason relationally was advanced by Boix-Adsera et al (2023), who formally prove that MLPs will never generalize to unseen symbols on relational tasks. Their proof however relies on a pathological input scheme that hinders learning. Below, we discuss their analysis on MLPs and offer our own remarks on the learnability of relational tasks. We also demonstrate empirically that, under conventional settings, MLPs do generalize on a relational task claimed by Boix-Adsera et al. (2023) to be impossible.\n# A.1 SUMMARY OF BOIX-ADSERA ET AL.\n# We begin with a brief summary of the relevant theorem in Boix-Adsera et al. (2023). Consider a template z consisting of a sequence of wildcards\nWe begin with a brief summary of the relevant theorem in Boix-Adsera et al. (2023). C template z consisting of a sequence of wildcards\n \u2208W A string x consisting of symbols x = x1x2 . . . xk \u2208X k satisfies z if there exists an injective map s : W \u2192X such that s(\u03b1i) = xi for all i, which we call a substitution map. Finally, we have a labeling function f \u2217: Wk \u2192R that assigns scalar labels to different templates.\nBoix-Adsera et al. (2023) consider MLP models with one-hot encodings of symbols\n \u2208 where exi \u2208R|X| is a vector with 1 at an index corresponding to xi and 0 elsewhere. The one-hot encodings are then flattened as flat(E(x)) \u2208Rk|X| before being passed directly into an MLP. For notation, we write fMLP(x; \u03b8t) as an MLP applied to string x with parameters \u03b8t obtained after t steps of stochastic gradient descent (SGD). We denote Xuns as symbols that are unseen during training, and Xseen as symbols that are seen during training. The theorem is stated as follows Theorem A.1 (From Boix-Adsera et al., failure of MLPs at generalizing on unseen symbols). Suppose the label function f \u2217is non-constant. Then for all SGD steps t, there exists a template z \u2208Wk and a string x consisting of symbols x1x2 . . . xk \u2208X k uns which satisfy z such that \ufffd \ufffd\n\ufffd\ufffd \ufffd\ufffd where c is a constant that depends only on f \u2217, and the expectation is taken over random initialization of parameters \u03b8 and subsequent SGD steps.\nTheir proof relies on the permutation invariance property of MLPs and SGD (Ng, 2004). Summarizing briefly, they argue that if x1, x2 \u2208Xuns, we can swap their one-hot encodings without any impact on the MLP. More generally, we can construct a permutation matrix \u03a0 \u2208Rk|X|\u00d7k|X| such that for all strings of symbols x(1), x(2) \u2208X k uns and x\u2032 \u2208X k seen, it remains true that \u03a0 flat(E(x(1))) = flat(E(x(2))) and \u03a0 flat(E(x\u2032)) = flat(E(x\u2032)). That is, we permute only the indices of unseen symbols, but leave the indices of seen symbols untouched. Then given the permutation symmetry of MLPs and SGD (Ng, 2004), because we preserve the indices of the seen symbols, we must have that\n\ufffd \ufffd \ufffd \ufffd Hence, if f \u2217(x(1)) \u0338= f \u2217(x(2)), the MLP cannot approach arbitrarily close to both labels, incurring an irreducible cost c > 0 that depends on the difference. In this way, MLPs cannot generalize to unseen symbols on any relational task.\n# A.2 A DIFFERENT INPUT SCHEME\nIs there a way to circumvent this impossibility result? One aspect of the proof that may seem suspect is its reliance on flattening one-hot encodings flat(E(x)) as direct input to the MLP. Going as far back as Word2vec (Mikolov et al., 2013), a well-established convention for processing one-hot inputs is to instead pass them through an embedding matrix W e, creating vector embeddings\nwhere d is the dimension of a single symbol\u2019s vector embedding.5 A practitioner then flattens and operates on the resulting vector embeddings, not the one-hot encodings directly. As we will shortly see, if we consider the more conventional input scheme that uses vector embeddings h0(x) and not the one-hot encodings directly, then the conclusion from Boix-Adsera et al. (2023) no longer holds. In particular, we consider an architecture where the input to the MLP is flat(h0(x)), rather than flat(E(x)). We can attempt the same logic as before, and identify a permutation \u03a0 such that for all strings of symbols x(1), x(2) \u2208X k uns and x\u2032 \u2208X k seen, we have that \u03a0 flat(h0(x(1))) = flat(h0(x(2))) and \u03a0 flat(h0(x\u2032)) = flat(h0(x\u2032)). Unfortunately, if the embedding matrix W e is randomly initialized like most neural network parameters, it is virtually impossible to find a permutation where \u03a0 flat(h0(x(1))) = flat(h0(x(2))) while x(1) \u0338= x(2). This is because the probability that any two elements of W e are identical is zero for typical random matrix ensembles used in practice, e.g. if the elements of W e are sampled i.i.d from a normal distribution. Hence, it is clear that the original proof strategy of permuting the input, now flat(h0(x)), has become unviable. However, a skeptical reader might now wonder whether Theorem A.1 might still be saved if we apply permutations to the one-hot encodings before they are passed to the embedding matrix. That is, given a permutation matrix \u03a0 \u2208R|X|\u00d7|X|, we construct\n \u2208 h\u03c0 0(x) = (W e(\u03a0ex1), W e(\u03a0ex2), . . . , W e(\u03a0exk))\u22ba.\nIn this way, Theorem A.1 might still be rescued through permutations on one-hots before the embedding matrix. This method sidesteps the issue with permuting flat(h0(x)) directly, and the MLP trained on SGD remains invariant to any permutation on the underlying one-hots. Hence, it seems the proof may remain valid, and the impossibility result might still holds. Unfortunately, this scheme runs into a different issue: it is impossible to find two inputs x(1), x(2) where \u03a0x(1) = x(2), but that f \u2217(x(1)) \u0338= f \u2217(x(2)). (Note, we have abused notation slightly and write \u03a0ex = \u03a0x.) Indeed, we next show that if x satisfies a template z, then any permutation \u03a0 on the symbols of x will also satisfy z. This can be seen quite simply by considering that 1) by definition, template satisfaction is invariant under a relabeling of symbols and 2) any permutation is a relabeling of symbols \u2014 hence, template satisfaction must be invariant under permutation. We phrase this formally below. Proposition A.1 (Permutation invariance of template satisfaction). For any template z \u2208Wk and any permutation \u03a0 : X \u2192X, if the string x satisfies z, then \u03a0x also satisfies z.\n X \u2192X 5Indeed, in their results on Transformers, Boix-Adsera et al. do use vector embeddings. It is unusual they would choose to omit them in their analysis of MLPs.\nProof. If symbols x = x1x2 . . . xk satisfy the template z = \u03b11\u03b12 . . . \u03b1k, then there exists an injective substitution map s such that s(\u03b1i) = xi. Because permutations \u03a0 are bijective, there must also exist an injective substitution map s\u2032 such that s\u2032(\u03b1i) = \u03a0(xi). Hence, \u03a0x satisfies the template z.\nIn this way, it is not actually possible to find two strings x(1), x(2) such that \u03a0x(1) = x(2) but for which f \u2217(x(1)) \u0338= f \u2217(x(2)) since they both satisfy the same template. Permuting over one-hot encodings before the embedding matrix is not viable. Alternatively, we could try permuting the output from an intermediate layer of the MLP, but this will fail for the same reason that permuting flat(h0(x)) failed. All in all, if we replace the input flat(E(x)) with the more conventional flat(h0(x)), Theorem A.1 is no longer valid.\n# A.3 CAN MLPS REASON RELATIONALLY?\nWe have argued that the impossibility theorem of Boix-Adsera et al. (2023) can be circumvented, but it remains to be seen whether MLPs can truly reason relationally. We next identify coarse conditions that would in principle allow an MLP to generalize to unseen symbols given finite training data. Intuitively, we can imagine that if the MLP\u2019s training data is sufficiently diverse and the model is sufficiently expressive and smooth, then any unseen input x will fall somewhat close to a seen input x\u2032, so fMLP(x) \u2248fMLP(x\u2032) \u2248f \u2217(x\u2032). If x and x\u2032 are labeled the same (not unreasonable, if they are close), then the MLP would indeed be generalizing on an unseen input example. We formalize this intuition in the following proposition, which establishes coarse conditions for a model f to generalize on unseen input. We adopt the same setting as above, but we now treat strings x as real-valued x \u2208Rn. This is equivalent to flattening the vector embeddings h0(x) generated from one-hot encoded symbols x1x2 . . . xk. Doing so simplifies the following discussion. Proposition A.2 (Conditions for generalizing to unseen inputs). Fix \u03b5 and select \u03b4 < \u03b5/3. Given a model f : Rn \u2192R and labeling function f \u2217: Rn \u2192R, if they satisfy the following three conditions 1. Smoothness: f and f \u2217are L-Lipschitz continuous 2. Expressivity: for all x that are seen, |f(x) \u2212f \u2217(x)| < \u03b4. 3. Data diversity: for all x\u2032 that are unseen, there exists an x that is seen such that ||x \u2212x\u2032|| < \u03b4/L then\nfor all x (seen and unseen).\n# Proof. This statement is a simple consequence of the triangle inequality. For any unseen x\u2032 in the \u03b4/L-neighborhood of seen x, we have that\nHence, if we select \u03b4 < \u03b5/3, we must have that |f(x\u2032) \u2212f \u2217(x\u2032)| < \u03b5.\nIn this way, if a model satisfies the above three conditions, it generalizes to unseen inputs for a task defined by the labeling function f \u2217. The first condition for smoothness is regularly achieved by standard neural networks (Khromov and Singh, 2023). The second condition corresponds to a notion of expressivity \u2014 that is, a model f should be able to approach arbitrarily close to zero on its training data. For modern neural network models trained on simple tasks, this is a frequent occurrence (Zhang et al., 2021). The third condition corresponds to a coarse description of data diversity. The training data should be sufficiently diverse such that all unseen examples are very close to an example seen during training. This condition may be difficult to achieve in practice, but it offers a very coarse upper bound on the requisite data diversity required to generalize on unseen examples. Nonetheless, an MLP trained online on a suitably constrained input space may very well achieve this condition.\n\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dba1/dba1f602-4c7b-48dd-a005-00a73cb18d9b.png\" style=\"width: 50%;\"></div>\nFigure 3: MLP accuracy on unseen symbols for the same-different task. The gray dashed line indicates chance-level performance. Shaded region indicates 95 percent confidence regions estimated from 5 replications. For higher data diversity (i.e. number of symbols in the task), the MLP generalizes progressively better. Beyond roughly 29 symbols in the task, the MLP performs substantially above chance, and approaches perfect generalization beyond 212 symbols.\n<div style=\"text-align: center;\">Figure 3: MLP accuracy on unseen symbols for the same-different task. The gray dashed line indicates chance-level performance. Shaded region indicates 95 percent confidence regions estimated from 5 replications. For higher data diversity (i.e. number of symbols in the task), the MLP generalizes progressively better. Beyond roughly 29 symbols in the task, the MLP performs substantially above chance, and approaches perfect generalization beyond 212 symbols.</div>\nGiven further assumptions on f (e.g. f is an MLP), it is likely possible to shrink this data diversity bound considerably. Regardless whether an MLP achieves these conditions exactly, we next show that with sufficient data diversity, an MLP equipped with an embedding matrix and trained through gradient descent does solve a relational task of the form posited in Theorem A.1, generalizing perfectly to unseen data.\n# A.4 SAME-DIFFERENT TASK\nWe now demonstrate empirically that a vanilla MLP trained with gradient descent will discover a solution that generalizes to unseen symbols. While the tasks we explore in Section 3 already indicate that MLPs solve relational tasks, to address any remaining doubt, we pursue an ostensibly impossible task as specified in Boix-Adsera et al. (2023). In particular, we examine the same-different task. As noted in Appendix A.1, the same-different task consists of two templates, z1 = \u03b1\u03b1 and z2 = \u03b1\u03b2, with labels f \u2217(\u03b1\u03b1) = 1 and f \u2217(\u03b1\u03b2) = 0. Following Boix-Adsera et al. (2023), we consider input strings x = x1x2 \u2208X 2 that are one-hot encoded before being passed through a randomly-initialized embedding matrix. As previously discussed, this embedding enables the model to circumvent the impossibility result (Theorem A.1) from Boix-Adsera et al. (2023). The remainder of our model is exactly the MLP described in Appendix C. We use an MLP with 4 hidden layers (in addition to an embedding layer) all of width 256 dimensions. The MLP is trained on batches of 128 examples sampled from a set of symbols with varying size |X|, with roughly even positive and negative examples. In each run, the MLPs are first trained for 10, 000 batches on half the symbols in X, then tested on the other half. All remaining hyperparameters are specified in Appendix C. We plot the performance of an MLP on this task in Figure 3. For this task, data diversity refers to the number of symbols in X. With higher data diversity, we see that the MLP improves progressively at generalizing on unseen symbols. Beyond about 212 symbols, the MLP generalizes near-perfectly, confirming that these models do, indeed, learn to reason relationally. This result is particularly interesting because it shows that, with sufficient data diversity, the MLP generalizes to completely novel symbols.\n# A.5 DISCUSSION\nOur results are consistent with Geiger et al. (2023), who also find empirically that MLPs (among other architectures) reason relationally and generalize robustly to unseen inputs. We complement their results by further evidencing the possible conditions where MLPs may continue to generalize successfully. Geiger et al. (2023) argue that neural networks require \u201cnon-featural input representations\" to generalize. A representation is featural if it encodes interpretable features of the task in axis-aligned dimensions. One-hot token encodings are featural, but randomized encodings are not.\nAs in Geiger et al. (2023), we show that featural representations like one-hot encodings remain usable provided they that pass through an embedding matrix, becoming non-featural and circumventing the impossibility result found by Boix-Adsera et al. (2023). In this way, with sufficient data diversity, an MLP still generalizes to unseen inputs, even if the inputs are unseen one-hot encodings. Despite our success above, many earlier studies document cases where common neural network architectures fail to reason relationally (Marcus et al., 1999; Kim et al., 2018; Lake and Baroni, 2018; Alhama and Zuidema, 2019). One important reason for the failure may be that the task inputs are very large and complex, as in visual reasoning (Kim et al., 2018; Serre, 2019). Proposition A.2 suggests that the data diversity required for successful generalization scales exponentially with the dimension of the inputs in the worst case. It is possible that given a sufficiently vast dataset, an MLP would perform well on visual reasoning tasks. Furthermore, having shown above that MLPs are decisively capable of relational reasoning (especially when presented with idealized stimulus embeddings, as in Section 3), their failure on complex tasks highlights a need to separate a model\u2019s ability to reason relationally from its ability to learn sufficiently rich feature representations. In realistic data-limited scenarios, perhaps an MLP paired with a more bespoke module for feature learning would reason quite successfully. We anticipate further work that more closely investigates whether these failures stem from data limitations, insufficient feature learning, or some other cause, thereby building a more complete and updated picture of relational reasoning in neural networks.\n# B EXPERIMENT: SIMPLE TASKS\nIn the main text, we showed that MLPs perform comparably with Transformers on ICL regression and classification, and better on relational tasks. In this separate set of experiments, we examine a setting in which MLPs are decisively superior. To do so, we depart from in-context tasks and consider simple (non-ICL) regression and classification.\nB.1 SIMPLE REGRESSION\n# B.1 SIMPLE REGRESSION\nFollowing the classic regression setup, the model receives as input a single point x \u2208Rn, and must output the corresponding y \u2208R which is related through y = x \u00b7 \u03b2. Note: this is not in-context regression, so the model receives only a single input x and the weights \u03b2 remain fixed throughout the duration of the task. For the Transformer, unless otherwise stated, each input coordinate is processed as a \u201ctoken\" with depth 1. Additional details are provided in Appendix C.11.\nResults. In Figure 4a, we plot the MSE of vanilla MLPs and Transformers as a function of compute on n = 64 dimensional regression. The gap between the two models is substantial. The Transformer seems to struggle especially for larger inputs. For smaller n, the compute gap shrinks between MLPs and Transformers (Figure 10). If your are stuck with large n, one potential strategy for improving the Transformer\u2019s performance is to manually chunk the inputs into larger tokens, reducing the total number of tokens. In the extreme case, we chunk the entire input into a single token (effectively transposing the input). As the token size increases, the Transformer\u2019s effiency smoothly improves until it reaches a level comparable to the MLP (Figure 4b). Indeed, in the extreme case of a single input token, the Transformer is almost identical to an MLP anyway.\n# B.2 SIMPLE CLASSIFICATION\nWe next consider a classic classification setup. The model receives a single point x \u2208Rn that was sampled from 1 of k different clusters. The model must output the correct label y of the corresponding cluster. This is not in-context classification, so the model receives only a single input x and the cluster/label mapping remains fixed throughout the duration of the task. Additional details are provided in Appendix C.12.\nResults. The same results continue to hold. As shown in Figures 4(c,d), for n = 64 dimensional classification, there is a wide compute gap between a vanilla MLP and a Transformer model, though the gap can be narrowed by manually chunking the inputs into larger tokens. Figure 10 gives performance for inputs of different dimensions, where smaller n narrow the gap between the two models.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2be0/2be09461-3a80-4bf1-bf4b-ddb64a9a3105.png\" style=\"width: 50%;\"></div>\nFigure 4: Simple regression and classification results. (a) MLPs attain substantially lower MSE at lower compute than Transformers. The red line corresponds to the minimum attainable MSE. (b) Transformers attain performance given larger token sizes. (c, d) Same as in (a, b), for classification, with k = 16 clusters. (all) We use n = 64 dimension inputs. Other parameterizations are explored in Appendix D. Shaded regions correspond to 95 percent confidence intervals estimated from 5 replications.\n# B.3 DISCUSSION\nEvidently simple tasks with long inputs work against the Transformer\u2019s attention mechanism. Shortening the context by reducing the task dimension, chunking inputs into larger tokens, or bypassing the attention mechanism altogether by stacking the input into a single token all improve the Transformer\u2019s efficiency. It is not immediately obvious why the Transformer performs so dramatically worse compared to the MLP for larger n, though it is well-known that Transformers can struggle with long inputs (Tay et al., 2020).\n# C MODEL AND TASK CONFIGURATIONS\nIn the following appendix, we provide all details on the specific model and task configurations used in this study, including architecture, hyperparameter settings, training methodology, and more.\n# C.1 CODE\nFor the most precise information on our setup, please refer to our GitHub code repository: https://github.com/wtong98/mlp-icl There, you will find all code used to reproduce the plots in this document, as well as any minor implementation details omitted from this appendix. If you notice an error, we welcome your pull requests!\n# For the most precise information on our setup, please refer to our GitHub code repository:\n# C.2 MLP\nThe MLP accepts inputs x \u2208Rn. If a task provides inputs of shape L \u00d7 D (length by token depth), the inputs are first flattened to size n = LD before being passed to the MLP. A model with \u2113hidden layers then proceeds as follows:\nFor all tasks, we use ReLU activation functions applied pointwise \u03d5(x) = max(x, 0). Widths of all hidden layers are fixed to the same value H. As with all models, all training examples are presented online with batch size 128. Training uses AdamW (Loshchilov and Hutter, 2017) with learning rate \u03b1 = 1 \u00d7 10\u22124 and weight decay \u03bb = 1 \u00d7 10\u22124. The hyperparameters used to train MLPs on each task are presented in Table 1.\n<div style=\"text-align: center;\">Table 1: MLP hyperparameters</div>\nTable 1: MLP hyperparameters\nTask\nDepth (\u2113)\nWidth (H)\nTrain iterations\nICL regression\n2 - 8\n128 - 2048\n\u22642, 048, 000\nICL classification\n2 - 8\n64 - 1024\n\u2264128, 000\nSimple regression\n1 - 4\n4 - 256\n\u226464, 000\nSimple classification\n1 - 4\n4 - 256\n\u226464, 000\nMatch-to-sample\n1 - 4\n4 - 256\n\u22648, 000\nSphere oddball\n1 - 4\n4 - 256\n\u22648, 000\nLine oddball\n1 - 4\n4 - 256\n\u22648, 000\n# C.3 MIXER\nThe MLP-Mixer accepts inputs X \u2208RL\u00d7D (length by token depth). If a task does not provid tokenized inputs, we assume D = 1 unless otherwise stated, and reshape accordingly. A model with \u2113hidden layers then proceeds as follows:\nThe MLP-Mixer accepts inputs X \u2208RL\u00d7D (length by token depth). If a task does not provide tokenized inputs, we assume D = 1 unless otherwise stated, and reshape accordingly. A model with \u2113hidden layers then proceeds as follows: h(X) = \u03d5(Z(b\u22ba  + XW ) + c)\nh\u2113(X) = \u03d5(Z\u2113(b\u22ba \u2113+ h\u2113\u22121(X)W \u2113) + c\u2113) f MIX(X) = W outh\u2113(X)(\u22121) + bout\nThe matrices W mix within token dimensions, and share a fixed hidden width H, so W i \u2208RH\u00d7H for 1 < i < \u2113. The matrices Z mix across spatial dimensions, and share a fixed channel width C, so Zi \u2208RC\u00d7C for 1 < i < \u2113. The bias vectors b and c are assumed to broadcast over unit dimensions as expected. The index \u22121 in h\u2113(X)(\u22121) refers to taking the last token in the layer, producing an output vector with length H. We again use point-wise ReLU activations \u03d5(X) = max(X, 0). Our Mixer is a simplified version of the original model proposed in Tolstikhin et al. (2021), and differs in a number of small ways:\n\u2022 We use only a single hidden layer per Mixer layer, rather than two. \u2022 We apply the point-wise activation after the final spatial mixing, and not between spati and token mixings.\n\u2022 We do not use layer norm or skip connections.\nUsing the full original model proved to be unnecessary in our setting, so we proceeded with this simpler version. As with all models, all training examples are presented online with batch size 128. Training uses AdamW with learning rate \u03b1 = 1 \u00d7 10\u22124 and weight decay \u03bb = 1 \u00d7 10\u22124. The hyperparameters used to train MLPs on each task are presented in Table 2.\n<div style=\"text-align: center;\">Table 2: Mixer hyperparameters</div>\nTable 2: Mixer hyperparameters\nTask\nDepth (\u2113)\nHidden width (H)\nChannel width (C)\nTrain iterations\nICL regression\n2 - 8\n32 - 512\n64\n\u2264500, 000\nICL classification\n2 - 8\n16 - 256\n64\n\u226424, 000\n# C.4 TRANSFORMER\nThe Transformer accepts inputs X \u2208RL\u00d7D (length by token depth). If a task does not provide tokenized inputs, we assume D = 1 unless otherwise stated, and reshape accordingly. A model with\nf TR(X) = W outh\u2113(X)(\u22121) + bout The attention matrices Ai are single-headed, and constructed as \ufffd \ufffd\n\ufffd \ufffd \ufffd\ufffd where \u201cmask\" corresponds to a causal attention mask, and \u03c3 refers to a softmax applied per query. As is now popular, we use GeLU activations applied pointwise for \u03d5. We fix the hidden dimension across all key, query, value, and weight matrices to be of width H. We use sinusoidal positional encodings for PE and layer normalization as indicated by LN. One exception is for ICL regression, which does not require positional encodings due to the input format (Appendix C.6), so they are omitted in this case. The bias vectors b and c are assumed to broadcast over unit dimensions as expected. The index \u22121 in h\u2113(X)(\u22121) refers to taking the last token in the layer, producing an output vector with length H. Our architecture is precisely the decoder-only Transformer architecture first described in Vaswani et al. (2017), with the exception that we do not use dropout. As with all models, all training examples are presented online with batch size 128. Training uses AdamW with learning rate \u03b1 = 1 \u00d7 10\u22124 and weight decay \u03bb = 1 \u00d7 10\u22124. The hyperparameters used to train MLPs on each task are presented in Table 3.\n<div style=\"text-align: center;\">Table 3: Transformer hyperparameters</div>\nTable 3: Transformer hyperparameters\nTask\nDepth (\u2113)\nWidth (H)\nTrain iterations\nICL regression\n2 - 8\n32 - 512\n\u2264600, 000\nICL classification\n2 - 8\n16 - 256\n\u226416, 000\nSimple regression\n1 - 4\n8 - 32\n\u2264256, 000\nSimple classification\n1 - 4\n8 - 32\n\u2264128, 000\nMatch-to-sample\n1 - 4\n8 - 32\n\u22648, 000\nSphere oddball\n1 - 4\n8 - 32\n\u22648, 000\nLine oddball\n1 - 4\n8 - 32\n\u22648, 000\n# C.5 RB MLP\nThe relationally-bottlenecked MLP is architecturally identically to the vanilla MLP described above in Appendix C.2, but with the crucial difference that the inputs are preprocessed to preserve only (dot-product) relations. The RB MLP accepts inputs X \u2208RL\u00d7D (length by token depth). The inputs are processed into a relation matrix R such that each entry is\n \u2212 \u00b7 \u2212 where xi \u2208RD refers to the ith row of X, and x = 1 L \ufffd i xi is the average across all xi. Relations vectors r are then generated by either selecting a specific column r = R(j) (as in the MTS task) or flattening the entire matrix of relations r = flat(R). The output of the RB MLP is then simply\nFor the \u201cdeep\" RB MLP used in the line oddball task, there is an additional set of two hidden layers between r and the readout weights W out, with width 256. All other training parameters are equivalent to the above models.\nWe prepare in-context regression in a setup that closely mimics Ravent\u00f3s et al. (2024), though without an autoregressive objective. The input consists of a sequence of values (x1, y1), (x2, y2), . . . , (xL, yL), where xi \u2208Rn and yi \u2208R. The xi, yi pairs are linearly related through a set of weights \u03b2 \u2208Rn such that yi = xi \u00b7 \u03b2 + \u03b5, where \u03b5 \u223cN(0, \u03c32) corresponds to noise. Finally, the input includes a query xq. The model output is a single scalar regressed against the corresponding yq. Inputs are sampled as x \u223cN(0, I) and weights are sampled as \u03b2 \u223cN (0, I/n). Before being presented to the model, all inputs are packed into an input matrix \u02dcX \u2208R(L+1)\u00d7(n+1) with the following structure (Zhang et al., 2023) \ufffd \ufffd\nThe model returns a scalar value estimate of yq, and is trained using the mean-squared-error. Note: this format does not require positional encodings. Following Zhang et al. (2023), we omit positional encodings for this task. As in Ravent\u00f3s et al. (2024), we fix a finite pool of weights before training \u03b21, \u03b22, . . . , \u03b2k, where \u03b2i \u223cN(0, I/n). For each training example, we sample a new \u03b2 by selecting uniformly at random one weight from the pool {\u03b2i}k i=1. We also consider the limit k \u2192\u221e, which corresponds to sampling \u03b2 \u223cN(0, I/n) afresh rather than drawing from a fixed pool. During testing, we probe the model\u2019s performance both on the training distribution where the weights are restricted to a finite pool \u03b2 \u223cU \ufffd {\u03b2i}k i=1 \ufffd and an unrestricted distribution where the weights are drawn freely \u03b2 \u223cN(0, I/n). Unless stated otherwise, all of our experiments use n = 8 dimensional regression with L = 8 points in the context, and noise level \u03c32 = 0.05. Bayes estimators. We compare our models to two different Bayes estimators that correspond to priors assuming finite or infinite k. For finite k where weights \u03b2 are sampled uniformly from a pool of k possibilities, the Bayes optimal estimator is given by the discrete minimum mean-squared error (dMMSE) estimator, based on the estimator formulated in Ravent\u00f3s et al. (2024)\nwhere the weights wi are given by\nnormalized such that \ufffd i wi = 1.\n\uf8f3 \uf8fe normalized such that \ufffd i wi = 1. In the case k \u2192\u221e, the Bayes optimal estimator is simply the familiar Ridge estimator with Baye optimal regularization \ufffd \ufffd\n \ufffd In the case k \u2192\u221e, the Bayes optimal estimator is simply the familiar Ridge estimator with Bay optimal regularization \ufffd \ufffd \n# C.7 ICL CLASSIFICATION\nWe prepare ICL classification in a setup that closely mimics Reddy (2024). We begin with a set of labels \u03b11, \u03b12, . . . \u03b1C \u2208Rn that correspond to class indices 1, 2, . . . C. Labels are sampled as \u03b1 \u223cN(0, I/n). The model ultimately predicts the class index, but the real-valued labels provide content of the correct dimension to fill an input without arbitrary padding (described further below). Points are sampled from a Gaussian mixture model Mk consisting of k components, where k \u2265C (we allow multiple clusters to have the same class label). Each component is associated with a center\nWe prepare ICL classification in a setup that",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The emergence of in-context learning (ICL) as a significant capability of neural networks, particularly Transformers, has led to extensive research into its mechanisms and applications. However, there is a lack of exploration into the potential of simpler architectures like multi-layer perceptrons (MLPs) in this domain. This benchmark addresses this gap by investigating the in-context learning abilities of MLPs and their performance relative to Transformers.",
            "purpose of benchmark": "The benchmark is designed to evaluate the in-context learning capabilities of MLPs on regression and classification tasks, comparing their performance against Transformers to highlight the potential of MLPs in this area."
        },
        "problem": {
            "definition": "The benchmark specifically targets the ability of models to learn and perform tasks based solely on provided context exemplars without weight updates, focusing on in-context regression and classification tasks.",
            "key obstacle": "Existing benchmarks predominantly focus on Transformer models, often overlooking the capabilities of MLPs. This benchmark aims to overcome the bias towards attention-based architectures and demonstrate that MLPs can effectively engage in in-context learning."
        },
        "idea": {
            "intuition": "The inspiration for this benchmark stems from the observation that MLPs, despite their simplicity, may possess the capability to learn in-context, a trait traditionally attributed to more complex architectures like Transformers.",
            "opinion": "The authors argue that understanding the in-context learning abilities of MLPs could reshape the perception of their effectiveness in machine learning tasks, promoting further exploration of simpler architectures.",
            "innovation": "This benchmark introduces a novel comparison of MLPs and Transformers on controlled ICL tasks, revealing that MLPs can match or exceed Transformer performance in specific scenarios, which challenges the prevailing notion of their limitations.",
            "benchmark abbreviation": "MLPS-ICL"
        },
        "dataset": {
            "source": "The dataset for the benchmark consists of synthetic data generated specifically for the in-context learning tasks, allowing for precise control over the characteristics of the tasks being evaluated.",
            "desc": "The dataset includes controlled regression and classification tasks designed to test the in-context learning capabilities of models, with a focus on relational reasoning.",
            "content": "The dataset contains pairs of input-output examples for regression tasks and context exemplars with associated labels for classification tasks, structured to facilitate evaluation of in-context learning.",
            "size": "1,000,000",
            "domain": "Relational Reasoning",
            "task format": "In-Context Learning"
        },
        "metrics": {
            "metric name": "Mean Squared Error (MSE), Cross Entropy Loss",
            "aspect": "The metrics measure the accuracy of the models' predictions in relation to the expected outputs.",
            "principle": "The choice of metrics is based on their established relevance in evaluating regression and classification tasks, ensuring that the performance can be quantitatively assessed.",
            "procedure": "Model performance is evaluated by comparing the predicted outputs against the true outputs using the specified metrics, with results averaged over multiple trials to ensure reliability."
        },
        "experiments": {
            "model": "The benchmark tests multiple architectures, including vanilla MLPs, MLP-Mixer models, and Transformers, to assess their performance on the ICL tasks.",
            "procedure": "Models are trained and tested on synthetic datasets designed for in-context learning, with controlled variations in task parameters to evaluate performance under different conditions.",
            "result": "Results indicate that MLPs can achieve competitive performance compared to Transformers, particularly in controlled settings, challenging the assumption that MLPs are inherently less capable.",
            "variability": "Variability in results is accounted for by conducting multiple trials and using different subsets of the dataset to ensure that findings are robust and not due to chance."
        },
        "conclusion": "The experiments demonstrate that MLPs can effectively learn in-context and perform competitively with Transformers on regression and classification tasks, suggesting a need for further research into their potential applications in machine learning.",
        "discussion": {
            "advantage": "The benchmark highlights the strengths of MLPs in learning from context, showing that they can perform well on tasks traditionally dominated by Transformers, thus expanding the perceived capabilities of simpler architectures.",
            "limitation": "The benchmark is limited to controlled, synthetic tasks and does not account for the complexities present in real-world applications, which may affect the generalizability of the findings.",
            "future work": "Future research should explore the performance of MLPs on more complex, real-world tasks and investigate the conditions under which they succeed or fail to learn in-context."
        },
        "other info": {
            "funding": "This work was supported by various grants including NSF Award DMS-2134157 and NSF CAREER Award IIS-2239780.",
            "acknowledgments": "The authors thank their colleagues for helpful discussions and feedback on the manuscript."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The emergence of in-context learning (ICL) as a significant capability of neural networks, particularly Transformers, has led to extensive research into its mechanisms and applications."
        },
        {
            "section number": "1.2",
            "key information": "However, there is a lack of exploration into the potential of simpler architectures like multi-layer perceptrons (MLPs) in this domain."
        },
        {
            "section number": "1.3",
            "key information": "The benchmark is designed to evaluate the in-context learning capabilities of MLPs on regression and classification tasks, comparing their performance against Transformers."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark specifically targets the ability of models to learn and perform tasks based solely on provided context exemplars without weight updates."
        },
        {
            "section number": "3.1",
            "key information": "Results indicate that MLPs can achieve competitive performance compared to Transformers, particularly in controlled settings."
        },
        {
            "section number": "4.1",
            "key information": "The authors argue that understanding the in-context learning abilities of MLPs could reshape the perception of their effectiveness in machine learning tasks."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark is limited to controlled, synthetic tasks and does not account for the complexities present in real-world applications."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrate that MLPs can effectively learn in-context and perform competitively with Transformers on regression and classification tasks."
        }
    ],
    "similarity_score": 0.7020957599577018,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/MLPs Learn In-Context on Regression and Classification Tasks.json"
}