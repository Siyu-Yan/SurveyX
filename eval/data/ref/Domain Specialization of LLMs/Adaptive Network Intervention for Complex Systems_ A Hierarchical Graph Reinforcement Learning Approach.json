{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.23396",
    "title": "Adaptive Network Intervention for Complex Systems: A Hierarchical Graph Reinforcement Learning Approach",
    "abstract": "Effective governance and steering of behavior in complex multi-agent systems (MAS) are essential for managing system-wide outcomes, particularly in environments where interactions are structured by dynamic networks. In many applications, the goal is to promote pro-social behavior among agents, where network structure plays a pivotal role in shaping these interactions. This paper introduces a Hierarchical Graph Reinforcement Learning (HGRL) framework that governs such systems through targeted interventions in the network structure. Operating within the constraints of limited managerial authority, the HGRL framework demonstrates superior performance across a range of environmental conditions, outperforming established baseline methods. Our findings highlight the critical influence of agentto-agent learning (social learning) on system behavior: under low social learning, the HGRL manager preserves cooperation, forming robust core-periphery networks dominated by cooperators. In contrast, high social learning accelerates defection, leading to sparser, chain-like networks. Additionally, the study underscores the importance of the system manager\u2019s authority level in preventing system-wide failures, such as agent rebellion or collapse, positioning HGRL as a powerful tool for dynamic network-based governance.",
    "bib_name": "chen2024adaptivenetworkinterventioncomplex",
    "md_text": "# Adaptive Network Intervention for Complex Systems: A Hierarchical Graph Reinforcement Learning Approach\nQiliang Chen\nMAGICS lab\nCollege of Engineering\nNortheastern University\nBoston, MA 02115, USA\nchen.qil@northeastern.edu\nBabak Heydari*\nMAGICS lab\nCollege of Engineering and Network Science Institute\nNortheastern University\nBoston, MA 02115, USA\nb.heydari@northeastern.edu\nQiliang Chen MAGICS lab College of Engineering Northeastern University Boston, MA 02115, USA chen.qil@northeastern.edu Babak Heydari* MAGICS lab College of Engineering and Network Science Institute Northeastern University Boston, MA 02115, USA b.heydari@northeastern.edu\n# Abstract\nEffective governance and steering of behavior in complex multi-agent systems (MAS) are essential for managing system-wide outcomes, particularly in environments where interactions are structured by dynamic networks. In many applications, the goal is to promote pro-social behavior among agents, where network structure plays a pivotal role in shaping these interactions. This paper introduces a Hierarchical Graph Reinforcement Learning (HGRL) framework that governs such systems through targeted interventions in the network structure. Operating within the constraints of limited managerial authority, the HGRL framework demonstrates superior performance across a range of environmental conditions, outperforming established baseline methods. Our findings highlight the critical influence of agentto-agent learning (social learning) on system behavior: under low social learning, the HGRL manager preserves cooperation, forming robust core-periphery networks dominated by cooperators. In contrast, high social learning accelerates defection, leading to sparser, chain-like networks. Additionally, the study underscores the importance of the system manager\u2019s authority level in preventing system-wide failures, such as agent rebellion or collapse, positioning HGRL as a powerful tool for dynamic network-based governance.\narXiv:2410.23396v\nKeywords: Network intervention, Multi-agent system, Graph neural networks, Deep reinforcement learning, Hierarchical structure\n# 1 Introduction\nGoverning complex multi-agent systems (MAS) has garnered increasing attention, particularly when network structures are incorporated, as they make these systems more realistic and relevant across various domains, such as socio-technical systems [1, 2, 3, 4], supply chain networks [5, 6, 7], communication systems [8, 9, 10] and others [11, 12, 13]. Network structures not only dictate agent interactions\u2014who engage with whom and how information is exchanged\u2014but also play a pivotal role in shaping system-wide performance and driving the emergence of complex collective behaviors like coordination, cooperation, fairness, and trust [14, 15, 16, 17]. These insights from network science open the door to a novel form of governance, where dynamically adjusting interaction patterns within the system can steer it toward desired outcomes. This approach complements more conventional\nethods of governance, such as classical engineering control or agent-level incentive designs from conomics and social sciences, offering a powerful new tool for managing complex systems.\nmethods of governance, such as classical engineering control or agent-level incentive designs from economics and social sciences, offering a powerful new tool for managing complex systems. Despite the potential of network-based governance, several significant challenges complicate the effective management of multi-agent systems through network interventions. These challenges include (1) the complex and evolving dynamics of agent interactions, (2) the limited authority of the system manager, and (3) the vastness of the state and action spaces involved in decision-making. The first challenge stems from the inherent complexity and evolving dynamics of these systems. Multi-agent systems typically involve interactions among individuals operating across varying levels of cooperation, coordination, and competition [18, 19, 20]. Each agent, acting autonomously, aims to optimize its own objectives within the environment, but these individual goals often conflict with the system\u2019s collective welfare, leading to well-known social dilemmas [21, 22]. The dynamic nature of these systems further complicates matters, as network topologies are not static but evolve through both agent interactions and external interventions[23]. Agents not only interact with their peers but also learn and adapt through social learning processes, observing and imitating the behaviors of others, particularly when those behaviors result in higher utilities [24]. As a result, these systems exhibit non-stationary dynamics, constantly changing and requiring the system manager to adopt highly flexible and adaptive governance strategies. The second challenge arises from the limited authority of the system manager. In most real-world applications, a system manager\u2019s ability to intervene is restricted by resource limitations and the level of compliance from the agents within the system, whether they are human or AI agents [25, 26, 27]. Excessive alterations to the network structure can incur prohibitively high resource costs, making it unfeasible for the system manager to continually change the network [28, 29]. Moreover, agents\u2014particularly human ones\u2014may resist directives that they perceive as overly controlling or misaligned with their own interests. As a result, the system manager must strike a careful balance between improving system performance and managing resources efficiently, all while operating under these constraints. Developing strategies that align with the limited authority of the manager becomes essential to navigating these challenges and effectively governing the system [30, 31]. The third challenge involves the vastness of both the state and action spaces in network interventions. As the number of agents N in a network grows, the complexity of the network topology scales exponentially, with the number of potential configurations represented as O(2N(N\u22121)/2). This makes it increasingly difficult for the system manager to monitor the system and make informed decisions about which links to add or remove [14]. Additionally, the action space\u2014the number of possible interventions available to the system manager\u2014grows similarly in complexity, expressed as O(N(N \u22121)/2). As a result, the system manager faces the daunting task of distilling critical information from an overwhelming number of possibilities, all within a limited number of training rounds. The combination of vast state and action spaces, along with the variability in agent features, creates a significant obstacle in learning efficient policies for network intervention. To tackle these multifaceted challenges, we introduce a novel Hierarchical Graph Reinforcement Learning (HGRL) framework, which combines Graph Neural Networks (GNN), Reinforcement Learning (RL), and hierarchical structures. The HGRL framework allows the system manager to efficiently learn network intervention policies, even in the face of complex, evolving multi-agent systems with constrained authority. By utilizing a hierarchical approach, HGRL significantly reduces the complexity of both the state and action spaces, leading to more effective and scalable governance. In the simulated environment we developed, the system manager adjusts the network structure using information from the entire system, while agents update their strategies through social learning, based\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/deab/deaba6fe-ffbc-4de2-9702-0eaa4a7d876d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Iterate for T time steps</div>\nFigure 1: The diagram of the general process in the environment. We have 3 entities in the whole process: HGRL framework (details are shown in Figure 2), autonomous agents and network topology. In each round of the game, agents with different types interact with their first-order neighbors in the networks; the HGRL framework will observe the information from agents and network topology to make decision of network intervention by adding or deleting links in the network; agents will then imitate others with higher utilities under different imitation probabilities. This process will be iterated for several time steps and we can generate analysis of performance and network evolution from different perspectives.\non their observations and interactions with other agents. This iterative process allows the system to evolve over time, with the system manager aiming to optimize social welfare. Figure 1 provides a visual representation of this process. Our evaluation shows that the HGRL framework consistently outperforms baseline models across a variety of conditions. Analysis of system behavior reveals that, with low levels of social learning, the HGRL manager preserves cooperation by forming a strong core-periphery network, where cooperators occupy the core. However, in high social-learning scenarios, defection spreads rapidly, resulting in a sparse, chain-like network structure. Our findings also highlight the importance of managerial authority in preventing system-wide failures, such as agent rebellion or collapse. The paper is organized as follows: Section 2 reviews relevant literature, Section 3 provides a comprehensive introduction to the HGRL framework and our training methodology, Section 4 details the environment design and discusses both the performance results and the learned behaviors of the HGRL system manager, and Section 5 concludes the paper.\n# 2 Background\nThis section reviews literature related to our paper. We begin by providing background information on network interventions in multi-agent systems, which is the central topic of this paper. Subsequently, we discuss papers that integrate Graph Neural Networks with Deep Reinforcement Learning, a methodology our approach also employs by combining these two methods.\nComprehensive research in Network Science strongly supports the idea that the performance and behaviors of systems with network structures can be effectively managed through network intervention. Besides, numerous studies build on this premise, developing general models and methodologies applicable to various contexts. Authors in [32] integrate graph and control theory to address challenges in structural controllability and optimal control of large-scale networks. Besides, Authors in [33] develop a series of algorithms to identify key nodes in networks. Intervention in these nodes can significantly influence the overall system. Apart from cooperative scenarios, authors in [34] introduce a mathematical framework using signed graphs and dynamical system theories in multi-agent systems with \u2019coopetition networks\u2019, leveraging distributed interaction laws and neural network-based estimators for disturbance adaptation. Additionally, several research efforts aim to solve network intervention tasks in specific areas, further expanding the scope and application of these methodologies. In the public health area, authors in [35] propose a new framework for network interventions in public administration, drawing on Valente\u2019s 2012 topology and public health concepts, to enhance behavioral change and improve outcomes at social, organizational, and community levels. In the socio-technical systems, the study in [36] explores how networks and network formation influence the implementation of a self-management support intervention in a community setting. In the infrastructure systems, the authors in [37] suggest the system manager build a supply chain with the presence of hub firms and redundancy and undertake a multi-sourcing strategy or intermediation between hub firms to increase system resilience. Most existing studies in this field concentrate on specific domains, employing extensive domain knowledge to devise strategies for network intervention in multi-agent systems. However, the transferability of these strategies across various fields is limited. Our work is pioneering in addressing these common challenges we proposed in the context of network intervention problems in multi-agent systems with an innovative HGRL framework, designed to be versatile and applicable across a variety of application areas.\n# 2.2 Frameworks with combination of GNN and DRL\nThe evolution of Reinforcement Learning (RL) has emerged as a universal solution for decisionmaking across various domains, yielding remarkable results in fields such as robotics [38, 39, 40], video gaming [41] and design and governance of engineering systems [42, 30]. Besides, Graph Neural Networks (GNNs) have demonstrated significant advancements in the perception of graph information in different fields like bio-medicine [43], computer vision [44], and social science [45, 46, 47]. Consequently, when addressing decision-making in graph-based environments, the integration of GNNs with RL becomes a popular approach. Authors in [48] proposed a GNN-based DRL agent to solve routing optimization problems and the results show that the DRL+GNN agent can outperform state-of-the-art solutions in the testing phase. To improve the network embedding task, authors in [49] combined deep reinforcement learning with a novel neural network structure based on graph convolutional networks and proposed a new algorithm for automatic virtual network embedding. In chemical process design, authors in [50] modeled chemical processes as graphs and used GNN and DRL to learn from process graphs, which is capable of designing viable flowsheets economically in an illustrative case study. In smart manufacturing, authors in [51] combined GNN and DRL to solve dynamic job shop scheduling problems, with\nhigher effectiveness and feasibility than regular DRL. Authors in [52] tried to tackle the problem of promoting pro-social behaviors through network intervention with GNN plus DRL, and the results on human groups show the superiority of the framework. While the integration of Graph Neural Networks (GNNs) with Deep Reinforcement Learning (DRL) has been applied across various domains, its implementation in addressing network intervention issues remains underexplored. Our research marks a significant advancement in crafting effective network intervention strategies within multi-agent systems. Additionally, our proposed Hierarchical Graph Reinforcement Learning (HGRL) framework considerably simplifies the complexity of the state and action spaces, enhancing the efficiency of the learning process.\nhigher effectiveness and feasibility than regular DRL. Authors in [52] tried to tackle the problem of promoting pro-social behaviors through network intervention with GNN plus DRL, and the results on human groups show the superiority of the framework.\nWhile the integration of Graph Neural Networks (GNNs) with Deep Reinforcement Learning (DRL) has been applied across various domains, its implementation in addressing network intervention issues remains underexplored. Our research marks a significant advancement in crafting effective network intervention strategies within multi-agent systems. Additionally, our proposed Hierarchical Graph Reinforcement Learning (HGRL) framework considerably simplifies the complexity of the state and action spaces, enhancing the efficiency of the learning process.\n# 3 Method\n# 3.1 Partial Observable Markov Decision Process and Deep Q-Learning\nIn our framework, we utilize a reinforcement learning approach based on the Partially Observable Markov Decision Process (POMDP) model, which is particularly well-suited for environments where the state space is not fully visible. A Partial Observable Markov Decision Process(POMDP) can be described using a tuple < S, A, O, T, R >. S is a set of possible states. A is a set of available actions. O is observation perceived from the state of the system which may not fully cover all information of the state. T is transition function, where T: O \u00d7 A \u2192O. R is reward function, where O \u00d7 A \u2192R. The objective in POMDP is to learn the optimal policy P\u03b8: O \u00d7 A \u2192[0, 1], which can maximize the expected return: \ufffdT t=0 \u03b3trt where \u03b3 is a discount factor, T is the time horizon and r is the reward at each time step. Deep Q-learning, as outlined in [41], is a widely utilized algorithm within the field of reinforcement learning (RL). In this approach, the agent first perceives an observation o, which is a representation of the environmental state s. Based on this observation, the agent utilizes the action-value function Q\u03c0(o, a) = E[R|o = ot, a = at] under a policy \u03c0, and selects actions in a greedy manner to maximize the action value. The core objective of Q-learning is to refine the action value function towards the optimal policy. This is achieved by minimizing the loss function L\u03b8 = Eo,a,r,o\u2032[(Q(o, a|\u03b8)\u2212y)2], where y is defined as r + \u03b3 maxa\u2032 Q\u2032(o\u2032, a\u2032). Here, Q function is parameterized by \u03b8 using a Deep Neural Network. The observation at the next time step is denoted as o\u2032, and a\u2032 represents the action taken at that time. The target Q function, Q\u2032, which aids in stabilizing the learning dynamics, is periodically updated and is a clone of the operational Q function.\n# 3.2 Graph neural network\nGraph Neural Network (GNN) [53] is a class of deep learning models that operate on graphs and have shown promising results in various tasks such as node classification, link prediction, and graph classification. Besides, Graph Neural Networks (GNNs) have demonstrated significant potential for efficient learning on large graphs [54]. Similar to Convolutional Neural Networks (CNNs), which utilize a filter that slides across an image, GNNs employ a neighborhood aggregation function. This function slides over the graph, thereby conserving computational resources when dealing with large graphs. GNNs also adopt a pooling strategy akin to CNNs, aggregating information for each node. Once the node embeddings are obtained, they can be utilized for various downstream tasks as required. The following paragraphs provide some details about GNNs. A graph can be represented as G = (V, E), where V is a set of nodes and E is a set of edges connecting these nodes. The goal of GNN is to learn a function f(G, X), where X is a matrix of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c927/c9274a37-b682-4af8-a11d-4876327f6153.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: General framework of HGRL. HGRL has two agents: node agent and link agent. The node agent will collect graph information using GNNs and select one node to intervene; the link agent will rely on the information related to the selected node to decide to add or delete links to this node. The action spaces for node agent and link agent are O(N), where N is the number of nodes in the network.</div>\nnode features, that maps the graph and its node features to a target variable. There are three main operations in GNN, which are message-passing, message aggregation, hidden representation update, and a readout operation, which converts all node embedding to satisfy specific requirements of tasks. The message-passing function can be represented as: m(l) v\u2192u = M (l) \ufffd h(l\u22121) u , h(l\u22121) v , eu,v \ufffd where h(l\u22121) u and h(l\u22121) v are the hidden representations of nodes u and v at layer l \u22121, eu,v is the edge feature between nodes u and v, M (l) is a message function that generates a message from node v to node u at layer l, and m(l) v\u2192u is the message passed from node v to node u at layer l. The message-passing function can be modeled using deep neural networks. The Aggregation function can be written as: a(l) u = AGG(l) \ufffd {m(l) v\u2192u|v \u2208N(u)} \ufffd where N(u) is the set of neighboring nodes of node u, AGG(l) is an aggregation function that aggregates the messages received by node u at layer l. The update function can be represented as: h(l) u = U (l) \ufffd h(l\u22121) u , a(l) u \ufffd where U (l) is an update function that updates the hidden representation of node u at layer l based on the hidden representation h(l\u22121) u from the last layer and the aggregated message a(l) u . The above equations are applied iteratively for multiple layers until the final representation of each node is obtained. The final node representations can then be used for various downstream tasks. In our case, we need to represent the whole graph, which is similar to the operation in GNN-based graph classification, so the readout function can be written as zG = READOUT(h(l) v |v \u2208G) where zG is the graph-level representation obtained by aggregating the hidden representations of all nodes in the graph G at the final layer K, and READOUT is an aggregation function that combines the hidden representations of all nodes. There are various options for the READOUT function, such as concatenation, max pooling, mean pooling, sum pooling and some LSTM-based pooling methods[53]. We used the concatenation for the READOUT function to aggregate the information of all nodes.\nTheoretically, to learn a policy of network intervention aimed at optimizing predefined goals, the standard Deep Q-Learning method (Flat-RL) can be employed, which utilizes system information to determine which links to add or remove in various situations. However, Flat-RL encounters significant challenges when applied to network structure interventions, primarily due to the rapidly increasing complexity of both the state and action spaces as the number of nodes increases. To illustrate, consider a network with N nodes, the potential structures for undirected networks scale as O(2N(N\u22121)/2), which become intractable easily. Besides, the variability of potential node features will make it even more complex. The action space, involving the addition or deletion of links, has a complexity of O(N(N \u22121)). Consequently, both the state and action spaces in Flat-RL become exceedingly complex and potentially unmanageable as the network size increases. To overcome this challenge, we propose the Hierarchical Graph Reinforcement Learning (HGRL) Framework. The overall process and structure of this framework are visualized in Figure 2. This framework leverages GNN for network representation, utilizes a hierarchical structure [55] and takes advantage of the separable cost idea in networks [56] to mitigate the issue of exploding state space and action space encountered in Flat-RL. Graph Neural Networks (GNN) efficiently embed network information, which reduces the state space complexity with less computational and memory demand. To tackle the action space reduction in link intervention, HGRL draws on the concept of separable costs in networks. Previous work used this idea by decomposing the link costs to the sum of costs from two separate end nodes, to find solutions for efficient network structures [56]. Inspired by this idea, links can be decomposed into pairs of end nodes. Instead of direct link selection, HGRL proposes selecting each end node independently. In undirected networks, the sequence of choosing these nodes is irrelevant. Thus, HGRL\u2019s approach involves a two-step hierarchical link intervention process. The initial step involves selecting one end node based on overall network information. The subsequent step depends on the first step, where the second end node is chosen based on the initially selected node and its associated information. This step effectively determines which links to add or remove from the chosen node. Essentially, HGRL simplifies the joint link selection process into two hierarchical, interdependent steps, significantly reducing the compounded action space from O(N 2) to O(N), while N is the number of nodes in the network. In the execution phase of the HGRL framework, the initial stage involves the node agent selecting the first end node based on graph information. This is achieved using a Graph Neural Network (GNN) to extract network data. Specifically, the node agent employs the message-passing process to embed each node as we introduced in section 3.2. This embedding considers both the hidden information of the node and its neighbors. The initial hidden features for layer 0 are the original node features. The embedded hidden information is then aggregated using mean pooling to obtain the first layer embedding. This embedding process can be repeated up to a total of l layers, where l is the number of GNN layers. The number of layers is critical as it defines the extent of the neighborhood perceived around each node; essentially, l layers capture the information of the l-th order neighbors for each node. However, it is crucial to limit the number of layers because using too many layers can lead to an over-smoothing problem, where node representations become indistinguishable as the number of layers increases, as highlighted in the study by [57]. For this application, a two-layer GNN is employed. After processing through these steps, the graph\u2019s embedding information, denoted as h(l) ui = GNN(u(0) i , E), is obtained. This information serves as the observation for the Q-function in the reinforcement learning algorithm. A fully connected neural network represents the Q-function, calculating the Q values for selecting each node: Qnode((V, E), aui) = Fullyconnect(h(l) ui ), where GNN means the combination of the message passing, message aggregation, and hidden representation\nupdate we mentioned above. u(0) i is the original node feature for node i, aui is selecting node i to intervene and h(l) ui is the hidden information of node i at l-layer GNNs. Then node agent will choose the node with the highest Q value.\nIn the second stage of the HGRL framework, the link agent is tasked with selecting the second end node to either form a new link or delete an existing link with the node chosen in the first stage. The link agent\u2019s observation includes both the aggregated information of the neighboring nodes of the initially selected node and the existing connection status of that node selected from the first stage. The Q-function in this stage can be represented either using a Graph Neural Network (GNN) or a fully connected neural network. This flexibility is due to the reduced requirement for embedding the entire graph; instead, the focus is on calculating the Q values for all possible actions (adding or deleting links) from the node chosen in the first stage. Upon processing these calculations, the link agent will select the top K actions that have the highest Q values (K = 1 in our experiment). This selection is based on the intervention capability of the agent, as defined by the manager\u2019s authority. The process of conducting the second stage is similar to the first stage with different state space and action space accordingly. Importantly, the action spaces for both the node agent in the first stage and the link agent in the second stage are O(N), where N is the number of nodes in the network. This approach effectively mitigates the complexity of the action space, which in Flat Reinforcement Learning (Flat-RL) is O(N 2). The training process consists of two distinct phases. In the first phase, we train the link agent while employing a random strategy for the policy of the node agent. Initially, the node agent randomly selects one node. Subsequently, the link agent learns the policy to select a second node in order to determine which link to add or delete, based on the first selected node. Exploration is one of the important topics in RL; we use the \u03f5-greedy strategy to balance the exploration and exploitation in the training phase. The core idea is that when making decisions at each time step, the RL agent will have epsilon probability to select actions randomly(exploration) instead of choosing the action with the highest Q values(exploitation), where epsion will decrease gradually as the training proceed to make the policy converge. After making decisions at each time step, the trajectory of observation, actions, rewards, and next observations will be stored in a replay buffer. When updating the policy, a mini-batch of trajectory will be sampled from the replay buffer, and the Q-function will be updated to minimize the loss function, as we introduced in section 3.1. Once the link agent\u2019s training is complete, its policy will be fixed, and we will proceed to train the node agent. The general procedure is similar, but only the definitions of observation, action and reward for node agent are changed accordingly. The observation of the node agent will become all nodes\u2019 features and the connection status of the whole network. The action space of the node agent is selecting the first node strategically. The objectives for both the node and link agents are to improve social welfare in the system.\n# 4 Experiment and results\nIn this section, we first introduce the environment we have designed in this work, which effectively captures the challenges of network intervention in the multi-agent systems we mentioned earlier. We then present and discuss the results of our proposed methods, examining both performance and behavioral aspects.\nTo assess the effectiveness of the proposed HGRL framework, it\u2019s necessary to create an environment that encapsulates the key aspects of challenges as previously discussed. This environment should accurately represent the intricate interactions among agents within a multi-agent system structured as a network. Additionally, it must reflect the dynamic and complex nature of agent behavior and account for the constrained authority held by the system manager. First, we chose the Prisoner\u2019s Dilemma (PD) as the central mechanism in our network model, which is grounded in its proven effectiveness in representing the complexities of cooperative and competitive interactions [58]. This fundamental game in Game theory exemplifies the conflict between individual rationality and collective benefit, making it an ideal framework for studying decision-making in socio-technical systems. This game has been widely studied in various disciplines, including economics [59], sociology [60], and evolutionary biology [61], to understand cooperative behavior among rational individuals. The nodes in the network are players, they will play PD with their neighbors using their own policies and receive rewards based on the utility matrix. Besides, agents can only observe their direct neighbors and imitate other\u2019s policies if the other\u2019s total utility is higher than theirs. The limited range of observation represents the partial observable challenges, and the process of imitation mirrors the social learning process, a fundamental aspect of human society, which is extensively discussed in Bandura\u2019s Social Learning Theory [24]. This imitation mechanism not only reflects the adaptability and evolution of strategies within a network but also underscores the importance of social influence and learning in shaping individual and collective behavior. In addition, the environment also takes into account the limited authority of the system manager. This is achieved by constraining the system manager\u2019s ability to intervene in the network structure, allowing modifications to only a limited number of links at each time step, which, in our specific scenario, is restricted to one link. This aspect is crucial for ensuring the realism and applicability of our study to real-world socio-technical systems. In practical scenarios, system managers often do not possess infinite power to enforce rules or dictate behaviors within a network. Their influence is invariably circumscribed by legal, ethical, and practical constraints[62]. By considering the constraints on the system manager\u2019s authority, our model accurately represents the complexities and limitations that are inherent in the management and regulation of socio-technical systems. We provide some details of the environmental properties as follows. The agents will be randomly initialized with one of two policy types: \"cooperator\" or \"defector\". A cooperator consistently opts to cooperate, while a defector consistently chooses to defect during the game. In our environment, the utility matrix for the Prisoner\u2019s Dilemma (PD) is defined as follows: for mutual cooperation (CC), both players receive a utility of (-0.5,-0.5); if one cooperates and the other defects (CD), the cooperator receives -4 while the defector gets 0; in the reverse scenario (DC), the cooperator receives -4 and the defector 0; and for mutual defection (DD), both players receive a utility of (-3,-3). It\u2019s important to highlight that the HGRL model is adaptable to various utility matrices and capable of learning different policies accordingly. The utility matrix used here serves merely as an illustrative example, without any specific intent or preference for this particular set of utilities. The initial network structures are randomly established using the Erd\u02ddos\u2013R\u00e9nyi model of network formation [63] with 0.5 probability of building links. The dynamics of agent interaction further involve a probability factor p. After interacting with their neighbors, each agent has a p chance of evaluating one of their neighbors. This evaluation is based on comparing the total utilities gained in the last time step. If the neighbor\u2019s total utilities are higher, the agent will then imitate the type of that selected neighbor. The\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/47cf/47cf6cca-c8bd-4fc4-adbf-a74eb08ed29e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The performance comparison of HGRL, Flat-RL and random strategy in the system with 10 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.</div>\nsystem manager can only add or delete one link at each time step because of limited authority, and its objective is to maximize the social welfare of the system. We tested our framework on 10-node networks and 20-node networks. After network intervention, the disconnected networks were not allowed in our experiments.\n# 4.2 Experiment results \u2013 performance\nIn our study, we compared the performance of the HGRL model with that of Flat-RL and random strategy within the designed environment in both 10-node networks and 20-node networks. The HGRL model and Flat-RL model use similar sizes of parameters in neural networks. We trained the system manager for 10,000 rounds, with each round including 50 time steps for the 10-node networks and 100 time steps for the 20-node networks. The results presented are averaged from 1,000 test rounds. Our experiment was conducted under three different scenarios, each characterized by varying imitation probabilities p of 0, 0.5, and 1. These probabilities represent different levels of imitation tendency among the agents. To assess the effectiveness of the models, we employed two metrics: average social welfare of the game and average final social welfare at the end of the game. The performance is also averaged by the number of agents in the system. Average social welfare offers a comprehensive evaluation of the models\u2019 effectiveness, encompassing not only the general performance but also the pace of improvement in social welfare, as a swifter enhancement results in a higher average social welfare. Meanwhile, the social welfare at the end of the game evaluates the models\u2019 ability to guide the system to an optimal or desirable state by the end of the game. This dual-metric approach allows for a comprehensive analysis of HGRL performance in managing the system with network structure. The results of the 10-node networks are in Figure 3, and the results of 20-node networks are in Figure 4. From the results, it\u2019s evident that under all scenarios, the HGRL method consistently outperforms both the Flat-RL and the random strategy in terms of both metrics. This superior performance of HGRL highlights its ability to learn more effective and adaptable policies for network structure intervention across various environmental settings. In contrast, Flat-RL can only surpass the random strategy with little improvement in small networks with 10 nodes, but in general, its performance is comparable to the random strategy, especially in large networks with 20 nodes. This suggests that with similar size of model parameters and training duration, Flat-RL struggles to develop a robust policy even in relatively small networks with just 10 nodes, which underscores the critical need and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d5a2/d5a2411b-3126-4209-b382-3686010c3ef3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The performance comparison of HGRL, Flat-RL and random strategy in the system with 20 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.</div>\npromising potential for HGRL in real-world applications. Given that many real-world multi-agent systems with large networks, Flat-RL\u2019s utility diminishes, whereas HGRL retains its capacity to learn effective policies by reducing the complexity of the state and action spaces. Besides, an insightful observation is that only the HGRL method demonstrates an ability to achieve higher social welfare at the end of games compared to the average social welfare. This observation reveals that HGRL uniquely facilitates a progressive enhancement in the system\u2019s social welfare throughout the game. In contrast, neither Flat-RL nor the random strategy exhibits this capability for gradual improvement. Various studies, such as those by [64, 18, 65, 66], have shown that defection can be \u2019contagious\u2019 in multi-agent systems due to the selfish behavior of each individual and also the modularity and cliques in network structures. Another key factor driving this trend is the bounded rationality of agents [67], particularly humans, who are often swayed by immediate rewards at the expense of long-term benefits. While defection may cater to an agent\u2019s inherent preferences, offering higher immediate rewards, it can prompt other agents to defect rather than cooperate, ultimately undermining the overall social welfare of the system. This phenomenon underscores the importance of strategies like those learned in HGRL, which not only counter short-term defection tendencies but also enhance long-term cooperative behavior, thereby improving the overall health of the system. Lastly, the performance of all methods decreases as the probability of imitation increases. Defection can be contagious and easily spread to the system because of social learning. When the probability of imitation increases, the speed of social learning becomes more rapid, and this tendency of defection spreading can easily dominate the whole system. In our experimental settings, if every agent becomes a defector and chooses to defect all the time, the social welfare will be the lowest, which justifies the results in the figure. In the real-world situation, we can also see some evidence. If every person is extremely selfish and only considers themselves, and when this behavior spreads rapidly, society will become unsympathetic and inconsiderate, the social welfare will decrease, and the system may even collapse.\n# 4.3 Experiment results \u2013 behavior\nIn the previous section, we demonstrated the superior performance of the proposed HGRL model over Flat-RL in various aspects, then it is equally crucial to examine the learned behavior of the HGRL manager. As AI models evolve, concerns about the \"black-box\" problem become increasingly\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd01/fd01715e-6549-49e2-9c5e-dcae02c2fa34.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: The evolution of different network metrics over time with various imitation probabilities. The figures show 3 network metrics: average node degree, network diameter, and network modularity. The x-axis is the time step.</div>\npronounced. Understanding the decision-making process of the HGRL model could provide valuable insights into the development of more efficient and transparent AI governance strategies. We only analyzed the behavior of the HGRL manager and the evolution of systems in 20-node networks. Three metrics were used to study the evolution of the networks over time under varying imitation probabilities: average node degrees, network diameter, and network modularity. The results are presented in Figure 5. Generally, the curves for different imitation probabilities begin to diverge after 30 time steps. This pattern indicates that the influence of varying levels of social learning becomes significant after 30 time steps. Consequently, the systems initially exhibit similar trends but gradually diverge under the flexible intervention of HGRL\u2019s policies. Specifically, the average node degrees decrease through time in all scenarios, with higher social learning scenarios resulting in lower average node degrees. The initial random networks form several irrational connections, such as Cooperator-Defector and Defector-Defector links. Deleting these links benefits all scenarios, leading to generally sparser networks. In scenarios with higher social learning, it is more difficult for the system to conserve cooperators. As a result, these systems tend to be sparser which can reduce connections between defectors. Additionally, the network diameter generally increases over time, with scenarios of higher social learning resulting in larger network diameters. This pattern indicates that the networks tend to become more chain-like over time. Specifically, in scenarios with an imitation probability of 1.0, the networks conclude with 2 node degrees and a network diameter of 8 on average, essentially forming chain-like networks with few branches. Finally, the network modularity in all scenarios tends to increase over time, with higher social learning scenarios resulting in higher modularity. This indicates that agents in the system become more separated during evolution. In scenarios with lower imitation probability, more cooperators can be conserved and the presence of more connections to cooperators is beneficial to social welfare, which tends to form a large single community with lower network modularity in these scenarios. We also visualized some snapshots of how the network evolves through time, we randomly selected one random seed as an example to take a look at the learned behaviors in 20-node scenarios. We took some snapshots of the system evolution under three different scenarios with imitation probability similar to previous settings, which can be seen in Figure 6. The first row illustrates the system\u2019s evolution under a scenario with 0 imitation probability, where agent types remain constant. Observing snapshots from different times during the game, it becomes apparent that the HGRL manager has learned to strategically manipulate network connections. This strategy involves fostering connections among cooperators, while simultaneously deleting links both between defectors and\n# pronounced. Understanding the decision-making process of the HGRL model could provide valuable insights into the development of more efficient and transparent AI governance strategies.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/93c1/93c14110-1bb6-4fc0-b629-c15e0fb6f742.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4cd2/4cd294bc-8fda-4431-8c7a-dd047e7b92a7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c1bc/c1bc1bdf-d497-4983-958a-ebdcee6f0d18.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"> Initial state with 0.0 imitation obability (b) Middle state with 0.0 imitation probability</div>\n<div style=\"text-align: center;\">mitation (b) Middle state with 0.0 imitation probability</div>\n<div style=\"text-align: center;\">(b) Middle state with 0.0 imitation probability</div>\n(a) Initial state with 0.0 imitation probability\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d05/3d05edf0-eb74-43bc-b42f-b155da65ee5d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a0c8/a0c8a9d1-38f4-4344-a7c1-32787afd701c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Initial state with 0.5 imitation probability (e) Middle state with 0.5 imitation probability</div>\n(d) Initial state with 0.5 imitation probability\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a4d9/a4d97f6d-0e30-4a72-920d-722e5cea93ef.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(h) Middle state with 1.0 imitation probability (i) Final state with 1.0 imitation probability</div>\n<div style=\"text-align: center;\">(g) Initial state with 1.0 imitation probability (h) Middle state with 1.0 imitation probability</div>\nFigure 6: The snapshots of the system evolution with 20 nodes in the initial, middle and final phases of the game under 0, 0.5 and 1 imitation probabilities. The green agents are cooperators, and the blue agents are the defectors.\nbetween cooperators and defectors. As a result, the final state of the system becomes a core-periphery network: a densely connected group of cooperators as cores and more loosely connected defectors. It\u2019s important to note that our environmental settings prohibit the disconnection of the network. Since interactions between cooperators and defectors, as well as between defectors themselves, tend to negatively impact social welfare, the framework will prevent these situations by disconnecting those interactions and promoting the existence of cooperator-cooperator connections. The second row showcases the system\u2019s evolution in a scenario with a 0.5 imitation probability, where agents gradually modify their types through social learning. During this process, defection swiftly spreads among the agents, with 3 cooperators out of 13 shifting to defectors in mid-game. In contrast to the 0 imitation probability case, where disconnecting between cooperators-defectors and deleting connections between defectors are similarly prioritized, the HGRL manager in this scenario focuses more on breaking links between cooperators and defectors. This approach is understandable given that defection, with its temptation of higher immediate rewards, spreads via interactions between cooperators and defectors. While deleting connections among defectors does enhance social welfare in the short term, the cooperators are at risk of being \"contaminated\" by defection through social learning. This shift can lead to a long-term decline in social welfare, as defection becomes\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d1b/1d1bee98-4e80-4ee9-969e-442437f28e3f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Final state with 0.0 imitation probability</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/49f9/49f99bea-6504-461b-8715-2dc2699f79f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(i) Final state with 1.0 imitation probability</div>\nmore dominant and agents increasingly opt to defect. The HGRL manager, therefore, adopts a forward-looking strategy that prioritizes long-term welfare over immediate gains.\nInterestingly, despite the spread of defection, it does not completely overpower the system by the game\u2019s end. There remain 10 cooperators due to the HGRL manager\u2019s strategic interventions. The final state reveals a robust cooperator community, resilient to defection despite some interactions with defectors. This cooperator community maintains a higher overall social welfare, safeguarding its members from deviating towards defection. To counter the spread of defection in this dynamic social learning environment, the HGRL manager\u2019s strategy is twofold: first, it targets critical links between cooperators and defectors for disconnection, and second, it rapidly builds a strong cooperator community before they become influenced by defectors. This dual-aspect approach not only preserves more cooperators but also elevates the system\u2019s overall social welfare. The third row provides insights into scenarios where the imitation probability is set to 1.0, indicating a highly aggressive adaptation of types through social learning. In such a scenario, the transition from cooperators to defectors happens rapidly. By mid-game, all cooperators have already converted to defectors. Given these circumstances, the HGRL manager\u2019s only viable strategy is to delete connections between defectors, aiming to mitigate the deteriorating situation. Consequently, at the game\u2019s end, we observe a sparse chain-like network comprising solely defectors. In this scenario, the authority of the system manager is deliberately constrained, allowing only minor alterations \u2014 one link change per time step. Given the rapid spread of defection, this limited authority proves insufficient to manage the situation effectively. Despite the manager\u2019s efforts to disconnect defectors, which may marginally preserve social welfare, defection ultimately prevails, leading to the system\u2019s collapse. This outcome offers valuable insights, particularly in the context of real-world applications. It underscores the importance of appropriately balancing the level of managerial authority. Excessive authority can negatively impact agent behavior, especially when agents are human. In such cases, agents might refuse to follow commands, feeling coerced into actions against their will. Conversely, insufficient authority risks leading to an anarchic system where negative outcomes can be inevitable in some scenarios, and all agents suffer. This delicate balance is crucial in ensuring effective governance and maintaining a stable, functional multi-agent system.\n# 5 Conclusion\nThis paper introduces the Hierarchical Graph Reinforcement Learning (HGRL) framework, designed to govern multi-agent systems with network structures through strategic network interventions. This framework is adaptable to a variety of initial network configurations and agent characteristics, accommodating the dynamics of social learning. Importantly, it operates under a realistic premise of limited authority, making it applicable to real-world systems where full control is often impractical. A key innovation of the HGRL framework is its ability to address the exponential complexity inherent in the state and action spaces of network structure interventions. It employs Graph Neural Networks (GNN) to manage the vast state space effectively and reduces the O(N 2) action space of traditional Flat Reinforcement Learning (Flat-RL) to O(N), due to its hierarchical structure. Furthermore, the paper introduces the creation of an environment that encapsulates critical aspects of multi-agent systems with network structures. This environment not only simulates strategic interactions among agents but also incorporates the element of social learning and constrained managerial authority. From the results in the testing phase, the HGRL framework demonstrates superior performance over conventional Flat-RL in enhancing overall system performance and in\nguiding systems to states of higher social welfare. The improvement is consistently observed across various scenarios, each characterized by differing levels of imitation behavior in the social learning process. The learned behaviors of the HGRL framework are analyzed, affirming the rationality of its learned strategies. These strategies generate valuable heuristics that can be applied more broadly. Although the HGRL manager cannot prevent system collapse in cases of intense social learning, this outcome highlights the importance of authority levels in real-world applications and strategical governance of multi-agent systems with the complex dynamics of agents and their interactions. One potential future work involves manipulating the information flow as a tool for network intervention. This approach aims to not only conserve the initial cooperators but also convert defectors to cooperators under non-trivial situations. Manipulating information as an intervention tool is advantageous because it is subtler and less noticeable to users, facilitating easier implementation. In the current framework, the policy of network structure intervention is trained to optimize social welfare, and the social learning process is influenced by the change of network structure dependently as indirect effects. Adding the idea of information manipulation, we can separate the social learning process and train another manager who recommends which agents to observe and imitate with reasonable constraints. Additionally, the imitation probability can be seen as the transparency or trustworthiness of the information. With a higher imitation probability, the information that agents can receive is more trustworthy, and they will base on it to update their strategy with less doubt. It is promising to consider changing the imitation probability dynamically to see how it can promote social welfare.\n# References\n[1] Babak Heydari and Michael J Pennock. Guiding the behavior of sociotechnical systems: The role of agent-based modeling. Systems Engineering, 21(3):210\u2013226, 2018. [2] Michael Sony and Subhash Naik. Industry 4.0 integration with socio-technical systems theory: A systematic review and proposed theoretical model. Technology in society, 61:101248, 2020. [3] Astrid Layton, Bert Bras, and Marc Weissburg. Designing industrial networks using ecological food web metrics. Environmental science & technology, 50(20):11243\u201311252, 2016. [4] Bryan M O\u2019Halloran, Nikolaos Papakonstantinou, Kristin Giammarco, and Douglas L Van Bossuyt. A graph theory approach to functional failure propagation in early complex cyber-physical systems (ccpss). In INCOSE International Symposium, volume 27, pages 1734\u20131748. Wiley Online Library, 2017. [5] Kannan Govindan, Mohammad Fattahi, and Esmaeil Keyvanshokooh. Supply chain network design under uncertainty: A comprehensive review and future research directions. European journal of operational research, 263(1):108\u2013141, 2017. [6] Erfan Yazdandoost, Young-Jun Son, and Mohammed Shafae. Taxonomy-driven graph-theoretic framework for manufacturing cybersecurity risk modeling and assessment. Journal of Computing and Information Science in Engineering, 24:071003\u20131, 2024. [7] Jiaxin Wu and Pingfeng Wang. Generative design for resilience of interdependent network systems. Journal of Mechanical Design, 145(3):031705, 2023. [8] Ruoyu Su, Dengyin Zhang, Ramachandran Venkatesan, Zijun Gong, Cheng Li, Fei Ding, Fan Jiang, and Ziyang Zhu. Resource allocation for network slicing in 5g telecommunication networks: A survey of principles and models. IEEE Network, 33(6):172\u2013179, 2019.\n# [1] Babak Heydari and Michael J Pennock. Guiding the behavior of sociotechnical systems: The role of agent-based modeling. Systems Engineering, 21(3):210\u2013226, 2018.\n[9] Dazhong Wu, David W Rosen, Jitesh H Panchal, and Dirk Schaefer. Understanding communication and collaboration in social product development through social network analysis. Journal of Computing and information Science in Engineering, 16(1):011001, 2016. [10] Ashish M Chaudhari, Erica L Gralla, Zoe Szajnfarber, and Jitesh H Panchal. Co-evolution of communication and system performance in engineering systems design: a stochastic networkbehavior dynamics model. Journal of Mechanical Design, 144(4):041706, 2022. [11] Xiaowen Lin, Qian Dang, and Megan Konar. A network analysis of food flows within the united states of america. Environmental science & technology, 48(10):5439\u20135447, 2014. [12] Varuneswara Panyam, Hao Huang, Katherine Davis, and Astrid Layton. Bio-inspired design for robust power grid networks. Applied Energy, 251:113349, 2019. [13] Tirth Dave and Astrid Layton. Designing ecologically-inspired robustness into a water distribution network. Journal of Cleaner Production, 254:120057, 2020. [14] Lenos Trigeorgis. Real options: Managerial flexibility and strategy in resource allocation. MIT press, 1996. [15] Qiliang Chen and Babak Heydari. The sos conductor: Orchestrating resources with iterative agent-based reinforcement learning. Systems Engineering, 2024. [16] David A Gianetto and Babak Heydari. Catalysts of cooperation in system of systems: The role of diversity and network structure. IEEE Systems Journal, 9(1):303\u2013311, 2013. [17] Mohsen Mosleh and Babak Heydari. Fair topologies: community structures and network hubs drive emergence of fairness norms. Scientific reports, 7(1):1\u20139, 2017. [18] David A Gianetto and Babak Heydari. Network modularity is essential for evolution of cooperation under uncertainty. Scientific reports, 5(1):9340, 2015. [19] Nicol\u00e1s F Soria Zurita, Mitchell K Colby, Irem Y Tumer, Christopher Hoyle, and Kagan Tumer. Design of complex engineered systems using multi-agent coordination. Journal of Computing and Information Science in Engineering, 18(1):011003, 2018. [20] Hao Ji and Yan Jin. Knowledge acquisition of self-organizing systems with deep multiagent reinforcement learning. Journal of Computing and Information Science in Engineering, 22(2):021010, 2022. [21] William Poundstone. Prisoner\u2019s dilemma: John von Neumann, game theory, and the puzzle of the bomb. Anchor, 1993. [22] Robyn M Dawes. Social dilemmas. Annual review of psychology, 31(1):169\u2013193, 1980. [23] Zhenghui Sha and Jitesh H Panchal. Estimating local decision-making behavior in complex evolutionary systems. Journal of Mechanical Design, 136(6):061003, 2014. [24] Albert Bandura and Richard H Walters. Social learning theory, volume 1. Englewood cliffs Prentice Hall, 1977. [25] Carlos Morato, Krishnanand N Kaipa, Boxuan Zhao, and Satyandra K Gupta. Toward safe human robot collaboration by using multiple kinects based real-time human tracking. Journal of Computing and Information Science in Engineering, 14(1):011006, 2014. [26] Mohsen Mosleh, Peter Ludlow, and Babak Heydari. Resource allocation through network architecture in systems of systems: A complex networks framework. In 2016 Annual IEEE Systems Conference (SysCon), pages 1\u20135. IEEE, 2016.\n[27] Xinyao Zhang, Sibo Tian, Xiao Liang, Minghui Zheng, and Sara Behdad. Early prediction of human intention for human\u2013robot collaboration using transformer network. Journal of Computing and Information Science in Engineering, 24(5), 2024. [28] Lu He, Changyang He, Tera L Reynolds, Qiushi Bai, Yicong Huang, Chen Li, Kai Zheng, and Yunan Chen. Why do people oppose mask wearing? a comprehensive analysis of us tweets during the covid-19 pandemic. Journal of the American Medical Informatics Association, 28(7):1564\u20131573, 2021. [29] Koen van der Zwet, Ana I Barros, Tom M van Engers, and Peter MA Sloot. Emergence of protests during the covid-19 pandemic: quantitative models to explore the contributions of societal conditions. Humanities and Social Sciences Communications, 9(1), 2022. [30] Qiliang Chen and Babak Heydari. Dynamic resource allocation in systems-of-systems using a heuristic-based interpretable deep reinforcement learning. Journal of Mechanical Design, 144(9):091711, 2022. [31] Mohsen Mosleh, Peter Ludlow, and Babak Heydari. Distributed resource management in systems of systems: An architecture perspective. Systems Engineering, 19(4):362\u2013374, 2016. [32] Guoqi Li, Lei Deng, Gaoxi Xiao, Pei Tang, Changyun Wen, Wuhua Hu, Jing Pei, Luping Shi, and H Eugene Stanley. Enabling controlling complex networks with local topological information. Scientific reports, 8(1):4593, 2018. [33] Jie Ding, Changyun Wen, Guoqi Li, and Zhenghua Chen. Key nodes selection in controlling complex networks via convex optimization. IEEE Transactions on Cybernetics, 51(1):52\u201363, 2019. [34] Xiaoge Zhang, Sankaran Mahadevan, Shankar Sankararaman, and Kai Goebel. Resilience-based network design under uncertainty. Reliability Engineering & System Safety, 169:364\u2013379, 2018. [35] Michael D Siciliano and Travis A Whetsell. Strategies of network intervention: A pragmatic approach to policy implementation and public problem resolution through network science. arXiv preprint arXiv:2109.08197, 2021. [36] Jaimie Ellis, Ivaylo Vassilev, Elizabeth James, and Anne Rogers. Implementing a social network intervention: can the context for its workability be created? a quasi-ethnographic study. Implementation Science Communications, 1:1\u201311, 2020. [37] Edward JS Hearnshaw and Mark MJ Wilson. A complex network approach to supply chain network theory. International Journal of Operations & Production Management, 33(4):442\u2013469, 2013. [38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [39] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020. [40] Laxmi Poudel, Wenchao Zhou, and Zhenghui Sha. A generative approach for scheduling multi-robot cooperative three-dimensional printing. Journal of Computing and Information Science in Engineering, 20(6):061011, 2020. [41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\n[42] Qiliang Chen, Babak Heydari, and Mohsen Moghaddam. Leveraging task modularity in reinforcement learning for adaptable industry 4.0 automation. Journal of Mechanical Design, 143(7):071701, 2021. [43] Xiao-Meng Zhang, Li Liang, Lin Liu, and Ming-Jing Tang. Graph neural networks and their current applications in bioinformatics. Frontiers in genetics, 12:690049, 2021. [44] Pingping Cao, Zeqi Zhu, Ziyuan Wang, Yanping Zhu, and Qiang Niu. Applications of graph convolutional networks in computer vision. Neural Computing and Applications, 34(16):13387\u2013 13405, 2022. [45] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In The world wide web conference, pages 417\u2013426, 2019. [46] Liqun Gao, Haiyang Wang, Zhouran Zhang, Hongwu Zhuang, and Bin Zhou. Hetinf: Social influence prediction with heterogeneous graph neural network. Frontiers in Physics, 9:787185, 2022. [47] Abhishek Tomy, Matteo Razzanelli, Francesco Di Lauro, Daniela Rus, and Cosimo Della Santina. Estimating the state of epidemics spreading with graph neural networks. Nonlinear Dynamics, 109(1):249\u2013263, 2022. [48] Paul Almasan, Jos\u00e9 Su\u00e1rez-Varela, Krzysztof Rusek, Pere Barlet-Ros, and Albert CabellosAparicio. Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case. Computer Communications, 196:184\u2013194, 2022. [49] Zhongxia Yan, Jingguo Ge, Yulei Wu, Liangxiong Li, and Tong Li. Automatic virtual network embedding: A deep reinforcement learning approach with graph convolutional networks. IEEE Journal on Selected Areas in Communications, 38(6):1040\u20131057, 2020. [50] Laura Stops, Roel Leenhouts, Qinghe Gao, and Artur M Schweidtmann. Flowsheet generation through hierarchical reinforcement learning and graph neural networks. AIChE Journal, 69(1):e17938, 2023. [51] Zhong Yang, Li Bi, and Xiaogang Jiao. Combining reinforcement learning algorithms with graph neural networks to solve dynamic job shop scheduling problems. Processes, 11(5):1571, 2023. [52] Kevin R McKee, Andrea Tacchetti, Michiel A Bakker, Jan Balaguer, Lucy Campbell-Gillingham, Richard Everett, and Matthew Botvinick. Scaffolding cooperation in human groups with deep reinforcement learning. Nature Human Behaviour, pages 1\u201310, 2023. [53] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381, 2020. [54] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [55] Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. Hierarchical reinforcement learning: A comprehensive survey. ACM Computing Surveys (CSUR), 54(5):1\u201335, 2021. [56] Babak Heydari, Mohsen Mosleh, and Kia Dalili. Efficient network structures with separable heterogeneous connection costs. Economics Letters, 134:82\u201385, 2015. [57] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3438\u20133445, 2020.\n[58] Robert Axelrod and William D Hamilton. The evolution of cooperation. science, 211(4489):1390\u20131396, 1981. [59] Daniel G Arce. Economics, ethics and the dilemma in the prisoner\u2019s dilemmas. The American Economist, 55(1):49\u201357, 2010. [60] Richard Swedberg. Sociology and game theory: Contemporary and historical perspectives. Theory and Society, 30(3):301\u2013335, 2001. [61] Stephen Le and Robert Boyd. Evolutionary dynamics of the continuous iterated prisoner\u2019s dilemma. Journal of theoretical biology, 245(2):258\u2013267, 2007. [62] Elinor Ostrom. Governing the commons: The evolution of institutions for collective action. Cambridge university press, 1990. [63] P ERDdS and A R&wi. On random graphs i. Publ. math. debrecen, 6(290-297):18, 1959. [64] Carlos Gracia-L\u00e1zaro, Jos\u00e9 A Cuesta, Angel S\u00e1nchez, and Yamir Moreno. Human behavior in prisoner\u2019s dilemma experiments suppresses network reciprocity. Scientific reports, 2(1):325, 2012. [65] David A Gianetto and Babak Heydari. Sparse cliques trump scale-free networks in coordination and competition. Scientific reports, 6(1):21870, 2016. [66] Zachary Fulker, Patrick Forber, Rory Smead, and Christoph Riedl. Spite is contagious in dynamic networks. Nature communications, 12(1):260, 2021. [67] Herbert Simon. A behavioral model of rational choice. Models of man, social and rational: Mathematical essays on rational human behavior in a social setting, 6(1):241\u2013260, 1957.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of governing complex multi-agent systems (MAS) through network interventions, emphasizing the importance of network structure in shaping agent interactions and system-wide performance. Previous methods have struggled to effectively manage these systems due to challenges such as evolving dynamics, limited managerial authority, and the vastness of state and action spaces. The introduction of the Hierarchical Graph Reinforcement Learning (HGRL) framework aims to overcome these challenges and enhance governance in MAS.",
        "problem": {
            "definition": "The problem focuses on the effective governance of complex multi-agent systems where interactions are influenced by dynamic network structures. The aim is to promote pro-social behavior among agents while managing the inherent complexities of these systems.",
            "key obstacle": "The main difficulty lies in the evolving dynamics of agent interactions and the limited authority of the system manager, compounded by the exponential growth of state and action spaces as the number of agents increases."
        },
        "idea": {
            "intuition": "The idea behind the HGRL framework is inspired by the need to manage complex systems through targeted interventions in network structures, utilizing insights from network science and reinforcement learning.",
            "opinion": "The proposed HGRL framework enables efficient learning of network intervention policies, addressing the challenges of traditional methods by reducing the complexity of state and action spaces.",
            "innovation": "HGRL innovates by integrating Graph Neural Networks (GNN) with Reinforcement Learning (RL) in a hierarchical structure, significantly simplifying the decision-making process compared to existing approaches."
        },
        "method": {
            "method name": "Hierarchical Graph Reinforcement Learning",
            "method abbreviation": "HGRL",
            "method definition": "HGRL is a framework that combines GNNs and RL to govern multi-agent systems through strategic network interventions, optimizing social welfare while operating under limited authority.",
            "method description": "The HGRL framework facilitates targeted interventions in network structures, enhancing cooperation among agents in complex multi-agent systems.",
            "method steps": "1. Node agent selects a node based on GNN-embedded information. 2. Link agent determines which links to add or remove based on the selected node's information. 3. Iterative process continues to optimize network structure.",
            "principle": "The effectiveness of HGRL lies in its hierarchical structure that reduces the complexity of both state and action spaces, allowing for scalable and adaptable governance in dynamic environments."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in simulated environments with 10-node and 20-node networks, utilizing the Prisoner's Dilemma as the interaction mechanism among agents. The system manager's authority was limited to modifying one link per time step.",
            "evaluation method": "Performance was assessed through metrics such as average social welfare during the game and average final social welfare, comparing HGRL against Flat-RL and random strategies across different scenarios of agent imitation probabilities."
        },
        "conclusion": "The HGRL framework outperforms traditional methods in managing multi-agent systems, demonstrating its ability to enhance overall system performance and guide networks toward higher social welfare despite challenges posed by social learning and limited authority.",
        "discussion": {
            "advantage": "HGRL's primary advantage is its ability to effectively manage complex agent interactions through a hierarchical approach, significantly improving social welfare outcomes compared to Flat-RL and random strategies.",
            "limitation": "A limitation of the HGRL framework is its reduced effectiveness under high imitation probabilities, where rapid defection can overwhelm the system despite the manager's interventions.",
            "future work": "Future research could explore manipulating information flow as a means of network intervention, aiming to enhance cooperation and mitigate defection within multi-agent systems."
        },
        "other info": {
            "keywords": [
                "Network intervention",
                "Multi-agent system",
                "Graph neural networks",
                "Deep reinforcement learning",
                "Hierarchical structure"
            ],
            "arXiv": "arXiv:2410.23396v"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of governing complex multi-agent systems (MAS) through network interventions, emphasizing the importance of network structure in shaping agent interactions and system-wide performance."
        },
        {
            "section number": "1.2",
            "key information": "The problem focuses on the effective governance of complex multi-agent systems where interactions are influenced by dynamic network structures. The aim is to promote pro-social behavior among agents while managing the inherent complexities of these systems."
        },
        {
            "section number": "2.1",
            "key information": "HGRL innovates by integrating Graph Neural Networks (GNN) with Reinforcement Learning (RL) in a hierarchical structure, significantly simplifying the decision-making process compared to existing approaches."
        },
        {
            "section number": "4.1",
            "key information": "HGRL is a framework that combines GNNs and RL to govern multi-agent systems through strategic network interventions, optimizing social welfare while operating under limited authority."
        },
        {
            "section number": "6.3",
            "key information": "A limitation of the HGRL framework is its reduced effectiveness under high imitation probabilities, where rapid defection can overwhelm the system despite the manager's interventions."
        },
        {
            "section number": "6.4",
            "key information": "Future research could explore manipulating information flow as a means of network intervention, aiming to enhance cooperation and mitigate defection within multi-agent systems."
        }
    ],
    "similarity_score": 0.5579670886986057,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0239_large/papers/Adaptive Network Intervention for Complex Systems_ A Hierarchical Graph Reinforcement Learning Approach.json"
}