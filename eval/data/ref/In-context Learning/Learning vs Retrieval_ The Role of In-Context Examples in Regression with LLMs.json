{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.04318",
    "title": "Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs",
    "abstract": "Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.",
    "bib_name": "nafar2024learningvsretrievalrole",
    "md_text": "# Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs\n1 Michigan State University 2 Florida Institute for Human and Machine Cognition nafarali@msu.edu, bvenable@ihmc.org, kordjams@msu.edu\nAbstract\nGenerative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.\narXiv:2409.04318v1\n# Introduction\nThe emergence of transformers (Vaswani et al. 2017) has revolutionized natural language processing, leading to the development of LLMs such as GPTs (Brown et al. 2020) and LLaMA (Touvron et al. 2023a). In addition to their impressive zero-shot performance, these models demonstrated the capability of in-context learning (ICL), by which they learn a task from examples provided in the context of the prompt (Brown et al. 2020). In tasks where both zero-shot and ICL settings are applicable, ICL consistently outperforms the zero-shot setting (Brown et al. 2020; Liu et al. 2022). However, the inner mechanism of ICL in LLMs remains a topic of discussion. Broadly speaking, current research identifies two main approaches to the explain ICL mechanism: 1) Meta-learning (Schmidhuber 1987), which suggests that transformers are capable of using the incontext input-output examples to learn their distribution function; 2) Knowledge retrieval, according to which LLMs utilize in-context inputs to retrieve knowledge from the data they were trained on and then apply it to the given input. We propose a different approach, arguing that ICL is not merely\nlearning or retrieving knowledge, but rather, its behavior lies on a spectrum between the two, which can be adjusted depending on various factors. Before exploring our proposed approach, we elaborate on the methods mentioned above 1. The first hypothesis suggests that transformers are effective meta-learners, and LLMs can generate accurate predictions based solely on the given input-output pairs. (Bai et al. 2023) theoretically proves and practically tests the capability of an encoder-based transformer to implement generalized linear models in-context. (Garg et al. 2022) uses a decoderbased model, a GPT-2 architecture modified for regression, that puts the input features inside the embeddings (instead of using tokens) and outputs a number. Their model\u2019s performance surpasses a 2-layer Multi-Layer Perceptron (MLP) and a decision tree by meta-learning. Further, (Vacareanu et al. 2024) directly uses LLMs to test regression capabilities using a limited number of features (less than 3) and concludes that LLMs are capable regressors. However, according to our findings, their claim of avoiding data contamination is not well-supported. We note that none of these research works use realistic datasets or consider the combination of meta-learning with knowledge retrieval. The second approach emphasizes knowledge retrieval while downplaying the learning aspect. For example, (Min et al. 2022) examines 12 LLMs across various classification tasks, concluding that altering output labels, which are needed for learning, has no impact on performance. They argue that the crucial elements of ICL are limited to defining the label space, input distribution, and overall task format. Meanwhile (Kossen, Gal, and Rainforth 2024) argues that LLMs utilize the labels but in an arbitrary manner when experimenting with the same classification tasks. (Li et al. 2024) further dismisses the significance of output labels and instead proposes two knowledge retrieval approaches. These include the retrieval of pre-existing solutions from training data (Min et al. 2022; Wang et al. 2024; Wies, Levine, and Shashua 2023), and a novel solution composition approach, inspired by (Hahn and Goyal 2023), which proposes that learned solutions can be combined. While this work offers valuable insights into ICL mechanisms, its findings are potentially skewed by experimental design choices and a narrow selection of datasets and models. A significant limita-\n1See Appendix A for a detailed discussion on related work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e43e/e43ee58a-56cd-4025-abbd-eeb1cb3f1845.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 1: The three main prompt configurations that we use in our regression task. In configuration a) the actual names of e features and the output are known, and the LLM is asked to guess the \u201cprice of a used Toyota or Maserati car in 2019\u201d. onfiguration b) is similar to a) except that the feature names are anonymized. Here, the LLM is asked to estimate the \u201cOutput\u201d. nally, in Configuration c), we replace the real prices of in-context examples with randomly (Gaussian) generated numbers.</div>\ntion is the exclusive use of LLaMA 2 (Touvron et al. 2023b), which is known to struggle with long token contexts (Machlab and Battle 2024; Zuhashaik et al. 2023) which negatively impacts the ICL. Our research challenges several aspects of their conclusions. For instance, they report that remapping the inputs of in-context input-output pairs to alternative text reduces the accuracy to that of a random model (even with up to 40 in-context examples). On the other hand, under similar conditions, our experiments indicate that learning occurs with optimal performance. We refrain from making specific claims about task selection or composition in our work. Instead, we categorize these processes under the broader umbrella of knowledge retrieval 2. Similarly, to investigate incontext learning, (Pan et al. 2023) uses simple classification tasks and older models (such as GPT-3 Ada) and as a result its findings are partially different from ours. We propose a different hypothesis that resolves the contradictions in the research community and support our claims with extensive empirical testing. We argue that ICL is not merely learning or retrieving knowledge but uses a combination of the two, which lies on a spectrum determined by factors we can manipulate. In this regard, we propose an evaluation framework and conduct a comparative study of different LLMs and datasets, focusing specifically on regression problems as our testing ground. In our evaluation framework, we query the LLM to estimate an output number based on a set of (feature, value) pairs given a set of ICL examples, as shown in Figure 1(a). We opted for regression tasks primarily for two reasons. First, this choice aligns our work with the majority of related ICL meta-learning research, thus facilitating direct comparisons and building upon existing findings. This task also has a complex output space (e.g., continuous or unbounded) which is challenging for LLMs (Fang et al. 2024). We show that LLMs can per-\n2See Appendix B for a more detailed analysis of the discrepancies between our findings and those of (Li et al. 2024).\nform regression on realistic datasets and measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on the factors we use in our framework: the number of (feature, value) pairs, the number of in-context examples, and the prompting strategies. In summary our contributions are as follows: 1) We demonstrate that LLMs can effectively learn from regression examples of realistic datasets in-context, extending previous work on synthetic data to more practical scenarios. 2) We propose an ICL mechanism hypothesis that combines both learning and knowledge retrieval during the LLM inference, reconciling the research community\u2019s findings. 3) We introduce an evaluation framework that allows for systematic comparison of ICL mechanisms across different LLMs, datasets, and prompt configurations3. 4) We provide a comprehensive analysis of how LLMs balance internal knowledge retrieval and learning from incontext examples and provide prompt engineering tools to control them.\n# Problem Setting\nOur study focuses on a regression task where we use LLMs to predict numerical outputs based on the given inputs. We assume a regression dataset, D = {(x1, y1), (x2, y2), . . . , (xn, yn)}, comprising input-output pairs, is given. Each xi comprises a set of pairs (fij, vij) where fij represents the feature\u2019s name and vij is a numerical value for fij. Furthermore, the target variable yi is the numerical value of the output. For example, if we consider a dataset providing the price of used cars given their fuel economy and mileage, we could have an input with 2 (feature, value) pairs, (Fuel Economy,16), (Mileage, 0), and an\n3The framework code and analytical tools used in this work are available at https://github.com/HLR/LvsR-LLM.\noutput answer of 95595 for the price. In our experimental setting, we first present the LLM with a subset of m input-output examples {(x1, y1), (x2, y2), . . . , (xm, ym)} from the in-context split of D for learning. Then, we query the LLM with the feature pairs of x from the test split of D and expect the answer y as the output.4 Figure 1(a) depicts our problem setting in the context of a regression task that involves predicting the \u201cprice of a used Toyota or Maserati car in 2019\u201d, which is the target variable name, given three (feature, value) pairs with a set of in-context examples provided beforehand. When m equals 0, we obtain the zero-shot setting where no prior examples are provided.\n# Prompt Configurations\nWe employ three main prompt configurations for prompting the models, as shown in Figure 1. These configurations vary in different ways, such as hiding the real name of features (Figure 1(b)) or the actual ground truth (Figure 1(c)). We also prompt the LLM with simple numerical generation as a baseline in our experiments. This results in a total of four prompt configurations. We also considered other prompt configuration which, either served as ablations, or didn\u2019t provide any significant insights. We do not formally define them but briefly discuss them later in our experiments. In each prompt configuration, the context of the prompt comprises a task instruction, in-context examples (sample regression task input-outputs), and a query. The task instruction asks the LLM to estimate the objective of the dataset (such as, the price of a used car) based on the given features by just providing a number and no explanation as the output. Each of these three parts can change or be removed according to the prompt configuration as explained below: Named Features (Configuration a): This is our most straightforward prompt configuration. It reveals the actual names of the features and the required target variable (depending on the dataset) in the context of the prompt. For instance, in the case of a dataset about the price of used cars, the LLM is instructed to estimate a \u201cUsed Car Price\u201d based on \u201cCity fuel Economy\u201d, \u201cMileage\u201d and \u201cPassenger Car Classification\u201d. Following this instruction, sample input-output examples are demonstrated to the model, and then the query is asked as shown in Figure 1(a). Anonymized Features (Configuration b): In this setup, which is shown in Figure 1(b), we present the LLM with input-output pairs where the actual name of the features and the target variable are changed to the generic names of \u201cFeature #\u201d and \u201cOutput\u201d, respectively. The instruction asks the LLM to estimate the \u201cOutput\u201d based on the given \u201cFeatures\u201d. Here, the LLM can not use its domain knowledge, and can only use the provided number of the features.\nRandomized Ground Truth (Configuration c): In our final main prompt configuration shown in Figure 1(c), we\n4We note that since we do not change the architecture of the LLMs, all numbers must be represented to and read by the LLM in textual form.\n4We note that since we do not change the architecture of the LLMs, all numbers must be represented to and read by the LLM in textual form.\nmaintain the named features but replace the ground truth values with randomly generated numbers (independent of the feature values). These random numbers are generated using a Gaussian distribution based on the dataset statistics. This setup mainly serves as a control to test to what degree the LLMs are truly learning from the provided ground truth. Direct Question Answering (Direct QA): To establish an LLM baseline, we ask our LLMs to estimate the target variable based on the given named features without any incontext examples (m = 0). In this prompt configuration, we also define the scope of the output by providing the mean and the standard definition of the questioned dataset in the instruction to the LLM. An example of this added information is: \u201cEstimate the insurance cost of this person given the information. An issuance cost is typically around 13270.42 with a standard deviation of 12110.01\u201d.\n# Models and Metrics\nIn our experiments, we evaluate several LLMs, including LLaMA 3 70B (AI@Meta 2024), GPT3.5 (Brown et al. 2020), and GPT4 (OpenAI 2023). We initially considered smaller LLMs such as Mistral 7B (Jiang et al. 2023), but found them insufficient for our regression tasks and subsequently excluded them. To provide a comprehensive comparison, we also employed classical machine learning techniques alongside these LLMs. Specifically, we utilized a straightforward Ridge regression (Hoerl and Kennard 1970) and the more advanced RandomForest model (Breiman 2001). Further details about these models, such as their hyper-parameters, are included in Appendix C. In our analysis, for each dataset, we test the LLMs with a mix of factors (we call these factors to distinguish them from the features fij defined in our problem setting). The first factor is the prompt configurations, which can be Named Feature, Anonymized Features, Randomized Ground Truth, or Direct QA. The second and third factors are the number of in-context examples and the number of (feature, value) pairs, which we will call the number of features going on. The number of in-context examples is 0, 10, 30, and 100, and the number of features is 1, 2, and 3 in our experiments. These feature numbers represent using the first feature (F1), the first two features (F2), and all three features (F3) (features are ordered and sorted in decreasing order of importance). Not all these factors can be used together as the 0 in-context examples factor can only be used with the Direct QA prompt configuration, and other prompt configurations can not be used with the 0 in-context examples factor. To assess the performance of the LLMs and the machine learning models on our regression tasks, we use Mean Squared Error (MSE) as our main comparison metric. We also calculate and report the coefficient of determination R2 and Mean Absolute Error (MAE) in Appendix D.\n# Experiments\nThis section presents our experimental findings, highlighting the interplay between various factors that influence the performance of LLMs across regression tasks and datasets.\nTo assess the LLMs\u2019 performance across diverse domains and complexity levels, we select three regression datasets.5 These datasets are pre-processed to select their most important features. All numerical values in the datasets are rounded to two decimal points. We divide each dataset into two splits: an in-context subset comprising 100 instances and a test subset with 300 instances.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2abe/2abe78c2-1482-4fb8-babd-d6eac005f86e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The relative importance of features across our datasets calculated with RandomForest (also reflecting improvements in our Ridge as later shown in Figure 7.</div>\nFigure 2: The relative importance of features across our datasets calculated with RandomForest (also reflecting improvements in our Ridge as later shown in Figure 7.\nAdmission Chance This dataset estimates the likelihood of admission to graduate programs for Indian students (Acharya, Armaan, and Antony 2019). Since this dataset is about Indian students, it may be less seen in our model\u2019s training data, reflecting the general imbalance that skews towards USA-centric data (Zhu et al. 2023). The dataset\u2019s selected features are CGPA (Cumulative Grade Point Average), GRE Score, and TOEFL Score. Among our three datasets, this is the only dataset with high inter-correlation among each feature pair (Pearson correlation (Pearson 1895) greater than 0.80). As a result, the feature importances, which are calculated with RandomForest (Louppe et al. 2013) and shown in Figure 2, assign minimal importance to the second and third features. The target variable has a mean of 0.72 and a standard deviation of 0.14. Insurance Cost This dataset focuses on predicting a similar distribution of the annual individual medical costs billed by health insurance companies in the USA, drawing from demographic statistics compiled by the U.S. Census Bureau (Lantz 2013). The selected features are Smoker Status, BMI, and Age with their importances shown in Figure 2. Smoking Status emerges as the most critical feature, followed by BMI, and then Age. The average and standard deviations of costs are 13, 270.42 and 12, 110.01, respectively. Used Car Prices We use selected subset of the used car prices dataset (Mital 2023) that involves predicting the price of a used Toyota or Maserati car in 2019. The features include City Fuel Economy, Mileage with similar high importance, and Passenger Car Classification with minimal importance, as shown in Figure 2. The target variable has a mean of 42, 279.49 and a standard deviation of 50, 014.51.\n5All datasets used in this study are publicly available. The preprocessing code for these datasets can be accessed at https://github. com/HLR/LvsR-LLM/tree/main/Datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc93/dc931a3c-36fe-4f67-9051-3a464b6f5cd9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 3: Baseline results of LLMs\u2019 (Direct QA configuration) across datasets and number of features. The dashed red line shows the performance of the Mean model.\n# Knowledge Retrieval Assessment (Direct QA)\nTo establish a baseline for our subsequent analyses, we first evaluate the LLMs\u2019 performance using only their knowledge retrieval, without any in-context examples. This baseline assessment is required for understanding how the incontext examples in other prompt configurations modulate the LLMs\u2019 performance and ICL mechanisms. The performance of LLMs with Direct QA prompt configuration is shown in Figure 3 on all three datasets. The red dashed line shows the performance of Mean model, which outputs the mean of the dataset independent of the input features. As can be observed the additional features generally improve the performance across tasks (with inconsistencies when using LLaMA 3). However, the LLMs\u2019 utilization of features does not directly correlate with previously established feature importances shown in Figure 2. As for the impact of datasets, the Admission Chance dataset consistently shows the poorest results with most outcomes at or above the Mean model\u2019s MSE. As mentioned earlier, this dataset is the least exposed to LLMs, which likely explains these findings. We further explored the Direct QA approach by asking the LLM to explain its reasoning process based on the given features before providing the final estimate. This variation yielded predictions comparable to the Direct QA results without offering significant additional insights and consequently was excluded from our main prompt configurations. See Appendix E for example answers and related diagrams.\n# Interplay of Knowledge Retrieval and Learning\nWe have defined four main prompt configurations to vary the degree to which the model uses its knowledge retrieval or learning from the outputs of the in-context examples. By comparing the performance of these prompt configurations, we can understand how LLMs utilize knowledge retrieval and learning from the outputs. Figure 4 presents a comprehensive comparison of these prompt configurations\u2019 effects on our models. The Randomized Ground Truth prompt configuration, shown with the lime color, consistently yields the worst results. Note that the names of the features are revealed to the model in this setting. As a result, this prompt configuration creates a scenario where patterns in the data may contradict the model\u2019s internal knowledge. For in-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/abdb/abdbcf63-5305-4b59-8ac9-1b38f9a0f1a2.png\" style=\"width: 50%;\"></div>\nFigure 4: Comprehensive comparison of prompt configurations\u2019 effects on our models across various factors, shown in a hierarchy. The top level for each dataset distinguishes between GPT-3, LLaMA 3, and GPT-4 results using black, grey, and white arcs, respectively. The notation ICi indicates the number of in-context examples, while F1, F2, and F3 represent the use of the first feature, the first two features, and all three features, respectively. The MSE scale of each dataset is shown at top left corner.\nstance, in the Insurance dataset, some in-context examples show that people who smoke less require lower insurance costs, while others indicate higher costs for the same group. The negative impact of this prompt configuration on performance becomes more significant as the number of in-context examples with random outputs increases. This is particularly evident when using 100 in-context examples. These results suggest that the LLMs are using the output variables to learn from the examples, and increasing the number of in-context examples shifts the spectrum from knowledge retrieval to learning from outputs. Our findings, which indicate that LLMs can be pushed to prioritize outputs from in-context examples over knowledge retrieval, challenge uselessness of the outputs claimed by (Min et al. 2022; Li et al. 2024).\nstance, in the Insurance dataset, some in-context examples show that people who smoke less require lower insurance costs, while others indicate higher costs for the same group. The negative impact of this prompt configuration on performance becomes more significant as the number of in-context examples with random outputs increases. This is particularly evident when using 100 in-context examples. These results suggest that the LLMs are using the output variables to learn from the examples, and increasing the number of in-context examples shifts the spectrum from knowledge retrieval to learning from outputs. Our findings, which indicate that LLMs can be pushed to prioritize outputs from in-context examples over knowledge retrieval, challenge uselessness of the outputs claimed by (Min et al. 2022; Li et al. 2024). Comparing the Named Features and Anonymized Features prompt configurations shows the power of combining the two paradigms of learning from in-context examples and knowledge retrieval. Anonymized Features prompt configuration, shown with the green color in Figure 4, allows only usage of the numeric part of the features for learning. It still achieves better results than the Direct QA and the Mean model. Named Features prompt configuration, which adds the actual name of the features to the examples, is shown in purple. By encouraging the use of knowledge resulting from the added names of the features, it consistently outperforms Anonymized Features prompt configuration across variations of the factors (number of in-context examples and number of features). These results show that these models can exploit both in-context examples\u2019 outputs for learning and clues like feature names for knowledge retrieval (In any form, such as task selection for a particular regression setting.). Note that when we replaced the feature names with random, unrelated names with the same ranges as the original features (e.g., replacing Smoker Status with Married since both are binary), the results remained the same as those of the Anonymized Features prompt configuration.\n# Knowledge Retrieval Compensates ICL Examples\nAs previously noted, the Named Feature prompt configuration, which combines knowledge and learning, generally outperforms the Anonymized Feature prompt configuration, which relies only on learning. Regarding these prompt configurations, analyzing our defined factors, such as the number of in-context examples, reveals more insights about ICL. Conventionally, in the realm of LLMs, more in-context examples tend to improve the task outcome. However, as demonstrated in Figure 4, performance deteriorates with more in-context examples when outputs are random, even if the model possesses knowledge about the subject. Figure 5, which compares the performance of different numbers of in-context examples, reveals that the Named Feature prompt configuration performs better when fewer in-context examples are used (solid vs dashed lines). A significant performance gap between the two prompt configurations is observed at 10 in-context examples, shown with the solid and dashed purple lines. However, at 100 in-context examples, shown with the solid and dashed lime lines, the performance levels converge. This underscores the potential for reducing the number of required in-context examples by providing task-specific information, thereby shifting the spectrum from learning to knowledge retrieval utilization. Moreover, Figure 6 shows the advantage of the Named Features prompt configuration over both the Anonymized Features prompt configuration and also the traditional machine learning models by comparing them across various numbers of in-context examples (In the diagram, the number of features is fixed at 3). With fewer in-context examples (30 and especially 10), Named Features prompt configuration models, shown with solid lines, generally outperform all other models. Their results indicate that the type of information in the LLMs\u2019 prompt can encourage exploiting their internal knowledge. Moreover, the LLMs\u2019 capability to retrieve knowledge makes them more data-efficient (i.e.,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae28/ae2807a2-c670-4672-82d2-485b5dc3e23f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Comparison of the number of in-context examples using Named Features (straight lines) and Anonymized Feature dashed lines) prompt configurations. ICi indicates i in-context examples. F1, F2 and F3 represents using 1st (F1), then 1st and 2nd (F2), and all three feature (F3).The MSE scale of each dataset is shown at top left corner.</div>\nfew-shot learners) than even classical machine learning models, shown with grey (RandomForest) and black (Ridge) dashed lines. All the models eventually converge at 100 examples when sufficient data is provided. Providing beyond 100 in-context examples yielded a similar performance, suggesting that the benefits of additional data become marginal.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b62/1b6265fe-43a1-4ded-8446-dc92a719fd33.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Performance of Named Features and Anonymized Features prompt configurations, Ridge, and RandomForest for 3 features based on the number of in-context examples.</div>\nFigure 6: Performance of Named Features and Anonymized Features prompt configurations, Ridge, and RandomForest for 3 features based on the number of in-context examples.\n# More Features Encourages Knowledge Retrieval\nUnlike the number of in-context examples, the number of features stands out as a unique factor that can improve both learning and knowledge retrieval in LLMs. As additional features are incorporated into the context, LLMs can better learn the relationships between features and outputs and also retrieve more relevant knowledge. This clearly helps the Named Features prompt configuration. However, the less obvious insights in this section emerge from the Anonymized Features prompt configuration, which relies only on learning and consistently improves with the addition of features. As shown in Figure 7 for 100 in-context examples (the dotted lines), each additional feature either\nimproves the results or maintains near-optimal performance (a similar, but less robust, trend is seen in the 30 in-context examples\u2019 diagram in Appendix E.). While adding more features improves results, this improvement is more significant in language models compared to machine learning models such as Ridge and RandomForest, black and grey dashed lines in Figure 7. Given that in this prompt configuration 1) feature names are anonymous, 2) only raw numerical data is available for output estimation, and 3) the improvement in the performance of LLMs does not correlate with the importance of the features, we conjecture that the results could be influenced by data contamination from the observed numbers. This assertion is supported by the fact that this phenomenon is absent in the Admission Chance dataset, the least likely seen dataset by the language models. In the Named Feature prompt configuration for 100 incontext examples depicted in Figure 7 (the solid lines), the results show an overall improvement, yet the trend is not as consistently downward or smooth as observed in the Anonymized Features prompt configuration when features are added. This fluctuation suggests that the knowledge retrieval aspect becomes more prominent and occasionally supersedes the learning component as the number of features increases. Based on these observations (that happen at a high number of 100 in-context examples), we hypothesize that adding features primarily enhances the knowledge retrieval aspect rather than contributing to the learning.\n# Discussion\nOur evaluation framework and extensive experiments provide insights into the ICL mechanisms of LLMs. In this section, we discuss the implications of our findings, address limitations, and suggest directions for future research.\n# Key Findings and Implications\nComprehending and Controlling ICL Mechanisms Understanding and manipulating ICL mechanisms in LLMs proves invaluable for practical applications and effective prompt engineering. Our experiments show that the LLMs use both learning from the in-context input-output exam-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/007d/007d91d6-aa1c-4a61-91b9-7a38cdb61101.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Performance of Anonymized Features prompt configuration, Ridge, and RandomForest for 100 in-context examples based on the number of features.</div>\nples and prior information with varying extents. We found that increasing the number of in-context examples encourages learning while adding more features mainly boosts knowledge retrieval. However, these benefits only materialize when there is room for improvement in the respective mechanism. For example, using more than 100 in-context examples did not help our models, and in the Admission Chance dataset, where the features were highly correlated, the addition of features did not help the performance either. This insight explains why (Min et al. 2022)\u2019s tasks appeared not to utilize output labels for learning; In their tasks, the dominant knowledge retrieval aspect likely eclipsed any learning benefits. Our findings also challenge the view of LLMs as merely meta-learners. While previous research demonstrates decoder models\u2019 capacity for meta-learning, the complex interplay of different training objectives (unknown in some proprietary models) and other factors introduce a significant knowledge retrieval component. Consequently, earlier findings can only be directly extrapolated to LLMs when considering all these dynamics. Practical Applications Our findings emphasize the importance of striking an optimal balance between the number of meaningful features and the number of in-context examples to optimize LLM performance. By strategically reducing the quantity of in-context examples while increasing the number of named features, it is possible to achieve resource efficiency and other potential benefits, such as mitigating data biases, without compromising performance. Conversely, when tackling tasks unfamiliar to the model, pruning less important features can free up space in the LLM\u2019s token context, allowing it to accommodate more incontext examples. This shifts the ICL mechanism towards learning from the examples\u2019 side of the spectrum and improves the model\u2019s performance by that. Data Contamination is Hard to Disentangle In our experiments, we showed that LLMs can perform regression on\n# Data Contamination is Hard to Disentangle In our ex\nperiments, we showed that LLMs can perform regression on realistic datasets, even when feature names are anonymized. However, as detailed in the experiments section and illus-\ntrated in Figure 7, the improvement does not correlate with the actual importance of input features in the dataset. This discrepancy strongly suggests that data contamination occurs even at the level of numerical values. These findings challenge the claims made by (Vacareanu et al. 2024), which uses a prompt configuration similar to Anonymized Features to avoid data contamination. Their study utilizes either wellknown Friedman formulas or formulas that can be closely approximated (using two decimal places) by common mathematical expressions6.\n# Limitations of our Framework and Future Work\nScope of Study Our framework focuses on regression tasks in line with most previous meta-learning research. We also utilize three diverse regression datasets. More regression (or classification) datasets with different characteristics, such as different relationships between their features, can be used for future investigation, which could elaborate more on the findings of our work with the previous work that dismissed the importance of outputs for learning.\nInterpretability We interact with LLMs as a black box and the underlying neural mechanisms remain opaque in our experiments. Integrating this approach with interpretability techniques could yield a deeper understanding of ICL.\nExperimental Constraints The token limit of some LLMs prevented us from testing beyond 100 in-context examples and three features in certain combinations. Although we found that results with 200 in-context examples closely mirrored those with 100, including a 4th feature could have further reinforced our findings. When tested, the results of the 4th feature aligned with our conclusions. However, we ultimately restricted our analysis to three features to maintain a comprehensive combination across all models.\nData Contamination Challenge Addressing the challenge of data contamination remains a complex issue (Sainz et al. 2023; Balloccu et al. 2024). As mentioned earlier, it is hard to distinguish the degree of Data Contamination from approximate Knowledge Retrieval. Based on our findings, data contamination can happen even with the sequence of numerical values without any linguistic clues. To mitigate this issue, we suggest using distributions that are neither widely recognized nor easily approximated by common mathematical expressions. Furthermore, for datasets, it is advisable to utilize information sourced from regions beyond the United States or outside the primary training domain of the language model. This approach helps to minimize the potential influence of knowledge retrieval in generating the results.\n# Conclusion\nIn this study, we have explored the ICL capabilities of LLMs through an empirical study on regression tasks. Our results demonstrate that LLMs utilize a blend of retrieved internal\n6E.g. y = 10x + sin(5\u03c0x) + cos(6\u03c0x) simplified as y = 10\n6E.g. y = 10x + sin(5\u03c0x) + cos(6\u03c0x) simplified as y = 10x.\nknowledge and learned information from in-context examples. Our findings extend the evaluations of prior hypotheses on ICL. For example, we evaluate the usage of outputs in in-context learning examples and show how to manipulate their effectiveness. This work not only advances our understanding of LLMs\u2019 in-context learning phenomenon but also offers practical insights for optimizing their application through careful prompt engineering.\nderstanding of LLMs\u2019 in-context learning phenomenon but also offers practical insights for optimizing their application through careful prompt engineering. References Acharya, M. S.; Armaan, A.; and Antony, A. S. 2019. A Comparison of Regression Models for Prediction of Graduate Admissions. In 2019 International Conference on Computational Intelligence in Data Science (ICCIDS), 1\u20135. AI@Meta. 2024. LLaMA 3 Model Card. Anthropic. 2024. Claude 3. https://www.anthropic.com. Large language model. Bai, Y.; Chen, F.; Wang, H.; Xiong, C.; and Mei, S. 2023. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. arXiv:2306.04637. Balloccu, S.; Schmidtov\u00b4a, P.; Lango, M.; and Du\u02c7sek, O. 2024. Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs. arXiv:2402.03927. Breiman, L. 2001. Random Forests. Machine Learning, 45(1): 5\u201332. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165. Cheng, X.; Chen, Y.; and Sra, S. 2024. Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context. arXiv:2312.06528. Coda-Forno, J.; Binz, M.; Akata, Z.; Botvinick, M.; Wang, J.; and Schulz, E. 2023. Meta-in-context learning in large language models. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 65189\u2013 65201. Curran Associates, Inc. Deutch, G.; Magar, N.; Natan, T. B.; and Dar, G. 2024. In-context Learning and Gradient Descent Revisited. arXiv:2311.07772. Fang, X.; Xu, W.; Tan, F. A.; Zhang, J.; Hu, Z.; Qi, Y. J.; Nickleach, S.; Socolinsky, D.; Srinivasan Sengamedu, S.; and Faloutsos, C. 2024. Large language models (LLMs) on tabular data: Prediction, generation, and understanding - a survey. Transactions on Machine Learning Research. Garg, S.; Tsipras, D.; Liang, P.; and Valiant, G. 2022. What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. ArXiv, abs/2208.01066. Hahn, M.; and Goyal, N. 2023. A Theory of Emergent In-Context Learning as Implicit Structure Induction. arXiv:2303.07971.\nAcharya, M. S.; Armaan, A.; and Antony, A. S. 2019. A Comparison of Regression Models for Prediction of Graduate Admissions. In 2019 International Conference on Computational Intelligence in Data Science (ICCIDS), 1\u20135. AI@Meta. 2024. LLaMA 3 Model Card. Anthropic. 2024. Claude 3. https://www.anthropic.com. Large language model. Bai, Y.; Chen, F.; Wang, H.; Xiong, C.; and Mei, S. 2023. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. arXiv:2306.04637. Balloccu, S.; Schmidtov\u00b4a, P.; Lango, M.; and Du\u02c7sek, O. 2024. Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs. arXiv:2402.03927. Breiman, L. 2001. Random Forests. Machine Learning, 45(1): 5\u201332. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165. Cheng, X.; Chen, Y.; and Sra, S. 2024. Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context. arXiv:2312.06528. Coda-Forno, J.; Binz, M.; Akata, Z.; Botvinick, M.; Wang, J.; and Schulz, E. 2023. Meta-in-context learning in large language models. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 65189\u2013 65201. Curran Associates, Inc. Deutch, G.; Magar, N.; Natan, T. B.; and Dar, G. 2024. In-context Learning and Gradient Descent Revisited. arXiv:2311.07772. Fang, X.; Xu, W.; Tan, F. A.; Zhang, J.; Hu, Z.; Qi, Y. J.; Nickleach, S.; Socolinsky, D.; Srinivasan Sengamedu, S.; and Faloutsos, C. 2024. Large language models (LLMs) on tabular data: Prediction, generation, and understanding - a survey. Transactions on Machine Learning Research. Garg, S.; Tsipras, D.; Liang, P.; and Valiant, G. 2022. What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. ArXiv, abs/2208.01066. Hahn, M.; and Goyal, N. 2023. A Theory of Emergent In-Context Learning as Implicit Structure Induction. arXiv:2303.07971.\nHoerl, A. E.; and Kennard, R. W. 1970. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1): 55\u201367. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023. Mistral 7B. arXiv:2310.06825. Kossen, J.; Gal, Y.; and Rainforth, T. 2024. In-Context Learning Learns Label Relationships but Is Not Conventional Learning. arXiv:2307.12375. Lantz, B. 2013. Machine Learning with R. Packt Publishing Ltd. ISBN 9781782162148. Li, J.; Hou, Y.; Sachan, M.; and Cotterell, R. 2024. What Do Language Models Learn in Context? The Structured Task Hypothesis. arXiv:2406.04216. Li, Y.; Sreenivasan, K.; Giannou, A.; Papailiopoulos, D.; and Oymak, S. 2023. Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. arXiv:2305.18869. Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2022. What Makes Good In-Context Examples for GPT3? In Agirre, E.; Apidianaki, M.; and Vuli\u00b4c, I., eds., Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 100\u2013114. Dublin, Ireland and Online: Association for Computational Linguistics. Louppe, G.; Wehenkel, L.; Sutera, A.; and Geurts, P. 2013. Understanding variable importances in forests of randomized trees. Advances in neural information processing systems, 26. Machlab, D.; and Battle, R. 2024. LLM In-Context Recall is Prompt Dependent. arXiv:2404.08865. Min, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.; Hajishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? ArXiv, abs/2202.12837. Mital, A. 2023. US Used Cars Dataset. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Pan, J.; Gao, T.; Chen, H.; and Chen, D. 2023. What InContext Learning \u201cLearns\u201d In-Context: Disentangling Task Recognition and Task Learning. In Rogers, A.; BoydGraber, J.; and Okazaki, N., eds., Findings of the Association for Computational Linguistics: ACL 2023, 8298\u20138319. Toronto, Canada: Association for Computational Linguistics. Pearson, K. 1895. VII. Note on regression and inheritance in the case of two parents. proceedings of the royal society of London, 58(347-352): 240\u2013242. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cournapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay, E. 2011. Scikitlearn: Machine Learning in Python. Journal of Machine Learning Research, 12: 2825\u20132830.\nHoerl, A. E.; and Kennard, R. W. 1970. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1): 55\u201367. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023. Mistral 7B. arXiv:2310.06825. Kossen, J.; Gal, Y.; and Rainforth, T. 2024. In-Context Learning Learns Label Relationships but Is Not Conventional Learning. arXiv:2307.12375. Lantz, B. 2013. Machine Learning with R. Packt Publishing Ltd. ISBN 9781782162148. Li, J.; Hou, Y.; Sachan, M.; and Cotterell, R. 2024. What Do Language Models Learn in Context? The Structured Task Hypothesis. arXiv:2406.04216. Li, Y.; Sreenivasan, K.; Giannou, A.; Papailiopoulos, D.; and Oymak, S. 2023. Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. arXiv:2305.18869. Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2022. What Makes Good In-Context Examples for GPT3? In Agirre, E.; Apidianaki, M.; and Vuli\u00b4c, I., eds., Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 100\u2013114. Dublin, Ireland and Online: Association for Computational Linguistics. Louppe, G.; Wehenkel, L.; Sutera, A.; and Geurts, P. 2013. Understanding variable importances in forests of randomized trees. Advances in neural information processing systems, 26. Machlab, D.; and Battle, R. 2024. LLM In-Context Recall is Prompt Dependent. arXiv:2404.08865. Min, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.; Hajishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? ArXiv, abs/2202.12837. Mital, A. 2023. US Used Cars Dataset. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Pan, J.; Gao, T.; Chen, H.; and Chen, D. 2023. What InContext Learning \u201cLearns\u201d In-Context: Disentangling Task Recognition and Task Learning. In Rogers, A.; BoydGraber, J.; and Okazaki, N., eds., Findings of the Association for Computational Linguistics: ACL 2023, 8298\u20138319. Toronto, Canada: Association for Computational Linguistics. Pearson, K. 1895. VII. Note on regression and inheritance in the case of two parents. proceedings of the royal society of London, 58(347-352): 240\u2013242. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cournapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay, E. 2011. Scikitlearn: Machine Learning in Python. Journal of Machine Learning Research, 12: 2825\u20132830.\nRazeghi, Y.; Logan IV, R. L.; Gardner, M.; and Singh, S. 2022. Impact of Pretraining Term Frequencies on FewShot Numerical Reasoning. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Findings of the Association for Computational Linguistics: EMNLP 2022, 840\u2013854. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Sainz, O.; Campos, J. A.; Garc\u00b4\u0131a-Ferrero, I.; Etxaniz, J.; de Lacalle, O. L.; and Agirre, E. 2023. NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. arXiv:2310.18018. Schmidhuber, J. 1987. Evolutionary principles in selfreferential learning. On learning how to learn: The metameta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1(2): 48. Shen, L.; Mishra, A.; and Khashabi, D. 2024. Do pretrained Transformers Learn In-Context by Gradient Descent? arXiv:2310.08540. Team, G. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. LLaMA 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288. Vacareanu, R.; Negru, V.-A.; Suciu, V.; and Surdeanu, M. 2024. From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given InContext Examples. arXiv:2404.07544. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. von Oswald, J.; Niklasson, E.; Schlegel, M.; Kobayashi, S.; Zucchet, N.; Scherrer, N.; Miller, N.; Sandler, M.; y Arcas, B. A.; Vladymyrov, M.; Pascanu, R.; and Sacramento, J. 2023. Uncovering mesa-optimization algorithms in Transformers. arXiv:2309.05858.\nWang, X.; Zhu, W.; Saxon, M.; Steyvers, M.; and Wang, W. Y. 2024. Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning. arXiv:2301.11916. Wies, N.; Levine, Y.; and Shashua, A. 2023. The Learnability of In-Context Learning. arXiv:2303.07895. Zhu, W.; Lv, Y.; Dong, Q.; Yuan, F.; Xu, J.; Huang, S.; Kong, L.; Chen, J.; and Li, L. 2023. Extrapolating Large Language Models to Non-English by Aligning Languages. arXiv:2308.04948. Zuhashaik; et al. 2023. LLaMA-2-70B Model: Challenges with Long Token Sequences. https://github.com/metaLLaMA/LLaMA/issues/887. GitHub Issue.\n# Appendix A: Related Work\nAs mentioned in the introduction, the research in this field can be categorized into meta-learning (Schmidhuber 1987) and knowledge retrieval. So far, most works are theoretical, and most can be categorized into one of these two groups. However, both of these aspects are changing as this field rapidly expands; New empirical research is coming along, and new ideas are emerging, such as ours, that can not easily fit into one of these groups. For example, (Li et al. 2023) proposes a structure that improves a basic transformer\u2019s incontext learning capabilities by breaking the input into multiple steps. The transformer still uses the outputs but also the knowledge that is given to it in its input. Another example would be (Kossen, Gal, and Rainforth 2024), which empirically tests LLMs and claims that they can learn from the output labels but in an arbitrary manner, a challenge our paper aims to address. Another interesting example that we cannot simply put into a group is (Coda-Forno et al. 2023), which performs meta-in-context learning by showing similar entire tasks and their solutions in the context of the model. Despite these emerging works, it\u2019s worth noting that most papers can still be classified into our established groups. When evaluating the literature, it\u2019s crucial to distinguish between theoretical claims and those based on empirical or synthetic data. Some theoretical papers suggest or imply that their findings might extend to LLMs, but such extrapolations should be approached with caution, as highlighted by (Deutch et al. 2024; Shen, Mishra, and Khashabi 2024). In the meta-learning group, (Bai et al. 2023) theoretically prove that transformers can be trained to implement linear models within a specific error threshold. Then, they practically test the capability of an encoder-based transformer to implement generalized linear models in context and show that they are also robust against noisy data. In order to test the ICL capability of transformers, (Garg et al. 2022) use a GPT-2 architecture which is closer to an LLM than a simple transformer. However, they modify this GPT-2 architecture for regression by inputting the input features inside the embeddings instead of using tokens and outputting a number at the end instead of generating tokens. Their model\u2019s performance surpasses linear models as well as KNN models, XGBoost, and a 2-layer neural network. Different from these, (von Oswald et al. 2023) hypothesize that the strong performance of Transformers comes from an architectural\nbias towards mesa-optimization which they test on simple sequencing tasks, and (Cheng, Chen, and Sra 2024) suggest Transformers can implement gradient descent in function space, enabling them to learn linear and non-linear models. In the realm of empirical research, our focus is exclusively on studies examining ICL, distinct from related works involving numerical concepts such as numerical reasoning (Razeghi et al. 2022). (Vacareanu et al. 2024) uses a lot of different LLMs such as Gemini-pro (Team 2024), Claude 3 (Anthropic 2024), GPT-4 (OpenAI 2023) to test their ICL regression capabilities. They use various settings with up to three features, but not all features are relevant to the output as limited as a result. This work conducts rigorous testing with many models, and the only issue it has is regarding its claim of avoiding data contamination which is not wellsupported. They either use well-known Friedman formulas or formulas that can be closely approximated (using two decimal places) by common mathematical expressions such as y = 10x + sin(5\u03c0x) + cos(6\u03c0x) simplified as y = 10x. Also, the data is based on formulas, which are not realistic regression datasets. The first paper that argues that output labels do not matter was (Min et al. 2022). It tested various LLMs across multiple tasks and concluded that replacing the outputs with random labels does not affect the results, and the only important elements of ICL are limited to defining the label space, input distribution, and overall task format. Following this work, (Pan et al. 2023) uses the same sentiment analysis, toxicity detection, natural language inference/paraphrase detection, and topic/stance classification tasks but argues that learning and retrieval occur under different circumstances. In contrast to our research, their approach employs less complex tasks (same tasks used in (Min et al. 2022)), which they acknowledge could influence the observed patterns. Additionally, they utilize more basic models, including earlier and smaller iterations of GPT-3 such as ada. As a result, their findings yield distinct trends and visual representations compared to our work. Inspired by (Hahn and Goyal 2023), (Li et al. 2024) dismisses the significance of output labels and proposes that learned solutions can be combined for inference. The main limitation of this work is the exclusive use of LLaMA 2 (Touvron et al. 2023b). Our experimental results differ from (Min et al. 2022; Pan et al. 2023; Li et al. 2024), which requires a unified platform for direct comparison, which is practically infeasible. We argue this conflict is mainly due to the task and model selection. We will explain further on (Li et al. 2024) in Appendix B.\n# Appendix B: Learning from Outputs\nIn this section, we examine the paper (Li et al. 2024) and why its findings differ from ours. As mentioned earlier, the main problem with this work lies in its exclusive use of LLaMA 2 (Touvron et al. 2023b), a model known to struggle with long token contexts (Machlab and Battle 2024; Zuhashaik et al. 2023) which negatively impacts the ICL. While the choice of LLaMA 2 as a powerful open-source LLM at the time is understandable, LLaMA 3 (AI@Meta 2024), which addressed many of LLaMA 2\u2019s issues and offered significantly improved capabilities, was already avail-\nable at the time of publication. Some of their experiments require direct access to the LLM architecture, which necessitates an open-source LLM. However, many of their experiences can be done with the GPT family or other powerful models. Based on the LLM choice alone, this work is severely limited. Further limiting the study, the authors selected three simple review sentiment/news-type datasets that even basic machine learning models can excel at. Regarding specific experiments, experiment 4.1 demonstrates that mapping each label to another fixed label yields results worse than a random baseline, even with 40 incontext examples. This finding contradicts our work and other related studies, which have shown that models can learn from outputs. We suspect this discrepancy may be attributed to LLaMA 2\u2019s well-documented issue of forgetting tasks as the number of tokens increases. Confusion ensues because the outputs likely appear nonsensical to LLaMA 2, and it has forgotten its instructions. It would have been beneficial for the authors to specify whether LLaMA 2 was outputting incorrect labels or entirely irrelevant answers. Experiments 4.2 and 5.1 also raise questions, as they rely on token embeddings, which, without further evidence, appear unrelated to ICL. For example, (Li et al. 2023) suggests that the level-by-level inference in transformers is necessary for ICL. The combination of embeddings and other factors, such as training the linear model for 80 epochs to match LLaMA 2\u2019s 80 layers, seems arbitrary and lacks clear justification in these experiments. In conclusion, while the paper (Li et al. 2024) offers interesting insights specifically regarding its categorization of the three hypotheses, its methodological choices and reliance on LLaMA 2 significantly limit its relevance to the broader discussions of in-context learning in language models.\n# Appendix C: Hyper-Parameters\nAs mentioned earlier, we evaluate several LLMs, which are LLaMA 3 70B (AI@Meta 2024), GPT3.5 (Brown et al. 2020), and GPT4 (OpenAI 2023). We also used Ridge regression (Hoerl and Kennard 1970) and RandomForest (Breiman 2001). The GPT-3 and GPT-4 versions we use are \u201dgpt-4-0125-preview\u201c, \u201dgpt-3.5-turbo-0125\u201c. In these models, the temperature is set to 0.1, and make tokens is set to 10. Each time the models do not generate a number (i.e., generate a disclaimer that says \u201cthe data is insufficient\u201d), the seed value, which is initialized to 100, is increased by one. This precaution was not necessary for GPT-4, but GPT-3.5 sometimes refused to answer the questions. For LLaMA 3, top p value is set to 0.99, max tokens to 6, and the temperature to 0.1. Similar to GPT models, we propmt LLaMA 3 again if it fails to generate a number. For Ridge, the default Python class of scikit(Pedregosa et al. 2011) has the alpha value ( the normalizing variable) of 1. For RandomForest, however, we wanted to make it more generalized. Consequently, we used 10, 000 estimators and a max depth of 2 for it.\n# Appendix D: MAE and R2\nIn this section, we depict the main diagrams in the paper with MAE and R2 metrics instead of MSE. MAE is less sensitive to outliers compared to MSE and is expressed in the same units as the original data. R2, also known as the coefficient of determination, ranges from 0 to 1 and represents the proportion of variance in the dependent variable that is predictable from the independent variable(s). An R2 value of 1 indicates that the model explains all the variability of the response data around its mean, while 0 means the model explains none of the variability. It can also be negative, indicating that the model fits the data worse than the Mean model. R2 differs from MSE and MAE because the higher, the better. As a result, to make R2 diagrams comparable, instead of R2, we show the results for 1 \u2212R2 (1 is included to show the negative results, which are worse than the Mean model). This also moves the baseline results of R2 to 1 for the Mean model. For Figures 3, 4, 5, 6 and 7, the MAE results are shown in Figures 8,10,12,14 and 16, respectively. For Figures 3, 4, 5, 6 and 7, the 1 \u2212R2 results are shown in Figures 9,11,13,15 and 17, respectively.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4492/44928a5f-90c2-4d28-ad7f-151b31a97cc7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 8: This figure shows Figure 3 with MAE instead of MSE. The figure shows the aseline results of LLMs\u2019 (Direct QA configuration) across datasets and number of features. The dashed red line shows the performance of the Mean model.\n# Appendix E: Reasoning Configuration and Additional Diagrams\nIn this section, we include some diagrams that could have provided more insight and were removed from the main text for space. We also show examples of the reasoning configurations and diagrams related to that.\n# Additional Diagrams of the Main Paper\nHere, we included the variants of Figures 6 and 7 that we omitted for space. Figure 6 with 1 and 2 features is shown in Figures 18 and 19. Figure 7, with 10 and 30 in-context examples, is shown in Figures 20 and 21.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8818/8818f83e-fed8-4d5a-a1af-c01927df9e89.png\" style=\"width: 50%;\"></div>\nFigure 9: This figure shows Figure 3 with R2 instead of MSE. The figure shows the baseline results of LLMs\u2019 (Direct QA configuration) across datasets and number of features. The dashed red line shows the performance of the Mean model.\n# Reasoning Examples\nIn this section, we show an examples of reasoning to estimate a the output for each dataset and LLM with 3 features. We will show the results of this setting in the next section.\nGPT-3 Reasoning over the Admission Chance Dataset: Based on the given information, the applicant has a high Cumulative GPA of 8.48, a decent GRE score of 311.0, and a good TOEFL score of 104.0. These scores are above average and indicate a strong academic background. Considering that the average Chance of Admission is 0.74 with a standard deviation of 0.14, and the applicant\u2019s strong academic profile, I would estimate that the Chance of Admission for this applicant is higher than the average. My final estimation is 0.80.\nLLaMA Reasoning over the Admission Chance Dataset: Based on the provided information, I will estimate the Chance of Admission. The given scores are all above average, indicating a strong applicant. The cumulative GPA of 8.48 is excellent, suggesting a high level of academic achievement. The GRE score of 311.0 is also above average, indicating strong verbal and quantitative skills. The TOEFL score of 104.0 is well above the average required for most graduate programs, demonstrating a high level of English proficiency. Considering these strong scores, I would expect the Chance of Admission to be higher than the average of 0.74. Given the standard deviation of 0.14, I would estimate the Chance of Admission to be around one standard deviation above the mean. My final estimation is 0.88.\nGPT-4 Reasoning over the Admission Chance Dataset: To estimate the Chance of Admission based on the provided information, we need to consider how the given data points (Cumulative GPA, GRE Score, TOEFL Score) might relate to the average Chance of Admission and its standard deviation. However, without specific information on how these\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df36/df3679d2-ce61-493b-a1fa-0cf40fde2232.png\" style=\"width: 50%;\"></div>\nFigure 10: This figure shows Figure 4 with MAE instead of MSE. The figure shows the comprehensive comparison of promp onfigurations\u2019 effects on our models across various factors, shown in a hierarchy. The top level for each dataset distinguishes between GPT-3, LLaMA 3, and GPT-4 results using black, grey, and white arcs, respectively. The notation ICi indicates the number of in-context examples, while F1, F2, and F3 represent the use of the first feature, the first two features, and all three eatures, respectively.\n<div style=\"text-align: center;\">Figure 10: This figure shows Figure 4 with MAE instead of MSE. The figure shows the comprehensive comparison of prompt configurations\u2019 effects on our models across various factors, shown in a hierarchy. The top level for each dataset distinguishes between GPT-3, LLaMA 3, and GPT-4 results using black, grey, and white arcs, respectively. The notation ICi indicates the number of in-context examples, while F1, F2, and F3 represent the use of the first feature, the first two features, and all three eatures, respectively.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f49b/f49bdbec-1b7c-40fd-8108-8e2fb04adbe1.png\" style=\"width: 50%;\"></div>\nFigure 11: This figure shows Figure 4 with R2 instead of MSE. The figure shows the comprehensive comparison of prompt configurations\u2019 effects on our models across various factors, shown in a hierarchy. The top level for each dataset distinguishes between GPT-3, LLaMA 3, and GPT-4 results using black, grey, and white arcs, respectively. The notation ICi indicates the number of in-context examples, while F1, F2, and F3 represent the use of the first feature, the first two features, and all three features, respectively.\nfactors correlate with admission chances, we\u2019ll have to make some general assumptions based on typical admissions criteria. 1. **Cumulative GPA**: An 8.48 GPA, assuming it\u2019s on a 10-point scale, is quite high. This would generally be seen as a strong positive in the admissions process, suggesting a higher than average chance of admission. 2. **GRE Score**: A score of 311 is somewhat above average (the average GRE scores for many programs tend to hover around the low to mid-300s). While not outstanding, it\u2019s competitive enough not to detract significantly from an application, especially with a strong GPA.\n3. **TOEFL Score**: A TOEFL score of 104 is good, indicating a strong proficiency in English. For most universities, this score is above the minimum requirement and should be sufficient to not negatively impact the chance of admission. Given the average Chance of Admission is 0.74 with a standard deviation of 0.14, and considering the strong GPA, adequate GRE, and TOEFL scores, it\u2019s reasonable to estimate the Chance of Admission to be above average but not exceptionally so due to the GRE score not being far above average. Taking into account the positive impact of a high GPA\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fef8/fef844ae-fcd2-441f-bd1e-8e714001a19c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: This figure shows Figure 5 with MAE instead of MSE. The figure shows the comparison of the number of in-contex xamples using Named Features (straight lines) and Anonymized Features (dashed lines) prompt configurations. ICi indicate  in-context examples. F1, F2 and F3 represents using 1st (F1), then 1st and 2nd (F2), and all three feature (F3).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae95/ae954c28-d7a9-446a-bf64-a76e10493661.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: This figure shows Figure 5 with R2 instead of MSE. The figure shows the comparison of the number of in-contex examples using Named Features (straight lines) and Anonymized Features (dashed lines) prompt configurations. ICi indicate i in-context examples. F1, F2 and F3 represents using 1st (F1), then 1st and 2nd (F2), and all three feature (F3).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/255b/255be192-4d40-4125-b6f1-01538f2ee639.png\" style=\"width: 50%;\"></div>\nFigure 14: This figure shows Figure 6 with MSE instead of MSE. The figure shows the performance of Named Features and Anonymized Features prompt configurations, Ridge, and RandomForest for 3 features based on the number of in-context examples.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/548c/548ce9f5-ad28-49e8-b668-3ec3c48354da.png\" style=\"width: 50%;\"></div>\nFigure 15: This figure shows Figure 6 with R2 instead of MSE. The figure shows the performance of Named Features and Anonymized Features prompt configurations, Ridge, and RandomForest for 3 features based on the number of in-context examples.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8df4/8df492c1-6be6-4bc9-b296-ae5e08fe8c46.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 16: This figure shows Figure 7 with MAE instead of MSE. The figure shows the performance of Anonymized Features prompt configuration, Ridge, and RandomForest for 100 in-context examples based on the number of features.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb58/eb581f33-6dd3-4b0e-8656-5babb62312e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 17: This figure shows Figure 7 with R2 instead of MSE. The figure shows the performance of Anonymized Features prompt configuration, Ridge, and RandomForest for 100 in-context examples based on the number of features.\nand good TOEFL score, balanced against the GRE score, a reasonable estimate might place the Chance of Admission slightly above the average but not by a large margin. Therefore, considering the given data and the average Chance of Admission, my final estimation is 0.78.\u201d GPT-3 Reasoning over the Insurance Cost Dataset: Based on the given information of being a smoker, having a BMI of 24.7, and being 43 years old, we can expect higher annual medical costs compared to non-smokers, individuals with lower BMIs, and younger individuals. Smokers tend to have higher medical costs due to the increased risk of various health conditions such as lung cancer, heart disease, and respiratory issues. Additionally, as individuals age, they are more likely to develop chronic health conditions that require ongoing medical care, which can also contribute to higher\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ab2/1ab2db22-eace-4e97-8110-ecbba226fbff.png\" style=\"width: 50%;\"></div>\nFigure 18: This figure shows Figure 6 of the main paper with 1 features instead of 3. The figure shows the performance of Named Features and Anonymized Features prompt configurations, Ridge, and RandomForest based on the number of in-context examples.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/495c/495c1195-623a-461e-8483-87acc5984fff.png\" style=\"width: 50%;\"></div>\nFigure 19: This figure shows Figure 6 of the main paper with 2 features instead of 3. The figure shows the performance of Named Features and Anonymized Features prompt configurations, Ridge, and RandomForest based on the number of in-context examples.\nmedical costs. A BMI of 24.7 is considered within the normal range, which may have a slightly lower impact on medical costs compared to individuals with higher BMIs. Taking into account these factors, along with the average annual individual medical costs billed by health insurance in the USA of $13,270.42 and a standard deviation of $12,110.01, I estimate that the annual individual medical costs for a 43-year-old smoker with a BMI of 24.7 in the USA would be higher than the average due to the increased health risks associated with smoking and age. Therefore, my final estimation is $20,000.\u201d \u201dBased on the provided information of being a non-smoker with a BMI of 25.8 and an age of 46, we can estimate that the individual is relatively healthy. Typically, younger individuals with a healthy BMI and who do not smoke tend to have lower medical\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e876/e87669f3-1aa1-430d-bb10-d6de247d0a46.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 20: This figure shows Figure 7 of the main paper with 10 in-context examples instead of 100. The figure shows the performance of Anonymized Features prompt configuration, Ridge, and RandomForest for based on the number of features.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7cf8/7cf81a8a-969a-4cf8-ac36-7e6eb1ff6d87.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 21: This figure shows Figure 7 of the main paper with 30 in-context examples instead of 100. The figure shows the performance of Anonymized Features prompt configuration, Ridge, and RandomForest for based on the number of features.\ncosts compared to older individuals or those with unhealthy habits. Considering the average annual individual medical costs billed by health insurance in the USA of $13,270.42 with a standard deviation of $12,110.01, we can assume that this individual\u2019s medical costs might be lower than the average due to their healthy profile. However, since medical costs can vary significantly based on individual health conditions and unforeseen circumstances, it is challenging to provide an accurate estimate without more specific information. Taking into account the individual\u2019s healthy profile and the average costs in the USA, my final estimation is $10,000.\u201d LLaMA Reasoning over the Insurance Cost Dataset: Based on the provided information, we can make an ed-\nucated estimation of the annual individual medical costs billed by health insurance in the USA. We are given the average annual individual medical costs billed by health insurance in the USA, which is $13,270.42, along with the standard deviation of $12,110.01. This suggests that the distribution of medical costs is likely to be skewed to the right, with a long tail of high-cost individuals. The additional information provided about the individual, such as being a smoker (smoker: 1.0), having a BMI of 24.7, and being 43 years old, can also influence the estimation. Smokers tend to have higher medical costs due to the increased risk of smoking-related illnesses. A BMI of 24.7 is considered overweight, which can also lead to higher medical costs. The age of 43 is relatively middle-aged, which may not have a significant impact on medical costs compared to older or younger individuals. Considering these factors, I would adjust the average annual individual medical costs upwards to account for the increased risk associated with smoking and being overweight. However, I would not adjust it too drastically, as the individual\u2019s age is not extremely high. My final estimation is $16,500.\u201d\nGPT-4 Reasoning over the Insurance Cost Dataset: To estimate the annual individual medical costs billed by health insurance in the USA for a 43-year-old smoker with a BMI of 24.7, we need to consider how the given factors (smoking status, BMI, and age) typically influence medical costs, based on the provided average and standard deviation. 1. **Smoking Status**: Being a smoker generally increases medical costs significantly due to the higher risk of diseases associated with smoking (e.g., lung cancer, heart disease). 2. **BMI**: A BMI of 24.7 is within the normal weight range (18.5-24.9), which might not significantly increase medical costs compared to those with higher BMI values. 3. **Age**: At 43 years old, an individual might start to experience more health issues than a younger person, but not as many as someone in a more advanced age group. This could mean slightly higher than average costs due to age but not the highest possible. Given the average annual medical costs of $13,270.42 and considering the significant impact of smoking, the effect of age, and the relatively neutral effect of a normal BMI, the costs for this individual would likely be higher than the average but not excessively so due to the mitigating factor of a healthy BMI. Considering the standard deviation of $12,110.01, which indicates a wide range of costs, and the fact that smoking is a major cost driver, it\u2019s reasonable to estimate that the costs for this individual would be above average but not at the extreme end of the spectrum. Given these considerations, a reasonable estimate for the annual individual medical costs for this person, factoring in the significant increase due to smoking but moderated by age and a healthy BMI, would be somewhat above the average but not by the full extent of one standard deviation. My final estimation is $20,000.\u201d\nGPT-3 Reasoning over the Used Car Prices Dataset: To estimate the price of a used car that can be either a Toyota or\n1. City Fuel Economy: The given city fuel economy of 14.0 suggests that the car may not be very fuel-efficient, which could potentially lower its price. 2. Mileage: The mileage of 0.0 indicates that the car is essentially brand new or has very low mileage, which generally tends to increase the price of a used car. 3. Is Toyota: The fact that the car is not a Toyota implies that it could be a Maserati, which is a luxury brand known for higher price tags compared to Toyota vehicles. Considering these factors, we can infer that the car is likely a Maserati due to its non-Toyota status, which would generally have a higher price tag. Given that the average price of a used car that can be either a Toyota or Maserati in 2019 is $50,014.51 with a standard deviation of $42,279.49, and considering the low mileage and potential luxury status of the car, my final estimation for the price of this used car would be higher than the average, so I estimate it to be $60,000.\u201d LLaMA Reasoning over the Used Car Prices Dataset: Based on the given information, we can see that the average price of a used car that can be either a Toyota or Maserati in 2019 is 50014.51 with a standard deviation of 42279.49. Since the mileage is 0.0, it\u2019s likely that the car is relatively new or has been well-maintained, which could increase its value. The city fuel economy is 14.0, which is a moderate value, but it\u2019s not a key factor in determining the price of a used car. The most important factor is the brand, and since Is Toyota is 0.0, it means the car is a Maserati, which is a luxury brand known for its high-performance and premium vehicles. Maserati cars tend to hold their value well, and their prices are generally higher than those of Toyota cars. Considering these factors, I would estimate the price of this used car to be higher than the average price. My final estimation is 65000.\u201d\nLLaMA Reasoning over the Used Car Prices Dataset: Based on the given information, we can see that the average price of a used car that can be either a Toyota or Maserati in 2019 is 50014.51 with a standard deviation of 42279.49. Since the mileage is 0.0, it\u2019s likely that the car is relatively new or has been well-maintained, which could increase its value. The city fuel economy is 14.0, which is a moderate value, but it\u2019s not a key factor in determining the price of a used car. The most important factor is the brand, and since Is Toyota is 0.0, it means the car is a Maserati, which is a luxury brand known for its high-performance and premium vehicles. Maserati cars tend to hold their value well, and their prices are generally higher than those of Toyota cars. Considering these factors, I would estimate the price of this used car to be higher than the average price. My final estimation is 65000.\u201d\nGPT-4 Reasoning over the Used Car Prices Dataset: Given the information, we have a car with a city fuel economy of 14.0, mileage of 0.0, and it is not a Toyota (implying it could be a Maserati since the options are either Toyota or Maserati). Maseratis are luxury vehicles and typically have a higher price point compared to Toyotas. The average price of a used car from the given data is $50,014.51 with a standard deviation of $42,279.49. Considering the car in question is a Maserati (as indicated by \u201d\u201dIs Toyota: 0.0\u201d\u201d), and it has 0.0 mileage, which suggests it\u2019s either new or nearly new, we can expect the price to be higher than the average of the mixed pool of Toyota and Maserati used cars. The low city fuel economy is characteristic of high-performance or luxury vehicles, which aligns with it being a Maserati. Given these factors, and knowing that Maseratis are premium vehicles, the price would likely be above the average. However, without knowing the exact model or year (beyond it being a used car as of 2019), we can\u2019t pinpoint the exact price but can infer it would be higher due to the brand and the newness indicated by zero mileage.\nConsidering the standard deviation and the average, and the fact that this is a luxury vehicle likely on the higher end of the price spectrum, a reasonable estimation, taking into account the luxury brand premium and the new condition, would be one standard deviation above the mean. Thus, my final estimation is $92,294.\u201d\n# Reasoning Configuration Diagrams\nThe diagrams that compare the Direct QA prompt configuration to the reasoning configuration are shown in Figure22. As mentioned in the text of the main paper, there is no rhyme or reason to the behaviour of the LLMs in this regard.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/daef/daef8940-a076-40c8-860f-cbf335665d5e.png\" style=\"width: 50%;\"></div>\nFigure 22: Comparison between Direct QA and Reasoning prompt configurations between data features.\n<div style=\"text-align: center;\">Figure 22: Comparison between Direct QA and Reasoning prompt configurations between dataset, LLMs and the number o features.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. We show that LLMs can perform regression on real-world datasets and design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples.",
        "problem": {
            "definition": "The problem focuses on leveraging LLMs to predict numerical outputs based on given inputs in regression tasks.",
            "key obstacle": "Existing methods fail to adequately account for the interplay between knowledge retrieval and learning from in-context examples, leading to inconsistent results."
        },
        "idea": {
            "intuition": "The idea arose from observing that ICL behavior in LLMs is not strictly about learning or retrieving knowledge; rather, it exists on a spectrum influenced by various factors.",
            "opinion": "We propose that ICL can be effectively evaluated through a framework that balances learning from examples and knowledge retrieval.",
            "innovation": "This method diverges from existing approaches by integrating both learning and retrieval mechanisms, allowing for a comprehensive understanding of ICL in LLMs."
        },
        "method": {
            "method name": "In-Context Learning Mechanism Evaluation Framework",
            "method abbreviation": "ICL-MEF",
            "method definition": "A framework designed to evaluate the mechanisms of in-context learning in LLMs by analyzing their ability to retrieve knowledge and learn from examples.",
            "method description": "The core of the method involves querying LLMs with regression tasks to assess their performance based on different prompting strategies.",
            "method steps": [
                "Define regression datasets and split them into in-context and test subsets.",
                "Design prompt configurations varying in feature representation and output labeling.",
                "Query LLMs with in-context examples and evaluate their predictions against known outputs."
            ],
            "principle": "The effectiveness of this method lies in its ability to manipulate the balance between knowledge retrieval and learning from examples, which is crucial for optimizing performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using three regression datasets, with LLMs and classical machine learning models compared across various prompt configurations and the number of in-context examples.",
            "evaluation method": "Performance was assessed using Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R2) metrics to evaluate the predictive accuracy of the models."
        },
        "conclusion": "The study demonstrates that LLMs utilize a blend of retrieved internal knowledge and learned information from in-context examples, providing insights on prompt engineering for optimizing their application.",
        "discussion": {
            "advantage": "The proposed approach effectively balances knowledge retrieval and learning, allowing LLMs to achieve better performance in regression tasks compared to traditional methods.",
            "limitation": "Data contamination remains a challenge, as the interplay between retrieval and learning can obscure the true performance of LLMs.",
            "future work": "Future research should explore additional datasets and task types, as well as methods to enhance interpretability and mitigate data contamination."
        },
        "other info": {
            "additional findings": "The framework allows for systematic comparison across different LLMs and datasets, revealing how varying prompt configurations affect performance.",
            "repository": "The framework code and analytical tools are available at https://github.com/HLR/LvsR-LLM."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) in LLMs involves retrieving internal knowledge and learning from in-context examples, particularly in regression tasks."
        },
        {
            "section number": "1.3",
            "key information": "Generative Large Language Models (LLMs) are capable of being in-context learners, with mechanisms that blend knowledge retrieval and learning from examples."
        },
        {
            "section number": "3",
            "key information": "The proposed In-Context Learning Mechanism Evaluation Framework (ICL-MEF) evaluates how LLMs retrieve knowledge and learn from examples through regression tasks."
        },
        {
            "section number": "3.1",
            "key information": "The proposed framework allows LLMs to balance knowledge retrieval and learning, enhancing their adaptability in various contexts."
        },
        {
            "section number": "4",
            "key information": "Insights from the study indicate that effective prompt engineering can optimize LLM performance by manipulating the balance between knowledge retrieval and learning."
        },
        {
            "section number": "6",
            "key information": "Data contamination poses a challenge in evaluating LLM performance, as the interplay between retrieval and learning can obscure true efficacy."
        },
        {
            "section number": "7",
            "key information": "The study concludes that LLMs utilize a blend of retrieved internal knowledge and learned information from in-context examples, highlighting future research directions."
        }
    ],
    "similarity_score": 0.7550089401454234,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Learning vs Retrieval_ The Role of In-Context Examples in Regression with LLMs.json"
}