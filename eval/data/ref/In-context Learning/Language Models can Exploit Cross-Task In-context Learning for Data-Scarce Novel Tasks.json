{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.10548",
    "title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks",
    "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zeroshot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of crosstask examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs\u2019 ability to solve novel tasks based on contextual signals from different task examples.",
    "bib_name": "chatterjee2024languagemodelsexploitcrosstask",
    "md_text": "# Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks\nAnwoy Chatterjee\u2217, Eshaan Tanwar\u2217, Subhabrata Dutta, Tanmoy Chakraborty Indian Institute of Technology Delhi\nanwoy.chatterjee@ee.iitd.ac.in, eshaantanwar2000@gmail.com subha0009@gmail.com, tanchak@iitd.ac.in\n# Abstract\nLarge Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zeroshot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of crosstask examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs\u2019 ability to solve novel tasks based on contextual signals from different task examples.\n# 1 Introduction\nLarge Language Models (LLMs) have revolutionized the state of Natural Language Processing for the past few years. With the ability of In-Context Learning (ICL), one can adopt an LLM to almost any task without costly gradient updates (Brown et al., 2020). At the same time, automated assistants, built on top of foundational LLMs, are\n*Equal contribution named in alphabetical order\nbeing popularized (Pahune and Chandrasekharan, 2023). A crucial challenge with this escalating usage popularity is handling novel tasks. Humongous models like GPT-4 are able to deliver up-to-par performance even in a zero-shot regime (OpenAI, 2023). However, the computational requirements of deploying such large-scale models counteract the practicality of their usage en masse. Relatively smaller LMs, on the other hand, suffer drastically in the absence of in-context examples. The availability of labeled examples usually varies across the use cases of the language model. For example, in an NLP research setup, expert users can quickly come up with a few handwritten examples. However, when we consider mass-scale usage of average users who are not experienced prompt engineers or need quick answers, the zero-shot performance of a model becomes extremely crucial. For example, assuming the popular usage of ChatGPT, very few non-expert users would opt to write down examples while asking ChatGPT to perform some tasks. This naturally raises the question of whether one can make an LLM generalize from labeled examples of a predefined set of tasks to an input defining a novel task. In the world of biological neurons, such abilities are commonplace: inculcating specific limb usage into an untrained limb while training the opposite limb (Ruddy and Carson, 2013), or relatively easier adoption of newer skills from the culminated experience of older skills (Qin et al., 2013; M\u00e1rton et al., 2021). Drawing a blunt parallel between biological neurons and LLMs would be naive. However, one can find a supporting intuition in the mechanistic interpretation of the Transformer architecture (Elhage et al., 2021; Conmy et al., 2023; Wang et al., 2022a). One can argue that if information pathways necessary to solve a novel task are similar to those corresponding to some different task from a task library, an LLM may gather useful information across tasks. Earlier\nevidence found by Tanwar et al. (2023) also elicits intuitive motivation as they showed that LLMs can learn to infer from cross-lingual examples if proper alignment is provided. In this work1, we design a cross-task prompting setup (Section 2) using three different LLMs: LLaMA-2 7B and 13B, and GPT 3.5; we select 50 different pairs of tasks where one serves as a source (i.e., context example task) and the other as target. Despite no examples from the target task presented in the context, LLMs can produce a staggering improvement over the zero-shot regime; on average, cross-task prompting improves performance by 107% for LLaMA-2 7B, 18.6% for LLaMA2 13B and 3.2% for GPT 3.5 (Section 3). With multiple source tasks, cross-task performance is even better than, if not comparable, usual in-task prompting (Contribution #1). However, learning from examples of different tasks is heavily sensitive to the choice of source task for a given target and the LLM is prone to copy the label space of the source task into the target. To circumvent this, we propose a pseudo-labeling based approach: in a data-scarce setup, cross-task prompting with majority voting is first employed to generate noisy, in-task examples; these are subsequently used for standard few-shot prompting (Contribution #2). Finally, we provide introductory analysis towards interpreting cross-task signal transfer by dissecting the model activations. We find that the cross-task signal transfer is abrupt and happens at later layers, with the effective layers widely varying for different target tasks (Contribution #3). In a nutshell, this is the first exploration of LLMs\u2019 ability to learn to solve novel tasks based on the contextual signals of different task examples.\n# 2 Prompting techniques\nTask definition for a given task is a natural language instruction for the LM describing what is asked of it (see Figure 1). Since a cross-task setup would result in in-context examples from different tasks (with different label spaces), such a definition is necessary to discriminate. Next, given two datasets Ds and Dt corresponding to two different tasks with task definitions ds and dt, respectively, we formalize the cross-task prompting as inferring the output \u02c6yt for an input xt \u2208Dt conditioned upon a demonstration from\n1Code available at https://github.com/C-anwoy/CrossTask-ICL\n\u02c6yt = argmax y p(y|(ds \u2295xs \u2295ys \u2295dt \u2295xt))\n\u02c6yt = argmax y p(y|(ds \u2295xs \u2295ys \u2295dt \u2295xt))\nIn this setup, we denote Ds and Dt as source and target task datasets, respectively. Note that the formalization is equivalent to 1-shot prompting where the only provided example input-output pair comes from a different dataset. Table 14 of Appendix F presents illustrative examples of prompts in this setup. Our experiments suggest that >1-shot setup in cross-task prompting does not improve (and often deteriorates) performance. Liu et al. (2022) showed that for a target input xt, generating context C from semantically similar examples leads to not only better results but also a more robust method of prompting. Similarly, Tanwar et al. (2023) showed that ICL could be done in a cross-lingual setup by aligning the semantic and task signals of the source and target languages. Drawing inspiration from these works, we set up the cross-task prompting regime with sampling examples xs from the source task dataset Ds that are semantically similar to the target input xt. To extract semantically similar examples, we first utilize Sentence-BERT (Reimers and Gurevych, 2019) to extract the sentence embedding of target and source inputs 2. Following this, based on the cosine similarity between the embeddings, we select top source examples. In-task examples combined with cross-task prompting. So far, we have assumed the unavailability of any labeled target dataset. But, if we do have a labeled target example, could prepending a source task boost its prompting performance? To emulate such a scenario, a labeled example from the target dataset Tt is sampled. This labeled example (xlt, ylt), where xlt \u2208Tt, is then used to construct prompt. This mixed setup can be formalized as,\n# 3 Results and Analysis\nDatasets and experimental setup. Our corpus of tasks consists of ten source and five target tasks.\n2We also experimented with E5 and LLaMA-2 7B last layer outputs, see Appendix B.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59c3/59c30231-0880-4236-8559-d0adc9223a1f.png\" style=\"width: 50%;\"></div>\nFigure 1: In this working example, we aim to solve a question from MedMCQA using demonstrations from BoolQ.\nTo do so, we sample a semantically similar demonstration from the source task and then use this demonstration\nalong with task descriptors of the source and target tasks to generate a cross-task prompt that is fed to an LLM.\nTAR\nSRC\nZero-shot\nARC-Easy\nAG-news\nBoolQ\nCom-QA\nC-POS\nC-NER\nMNLI\nQQP\nRACE\nSST2\nLLaMA-2 7B\nARC-Challenge\n4.6\n43.6\n33.6\n35.0\n43.6\n33.8\n34.2\n34.2\n36.4\n42.8\n33.2\nFinancial-Phrasebank\n34.1\n43.5\n62.1\n40.9\n14.6\n1.4\n0.4\n62.7\n44.7\n53.4\n65.0\nMedMCQA\n4.2\n31.4\n26.8\n28.0\n33.0\n26.2\n24.0\n23.0\n26.0\n31.6\n23.2\nSciQ\n8.0\n59.0\n45.4\n49.0\n65.6\n34.4\n29.7\n44.8\n25.4\n64.4\n39.0\nSocial-i-QA\n41.1\n44.3\n40.1\n40.3\n48.5\n38.9\n39.1\n38.9\n39.7\n49.1\n39.3\nAverage\n18.4\n44.3\n41.6\n38.6\n40.71\n26.9\n25.5\n40.7\n34.4\n48.3\n39.9\nLLaMA-2 13B\nARC-Challenge\n52.0\n59.2\n50.6\n54.6\n57.4\n50.6\n49.8\n49.2\n52.8\n56.8\n50.8\nFinancial-Phrasebank\n65.4\n66.6\n78.6\n65.8\n61.7\n2.2\n13.17\n79.0\n82.4\n66.5\n72.8\nMedMCQA\n9.2\n37.2\n34.2\n36.6\n38.8\n31.6\n32.6\n30.2\n30.6\n39.0\n31.4\nSciQ\n55.8\n83.4\n76.2\n80.2\n83.4\n76.4\n78.6\n77.2\n71.2\n82.8\n72.6\nSocial-i-QA\n55.3\n60.8\n55.8\n56.5\n63.5\n56.8\n55.9\n53.5\n55.7\n63.7\n53.1\nAverage\n47.5\n61.4\n59.1\n58.7\n60.9\n43.5\n46.0\n57.8\n58.5\n61.7\n56.1\nGPT3.5\nARC-Challenge\n74.6\n77.2\n74.4\n77.8\n76.2\n70.2\n70.8\n75.2\n74.0\n78.2\n72.6\nFinancial-Phrasebank\n57.5\n79.8\n83.6\n78.6\n82.0\n50.3\n64.3\n72.2\n74.2\n73.4\n75.6\nMedMCQA\n49.6\n49.8\n48.5\n50.0\n48.0\n47.4\n45.0\n46.0\n49.0\n47.6\n48.0\nSciQ\n91.2\n91.4\n89.9\n90.6\n91\n87.8\n82.0\n88.8\n89.8\n92.2\n89.0\nSocial-i-QA\n76.0\n76.2\n75.8\n74.0\n75.2\n73.2\n74.0\n75.8\n73.0\n77.2\n74.8\nAverage\n69.8\n74.9\n74.5\n74.2\n74.5\n65.8\n67.2\n71.6\n72.0\n73.7\n72.0\nTable 1: Accuracy for cross-task setup using one source example. Source tasks are mentioned in columns, and target tasks are in rows. Cross-task prompting brings improvement over zero-shot prompting for certain source-target pairs. (Abbreviations: Com-QA \u2192Commonsense-QA, C-POS \u2192Conll2003-POS, C-NER \u2192Conll2003-NER).\nWe consider ARC-Easy (Clark et al., 2018), AGnews (Zhang et al., 2015), BoolQ (Clark et al., 2019), Commonsense-QA (Talmor et al., 2019), Conll2003-POS (Tjong Kim Sang and De Meulder, 2003), Conll2003-NER (Tjong Kim Sang and De Meulder, 2003), MNLI (Williams et al., 2018), QQP (Sharma et al., 2019), RACE (Lai et al., 2017) and SST2 (Socher et al., 2013) to be our source tasks. Our motivation remains to incorporate domain diversity and difficulty of the problems while choosing target tasks to emulate the \u201cnovel task\u201d phenomenon as closely as possible\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a0d2/a0d20421-1d38-49bd-b76a-3d3d370c4f2e.png\" style=\"width: 50%;\"></div>\nsince learning from cross-task examples makes sense only when the target task is truly data-scarce. Our target tasks are Social-i-QA (Sap et al., 2019), SciQ (Auer et al., 2023), MedMCQA (Pal et al., 2022), Financial-Phrasebank (Malo et al., 2014), and ARC-Challenge (Clark et al., 2018); first four of these require domain expertise, while the last one is a more challenging version of the one present in the source tasks. In total, this gives us 50 unique cross-task setups (see Appendix A for dataset details. Table 16 and Table 17 of Appendix F show task definitions corresponding to source and target\ntasks, respectively). The dataset size is standardised by sampling 10, 000 and 500 examples for each source and target task, i.e., |Ds| = 10, 000 and |Dt| = 500. For our experiments, we used the 7-billion and 13-billion variants of LLaMA-2 (Touvron et al., 2023), and text-davinci-003 (Brown et al., 2020) referred to as GPT3.5. We experiment with greedy and force decoding setup and selected greedy decoding as our standard for all experiments (see Appendix D for more details). We set the number of examples used to create cross-task context as one for all our experiments, unless mentioned explicitly. Does cross-task prompting work? As evident from Table 1, cross-task prompting significantly improves performance compared to zero-shot prompting (see Table 20 in Appendix for results of significance testing). The best overall source-target pair improves performance by 162% for LLaMA-2 7B, 30% for LLaMA-2 13B and 7% for GPT3.5. We note that different models give the best performance in different source-target pairs and not all coupling of source-target tasks seem to work; e.g., Commonsense-QA as a source task decreases performance on Financial-Phrasebank, but is the best source task for ARC-Challenge and MedMCQA (for LLaMA-2 7B). Token classification tasks (POS and NER) seem to depreciate performance for LLaMA-2 13B and GPT3.5; their performance for LLaMA-2 7B is also sub-par compared to other source-target pairings. RACE and ARC-Easy are robust source tasks that improve performance for all target tasks in all models. ARC-Easy, MNLI and BoolQ can also be considered to have this robustness to some degree, as they only hurt the performance in one or two cases. On average, cross-task prompting improves performance by 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B and 3.2% for GPT3.5. This is a strong argument for prompting in a cross-task manner when we lack labeled target data, especially using small models, which have poor zero-shot abilities (Wei et al., 2022). We also experiment with LLaMA-2 7B Chat model to analyse the performance of cross-task prompting for instructiontuned models (see Appendix C). Performance with dissimilar source tasks. Few of the chosen source tasks have slight overlap in their broad topic or format with some of the target tasks, for example, source datasets like ARCEasy, Commonsense-QA, RACE consists of ques-\ntions in multiple-choice format, similar to target datasets like ARC-Challenge, Social-i-QA, SciQ and MedMCQA. To analyse the efficacy of crosstask prompting with completely dissimilar source tasks, let us consider only the source tasks whose topic and format are absolutely disjoint from all target tasks. The following five source tasks can be considered dissimilar to the target tasks \u2013 BoolQ, Conll2003-POS, Conll2003-NER, MNLI and QQP. As reported in Table 1, even with these dissimilar source tasks, we observe a substantial performance improvement of 80.5% for LLaMA-2 7B, 11.4% for LLaMA-2 13B, and 0.5% for GPT3.5 on average over zero-shot prompting. Moreover, if we consider only BoolQ, MNLI and QQP, the average performance gains become 105.9% for LLaMA-2 7B, 22.8% for LLaMA-2 13B, and 4% for GPT3.5. This shows that cross-task prompting can especially enable the relatively smaller LMs to substantially boost their zero-shot performance even using tasks dissimilar to the target. Importance of source task definitions. To further investigate the role of task definitions in crosstask prompting, we check the effect of removing source task definitions on performance. We note an average drop of 11% for LLaMA-2 7B and 8% for LLaMA-2 13B when we prompt without using source task definitions (see Table 13 in Appendix). Hence, definitions play a crucial role in cross-task prompting. Increasing number of examples for cross-task prompting. Unlike in-task prompting (Brown et al., 2020), where increasing the number of examples increases prompting performance, in crosstask prompting, performance does not improve with an increase in source examples (c.f. Fig 3). For most target tasks, increasing source examples does not affect performance, while for some, the performance decreases. Semantically similar vs random example selection. As shown in Table 2, in a cross-task setup, choosing the examples randomly leads to substantially poorer performance than when we generate the context C with semantically similar examples; concerning is the fact that, in many cases, it causes 0% accuracy. This may be caused by the model getting confused without any semantic alignment in the prompt. Furthermore, random labeling of in-context examples (Wei et al., 2023) result in nearrandom performance (see Table 12 in Appendix E). Mixed cross-task prompting. So far, we have seen\nTAR\nSRC\nAG-news\nBoolQ\nCom-QA\nMNLI\nQQP\nRandom\nARC-Challenge\n26.63.0\n3.70.7\n0.00.0\n29.53.7\n26.82.9\nFinancial-Phrasebank\n00\n1.41.3\n13.914.75\n0.00.0\n0.00.0\nMedMCQA\n26.11.5\n20.81.1\n0.00.0\n25.10.9\n25.00.2\nSciQ\n34.53.2\n10.04.25\n0.00.0\n34.04.1\n28.70.8\nSocial-i-QA\n36.81.3\n0.60.1\n0.00.0\n24.92.7\n29.273.9\nSemantically Similar\nARC-Challenge\n36.6\n35.0\n43.6\n34.2\n36.4\nFinancial-Phrasebank\n62.1\n40.9\n14.6\n62.7\n44.7\nMedMCQA\n26.8\n28.0\n33.0\n23.0\n26.0\nSciQ\n45.4\n49.0\n65.6\n44.8\n25.4\nSocial-i-QA\n40.1\n40.3\n48.5\n38.9\n39.7\n<div style=\"text-align: center;\">AG-news BoolQ Com-QA MNLI QQP</div>\nTable 2: Accuracy when cross-task examples are picked randomly vs when they are pickled based on semantic similarity. Experiments were done on the LLaMA-2 7B model. For random prompting, we generated the results over three seeds and reported the average performance (Com-QA: Commonsense-QA).\nBest source cross-task\nRandom mixed cross-task\nBest mixed cross-task\nLLaMA-2 7B\nARC-Challenge\n51.6\n42.8\n44.0\nFinancial-Phrasebank\n64.7\n48.3\n56.0\nMedMCQA\n34\n28.6\n29.0\nSciQ\n46.2\n60.5\n67.2\nSocial-i-QA\n42.7\n42.9\n42.9\nAverage\n47.84\n44.62\n47.82\nLLaMA-2 13B\nARC-Challenge\n66.4\n57.2\n60.4\nFinancial-Phrasebank\n76.6\n71.4\n61.5\nMedMCQA\n38.4\n38.3\n41.2\nSciQ\n84.6\n80.5\n84.2\nSocial-i-QA\n49.3\n61.9\n61.27\nAverage\n63.06\n61.86\n61.71\n<div style=\"text-align: center;\">Table 3: Accuracy in mixed cross-task prompting. Four examples are used to create context C in all setups.</div>\nthat cross-task prompting works for single sourcetarget task pairs. Next, we experiment on using multiple source tasks to construct the prompt context. To explore such a setup, we prompt LLaMA models using three methods: 1. Best source cross-task: We select the best source task for every target task using Table 1 and sample four semantically similar examples from that source task. 2. Random mixed cross-task: To see if a diverse set of tasks is beneficial, we randomly sample four source tasks and construct the prompt using most semantically similar examples. 3. Best mixed cross-task: This method is a combination of the first two; we use the top four best source tasks from Table 1 and sample a semantically similar source example from each task. Table 3 shows that a mixed prompting mechanism does not perform better than the \u201cbest source cross-task\u201d prompting method. On the contrary, it seems to hurt the performance of the model; in fact, single-source task prompting with only one example (Table 1) seems to do better than mixed prompting. Diversity seems to lead the model to\nget more confused, thus hurting its performance. Combining in-task with cross-task prompting. Combining cross-task prompting with labeled target examples improves performance, as seen in Table 4; apart from Financial-Phrasebank in LLaMA2 13B and MedMCQA in GPT3.5, for all other instances there exist a source task that improves the performance when coupled with the in-task example. However, we see that this improvement is immensely dependent on the source-target task pair chosen and unlike Table 1, we are unable to find robust source datasets that improve the performance throughout the setup. Nevertheless, in multiple source-target instances, there is a noteworthy improvement. To study the interaction between heterogeneous tasks in the context, we experiment by varying the number of source tasks and target task demonstrations in the context. Figure 2 shows the variation of accuracy for 8-shots, as we gradually move from an entirely in-task context to a complete cross-task context. We observe that for all target tasks, apart from Financial-Phrasebank, the accuracy with an 8shot in-task prompt and an 8-shot cross-task prompt (with best-mixed cross-task strategy) is almost the same in all three LLMs.\n# 4 Pseudo-label generation using cross-task prompting\nThus far we observe that cross-task prompting, though sometimes capable of even outperforming standard in-task prompting, is particularly sensitive to the choice of source task. Furthermore, the performance does not scale with the number of source task examples provided. Drawing inspiration from earlier works on pseudo-labeled examples to construct prompts (He et al., 2022; Vu et al., 2021), we propose a more practical method for potential usage of cross-task prompting. Given a small unlabeled dataset Dpl \u2282Dt, we assign a pseudo-label to the example using crosstask prompting. This is done using all source tasks available to us, and then a final ypl is assigned to xpl \u2208Dpl based on a majority vote from all the generated answers. Finally, this pseudo-labeled dataset Dpl is used to construct the prompt context in an in-context prompting setup. Cross task generated pseudo-examples vs gold label. To see the efficacy of our proposed method, we utilise a Dpl of size 8 and have three setups to create examples for context: 1. Gold-label: We\nTAR\nSRC\nIn-task\n+ARC-Easy\n+AG-news\n+BoolQ\n+Com-QA\n+C-POS\n+C-NER\n+MNLI\n+QQP\n+RACE\n+SST2\nLLaMA-2 7B\nARC-Challenge\n49.8\n51.2\n41.6\n45.8\n49.6\n43.6\n40.2\n41.6\n45.2\n50.0\n42.8\nFinancial-Phrasebank\n33.3\n33.3\n33.3\n33.3\n33.5\n33.3\n34.5\n35.7\n33.3\n34.5\n50.3\nMedMCQA\n32.4\n33.6\n29.2\n30.6\n32.8\n31.0\n30.8\n28.2\n30.2\n34.2\n31.2\nSciQ\n67.4\n73.4\n60.4\n68.6\n71.6\n59.0\n57.4\n58.6\n57.9\n71.2\n62.4\nSocial-i-QA\n43.3\n49.9\n46.7\n44.5\n58.3\n44.7\n44.3\n41.5\n43.9\n51.7\n43.1\nAverage\n45.2\n48.3\n42.2\n44.5\n48.1\n42.3\n41.4\n41.1\n42.1\n48.3\n45.9\nLLaMA-2 13B\nARC-Challenge\n61.8\n63.0\n60.8\n61.6\n61.0\n61.2\n60.0\n60.0\n59.6\n61.2\n58.8\nFinancial-Phrasebank\n86.0\n77.0\n71.2\n77.6\n82.4\n65.1\n69.4\n75.2\n64.8\n81.8\n85.6\nMedMCQA\n40.2\n40.8\n37.6\n40.4\n41.0\n38.2\n39.0\n38.6\n38.6\n41.8\n37.2\nSciQ\n84.8\n85.8\n83.0\n83.8\n85.6\n81.4\n81.4\n80.2\n79.2\n84.0\n79.6\nSocial-i-QA\n54.8\n62.1\n52.09\n52.3\n60.9\n53.3\n53.7\n54.1\n53.5\n61.6\n53.9\nAverage\n65.5\n65.7\n60.9\n63.1\n66.2\n59.8\n60.7\n61.6\n59.1\n66.0\n63.0\nGPT3.5\nARC-Challenge\n78.6\n78.2\n78.2\n78.8\n79.8\n77.4\n78.4\n78.4\n78.4\n77.6\n79.0\nFinancial-Phrasebank\n75.4\n80.2\n79.4\n79.6\n83.8\n70.4\n80.2\n80.2\n81.0\n81.6\n77.6\nMedMCQA\n52.0\n51.2\n48.6\n51.2\n47.4\n48.2\n49.4\n49.4\n48.8\n49.8\n49.8\nSciQ\n92.6\n91.2\n91.8\n92.6\n92.4\n91.8\n92.0\n92.0\n91.4\n93.0\n91.6\nSocial-i-QA\n75.6\n77.0\n76.2\n74.4\n76.6\n76.2\n77.4\n77.4\n75.0\n77.0\n75.6\nAverage\n74.8\n75.6\n74.8\n75.3\n76.0\n72.8\n74.2\n75.0\n74.9\n75.8\n74.7\nTable 4: Accuracy for in-task combined with cross-task prompting setup; prompt context is created using one source example and one human-labelled target example. Cross-task prompting compliments in-task promoting, leading to better performance (Com-QA: Commonsense-QA, C-POS: Conll2003-POS, C-NER: Conll2003-NER).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/304f/304f3377-1269-482e-a18d-397c0e415596.png\" style=\"width: 50%;\"></div>\nuse the annotated version of Dpl in the context; 2. Pseudo-label (ZS): We use zero-shot prompting to label Dpl for the context examples; 3. Pseudo-label (CT): We use the cross-task method as proposed in Section 4. The results of this experiment are shown in Table 5. For LLaMA models, as evident, pseudo labels generated via cross-task prompting are substantially better than zero-shot pseudo labels; they are of higher quality and lead to comparable performance as Gold-label. In the case of GPT 3.5, the scale of improvement is much smaller though. Interestingly, with only two datasets, namely, SciQ and Social-i-QA, we observe the pseudo examples from cross-task prompting to perform worse than gold-label examples, in all three models. Given the comparable performance of cross-task prompt-generated pseudo examples, we expect this to become a viable alternative to traditional ICL in data-scarce scenarios.\nFigure 2: Variation of accuracy with the number of crosstask demonstrations in the context C for 8-shot prompt, where C consists of a mixture of cross-task and in-task examples, using (a) LLaMA2 7B, (b) LLaMA-2 13B and (c) GPT3.5. Except for Financial-Phrasebank, an increase in cross-task examples give a stable performance.\nGold-label\nPseudo-examples(ZS)\nPseudo-examples(CT)\nLLaMA-2 7B\nARC-Challenge\n44.40.37\n7.24.52\n44.40.37\nFinancial-Phrasebank\n48.49.56\n56.52.01\n56.82.37\nMedMCQA\n30.60.84\n21.014.61\n30.61.03\nSciQ\n67.70.65\n3.31.88\n64.61.51\nSocial-i-QA\n46.81.69\n46.71.57\n46.41.94\nAverage\n47.62.62\n26.94.91\n48.51.44\nLLaMA-2 13B\nARC-Challenge\n56.80.47\n59.70.84\n58.41.79\nFinancial-Phrasebank\n67.95.03\n67.95.03\n67.995.03\nMedMCQA\n38.21.31\n2.61.8\n39.40.37\nSciQ\n84.10.75\n29.339.5\n84.10.75\nSocial-i-QA\n63.62.78\n43.627.7\n61.43.66\nAverage\n62.12.06\n40.6214.97\n62.22.32\nGPT3.5\nARC-Challenge\n76.70.89\n77.41.13\n77.460.94\nFinancial-Phrasebank\n80.44.35\n86.22.84\n83.436.41\nMedMCQA\n43.25.65\n43.43.30\n47.82.97\nSciQ\n92.81.04\n91.20.56\n91.50.25\nSocial-i-QA\n77.81.06\n76.60.65\n76.60.43\nAverage\n74.22.59\n74.91.69\n75.32.2\nTable 5: Accuracy in pseudo-label generated prompting (ZS: zero-shot, CT: cross-task). Pseudo-label (CT) performs comparably to gold-label examples.\nIncreasing number of pseudo-examples for intask prompting. Figure 3 shows the relation between the number of pseudo-demonstrations and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4005/4005b860-5cca-4e83-82c8-dc6b82772e26.png\" style=\"width: 50%;\"></div>\nFigure 3: Variation of accuracy with change in the number of \u2013 (a) source demonstrations from Ds (the performance largely stagnates at k = 1), (b) pseudo-label sampled from Dpl (the performance seems to increase with an increase in k). All experiments are done on LLaMA-2 7B model. accuracy. The general trend shows a rise in performance when more demonstrations are used in creating context C, except for Financial-Phrasebank, whose performance decreases with an increase in demonstrations.\n# 5 Interpretability analysis\nIn this section, we focus on the internal workings of the LLMs using the hidden states of the model and build an understanding of why and how cross-task prompting works. What is an ideal source task? Intuitively thinking of a source task that is similar to the target will result in better transfer of generalised signal from context to target inputs. To test this, we first compare the final layer outputs of the model corresponding to the source and target task definitions, ds and dt, respectively. Figure 4 shows the cosine similarity between the final layer hidden states corresponding to ds and dt. We note that for 80% of the time, the source definition that is the most semantically similar to the target definition also serves as its best suitor, leading to the best performance in Table 1. What does internal activation indicate? To get\nan even more in-depth picture, we analyse the layerwise activations for the LLaMA-2 7B model. For each layer, we compute the cosine similarities between the mean activations corresponding to the source and target task definitions. For a given target task, we then compute the rank correlation (in terms of Spearman correlation coefficient, Pearson coefficient, and, Kendall\u2019s tau) between the cosine similarities and the absolute point performance change from zero-shot prediction for each source task (see Table 19 in Appendix). Precisely, this gives us an approximate idea about the underlying mechanism of cross-task signal transfer. For all the tasks, we can see a U-shaped pattern in the correlation values (c.f. Fig 5) \u2013 at the starting layers, there is a high correlation between the cross-task activation similarity and the cross-task improvement (likely due to semantically similar example selection), that quickly drops in the middle layers, and increases again in the later layers (only exception being the MedMCQA target task). One can intuitively claim that in the initial layers of the model (Layers 2 to 5), there is more taskspecific computation going on where the cross-task transfer of information is the least. For FinancialPhrasebank, these task-specific layers cover more than 80% of the layers, with a gradual increase in correlation observable only after Layer 28. Hendel et al. (2023) provided a similar finding that the influence of the context kicks in only after a certain number of layers via task vectors. We see that the exact layer after which cross-task demonstrations will start signal to transfer is much more dependent on the target task and can vary widely.\n# 6 Error analysis\nWe observe four types of error occurring in crosstask prompting (c.f. Table 6), as follows: Label space replication. In example #1, the source example is from the Conll2003-POS dataset, a POS tagging task. In contrast, the target task is to predict the financial sentiment analysis. Therefore, the generation should be either negative, positive or neutral; however, we observe that the output is a sequence of POS tags instead. Here, the LLM is replicating the label space of the source task and the definition of the target task is not able to guide it to the correct target label space. Junk prediction. In some cases, we observe that the output is neither from the label space of the source task nor from that of the target task. As\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d8c/3d8c7277-6cd4-4e65-8618-b6a883592d93.png\" style=\"width: 50%;\"></div>\nFigure 4: Heat map of cosine similarities between the mean activations of the task definitions for source-target pairs, extracted from the final layer of (a) LLaMA-2 7B (b) LLaMA-2 13B. The values in the cells show the absolute performance improvement over zero-shot prompting. For a target task, the source-target pair with the highest cosine similarity shows the most improvement in 80% cases. (ARC-C: ARC-Challenge, F-Phr: Financial-Phrasebank, MMCQ: MedMCQA, S-QA: Social-i-QA, AGN: AG-news, ARCE: ARC-Easy, Com-QA: Commonsense-QA, C-POS: Conll2003-POS, C-NER: Conll2003-NER).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/93e2/93e26d95-8e37-4b6e-8cf0-d576049ddb1b.png\" style=\"width: 50%;\"></div>\nTable 6: Error analysis of cross-task prompting. Four examples represent the major error characteristics (discussed in Section 6). The outputs shown are generated by LLaMA-2 13B. The examples shown here are shortened, complete examples are provided in Table 18 of Appendix.\nin example #2, the source task is to classify each token into one named entity category (NER), and the target task is to predict the financial sentiment. The output in this case is found to be junk \u2014 it is a sequence of N\u2019s but N is not a pre-defined named entity category, neither is it a label for the target task. Further, we observe that the correct label in this example is \u2018neutral\u2019 \u2013 the LLM could not follow the target task definition provided and gets confused between the correct prediction label of the target task and the token classification task where the output is expected to be a sequence of labels, and instead outputs a sequence with the initial letter of the correct label.\nCopying effect: We also notice the copying effect, which has been observed by Lyu et al. (2023), where the predicted label is the same as the label of the context example which is semantically very similar to the target input. In example #3, the target input is semantically very similar to the context ex-\nample from the source task provided in the prompt, and consequently, the LLM incorrectly outputs the same label as that of the context example. Definition not followed: We observe that for some instances the LLM does not adhere to the definition given in the form of task definition for the target task. In example #4, the LLM is supposed to output one among the four options A, B, C, D, instead it outputs the text corresponding to the option D \u2013 though D is the correct answer in this case, the LLM is not able to follow the definition properly.\n# 7 Related work\nIn-context learning without gradient updates to the model was introduced by Brown et al. (2020) using the GPT-3 model. Multiple recent works sought robust ICL setup via different techniques: selecting the examples that are semantically most similar to the input (Liu et al., 2022), choosing low perplexity examples (Gonen et al., 2022), training a\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9275/9275bf0e-13ba-49a6-805b-e6ddc7f87538.png\" style=\"width: 50%;\"></div>\nFigure 5: Variation of (a) Spearman\u2019s correlation coefficient, (b) Pearson correlation coefficient and (c) Kendall\u2019s tau, with layers as reported in Table 19 for LLaMA-2 7B.\ntwo-stage retrieval system to select the best examples (Rubin et al., 2022), etc. However, these works primarily aim to construct better in-task prompts where the examples and the input come from the same task. Tanwar et al. (2023) showed that crosslingual ICL can be elicited with proper alignment between the source and target language examples. Raffel et al. (2020) introduced the T5 transformer model which has been trained to perform different text-processing tasks. Zhang et al. (2022) proposed a task prefix guided multi-task pre-training framework to solve this problem. More recently, a new prompting method called Meta-CoT has been proposed by Zou et al. (2023) which generalizes chain-of-thought prompting to include multi-task examples in the prompt and it has shown improvement in a number of tasks.\nIn this paper, we addressed LLMs\u2019 adaptability to novel tasks. Exploiting the inherent ICL capabilities of these models, we established that LLMs can substantially enhance their performance in novel task scenarios, even when direct examples are lacking. This encouraging outcome unveils fresh possibilities for the practical integration of LLMs across a broader spectrum of applications. Furthermore, our study demonstrated the significance of generating pseudo-labels using cross-task prompting, presenting a potential solution for situations where annotated data is scarce. The observed correlation between the impact of cross-task examples and the similarity in model activations between source and target input tokens offers valuable insights into the underlying mechanisms of this phenomenon.\n# Limitations\nDespite presenting a potential future direction towards training-free task generalization using LLMs, this study has some important limitations. It is evident that the similarity between the source and the target tasks plays an important role in the performance. Hence, in real-world scenarios where the task novelty is extreme, such a method may fail to provide suitable performance. This is directly related to the fact that ours is the first study in this direction, and we have primarily focused on the empirical viability. A deeper understanding of generalizable task information captured inside the LLM circuits would help to come up with sophisticated solutions. Task novelty in our discussion does not presuppose access to novel knowledge. Hence, one can not mitigate the gap if a novel task requires the model to access newer information not present in the pretraining data or the in-context examples.\n# References\nS\u00f6ren Auer, Dante A. C. Barone, Cassiano Bartz, Eduardo G. Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, Ivan Shilin, Markus Stocker, and Eleni Tsalapati. 2023. The sciqa scientific question answering benchmark for scholarly knowledge. Scientific Reports, 13(1):7240.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457. Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri\u00e0 GarrigaAlonso. 2023. Towards automated circuit discovery for mechanistic interpretability. arXiv preprint arXiv:2304.14997. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1. Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity estimation. Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, and Mohammad Norouzi. 2022. Generate, Annotate, and Learn: NLP with Synthetic Text. Transactions of the Association for Computational Linguistics, 10:826\u2013842. Roee Hendel, Mor Geva, and Amir Globerson. 2023. In-context learning creates task vectors. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9318\u20139333, Singapore. Association for Computational Linguistics. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785\u2013 794, Copenhagen, Denmark. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nChristian David M\u00e1rton, L\u00e9o Gagnon, Guillaume Lajoie, and Kanaka Rajan. 2021. Efficient and robust multi-task learning in the brain with modular latent primitives. arXiv preprint arXiv:2105.14108.\n# OpenAI. 2023. Gpt-4 technical report.\nSaurabh Pahune and Manoj Chandrasekharan. 2023. Several categories of large language models (llms): A short survey. arXiv preprint arXiv:2307.10188.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: A large-scale multisubject multi-choice dataset for medical domain question answering. In Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248\u2013260. PMLR. Wen Qin, Chunshui Yu, et al. 2013. Neural pathways conveying novisual information to the visual cortex. Neural plasticity, 2013. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367. Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. ArXiv, cmp-lg/9505040. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics.\nKathy Ruddy and Richard Carson. 2013. Neural pathways mediating cross education of motor function. Frontiers in Human Neuroscience, 7. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. CoRR, abs/1904.09728. Lakshay Sharma, Laura Graesser, Nikita Nangia, and Utku Evci. 2019. Natural language understanding with the quora question pairs dataset. CoRR, abs/1907.01041. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, Minneapolis, Minnesota. Association for Computational Linguistics. Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy Chakraborty. 2023. Multilingual LLMs are better cross-lingual in-context learners with alignment. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6292\u20136307, Toronto, Canada. Association for Computational Linguistics. Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013 147. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Tu Vu, Minh-Thang Luong, Quoc V. Le, Grady Simon, and Mohit Iyyer. 2021. Strata: Self-training with task augmentation for better few-shot learning. CoRR, abs/2109.06270. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022a. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593.\nways mediating cross education of motor function. Frontiers in Human Neuroscience, 7. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. CoRR, abs/1904.09728. Lakshay Sharma, Laura Graesser, Nikita Nangia, and Utku Evci. 2019. Natural language understanding with the quora question pairs dataset. CoRR, abs/1907.01041. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, Minneapolis, Minnesota. Association for Computational Linguistics. Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy Chakraborty. 2023. Multilingual LLMs are better cross-lingual in-context learners with alignment. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6292\u20136307, Toronto, Canada. Association for Computational Linguistics. Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013 147. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Tu Vu, Minh-Thang Luong, Quoc V. Le, Grady Simon, and Mohit Iyyer. 2021. Strata: Self-training with task augmentation for better few-shot learning. CoRR, abs/2109.06270. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022a. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022b. Text embeddings by weakly-supervised contrastive pre-training. ArXiv, abs/2212.03533. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837. Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. 2023. Symbol tuning improves in-context learning in language models. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122. Association for Computational Linguistics.\nZhuosheng Zhang, Shuohang Wang, Yichong Xu, Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chenguang Zhu, and Michael Zeng. 2022. Task compass: Scaling multi-task pre-training with task prefix. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5671\u20135685, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nAnni Zou, Zhuosheng Zhang, Hai Zhao, and Xiangru Tang. 2023. Meta-cot: Generalizable chain-ofthought prompting in mixed-task scenarios with large language models.\n# A Dataset details\n# A.1 Source datasets\nWe have used the following datasets as source datasets:\nARC-Easy: ARC-Easy (Clark et al., 2018) is a part of the ARC (AI2 Reasoning Challenge) dataset consisting of easy natural science questions targeted for students of 3rd to 9th grade. The questions are of multiple-choice format, where one of the four given options is correct. The training set consists of 2251 questions that we use for selecting our source examples. AG-news: AG-news (Zhang et al., 2015) is a text classification dataset containing news\nAG-news: AG-news (Zhang et al., 2015) is a text classification dataset containing news\narticles categorized into four classes - sports, business, technology, and world. It has a training set size of 120K, from which we have randomly sampled 10K news articles to construct the source dataset for our experiments.\nBoolQ: BoolQ (Clark et al., 2019) is a reading comprehension dataset consisting of yes/no questions asked from a passage given for each question. The questions are mostly nonfactoid, and considerable inference ability is required to answer them based on the passages provided. In our usage, each question is labeled as either true or false. The training set consists of 9427 labeled question-passage pairs from which we select the source examples.\nCommonsense-QA: Commonsense-QA (Talmor et al., 2019) is a commonsense questionanswering dataset that consists of multiplechoice questions where one of the five options provided is correct. To answer the questions, logical reasoning abilities and in some cases prior knowledge are required. The training set consists of 9740 labeled questions from which source examples are chosen by us.\nConll2003-POS: Conll2003-POS (Tjong Kim Sang and De Meulder, 2003) is a collection of the data which is a part of the CoNLL-2003 shared task. In this dataset, each sentence is labeled as a sequence of part-ofspeech (POS) tags (each token is assigned a POS tag). We construct our source dataset by sampling 10K sentences from the 14,041 sentences in the training set.\nConll2003-NER: Conll2003-NER (Tjong Kim Sang and De Meulder, 2003) is also a part of the CoNLL-2003 shared task. Here, each sentence is labeled as a sequence of named entity tags. The task in this case is to perform named-entity recognition (NER). IOB tagging scheme (Ramshaw and Marcus, 1995) is used for assigning the tags. Four types of entities are assumed to be present in the data \u2013 persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC). The training set consists of 14,041 sentences of which 10K are sampled by us as our source data.\nMNLI: Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) corpus is one of the largest available resources for natural language inference. The corpus consists of 393K labeled examples, where each example consists of a pair of sentences and the label is one among neutral, contradiction or entailment based on the relationship between their meanings. Our source dataset is constructed by sampling 10K examples from the 393K examples available.\nQQP: Quora Question Pairs (QQP) (Sharma et al., 2019) dataset is curated for the task of natural language understanding. This dataset consists of question pairs collected from the popular question-answering website Quora, and the task is to determine if the questions are duplicates of each other. The training set consists of 364K question pairs, each labeled as duplicate or not duplicate. We sample 10K labeled question pairs from the training set for our source data.\nRACE: RACE (Lai et al., 2017) is a reading comprehension dataset consisting of passages along with questions asked from them. The passages and questions are collected from English exams of school students aged between 12 to 18. The questions are of multiple-choice format where one of the four options is correct. Our source dataset is gathered by picking 10K passage-question pairs from 87.9K such pairs available in the training set.\nSST2: SST2 (Socher et al., 2013) dataset is a part of the Stanford Sentiment Treebank corpus. It contains movie review snippets collected from the Rotten Tomatoes website. Each review is labeled as positive or negative. The 10K reviews for our source dataset are sampled from the training set consisting of 67.3K labeled reviews.\n# A.2 Target datasets We have used the following datasets as target datasets:\nARC-Challenge: ARC-Challenge (Clark et al., 2018) is also a part of the ARC (AI2 Reasoning Challenge) dataset and it consists of hard natural science questions targeted for students of 3rd to 9th grade. Those\nARC-Challenge: ARC-Challenge (Clark et al., 2018) is also a part of the ARC (AI2 Reasoning Challenge) dataset and it consists of hard natural science questions targeted for students of 3rd to 9th grade. Those\nquestions that were answered incorrectly by both a retrieval-based algorithm and a word co-occurrence method are considered to be \u2018hard\u2019enough for inclusion in this dataset. The questions are of multiple-choice format, where one of the four options is correct. The test set consists of 1172 questions out of which 500 were randomly selected for our target dataset.\nSocial-i-QA: Social-i-QA (Sap et al., 2019) is a commonsense reasoning dataset focusing on social situations. This dataset contains examples consisting of a social situation or action given as context and a multiple-choice question asked based on the context aimed at testing emotional and social intelligence. Each question has three options, out of which one is correct. We select 500 examples for our target data from the 1954 examples available in the validation set.\nMedMCQA: MedMCQA (Pal et al., 2022) is a multiple-choice question-answering dataset consisting of questions from post-graduate medical entrance exams in India. Each question has four options of which one is correct. Our target dataset is constructed by selecting 500 questions from the 4183 questions in the validation set.\n# Financial-Phrasebank:\nPhrasebank (Malo et al., 2014) is a financial sentiment analysis dataset containing sentences mined from a corpus of English news on all listed companies in OMX Helsinki. Each sentence is labeled as one of the three categories \u2013 positive, negative, or neutral, based on the influence the news snippet may have on the stock price. The training set consists of 2264 labeled sentences, from which 500 are sampled for our target dataset.\nIn each case, for preparing our target dataset we have selected 500 examples. The selection,\nthough random, is done in such a way that our target datasets are balanced, i.e. the number of examples with each of the different possible labels is almost equal.\n# B Semantic context creation\nOne might assume that taking internal embeddings of the models, instead of an external model like Sentence-BERT, for extracting semantically similar examples might be better suited. However, we found that doing so for LLaMA-2 7B is computationally more expensive, with no significant gain in performance, as reported in Table 7. We also experimented with Sentence-BERT and E5 (Wang et al., 2022b) to test which external model produces superior semantically similar context C for the LLMs to use. Table 8 shows the results for LLaMA-2 7B and GPT3.5 performance using C from the E5 model. We proceeded with Sentence-BERT instead of E5 as our external model, as the former has already been used for a similar role in prior works (Liu et al., 2022; Tanwar et al., 2023), and its performance is also comparable to that of E5.\n# C Cross-task prompting in instruction-tuned models\nThe performance gain with cross-task prompting for instruction-tuned models is lesser compared to that for the corresponding base model, due to the improvement in zero-shot performance of instruction-tuned models. For instance, Table 9 shows the performance for LLaMA-2 7B Chat model. We observe that it\u2019s zero-shot performance is much better compared to the LLaMA-2 7B base model.\n# D Force decoding\nUnlike greedy decoding, where the most probable token from the entire vocabulary of the model is the output, in force decoding the vocabulary is restricted to a set of tokens and the output is assigned out of these restricted tokens. Formally, for context C and a target task T, the output in the case of force decoding is:\nwhere, LT is the label space of target task T. The label space of all five target tasks is shown in Table 11.\nTAR\nSRC\nZero-shot\nARC-Easy\nAG-news\nBoolQ\nCom-QA\nC-POS\nC-NER\nMNLI\nQQP\nRACE\nSST2\nLLaMA-2 7B\nARC-Challenge\n4.6\n44\n32.2\n36.6\n43.2\n36.8\n35.2\n34.6\n33.8\n37.4\n33.2\nFinancial-Phrasebank\n34.1\n51.1\n58.28\n39.9\n11.2\n0.2\n0\n65.6\n42.31\n40.3\n60.4\nMedMCQA\n4.2\n30.2\n25.8\n27\n32.4\n24.6\n25.2\n24.8\n25.6\n30.6\n25.2\nSciQ\n8.0\n60.2\n44.8\n46.6\n66.8\n33.6\n28\n47.8\n25.6\n61.8\n42\nSocial-i-QA\n41.1\n41.7\n35.5\n37.3\n47.3\n39.3\n37.9\n37.9\n36.3\n47.3\n39.7\n<div style=\"text-align: center;\">Table 7: Accuracy for cross-task setup using one source example which is most similar to the target input based on their final hidden layer representation from LLaMA-2 7B. (Abbreviations: Com-QA \u2192 Commonsense-QA, C-POS \u2192Conll2003-POS, C-NER \u2192Conll2003-NER).</div>\n \u2192 \u2192\nTAR\nSRC\nZero-shot\nARC-Easy\nAG-news\nBoolQ\nCom-QA\nC-POS\nC-NER\nMNLI\nQQP\nRACE\nSST2\nLLaMA-2 7B\nARC-Challenge\n4.6\n44.80\n34.20\n36.00\n44.00\n32.00\n31.80\n35.80\n35.20\n48.80\n33.60\nFinancial-Phrasebank\n34.1\n43.91\n60.08\n38.92\n38.32\n4.39\n0.00\n67.86\n56.09\n50.30\n46.71\nMedMCQA\n4.2\n30.40\n24.40\n28.20\n32.80\n24.20\n24.60\n23.60\n25.60\n29.20\n23.40\nSciQ\n8.0\n62.40\n38.80\n46.00\n68.40\n31.80\n32.40\n36.60\n27.60\n65.40\n37.20\nSocial-i-QA\n41.1\n42.51\n37.33\n37.92\n46.91\n37.72\n40.52\n41.72\n39.52\n47.90\n41.52\nGPT3.5\nARC-Challenge\n74.6\n77.00\n74.80\n75.80\n76.60\n70.20\n69.80\n72.40\n74.40\n77.40\n73.00\nFinancial-Phrasebank\n57.5\n78.44\n82.63\n75.25\n77.45\n63.67\n70.46\n67.66\n74.25\n76.65\n63.67\nMedMCQA\n49.6\n47.00\n49.60\n49.20\n49.80\n48.40\n46.20\n47.20\n46.60\n49.60\n46.80\nSciQ\n91.2\n91.20\n89.00\n91.00\n91.00\n85.80\n84.60\n89.20\n89.20\n93.40\n90.60\nSocial-i-QA\n76.0\n77.05\n74.65\n74.85\n77.05\n73.85\n73.05\n74.05\n73.25\n77.25\n71.86\nTable 8: Accuracy for cross-task setup using one source example which is most similar to the target input based on their embeddings generated by the E5 model. (Abbreviations: Com-QA \u2192Commonsense-QA, C-POS \u2192Conll2003-POS, C-NER \u2192Conll2003-NER).\n \u2192 \u2192\nTAR\nSRC\nZero-shot\nARC-Easy\nAG-news\nBoolQ\nCom-QA\nC-POS\nC-NER\nMNLI\nQQP\nRACE\nSST2\nLLaMA-2 7B Chat\nARC-Challenge\n42.00\n51.00\n39.20\n45.00\n51.40\n37.00\n30.20\n37.00\n37.20\n51.40\n39.00\nFinancial-Phrasebank\n79.64\n76.05\n83.63\n77.84\n68.66\n43.91\n12.18\n77.25\n80.84\n78.84\n83.43\nMedMCQA\n30.60\n31.00\n30.20\n31.60\n34.60\n30.00\n26.80\n30.20\n27.40\n31.00\n30.20\nSciQ\n65.60\n66.60\n59.00\n67.00\n71.80\n58.20\n45.00\n53.80\n60.80\n69.20\n59.60\nSocial-i-QA\n52.10\n48.10\n47.70\n47.70\n49.50\n47.90\n36.33\n42.32\n41.92\n53.69\n45.91\nTable 9: Accuracy for cross-task setup, using one source example, with the LLaMA-2 7B Chat model. (Abbreviations: Com-QA \u2192Commonsense-QA, C-POS \u2192Conll2003-POS, C-NER \u2192Conll2003NER).\nTable 9: Accuracy for cross-task setup, using one source example, with the LLaMA-2 7B Chat model. (Abbreviations: Com-QA \u2192Commonsense-QA, C-POS \u2192Conll2003-POS, C-NER \u2192Conll2003-\nTable 10 reports the performance of cross-task prompting for all source-target pairs using force decoding in LLaMA-2 7B and LLaMA-2 13B models. We noted that force decoding improves the performance of zero-shot prediction by a great margin. However, the same is not true for every sourcetarget pair.\n# E Random labeling\nRecently, Wei et al. (2023) proposed using a random label space for pseudo-labeling examples of (target) task and utilise these to generate the context C. We experimented with this setup as it is a better alternative to zero-shot prompting, but our results (Table 12) showed that the model output is random with such C.\n# F Prompt details\nWe show a few examples of cross-task prompts in Table 14 and in-task combined with cross-task prompts in Table 15. Additionally, task definitions of source and target tasks are provided in Table 16 and Table 17 respectively.\n# G Error analysis\nTable 18 contains detailed examples for erroneous predictions in cross-task prompting setup.\nH Activation analysis on LLaMA-2 7B Full correlation analysis is presented in Table 19.\nFull correlation analysis is presented in Table 19.\nTAR\nSRC\nZero-shot\nARC-Easy\nAG-news\nBoolQ\nCom-QA\nC-POS\nC-NER\nMNLI\nQQP\nRACE\nSST2\nLLaMA-2 7B\nARC-Challenge\n38.80\n45.40\n33.60\n36.00\n45.00\n35.20\n32.80\n35.40\n35.20\n46.60\n35.00\nFinancial-Phrasebank\n36.33\n47.11\n63.27\n60.28\n45.91\n50.50\n53.49\n62.87\n57.29\n44.91\n53.09\nMedMCQA\n26.00\n28.00\n26.60\n28.80\n32.60\n28.20\n26.80\n27.00\n27.00\n29.60\n27.40\nSciQ\n59.60\n61.60\n37.00\n49.80\n65.60\n37.00\n31.60\n33.20\n27.20\n62.00\n32.40\nSocial-i-QA\n47.11\n43.31\n41.32\n39.12\n48.90\n40.92\n40.92\n43.11\n39.72\n48.10\n40.52\nLLaMA-2 13B\nARC-Challenge\n52.80\n58.80\n54.60\n56.00\n56.80\n51.20\n50.20\n53.40\n54.00\n57.20\n52.00\nFinancial-Phrasebank\n62.08\n73.25\n79.64\n70.06\n66.87\n66.27\n67.47\n75.65\n80.24\n70.86\n77.64\nMedMCQA\n36.60\n38.20\n36.00\n37.60\n38.80\n36.00\n32.20\n33.80\n31.20\n39.60\n33.00\nSciQ\n82.00\n83.60\n81.40\n82.80\n82.20\n80.60\n79.00\n81.20\n79.80\n82.80\n77.80\nSocial-i-QA\n57.68\n62.08\n58.08\n56.49\n62.28\n57.49\n57.49\n55.29\n56.49\n65.07\n57.88\nTable 10: Accuracy for cross-task setup using one source example by force decoding, where the labe from the label space to which the LLM assigns the highest probability is considered as the output (Abbreviations: Com-QA \u2192Commonsense-QA, C-POS \u2192Conll2003-POS, C-NER \u2192Conll2003 NER).\nTarget task\nLabel space\nARC-Challenge\n{A, B, C, D}\nFinancial-Phrasebank\n{positive, neutral, negative}\nMedMCQA\n{A, B, C, D}\nSciQ\n{A, B, C, D}\nSocial-i-QA\n{A, B, C}\n<div style=\"text-align: center;\">Table 11: Label space of target tasks</div>\nShots\nTRC\nARC-Challenge\nFinancial-Phrasebank\nMedMCQA\nSciQ\nSocial-i-QA\nLLaMA-2 7B\n1\n25.07\n33.33\n25.00\n25.00\n33.27\n8\n25.00\n34.13\n24.80\n25.40\n32.73\nLLaMA-2 13B\n1\n25.13\n31.00\n25.00\n25.07\n33.33\n8\n24.40\n33.33\n26.00\n25.20\n33.53\nGPT3.5\n1\n24.00\n49.50\n22.60\n26.60\n32.58\n8\n24.00\n56.89\n24.40\n37.00\n33.13\nTable 12: Accuracy for random label space labelling. We experimented with random labeled context C of size one and eight.\nWith source definitions\nW/O source definitions\nLLaMA-2 7B\nARC-Challenge\n37.0\n33.0\nFinancial-Phrasebank\n38.8\n27.0\nMedMCQA\n27.3\n17.06\nSciQ\n45.7\n48.1\nSocial-i-QA\n41.8\n42.0\nAverage\n38.1\n33.8\nLLaMA-2 13B\nARC-Challenge\n53.2\n52.42\nFinancial-Phrasebank\n58.9\n48.0\nMedMCQA\n34.2\n24.36\nSciQ\n78.2\n75.3\nSocial-i-QA\n57.5\n58.1\nAverage\n56.4\n51.6\nTable 13: Accuracy With and Without (W/O) source definition. We note a drop in accuracy when source definitions are removed.\nTarget\ntask\nSource task\nPrompt\nFinancial-\nPhrasebank\nQQP\nDefinition: Given two question pairs do text classification based on whether they are duplicates\nor not. The questions are mined from the popular online discussion forum Quora. As duplicate\nquestions might be present on Quora, the task is to label two identical questions as \"duplicate\" if\nthey ask the same query else label the pair as \"not duplicate\".\nQuestion 1: My yearly income went from $0 to $55,000. By how many times did it increase?\nQuestion 2:What can I do with approximately 10.000 Baht per month to increase my income?\nLabel: not duplicate\nDefinition: Given a sentence mined from a financial news article, you are to determine the sentiment\npolarity of the sentence. The task deals with financial sentiment analysis. Based on the sentiment\nconveyed by the sentence, label the sentence as \"negative\", \"positive\" or \"neutral\"\nSentence: Net income from life insurance doubled to EUR 6.8 mn from EUR 3.2 mn , and net\nincome from non-life insurance rose to EUR 5.2 mn from EUR 1.5 mn in the corresponding period\nin 2009 .\nLabel:\nSciQ\nCommonsense-\nQA\nDefinition: The following task relates to commonsense reasoning. It consists of a question that can\nbe easily solved using logical abilities and reasoning, a set of five options \"A.\", \"B.\", \"C.\", \"D.\" and\n\"E.\" are also provided along with the question, one of these options answers the question logically.\nUse your reasoning ability to select the most appropriate answer from the provided choices \"A.\",\n\"B.\", \"C.\", \"D.\" and \"E.\" and assign these choices (i.e \"A.\", \"B.\", \"C.\", \"D.\" and \"E.\") as the label\nQuestion:In what substance do clouds float?\nA. sky\nB. top of mountain\nC. air\nD. ground level\nE. outer space\nAnswer: C\nDefinition: Given a question from a scientific exam about Physics, Chemistry, and Biology, among\nothers. The question is in multiple choice format with four answer options \"A.\", \"B.\", \"C.\" and \"D.\".\nUsing your knowledge about the scientific fields answer the question and provide the label \"A\", \"B\",\n\"C\" and \"D\" as answer\nQuestion: What term means the amount of water vapor in the air?\nA. pressure\nB. humidity\nC. temperature\nD. ambient\nAnswer:\nSocial-i-\nQA\nRACE\nDefinition: Given a reading comprehension type question-answering from an english exam for\nschool students. You are given a context and multiple choice question containing four options \"A.\",\n\"B.\", \"C.\" and \"D.\". The question is answerable from the comprehension. Based on the question,\nthe option and the context select the most appropriate answer from the provided choices \"A.\", \"B.\",\n\"C.\" and \"D.\".\nContext: Mike is a factory worker. He is often very tired after a day\u2019s work. His wife, Jenny, has no\njob, so she stays at home to cook the meals. Every day he can have his dinner when he gets home\nfrom his factory. One day, Mike came home very late because he was very busy in the factory. He\nwas very hungry when he got home. He was not happy when he found his dinner was not ready. He\nwas very angry with his wife. He shouted at her, \"I\u2019m going out to eat in a restaurant.\" \"Wait for\nfive minutes,\" said his wife. \"Why? Do you think that dinner will be ready in five minutes?\" asked\nMike. \"Of course not,\" she answered. \"But I can be ready to go with you in five minutes.\"\nQuestion: Mike works in _ .\nA. a factory\nB. an office\nC. a school\nD. a hospital\nAnswer: A\nDefinition: Given an action as the context and a related question, you are to answer the question\nbased on the context using your social intelligence. The question is of multiple choice form with\nthree options \"A\", \"B\" and \"C\". Select the most appropriate answer from the provided choices \"A\",\n\"B\" and \"C\".\nContext: Jesse is patient and hardworking and hungry in the morning.\nQuestion: How would you describe Jesse?\nA. ill prepared\nB. Exhausted and starved\nC. thoughtful\nAnswer:\nTable 14: Examples of prompts for cross-task prompting technique. The number of source examples in each case\nTable 14: Examples of prompts for cross-task prompting technique. The number of source examples in each case one. The outputs shown are generated by LLaMA-2 13B.\nTarget task Source task Prompt FinancialPhrasebank SST2 Definition: Given a movie review do text classification, based on the sentiment conveyed by the review label it as \"positive\" or \"negative\" Sentence: results is the best performance from either in years Label: positive Definition: Given a sentence mined from a financial news article, you are to determine the sentiment polarity of the sentence. The task deals with financial sentiment analysis. Based on the sentiment conveyed by the sentence, label the sentence as \"negative\", \"positive\" or \"neutral\" Sentence: For the last quarter of 2010 , Componenta \u2019s net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m . Label: positive Sentence: Both operating profit and net sales for the six-month period increased , respectively from EUR18 .1 m and EUR127 .6 m , as compared to the corresponding period in 2006 . Label: MedMCQA RACE Definition: Given a reading comprehension type question-answering from an english exam for school students. You are given a context and multiple choice question containing four options \"A.\", \"B.\", \"C.\" and \"D.\". The question is answerable from the comprehension. Based on the question, the option and the context select the most appropriate answer from the provided choices \"A.\", \"B.\", \"C.\" and \"D.\". Context: Plants need green leaves to make food. A plant needs sunlight and carbon dioxide from the air for making food and it also needs water and salts from the soil to make food too. There are certain cells in the leaves which change carbon dioxide and water into sugar. To do this the cells needs energy, which they get from the sunlight. Green leaves make food for the whole plant. A red leaf can make food too because under the red color1ing of the leaf there are food\u2014-making cells. There are no leaves which are completely yellow, for they can\u2019t make food. The plant makes sugar for its food. In sunlight green leaves make a lot of sugar. The veins can\u2019t carry all this sugar away, so the leaves change the sugar into starch, which is kept and so stored in the leaves. At night, the starch changes back to sugar. It is then carried away from the leaves. Some of the sugar is used as food by the plant while the rest is stored as starch. In some plants, food is stored in the roots, in others it is stored in the stem and in leaves, fruits and seeds. Question:Sugar is made for its food by A. sunlight B. veins C. stems D. green leaves Answer: D Definition: Given a multiple choice question containing four options \"A.\", \"B.\", \"C.\" and \"D.\" from a medical entrance exam. The question is related to a sub-field of medical science like Microbiology, Radiology, Ophthalmology, Surgery, Human anatomy, etc. Based on the question, the option and your knowledge of the medical field select the most appropriate answer from the provided choices \"A.\", \"B.\", \"C.\" and \"D.\". Question: Growth hormone has its effect on growth through? A. Directly B. IG1-1 C. Tyroxine D. Intranuclear receptors Answer: B Question:Which of the following has intracellular receptor A. Glucagon B. Insulin C. Epinephrine D. Thyroxine Answer: SciQ ARC-Easy Definition: Given a question answering task from the 3rd to 9th-grade science exam. The question contains four options \"A.\", \"B.\", \"C.\" and \"D.\" Select the most appropriate choice that answers the question Question: From which part of the plant does a bee get food? A. flower B. seed C. stem D. root Answer: A Definition: Given a question from a scientific exam about Physics, Chemistry, and Biology, among others. The question is in multiple choice format with four answer options \"A.\", \"B.\", \"C.\" and \"D.\". Using your knowledge about the scientific fields answer the question and provide the label \"A\", \"B\", \"C\" and \"D\" as answer Question: What type of organism is commonly used in preparation of foods such as cheese and yogurt? A. Viruses B. Protozoa C. Gymnosperms D. Mesophilic organisms Answer: D Question: A bee will sometimes do a dance to tell other bees in the hive where to find what? A. water B. food C. honey D. enemies Answer:\nFinancial-\nPhrasebank\nSST2\nDefinition: Given a movie review do text classification, based on the sentiment conveyed by the review\nlabel it as \"positive\" or \"negative\"\nSentence: results is the best performance from either in years\nLabel: positive\nDefinition: Given a sentence mined from a financial news article, you are to determine the sentiment\npolarity of the sentence. The task deals with financial sentiment analysis. Based on the sentiment\nconveyed by the sentence, label the sentence as \"negative\", \"positive\" or \"neutral\"\nSentence: For the last quarter of 2010 , Componenta \u2019s net sales doubled to EUR131m from EUR76m\nfor the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\nLabel: positive",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of adapting large language models (LLMs) to novel tasks, particularly in the context of their in-context learning (ICL) capabilities. Previous methods have struggled with the computational demands of colossal models and the zero-shot performance of smaller models without context. This study investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks, thereby enhancing their adaptability.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of effectively adapting large language models to novel tasks without access to examples from those tasks.",
            "key obstacle": "The main difficulty is that while large models like GPT-4 perform well in zero-shot settings, their high computational requirements hinder widespread usage, and smaller models lack the necessary context to perform adequately."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is inspired by biological neurons, which can transfer learning from one task to another, suggesting that LLMs might similarly leverage information from related tasks to improve performance on new tasks.",
            "opinion": "The proposed idea involves a cross-task prompting setup, where examples from one task are used to inform the model's performance on a different, novel task, with the aim of enhancing adaptability and performance.",
            "innovation": "The innovation lies in the method of cross-task prompting, which allows LLMs to achieve significant performance improvements on novel tasks by leveraging contextual signals from examples of different tasks, a departure from traditional in-task prompting."
        },
        "method": {
            "method name": "Cross-task prompting",
            "method abbreviation": "CTP",
            "method definition": "Cross-task prompting is a technique that utilizes labeled examples from one task to enhance the performance of a language model on a different, novel task by providing contextual information.",
            "method description": "This method revolves around generating prompts that incorporate examples from a source task to assist in performing a target task, thereby facilitating better learning and adaptation.",
            "method steps": [
                "Select a source task with labeled examples.",
                "Identify a target task that requires adaptation.",
                "Generate prompts that combine definitions and examples from both tasks.",
                "Feed the prompts into the language model for inference."
            ],
            "principle": "The effectiveness of this method is grounded in the idea that similar information pathways exist between tasks, allowing models to transfer knowledge and improve their performance on novel tasks."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involves using three different LLMs (LLaMA-2 7B, LLaMA-2 13B, and GPT 3.5) across 50 unique source-target task pairs to evaluate performance improvements.",
            "evaluation method": "Performance is assessed by comparing the accuracy of the models under zero-shot prompting conditions against the accuracy achieved using cross-task prompting, with results analyzed for statistical significance."
        },
        "conclusion": "The study concludes that cross-task prompting can significantly improve the performance of LLMs on novel tasks, demonstrating the potential for these models to generalize learning across different contexts, especially in data-scarce scenarios.",
        "discussion": {
            "advantage": "Key advantages of the proposed approach include substantial performance improvements on novel tasks and the ability to utilize existing labeled examples from related tasks, making it practical for real-world applications.",
            "limitation": "A limitation of the method is its sensitivity to the choice of source tasks; if the source and target tasks are too dissimilar, performance may not improve or could even degrade.",
            "future work": "Future research could explore the development of more sophisticated methods for task selection and the generation of contextual signals to enhance cross-task learning efficacy."
        },
        "other info": {
            "info1": "The study provides a first-of-its-kind exploration into the ability of LLMs to learn from contextual signals across tasks.",
            "info2": {
                "info2.1": "The correlation between model activation similarities and performance improvements is analyzed.",
                "info2.2": "The research suggests practical applications for LLMs in scenarios where labeled data is scarce."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of adapting large language models (LLMs) to novel tasks, particularly in the context of their in-context learning (ICL) capabilities."
        },
        {
            "section number": "1.2",
            "key information": "The study investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks, thereby enhancing their adaptability."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, cross-task prompting (CTP), utilizes labeled examples from one task to enhance the performance of a language model on a different, novel task."
        },
        {
            "section number": "3.2",
            "key information": "The effectiveness of cross-task prompting is grounded in the idea that similar information pathways exist between tasks, allowing models to transfer knowledge and improve their performance on novel tasks."
        },
        {
            "section number": "4.1",
            "key information": "Key advantages of the cross-task prompting approach include substantial performance improvements on novel tasks and the ability to utilize existing labeled examples from related tasks."
        },
        {
            "section number": "6",
            "key information": "A limitation of the cross-task prompting method is its sensitivity to the choice of source tasks; if the source and target tasks are too dissimilar, performance may not improve or could even degrade."
        },
        {
            "section number": "7",
            "key information": "The study concludes that cross-task prompting can significantly improve the performance of LLMs on novel tasks, demonstrating the potential for these models to generalize learning across different contexts."
        }
    ],
    "similarity_score": 0.7320420485615652,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks.json"
}