{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.12038",
    "title": "Soft Prompting for Unlearning in Large Language Models",
    "abstract": "The widespread popularity of Large Language Models (LLMs), partly due to their unique ability to perform in-context learning, has also brought to light the importance of ethical and safety considerations when deploying these pretrained models. In this work, we focus on investigating machine unlearning for LLMs motivated by data protection regulations. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize the unlearning of a subset of training data. With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed method and our results indicate that SPUL can significantly improve the trade-off between utility and forgetting in the context of text classification and question answering with LLMs. We further validate our method using multiple LLMs to highlight the scalability of our framework and provide detailed insights into the choice of hyperparameters and the influence of the size of unlearning data. Our implementation is available at https://github. com/karuna-bhaila/llm_unlearning.",
    "bib_name": "bhaila2024softpromptingunlearninglarge",
    "md_text": "# Prompting for Unlearning in Large Language M\nKaruna Bhaila Minh-Hao Van Xintao Wu University of Arkansas {kbhaila, haovan, xintaowu}@uark.edu\nUniversity of Arkansas {kbhaila, haovan, xintaowu}@uark.edu\n# Abstract\nThe widespread popularity of Large Language Models (LLMs), partly due to their unique ability to perform in-context learning, has also brought to light the importance of ethical and safety considerations when deploying these pretrained models. In this work, we focus on investigating machine unlearning for LLMs motivated by data protection regulations. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize the unlearning of a subset of training data. With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed method and our results indicate that SPUL can significantly improve the trade-off between utility and forgetting in the context of text classification and question answering with LLMs. We further validate our method using multiple LLMs to highlight the scalability of our framework and provide detailed insights into the choice of hyperparameters and the influence of the size of unlearning data. Our implementation is available at https://github. com/karuna-bhaila/llm_unlearning.\n5\narXiv:2406.12038v2\n# 1 Introduction\nWith advancements in transformer models (Vaswani et al., 2017) and the availability of massive text corpus, language models have rapidly evolved over the past decade. The pre-train and fine-tune pipeline has garnered wide popularity, especially since the release of LLMs such as GPT (OpenAI, 2024) and LLaMA (Touvron et al., 2023). However, ethical and security concerns have been raised due to the inclusion of private and sensitive information in the training data. For\nexample, LLMs can regurgitate individual personal information (Nasr et al., 2023), or mimic harmful and/or hateful behavior as a consequence of such content being prevalent in the data (Wen et al., 2023). The non-consented and unwarranted use of copyrighted content for LLM training has also raised significant concerns (Eldan and Russinovich, 2023; Grynbaum and Mac, 2023). Current policies governing the use and distribution of such models do not encompass all ethical avenues; nonetheless, certain regulations such as California Consumer Privacy Act (CCPA) and GDPR\u2019s Right to be Forgotten (RTBF) serve as guidelines for organizations to ensure that their operations do not infringe upon user privacy. Specifically, these regulations stipulate that businesses and data collectors provide and exercise an optout mechanism essentially allowing individuals to request the deletion of their data on reasonable grounds. In machine learning literature, these regulations have been conceptualized as machine unlearning (Cao and Yang, 2015; Bourtoule et al., 2021), which aims to eliminate the influence of unwanted data points on a model\u2019s behavior as if they had never been observed during training. Naturally, machine unlearning should be integrated into the LLM pipeline to address the previously outlined issues resulting from the presence of sensitive data in pre-training. However, unlearning in LLMs faces unique challenges due to the inaccessibility of model and pre-training data, and the sheer size of the pre-trained LLMs making re-training practically infeasible. Much of the research in this direction therefore focuses on the fine-tuning approach which involves training all or a subset of LLM parameters to enforce unlearning (Jang et al., 2023; Chen and Yang, 2023; Yao et al., 2024b; Maini et al., 2024; Yao et al., 2024a). In this work, we propose a novel approach to unlearning in LLMs via soft prompting which is less resource-intensive than fine-tuning. To the best\nof our knowledge, this is the first work to investigate the use of soft prompting for unlearning in LLMs. Soft prompting simplifies the process of adapting LLMs to an arbitrary downstream task by optimizing learnable token embeddings that encode signals from a corresponding dataset (Lester et al., 2021; Li and Liang, 2021). The soft prompts are trained end-to-end and essentially act as instructions for a frozen pre-trained LLM during inference. We leverage this ability to modulate LLM outputs using prompts and formulate Soft Prompting for Unlearning (SPUL), a resource-efficient mechanism to achieve LLM unlearning in text classification and multiple-choice question answering (MCQA). We optimize a set of soft prompt parameters that learn to encode underlying information in the data relevant for unlearning. When prepended to the input tokens of an LLM during inference, the soft prompts guide the LLM towards a generic response. We implement a multi-objective loss aligned with specific unlearning goals to facilitate the learning of soft prompts. SPUL unlearns undesirable outcomes without updating large-scale LLM parameters and can fully capitalize on the language understanding capability offered by the pre-trained LLMs. Consequently, our framework can utilize the same pre-trained LLM for different unlearning tasks and datasets during inference. We evaluate SPUL for sentiment classification and MCQA tasks on benchmark NLP datasets when unlearning a subset from the corresponding training dataset and compare against various finetuning-based methods. We show that SPUL can effectively induce forgetting during inference while preserving the pre-trained utility with significant improvements over baselines. We conduct experiments to analyze the influence of SPUL hyperparameters including the contribution of loss components and the size of the soft prompts. We further validate SPUL on multiple pre-trained LLMs of different parameter sizes and different sizes of unlearning sets.\n# 2 Related Work\n# 2.1 Soft prompting\nSoft prompting or prompt tuning emerged as a lightweight alternative to fine-tuning while keeping pre-trained LLM parameters frozen. Motivated by discrete prompts that guide pre-trained LLMs via task-specific instructions or demonstration examples, soft prompting makes prompt design more\nefficient by employing trainable prompt parameters. The idea was conceived by Lester et al. (2021); they added trainable continuous embeddings to the encoder input sequence of an LLM and showed that the learned prompts achieve performance comparable to fine-tuning on NLP classification tasks with models having over 10B parameters. Simultaneously, Li and Liang (2021) developed the notion of prefix tuning which prepends task-specific prefixes to the input embeddings along with the encoder and decoder inputs of an autoregressive LM and showed that their method is comparable to finetuning approaches for text generation tasks. Liu et al. (2021) concatenated trainable continuous prompts with discrete prompts along with a prompt encoder module that maps prompts to model inputs to improve performance on supervised and fewshot tasks. Subsequent research showed that deep prompt tuning achieves comparable performance to fine-tuning across several tasks on models of varying scales by inserting tunable parameters into every LLM layer (Liu et al., 2022a).\n# 2.2 Unlearning in LLMs\nMachine unlearning arose as a promising solution to address data protection guidelines by efficiently forgetting training samples corresponding to unlearning requests in place of costly retraining (Bourtoule et al., 2021; Cao and Yang, 2015; Liu et al., 2022b; Guo et al., 2020; Sekhari et al., 2021; Golatkar et al., 2020). In the context of LLMs, machine unlearning is quickly gaining prominence due to concerns stemming from bias, toxicity, and privacy (Si et al., 2023; Liu et al., 2024). Some works in this direction emphasize model parameter optimization via gradient ascent (Jang et al., 2023; Chen and Yang, 2023; Yao et al., 2024b; Maini et al., 2024; Yao et al., 2024a) to unlearn unwanted responses for specific examples or datasets. They also fine-tune the model with various knowledge alignment objectives to maintain model utility. Other works leverage parameter optimization via relabeling of unlearning data. For instance, Eldan and Russinovich (2023) unlearn Harry Potter content by fine-tuning the model via gradient descent to replace the model\u2019s response for queries related to Harry Potter with outputs containing generic translations. In contrast to these works, Jia et al. (2024) utilize similar fine-tuning objectives but focus on optimizer selection and propose a framework that performs influence-based model updates via second-order optimization. Additionally, some\nworks propose localization-based objectives that aim to identify a subset of model units that represent information about unlearning data and effectively delete them (Meng et al., 2022; Yu et al., 2023; Wu et al., 2023). A few works also focus on modifying LLM input sequences to promote unlearning for black-box LLMs but are limited in the size of data that can be unlearned. For instance, Pawelczyk et al. (2023) perform in-context unlearning by crafting input comprised of unlearn samples paired with flipped labels and other demonstrations with correct labels. Thaker et al. (2024) investigate guardrail techniques for unlearning by instructing models to withhold unwanted knowledge or filtering undesirable LLM outputs. Unlike most fine-tuning-based approaches, our goal in this work is to develop a soft prompting strategy to facilitate unlearning in LLMs. We aim to modulate LLM behavior using prompts similar to input modification strategies. However, instead of specifying manual instructions or providing demonstration samples as context, we leverage soft prompting to automate prompt optimization while adhering to unlearning objectives through loss specifications.\n# 3 Soft Prompting for Unlearning\n# 3 Soft Prompting for Unlearning 3.1 Soft Prompting\n# 3.1 Soft Prompting\nLet D = {si, yi}N i=1 denote a dataset containing N input-output pairs where si is a text sequence containing ni tokens and yi is the corresponding output. Also, let h\u03b8 represent a pre-trained LLM with parameters \u03b8; h\u03b8 can be prompted with si to obtain an output \u02c6yi. Assume xi \u2208Rni\u00d7d denotes the token embeddings obtained for an arbitrary text sample si from the embedding module of h\u03b8 where d is the dimension of the embedding space. We first define p prompt tokens as \u03d5 = {\u03d51, \u00b7 \u00b7 \u00b7 , \u03d5p} where \u03d5i \u2208Rd. To adapt h\u03b8 over D using soft prompts, \u03d5 is appended to xi to form the sequence {\u03d5; xi} \u2208R(p+ni)\u00d7d as input to the encoder or decoder in h\u03b8. During backpropagation, the pretrained parameters \u03b8 are frozen and gradient updates are applied only to \u03d5 when maximizing the likelihood of the output yi as\n(1)\nThe size of the learnable prompts \u03d5 is very small compared to that of the pre-trained parameters \u03b8. Nonetheless, soft prompting has shown considerable performance over various language tasks with\nresults comparable to fine-tuning. This motivates us to consider whether we can achieve unlearning in LLMs by optimizing continuous prompt tokens.\n# 3.2 Problem Formulation\nGiven a training dataset Dtr that was observed during pre-training of h\u03b8, we assume a forget set, Dtr f \u2282Dtr, as the data intended for forgetting/removal from h\u03b8. Simultaneously, we define a retain set Dtr r = Dtr \\ Dtr f comprising the remaining samples. Then, the goal of unlearning is to forget the token sequences in Dtr f while maintaining inference utility on Dtr r . For our work, we focus on the task of text classification and question answering and interpret unlearning as the forgetting of the predictive output token sequences yi \u2208Dtr f . Essentially, we de-correlate text features and their corresponding labels for the relevant forget samples but preserve the predictive performance on the retain samples. To this end, we aim to design a soft prompting framework to obtain optimized prompt tokens that can guide the base model toward the forget and retain objectives. With our framework, we aim to address the following research questions. RQ1: How can soft prompting be utilized to effectively unlearn subsets of training data in the text classification/QA domain? RQ2: How can soft prompting be implemented to achieve utility preservation with forgetting? RQ3: How efficient is soft prompting-based unlearning compared to fine-tuning and re-training?\n# 3.3 Method\nAs soft prompts can be trained to encode signals from a dataset with the purpose of adapting a pretrained LLM to a specific downstream task, we anticipate that the strategy can also be utilized to encode relevant information from an unlearning dataset containing forget and retain samples. Here, we propose the framework SPUL that leverages soft prompting to obtain effective prompt tokens \u03d5 from an unlearning dataset Dtr. Since one of the unlearning objectives in our framework is to promote feature and text de-correlation for forget samples, we design a loss attuned to enforcing incorrect predictions for the respective text inputs. Specifically, we force the model to associate each input forget text sequence with a generic output token instead of its true label. We construct a generic label set \u00afY that is disjoint from the task labels and contains tokens such as neutral, unknown, or None\nand define a loss over the forget inputs,\n(2)\nwhere \u00afyi denotes a uniform random sample drawn from the pre-defined generic label set \u00afY , and l(\u00b7) refers to the standard cross-entropy loss. Ideally, Lf allows the prompt tokens \u03d5 to capture specific nuances from the samples in Dtr f and consequently guide the LLM to change its predictive sequence for an arbitrary example containing the learned distinctions. Simultaneously, unlearning also aims to preserve the predictive performance for samples not included in the forget set. In our SPUL framework, the prepended prompt tokens \u03d5 should not change the predictive sequences for xj \u2208Dtr r . Therefore, to preserve inference utility on the retain set, we define a loss using their true labels as\n(3)\nwhere l(\u00b7) again represents the cross-entropy loss. Lr ensures that the model\u2019s utility on the retain set does not degrade with the addition of prompt tokens. In addition to maintaining performance on the retain set, the model after unlearning should closely resemble the model before unlearning. In our framework, we constrain the predictive distribution of the model such that h\u03b8({\u03d5, xj}) reflects h\u03b8(xj) for any xj \u2208Dtr r . We quantify this difference using KL divergence as\n(4)\nwhere KL(\u00b7) denotes the KL divergence term. h\u03b8({\u03d5, xj}) represents the base model\u2019s predictive distribution conditioned on inputs prepended with the learnable prompt tokens and h\u03b8(xj) refers to the output distribution conditioned only on the input text sequence. We utilize Lkl in addition to Lr to avoid large deviations in the base model\u2019s output due to the influence from Lf. Finally, at each time step t during training, we update \u03d5 by optimizing the overall loss obtained as\n(5)\nwhere \u03b1 and \u03b2 are hyperparameters that specify the contribution of the respective loss components.\nDataset\nDtr\nf\nDtr\nr\nDte\nf\nDte\nr\nSST-2\n1425\n46331\n610\n19855\nYelp polarity\n5081\n95012\n885\n18089\nWMDP+SciQ\n900\n12679\n373\n1000\n# 4 Experiments\n# 4 Experiments 4.1 Experimental Setup\n# 4.1 Experimental Setup\nDatasets We evaluate SPUL on standard NLP datasets SST-2 (Socher et al., 2013) and Yelp polarity (Zhang et al., 2015) for the task of sentiment classification and datasets WMDP (Li et al., 2024) and SciQ (Welbl et al., 2017) for the task of multiple-choice question answering. SST-2 and Yelp contain reviews with each text sequence being labeled as a positive or negative sentiment. SciQ consists of science exam questions about Physics, Chemistry, and Biology, among others in a fourway multiple-choice format where each answer choice is associated with symbols such as \u201cA\u201d, \u201cB\u201d, etc. and WMDP contains questions about hazardous knowledge in biosecurity, cybersecurity, and chemical security in the same format. To build a realistic unlearning scenario where unlearning requests from each user would likely include multiple related training samples, we preprocess the classification datasets to construct the forget and retain sets such that the forget samples are semantically similar to each other (Yelp) or refer to common entities (SST-2). For MCQA, we construct forget sets to unlearn potentially harmful information while retaining general science knowledge. For SST-2, we first perform Named Entity Recognition to identify named personalities, select a specific set of entities, and sample all related reviews to form the forget set Dtr f . The remaining reviews are consequently assigned to the retain set Dtr r . We perform a similar partitioning using the selected entities on the test set to obtain Dte f and Dte r . For Yelp, we perform k-means clustering with cosine distance on the training data to divide the reviews into semantically similar groups. We randomly select a subset of the clusters and group them to form the Dtr f and the rest as Dtr r . We utilize the same cluster centers to infer cluster identities for the test data and form the sets Dte f and Dte r accordingly. For the MCQA task, we construct the forget sets from WMDP containing questions about hazardous knowledge in biosecurity and the retain sets from SciQ with general science questions. We\nrefer to this dataset as WMDP+SciQ. The sizes of the constructed forget and retain sets are reported in Table 1.\nBaselines We assess the effectiveness of SPUL by comparing its performance against multiple SOTA parameter-tuning baselines. Gradient Ascent (GA) (Jang et al., 2023) optimizes pre-trained LLM parameters on the forget set by maximizing the cross-entropy loss in place of the standard minimization. Fine-tuning with Random Labels (RL) (Golatkar et al., 2020; Yao et al., 2024a) similarly optimizes the base model on the forget set but by enforcing convergence on random labels. We use the generic label set discussed in Section 3.3 as the random labels for RL. Gradient Ascent + KL Divergence (GA + KL) and Gradient Ascent + Descent (GA+GD) integrate parameter optimization using the retain set with GA to balance forgetting effectiveness with utility (Yao et al., 2024a). The former defines a KL-divergence constraint on the LLM\u2019s output distribution and the latter implements the standard cross-entropy loss. Note that for all four baselines, we perform full fine-tuning of the LLM following prior works based on their publicly available implementations. Settings We use LLaMA-2-7B (Touvron et al., 2023) as the base LLM to evaluate our SPUL framework. We further validate the unlearning effectiveness of our method with OPT-1.3B (Zhang et al., 2022) and LLaMA-2-13B (Touvron et al., 2023). To ensure familiarization with the unlearning dataset, we fine-tune the base LLMs on the full training dataset Dtr = Dtr f \u222aDtr r for 10 epochs on SST-2, 2 epochs on Yelp, and 5 epochs on WMDP+SciQ with a learning rate set to 0.0001 and context length to 1024 using QLoRA (Dettmers et al., 2023). We treat this fine-tuned version of the LLM as the base model for unlearning. As for the configurations of SPUL, we fix the learning rate at 0.0001 across all LLMs, and datasets and vary prompt token length p among {10, 20, 30, 40, 50}. We also vary the regularization parameters \u03b1 as {0.1, 0.5, 1.0} and \u03b2 as {0.0, 0.1, 0.5, 1.0}1. We train our unlearning framework for a total of 10 epochs. As for baseline model specifications, we follow earlier works (Yao et al., 2024a) and conduct a parameter search for the best learning rates to report the most competitive results after 1 1We note that advanced approaches, e.g., utility function, Pareto-based, and constraint-based methods, can be potentially adopted to determine values of \u03b1 and \u03b2.\nBaselines We assess the effectiveness of SPUL by comparing its performance against multiple SOTA parameter-tuning baselines. Gradient Ascent (GA) (Jang et al., 2023) optimizes pre-trained LLM parameters on the forget set by maximizing the cross-entropy loss in place of the standard minimization. Fine-tuning with Random Labels (RL) (Golatkar et al., 2020; Yao et al., 2024a) similarly optimizes the base model on the forget set but by enforcing convergence on random labels. We use the generic label set discussed in Section 3.3 as the random labels for RL. Gradient Ascent + KL Divergence (GA + KL) and Gradient Ascent + Descent (GA+GD) integrate parameter optimization using the retain set with GA to balance forgetting effectiveness with utility (Yao et al., 2024a). The former defines a KL-divergence constraint on the LLM\u2019s output distribution and the latter implements the standard cross-entropy loss. Note that for all four baselines, we perform full fine-tuning of the LLM following prior works based on their publicly available implementations.\nSettings We use LLaMA-2-7B (Touvron et al., 2023) as the base LLM to evaluate our SPUL framework. We further validate the unlearning effectiveness of our method with OPT-1.3B (Zhang et al., 2022) and LLaMA-2-13B (Touvron et al., 2023). To ensure familiarization with the unlearning dataset, we fine-tune the base LLMs on the full training dataset Dtr = Dtr f \u222aDtr r for 10 epochs on SST-2, 2 epochs on Yelp, and 5 epochs on WMDP+SciQ with a learning rate set to 0.0001 and context length to 1024 using QLoRA (Dettmers et al., 2023). We treat this fine-tuned version of the LLM as the base model for unlearning. As for the configurations of SPUL, we fix the learning rate at 0.0001 across all LLMs, and datasets and vary prompt token length p among {10, 20, 30, 40, 50}. We also vary the regularization parameters \u03b1 as {0.1, 0.5, 1.0} and \u03b2 as {0.0, 0.1, 0.5, 1.0}1. We train our unlearning framework for a total of 10 epochs. As for baseline model specifications, we follow earlier works (Yao et al., 2024a) and conduct a parameter search for the best learning rates to report the most competitive results after 1\n1We note that advanced approaches, e.g., utility function, Pareto-based, and constraint-based methods, can be potentially adopted to determine values of \u03b1 and \u03b2.\nepoch of training. All experiments are conducted on NVIDIA A100 GPUs with 40GB RAM and we report the evaluation metrics over a single run due to the resource-intensive nature of the experiments.\nEvaluation We demonstrate the efficacy of the unlearning framework by evaluating the methods based on the research questions posed in Section 3.2. To quantify how well our SPUL framework addresses RQ1, we report the accuracy and weighted F1 on the forget set, Dtr f , which signifies whether the learned soft prompts can de-correlate the text features and labels. As Dte f is composed of text sequences semantically or lexically similar to Dtr f , the prompt tokens should result in a comparable performance decline on Dte f . To evaluate SPUL based on RQ2, we report model performance on Dtr r and consequently Dte r . We emphasize the differences in the accuracy and F1 scores of the base model before and after unlearning to signify utility preservation. Finally, to answer RQ3, we report the number of training parameters and required GPU hours and compare them against baseline metrics.\n# 4.2 Experimental Results\nMain Results We include our main results with LLaMA-2-7B in Table 2. We report performance metrics for the original pre-trained LLM denoted as Vanilla and the fine-tuned base model denoted as QLoRA. We notice that the Vanilla results are considerably poorer for SST-2 compared to Yelp which validates our setup of fine-tuning the original LLM on the datasets for memorization. We attribute the difference in utility to the fact that the text sequences in Yelp are significantly longer and provide more contextual information. Nonetheless, after fine-tuning with QLoRA, the LLM\u2019s performance increases to similar margins for both datasets. QLoRA fine-tuning similarly improves LLM\u2019s predictions for the WMDP+SciQ dataset. From Table 2, we observe that SPUL significantly reduces accuracy and F1 on Dtr f compared to QLoRA demonstrating forgetting efficiency. At the same time, the difference in utility between SPUL and QLoRA for Dtr r is minimal showing that our method can promote unlearning while also preserving inference utility. Moreover, the metrics for Dte f and Dte r reflect those reported for Dtr f and Dtr r showing that the soft prompts effectively impose unlearning constraints on samples unseen during training. We observe similar performance trends for Yelp and WMDP+SciQ. Although the\n<div style=\"text-align: center;\">Table 2: SPUL Unlearning performance compared to baselines</div>\nDataset\nMethod\nTrain Retain (Dtr\nr )\nTrain Forget (Dtr\nf )\nTest Retain (Dte\nr )\nTest Forget (Dte\nf )\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\nSST-2\nVanilla\n37.50\n44.66\n31.79\n38.34\n37.51\n44.67\n29.67\n36.85\nQLoRA\n99.89\n99.89\n99.72\n99.72\n95.57\n95.57\n96.07\n96.07\nGA\n55.66\n39.80\n53.93\n37.83\n55.96\n40.16\n56.89\n41.25\nRL\n33.31\n48.08\n13.82\n22.97\n31.00\n45.56\n14.26\n24.18\nGA+KL\n55.64\n39.87\n53.96\n38.07\n55.94\n40.24\n56.89\n41.47\nGA+GD\n97.17\n97.50\n13.75\n20.58\n94.43\n94.76\n11.31\n17.18\nSPUL\n99.15\n99.39\n12.98\n22.94\n94.93\n95.24\n16.07\n27.42\nYelp\nVanilla\n89.55\n89.88\n89.29\n89.62\n90.03\n90.33\n86.89\n87.37\nQLoRA\n99.31\n99.31\n99.49\n99.49\n98.42\n98.41\n98.76\n98.76\nGA\n66.11\n63.48\n67.90\n64.62\n65.13\n62.37\n67.91\n64.24\nRL\n53.00\n67.75\n52.84\n66.78\n52.75\n67.40\n49.94\n65.01\nGA+KL\n46.85\n32.90\n50.32\n35.57\n46.27\n32.26\n51.19\n35.97\nGA+GD\n99.23\n99.42\n79.69\n86.98\n97.76\n98.00\n80.90\n88.19\nSPUL\n89.74\n93.43\n55.03\n70.48\n89.63\n93.29\n60.23\n74.69\nWMDP+SciQ\nVanilla\n47.75\n46.85\n26.78\n17.46\n46.20\n46.11\n23.59\n14.86\nQLoRA\n99.74\n99.74\n98.11\n98.11\n91.80\n91.80\n62.73\n62.83\nGA\n99.35\n99.35\n86.89\n87.44\n90.70\n90.71\n57.64\n58.66\nRL\n99.32\n99.32\n84.11\n89.57\n90.40\n90.40\n53.35\n59.54\nGA+KL\n98.84\n98.85\n67.44\n68.91\n90.20\n90.24\n49.33\n50.26\nGA+GD\n99.42\n99.42\n27.22\n13.38\n90.00\n90.02\n22.25\n8.84\nSPUL\n99.38\n99.45\n5.44\n10.20\n89.70\n89.75\n3.22\n6.07\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/97ab/97abc56d-7f22-4cd1-8ff9-9b78dbe1b40b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 1: Embedding visualization results on SST-2 with QLoRA and SP</div>\nperformance drop for Dtr f and Dte r in Yelp are not equally as large as SST-2, the forget utility with the learned tokens is significantly lesser in comparison to the base model. We conjecture that the additional context provided by descriptive Yelp reviews restricts the forgetting capacity of the LLM. Also point out that utility loss in retain sets is smaller than forget sets. We also notice that SPUL performs exceedingly well on the WMDP+SciQ with the highest differences between the retain and forget metrics. Furthermore, SPUL outperforms baseline methods by a significant margin; compared to GA and RL, which optimize model parameters based only on the Dtr f , SPUL consistently preserves inference\nutility on the retain sets with comparable or even lower metrics on the forget set. For WMDP+SciQ, both baselines underperform in the forgetting task. GA+KL and GA+GD optimize model parameters based on both Dtr f and Dtr r . However, GA+KL performs poorly on all three datasets. GA+GD performs especially well on SST-2 and WMDP+SciQ but fails to enhance forget quality on Yelp which has more descriptive reviews than SST-2. The proposed SPUL framework can however attain effective unlearning with the least loss of model utility. Among the compared methods, SPUL achieves significantly better overall trade-offs between the contrasting unlearning objectives of performance degradation and utility preservation for both tasks.\n<div style=\"text-align: center;\">Table 3: SPUL performance on SST-2 across varying \u03b1 and \u03b2 values at p = 30</div>\n\u03b1\n\u03b2\nTrain Retain (Dtr\nr )\nTrain Forget (Dtr\nf )\nTest Retain (Dte\nr )\nTest Forget (Dte\nf )\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\n0.1\n0.0\n90.84\n92.69\n9.12\n16.55\n89.50\n91.15\n10.33\n18.40\n0.1\n92.59\n93.75\n6.81\n12.62\n90.77\n91.85\n10.16\n18.29\n0.5\n96.77\n97.91\n8.70\n15.98\n93.01\n94.10\n11.15\n19.81\n1.0\n85.19\n88.00\n8.49\n15.47\n84.64\n87.19\n10.66\n19.02\n0.5\n0.0\n98.17\n98.69\n11.86\n21.17\n94.34\n94.87\n14.59\n25.07\n0.1\n97.57\n97.95\n11.09\n19.88\n94.22\n94.58\n11.97\n21.08\n0.5\n97.74\n98.35\n13.82\n24.21\n93.97\n94.57\n17.21\n29.08\n1.0\n93.87\n94.66\n11.51\n20.39\n91.62\n92.36\n14.59\n25.03\n1.0\n0.0\n97.52\n97.91\n12.14\n21.60\n94.22\n94.65\n15.57\n26.50\n0.1\n98.64\n98.96\n12.14\n21.54\n94.63\n94.97\n16.07\n27.41\n0.5\n99.15\n99.39\n12.98\n22.94\n94.93\n95.24\n16.07\n27.42\n1.0\n95.70\n96.19\n14.88\n25.75\n93.05\n93.55\n17.38\n29.18\nVisualization We also visualize model outputs to show the effectiveness of our SPUL method. We utilize outputs from the last embedding layer of the LLM and map them onto a t-SNE diagram as shown in Fig. 1. The plots represent 500 data points randomly sampled from the training dataset in SST-2 for each label. In the plots, we use colors to differentiate the retain and forget examples and use shapes to differentiate the positive and negative examples. We visualize the embeddings from QLoRA, i.e., the base model before unlearning and we observe a clear divide between the positively and negatively labeled samples in the embedding space. The retain and forget samples are clustered together within the regions defined by each label. For the t-SNE plot of SPUL, i.e., the embeddings obtained after pretending the learned soft prompts, we notice a clear separation between the retain and forget samples as indicated by the blue and orange regions in Fig. 1. This shows that the soft prompts truly capture the differences between the forget and retain sets. Moreover, the retain samples are further grouped into clusters per their labels. On the other hand, the positive and negative forget samples are mixed together. This shows that the soft prompt tokens learned by SPUL successfully guide the LLM to unlearn text and label correlation for the forget samples while preserving predictive utility on the retain set. Referring back to Table 2, SPUL metrics on Dtr f and Dte f closely resemble each other for both SST-2 and Yelp. We make similar observations for Dtr r and Dte r . Our visualization results also show that the output embeddings for forget samples are not distinguishable between labels. Compared to QLoRA visualization, model outputs for positive\nand negative retain samples are closer in the embedding space as well. As a result, in a black-box Membership Inference Attack (MIA) (Shokri et al., 2017) scenario, it would be challenging to infer whether a particular forget sample was observed during training based only on model outputs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0773/0773bef7-0491-4a7e-a5c5-6ad3473a84ec.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: SPUL performance on SST-2 across varying p at \u03b1 = 1 and \u03b2 = 1</div>\nAblation Study We conduct a series of experiments to investigate the influence of Lr and Lkl on the unlearning performance of the proposed SPUL framework and report the results in Table 3 for the SST-2 dataset. The hyperparameters \u03b1 and \u03b2 control the influence of the retain set on the learned soft prompts via losses Lr and Lkl respectively. We fix the number of prompt tokens p at 30 for all results and vary \u03b1 in {0.1, 0.5, 1.0} and \u03b2 among {0.0, 0.1, 0.5, 1.0}. From Table 3, we observe that at a fixed \u03b1, unlearning efficacy is fairly unaffected by the change in the value of \u03b2. Model utility on the retain set, however, slightly increases as \u03b2 increases from 0.0 to 0.5 as Lkl gets more significance in the overall loss. We generally observe the best retain performance at \u03b2 = 0.5. The value of \u03b1 influences performance on both forget and retain sets; higher \u03b1 values benefit retain performance by\n<div style=\"text-align: center;\">Table 4: SPUL performance on SST-2 across varying sizes of forget sets</div>\n\u03c4\nTrain Retain (Dtr\nr )\nTrain Forget (Dtr\nf )\nTest Retain (Dte\nr )\nTest Forget (Dte\nf )\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\n25%\n99.37\n99.60\n26.69\n42.07\n95.10\n95.38\n39.84\n56.22\n50%\n97.66\n98.47\n18.96\n31.78\n93.80\n94.62\n23.61\n37.60\n100%\n95.70\n96.19\n14.88\n25.75\n93.05\n93.55\n17.38\n29.18\nprioritizing utility preservation whereas lower \u03b1 values improve unlearning efficacy.\nHyperparameter Study We also study the effect of the number of prompt tokens, represented by p, on the unlearning effectiveness of SPUL. We fix both \u03b1 and \u03b2 at 1 and run experiments with p ranging from 10 to 50 on SST-2 and report results in Fig. 2. We find that inference utility on retain sets Dtr r and Dte r is largely unaffected by the different choice of p. However, we observe the most competitive forget performance at p = 30 with increasing accuracy and F1 as p increases/decreases. We speculate that the soft prompts mostly encode information from the forget set, for instance, the named entities in SST-2 whose reviews are unlearned, and ultimately instruct the LLM to misclassify examples with similar encodings. Accordingly, a larger p generally benefits our soft prompting framework as made evident by the decline in forget metrics but may require longer training for optimal performance.\nForget Set Size To demonstrate the stability of our method w.r.t. the size of forget data, we evaluate SPUL on varying sizes of the train forget set Dtr f by sub-sampling \u03c4 = {25%, 50%, 100%} of the original forget set constructed for SST-2. For the test forget set Dte f and the retain sets Dts r and Dte r , we use the same sets defined in Section 4.1 for all three configurations of Dtr f to facilitate comparison. We present the results from this experiment on SST-2 in Table 4. Our results indicate that SPUL can achieve utility preservation across differing numbers of forget samples with minimal loss as more forget samples are added to Dtr f . In contrast to the retain metrics, SPUL clearly performs better for the forget metrics when more forget samples are present in the data for SST-2. Experimental results on Yelp presented in Table 2 also highlight the robustness of SPUL against large forget sets as we assign more than 5000 samples to Dtr f . As the training data contains comparatively fewer forget samples than retain samples, having a larger Dtr f\nallows the framework to emphasize the forgetting objective thus improving the unlearning efficacy.\nResults on LLaMA-2-13B and OPT-1.3B We additionally evaluate the unlearning efficacy of our SPUL on different LLMs. In particular, we purposely choose OPT-1.3B with fewer parameters and LLaMA-2-13B with almost double the parameters compared to LLaMA-2-7B. In addition to the unlearning efficacy, this study also evaluates the scalability of our SPUL framework. We fix the hyperparameters \u03b1 and \u03b2 at 1 and p at 30 and report the results for SST-2 in Table 5. We first observe that the Vanilla inference with OPT-1.3B model performs noticeably poorer than LLaMA-27B whereas LLaMA-2-13B significantly improves over the initial metrics. This may be attributed to the pre-trained models\u2019 complexity which affects their generalization ability. We similarly perform fine-tuning using QLoRA to ensure the respective LLM has memorized the unlearning dataset. Moreover, SPUL can effectively achieve the forget and retain unlearning objectives as made evident by the low forget accuracy and F1 compared to the retain metrics that closely resemble the base model\u2019s performance. The results also indicate that the larger the LLM, the better it adapts to the unlearning task in our SPUL framework. Additionally, our results with OPT-1.3B indicate that SPUL significantly outperforms all baseline methods for both retain and forget metrics. We note that we could not run experiments on baselines with LLaMA-2-13B due to limited GPU as all baseline methods require full fine-tuning of the LLM. This further highlights the advantage of SPUL over baselines in terms of parameter efficiency.\nEfficiency For LLMs, retraining from scratch is practically infeasible due to computational time and resources required for a huge set of parameters. Although fine-tuning pre-trained LLMs incurs less costs than retraining, the cost is still high. For instance, the LLM architectures used in our experiments require gradient updates for 1.42B, 6.74B,\n<div style=\"text-align: center;\">Table 5: SPUL performance on SST-2 dataset using OPT-1.3B and LLaMA-2-13B</div>\nLLM\nMethod\nTrain Retain (Dtr\nr )\nTrain Forget (Dtr\nf )\nTest Retain (Dte\nr )\nTest Forget (Dte\nf )\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\nACC(%)\u2191\nF1(%)\u2191\nACC(%)\u2193\nF1(%)\u2193\nOPT-1.3B\nVanilla\n3.05\n5.68\n1.68\n3.20\n3.24\n6.03\n3.28\n6.08\nQLoRA\n99.47\n99.47\n99.16\n99.16\n95.39\n95.39\n95.25\n95.25\nGA\n79.50\n78.01\n70.67\n67.02\n78.70\n77.05\n71.97\n67.99\nRL\n55.66\n39.80\n53.96\n37.83\n55.96\n40.16\n56.89\n41.25\nGA+KL\n81.30\n80.15\n60.49\n50.94\n79.08\n77.60\n64.75\n56.74\nGA+GD\n87.56\n87.59\n50.53\n49.07\n86.47\n86.50\n55.25\n54.59\nSPUL\n94.87\n96.89\n16.84\n28.74\n91.65\n93.51\n17.87\n29.84\nLLaMA-2-13B\nVanilla\n61.04\n70.96\n59.65\n69.51\n60.32\n70.38\n59.18\n68.79\nQLoRA\n99.48\n99.48\n99.30\n99.30\n96.02\n96.02\n95.90\n95.90\nSPUL\n98.87\n98.93\n5.97\n11.25\n95.50\n95.60\n7.38\n13.54\nand 13B parameters for OPT-1.3B, LLaMA-2-7B, and LLaMA-2-13B respectively when implementing unlearning based on fine-tuning. When p = 30, our SPUL reduces the computation cost by only optimizing 604K, 1.19M, and 1.49M parameters while freezing LLM parameters. Further increasing p only linearly scales the number of training parameters. We also look at the running time of SPUL on the SST-2 compared against baseline methods and find the execution time required by each model of SPUL, GA + KL, and GA+GD for one training epoch is fairly similar, around 1020 GPU seconds, as SPUL also accesses LLM parameters during backpropagation. GA and RL methods are much quicker with approximate 40 GPU seconds of per epoch training time as these methods only consider the forget set. Nonetheless, SPUL avoids the overhead associated with updating LLM parameters, making it more resource-efficient.\n# 5 Conclusion\nIn this work, we investigate unlearning in LLMs to remove the influence of unwanted training examples during text classification and multiple-choice question answering. We present a soft prompting strategy to unlearn subsets of training data while keeping pre-trained LLM parameters frozen to maintain the model\u2019s generalizability. Our SPUL framework optimizes a small number of prompt tokens using a multi-objective loss function defined on disjoint training data subsets representing the forget data that is subjected to removal and the retain data that aims to preserve model utility. Experimental evaluation on sentiment classification and QA datasets demonstrates the superior efficiency of our soft prompting-based unlearning over fine-tuning-based baselines. We also empirically show that SPUL can adapt to multiple LLMs and\nis robust to a high number of unlearning samples\n# Acknowledgements\nThis work was supported in part by NSF grants 1946391 and 2119691.\nLimitations\nWe address the limitations of this work in the following. Our experiments primarily focus on opensource LLMs as the soft prompting framework requires access to frozen pre-trained parameters to compute gradients for the soft prompts despite not needing to update the LLM parameters. Furthermore, this work focuses on the task of text classification, specifically sentiment classification, and question answering for the formulation of the unlearning framework and evaluation. Future research could explore the efficiency of soft prompting to achieve unlearning in the context of NLP tasks such as text generation, text summarization, and so on. Also, the soft prompting unlearning framework has not been evaluated comprehensively as we emphasize performance metrics to demonstrate unlearning efficacy. We note that there is a lack of an extensive evaluation pipeline for LLM unlearning in the current literature. Further research is needed to evaluate the robustness of the framework subject to model-stealing attacks, MIAs, and jailbreaking attempts.\n# Broader Impacts\nIn this study, our focus is to achieve LLM unlearning in a resource-efficient manner. We aim to enable forgetting of unwanted and undesirable knowledge as per users\u2019 requests while maintaining model efficiency to avoid exploitation of sensitive information. The datasets used for evalua-\ntion are publicly available and implemented within the intended use. Our usage of publicly available pre-trained LLMs also adheres to the associated licenses. We hope our study can further the research and literature on resource-efficient LLM unlearning.\n# References\n# Ronen Eldan and Mark Russinovich. 2023. Who\u2019s harry potter? approximate unlearning in llms. CoRR, abs/2310.02238.\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9301\u20139309. Computer Vision Foundation / IEEE.\nMicheal Grynbaum and Ryan Mac. 2023. The times sues openai and microsoft over a.i. use of copyrighted work. The New York Times.\nChuan Guo, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten. 2020. Certified data removal from machine learning models. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3832\u20133842. PMLR.\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon\nSeo. 2023. Knowledge unlearning for mitigating privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 14389\u201314408. Association for Computational Linguistics.\nJinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, and Sijia Liu. 2024. Soul: Unlocking the power of second-order optimization for llm unlearning. Preprint, arXiv:2404.18239.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 3045\u2013 3059. Association for Computational Linguistics.\nNathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin TienkenHarder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, and Dan Hendrycks. 2024. The wmdp benchmark: Measuring and reducing malicious use with unlearning. Preprint, arXiv:2403.03218.\nsociation for Computational Linguistics. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. CoRR, abs/2103.10385. Yi Liu, Lei Xu, Xingliang Yuan, Cong Wang, and Bo Li. 2022b. The right to be forgotten in federated learning: An efficient realization with rapid retraining. In IEEE INFOCOM 2022 - IEEE Conference on Computer Communications, London, United Kingdom, May 2-5, 2022, pages 1749\u20131758. IEEE. Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. 2024. TOFU: A task of fictitious unlearning for llms. CoRR, abs/2401.06121. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. 2023. Scalable extraction of training data from (production) language models. CoRR, abs/2311.17035. OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. 2023. In-context unlearning: Language models as few shot unlearners. Preprint, arXiv:2310.07579. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. 2021. Remember what you want to forget: Algorithms for machine unlearning. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 18075\u201318086. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 3\u201318. IEEE Computer Society. Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. 2023. Knowledge unlearning for llms: Tasks, methods, and challenges. CoRR, abs/2311.15766. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. CoRR, abs/2103.10385.\n# OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1631\u20131642. ACL.\n# Pratiksha Thaker, Yash Maurya, and Virginia Smith. 2024. Guardrail baselines for unlearning in llms. CoRR, abs/2403.03329.\nPratiksha Thaker, Yash Maurya, and Virginia Smith. 2024. Guardrail baselines for unlearning in llms. CoRR, abs/2403.03329.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. CoRR, abs/1707.06209.\n# Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. CoRR, abs/1707.06209.\nJiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang. 2023. Unveiling the implicit toxicity in large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1322\u2013 1338. Association for Computational Linguistics.\nXinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. 2023. DEPN: detecting and editing privacy neurons in pretrained language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 2875\u20132886. Association for Computational Linguistics.\nYuanshun Yao, Xiaojun Xu, and Yang Liu. 2024b. Large language model unlearning. Preprint, arXiv:2310.10683.\nCharles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 6032\u20136048. Association for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. Preprint, arXiv:2205.01068. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649\u2013657.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of ethical and safety considerations in deploying Large Language Models (LLMs), particularly focusing on machine unlearning motivated by data protection regulations. Existing methods primarily rely on fine-tuning approaches that are resource-intensive, highlighting the necessity for more efficient alternatives.",
        "problem": {
            "definition": "The problem focuses on the challenge of removing the influence of unwanted training examples from LLMs while maintaining their generalizability.",
            "key obstacle": "The main difficulty lies in the inaccessibility of model parameters and the impracticality of retraining large-scale LLMs, making existing fine-tuning methods inefficient for unlearning."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for an efficient mechanism to achieve unlearning in LLMs without the overhead of full model retraining.",
            "opinion": "The proposed method, Soft Prompting for Unlearning (SPUL), leverages soft prompting to optimize prompt tokens that guide the model towards unlearning specific examples during inference.",
            "innovation": "SPUL innovates by allowing unlearning through the use of soft prompts that do not require updating the LLM parameters, thus providing a lightweight alternative to traditional fine-tuning methods."
        },
        "method": {
            "method name": "Soft Prompting for Unlearning",
            "method abbreviation": "SPUL",
            "method definition": "SPUL is a framework that learns prompt tokens to induce unlearning of specific examples in LLMs without modifying the underlying model parameters.",
            "method description": "SPUL uses a set of learnable prompt tokens that are appended to input queries, guiding the LLM to forget certain training examples while preserving performance on others.",
            "method steps": [
                "Define the forget and retain sets from the training data.",
                "Train soft prompt tokens by optimizing a multi-objective loss that balances forgetting and utility preservation.",
                "Apply the learned prompt tokens to input queries during inference to induce unlearning."
            ],
            "principle": "The effectiveness of SPUL stems from its ability to modulate the output of the LLM using soft prompts that enforce forgetting while maintaining general performance on retained data."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on standard NLP datasets including SST-2 for sentiment classification and WMDP+SciQ for multiple-choice question answering, comparing SPUL against various fine-tuning baselines.",
            "evaluation method": "Performance metrics such as accuracy and F1 scores were used to assess the effectiveness of SPUL in unlearning tasks, alongside comparisons to baseline methods."
        },
        "conclusion": "The experimental results demonstrate that SPUL effectively achieves unlearning in LLMs with minimal loss of utility, outperforming traditional fine-tuning methods and showcasing the framework's adaptability to various models and datasets.",
        "discussion": {
            "advantage": "SPUL provides a resource-efficient method for unlearning in LLMs, significantly reducing computational costs compared to full fine-tuning approaches.",
            "limitation": "The method has only been evaluated on open-source LLMs, and its applicability to other NLP tasks beyond text classification and question answering remains to be explored.",
            "future work": "Future research could investigate the use of SPUL in other NLP applications such as text generation and summarization, as well as its robustness against model-stealing attacks and other security concerns."
        },
        "other info": {
            "acknowledgements": "This work was supported in part by NSF grants 1946391 and 2119691.",
            "limitations": "The soft prompting framework requires access to frozen pre-trained parameters, limiting its application to open-source models."
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The paper addresses ethical and safety considerations in deploying Large Language Models (LLMs), highlighting the significance of in-context learning in ensuring data protection."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Soft Prompting for Unlearning (SPUL), demonstrates how LLMs can adapt to contexts by optimizing prompt tokens to guide the model towards unlearning specific examples."
        },
        {
            "section number": "3.4",
            "key information": "SPUL explores how LLMs utilize memory and adapt to context to facilitate in-context learning by applying learned prompt tokens during inference."
        },
        {
            "section number": "6.1",
            "key information": "The main difficulty identified in the paper is the inaccessibility of model parameters and the impracticality of retraining large-scale LLMs, which relates to model bias and context sensitivity in in-context learning."
        },
        {
            "section number": "6.2",
            "key information": "The discussion highlights that SPUL provides a resource-efficient method for unlearning in LLMs, significantly reducing computational costs compared to full fine-tuning approaches."
        },
        {
            "section number": "6.4",
            "key information": "The limitation of SPUL requiring access to frozen pre-trained parameters restricts its scalability and applicability to only open-source models."
        }
    ],
    "similarity_score": 0.6963279954828864,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Soft Prompting for Unlearning in Large Language Models.json"
}