{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.01119",
    "title": "Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models",
    "abstract": "The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.",
    "bib_name": "kaddour2024syntheticdatagenerationlowresource",
    "md_text": "# Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models\nJean Kaddour University College London\n8 Jan 2024\nThe in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model\u2019s performance, occasionally necessitating only a minor fraction of the original training dataset.\n# 1 Introduction\nLarge language models (LLMs) have demonstrated in-context learning (ICL) capabilities in various natural language processing tasks, which allow us to perform an unseen downstream task by prompting the model with a collection of input-target pairs and a single unlabeled example [8]. Crucially, ICL requires relatively few labeled examples but large model sizes [37]. However, deploying LLMs in real-world systems is challenging due to their computational costs and inference latency [21]. An alternative paradigm that enables good results with much smaller models is to specialize a pretrained model for a single task through gradient-based supervised fine-tuning (SFT) [11, 12]. The drawback of this approach is that it relies on labeled examples, which require human annotators and, therefore, is expensive and time-consuming. Especially in low-resource settings with only a handful of examples, SFT can be challenging [39]. In this work, we attempt to yield the best of both worlds by fine-tuning smaller models with training data generated by an LLM. Following recent work on training data generation [32, 27, 21], we show that by (i) annotating unlabeled examples or (ii) generating entirely new ones, we can effectively transfer knowledge from the LLM (teacher) to the specialized model (student), which can be several magnitudes of orders smaller, akin to knowledge distillation [18] but only via the exchange of data. We find that synthetic data generation after fine-tuning the teacher LLM, even on only extremely limited data, improves the synthetic data quality as measured by the downstream model generalization performance. For example, fine-tuning a 20B LLM on as few as 125 examples (5% of the RTE dataset [34]) increases the augmented-data-fine-tuned downstream model\u2019s performance by multiple percentage points. We empirically verify our approach on four text classification and two natural language generation tasks, finding that both (i) and (ii) consistently improve the student model\u2019s downstream performance. We provide ablation studies on varying amounts of synthetic data, comparisons with GPT3.5 [8] as a teacher model, and evaluate the teacher LLMs directly on the downstream tasks.\nPreprint. Under review.\nQi Liu University of Hong Kong\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f840/f840bf19-e789-4328-b6c9-4159401fd02a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Synthetic Data Generation via Fine-Tuning the Teacher LLM: Step 1: We fine-tune (FT) an LLM (GPT-NeoX-20B [6]). Step 2: We either annotate unlabeled instances or generate entirely new ones. Step 3: We train a small downstream model on the augmented training dataset.</div>\n# 2 Method\n# 2.1 Problem Definition\nConsider a task-specific and labeled dataset DT := {(xi, yi)}N i=1 of text input-output pairs for some task T , a teacher LLM paratemerized by \u03b8T and lightweight student model \u03b8S, with |\u03b8S| \u226a|\u03b8T|. We aim to yield \u03b8S that performs similarly to \u03b8T in solving T . The premise is that \u03b8T performs much better than \u03b8S but is more expensive to deploy. Our strategy is to generate synthetic data \ufffdDT using \u03b8T, and then fine-tune \u03b8S on DT \u222a\ufffdDT . We further distinguish the cases of (i) annotating an unlabeled set of instances {xi}M i=1 such that \ufffdDT := {(xi, \u02c6yi)}M i=1, or (ii) generating entire input-output pairs \ufffdDT := {(\u02c6xi, \u02c6yi)}M i=1, where \u02c6xi, \u02c6yi refer to synthetic input and output sequences, respectively. (i) is relevant in scenarios where abundant unlabeled text is available (e.g., scraped from the internet), but obtaining annotations is expensive. In turn, (ii) is suitable even in situations without unlabeled instances.\n# 2.2 Teacher model\nThe teacher LLM is the 20B GPT-NeoX [6], which, at the time of our experimentation phase, was considered state-of-the-art 1. We adopt the text-to-text framework [28], where the LLM receives a text query as input and produces text output. For instance, when generating dialog responses, the query corresponds to incomplete dialog utterances, and the model generates the response. As for classification tasks, the query corresponds to the textual input, and the model generates the class label. Further, we fine-tune GPT-NeoX for multi-task instruction-following on several task mixtures, using the Super-NaturalInstructions data set [36]. Each prompt consists of a task description prefix and a small subset of training input-output pairs, separated by [INPUT] and [OUTPUT] tags. Further, depending on whether we annotate unlabeled text or generate entirely new examples, the prompt ends with either [OUTPUT] or [INPUT], respectively. We fine-tune all layers and use Adam for up to 5 epochs with a cosine schedule and a learning rate of 1e \u22125. Our code relies upon the GPT-Neox library [2], which utilizes DeepSpeed [29] to facilitate distributed training. We use NVIDIA A100 GPUs for all experiments as provided by an internal compute cluster. An important inference hyper-parameter is the generation temperature, which controls the LLM output randomness. We find that a low temperature of 0.1 works best for annotations, while a high temperature of 0.8 works best for generations (due to higher diversity in the samples).\nAn important inference hyper-parameter is the generation temperature, which controls the LLM output randomness. We find that a low temperature of 0.1 works best for annotations, while a hig temperature of 0.8 works best for generations (due to higher diversity in the samples).\n# 2.3 Student models\nFor text classification and generation tasks, we use RoBERTa-Large [26], and BART-Large [23] models, respectively. We fine-tune their publicly available checkpoints. We fine-tune up to 320 epochs for TC tasks using Adam, a batch size of 50, and a learning rate of 1e \u22125. We fine-tune with\nAdam for 5 epochs for TG tasks, using a linear learning schedule, batch size 32, and learning rate 5e \u22125. We report the results using the model with the best validation loss.\n# 3 Experiments\n# 3.1 Datasets\nWe list the dataset sizes in Table 1. One can apply the fractions in later tables to this table to calculate the number of fine-tuning samples for the teacher LLM.\nText classification We report classification accuracies across four classification tasks. SLURP (Spoken Language Understanding Resource Package) [4] is a multi-domain dataset for end-to-end spoken language understanding, consisting of single-turn user interactions with a home assistant. BoolQ [10] contains naturally occurring yes/no questions about a provided paragraph. RTE (Recognizing Textual Entailment) [33] is a dataset for recognizing whether, given two texts,\nEntailment) [33] is a dataset for recognizing whether, given two texts, the meaning of one is entailed (can be inferred) from the other. MultiRC (Multi-Sentence Reading Comprehension) [22] is a multisentence question-answering dataset, where each input consists of a short paragraph and question, which requires combining information from multiple sentences.\nNatural language generation We report Rouge-{1,2,L} scores [25] on both development and test sets across two tasks. Schema Guided Dialog (SGD) [30] includes task-oriented conversations between a human and a virtual assistant. Here, we follow Gehrmann et al. [16] and construct the task of generating an utterance given an unfinished dialog as input. WebNLG is a data-to-text [15] task, which aims at generating a text that verbalizes the input triples grammatically correct.\n# 3.2 Main results\nSection 3.2 shows the main results. We calculate the data amounts relatively w.r.t. the original training dataset size. For example, an entry with 1% original and 10% synthetic data means we construct the augmented training dataset by concatenating 1% of data randomly sampled from the original training dataset and 10% synthetic data. We observe that both data annotations and generations improve performance across all scores. Interestingly, the smaller the initial training dataset, the larger the gains. This is interesting because it implies that we do not need many samples for the teacher\u2019s fine-tuning phase to succeed.\n# 3.3 Varying amount of synthetic data\nTo understand the relationship between the amount of synthetic data added and its downstream performance improvements, we vary the amount of synthetic data for the NLG tasks. Figure 5 shows the results of different augmentation amounts with 1% original training data. For SGD, we observe that up until 30%, the downstream model performance increases. However, for WebNLG, we observe diminishing returns and even slightly worse performance as the amount of synthetic data increases.\nTo understand the relationship between the amount of synthetic data added and its downstream performance improvements, we vary the amount of synthetic data for the NLG tasks.\n# 3.4 Evaluating the teacher LLMs directly\nWe report the performances of the teacher models, i.e., without generating additional data points and fine-tuning a student model, in Figure 8. Interestingly, the fine-tuned NeoX 20B model outperforms the fine-tuned davinci-002 model with 175B parameters (fine-tuned with default hyper-parameters as provided by OpenAI\u2019s API).\n# 3.5 Comparing Teacher LLMs NeoX-20B and GPT-3.5 175B\nWe investigate how effective our framework is when combined with other teacher models. W compare NeoX-20B with OpenAI\u2019s GPT-3.5 175B [8] (called davinci-002 in their API) in Figure\nDataset\n# Examples\nTrain\nDev\nTest\nSLURP [4]\n11514\n2033\n2974\nRTE [34]\n2500\n277\n300\nBoolQ [10]\n9427\n3270\n3245\nMultiRC [22]\n5100\n953\n1800\nPubMedQA [20]\n212300\n-\n1000\nSGD [30]\n164982\n10000\n10000\nWebNLG [15]\n35426\n1667\n1779\nTable 1: Dataset Statistics.\nDataset\nType\nData amount in %\nDev Acc\nTest Acc\nOriginal\nOurs\nSLURP\n[4]\n1%\n0 %\n42.25\n43.95\nX, Y\n26 %\n54.57\n54.25\nY |X\n78 %\n76.14\n76.09\n5%\n0 %\n73.49\n71.59\nX, Y\n43 %\n77.39\n76.89\nY |X\n78 %\n85.00\n83.96\n-\n10%\n0%\n80.12\n80.04\nX, Y\n43 %\n82.59\n81.91\nY |X\n43 %\n86.13\n86.48\n-\n100 %\n0 %\n88.64\n87.70\nBoolQ\n[10]\n1%\n0 %\n62.84\nN/A\nX, Y\n31 %\n68.96\nY |X\n44 %\n79.72\n5%\n0 %\n62.97\nX, Y\n31 %\n66.02\nY |X\n44 %\n80.09\n-\n10%\n0 %\n68.29\nX, Y\n31 %\n77.22\nY |X\n44 %\n81.93\n-\n100 %\n0 %\n85.05\nRTE\n[33]\n5%\n0 %\n60.65\nN/A\nX, Y\n80 %\n66.79\nY |X\n80 %\n83.20\n10%\n0 %\n65.43\nX, Y\n80 %\n69.68\nY |X\n80 %\n83.75\n-\n20%\n0 %\n74.73\nX, Y\n80 %\n76.84\nY |X\n80 %\n85.20\n-\n100 %\n0 %\n86.60\nMultiRC\n[22]\n1%\n0 %\n57.50\n8.18\nX, Y\n40 %\n63.40\n15.11\nY |X\n254 %\n71.46\n24.24\n5%\n0 %\n67.70\n18.05\nX, Y\n40 %\n72.85\n21.86\nY |X\n254 %\n71.51\n32.63\n-\n10%\n0 %\n70.63\n22.35\nX, Y\n40 %\n73.88\n24.97\nY |X\n254 %\n76.90\n34.94\n-\n100%\n0 %\n82.12\n48.16\nDataset\nType\nData amount in %\nValidation scores\nTest scores\nOriginal\nOurs\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nSGD\n[30]\n-\n1%\n0 %\n21.78\n9.29\n20.09\n21.91\n9.40\n20.06\nX, Y\n10%\n43.72\n25.09\n39.84\n40.82\n22.36\n37.00\nY |X\n47.44\n28.19\n43.47\n43.37\n24.49\n39.58\n-\n5%\n0 %\n33.17\n16.86\n30.32\n30.92\n14.86\n28.09\nX, Y\n10%\n45.27\n26.39\n41.48\n41.71\n23.25\n37.98\nY |X\n48.99\n29.87\n45.02\n44.60\n25.76\n40.66\n-\n10%\n0 %\n35.48\n18.48\n32.47\n33.40\n16.40\n30.37\nX, Y\n10%\n48.49\n29.59\n44.62\n43.89\n25.29\n40.16\nY |X\n49.31\n30.15\n45.32\n44.89\n25.94\n40.98\n-\n100%\n0 %\n57.62\n39.64\n53.63\n50.23\n32.03\n46.30\nWebNLG\n[15]\n-\n1%\n0 %\n53.94\n31.76\n42.58\n50.21\n28.44\n39.72\nX, Y\n10%\n72.99\n47.16\n56.06\n69.63\n42.99\n52.98\nY |X\n75.69\n51.18\n59.60\n70.21\n44.34\n54.51\n-\n5%\n0 %\n56.58\n35.74\n46.46\n53.17\n32.57\n44.65\nX, Y\n10%\n76.57\n51.23\n60.12\n71.63\n44.38\n54.37\nY |X\n78.02\n53.60\n61.46\n72.11\n45.50\n55.29\n-\n10%\n0 %\n59.02\n38.21\n48.37\n54.07\n32.47\n44.06\nX, Y\n10%\n77.72\n53.56\n61.60\n72.02\n45.20\n54.71\nY |X\n78.38\n54.56\n62.72\n72.73\n46.02\n55.96\n-\n100%\n0 %\n80.65\n58.50\n65.73\n73.27\n46.90\n55.76\nFigure 2: Text Classification.\nFigure 4: Comparison of Performances with and without Ours. We find that Ours is most effective in settings with the smallest amount of initial data despite the teacher LLM being fine-tuned on that small amount of data. For reference, we list the performance of the student model when fine-tuned on 100% training data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d763/d763f760-0e37-49ca-a4d8-db7dfe8bc117.png\" style=\"width: 50%;\"></div>\nDataset\nType\nData amount\nRouge-L\nOriginal\nOurs\nDev\nTest\nSGD\n-\n1%\n0%\n20.09\n20.06\nY |X\n1%\n10%\n43.47\n39.58\n20%\n45.22\n40.89\n30%\n46.02\n41.29\nX, Y\n1%\n10%\n39.84\n37.00\n20%\n40.92\n37.62\n30%\n42.27\n39.30\n-\n100%\n0%\n53.63\n46.30\nWebNLG\n-\n1%\n0%\n42.58\n39.72\nY |X\n1%\n1%\n57.59\n52.92\n2%\n58.94\n54.50\n3%\n59.44\n54.44\n4%\n59.79\n54.42\n5%\n59.56\n54.44\nX, Y\n1%\n1%\n57.09\n52.81\n2%\n57.00\n52.29\n3%\n57.08\n52.64\n4%\n56.92\n52.72\n5%\n56.92\n52.76\n-\n100%\n0%\n65.73\n55.76\nFigure 5: Different amounts of Ours data added to the student model training set. While both the SGD and WebNLG tasks benefit from the synthetic data, we observe diminishing returns.\nFigure 3: Natural Language Generation.\nDataset\nType\nData amount\nRouge-L\nOriginal\nOurs\nDev\nTest\nSGD\n-\n1%\n0%\n20.09\n20.06\nY |X\n1%\n10%\n43.47\n39.58\nX, Y\n1%\n10%\n39.84\n37.00\nY |X; X, Y\n1%\n5% each\n42.46\n39.18\nWebNLG\n-\n1%\n0%\n42.58\n39.72\nY |X\n1%\n10%\n59.60\n54.51\nX, Y\n1%\n1%\n56.06\n52.98\nY |X; X, Y\n1%\n5% each\n59.41\n54.17\nFigure 6: Combining Annotation (Y |X) and Generation (X, Y ) equally performs almost as well as annotating the same amount, while the latter assumes access to unlabeled instances, which can be difficult in practice.\nModel\nDev\nTest\nRouge 1\nRouge 2\nRouge L\nRouge 1\nRouge 2\nRouge L\ndavinci-002\n35.47\n17.99\n32.35\n34.62\n17.19\n31.46\nNeoX\n47.44\n28.19\n43.47\n43.37\n24.49\n39.59\nFigure 7: Comparing the Teacher LLMs: We report the student LM\u2019s downstream performance after being fine-tuned on 11% of the original training dataset size, where 1% is the original data (also used to fine-tune the teacher LLMs) and 10% teacher-annotated examples. The task is SGD.\nModel\nDev\nTest\nRouge 1\nRouge 2\nRouge L\nRouge 1\nRouge 2\nRouge L\ndavinci-002\n30.71\n14.38\n28.06\n30.54\n14.14\n27.70\nNeoX\n38.38\n20.84\n35.44\n37.78\n20.13\n34.58\ngure 8: Evaluating the Teacher LLMs on SGD after being fine-tuned with 1% training data.\nThe fine-tuned NeoX model consistently produces better training data as measured in the student LM\u2019s downstream performance on the tasks\u2019 test sets. This is an interesting result, given the difference in model size.\n# 3.6 Combining annotation and generation\nIn Section 3.2, we observe that annotations of existing, unlabeled data yield more significant gains than generating entire input-output pairs. In many real-world settings, however, access to unlabeled data may still be more costly than generating data points entirely de novo. Hence, we want to investigate whether adding generated data points to annotated ones in an equal amount can yield performances closer to a larger set of annotated ones while potentially being much cheaper in practice. In the setting of 1% original training data, we confirm this hypothesis in the affirmative, as shown in Figure 6. When mixing both data sources with 5% each, we yield almost the same performance as if we had annotated 10% of the whole data set.\n# 3.7 Qualitative Analysis of Generated Text\nIn Appendix A, we showcase both annotated instances as well as entirely generated ones. By manually inspecting and verifying them, we find all generated examples coherent and syntactically correct. However, not all instances are factually truthful, a phenomenon often referred to as \u201challucination\u201d. A post-hoc factuality filtering process of the generated examples could be an exciting avenue for future work [24].\n# 4 Related Work\nThere is extensive literature on synthetic data augmentation for text data leveraging large teacher models [5, 21]. However, most of these works do not fine-tune the teacher LLM on limited data to improve the quality of the generated data. Efrat & Levy [14] examine a model\u2019s ability to follow natural language instructions, including annotating unlabeled dataset examples. Schick & Sch\u00fctze [32] study synthetic data generation on semantic textual similarity datasets without task-specific training examples. Similarly, other work has focused on information retrieval [7], code generation [3], and reasoning tasks [19]. Yoo et al. [38] propose to transfer knowledge from LLMs to student models by generating synthetic examples and knowledge distillation using soft labels. Wang et al. [35] explore GPT-3 as a low-cost data labeler to train other models. For NLG and NLU tasks, they find that it costs 50% to 96% less to use GPT-3 generated labels than human annotations. Similarly, Ding et al. [13] evaluates GPT-3\u2019s effectiveness as a data annotator on classification and named entity recognition tasks.\nChen et al. [9], Zheng et al. [40], Gunasekar et al. [17], Li et al. [24] follow a similar approach to ours, augmenting pre-training data with synthetically generated documents. For example, Gunasekar et al. [17] generate textbooks and exercises to train a comparatively small model, which outperforms much bigger ones on coding tasks. In contrast, we consider the fine-tuning phase and classification and natural language generation tasks, similar to the work by Sahu et al. [31], Chen et al. [9] but with different tasks. In spirit, our approach is similar to knowledge distillation (KD) [18], where one uses the teacher model\u2019s output logits as targets when training the student model. However, state-of-the-art LLMs are often served via cloud-based commercial APIs, exposing only a truncated output distribution (e.g., top-5 tokens). In contrast, our synthetic data generation approach does not require the teacher\u2019s output distribution.\nChen et al. [9], Zheng et al. [40], Gunasekar et al. [17], Li et al. [24] follow a similar approach to ours, augmenting pre-training data with synthetically generated documents. For example, Gunasekar et al. [17] generate textbooks and exercises to train a comparatively small model, which outperforms much bigger ones on coding tasks. In contrast, we consider the fine-tuning phase and classification and natural language generation tasks, similar to the work by Sahu et al. [31], Chen et al. [9] but with different tasks.\nIn spirit, our approach is similar to knowledge distillation (KD) [18], where one uses the teacher model\u2019s output logits as targets when training the student model. However, state-of-the-art LLMs are often served via cloud-based commercial APIs, exposing only a truncated output distribution (e.g., top-5 tokens). In contrast, our synthetic data generation approach does not require the teacher\u2019s output distribution.\n# 5 Conclusion, Limitations and Future Work\nIn this work, we have elucidated that fine-tuning teacher LLMs to both annotate unlabeled instances and generate new data points can effectively improve a downstream model\u2019s performance. Our empirical investigations spanned six tasks, four in classification and two pertaining to natural language generation. A potential constraint of our approach is that fine-tuning a large model necessitates significant resources. In future work, we aim to delve deeper into quantifying the fine-tuning required to steer the teacher model towards producing high-quality synthetic data.\n# References\n# [1] Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4, 2023. URL https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.\n[2] Andonian, A., Biderman, S., Black, S., Gali, P., Gao, L., Hallahan, E., Levy-Kramer, J., Leahy, C., Nestler, L., Parker, K., Pieler, M., Purohit, S., Songz, T., Phil, W., and Weinbach, S. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, August 2021. URL https://www.github.com/eleutherai/gpt-neox. [3] Azerbayev, Z., Ni, A., Schoelkopf, H., and Radev, D. Explicit knowledge transfer for weaklysupervised code generation, 2022. URL https://arxiv.org/abs/2211.16740. [4] Bastianelli, E., Vanzo, A., Swietojanski, P., and Rieser, V. Slurp: A spoken language understanding resource package, 2020. URL https://arxiv.org/abs/2011.13205. [5] Bayer, M., Kaufhold, M.-A., and Reuter, C. A survey on data augmentation for text classification. ACM Computing Surveys, 55(7):1\u201339, 2022. [6] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. [7] Bonifacio, L., Abonizio, H., Fadaee, M., and Nogueira, R. Inpars: Data augmentation for information retrieval using large language models, 2022. URL https://arxiv.org/abs/ 2202.05144. [8] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [9] Chen, M., Papangelis, A., Tao, C., Rosenbaum, A., Kim, S., Liu, Y., Yu, Z., and HakkaniTur, D. Weakly supervised data augmentation through prompting for dialogue understanding. In NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research, 2022. URL https://openreview.net/forum?id=r2_9r7seD-q. 10] Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n[11] Dai, A. M. and Le, Q. V. Semi-supervised sequence learning, 2015. URL https://arxiv. org/abs/1511.01432. [12] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018. URL https://arxiv.org/abs/1810. 04805. [13] Ding, B., Qin, C., Liu, L., Bing, L., Joty, S., and Li, B. Is gpt-3 a good data annotator?, 2022. URL https://arxiv.org/abs/2212.10450. [14] Efrat, A. and Levy, O. The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982, 2020. [15] Gardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. Creating training corpora for nlg micro-planning. In 55th annual meeting of the Association for Computational Linguistics (ACL), 2017. [16] Gehrmann, S., Adewumi, T., Aggarwal, K., Ammanamanchi, P. S., Anuoluwapo, A., Bosselut, A., Chandu, K. R., Clinciu, M., Das, D., Dhole, K. D., et al. The gem benchmark: Natural language generation, its evaluation and metrics. arXiv preprint arXiv:2102.01672, 2021. [17] Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. [18] Hinton, G., Vinyals, O., Dean, J., et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. [19] Ho, N., Schmid, L., and Yun, S.-Y. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071, 2022. [20] Jin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2567\u20132577, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259. [21] Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and McHardy, R. Challenges and Applications of Large Language Models, July 2023. URL http://arxiv.org/abs/2307. 10169. arXiv:2307.10169 [cs]. [22] Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., and Roth, D. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018. [23] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [24] Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. [25] Lin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74\u201381, 2004. [26] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [27] Meng, Y., Huang, J., Zhang, Y., and Han, J. Generating training data with language models: Towards zero-shot language understanding. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=4G1Sfp_1sz7.\n[28] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020. [29] Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505\u20133506, 2020. [30] Rastogi, A., Zang, X., Sunkara, S., Gupta, R., and Khaitan, P. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset, 2019. URL https://arxiv. org/abs/1909.05855. [31] Sahu, G., Rodriguez, P., Laradji, I. H., Atighehchian, P., Vazquez, D., and Bahdanau, D. Data augmentation for intent classification with off-the-shelf large language models. arXiv preprint arXiv:2204.01959, 2022. [32] Schick, T. and Sch\u00fctze, H. Generating datasets with pretrained language models, 2021. URL https://arxiv.org/abs/2104.07540. [33] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. [34] Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019. [35] Wang, S., Liu, Y., Xu, Y., Zhu, C., and Zeng, M. Want to reduce labeling cost? gpt-3 can help, 2021. URL https://arxiv.org/abs/2108.13487. [36] Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., Pal, K. K., Moradshahi, M., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R., Verma, P., Puri, R. S., Karia, R., Sampat, S. K., Doshi, S., Mishra, S., Reddy, S., Patro, S., Dixit, T., Shen, X., Baral, C., Choi, Y., Smith, N. A., Hajishirzi, H., and Khashabi, D. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks, 2022. [37] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [38] Yoo, K. M., Park, D., Kang, J., Lee, S.-W., and Park, W. Gpt3mix: Leveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826, 2021. [39] Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q., and Artzi, Y. Revisiting Few-sample BERT Fine-tuning, March 2021. URL http://arxiv.org/abs/2006.05987. arXiv:2006.05987 [cs]. [40] Zheng, C., Sabour, S., Wen, J., Zhang, Z., and Huang, M. Augesc: Dialogue augmentation with large language models for emotional support conversation, 2023.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e368/e3684da0-6999-4d39-895e-65fab350fd33.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 3: Examples of generations for WebNLG [15].</div>\nInput: [CONTEXT] Astronaut [DATA] Elliot_See | almaMater | University_of_Texas_at_Austin\nUniversity_of_Texas_at_Austin | affiliation | University_of_Texas_System Elliot_See | occupation\n| Test_pilot Elliot_See | nationality | United_States Elliot_See | birthPlace | Dallas Elliot_See |\ndeathPlace | St._Louis Elliot_See | status | Deceased\nOutput: Elliot See was a US national, who was born in Dallas. He was a student at the University\nof Texas at Austin (affiliated to the university of Texas system). He was a test pilot who died in St.\nLouis.\nInput:\n[CONTEXT]\nAirport\n[DATA]\nAl_Asad_Airbase\n|\noperatingOrganisation\n|\nUnited_States_Air_Force\nUnited_States_Air_Force\n|\ntransportAircraft\n|\nLockheed_C-\n130_Hercules\nOutput: Al Asad Airbase is operated by the United States Air Force which use the Lockheed\nC-130 Hercules as a transport aircraft.\nInput:\n[CONTEXT] Food [DATA] Ajoblanco | ingredient | Almond Spain | leader | Fe-\nlipe_VI_of_Spain Ajoblanco | region | Andalusia Andalusia | leader | Susana_D\u02d800edaz Ajoblanco\n| country | Spain\nOutput: Ajoblanco originates from Spain where Felipe VI is leader. Ajoblanco is found in\nAndalusia where Susana Diaz is leader. One of the ingredients of ajoblanco is almonds.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/24c1/24c121c6-edd2-46d4-b95c-bb20c27171d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c490/c490a7a2-93fe-4d57-836a-3797097c6ade.png\" style=\"width: 50%;\"></div>\n# Table 5: Examples of generations for SGD [30].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a84/2a84f757-82b6-48f5-b5ed-04d73045cc6f.png\" style=\"width: 50%;\"></div>\nInput: [CONTEXT] User: I want to eat at a place in San Francisco. System: What type of food? You can say Indian, Italian, Vegetarian, or something else. User: Small plates would be perfect. System: Aatxe has small plates and is in San Francisco. User: What\u2019s the address and is their live music? System: Their address is 2174 Market Street and they don\u2019t have live music. User: What else is available? Check Redwood City. System: La Viga Seafood & Cocina Mexicana is in Redwood City. User: Yes, that will work. System: Do you want to book a table? User: Not now. Can you find a salon in Redwood City? System: August Barbershop is in Redwood City. User: Yes, I\u2019ve heard of them. System: Shall I make an appointment. User: Please do, book it for half past 2 in the afternoon. System: What date do you want it for? User: The 4th of March and book it for two pm. System: Please confirm the following: Booking an appointment with August Barbershop for March 4th at 2 pm. User: No, do it for the 1st of March. System: Please confirm the following: Booking an appointment with August Barbershop for March 1st. User: Yes, that\u2019s correct. System: Your appointment went through successfully! User: Could you try booking a table at that La Viga place?\nInput: [CONTEXT] I\u2019m looking for a rental car. System: What day and time do you want to pick it up? How long will you be needing it? User: I need it at three pm on the 8th of March, until the 11th of March. System: Where do you want to pick it up? User: I want to get it in Anaheim, CA. System: I have 5 cars you might like. How about a Standard Chevrolet Cruze? It\u2019s available at John Wayne Airport on March 8th. User: What else do you have? System: I have a Standard Honda Accord at John Wayne Airport on March 8th. User: What else do you have? System: I have a Standard Toyota Camry at Anaheim Intermodal Center on March 8th. User: That one sounds good. System: Do you want to reserve it? User: Yes, please reserve it. System: Please confirm the following: You are renting a Standard car from Anaheim Intermodal Center on March 8th at 3 pm. You\u2019ll return it on March 11th. User: Actually, I want to keep it until March 12th. System: Please confirm the following details: You are keeping the car until March 12th. User: That\u2019s correct. How much does it cost?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ebf/0ebf4399-8211-4078-9ccd-5b9d879e6c20.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of synthetic data generation in low-resource settings, highlighting the challenges posed by the high computational costs of large language models (LLMs) and the difficulty of obtaining sufficient labeled training data for smaller models.",
        "problem": {
            "definition": "The problem is defined as the need to improve the performance of smaller models that cannot be deployed due to the high costs associated with larger LLMs, particularly in scenarios where labeled data is scarce.",
            "key obstacle": "The main obstacle is the reliance on labeled examples, which are expensive and time-consuming to obtain in low-resource settings, making it challenging for smaller models to achieve good performance."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is to leverage the in-context learning capabilities of LLMs to generate synthetic training data that can be used to fine-tune smaller models.",
            "opinion": "The proposed idea entails using a fine-tuned teacher LLM to either annotate unlabeled instances or generate new training examples, thereby enhancing the training dataset for smaller student models.",
            "innovation": "The innovation lies in the method of generating synthetic data through a fine-tuned teacher LLM, which significantly improves the performance of smaller models without the need for extensive labeled datasets."
        },
        "method": {
            "method name": "Synthetic Data Generation via Fine-Tuning of Large Language Models",
            "method abbreviation": "SDG-FLLM",
            "method definition": "This method involves fine-tuning a large language model to generate synthetic training data for smaller models, which can be used to enhance their performance on specific tasks.",
            "method description": "The core of the method involves generating synthetic data by fine-tuning a teacher LLM to produce either annotated or entirely new examples.",
            "method steps": [
                "Fine-tune the teacher LLM on a small labeled dataset.",
                "Generate synthetic data by annotating unlabeled instances or creating new input-output pairs.",
                "Fine-tune the student model on the combined dataset of original and synthetic data."
            ],
            "principle": "The method is effective because it utilizes the teacher LLM's ability to generalize from limited data, allowing smaller models to benefit from high-quality synthetic training data."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted across four text classification tasks (SLURP, BoolQ, RTE, MultiRC) and two natural language generation tasks (SGD, WebNLG), using various fractions of original and synthetic data.",
            "evaluation method": "Performance was assessed by comparing the accuracy and scores of models trained with synthetic data against those trained with only original data, using metrics such as classification accuracy and Rouge scores."
        },
        "conclusion": "The study concludes that fine-tuning teacher LLMs to generate synthetic data can significantly enhance the performance of smaller models across multiple tasks, demonstrating the potential of this approach in low-resource settings.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to improve model performance with minimal labeled data, effectively reducing the costs and time associated with data annotation.",
            "limitation": "A limitation of the method is the substantial computational resources required for fine-tuning large models, which may not be feasible for all researchers or practitioners.",
            "future work": "Future research should explore optimizing the fine-tuning process to minimize resource usage while maximizing the quality of the synthetic data generated."
        },
        "other info": {
            "info1": "The teacher model used is the 20B GPT-NeoX.",
            "info2": {
                "info2.1": "The student models used include RoBERTa-Large for classification tasks and BART-Large for generation tasks.",
                "info2.2": "The method shows that even small amounts of original data can lead to significant performance gains when supplemented with synthetic data."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of synthetic data generation in low-resource settings, highlighting the challenges posed by the high computational costs of large language models (LLMs) and the difficulty of obtaining sufficient labeled training data for smaller models."
        },
        {
            "section number": "1.2",
            "key information": "The need to improve the performance of smaller models that cannot be deployed due to the high costs associated with larger LLMs, particularly in scenarios where labeled data is scarce."
        },
        {
            "section number": "3.1",
            "key information": "The method involves fine-tuning a large language model to generate synthetic training data for smaller models, which can be used to enhance their performance on specific tasks."
        },
        {
            "section number": "3.4",
            "key information": "The method is effective because it utilizes the teacher LLM's ability to generalize from limited data, allowing smaller models to benefit from high-quality synthetic training data."
        },
        {
            "section number": "4.1",
            "key information": "The proposed idea entails using a fine-tuned teacher LLM to either annotate unlabeled instances or generate new training examples, thereby enhancing the training dataset for smaller student models."
        },
        {
            "section number": "6.2",
            "key information": "A limitation of the method is the substantial computational resources required for fine-tuning large models, which may not be feasible for all researchers or practitioners."
        },
        {
            "section number": "7",
            "key information": "The study concludes that fine-tuning teacher LLMs to generate synthetic data can significantly enhance the performance of smaller models across multiple tasks, demonstrating the potential of this approach in low-resource settings."
        }
    ],
    "similarity_score": 0.6988716244570016,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models.json"
}