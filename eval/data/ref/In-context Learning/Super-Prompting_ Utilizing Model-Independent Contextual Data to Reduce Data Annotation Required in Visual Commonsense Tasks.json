{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2204.11922",
    "title": "Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks",
    "abstract": "Pre-trained language models have shown excellent results in few-shot learning scenarios using in-context learning. Although it is impressive, the size of language models can be prohibitive to make them usable in on-device applications, such as sensors or smartphones. With smaller language models, task-specific data annotation is needed to fine-tune the language model for a specific purpose. However, data annotation can have a substantial financial and time burden for small research groups, startups, and even companies. In this paper, we analyze different prompt-based finetuning techniques to improve results on both language and multimodal causal transformer models. To evaluate our results, we use a dataset focusing on visual commonsense reasoning in time. Our results show that by simple model-agnostic promptbased fine-tuning, comparable results can be reached by only using 35%-40% of the fine-tuning training dataset. The proposed approaches result in significant time and financial savings. As the proposed methods make minimal architectural assumptions, other researchers can use the results in their transformer models with minimal adaptations. We plan to release the source code freely to make it easier for the community to use and contribute to our work.",
    "bib_name": "rezaei2022superpromptingutilizingmodelindependentcontextual",
    "md_text": "# Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks\nNavid Rezaei and Marek Z. Reformat University of Alberta Edmonton, T6G 1H9, Canada\n{nrezaeis,marek.reformat}@ualberta.ca\n# Abstract\nPre-trained language models have shown excellent results in few-shot learning scenarios using in-context learning. Although it is impressive, the size of language models can be prohibitive to make them usable in on-device applications, such as sensors or smartphones. With smaller language models, task-specific data annotation is needed to fine-tune the language model for a specific purpose. However, data annotation can have a substantial financial and time burden for small research groups, startups, and even companies. In this paper, we analyze different prompt-based finetuning techniques to improve results on both language and multimodal causal transformer models. To evaluate our results, we use a dataset focusing on visual commonsense reasoning in time. Our results show that by simple model-agnostic promptbased fine-tuning, comparable results can be reached by only using 35%-40% of the fine-tuning training dataset. The proposed approaches result in significant time and financial savings. As the proposed methods make minimal architectural assumptions, other researchers can use the results in their transformer models with minimal adaptations. We plan to release the source code freely to make it easier for the community to use and contribute to our work.\n# 1. Introduction\nHuman annotation is time-consuming and is also a financial burden for research groups, startups, and companies. To put it in context, almost $240,000 has been spent on the annotation of the Visual Commonsense Reasoning in Time (VisualCOMET) dataset and this figure only includes the payment to crowd-workers from Amazon Mechanical Turk [19]. The real financial burden can be much higher when including the time value of the staff involved in the annotation\nprocess. Although large pre-trained language models, such as GPT-3 transformer [2], are impressive at multi-task fewshot learning, their huge size can be prohibitive for different scenarios, including on-device applications. Fine-tuning still plays an important role in achieving the state of the art, even with a relatively smaller model. As an example, the two current leading models1 (better than human baseline) on SuperGLUE task [28] are fine-tuned variants of T5 [21] and DeBERTa [9] language models, while GPT-3 is at 14th place. Our goal is to devise a model-independent process that could improve results based on fine-tuning with much less annotated training data.\nSeveral recent works have focused on improving finetuning methods in language models, such as [12], [5], [14], and [30]. The focus has been put mostly on optimization and regularization, but not on using less data for fine-tuning. The results from those studies are complementary to our work. Some previous efforts have been put on prompt-based fine-tuning to improve classification or regression tasks in natural language processing (NLP). [24] and [25] convert textual inputs into cloze-style questions with a task description. [6] studies smaller language models for fewshot learning capability by using automatically-generated prompts for fine-tuning and by incorporating demonstrations into context. On another topic, a group of recent research studies, including [11], [20] and [16], aim at task-dependent added parameters to adapt models to different tasks. This way, one does not need to re-train a complete model to fine-tune it to a specific task but only needs to re-train a fraction of parameters.\nThere is a recent body of work that utilizes inherent knowledge of language models combined with fine-tuning on specialized large-scale training datasets to infer different commonsense and causal scenarios. [1] uses generative language models to expand on ATOMIC [23] and ConceptNet [26] commonsense knowledge graphs. [13] introduces an updated knowledge graph similar to ATOMIC and uses BART [15] encoder-decoder model to generate new knowledge. [17] uses generative language models to expand on an introduced knowledge base of causal mini-story explanations. Given the success of prompt-based fine-tuning and incontext learning in classification and regression tasks, we are motivated to assess similar principles in the context of commonsense generation using generative language models, which are fine-tuned on a commonsense knowledge graph.\n# 3. Dataset\nFor this paper, we have selected a multi-modal commonsense knowledge graph for fine-tuning. The Visual Commonsense Reasoning in Time (VisualCOMET) dataset [19] consists of 1.4 million commonsense inferences over 59,356 images and 139,377 specific events at present. The dataset has human-annotated inferences regarding three different aspects: the intention of the person mentioned, the possible events that could happen next, and the possible preceding events. The inferences are made based on a single image. The annotators have access to short clips before and after the event, which are not part of the dataset. Each image is also annotated with event and place descriptions. There is a total amount of 1,465,704 commonsense inferences. The images are sourced from the VCR dataset [29]. The images usually have a complex visual scene with multiple people and activities present. This dataset includes automatically-detected object bounding boxes and people are annotated with numerical tags.\n# 4. Method\nIn this work, we focus on using generative language models and analyze how prompt-based fine-tuning and incontext learning could help to reduce the size of the data required for fine-tuning training. As seen in Fig. 1, there are several scenarios where extra context could help lead the generative language model to a correct answer, but lack of correct understanding about the scene and the event text can result in incorrect results. Extra human annotations, focused on these shortcomings, could improve the results, but that comes with extra time and money expenditure. We propose using the underutilized context already present in text and image, then transforming them to a form\nthat is usable by most transformer models, which is a sequence. We analyze if this kind of addition helps the language model achieve better results in the case of limited annotated data available. Assuming the added context text is represented with c and its tokenized version with {c}, we can represent the context with {c} = {wc 1, wc 2, ...wc q}, where wc i represents each token created from tokenization of the context c. This context is merged with tokenized versions of event and place, which are represented as: {e} = {we 1, we 2, ...we n} and {p} = {wp 1, wp 2, ...wp m}, respectively. Using the merged versions of event and place texts with context, the updated sequence-to-sequence loss can be written as:\n(1)\nwhere v represents visual features, including overall images and person-specific boxes, r represents inference prompts, which could be intent, before and after, and w\u2217 <i represents past tokens for each case.\n# 5. Experiments\nThe goal of the experiments is to see how much we could reduce the annotated data and still achieve results comparable to a case where the full human-annotated data is used. We tried different contextual data, which did not require extra annotations, such as captions, facial expressions, and related concepts. As shown in Fig. 1, we can intuitively see that some extra context could potentially help the language model to reach a more logical deduction of intention, past, and future events. For each scenario, the VisualCOMET dataset provides several human annotations for comparison, each showing intent of a person, what could happen next, and what happened before. The experiments are evaluated using BLEU [18], METEOR [4] and CIDEr [27] automatic metrics to compare the generated texts for different scenarios of before, intent and after with the human-annotated texts. We tried two different methods of adding relevant concepts. One method is based on converting relevant concept graphs into a readable sentence and the other method is based on only prepending concept words to the target sentence. In either method, the text is scanned for concepts, and the related concepts are extracted based on a commonsense knowledge graph such as in [26, 22]. Although sentencebased inputs perform well, they require a longer input width\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/32e9/32e9ff9b-d9cd-4a1e-af9c-45054a79b04f.png\" style=\"width: 50%;\"></div>\nMethod\nBLEU-2\nMETEOR\nCIDEr\nGPT-2 [19]\n13.81\n10.85\n15.37\nConcept Word (NVP)\n17.25\n12.17\n19.79\nConcept Word (VP)\n17.17\n12.28\n19.34\nConcept Sent. (NVP)\n14.92\n11.25\n16.9\nFigure 1: Predictions based on the fine-tuned language model introduced in [19]. Each example shows a piece of missing contextual information that could be utilized.\nthat may not be available given the language model. To sort relevant concepts, crowd-based scores or frequency scores\nTable 1: Effects of adding relevant concepts. Results are shown at the fourth epoch using almost 25,000 (22%) of the available annotated data. NVP: No Validation Prompt. VP: Validation Prompt.\nMethod\nBLEU-2\nMETEOR\nCIDEr\nGPT-2 [19]\n13.81\n10.85\n15.37\nFE (NVP)\n14.45\n11.27\n15.7\nFE (VP)\n15.11\n11.23\n16.03\nTable 2: Effects of adding information about facial expressions. Results are shown at the fourth epoch using almost 25,000 (22%) of the available annotated data. NVP: No Validation Prompt. VP: Validation Prompt. FE: Facial Expressions.\nare used based on the specific knowledge graph used. These triples are then converted to text with some hand-designed rules. An example of this process is shown in Fig. 2a. Table 1 shows three of the top-performing models with added conceptual contexts. They are compared with the original data, which does not have any added context. Evaluation is done on a validation dataset with a size of a hundred. Concept words added in this specific scenario are connected via HasProperty and PartOf predicates. Concept sentences use the HasProperty predicate. Adding similar information during inference time does not result in much improvement in this specific case. More comparisons can be found in the Appendix. As seen in Fig. 1a, lack of the model\u2019s attention to some visual cues, such as facial expressions, could also result in errors of judgment. To fix this issue, we trained a ResNet [8] model on FER2013 [7] dataset with almost 70% accuracy. The dataset consists of human face images and emotion labels of angry, disgust, fear, happy, neutral, sad, and surprise. Only the emotion of people mentioned in the event text is processed. The results are then prepended to the event text. An example of this process is shown in Fig. 2b. Table 2 shows effects of adding facial expressions as a context in the final performance of the model. Contrary to relevant concepts, adding facial expressions during inference time improves the results. Evaluation is done on a validation data size of a hundred. Another type of automatically-generated context that\nMethod\nBLEU-2\nMETEOR\nCIDEr\nGPT-2 [19]\n13.81\n10.85\n15.37\nCaption (NVP)\n14.08\n10.78\n15.63\nCaption (VP)\n16.49\n11.85\n18.8\nTable 3: Effects of adding image captions. Results are shown at the fourth epoch using almost 25,000 (22%) of the available annotated data. NVP: No Validation Prompt. VP: Validation Prompt.\nwe experimented with is image captioning. The idea is that some of the image dynamics may have been missed, even though image features are fed into the GPT-2 model. Adding generated captions proves to be effective as shown in Table 3. Meshed-Memory transformer model [3] with beam search decoding is used for image captioning. The process of adding these captions is illustrated in Fig. 2c. A mixture of different contextual information is shown to be more effective than individual ones. A combination of concept words, image captions, and facial expressions of relevant individuals in the image achieve the best result compared to other experiments. As seen in Table 4, this combination can achieve comparable results to fulldata finetuning by only using 35%-40% of the annotated data. This results in less human time spent doing annotations and can potentially reduce costs and completion times of projects. Results of other experiments are included in the Appendix. To reduce the effects of other variables in these experiments, we have limited ourselves to only train the final models for five epochs. The decoding method and hyperparameters are also kept constant throughout the experiments. We use nucleus sampling [10] with p = 0.9 to generate five sentences for each scenario of intent, before and after. The finetuning was run on two NVIDIA RTX GPUs with 24 GB memory each. For the case with all concept words, captions, and facial expression contexts, the fine-tuning time is around 1.5 hours per epoch while using mixed precision.\n# 6. Conclusion\nIn this work, we analyzed the effects of automaticallygenerated contexts in multimodal transformer models used in a commonsensical task. These prompts can help us reduce the human annotation needed in the task by as much as 60%-65% and still, achieve comparable results to when the whole human-annotated dataset is used. These findings result in time and cost savings for future multimodal data annotation projects. As future work, it is interesting to find a lower bound for data annotation reduction without affecting the final result of a model. It is also useful to find a method to automat-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce57/ce570fec-4e36-4abf-b835-2b7bf00b4b15.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9ac7/9ac7f80b-ebb4-4068-b2ec-e706b3dac7d8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Process of adding image captions</div>\nFigure 2: The process of extracting and adding prompts shown through examples.\nically find and apply the best contextual data for different tasks and models.\n# References\n[1] Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762\u2013 4779, Florence, Italy, July 2019. Association for Computational Linguistics. 2 [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 1 [3] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-Memory Transformer for Image Captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 4\nMethod\nInference Data\nData Size\nBLEU-2\nMETEOR\nCIDEr\nGPT-2 [19]\nN/A\n111,796 (100%)\n18.05\n13.21\n22.72\nCW + C + FE\nC + CW + FE\n39,000 ( 35%)\n18.38\n12.97\n22.65\nCW + C + FE\nC + CW + FE\n45,000 ( 40%)\n18.58\n13.01\n22.97\nle 4: Analyzing the effect of combining multiple contextual data. All models are finetuned for five epochs. Contexts ar ed based on the order shown. CW: Concept Words. C: Captions. FE: Facial Expressions.\n# A. Appendix\nExperimentation results from using different types of training and inference prompts are included in this appendix. The model used in the experiments is GPT-2 as described in [19]. The best types of prompts are chosen to be combined. The experiments show that the order in which prompts are added can affect the final results. The vision-based inference prompts seem to better affect the final metric results when compared to the text-based inference prompts. This could be due to the lack of enough visual attention paid during the decoding process. Future work could involve developing a multimodal model that makes better use of visual contexts not only during the training phase, but also the inference time. The quality of the annotated data can have an impact on the training model. We do not hand-select the annotated data based on quality and this may result in variability in final results when training with different data sizes. It can be a good practice to assess the quality of the annotated data and prompts based on the final goal of the model.\nTraining Prompt\nInference Prompt\nTraining Data Size\nBLEU-2\nMETEOR\nCIDEr\nNone\nNone\n111,796 (100%)\n17.94\n13.14\n22.71\nNone\nNone\n25,000 ( 22%)\n13.81\n10.85\n15.37\nCS (AtLocation)\nNone\n25,000 ( 22%)\n12.26\n10.42\n15.09\nCS (AtLocation) + Place\nNone\n25,000 ( 22%)\n14.3\n11.08\n14.66\nCS (CapableOf)\nNone\n25,000 ( 22%)\n12.65\n10.6\n15.9\nCS (CapableOf) + Place\nNone\n25,000 ( 22%)\n14.78\n11.16\n15.1\nCS (HasA)\nNone\n25,000 ( 22%)\n12.73\n10.65\n15.83\nCS (HasA) + Place\nNone\n25,000 ( 22%)\n14.58\n11.15\n15\nCS (HasProperty)\nNone\n25,000 ( 22%)\n12.25\n10.5\n15.52\nCS (HasProperty) + Place\nNone\n25,000 ( 22%)\n15.25\n11.38\n16.3\nCS (IsA)\nNone\n25,000 ( 22%)\n12.73\n10.41\n15.73\nCS (IsA) + Place\nNone\n25,000 ( 22%)\n14.04\n11.03\n14.32\nCS (PartOf)\nNone\n25,000 ( 22%)\n12.65\n10.51\n15.71\nCS (PartOf) + Place\nNone\n25,000 ( 22%)\n14.26\n11.19\n14.64\nFE\nNone\n25,000 ( 22%)\n14.45\n11.27\n15.7\nFE\nFE\n25,000 ( 22%)\n15.11\n11.23\n16.03\nCW (PartOf + HasProperty)\nNone\n25,000 ( 22%)\n17.25\n12.17\n19.79\nCW (PartOf + HasProperty)\nCW (PartOf + HasProperty)\n25,000 ( 22%)\n17.17\n12.28\n19.34\nC\nNone\n25,000 ( 22%)\n14.08\n10.78\n15.63\nC\nC\n25,000 ( 22%)\n16.49\n11.85\n18.8\nC + FE\nC + FE\n25,000 ( 22%)\n16.75\n12.19\n19.16\nCW + C + FE\nNone\n25,000 ( 22%)\n12.6\n10.18\n14.57\nCW + C + FE\nCW + C + FE\n25,000 ( 22%)\n17.4\n11.97\n20.03\nCW + C + FE\nCW + FE\n39,000 ( 35%)\n16.57\n12.32\n19.68\nCW + C + FE\nC + FE\n39,000 ( 35%)\n16.71\n12.27\n19.01\nCW + C + FE\nCW + C + FE\n39,000 ( 35%)\n16.75\n12.4\n19.86\nCW + C + FE\nCW + C + FE + Syns\n39,000 ( 35%)\n16.7\n12.43\n19.94\nCW + C + FE + PCW\nCW + C + FE + PCW\n39,000 ( 35%)\n17.74\n12.73\n20.52\nCW + C + FE\nC + CW + FE\n39,000 ( 35%)\n17.34\n12.45\n20.11\nCW + C + FE\nCW + C + FE\n45,000 ( 40%)\n17.46\n12.84\n21.56\nCW + C + FE\nC + CW + FE\n45,000 ( 40%)\n17.66\n12.95\n21.41\nCW (PartOf + HasProperty) CW (PartOf + HasProperty)\nTable 5: Experimentation results of using different training and inference prompts. The model used is GPT-2 [19]. Results are shown at epoch four and evaluated on validation data of size 100. Prompts are added based on the order shown. CW: Concept Words. C: Captions. FE: Facial Expressions. CS: Concept Sentences. Syns: Synonyms. PCW: Place Concept\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of high costs and time associated with human annotation in visual commonsense reasoning tasks, particularly highlighting the significant financial burden on small research groups and companies. Previous methods have relied on large pre-trained models which are not always feasible for on-device applications, necessitating a more efficient approach to fine-tuning with less annotated data.",
        "problem": {
            "definition": "The problem is the excessive requirement for human-annotated data in fine-tuning language models for visual commonsense reasoning tasks, which poses a barrier for smaller entities in terms of time and financial resources.",
            "key obstacle": "The main challenge is the financial and time burden of data annotation, which limits the ability of smaller research groups and companies to effectively utilize large language models in specific tasks."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that existing models can achieve satisfactory performance with less annotated data by leveraging inherent contextual information available in text and images.",
            "opinion": "The proposed idea involves utilizing model-independent contextual data to reduce the amount of required annotated training data while maintaining performance levels.",
            "innovation": "The innovation lies in the model-agnostic prompt-based fine-tuning approach, which enables significant reductions in the amount of annotated data needed (by 60%-65%) while achieving comparable results to full data fine-tuning."
        },
        "method": {
            "method name": "Super-Prompting",
            "method abbreviation": "SP",
            "method definition": "Super-Prompting is a method that integrates contextual information from multimodal sources to enhance the performance of language models in commonsense reasoning tasks while minimizing the need for extensive human annotations.",
            "method description": "The core of the method involves merging context from textual and visual inputs to optimize the fine-tuning process of language models.",
            "method steps": "1. Identify relevant contextual data (e.g., captions, facial expressions). 2. Merge this data with event and place descriptions. 3. Fine-tune the model using the combined context. 4. Evaluate the model's performance against human-annotated benchmarks.",
            "principle": "This method is effective because it leverages existing contextual data to provide additional information that aids the model in making more accurate inferences, thus reducing reliance on extensive human annotations."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized the Visual Commonsense Reasoning in Time (VisualCOMET) dataset, comparing results from models fine-tuned with varying amounts of contextual data against human-annotated baselines.",
            "evaluation method": "Performance was assessed using automatic metrics such as BLEU, METEOR, and CIDEr, measuring the generated outputs against human annotations for various scenarios."
        },
        "conclusion": "The study concludes that the Super-Prompting method can significantly reduce the need for human annotation by 60%-65% while still achieving results comparable to traditional full-data fine-tuning approaches, offering substantial time and cost savings for future projects.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include reduced annotation costs, faster project completion, and the ability to utilize existing contextual data effectively.",
            "limitation": "A limitation of the method is that it may not perform as well in scenarios where the contextual data is insufficient or not representative of the specific task requirements.",
            "future work": "Future research could explore optimizing the selection of contextual data and finding lower bounds for data annotation reduction without compromising model performance."
        },
        "other info": {
            "source code": "The authors plan to release the source code to facilitate community engagement and further development of the proposed methods."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational concept of leveraging contextual information from multimodal sources to enhance language model performance in commonsense reasoning tasks."
        },
        {
            "section number": "1.2",
            "key information": "In-context learning is significant as it reduces the excessive requirement for human-annotated data, particularly benefiting smaller research groups and companies."
        },
        {
            "section number": "1.3",
            "key information": "The paper discusses the challenges of utilizing large language models for visual commonsense reasoning due to high costs and time associated with human annotation."
        },
        {
            "section number": "3.1",
            "key information": "The Super-Prompting method enables models to adapt to various contexts by integrating contextual data from textual and visual inputs."
        },
        {
            "section number": "3.4",
            "key information": "The method merges context from event and place descriptions with relevant contextual data, facilitating better adaptation in in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "Effective prompt design in the Super-Prompting method allows for a significant reduction in the amount of required annotated training data while maintaining performance levels."
        },
        {
            "section number": "6.1",
            "key information": "The limitation of the proposed method is that it may not perform well when contextual data is insufficient or not representative of specific task requirements."
        },
        {
            "section number": "6.2",
            "key information": "The financial and time burden of data annotation poses challenges for smaller entities, highlighting the need for more efficient approaches."
        }
    ],
    "similarity_score": 0.6991142428025126,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Super-Prompting_ Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks.json"
}