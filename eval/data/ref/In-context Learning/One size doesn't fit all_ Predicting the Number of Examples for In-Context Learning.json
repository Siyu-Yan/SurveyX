{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.06402",
    "title": "One size doesn't fit all: Predicting the Number of Examples for In-Context Learning",
    "abstract": "In-context learning (ICL) refers to the process of adding a small number of localized examples (ones that are semantically similar to the input) from a training set of labelled data to an LLM\u2019s prompt with an objective to effectively control the generative process seeking to improve the downstream task performance. Existing ICL approaches use an identical number of examples (a pre-configured hyper-parameter) for each data instance. Our work alleviates the limitations of this \u2018one fits all\u2019 approach by dynamically predicting the number of examples for each data instance to be used in few-shot inference with LLMs. In particular, we employ a multi-label classifier, the parameters of which are fitted using a training set, where the label for each instance in the training set indicates if using a specific value of k (number of most similar examples from 0 up to a maximum value) leads to correct k-shot downstream predictions. Our experiments on a number of text classification benchmarks show that AICL substantially outperforms standard ICL by up to 17%.",
    "bib_name": "chandra2024sizedoesntfitall",
    "md_text": "# One size doesn\u2019t fit all: Predicting the Number of Examples for In-Context Learning\n# Manish Chandra, Debasis Ganguly and Iadh Ounis University of Glasgow\nUniversity of Glasgow Glasgow, United Kingdom\nm.chandra.1@research.gla.ac.uk, Debasis.Ganguly@glasgow.ac.uk, iadh.ounis@glasgow.ac.uk\n# Abstract\nIn-context learning (ICL) refers to the process of adding a small number of localized examples (ones that are semantically similar to the input) from a training set of labelled data to an LLM\u2019s prompt with an objective to effectively control the generative process seeking to improve the downstream task performance. Existing ICL approaches use an identical number of examples (a pre-configured hyper-parameter) for each data instance. Our work alleviates the limitations of this \u2018one fits all\u2019 approach by dynamically predicting the number of examples for each data instance to be used in few-shot inference with LLMs. In particular, we employ a multi-label classifier, the parameters of which are fitted using a training set, where the label for each instance in the training set indicates if using a specific value of k (number of most similar examples from 0 up to a maximum value) leads to correct k-shot downstream predictions. Our experiments on a number of text classification benchmarks show that AICL substantially outperforms standard ICL by up to 17%.\n18 Oct 2024\narXiv:2403.06402v2 \n# 1 Introduction\nLarge Language Models (LLMs) exhibit remarkable abilities to model text semantics in an abstract and general manner without any task specific training (Radford et al., 2018). In-context learning (ICL)1 makes use of this abstract representation and knowledge representation capabilities of LLMs (Brown et al., 2020; Arora et al., 2022; Weidinger et al., 2022) to address a range of different downstream tasks. For instance, such models can provide effective solutions for tasks including assessing reviews (Mysore et al., 2023), answering questions (Li et al., 2023a), recommending relevant documents (Pradeep et al., 2023) etc., by using only a\n1also interchangeably known as few-shot learning or retrieval-augmented generation (RAG) with ground-truth labels\nsmall number of (even zero) examples without any task-specific training. More formally, ICL refers to the process of conditioning an LLM\u2019s decoder (frozen parameters) towards generating potentially relevant outputs that could then be interpreted in the context of a specific task, e.g., words such as \u2018great\u2019 and \u2018wow\u2019 can be mapped to a positive sentiment (Schick et al., 2020). The output of an LLM\u2019s decoder is controlled by varying the input to the LLM, which is usually structured in the form of an instruction (the task description), and a small number of representative examples (Ni et al., 2022). Figure 1a shows an example of how ICL works for the downstream task of movie review sentiment analysis. It can be seen that both demonstrations (shown in blue) and the current instance form a part of the input to an LLM. Researchers have worked towards adapting an ICL workflow in several ways, which range from using localised examples (Liu et al., 2022a) instead of random ones, to diversifying these examples (Levy et al., 2023), and also ideally ordering them (Kumar and Talukdar, 2021; Rubin et al., 2022). What prior work has not addressed so far is the effect of dynamically selecting the number of examples in an ICL workflow. Our idea takes motivation from information retrieval (IR), where different queries exhibit different levels of retrieval performance mainly due to the inherent characteristic of the information need itself (Kanoulas et al., 2011), or how well-formulated the query is (Datta et al., 2022). Drawing the parallel that a query in IR is analogous to a test instance in ICL and that the localised examples are potentially relevant documents (Rubin et al., 2022), we hypothesise that some test instances are associated with better candidates for training examples (i.e., examples which are useful in the sense that including them as a part of the prompt leads to correct predictions), and hence including a small number of these examples should be adequate. On the other hand, the retrieval\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ddc/7ddc9f17-936d-4010-adf2-af4cdf825891.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\nFigure 1: a) Example workflow of ICL for sentiment classification. The illustrative example shows a test instance for which a single demonstration (as retrieved from the training set) does not result in a correct prediction (prediction workflow of the red arrows). The example also shows that increasing the number of demonstrations from one to two results in a correct prediction (green arrows). We propose a method to estimate the number of examples that are likely to yield a correct prediction. b) Motivation behind using a variable number of examples for ICL across the test instances: 1) The test sample \u2018?\u2019 is located within a homogeneous neighborhood of negative data points indicating that an LLM may perform well with only a few nearest neighbour demonstrations. 2) The test instance \u2018?\u2019 is located within a heterogeneous neighborhood, as a result of which, an LLM may require a higher number of such demonstrations to correctly predict its class.\nquality for some test instances used as queries do not yield good candidates, as a result of which, one needs to further look down the ranked list to collect the useful ones (Bahri et al., 2020; Ganguly and Yilmaz, 2023). The parallel with IR means that the notion of relevance of a document to a query needs to be replaced by the downstream usefulness of an example to a test instance. The idea of choosing a variable number of localised examples is also somewhat similar to selecting a variable-sized neighborhood for k-NN classification (Zhong et al., 2017), where the key idea is that a homogeneous neighborhood is likely to require a relatively small-sized neighborhood for a correct prediction, and a heterogeneous one would likely require a larger one (see Figure 1b). The same idea can be applied to ICL, where a test instance that is similar to a number of training instances with conflicting labels may require a larger number of examples. In our work, we apply a supervised learning based workflow to learn a mapping between the features of a test instance (embedding plus the label distribution of its neighborhood) and the ideal number of examples that should be used in ICL for the downstream prediction. More specifically, for each instance of the training set we vary k - the\nnumber of ICL examples - within a range of 0 to a pre-configured maximum value (say M) and store each indicator of whether a particular value of k leads to a correct prediction as an (M + 1)-length Boolean vector (e.g., the indicator for k = 1 in Figure 1a is negative, whereas the one for k = 2 is positive). We then train a multi-label classifier on these pairs of instances and the Boolean indicators, the assumption being that similar instances with similar label distributions should also exhibit a similar distribution over the values of k leading to correct k-shot learning. During inference time, for each test instance, we apply the multi-label classifier to obtain a candidate list of predicted k values, and we select the one for which the prediction confidence is the highest. We then use these many examples for the ICL-based downstream prediction. This means that as per the schematics of Figure 1a, we can potentially find out a \u2018green path\u2019 leading to a correct prediction for each test instance among several other \u2018red paths\u2019 that lead to incorrect ones.\n# 2 Related Work\nPrompt tuning and searching. LLMs, when scaled from millions to billions of parameters, have been demonstrated to be adaptable to a broad set of tasks due to instruction tuning (Ouyang et al.,\n2022; Brown et al., 2020), in the sense that they are not only able to produce semantically correct and coherent text, but are also able to adapt themselves surprisingly well with small changes in contexts supplied as inputs, commonly called prompts (Arora et al., 2022). Previous research studies the problem of constructing an appropriate prompt for LLMs from two broad perspectives: a) prompt tuning in the embedding space (Li and Liang, 2021; Liu et al., 2022b; Qin and Eisner, 2021; Liu et al., 2023), and b) prompt searching in the text space (Lu et al., 2022; Zhang et al., 2023; Diao et al., 2023; Liu et al., 2022a; Shi et al., 2023). Prompt tuning is a lightweight alternative to fine-tuning, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors. The key idea of prompt tuning is to inject task-specific embedding into hidden layers and then tune these embeddings using gradient-based optimization. However, these methods require the modification of the original inference process of the model, which is impractical for the case of black-box LLM services, such as GPT3 (Brown et al., 2020). Furthermore, prompt tuning introduces additional computational and storage costs, which is typically expensive for LLMs. A more efficient way is to optimize prompting via searching appropriate demonstration samples and ordering them in the original text space.\nIn-context Learning (ICL). ICL has advanced the use of LLMs for task-specific applications with minimal examples, forming a solid foundation for subsequent investigations into optimizing this learning paradigm (Dong et al., 2022). In ICL, a small number of labeled examples from a training set are appended to a prompt instruction to control the text generation of LLM so that it is beneficial to a downstream task (Mysore et al., 2023; Li et al., 2022; Ni et al., 2021; Pradeep et al., 2023). In addition to leveraging ICL for a purely generative task, e.g., question answering or abstractive summarisation (Brown et al., 2020; Li et al., 2023a; Tang et al., 2023), a more common use is in a predictive task, such as text classification (Lu et al., 2022; Milios et al., 2023; Wei et al., 2021), where each class is specified by a set of words, commonly called a verbaliser (Schick and Sch\u00fctze, 2021). Once each class for a predictive task is well-defined, the generated text can be mapped to the most likely class by using the probabilities of the tokens generated by the decoder.\nA crucial aspect of ICL is the adept selection and utilization of the demonstrations for task comprehension. Recent works have explored the dynamics of in-context examples, elucidating how ICL enhances the language model\u2019s performance across various NLP tasks by providing a minimal set of examples at inference time (Han et al., 2023). Furthermore, the effectiveness of ICL is significantly shaped by the strategies for selecting and ordering demonstrations. For instance, the work in (Luo et al., 2023) expanded the applicability of retrievalbased ICL approaches by demonstrating that even simple word-overlap similarity measures such as BM25 outperform randomly selected demonstrations. Methods such as KATE (Liu et al., 2022a) also showed that localised examples work better than random ones. Different from KATE that uses a fixed-size neighbourhood for each data instance, we propose to modify a standard ICL workflow by employing a variable number of examples.\n# 3 Proposed Methodology\n# 3.1 Standard In-Context Learning (ICL)\nIn-context learning (ICL), unlike supervised learning, does not involve training a set of parameters on labeled examples. Rather, the posteriors are now a function of the following: a) text of the input test instance, b) the decoder parameters of a pre-trained LLM, c) a prompt instruction, and d) optionally, a set of k input examples, commonly called k-shot prompting (Liu et al., 2022a). Formally,\nP(y|x, k) = f(x, Nk(x); \u03d5LLM),\nwhere, different from a supervised setup, the function f does not have a parameterized representation that can be learned using a training set with gradient descent. The function itself depends on the pre-trained frozen parameters \u03d5LLM of an LLM, the current inputs for which a label is to be predicted, and a prompt comprising a set of k text units denoted by Nk(x). This set Nk(x) = {z1, . . . , zk} of Equation 1 (where each zj \u2208T , a training set of labelled data instances) in ICL is usually comprised of localised examples, i.e., examples that are topically similar to the current instance (Liu et al., 2022a; Luo et al., 2024). Particularly for our experiments, we employ SBERT (Reimers and Gurevych, 2019) as the neighborhood similarity computation function as per the findings of (Liu et al., 2022a).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/830c/830c661a-fc25-4f1a-b256-73631d6da4bd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Schematic diagram of Adaptive In-Context Learning (AICL) workf</div>\n# 3.2 Adaptive ICL\nWe now describe our methodology that uses a variable number of examples by extending the standard ICL workflow of Equation 1. We call our method \u2018Adaptive In-Context Learning\u2019, or AICL for short. The idea of AICL centres around choosing the context Nk(x) in a data-driven manner, i.e., making k a function of the data, i.e., the current instance x itself. This is analogous to choosing a different value of k for a k-NN based non-parametric model or choosing a different rank cut-off for topretrieved set of documents for different queries (Bahri et al., 2020; Ganguly and Yilmaz, 2023). The motivation is that classifying some instances would be more difficult than others, in which cases they are potentially to benefit from a larger value of k (more context). On the other hand, for relatively easier data instances, using too much context may be detrimental for an effective prediction. Formally speaking, the difference of AICL with that of ICL (Equation (1)) is that the value k, indicating the size of the neighborhood, is no longer a constant. Instead, we denote it by a parameterised function \u03ba(x) such that\n(2)\nwhere \u03ba : x \ufffd\u2192{0, . . . , M}, and M is an upper bound on the number of example instances.\nFigure 2 presents an overarching view of the AICL workflow. In contrast to ICL, AICL involves an additional phase of training a classifier, \u03ba(x) of Equation (2), to predict an appropriate number of examples by leveraging the training instances (the \u2018classifier training\u2019 block in Figure 2). The \u2018LLM inference\u2019 block shows the inference phase, where, given a test instance, we first predict the number of examples to be used and then follow the standard ICL procedure with those many examples fed as part of the additional context to the LLM.\n# Obtaining the ground-truth values of the number of ICL examples for each training set in-\nstance. For each training set instance x \u2208T , we first execute k-shot inference with progressively increasing values of k within {0, . . . , M}, where M is a pre-configured threshold (specifically, M = 10 in our experiments). This is depicted by the progressively increasing colored dotted regions within the \u2018classifier training\u2019 block of Figure 2. After obtaining M + 1 posteriors P(\u02c6y|x) for an instance x of the training set, we construct an (M + 1) dimensional Boolean vector, the ith component of which indicates if using i ICL examples lead to a correct downstream prediction for the current instance x (note that since x is a training set instance, the knowledge of its true label y(x) is available).\nwhere K(x) = {K0(x), . . . , KM(x)} is an (M + 1) dimensional Boolean vector (note that we allow provision for the number of examples to be 0 as well, thus allowing provision for zero-shot prediction).\nTraining a multi-label classifier. As a next step, we learn a parameterized map from the inputs to the M + 1 dimensional indicators on the number of ICL examples, i.e.,\n(4)\nwhere K(x) \u2208{0, 1}M+1 as defined in Equation 3 and \u03b8 is a set of parameters learned via crossentropy loss. Our decision to use a multi-label classification approach is motivated from a set of initial experiments, where we found that it outperformed a multi-class classifier approach (which we thus do not report in the paper). The most likely reason why a multi-label classifier works more effectively for this predictive task is that it allows provision to select the most likely candidate number of ICL examples from a number of alternatives rather than relying on a single predicted value of \u03b8(x). Of course, this also means that there needs to be a precise selection criteria to choose the number of examples to be used in the inference time from a list of choices, i.e., the result of the multi-label prediction where the predicted label is 1 (sigmoid posterior is higher than 0.5). We tried two different heuristics for this selection - i) choosing the smallest index from the M + 1 dimensional vector with a predicted value of 1 (sigmoid > 0.5), and ii) selecting the component with the highest confidence (sigmoid posterior). Again after conducting a set of initial experiments, we found that the second heuristics almost always outperforms the first one, and hence our workflow of Figure 2 employs the max-confidence heuristic for predicting the number of ICL examples to be used during inference (see Equation 2). More formally,\n(5)\nwhere \u03b8(x) \u2208[0, 1]M+1 denotes the posteriors (sigmoid values) obtained from a multi-label classifier trained with the labels as defined in Equation 3.\nDistribution of the downstream-task class labels as additional features. The multi-label classifier \u03b8 : x \ufffd\u2192K(x) means that the number of ICL examples depend only on the document content, or in other words, topically similar content potentially requires a similar number of ICL examples for accurate predictions. However, this does not take into account the class label distribution (the y(x) values) of the examples. In fact, it is reasonable to assume that an instance with a more homogeneous neighborhood (in terms of the class label distribution) likely indicates an easier instance and hence may require a smaller number of examples, whereas a more heterogeneous neighborhood may indicate a larger number of examples (e.g., see Figure 1b). This hypothesis means that the use of neighborhood class distribution as additional features to encode each training data instance (x \u2208T ) is likely to lead to a better predictor \u03b8, which in turn, is likely to lead to better estimations of \u03ba(x) for AICL. Specifically, to incorporate the class prior information to aid predicting the number of ICL examples, we append an M dimensional vector l \u2208ZM p (p is the number of class labels) to each SBERT embedded input vector x \u2208T . The ith component of this vector contains the class label of the ith neighbor in NM(x), i.e., l = {y(z1), . . . , y(zM)}. With this modification, the multi-label classifier now learns:\n\u03b8 : x \u2295l \ufffd\u2192K(x),\nwhere l denotes the class label distribution of M nearest neighbors, and \u2295denotes the concatenation operator.\n# 4 Evaluation\n# 4.1 Research Questions and Datasets\nIn Section 3, we proposed a classifier-based approach to learn the optimal number of examples. In our experiments, we compare this approach for adaptive k-shot with fixed k-shot on standard datasets for text classification. In particular, we investigate the following research questions.\n\u2022 RQ-1: Does adaptively selecting the number of examples in ICL (AICL) lead to improved downstream effectiveness?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6899/6899b001-95fa-4f8e-bf1b-18a6abc14d07.png\" style=\"width: 50%;\"></div>\nSST2\nSentiment Analysis\n2\nMovie Reviews\n19.3\nTREC\nQuestion classification\n6\nOpen Domain Questions\n10.2\nCoLA\nGrammatical Error Detection\n2\nNews and Wikipedia\n6.7\nRTE\nLanguage Inference\n2\nMiscellaneous\n52.4\n<div style=\"text-align: center;\">Table 1: The characteristics of the datasets used in our experiments</div>\n# \u2022 RQ-3: Does AICL generalise well across different LLMs or datasets?\n\u2022 RQ-4: Can the multi-label classification model (Equation 6) be trained effectively on a small subsample of the training set, thus reducing the number of LLM calls during the training?\nAs per the setup of (Ma et al., 2023; Zhang et al., 2022), we conducted our experiments on four standard text classification datasets, namely SST2, TREC (Li and Roth, 2002), CoLA and RTE. Except TREC, the other three datasets are part of the GLUE benchmark (Wang et al., 2018) exhibiting diversity in: i) domain, e.g., from movie reviews to news, ii) number of classes, ranging from 2 to 6, iii) average length of the documents ranging from sentences (e.g., 6.7 words on an average in CoLA) to relatively large text (e.g., 52.4 words on an average in RTE). This diversity in the characteristics of the respective datasets (see Table 1) thus allows provision to assess the generalizability of our proposed approach. More details on each dataset is as follows.\n\u2022 SST2: The Stanford Sentiment Treebank (SST) corpus consists of 11855 sentences extracted from movie reviews. The SST2 (also called SSTbinary) dataset is a subset of SST, specifically prepared for the task of binary sentiment classification by discarding neutral sentences.\n\u2022 TREC: The task is that of open-domain question classification of fact-based questions categorized into a total of 6 types (Ma et al., 2023), constituting 5500 labeled questions in the training set and 500 in the test set.\n\u2022 CoLA: The Corpus of Linguistic Acceptability (CoLA) dataset consists of 10657 sentences from 23 linguistics publications annotated for grammatical correctness. It has a wide coverage of syntactic and semantic phenomena.\n\u2022 RTE: The Recognizing Textual Entailment (RTE) dataset comes from a series of annual textual entailment challenges. The authors of the\n<div style=\"text-align: center;\">Avg Len (#words)</div>\n# GLUE benchmark (Wang et al., 2018) combined the data from RTE1, RTE2, RTE3 and RTE5.\nGLUE benchmark (Wang et al., 2018) combined the data from RTE1, RTE2, RTE3 and RTE5.\n# 4.2 Methods Investigated\nBaselines. We compare our proposed AICL methodology with the following baselines:\n\u2022 0-shot: This approach tests a model\u2019s innate capabilities, i.e., the capability to operate without any training data (information on class labels). Specifically, this requires setting k = 0 in Equation 1.\n FICL (Liu et al., 2022a) (Fixed ICL): This refers to the standard method of applying a static (fixed) number of localized (semantically similar) examples as input. The number of examples to use in FICL is determined by optimizing k \u2208{1, . . . , 10} by means of grid search on a validation set, which comprises 10% of the training set (the same training set is also used in AICL).\nVariants of AICL. We employ the following variants for our proposed methodology of employing a variable number of ICL examples.\n\u2022 AICL with Embedding-only - AICL(E): This is an ablation study, where the number of examples for each data instance is only a function of its textual content - more specifically its SBERT (Reimers and Gurevych, 2019) embedding (see Equation 4).\n# \u2022 AICL with Embedding + Neighborhood class\nlabels - AICL(E+N): Similar to AICL(E) except that the input to the multi-label classifier is now the embedding of a training data instance along with the class labels of its neighbors (see Equation 6).\nAs part of the analysis, we also report the performance of AICL under an ideal setting, i.e., when one can choose the best prediction from among a set of predictions obtained with different values of k in k-shot LLM-based inference by making use of the test set labels. Despite being non-pragmatic,\nLlama-2-7b\nPhi-2\nLlama-2-13b\nMethod\nk\nPrec\nRec\nF1\nk\nPrec\nRec\nF1\nk\nPrec\nRec\nF1\nSST2\n0-shot\n0\n.7227\n.6123\n.5589\n0\n.8988\n.8919\n.8914\n0\n.7341\n.6443\n.5891\nFICL\n9\n.8744\n.8687\n.8682\n10\n.9279\n.9253\n.9252\n9\n.8912\n.8733\n.8809\nAICL(E)\n8.63\n.9089\n.8946\n.8925\n6.39\n.9307\n.9300\n.9300\n5.73\n.9175\n.9065\n.9028\nAICL(E+N)\n8.23\n.9099\n.8964\n.8954\n5.64\n.9350\n.9345\n.9345\n4.89\n.9189\n.9071\n.9034\nAICL*\n1.27\n.9765\n.9753\n.9753\n.17\n.9865\n.9863\n.9863\n.84\n.9866\n.9859\n.9848\nTREC\n0-shot\n0\n.1697\n.1687\n.0100\n0\n.6289\n.4273\n.3526\n0\n.3212\n.3685\n.3016\nFICL\n9\n.7388\n.7324\n.6608\n8\n.6842\n.6582\n.6192\n9\n.7612\n.7922\n.7071\nAICL(E)\n9.60\n.7654\n.8049\n.7325\n4.72\n.7146\n.7549\n.7196\n5.51\n.7659\n.8218\n.7383\nAICL(E+N)\n9.58\n.7682\n.8075\n.7357\n5.57\n.7254\n.7673\n.7291\n4.93\n.7737\n.8288\n.7532\nAICL*\n3.35\n.8496\n.9334\n.8616\n1.27\n.9337\n.9294\n.9313\n2.25\n.9513\n.9367\n.9413\nCoLA\n0-shot\n0\n.5585\n.5323\n.3699\n0\n.4315\n.4899\n.2558\n0\n.6474\n.4321\n.3455\nFICL\n10\n.6240\n.6415\n.6008\n9\n.7071\n.6306\n.6433\n8\n.7167\n.6338\n.6472\nAICL(E)\n8.32\n.6556\n.6741\n.6582\n7.73\n.7289\n.6471\n.6601\n2.33\n.7412\n.6447\n.6667\nAICL(E+N)\n7.43\n.6580\n.6765\n.6616\n7.93\n.7392\n.6486\n.6613\n1.99\n.7432\n.6558\n.6714\nAICL*\n2.42\n.9417\n.9281\n.9299\n.69\n.9663\n.9232\n.9413\n.78\n.9408\n.9466\n.9483\nRTE\n0-shot\n0\n.4913\n.4992\n.3985\n0\n.6741\n.6741\n.6741\n0\n.5345\n.5444\n.4403\nFICL\n10\n.6095\n.6051\n.5967\n1\n.7347\n.7239\n.7240\n4\n.6287\n.6233\n.6214\nAICL(E)\n8.57\n.6304\n.6277\n.6227\n2.44\n.7536\n.7412\n.7415\n5.16\n.7592\n.6441\n.6700\nAICL(E+N)\n8.32\n.6342\n.6311\n.6252\n1.15\n.7551\n.7465\n.7471\n3.68\n.7631\n.6485\n.6738\nAICL*\n.95\n.9783\n.9783\n.9783\n.81\n.9296\n.9214\n.9234\n.65\n.9734\n.9733\n.9733\nTable 2: A comparison between the two variants of our proposed Adaptive ICL approach and the baselines on the 4 datasets used in our experiments. The column \u2018k\u2019 denotes the number of few-shot examples. In particular, for AICL, this denotes the average of the number of examples used for each test instance. In our experiments, the maximum value of k (i.e., M of Equation 3) is set to 10 for both AICL and FICL. We found that the AICL(E+N) results are statistically distinguishable with respect to the FICL results (McNemar\u2019s test with p = 95% confidence interval).\nthis method provides an upper bound on the effectiveness that could, in principle, be obtained with the AICL framework. To avoid unfair comparisons of AICL* with AICL, we gray out the AICL* results indicating that these results are only to be used as an apex-line. Corresponding to RQ-4, since constructing the ground-truth via Equation 3 requires a total of (M + 1) LLM calls for each training instance, for efficiency reasons, we first train the multi-label classifier of the AICL workflow only on a subset of training data obtained by randomly sampling 50% of data from each class. Later on, as post-hoc analysis towards answering RQ-4, we also investigate if AICL results further improve (or degrade) with larger (or smaller) subsets sampled from the training data.\n# 4.3 Model and hyper-parameter settings\nTo learn the number of examples to be used during inference, the AICL workflow relies on training the \u03ba(x) predictor (multi-label classifier of Equations 4 and 6), which in turn, depends on the groundtruth labels (K of Equation 3) of the number of examples that lead to correct predictions via LLMbased few-shot inference. But this means that these ground-truth labels depend on a particular choice\nof the LLM that is used during the training phase to check the accuracy of the downstream task labels (see Equation 3).\nTo analyze the variations that may be caused due to this choice of LLMs, we conduct our experiments on three different LLMs - two each from the same family with different sizes, and one from a different family. The objective is to analyze the effects of variations in AICL\u2019s performance corresponding to the size of a model from the same family, or variations in characteristics in models across different families. In particular, as instances of LLMs from the same family we use Meta\u2019s Llama-2-7b and Llama-2-13b (Touvron et al., 2023), whereas as another LLM from a different family, we use Microsoft\u2019s Phi-2 (Li et al., 2023b). Details on these models are as follows: Llama-2 is a collection of open-source pretrained and fine-tuned text models with a maximum context length of 4096 tokens (we use the 7B and the 13B parameter models). Phi-2 (Li et al., 2023b) is a transformer with 2.7 billion parameters. It is trained on documents comprising Python code, StackOverflow, websites and natural language text generated by GPT-3.5. Phi-2 with less than 3B parameters has been shown to yield comparable performance with those achieved with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/443a/443ad955-d6f8-433e-9aaa-474e668b56e3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) CoLA</div>\n<div style=\"text-align: center;\">Figure 3: Macro-averaged F1 scores of FICL with different number of examples on the test splits of the different datasets. We also include the AICL results as a point for comparison. Since AICL does not depend on a fixed value of k, these results ppear as a horizontal line. These plots demonstrate that AICL can be applied on any dataset without requiring to optimize any yper-parameter (e.g., k in FICL).</div>\nmuch larger number of parameter, e.g., the 13B variant of Llama-2. The maximum input length of Phi-2 is 2048 tokens. In our experiments, we set M (the maximum number of ICL examples) to 10 (see Equation 3). Furthermore, for training the multi-label classifier parameters \u03ba(x) of Equations 4 and 6, we choose a single hidden layer network with 64 neurons. These decision choices are based on initial experiments of hyper-parameter tuning conducted on the SST2 dataset with the Llama-2-7b model.\n# 4.4 Results\nMain observations. Table 2 shows a comparison between different ICL approaches on the four datasets. The reported precision, recall and Fscores are averaged over 5 runs of the experiments. The number of examples in FICL, as shown in the corresponding column, were obtained by a grid search over k from 1 to 10 on a validation set. From Table 2, it can be seen that AICL(E+N) turns out to be the best among the competing approaches - this method being statistically distinguishable than the\n<div style=\"text-align: center;\">(d) RTE</div>\nFICL results (McNemar\u2019s test with 95% confidence interval). The likely reason it outperforms the baselines is that AICL is able to effectively adapt the number of examples to use, thereby preventing itself from the degradation effects of non-relevant (not useful) examples. In effect, it learns a latent relationship between the topical content and the quantity of context required to guide the decoder\u2019s output towards improving a downstream task. Our observations reveal that RQ-1 is answered in the affirmative, i.e., an adaptive selection of the number of examples in ICL does improve the downstream effectiveness. For the AICL approaches, we report the average number of examples, as predicted by the output of the multi-label classifier (Equations 4 and 6), in the column named k. In contrast to the grid-searched integer k values in FICL, for AICL the number of examples (k) is an average over the test instances, and hence is a non-integer. One interesting observation is that the oracle version of AICL (AICL*) performs substantially better than AICL with much smaller values of k,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c5d6/c5d610c4-661c-40e6-b7ce-d5cc7d9de487.png\" style=\"width: 50%;\"></div>\nFigure 4: Macro-averaged F1 scores with different proportions of the training set used to train \u03ba(x) - the multi-label classif of AICL (Equation 6).\nwhich testifies the fact that selecting the correct value of k independently for each test instance can lead to large improvements in few-shot prediction effectiveness. This also means that there is a scope to further improve the AICL workflow.\nSensitivity Analysis of FICL (Fixed-k ICL). Figure 3 shows the F1-scores of FICL on the test splits of the different datasets with a varying number of demonstrations. Firstly, it can be observed from Figure 3 that there is no consistent improvement in the performance of FICL with an increase in k. Since the FICL results are sensitive to k, it is difficult to choose a particular value of k for a given downstream task because such a grid search is not a practical solution as the ground-truth is not supposed to be known apriori. We already showed in Table 2 that optimizing k by grid search on a validation set produces worse results than AICL. Additionally, Figure 3 shows that the AICL results also outperform FICL with k optimized on the test set. This indicates that AICL can, in principle, be deployed on any downstream task without requiring any hyper-parameter optimization of k.\nEffect of neighbourhood. Among the two variants of AICL, it can be seen from Table 2 that the use of neighbor class labels (the \u2018E+N\u2019 variant) yields better results than the ablated (\u2018E\u2019 variant), which means that RQ-2 is answered in affirmative. In relation to RQ-3, Table 2 shows that the improvements in AICL with respect to FICL is consistent across different datasets and also across different LLMs.\nMulti-label classifier in AICL with reduced training data. It is evident from Table 2 that for each dataset, the best value of k in FICL differs from one LLM to another. This means to construct the correct ground-truth for training the multi-label classifier in AICL (Equation 6), M + 1 LLM predictions need to be made for each instance, which means that the total number of LLM invocations is |T |(M + 1), where T denotes the training set. A solution to reduce training time is to use a small subsample of the training data to learn the parameters of Equation 6. Figure 4 shows the variations in F1 scores for different proportions of the training set used to learn \u03ba(x). An interesting observation is that AICL is able to outperform the\nbaseline with as small a proportion as 30% (or even less in some cases) of the training set being used to train \u03ba(x), which means that RQ-4 is answered in the affirmative.\n# 5 Conclusion and Future work\nTo improve the effectiveness of LLM-based fewshot inference, we proposed a supervised multilabel classification approach that learns how many examples to use for which data instance. The classifier itself is trained on a set of labeled examples, where the ground-truth for each training data instance constitutes the set of k (number of examples) that lead to correct downstream task prediction via k-shot LLM inference.\n# References\nSimran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher R\u00e9. 2022. Ask me anything: A simple strategy for prompting language models.\nSimran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher R\u00e9. 2022. Ask me anything: A simple strategy for prompting language models. Dara Bahri, Yi Tay, Che Zheng, Donald Metzler, and Andrew Tomkins. 2020. Choppy: Cut transformer for ranked list truncation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201920, page 1513\u20131516, New York, NY, USA. Association for Computing Machinery. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc. Suchana Datta, Debasis Ganguly, Derek Greene, and Mandar Mitra. 2022. Deep-qpp: A pairwise interaction-based deep learning model for supervised query performance prediction. In WSDM \u201922: The Fifteenth ACM International Conference on Web Search and Data Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022, pages 201\u2013209. ACM. Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active prompting with chain-ofthought for large language models. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nDara Bahri, Yi Tay, Che Zheng, Donald Metzler, and Andrew Tomkins. 2020. Choppy: Cut transformer for ranked list truncation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201920, page 1513\u20131516, New York, NY, USA. Association for Computing Machinery.\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nZhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234. Debasis Ganguly and Emine Yilmaz. 2023. Queryspecific variable depth pooling via query performance prediction. In SIGIR, pages 2303\u20132307. ACM. Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. 2023. Understanding in-context learning via supportive pretraining data. arXiv preprint arXiv:2306.15091. Evangelos Kanoulas, Ben Carterette, Paul D. Clough, and Mark Sanderson. 2011. Evaluating multi-query sessions. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201911, page 1053\u20131062, New York, NY, USA. Association for Computing Machinery. Sawan Kumar and Partha P. Talukdar. 2021. Reordering examples helps during priming-based fewshot learning. In ACL/IJCNLP (Findings), volume ACL/IJCNLP 2021 of Findings of ACL, pages 4507\u2013 4518. Association for Computational Linguistics. Itay Levy, Ben Bogin, and Jonathan Berant. 2023. Diverse demonstrations improve in-context compositional generalization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401\u2013 1422, Toronto, Canada. Association for Computational Linguistics. Minghan Li, Xueguang Ma, and Jimmy Lin. 2022. An encoder attribution analysis for dense passage retriever in open-domain question answering. In Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022), pages 1\u201311, Seattle, U.S.A. Association for Computational Linguistics. Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu Chen. 2023a. Few-shot in-context learning on knowledge base question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6966\u20136980, Toronto, Canada. Association for Computational Linguistics. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u2013 4597, Online. Association for Computational Linguistics. Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, page 1\u20137, USA. Association for Computational Linguistics.\nItay Levy, Ben Bogin, and Jonathan Berant. 2023. Diverse demonstrations improve in-context compositional generalization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401\u2013 1422, Toronto, Canada. Association for Computational Linguistics.\nXin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, page 1\u20137, USA. Association for Computational Linguistics.\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022a. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61\u201368, Dublin, Ireland. Association for Computational Linguistics. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. Gpt understands, too. AI Open. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr. icl: Demonstration-retrieved in-context learning. arXiv preprint arXiv:2305.14128. Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, and Mehran Kazemi. 2024. In-context learning with retrieved demonstrations for language models: A survey. Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, and Bingzhe Wu. 2023. Fairnessguided few-shot prompting for large language models. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS). Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. 2023. In-context learning for text classification with many labels. In Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pages 173\u2013184, Singapore. Association for Computational Linguistics. Sheshera Mysore, Andrew Mccallum, and Hamed Zamani. 2023. Large language model augmented narrative driven recommendations. In Association for Computing Machinery, RecSys \u201923, page 777\u2013783, New York, NY, USA.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. Gpt understands, too. AI Open.\nianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844\u20139855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021. Large dual encoders are generalizable retrievers.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics.\nTimo Schick, Helmut Schmid, and Hinrich Sch\u00fctze. 2020. Automatically identifying words that can serve as labels for few-shot text classification. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5569\u20135578, Barcelona,\nSpain (Online). International Committee on Computational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255\u2013269, Online. Association for Computational Linguistics.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics.\nJason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, and Shiqi Xu. 2021. Few-shot text classification with triplet networks, data augmentation, and curriculum learning. arXiv preprint arXiv:2103.07552.\nason Wei, Chengyu Huang, Soroush Vosoughi, Yu Cheng, and Shiqi Xu. 2021. Few-shot text classification with triplet networks, data augmentation, and curriculum learning. arXiv preprint arXiv:2103.07552.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 214\u2013229. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations (ICLR 2023). Xiao-Feng Zhong, Shi-Ze Guo, Liang Gao, Hong Shan, and Jing-Hua Zheng. 2017. An improved k-nn classification with dynamic k. In Proceedings of the 9th International Conference on Machine Learning and Computing, ICMLC \u201917, page 211\u2013216, New York, NY, USA. Association for Computing Machinery.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of existing in-context learning (ICL) methods that use a fixed number of examples for each data instance. It highlights the necessity for a dynamic approach that predicts the optimal number of examples to improve downstream task performance.",
        "problem": {
            "definition": "The problem is the inefficiency of using a fixed number of examples in ICL, which does not account for the varying needs of different instances, leading to suboptimal performance.",
            "key obstacle": "The main challenge is that existing methods fail to adaptively select the number of examples based on the specific characteristics of each test instance, which can lead to either insufficient context or overwhelming irrelevant information."
        },
        "idea": {
            "intuition": "The idea is inspired by information retrieval, where the effectiveness of retrieved documents varies based on the query's nature, suggesting that the number of examples in ICL should also be adaptable.",
            "opinion": "The proposed idea, Adaptive In-Context Learning (AICL), involves predicting the number of examples needed for each instance, rather than using a fixed number. This aims to enhance the relevance and effectiveness of the provided context.",
            "innovation": "The key innovation of AICL is the dynamic prediction of the number of examples to use for each instance, contrasting with traditional ICL methods that apply a static number of examples universally."
        },
        "method": {
            "method name": "Adaptive In-Context Learning",
            "method abbreviation": "AICL",
            "method definition": "AICL is a supervised multi-label classification approach that determines the optimal number of examples to use for each data instance in ICL, based on training data.",
            "method description": "AICL predicts the number of context examples needed for each test instance to improve downstream task performance.",
            "method steps": [
                "Train a multi-label classifier using training data to learn the mapping between test instance features and the optimal number of examples.",
                "For each test instance, use the classifier to predict the number of examples needed.",
                "Implement the ICL process with the predicted number of examples for downstream predictions."
            ],
            "principle": "AICL is effective because it tailors the number of examples to the specific requirements of each instance, thereby reducing the noise from irrelevant examples and enhancing prediction accuracy."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on four standard text classification datasets: SST2, TREC, CoLA, and RTE, comparing AICL against fixed ICL methods.",
            "evaluation method": "Performance was measured using precision, recall, and F1 scores, averaged over multiple runs, to assess the effectiveness of the adaptive selection of examples."
        },
        "conclusion": "The experiments demonstrated that AICL significantly improves downstream task performance compared to fixed ICL methods, confirming that adaptively selecting the number of examples enhances effectiveness.",
        "discussion": {
            "advantage": "AICL's main advantage is its ability to dynamically adjust the number of examples based on the instance's needs, leading to better performance without the need for hyper-parameter tuning.",
            "limitation": "A potential limitation of AICL is the dependency on the quality of the training data used to train the multi-label classifier, which may affect its generalizability across different tasks.",
            "future work": "Future research could explore further optimizations of the classifier, investigate its application to other NLP tasks, and assess its performance across a broader range of datasets."
        },
        "other info": {
            "authors": [
                "Manish Chandra",
                "Debasis Ganguly",
                "Iadh Ounis"
            ],
            "institution": "University of Glasgow",
            "contact": [
                "m.chandra.1@research.gla.ac.uk",
                "Debasis.Ganguly@glasgow.ac.uk",
                "iadh.ounis@glasgow.ac.uk"
            ],
            "date": "18 Oct 2024",
            "arxiv": "arXiv:2403.06402v2"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the limitations of existing in-context learning methods that use a fixed number of examples for each data instance."
        },
        {
            "section number": "1.2",
            "key information": "The necessity for a dynamic approach in in-context learning is highlighted to improve downstream task performance."
        },
        {
            "section number": "3.1",
            "key information": "AICL (Adaptive In-Context Learning) predicts the number of context examples needed for each test instance to enhance prediction accuracy."
        },
        {
            "section number": "3.2",
            "key information": "The proposed AICL method tailors the number of examples to the specific requirements of each instance, reducing noise from irrelevant examples."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of AICL is inspired by information retrieval principles, where the number of examples should be adaptable based on the nature of the query."
        },
        {
            "section number": "6.1",
            "key information": "A potential limitation of AICL is its dependency on the quality of the training data used to train the multi-label classifier."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrated that AICL significantly improves downstream task performance compared to fixed ICL methods."
        }
    ],
    "similarity_score": 0.743515453113,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/One size doesn't fit all_ Predicting the Number of Examples for In-Context Learning.json"
}