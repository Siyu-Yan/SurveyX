{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1409.7552",
    "title": "The Advantage of Cross Entropy over Entropy in Iterative Information Gathering",
    "abstract": "Gathering the most information by picking the least amount of data is a common task in experimental design or when exploring an unknown environment in reinforcement learning and robotics. A widely used measure for quantifying the information contained in some distribution of interest is its entropy. Greedily minimizing the expected entropy is therefore a standard method for choosing samples in order to gain strong beliefs about the underlying random variables. We show that this approach is prone to temporally getting stuck in local optima corresponding to wrongly biased beliefs. We suggest instead maximizing the expected cross entropy between old and new belief, which aims at challenging refutable beliefs and thereby avoids these local optima. We show that both criteria are closely related and that their difference can be traced back to the asymmetry of the Kullback-Leibler divergence. In illustrative examples as well as simulated and real-world experiments we demonstrate the advantage of cross entropy over simple entropy for practical applications.",
    "bib_name": "kulick2015advantagecrossentropyentropy",
    "md_text": "# The Advantage of Cross Entropy over Entropy in Iterative Information Gathering\n# Johannes Kulick Robert Lieck Marc Toussaint September 17, 2015\nJohannes Kulick Robert Lieck Marc Toussaint September 17, 2015\nAbstract\nGathering the most information by picking the least amount of data is a common task in experimental design or when exploring an unknown environment in reinforcement learning and robotics. A widely used measure for quantifying the information contained in some distribution of interest is its entropy. Greedily minimizing the expected entropy is therefore a standard method for choosing samples in order to gain strong beliefs about the underlying random variables. We show that this approach is prone to temporally getting stuck in local optima corresponding to wrongly biased beliefs. We suggest instead maximizing the expected cross entropy between old and new belief, which aims at challenging refutable beliefs and thereby avoids these local optima. We show that both criteria are closely related and that their difference can be traced back to the asymmetry of the Kullback-Leibler divergence. In illustrative examples as well as simulated and real-world experiments we demonstrate the advantage of cross entropy over simple entropy for practical applications.\n# Information gain \u00b7 Experimental design \u00b7 Exploration \u00b7 Active learning \u00b7 Cross entropy \u00b7 Robotics\nInformation gain \u00b7 Experimental design \u00b7 Exploration \u00b7 Active learning  Cross entropy \u00b7 Robotics\n# 1 Introduction\nWhen gathering information, agents need to decide where to sample new data. For instance, an agent may want to know the latent parameters of a model for making predictions or it has a number of possible hypotheses and wants to know which one is true. If acquiring data is expensive, as it is the case in real-world environments or if a human expert answers queries, it is desirable to use the least amount of data for gathering the most information possible. An agent therefore should choose queries most informative for its learning progress\u2014 which is referred to as active learning and experimental design. The commonly addressed task in these areas is to reduce the predictive uncertainty, that is, to choose queries as efficiently as possible with the aim of reducing the prediction errors of the model. In this paper, however, we also consider a slighly different task, namely to reduce the uncertainty over some\n(hyper) parameter of the model which is not observable. In the generative model Fig. 1, reduction of predictive uncertainty aims at learning the function f, whereas the alternative task is to learn about the parameter \u03b8. A widely used approach for minimizing uncertainty over the hyper parameters is to greedily minimize the expected entropy of the posterior distribution p(\u03b8|x, y, D). However, we can show that this greedy method can get trapped in erroneous low-entropy beliefs over \u03b8. We therefore suggest an alternative measure: maximizing the expected cross entropy between the prior p(\u03b8|D) and the posterior p(\u03b8|x, y, D). Although also being a one-step criterion, our cross entropy criterion avoids local optima in cases where the standard entropy criterion gets trapped. We demonstrate superior convergence rates in empirical evaluations. We show that the difference between the two criteria can be traced back to the asymmetry of the Kullback-Leibler divergence (KL-divergence) and discuss this in detail. Furthermore, we show that even in the standard case of reducing predictive uncertainty our criterion can be used to improve the convergence rate by combining it with standard uncertainty sampling. In general, computing optimal solutions to experimental design problems requires taking all possible future queries into account. This translates to solving a partially observable Markov decision problem (POMDP) [Chong et al., 2009], which generally is unfeasible to compute (see e.g. Kaelbling et al. [1998]). In the special case of submodular objective functions greedy one-step optimization has bounded regret with respect to the optimal solution [Nemhauser et al., 1978]. However, we show that the standard expected entropy criterion for the \u03b8-belief is not submodular. Therefore, the naive greedy criterion of reducing \u03b8-belief entropy is not guaranteed to have bounded regret, which is consistent with out empirical finding that it can get trapped in erroneous low-entropy belief states. Our cross entropy criterion, which measures change in belief space rather than entropy reduction, is less prone to getting trapped. In the remainder of this paper we will first discuss related work. We then formally introduce our method MaxCE and discuss the difference to the standard approach of minimizing expected entropy. After this we draw the connection to active learning methods for reducing predictive uncertainty. We empirically evaluate our method in a synthetic regression and classification scenario as well as a high-dimensional real-world scenario comparing it to various common measures. We then show how MaxCE can be used in a robotic exploration task to guide actions. Finally we discuss the results and give an outlook to future work.\n# 2 Related Work\nOur method is closely related to Bayesian experimental design, where Bayesian techniques are used to optimally design a series of experiments. The field was coined by Lindley [1956]. Chaloner and Verdinelli [1995] give an overview of the method and its various utility functions. An experiment, possibly consisting of several measurements, in this context can be seen as a single sample taking in some parameter space. The classic utility function is to maximize the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7de7/7de7c9d7-6526-4739-b4e0-9968ee8e14da.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Generative model of the data as Bayesian network. The already known data D as well as the new query x and label y are drawn from a distribution f that is determined by (hyper) parameters \u03b8. Dashed arcs describe the dependencies after marginalizing out f, dotted arcs describe the dependencies before that.</div>\nFigure 1: Generative model of the data as Bayesian network. The already known data D as well as the new query x and label y are drawn from a distribution f that is determined by (hyper) parameters \u03b8. Dashed arcs describe the dependencies after marginalizing out f, dotted arcs describe the dependencies before that.\nexpected Shannon information [Shannon, 1948] of the posterior over a latent variable of interest, which corresponds to maximizing the expected neg. entropy of the posterior or equivalently the expected KL-divergence from posterior to prior (see Sec. 3.1). Our MaxCE method is closely related in that maximizing the expected cross entropy also corresponds to maximizing the expected KL-divergence but from prior to posterior, that is, in the opposite direction. As a consequence, while the traditional experimental design objective is not well suited for greedy iterative optimization as it may get stuck in local optima (see Sec. 3.2) our MaxCE criterion overcomes this flaw while retaining the desired property of converging to low-entropy posteriors. Bayesian experimental design recently has regained interest due to an efficient implementation, the Bayesian Active Learning by Disagreement (BALD) algorithm [Houlsby et al., 2011], which exploits the equivalence of the experimental design criterion to a mutual information in order to make the computations tractable. The general approach to minimize the number of queries for a learning task is often called active learning. As a general framework, active learning comprises a variety of methods (see Settles [2012] for an overview) and is successfully used in different fields of machine learning and for a wide range of problems, as shown in a survey of projects using active learning [Tomanek and Olsson, 2009]. However, it mainly focuses on reducing predictive uncertainty (predictive error or predictive entropy) for a single model, whereas our method aims at learning hyper parameters such as selecting the correct model out of several possible candidates. Model selection techniques on the other side mainly focus on criteria to estimate the best model (or hypothesis) given a set of training data, which can also be seen as learning of latent parameters of a model. Well known criteria are Akaikes Information Criterion (AIC) [Akaike, 1974, Burnham and Anderson, 2004] or the Bayesian Information Criterion (BIC) [Schwarz, 1978, Bhat and Kumar, 2010]. Both are based on the likelihood ratios of models and are approximations of the distribution over the latent variable. Specifying concrete likelihood models we can infer the distribution of the variable of interest\ndirectly and do not need to approximate them. In certain cases it might however be useful to apply approximations to speed up the method. Another approach to rate a model is cross-validation (see e.g. Kohavi [1995]), which statistically tests models with subsets of the training data for their generalization error. All model selection techniques have in common that they measure the quality of a model given a data set. They are not often used for actively sampling queries and in the case of predictive error, this might actually fail for the \u201cActive Learning with Model Selection Dilemma\u201d [Sugiyama and Rubens, 2008]. We observe a similar problem, when measuring predictive error in our experiments (see Sec. 5). Our methods on the other side is developed for actively choosing queries. Query-by-committee (QBC) as introduced by Seung et al. [1992] tries to use active learning methods for version space reduction. The version space is the space of competing hypothesis. QBC finds new samples by evaluating the disagreement within a set of committee members (that is, different hypotheses) concerning their predictions. These samples are then used to train the different models. In a binary classification scenario disagreement is easy to determine. In multi-class or regression scenarios it is harder to define. One approach, as suggested by McCallum and Nigam [1998], is to use as measure of disagreement the sum of KL-divergences from each committee member\u2019s predictive belief to the mean of all committee member\u2019s predictive belief. While QBS amis at finding the correct hypothesis, it still focuses on the prediction error. We will empirically compare our approach to QBC. Another variant of active learning are expected model change methods such as the expected gradient length algorithm [Settles et al., 2008]. These methods measure the change a model undergoes by adding another observation. Our method is in spirit related and might arguably be classified as a variant of expected model change since we are also interested in finding samples that contain a maximum amount of information with respect to the model. However, we apply the idea of greatest model change directly to the distribution of hypotheses by measuring the KL-divergence between the distribution before and after new observations have been incorporated. In contrast, existing methods stay within one fixed model and measure the change of this fixed model. Those methods can thus not directly be used for discriminating between hypotheses. In our work on joint dependency structure exploration [Kulick et al., 2015], we used our method. There we focused on modeling the joint dependency structure. We analyze these experiments with respect to the MaxCE method further in Sec. 6. Similar to our robot experiment is the work of Hausman et al. [2015]. They state that the KL divergence is the information gain about a distribution, but turn it around without further explanation and analysis. In this way, they implemented our MaxCE criterion, as we will show. Their results support our finding that MaxCE is an improvement above traditional Bayesian experimental design. For some experiments we use Gaussian Processes for regression and classification. See Rasmussen and Williams [2006] for an extensive introduction.\n# 3 Information Gathering Process\nLet \u03b8, x, y, D, and f be random variables. \u03b8 denotes a latent random variable indicating the hypothesis of interest, e.g. model class, hyper parameter or other latent parameter. Conditional to \u03b8 we assume a distribution over functions f, for instance a Gaussian Process prior, generating the observable data. (In the classification case these are discriminative functions.) D are the data observed so far consisting of (xi, yi) input-output pairs where P(yi|xi, f) depends on f. x is the input that is to be chosen actively and y is the corresponding output received in response. The graphical model in Fig. 1, neglecting the dashed arcs, describes their dependence. For gathering information over \u03b8 we will have to express the expected information gain about \u03b8 depending on x and usually eliminate f. In the graphical model, after eliminating f, the dashed arcs describe the dependencies. The task now is to gather the most information with the least queries. If we assume a ground truth distribution P(\u03b8\u2217), we can formulate this task for a given horizon of K queries as to minimize the KL-divergence between P(\u03b8\u2217) and the posterior over \u03b8.\n(x\u2217 1, . . . , x\u2217 K) = argmin (x1,...,xK) DKL (P(\u03b8\u2217) \u2225P(\u03b8|D))\nwith D = {(x1, y1), . . . , (xK, yK)}. But unfortunately the distribution P(\u03b8\u2217) is unknown and is in fact the desired piece of information we want to infer. Thus we can immanently not compute this KL-divergence. In many cases it is however reasonable to assume P(\u03b8\u2217) having a low entropy. This assumption holds e.g. for the case of a single \u201ctrue\u201d hypothesis or a single \u201ctrue\u201d value of a measured constant. Under these circumstances it is reasonable to minimize the entropy of P(\u03b8|D)\nwith D = {(x1, y1), . . . , (xK, yK)}. This still includes reasoning over all K future queries and is generally computationally intractable. Thus the scenario we describe here will be iterative. The choice of a new query point x is guided by an objective function that scores all candidate points. The candidate with an optimal objective value is then used to generate a new data point (x, y) which, for the next iteration, is added to D.\n# 3.1 Expected Entropy versus Cross-Entropy\nFrom Eq. (2) and the general task to gather information it is very intuitive to minimize expected hypotheses entropy1 in each step. This is a common utility\n1Note that this is often called the conditional entropy and written H(\u03b8|y). To avo confusion, we will not use this shorthand notation but explicitly state the distribution we tak\n1Note that this is often called the conditional entropy and written H(\u03b8|y). To avoid confusion, we will not use this shorthand notation but explicitly state the distribution we take\n(1)\n(2)\nIt is very instructive to rewrite this same criterion in various ways. We can for instance subtract H[p(\u03b8|D)], as it is a constant offset to the maximizing operator (see App. A for the detailed transformations):\nThese rewritings of expected entropy establish the direct relation to Eq. (3) and (4) in Chaloner and Verdinelli [1995]. We find that xNE can be interpreted both as maximizing the expected neg. entropy, as in Eq. (3), or maximizing the expected KL divergence, as in Eq. (6). Minimizing the expected model entropy is surely one way of maximizing information gain about \u03b8. However, in our iterative setup we empirically show that this criterion can get stuck in local optima: Depending on the stochastic sample D, the hypotheses posterior p(\u03b8|D) may be \u201cmislead\u201d, that is, having low entropy while giving high probability to a wrong choice of \u03b8. The same situation arises when having a strong prior belief over \u03b8, which is a common technique in Bayesian modeling for incorporating knowledge about the domain. The knowledge is formalized as probability distribution over the possible outcomes, as for instance in our robotic experiments in Sec. 6. As detailed below, the attempt to further minimize the entropy of p(\u03b8|D, x, y) in such a situation may lead to suboptimal choices of xNE that confirm the current belief instead of challenging it. This is obviously undesirable, instead we want a robust belief that cannot be changed much by future observations. We therefore want to induce the biggest change possible with every added observation. In that way, we avoid local minima that occur if a belief is wrongly biased. While minimizing the entropy would in this situation avoid observations that change the belief, measuring the change of the belief regards an increase of entropy as a desirable outcome. While a naive approach could be to maximize the expected change of the entropy\n\ufffd\ufffd \ufffd\ufffd the entropy of and over what random variable we will take the expectation over. Also see Sec. 3.3.\n\ufffd\ufffd \ufffd\ufffd the entropy of and over what random variable we will take the expectation over. Also see Sec. 3.3.\n(3)\n(4)\n(5)\n(6)\n(7)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cc97/cc978b32-43e4-4c4d-b013-a65eeb01d520.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 2: Characteristics of three different criteria for choosing samples: Cross entropy, neg. entropy, and change of entropy. The belief is over a binary variable. The black dot indicates the prior belief of 0.25, the blue and yellow dot indicate the posterior after having seen two different observations. Cross entropy regards a change in any direction as an improvement. Neg. entropy, in contrast, prefers changes that support the current belief over those that challenge it \u2013 unless the posterior belief flips to having an even lower entropy than the prior. Change of entropy is similar to cross entropy in that for small changes it regards any direction as an improvement. For larger changes, however, is has a local optimum for a flat posterior of 0.5 and a local minimum for a flipped posterior with the same entropy as the prior.\nthis criterion has two undesirable pathologies (1) it always has a local maximum for a flat posterior belief with maximum entropy\u2014unless the prior is already flat\u2014and (2) changing a strong belief, say 0.25/0.75 for a binary hyper parameter, to the equally strong but contradictory belief of 0.75/0.25 is one of the global minima with zero change of entropy (see Fig. 2). Another criterion that measures the change of the belief is the cross entropy between the current and the expected belief. This can be seen in Fig. 3. Whereas the neg. entropy is the same for all prior beliefs, the cross entropy is high, when prior and posterior belief disagree. Intuitively neg. entropy actually does not measure the change of distributions, but only the information of the posterior belief p(\u03b8|D, x, y). We therefore propose the MaxCE strategy which maximizes the expected cross entropy between the prior hypotheses belief p(\u03b8|D) and the posterior hypotheses belief p(\u03b8|D, x, y). This, again, can be transformed to maximizing the KL-divergence, but now with switched arguments (see again App. A for details).\nwhere H[p(z), q(z)] = \u2212 \ufffd z p(z) log q(z).\n<div style=\"text-align: center;\"></div>\n(8)\n(9)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5256/5256f0ce-1f86-4cdd-a80e-0c6b4e90bdd9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 3: Cross entropy and neg. entropy as a function of prior and posterior belief of two possible hypotheses. Values are normalized and the axes show the probability of one of the two hypotheses. Maximizing the cross entropy prefers a high entropy only if reached by a change of the belief while maximizing the neg. entropy ignores the prior belief.\nThe KL-divergence DKL (p(\u03b8|D) \u2225p(\u03b8|D, x, y)) literally quantifies the additional information captured in p(\u03b8|D, x, y) relative to the previous knowledge p(\u03b8|D). This does not require the entropy to decrease: the expected divergence DKL (p(\u03b8|D) \u2225p(\u03b8|D, x, y)) can be high even if the expected entropy of the distribution p(\u03b8|D, x, y) is higher than H[p(\u03b8|D)]\u2014so our criterion is not the same as minimizing expected model entropy. In comparison to the KL-divergence formulation Eq. (6) of expected entropy the two arguments are switched. The following example and the later quantitative experiments will demonstrate the effect of this difference.\n# 3.2 An Example for Maximizing Cross Entropy Where Minimizing Entropy Gets Trapped\nBayesian experimental design suggests to minimize the expected entropy of the model distribution Eq. (5). As we stated earlier, this may lead to getting stuck in local optima for an iterative scenario. We now explicitly show an example of such a situation. Assume a regression scenario where two GP hypotheses should approximate a ground truth function. Both GPs use a squared exponential kernel, but have a different length scale hyper parameter. One of these GPs is the correct underlying model. Consider now a case where the first two observations by chance support the wrong hypothesis. This may happen due to the fact that the ground truth function that is actually sampled is itself only a random sample from the prior over all functions described by the underlying GP. Furthermore observations may be noisy, which may lead to a similar effect. Such a scenario\u2014one that actually occurred in our experiments\u2014is shown in Fig. 4. The probability for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a970/a9707a0c-9f8b-477c-bc2a-3f84bc0d4646.png\" style=\"width: 50%;\"></div>\nFigure 4: The top graph shows two competing hypotheses where one corresponds to the correct model (the Gaussian process the data are actually drawn from) and the other is wrong (a Gaussian process with a narrower kernel). For the two observations seen so far, the current belief is biased towards the wrong model because it is the more flexible one. The two curves below correspond the expected neg. entropy Eq. (5) and the expected cross entropy Eq. (6) of the belief after performing a query at the corresponding location. The arrows indicate the query location following each of the two objectives.\nthe wrong model in this scenario is already around 90%. If we now compute the expected neg. entropy from Eq. (5) it has its maximum very close to the samples we already got. This is due to the fact that samples possibly supporting the other\u2014the correct\u2014model would temporarily decrease the neg. entropy. It would only increase again if the augmented posterior actually flipped and the probability for the correct model got higher then 90%. The MaxCE approach of maximizing cross entropy (see Eq. (8)) on the other hand favors changes of the hypotheses posterior in any direction, not only to lower entropy, and therefore recovers much faster from the misleading first samples. Fig. 4 shows both objectives for this explicit example.\n# 3.3 The Conditional (Posterior) Hypotheses Entropy i not Submodular\n# .3 The Conditional (Posterior) Hypotheses Entropy is\nWhile at the first glance this might contradict the finding that the entropy is submodular [Fujishige, 1978] and optimizing submodular functions can be done efficiently [Nemhauser et al., 1978, Iwata et al., 2001], we want to assure that it does not interfere with these facts. The submodular entropy function is a set function on set of random variables, where the entropy of the joint distribution of all variables in the set is computed. Formally if \u2126= {V1, . . . , Vn} is a set of random variables, than for any S \u2286\u2126the entropy of this subset H(S) is submodular. In contrast, we compute the entropy of the distribution of a fixed random variable, conditioned on a set of random variables (see Eq. (3)). As noted earlier this is the conditional entropy. In App. B we proof that this is not submodular. The conditional entropy is, however, monotone. This means the expectation of the entropy decreases. For particular values of the variables in S it might increase.\n# 4 Comparison to Active Learning Strategies\n# 4.1 Hypotheses Belief and Predictive Belief\n1 Hypotheses Belief and Predictive Belief\nAs opposed to existing active learning methods we define the objective function directly in terms of the hypotheses belief P(\u03b8|D) instead of the predictive belief P(y|x, D). We call P(\u03b8|D, x, y) the posterior hypotheses belief after we have seen an additional data point (x, y). Accordingly we call P(\u03b8|D) the prior hypotheses belief, even though it is already conditioned on observed data. It does, however, play the role of a Bayesian prior in computing the posterior hypotheses belief. The relation between the hypotheses posterior and predictive belief is\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd Minimizing the expected entropy on the predictive belief is the direct trans lation from Bayesian experimental design to the predictive belief:\nIn the case of Gaussian distributions of P(y|x, \u03b8, D) this is the same as minimizing the expected variance of the predictive belief, since H[N(\u00b5, \u03c32)] = 1 2 log(2\u03c0e\u03c32), which is a strictly monotonically increasing function on \u03c32. Minimizing the expected mean variance over the whole predictive space is introduced as active learning criterion by Cohn et al. [1996].\n(10)\n(11)\nAn even simpler but widely used technique is uncertainty sampling [Lewis and Gale, 1994]. This techniques samples at points of high uncertainty, measured in variance or entropy of the predictive belief. No expectation is computed here. The assumption is that samples at regions with high uncertainty will reduce the uncertainty most [Sebastiani and Wynn, 1997]. Normally, uncertainty sampling is used to train a single model, i.e. only one hypothesis is assumend, while for comparing it to our methods we have to consider a set of hypotheses. The most natural way seems to handle the set of models as a mixture model and then minimize the variance of this mixture model\n# E\u03b8 [P(y|x, \u03b8, D) \u2212E\u03b8 [P(y|x, \u03b8, D)]]\nwhere E\u03b8[\u00b7] is the expectation over all models. A mix between both worlds is Query-by-Committee (QBC) [Seung et al., 1992, McCallum and Nigam, 1998]. While aiming at discriminating between different hypotheses, it uses the predictive belief for measurements. It works as follows: QBC handles a set of hypotheses, the committee. When querying a new sample it chooses the sample with the largest disagreement among the committee members. These samples are considered to be most informative, since large parts of the committee are certainly wrong. In a binary classification scenario disagreement is easy to determine. In multi-class or regression scenarios it is harder to define. One approach, as suggested by McCallum and Nigam [1998], is to use as measure of disagreement the sum of KL-divergences from each committee member\u2019s predictive belief to the mean of all committee member\u2019s predictive beliefs.\n\ufffd\ufffd While they assign a uniform prior over hypotheses in every step, we can also compute the posterior hypotheses belief and use it is to weight the average:\n# \ufffd 4.2 Mixing Active Learning and Information Gathering\nWhile measuring the expected cross entropy is a good measure to find samples holding information about latent model parameters of competing hypothesis it might actually not query points that lead to minimal predictive error. For example, regions that are important for prediction but do not discriminate between hypothesis would not be sampled. Nevertheless, information about latent model parameters may help to increase the predictive performance as well. For our experiments we therefore additionally tested a linear combination of the MaxCE measure fCE from Eq. (8) with the uncertainty sampling measure fUS\n(12)\nIn\n(13)\n(14)\nfmix = \u03b1 \u00b7 fCE + (1 \u2212\u03b1) \u00b7 fUS .\n# 5 Experiments: Regression and Classification\nTasks that occur in real world scenarios can often be classified as either regression (predicting a function value) or classification (predicting a class label) tasks. We tested both task classes on synthetic data. The regression scenario we also tested on a real world data set. Typically one is interested in prediction performance. However, finding the correct hypothesis might help for that task as well as generalizing to further situations. We tested both in our experiments.\n# 5.1 Compared Methods\nWe compared six different strategies: our MaxCE, which maximizes the expected cross entropy (see Eq. (8)); classical Bayesian experimental design, which minimizes the expected entropy (see Eq. (3)); query-by-committee which optimizes Kullback-Leibler to the mean (see Eq. (14)); uncertainty sampling (see Eq. (12)), and random sampling, which randomly choses the next sample point. Additionally we tested a mixture of MaxCE and uncertainty sampling (see Eq. (15)). The mixing coefficient, which was found by a series of trial runs, was \u03b1 = 0.5 for both synthetic data sets and \u03b1 = 0.3 for the CT slices data set.\n# 5.2 Measures\nTo measure progress in discriminating between hypotheses we computed the entropy of the posterior hypotheses belief for each method. To measure progress in the predictive performance we plot the classification accuracy and the mean squared error for classification and regression, respectively. To compute an overall predictive performance for a method we took the weighted average over the different models, with the posterior probabilities as weights. This corresponds to the maximum a posteriori estimate of the marginal prediction\np(y|D, X) = \ufffd \u03b8 p(\u03b8|D) p(y|\u03b8, D, X) .\nFig. 5 show these measures for all our experiments.\n# 5.3 Synthetic Data\nWe tested our method in both a 3D-regression and a 3D-classification task. The setup for both experiments was essentially the same: A ground truth Gaussian Process (GP) was used to generate the data. The kernel of the ground truth GP was randomly chosen to depend either on all three dimensions (x, y, z), only a subset of two dimensions (x, y), (y, z) or (x, z), or on only one dimension (x),\n(16)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a6f5/a6f5767e-5ea9-4086-9387-1d3d85dcfbc7.png\" style=\"width: 50%;\"></div>\nFigure 5: The mean performance of the different methods for the classification tand regression tasks.\n(y) or (z). Finding the correct hypothesis in this case corresponds to a feature selection problem: uncovering on which features the unknown true GP depends on. The latent variable \u03b8, to be uncovered by the active learning strategies, enumerates exactly those seven possibilities. One run consisted of each method independently choosing fifty queries one-by-one from the same ground truth model. After each query the corresponding candidate GP was updated and the hypotheses posterior was computed. Fig. 5(a), 5(b), 5(c) and 5(d) show the mean performance over 100 runs of the synthetic classification and regression tasks, respectively. Since we average over 100 runs, the error bars of the mean estimators are very small. Both hypotheses belief entropy and accuracy/mean squared error are shown. On this synthetic data MaxCE significantly outperforms all other tested methods in terms of entropy, followed by Bayesian experimental design, and the mixture of MaxCE and uncertainty sampling (Fig. 5(a) and 5(c)). As expected, In terms of classification accuracy and predictive error both MaxCE and Bayesian experimental design perform poorly. This is because their objectives are not designed for prediction but for hypothesis discrimination. However, the mixture of MaxCE and uncertainty sampling, performs best (Fig. 5(b) and 5(d)), which is presumably due to its capability to uncover the correct hypothesis quickly.\n# 5.4 CT-Slice Data\nWe also tested our methods on a high dimensional (384 dimensions) real world data set from the machine learning repository of the University of California, Irvine [Bache and Lichman, 2013]. The task on this set is to find the relative position of a computer tomography (CT) slice in the human body based on two histograms measuring the position of bone (240 dimensions) and gas (144 dimensions). We used three GPs with three different kernels: a \u03b3-exponential kernel with \u03b3 = 0.4, an exponential kernel, and a squared exponential kernel. Although obviously none of these processes generated the data we try to find the best matching process alongside with a good regression result. Fig. 5(e) and 5(f) show the mean performance over 40 runs on the CT slice data set. In the CT slice data set neither MaxCE nor Bayesian experimental design minimize the entropy quickly (Fig. 5(e)). This may be a consequence of the true model not being among the available alternatives. As a consequence both methods continuously challenge the belief thereby preventing it from converging. QBC may be subject to the same struggle, here even resulting in an increase of entropy after the first 25 samples. In contrast for uncertainty sampling, our mixture method, and random sampling the entropy converges reliably. Concerning the predictive performance MaxCE, Bayesian experimental design, and QBC do not improve noticeably over time (cf. explanation above). Again uncertainty sampling and our mixture method perform much better, while here the difference between them is not significant.\n# 6 Robot Experiment: Joint Dependency Struc-\n# 6 Robot Experiment: Joint Dependency Structure Learning\nIn another experiment we used the MaxCE method to uncover the structure of dependencies between different joints in the environment of a robot. Consider a robot entering an unknown room. To solve tasks successfully it is necessary to explore the environment for joints that are controllable by the robot, such that it is able to e.g. open drawers, push buttons or unlock a door. In earlier work we have shown how such exploration can be driven by information theoretic measures [Otte et al., 2014]. Many joints are however dependent on each other, such as keys can lock cupboards or handles need to be pressed before a door can be opened. We modeled these dependencies with a probabilistic model that captures the insight that many real world mechanisms are equipped with some sort of feedback, as for instance a force raster or click-sounds that support the use of the mechanisms to find dependencies more quickly [Kulick et al., 2015]. For the details on the model of feedback we refer to that publication. Here we show a simplified model, necessary to follow the introduction of MaxCE in this context. Fig. 6 shows the simplified graphical model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1446/144610f0-23a8-4a4d-b6a0-2c66a38c41d4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: A simplified version of the graphical model from Kulick et al. [2015], omitting the feedback of the explored object. The latent distribution to be learned is P(Dj), which captures the dependency structure as discrete distribution.</div>\nConsider an environment with N joints, where each joint might be locked or unlocked over time. The locking state might be dependent on the position of the other joints. Let Qj t, Lj t and Dj be random variables. Qj t is the joint state of the j-th joint in the environment at time t. Lj t is the locking state of the j-th joint at time t and Dj is the dependency structure of the j-th joint. Dj is a discrete variable with domain {1, . . . , N + 1}. The i-th outcome indicates that joint j is dependent on i, whereas the last outcome indicates that the joint is independent from other joints. We now want to uncover the dependency structure of all joints. Thus we want to know the distribution of all Dj. Dj here is the latent variable we want to gather information about (called \u03b8 throughout the former parts of the paper). Qj t and Lj t are the data observed so far (i.e. x and y respectively). If we want to use MaxCE to learn about the dependency structure we need to compute the expected one-step cross entropy between the current joint dependency structure PDj t and the expected joint dependency structure distribution one time step\nahead PDj t+1 and maximize this expectation to get the optimal next sample position, corresponding directly to Eq. (8):\nwith\n\ufffd \ufffd We conducted two versions of this experiment. A quantitative, but simulated version of the experiment and second a qualitative real-world experiment on a PR2 robot (see Fig. 7). In the simulated version the agent is presented an environment with three randomly instantiated furnitures as described in Table 1. In the real world experiment the PR2 robot has to uncover that a key is locking the drawer of an office cabinet.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f681/f681be6f-cb4d-4ada-b084-5fb26e0e7e2d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: A PR2 robot tries to uncover the dependency structure of a typical office cabinet by exploring the joint space of the key and the drawer.</div>\nFigure 7: A PR2 robot tries to uncover the dependency structure of a typical office cabinet by exploring the joint space of the key and the drawer.\n# 6.1 Actions and Observations\nThe robot can directly observe the joint state of all joint over time and can move the joints to a desired position. At a given position the robot can ask an oracle about the locking state of one joint.\n# 6.2 Prior\nFor the dependency distribution P(Dj) we choose the following prior:\nP(Dj = i) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if i = j (self-dependent) .7 if i = N + 1 (independent) 1 d(i,j)cN else (j depends on i)\n(17)\n(18) (19)\n(18)\n(20)\nName\nDescription\nLocking mechanism\nCupboard with handle\nA cupboard with a door\nand a handle attached to\nit.\nThe handle must be at\nupper or lower limit to\nunlock the door\nCupboard with lock\nA cupboard with a door\nand a key in a lock\nThe key must be in a par-\nticular position to unlock\nthe door\nDrawer with handle\nA drawer with a movable\nhandle\nThe handle must be at\nupper or lower limit to\nunlock the drawer\nDrawer with lock\nA drawer with a key in a\nlock\nThe key must be in a par-\nticular position to unlock\nthe drawer\n<div style=\"text-align: center;\">Table 1: Furniture used in the simulation.</div>\nwith d(i, j) being the (euclidean) distance between joint i and j and cN being a normalization constant. This captures our intuition that most joints are movable independently from the state of other joints, e.g. most joints are not lockable etc. Additionally it models our knowledge that joints that can lock each other are often close to each other. The hard zero prior for self-dependence rules out the possibility of a joint locking itself.\n# 6.3 Results and Discussion of the Simulated Experiment\nWe tested the MaxCE method, expected neg. entropy and a random strategy, each 50 times. As results we show in Fig. 8 two things. First the sum of the entropies of all P(Dj). Here one can see that only MaxCE is able to decrease the entropy significantly. As expected random apparently performs worse than neg. expected entropy. But this is a wrong conclusion: In the second plot we show how many dependencies are classified correctly, if we apply an (arbitrary) decision boundary at 0.5. Neg. expected entropy is not able to classify anything correct, but the three independent joints, which are already covered by the prior P(Dj) (whereas random is able to slowly uncover other joint dependencies). The strong prior\u2014which arguably is a reasonable one\u2014let the classical Bayesian experimental design strategy pick queries that do not uncover the true distribution but stays at the local minimum. The entropy increases in the random strategy, since the prior is already a strong belief. So during the exploration of the joints the entropy first increases and only later decreases again. The neg. entropy strategy on the other hand does not change the belief and thus keeps a lower entropy. Note that after the first step also the cross entropy criterion has slightly increased the entropy and only after three observations drop below the neg. entropy strategy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/69a0/69a047d0-bfd9-4562-8822-ceb409cfcce7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Results of simulation experiments. Left we show the sum of entropies over all dependency beliefs. Right we show the mean correctly classified joints with an arbitrary decision boundary at 0.5. (Similar figure as in [Kulick et al., 2015].)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d8f/0d8ff707-689b-4f96-90d1-962cd5183416.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Results from the real world experiment. We show the belief over the dependency structure of both joints of the drawer. (Figure as in [Kulick et al., 2015].)</div>\n# 6.4 Results and Discussion of the Real World Experiment\nIn the real world experiment we let the PR2 robot explore the office cabinet with the MaxCE strategy. It could identify the correct dependency structure after a few interactions. We show the two P(Dj) distributions in Fig. 9. Notably the distribution of the independent joint doesn\u2019t change. This comes from the fact that the robot can find no strong evidence of independence, as long as it does not have covered the whole joint space of the other joint. To understand this note that the locking state from the key never changes, i.e. it is always movable. So there is no evidence against the possibility of a dependency from the drawer to the key, since there might be a position of the drawer which locks the key. Only if the agent has seen every possible state of the drawer it can be sure that the key is independent. Since only a handful of drawer states are observed, the prior distribution almost preserves during the whole experiment.\n# 7 Conclusion and Outlook\nThe presented results strongly suggest that our newly developed strategy of maximizing the expected cross entropy is superior to classical Bayesian experimental design for uncovering latent parameters in an iterative setting. The results on predictive performance additionally demonstrate a successful application of MaxCE for prediction by mixing it with an uncertainty sampling objective. The resulting objective at the same time actively learns the latent parameters and accurate predictions. This initially goes at the expense of accurate predictions but at some point more than compensates this fall-back. This might be the case, because this way areas which are important for false model hypothesis can be ignored and thus the right model is trained better. So far our mixing strategy is rather simple. But the results suggest that the mixing helps. Investigating better mixing strategies might lead to more improvements. So far we only investigated the discrete case of k distinct models. The same techniques described in this paper may be useful to find samples to optimize continuous hyper parameter. In this case the sum over models will become an integral and efficient integration techniques need to be applied to the method to keep it computationally tractable. It also might be applicable to leverage the insight of Ko et al. [1995] that the entropy is submodular to implement efficient approximations of the optimization. Another direction of research would involve finding better optimization techniques to find the actual maxima to up the process. When using GPs all involved distributions are Gaussian (or approximated by Gaussians for the classification case). As such they are infinitely differentiable, so higher order methods might prove useful.\n# Acknowledgments\nThe CT slices database was kindly provided by the UCI machine learning repository [Bache and Lichman, 2013]. We thank Stefan Otte for help with the robot experiments. Johannes Kulick was funded by the German Research Foundation (DFG, grant TO409/9-1) within the priority programm \u201cAutonomous learning\u201d (SPP1597). Robert Lieck was funded by the German National Academic Foundation.\n# References\nHirotugo Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, pages 716\u2013723, 1974. K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http: //archive.ics.uci.edu/ml.\nHirotugo Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, pages 716\u2013723, 1974.\nHirotugo Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, pages 716\u2013723, 1974. K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http: //archive.ics.uci.edu/ml.\nHarish S. Bhat and Nitesh Kumar. On the Deviation of the Bayesian Information Criterion. Technical report, University of California, Merced, 2010. Kenneth P. Burnham and David R. Anderson. Multimodel Inference - Understanding AIC and BIC in Model Selection. Sociological Methods and Research, 33:261\u2013304, 2004. Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical Science, pages 273\u2013304, 1995. Edwin KP Chong, Christopher M Kreucher, and Alfred O Hero Iii. Partially Observable Markov Decision Process Approximations for Adaptive Sensing. Discrete Event Dynamic Systems, 19(3):377\u2013422, 2009. David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. Journal of Artificial Intelligence Research (JAIR), 4 (1):129\u2013145, 1996. Satoru Fujishige. Polymatroidal Dependence Structure of a Set of Random Variables. Information and Control, 39(1):55\u201372, 1978. Karol Hausman, Scott Niekum, Sarah Osnetoski, and Gaurav S. Sukhatme. Active Articulation Model Estimation through Intaractive Perception. 2015. Neil Houlsby, Ferenc Husz\u00b4ar, Zuobin Ghahramani, and M\u00b4at\u00b4e Lengyel. Bayesian Active Learning for Classification and Preference Learning. arXiv, 1112.5745 (stat.ML), 2011. Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. A combinatorial strongly polynomial algorithm for minimizing submodular functions. Journal of the ACM, 48(4):761\u2013777, 2001. Leslie P. Kaelbling, Michael Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains . Artificial Intelligence Journal, 101:99\u2013134, 1998. C. Ko, J. Lee, and M. Queyranne. An exact algorithm for maximum entropy sampling. Ops Research, 43:684\u2013691, 1995. Ron Kohavi. A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. In Proc. of the Int. Conf. on Artificial Intelligence (IJCAI), 1995. Johannes Kulick, Stefan Otte, and Marc Toussaint. Active Exploration of Joint Dependency Structures. In Proc. of the IEEE Int. Conf. on Robotics & Automation (ICRA), 2015. David D. Lewis and William A. Gale. A sequential algorithm for training text classifiers. In Proc. of the Ann. Int. Conf. on Research and Development in Information Retrieval, pages 3\u201312, 1994.\nD. V. Lindley. On a Measure of the Information Provided by an Experiment. Ann. Math. Statist., 27(4):986\u20131005, December 1956. Andrew McCallum and Kamal Nigam. Employing EM in pool-based active learning for text classification. In Proc. of the Int. Conf. on Machine Learning (ICML), pages 359\u2013367, 1998. George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An Analysis of Approximations for Maximizing Submodular Set Functions. Mathematical Programming, 14(1):265\u2013294, 1978. Stefan Otte, Johannes Kulick, and Marc Toussaint. Entropy Based Strategies for Physical Exploration of the Environment\u2019s Degrees of Freedom. In Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2014. Carl Rasmussen and Christopher Williams. Gaussian processes for machine learning. MIT Press, 2006. Gideon E. Schwarz. Estimating the dimension of a model. Annals of Statistics, 6:461\u2013464, 1978. Paola Sebastiani and Henry P Wynn. Bayesian experimental design and shannon information. In Proceedings of the Section on Bayesian Statistical Science, volume 44, pages 176\u2013181, 1997. Burr Settles. Active Learning. In Ronald Brachman, William Cohen, and Thomas Dietterich, editors, Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan and Claypool, 2012. Burr Settles, Mark Craven, and Soumya Ray. Multiple-Instance Active Learning. In Proc. of the Conf. on Neural Information Processing Systems (NIPS), pages 1289\u20131296, 2008. H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Proc. of the Annual Conf. on Computational Learning Theory, pages 287\u2013294, 1992. Claude Shannon. A mathematical theory of communication. Bell System Technical Journal, 27:379\u2013423, 623\u2013656, 1948. Masashi Sugiyama and Neil Rubens. Active Learning with Model Selelction in Linear Regression. In Proc. of the Int. Conf. of Data Mining, pages 518\u2013529, 2008. Katrin Tomanek and Fredrik Olsson. A web survey on the use of active learning to support annotation of text data. In Proceedings of the Workshop an Active Learning for Natural Language Processing, pages 45\u201348, 2009.\n# A Expected Kullback-Leibler divergence transformations\n# A Expected Kullback-Leibler divergence trans-\nThe KL-divergence, entropy, and cross-entropy of two distributions p and q are closely related (rows in Eq. (21)) and can be rewritten as expectation values (columns in Eq. (21))\nWhen taking the expectation of the KL-divergence over p(y|x, D), depending on the direction of the KL-divergence, either the entropy or the cross-entropy term is constant with respect to x (and therefore drops out when taking the argmaxx)\n\ufffd \ufffd\ufffd \ufffd For the step from Eq. (23) to Eq. (24), note that p(\u03b8|x, D) = p(\u03b8|D) since \u03b8 is independent of x so that for any function f(\u03b8, D) that depends only on \u03b8 and D, such as log p(\u03b8|D) above, an expectation over p(\u03b8|y, x, D) and p(y|x, D) is\n(21)\n\ufffd p(y|x,D) (22)\nx,D) (22)\n(23) (24)\n(24)\n(25)\n(26)\n(26) (27)\n(27)\nequal to an expectation over just p(\u03b8|D)\n# B Conditional Entropy Is Not Submodular\nB Conditional Entropy Is Not Submodular\nDefinition 1. For a set \u2126, the set function f : 2\u2126\u2192R is submodular if and only if\nDefinition 1. For a set \u2126, the set function f : 2\u2126\u2192R is submodular if a only if\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd with D \u2282\u2126and y1, y2 \u2208\u2126\\ D. Definition 2. For a random variable \u03b8 and a set of random variables Y ,\nLet \u03b8 be a binary random variable, and y1 and y2 be identically distribute binary random variables with\n(29) (30) (31) (32)\n(29)\n(31) (32)\n(33)\n(34)\n(35)\n(36) (37) (38) (39)\n(36)\n(38) (39)\n(39)\n# and\nH(\u03b8|y1) + H(\u03b8|y2) = 2 \u00b7 0.325 < 0.693 = H(\u03b8) \u2264H(\u03b8|y1, y2) + H(\u03b8) , (42 which contradicts Eq. (35).\n(41)\n(42)\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of efficiently gathering information in experimental design and reinforcement learning, highlighting the limitations of traditional entropy-based approaches in avoiding local optima. It introduces a novel method that maximizes expected cross entropy to improve sampling strategies.",
        "problem": {
            "definition": "The problem is to effectively gather the most information with the least amount of data, particularly focusing on reducing uncertainty over latent parameters in models.",
            "key obstacle": "Existing methods, particularly those minimizing expected entropy, can become trapped in local optima, leading to biased beliefs about the underlying parameters."
        },
        "idea": {
            "intuition": "The idea stems from observing that traditional entropy minimization can lead to suboptimal sampling strategies that reinforce incorrect beliefs.",
            "opinion": "The proposed method, MaxCE, aims to maximize the expected cross entropy between prior and posterior beliefs, thereby encouraging exploration that challenges current beliefs.",
            "innovation": "The key innovation lies in the shift from minimizing entropy to maximizing cross entropy, which allows for better exploration of the belief space and avoids local optima."
        },
        "method": {
            "method name": "MaxCE",
            "method abbreviation": "MaxCE",
            "method definition": "MaxCE is defined as maximizing the expected cross entropy between the prior and posterior distributions of the latent parameters.",
            "method description": "The method focuses on selecting queries that maximize the expected change in belief rather than simply minimizing uncertainty.",
            "method steps": [
                "Initialize prior beliefs about the latent parameters.",
                "Select a query point that maximizes the expected cross entropy.",
                "Update the posterior beliefs based on observed data.",
                "Iterate the process to refine the beliefs and improve sampling."
            ],
            "principle": "MaxCE is effective because it measures the change in belief space rather than merely reducing entropy, allowing for a more robust exploration of the parameter space."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in synthetic regression and classification tasks, as well as a real-world robotic exploration scenario, comparing MaxCE against various baseline methods.",
            "evaluation method": "Performance was assessed using metrics such as posterior belief entropy, classification accuracy, and mean squared error across different sampling strategies."
        },
        "conclusion": "The results indicate that MaxCE outperforms traditional entropy-based methods in uncovering latent parameters and achieving better predictive performance through a combination with uncertainty sampling.",
        "discussion": {
            "advantage": "MaxCE effectively avoids local optima and allows for more informative sampling, leading to faster convergence in identifying the correct models.",
            "limitation": "The method may not always guarantee optimal predictive performance, especially in scenarios where the true model is not among the considered hypotheses.",
            "future work": "Future research could explore more sophisticated mixing strategies for MaxCE and investigate its applicability to continuous hyperparameter optimization."
        },
        "other info": {
            "acknowledgments": "The CT slices database was provided by the UCI machine learning repository, and funding was received from the German Research Foundation (DFG)."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of efficiently gathering information in experimental design and reinforcement learning, highlighting the limitations of traditional entropy-based approaches."
        },
        {
            "section number": "1.2",
            "key information": "The main goal of the paper is to introduce a novel method, MaxCE, that maximizes expected cross entropy to improve sampling strategies."
        },
        {
            "section number": "1.3",
            "key information": "Understanding the proposed method is crucial as it addresses the problem of reducing uncertainty over latent parameters in models, which is a significant challenge in AI and reinforcement learning."
        },
        {
            "section number": "2.3",
            "key information": "The paper discusses the evolution of methods in information gathering, particularly focusing on the shift from traditional entropy minimization to maximizing cross entropy."
        },
        {
            "section number": "4.3",
            "key information": "The paper identifies challenges faced in traditional methods, particularly the issue of local optima in entropy-based sampling strategies."
        },
        {
            "section number": "8.2",
            "key information": "The limitations of existing methods, such as those minimizing expected entropy, can lead to biased beliefs about the underlying parameters, highlighting the need for better approaches."
        },
        {
            "section number": "8.4",
            "key information": "The discussion on future work suggests exploring more sophisticated mixing strategies for MaxCE and investigating its applicability to continuous hyperparameter optimization."
        }
    ],
    "similarity_score": 0.5358928141424806,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0141_,arti/papers/The Advantage of Cross Entropy over Entropy in Iterative Information Gathering.json"
}