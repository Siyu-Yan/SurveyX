{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2301.11916",
    "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning",
    "abstract": "In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the realworld LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information. 1",
    "bib_name": "wang2024largelanguagemodelslatent",
    "md_text": "# Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning\nXinyi Wang1, Wanrong Zhu1, Michael Saxon1, Mark Steyvers2, William Yang Wang1 1Department of Computer Science, University of California, Santa Barbara 2Department of Cognitive Sciences, University of California, Irvine {xinyi_wang, wanrongzhu, saxon}@ucsb.edu, msteyver@uci.edu, william@cs.ucsb.edu\n# Abstract\nIn recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the realworld LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information. 1\n# 1 Introduction\nTransformer-based [41] pre-trained large language models (LLMs) have demonstrated significant advancements in a variety of natural language processing (NLP) tasks. As the size of these LLMs increases, they gain \u201cin-context learning\u201d capabilities, whereby the models achieve state-of-the-art (SOTA) or near-SOTA performance by conditioning on a small number of demonstration examples at inference time, without any need for updating model parameters [4]. Below is an example input sequence for semantic analysis with in-context learning: Great movie. Positive.\\n The worst movie ever. Negative.\\n Can\u2019t wait to see the second movie! The first two lines are two demonstrations, and the third line is a test input. We expect an LLM to output the correct label Positive as a continuation. In-context learning has been demonstrated to be an effective technique for a wide range of NLP tasks. However, it is sensitive to the choice, format, and even the order of the demonstrations used [29, 20]. This makes achieving optimal performance with in-context learning a significant challenge, requiring real human effort to adjust the format and selection of demonstration examples. Heuristic solutions, such as selecting demonstrations based on the similarity between the demonstrations and test input\n[19, 37] have been proposed, but a comprehensive understanding of why certain demonstrations are effective while others are not remains elusive. Additionally, the mechanisms by which LLMs acquire in-context learning capabilities through training on natural text under the standard language model pre-training objective are not fully understood. Recent works on understanding in-context learning provide valuable insights and theoretical results [5, 1, 42, 14, 12], but are limited in scope, focusing on synthetic experiments to validate their hypotheses, while it remains unclear if these results generalize to LLMs pre-trained on real-world natural language data. Xie et al. [50] introduced a prominent result providing a latent topic (concept) variable interpretation for in-context learning. They showed that the in-context learning predictor approaches the Bayes optimal predictor when the number of demonstrations approaches infinity, under the assumption that both the pre-training data distribution and task-specific data distribution are Hidden Markov Models (HMM). However, the assumption that the data generation process is Hidden Markovian makes extrapolation of the result to natural language questionable, and restricts empirical verification to synthetic data with toy models. We are inspired by this prior work and introduce a more general and natural explanation built on realistic assumptions, which gives rise to a practical demonstration selection algorithm. Our explanation is inspired by the generation process of a topic model, i.e. a simple latent variable model: \ufffd\n\ufffd Where \u03b8 \u2208\u0398 represents a potentially high dimensional topic/concept variable, \u0398 is the space of the topic/concept variable, and w1:T refers to the token sequence of a piece of text. Note that the topic model here refers to the modern neural topic models [23, 22]. On the other hand, generative LLMs model text data according to the general probabilistic decomposition:\n\ufffd While in practice, LLMs generate new tokens based on all previous tokens, we investigate whether a simplified assumption similar to that of topic models can be made for LLMs: \ufffd\n\ufffd In this scenario, the generated tokens are assumed to be conditionally independent of previous tokens, given the latent topic (concept) variable that acts like an approximate sufficient statistic for the posterior information related to the prompt w1:t. For in-context learning, this concept variable includes format and task information. By conditioning on an appropriate latent concept variable, LLMs would generate the desired continuation with P(wt+1:T |\u03b8). As LLMs do not explicitly learn a latent variable distribution like LDA-style topic models [3], we can instead utilize this formulation under an Empirical Bayesian formulation inspired by Lester et al. [17] to only approximate the optimal latent variable value for a desired task, using a small LLM (with less than 1B parameters), which is computationally efficient. We empirically validate our explanation by selecting examples (w1:t in the equations) that are most likely to infer the optimal latent variable value (those with the highest posterior probability P(\u03b8|wt+1:T )). We then directly use them as demonstrations for in-context learning with other larger LLMs (up to 175B parameters) and observed a significant performance improvement. The generalization of demonstrations between LLMs is likely a result of similar pre-training data distributions. While our work is inspired by that of Xie et al. [50], our approach differs significantly in both theoretical analysis and experimental settings. Our main contributions are as follows: \u2022 We assume a general data generation process specified by a three-variable causal graph, without constraints on the distribution function or the number of demonstrations. \u2022 We prove under these realistic assumptions that the in-context learning predictor can reach the Bayes optimal predictor with a finite number of demonstrations chosen using the latent concept variable. \u2022 We introduce an efficient, practical demonstration selection algorithm based on our theoretical results, which can select demonstrations using a small LLM and then directly generalize the demonstrations to other LLMs. The effectiveness of our algorithm is empirically validated using real-world LLMs on both text classification tasks and math word problems.\nOur goal is to close the gap between theoretical understandings and real-world LLMs. To the best of our knowledge, our proposed latent variable explanation of in-context learning is the first Bayesian explanation that yields an effective algorithm in real-world scenarios.\n# 2 Theoretical Analysis\nIn in-context learning, the prompt w1:t is composed of several demonstrations and a test input. The generated tokens wt+1:T represent the model\u2019s prediction for the test input.\n# 2.1 Notations and Problem Setting\nSuppose the objective of our task is to predict a discrete target variable Y \u2208Y, given a token sequence X \u2208X, where X is the space of all possible token sequences. \u03b8 \u2208\u0398 is a potentially high dimensional latent variable, where \u0398 is the high dimensional space of the variable. Unlike the traditional topic model, \u03b8 is not assumed to be discrete, but continuously distributed over \u0398. To define the data generation process, we posit the existence of an underlying causal relation between X, Y , and \u03b8. We examine two potential directions of this causal relation, namely X \ufffdY \ufffd\u03b8 and Y \ufffdX \ufffd\u03b8, which can be represented mathematically as the following structural equations:\n \ufffd \ufffd Here \u03f5 \u2208E is an independent noise variable, f : X \u00d7 \u0398 \u00d7 E \u2192Y and g : Y \u00d7 \u0398 \u00d7 E \u2192X are two deterministic functions. Furthermore, we denote the joint data distribution by X, Y, \u03b8 \u223cP, and assume that Y is sampled from a uniform distribution over Y. The distinction between these two directions is crucial, as it allows us to utilize the direction in which the child variable (Y or X) is independent of the other variables, given its parents. We hypothesize that the causal direction depends on the nature of the task. For instance, in the task of predicting the sentiment (Y ) of a movie review (X), it is reasonable to assume that the opinion about the movie is formed before writing the review, thus making Y the cause of X, along with the task concept of \u201cwriting a passage to express one\u2019s opinion about the movie\" (\u03b8). Conversely, for the task of classifying whether a product review (X) is helpful to other customers (Y ), it is the quality of the review (X) that cause other customers to upvote it (Y ), along with the task concept of \u201crating the helpfulness of this review\" (\u03b8). In the rest of the paper, we will focus on the X \ufffdY \ufffd\u03b8 direction and leave a detailed discussion of the other direction in the Appendix. Suppose we are interested in a task (e.g. semantic analysis) denoted by d \u2208T , where T is the space of all possible tasks. We assume there is an injective function between T and \u0398. i.e. for each task d, there is a concept variable \u03b8d, such that each data (Xd, Y d) sampled from task d is generated by:\nTo perform in-context learning with an LLM (generically denoted by model label M), we condition on a fixed set of k demonstration examples (Xd 1, Y d 1 ), (Xd 2, Y d 2 ), ..., (Xd k, Y d k ) sampled from task d. Following previous works [24, 26], as we are not using any instruction fine-tuned models, we do not include a task description in the prompt, with the aim of focusing on the examination of the demonstrations. To naturally project Y into the token space X, we define injective mappings \u03c4 d : Y \u2192X, which are typically defined by human understanding of the task d. e.g. for sentiment analysis, \u03c4 d map positive class to the token \u201cpositive\" and negative class to the token \u201cnegative\". Additionally, a delimiter token wd is defined, typically an empty space or a new line token, to separate the demonstrations when concatenated. We denote the LLM output probability of X, Y , and \u03b8, with the aforementioned preprocessing applied, by P d M:\n# 2.2 Problem Analysis and Theoretical Results\nSuppose a set of observed data sampled from task d, denoted as Dd, is available, allowing for the selection of the k most suitable demonstrations from it. For any incoming test example X, we have: \ufffd\nSuppose a set of observed data sampled from task d, denoted as Dd, is available, allowing for the selection of the k most suitable demonstrations from it. For any incoming test example X, we have:  d d  d d  d \ufffd  d  d d  d d  d (1)\nSuppose a set of observed data sampled from task d, denoted as Dd, is available, allowing for the selection of the k most suitable demonstrations from it. For any incoming test example X, we have: P d M(Y |Xd 1, Y d 1 , ..., Xd k, Y d k , X) = \ufffd \u0398 P d M(Y |\u03b8, X)P d M(\u03b8|Xd 1, Y d 1 , ..., Xd k, Y d k , X)d\u03b8 (1)\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2918/29188955-eb89-48cb-afd9-13215fd8f85b.png\" style=\"width: 50%;\"></div>\nFigure 1: An overview of our proposed two-phased algorithm. Demonstration selection and latent concept learning share the same LLM as demonstration selection needs to reuse the learned concept tokens, while at the in-context learning time, any other generative LLMs can be used. Here we only illustrate the X \ufffdY \ufffd\u03b8 direction. The Y \ufffdX \ufffd\u03b8 direction can be illustrated similarly by exchanging X and Y in the above figure. Here, we assume the sampling of the test example is independent of the sampling of the demonstrations, so Y is independent of the demonstrations given \u03b8 and X. We also assume that the pre-trained data distribution P d M is a suitable approximation of the assumed data distribution P: Assumption 2.1. Assume that PM(X) = P(X), and P d M(Y |\u03b8, X) \u221dP(Y |\u03b8, X) for X \ufffdY \ufffd\u03b8. Note that the assumption that a large language model captures the true distribution of language is fairly common in the literature studying LLMs [50, 34, 47]. With this assumption, we establish: Proposition 2.2. If task d follows the X \ufffdY \ufffd\u03b8 direction, then arg maxy\u2208Y P d M(Y = y|\u03b8d, X) is the Bayes optimal classifier. In this case, only when P d M(\u03b8|Xd 1, Y d 1 , ..., Xd k, Y d k , X) completely concentrate on \u03b8d, can the incontext learning classifier become the Bayes optimal classifier [11]: Theorem 2.3. If task d follows the X \ufffdY \ufffd\u03b8 direction, then the in-context learning classifier arg max y\u2208Y P d M(Y = y|Xd 1, Y d 1 , ..., Xd k, Y d k , X) always has a higher or equal probability of misclassification to the Bayes optimal classifier arg maxy\u2208Y P d M(Y = y|\u03b8d, X). Equality only holds when \u2200x \u2208X, P d M(\u03b8d|Xd 1, Y d 1 , ..., Xd k, Y d k , X = x) = 1. A similar argument can be made for the Y \ufffdX \ufffd\u03b8 direction. 2 Here, Equation (1) would become: P d M(X|Y d 1 , Xd 1, ..., Y d k , Xd k, Y ) = \ufffd \u0398 P d M(X|\u03b8, Y )P d M(\u03b8|Y d 1 , Xd 1, ..., Y d k , Xd k, Y )d\u03b8 (2) Note that the left-hand side of Equation (1) and Equation (2) are similar to the direct and channel method introduced by Min et al. [24]. However, our analysis differs from theirs in that we do not treat (Y \ufffdX \ufffd\u03b8) as the universally superior channel direction for modeling in-context learning, rather arguing that depending on the end task, the causal direction (X \ufffdY \ufffd\u03b8) is sometimes better. This view is supported by our empirical results in Appendix B.\nA similar argument can be made for the Y \ufffdX \ufffd\u03b8 direction. 2 Here, Equation (1) would become P d M(X|Y d 1 , Xd 1, ..., Y d k , Xd k, Y ) = \ufffd \u0398 P d M(X|\u03b8, Y )P d M(\u03b8|Y d 1 , Xd 1, ..., Y d k , Xd k, Y )d\u03b8 (2)\n\ufffd Note that the left-hand side of Equation (1) and Equation (2) are similar to the direct and channel method introduced by Min et al. [24]. However, our analysis differs from theirs in that we do not treat (Y \ufffdX \ufffd\u03b8) as the universally superior channel direction for modeling in-context learning, rather arguing that depending on the end task, the causal direction (X \ufffdY \ufffd\u03b8) is sometimes better. This view is supported by our empirical results in Appendix B.\n# 3 Method\nHere we demonstrate how the proposed theory can be practically applied to select optimal demonstration examples. Since latent variable \u03b8 encodes both the task and format information, the whole distribution over \u0398 is too complex to model. Unlike traditional topic models, we will only focus on estimating an optimal value \u03b8d corresponding to task d. First, we perform latent concept learning, wherein the task latent \u03b8d is learned as a set of new token embeddings using prompt tuning over the full demonstration candidate set. With this optimal task latent, we then perform demonstration selection, where a smaller set of demonstrations is chosen to maximize the likelihood of postfixing the latent concept tokens. We only need to use a small LLM to do the above steps to obtain an optimal set of demonstrations that can be directly transferred to other LLMs. Figure 1 is an overall illustration of our proposed method.\nHere we demonstrate how the proposed theory can be practically applied to select optimal demonstration examples. Since latent variable \u03b8 encodes both the task and format information, the whole distribution over \u0398 is too complex to model. Unlike traditional topic models, we will only focus on estimating an optimal value \u03b8d corresponding to task d.\nFirst, we perform latent concept learning, wherein the task latent \u03b8d is learned as a set of new token embeddings using prompt tuning over the full demonstration candidate set. With this optimal task latent, we then perform demonstration selection, where a smaller set of demonstrations is chosen to maximize the likelihood of postfixing the latent concept tokens. We only need to use a small LLM to do the above steps to obtain an optimal set of demonstrations that can be directly transferred to other LLMs. Figure 1 is an overall illustration of our proposed method.\n2The detailed argument of the Y \ufffdX \ufffd\u03b8 direction can be found in Appendix \n(2)\nAlgorithm 1 Latent concept learning\nInput: Dataset D = {(xi, yi, di)}i associated with a set of tasks S, LLM M, number of concept\ntokens per task c, learning rate \u03b1, and number of training steps N.\nOutput: LLM M \u2032 with fine-tuned concept tokens.\nAdd c|S| new tokens to the vocabulary. i.e. The concept tokens \u02c6\u03b8d for each task in S. Randomly\ninitialize their embeddings Enew. Freeze all parameters in M except Enew;\nfor step = 1 to N do\nSample a random batch B in D and initialize gradient g \u21900;\nfor each data point (x, y, d) in B do\ng = g + \u2202\u2113(X,Y ;\u02c6\u03b8d)\n\u2202Enew\n;\nend for\nEnew = Enew \u2212\u03b1g;\nend for\n# 3.1 Latent Concept Learning\nWe want to first find the optimal value of the latent concept variable \u03b8d corresponding to a task d \u2208T . As arg maxy\u2208Y P d M(Y = y|\u03b8d, X) is the Bayes optimal classifier according to Proposition 2.2, \u03b8d should be able to minimize \u2212EX,Y,d[log P d M(Y |\u03b8d, X)] for the X \ufffdY \ufffd\u03b8 direction. In practice, we try to align \u03b8d to the token embedding space by adding new tokens to the vocabulary. After this alignment, we hope to be able to use the learned new tokens of \u03b8d as regular tokens. More specifically, building upon the methodology proposed by Lester et al. [17], for each specific task d, c new concept tokens (denoted as \u02c6\u03b8d) are added to the original vocabulary of LLM M to represent the corresponding task concept \u03b8d. Subsequently, the embedding of these new tokens Enew(\u02c6\u03b8d) is fine-tuned while freezing the remaining parameters of LLM M. The variable c is treated as a hyperparameter. In practice, in order to condition on \u03b8d, the corresponding c concept tokens are appended to the input X (or Y ) as shown in the example provided below, where c = 2: <sentiment_token_1><sentiment_token_2> Can\u2019t wait to see the second movie! By giving the above input tokens, we ask the LLM to predict the correct label Positive for us. Note that <sentiment_token_1> here is just a label assigned to the newly added concept token. It can be anything as long as it does not overlap with the original vocabulary of LLM. The fine-tuning objective would then be minimizing L(\u02c6\u03b8d) = EX,Y [\u2113(X, Y ; \u02c6\u03b8d)], where \u2113(X, Y ; \u02c6\u03b8d) = \ufffd \u2212log P d M(Y |\u02c6\u03b8d, X) if X \ufffdY \ufffd\u03b8 \u2212log P d (X|\u02c6\u03b8d, Y ) if Y \ufffdX \ufffd\u03b8.\n\u2113(X, Y ; \u02c6\u03b8d) = \ufffd \u2212log P d M(Y |\u02c6\u03b8d, X) if X \ufffdY \ufffd\u03b8 \u2212log P d M(X|\u02c6\u03b8d, Y ) if Y \ufffdX \ufffd\u03b8.\n \ufffd \ufffd Theoretically, if we can minimize the above loss function, a Bayes optimal classifier can be obtained, and the concept tokens would be a reasonable delegate of the real latent concept variable: Proposition 3.1. When L(\u02c6\u03b8d) is minimized, P d M(Y |\u02c6\u03b8d, X) = P(Y |\u03b8d, X) for X \ufffdY \ufffd\u03b8. If the LLM M is invertible, then \u02c6\u03b8d = \u03b8d.3 We denote the LLM M with fine-tuned concept tokens by M \u2032. Since we add the concept tokens into the regular token vocabulary, the raw LLM output probability PM \u2032(\u02c6\u03b8d|w1:t) (w1:t denote a given prompt) would be in the token sequence space X instead of the concept space \u0398. Since learning all possible \u03b8d \u2208\u0398 is infeasible, we propose to approximate the concept space \u0398 by sampling a diverse subset of tasks S \u2286T . Then the estimated conditional probability of \u03b8d would be:\n\ufffd To obtain the concept tokens for all tasks in S, we fine-tune all tasks together with the loss \ufffd d\u2208S L(\u03b8d). We summarize the proposed algorithm in Algorithm 1.\n3More discussion can be found in Appendix A.3.\nAlgorithm 2 Demonstration selection\nInput: dataset Dd for a task d. LLM with fine-tuned concept tokens M \u2032. The number of\ndemonstrations k.\nOutput: A set of selected demonstrations.\nfor each (Xd, Y d) in Dd do\nCompute \u02c6P d\nM(\u02c6\u03b8d|Xd, Y d);\nend for\nSelect top k examples with the largest \u02c6P d\nM(\u02c6\u03b8d|Xd, Y d), denoted as (Xd\n1, Y d\n1 ), ..., (Xd\nk, Y d\nk );\nNote that the embedding matrix of a generative LLM is shared on both the input and output sides. So while we only see the concept tokens on the input side at the training time, they can be viewed as regular word tokens that can be generated on the output side.\n# 3.2 Demonstration Selection\nAccording to Theorem 2.3, for a task d, to make the in-context learning classifier closer to the Bayes optimal classifier, we need to select demonstrations (Xd 1, Y d 1 ), ..., (Xd k, Y d k ) that maximize P d M(\u03b8d|Xd 1, Y d 1 , ..., Xd k, Y d k , X) for all X \u2208X. Then our goal then becomes selecting demonstrations that can best infer the task concept for all test inputs on average:\nAs test examples are sampled independent of the demonstrations, and PM(X) = P(X) according to Assumption 2.1, we have\nAs test examples are sampled independent of the demonstrations, and PM(X) = P(X) according to Assumption 2.1, we have\nP d M(\u03b8d|Xd 1, Y d 1 , ..., Xd k, Y d k ) = \ufffdk i=1 P d M(\u03b8d|Xd i , Y d i ) P d M(\u03b8d)k\u22121\nAssuming that \u03b8 has a uniform prior, then our goal becomes finding the top k demonstrations that maximize \u02c6P d M \u2032(\u02c6\u03b8d|Xd i , Y d i ). Note that the independence between demonstrations is a simplified assumption to reduce the combinatory search space of (Xd 1, Y d 1 ), ..., (Xd k, Y d k ). In practice, selected demonstrations are likely correlated as some demonstrations may work well together but not necessarily work well by themselves. However, it would be too expensive to search the O(|Dd|k) combinations over the candidate set Dd. In practice, this simplification works reasonably well. We leave this combinatory search problem to future research. Also, as we are using an LLM to approximate the data distribution, the order of the demonstrations might matter. We will show in the Experiment section that the order does not matter, so no reordering of the selected demonstrations is needed. The full selection algorithm is shown in Algorithm 2.\nAlso, as we are using an LLM to approximate the data distribution, the order of the demonstrations might matter. We will show in the Experiment section that the order does not matter, so no reordering of the selected demonstrations is needed. The full selection algorithm is shown in Algorithm 2.\n# 4 Experiments\nDatasets. We conduct experiments on eight datasets from five different types of NLP classification tasks: sentiment analysis, linguistic analysis, topic classification, emotion classification, and hate speech detection. For sentiment analysis, we choose the Stanford Sentiment Treebank (SST2) dataset [35] from the GLUE benchmark [43] and the financial phrase bank (FPB) dataset [21]. SST2 is constructed based on movie reviews labeled \u201cpositive\" or \u201cnegative\", and FPB is based on financial news labeled \u201cpositive\", \u201cnegative\", or \u201cneutral\". For linguistic analysis, we choose the Corpus of Linguistic Acceptability (COLA) dataset [46] from the GLUE benchmark, based on sentences collected from linguistic books, labeled with \u201cacceptable\" or \u201cunacceptable\". For topic classification, we choose the DBpedia ontology classification dataset [52], based on DBpedia 2014 [16], labeled with 14 different ontology classes. For emotion classification, we choose the dataset from Chatterjee et al. [6] and Saravia et al. [33], both of which are collected from Twitter. Chatterjee et al. [6] (EmoC)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/576b/576b6441-6032-441c-b610-0e47f006c43a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Accuracy of 4-shot in-context learning using demonstrations selected by our method and other baselines, averaged over eight datasets. Our demonstrations are selected using GPT2-large, and the same set of demonstrations is then applied to all other LLMs.</div>\npredict emotion given a three-turn contextual dialogue, while Saravia et al. [33] predict emotion given a Twitter message with clear emotion. For hate speech detection, we choose the online hate speech detection dataset (ETHOS) [27], collected from online social media platforms. Here we detect two types of hate speech: sexual orientation (ETHOS-SO) and religion (ETHOS-R). While in Section 2, we assume that all tasks share the same label space Y, here we relax such assumption and allow a different number of labels for different tasks. We use minimal formatting to process each example. A detailed description of the datasets and our data processing procedure can be found in Appendix B. Experiment settings. To determine the causal direction for each task, we select the direction that can give higher accuracy when using random demonstrations4. We adopt the Y \u2192X \u2190\u03b8 direction for sentiment analysis, topic classification, and emotion classification tasks, which is consistent with the intuition that people usually have some sentiment, topic, or emotion in mind before writing a piece of text. We adopt the X \u2192Y \u2190\u03b8 direction for the linguistic analysis and hate speech detection type of tasks. While this is less intuitive, we can understand this as linguistic error and hate speech detection are more of a post hoc task in contrast to the previous tasks. Without specification, we use k = 4 number of demonstrations and c = 10 number of concept tokens per dataset for our experiments, as the context length of GPT2 is 1024, and a larger number of demonstrations may not be able to completely fit into it. We use GPT2-large to learn the concept tokens and then compute the probability of each candidate demonstration example. We select our demonstrations from a randomly selected 100 example subset of the train set as the candidate set Dd. We use the same set of demonstrations selected by GPT2-large for all other LLMs. We test the performance of the selected demonstrations using at most 1000 examples randomly sampled from the test set. Each experiment is repeated for five runs with different random seeds (the randomness comes from the sampling of the candidate set and the sampling of the test set). We adopt a large portion of the code from Min et al. [25], which is based on Huggingface [49]. Baselines. We consider the following baselines:\n\u2022 Uniform: We uniformly select k demonstrations from D for each test example. \u2022 Similar: According to Liu et al. [19], demonstrations that are semantically similar to the test example would hare more performant. Following their method, we use a pre-trained sentence Transformer [31] to calculate the cosine similarity between the demonstrations and test examples. We choose the top k similar demonstrations from D for each test example.\nMain results.5 Figure 2 shows our main results averaged over all eight datasets, using the firstgeneration GPT2s and GPT3s, without any instruction fine-tuning [28] or Reinforcement Learning from Human Feedback (RLHF) [36]. Our method significantly outperforms baselines on eight different LLMs, with 12.5% relative improvement to the uniform selection baseline on average, which shows the effectiveness of our method. The demonstrations selected by our method are exclusively based on GPT2-large, while the same set of demonstrations can be generalized to all other GPTs. Results with non-GPT models. In Figure 3a, we test the demonstrations selected by our method using GPT2-large on more LLMs (GPT3 [4], GPT3-instruct [28, 36], GPT-J [44], OPT [51], and LLaMA [38]) with similar sizes (6-7B), and show that the selected demonstrations improve in-context learning performance of all of them. The fact that GPT3-curie obtains the largest performance improvement is likely because similar pre-training data distributions help the generalization of the\n4Detailed results see Figure 6 in Appendix B. 5The complete results with standard deviations in this section can be found in A\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5447/54472321-cba3-4018-a072-94ae652033e1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Proposed method v.s. randomly selected demonstrations randomly selected tokens Figure 3: In-context learning accuracy averaged over all eight datasets.</div>\n<div style=\"text-align: center;\">Figure 3: In-context learning accuracy averaged over all eight datasets.</div>\nUniform\nSimilar\nOurs w/ Llama 2 (7B)\nOurs w/ GPT2-XL (1.5B)\nPrompt tuning\n-\n-\n15.2\n7.3\nLlama 2 (7B)\n11.4\n13.1\n19.3\n15.9\nLlama 2 (13B)\n17.0\n18.3\n21.6\n20.5\nLlama 2 (70B)\n50.2\n53.5\n54.3\n52.9\nChatGPT (gpt-3.5-turbo)\n76.5\n78.1\n81.2\n80.4\nTable 1: Prompt tuning and 4-shot in-context learning accuracy on a subset of GSM8K test set. Our\nselected demonstrations. Different-size GPT2 models share the same pre-training corpus [30], while GPT3s are pre-trained on a dataset expanded from the GPT2 pre-training corpus [4]. Thus the pre-training distribution of GPT3-curie and GPT2-large can be assumed to be similar. Results on GSM8K. Since our primary goal is to connect the theory with real-world models and datasets, we did not try to include harder tasks in the main results in Figure 2. In practice, our proposed method is most effective with hard tasks that even parameter-efficient fine-tuning with smaller models cannot outperform in-context learning with the same or larger models. To showcase the usefulness of our proposed algorithm, We added a new dataset, GSM8K [9], which is a math word problem-solving dataset with chain-of-thoughts solutions. Table 1 shows the test accuracy of the final numerical answer with greedy generation. We randomly select a test set of 200 examples instead of using the full test set for computation efficiency. 6 As shown in the first row of Table 1, prompt tuning with ten new tokens can only obtain less than 4% accuracy on the GSM8K test set. The last four rows show the in-context learning results with different size Llama 2 models [39] and ChatGPT. Our proposed demonstration selection method (last two columns) significantly outperformed the Uniform and Similar baseline. We also find that the demonstrations selected with a larger model (7B) are more effective than those selected with a smaller model (1.5B). The results show that our demonstration selection method is a good choice under a low data setting, with a small computing budget and minimal inference latency. Our proposed method can also potentially be combined with other prompting techniques [8] to boost performance further. Learned tokens v.s. Random tokens. To confirm the critical role of the latent concept variable in the proposed demonstration selection algorithm, we compare the performance of using the learned concept tokens versus using randomly selected tokens from the original vocabulary in Figure 3b. The demonstrations selected by random tokens only obtain the same performance as randomly selected demonstrations, showing that the performance gain of our method comes from the learned concept tokens containing the task and format information, not other elements of our algorithm. k ablation study. While we use k = 4 demonstrations for all experiments, we also test the effectiveness of our method using different k. As shown in Figure 4a, our method significantly outperforms the random selection baseline with k = 2, 4, 8, and 16. To fit in large ks, we use GPT3-ada with a longer context length (2048). Note that for real-world tasks, it is in general not true that more demonstrations guarantee higher performance [7]. We can see that the uniform baseline performance increases from k = 2 to k = 8, then drops a little at k = 16. Our method improves the uniform baseline by around 5% absolute for all ks, while k = 4 improves the most (6.6%). Our method appears to have a diminishing effect when k becomes larger, which is likely because the effect of more demonstrations overwhelms the effect of demonstration choices.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6dbe/6dbee534-494d-4b28-8fc8-205260a67229.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Proposed method v.s. using randomly selected tokens</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f1c/3f1ca00e-0421-4a2a-ac01-e9f78c63e66a.png\" style=\"width: 50%;\"></div>\nc ablation study. While we use c = 10 number of concept tokens for all experiments, we also investigate the effect of different c on our method. When c is small (c = 5), the concept tokens cannot effectively capture the task and format information, thus cannot im-\n<div style=\"text-align: center;\">(a) k ablation study.</div>\n<div style=\"text-align: center;\">Figure 4: In-context learning accuracy of our method versus random selection baseline averaged over all eight datasets with GPT3-ada.</div>\nprove the performance. When c increases from 10 to 20, we observe a drop in the performance. It is likely because the selectivity of the concept tokens decreases when c increases. The longer the concept token sequence is, the more likely it will contain meaningless tokens that do not contribute to demonstration selection.\nEffect of demonstrations\u2019 order. We find that the demonstrations selected by our method are insensitive to their order in most cases.7 An exception is the EmoC dataset, where our method has a high variance. On the contrary, Lu et al. [20] found that the order of the demonstration matters, and a good ordering cannot be transferred between different LLMs. We suspect that the ordering only matters when the demonstration selection method is not robust. Since Lu et al. [20] randomly selects one set of demonstrations for the whole test set, the variance in performance is high with different demonstrations, thus ordering matters. And since such ordering is not transferable while our selected demonstrations are highly transferable, we suspect the core task information is stored in the content of the demonstrations, while the ordering mainly captures model-specific artifacts.\nQualitative analysis. In Figure 5, we provide a t-SNE [40] projection of the learned concept token embeddings. The tokens corresponding to semantically similar tasks are close together. Note that this result only aims to provide a straightforward illustration of concept tokens. The effect of concept tokens should be understood by the previous quantitative results.8 We also list the top 4 selected demonstrations in Table 14 in Appendix B. Compared to the examples with lower scores, the selected examples for GSM8K have more deductive reasoning (i.e. with the connecting words \u2018so\u2019, \u2018then\u2019, \u2018thus\u2019, etc.), instead of listing parallel conditions. For SST2, the selected examples are longer and more complex, sometimes including a \u2018but\u2019. This can be understood as these\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/027b/027bb897-3f73-4c9a-ac9f-860a81a4cd64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: t-SNE plot of the learned concept tokens for each task. Concept tokens that can be explained by similar tokens are summarized in the graph.</div>\nharder examples can represent the task more comprehensively. This conclusion also aligns with the findings in [13] that hard examples in the pre-training data contribute to in-context learning the most. The label distribution of the selected demonstrations is usually balanced in class, which reduces the possible biases introduced by the demonstrations.\n# 5 Related Work\nHeuristic solutions, such as selecting demonstrations based on the similarity between the demonstrations and test input [19, 37, 32] have been proposed. [20] propose to reorder the demonstration based on the entropy of the predicted labels. In this paper, we use the similarity-based selection method\n7Detailed results see Figure 9 in Appendix B. 8The list of similar tokens for these concept tokens can be found in Table 13 in A\nas a baseline while do not include the label entropy-based reordering method as we show that the ordering of the demonstrations does not matter for our method.\nas a baseline while do not include the label entropy-based reordering method as we show that the ordering of the demonstrations does not matter for our method. Previous research on the phenomenon of in-context learning in Transformers has identified a number of pre-training data distributions that can lead to the emergence of this capability, including a Hidden Markov Model distribution [50] and a skewed Zipfian distribution with high burstiness [5]. Other studies have sought to understand the underlying mechanisms of in-context learning by making connections with gradient descent [42, 10, 1], formalizing it as an algorithm learning problem [18], or proposing a latent variable theory similar as ours [14, 12, 50]. While providing valuable insights on how in-context learning works, these works are limited to synthetic datasets and toy Transformers, while it remains unclear if these results generalize to LLMs pre-trained on real-world text data and whether these results can help in-context learning performance. In contrast, we propose a Bayesian explanation of in-context learning that can be verified with real-world LLMs on various NLP datasets. Dai et al. [10] provide a practical algorithm based on the understanding that the Transformer has a dual form of gradient descent. However, their empirical results are smaller in scale, with six datasets and only one model (350M), and has less significant improvements (5.4% relative to baseline). There are also works trying to understand in-context learning from an empirical perspective [2, 24]. Min et al. [26] found demonstrations\u2019 ground truth labels do not matter for in-context learning, which we find is not entirely accurate in Appendix B. On the other hand, chain-of-thoughts prompting [48, 53, 45] find that providing step-by-step explanations improves in-context learning performance.\n# 6 Conclusion\nIn this work, we endeavor to comprehend large language models (LLMs) through a Bayesian lens and posit them as implicit topic models that infer a latent conceptual variable from prompts. Motivated by this understanding, we propose a two-step algorithm that first extracts latent conceptual tokens from a small LLM and then selects demonstrations that have the greatest probability of predicting the corresponding conceptual tokens. The selected demonstrations can then be directly generalized to other LLMs. The efficacy of our algorithm across various text classification datasets and GPT models validates our explanation of in-context learning.\n# Acknowledgements\nThis work was supported by the National Science Foundation award #2048122. The views expressed are those of the author and do not reflect the official policy or position of the US government. We thank Google for its generous gift to the University of California.\n# References\n[1] E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. [2] H. Bansal, K. Gopalakrishnan, S. Dingliwal, S. Bodapati, K. Kirchhoff, and D. Roth. Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale. arXiv preprint arXiv:2212.09095, 2022. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3 (null):993\u20131022, mar 2003. ISSN 1532-4435. [4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [5] S. C. Chan, A. Santoro, A. K. Lampinen, J. X. Wang, A. Singh, P. H. Richemond, J. McClelland, and F. Hill. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022. [6] A. Chatterjee, K. N. Narahari, M. Joshi, and P. Agrawal. Semeval-2019 task 3: Emocontext contextual emotion detection in text. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 39\u201348, Minneapolis, Minnesota, USA, 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/S19-2005. URL https://www.aclweb.org/ anthology/S19-2005. [7] J. Chen, L. Chen, and T. Zhou. It takes one to tango but more make trouble? in-context training with different number of demonstrations. arXiv preprint arXiv:2303.08119, 2023. [8] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. [9] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. [11] L. Devroye, L. Gy\u00f6rfi, and G. Lugosi. A probabilistic theory of pattern recognition. In Stochastic Modelling and Applied Probability, 1996. [12] M. Hahn and N. Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv preprint arXiv:2303.07971, 2023. [13] X. Han, D. Simig, T. Mihaylov, Y. Tsvetkov, A. Celikyilmaz, and T. Wang. Understanding in-context learning via supportive pretraining data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12660\u201312673, 2023. [14] H. Jiang. A latent space theory for emergent abilities in large language models. arXiv preprint arXiv:2304.09960, 2023. [15] B. LeBrun, A. Sordoni, and T. J. O\u2019Donnell. Evaluating distributional distortion in neural language modeling. In International Conference on Learning Representations, 2022. [16] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. Van Kleef, S. Auer, et al. Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167\u2013195, 2015. [17] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, 2021. [18] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067, 2023. [19] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10. [20] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, 2022. [21] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65, 2014. [22] Y. Miao, L. Yu, and P. Blunsom. Neural variational inference for text processing. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1727\u20131736, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL https://proceedings.mlr. press/v48/miao16.html. [23] Y. Miao, E. Grefenstette, and P. Blunsom. Discovering discrete latent topics with neural variational inference. In International conference on machine learning, pages 2410\u20132419. PMLR, 2017.\n[24] S. Min, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, 2022. [25] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-main.201. URL https://aclanthology.org/2022.naacl-main.201. [26] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022. [27] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas. Ethos: an online hate speech detection dataset, 2020. [28] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. [29] E. Perez, D. Kiela, and K. Cho. True few-shot learning with language models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=ShnM-rRh4T. [30] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019. [31] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084. [32] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021. [33] E. Saravia, H.-C. T. Liu, Y.-H. Huang, J. Wu, and Y.-S. Chen. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687\u20133697, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1404. URL https://www.aclweb.org/anthology/D18-1404. [34] N. Saunshi, S. Malladi, and S. Arora. A mathematical exploration of why language models help solve downstream tasks. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=vVjIW3sEc1s. [35] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u2013 1642, Seattle, Washington, USA, Oct. 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170. [36] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. [37] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022. [38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov,\nP. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. 7 2023. URL http://arxiv.org/abs/2307.09288. [40] L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008. URL http://jmlr.org/papers/v9/vandermaaten08a. html. [41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [42] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. [43] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. EMNLP 2018, page 353, 2018. [44] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021. [45] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [46] A. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018. [47] C. Wei, S. M. Xie, and T. Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. Neural Information Processing Systems (NeurIPS), 2021. [48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. [49] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [50] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI. [51] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [52] X. Zhang, J. Zhao, and Y. LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. [53] D. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n# A Proofs\n# A.1 Direct direction\nAssumption A.1. (Assumption 2.1) Assume that PM(X) = P(X), and P d M(Y |\u03b8, X) \u221dP(Y |\u03b8, X) for X \ufffdY \ufffd\u03b8. Proposition A.2. (Proposition 2.2) If task d follows the X \ufffdY \ufffd\u03b8 direction, arg maxy\u2208Y P d M(Y = y|\u03b8d, X) is the Bayes optimal classifier.\narg max y\u2208Y P d M(Y = y|\u03b8d, X) = arg max y\u2208Y P(Y = y|\u03b8d, X).\nThus arg maxy\u2208Y P d M(Y = y|\u03b8d, X) is the Bayes optimal classifier. Theorem A.3. (Theorem 2.3) If task d follows the X \ufffdY \ufffd\u03b8 direction, then the in-context learning classifier arg max y\u2208Y P d M(Y = y|Xd 1, Y d 1 , ..., Xd k, Y d k , X)\nalways has a higher or equal probability of misclassification to the Bayes optimal classifier arg maxy\u2208Y P d M(Y = y|\u03b8d, X). Equality only takes when \u2200x \u2208X, P d M(\u03b8d|Xd 1, Y d 1 , ..., Xd k, Y d k , X = x) = 1.\nalways has a higher or equal probability of misclassification to the Bayes optimal classifier arg maxy\u2208Y P d M(Y = y|\u03b8d, X). Equality only takes when\nProof. Recall that in Equation (1), we have\nBy Proposition A.2, arg maxy\u2208Y P d M(Y = y|\u03b8d, X) is the Bayes optimal classifier. Let C\u03b8(X) = arg maxy\u2208Y P d M(Y = y|\u03b8, X), then the risk is defined as the probability of misclassification\nSuch risk is minimized if and only if Ck(X) = C\u03b8d(X), which only holds when P d M(\u03b8d|Xd 1, Y d 1 , ..., Xd k, Y d k , X = x) = 1 for all x \u2208X.\n# A.2 Channel direction\n\nBy Proposition A.5, arg maxy\u2208Y P d M(X|\u03b8d, Y = y) is the Bayes optimal classifier. Let C\u03b8(X) = arg maxy\u2208Y P d M(X|\u03b8, Y = y), then the risk is defined as the probability of misclassification\nDenote the in-context learning classifier arg maxy\u2208Y P d M(X|Y d 1 , Xd 1, ..., Y d k , Xd k, Y = y) by Ck(X We then have \ufffd\nSuch risk is minimized if and only if Ck(X) = C\u03b8d(X), which only holds when P d M(\u03b8d|Y d 1 , Xd 1, ..., Y d k , Xd k, Y = y) = 1 for all y \u2208Y.\n# A.3 Method\nProposition A.7. (Proposition 3.1) When L(\u02c6\u03b8d) is minimized, P d M(Y |\u02c6\u03b8d, X) = P(Y |\u03b8d, X) for X \ufffdY \ufffd\u03b8, and P d M(X|\u02c6\u03b8d, Y ) = P(X|\u03b8d, Y ) for Y \ufffdX \ufffd\u03b8. If the LLM M is invertible, then \u02c6\u03b8d = \u03b8d.\nProof. The proof of this proposition is straightforward. Since\nProof. The proof of this proposition is straightforward.\nL(\u02c6\u03b8d) = H(P(Y |\u03b8d, X)) + KL(P(Y |\u03b8d, X)||P d M(Y |\u02c6\u03b8d, X))\nwhen L(\u02c6\u03b8d) is minimized, we have P d M(Y |\u02c6\u03b8d, X) = P(Y |\u03b8d, X) for X \ufffdY \ufffd\u03b8, and P d M(X|\u02c6\u03b8d, Y ) = P(X|\u03b8d, Y ) for Y \ufffdX \ufffd\u03b8. If M is invertible, since the embedding matrix is invertible with or without new concept tokens, P d M(Y |\u02c6\u03b8, X) = P d M(Y |\u02c6\u03b8\u2032, X) implies that \u02c6\u03b8 = \u02c6\u03b8\u2032. Thus \u03b8 is identifiable, which means \u02c6\u03b8d = \u03b8d.\n<div style=\"text-align: center;\">Table 2: Prompt template and label mapping for the datasets we use. Since almost all sentences from ETHOS contain offensive content, we mask out the key offensive words in the examples below.</div>\nS contain offensive content, we mask out the key offensive words in the examples belo\nDataset\nPrompt\nLabel Mapping\nSST-2\nsentence: well worth revisiting as many times\npositive\nnegative/positive\nFPB\nThe company anticipates its turnover for the whole 2010 to\nsurpass that of the previous year when it was EUR 67.1 million .\npositive\nnegative/neutral/positive\nCOLA\nIt is this hat that I know the boy who is wearing.\nunacceptable\nacceptable/unacceptable\nDBPedia\nThe Nucet River is a tributary of the Chiojdeanca\nRiver in Romania.\nNaturalPlace\nAlbum/Animal/Artist/\nAthlete/Building/Company/\nEducationalInstitution/Film/\nMeanOfTransportation/\nNaturalPlace/OfficeHolder/\nPlant/Village/WrittenWork\nEmoC\nfast i mean fastingis a way of skipping meals i mena\nyou move on too fast\nothers\nangry/happy/others/sad\nEmoS\ni feel this place was tragic\nsadness\nanger/fear/joy/love/\nsadness/surprise\nETHOS-SO\n[Masked] should be removed from the face of the earth\ntrue\nfalse/true\nETHOS-R\nI hate being a [Masked], wish I was a [Masked]\nand no [Masked] on earth existed\nfalse\nfalse/true\n# B Experiments\nDateset. In Table 2, we show how we process the text classification datasets into prompts. For each dataset, we take at most 16384 examples from the training set for training, and uniformly sample at most 1000 examples from the test set to test the in-context learning performance. In Table 3, we show the train size and test size we used for each dataset. We also list the set of diverse tasks trained with each dataset, which are denoted by their name in Huggingface datasets.9 The license for SST2, ETHOS-SO and ETHOS-R is GNU General Public License v3. FPB is under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Note that these two datasets are hate speech detection datasets for different kinds of hate speech and contain many offensive texts. COLA is excerpted from the published works available on the website, and the copyright (where applicable) remains with the original authors or publishers. DBpedia is under a Creative Commons Attribution-ShareAlike License and the GNU Free Documentation License. EmoC and EmoS should be used for educational and research purposes only. Experiment details. We run our experiments on A100, V100, and A6000 GPUs. We adopt a large portion of the code from the MetaICL repository [25]10. The training takes around 20 to 40 hours on a single GPU. We use a learning rate of 1e-4 and a batch size of 16, and train for 10k steps in total. Main results. In Table 4, we list the detailed results of our method and baselines with different LLMs on different datasets in Figure 2. Causal direction results. The detailed results with anti-causal direction (the opposite direction to what we described in Section 4 are in Table 7) are shown in Table 7, corresponding to Figure 6 in the main text. Other LLMs results. The detailed results with other LLMs are shown in Table 6, corresponding to Figure 3a in the main text. Random token results. The detailed results with random tokens are shown in Table 5, corresponding to Figure 3b in the main text.\n9https://huggingface.co/docs/datasets/index 10https://github.com/facebookresearch/MetaICL\ndatset d\ntrain size\ntest size\ntask set S\nSST2 (glue-sst2)\n16384\n1000\nglue-cola/glue-mnli/glue-qqp/\nglue-mrpc/glue-qnli/glue-rte/glue-sst2/glue-wnli\nFPB (financial_phrasebank)\n1811\n453\nglue-sst2/glue-mnli/math_qa/sciq/\nsocial_i_qa/wino_grande/glue-qqp/\nag_news/financial_phrasebank/\npoem_sentiment/anli/quarel/quartz/\nmedical_questions_pairs/paws/dbpedia_14\nCOLA (cola-sst2)\n8551\n1000\nglue-cola/glue-mnli/glue-qqp/glue-mrpc/\nglue-qnli/glue-rte/glue-sst2/glue-wnli\nDBpedia (dbpedia_14)\n16384\n1000\nglue-sst2/glue-mnli/math_qa/sciq/\nsocial_i_qa/wino_grande/glue-qqp/\nag_news/financial_phrasebank/\npoem_sentiment/anli/quarel/quartz/\nmedical_questions_pairs/paws/dbpedia_14\nEmoC (emo)\n16384\n1000\nglue-sst2/amazon_polarity/\nfinancial_phrasebank/poem_sentiment/\nyelp_polarity/glue-cola/blimp/ag_news/\ndbpedia_14/ethos/emo/emotion\nEmoS (emotion)\n16000\n1000\nglue-sst2/amazon_polarity/\nfinancial_phrasebank/poem_sentiment/\nyelp_polarity/glue-cola/blimp/ag_news/\ndbpedia_14/ethos/emo/emotion\nETHOS-SO (ethos-sexual_orientation)\n346\n87\nglue-sst2/amazon_polarity/\nfinancial_phrasebank/poem_sentiment/\nyelp_polarity/glue-cola/blimp/ag_news/\ndbpedia_14/ethos/emo/emotion\nETHOS-R (ethos-religion)\n346\n87\nglue-sst2/amazon_polarity/\nfinancial_phrasebank/poem_sentiment/\nyelp_polarity/glue-cola/blimp/ag_news/\ndbpedia_14/ethos/emo/emotion\nTable 3: Dataset details\nTable 3: Dataset details\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f865/f8653116-7b6c-45a9-80d0-2a9ea0155ee5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Accuracy of randomly selected demonstrations averaged over seven different LLMs except or GPT3-davinci, using the adopted causal direction and the anti-causal direction.</div>\nk-ablation study results. The detailed results of k ablation study are shown in Table 10, corresponding to Figure 4a in the main text. In this experiment, we do not reorder the selected demonstrations according to Equation (3), as we need to use GPT2-large for the reordering, and it cannot fit in all the demonstrations. Instead, we order the selected demonstrations from the largest \u02c6P d M(\u03b8d|Xd, Y d) to the smallest. c-ablation study results. The detailed results of c ablation study are shown in Table 11, corresponding to Figure 4b in the main text.\nEffect of using ground truth labels. According to [26], the ground truth label is not necessary for demonstrations to have a good in-context learning performance, which we found is not entirely true for all the tasks. We compare our method with the randomly selected demonstration baseline under three scenarios: (a) Original: demonstrations with the correct labels; (b) Random words: using a random label projection map \u03c4 d instead of a meaningful one. i.e., map each label to a fixed random word. In this case, the mapping from the input tokens X to the labels Y is still preserved; (c) Random labels: assign a random label to each demonstration, with the original label projection map \u03c4 d. As shown in Figure 7, by using a random label projection map or randomly assigning the labels, the performance of the randomly selected demonstration baseline drops considerably. And randomize the label assignment gives a larger performance drop than only using a random label projection map,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c68/1c68777f-9547-40f1-8d2a-886470efdaa6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: In-context learning accuracy of our method versus random selection baseline, with (a) ground truth labels (original), (b) random label mapping (random words), or random label assignments (random label), averaged over all eight datasets. Numbers are obtained with GPT2-large.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/346f/346fb672-d877-406e-be03-2ff585b34ba0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Accuracy of in-context learning using our method versus the theoretical maximum accuracy obtained using the learned concept tokens as prefixes. Numbers are obtained with GPT2-large.</div>\nwhich shows that the mapping between X and Y in the demonstrations matters. This indicates that in-context learning infers the mapping between X and Y from the demonstrations instead of merely invoking some learned function stored in the LLM parameters based on the appearance of X and Y . We also show that the demonstrations selected by our method represent the X \u2212Y mapping better, as under the Random words condition, our method performs better than the random selection baseline, while our method does not improve the random selection baseline under the Random labels condition. The detailed results with random words and random labels are shown in Table 8 Optimal performance As stated in Theorem 2.3, the optimal performance of an in-context learning classifier is the Bayes optimal classifier arg maxy\u2208Y P d M(Y = y|\u03b8d, X), which is approximated by using the learned concept tokens as prefixes. Note that this approximated Bayes optimal classifier cannot be transferred across different LLMs, as the learned concept tokens embeddings are aligned with a specific LLM. The advantage of in-context learning with our method is that the demonstrations can be transferred to any LLMs without training. Here we only compare the accuracy of in-context learning with our method and the approximated Bayes optimal classifier using GPT2-large, as it is the LLM that concept tokens are fine-tuned with. As shown in Figure 8, our method comes close to the optimal accuracy on many datasets, while there are some datasets that our method is lagging. This indicates that there are two ways to improve our method: the first is to improve the performance of the optimal classifier, by introducing a better latent concept learning algorithm. The other way is to reduce the performance gap between our method and the optimal classifier, by improving the demonstration selection algorithm. The detailed results using the learned concept tokens as prefixes are shown in Table 9. Reordering results. We reorder the selected demonstrations to maximize the posterior of the concept tokens:\nWhere \u03c0((Xd 1, Y d 1 ), ..., (Xd k, Y d k )) is a permutation of (Xd 1, Y d 1 ), ..., (Xd k, Y d k ). \u03a0 is the set of all possible permutations of the k demonstrations. The detailed results with and without reordering are shown in Table 12, corresponding to Figure 9. Similar tokens. We show the top ten similar tokens to some learned concept tokens in Table 13, as summarized in Figure 5 in the main text.\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/23f2/23f2aa16-f773-44e1-a758-7179577dfbfa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: In-context learning accuracy of our method versus random selection baseline, with an without reordering. The red error bars represent the standard deviation across five runs. Numbers a obtained with GPT2-large.</div>\nGPT3s.\nLLM\nMethod\nSST2\nFPB\nCOLA\nDBpedia\nEmoC\nEmoS\nETHOS-SO\nETHOS-R\nAvg\nGPT2\nUniform\n69.7 \u00b1 1.8\n52.9 \u00b1 2.3\n61.9 \u00b1 1.4\n48.0 \u00b1 0.7\n35.3 \u00b1 1.7\n26.4 \u00b1 1.0\n64.1 \u00b1 4.8\n71.0 \u00b1 1.8\n53.7\n(124M)\nSimilar\n69.5 \u00b1 0.6\n55.9 \u00b1 1.7\n63.2 \u00b1 1.2\n44.7 \u00b1 3.1\n36.4 \u00b1 2.0\n26.6 \u00b1 1.3\n77.7 \u00b1 2.7\n80.0 \u00b1 3.7\n56.8\nOurs\n76.8 \u00b1 2.9\n64.5 \u00b1 3.2\n69.1 \u00b1 0.2\n53.5 \u00b1 2.95\n37.2 \u00b1 11.1\n30.6 \u00b1 4.8\n80.9 \u00b1 1.9\n76.8 \u00b1 2.6\n61.2\nGPT2-m\nUniform\n70.8 \u00b1 1.3\n52.0 \u00b1 1.7\n57.8 \u00b1 1.3\n49.3 \u00b1 2.0\n34.2 \u00b1 1.8\n34.2 \u00b1 1.8\n76.3 \u00b1 4.9\n74.7 \u00b1 2.2\n56.2\n(355M)\nSimilar\n75.0 \u00b1 1.9\n57.7 \u00b1 2.0\n57.5 \u00b1 2.2\n47.9 \u00b1 6.0\n37.2 \u00b1 3.6\n35.2 \u00b1 1.8\n86.9 \u00b1 2.9\n84.6 \u00b1 4.3\n60.3\nOurs\n81.2 \u00b1 1.3\n59.3 \u00b1 4.3\n69.0 \u00b1 0.2\n52.9 \u00b1 2.3\n40.4 \u00b1 21.5\n37.2 \u00b1 2.4\n83.7 \u00b1 1.1\n76.8 \u00b1 1.1\n62.6\nGPT2-l\nUniform\n77.1 \u00b1 1.2\n51.3 \u00b1 2.4\n62.7 \u00b1 0.8\n54.4 \u00b1 0.9\n38.7 \u00b1 2.1\n34.5 \u00b1 1.2\n67.6 \u00b1 4.3\n72.9 \u00b1 2.8\n57.4\n(774M)\nSimilar\n80.7 \u00b1 1.6\n54.8 \u00b1 3.8\n50.9 \u00b1 1.4\n51.1 \u00b1 5.2\n39.9 \u00b1 2.6\n35.1 \u00b1 2.1\n80.9 \u00b1 2.8\n84.4 \u00b1 2.6\n59.7\nOurs\n86.2 \u00b1 1.4\n60.4 \u00b1 2.5\n69.1 \u00b1 0.2\n56.5 \u00b1 3.2\n48.4 \u00b1 17.0\n38.6 \u00b1 2.8\n82.5 \u00b1 1.5\n76.6 \u00b1 1.2\n64.8\nGPT2-xl\nUniform\n74.7 \u00b1 0.9\n53.2 \u00b1 1.9\n55.8 \u00b1 1.6\n53.0 \u00b1 1.9\n38.2 \u00b1 1.5\n38.2 \u00b1 1.5\n67.8 \u00b1 6.4\n72.6 \u00b1 4.1\n56.7\n(1.5B)\nSimilar\n80.6 \u00b1 1.3\n53.0 \u00b1 2.5\n55.0 \u00b1 2.5\n51.6 \u00b1 5.9\n39.9 \u00b1 2.0\n32.9 \u00b1 2.1\n82.8 \u00b1 2.2\n83.9 \u00b1 4.5\n60\nOurs\n83.1 \u00b1 3.6\n62.0 \u00b1 2.5\n68.9 \u00b1 0.2\n58.6 \u00b1 3.3\n43.6 \u00b1 16.4\n43.6 \u00b1 16.4\n83.0 \u00b1 1.3\n77.9 \u00b1 1.3\n65.1\nGPT3-a\nUniform\n76.9 \u00b1 0.7\n56.6 \u00b1 1.1\n53.1 \u00b1 1.8\n62.1 \u00b1 1.4\n38.6 \u00b1 1.4\n27.7 \u00b1 1.3\n65.5 \u00b1 5.7\n74.0 \u00b1 3.0\n56.8\n(350M)\nSimilar\n78.7 \u00b1 1.0\n52.2 \u00b1 2.7\n53.1 \u00b1 1.8\n54.6 \u00b1 1.7\n42.4 \u00b1 3.5\n37.2 \u00b1 1.1\n84.1 \u00b1 2.2\n87.8 \u00b1 3.5\n61.3\nOurs\n85.4 \u00b1 1.7\n61.9 \u00b1 10.5\n58.2 \u00b1 7.0\n64.0 \u00b1 4.4\n43.0 \u00b1 7.2\n37.9 \u00b1 2.3\n84.4 \u00b1 1.4\n78.9 \u00b1 0.9\n64.2\nGPT3-b\nUniform\n80.8 \u00b1 0.6\n55.2 \u00b1 3.3\n46.8 \u00b1 2.0\n66.5 \u00b1 1.4\n42.0 \u00b1 0.7\n27.0 \u00b1 1.2\n71.0 \u00b1 4.6\n72.6 \u00b1 3.1\n57.7\n(1.3B)\nSimilar\n83.9 \u00b1 1.3\n56.2 \u00b1 2.3\n45.1 \u00b1 1.8\n59.8 \u00b1 1.8\n42.9 \u00b1 3.5\n38.1 \u00b1 1.7\n86.7 \u00b1 3.0\n86.4 \u00b1 3.0\n62.4\nOurs\n87.3 \u00b1 2.0\n64.3 \u00b1 5.9\n67.2 \u00b1 0.9\n70.2 \u00b1 3.2\n43.6 \u00b1 13.0\n38.9 \u00b1 5.0\n84.6 \u00b1 0.9\n78.9 \u00b1 1.2\n66.9\nGPT3-c\nUniform\n84.2 \u00b1 1.4\n52.6 \u00b1 1.8\n59.1 \u00b1 1.5\n70.6 \u00b1 0.8\n44.3 \u00b1 2.5\n32.3 \u00b1 1.9\n77.5 \u00b1 4.7\n77.5 \u00b1 0.6\n62.3\n(6.7B)\nSimilar\n85.7 \u00b1 1.4\n62.2 \u00b1 0.9\n58.0 \u00b1 1.7\n62.2 \u00b1 2.0\n47.4 \u00b1 4.3\n39.8 \u00b1 1.7\n89.2 \u00b1 1.4\n89.7 \u00b1 1.9\n66.8\nOurs\n88.8 \u00b1 0.7\n64.1 \u00b1 5.7\n69.0 \u00b1 0.3\n73.6 \u00b1 2.9\n50.3 \u00b1 11.9\n43.1 \u00b1 4.6\n86.2 \u00b1 0.0\n78.2 \u00b1 0.0\n69.2\nGPT3-d\nUniform\n86.5 \u00b1 0.9\n59.2 \u00b1 2.4\n45.5 \u00b1 2.8\n73.6 \u00b1 1.9\n39.4 \u00b1 0.7\n40.6 \u00b1 1.7\n77.2 \u00b1 2.6\n76.8 \u00b1 3.5\n62.4\n(175B)\nSimilar\n88.5 \u00b1 0.8\n55.4 \u00b1 3.3\n45.4 \u00b1 1.5\n67.2 \u00b1 1.8\n37.6 \u00b1 1.6\n39.8 \u00b1 1.4\n86.9 \u00b1 2.4\n89.0 \u00b1 3.8\n63.7\nOurs\n87.8 \u00b1 3.4\n62.7 \u00b1 3.3\n58.5 \u00b1 8.2\n75.5 \u00b1 2.4\n41.3 \u00b1 3.6\n42.7 \u00b1 3.9\n85.1 \u00b1 0.0\n79.3 \u00b1 0.0\n66.6\nAvg\nUniform\n77.6\n54.1\n55.3\n59.7\n38.8\n32.6\n70.9\n74.0\n57.9\nSimilar\n80.3\n55.9\n53.5\n54.9\n40.5\n35.6\n84.4\n85.7\n61.4\nOurs\n84.6\n62.4\n66.1\n63.1\n43.5\n39.1\n83.8\n77.9\n65.0\nLikelihood histogram. We also show histograms of the probability of each example predicting corresponding concept tokens in different datasets. We can see that the probability of prediction concept tokens can well differentiate examples in a dataset. Selected demonstrations. Table 14 shows the selected top 4 demonstration by our proposed algorithm.\nGSM8K\nQuestion:\nIt takes Bryan 5 minutes to walk from his house\nto the bus station.\nThen he rides the bus for 20 minutes.\nAfter that, he walks 5 minutes from the bus station to his\njob.\nIt takes the same amount of time in the morning and\nthe evening.\nHow many hours per year does Bryan spend\ntraveling to and from work, if he works every day?\nBryan\nspends 5+20+5 =\u00ab5+20+5=30\u00bb30 minutes traveling to work.\nHe\ndoes this twice a day, so he spends 30*2=\u00ab30*2=60\u00bb60 minutes\ntraveling per day.\nThus, he spends 60/60=\u00ab60/60=1\u00bb1 hour\ntraveling to and from work every day.\nThus, he spends 1\nhour*365 days =\u00ab1*365=365\u00bb365 hours traveling per year.\nThe\nanswer is:\n365\nGSM8K\nQuestion:\nCherry put up a delivery service.\nShe charges\n$2.50 for a 3-5 kilograms cargo and $4 for a 6-8 kilograms\ncargo.\nIf she delivers four 5 kilograms cargo and two\n8 kilograms cargo per day, how much money will she earn\nin a week?\nCherry earns $2.50 x 4= $\u00ab2.5*4=10\u00bb10 after\ndelivering four 5 kilograms of cargo per day.\nShe earns $4\nx 2= $\u00ab4*2=8\u00bb8 after delivering two 8 kilograms of cargo per\nday.\nSo, her total earning per day is $8+$10= $\u00ab8+10=18\u00bb18.\nTherefore, she will earn $18 x 7= $\u00ab18*7=126\u00bb126 in a week.\nThe answer is:\n126\nGSM8K\nQuestion:\nBill is laying power cable for a new neighborhood.\nThere are going to be 18 east-west streets that are 2 miles\nlong and 10 north-south streets that are four miles long.\nIt takes 5 miles of cable to electrify 1 mile of street.\nIf\ncable costs $2000/mile, what is the total cost of cable\nfor the neighborhood?\nFirst find the total distance\nof the east-west streets:\n18 streets * 2 miles/street\n= \u00ab18*2=36\u00bb36 miles.\nThen find the total distance of\nthe north-south streets:\n10 streets * 4 miles/street =\n\u00ab10*4=40\u00bb40 miles.\nThen add the number of miles from each\ntype of street to find the total distance:\n36 miles + 40\nmiles = \u00ab36+40=76\u00bb76 miles.\nThen multiply that number by\n5 to find the number of miles of cable needed:\n76 miles\nstreet * 5 miles cable/mile street = \u00ab76*5=380\u00bb380 miles of\ncable.\nThen multiply that number by the cost of one mile\nof cable to find the total cost:\n380 miles * $2000/mile =\n$\u00ab380*2000=760000\u00bb760,000.\nThe answer is:\n760000\nGSM8K\nQuestion:\nJohn buys a gaming PC for $1200.\nHe decides to\nreplace the video card in it.\nHe sells the old card for\n$300 and buys a new one for $500.\nHow much money did he\nspend on his computer, counting the savings from selling\nthe old card?\nHe spent an extra 500-300=$\u00ab500-300=200\u00bb200\non the video card.\nThat means the total cost was\n1200+200=$\u00ab1200+200=1400\u00bb1400.\nThe answer is:\n1400\nSST2\nsentence:\nfaced and spindly attempt at playing an ingenue\nmakes her nomination as best actress even more of a an a\npositive\nSST2\nsentence:\nholofcener\u2019s film offers just enough insight to\nkeep it from being simpleminded, and positive\nSST2\nsentence:\ni\u2019m not a fan of the phrase \u2018 life affirming\u2019\nbecause it usually means \u2018 schmaltzy,\u2019 but real women have\ncurves truly is life affirming negative\nSST2\nsentence:\nthe script is about as interesting as a recording\nof conversations at the wal-mart checkout line negative\nDBpedia\nOfficeHolder Lucie Papin (born September 7 1936) is a former\nCanadian politician who served in both the House of Commons\nand Senate.\nDBpedia\nVillage Kunkalamarru is very renowned village under\nKaramchedu Mandal which is located about 15 km from the\nbusy commercial town of Chirala in Prakasam district in the\nstate of Andhra Pradesh India.Its neighbouring villages are\nKaramchedu Veerannapalem.\nDBpedia\nEducationalInstitution The Pontifical Catholic University\nof Puerto Rico at Mayagez is a university located in the\ncity of Mayagez Puerto Rico.\nIt is part of the Pontifical\nCatholic University of",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning in large language models (LLMs), highlighting the sensitivity of this capability to the selection of few-shot demonstrations and the lack of a comprehensive understanding of the underlying mechanisms. Previous methods have not effectively solved this problem, necessitating a new approach.",
        "problem": {
            "definition": "The problem is the difficulty in achieving optimal performance in in-context learning due to its sensitivity to the choice, format, and order of demonstrations used.",
            "key obstacle": "The main challenge is the lack of a comprehensive understanding of why certain demonstrations are effective while others are not, which limits the ability to select optimal demonstrations."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by viewing LLMs as latent variable models, suggesting that they can infer task information from demonstrations.",
            "opinion": "We propose an algorithm that selects optimal demonstrations from a set of annotated data using a smaller language model, which can then be generalized to larger LLMs.",
            "innovation": "The primary innovation lies in the introduction of a practical demonstration selection algorithm that utilizes a small LLM to select demonstrations based on a latent concept variable, which is a significant improvement over existing heuristic methods."
        },
        "method": {
            "method name": "Latent Variable Demonstration Selection",
            "method abbreviation": "LVDS",
            "method definition": "A method that selects optimal demonstrations for in-context learning by utilizing a small language model to infer a latent variable containing task information.",
            "method description": "The method involves latent concept learning followed by demonstration selection to maximize the likelihood of task-specific outputs.",
            "method steps": [
                "Perform latent concept learning to identify the optimal latent variable for the task.",
                "Select a smaller set of demonstrations based on the learned latent variable.",
                "Generalize the selected demonstrations to larger language models."
            ],
            "principle": "The effectiveness of the method is based on the assumption that conditioning on an appropriate latent variable allows LLMs to generate desired outputs more accurately."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on eight datasets from various NLP classification tasks, including sentiment analysis and hate speech detection, using multiple LLMs for comparison.",
            "evaluation method": "The performance of the proposed method was assessed by comparing it against baseline methods using metrics such as accuracy across different datasets."
        },
        "conclusion": "The study concludes that the proposed latent variable explanation of in-context learning provides a robust algorithm that significantly improves performance across various NLP tasks, validating the hypothesis that LLMs can infer latent variables from demonstrations.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved performance over baseline methods and the ability to generalize selected demonstrations across different LLMs.",
            "limitation": "A limitation of the method is the potential for the selected demonstrations to be correlated, which may affect performance in practice.",
            "future work": "Future research could focus on enhancing the demonstration selection algorithm and exploring additional methods to improve the performance of the latent concept learning process."
        },
        "other info": {
            "acknowledgements": "This work was supported by the National Science Foundation award #2048122.",
            "notable contributions": [
                "Introduction of a two-step algorithm for demonstration selection.",
                "Empirical validation of the method across real-world datasets."
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses in-context learning in large language models (LLMs), highlighting the sensitivity of this capability to the selection of few-shot demonstrations."
        },
        {
            "section number": "1.2",
            "key information": "The study concludes that the proposed latent variable explanation of in-context learning provides a robust algorithm that significantly improves performance across various NLP tasks."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method, Latent Variable Demonstration Selection (LVDS), selects optimal demonstrations for in-context learning by utilizing a small language model to infer a latent variable containing task information."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge is the lack of a comprehensive understanding of why certain demonstrations are effective while others are not, which limits the ability to select optimal demonstrations."
        },
        {
            "section number": "4.1",
            "key information": "The proposed algorithm selects optimal demonstrations from a set of annotated data, suggesting that effective prompt design can enhance in-context learning outcomes."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the method is the potential for the selected demonstrations to be correlated, which may affect performance in practice."
        }
    ],
    "similarity_score": 0.7725251745257197,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Large Language Models Are Latent Variable Models_ Explaining and Finding Good Demonstrations for In-Context Learning.json"
}