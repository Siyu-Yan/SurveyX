{
    "from": "google",
    "scholar_id": "0YpGhnk0oW8J",
    "detail_id": null,
    "title": "Leveraging large language models for sequential recommendation",
    "abstract": "# Leveraging Large Language Models for Sequential Recommendation\n\nJESSE HARTE, Delivery Hero Research, Germany and Delft University of Technology, The Netherland WOUTER ZORGDRAGER, Delivery Hero Research, Germany PANOS LOURIDAS, Athens University of Economics & Business, Greece ASTERIOS KATSIFODIMOS, Delft University of Technology, The Netherlands DIETMAR JANNACH, University of Klagenfurt, Austria MARIOS FRAGKOULIS, Delivery Hero Research, Germany\n\nSequential recommendation problems have received increasing attention in research during the past few years, leading to the inception\nof a large variety of algorithmic approaches. In this work, we explore how large language models (LLMs), which are nowadays\nintroducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches.\nSpecifically, we devise and evaluate three approaches to leverage the power of LLMs in different ways. Our results from experiments\non two datasets show that initializing the state-of-the-art sequential recommendation model BERT4Rec with embeddings obtained\nfrom an LLM improves NDCG by 15-20% compared to the vanilla BERT4Rec model. Furthermore, we find that a simple approach\nthat leverages LLM embeddings for producing recommendations, can provide competitive performance by highlighting semantically\nrelated items. We publicly share the code and data of our experiments to ensure reproducibility. 1\n\nAdditional Key Words and Phrases: Recommender Systems, Large Language Models, Sequential Recommendation, Evaluation\nACM Reference Format:\nJesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging\nLarge Language Models for Sequential Recommendation. In Seventeenth ACM Conference on Recommender Systems (RecSys \u201923),\nSeptember 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3604915.3610639\n\nAdditional Key Words and Phrases: Recommender Sy",
    "bib_name": "harte2023leveraging",
    "md_text": "# Leveraging Large Language Models for Sequential Recommendation\n\nJESSE HARTE, Delivery Hero Research, Germany and Delft University of Technology, The Netherland WOUTER ZORGDRAGER, Delivery Hero Research, Germany PANOS LOURIDAS, Athens University of Economics & Business, Greece ASTERIOS KATSIFODIMOS, Delft University of Technology, The Netherlands DIETMAR JANNACH, University of Klagenfurt, Austria MARIOS FRAGKOULIS, Delivery Hero Research, Germany\n\nSequential recommendation problems have received increasing attention in research during the past few years, leading to the inception\nof a large variety of algorithmic approaches. In this work, we explore how large language models (LLMs), which are nowadays\nintroducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches.\nSpecifically, we devise and evaluate three approaches to leverage the power of LLMs in different ways. Our results from experiments\non two datasets show that initializing the state-of-the-art sequential recommendation model BERT4Rec with embeddings obtained\nfrom an LLM improves NDCG by 15-20% compared to the vanilla BERT4Rec model. Furthermore, we find that a simple approach\nthat leverages LLM embeddings for producing recommendations, can provide competitive performance by highlighting semantically\nrelated items. We publicly share the code and data of our experiments to ensure reproducibility. 1\n\nAdditional Key Words and Phrases: Recommender Systems, Large Language Models, Sequential Recommendation, Evaluation\nACM Reference Format:\nJesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging\nLarge Language Models for Sequential Recommendation. In Seventeenth ACM Conference on Recommender Systems (RecSys \u201923),\nSeptember 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3604915.3610639\n\nAdditional Key Words and Phrases: Recommender Systems, Large Language Models, Sequential Recommendation, Evaluation\n\nJesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging\nLarge Language Models for Sequential Recommendation. In Seventeenth ACM Conference on Recommender Systems (RecSys \u201923)\nSeptember 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3604915.3610639\n\n# 1 INTRODUCTION\n\nSequential recommendation problems have received increased interest recently [31, 38]. In contrast to the traditional,\nsequence-agnostic matrix-completion setup [33], the problem in sequential recommendation is to predict the next\nuser interest or action, given a sequence of past user interactions. Practical applications of sequential recommendation\ninclude next-purchase prediction, next-track music recommendation, or next Point-of-Interest suggestions for tourism.\nDue to their high practical relevance, a multitude of algorithmic approaches have been proposed in the past few years\n[14, 21, 30, 35], including approaches that utilize side information about the items, such as an item\u2019s category [23, 41].\nFrom a technical perspective, the sequential recommendation problem shares similarities with the next word predic\ntion problem [7, 32]. Under this light, we can observe a parallel between research in Natural Language Processing (NLP)\nand sequential recommendation, where novel recommendation models are inspired by NLP models [6]. GRU4Rec [14]\nadopted the Gated Recurrent Unit (GRU) mechanism from [5], SASRec [21] used the transformer architecture from [37],\n1 https://github.com/dh-r/LLM-Sequential-Recommendation\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided t made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrig components of this work must be honored. For all other uses, contact the owner/author(s).\n\u00a9 2023 Copyright held by the owner/author(s). Manuscript submitted to ACM\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\u00a9 2023 Copyright held by the owner/author(s). Manuscript submitted to ACM\n\nd BERT4Rec [35] adopted BERT [7]. The influence of NLP research to sequential recommendation models extends\nturally to Large Language Models (LLMs). LLMs, in particular ones based on Generative Pretrained Transformers [32],\ne exhibiting disruptive effects in various AI-based applications with their semantically rich and meaningful responses.\nHowever, limited research exists so far on leveraging the inherent semantic information of LLMs, which the\novementioned approaches lack, for sequential recommendation problems. A number of recent works in fact started\nexplore the potential of relying on LLMs for recommendation tasks; see [27, 40] for recent surveys. Here, we extend\ns line of research for sequential recommendation problems, providing the following contributions and insights.\n\u2022 We devise three orthogonal methods of leveraging LLMs for sequential recommendation. In our first approach\n(LLMSeqSim), we retrieve a semantically-rich embedding from an existing LLM (from OpenAI) for each item in\na session. We then compute an aggregate session embedding to recommend catalog products with a similar\nembedding. In the second approach (LLMSeqPrompt), we fine-tune an LLM with dataset-specific information\nin the form of prompt-completion pairs and ask the model to produce next item recommendations for test\nprompts. Finally, our third approach (LLM2BERT4Rec) consists of initializing existing sequential models with item\nembeddings obtained from an LLM.\n\u2022 Experiments on two datasets, including a real-world dataset from Delivery Hero, reveal that initializing a\nsequential model with LLM embeddings is particularly effective: applying it to the state-of-the-art model BERT4Rec\nimproves accuracy in terms of NDCG by 15-20%, making it the best-performing model in our experiments.\n\u2022 Finally, we find that in certain applications simply using LLM embeddings to find suitable items for a given\nsession (LLMSeqSim) can lead to state-of-the-art performance.\n\nThe recent developments in LLMs have taken the world by surprise. Models like OpenAI GPT [4], Google BERT [7], and\nFacebook LLaMA [36], which employ deep transformer architectures, demonstrate how innovations in NLP can reshape\nmainstream online activities, such as search, shopping, and customer care. Inevitably, research in recommender systems\nis significantly impacted by the developments in the area of LLMs as well. According to recent surveys [27, 40], LLMs\nare mainly utilized for recommendation problems in two ways: by providing embeddings that can be used to initialize\nexisting recommendation models [29, 39, 43], and by producing recommendations leveraging their inherent knowledge\nencoding [2, 13, 22]. LLMs as recommendation models can provide recommendations given a) only a task specification\n(zero-shot), b) a few examples given inline to the prompt of a task (few-shot), or c) after fine-tuning the model\u2019s weights\nfor a task given a set of training examples [4]. This incremental training process deviates from typical recommendation\nmodels, which have to be trained from zero on domain data. In fact, LLMs show early indications of adaptability\nto different recommendation domains with modest fine-tuning [15, 16]. Finally, LLMs have been applied in various\nrecommendation tasks, such as rating prediction [25], item generation [26], and reranking [17] across domains [29, 39].\nIn this work we explore the potential of using LLMs for sequential recommendation problems [20]. In short, in\nsequential recommendation problems, we consider as input a sequence of user interactions \ud835\udc46 \ud835\udc62 = (\ud835\udc46 \ud835\udc62 1,\ud835\udc46 \ud835\udc62 2, ...,\ud835\udc46 \ud835\udc62\ud835\udc5b), for\nuser \ud835\udc62, where \ud835\udc5b is the length of the sequence and \ud835\udc46 \ud835\udc62 \ud835\udc56 are individual items. The aim is to predict the next interaction of\nthe given sequence. Besides the recent sequential recommendation models mentioned in the introduction [14, 21, 35], in\nearlier works, the sequential recommendation problem has been modelled as a Markov Chain [9] or a Markov Decision\nProcess [34]. Neighborhood-based approaches, such as SKNN [19], have also been proposed.\n\nEarly research work regarding LLMs for sequential recommendation problems has shown mixed results [8, 11, 17, 28,\n44]. The very recent VQ-Rec model [15] employs a transformer architecture and applies a novel representation scheme\nto embeddings retrieved from BERT in order to adapt to new domains. VQ-Rec outperforms a number of sequential\nrecommendation models across datasets of different domains, and it has been shown that SASRec with LLM embeddings\nis better than the original SASRec method for half of the datasets representing different domains. Finally, in an upcoming\nwork [42], SASRec with LLM embeddings is shown to improve over SASRec. The recent approaches presented in [15]\nand [42] differ from our work in particular in terms of the goals they pursue. VQ-Rec [15] targets cross-domain\nrecommendations with a novel item representation scheme, while [42] evaluates whether recommendation models\nleveraging different modalities perform better than existing recommendation models that rely on item identifiers.\nThe work presented in this paper complements these recent lines of research and proposes and evaluates three\nalternative ways of leveraging LLMs for sequential recommendation. Differently from earlier approaches, our work\nshows that initializing an existing sequential model with LLM-based embeddings is highly effective and helps to\noutperform existing state-of-the-art models. In addition, we find that retrieving relevant items solely based on LLM\nembedding similarity can lead to compelling recommendations depending on the dataset.\n\n# 3 THREE LLM-BASED APPROACHES FOR SEQUENTIAL RECOMMENDATIONS\n\n# SED APPROACHES FOR SEQUENTIAL RECOMMENDATIO\n\ns section, we describe the three technical approaches sketched i\n\nwe describe the three technical approaches sketched in Section 1\n\n# 3.1 LLMSeqSim: Recommending Semantically Related Items via LLM Embeddings\n\nWith this first approach, our goal is to explore if recommendations can benefit from a holistic notion of similarity\nprovided by LLMs. To achieve this, we leverage LLM embeddings to produce recommendations in three steps. First, we\nquery the text-embedding-ada-002 2 OpenAI embedding model with the names of the products in the item catalog\nand retrieve their embeddings. Second, we compute a session embedding for each session in our test set by combining\nthe embeddings of the individual products in the session. Here, we try different combination strategies: a) the average of\nthe product embeddings, b) a weighted average using linear and exponential decay functions depending on the position\nof the item in the session, and c) only the embedding of the last product. 3 Third, we compare the session embedding to\nthe embeddings of the items in the product catalog using cosine, Euclidean, and dot product similarity. 4 Finally, we\nrecommend the topk products from the catalog with the highest embedding similarity to the session embedding.\n\n# 3.2 LLMSeqPrompt: Prompt-based Recommendations by a Fine-Tuned LLM\n\n# LLMSeqPrompt: Prompt-based Recommendations by a F\n\nIn this approach, we inject domain knowledge to the collective information that a base LLM incorporates, with the goal\nof increasing the quality of the recommendations by an LLM that is given information about an ongoing session in\nthe form of a prompt. To this end, we fine-tune an OpenAI ada model on training samples consisting of a prompt (the\ninput) and a completion (the intended output). In our case, the prompt is a session, which contains a list of product\nnames except for the last product, and the completion is the name of the last product in the same session, see Figure 1.\nTo optimize performance, we fine-tune the model until the validation loss converges. After training, we provide the\nprompts of the sessions in the test set to the fine-tuned model to obtain recommendations. We note that we make no\nstrong assumption regarding the order of the returned recommendations. Therefore, we use the tendency of the model\n\n2 https://platform.openai.com/docs/guides/embeddings/second-generation-models 3 We also tried to create an aggregated session embedding by concatenating the plain product names and then querying the Open AI em This however led to worse results. 4 The choice of the similarity measure did not significantly impact the results.\n\n2 https://platform.openai.com/docs/guides/embeddings/second-generation-models 3 We also tried to create an aggregated session embedding by concatenating the plain product names and then querying the Open AI embeddings API. This however led to worse results. 4 The choice of the similarity measure did not significantly impact the results.\n\nto provide duplicate recommendations as a proxy of its confidence and rank the recommendations by frequency of\nappearance. Then, to create a full slate of unique recommendations, we retrieve the embedding of each duplicate product\nusing the OpenAI embeddings API and take the catalog\u2019s product that is closest in terms of embedding similarity\nusing the dot product measure. Finally, we note that the fine-tuned LLM, being a generative model, may also return\nhallucinated products, which we map to catalog products using the same method as for duplicate products.\n\n{\"prompt\":\"1. Burt's Bees Rosewater Toner 8oz\\n 2. Philosophy Lip Shine, Mimosa, 0.5 Ounce\\n 3. LaLicious Sugar Souffle Body Scrub 16 fl oz.\\n 4. Marc Jacobs Daisy Eau So Fresh Eau de Toilette Spray-125ml/4.25 oz.\\n \\n\\n###\\n\\n\",\"completion\": \" Bcbg Max Azria Eau De Parfum Spray for Women, 3.4 Ounce ###\"}\n\n# Recommending with an LLM-enhanced Sequential Mode\n\nIn our third approach, our goal is to leverage the semantically-rich item representations provided by an LLM to enhance\nan existing sequential recommendation model. Specifically, in our work we focus on BERT4Rec [35], a state-of-the-art\ntransformer-based model, which employs the transformer architecture [37] of BERT [7].\nBERT \u2019s transformer architecture consists of an embedding layer, a stack of encoder layers, and a projection head.\nFurthermore, BERT features a masked language model training protocol, which involves masking items at random\npositions and letting the model predict their true identity. Initially, the embedding layer embeds an input sequence of\n(potentially masked) item IDs into a sequence of embeddings using both the item ID and the item position. Then the\ntransformer encoder layers process the embedding sequence using a multi-head attention module and a feed-forward\nnetwork shared across all positions. Finally, the projection head projects the embeddings at each masked position to a\nprobability distribution in order to obtain the true identity of the masked item. The projection head reuses the item\nembeddings of the embedding layer to reduce the model\u2019s size and to avoid overfitting.\nTo allow BERT4Rec to leverage the rich information encoded in LLMs, we initialize BERT4Rec \u2019s item embeddings using\nthe LLM embeddings described in Section 3.1. In order to align the embedding dimension of the LLM embeddings (1536)\nwith the configured dimension of BERT4Rec \u2019s embedding layer (e.g., 64), we employ Principal Components Analysis\n(PCA) to get 64 principal components of the LLM embeddings, which we then use to initialize the item embeddings of\nBERT4Rec \u2019s embedding layer. Finally, we train the enhanced model the same way as our baseline BERT4Rec model.\n\n# 4 EXPERIMENTAL EVALUATION\n\nn this section, we describe our experimental setup (Section 4.1) and the results of our empirical evaluation (Section 4.2).\n\nIn this section, we describe our experimental setup (Section 4.1) and the results of our empirical evaluation (Section 4.2).\n\n# 4.1 Experimental setup\n\nDatasets and Data Splitting. We use the public Amazon Beauty [12] dataset and a novel, real-world e-commerce\ndataset from Delivery Hero 5 for our experiments. The Beauty dataset contains product reviews and ratings from\nAmazon. In line with prior research [1], we pre-processed the dataset to include at least five interactions per user and\nitem (p-core = 5). The Delivery Hero dataset contains anonymous QCommerce sessions for dark store and local shop\norders. To better simulate a real-world setting, we did not preprocess this dataset, except that we removed sessions\n5 https://www.deliveryhero.com\n\n5 https://www.deliveryhero.com\n\nDataset\n# sessions\n# items\n# interactions\nAvg. length\nDensity\nBeauty 5-core\n22,363\n12,101\n198,502\n8.9\n0.073%\nDelivery Hero\n258,710\n38,246\n1,474,658\n5.7\n0.015%\nTable 1. Dataset statistics\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca54/ca54f9c8-a905-4908-ac91-afca92c7fba0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">popularity (left) and histogram of session length (right) for the datase\n</div>\nwith only one interaction from the test set. QCommerce is a segment of e-Commerce focusing on fast delivery times on\nthe last mile. Dataset statistics are given in Table 1. To create a train and test set in a sound way, we first split a dataset\ncontaining sessions temporally such that all test sessions succeed train sessions in time. Then in the test set, we adopt\nthe leave-one-out approach followed by [21, 35] where all but the last interaction of each session represent the prompt,\nwhile the last interaction serves as the ground truth.\nMetrics. We use the standard ranking accuracy metrics NDCG, MRR, and HitRate at the usual cut-off lengths of 10\nand 20. Furthermore, we consider the following beyond-accuracy metrics to obtain a more comprehensive picture of\nthe performance of the different algorithms: catalog coverage, serendipity, and novelty. Catalog coverage represents\nthe fraction of catalog items that appeared in at least one top-n recommendation list of the users in the test set [18].\nSerendipity measures the average number of correct recommendations for each user that are not recommended by a\npopularity baseline [10]. Novelty computes the negative log of the relative item popularity, or self-information [45].\nModels. We include both session-based algorithms of different families, GRU4Rec [14], and SKNN [19], as well as two\nstate-of-the-art sequential models, BERT4Rec [35] and SASRec [21]. We tested all variants of the SKNN nearest-neighbor\nmethod proposed in [30] and report the results in the online material. In addition, we include the three LLM-based\napproaches proposed in Section 3. Finally, we include a popularity-based baseline (MostPopular) in the experiments.\nHyperparameter Tuning. We systematically tuned all models (except the LLMSeqSim and the LLMSeqPrompt) on three\nvalidation folds with the Tree Parzen Estimator (TPE) sampler [3], and used the average NDCG@20 across the folds as\nthe optimization goal. For LLMSeqPrompt, we applied manual hyperparameter search. The examined hyperparameter\nranges and optimal values for each dataset are reported in the online material.\n\n# 4.2 Results and Discussion\n\nTable 2 and Table 3 show the results obtained for the Amazon Beauty and the Delivery Hero dataset on the hidden test\nset, respectively. We report the best results of 5 runs. The table is sorted according to NDCG@20.\n\nTop@10\nTop@20\nModel\nnDCG\nHR\nMRR\nCatCov\nSeren\nNovel\nnDCG\nHR\nMRR\nCatCov\nSeren\nNovel\nLLM2BERT4Rec\n0.041\n0.076\n0.030\n0.180\n0.072\n11.688\n0.051\n0.118\n0.033\n0.260\n0.110\n11.888\nLLMSeqSim\n0.044\n0.063\n0.038\n0.763\n0.063\n13.819\n0.048\n0.079\n0.039\n0.889\n0.079\n13.858\nV_SKNN\n0.041\n0.071\n0.033\n0.673\n0.069\n12.241\n0.047\n0.095\n0.034\n0.889\n0.091\n12.492\nBERT4Rec\n0.034\n0.067\n0.024\n0.231\n0.064\n12.293\n0.043\n0.103\n0.027\n0.312\n0.098\n12.423\nGRU4Rec\n0.027\n0.051\n0.020\n0.145\n0.047\n11.409\n0.035\n0.082\n0.022\n0.214\n0.074\n11.597\nSASRec\n0.026\n0.051\n0.019\n0.121\n0.048\n11.485\n0.033\n0.080\n0.021\n0.182\n0.073\n11.678\nLLMSeqPrompt\n0.025\n0.045\n0.019\n0.500\n0.044\n13.001\n0.030\n0.064\n0.020\n0.688\n0.063\n13.361\nMostPopular\n0.005\n0.010\n0.003\n0.001\n0.001\n9.187\n0.006\n0.018\n0.003\n0.002\n0.001\n9.408\nTable 2. Evaluation results for the Amazon Beauty dataset\nTable 2. Evaluation results for the Amazon Beauty dataset\n\nAccuracy Results. The highest values in terms of NDCG@20 are obtained by LLM2BERT4Rec for both datasets. In\nboth cases, the gains obtained by using LLM-based item embeddings are substantial, demonstrating the benefits of\nrelying on semantically-rich embeddings in this sequential model. The NDCG value increased by more than 20% for\nBeauty and over 15% on the Delivery Hero dataset. 6 To confirm that the semantics of the LLM embeddings is the driver\nof performance, we ran an experiment in which we permuted the item embeddings such that the embedding of each\nitem is initialized to the principal components of the LLM embedding of another product from the catalogue. The\nexperiment maintains the statistical properties of the embeddings, but deprives the item embeddings of the semantics\nof the LLM embeddings. The resulting model exhibited worse performance than the baseline BERT4Rec model with\nrandomly-initialized item embeddings clearly showing that the performance improvement cannot be credited to the\nstatistical properties of the embeddings.\nThe relative performance of LLMSeqSim, again considering NDCG values, varies across the two datasets. On the\nBeauty dataset, the model is highly competitive, with NDCG@20 values only being slightly lower than LLM2BERT4Rec.\nAt shorter list lengths, i.e., at NDCG@10, the LLMSeqSim model even leads to the best performance for this dataset.\nNotably, the embedding combination strategy that led to the best results considered only the last item of the session\n(see Section 3.1). For the Delivery Hero dataset, in contrast, the picture is very different, and LLMSeqSim leads to quite\npoor performance, only outperforming the popularity-based baseline. We hypothesize that this phenomenon is a result\nof the quite different characteristics of the two datasets. For example, in Figure 2, we observe that many items in the\nreal-world Delivery Hero dataset occur very infrequently. This may limit the capacity of LLMSeqSim to find similar\nitems, given also the substantially broader item catalog in the Delivery Hero dataset. Furthermore, a manual inspection\nof a sample of test prompts, recommendations, and ground truths of the two datasets indicates that users in the Beauty\ndataset frequently rate items of a certain brand. Since brand names are part of the product names that are input to the\nLLM, recommending similar items may turn out to be particularly effective.\nLooking at the other accuracy metrics (Hit Rate and MRR), we find that these are generally highly correlated with\nthe NDCG results. A notable exception are the MRR values of the LLMSeqSim model and the V_SKNN approach on the\nBeauty dataset. While these two approaches lead to slightly inferior results at NDCG@20 and in particular also for\nHR@20, they are superior in terms of MRR. This means that these methods place the hidden target item higher up\nin the recommendation list in case the target item is included in the top 20. Similar observations regarding the good\nperformance of some methods in terms of MRR on specific datasets were previously reported also in [30].\n\n6 We also examined the value of LLM embeddings for the SASRec model, where we observed marked increases in the NDCG, but no outperformed LLM2BERT4Rec. We report these additional results in the online material.\n\n6 We also examined the value of LLM embeddings for the SASRec model, where we observed marked increases in the NDCG, but not to the extent that it outperformed LLM2BERT4Rec. We report these additional results in the online material.\n\nTop@10\nTop@20\nModel\nnDCG\nHR\nMRR\nCatCov\nSeren\nNovel\nnDCG\nHR\nMRR\nCatCov\nSeren\nNovel\nLLM2BERT4Rec\n0.102\n0.179\n0.078\n0.245\n0.151\n10.864\n0.120\n0.252\n0.083\n0.311\n0.198\n11.050\nBERT4Rec\n0.088\n0.157\n0.067\n0.325\n0.128\n10.821\n0.104\n0.221\n0.071\n0.429\n0.165\n11.032\nGRU4Rec\n0.085\n0.153\n0.064\n0.127\n0.124\n10.570\n0.101\n0.218\n0.068\n0.172\n0.161\n10.823\nSASRec\n0.084\n0.149\n0.065\n0.170\n0.120\n10.674\n0.100\n0.212\n0.069\n0.229\n0.156\n10.913\nV_SKNN\n0.087\n0.148\n0.068\n0.381\n0.120\n10.444\n0.100\n0.200\n0.072\n0.452\n0.146\n10.602\nLLMSeqPrompt\n0.063\n0.116\n0.047\n0.400\n0.107\n12.048\n0.070\n0.144\n0.049\n0.611\n0.123\n13.788\nLLMSeqSim\n0.039\n0.069\n0.029\n0.633\n0.069\n16.315\n0.046\n0.096\n0.031\n0.763\n0.093\n16.536\nMostPopular\n0.024\n0.049\n0.017\n0.000\n0.000\n7.518\n0.032\n0.079\n0.019\n0.001\n0.000\n7.836\nTable 3. Evaluation results for the Delivery Hero dataset\nTable 3. Evaluation results for the Delivery Hero dataset\n\nInterestingly, as also reported in [24, 30], nearest-neighbor approaches can be quite competitive depending on the\ndataset. On Beauty, V_SKNN outperforms all of the more sophisticated neural models (BERT4Rec, GRU4Rec, SASRec) in all\naccuracy metrics except Hit Rate@20. On the Delivery Hero dataset, in contrast, the neural models perform better in all\naccuracy metrics except MRR and NDCG@10. Further inspection (see online material) showed that SKNN \u2019s performance\ndrops as the length of sessions increases, while the performance of the other models remains stable.\nThe performance of the LLMSeqPrompt model again depends on the dataset. On the Beauty dataset, it leads to\naccuracy values that are often only slightly lower than SASRec, which is typically considered a strong state-of-the-art\nbaseline. On the Delivery Hero dataset, in contrast, the drop in performance compared to the other models is substantial.\nStill, LLMSeqPrompt leads to accuracy values that are markedly higher than the popularity baseline. Given its versatility,\nease of configuration and promising performance, LLMSeqPrompt merits further research.\nBeyond-Accuracy Results. We make the following observations for coverage, serendipity and novelty. The  LLM\nSeqSim model consistently leads to the best coverage and novelty. This is not too surprising, given the nature of the\napproach, which is solely based on embeddings similarities. Unlike other methods that use collaborative signals, i.e.,\npast user-item interactions, the general popularity of an item in terms of the amount of observed past interactions\ndoes not play role in LLMSeqSim, neither directly nor implicitly. Thus, the model has no tendency to concentrate\nthe recommendations on a certain subset of (popular) items. We recall that the used novelty measure is based on\nthe popularity of the items in the recommendations. The serendipity results are largely aligned with the accuracy\nmeasures across the datasets. This generally confirms the value of personalizing the recommendations to individual user\npreferences, compared to recommending mostly popular items to everyone. We iterate that our serendipity measure\ncounts the fraction of correctly recommended items that would not be recommended by a popularity-based approach.\n\n# 5 CONCLUSIONS\n\nIn this work, we devised and evaluated three approaches that leverage LLMs for sequential recommendation problems. A\nsystematic empirical evaluation revealed that BERT4Rec initialized with LLM embeddings achieves the best performance\nfor two datasets, and that the LLM-based initialization leads to a substantial improvement in accuracy. In our future\nwork, we plan to investigate if our findings generalize to different domains, using alternative datasets with diverse\ncharacteristics. Furthermore, we will explore if using other LLMs, e.g., ones with different architectures and training\ncorpora, will lead to similar performance gains, including a hybrid of LLM2BERT4Rec with LLMSeqSim towards combining\ntheir accuracy and beyond-accuracy performance. Finally, it is open so far if passing other types of information besides\nproduct names, e.g., category information, to an LLM can help to further improve the performance of the models.\n\n# REFERENCES\n\n[1] Vito Walter Anelli, Alejandro Bellog\u00edn, Tommaso Di Noia, Dietmar Jannach, and Claudio Pomo. 2022. Top-N Recommendation Algorithms: A Quest for the State-of-the-Art. In Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization (Barcelona, Spain) (UMAP \u201922). Association for Computing Machinery, 121\u2013131.\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. arXiv:2305.00447 [cs.IR]\n[3] James Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. 2011. Algorithms for Hyper-Parameter Optimization. In Advances in Neural Information Processing Systems, Vol. 24. Curran Associates, Inc.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., 1877\u20131901.\n[5] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 1724\u20131734.\n[6] Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge. 2021. Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation. In Proceedings of the 15th ACM Conference on Recommender Systems (RecSys \u201921). Association for Computing Machinery, 143\u2013153.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 4171\u20134186.\n[8] Hao Ding, Anoop Deoras, Yuyang (Bernie) Wang, and Hao Wang. 2022. Zero shot recommender systems. In ICLR 2022 Workshop on Deep Generative Models for Highly Structured Data.\n[9] Florent Garcin, Christos Dimitrakakis, and Boi Faltings. 2013. Personalized News Recommendation with Context Trees. In Proceedings of the 7th ACM Conference on Recommender Systems (RecSys \u201913). Association for Computing Machinery, 105\u2013112.\n[10] Mouzhi Ge, Carla Delgado-Battenfeld, and Dietmar Jannach. 2010. Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity. In Proceedings of the Fourth ACM Conference on Recommender Systems (RecSys \u201910). Association for Computing Machinery, 257\u2013260.\n[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In Proceedings of the 16th ACM Conference on Recommender Systems (RecSys \u201922). Association for Computing Machinery, 299\u2013315.\n[12] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. In Proceedings of the 25th International Conference on World Wide Web (WWW \u201916). International World Wide Web Conferences Steering Committee, 507\u2013517.\n[13] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. 2023. TabLLM: Few-shot Classification of Tabular Data with Large Language Models. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 206). PMLR, 5549\u20135581.\n[14] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR.\n[15] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders. In Proceedings of the ACM Web Conference 2023 (WWW \u201923). Association for Computing Machinery, 1162\u20131171.\n[16] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \u201922). Association for Computing Machinery, 585\u2013593.\n[17] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large Language Models are Zero-Shot Rankers for Recommender Systems. arXiv:2305.08845 [cs.IR]\n[18] Dietmar Jannach, Lukas Lerche, Iman Kamehkhosh, and Michael Jugovac. 2015. What recommenders recommend: an analysis of recommendation biases and possible countermeasures. User Modeling and User-Adapted Interaction 25, 5 (2015), 427\u2013491.\n[19] Dietmar Jannach and Malte Ludewig. 2017. When Recurrent Neural Networks Meet the Neighborhood for Session-Based Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender Systems (RecSys \u201917). Association for Computing Machinery, 306\u2013310.\n[20] Dietmar Jannach, Bamshad Mobasher, and Shlomo Berkovsky. 2020. Research directions in session-based and sequential recommendation. User Modeling and User-Adapted Interaction 30, 4 (2020), 609\u2013616.\n[21] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Recommendation. In Proceedings of the 18 th IEEE International Conference on Data Mining (ICDM 2018). 197\u2013206.\n\n[22] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv:2305.06474 [cs.IR]\n[23] Siqi Lai, Erli Meng, Fan Zhang, Chenliang Li, Bin Wang, and Aixin Sun. 2022. An Attribute-Driven Mirror Graph Network for Session-based Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1674\u20131683.\n[24] Sara Latifi, Dietmar Jannach, and Andr\u00e9s Ferraro. 2022. Sequential Recommendation: A Study on Transformers, Nearest Neighbors and Sampled Metrics. Information Sciences 609 (2022), 660 \u2013 678.\n[25] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. Text Is All You Need: Learning Language Representations for Sequential Recommendation. In KDD \u201923: The 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.\n[26] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation. arXiv:2304.03879 [cs.IR]\n[27] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv:2306.05817 [cs.IR]\n[28] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv:2304.10149 [cs.IR] [29] Yiding Liu, Weixue Lu, Suqi Cheng, Daiting Shi, Shuaiqiang Wang, Zhicong Cheng, and Dawei Yin. 2021. Pre-trained Language Model for Web-scale Retrieval in Baidu Search. In KDD \u201921: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 3365\u20133375.\n[30] Malte Ludewig and Dietmar Jannach. 2018. Evaluation of Session-based Recommendation Algorithms. User-Modeling and User-Adapted Interaction 28, 4\u20135 (2018), 331\u2013390.\n[31] Massimo Quadrana, Paolo Cremonesi, and Dietmar Jannach. 2018. Sequence-Aware Recommender Systems. Comput. Surveys 51, 4 (2018). [32] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. [33] Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. 1994. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of the 1994 ACM Conference on Computer Supported Cooperative Work. 175\u2013186.\n[34] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. Journal of Machine Learning Research 6, 43 (2005), 1265\u20131295.\n[35] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM \u201919). 1441\u20131450.\n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971 [cs.CL]\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates, Inc.\n[38] Shoujin Wang, Longbing Cao, Yan Wang, Quan Z. Sheng, Mehmet A. Orgun, and Defu Lian. 2021. A Survey on Session-Based Recommender Systems. ACM Comput. Surv. 54, 7 (2021).\n[39] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering News Recommendation with Pre-Trained Language Models. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201921). Association for Computing Machinery, 1652\u20131656.\n[40] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023. A Survey on Large Language Models for Recommendation. arXiv:2305.19860 [cs.IR]\n[41] Hao Xu, Bo Yang, Xiangkun Liu, Wenqi Fan, and Qing Li. 2022. Category-aware Multi-relation Heterogeneous Graph Neural Networks for Session-based Recommendation. Knowledge-Based Systems 251 (2022).\n[42] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited. In SIGIR (To appear).\n[43] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang, and Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recommendation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conferences on Artificial Intelligence Organization, 3356\u20133362.\n[44] Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021. Language models as recommender systems: Evaluations and limitations. In NeurIPS 2021 Workshop on I (Still) Can\u2019t Believe It\u2019s Not Better.\n[45] Tao Zhou, Zolt\u00e1n Kuscsik, Jian-Guo Liu, Mat\u00fa\u0161 Medo, Joseph Rushton Wakeling, and Yi-Cheng Zhang. 2010. Solving the apparent diversity-accuracy dilemma of recommender systems. Proceedings of the National Academy of Sciences 107, 10 (2010), 4511\u20134515.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Sequential recommendation problems have received increased interest recently, leading to various algorithmic approaches. This paper explores how large language models (LLMs) can be utilized to enhance sequential recommendation methods. The research highlights a gap in leveraging the inherent semantic information of LLMs for sequential recommendations, which previous methods have overlooked.",
        "problem": {
            "definition": "The problem addressed in this paper is the challenge of predicting the next user interest or action based on a sequence of past user interactions in sequential recommendation systems.",
            "key obstacle": "The main difficulty lies in the limitations of existing methods that do not effectively utilize the rich semantic information provided by large language models."
        },
        "idea": {
            "intuition": "The idea is inspired by the parallel between sequential recommendation problems and natural language processing tasks, particularly the potential of LLMs to capture semantic relationships.",
            "opinion": "The proposed idea involves leveraging LLMs to improve the accuracy of sequential recommendations by utilizing their embeddings in various ways.",
            "innovation": "The primary innovation is the introduction of three distinct methods that utilize LLM embeddings, significantly improving upon traditional sequential recommendation models."
        },
        "method": {
            "method name": "LLM2BERT4Rec",
            "method abbreviation": "LLM2BERT",
            "method definition": "A method that initializes the item embeddings of the BERT4Rec model with embeddings obtained from a large language model.",
            "method description": "This method enhances the BERT4Rec model by incorporating semantically rich embeddings from LLMs.",
            "method steps": [
                "Retrieve item embeddings from an LLM.",
                "Apply Principal Components Analysis (PCA) to align the dimensions of the embeddings.",
                "Initialize the BERT4Rec model's item embeddings with the processed LLM embeddings.",
                "Train the BERT4Rec model with the initialized embeddings."
            ],
            "principle": "This method is effective because it leverages the semantic richness of LLM embeddings, which enhances the model's ability to understand user preferences and make accurate recommendations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the Amazon Beauty dataset and a real-world dataset from Delivery Hero, with a focus on ranking accuracy metrics such as NDCG and MRR.",
            "evaluation method": "The performance of the methods was assessed using standard ranking accuracy metrics, along with beyond-accuracy metrics like catalog coverage and novelty, across multiple experimental runs."
        },
        "conclusion": "The study concludes that initializing BERT4Rec with LLM embeddings leads to significant improvements in recommendation accuracy, outperforming existing state-of-the-art models on both datasets tested.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include substantial accuracy improvements and the ability to leverage semantic information from LLMs.",
            "limitation": "A limitation noted is that the performance of certain methods varies significantly across different datasets, indicating potential challenges in generalization.",
            "future work": "Future research will explore the generalizability of these findings across different domains and investigate the impact of using alternative LLM architectures."
        },
        "other info": {
            "additional keywords": [
                "Recommender Systems",
                "Large Language Models",
                "Sequential Recommendation",
                "Evaluation"
            ],
            "code availability": "The code and data from the experiments are publicly shared to ensure reproducibility."
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The paper introduces three distinct methods that utilize LLM embeddings, significantly improving upon traditional sequential recommendation models."
        },
        {
            "section number": "4.1",
            "key information": "The study explores how large language models (LLMs) can be utilized to enhance sequential recommendation methods, highlighting their ability to capture semantic relationships."
        },
        {
            "section number": "4.2",
            "key information": "The proposed method, LLM2BERT4Rec, initializes the item embeddings of the BERT4Rec model with embeddings obtained from a large language model, enhancing the model's ability to understand user preferences."
        },
        {
            "section number": "5.1",
            "key information": "The paper addresses the challenge of predicting the next user interest based on a sequence of past user interactions in sequential recommendation systems."
        },
        {
            "section number": "10.2",
            "key information": "Future research will explore the generalizability of the findings across different domains and investigate the impact of using alternative LLM architectures."
        }
    ],
    "similarity_score": 0.776262202517729,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca54/ca54f9c8-a905-4908-ac91-afca92c7fba0.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Leveraging large language models for sequential recommendation.json"
}