{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.15530",
    "title": "An Information-Theoretic Analysis of In-Context Learning",
    "abstract": "Previous theoretical results pertaining to metalearning on sequences build on contrived assumptions and are somewhat convoluted. We introduce new information-theoretic tools that lead to an elegant and very general decomposition of error into three components: irreducible error, meta-learning error, and intra-task error. These tools unify analyses across many meta-learning challenges. To illustrate, we apply them to establish new results about in-context learning with transformers. Our theoretical results characterizes how error decays in both the number of training sequences and sequence lengths. Our results are very general; for example, they avoid contrived mixing time assumptions made by all prior results that establish decay of error with sequence length.",
    "bib_name": "jeon2024informationtheoreticanalysisincontextlearning",
    "md_text": "# An Information-Theoretic Analysis of In-Context Learning\nHong Jun Jeon 1 Jason D. Lee 2 Qi Lei 3 Benjamin Van Roy 4\n[cs.LG]  28 Jan 2024\nAbstract\n# Abstract\nPrevious theoretical results pertaining to metalearning on sequences build on contrived assumptions and are somewhat convoluted. We introduce new information-theoretic tools that lead to an elegant and very general decomposition of error into three components: irreducible error, meta-learning error, and intra-task error. These tools unify analyses across many meta-learning challenges. To illustrate, we apply them to establish new results about in-context learning with transformers. Our theoretical results characterizes how error decays in both the number of training sequences and sequence lengths. Our results are very general; for example, they avoid contrived mixing time assumptions made by all prior results that establish decay of error with sequence length.\narXiv:2401.15530v1\n15530v\n15530\n# 1. Introduction\narXiv:240\nIn recent years, we have observed the capability of large language models (LLMs) to learn from data within just its context window. This puzzling phenomenon referred to as in-context learning (ICL) (Brown et al., 2020), has captured the attention of the theoretical machine learning community. As the data available in-context is dwarfed by the extensive pretraining set, meta-learning stands as a prevailing explanation for ICL (Xie et al., 2022). As aforementioned, Xie et al. (2022) introduced the idea that ICL could be interpreted as implicit Bayesian inference within a mixture of HMMs. While their theoretical results rely on contrived assumptions and fail to explain how ICL is possible with such short sequences, their work initiated the study of modeling ICL as Bayesian inference\nIn recent years, we have observed the capability of large language models (LLMs) to learn from data within just its context window. This puzzling phenomenon referred to as in-context learning (ICL) (Brown et al., 2020), has captured the attention of the theoretical machine learning community. As the data available in-context is dwarfed by the extensive pretraining set, meta-learning stands as a prevailing explanation for ICL (Xie et al., 2022).\n1Department of Computer Science, Stanford University, Stanford, CA, USA 2Princeton University, Princeton, NJ, USA 3New York University, New York City, NY, USA 4Stanford University, Stanford, CA, USA. Correspondence to: Hong Jun Jeon <hjjeon@stanford.edu>.\n1Department of Computer Science, Stanford University, Stanford, CA, USA 2Princeton University, Princeton, NJ, USA 3New York University, New York City, NY, USA 4Stanford University, Stanford, CA, USA. Correspondence to: Hong Jun Jeon <hjjeon@stanford.edu>.\nor other thoroughly studied learning processes such as empirical risk minimization. As much of the theoretical community is most familiar with error analyses of empirical risk minimization, much of the existing results (Li et al., 2023a; Bai et al., 2023; Edelman et al., 2021) study the error of an ICL under the assumption that ICL is competitive in out-of-sample performance with empirical risk minimization. However, each of these error bounds is limited in some way such as exponential depth dependence (Edelman et al., 2021; Li et al., 2023a) or error which decays only with the number of sequences and not the length of the sequences (Edelman et al., 2021; Bai et al., 2023). The results which do demonstrate that error decays in both the number of training sequences and sequence length often rely on contrived mixing time assumptions (Zhang et al., 2023b) or stability conditions which are equivalent to fast mixing (Li et al., 2023a).\nOur work revisits the idea of modeling ICL as Bayesian inference. In this work, we introduce new informationtheoretic tools based on work by Jeon et al. (2023) which lead to an elegant and very general decomposition of error in meta-learning from sequences. This decomposition consists of three components: irreducible error, meta-learning error, and intra-task error. This unifies theoretical error analyses across many meta-learning challenges. Notably, our results provide an error bound which decays linearly in both the number of sequences and the lengths of the sequences without explicit reliance on any stability or mixing assumptions within the sequence. To demonstrate the use of our results, we specialize our theory to reproduce existing results in linear representation learning and to produce new results pertaining to a sparse mixture of transformer models. The latter result provides a compelling narrative as to how ICL is possible with such few examples. As some of our tools are non-standard to much of the community, we begin by introducing our framework in the simpler setting of learning from a single sequence of data. In the following section, we naturally extend the analysis to meta-learning from many sequences and present our main result (Theorem 4.2). Since our results are very general and abstract, we demonstrate the application of these results to\nOur work revisits the idea of modeling ICL as Bayesian inference. In this work, we introduce new informationtheoretic tools based on work by Jeon et al. (2023) which lead to an elegant and very general decomposition of error in meta-learning from sequences. This decomposition consists of three components: irreducible error, meta-learning error, and intra-task error. This unifies theoretical error analyses across many meta-learning challenges. Notably, our results provide an error bound which decays linearly in both the number of sequences and the lengths of the sequences without explicit reliance on any stability or mixing assumptions within the sequence. To demonstrate the use of our results, we specialize our theory to reproduce existing results in linear representation learning and to produce new results pertaining to a sparse mixture of transformer models. The latter result provides a compelling narrative as to how ICL is possible with such few examples.\nprovide concrete examples which resemble learning from data generated by a deep transformer model and in the appendix we provide simpler problem instances for reference (logistic regression, linear representation learning).\n# 2. Related Works\nIn-context Learning and Transformer. LLMs based on the transformer architecture (Vaswani et al., 2023) have exhibited the ability to learn from data within the context of a prompt (Brown et al., 2020). This phenomenon, referred to as in-context learning (ICL), has received significant empirical investigation (Liu et al., 2021; Min et al., 2021; Lu et al., 2021; Zhao et al., 2021; Rubin et al., 2021; Elhage et al., 2021; Kirsch et al., 2022; Wei et al., 2023; Brown et al., 2020; Dong et al., 2022). However, theoretical understanding of ICL is still relatively nascent (Xie et al., 2022; Garg et al., 2022; Von Oswald et al., 2023; Dai et al., 2022; Giannou et al., 2023; Li et al., 2023a; Raventos et al., 2023). Among the existing theoretical work, most focuses on the optimization dynamics (Tian et al., 2023a;b; Jelassi et al., 2022; Li et al., 2023b; Tarzanagh et al., 2023; Zhang et al., 2023a; Huang et al., 2023; Ahn et al., 2023; Mahankali et al., 2023) or the representation power (Sanford et al., 2023; Song & Zhong, 2023; Von Oswald et al., 2023; Giannou et al., 2023; Liu et al., 2022) regarding the transformer architecture. In the realm of statistical results, much of the existing work is confined to how transformers can perform ICL by simulating gradient descent (Von Oswald et al., 2023; Aky\u00fcrek et al., 2022; Dai et al., 2022; Giannou et al., 2023). However, as they provide no concrete sample complexity results, they are therefore not directly comparable to our work. The work that is perhaps most relevant to ours include those which analyze the sample complexity of ICL under the assumption that its performance is comparable to empirical risk minimization or Bayesian inference (Xie et al., 2022; Li et al., 2023a; Bai et al., 2023; Edelman et al., 2021; Zhang et al., 2023b). Despite their quantitative sample complexity results, as mentioned in the introduction, these results are ultimately limited by either their restrictive assumptions on mixing times of the data sequence or their inability to capture how sequence length contributes to reduction in error.\nMeta-learning. As our work analyzes ICL under the lens of meta-learning, we provide a brief exposition of its existing work. Recent empirical advancements have sparked interest in the theoretical foundations of metalearning (Baxter, 2000; Denevi et al., 2018; Finn et al., 2019). In settings such as tasks drawn from a shared meta-distribution, several works (Maurer, 2009;\nPontil & Maurer, 2013; Maurer et al., 2016) have derived generalization bounds albeit for simplistic settings such as linear representation or linear classifiers. Under strong assumptions such as large margin or large number of tasks Srebro & Ben-David (2006); Aliakbarpour et al. (2023) were also able to establish such bounds. However, these results all rely on the assumption that the data within each meta-task is independently and identically distributed (iid) under an (unknown) probability distribution. However, in the context of LLMs, for which the meta-tasks are separate documents, the sequence of tokens within each document is certainly not iid. Our work provides novel theoretical tools which facilitate the analysis of meta-learning from sequential data which may not be iid.\n# 3. Learning from Sequential Data\nFor exposition, we begin by introducing our general information-theoretic tools for the analysis of standard supervised learning on sequential data. Examples of such learning problems include but are not limited to natural language modeling and learning from video/audio data. Phenomena such as ICL in LLMs is another fascinating instance of machine learning from sequential data. Results from this section draw inspiration from (Jeon et al., 2023) which focused on the analysis of supervised learning from iid data.\nFor exposition, we begin by introducing our general information-theoretic tools for the analysis of standard supervised learning on sequential data. Examples of such learning problems include but are not limited to natural language modeling and learning from video/audio data. Phenomena such as ICL in LLMs is another fascinating instance of machine learning from sequential data. Results from this section draw inspiration from (Jeon et al., 2023) which focused on the analysis of supervised learning from iid data. We model all uncertain quantities as random variables. Each random variable we consider is defined with respect to a common probability space (\u2126, F, P). Of particular interest to our analysis is a sequence X1, X2, . . . , XT of discrete random variables which represent observations. This sequence is generated by an autoregressive model parameterized by a random variable \u03b8 such that for all t \u2208Z+, Xt+1 may depend on \u03b8 and the entire history X1, . . . , Xt, which we abbreviate as Ht.\nWe model all uncertain quantities as random variables. Each random variable we consider is defined with respect to a common probability space (\u2126, F, P). Of particular interest to our analysis is a sequence X1, X2, . . . , XT of discrete random variables which represent observations. This sequence is generated by an autoregressive model parameterized by a random variable \u03b8 such that for all t \u2208Z+, Xt+1 may depend on \u03b8 and the entire history X1, . . . , Xt, which we abbreviate as Ht.\n# 3.1. Bayesian Error\nOur framework is Bayesian in the sense that it treats learning as the process of reducing uncertainty about \u03b8, which is taken to be a random variable. A learning algorithm produces, for each t, a predictive distribution Pt of Xt+1 after observing the history Ht. We express such an algorithm in terms of a function \u03c0 for which Pt = \u03c0(Ht). For a horizon T \u2208Z++, we quantify the error realized by predictions Pt for t < T in terms of the average cumulative expected log-loss:\nLT,\u03c0 = 1 T T \u22121 \ufffd t=0 E\u03c0 [\u2212ln Pt(Xt+1)] .\n# 3.2. Achievable Bayesian Error\nA natural question is: which \u03c0 minimizes the Bayesian error? The following result establishes that across all problem instances, the optimal algorithm \u03c0 sets Pt = P(Xt+1 \u2208\u00b7|Ht) for all t. We denote this Bayesian posterior by \u02c6Pt.\nLemma 3.1. (Bayesian posterior is optimal) For all t \u2208 Z+,\nE \ufffd \u2212ln \u02c6Pt(Xt+1) \ufffd\ufffdHt \ufffda.s. = min \u03c0 E\u03c0 [\u2212ln Pt(Xt+1)|Ht] .\nProof. In the below proof take all equality to hold almost surely.\nThe result follows from the fact that dKL( \u02c6Pt\u2225Pt) > 0 for all Pt \u0338= \u02c6Pt.\nWe use LT to denote the optimal achievable Bayesian error:\nWe use LT to denote the optimal achievable Bayesian error:\nLT = 1 T T \u22121 \ufffd t=0 E \ufffd \u2212ln \u02c6Pt(Xt+1) \ufffd .\nIn the main text we restrict our attention to the study of optimal achievable Bayesian error but we provide an extension to arbitrary predictors which depend on the history Ht in Appendix C. The following result provides an exact characterization of the optimal cumulated expected log-loss.\nTheorem 3.2. (Bayesian error) For all T \u2208Z+,\nProof.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c07f/c07fd86e-0e95-49eb-8c28-b00178d095ae.png\" style=\"width: 50%;\"></div>\nwhere (a) and (b) follow from the chain rule of conditional mutual information.\nJeon et al. (2023) establish Theorem 3.2 in the setting in which the sequence is iid when conditioned on \u03b8. We refer to H(HT |\u03b8) as the irreducible error because it is the error incurred by even the omniscient predictor P(Xt+1 \u2208 \u00b7|\u03b8, Ht). The estimation error represents statistical error incurred by an agent that produces estimates of the future Xt+1 from the past sequence Ht. Since estimation error encompasses error which is reducible via learning, our analysis will focus on characterizing this quantity. We use\nto denote the estimation error. LT will often vanish as n \u2192\u221e. For instance, if H(\u03b8) < \u221e, then this will trivially be the case as I(Ht; \u03b8) \u2264H(\u03b8) for all t. However, even in problems for which H(\u03b8) = \u221e, for example if \u03b8 is a continuous random variable, the estimation error will still often vanish as n \u2192\u221e. Note that H(\u03b8) should not be confused with h(\u03b8), the differential entropy of \u03b8. The differential entropy does not capture the same qualitative properties as discrete entropy, namely 1) invariance under change of variables, 2) non-negativity. While differences in differential entropy still provide meaningful insight via mutual information (I(X; Y ) = h(X) \u2212h(X|Y )), the quantity itself is largely vacuous for the purposes of measuring information content and therefore deriving error bounds. The appropriate extension of discrete entropy to continuous random variables can be made via rate-distortion theory. Definition 3.3. (rate-distortion function) Let \u01eb \u22650, \u03b8 : \u2126\ufffd\u2192\u0398 be a random variable, and \u03c1 a distortion function which maps \u03b8 and a random variable \u02dc\u03b8 to \u211c. The rate-\ndistortion function evaluated for random variable \u03b8 at tolerance \u01eb takes the value:\nwhere\nOne can think of \u02dc\u03b8 as a lossy compression of the random variable \u03b8. The objective I(\u03b8; \u02dc\u03b8), referred to as the rate, characterizes the number of nats that \u02dc\u03b8 retains about \u03b8. Meanwhile, the distortion function \u03c1 characterizes how lossy the compression is. When we apply rate-distortion theory to the analysis of machine learning, we restrict our attention to the case in which\n\u03c1(\u03b8, \u02dc\u03b8) = E \ufffd dKL(P(Xt+1 \u2208\u00b7|\u03b8, Ht)\u2225P(Xt+1 \u2208\u00b7|\u02dc\u03b8, Ht)) \ufffd = I(Xt+1; \u03b8|\u02dc\u03b8, Ht).\n= E \ufffd dKL(P(Xt+1 \u2208\u00b7|\u03b8, Ht)\u2225P(Xt+1 \u2208\u00b7|\u02dc\u03b8, Ht)) \ufffd = I(Xt+1; \u03b8|\u02dc\u03b8, Ht).\nWe assume that \u02dc\u03b8 \u22a5Xt+1|(\u03b8, Ht) (the compression \u02dc\u03b8 does not contain exogenous information about Xt+1, such as aleatoric noise, which cannot be determined from (\u03b8, Ht)). We use the notation H\u01eb,T(\u03b8) to denote the rate-distortion function w.r.t. this KL-divergence distortion function averaged across horizon T :\nH\u01eb,T(\u03b8) = inf \u02dc\u03b8\u2208\u02dc\u0398\u01eb.T I(\u03b8; \u02dc\u03b8),\nwhere\nWith this notation established, we present the following result for sequential learning. The proof can be found in Appendix A. Theorem 3.4. (rate-distortion estimation error bound) For all T \u2208Z+,\nAn interpretation of the above result is that the Bayesian posterior implicitly finds the compression \u02dc\u03b8 that optimally trades off learning complexity I(\u03b8; \u02dc\u03b8) and distortion I(HT ; \u03b8|\u02dc\u03b8). While these results are very general, they remain abstract. In Appendix A.1 we provide a simple logistic regression example. In the main text, we provide an analysis for learning from a sequence generated by a deep transformer model.\n3.3. Deep Transformer\n# 3.3. Deep Transformer\nIn the transformer environment, we let (X1, X2, . . .) be a sequence in {1, . . . , d}, where d denotes the size of the vocabulary. Each of the d outcomes is associated with a known embedding vector which we denote as \u03a6j for j \u2208{1, . . . , d}. We assume that for all j, \u2225\u03a6j\u22252 = 1. For brevity of notation, we let \u03c6t = \u03a6Xt i.e. the embedding associated with token Xt. Let K denote the context length of the transformer, L denote it\u2019s depth, and r denote the attention dimension. We assume that the first token X1 is sampled from an arbitrary pmf on {1, . . . , d} but subsequent tokens are sampled based on the previous K tokens within the context window and the weights of a depth L transformer model. We use Ut,i to denote the output of layer i at time t (Ut,0 = \u03c6t\u2212K+1:t) (the embeddings associated with the past K tokens). For all t \u2264T, i < L, let\nIn the transformer environment, we let (X1, X2, . . .) be a sequence in {1, . . . , d}, where d denotes the size of the vocabulary. Each of the d outcomes is associated with a known embedding vector which we denote as \u03a6j for j \u2208{1, . . . , d}. We assume that for all j, \u2225\u03a6j\u22252 = 1. For brevity of notation, we let \u03c6t = \u03a6Xt i.e. the embedding associated with token Xt.\nWe use Ut,i to denote the output of layer i at time t (Ut,0 = \u03c6t\u2212K+1:t) (the embeddings associated with the past K tokens). For all t \u2264T, i < L, let\ndenote the attention matrix of layer i where \u03c3 denotes the softmax function applied elementwise along the columns. The matrix Ai \u2208\u211cr\u00d7r can be interpreted as the product of the key and query matrices and without loss of generality, we assume that the elements of the matrices Ai are distributed iid N(0, 1) (Gaussian assumption is not crucial but known mean and unit variance is).\nUt,i = Clip (ViUt,i\u22121Attni(Ut,i\u22121)) ,\nwhere Clip ensures that each column of the matrix input has L2 norm at most 1. The matrix Vi resembles the value matrix and without loss of generality, we assume that the elements of Vi are distributed iid N(0, 1/d) (same generality conditions as above).\nFinally, the next token is generated via sampling from the softmax of the final layer:\nXt+1 \u223c\u03c3 (Ut,L[\u22121]) ,\nwhere Ut,L[\u22121] denotes the right-most column of Ut,L. At each layer i, the parameters \u03b8i consist of the matrices Ai, Vi. We will use the notation \u03b8i:j for i \u2264j to denote the collection (\u03b8i, \u03b8i+1, . . . , \u03b8j). Theorem 3.5. (transformer estimation error bound) For all d, r, L, K, if \u03b81:L is the transformer environment, then\nWe note that even if the sequence generated by the transformer is not iid, we observe that LT decays linearly in T , the length of the sequence. Furthermore, we observe that LT is upper bounded linearly in the product of parameter count and depth of the transformer model as in Bai et al. (2023). In the following section, we will draw the connection to ICL by studying meta-learning in a data generating process which resembles a sparse mixture of deep transformers.\n# 4. Meta-Learning from Sequential Data\nIn this section, we analyze the achievable performance of meta-learning from sequences. The tools of the Bayesian framework apply exactly as they do in standard supervised learning from sequences. An example of meta-learning from sequences includes language model pretraining in which each \u201cmeta-task\u201d can be interpreted as a separate document and the \u201csequence\u201d as the tokens which comprise the document. We will use the terminology document going forward to refer to a \u201cmeta-task\u201d in meta-learning.\n\ufffd \ufffd We note that this objective largely resembles the objective LLMs minimize in the process of pre-training.\n# 4.1. Data Generating Process\nWe now consider sequential data which resembles a corpus of text documents. We assume that all documents in the corpus have an identical length wich we denote by T . For each document m, we let Dm = X(m) 1 , . . . , X(m) T be the sequence of discrete random variables which resembles its constituent tokens.\nEach document is associated with a random variable \u03b8m which encodes information that is specific to document m. As in the previous section, we assume that the sequence Dm is produced by an autoregressive process. As such, for all t, the value of X(m) t+1 depends on \u03b8m and the prior tokens (X(m) 1 , . . . , X(m) t ) in Dm.\nFinally, we assume that there exists a random variable \u03c8 such that conditioned on \u03c8, (\u03b81, \u03b82, . . .) is an iid sequence. Note that \u03c8 encodes information which learnable across documents in a corpus. As such, \u03c8 represent the meta parameters while (\u03b81, \u03b82, . . .) represent the intra-task parameters. Two natural conditional independence results follow from our formulation. 1) for all m, Dm \u22a5\u03c8|\u03b8m; the meta parameters do not contain information about Dm beyond what is contained in \u03b8m. 2) X(m) t \u22a5X(n) t |\u03c8 for all m \u0338= n; tokens across documents do not contain information about each other beyond what is contained in \u03c8.\n# 4.2. Bayesian Error\nOur framework is Bayesian in the sense that it treats learning as the process of reducing uncertainty about \u03b81, . . . , \u03b8m, \u03c8, which are taken to be random variables.\nFor a meta-learning problem with M documents each of length T , a learning algorithm produces, for each (m, t) \u2208 [M] \u00d7 [T ], a predictive distribution Pm,t of X(m) t+1 after observing the concatenated history which we denote by\nHm,t consists of all tokens from documents 1, . . . , m \u22121 and up to the tth token of document m. We express our meta-learning algorithm in terms of a function \u03c0 for which Pm,t = \u03c0(Hm,t). For all M, T \u2208Z++, we quantify the error realized by predictions Pm,t for (m, t) \u2208[M] \u00d7 [T ] in terms of the average cumulative expected log-loss:\n# 4.3. Achievable Bayesian Error\nWe are in particular interested in the algorithm \u03c0 which minimizes Bayesian error. Just as in supervised learning from sequences, across all problem instances, the optimal algorithm \u03c0 sets Pm,t = P(X(m) t+1 \u2208\u00b7|Hm,t) for all m, t. We denote this Bayesian posterior by \u02c6Pm,t. Lemma 4.1. (Bayesian posterior is optimal) For all m, t \u2208Z+,\nWe use LM,T to denote the optimal achievable Bayesian error:\nWe will restrict our attention to the performance of the optimal predictor \u02c6Pt. We now present the main result of this paper which decomposes optimal Bayesian error into 3 intuitive terms. The following result provides an exact characterization of LM,T . Theorem 4.2. (Main Result) For all M, T \u2208Z+ and m \u2208 {1, 2, . . ., M},\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98c8/98c8ab8b-50eb-4f7e-a480-79fc3a890ffd.png\" style=\"width: 50%;\"></div>\nProof.\nwhere (a) follows from Theorem 3.2, and (b), (c), (d) follow from the chain rule of mutual information.\nThe irreducible error represents the Bayesian error incurred by even the omniscent predictor P(X(m) t+1 \u2208\u00b7|\u03b8m, Hm,t) which conditions on document-specific information \u03b8m and the document history Hm,t.\nThe irreducible error represents the Bayesian error incurred by even the omniscent predictor P(X(m) t+1 \u2208\u00b7|\u03b8m, Hm,t) which conditions on document-specific information \u03b8m and the document history Hm,t. The meta-estimation error represents the statistical error incurred in the process of estimating the meta parameters \u03c8. Since all tokens across all documents contain information about \u03c8, it is intuitive that meta-estimation error term decays linearly in MT . Since M could in practice be very large (for example in a pretraining dataset), LM,T could be small even for small T if significant learning complexity is contained in \u03c8. Finally, the intra-document estimation error represents the statistical error incurred in the process of learning \u03b8m after already conditioning on \u03c8. As only the data from document m (Dm) pertains to \u03b8m, this error intuitively decays linearly in T , the length of the document. As mentioned before, if much of the learning complexity is contained in \u03c8, then I(H(1) T ; \u03b8m|\u03c8) will be small and therefore the intradocument estimation error may be small even for short document length T . We will revisit this idea in section 4.5 when we analyze ICL within this framework. Our subsequent analysis will focus on estimation error as\nThe meta-estimation error represents the statistical error incurred in the process of estimating the meta parameters \u03c8. Since all tokens across all documents contain information about \u03c8, it is intuitive that meta-estimation error term decays linearly in MT . Since M could in practice be very large (for example in a pretraining dataset), LM,T could be small even for small T if significant learning complexity is contained in \u03c8.\nit represents error which is reducible via learning. In metalearning, the total estimation error is:\ni.e. the sum of meta and intra-document estimation errors. We note that Theorem 4.2 holds for all data generating processes which meet the natural assumptions made in subsection 4.1. It is surprising that we can arrive at such a result which decays linearly in both M, the number of documents, and T , the lengths of the documents without any explicit reliance on stability or mixing assumptions. While the main result is useful for conceptual understanding, we need further tools to facilitate the theoretical analysis of concrete meta-learning problem instances. To extend this result, we again use rate-distortion theory under the following modified rate-distortion functions:\nH\u01eb,T(\u03b8m|\u03c8) = inf \u02dc\u03b8m\u2208\u02dc\u0398\u01eb,T I(\u03b8m; \u02dc\u03b8m|\u03c8),\nwhere\nand\nwhere\nWith this notation in place, we establish the following upper and lower bounds on LM,T in terms of the above rate distortion functions. Theorem 4.3. (rate-distortion estimation error bound) For all M, T \u2208Z+, and m \u2208{1, . . . , M},\nA direct consequence of Theorem 4.3 is an upper bound on Bayesan error with respect to entropy (by setting \u01eb, \u01eb\u2032 to\n0). While the utility of such a bound is limited to settings in which \u03c8, \u03b81:M are discrete random variables, it may be useful to the reader conceptually. The bound is captured in the following Corollary: Corollary 4.4. (entropy estimation error bound) For all\nIn the following section, we will apply Theorem 4.3 to derive error bounds for a sparse mixture of (deep) transformers. For a simpler linear representation learning example, we refer the reader to Appendix B.1.\n# 4.4. Sparse Mixture of Transformers\nIn the sparse mixture of transformers environment, for all documents m, we let its tokens (X(m) 1 , X(m) 2 , . . .) be a sequence in {1, . . . , d}, where d denotes the size of the vocabulary. Each of the d outcomes is associated with a known embedding vector which we denote as \u03a6j for j \u2208{1, . . . , d}. We assume that for all j, \u2225\u03a6j\u22252 = 1. For brevity of notation, we let \u03c6(m) t = \u03a6X(m) t i.e. the em(m)\nbedding associated with token X(m) t .\nEach document is generated by a transformer model which is sampled iid from a mixture. We assume that sampling is performed according to a categorical distribution parameterized by \u03c8 with prior distribution P(\u03c8 \u2208\u00b7) = Dirichlet(N, [R/N, . . . , R/N]) for a scale parameter R \u226a N. Under this prior distribution, the expected number of unique outcomes grows linearly in R and only logarithmically in the number of draws (M in our case). As a result, we permit the size of the mixture N to potentially be exponentially large, but we assume that the mixture\u2019s complexity is controlled by the sparsity parameter R. Each of the N elements of the mixture corresponds to a deep transformer network as outlined in Section 3.3. Let K denote the context lengths of the transformers, L denote their depths, and r their attention dimensions. We assume that for all documents, the first token X(m) 1 is sampled from an arbitrary pmf on {1, . . . , d} but subsequent tokens are sampled based on the previous K tokens within the context window and the weights of the sampled transformer model. The tokens of each document are generated according to the weights of the sampled transformer and the previous K tokens. The generation of token X(m) t+1 will depend on \u03b8m and X(m) t\u2212K+1, . . . , X(m) t . For all m, t, we let (U (m) t,0 = \u03c6t\u2212K+1:t) refer to the embeddings associated with the past K tokens. For i > 0, we let U (m) t,i denote the output of layer i of the transformer with input U (m) t,0 . For all t \u2264 T, i < L, m \u2264M, let\ndenote the attention matrix of layer i for document m where \u03c3 denotes the softmax function applied elementwise along the columns. The matrix A(m) i \u2208\u211cr\u00d7r can be interpreted as the product of the key and query matrices and without loss of generality, we assume that the elements of the matrices A(m) i are distributed iid N(0, 1) (Gaussian assumption is not crucial but known mean and unit variance is).\nSubsequently, we let\n\ufffd \ufffd where Clip ensures that each column of the matrix input has L2 norm at most 1. The matrix V (m) i resembles the value matrix and without loss of generality, we assume that the elements of V (m) i are distributed iid N(0, 1/d) (same generality conditions as above). Finally, the next token is generated via sampling from the softmax of the final layer:\n\ufffd \ufffd where U (m) t,L [\u22121] denotes the right-most column of U (m) t,L . At each layer i, the parameters \u03b8m,i consist of the matrices A(m) i , V (m) i . We provide the following novel result which upper bounds the error of the optimal Bayesian learner when learning from data generated by the sparse mixture of transformers. Theorem 4.5. (mixture of transformers estimation error bound) For all d, r, K, L, M, T \u2208Z++, if \u03b81, . . . , \u03b8M, \u03c8 are the sparse mixture of transformers environment and r \u2264d, then\nU (m) t,L .\nWe now provide some qualitative comments about this result. The first and second terms denote the meta estimation error, and the third term denotes the intra-document estimation error.\nWe now provide some qualitative comments about this result. The first and second terms denote the meta estimation error, and the third term denotes the intra-document estimation error. The first term is the error incurred in the process of learning \u03c8, the probabilities by which the models of the mixture are sampled. Note that even if there are N models in the\nThe first term is the error incurred in the process of learning \u03c8, the probabilities by which the models of the mixture are sampled. Note that even if there are N models in the\nThe first term is the error incurred in the process of learning \u03c8, the probabilities by which the models of the mixture are sampled. Note that even if there are N models in the\nmixture, due to the Dirichlet assumption, the error depends linearly on R the sparsity parameter and only logarithmically on N. Note that this term decays linearly in MT since data across documents provide information about \u03c8. The second term measures the error incurred from learning the weights of the sampled models within the mixture. Note that again, due to the Dirichlet assumption, this term scales only logarithmically in M. This is because even if a model is resampled for every document, several documents may still be generated by the same model from the mixture. As a result, the dependence is linear in R and only logarithmic in M. The remaining terms are linear in the product of parameter count and depth, which corroborates the results of Bai et al. (2023). However, our result decays linearly in MT as opposed to just M as in (Bai et al., 2023). This is intuitive as the error ought to decrease in both the number of documents M and the length of the documents T . This is an advantage of the Bayesian framework as it does not rely on a uniform convergence argument which requires mixing time assumption on the tokens within the document to obtain linear decay in T .\nFinally, the third term is the intra-document estimation error which is the error incurred in the process of learning which model from the mixture generated each document. Since there are N different elements in the mixture, the log(N)/T is straightforward. The longer the document length T , the more certain we should be about which model generated the document, hence lower error. In the following section, we explicitly outline the connection between this example and ICL.\n# 4.5. In-context Learning as Meta-Learning from Sequences\n# 4.5. In-context Learning as Meta-Learning from\nWe now explicitly draw the connection between ICL and meta-learning from sequences. We assume that the pretraining dataset consists of M documents, each of length T . We assume that a new M + 1th document type is drawn and an in-context learner is described by an algorithm which produces for each t a predictive distribution P in t of X(M+1) t+1 after observing the history HM+1,t which consists of the pretraining data an the t provided in the current context. We let DM+1 = (X(M+1) 1 , . . . , X(M+1) \u03c4 ) denote the entire in-context sequence. Note that we have summarize the effect of pretraining by allowing P (in) t to depend on the pretraining history HM,T . We quantify error realized by predictions P in t in terms of the average cumulative expected log-loss:\nLM,T,\u03c4,\u03c0 = 1 \u03c4 \u03c4\u22121 \ufffd t=0 E\u03c0 \ufffd \u2212log P in t \ufffd X(M+1) t+1 \ufffd\ufffd ,\nwhere \u03c4 denotes the full length of the in-context sequence. We assume that \u03c4 \u2264T as \u03c4 can be at most K, the contextlength of the transformer and the document lengths T in pretraining are often much larger than K. As before, we establish that \u02c6Pt(X(M+1) t+1 \u2208\u00b7) = P(X(M+1) t+1 \u2208\u00b7|HM+1,t) minimizes this loss almost surely. Theorem 4.6. For all M, T, t \u2208Z+,\nGoing forward, we will restrict our attention to the performance of \u02c6Pt which we denote as:\nWith this notation in place, we present an upper bound for the ICL error. A proof can be found in Appendix B.3. Theorem 4.7. (in context learning error bound) For all M, T, \u03c4 \u2208Z++, if \u03c4 \u2264T , then\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8fb7/8fb769e4-332f-4b37-a67f-023a67b4163f.png\" style=\"width: 50%;\"></div>\nNote that if M is large i.e. the number of pretraining documents is large, then almost all of the error will be attributed to the in-context estimation error: Remark 4.8. For sufficiently large M (number of pretraining documents),\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6cdb/6cdbf9f5-4828-4dec-8c39-0740cc08966c.png\" style=\"width: 50%;\"></div>\n# 4.6. Discussion of Results\n<div style=\"text-align: center;\">4.6. Discussion of Results</div>\nIf each pretraining document is generated by a transformer model which is drawn from a mixture as in the previous section, the above remark suggests for a sufficiently large pretraining set, the in-context error can be small for even modest values of \u03c4. The in-context error is upper bounded by log(N)/\u03c4 where N is the size of the mixture. Effectively, the in-context data only needs to distinguish which model from the mixture generated the current sequence. As a result, the complexity is at most log(N) and the error decays\nlinearly in the length of the in-context sequence \u03c4. This corroborates work by Min et al. (2022) which established that an in-context sequence largely augments performance via providing information about the distributions of the inputs and labels as well as the format of the sequence. The LLMs is not literally learning from the examples, as even when the labels of examples were randomly scrambled, performance on downstream tasks was only marginally impacted. This lends credence to the hypothesis that ICL pinpoints which model from the mixture is most suitable for the given in-context sequence.\n# 5. Conclusion\nIn this work, we introduced novel information-theoretic tools to analyze the error of meta-learning from sequences. Our tools produced very general and intuitive results which suggest that the error should decay in both the number of training sequences and the sequence lengths. Notably, these results hold without relying on contrived mixing time assumptions as common in existing work. By applying these tools, we developed novel results about ICL in transformers and a plausible mathematical hypothesis for how learning is possible even when only a small amount of data is provided in-context. While the results of the main text are limited to exact Bayesian inference, we provide results in the Appendix which extend to suboptimal algorithms as well. A further rigorous investigation into the mechanisms by which transformers may be implementing a mixture of models would provide stronger credence to the hypothesis and results provided in this work.\n# References\nAhn, K., Cheng, X., Daneshmand, H., and Sra, S. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. Aky\u00fcrek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Aliakbarpour, M., Bairaktari, K., Brown, G., Smith, A., and Ullman, J. Metalearning with very few samples per task. arXiv preprint arXiv:2312.13978, 2023. Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context learning with in-context algorithm selection, 2023. Baxter, J. A model of inductive bias learning. Journal of artificial intelligence research, 12:149\u2013198, 2000. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Dai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., and Wei, F. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M. Incremental learning-to-learn with statistical guarantees. arXiv preprint arXiv:1803.08089, 2018. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Edelman, B. L., Goel, S., Kakade, S. M., and Zhang, C. Inductive biases and variable creation in self-attention mechanisms. CoRR, abs/2110.10090, 2021. URL https://arxiv.org/abs/2110.10090. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021. Finn, C., Rajeswaran, A., Kakade, S., and Levine, S. Online meta-learning. In International Conference on Machine Learning, pp. 1920\u20131930. PMLR, 2019. Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023. Huang, Y., Cheng, Y., and Liang, Y. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023. Jelassi, S., Sander, M., and Li, Y. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822\u201337836, 2022. Jeon, H. J., Zhu, Y., and Van Roy, B. An informationtheoretic framework for supervised learning, 2023. Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nKirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nS. Transformers as algorithms: Generalization and stability in in-context learning. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 19565\u201319594. PMLR, 23\u201329 Jul 2023a. URL https://proceedings.mlr.press/v202/li2 Li, Y., Li, Y., and Risteski, A. How do transformers learn topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245, 2023b. Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021. Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Mahankali, A., Hashimoto, T. B., and Ma, T. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023. Maurer, A. Transfer bounds for linear feature learning. Machine learning, 75(3):327\u2013350, 2009. Maurer, A., Pontil, M., and Romera-Paredes, B. The benefit of multitask representation learning. Journal of Machine Learning Research, 17(81):1\u201332, 2016. Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work?, 2022. Pontil, M. and Maurer, A. Excess risk bounds for multitask learning with trace norm regularization. In Conference on Learning Theory, pp. 55\u201376. PMLR, 2013. Raventos, A., Paul, M., Chen, F., and Ganguli, S. The effects of pretraining task diversity on in-context learning of ridge regression. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\nLiu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\n<div style=\"text-align: center;\">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023.</div>\nPontil, M. and Maurer, A. Excess risk bounds for multitask learning with trace norm regularization. In Conference on Learning Theory, pp. 55\u201376. PMLR, 2013.\nRaventos, A., Paul, M., Chen, F., and Ganguli, S. The effects of pretraining task diversity on in-context learning of ridge regression. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\nRaventos, A., Paul, M., Chen, F., and Ganguli, S. The effects of pretraining task diversity on in-context learning of ridge regression. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\nZhang, R., Frei, S., and Bartlett, P. L. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023a. Zhang, Y., Zhang, F., Yang, Z., and Wang, Z. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization, 2023b. Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021.\nA. Learning from Sequential Data Theorem 3.2. (Bayesian error) For all T \u2208Z+,\nTheorem 3.2. (Bayesian error) For all T \u2208Z+,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a806/a806bfef-2e39-4410-832c-630a5cbe371a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">where (a) and (b) follow from the chain rule of conditional mutual information.</div>\nsup \u01eb\u22650 min \ufffdH\u01eb,T(\u03b8) T , \u01eb \ufffd \u2264LT \u2264inf \u01eb\u22650 H\u01eb,T(\u03b8) T + \u01eb.\nProof.\n12\nSuppose that I(HT ; \u03b8) < H\u01eb,T . Let \u02dc\u03b8 = \u02dcHT /\u2208\u02dc\u0398\u01eb,T where \u02dcHT is another history sampled in the same manner as HT .\nSuppose that I(HT ; \u03b8) < H\u01eb,T . Let \u02dc\u03b8 = \u02dcHT /\u2208\u02dc\u0398\u01eb,T where \u02dcHT is another history sampled in the same manner as HT .\nwhere (a) follows from the fact that conditioning reduces entropy and that Xt+1 \u22a5\u02dcHt|(\u03b8, Ht) and (b) follows from the fact that \u02dc\u03b8 /\u2208\u02dc\u0398\u01eb,T . Therefore, for all \u01eb \u22650, I(HT ; \u03b8) \u2265min{H\u01eb,T, \u01ebT }. The result follows.\n<div style=\"text-align: center;\">where (a) follows from the fact that conditioning reduces entropy and that Xt+1 \u22a5\u02dcHt|(\u03b8, Ht) and (b) follows from th fact that \u02dc\u03b8 /\u2208\u02dc\u0398\u01eb,T . Therefore, for all \u01eb \u22650, I(HT ; \u03b8) \u2265min{H\u01eb,T, \u01ebT }. The result follows.</div>\n# A.1. Logistic Regression\nWe introduce a simple logistic regression problem as a concrete instance to demonstrate an application of the general aforementioned results. We assume that X0 = \u00afX0 and Xt = (Yt, \u00afXt) for all t \u22651. The \u201cinputs\u201d ( \u00afX0, . . . , \u00afXT ) are generated according to an iid random process for which Xj \u223cN(0, Id). Meanwhile, we assume that Yt+1 is generated by the following process:\n# Yt+1 = \ufffd 1 w.p. 1 1+e\u2212\u03b8\u22a4Xt \u22121 otherwise ,\nwhere \u03b8 denotes the parameters of the logistic model and we assume the prior distribution P(\u03b8 \u2208\u00b7) = Unif({\u03bd \u2208\u211cd :\nwhere \u03b8 denotes the parameters of the logistic model and we assume the prior distribution P(\u03b8 \u2208\u00b7) = Unif({\u03bd \u2208\u211cd : \u2225\u03bd\u22252 \u22641}). In this environment, \u03b8 is the only unknown quantity and as such, the distributions of all random variables are known to the algorithm designer. In this example, the sequence is iid once conditioned on \u03b8. We begin with this example for simplicity and to demonstrate that our analytical tools are general enough to subsume the analysis of supervised learning from iid data.\nLT \u2264 d 2T \ufffd 1 + ln \ufffd 1 + T 4d \ufffd\ufffd .\nZ \u223cN(0, 8\u01eb/d). Then,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d3c/4d3c236e-6d51-4e26-b4c2-8a579d7ecd39.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/db17/db17dbca-2dec-451b-9d22-2d305ab51626.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">where (a) follows from Lemma 3.1 and (b) follows from the fact that for all x, y \u2208\u211c,</div>\nwhere (a) follows from Lemma 3.1 and (b) follows from the fact that for all x, y \u2208\u211c,\nTherefore, \u03b8 \u2208\u0398\u01eb so it suffices to upper bound the rate I(\u03b8; \u02dc\u03b8).\nTherefore,\n\ufffd \ufffd where (a) follows from Theorem 3.4 and (b) follows by setting \u01eb = d/(2n).\nAs one would expect, the above result establishes that the Bayesian error of an optimal learning algorithm is O( d n log n d ). The proof illustrates a common technique for bounding the rate-distortion function i.e. considering a compression \u02dc\u03b8 = \u03b8+Z where Z is independent zero-mean Gaussian noise with tunable variance. In the following section, we use the same set of tools to analyze a much more complex supervised learning problem involving a sequence generated by a deep transformer model.\n# A.2. Transformers\nLemma A.2. For all L \u2208Z++ and i \u2208{1, . . ., L}, if \u03b8i \u22a5\u03b8j, \u02dc\u03b8i \u22a5\u02dc\u03b8j, and \u03b8i \u22a5\u02dc\u03b8j for i \u0338= j, then I(Xt+1; \u03b8i|\u03b8i+1:L, \u02dc\u03b81:i, Ht) \u2264I(Ht+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, X0).\nProof.\nI(Xt+1; \u03b8i|\u03b8i+1:L, \u02dc\u03b81:i, Ht) (a) = I(Ht+1, \u02dc\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i) \u2212I(Ht, \u02dc\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i)\n# I(Xt+1; \u03b8i|\u03b8i+1:L, \u02dc\u03b81:i, Ht) (a) = I(Ht+1, \u02dc\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i) \u2212I(Ht, \u02dc\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i)\n| | \u2212| (b) \u2264I(Ht+1, \u02dc\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) (c) \u2264I(Ht+1, \u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) (d) = I(Ht+1, \u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) \u2212I(\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) (e) = I(Ht+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, X0)\n(b) \u2264I(Ht+1, \u02dc\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) (c) \u2264I(Ht+1, \u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) (d) = I(Ht+1, \u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) \u2212I(\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) (e) = I(Ht+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, X0)\nwhere (a) follows from the chain rule of mutual information, (b) follows from the independence assumptions, (c) follows from the data processing inequality applied to the markov chain \u03b8i \u22a5\u02dc\u03b81:i\u22121|(Ht+1, \u03b8i+1:L, \u03b81:i\u22121, XK 0 ), (d) follows from the fact that I(\u03b81:i\u22121; \u03b8i|\u03b8i+1:L, \u02dc\u03b8i, X0) = 0, and (e) follows from the chain rule of mutual information. Lemma A.3. (transformer layer Lipschitz constant) For all d, r, K \u2208Z++,\nE \ufffd \u2225f\u03b8i(X) \u2212f\u03b8i( \u02dcX)\u22252 F |X, \u02dcX \ufffda.s. \u22642(K + K2) \u00b7 \u2225X \u2212\u02dcX\u22252 F .\nE \ufffd \u2225f\u03b8i(X) \u2212f\u03b8i( \u02dcX)\u22252 F |X, \u02dcX \ufffda.s. \u22642(K + K2) \u00b7 \u2225X\nProof. Take all equality and inequality below to hold almost surely\n\ufffd \ufffd where (a) follows from the fact that Clip is a contraction mapping, where in (b), Xk denotes the kth column of X \u2208\u211cd\u00d7K, (c) follows from the fact that softmax is 1-Lipschitz and (d) follows from the fact that for all k, \u2225\u02dcXk\u22252 2 \u22641.\nLemma A.4. For all d, r, K \u2208Z++ and \u01eb \u22650, if V \u2208\u211cd\u00d7d consists of elements distributed iid N(0, 1/d), A \u2208\u211cr\u00d7r consists of elements distributed N(0, 1), E[\u2225V \u2212\u02dcV \u22252 F] \u2264\u01eb, and E[\u2225A \u2212\u02dcA\u22252 F] \u2264\u01eb/r, then E \ufffd \u2225f\u03b8(X) \u2212f\u02dc\u03b8(X)\u22252 F \ufffd \u22642K2\u01eb (1 + Kd) ,\nwhere \u03b8 = (V, A), \u02dc\u03b8 = ( \u02dcV , \u02dcA).\nProof.\nwhere (a) follows from the fact that \u2225a + b\u22252 F \u22642\u2225a\u22252 F + 2\u2225b\u22252 F for all matrices a, b, (b) follows from the fact that \u2225ab\u22252 F \u2264\u2225a\u22252 \u03c3\u2225b\u22252 F and \u2225a\u22252 \u03c3 \u2264\u2225a\u22252 F for all matrices a, b, (c) follows from the fact that E \ufffd \u2225V \u2212\u02dcV \u22252 F \ufffd = \u01eb, (d) follows from the fact that E[\u2225V \u22252 F ] = d, and the fact that softmax is 1-Lipschitz, and where in (e), xi denotes the ith column of matrix x.\nLemma A.5. (sequence transformer distortion bound) For all d, r, t, K, L \u2208Z++, 0 \u2264\u01eb \u22642d, and i \u2264L, if \u02dc\u03b8i = ( \u02dcVi, \u02dcAi) for which \u02dcVi = Vi + ZV i , \u02dcAi = Ai + ZA i , (Vi, Ai) \u22a5(ZV i , ZA i ), ZV i consists of elements distributed iid N(0, \u01eb/d2), and ZA i consists of elements distributed iid N(0, \u01eb/r), then\nI(Xt+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, Ht) \u2264\u01ebKd \ufffd 2K + 2K2\ufffdL\u2212i+1 \nProof.\nI(Xt+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, Ht) = E \ufffd dKL \ufffd P(Xt+1 \u2208\u00b7|\u03b81:L, Ht) \u2225P(Xt+1 \u2208\u00b7|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, Ht) \ufffd\ufffd\n\ufffd \ufffd where (a) follows from Lemma 3.1, (b) follows from Lemma B.2, where in (c), Ut,i = f\u03b81:i(Ht), (d) follows from Lemma A.3, and (e) follows from Lemma A.4. Lemma A.6. (sequence transformer distortion bound) For all d, r, K, L \u2208Z++, 0 \u2264\u01eb \u22642d and i \u2264L, if \u02dc\u03b8i = ( \u02dcVi, \u02dcAi) for which \u02dcVi = Vi + ZV i , \u02dcAi = Ai + ZA i , (Vi, Ai) \u22a5(ZV i , ZA i ), ZV i consists of elements distributed iid N(0, \u01eb/d2), and ZA i consists of elements distributed iid N(0, \u01eb/r), then I(Ht+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i) \u2264\u01ebK(t + 1)d \ufffd 2K + 2K2\ufffdL\u2212i+1 .\nProof.\nwhere (a) follows from Lemma A.5.\nLemma A.7. For all d, r, t, K, L \u2208Z++, if for all i \u2264L, \u02dc\u03b8i = ( \u02dcVi, \u02dcAi) for which \u02dcVi = Vi + ZV i , \u02dcAi = Ai + ZA i , (Vi, Ai) \u22a5(ZV i , ZA i ), ZV i consists of elements distributed iid N(0, \u01eb/d2), \u02dcAi = Ai + ZA i , (Vi, Ai) \u22a5(ZV i , ZA i ), ZV i consists of elements distributed iid N(0, \u01eb/d2), and ZA i consists of elements distributed iid N(0, \u01eb/r), then I(Xt+1; \u03b81:L|\u02dc\u03b81:L, Ht) \u2264\u01ebKL(t + 1)d \ufffd 2K + 2K2\ufffdL .\nProof.\n(Xt+1; \u03b81:L|\u02dc\u03b81:L, Ht) = L \ufffd i=1 I(Xt+1; \u03b8i|\u02dc\u03b81:L, \u03b8i+1:LHt) (a) \u2264 L \ufffd i=1 I(Ht+1; \u03b8i|\u03b8i+1:L, \u03b81:i\u22121, \u02dc\u03b8i, X0) (b) \u2264\u01ebKL(t + 1)d \ufffd 2K + 2K2\ufffdL ,\nre (a) follows from Lemma A.2, and (b) follows from Lemma A.6.\nTheorem 3.5. (transformer estimation error bound) For all d, r, L, K, if \u03b81:L is the transformer environment, then\nProof. Let \u01eb = \u01eb\u2032 dKLT (2K+2K2)L .\nI(\u03b81:L; \u02dc\u03b81:L) = h(\u02dc\u03b81:L) \u2212h(\u02dc\u03b81:L|\u03b81:L)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bde2/bde2c48f-f357-455a-82c8-c559b7753271.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">where (a) holds for \u01eb\u2032 < d2KLT (2K + 2K2)L. Setting \u01eb\u2032 = (d2 + r2)L2 log(2K + 2K2)/2T gives the result.</div>\nB. Meta-Learning from Sequential Data Lemma 4.1. (Bayesian posterior is optimal) For all m, t \u2208Z+,\nProof. In the below proof take all equality to hold almost surely.\n\ufffd\ufffd sult follows from the fact that dKL( \u02c6Pm,t\u2225Pm,t) > 0 for all Pm,t \u0338= \u02c6Pm,t\norem 4.2. (Main Result) For all M, T \u2208Z+ and m \u2208{1, 2, . . ., M},\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c438/c4386c82-d75d-42e2-b262-e4dbf18bb641.png\" style=\"width: 50%;\"></div>\nheorem 4.3. (rate-distortion estimation error bound) For all M, T \u2208Z+, and m \u2208{1, . . ., M},\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b25d/b25ddf1d-4b11-4224-aa29-395ff9688d3e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">19</div>\nProof. We begin by showing the upper bound:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be48/be488393-18e3-4a8d-9316-4e19a3ff1994.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e (a) and (b) follow from the data processing inequality and (c) follows from the definition of the rate-distortion tions. The upper bound follows from the fact that inequality (c) holds for all \u01eb \u22650.</div>\n<div style=\"text-align: center;\">where (a) and (b) follow from the data processing inequality and (c) follows from the definition of the rate-distortion functions. The upper bound follows from the fact that inequality (c) holds for all \u01eb \u22650. We now prove the lower bound. Suppose that I(HM,T ; \u03c8) < H\u01eb,M,T(\u03c8) Let \u02dc\u03c8 = \u02dcHM,T /\u2208\u02dc\u03a8\u01eb,M,T where \u02dcHM,T is another history sampled in the same manner as HM,T .</div>\nfunctions. The upper bound follows from the fact that inequality (c) holds for all \u01eb \u22650. We now prove the lower bound. Suppose that I(HM,T ; \u03c8) < H\u01eb,M,T(\u03c8) Let \u02dc\u03c8 = \u02dcHM,T /\u2208\u02dc\u03a8\u01eb,M,T where \u02dcHM,T is anothe history sampled in the same manner as HM,T .\nwhere (a) follows from the fact that conditioning reduces entropy and that X(m) t+1 \u22a5\u02dcHM,T |(\u03c8, Hm,t) and (b) follows from the fact that \u02dc\u03c8 /\u2208\u02dc\u03a8\u01eb,M,T. Therefore, for all \u01eb \u22650, I(HM,T ; \u03c8) \u2265min{H\u01eb,M,T(\u03c8), \u01ebMT }. Suppose that I(H(m) T ; \u03b8m|\u03c8) < H\u01eb,T(\u03b8m|\u03c8). Let \u02dc\u03b8m = \u02dcDm /\u2208\u02dc\u0398\u01eb,T where \u02dcDm is another history sampled in the same manner as Dm.\nwhere (a) follows from the fact that conditioning reduces entropy and that X(m) t+1 \u22a5\u02dcDm|(\u03c8, X(m) 1 , . . . , X(m) t ) and (b) follows from the fact that \u02dc\u03b8m /\u2208\u02dc\u0398\u01eb,T . Therefore, for all \u01eb \u22650, I(Dm; \u03b8m|\u03c8) \u2265min{H\u01eb,M,T(\u03b8m), \u01ebT }. The lower bound follows as a result.\n# B.1. Linear Representation Learning Example\nWe introduce a simple linear representation learning problem as a concrete example of meta-learning to demonstrate our method of analysis. Just as in the logistic regression example, the documents in this example consist of iid data but we begin with such an example for simplicity and to demonstrate this as a special case of meta-learning from sequences under\nWe introduce a simple linear representation learning problem as a concrete example of meta-learning to demonstrate our method of analysis. Just as in the logistic regression example, the documents in this example consist of iid data but we begin with such an example for simplicity and to demonstrate this as a special case of meta-learning from sequences under our framework. For all d, r \u2208Z++, we let \u03c8 : \u2126\ufffd\u2192\u211cd\u00d7r be distributed uniformly over the set of d\u00d7r matrices with orthonormal columns. We assume that d \u226br. For all i, let \u03bei : \u2126\ufffd\u2192\u211cr be distributed iid N(0, Ir/r). We let \u03b8i = \u03c8\u03bei and hence \u03c8 induces a distribution on \u03b8i. As for the observable data, for each (i, j), let X(i) j = \u2205and Y (i) j+1 be drawn as according to the following probability law:\nY (i) j+1 = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 w.p. \u03c3(\u03b8i)1 2 w.p. \u03c3(\u03b8i)2 . . . d w.p. \u03c3(\u03b8i)d ,\nwhere \u03c3(\u03b8i)j = e\u03b8i,j/ \ufffdd k=1 e\u03b8i,k denotes softmax. Note that in this problem, the input X does not influence the output Y . For each task i, the algorithm is tasked with estimating a vector \u03b8i from noisy observations (Y (i) 1 , . . . , Y (i) n ). By reasoning about data from previous tasks, the algorithm can estimate \u03c8 which reduces the burden of estimating \u03b8i to just estimating \u03bei for each task. This is significant given the assumption that d \u226br. We now present the theoretical result. Theorem B.1. (linear representation learning Bayesian error bound) For all d, r, M, T \u2208Z++,\nwhere \u03c3(\u03b8i)j = e\u03b8i,j/ \ufffdd k=1 e\u03b8i,k denotes softmax. Note that in this problem, the input X does not influence the output Y . For each task i, the algorithm is tasked with estimating a vector \u03b8i from noisy observations (Y (i) 1 , . . . , Y (i) n ). By reasoning about data from previous tasks, the algorithm can estimate \u03c8 which reduces the burden of estimating \u03b8i to just estimating \u03bei for each task. This is significant given the assumption that d \u226br. We now present the theoretical result.\nLM,T \u2264dr \ufffd 1 + log \ufffd 1 + M r \ufffd\ufffd 2MT + r \ufffd 1 + log(1 + 2n r ) \ufffd 2T .\nThe first term indicates the standard irreducible error. The second term indicates the statistical error incurred in the process of estimating \u03c8. Since \u03c8 \u2208\u211cd\u00d7r and there are m \u00d7 n data points in total which contain information about \u03c8. The final term represents statistical error incurred in the process of estimating \u03be1, . . . , \u03bem. Since each \u03bei \u2208\u211cr and there are n data points which contain information about each \u03bei the \u02dcO(r/n) follows standard statistical intuition. We note that this tightens a result shown in (Tripuraneni et al., 2021) which studies an almost identical problem. Their proposed upper bound is \u02dcO( dr2 MT + r T ) which contains an extra factor of r in the meta-estimation error. In the following, we will provide a result which requires a change of measure. For all random variables X : \u2126\ufffd\u2192X, Y : \u2126\ufffd\u2192Y and realizations y \u2208Y, one may consider the distribution P(X \u2208\u00b7|Y = y). Let function f(y) = P(X \u2208\u00b7|Y = y). Then, for any random variable Z : \u2126\ufffd\u2192Z for which Z \u2286Y, we use P(X \u2208\u00b7|Y \u2190Z) to denote f(Z). Lemma B.2. (sq error upper bounds softmax KL-divergence) For all d \u2208Z++ and random vectors \u03b8, \u02dc\u03b8 \u2208\u211cd,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f591/f59161c1-188c-450b-bef4-79d7c061fb74.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f1f/1f1fe0d4-df82-43fc-8fe1-586b15861d3f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">21</div>\nProof.\nwhere (a) follows from the log-sum inequality and (b) follows from the fact that the softmax function is 1-Lipschitz.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/27ea/27ea0fe2-4e50-4add-9f0e-554798138b40.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/36b8/36b85e45-388c-48e3-93c2-b0bb6cc4b24f.png\" style=\"width: 50%;\"></div>\nProof. Let \u02dc\u03c8 = \u03c8 + Z where Z \u2208\u211cd\u00d7k is Z \u22a5\u03c8 and consists of elements which are distributed iid N(0, \u01eb\n\u211cd\u00d7k is Z \u22a5\u03c8 and consists of elements which are distributed iid N(0, \u01eb\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0091/0091924e-00e2-47a0-ae35-ddfe7b5649ae.png\" style=\"width: 50%;\"></div>\nwhere (a) follows from the fact that Hm,n \u22a5\u02dc\u03c8|\u03c8, (b) follows from the chain rule of mutual information, (c) follows from the chain rule of mutual information and the fact that H(i) n are iid |\u03c8, (d) follows from the fact that conditioning reduces differential entropy, and (e)/(f) both follow from the data processing inequality applied to the markov chains \u02dc\u03c8 \u22a5Hm,n|\u03c8 and \u03c8 \u22a5H(1) n |\u03b81, \u02dc\u03c8. We now bound the two above terms.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7dfc/7dfc2177-9798-4278-812f-a5c39f347b72.png\" style=\"width: 50%;\"></div>\nwhere (a) follows from the maximum differential entropy of a random variable of fixed variance being upper bounded by a Gaussian random variable.\nLet \u03b8\u03b4 = \u03b81 + \u03b4Z where Z \u223cN(0, Id) and Z \u22a5\u03b81.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f8ec/f8ec2996-b2b2-4a67-ae27-417e968c0b7b.png\" style=\"width: 50%;\"></div>\nwhere (a), (b) follows from continuity of the KL-divergence between two multivariate normal distributions w.r.t the covarince matrix, (c) follows from the fact that the trace term is upper bounded by d, (d) follows from the matrix determinant emma, \u01eb = 1 m, and (e) follows from Jensen\u2019s inequality.\nLemma B.4. (distortion upper bound) For all n, r \u2208Z++,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d75c/d75c9bd8-5d68-4f4b-98ae-7f8411a61ebf.png\" style=\"width: 50%;\"></div>\nProof. Let \u02dc\u03be = \u03be + Z where Z \u22a5\u03be and Z \u223cN(0, \u01ebIr).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e16/7e16eb02-4441-466e-9af1-20ed5d9fda27.png\" style=\"width: 50%;\"></div>\nwhere (a) follows from the fact that H1,n \u22a5\u02dc\u03be|\u03c8, \u03b81, (b) follows from the chain rule of mutual information, (c) follows from the fact that (X(1) j , Y (1) j ) is iid |\u03b81, (d) follows from the fact that conditioning reduces differential entropy, and (e follows from the data processing inequality applied to the markov chain H1,n \u22a5\u02dc\u03be|(\u03be, \u03c8). We now upper bound the two above terms.\nLet \u02dc\u03b8 = \u03c8\u02dc\u03be. Then,\nI(Y (1) j ; \u03b81|\u02dc\u03be, \u03c8) \u2264I(Y (1) j ; \u03b81|\u02dc\u03b8)\n25\nwhere (a) follows from Lemma 3.1, and (b) follows from Lemma B.2. Theorem B.1. (linear representation learning Bayesian error bound) For all d, r, M, T \u2208Z++,\nTheorem B.1. (linear representation learning Bayesian error bound) For all d, r, M, T \u2208Z++,\nTheorem B.1. (linear representation learning Bayesian error bound) For all d, r, M, T \u2208Z++,\nwhere (a) follows directly from Lemmas B.3 and B.4, and (b) follows from setting \u01eb = 1 m and \u01eb\u2032 = 1 2n. We choose these values because they are analytically simpler than the optimal values of \u01eb, \u01eb\u2032 but are asymptotically identical to these optimal values.\n# B.2. Mixture of Transformer\n# mma B.5. (sparse mixture meta-estimation error) For all R, M, T \u2208Z++,\n# Lemma B.5. (sparse mixture meta-estimation error) For all R, M, T \u2208Z++,\nLemma B.5. (sparse mixture meta-estimation error) For all R, M, T \u2208Z++,\nI(HM,T ; \u03c8) \u2264R log \ufffd 1 + M R \ufffd log(MN).\nProof. Recall that \u03b81:M is distributed Dirichlet-Multinomial(M, [R/N, . . . , R/N]). Consider the following prefix-free coding scheme for \u03b81:M: For every nonzero category, allocate log(M) bits to designate the number of times that category was selected in \u03b81:M with and an additional log(N) bits to designate the category (1, . . . , N). We concatenate the bit strings for each such nonzero category. As a result:\nwhere (a) follows from the data processing inequality, (b) follows from the fact that entropy is the minimum av erage prefix-free code length, and (c) follows from the fact that the average number of non-zero outomes for  Dirichlet-Multinomial(M, [R/N, . . . , R/N]) random variable is upper bounded by R log(1 + M/R).\nwhere (a) follows from the data processing inequality, (b) follows from the fact that entropy is the minimum average prefix-free code length, and (c) follows from the fact that the average number of non-zero outomes for a Dirichlet-Multinomial(M, [R/N, . . . , R/N]) random variable is upper bounded by R log(1 + M/R). Theorem 4.5. (mixture of transformers estimation error bound) For all d, r, K, L, M, T \u2208Z++, if \u03b81, . . . , \u03b8M, \u03c8 are the sparse mixture of transformers environment and r \u2264d, then\n\nProof. Let \u02dc\u0398N = {\u03b8 + Z\u03b8 : \u03b8 \u2208\u0398}. \u0398 is the set of N transformer model weights for each of the N models in the mixture and Z\u03b8 \u22a5\u03b8 is random noise of the following characteristic: \u03b8 = (A1:L, V1:L), \u02dc\u03b8 = ( \u02dcA1:L, \u02dcV1:L), Z\u03b8 = (Z\u03b8,A 1:L , Z\u03b8,V 1:L ), for all i, \u02dcAi = Ai + Z\u03b8,A i , \u02dcVi = Vi + Z\u03b8,V i where Z\u03b8,A i consists of elements drawn iid N(0, 2\u01ebT r(d2+r2)L2 log(4K2)) and Z\u03b8,A i consists of elements drawn iid N(0, 2\u01ebT d2(d2+r2)L2 log(4K2)). \u02dc\u0398N hence is a collection of lossy compressions of the models in the mixture. Let \u02dcB \u2208{1, . . . , N}M be the collection containing the outcomes which model from the mixture was ascribed to \u03b81, . . . , \u03b8M. Since there are N different transformers in the mixture, \u02dcB takes values in the set {1, . . ., N}M.\nProof. Let \u02dc\u0398N = {\u03b8 + Z\u03b8 : \u03b8 \u2208\u0398}. \u0398 is the set of N transformer model weights for each of the N models in the mixture and Z\u03b8 \u22a5\u03b8 is random noise of the following characteristic: \u03b8 = (A1:L, V1:L), \u02dc\u03b8 = ( \u02dcA1:L, \u02dcV1:L), Z\u03b8 = (Z\u03b8,A 1:L , Z\u03b8,V 1:L ), for all i, \u02dcAi = Ai + Z\u03b8,A i , \u02dcVi = Vi + Z\u03b8,V i where Z\u03b8,A i consists of elements drawn iid N(0, 2\u01ebT r(d2+r2)L2 log(4K2)) and Z\u03b8,A i consists of elements drawn iid N(0, 2\u01ebT d2(d2+r2)L2 log(4K2)). \u02dc\u0398N hence is a collection of lossy compressions of the models in the mixture. Let \u02dcB \u2208{1, . . . , N}M be the collection containing the outcomes which model from the mixture was ascribed to \u03b81, . . . , \u03b8M. Since there are N different transformers in the mixture, \u02dcB takes values in the set {1, . . ., N}M.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/458d/458df941-7a23-4e92-b9ae-26d8ee546972.png\" style=\"width: 50%;\"></div>\n# B.3. In-context Learning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4303/43033f0c-44c3-4e1e-a3be-9afba2ff96d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">27</div>\nProof.\nwhere (a) and (b) follow from the chain rule of mutual information, (c) follows from the fact that \u03c8 \u22a5HM+1,\u03c4|HM+1,T for \u03c4 \u2264T and the data processing inequality, and (d) follows from the fact that for all m, I(Hm+1,T ; \u03c8|Hm,T ) \u2264 I(Hm,T ; \u03c8|Hm\u22121,T) and the chain rule of mutual information.\n# C. Analysis of Suboptimal Meta-Learning Algorithms\nAll of the prior results bound the error incurred by the optimal algorithm which produces a prediction of the next token conditioned on the entire past sequence. In this section, we will derive some simple results which pertain to suboptimal algorithms. The following result quantifies the shortfall incurred by an algorithm which produces an arbitrary prediction \u02dcPm,t which may depend on the history Hm,t. Lemma C.1. (loss of an arbitrary predictor) For all M, T \u2208Z++, if for all (m, t) \u2208[M] \u00d7 [T ], \u02dcPm,t is a predictive distribution which may depend on the previous data Hm,t and \u02dcLM,T denotes its cumulative average log-loss, then\nAll of the prior results bound the error incurred by the optimal algorithm which produces a prediction of the next token conditioned on the entire past sequence. In this section, we will derive some simple results which pertain to suboptimal\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a8d/2a8dc901-ed28-4c77-8d98-4e50ec3720a3.png\" style=\"width: 50%;\"></div>\nNote that because KL divergence is always non-negative and Lm,n is the loss of the Bayesian posterior estimator \u02c6P, any prediction other than \u02c6P will incur nonzero misspecification error. For a particular class of predictors \u02dcP, we can retrieve the following upper bound on the misspecification error. We consider predictors which perform Bayesian inference with respect to an incorrectly specified prior distribution \u02dcP0. Theorem C.2. (misspecified prior error bound) For all M, T \u2208Z++ and m, t \u2208[M] \u00d7 [T ], if \u02dcPm,t is the Bayesian posterior under the prior \u02dcP0(\u03c8), then\n<div style=\"text-align: center;\">Note that because KL divergence is always non-negative and Lm,n is the loss of the Bayesian posterior estimator \u02c6P, any prediction other than \u02c6P will incur nonzero misspecification error. For a particular class of predictors \u02dcP, we can retrieve the following upper bound on the misspecification error. We consider predictors which perform Bayesian inference with respect to an incorrectly specified prior distribution \u02dcP0. Theorem C.2. (misspecified prior error bound) For all M, T \u2208Z++ and m, t \u2208[M] \u00d7 [T ], if \u02dcPm,t is the Bayesian posterior under the prior \u02dcP0(\u03c8), then</div>\nProof.\nwhere (a) and (b) follow from the chain rule of KL divergence and (c) follows from the data processing inequality of KL Divergence. Theorem C.2 suggests that so long as the KL divergence between prior distributions is finite, the misspecification error should decrease to 0 as M and T \u2192\u221e. This can be ensured so long as the algorithm\u2019s prior \u02dcP0(\u03c8) does not assign 0 probability mass to any set for which the environment prior P(\u03c8) assigns non-zero probability. With these results in place, we provide the following Corollary which exactly characterizes the loss of a predictor \u02dcP which produces predictions via Bayesian inference with respect to a arbitrary prior distribution \u02dcP0(\u03c8 \u2208\u00b7). Corollary C.3. For all M, T \u2208Z++ and m, t \u2208[M] \u00d7 [T ], if \u02dcPm,t computes probabilities under an arbitrary prior distribution \u02dcP0(\u03c8 \u2208\u00b7) and \u02dcLM,T denotes its cumulative average log-loss\u201e then\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the theoretical understanding of in-context learning (ICL) in large language models (LLMs), which has garnered significant attention due to its puzzling nature of learning from limited context data. Previous theoretical frameworks have often relied on contrived assumptions, prompting the need for more robust analytical tools.",
        "problem": {
            "definition": "The central problem is to understand the error dynamics in meta-learning from sequential data, specifically how error decays with the number of training sequences and sequence lengths in the context of ICL.",
            "key obstacle": "Existing theoretical results often depend on unrealistic mixing time assumptions or fail to adequately explain ICL's effectiveness with short sequences."
        },
        "idea": {
            "intuition": "The idea emerged from the observation that ICL can be viewed as a form of Bayesian inference, which allows for more coherent error analysis.",
            "opinion": "The authors propose a new perspective that utilizes information-theoretic tools to analyze error in meta-learning frameworks.",
            "innovation": "The primary innovation is the introduction of a decomposition of error into irreducible error, meta-learning error, and intra-task error, allowing for a unified analysis across various meta-learning challenges."
        },
        "Theory": {
            "perspective": "The theoretical framework is grounded in Bayesian inference, treating the learning process as one of reducing uncertainty about underlying parameters.",
            "opinion": "The authors assume that the optimal learning algorithm can be characterized by predictive distributions conditioned on historical data.",
            "proof": "The paper derives new results showing that error can be bounded in terms of the number of training sequences and their lengths without relying on stability or mixing assumptions."
        },
        "experiments": {
            "evaluation setting": "The evaluation is conducted using datasets that simulate learning from sequential data, including examples from linear representation learning and transformer models.",
            "evaluation method": "The authors apply their theoretical tools to derive error bounds and validate their framework against existing results."
        },
        "conclusion": "The study concludes that the proposed information-theoretic tools yield general results that effectively characterize error decay in ICL, providing insights into how LLMs can learn from limited data.",
        "discussion": {
            "advantage": "The main advantage of this approach is its ability to unify various theoretical analyses of meta-learning without relying on unrealistic assumptions.",
            "limitation": "A limitation noted is that the results are primarily focused on exact Bayesian inference, which may not cover all practical scenarios.",
            "future work": "Future research could explore the mechanisms by which transformers implement a mixture of models and further investigate the implications of suboptimal algorithms."
        },
        "other info": [
            {
                "info1": "The paper includes discussions on related works, particularly those addressing ICL and transformer architectures."
            },
            {
                "info2": {
                    "info2.1": "Appendices provide additional examples and proofs to support the main results.",
                    "info2.2": "Theoretical tools introduced could have broader applications beyond ICL."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the theoretical understanding of in-context learning (ICL) in large language models (LLMs), emphasizing its puzzling nature of learning from limited context data."
        },
        {
            "section number": "1.2",
            "key information": "In-context learning has garnered significant attention due to its potential to improve performance in natural language processing tasks."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical framework is grounded in Bayesian inference, treating the learning process as one of reducing uncertainty about underlying parameters."
        },
        {
            "section number": "3.3",
            "key information": "The primary innovation is the introduction of a decomposition of error into irreducible error, meta-learning error, and intra-task error, allowing for a unified analysis across various meta-learning challenges."
        },
        {
            "section number": "6",
            "key information": "A limitation noted is that the results are primarily focused on exact Bayesian inference, which may not cover all practical scenarios."
        },
        {
            "section number": "6.1",
            "key information": "Existing theoretical results often depend on unrealistic mixing time assumptions or fail to adequately explain ICL's effectiveness with short sequences."
        },
        {
            "section number": "7",
            "key information": "The study concludes that the proposed information-theoretic tools yield general results that effectively characterize error decay in ICL, providing insights into how LLMs can learn from limited data."
        }
    ],
    "similarity_score": 0.7011667547303998,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/An Information-Theoretic Analysis of In-Context Learning.json"
}