{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2306.05212",
    "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
    "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrievalaugmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a RETreivalAugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETALLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including request rewriting, document retrieval, passage extraction, answer generation, and fact checking modules. Our toolkit is publicly available at https://github.com/ RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
    "bib_name": "liu2023retallmretrievalaugmentedlargelanguage",
    "md_text": "Jiongnan Liu1, Jiajie Jin2, Zihan Wang1, Jiehan Cheng1, Zhicheng Dou1\u2217, and Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China 2University of Science and Technology of China 1{liujn, wangzihan0527, jiehan_cheng, dou, jrwen}@ruc.edu.cn 2jinjiajie@mail.ustc.edu.cn\n# Abstract\nAlthough Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrievalaugmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a RETreivalAugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETALLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including request rewriting, document retrieval, passage extraction, answer generation, and fact checking modules. Our toolkit is publicly available at https://github.com/ RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.\narXiv:2306.05212v1\n# Introduction\nLarge language models (LLMs) have attracted increasing attention from both research community and industry (Brown et al., 2020; OpenAI, 2023; Ouyang et al., 2022; Touvron et al., 2023; Chowdhery et al., 2022; Zhao et al., 2023; Zeng et al., 2022). With tremendous world knowledge stored in parameters (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020) and the Reinforcement Learning\nfrom Human Feedback (RLHF) techniques (Christiano et al., 2017; Ziegler et al., 2019), LLMs can generate helpful, detailed, and polite texts in response to user inputs. Many studies have demonstrated LLMs\u2019 extraordinary abilities in various areas, including nature language processing (Moslem et al., 2023), information retrieval (Sun et al., 2023; Wang et al., 2023; Mao et al., 2023), and recommendation (Hou et al., 2023; Zhang et al., 2023). However, LLMs still tend to hallucinate and sometimes generate texts opposite to facts (Zhou et al., 2021; Zhao et al., 2023). To tackle these problems, researchers have proposed a new paradigm to strengthen LLMs with information retrieval systems (retrieval-augmented LLMs) (Shi et al., 2023; Jiang et al., 2023; Nakano et al., 2022), which enables LLMs to retrieve relevant contents from an external repository (knowledge corpus) to generate texts based on them. It has been verified that retrieval-augmented LLMs can generate texts in response to user input with fewer hallucinations (Nakano et al., 2022). Furthermore, by incorporating customized private data resources, retrieval-augmented LLMs can respond to in-domain queries that cannot be answered by LLMs trained with public data. To support research in this area and help users build their own in-domain LLM-based systems, we devise RETA-LLM, a RETreival-Augmented LLM toolkit. Different from previous general LLMenhanced toolkits such as LangChain,1 RETALLM focuses on the retrieval-augmented LLMs and provides more plug-in modules. Typically, retrieval-augmented LLMs use a retrieve-andgenerate strategy with two modules: First, they retrieve documents or passages based on user request (document retrieval module); then, they generate answers utilizing these relevant documents as references (answer generation module). In addi-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/378c/378c86e9-0ffb-4a2d-86a6-19fa8d1e3043.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">igure 1: The RETA-LLM framework. Examples are taken from an intelligent university information seekin ystem powered by RETA-LLM.</div>\ntion to these two basic modules, our RETA-LLM provides three optional modules: (1) a request rewriting module to make user\u2019s current request more complete and clear; (2) a passage extraction module to extract relevant passages or fragments from the whole retrieved document contents; and (3) a fact checking module to verify whether there exist factual errors in the generated answers. These optional modules can make the interaction between IR systems and LLMs more effective and smooth. The disentanglement between LLMs and IR systems in our RETA-LLM is more thorough, which makes the customization of search engines and LLMs more convenient. Furthermore, to make the usage easier, we provide a complete and ready-touse pipeline for researchers and users to build their RETA-LLM toolkits based on their own repository for in-domain LLM-based systems from scratch.\nRETA-LLM is part of YuLan, a open source LLM initiative proposed by Gaoling School of Artificial Intelligence, Renmin University of China. RETA-LLM is still under development and there are many issues that need to be solved with great efforts. We sincerely welcome contributions on this open source toolkit.\n# 2 RETA-LLM Framework\nAs aforementioned, compared with Langchain, which is a common LLM-augmented toolkit, our RETA-LLM toolkit focuses specifically on retrieval-augmented LLMs. We provide five plugin modules in RETA-LLM to interact with LLMs and IR systems. The modules include request rewriting, document retrieval, passage extraction, answer generation, and fact checking modules. The framework of our RETA-LLM is shown in Figure 1. The workflow of RETA-LLM is as follows: First, RETA-LLM uses the request rewriting module to revise the current user request to make it complete and clear. Because users can issue a series of questions to the RETA-LLM, the semantics of the current user request may be incomplete. For example, A user may ask \u201cHow about the School of Economics?\u201d while the historical request is \u201cIntroduce the majors in School of Information\u201d. In this case, the precise meaning of the user is \u201cIntroduce the majors in School of Economics\u201d. Since LLMs have shown remarkable abilities in rewriting queries in conversational dense retrieval (Mao et al., 2023), we feed the current user request and the previous conversation histories to LLMs to perform rewriting.\nThen, RETA-LLM uses the document retrieval module to retrieve relevant documents from the external corpus based on the revised user request. The document retrieval module is the module connected to the IR system. It retrieves relevant documents from the external knowledge corpus and returns top-K of them. The K is set to 3 in our default configuration. We provide a default dense retriever in our repository. The detailed description can be found in the next section. Next, RETA-LLM uses the passage extraction module to extract fragments related to the user request from the retrieved documents to form the references. Because of the input length limitations (typically 2048 or 4096 tokens) of LLMs, it is impossible to directly concatenate the contents of all top-K relevant document contents as references for them to generate answers. Trivial methods by truncating the document contents may lose important information in them. Therefore, we reuse the LLMs themselves to extract related fragments from retrieved documents based on the revised request. Since the length of one document may also exceed the limitations, we apply the sliding window strategy to extract fragments step by step. The sliding window size and step are set to 512 and 256 in our default configuration. These fragments are then concatenated together as the references. Besides, RETA-LLM uses the answer generation module to generate answers for the user request. As previous researches (Nakano et al., 2022; Shi et al., 2023; Jiang et al., 2023) suggest, by feeding the references retrieved from the external corpus, LLMs can generate more factual answers. Finally, RETA-LLM uses the fact checking module to verify whether the generated answers contain factual mistakes and output final responses for the user request. Though providing additional evidence for generation, LLMs may also hallucinate (Nakano et al., 2022). It is necessary to devise a module to conduct further fact verification. Because of the strong natural language understanding abilities of LLMs, we feed the references and generated answers to them to make judgments. Therefore, RETA-LLM can decide whether to output the generated answers or just say \u201cI cannot answer this question\u201d. Noticed that all the inputs to the LLMs are wrapped in instructions or prompts. As shown in Figure 1, we disentangle the IR systems and LLMs entirely in our RETA-LLM. This separate design\nin our RETA-LLM leads users can customize their personal search engines and LLMs.\n# 3 RETA-LLM Usage Pipeline\nTo make the toolkit more convenient for personal usage, we provide a complete pipeline to build in-domain LLM-based system based on html resources. The pipeline is as follows: First, RETA-LLM uses Beautiful Soup package to convert the raw html files into json data in our HTML Converter.2 Second, RETA-LLM follows the implementation of disentangled-retriever (Zhan et al., 2022) to build dense indexes and to conduct domain adaption from the converted json data in our Index Builder.3 Specifically, our method supports unsupervised training of dense retrieval models on local document collections, enabling the model to learn domain-specific knowledge in advance. Compared with the retrieval module in the popular LangChain library, our retrieval method has two advantages: (1) the model learns knowledge within the domain of local documents, enabling it to match queries more accurately, and (2) our method does not segment text, thus avoiding any negative impact on the overall semantic information of the text. We also provide a sparse retriever applying faiss (Johnson et al., 2019) package to build sparse indexes.4 Otherwise, users can also use their customized search engines as the document retrieval module. Third, users need to prepare LLMs for question answering. For LLM loading and responding, we provide the template for Alpaca (Taori et al., 2023),5, YuLan-Chat,6 ChatGLM (Zeng et al., 2022; Du et al., 2022),7 and GPT-3.5 API (Ouyang et al., 2022).8 If users use other LLMs, they can edit the codes and configurations in our toolkit. Finally, users can start their own RETA-LLM services using streamlit package.9 2Beautiful Soup, https://beautiful-soup-4. readthedocs.io/en/latest/ 3disentagled-retriever, https://github.com/ jingtaozhan/disentangled-retriever 4Faiss, https://github.com/facebookresearch/ faiss 5Alpaca,https://github.com/tatsu-lab/stanford_ alpaca 6YuLan-Chat, https://github.com/RUC-GSAI/ YuLan-Chat 7ChatGLM, https://github.com/THUDM/ChatGLM-6B 8OpenAI\u2019s API, https://api.openai.com/v1/ completions 9streamlit, https://github.com/streamlit/ streamlit\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d3b6/d3b65ae3-c944-4b53-80c2-728f715066d7.png\" style=\"width: 50%;\"></div>\nMore details about the usage pipeline can be found on our GitHub repository.\n# 4 A RETA-LLM Service Case\nBased on the RETA-LLM and the usage pipeline, we use the web pages on Renmin University of China\u2019s enrollment online platform, 10 to build an RUC-enrollment-assistant system. The system uses a dense document retrieval module and adopts YuLan-13B as the backbone LLM. A using case is shown in 2. By enhancing the IR systems, LLMs can answer in-domain questions which cannot be answered by their own knowledge.\n# 5 Conclusion and Future Work\nIn this paper, we propose RETA-LLM to facilitate research and development of retrieval-augmented LLMs. We provide five independent modules: request rewriting, document retrieval, passage extraction, answer generation, and fact checking modules in our toolkit. Furthermore, we provide a pipeline to help users build their in-domain LLM-based systems. In the future, we are going to include more retrieval-augmented LLM strategies such as active retrieval augmented generation (Jiang et al., 2023). Besides, we plan to make RETA-LLM more modulized and configurable.\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In NIPS, pages 4299\u20134307. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can We Know What Language Models Know? Transactions of the Association for Computational Linguistics, 8:423\u2013438. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418\u20135426, Online. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652. Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agent.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Large language models (LLMs) have shown extraordinary capabilities but still tend to hallucinate and generate fictitious responses. This necessitates the integration of information retrieval (IR) systems to enhance LLMs' factual accuracy and ability to answer in-domain questions.",
            "purpose of benchmark": "The benchmark aims to support research and facilitate the development of retrieval-augmented LLM systems, enabling users to create customized in-domain LLM-based systems."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of LLMs generating inaccurate information and their inability to answer specific in-domain queries without external knowledge.",
            "key obstacle": "Existing benchmarks do not adequately support the integration of IR systems with LLMs, limiting their effectiveness in reducing hallucinations and improving factual accuracy."
        },
        "idea": {
            "intuition": "The thought process behind RETA-LLM stems from the observation that augmenting LLMs with retrieval capabilities can significantly improve their performance in generating factual responses.",
            "opinion": "The authors believe that RETA-LLM is crucial for advancing retrieval-augmented LLM technology and enhancing user interaction with these models.",
            "innovation": "RETA-LLM introduces a modular design with plug-and-play capabilities for better interaction between IR systems and LLMs, offering improvements over previous toolkits like LangChain.",
            "benchmark abbreviation": "RETA-LLM"
        },
        "dataset": {
            "source": "The dataset is created from real-world data sourced from web pages and documents relevant to specific in-domain queries.",
            "desc": "The dataset includes a diverse collection of documents designed to enhance the retrieval capabilities of LLMs, enabling them to answer queries more accurately.",
            "content": "The dataset contains text data that includes information about various topics relevant to the queries posed to the LLM.",
            "size": "-",
            "domain": "Information Retrieval",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "Accuracy, F1-score",
            "aspect": "The metrics measure the factual accuracy and relevance of the responses generated by the LLMs.",
            "principle": "The metrics were chosen to provide a comprehensive evaluation of the performance of retrieval-augmented LLMs in generating accurate and relevant answers.",
            "procedure": "Model performance is evaluated by comparing generated responses against a set of ground truth answers and calculating the specified metrics."
        },
        "experiments": {
            "model": "The benchmark tests state-of-the-art retrieval-augmented LLMs.",
            "procedure": "Models are trained using a pipeline that includes request rewriting, document retrieval, passage extraction, answer generation, and fact-checking.",
            "result": "The models demonstrated improved accuracy and reduced hallucinations, with results indicating statistical significance.",
            "variability": "Variability was accounted for through multiple trials and evaluation across different subsets of the dataset."
        },
        "conclusion": "The RETA-LLM benchmark demonstrates significant improvements in the performance of retrieval-augmented LLMs, highlighting its potential to enhance user interactions and factual accuracy in generated responses.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive framework that enhances the integration of IR systems with LLMs, leading to better performance in generating factual responses.",
            "limitation": "The benchmark may not cover all possible in-domain queries, which could limit its applicability in certain specialized areas.",
            "future work": "Future research will focus on incorporating more advanced retrieval strategies and enhancing the modularity and configurability of RETA-LLM."
        },
        "other info": [
            {
                "info1": "The toolkit is publicly available on GitHub for community contributions and further development."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark aims to support research and facilitate the development of retrieval-augmented LLM systems, enabling users to create customized in-domain LLM-based systems."
        },
        {
            "section number": "1.2",
            "key information": "Large language models (LLMs) have shown extraordinary capabilities but still tend to hallucinate and generate fictitious responses."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the challenge of LLMs generating inaccurate information and their inability to answer specific in-domain queries without external knowledge."
        },
        {
            "section number": "2.3",
            "key information": "RETA-LLM introduces a modular design with plug-and-play capabilities for better interaction between IR systems and LLMs."
        },
        {
            "section number": "4.1",
            "key information": "The metrics measure the factual accuracy and relevance of the responses generated by the LLMs."
        },
        {
            "section number": "4.2",
            "key information": "Models are trained using a pipeline that includes request rewriting, document retrieval, passage extraction, answer generation, and fact-checking."
        },
        {
            "section number": "10.2",
            "key information": "Future research will focus on incorporating more advanced retrieval strategies and enhancing the modularity and configurability of RETA-LLM."
        }
    ],
    "similarity_score": 0.756155181482463,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/RETA-LLM_ A Retrieval-Augmented Large Language Model Toolkit.json"
}