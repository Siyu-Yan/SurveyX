{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.11711",
    "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
    "abstract": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs\u2019 deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-ofconcept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.",
    "bib_name": "benechehab2024zeroshotmodelbasedreinforcementlearning",
    "md_text": "# ZERO-SHOT MODEL-BASED REINFORCEMENT LEARNING USING LARGE LANGUAGE MODELS\nAbdelhakim Benechehab* Huawei Noah\u2019s Ark Lab, EURECOM \u2020 Youssef Attia El Hili Huawei Noah\u2019s Ark Lab Ambroise Odonnat Huawei Noah\u2019s Ark Lab, Inria\u2021 Oussama Zekri ENS Paris-Saclay\u22c4 Albert Thomas Huawei Noah\u2019s Ark Lab Giuseppe Paolo Huawei Noah\u2019s Ark Lab Maurizio Filippone Ievgen Redko Bal\u00b4azs K\u00b4egl\nAbdelhakim Benechehab* Huawei Noah\u2019s Ark Lab, EURECOM \u2020 Youssef Attia El Hili Huawei Noah\u2019s Ark Lab\nOussama Zekri ENS Paris-Saclay\u22c4\nIevgen Redko Huawei Noah\u2019s Ark Lab\nMaurizio Filippone KAUST\u22cf\n# Maurizio Filippone KAUST\u22cf\nABSTRACT\n# ABSTRACT\nThe emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs\u2019 deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-ofconcept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.\n# INTRODUCTION\nThe rise of large language models (LLMs) has significantly impacted the field of Natural Language Processing (NLP). LLMs (Brown et al., 2020; Hugo Touvron & the Llama 2 team., 2023; Dubey & the Llama 3 team, 2024), which are based on the transformer architecture (Vaswani et al., 2017), have redefined tasks such as machine translation (Brown et al., 2020), sentiment analysis (Zhang et al., 2023b), and question answering (Roberts et al., 2020; Pourkamali & Sharifi, 2024) by enabling machines to understand and generate human-like text with remarkable fluency. One of the most intriguing aspects of LLMs is their emerging capabilities, particularly in-context learning (ICL) (von Oswald et al., 2023). Through ICL, an LLM can learn to perform a new task simply by being provided examples of the task within its input context, without any gradient-based optimization. This phenomenon has been observed not only in text generation but also in tasks such as image classification (Abdelhamed et al., 2024; Zheng et al., 2024) and even solving logic puzzles (Giadikiaroglou et al., 2024), which is unexpected in the context of the standard statistical learning theory. To our knowledge, ICL capabilities of pre-trained LLMs have been only scarcely explored in reinforcement learning (Wang et al., 2023) despite the demonstrated success of the former in understanding the behavior of deterministic and chaotic dynamical systems (Liu et al., 2024c). In this paper, we show how ICL with pre-trained LLMs can improve the sample efficiency of Reinforcement Learning (RL), with two proof-of-concepts in policy evaluation and data-augmented off-policy RL. Following the dynamical system perspective on ICL introduced in Li et al. (2023) and experimentally studied in Liu et al. (2024c), we use the observed trajectories of a given agent *Correspondence to abdelhakim.benechehab1@huawei.com \u22c4Work done while at Huawei Noah\u2019s Ark Lab \u22cf\n*Correspondence to abdelhakim.benechehab1@huawei.com \u22c4Work done while at Huawei Noah\u2019s Ark \u2020Department of Data Science, EURECOM \u2021Univ. Rennes 2, CNRS, IRISA \u22cfStatistics Program, KAU\nGiuseppe Paolo Huawei Noah\u2019s Ark Lab\nBal\u00b4azs K\u00b4egl Huawei Noah\u2019s Ark Lab\nto predict its future state and reward in commonly used realistic RL environments. To achieve this, we solve two crucial challenges related to considering continuous state-space Markov Decision Processes (MDP): 1) incorporating the action information into the LLM\u2019s context and 2) handling the interdependence between the state-actions dimensions, as prior approaches were known to treat multivariate data\u2019s covariates independently. Our approach leads to several novel insights and contributions, which we summarize as follows: 1. Methodological. We develop a novel approach to integrate state dimension interdependence and action information into in-context trajectories. This approach, termed Disentangled In-Context Learning (DICL), leads to a new methodology for applying ICL in RL environments with continuous state spaces. We validate our proposed approach on tasks involving proprioceptive control. 2. Theoretical. We theoretically analyze the policy evaluation algorithm resulting from multibranch rollouts with the LLM-based dynamics model, leading to a novel return bound. 3. Experimental. We show how the LLM\u2019s MDP modeling ability can benefit two RL applications: policy evaluation and data-augmented offline RL. Furthermore, we show that the LLM is a calibrated uncertainty estimator, a desirable property for MBRL algorithms.\nto predict its future state and reward in commonly used realistic RL environments. To achieve this, we solve two crucial challenges related to considering continuous state-space Markov Decision Processes (MDP): 1) incorporating the action information into the LLM\u2019s context and 2) handling the interdependence between the state-actions dimensions, as prior approaches were known to treat multivariate data\u2019s covariates independently.\n1. Methodological. We develop a novel approach to integrate state dimension interdependence and action information into in-context trajectories. This approach, termed Disentangled In-Context Learning (DICL), leads to a new methodology for applying ICL in RL environments with continuous state spaces. We validate our proposed approach on tasks involving proprioceptive control. 2. Theoretical. We theoretically analyze the policy evaluation algorithm resulting from multibranch rollouts with the LLM-based dynamics model, leading to a novel return bound. 3. Experimental. We show how the LLM\u2019s MDP modeling ability can benefit two RL applications: policy evaluation and data-augmented offline RL. Furthermore, we show that the LLM is a calibrated uncertainty estimator, a desirable property for MBRL algorithms.\nOrganization of the paper. The paper is structured as follows: Section 2 introduces the main concepts from the literature used in our work (while a more detailed related work is differed to Appendix B). We then start our analysis in Section 3.1, by analyzing LLM\u2019s attention matrices DICL is presented in Section 3.3, while Section 4 contains different applications of the proposed method in RL, along with the corresponding theoretical analysis.\n# 2 BACKGROUND KNOWLEDGE\nReinforcement Learning (RL). The standard framework of RL is the infinite-horizon Markov decision process (MDP) M = \u27e8S, A, P, r, \u00b50, \u03b3\u27e9where S represents the state space, A the action space, P : S \u00d7 A \u2192S the (possibly stochastic) transition dynamics, r : S \u00d7 A \u2192R the reward function, \u00b50 the initial state distribution, and \u03b3 \u2208[0, 1] the discount factor. The goal of RL is to find, for each state s \u2208S, a distribution \u03c0(s) over the action space A, called the policy, that maximizes the expected sum of discounted rewards \u03b7(\u03c0) := Es0\u223c\u00b50,at\u223c\u03c0, st>0\u223cP t[\ufffd\u221e t=0 \u03b3tr(st, at)]. Under a policy \u03c0, we define the state value function at s \u2208S as the expected sum of discounted rewards, starting from the state s, and following the policy \u03c0 afterwards until termination: V \u03c0(s) = Eat\u223c\u03c0,st>0\u223cP t\ufffd\ufffd\u221e t=0 \u03b3tr(st, at) | s0 = s \ufffd . Model-based RL (MBRL). MBRL algorithms address the supervised learning problem of estimating the dynamics of the environment \u02c6P (and sometimes also the reward function \u02c6r) from data collected when interacting with the real system. The model\u2019s loss function is typically the log-likelihood L(D; \u02c6P) = 1 N \ufffdN i=1 log \u02c6P(si t+1|si t, ai t) or Mean Squared Error (MSE) for deterministic models. The learned model can subsequently be used for policy search under the MDP \ufffd M = \u27e8S, A, \u02c6P, r, \u00b50, \u03b3\u27e9. This MDP shares the state and action spaces S, A, reward function r, with the true environment M, but learns the transition probability \u02c6P from the dataset D. The policy \u02c6\u03c0 = arg max\u03c0 \u02c6\u03b7(\u03c0) learned on \ufffd M is not guaranteed to be optimal under the true MDP M due to distribution shift and model bias. Large Language Models (LLMs). Within the field of Natural Language Processing, Large Language Models (LLMs) have emerged as a powerful tool for understanding and generating humanlike text. An LLM is typically defined as a neural network model, often based on the transformer architecture (Vaswani et al., 2017), that is trained on a vast corpus of sequences, U = {U1, U2, . . . , Ui, . . . , UN}, where each sequence Ui = (u1, u2, . . . , uj, . . . , uni) consists of tokens uj from a vocabulary V. Decoder-only LLMs (Radford et al., 2019; Dubey & the Llama 3 team, 2024) typically encode an autoregressive distribution, where the probability of each token is conditioned only on the previous tokens in the sequence, expressed as p\u03b8(Ui) = \ufffdni j=1 p\u03b8(uj|u0:j\u22121). The parameters \u03b8 are learned by maximizing the probability of the entire dataset, p\u03b8(U) = \ufffdN i=1 p\u03b8(Ui). Every LLM has an associated tokenizer, which breaks an input string into a sequence of tokens, each belonging to V.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/969d/969df19f-1506-48b8-a5e9-0334452a1729.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: LLM can perceive time patterns. The LLM is fed with 3 time series presenting distinct patterns. (a) Rectangular pulse. (b) Rectangular signal with constant sub-parts. (c) The fthigh dimension of HalfCheetah under an expert policy. Tokens belonging to constant slots (or peaks) attend to all the similar ones that precede them, yet they focus more on the first occurrence of that phenomena.</div>\nFigure 1: LLM can perceive time patterns. The LLM is fed with 3 time series presenting distinct patterns. (a) Rectangular pulse. (b) Rectangular signal with constant sub-parts. (c) The fthigh dimension of HalfCheetah under an expert policy. Tokens belonging to constant slots (or peaks) attend to all the similar ones that precede them, yet they focus more on the first occurrence of that phenomena.\nIn-Context Learning (ICL). In order to use trajectories as inputs in ICL, we use the tokenization of time series proposed in Gruver et al. (2023b) and Jin et al. (2024). The latter works use a subset of the LLM sub-vocabulary Vnum representing digits as summarized in Algorithm 1. Given an univarite time se-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee35/ee35270f-ff36-45d6-852c-8b84241bc6c0.png\" style=\"width: 50%;\"></div>\nries, we first rescale it into a specific range (Liu et al., 2024b; Zekri et al., 2024a; Requeima et al., 2024) then encode the rescaled time series with k digits. The logits corresponding to tokens in Vnum can be further used to predict the next timestamp transition rule for Markovian systems (Liu et al., 2024c).\n3 ZERO-SHOT DYNAMICS LEARNING USING LARGE LANGUAGE MODELS\n# 3.1 MOTIVATION\nBefore considering the multivariate trajectories of agents collected in RL environments, we first want to verify whether a pre-trained LLM model is sensitive to the primitive univariate signals akin to those encountered in them. For this, we investigate the attention mechanism of the Llama3 8B model (Dubey & the Llama 3 team, 2024) when we feed it with different signals, including the periodic fthigh dimension from the HalfCheetah system (Brockman et al., 2016). By averaging the attention matrices over the 32 heads for each of the 32 layers of the multi-head attention in Llama3, we observed distinct patterns that provide insight into the model\u2019s focus and behavior (Fig. 1 shows selected attention layers for each signal). The attention matrices exhibit a diagonal pattern, indicative of strong self-correlation among timestamps, and a subtriangular structure due to the causal masked attention in decoder-only transformers. Further examination of the attention matrices reveals a more intricate finding. Tokens within repeating patterns (e.g., signal peaks, constant parts) not only attend to past tokens within the same cycle but also to those from previous occurrences of the same pattern, demonstrating a form of in-context learning. The ability to detect and exploit repeating patterns within such signals is especially valuable in RL, where state transitions and action outcomes often exhibit cyclical or recurring dynamics, particularly in continuous control tasks. However, applying this insight to RL presents two critical challenges related to 1) the integration of actions into the forecasting process, and 2) handling of the multivariate nature of RL problems. We now address these challenges by building on the insights from the analysis presented above.\n# 3.2 PROBLEM SETUP\nGiven an initial trajectory T = (s0, a0, r1, s1, a1, r2, s2, . . . , rT \u22121, sT \u22121) of length T, with st \u2208S, at = \u03c0(st) \u2208A1, where the policy \u03c0 is fixed for the whole trajectory, and rt \u2208R, we want to predict future transitions: given (sT \u22121, aT \u22121) predict the next state and reward (sT , rT ) and subsequent transitions autoregressively. For simplicity we first omit the actions and the reward, focusing instead on the multivariate sequence \u03c4 \u03c0 = (s0, s1, . . . , sT ) where we assume that the state dimensions are independent. Later, we show how to relax the assumptions of omitting actions and rewards, as well as state independence, which is crucial for applications in RL. The joint probability density function of \u03c4 \u03c0 can be written as:\n\ufffd Using the decoder-only nature of the in-context learner defined in Section 2, we can apply Algorithm 1 to each dimension of the state vector to infer the transition rule of each visited state in \u03c4 \u03c0 conditioned on its relative history: for all j \u2208{1, . . . , ds},\n{ | \u2212 } where \u03b8 are the fixed parameters of the LLM used as an in-context learner, and T its context length. Assuming complete observability of the MDP state, the Markovian property unveils an equivalence between the learned transition rules and the corresponding Markovian ones: \u02c6P\u03b8(st|st\u22121, . . . , s1, s0) = \u02c6P\u03b8(st|st\u22121). This approach, that we name vICL (for vanilla ICL), thus applies Algorithm 1 on each dimension of the state individually, assuming their independence. Furthermore, the action information is integrated-out (as depicted in Eq. (1)), which in theory, limits the application scope of this method to quantities that only depend on a policy through the expectation over actions (e.g., the value function V \u03c0(s)). We address these limitations in the next section.\nwhere \u03b8 are the fixed parameters of the LLM used as an in-context learner, and T its context length. Assuming complete observability of the MDP state, the Markovian property unveils an equivalence between the learned transition rules and the corresponding Markovian ones: \u02c6 \u02c6.\n|| This approach, that we name vICL (for vanilla ICL), thus applies Algorithm 1 on each dimension of the state individually, assuming their independence. Furthermore, the action information is integrated-out (as depicted in Eq. (1)), which in theory, limits the application scope of this method to quantities that only depend on a policy through the expectation over actions (e.g., the value function V \u03c0(s)). We address these limitations in the next section.\n<div style=\"text-align: center;\">3.3 STATE AND ACTION DIMENSION INTERDEPENDENCE</div>\n3.3 STATE AND ACTION DIMENSION INTERDEPENDENCE\nIn this section we address the two limitations of vICL discussed in Section 3.2 by introducing Disentangled InContext Learning (DICL), a method that relaxes the assumption of state feature independence and reintroduces the action by employing strategies that aim to map the state-action vector to a latent space where the features are independent. We can then apply vICL, which operates under the assumption of feature independence, to the latent representation. An added benefit of using such a latent space is that it can potentially reduce the dimensionality, leading to a speed-up of the overall approach. While sophisticated approaches2 like disentangled autoencoders could be considered for DICL, in this work we employ Principal Component Analysis (PCA). In fact, the absence of pre-trained models for this type of representation learning requires training from scratch on a potentially large dataset. This goes against our goal of leveraging the pre-trained knowledge of LLMs and ICL. Instead, we find that PCA, which generates new linearly uncorrelated features and can reduce dimensionality, strikes a good balance between simplicity, tractability, and performance (Fig. 2 and Fig. 3). Nonetheless, DICL is agnostic to this aspect and any approach that can disentangle featur\n1In practice, states and actions are real valued vectors spanning a space of dimensions respectively ds and da S = Rds, A = Rda 2A more detailed discussion of alternative approaches to PCA is provided in Appendix C\n(1)\n(2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6b9/b6b9e09b-5189-4633-b2ba-f44f9bab28e3.png\" style=\"width: 50%;\"></div>\nFigure 2: The covariance matrix computed from the D4RL expert dataset in the Halfcheetah environment indicates linear correlations between stateaction features. We perform PCA before applying ICL to achieve a potentially lower-dimensional space with linearly uncorrelated features.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5078/5078e2c3-101d-497a-ad2c-ac12ccb65dee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: PCA-based methods achieve smaller multi-step error in less computational time. We compare DICL-(s) and DICL-(s, a) using a number of components equal to half the number of features, with the vanilla approach vICL and an MLP baseline. The baseline is trained for next state prediction using the transitions present in the context of the LLM.</div>\n<div style=\"text-align: center;\">Figure 3: PCA-based methods achieve smaller multi-step error in less computational time. We</div>\nFigure 3: PCA-based methods achieve smaller multi-step error in less computational time. We compare DICL-(s) and DICL-(s, a) using a number of components equal to half the number of features, with the vanilla approach vICL and an MLP baseline. The baseline is trained for next state prediction using the transitions present in the context of the LLM.\nIn the rest of the paper, we refer to the routine that applies PCA to feature space of states and actions and then runs Algorithm 1 in the projection space of principal components as DICL-(s, a). In some cases, integrating the action may not be necessary, such as when one is only interested in estimating the value function V \u03c0(s). To address this, we introduce DICL-(s) which denotes the combination of PCA and ICL applied solely to the states.\n3.4 AN ILLUSTRATIVE EXAMPLE\n# 3.4 AN ILLUSTRATIVE EXAMPLE\nIn this section, we aim to challenge our approach against the HalfCheetah system from the MuJoCo Gym environment suite (Brockman et al., 2016; Todorov et al., 2012). All our experiments are conducted using the Llama 3 series of models (Llama 3 8B, Llama 3.2 1B, Llama 3.2 3B) (Dubey & the Llama 3 team, 2024). Fig. 3a shows the average MSE over a prediction horizon of h \u2208 {1, . . . , 20} steps for each state dimension. Fig. 3b shows predicted trajectories for selected state dimensions of the HalfCheetah system (the details of the experiment, the metrics and the remaining state dimensions are differed to Appendix F). We first observe that the LLM-based dynamics forecasters exhibit a burn-in phase (\u224870 steps in Fig. 3b) that is necessary for the LLM to gather enough context. For multi-step prediction, Fig. 3a, showing the average MSE over prediction horizons and trajectories, demonstrates that both versions of DICL improve over the vanilla approach and the MLP baseline, trained on the context data, in almost all state dimensions. Indeed, we hypothesize that this improvement is especially brought by the projection in a linearly uncorrelated space that PCA enables. Furthermore, we also leveraged the dimensionality reduction feature by selecting a number of components equal to half the number of the original features. This results in a significant decrease in the computational time of the method without loss of performance, as showcased by Fig. 3c. We now move to integrating our methodology in RL-related applications.\n# 4 USE-CASES IN REINFORCEMENT LEARNING\nAs explored in the preceding sections, LLMs can be used as accurate dynamics learners for proprioceptive control through in-context learning. We now state our main contributions in terms of the integration of DICL into MBRL. First, we generalize the return bound of Model-Based Policy Optimization (MBPO) (Janner et al., 2019) to the more general case of multiple branches and use it to analyze our method. Next, we leverage the LLM to augment the replay buffer of an off-policy RL algorithm, leading to a more sample-efficient algorithm. In a second application, we apply our method to predict the reward signal, resulting in a hybrid model-based policy evaluation technique. Finally, we show that the LLM provides calibrated uncertainty estimates and conclude with a discussion of our results.\nWhen using a dynamics model in MBRL, one ideally seeks monotonic improvement guarantees, ensuring that the optimal policy under the model is also optimal under the true dynamics, up to some bound. Such guarantees generally depend on system parameters (e.g., the discount factor \u03b3), the prediction horizon k, and the model generalization error \u03b5m. As established in Janner et al. (2019) and Frauenknecht et al. (2024), the framework for deriving these theoretical guarantees is the one of branched model-based rollouts.\nA branched rollout return \u03b7branch[\u03c0] of a policy \u03c0 is defined in Janner et al. (2019) as the return of a rollout which begins under the true dynamics P and at some point in time switches to rolling out under learned dynamics \u02c6P for k steps. For our LLM-based dynamics learner, we are interested in studying a more general branching scheme that will be later used to analyze the results of our data-augmented off-policy algorithm. We begin by defining the multi-branch rollout return.\nDefinition 4.1 (Multi-branch rollout return). The multibranch rollout return \u03b7llm p,k,T [\u03c0] of a policy \u03c0 is defined as the expected return over rollouts with the following dynamics:\n2. for t \u2265 T, with probability p, the rollout switches to the LLM-based dynamics \u02c6Pllm for k steps, otherwise the rollout continues with the true dynamics P.\nThese different rollout realizations, referred to as branches, can overlap, meaning that multipl LLM-based dynamics can run in parallel if multiple branchings from the true dynamics occur withi the k-step window (see Fig. 4).\nWith this definition, we now state our main theoretical result, consisting of a return bound betwee the true return and the multi-branch rollout return.\nTheorem 4.2 (Multi-branch return bound). Let T be the minimal length of the in-context trajecto ries, p \u2208[0, 1] the probability that a given state is a branching point. We assume that the reward i bounded and that the expected total variation between the LLM-based model and the true dynamic under a policy \u03c0 is bounded at each timestep by maxt\u2265T Es\u223cP t,a\u223c\u03c0[DTV(P(.|s, a)|| \u02c6Pllm(.|s, a))] \u2264 \u03b5llm(T). Then under a multi-branched rollout scheme with a branch length of k, the return i bounded as follows:\nwhere rmax = maxs\u2208S,a\u2208A r(s, a).\nTheorem 4.2 generalizes the single-branch return presented in Janner et al. (2019), incorporating an additional factor of the prediction horizon k due to the presence of multiple branches, and directly accounting for the impact of the amount of LLM training data through the branching factor p. Additionally, the bound is inversely proportional to the minimal context length T, both through the power in the discount factor \u03b3T and the error term \u03b5llm(T). Indeed, the term \u03b5llm(T) corresponds to the generalization error of in-context learning. Several works in the literature studied it and showed that it typically decreases in O(T \u22121/2) with T the length of the context trajectories (Zekri et al., 2024b; Zhang et al., 2023c; Li et al., 2023).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/da5a/da5ab788-736a-4b6c-9bd7-92d6be956369.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 4: Multi-branch return. The rollout following the true dynamics P is shown in blue. The branched rollouts following LLM-based dynamics \u02c6Pllm are in purple. Branched rollouts can overlap and the corresponding return is the expectation over the overlapping branches.\n(3)\nIn this section, we show how DICL can be used for data augmentation in off-policy model-free RL algorithms such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018). The idea is to augment the replay buffer of the off-policy algorithm with transitions generated by DICL, using trajectories already collected by previous policies. The goal is to improve sample-efficiency and accelerate the learning curve, particularly in the early stages of learning as the LLM can generate accurate transitions from a small trajectory. We name this application of our approach DICL-SAC.\nAs defined in Corrado & Hanna (2023), data-augmented off-policy RL involves perturbing previously observed transitions to generate new transitions, without further interaction with the environment. The generated transitions should ideally be diverse and feasible under the MDP dynamics to enhance sample efficiency while ensuring that the optimal policy remains learnable. Algorithm 2 (DICL-SAC) integrates multiple components to demonstrate a novel proof-ofconcept for improving the sample efficiency of SAC using DICL for data augmentation. Let T = (s0, a0, r0, . . . , sTmax, aTmax, rTmax) be a real trajectory collected with a fixed policy \u03c0\u03d5, sampled from the real transitions being stored in a replay buffer R. We generate synthetic transitions (st, \u02dcat, rt, \u02c6st+1)T \u2264t\u2264Tmax, where  is the next state generated by th\nAlgorithm 2 DICL-SAC\n1: Inputs: LLM-based dynamics learner (e.g. DICL-(s)),\nbatch size b, LLM data proportion \u03b1, minimal context\nlength T, and maximal context length Tmax\n2: Initialize policy \u03c0\u03d5, critic Q\u03c8, replay buffer R, and\nLLM replay buffer Rllm, and context size Tmax\n3: for t = 1, . . . , N interactions do\n4:\nNew transition (st, at, rt, st+1) from \u03c0\u03b8\n5:\nAdd (st, at, rt, st+1) to R\n6:\nStore auxiliary action \u02dcat \u223c\u03c0\u03b8(.|st)\n7:\nif Generate LLM data then\n8:\nSample trajectory T = (s0, . . . , sTmax) from R\n9:\n{\u02c6si+1}0\u2264i\u2264Tmax \u223cDICL-(s) (T )\n10:\nAdd {(si, \u02dcai, ri, \u02c6si+1)}T \u2264i\u2264Tmax to Rllm\n11:\nend if\n12:\nif update SAC then\n13:\nSample batch B of size b from R\n14:\nSample batch Bllm of size \u03b1 \u00b7b from Rllm\n15:\nUpdate \u03b8 and \u03c8 on B \u222aBllm\n16:\nend if\n17: end for\n \u2264\u2264 \u02c6st+1 is the next state generated by the LLM model applied on the trajectory of the states only, \u02dcat is an action sampled from the data collection policy \u03c0\u03d5(.|st), and T is the minimal context length. These transitions are then stored in a separate replay buffer Rllm. At a given update frequency, DICL-SAC performs G gradient updates using data sampled from R and \u03b1% \u00b7 G gradient updates using data sampled from Rllm. Other hyperparameters of our method include the LLM-based method (vICL or DICL-(s)), how often we generate new LLM data and the maximal context length Tmax (see Appendix D for the full list of hyperparameters). It is possible to use the LLM as a transition model with DICL-(s, a), generating the next state based on the previous state and an action (potentially sampled from a different policy), this approach would make data generation very expensive due to the high inference cost of the LLM. Indeed, thanks to Eq. (2), our data augmentation strategy, applies the LLM only to the states, allowing us to generate Tmax \u2212T transitions with ds calls to the LLM, one call for each dimension of the state. However, using the LLM as a transition model, accounting for actions, would require (Tmax \u2212T) \u00b7 ds calls as we would need one call for each action we modify. This is also why we assume a fixed policy in the context, enabling the LLM to learn P \u03c0\u03d5 using only the states. Fig. 5 compares the return curves obtained by DICL-SAC against SAC in three control environments from the Gym library (Brockman et al., 2016). As anticipated with our data augmentation approach, we observe that our algorithm improves the sample efficiency of SAC at the beginning of training. This improvement is moderate but significant in the Pendulum and HalfCheetah environments, while the return curves tend to be noisier in the Hopper environment. Furthermore, as the proportion of LLM data \u03b1 increases, the performance of the algorithm decreases (particularly in HalfCheetah), as predicted by Theorem 4.2. Indeed, a larger proportion of LLM data correlates with a higher probability of branching p, as more branching points will be sampled throughout the training. Regarding the other parameters of our bound in Theorem 4.2, we set T = 1, meaning all LLM-generated transitions are added to Rllm, and k = 1 to minimize LLM inference time.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d474/d47487ab-276e-47ea-b2f2-a95a0c1532d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Data-augmented off-policy RL. In the early stages of training DICL-SAC improves the sample efficiency of SAC on three Gym control environments.</div>\ngure 5: Data-augmented off-policy RL. In the early stages of training DICL-SAC improves the mple efficiency of SAC on three Gym control environments.\nIn this section we show how DICL can be used for policy evaluation. System engineers are often presented with several policies to test on their systems. On the one hand, off-policy evaluation (e.g., Uehara et al. (2022)) involves using historical data collected from a different policy to estimate the performance of a target policy without disrupting the system. However, this approach is prone to issues such as distributional shift and high variance. On the other hand, online evaluation provides a direct and unbiased comparison under real conditions. System engineers often prefer online evaluation for a set of pre-selected policies because it offers real-time feedback and ensures that deploy-\nment decisions are based on live data, closely reflecting the system\u2019s true performance in production. However, online evaluations can be time-consuming and may temporarily impact system performance. To address this, we propose a hybrid approach using LLM dynamics predictions obtained through ICL to reduce the time required for online evaluation: the initial phase of policy evaluation is conducted as a standard online test, while the remainder is completed offline using the dynamics predictions enabled by the LLM\u2019s ICL capabilities. Fig. 6 illustrates the relative error in value obtained by predicting the trajectory of rewards for k steps, given a context length of T = 500. When k \u2264500, we complete the remaining steps of the 1000-step episode using the actual rewards. For the two versions of DICL, the reward vector is concatenated to the feature space prior to applying PCA. In the Hopper environment, it is evident that predicting the reward trajectory alone is a challenging task for the vanilla method vICL. On the contrary, both DICL-(s) and DICL-(s, a) effectively capture some of the dependencies of the reward signal on the states and actions, providing a more robust method for policy evaluation, and matching the MLP baseline that has been trained on a dataset of transitions sampled from the same policy. However, in HalfCheetah we observe that the vanilla method largely improves upon both the baseline and DICL. We suspect that this is due to the fact that the reward signal is strongly correlated with the \u02d9 rootx dimension in HalfCheetah, which proved to be harder to predict by our approach, as can be seen in Fig. 3a. Note that the experimental setup that we follow here is closely related to the concept of Model-based\nNote that the experimental setup that we follow here is closely related to the concept of Model-based Value Expansion (Feinberg et al., 2018; Buckman et al., 2018), where we use the dynamics mode to improve the value estimates through an n-step expansion in an Actor Critic algorithm.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/20c1/20c111b1-dd28-4007-830e-487d3f0d6682.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Policy evaluation with DICL. Relative error on the predicted value over k = 500 steps, with context length of T = 500.</div>\nAn intriguing property observed in Fig. 3b is the confidence interval around the predictions. As detailed in Algorithm 1, one can extract a full probability distribution for the next prediction given the context, enabling uncertainty estimation in the LLM\u2019s predictions. Notably, this uncertainty is pronounced at the beginning when context is limited, around peaks, and in regions where the average prediction exhibits large errors. We explore this phenomenon further in the next section by evaluating the calibration of the LLM\u2019s uncertainty estimates. Calibration is known to be an important property of a dynamics model when used in reinforcement learning (Malik et al., 2019). In this section, we aim to investigate whether the uncertainty estimates derived from the LLM\u2019s logits are well-calibrated. We achieve this by evaluating the quantile calibration of the probability distributions obtained for each LLM-based method. Quantile calibration. For a regression problem with variable y \u2208Y = R, and a model that outputs a cumulative distribution function (CDF) over (where indexes data points), quantile\nQuantile calibration. For a regression problem with variable y \u2208Y = R, and a model that outputs a cumulative distribution function (CDF) Fi over yi (where i indexes data points), quantile calibration implies that yi (groundtruth) should fall within a p%-confidence interval p% of the time:\nwhere F \u22121 i : [0, 1] \u2192Y denotes the quantile function F \u22121 i (p) = inf{y : p \u2264Fi(y)} for all p \u2208[0, 1], and N the number of samples.\nLLMs are well-calibrated forecasters. Fig. 7 shows the reliability diagram for the bfoot dimension of the HalfCheetah system. The overall conclusion is that, regardless of the LLM-based sub-routine used to predict the next state, the uncertainty estimates derived from the LLM\u2019s logits are well-calibrated in terms of quantile calibration. Ideally, forecasters should align with the diagonal in Fig. 7, which the LLM approach nearly achieves. Furthermore, when comparing with a naive baseline (the details are differed to Appendix G), the LLM-forecaster matches the baseline when it\u2019s already calibrated, and improves over it when it\u2019s not. To quantify a forecaster\u2019s calibration with a point statistic, we compute the Kolmogorov-Smirnov goodness-of-fit test (Eq. (10)) that is shown in the legend of Fig. 7.\nFigure 7: Quantile calibration reliability diagram. We show that the LLM uncertainty estimates are well-calibrated. Vertical lines show the Kolmogorov-Smirnov statistic for each fit.\n# 5 CONCLUSION\nIn this paper, we ask how we can leverage the emerging capabilities of Large Language Models to benefit model-based reinforcement learning. We build on previous work that successfully conceptualized in-context learning for univariate time series prediction, and provide a systematic methodology to apply ICL to an MDP\u2019s dynamics learning problem. Our methodology, based on a projection of the data in a linearly uncorrelated representation space, proved to be efficient in capturing the dynamics of typical proprioceptive control environments, in addition to being more computationally efficient through dimensionality reduction. To derive practical applications of our findings, we tackled two RL use-cases: data-augmented offpolicy RL, where our algorithm DICL-SAC improves the sample efficiency of SAC, and benefits from a theoretical guarantee under the framework of model-based multi-branch rollouts. Our second application, consisted in predicting the trajectory of rewards in order to perform hybrid online and model-based policy evaluation. Finally, we showed that the LLM-based dynamics model also provides well-calibrated uncertainty estimates.\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/38e9/38e9304a-20e7-4ed8-b833-33bc7370c6eb.png\" style=\"width: 50%;\"></div>\nREPRODUCIBILITY STATEMENT\nIn order to ensure reproducibility we release the code at https://github.com/abenechehab/dicl. The implementation details and hyperparameters are listed in Appendix D.\n# REFERENCES\nAbdelrahman Abdelhamed, Mahmoud Afifi, and Alec Go. What do you see? enhancing zero-shot image classification with multimodal large language models. arXiv preprint arXiv:2405.15668, 2024. Ekin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models, May 2023. URL http: //arxiv.org/abs/2211.15661. arXiv:2211.15661 [cs]. Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. In International Conference on Learning Representations, 2021. Abdelhakim Benechehab, Albert Thomas, and Bal\u00b4azs K\u00b4egl. Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning. February 2023. URL https: //openreview.net/forum?id=gvOSQjGTtxj. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI gym, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners, July 2020. URL http://arxiv.org/abs/2005.14165. arXiv:2005.14165 [cs]. Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sampleefficient reinforcement learning with stochastic ensemble value expansion. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, pp. 8234\u20138244, Red Hook, NY, USA, 2018. Curran Associates Inc. Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, and Yun Li. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods, 2024. URL https://arxiv.org/abs/2404.00282. Thomas Carta, Cl\u00b4ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning, 2023. URL https://arxiv.org/abs/2302.02662. Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models, 2022. URL https://arxiv.org/abs/2202.09481. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling, 2021. URL https://arxiv.org/abs/2106.01345. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems 31, pp. 4754\u20134765. Curran Associates, Inc., 2018. Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X. Wang, and Eric Schulz. Meta-in-context learning in large language models, May 2023. URL http://arxiv.org/ abs/2305.12907. arXiv:2305.12907 [cs]. Nicholas E Corrado and Josiah P Hanna. Understanding when dynamics-invariant data augmentations benefit model-free reinforcement learning updates. arXiv preprint arXiv:2310.17786, 2023.\nAbhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting, April 2024. URL http://arxiv.org/abs/2310.10688. arXiv:2310.10688 [cs]. Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the International Conference on Machine Learning, 2011. Andreas Draeger, Sebastian Engell, and Horst Ranke. Model predictive control using neural networks. IEEE Control Systems, 15:61\u201366, 1995. ISSN 1066033X. doi: 10.1109/37.466261. Abhimanyu Dubey and the Llama 3 team. The llama 3 herd of models, 2024. URL https: //arxiv.org/abs/2407.21783. Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey Levine. Model-based value estimation for efficient model-free reinforcement learning, 2018. URL https://arxiv.org/abs/1803.00101. Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, and Ji-Rong Wen. Large Language Model-based Human-Agent Collaboration for Complex Task Solving, February 2024. URL http://arxiv.org/abs/2402.12914. arXiv:2402.12914 [cs]. Bernd Frauenknecht, Artur Eisele, Devdutt Subhasish, Friedrich Solowjow, and Sebastian Trimpe. Trust the model where it trusts itself \u2013 model-based actor-critic with uncertainty-aware rollout adaption, 2024. URL https://arxiv.org/abs/2405.19014. Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2052\u20132062. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr. press/v97/fujimoto19a.html. Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving PILCO with Bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop, International Conference on Machine Learning, 2016. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, August 2023. URL http://arxiv. org/abs/2208.01066. arXiv:2208.01066 [cs]. Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou. Puzzle solving using reasoning of large language models: A survey. arXiv preprint arXiv:2402.11291, 2024. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large Language Models Are Zero-Shot Time Series Forecasters, October 2023a. URL http://arxiv.org/abs/2310. 07820. arXiv:2310.07820 [cs]. Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https://openreview.net/forum?id=md68e8iZK1. David Ha and J\u00a8urgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2450\u20132462. Curran Associates, Inc., 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1861\u20131870. PMLR, 10\u201315 Jul 2018.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2555\u20132565, 2019. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZbOu. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. TabLLM: Few-shot Classification of Tabular Data with Large Language Models, March 2023. URL http://arxiv.org/abs/2210.10723. arXiv:2210.10723 [cs]. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl. Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Jo\u02dcao G.M. Ara\u00b4ujo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):1\u201318, 2022. URL http://jmlr.org/papers/v23/21-1342.html. Louis Martin Hugo Touvron and the Llama 2 team. Llama 2: Open foundation and fine-tuned chat models, 2023. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Michael Janner, Qiyang Li, and Sergey Levine. Offline Reinforcement Learning as One Big Sequence Modeling Problem, November 2021. URL http://arxiv.org/abs/2106. 02039. arXiv:2106.02039 [cs]. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models, 2024. Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, and Byung-Cheol Min. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models, March 2024. URL http://arxiv.org/abs/2309.10062. arXiv:2309.10062 [cs]. Bal\u00b4azs K\u00b4egl, Gabriel Hurtado, and Albert Thomas. Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= p5uylG94S68. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21810\u201321823. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf. Hyunjik Kim and Andriy Mnih. Disentangling by factorising, 2019. URL https://arxiv. org/abs/1802.05983. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2555\u20132565, 2019. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZbOu. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. TabLLM: Few-shot Classification of Tabular Data with Large Language Models, March 2023. URL http://arxiv.org/abs/2210.10723. arXiv:2210.10723 [cs]. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl. Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Jo\u02dcao G.M. Ara\u00b4ujo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):1\u201318, 2022. URL http://jmlr.org/papers/v23/21-1342.html. Louis Martin Hugo Touvron and the Llama 2 team. Llama 2: Open foundation and fine-tuned chat models, 2023. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Michael Janner, Qiyang Li, and Sergey Levine. Offline Reinforcement Learning as One Big Sequence Modeling Problem, November 2021. URL http://arxiv.org/abs/2106. 02039. arXiv:2106.02039 [cs]. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models, 2024. Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, and Byung-Cheol Min. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models, March 2024. URL http://arxiv.org/abs/2309.10062. arXiv:2309.10062 [cs]. Bal\u00b4azs K\u00b4egl, Gabriel Hurtado, and Albert Thomas. Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= p5uylG94S68. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21810\u201321823. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf. Hyunjik Kim and Andriy Mnih. Disentangling by factorising, 2019. URL https://arxiv. org/abs/1802.05983. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models, 2023. URL https://arxiv.org/abs/2303.00001. Byung-Jun Lee, Jongmin Lee, and Kee-Eung Kim. Representation balancing offline model-based reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=QpNz8r_Ri2Y. Sergey Levine and Vladlen Koltun. Guided policy search. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1\u20139, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/levine13.html. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 19565\u201319594. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/li23l.html. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language Model Programs for Embodied Control, May 2023. URL http://arxiv.org/abs/2209.07753. arXiv:2209.07753 [cs]. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan. Learning to Model the World with Language, May 2024. URL http://arxiv.org/abs/ 2308.01399. arXiv:2308.01399 [cs]. Ruizhen Liu, Zhicong Chen, and Dazhi Zhong. Dromo: Distributionally robust offline model-based policy optimization. 2021. Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. RL-GPT: Integrating Reinforcement Learning and Code-as-policy, February 2024a. URL http://arxiv.org/abs/2402.19299. arXiv:2402.19299 [cs]. Toni J. B. Liu, Nicolas Boull\u00b4e, Rapha\u00a8el Sarfati, and Christopher J. Earls. LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law, 2024b. Toni J. B. Liu, Nicolas Boull\u00b4e, Rapha\u00a8el Sarfati, and Christopher J. Earls. LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law, February 2024c. URL http://arxiv.org/abs/2402.00795. arXiv:2402.00795 [cs]. Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, and Rasool Fakoor. TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models, October 2023. URL http://arxiv.org/abs/2310.05905. arXiv:2310.05905 [cs]. Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, and Jens Kober. ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models, March 2024. URL http://arxiv. org/abs/2403.09583. arXiv:2403.09583 [cs]. Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and Stefano Ermon. Calibrated Model-Based Deep Reinforcement Learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4314\u20134323. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/malik19a.html. Vincent Micheli, Eloi Alonso, and Franc\u00b8ois Fleuret. Transformers are Sample-Efficient World Models. September 2022. URL https://openreview.net/forum?id=vhFu1Acb0xb. Eduardo Pignatelli, Johan Ferret, and Tim Rocktaschel. Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL.\nCristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal Rolinek, and Georg Martius. Sample-efficient cross-entropy method for real-time planning. In Conference on Robot Learning 2020, 2020. URL https://corlconf.github.io/ corl2020/paper_217/. Rudra P. K. Poudel, Harit Pandya, Chao Zhang, and Roberto Cipolla. LanGWM: Language Grounded World Model, November 2023. URL https://arxiv.org/abs/2311. 17593v1. Nooshin Pourkamali and Shler Ebrahim Sharifi. Machine translation with large language models: Prompt engineering for persian, english, and russian directions. arXiv preprint arXiv:2401.08429, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can Wikipedia Help Offline Reinforcement Learning?, July 2022. URL http://arxiv.org/abs/2201.12122. arXiv:2201.12122 [cs]. James Requeima, John Bronskill, Dami Choi, Richard E. Turner, and David Duvenaud. LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language, May 2024. URL http://arxiv.org/abs/2405.12856. arXiv:2405.12856 [cs, stat]. Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5418\u20135426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020. emnlp-main.437. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, and Huazhe Xu. Unleashing the Power of Pretrained Language Models for Offline Reinforcement Learning, November 2023. URL http: //arxiv.org/abs/2310.20587. arXiv:2310.20587 [cs]. Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2:160\u2013163, 7 1991. ISSN 0163-5719. doi: 10.1145/122344.122377. URL https://dl.acm.org/doi/10.1145/122344.122377. Richard S Sutton, Csaba Szepesv\u00b4ari, Alborz Geramifard, and Michael Bowling. Dyna-style planning with linear function approximation and prioritized sweeping. Moore and Atkeson, 1992. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033, 2012. doi: 10.1109/IROS.2012.6386109. Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning, 2022. URL https://arxiv.org/abs/2212.06355. Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples, September 2024. URL http://arxiv.org/abs/2404.07544. arXiv:2404.07544 [cs]. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pp. 6000\u20136010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u02dcao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, May 2023. URL http://arxiv.org/abs/2212.07677. arXiv:2212.07677 [cs].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6672/66729eba-df0f-49dc-bdd9-70fc66505abf.png\" style=\"width: 50%;\"></div>\nYen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt a Robot to Walk with Large Language Models, November 2023. URL http://arxiv.org/abs/2309.09969. arXiv:2309.09969 [cs, eess]. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6. Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, and Tom M. Mitchell. Read and reap the rewards: Learning to play atari with the help of instruction manuals, 2024. URL https: //arxiv.org/abs/2302.04449. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An Explanation of In-context Learning as Implicit Bayesian Inference, July 2022. URL http://arxiv.org/abs/2111. 02080. arXiv:2111.02080 [cs]. Hao Xue and Flora D. Salim. PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting, December 2023. URL http://arxiv.org/abs/2210.08964. arXiv:2210.08964 [cs, math, stat]. Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation Models for Decision Making: Problems, Methods, and Opportunities, March 2023. URL http: //arxiv.org/abs/2303.04129. arXiv:2303.04129 [cs]. Yu Yang and Pan Xu. Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer, August 2024. URL http://arxiv.org/abs/2408.01402. arXiv:2408.01402 [cs]. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 14129\u201314142. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf. Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 28954\u201328967. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ f29a179746902e331572c483c45e5086-Paper.pdf. Oussama Zekri, Abdelhakim Benechehab, and Ievgen Redko. Can LLMs predict the convergence of Stochastic Gradient Descent? June 2024a. URL https://openreview.net/forum? id=FraikHzMu9. Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boull\u00b4e, and Ievgen Redko. Large language models as markov chains, 2024b. URL https://arxiv.org/ abs/2410.02724. Xianyuan Zhan, Xiangyu Zhu, and Haoran Xu. Model-based offline planning with trajectory pruning. 2021. Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, and Zhaoran Wang. How Can LLM Guide RL? A Value-Based Approach, February 2024. URL http://arxiv.org/abs/2402.16181. arXiv:2402.16181 [cs].\nWeipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning, October 2023a. URL https: //arxiv.org/abs/2310.09615v1. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: A reality check, 2023b. URL https://arxiv.org/abs/ 2305.15005. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization, 2023c. URL https://arxiv.org/abs/2305.19420. Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, and Ram Nevatia. Large language models are good prompt learners for low-shot image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28453\u201328462, 2024.\n# Appendix\nOutline. In Appendix A, we prove our main theoretical result (Theorem 4.2). We provide an extended related work in Appendix B. Additional materials about the state and action dimensions interdependence are given in Appendix C. The implementation details and hyperparameters of our methods are given in Appendix D. Finally, we provide additional experiments about multi-step errors (Appendix F), calibration (Appendix G), and the impact of the data collecting policy on the prediction error (Appendix E).\n# TABLE OF CONTENTS\n# A Theoretical analysis\nA.1 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nA.1 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B Related Work\n# B Related Work\n# C State and action dimensions interdependence - additional materials\nC.1 Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . . . . 21 C.2 Independent Component Analysis (ICA) . . . . . . . . . . . . . . . . . . . . . . . 22 C.3 AutoEncoder-based approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.4 Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n# D Algorithms\nD.1 Soft-Actor Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 DICL-SAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nD.1 Soft-Actor Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 DICL-SAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n# E What is the impact of the policy on the prediction error?\n# F Multi-step prediction errors\n# G Calibration\nG Calibration\n20\n# A THEORETICAL ANALYSIS\n# A.1 PROOF OF THEOREM 4.2\nWe start by formally defining the LLM multi-branch return \u03b7llm p,k,T . To do so, we first denote At the random event of starting a k-step LLM branch at timestep t and we denote Xt the associated indicator random variable Xt = 1[At]. We assume that the (Xt)t\u2265T are independent. We then define the random event Ak t that at least one of the k preceding timesteps has been branched, meaning that the given timestep t belongs to at least one LLM branch among the k possible branches: Ak t = \ufffdk\u22121 i=0 At\u2212i. The LLM multi-branch return can then be written as follows:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/46e1/46e1b4e9-9fa3-4f42-b65c-1a5a839f261f.png\" style=\"width: 50%;\"></div>\n\ufffd \ufffd\ufffd \ufffd where P t = P(.|P t\u22121) with P 0 = \u00b50 the initial state distribution and \u02c6P i t,llm = \u02c6P i llm(.|P t\u2212i). Before continuing, we first need to establish the following lemma. Lemma A.1. (Multi-step Error Bound, Lemma B.2 in Frauenknecht et al. (2024) and Janner et al. (2019).) Let P and \u02dcP be two transition functions. Define the multi-step error at time step t, starting from any initial state distribution \u00b50, as:\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd where P t = P(.|P t\u22121) with P 0 = \u00b50 the initial state distribution and \u02c6P i t,llm = \u02c6P i llm(.|P t\u2212i). Before continuing, we first need to establish the following lemma. Lemma A.1. (Multi-step Error Bound, Lemma B.2 in Frauenknecht et al. (2024) and Janner et (2019).) Let P and \u02dcP be two transition functions. Define the multi-step error at time step t, start from any initial state distribution \u00b50, as:</div>\n\u00b7|\u2225\u00b7| with P 0 = \u02dcP 0 = \u00b50. Let the one-step error at time step t \u22651 be defined as: \u03bet := Es\u223cP t\u22121(\u00b7|\u00b50) \ufffd DTV(P(\u00b7|s)\u2225\u02dcP(\u00b7|s)) \ufffd\nand \u03be0 = \u03b50 = 0.\nThen, the multi-step error satisfies the following bound: \u03b5t \u2264 t \ufffd i=0 \u03bei.\nThen, the multi-step error satisfies the following bound:\nProof. Let t > 0. We start with the definition of the total variation distance:\nProof. Let t > 0. We start with the definition of the total variation distance:\n\u03b5t = DTV(P t(\u00b7|\u00b50)\u2225\u02dcP t(\u00b7|\u00b50)) \ufffd \ufffd\nt = DTV(P t(\u00b7|\u00b50)\u2225\u02dcP t(\u00b7|\u00b50)) = 1 2 \ufffd s\u2032\u2208S \ufffd\ufffd\ufffdP t(s\u2032|\u00b50) \u2212\u02dcP t(s\u2032|\u00b50) \ufffd\ufffd\ufffdds\u2032 = 1 2 \ufffd s\u2032\u2208S \ufffd\ufffd\ufffd\ufffd \ufffd s\u2208S P(s\u2032|s)P t\u22121(s|\u00b50) \u2212\u02dcP(s\u2032|s) \u02dcP t\u22121(s|\u00b50) ds \ufffd\ufffd\ufffd\ufffdds\u2032 \u22641 2 \ufffd s\u2032\u2208S \ufffd s\u2208S \ufffd\ufffd\ufffdP(s\u2032|s)P t\u22121(s|\u00b50) \u2212\u02dcP(s\u2032|s) \u02dcP t\u22121(s|\u00b50) \ufffd\ufffd\ufffdds ds\u2032 = 1 2 \ufffd s\u2032\u2208S \ufffd s\u2208S \ufffd\ufffd\ufffdP(s\u2032|s)P t\u22121(s|\u00b50) \u2212\u02dcP(s\u2032|s) \u02dcP t\u22121(s|\u00b50) \ufffd\ufffd\ufffdds ds\u2032\n\ufffd \u2208S \ufffd \u2208S \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd = Es\u223cP t\u22121(\u00b7|\u00b50) \ufffd DTV(P(\u00b7|\u00b50)\u2225\u02dcP(\u00b7|s)) \ufffd + DTV(P t\u22121(\u00b7|\u00b50)\u2225\u02dcP t\u22121(\u00b7|\u00b50) = \u03bet + \u03b5t\u22121\nGiven that \u03be0 = \u03b50 = 0, by induction we have:\nWe now restate and prove Theorem 4.2:\nTheorem A.2 (Multi-branch return bound). Let T be the minimal length of the in-context trajecto ries, p \u2208[0, 1] the probability that a given state is a branching point. We assume that the reward is bounded and that the expected total variation between the LLM-based model and the true dynamics under a policy \u03c0 is bounded at each timestep by maxt\u2265T Es\u223cP t,a\u223c\u03c0[DTV(P(.|s, a)|| \u02c6Pllm(.|s, a))] \u2264 \u03b5llm(T). Then under a multi-branched rollout scheme with a branch length of k, the return is bounded as follows:  \u03b3T\nwhere rmax = maxs\u2208S,a\u2208A r(s, a).\n# Proof. Step 1: Expressing the bound in terms of horizon-dependent errors.\n# Proof. Step 1: Expressing the bound in terms of horizon-dependent errors.\n|\u03b7(\u03c0) \u2212\u03b7llm p,k,T (\u03c0)| = \ufffd\ufffd\ufffd\ufffd \u221e \ufffd t=T \u03b3tEst\u223cP t,at\u223c\u03c0 \ufffd r(st, at) \ufffd\n(6)\n\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd We then expand the integrals in the terms Est\u223cP t,at\u223c\u03c0 \ufffd r(st, at) \ufffd \u2212Est\u223c\u02c6 P i t,llm,at\u223c\u03c0 \ufffd r(st, at) \ufffd and express it in terms of horizon-dependent multi-step model errors:\nStep 2: Simplifying the bound. By applying Lemma A.1 we can bound the multi-step errors using the bound on one-step errors:\nStep 2: Simplifying the bound. By applying Lemma A.1 we can bound the multi-step errors using the bound on one-step errors: D(P t|| \u02c6P i ) \u2264i \u03b5(T) \u2264k \u03b5(T) (\nDTV(P t|| \u02c6P i t,llm) \u2264i \u03b5llm(T) \u2264k \u03b5llm(T)\nTherefore, the bound becomes:\nModel-based reinforcement learning (MBRL). MBRL has been effectively used in iterated batch RL by alternating between model learning and planning (Deisenroth & Rasmussen, 2011; Hafner et al., 2021; Gal et al., 2016; Levine & Koltun, 2013; Chua et al., 2018; Janner et al., 2019;\n(7)\n(8)\nK\u00b4egl et al., 2021), and in the offline (pure batch) RL where we do one step of model learning followed by policy learning (Yu et al., 2020; Kidambi et al., 2020; Lee et al., 2021; Argenson & Dulac-Arnold, 2021; Zhan et al., 2021; Yu et al., 2021; Liu et al., 2021; Benechehab et al., 2023). Planning is used either at decision time via model-predictive control (MPC) (Draeger et al., 1995; Chua et al., 2018; Hafner et al., 2019; Pinneri et al., 2020; K\u00b4egl et al., 2021), or in the background where a model-free agent is learned on imagined model rollouts (Dyna; Janner et al. (2019); Sutton (1991); Sutton et al. (1992); Ha & Schmidhuber (2018)), or both. For example, model-based policy optimization (MBPO) (Janner et al., 2019) trains an ensemble of feed-forward models and generates imaginary rollouts to train a soft actor-critic agent. LLMs in RL. LLMs have been integrated into reinforcement learning (RL) (Cao et al., 2024; Yang et al., 2023), playing key roles in enhancing decision-making (Kannan et al., 2024; Pignatelli et al.; Zhang et al., 2024; Feng et al., 2024), reward design (Kwon et al., 2023; Wu et al., 2024; Carta et al., 2023; Liu et al., 2023), and information processing (Poudel et al., 2023; Lin et al., 2024). The use of LLMs as world models is particularly relevant to our work. More generally, the Transformer architecture (Vaswani et al., 2017) has been used in offline RL (Decision Transformer Chen et al. (2021); Trajectory Transformer Janner et al. (2021)). Pre-trained LLMs have been used to initialize decision transformers and fine-tune them for offline RL tasks (Shi et al., 2023; Reid et al., 2022; Yang & Xu, 2024). As world models, Dreamer-like architectures based on Transformers have been proposed (Micheli et al., 2022; Zhang et al., 2023a; Chen et al., 2022), demonstrating efficiency for long-memory tasks such as Atari games. In text-based environments, LLMs have found multiple applications (Lin et al., 2024; Feng et al., 2024; Zhang et al., 2024; Ma et al., 2024), including using code-generating LLMs to generate policies in a zero-shot fashion (Liang et al., 2023; Liu et al., 2024a). The closest work to ours is Wang et al. (2023), where a system prompt consisting of multiple pieces of information about the control environment (e.g., description of the state and action spaces, nature of the controller, historical observations, and actions) is fed to the LLM. Unlike our approach, which focuses on predicting the dynamics of RL environments, Wang et al. (2023) aim to directly learn a low-level control policy from the LLM, incorporating extra information in the prompt. Furthermore, Wang et al. (2023) found that only GPT-4 was usable within their framework, while we provide a proof-of-concept using smaller open LLMs such as Llama 3.2 1B. ICL on Numerical Data. In-context learning for regression tasks has been theoretically analyzed in several works, providing insights based on the Transformer architecture (Li et al., 2023; von Oswald et al., 2023; Aky\u00a8urek et al., 2023; Garg et al., 2023; Xie et al., 2022). Regarding time series forecasting, LLMTime (Gruver et al., 2023a) successfully leverages ICL for zero-shot extrapolation of one-dimensional time series data. Similarly, Das et al. (2024) introduce a foundational model for one-dimensional zero-shot time series forecasting, while Xue & Salim (2023) combine numerical data and text in a question-answer format. ICL can also be used to approximate a continuous density from the LLM logits. For example, Liu et al. (2024c) develop a Hierarchical softmax algorithm to infer the transition rules of uni-dimensional Markovian dynamical systems. Building on this work, Zekri et al. (2024a) provide an application that predicts the parameter value trajectories in the Stochastic Gradient Descent algorithm. More relevant to our work, Requeima et al. (2024) presented LLMProcesses, a method aimed at extracting multi-dimensional distributions from LLMs. Other practical applications of ICL on numerical data include few-shot classification on tabular data (Hegselmann et al., 2023), regression (Vacareanu et al., 2024), and meta ICL (Coda-Forno et al., 2023).\nK\u00b4egl et al., 2021), and in the offline (pure batch) RL where we do one step of model learning followed by policy learning (Yu et al., 2020; Kidambi et al., 2020; Lee et al., 2021; Argenson & Dulac-Arnold, 2021; Zhan et al., 2021; Yu et al., 2021; Liu et al., 2021; Benechehab et al., 2023). Planning is used either at decision time via model-predictive control (MPC) (Draeger et al., 1995; Chua et al., 2018; Hafner et al., 2019; Pinneri et al., 2020; K\u00b4egl et al., 2021), or in the background where a model-free agent is learned on imagined model rollouts (Dyna; Janner et al. (2019); Sutton (1991); Sutton et al. (1992); Ha & Schmidhuber (2018)), or both. For example, model-based policy optimization (MBPO) (Janner et al",
    "paper_type": "method",
    "attri": {
        "background": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. This paper investigates how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes, identifying key challenges such as handling multivariate data and incorporating control signals that limit the potential of LLMs\u2019 deployment.",
        "problem": {
            "definition": "The paper aims to solve the problem of predicting future transitions in continuous Markov decision processes, specifically focusing on the integration of action information and the interdependence between state-action dimensions.",
            "key obstacle": "The main difficulty is the treatment of multivariate data's covariates independently, which prior approaches have struggled with, preventing effective predictions in reinforcement learning environments."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that leveraging the in-context learning capabilities of LLMs can enhance the sample efficiency of reinforcement learning by accurately modeling the dynamics of continuous environments.",
            "opinion": "The proposed idea, Disentangled In-Context Learning (DICL), integrates state dimension interdependence and action information into in-context trajectories, offering a new approach to applying ICL in RL environments.",
            "innovation": "The primary innovation of DICL lies in its methodological approach to integrate state-action interdependence and action information, which significantly improves upon existing methods that treat these dimensions independently."
        },
        "method": {
            "method name": "Disentangled In-Context Learning",
            "method abbreviation": "DICL",
            "method definition": "DICL is defined as a methodology that applies in-context learning to continuous state-space Markov Decision Processes by integrating action information and addressing the interdependence between state dimensions.",
            "method description": "DICL combines Principal Component Analysis (PCA) with in-context learning to enhance the modeling of dynamics in reinforcement learning.",
            "method steps": [
                "1. Collect initial trajectory data from the environment.",
                "2. Apply PCA to reduce the dimensionality of the state-action features.",
                "3. Utilize the reduced features to apply in-context learning for predicting future states and rewards."
            ],
            "principle": "The effectiveness of DICL stems from its ability to model the interdependence of state-action pairs and reduce dimensionality, allowing for more accurate predictions and improved sample efficiency in reinforcement learning."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the HalfCheetah system from the MuJoCo Gym environment, comparing DICL against vanilla in-context learning and MLP baselines across multiple state dimensions.",
            "evaluation method": "Performance was assessed through multi-step prediction accuracy, measuring mean squared error (MSE) over various prediction horizons and analyzing the calibration of uncertainty estimates."
        },
        "conclusion": "The paper concludes that leveraging the capabilities of LLMs through DICL can significantly enhance the efficiency of model-based reinforcement learning, providing well-calibrated uncertainty estimates and improving sample efficiency in RL applications.",
        "discussion": {
            "advantage": "The key advantages of DICL include improved sample efficiency, well-calibrated uncertainty estimates, and the ability to effectively integrate action information into state predictions.",
            "limitation": "One limitation of the method is its dependency on the quality of the pre-trained LLM and the potential challenges in generalizing across different environments or tasks.",
            "future work": "Future research could explore enhancing the integration of DICL with different types of LLMs, as well as investigating its applicability to a broader range of reinforcement learning environments."
        },
        "other info": {
            "code repository": "The code for the proposed method is available at https://github.com/abenechehab/dicl.",
            "related work": {
                "previous methods": "The paper builds on prior work that conceptualized in-context learning for univariate time series prediction and extends it to continuous state-space dynamics."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks."
        },
        {
            "section number": "1.3",
            "key information": "The intuition behind the proposed idea is that leveraging the in-context learning capabilities of LLMs can enhance the sample efficiency of reinforcement learning by accurately modeling the dynamics of continuous environments."
        },
        {
            "section number": "3.1",
            "key information": "DICL integrates state dimension interdependence and action information into in-context trajectories, offering a new approach to applying in-context learning in reinforcement learning environments."
        },
        {
            "section number": "3.4",
            "key information": "The effectiveness of DICL stems from its ability to model the interdependence of state-action pairs and reduce dimensionality, allowing for more accurate predictions and improved sample efficiency in reinforcement learning."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the method is its dependency on the quality of the pre-trained LLM and the potential challenges in generalizing across different environments or tasks."
        },
        {
            "section number": "6.4",
            "key information": "Future research could explore enhancing the integration of DICL with different types of LLMs, as well as investigating its applicability to a broader range of reinforcement learning environments."
        }
    ],
    "similarity_score": 0.7090658370386238,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Zero-shot Model-based Reinforcement Learning using Large Language Models.json"
}