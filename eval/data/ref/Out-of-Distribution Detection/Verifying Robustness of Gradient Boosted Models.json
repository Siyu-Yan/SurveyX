{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1906.10991",
    "title": "Verifying Robustness of Gradient Boosted Models",
    "abstract": "Gradient boosted models are a fundamental machine learning technique. Robustness to small perturbations of the input is an important quality measure for machine learning models, but the literature lacks a method to prove the robustness of gradient boosted models. This work introduces VERIGB, a tool for quantifying the robustness of gradient boosted models. VERIGB encodes the model and the robustness property as an SMT formula, which enables state of the art verification tools to prove the model\u2019s robustness. We extensively evaluate VERIGB on publicly available datasets and demonstrate a capability for verifying large models. Finally, we show that some model configurations tend to be inherently more robust than others.",
    "bib_name": "einziger2019verifyingrobustnessgradientboosted",
    "md_text": "# Verifying Robustness of Gradient Boosted Models\nGil Einziger, Maayan Goldstein, Yaniv Sa\u2019ar, Itai Segall Nokia, Bell Labs gilein@bgu.ac.il, {maayan.goldstein, yaniv.saar, itai.segall}@nokia-bell-labs.com\nLG]  26 Jun 2019\ngilein@bgu.ac.il, {maayan.goldstein, yaniv.saar, itai.segall}@nokia-bell-labs.com\n{maayan.goldstein, yaniv.saar, itai.segall}@nokia-bell-labs.c\n  26 Jun 20\n# Abstract\nGradient boosted models are a fundamental machine learning technique. Robustness to small perturbations of the input is an important quality measure for machine learning models, but the literature lacks a method to prove the robustness of gradient boosted models. This work introduces VERIGB, a tool for quantifying the robustness of gradient boosted models. VERIGB encodes the model and the robustness property as an SMT formula, which enables state of the art verification tools to prove the model\u2019s robustness. We extensively evaluate VERIGB on publicly available datasets and demonstrate a capability for verifying large models. Finally, we show that some model configurations tend to be inherently more robust than others.\narXiv:1906.10991v1\n# 1 Introduction\nGradient boosted models are fundamental in machine learning and are among the most popular techniques in practice. They are known to achieve good accuracy with relatively small models, and are attractive in numerous domains ranging from computer vision to transportation [26, 27, 6, 10, 7, 28]. They are easy to use as they do not require normalization of input features, and they support custom loss functions as well as classification and regression. Finally, the method has a solid theoretical grounding [20]. Machine learning models are often vulnerable to adversarial perturbations, which may cause catastrophic failures (e.g., by misclassification of a traffic sign). Specifically, Figure 1 exemplifies that gradient boosted models are indeed vulnerable to such perturbations. Thus, iden-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/00be/00be4f0b-8313-4a8e-8031-28c0d64a0398.png\" style=\"width: 50%;\"></div>\nFigure 1: Example of the lack of robustness in a gradient boosted model trained over a traffic signs dataset. In the first row, an \u201c80 km/h speed limit\u201d sign is misclassified as a \u201c30 km/h speed limit\u201d. In the second row, a \u201cturn left\u201d sign is misclassified as \u201cahead only\u201d. Observe in the third column (delta, computed as the difference in pixel values of the two images) that the applied changes are barely visible to the naked eye (delta of +/-3 in the range of 256 values per pixel per color). The fourth column highlights the modified pixels.\ntifying which models are robust to such manipulations and which are not is critical. Indeed, numerous works suggested training techniques that increase the robustness [19, 25]. However, there is currently no method to formally verify gradient boosted models. Furthermore, it is not clear how the configuration parameters of such models affect their robustness. These knowledge gaps make it challenging to guarantee the reliability of gradient boosted solutions. In the last couple of decades, formal methods successfully increased the reliability of numerous software and\nhardware systems. Such success gave rise to diverse verification methods such as model checking, termination analysis, and abstract interpretation. Formal methods are especially appealing in situations where the cost of mistakes is exceptionally high. Examples include missioncritical solutions as well as mass-produced hardware. Unfortunately, machine learning models are fundamentally different from traditional software artifacts, and we cannot directly use existing verification techniques for machine learning models. The research community already started addressing the problem for neural network models [23, 17, 15, 11, 22]. Here we focus on an area that has not been covered so far \u2013 verification of robustness of gradient boosted models. The main contribution of this work is the VERIGB tool for verifying the robustness of gradient boosted models. VERIGB encapsulates novel and formally proven methods that translate such models, and robustness properties into SMT formulas. Then, we feed these formulas to a standard SMT solver, which proves the robustness or provides a counter-example. VERIGB includes runtime optimizations that make the verification process practical. We extensively evaluate it with public datasets and demonstrate scalability for large and accurate models. Finally, we highlight that some model configurations are fundamentally more robust than others. The rest of this paper is organized as follows: In Section 2 we provide background on logic, decision trees, and gradient boosted models. Next, in Section 3, we formally define the robustness properties. The SMT formula representation of gradient boosted models is given in Section 4, and that of the robustness property in Section 5. Next, Section 6 suggests optimizations of these encodings improving their runtime. Section 7 evaluates VERIGB on several publicly-available datasets, while Section 8 surveys related work. We conclude in Section 9, which discusses the implications of our work and suggests directions for future research.\n# 2 Preliminaries\n# 2.1 Logic and Linear Arithmetic\nA propositional formula is defined inductively as one of the following: (i) \u2018True\u2019 and \u2018False\u2019 constants (T and F).\n(ii) a variable xi \u2208{x1, . . . , xm}; (iii) if \u03d5 and \u03c8 are propositional formulas then so are \u00ac\u03d5, \u03d5 \u2228\u03c8, \u03d5 \u2227\u03c8, \u03d5 \u2192\u03c8, \u03d5 \u2194\u03c8 (with their usual interpretation). Given a propositional formula \u03d5, the Boolean satisfiability problem (SAT) determines whether there exists an assignment under which \u03d5 evaluates to True. Satisfiability Modulo Theories (SMT) extends the Boolean SAT problem by combining a variety of underlying theories [1]. We use the linear real arithmetic theory, which extends the propositional fragment with all rational number constants, and with the symbols: {+, \u2212, \u00b7, \u2264, \u2265}. A formula \u03d5 (be that an SMT or SAT instance) is said to be satisfiable, if \u03d5 evaluates to True for some assignment \u20d7x \u2208Rm. If there is no such assignment, we say that \u03d5 is unsatisfiable.\n# 2.2 Decision Trees\nDecision trees are functions that receive an assignment \u20d7x \u2208Rm and return a value. Formally, a decision tree structure (DTS) D = \u27e8N, I, L\u27e9is defined as follows:\nDecision trees are functions that receive an assignment \u20d7x \u2208Rm and return a value. Formally, a decision tree structure (DTS) D = \u27e8N, I, L\u27e9is defined as follows: \u2022 N = {n1, . . . , nk}: is the set of nodes in the tree, and n1 is defined to be the root node of the tree. \u2022 I \u2286N: is the subset of internal nodes in the tree. An internal node is a triplet n = \u27e8Sn, Tn, Fn\u27e9, where Sn is a condition expressing the decision of node n (an SMT formula), and Tn \u2208N (resp., Fn \u2208N) is the target successor node when the condition evaluates to True (resp., False). \u2022 L = N \\ I: is the subset of leaf nodes in the tree, i.e. nodes for which there is no successor. A leaf node n = \u27e8Wn\u27e9also has a weight Wn \u2208R. Intuitively, S (resp., T and F) is a dictionary that associates to every n \u2208I a condition Sn (resp., a positive child Tn \u2208N and a negative child Fn \u2208N). W is a dictionary that associates to every n \u2208L a weight Wn \u2208R. A DTS D is said to be well-formed if, and only if, every node n \u2208N has exactly one predecessor node, except for the root node that has no predecessor. In a well-formed tree, we denote by Pn the predecessor of node n \u2208N. Given an input vector \u20d7x \u2208Rm, the valuation of a DTS D on \u20d7x is a function \u02c6D : Rm \u2192R. Tree D is traversed according to \u20d7x, ending in a leaf node n \u2208L, and function \u02c6D(\u20d7x) is the weight of that node, i.e. Wn \u2208R.\n \u27e8\u27e9 \u2022 N = {n1, . . . , nk}: is the set of nodes in the tree, and n1 is defined to be the root node of the tree.\n \u27e8\u27e9 \u2208 Intuitively, S (resp., T and F) is a dictionary that associates to every n \u2208I a condition Sn (resp., a positive child Tn \u2208N and a negative child Fn \u2208N). W is a dictionary that associates to every n \u2208L a weight Wn \u2208R. A DTS D is said to be well-formed if, and only if, every node n \u2208N has exactly one predecessor node, except for the root node that has no predecessor. In a well-formed tree, we denote by Pn the predecessor of node n \u2208N. Given an input vector \u20d7x \u2208Rm, the valuation of a DTS D on \u20d7x is a function \u02c6D : Rm \u2192R. Tree D is traversed according to \u20d7x, ending in a leaf node n \u2208L, and function \u02c6D(\u20d7x) is the weight of that node, i.e. Wn \u2208R.\n# 2.3 Gradient Boosted Trees\nGradient boosted regression is an ensemble technique that constructs a strong learner by iteratively adding weak learners (typically decision trees) [20]. Formally, a Gradient Boosted Regressor (GBR) is a sequence of r decision trees R = \u27e8D1, . . . , Dr\u27e9. Given an input vector \u20d7x \u2208Rm, the valuation of a GBR R is the sum of valuations of its r decision trees. That is, \u02c6R(\u20d7x) = \ufffdr i=1 \u02c6Di(\u20d7x). Gradient boosted classification is a tree ensemble technique that constructs a strong learner per each class (again, by iteratively adding weak learners), to assign a class for a given input. Let c be the number of classes. Formally, a Gradient Boosted Classifier (GBC) C = \u27e8R1, . . . , Rc\u27e9is a sequence of c gradient boosted regressors, where regressor Rj = \u27e8Dj 1, . . . , Dj r\u27e9. Given an input vector \u20d7x \u2208Rm, the valuation of C, valuates all c regressors over \u20d7x and returns the class associated with the maximal value, namely: \u02c6C(\u20d7x) = arg maxj( \u02c6Rj(\u20d7x)). We assume that there is an association between each input vector and a single class1.\n# 3 Robustness of Machine Learning Models\nRobustness means that small perturbations in the input have little effect on the outcome. That is, for classifiers the classification remains the same, and for regressors, the change in valuation is bounded. This section formally defines robustness properties, in a similar manner to [24, 22, 17, 21]. Consider a regression model R, and let \u02c6R(\u20d7x) be the valuation function of R for an input \u20d7x \u2208Rm. We define local adversarial (\u03f5, \u03b4)-robustness for an input \u20d7x, as follows:\nDefinition 3.1 (local adversarial robustness of regressors). A regression model R is said to be (\u03f5, \u03b4)-robust for an input \u20d7x, if for every input \u20d7x\u2032 such that ||\u20d7x \u2212\u20d7x\u2032||p < \u03f5, the output is bound by \u03b4, i.e., | \u02c6R(\u20d7x) \u2212\u02c6R(\u20d7x\u2032)| \u2264\u03b4.\nHere, ||\u20d7x \u2212\u20d7x\u2032||p is used to specify the distance between two vectors \u20d7x and \u20d7x\u2032 according to some norm p. For example, one may compute the distance between two images\n1In cases where multiple regressors return the same maximal value we can break the symmetry using their indices.\nas the maximal difference between pairs of corresponding pixels (i.e., p = \u221e), or the sum of these differences (i.e., p = 1). Throughout this paper we use norm p = \u221e, but our techniques are applicable to any norm that is linear to the input. Next, consider a classification model C and let \u02c6C(\u20d7x) be the valuation function of C for an input \u20d7x \u2208Rm. We define local adversarial \u03f5-robustness for an input \u20d7x as follows:\nDefinition 3.2 (local adversarial robustness of classifiers). A classification model C is said to be \u03f5-robust for an input \u20d7x, if for every input \u20d7x\u2032 such that ||\u20d7x\u2212\u20d7x\u2032||p < \u03f5, the output does not change its classification, i.e., \u02c6C(\u20d7x) = \u02c6C(\u20d7x\u2032).\nThe above definitions aim to certify a given input but do not guarantee much regarding the model itself. Therefore, we extend these definitions to capture the behavior over a set of inputs A. We define \u03c1-universal adversarial (\u03f5, \u03b4)robustness on a set of inputs A, as follows:\nDefinition 3.3 (universal adversarial robustness of regressors). A regression model R is said to be \u03c1-universally (\u03f5, \u03b4)-robust over the set of inputs A, if it is (\u03f5, \u03b4)-robust for at least \u03c1 \u00b7 |A| inputs in A.\nFinally, we extend the classifier definition of local \u03f5-robustness, and define \u03c1-universal adversarial \u03f5robustness on a set of inputs A, as follows:\nDefinition 3.4 (universal adversarial robustness of classifiers). A classification model C is said to be \u03c1-universally \u03f5-robust over the set of inputs A, if it is \u03f5-robust for at least \u03c1 \u00b7 |A| inputs in A.\nDefinition 3.3 and Definition 3.4 capture the universal adversarial robustness properties for regressors and classifiers. The parameter \u03f5 determines the allowed perturbation change, that is, how much an attacker can change the input. For regressors, we also require the parameter \u03b4 that defines the acceptable change in the output, while for classifiers we require that the classification stays the same. Finally, the parameter \u03c1 measures the portion of robust inputs. In Section 7, we evaluate the \u03c1 values of varying models instead of selecting a \u03c1 value in advance.\n# 4 Encodings of Gradient Boosted Models\n# 4 Encodings of Gradient Boosted Models Given DTS D = \u27e8N, I, L\u27e9, we now define the encoding of tree D to be the formula \u03a0(D) as follows:\ncodings of Gradient Boosted dels Given DTS D = \u27e8N, I, L\u27e9, we now define the encoding of tree D to be the formula \u03a0(D) as follows:\nThis section explains the encoding of gradient boosted models into SMT formulas. We start by translating a single path in a decision tree and then work our way up until we end up with a formula for the entire model.\n# 4.1 Encoding of Decision Trees\nGiven a well-formed DTS D = \u27e8N, I, L\u27e9and a leaf l \u2208 L, we define path(l) to be the set of nodes on the path in the tree between the leaf node l and the root node n1 (including both nodes). We define the encoding of leaf l in tree D to be the formula \u03c0(l) as follows:\n\u03c0(l) :\nThe encoding \u03c0(l) restricts the decision tree valuation variable wl to be the weight of the leaf (wl = Wl), and for each node n in the path except for the root, if node n is the positive child of its parent (TPn = n) then the parent condition should hold (SPn), and if node n is the negative child of its parent (FPn = n) then the negation of the parent condition should hold (\u00acSPn).\nLemma 4.1 (leaf encoding). Let \u02c6D be the valuation function of the well-formed tree D. If \u03c0(l) evaluates to True, then there exists a truth assignment \u20d7x \u2208Rm, wl \u2208R such that \u02c6D(\u20d7x) reaches leaf l , and \u02c6D(\u20d7x) = Wl = wl.\nProof. Assume that the leaf encoding \u03c0(l) evaluates to True, then there exists a truth assignment \u20d7x \u2208Rm, wl \u2208R. Since the tree is well-formed and following the definition of path(l), we know that every internal node n\u2032 \u2208path(l) \u2229I is a predecessor of some node n \u2208path(l), i.e., n\u2032 = Pn. If n is the positive successor of n\u2032, then (TPn = n) holds, implying that Sn\u2032 holds for \u20d7x as well. Thus, when the valuation of \u02c6D(\u20d7x) traverses tree D and reaches node n\u2032, we know that it indeed turns to the positive child. The same reasoning applies to the negative successor of n\u2032. By applying this reasoning recursively from the root node, we show that the traversal of the valuation reaches leaf l, and outputs \u02c6D(\u20d7x) = Wl = wl.\nGiven DTS D = \u27e8N, I, L\u27e9, we now define the encoding of tree D to be the formula \u03a0(D) as follows:\n\u03a0(D) : \ufffd l\u2208L \u03c0(l)\nNamely, \u03a0(D) is a disjunction of formulas, where each disjunct represents a concrete path to one of the leaves in D and its respective valuation.\nLemma 4.2 (tree encoding). Let \u02c6D be the valuation function of the well-formed tree D. If \u03a0(D) evaluates to True, then there exists a truth assignment \u20d7x \u2208Rm, wl \u2208R, and a single leaf l \u2208L for which \u02c6D(\u20d7x) reaches l and outputs \u02c6D(\u20d7x) = Wl = wl.\nProof. Assume that the tree encoding \u03a0(D) evaluates to True, then there exists a truth assignment \u20d7x \u2208Rm, wl \u2208R. Clearly, at least one clause in \u03a0(D) evaluates to True. Since tree D is well formed, at most one clause in \u03a0(D) evaluates to True, otherwise there exists an internal node in the path n \u2208path(l) \u2229I for which Sn is inconsistent over \u20d7x. Therefore, there exists exactly one clause in \u03a0(D) that evaluates to True, and exactly one leaf l \u2208L for which \u03c0(l) evaluates to True. If \u03c0(l) evaluates to True, then following the same reasoning of Lemma 4.1, the truth assignment \u20d7x \u2208Rm, wl \u2208R reaches leaf l and outputs \u02c6D(\u20d7x) = Wl = wl.\n# 4.2 Encoding of Gradient Boosted Trees\nGiven GBR R = \u27e8D1, . . . , Dr\u27e9 and following Lemma 4.1, and Lemma 4.2, we define the encoding of regressor R to be the formula \u03a5(R) as follows:\nIntuitively, \u03a5(R) consists of two parts: (i) the conjunction of all tree encodings, ensuring that the decision tree valuation variables of each tree wl1, . . . , wlr are restricted to their respective tree valuations; and (ii) a restriction of the regressor valuation variable out to be the sum of all decision tree valuation variables wl1, . . . , wlr. Therefore, encoding \u03a5(R) characterizes regressor R.\nTheorem 4.3 (regressor encoding). Let \u02c6R be the valuation function of regressor R. If \u03a5(R) evaluates to True,\nthen there exist a truth assignment \u20d7x \u2208Rm, out \u2208R, such that \u02c6R(\u20d7x) = out.\nProof. The proof follows from the definitions and Lemma 4.2.\nGiven GBC C = \u27e8R1, . . . , Rc\u27e9and following Theorem 4.3, we define the encoding of classifier C to be the formula \u0393(C) as follows:\nIntuitively, \u0393(C) consists of two parts: (i) the conjunction of all regressor encodings, ensuring that the regressor valuation variables out1, . . . , outr are restricted to their respective regressor valuations; and (ii) a restriction of the classifier valuation variable arg to be the maximal regressor valuation (i.e., operator arg max). Therefore, \u0393(C) charactarizes classifier C.\nTheorem 4.4 (classifier encoding). Let \u02c6C be the valuation function of classifier C. If \u0393(C) evaluates to True, then there exist a truth assignment \u20d7x \u2208Rm, arg \u2208 {1, . . . , c}, such that \u02c6C(\u20d7x) = arg.\nProof. The proof follows from the definitions, theorem, and lemmas above.\n# 5 Encodings of Local Robustness Properties\nIn this section, we encode the local robustness properties defined in Section 3. Recall that a regression model (resp., classification model) satisifies local adversarial robustness for an input \u20d7x (Definitions 3.1, and 3.2), if for all \u20d7x\u2032, if ||\u20d7x \u2212\u20d7x\u2032||p < \u03f5, then the difference between the valuation of \u20d7x, and that of \u20d7x\u2032 is bound (resp., we get the same classification for \u20d7x, and for \u20d7x\u2032). Our goal is to find whether there exists an assignment to \u20d7x\u2032 that satisfies both the model encoding, and the negation of the local adversarial robustness property. An assignment \u20d7x\u2032 that satisfies both conjuncts constitutes a counterexample that disproves local adversarial robustness of the given input \u20d7x. Alternatively, local adversarial robustness holds if there is no such assignment.\nGiven an input \u20d7x, and \u03f5, \u03b4 \u22650, we define the encoding of local adversarial robustness to be a formula \u03a6 as follows:\nxi \u2208R\nWhere \u03c6 is | \u02c6R(\u20d7x) \u2212\u02c6R(\u20d7x\u2032)| \u2265\u03b4 for regression model, and \u03c6 is \u02c6C(\u20d7x) \u0338= \u02c6C(\u20d7x\u2032) for classification model. Note that the second range of conjuncts in the expression, characterizes the allowed pertubations (||\u20d7x\u2212\u20d7x\u2032||p < \u03f5) for norm p = \u221e, which is handled differently for real, and integer features.\n# 6 Optimizations\nWhile the construction in Sections 4 and 5 is sound and complete, it is not always the most efficient one. Thus, we now provide two optimizations based on eliminating redundant clauses that cannot be satisfied, and on parallelizing the verification process.\n# 6.1 Pruning\n\u201cPruning\u201d is a somewhat overloaded term. In the context of machine learning, pruning typically refers to the process of removing sections of decision trees that have little impact on the outcome, thus reducing over-fitting. In the model-checking community, pruning is the process of trimming unreachable parts of the search space, thus helping the model checker focus its search. Our approach combines these two notions. Namely, we remove all unsatisfiable leaf clauses with respect to the robustness parameter (\u03f5), which allows for faster calculation. Formally, given DTS D = \u27e8N, I, L\u27e9and property \u03a6, we define the \u03a6-pruned encoding of leaf l in tree D to be:\n\u03c0\u03a6(l) = \ufffd \u03c0(l), \u03c0(l) \u2227\u03a6 is satisfiable False, \u03c0(l) \u2227\u03a6 is unsatisfiable\nNote that pruning can be applied to diverse properties, but this work is focused on the robustness property. Next, we define the corresponding \u03a5\u03a6 (resp., \u0393\u03a6) to be the \u03a6-pruned encoding of regressor R (resp., \u03a6-pruned\nencoding of classifier C), which replaces each occurrence of leaf encoding \u03c0(l) with its pruned version \u03c0\u03a6(l). The following theorem establishes the correctness of \u03a6pruning:\n# Theorem 6.1 (safe pruning).\n1. Regressor: the conjunction \u03a5(R) \u2227\u03a6 is satisfiable, if and only if, the conjunction \u03a5\u03a6(R) \u2227\u03a6 is satisfiable.\n2. Classifier: the conjunction \u0393(C) \u2227\u03a6 is satisfiable, if and only if, the conjunction \u0393\u03a6(C) \u2227\u03a6 is satisfiable.\nProof. The proofs follow immediately from the associativity property of propositional logic.\nIn principle, we may use an SMT solver to check the satisfiability of \u03c0(l) \u2227\u03a6 for each leaf, in each tree. In practice, we reduce the dependence on SMT solvers and increase scalability by evaluating the robustness property during the encoding of the tree, where each internal node condition constraints a single feature xi. For norm p = \u221e, the leaf valuation \u03c0(l) is satisfiable, if and only if for every node n \u2208path(l) that refers to feature xi, |xi \u2212 x\u2032 i| \u2264\u03f5. For norm p = 1, a necessary condition for the satisfiability of \u03c0(l), is that all features of \u20d7x\u2032, \ufffdm i=1 di \u2264 \u03f5, where:\nThe pruning process removes paths where the given vector \u20d7x\u2032 is \u201cfar\u201d from the required thresholds by more than \u03f5, where the notion of distance is determined by the norm.\n# 6.2 Parallelization\nIt is difficult to parallelize general SMT formulas efficiently. To increase scalability, we design our encoding in a manner that allows for parallel evaluation of gradient boosted classifiers. We do so by checking the robustness property separately for each class index. If all parallel evaluations are found robust, then the robustness property holds. Otherwise, there exists an assignment \u20d7x, and an index q, such that the robustness property does not hold, and \u20d7x is a counter-example. The thread of class q would discover this case and abort all other threads.\nWhere the parameter q is within [1, c], and each thread verifies a different value of q. For example, if an input is classified as class a, we invoke c \u22121 threads for classes {1, . . . , c} \\ {a}, where each thread tries to verify robustness with respect to a specific class.\n# 7 Evaluation\nWe now introduce VERIGB (Verifier of Gradient Boosted models), which implements our approach in Python. VERIGB utilizes Z3 [8] as the underlying SMT solver. We used the sklearn [5] and numpy [16] packages to train models. We conducted the experiments on a VM with 36 cores, a CPU speed of 2.4 GHz, a total of 150 GB memory, and the Ubuntu 16.04 operating system. The VM is hosted by a designated server with two Intel Xeon E52680v2 processors (each processor is made of 28 cores at 2.4 Ghz), 260 GB memory, and Red Hat Enterprise Linux Server 7.3 operating system. For tractability, we capped the runtime of verifying the local robustness property by 10 minutes. We evaluated VERIGB using the following three datasets:\n1. The House Sales in King County (HSKC) dataset containing 22K observations of houses sold in between May 2014 and May 2015 in King County, USA [14]. Each observation has 19 house features, as well as the sale price.\n1. The House Sales in King County (HSKC) dataset containing 22K observations of houses sold in between May 2014 and May 2015 in King County, USA [14]. Each observation has 19 house features, as well as the sale price. 2. The Modified National Institute of Standards and Technology (MNIST) dataset containing 70K images of handwritten digits [18]. The images are of size 28 x 28 pixels, each with a grayscale value ranging from 0 to 255. The images are classified into 10 classes, one for each digit.\n2. The Modified National Institute of Standards and Technology (MNIST) dataset containing 70K images of handwritten digits [18]. The images are of size 28 x 28 pixels, each with a grayscale value ranging from 0 to 255. The images are classified into 10 classes, one for each digit.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d6a/4d6a02d8-e119-4749-8311-fd7d2495b5d0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 2: Universal robustness eveluation for \u03f5 = 160 sq/ft, and \u03b4 = 100K$, and regressors with a similar score. Illustrating the attainable portion of robust observations \u03c1, varying the number of trees and the tree depth.\n3. The German Traffic Sign Recognition Benchmark (GTSRB) dataset containing 50K colored images of traffic signs [13]. The images are of size 32 x 32 pixels, each with three values (RGB) ranging from 0 to 255. The images are classified into 43 classes, one for each traffic sign.\n# 7.1 Regressor Evaluation\nWe start by demonstrating VERIGB\u2019s scalability to large gradient boosted regression models using the HSKC dataset. We trained regressors varying the learning rates in {0.1, 0.2, 0.3}, the number of trees between 50 and 500, and the tree depth in {3, 5, 8, 10}. All models have a similar score2 that varies between 0.84 and 0.88. Then we randomly selected 200 observations and evaluated the \u03c1-universal (\u03f5, \u03b4)-robustness property with an \u03f5 value of 160 sq/ft, for the 6 numerical features that refer to square footage, and a \u03b4 value of 100K$ in the price. Note that there were no timeouts (where it took the SMT solver more than 10 minutes to reach a decision) for models with less than 500 trees, and even with 500 trees we had only 16% timeouts. Figure 2 illustrates the results for a learning rate of 0.1, while the results for other learning rates are similar. Notice that (i) robustness degrades as the number of trees increases. (ii) robustness seems to be negatively correlated\n2The term score refers to the coefficient of determination R2 of the prediction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c56e/c56e142f-0149-4ea7-b8bd-ce297347208c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Examples of GTSRB images that satisfy the local adversarial robustness property for \u03f5 = 3.</div>\nwith the tree depth. That is, a model trained with a tree depth of 3 is more robust than a depth of 5, which is more robust than 8 and 10.\n# 7.2 Classifier Evaluation\nNext, we demonstrate VERIGB\u2019s capability to verify the robustness of accurate classification models. We trained gradient boosted models for the MNIST and GTSRB datasets with a learning rate of 0.1. We varied the number of trees between 20 and 100, and the maximal tree depth between 3 and 20. The accuracy of said models varied between 87.9% and 97.3% for MNIST, and between 90% and 96.86% for GTSRB. We evaluated the \u03c1-universal \u03f5robustness property with \u03f5 values of 1, 3, and 5. We randomly selected 20 images from each class in the training set (200 images for MNIST, and 860 images for GTSRB). The illustration in Figure 1 is an artifact of this evaluation. Recall, that it shows two examples where the local adversarial robustness property does not hold for \u03f5 = 3 for a model trained for the GTSRB dataset. In the first example, an \u201c80\u201d km/h speed limit sign is misclassified as a \u201c30\u201d km/h limit. In the second example, a \u201cturn left\u201d sign is misclassified as an \u201cahead only\u201d sign. Alternatively, Figure 3 shows examples of signs that do satisfy the local adversarial robustness property for \u03f5 = 3. That is, their classification would not change under any adversarial perturbation that changes each pixel\u2019s RGB values by at most 3. Figure 4 shows examples of handwritten digits that satisfy the local adversarial robustness property for \u03f5 = 3, for models trained for the MNIST dataset. Alternatively, Figure 5 shows two examples where the local adversarial robustness property does not hold. In the first example, an image of \u201c1\u201d is misclassified as \u201c7\u201d. The second image is\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c330/c330b3f5-c1aa-4f5f-b8b0-5bb63f731957.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Examples of MNIST images that satisfy the local adversarial robustness property for \u03f5 = 3.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a2cf/a2cf1483-1655-4ea2-b3cd-13101ec16826.png\" style=\"width: 50%;\"></div>\nFigure 5: Examples of MNIST images that do not satisfy local adversarial robustness for \u03f5 = 3. In the first row, an image of \u201c1\u201d is misclassified as \u201c7\u201d. In the second row, an image of \u201c5\u201d is misclassified as \u201c0\u201d. Observe in the third column (delta) that the applied changes are barely visible to the naked eye (delta of +/-3 in the range of 256 values per pixel per color). The fourth column highlights the modified pixels.\nmisclassified as \u201c0\u201d instead of \u201c5\u201d under very slight perturbation. These modifications are almost invisible to a human eye. Note that the model\u2019s confidence does not indicate robustness. E.g., in the first example the image has 95% confidence to be classified as 1, while after applying the perturbation, it has 90% confidence while being misclassified as 7.\n# Scalability and limitations\nTable 1 summarizes the results for selected models trained for the MNIST dataset. In the table, the abbreviations \u201cT/O\u201d and \u201cC/E\u201d stand for the portion of timeouts and counter-examples, respectively. Note that for a fixed tree\ndepth, the portion of counter-examples found is negatively correlated with the model\u2019s accuracy. This is also true for a fixed number of trees. In this example, large models with 100 trees and high tree depth already exhibit a non-negligible portion of timeouts, indicating the limitations of VERIGB. Despite that fact, it successfully verifies highly accurate models for the MNIST dataset. We run similar experiments on models trained for the GTSRB dataset, with roughly similar results. Unlike MNIST, the portion of timeouts was only 1%, even for large models. As with MNIST, the portion of counter-examples varies between 10% and 22%. Finally, the ratio of robust images varies between 78% and 88%.\nThe effect of model structure on robustness As a side-effect of this research, we noticed that certain configuration parameters tend to result in more robust models. Hereafter, we briefly discuss our observations. Table 2 summarizes selected results for models with a similar accuracy which is achieved by varying the number of trees, and the tree depth. As can be observed, models with smaller tree depth have a higher \u03c1 value. The results show that the tree depth has a potentially large impact on robustness. That is, increasing the tree depth leads to less robust results. Notice that tree depth similarly affects the robustness of regression models, as is clearly indicated in Figure 2. It is interesting to mention, that tree depth also plays a role in the over-fitting problem of gradient boosted models. Models with large tree depth are more likely to suffer from over-fitting [12]. In our context, a small tree depth yields better robustness and is also easier to verify, making VERIGB attractive for practical use cases.\n# 8 Related Work\nReliability and security are of increasing interest by the research community. Numerous works demonstrate the (lack of) security of popular machine learning models [4, 3, 2]. Others show methods to generate adversarial inputs to such models [29]. Thus, certifying that specific models are robust to adversarial inputs is an important research challenge. Indeed [22, 17, 11], introduced methods for verifying robustness for various types of neural network models. The robustness of gradient boosted mod-\nDepth\nTrees\nAccuracy\n\u03f5 = 1\n\u03f5 = 3\n\u03f5 = 5\nVerified (\u03c1)\nT/O\nC/E\nVerified (\u03c1)\nT/O\nC/E\nVerified (\u03c1)\nT/O\nC/E\n3\n20\n87.9\n16.5%\n0%\n83.5%\n10%\n0%\n90%\n10%\n0%\n90%\n3\n50\n92.4\n24%\n0%\n76%\n24%\n0%\n79%\n21%\n0%\n79%\n3\n100\n94.4\n39.5%\n0.5%\n60%\n31.5%\n0.5%\n68%\n31.5%\n0.5%\n68%\n8\n20\n94.8\n39.5%\n0%\n60.5%\n21%\n0%\n79%\n21%\n0%\n79%\n8\n50\n96.4\n53.5%\n6%\n40.5%\n40%\n9.5%\n50.5%\n42.5%\n7%\n50.5%\n8\n100\n97\n29%\n41.5%\n29.5%\n20%\n45%\n35%\n22%\n43.5%\n34.5%\n10\n20\n95.6\n39.5%\n0%\n60.5%\n25%\n0%\n75%\n25%\n0%\n75%\n10\n50\n96.7\n53%\n8.5%\n38.5%\n39.6%\n10.6%\n49.8%\n46%\n8.5%\n45.5%\n10\n100\n97.3\n15%\n60%\n25%\n10.5%\n62.5%\n27%\n11.5%\n62.5%\n26%\nTable 1: MNIST dataset: Evaluating the attainable portion of robust observations \u03c1, for models with varying number of trees, tree depth, and \u03f5. The abbreviations \u201cT/O\u201d and \u201cC/E\u201d stand for the portion of timeouts and counter-examples, respectively.\nTable 1: MNIST dataset: Evaluating the attainable portion of robust observations \u03c1, for models with varying number of trees, tree depth, and \u03f5. The abbreviations \u201cT/O\u201d and \u201cC/E\u201d stand for the portion of timeouts and counter-examples,\nDepth\nTrees\nAccuracy\nVerified (\u03c1)\nT/O\nC/E\n4\n100\n95.6\n53%\n3%\n44%\n5\n65\n95.7\n52%\n1%\n47%\n7\n40\n95.8\n52%\n0.5%\n47.5%\n10\n20\n95.6\n39.5%\n0%\n60.5%\n20\n18\n95.8\n27.5%\n0.0%\n72.5%\nTable 2: MNIST dataset: Impact of boosted model\u2019s architecture on the attainable \u03c1 for the universal adversarial robustness property with \u03f5 = 1.\nels is also of interest, but existing works are focused on empirical evaluation [19], or on training methods that increase robustness [25], while our work is the first to certify gradient boosted models with formal and rigorous analysis. Since our work is the first and only work that verifies gradient boosted model, we survey existing works that verify other machine learning models. In [15], the authors suggest an SMT based approach for verifying feedforward multi-layer neural networks. They use a white box approach to analyze the neural network layer by layer and also apply a set of methods to discover adversarial inputs. Note that gradient boosted models are fundamentally different from neural networks and thus their method does not extend to such models. In [17], the authors describe a Simplex based verification technique, that is extended to handle the non-convex Rectified Linear Unit (ReLU) activation functions. Such activation is fundamental in modern neural networks and is not expressible with linear programming. The main disadvantage of that\napproach is its inability to scale up to large networks with thousands of ReLU nodes. Alternatively, AI2 [11] uses \u201cabstract transformers\u201d to overcome the difficulty of formally describing non-linear activation functions. Safety properties such as robustness are then proved based on the abstract interpretation. The over-approximation that is inherent in the technique allows for scalable analysis. However, since they use abstractions, the counter-examples provided are not always real counter-examples, and thus a refinement process is required to end up with a concrete counter-example. Finally, the authors of [22] adapt Boolean satisfiability to verify the robustness of Binarized Neural Networks (BNN). Specifically, they apply a counter-exampleguided search procedure to check for robustness to adversarial perturbations. They verified BNN models for the MNIST dataset. In comparison, VERIGB verifies slightly more accurate gradient boosted models for the same dataset. Similarly, in [9] the authors propose a method for verification of feed-forward neural networks. Their approach leverages piece-wise linear activation functions. The main idea is to use a linear approximation of the overall network behavior that can then be solved by SMT or ILP.\n# 9 Conclusions and Future Work\nOur work is the first to verify robustness to adversarial perturbations for gradient boosted models. Such models are among the most popular machine learning techniques\nin practice. Our work introduces a model verification tool called VERIGB that transforms the challenge of certifying gradient boosted regression and classification models into the task of checking the satisfiability of an SMT formula that describes the model and the required robustness property. This novel encoding is an important contribution of our work and includes formal correctness proofs as well as performance optimizations. Once we have such an (optimized) SMT formula, we check its satisfiability with a standard solver. The solver either proves the robustness property or provides a counter-example. We extensively evaluated VERIGB, with 3 public datasets, and demonstrated its scalability to large and accurate models with hundreds of trees. Our evaluation shows that the classification\u2019s confidence does not provide a good indication of robustness. Further, it indicates that models with a small tree depth tend to be more robust even if the overall accuracy is similar. Such models are also known to suffer less from over-fitting. We believe that there may be an implicit correlation between robustness and good generalization, and leave further investigation to future work. Additionally, the counter-examples generated by VERIGB may be leveraged in the training phase of the gradient boosted models to optimize their robustness. However, we leave such usage for future work.\n# References\n[1] C. W. Barrett, R. Sebastiani, S. A. Seshia, and C. Tinelli. Satisfiability modulo theories. In Handbook of Satisfiability, pages 825\u2013885. IOS Press, 2009. [2] B. Biggio, I. Corona, B. Nelson, B. I. P. Rubinstein, D. Maiorca, G. Fumera, G. Giacinto, and F. Roli. Security Evaluation of Support Vector Machines in Adversarial Environments, pages 105\u2013153. Springer International Publishing, Cham, 2014. [3] B. Biggio, G. Fumera, and F. Roli. Pattern recognition systems under attack: Design issues and research challenges. IJPRAI, 28(7), 2014. [4] B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern classifiers under attack. IEEE Transactions on Knowledge and Data Engineering, 26(4):984\u2013996, April 2014. [5] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Prettenhofer,\nA. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varoquaux. API design for machine learning software: experiences from the scikitlearn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108\u2013 122, 2013.\n[6] R. Caruana and A. Niculescu-Mizil. An empirical comparison of supervised learning algorithms. In Proceedings of the 23rd International Conference on Machine Learning, ICML \u201906, pages 161\u2013168, New York, NY, USA, 2006. ACM. [7] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. In Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14, YLRC\u201910, pages 1\u201324. JMLR.org, 2010. [8] L. De Moura and N. Bj\u00f8rner. Z3: An efficient smt solver. In Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS\u201908/ETAPS\u201908, pages 337\u2013340, Berlin, Heidelberg, 2008. Springer-Verlag. [9] R. Ehlers. Formal verification of piece-wise linear feed-forward neural networks. CoRR, abs/1705.01320, 2017. [10] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119 \u2013 139, 1997. [11] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. T. Vechev. Ai: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy (SP), 2018. [12] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001. [13] S. Houben, J. Stallkamp, J. Salmen, M. Schlipsing, and C. Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In The 2013 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2013. [14] House Sales in King County, USA. https://www.kaggle.com/harlfoxem/housesalespredictio 2018.\n[15] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety verification of deep neural networks. In R. Majumdar and V. Kun\u02c7cak, editors, Computer Aided Verification, pages 3\u201329, Cham, 2017. Springer International Publishing. [16] E. Jones, T. Oliphant, P. Peterson, et al. SciPy: Open source scientific tools for Python, 2001. [17] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In R. Majumdar and V. Kuncak, editors, Computer Aided Verification - 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I, volume 10426 of Lecture Notes in Computer Science, pages 97\u2013117. Springer, 2017. [18] Y. LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998. [19] C. Leistner, A. Saffari, P. M. Roth, and H. Bischof. On robustness of on-line boosting - a competitive study. In 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, pages 1362\u20131369, Sept 2009. [20] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS\u201999, pages 512\u2013518, Cambridge, MA, USA, 1999. MIT Press. [21] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 86\u201394, July 2017. [22] N. Narodytska, S. P. Kasiviswanathan, L. Ryzhyk, M. Sagiv, and T. Walsh. Verifying properties of binarized deep neural networks. In S. A. McIlraith and K. Q. Weinberger, editors, Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence,\nNew Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press, 2018. [23] L. Pulina and A. Tacchella. An abstraction-refinement approach to verification of artificial neural networks. In Proceedings of the 22Nd International Conference on Computer Aided Verification, CAV\u201910, pages 243\u2013257, Berlin, Heidelberg, 2010. Springer-Verlag. [24] L. Pulina and A. Tacchella. Challenging smt solvers to verify neural networks. AI Communications, 25(2):117\u2013135, Apr. 2012. [25] Y. Sun, S. Todorovic, and J. Li. Increasing the robustness of boosting algorithms within the linearprogramming framework. The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology, 48(1):5\u201320, Aug 2007. [26] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, volume 1, pages I\u2013511\u2013I\u2013518 vol.1, 2001. [27] B. Yang, J. Yan, Z. Lei, and S. Z. Li. Convolutional channel features for pedestrian, face and edge detection. CoRR, abs/1504.07339, 2015. [28] Y. Zhang and A. Haghani. A gradient boosting method to improve travel time prediction. In Transportation Research Part C Emerging Technologies, volume 58, 03 2015. [29] Y. Zhou, M. Kantarcioglu, B. Thuraisingham, and B. Xi. Adversarial support vector machine learning. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, pages 1059\u20131067, New York, NY, USA, 2012. ACM.\n",
    "paper_type": "method",
    "attri": {
        "background": "Gradient boosted models are fundamental in machine learning and are known to achieve good accuracy with relatively small models. However, they are vulnerable to adversarial perturbations, which can lead to misclassification. There is a lack of formal methods to verify the robustness of these models, which is critical for ensuring their reliability in high-stakes applications.",
        "problem": {
            "definition": "The paper aims to address the challenge of verifying the robustness of gradient boosted models against small input perturbations that could lead to misclassification.",
            "key obstacle": "Existing methods for verifying robustness are not applicable to gradient boosted models, and there is uncertainty about how configuration parameters affect robustness."
        },
        "idea": {
            "intuition": "The idea emerged from the need to formalize the verification process for gradient boosted models, similar to what has been done for neural networks.",
            "opinion": "The proposed solution, VERIGB, aims to encode the model and its robustness properties into SMT formulas, enabling the use of state-of-the-art verification tools.",
            "innovation": "VERIGB introduces a novel encoding method that translates gradient boosted models and their robustness properties into SMT formulas, which can then be verified for robustness."
        },
        "method": {
            "method name": "VERIGB",
            "method abbreviation": "VERIGB",
            "method definition": "VERIGB is a tool that verifies the robustness of gradient boosted models by encoding the model and its robustness properties as SMT formulas.",
            "method description": "The core of VERIGB involves translating gradient boosted models into SMT formulas and using an SMT solver to check for robustness.",
            "method steps": [
                "Translate the gradient boosted model into an SMT formula.",
                "Define the robustness properties as SMT formulas.",
                "Use an SMT solver to check the satisfiability of the combined formula."
            ],
            "principle": "The effectiveness of VERIGB lies in its ability to mathematically represent the model and robustness properties, allowing for rigorous verification using established SMT solving techniques."
        },
        "experiments": {
            "evaluation setting": "VERIGB was evaluated on three publicly available datasets: House Sales in King County, MNIST, and GTSRB, using a VM with 36 cores and a runtime cap of 10 minutes for each verification.",
            "evaluation method": "The performance of VERIGB was assessed by measuring the portion of robust observations (\u03c1) and the occurrence of timeouts and counter-examples during the robustness verification process."
        },
        "conclusion": "VERIGB is the first tool to verify the robustness of gradient boosted models. The experiments demonstrated its scalability and effectiveness in certifying robustness, revealing that models with smaller tree depths tend to be more robust.",
        "discussion": {
            "advantage": "VERIGB provides a formal and rigorous method for verifying the robustness of gradient boosted models, filling a significant gap in existing verification techniques for machine learning.",
            "limitation": "The method may encounter challenges with larger models, as indicated by the occurrence of timeouts during verification, especially with deeper trees.",
            "future work": "Future research could explore the correlation between model robustness and generalization, as well as the potential for using counter-examples generated by VERIGB to improve model training."
        },
        "other info": {
            "additional details": {
                "datasets": [
                    "House Sales in King County (22K observations)",
                    "MNIST (70K images of handwritten digits)",
                    "GTSRB (50K images of traffic signs)"
                ],
                "performance metrics": {
                    "timeout percentage": "Varied by dataset and model complexity",
                    "accuracy range": "87.9% to 97.3% for MNIST, 90% to 96.86% for GTSRB"
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "5.2",
            "key information": "The paper discusses how adversarial perturbations can lead to misclassification in gradient boosted models, highlighting the impact of adversarial examples on model robustness."
        },
        {
            "section number": "5.3",
            "key information": "The proposed solution, VERIGB, aims to enhance robustness against adversarial examples by formalizing the verification process for gradient boosted models."
        },
        {
            "section number": "6.1",
            "key information": "The paper introduces VERIGB as a tool that verifies the robustness of gradient boosted models by encoding the model and its robustness properties as SMT formulas, which is relevant for quantifying uncertainty in model robustness."
        },
        {
            "section number": "7.1",
            "key information": "The paper identifies the limitation that existing methods for verifying robustness are not applicable to gradient boosted models, indicating current challenges in OOD detection methods."
        },
        {
            "section number": "7.2",
            "key information": "Future research directions suggested in the paper include exploring the correlation between model robustness and generalization, which aligns with potential advancements in OOD detection."
        }
    ],
    "similarity_score": 0.6152908977616303,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Verifying Robustness of Gradient Boosted Models.json"
}