{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1912.00947",
    "title": "Restoring Chaos Using Deep Reinforcement Learning",
    "abstract": "A catastrophic bifurcation in non-linear dynamical systems, called crisis, often leads to their convergence to an undesirable non-chaotic state after some initial chaotic transients. Preventing such behavior has proved to be quite challenging. We demonstrate that deep Reinforcement Learning (RL) is able to restore chaos in a transiently-chaotic regime of the Lorenz system of equations. Without requiring any a priori knowledge of the underlying dynamics of the governing equations, the RL agent discovers an effective perturbation strategy for sustaining the chaotic trajectory. We analyze the agent's autonomous control-decisions, and identify and implement a simple control-law that successfully restores chaos in the Lorenz system. Our results demonstrate the utility of using deep RL for controlling the occurrence of catastrophes and extreme-events in non-linear dynamical systems.",
    "bib_name": "vashishtha2019restoringchaosusingdeep",
    "md_text": "# Restoring Chaos Using Deep Reinforcement Learning\nSumit Vashishtha\u2217and Siddhartha Verma\u2020 Department of Ocean and Mechanical engineering, Florida Atlantic University, Boca Raton, FL 33431, USA (Dated: December 3, 2019)\nA catastrophic bifurcation in non-linear dynamical systems, called crisis, often leads to their convergence to an undesirable non-chaotic state after some initial chaotic transients. Preventing such behavior has proved to be quite challenging. We demonstrate that deep Reinforcement Learning (RL) is able to restore chaos in a transiently-chaotic regime of the Lorenz system of equations. Without requiring any a priori knowledge of the underlying dynamics of the governing equations, the RL agent discovers an effective perturbation strategy for sustaining the chaotic trajectory. We analyze the agent\u2019s autonomous control-decisions, and identify and implement a simple control-law that successfully restores chaos in the Lorenz system. Our results demonstrate the utility of using deep RL for controlling the occurrence of catastrophes and extreme-events in non-linear dynamical systems.\n 27 Nov 2019\n27 Nov\nChaos is desirable and advantageous in many situations. For instance, in mechanics, exciting the chaotic motion of several modes spreads energy over a wide frequency range [1], thereby preventing undesirable resonance. Chaotic advection in fluids enhances mixing, as chaos brings about an exponential divergence of fluid packets that are initially in close proximity [2]. In biology, the absence of chaos may lead to an emergence of synchronous dynamics in the brain, which can result in epileptic seizures [3]. Moreover, the absence of chaos may also indicate the presence of other pathological conditions [4, 5]. In some cases, Chaos can become transient in nature, where the dynamics eventually converge to non-chaotic attractors. The typical route by which this happens is known as a crisis [6], where for certain parametervalues of the non-linear system, a chaotic-attractor collides with its basin-boundary and becomes a saddle. A saddle has a fractal structure with infinitely many gaps along its unstable-manifold. Any initial condition attracted towards this chaotic-attractor-turned-saddle escapes to an external periodic- or a fix-point-attractor. Such transient-chaos is often undesirable, and has been conjectured to be the culprit for phenomena such as voltage collapse in electric power systems [7] and species extinction in ecology [8]. It also plays a crucial role in governing the dynamics of shear flows in pipes and ducts at low Reynolds numbers [9, 10]. Given the importance of these phenomena, controlling transient-chaos is a pressing issue. Some attempts to restore chaos in such scenarios have been made in the past. Yang et al. [5] maintained chaos in transiently chaotic regimes of one- and two-dimensional maps using small perturbations. Their method relied on accurate analytical knowledge of the dynamical system, and required a priori phase-space knowledge of escape regions from chaos. Another method utilized the natural dynamics around the saddle [11, 12], where small regions near a chaotic-saddle through which trajectories escape\nwere identified. Then a set of \u201ctarget\u201d points in these regions were found, which yield trajectories that can stay near the chaotic saddle for a relatively long time. When the solution trajectory falls in this escape region, it is perturbed to the nearest target point so that the trajectory can persist near the chaotic saddle for a long time. The identification of such escape regions and target points can be challenging, and requires either an a priori computation of the probability distribution of escape times in different regions of state-space [11], or information from the return map constructed from local maxima or minima of a measured time series [12]. Such approaches become difficult for high-dimensional dynamical systems, and have been illustrated for 2D maps/flows at the most. One particular control technique that worked for the 3D Lorenzsystem was described in Cape\u00b4ans et al. [13]. The method was based on finding a certain control-perturbation set in the phase space, called a \u201csafe set\u201d, which avoids the escape of the trajectories to the fix-points. Identifying such a safe set can be prohibitively expensive computationally, and such safe sets may not exist for all dynamical systems. In recent years, a machine-learning technique called deep Reinforcement Learning (RL) has shown great promise in control-optimization problems [14], and it has been successfully used to uncover complex underlying physics in Navier-Stokes simulations of fishswimming [15]. The aim of this letter is to illustrate the utility of deep RL in determining small controlperturbations to parameters of the Lorenz system [16], such that a sustained chaotic behavior is maintained despite the uncontrolled dynamics being transientlychaotic. In doing so, no prior analytical knowledge about the dynamical system, and no special schemes to find escape regions, target points and safe sets will be employed. The RL algorithm is able to autonomously determine an optimal strategy to restore chaos, by continually interacting with the dynamical-system. As depicted in Fig. 1, a reinforcement learning prob-\nlem consists of five major elements - a learning agent, an environment described by a model Y (the Lorenz system in our case), state-space S, action-space A, and reward rt. Initially, the RL agent interacts with its environment in a trial-and-error manner. At each time step t, the agent receives the current state st of the environment, and selects an action at following a policy \u03a0(at|st). This action allows the agent to perturb the state of the environment, and move to a new state st+1 by evaluating the given model Y of the environment. Upon affecting this transition, the agent is rewarded (or punished) with reward rt. This process continues until the agent reaches a terminal state, at which point a new episode starts over. The return received from each episode is the discounted cumlative reward with discount factor \u03b3, which lies between 0 and 1. The discount factor makes it feasible to emphasize the importance of maximizing long-term reward, which enables the agent to prefer actions that are beneficial in the long-term. The cumulative reward, R(at|st), is given as,\n(1)\nThe goal of the RL agent is to maximize this cumulative reward by discovering an optimal policy \u03a0\u2217. There are a variety of methods available for attaining this. We make use of Proximal Policy Optimization (PPO) [17] which is a type of policy Gradient Method (PGM) [18]. PPO is suitable for continuous-control problems [19], and it is simpler in its mathematical implementation compared to other PGM based RL algorithms [20]. Moreover, PPO requires comparitively little hyper-parameter tuning for use in a variety of different problems. The specific implementation of the algorithm that we used, PPO2, is available as part of the OpenAI stable-baselines library [21]. The ergodic and unsteady nature of chaotic dynamics necessitates the use of a version of PPO2 wherein the policy is defined by deep recurrent neural networks comprised of long-short-term memory cells, instead of traditional feed-forward neural networks. The environment for the Lorenz system is written in a OpenAI gym [22] -compatible python format, and is provided as part of the supplementary materials. The relevant equations are given as,\n(2a)\n(2b) (2c)\n(2c)\nWith \u03c3 = 10 and \u03b2 = 8/3, \u03c1 = 28 gives rise to chaotic trajectories, whereas transient chaos is found in the interval \u03c1 \u2208[13.93, 24.06] [23]. Without any control implemented, the solution will converge to specific fix-points\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4da/b4da2e84-7feb-4582-a229-bc0dd80c94c3.png\" style=\"width: 50%;\"></div>\nFIG. 1. Schematic illustrating the basic framework of a reinforcement learning problem. An agent continually perturbs the environment (the Lorenz equations in our case) by taking an action, and records the resulting states. The agent is rewarded when a desirable state is reached, and punished otherwise.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2993/29937894-e6c0-459b-94c5-8e2e568fb354.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8513/8513849b-6f6d-45f1-8ac0-b1cf52999f3a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b)</div>\nFIG. 2. Solution of the Lorenz system of equations in (a) the chaotic regime with \u03c1 = 28, and (b) the meta-stable chaotic regime with \u03c1 = 20. Note that the solution traverses a chaotic trajectory in the first case, whereas it converges to P\u2212after a few chaotic transients in the second case.\nafter a short transient, as shown in Fig. 2. The two fixpoints in our case are given by P+ = (7.12, 7.12, 19) and P\u2212= (\u22127.12, \u22127.12, 19). We use reinforcement learning to prevent such a transient from chaotic- to fix-point-solutions. This is done by perturbing the parameters in Eqs. 2 (\u03c1 = (\u03c3, \u03c1, \u03b2)) by \u2206\u03c1 = (\u2206\u03c3, \u2206\u03c1, \u2206\u03b2), with \u2206\u03c1 \u2208[\u2212\u03c1/10, \u03c1/10]. The instantaneous value of the solution vector X(t) = (x, y, z) and its time-derivative (velocity) \u02d9X(t) = (Vx(t), Vy(t), Vz(t)) = ( dx dt , dy dt , dz dt ) constitute the state space S for the RL algorithm. For training the RL agent to retain a chaotic trajectory, we utilize the fact that\n|V (t)| will decrease consistently as the solution converges to one of the fix-points, eventually becoming zero. On the other hand, |V (t)| will have a non-zero average value when the solution traces the chaotic attractor. Thus, whenever the agent determines suitable action values \u2206\u03c1 for which |V (t)| is maintained above the predefined threshold value V0 = 40, it is rewarded, otherwise it is punished. In doing so over several iterations, the agent eventually learns to keep the trajectory chaotic. The reward allocated to the agent consists of two parts: a stepwise reward rt provided at each time step, and a one-time terminal reward rterminal given at the end of each episode. The two terms take the following form,\n(3a)\n(3b)\nThe average \u00afrt is defined over the last 2000 time steps of an episode, and facilitates learning to keep the trajectory chaotic over long periods of time. The training of the agent is divided into episodes of 4000 time steps each, with time step size dt = 2e\u22122. The RL agent is expected to learn suitable action values \u2206\u03c1 for any state permissible by the system environment, such that the long-term reward accumulated is maximized. Fig. 3 il-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d967/d967c6cb-4840-4fbf-9b7e-7caaa9527820.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1200 1400 1600 1800 2000 2200 2400 2600 Training time</div>\nFIG. 3. Training of the RL agent with time. Note that the solution is non-chaotic until around t = 2000, beyond which the agent is able to take effective decisions to keep the solutiontrajectory chaotic for further instances. lustrates the training of the RL agent with time. The underlying neural network is trained for 2 \u00d7 105 time steps, which corresponds to 50 independent episodes in total, with each episode beginning with random values of the state variables X between -40 and 40; the corresponding values for \u02d9X are determined using the Lorenz equations (Eqs. 2). Initially, the solution keeps converging to the fix-points, since the network is unable to provide optimal action-decisions. After the network has trained for some time, it successfully learns the optimal actions for keeping the value of |V (t)| above V0. As a consequence, the agent learns that the best way of maximizing reward is\nby maintaining the dynamics over the chaotic-attractor, which, although non-attracting for the given set of parameters, is a natural solution of the system.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d30/1d30b18f-986e-4398-9946-1422a1e5f49b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/85fc/85fcab65-0fcb-4b46-9966-dc32a9420813.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b)</div>\nFIG. 4. (a) Distribution of the perturbation \u2206\u03c1 learned by the RL agent to keep the dynamics on the chaotic-attractor. The red dots indicate locations where the perturbation values are positive, and the blue dots correspond to negative values. (b) Velocity vectors for the corresponding solution in the state space. Note that \u2206\u03c1 is predominantly negative in the region \u211cwhere Vz < 0.\nFigure 4 shows the distribution of the perturbations \u2206\u03c1 employed by the trained agent, which allow it to keep the dynamics on the chaotic-attractor. This distribution was obtained by plotting the controlled-trajectories for 400 random initial values for the variables x, y and z, lying between -40 and 40. Note that a similar distribution was obtained for the other perturbations \u2206\u03b2 and \u2206\u03c3. However, we find that an execution of the converged RL control-policy with \u2206\u03b2 and \u2206\u03c3 explicitly set to zero does not make a difference in the control outcome; the agent is still able to maintain a chaotic trajectory. This may be attributed to the dominating magnitude of the parameter \u03c1 compared to the other two parameters. As depicted in Fig. 4(a,b), the perturbation values are predominantly negative in the region \u211cwhere Vz < 0 and positive elsewhere. The success of this control-policy \u03a0\u2217in keeping the trajectory over the chaotic-attractor can be explained using the sensitivity of the solutions of Eqs.2 to the perturbation \u2206\u03c1. For x < 0 in \u211c, a negative \u2206\u03c1 makes Vy in Eq.(2b) more positive, which in turn makes y and hence Vx in Eq.(2a) more positive, thus drifting the trajectory away from the fix-point P\u2212. Similarly, when x > 0 in \u211c, a negative \u2206\u03c1 makes Vy in Eq.(2b) more negative, which in turn makes y and hence Vx more negative, thereby drifting the trajectory away\nfrom the fix-point P+. The role of positive perturbations outside \u211cin avoiding the escape of the trajectory from the chaotic attractor to the fix-points can be explained likewise. Positive perturbations of \u03c1 lead to an increase in Vy (Eq. 2b). The subsequent increase in y leads to an increase in Vx (Eq. 2a), and the higher values of x and y lead to an increase in Vz (Eq. 2c). The overall effect is to increase the speed, which prevents the trajectory from spiralling in to the fix-points.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ceec/ceec3c1d-a9dd-4e5b-b6cc-a9dfc5aac747.png\" style=\"width: 50%;\"></div>\nFIG. 5. Comparison of the trajectory with and without the application of rule-based control. The blue trajectory corresponds to the controlled solution, starting from the initial condition Q = (10, 15, 35). The yellow uncontrolled solution starts from the same initial condition, and spirals in to the fix-point P\u2212.\nBased on this strategy, we formulate a simple rulebased controller which perturbs the parameter \u03c1 by \u2212\u03c1/10 whenever the trajectory visits the region \u211c, i.e., whenever Vz < 0. All parameters remain unperturbed outside this region. The success of the rule-based binarycontrol is demonstrated in Fig. 5, where the uncontrolled trajectory (yellow) converges to the fix-point, whereas its controlled counterpart (blue) remains chaotic. Note that, unlike other control-techniques, RL-based control requires no a-priori analytical knowledge about the dynamical system regarding its escape-regions, target-points and safe-sets. The RL agent learns the optimal strategy \u03a0\u2217to prevent the transition from chaotic to fix-point solutions completely autonomously, by continually interacting with the environment defined by the Lorenz system of equations exhibiting transient-chaos. To conclude, we have demonstrated the utility of deep reinforcement learning in restoring chaos for a transiently-chaotic system. Since, transient chaos is a consequence of a catastrophic bifurcation (crisis) [24], our results pave the way for RL enabled control of extremeevents and catastrophes in non-linear dynamical systems.\n\u2020 vermas@fau.edu [1] I. T. Georgiou and I. B. Schwartz, Int. J. Bifurcation Chaos 6, 673 (1996). [2] H. Haken, in Chaos and order in nature (Springer, 1981) pp. 2\u201311. [3] J. C. Sackellares, L. D. Iasemidis, D.-S. Shiau, R. L. Gilmore, and S. N. Roper, in Chaos in Brain? (World Scientific, 2000) pp. 112\u2013133. [4] A. L. Goldberger, D. R. Rigney, and B. J. West, Sci. Am. 262, 42 (1990). [5] W. Yang, M. Ding, A. J. Mandell, and E. Ott, Phys. Rev. E 51, 102 (1995). [6] C. Grebogi, E. Ott, and J. A. Yorke, Physica D: Nonlinear Phenomena 7, 181 (1983). [7] I. Dobson and H.-D. Chiang, Systems & Control Lett. 13, 253 (1989). [8] K. McCann and P. Yodzis, The American Naturalist 144, 873 (1994). [9] K. Avila, D. Moxey, A. de Lozar, M. Avila, D. Barkley, and B. Hof, Science 333, 192 (2011). [10] T. Kreilos, B. Eckhardt, and T. M. Schneider, Phys. Rev. Lett. 112, 044503 (2014). [11] I. B. Schwartz and I. Triandaf, Phys. Rev. Lett. 77, 4740 (1996). [12] M. Dhamala and Y.-C. Lai, Phys. Rev. E 59, 1646 (1999). [13] R. Cape\u00b4ans, J. Sabuco, M. A. Sanju\u00b4an, and J. A. Yorke, Philos. Trans. R. Soc. London, Ser. A 375, 20160211 (2017). [14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., Nature 529, 484 (2016). [15] S. Verma, G. Novati, and P. Koumoutsakos, Proc. Natl. Acad. Sci. USA 115, 5849 (2018). [16] E. N. Lorenz, The essence of chaos (University of Washington Press, 1995). [17] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, arXiv preprint arXiv:1707.06347 (2017). [18] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (MIT press, 2018). [19] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Nature 518, 529 (2015). [20] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, in ICML (2015) pp. 1889\u20131897. [21] A. Hill, A. Raffin, M. Ernestus, A. Gleave, R. Traore, P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu, \u201cStable baselines,\u201d https://github.com/ hill-a/stable-baselines (2018). [22] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, arXiv preprint arXiv:1606.01540 (2016). [23] J. L. Kaplan and J. A. Yorke, Comm. Math. Phys. 67, 93 (1979). [24] W.-X. Wang, R. Yang, Y.-C. Lai, V. Kovanis, and C. Grebogi, Phys. Rev. Lett. 106, 154101 (2011).\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of restoring chaos in transiently-chaotic regimes of non-linear dynamical systems, particularly the Lorenz system, which often converges to undesirable non-chaotic states after initial chaotic transients. Previous methods have struggled to maintain chaos due to their reliance on accurate analytical knowledge and computationally expensive processes.",
        "problem": {
            "definition": "The problem involves preventing the transition from chaotic to fix-point solutions in the Lorenz system, where transient chaos leads to convergence to non-chaotic attractors.",
            "key obstacle": "The main challenge lies in the difficulty of identifying escape regions and target points for perturbation, especially in high-dimensional dynamical systems."
        },
        "idea": {
            "intuition": "The idea was inspired by the potential of deep reinforcement learning (RL) to autonomously discover effective perturbation strategies without requiring prior knowledge of the system's dynamics.",
            "opinion": "The proposed method involves using deep RL to maintain chaotic behavior in the Lorenz system by learning optimal control strategies through interaction with the system.",
            "innovation": "The key innovation is the application of deep RL, which allows for the autonomous determination of perturbations to sustain chaos, unlike previous methods that required extensive prior knowledge and computation."
        },
        "method": {
            "method name": "Deep Reinforcement Learning for Chaos Restoration",
            "method abbreviation": "DRL-CR",
            "method definition": "This method employs deep reinforcement learning to continuously perturb the parameters of the Lorenz system, keeping the trajectory within chaotic dynamics.",
            "method description": "The core of the method involves an RL agent learning to apply perturbations to maintain chaos in a transiently-chaotic regime of the Lorenz system.",
            "method steps": [
                "Define the state space and action space for the RL agent.",
                "Initialize the RL agent and environment based on the Lorenz system.",
                "Implement the Proximal Policy Optimization (PPO) algorithm for training the agent.",
                "Train the agent through episodes of interaction with the environment, applying perturbations based on learned policies.",
                "Evaluate the performance of the agent in maintaining chaotic behavior."
            ],
            "principle": "The method is effective due to the RL agent's ability to learn optimal perturbations that maintain a non-zero average velocity in the chaotic trajectory, preventing convergence to fix-points."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involves the Lorenz system with parameters set to induce chaotic and transient chaotic regimes, with training episodes designed to test the agent's ability to maintain chaos.",
            "evaluation method": "The performance of the method is assessed by monitoring the velocity of the trajectory and its ability to remain above a predefined threshold, with rewards allocated based on maintaining chaotic dynamics."
        },
        "conclusion": "The results demonstrate that deep reinforcement learning can successfully restore chaos in transiently-chaotic systems, providing a novel approach to controlling extreme events and catastrophes in non-linear dynamical systems.",
        "discussion": {
            "advantage": "The proposed approach stands out due to its autonomy and lack of requirement for prior analytical knowledge, making it applicable to complex, high-dimensional systems.",
            "limitation": "One limitation is the potential computational cost associated with training the RL agent, which may require significant time and resources.",
            "future work": "Future research could explore the application of this method to other non-linear dynamical systems and investigate improvements in training efficiency and effectiveness."
        },
        "other info": {
            "info1": "The RL agent utilizes deep recurrent neural networks for policy definition.",
            "info2": {
                "info2.1": "The training involves 50 independent episodes, each with 4000 time steps.",
                "info2.2": "The method can be implemented using the OpenAI stable-baselines library."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The proposed method involves using deep reinforcement learning to maintain chaotic behavior in the Lorenz system by learning optimal control strategies through interaction with the system."
        },
        {
            "section number": "2.1",
            "key information": "This paper addresses the issue of restoring chaos in transiently-chaotic regimes of non-linear dynamical systems, particularly the Lorenz system, which often converges to undesirable non-chaotic states after initial chaotic transients."
        },
        {
            "section number": "4.1",
            "key information": "The core of the method involves an RL agent learning to apply perturbations to maintain chaos in a transiently-chaotic regime of the Lorenz system."
        },
        {
            "section number": "6.2",
            "key information": "One limitation is the potential computational cost associated with training the RL agent, which may require significant time and resources."
        },
        {
            "section number": "7.1",
            "key information": "Future research could explore the application of this method to other non-linear dynamical systems and investigate improvements in training efficiency and effectiveness."
        }
    ],
    "similarity_score": 0.525105806232175,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0928_,arti/papers/Restoring Chaos Using Deep Reinforcement Learning.json"
}