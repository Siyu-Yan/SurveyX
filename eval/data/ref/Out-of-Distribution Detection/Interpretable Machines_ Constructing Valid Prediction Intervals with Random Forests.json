{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2103.05766",
    "title": "Interpretable Machines: Constructing Valid Prediction Intervals with Random Forests",
    "abstract": "An important issue when using Machine Learning algorithms in recent research is the lack of interpretability. Although these algorithms provide accurate point predictions for various learning problems, uncertainty estimates connected with point predictions are rather sparse. A contribution to this gap for the Random Forest Regression Learner is presented here. Based on its Out-of-Bag procedure, several parametric and non-parametric prediction intervals are provided for Random Forest point predictions and theoretical guarantees for its correct coverage probability is delivered. In a second part, a thorough investigation through Monte-Carlo simulation is conducted evaluating the performance of the proposed methods from three aspects: (i) Analyzing the correct coverage rate of the proposed prediction intervals, (ii) Inspecting interval width and (iii) Verifying the competitiveness of the proposed intervals with existing methods. The simulation yields that the proposed prediction intervals are robust towards non-normal residual distributions and are competitive by providing correct coverage rates and comparably narrow interval lengths, even for comparably small samples.",
    "bib_name": "ramosaj2021interpretablemachinesconstructingvalid",
    "md_text": "# Interpretable Machines: Constructing Valid Prediction Intervals with Random Forests\nBurim Ramosaj\u2217\nDepartment of Statistics Institute of Mathematical Statistics and Applications in Industry TU Dortmund University 44227 Dortmund, Germany\n9 Mar 2021\nAbstract\nAn important issue when using Machine Learning algorithms in recent research is the lack of interpretability. Although these algorithms provide accurate point predictions for various learning problems, uncertainty estimates connected with point predictions are rather sparse. A contribution to this gap for the Random Forest Regression Learner is presented here. Based on its Out-of-Bag procedure, several parametric and nonparametric prediction intervals are provided for Random Forest point predictions and theoretical guarantees for its correct coverage probability is delivered. In a second part, a thorough investigation through MonteCarlo simulation is conducted evaluating the performance of the proposed methods from three aspects: (i) Analyzing the correct coverage rate of the proposed prediction intervals, (ii) Inspecting interval width and (iii) Verifying the competitiveness of the proposed intervals with existing methods. The simulation yields that the proposed prediction intervals are robust towards non-normal residual distributions and are competitive by providing correct coverage rates and comparably narrow interval lengths, even for comparably small samples. Keywords: Interpretable Machine Learning, Accurate Coverage, Prediction Interval, Random Forest, Regression Learning\narXiv:2103.05766v\n# 1. Introduction\nSeveral prediction tasks using bagging and boosting procedures among Machine Learning (ML) algorithms have revealed favourable point predictions. Examples can be found in [1], [2], [3], [4] or [5], where point predictions have been constructed accurately when measured through L2-error loss, the area-under-the-ROC-curve (AUC), the mis-classification error or other measures for evaluating prediction accuracy. The restriction to bagging and boosting as a class of ML algorithms has several advantages: the methods make use of easy to construct weak learners such as decision trees and aggregate them through a weighted voting scheme or by introducing bootstrap methods. They usually result into methods, that have few hyperparameters to tune and can be trained in reasonable time complexity, while significantly increasing prediction power compared to their single learners. Especially the Random Forest method, which is an ensemble of randomized trees,\n\u2217Corresponding Author: Burim Ramosaj Email address: burim.ramosaj@tu-dortmund.de\nPreprint submitted to Journal of LATEX Templates\nhave shown favourable results from three aspects: accurate point predictions, less tuning efforts and the ability to select important features, see e.g. [6], [7], [3], [8] or [9] . These trends have made recent research in statistical ML to focus on the derivation of statistical properties for such models. In [10], for example, the authors proposed a modified Random Forest model in order to establish consistency and derive central limit type theorems for these modified Random Forest models. In [11], for example, the authors focused on the original version of Breiman\u2019s Random Forest [6] and could show under conditions such as additive regression functions that the Random Forest regression learner is L2-consistent. In several other authors works such as in [12] or [13], for example, a connection between a Random Forest learner and incomplete U-statistics have been established. This, for the reason to make use of established central limit type theorems in order to ease the way to statistical testing procedures. The latter is closely connected to the quantification of uncertainty in point predictions, see e.g. [14]. The theoretical work conducted so far can have severe implications to the general use of ML-based methods. While applicants of such methods have criticized ML-based methods for being not interpretable in terms of lacking the involvement of uncertainty estimates in prediction tasks, the recent research trend indicates that interpretable ML is an active issue in statistical ML, see e.g. [15], [16], [17] or [18]. It is aimed to contribute to the general issue of interpretable ML from a statistical perspective through the construction of valid prediction intervals, that are mainly based on the Random Forest method. The construction of such intervals enables an answer to the general question researchers and practitioners have been stating [16]: How trustworthy is the ML-based algorithm in its point-prediction for future observations? The access to a certain range for point-predictions within a given certainty delivers a potential awnser to this question. The construction of such intervals, however, requires statistical properties such as consistency of the underlying ML algorithm. As mentioned previously, consistency results have already been established such that the gap towards the construction of point prediction intervals can be settled up based on these findings. Regarding the construction of prediction intervals using the Random Forest, recent findings have been made in [19], [20] or [21]. Therein, the authors mainly derive point prediction intervals based on the Random Forest from a simulation based perspective. In [19], for example, the authors recommended the construction of uncertainty estimates in geographical maps, for e.g. forest fire behaviour. In [20], a special focus has been put on Quantile Regression Forest, the linear quantile regression and modifications of the Random Forest. Therein, the authors conducted numerical experiments and recommended not to use Quantile Regression Forests for the construciton of point-prediction intervals. Regarding the latter, a theoretical work has been given in [22], where the authors delivered theoretical guarantees for the validity of quantile based prediction intervals using the Random Forest. However, assumptions regarding the tree construction process have been set, that are rarely met when using Breiman\u2019s Random Forest method [6] as implemented in the R-package randomForest. Therefore, obtaining correct coverage rates under the Quantile Regression Forest scheme as given in [22] remains unclear and will be investigated within an extensive simulation study in this work. In addition to the stated examples, the authors in [21] proposed several prediction intervals\ndistinguishing between parametric- and non-parametric designs. Theoretical guarantees for its correct coverage are still lacking there. This is in contrast to the work of [23], where the authors constructed both, a theoretical framework for the validity of Random Forest based prediction intervals. In its core, they propose to use empirical quantiles of the Random Forest method that has been computed on the Out-of-Bag set. Under the assumptions of L2-consistent Out-of-Bag Random Forests, they deliver theoretical guarantees for several types of coverage rates for prediction intervals. This work is focused on the derivation of theoretical guarantees for Random Forest based prediction intervals that have been i) constructed parametrically and ii) non-parametrically. Compared to the work of [23], assumptions are relaxed while still maintaining correct coverage rates. Furthermore, an additional Random Forest Out-of-Bag prediction interval is proposed, that is of parametric nature and give theoretical guarantees for their correct (asymptotic) coverage. In its core, it is based on consistently estimating residual variance in regression learning problems. The latter can be considered as a non-trivial underpinning and has been partly investigated in e.g. [24]. Additional residual variance estimators are proposed that correct for the finite choices of decision trees in the ensemble. In addition to these findings, an extensive simulation study is conducted indicating the good performance of the proposed method for obtaining parametric prediction intervals based on the Random Forest Out-of-Bag prediction. Therein, both, coverage rates and interval lengths for several types of prediction intervals were analyzed, as classified in [23]. This work is structured as follows: In Section 2, the general model framework is introduced, which is restricted to the univariate-response regression. A general, theoretical framework is derived, under which (asymptotically) valid prediction intervals can be obtained and a formal definition for different types of prediction intervals is set, similar to [23]. In Section 3, the Random Forest regression method and the idea of using Out-of-Bag observations in the forest is shortly introduced. Furthermore different types of residual variance estimators are established and their consistency is proven. This way, potential sources having impact on the speed of converges are identified and corrected for. Based on these findings, several parametric prediction intervals are proposed. In Section 4, alternative methods such as the XGBoost or the Stochastic Gradient Tree Boosting method together with the Quantile Regression Forest method are considered for potential benchmarking with Random Forest based prediction intervals. The idea is to establish an analogy to the Random Forest based prediction intervals to boosting methods. The simulation design and framework are then presented in Section 5, while Section 6 briefly summarize the main findings in this simulation. In the last section, the main results are summarized and a short overview on future research in this field is given. Note that this paper consists of supplementary material covering all proofs for the theoretical guarantees and additional simulation results.\n# 2. Model Framework and Valid Prediction Intervals\nThroughout the paper, assume that one has access to a set Dn = {[X\u22a4 i , Yi]\u22a4\u2208Rn\u00d7(p+1) : i = 1, . . . , n} consisting of iid random vectors with metric outcome Y d= Y1 \u2208R. Assuming furthermore that the relation between the response Y and the covariate X d= X1 can be described through a measurable function m : Rp \u2212\u2192R with\nYi = m(Xi) + \u01ebi\nfor all i = 1, . . . , n, where {\u01ebi}n i=1 is a sequence of iid random variables with E[\u01eb1] = 0 and V ar(\u01eb1) = \u03c32 \u2208(0, \u221e) independent of {Xi}n i=1. Prediction intervals usually cover the aspect of deriving a measurable region in form of an interval, for which the target at new and unseen feature input X0, i.e. Y (X0), lies in with a pre-specified certainty, say 1 \u2212\u03b1 \u2208(0, 1). In order to formally define the random interval-type set, it is required to distinguish, similarly to [23], between different types of prediction intervals. Note that the obtained interval is potentially random and it is denoted by Cn,1\u2212\u03b1 = C(Dn, 1 \u2212\u03b1) to emphasize its dependence towards the data set Dn and the error rate \u03b1. Definition 1. Consider the regression model in (1) and fix \u03b1 \u2208(0, 1). Assume furthermore that X0 d= X1 is unseen. The interval-type set Cn,1\u2212\u03b1 is called a prediction interval of 1. type-I, if P[Y (X0) \u2208Cn,1\u2212\u03b1] \u22651 \u2212\u03b1 holds. 2. type-II, if P[Y (X0) \u2208Cn,1\u2212\u03b1|Dn] \u22651 \u2212\u03b1 holds almost surely. 3. type-III, if P[Y (X0) \u2208Cn,1\u2212\u03b1|X0 = x0] \u22651 \u2212\u03b1 holds for a fixed x0 \u2208Rp. 4. type-IV, if P[Y (X0) \u2208Cn,1\u2212\u03b1|Dn, X0 = x0] \u22651 \u2212\u03b1 holds almost surely. The prediction interval is said to be asymptotically type-I, type-II, type-III or type-IV valid, if the respective relation above holds for an increasing sample size n \u2192\u221e, while the almost sure statements are substituted by convergence in probability. In practice, the type-IV prediction interval represent the most realistic quantification of uncertainty regarding future prediction points. This, because the obtained interval Cn,1\u2212\u03b1 is usually computed on an observed sample Dn, while the future point to be predicted is also treated as fixed X0 = x0. Note that a prediction interval of type-IV is also a prediction interval of type-III but not vice versa. A similar relation also holds between a type-I and type-II prediction interval. The latter implies the validity of the type-I condition, but not vice-versa. Furthermore, the type-III prediction interval also implies the type-I condition resulting into a hierarchical system of implications. Therefore, the type-I prediction interval can be considered as the weakest form of a prediction interval. This, because an averaging also happens on the future prediction inputs X0 and makes a verbal interpretation more difficult. Instead, it can be considered as an averaged prediction interval, in which, on average, future observations will fall in with a probability of at least 1 \u2212\u03b1. In order to specify the structure of the interval more into detail, one distinguish between parametric and\nThe prediction interval is said to be asymptotically type-I, type-II, type-III or type-IV valid, if the respective relation above holds for an increasing sample size n \u2192\u221e, while the almost sure statements are substituted by convergence in probability. In practice, the type-IV prediction interval represent the most realistic quantification of uncertainty regarding future prediction points. This, because the obtained interval Cn,1\u2212\u03b1 is usually computed on an observed sample Dn, while the future point to be predicted is also treated as fixed X0 = x0. Note that a prediction interval of type-IV is also a prediction interval of type-III but not vice versa. A similar relation also holds between a type-I and type-II prediction interval. The latter implies the validity of the type-I condition, but not vice-versa. Furthermore, the type-III prediction interval also implies the type-I condition resulting into a hierarchical system of implications. Therefore, the type-I prediction interval can be considered as the weakest form of a prediction interval. This, because an averaging also happens on the future prediction inputs X0 and makes a verbal interpretation more difficult. Instead, it can be considered as an averaged prediction interval, in which, on average, future observations will fall in with a probability of at least 1 \u2212\u03b1. In order to specify the structure of the interval more into detail, one distinguish between parametric and\n(1)\nnon-parametric intervals. The upcoming sections state general conditions, under which prediction coverage rates as given in Definition 1 are valid. Regarding non-parametric prediction intervals, conditions for the validity of Definition 1 for the Random Forest method have been given in [23].\n# 2.1. Parametric Prediction Intervals\n(P3) Any type of residual variance estimator \ufffd\u03c32 n is P-consistent, i.e. \ufffd\u03c32 n P \u2212\u2192\u03c32, as n \u2192\u221e.\n \ufffd  \ufffd The assumptions of Gaussian error rates can be relaxed by assuming a certain other limiting, (parametric) distribution for the standardized residuals {\u01ebi/\u03c3}n i=1 that is continuous. In order to have type-III or typeIV prediction coverage, it is required to substitute the P-consistency of the first condition to a pointwise consistency for fixed x0 = X0. Note that the above conditions do not deal with heteroscedastic residual variance in the form of \u03c32 = \u03c32(x0). The latter will not be covered in this setting. Therefore, any type of learning algorithm that fulfills conditions (P1) - (P3) will lead to a type-I prediction interval that is asymptotically valid. The corresponding type-I prediction interval has then the following form\nCn,1\u2212\u03b1 = [ \ufffdmn(X0) + q\u03b1/2 \u00b7 \ufffd\u03c3n, \ufffdmn(X0) + q1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n]\n \ufffd \ufffd \ufffd \ufffd where q\u03b1/2 denotes the corresponding \u03b1/2-quantile of the (standardized) residual distribution. A forma proof for the validity of this parametric prediction interval is given in [25] on page 51, for example.\n# 2.2. Non-Parametric Prediction Intervals\nNon-parametric prediction intervals do not make any assumptions on the residual distribution, except the conditions arising from the regression model in (1). Therefore, assumption (P2) is usually dropped and substituted by any residual distribution that is continuous. This scenario has been considered in [23], where the authors revealed under the mentioned conditions the asymptotic type-I validity of the prediction interval given by\nCNP n,1\u2212\u03b1 = [ \ufffdmn(X0) + \ufffdDn,\u03b1/2, \ufffdmn(X0) + \ufffdDn,1\u2212\u03b1/2].\n \ufffd \ufffd \ufffd \ufffd The interval length is mainly driven by the empirical quantiles \ufffdDn,\u03b1, which is computed based on the empirical residuals \ufffd\u01ebi,n := Yi \u2212\ufffdmn(Xi) using the \u2308\u03b1 \u00b7 n\u2309-smallest observation among the order statistic \ufffd\u01eb(1),n,\ufffd\u01eb(2),n . . . ,\ufffd\u01eb(n),n. Slight modifications to the computed residuals do exists such as the split-conformal prediction interval, where the absolute residuals are considered among a prior training set seperation. For\n(2)\n(3)\ndetails on this, it is recommended to check [26] and [23]. Similarly to the previous arguments, type-III and type-IV coverage can be obtained also for CNP n,1\u2212\u03b1, if assumption (P1) is substituted to the pointwise convergence for fixed x0 = X0. The benefits of relaxing the Gaussian distribution by any, potentially nonparametric distribution with continuous distribution function expects to have a certain drawback: the interval length might be larger compared to its parametric counterpart. However, this effect needs to be analyzed more thoroughly and will be considered in the simulation Sections 5 and 6. Beside the prediction interval CNP n,1\u2212\u03b1, a Random Forest based type-III asymptotic prediction interval was proposed in [22] mainly focusing on the quantiles estimating directly on obtained response values {\ufffdYi(x0)}i. A more detailed description on this will be delivered in Section 4 under the quantile regression scheme.\n# 3. Random Forest Regression\nRandom Forest Regression according to [6] is an ensemble of randomized decision trees combining elements of bagging and feature sub-spacing. The final aggregation and therefore prediction is conducted by averaging the target variable falling in the same node of the tree as the input X0. For a detailed algorithmic description of the Random Forest Regression, the works [6] or [11] are recommended. However, the important hyperparameters for the Random Forest learner are shortly listed:\ne number of samples points an \u2208{1, . . . , n} in the bagging step.\n The number of terminal nodes tn \u2208N each decision tree can have\nThe method enables the construction of internal accuracy measures through the usage of the Out-of-Bag scheme. Hence, a prediction at an input X potentially being part of the training set Dn is computed along all those decision trees in the ensemble, where X has not been part of the tree construction process. This is possible, since Random Forest is a bagging learner resulting into decision trees, that do not include all observational points during the tree construction process. The Out-of-Bag scheme is especially used to obtain internal accuracy measures regarding prediction performance of the Random Forest. Although the Out-of-Bag scheme does not primarily impact the point prediction of the Random Forest method, it will play a key role in the estimation of the residual variance in parametric prediction intervals. This, because the unseen input X0 is not part of the training set making the usage of all trees in the forest for prediction possible. Therefore, one defines a Random-Forest Out-of-Bag prediction at input Xi for a fixed i \u2208{1, . . ., n} using M decision trees as mOOB n,M (Xi) and the corresponding prediction at any other prediction point X0 not included in Dn as mn,M(X0). Note that mn,M(X0) = mOOB n,M (X0). One furthermore distinguishes among these point estimators by introducing a theoretically infinite number of decision trees, i.e. mn,\u221e(X0) = lim M\u2192\u221emn,M(X0) is the Random Forest prediction at X0 using an infinite number of decision trees. Regarding its Out-of-Bag counterpart with an infinite number of decision trees denoted as mOOB n,\u221e(Xi),\nit is proven in the Supplement (Proposition 1) that it exits and attains a conditional expectation form. Any bias introduced through the usage of a finite number of decision trees is called as the finite-M-bias theoretically given by mn,M(X0)\u2212mn,\u221e(X0). That the latter is a serious source of prediction accuracy loss has been identified in [14] and [25]. This, because theoretical results such as Central Limit Theorems or consistency results are based on the infinite Random Forest. The latter results will be used in the construction of valid residual variance estimators. The construction of Random Forest based type-I prediction intervals of the form as in (2) requires for its correct coverage the fulfillment of the conditions (P1) \u2212(P3). Regarding condition (P1), the authors in [11] established a formal proof for the infinite Random Forest mn,\u221eunder regression functions being continuous and additive, while side conditions on the tuning parameters such as tn \u2192\u221e, an \u2192\u221eand tn(log(an))9/an \u21920 for n \u2192\u221ehave been set. In addition to that, Gaussian error terms are assumed leading to the same condition as in (P2). One is therefore left with condition (P3), which mainly focuses on consistent residual variance estimators.\nit is proven in the Supplement (Proposition 1) that it exits and attains a conditional expectation form. Any bias introduced through the usage of a finite number of decision trees is called as the finite-M-bias theoretically given by mn,M(X0)\u2212mn,\u221e(X0). That the latter is a serious source of prediction accuracy loss has been identified in [14] and [25]. This, because theoretical results such as Central Limit Theorems or consistency results are based on the infinite Random Forest. The latter results will be used in the construction of valid residual variance estimators. The construction of Random Forest based type-I prediction intervals of the form as in (2) requires for its correct coverage the fulfillment of the conditions (P1) \u2212(P3). Regarding condition (P1), the authors in [11] established a formal proof for the infinite Random Forest mn,\u221eunder regression functions being\nit is proven in the Supplement (Proposition 1) that it exits and attains a conditional expectation form. Any bias introduced through the usage of a finite number of decision trees is called as the finite-M-bias theoretically given by mn,M(X0)\u2212mn,\u221e(X0). That the latter is a serious source of prediction accuracy loss has been identified in [14] and [25]. This, because theoretical results such as Central Limit Theorems or consistency results are based on the infinite Random Forest. The latter results will be used in the construction of valid residual variance estimators.\n# 3.1. Consistent Residual Variance Estimators\nThe issue of consistently estimating residual variance using Random Forest Out-of-Bag predictions is relatively new. In [24], for example, the issue has been tackled for the infinite Out-of-Bag Random Forest focusing on sample variance estimates on the Out-of-Bag residuals \ufffd\u01ebOOB i,n,\u221e= Yi \u2212mOOB n,\u221e(Xi) leading to the natural estimate of the form\n\ufffd \ufffd However, a central practical drawback of this type of estimator is the infinite number of decision trees used in the Random Forest model. In practice, only a finite number of decision trees can be computed leading to a potential bias for a finite choice of M. Therefore, a theoretical framework is established, which allows the introduction of a finite-M-bias corrected residual variance estimator. In doing so, the following conditions are set:\n# Assumptions.\n(A1) The support of the feature input is the p-dimensional hyper-rectangular unit-cell, i.e. supp(X) = [0, 1]p. (A2) The regression function is bounded, i.e. ||m||\u221e= sup x\u2208[0,1]p |m(x)| =: K < \u221e. (A3) The residuals are centered Gaussian as in condition (P2).\nA1) The support of the feature input is the p-dimensional hyper-rectangular unit-cell, i.e. supp(X) = [0, 1]p\n(A2) The regression function is bounded, i.e. ||m||\u221e= sup x\u2208[0,1]p |m(x)| =: K < \u221e. (A3) The residuals are centered Gaussian as in condition (P2).\n(A4) The Random-Forest estimator mn,\u221e(X0) is L2 consistent, i.e. lim n\u2192\u221eE[(mn,\u221e(X0) \u2212m(X0))2] = 0. Note that assumption (A1) is not severe when using the Random Forest method. The reason is that the latter is invariant to monotone transformation (see e.g. [25], page 26) including this way a rich class of distributions for the features {Xi}n i=1. Assumption (A3) is in line with the assumptions given in [11] in order\nNote that assumption (A1) is not severe when using the Random Forest method. The reason is tha the latter is invariant to monotone transformation (see e.g. [25], page 26) including this way a rich class o distributions for the features {Xi}n i=1. Assumption (A3) is in line with the assumptions given in [11] in orde\n(4)\nto obtain consistency as required in (A4). Note that the latter assumption does not imply the consistency of the finite-M Out-of-Bag Random Forest estimator mOOB n,M . The latter, however, was treated in [24] and guarantees the consistency of the Out-of-Bag Random Forest under (A4) and the stated model framework in (1). Regarding the consistency of the residual variance estimator in (4), only assumptions (A1) and (A4) within regression model (1) together with E[|m(X1)|2] < \u221eis required, in order to have \ufffd\u03c32 n,\u221e L1 \u2212\u2192\u03c32 as n \u2192\u221e. This has been shown in Theorem 1 in [24] and is not part of this work. Instead, the focus will be on the derivation of residual variance estimators, that correct for potential bias due to the finite choice of M. Denoting with (n, M) seq \u2192a the sequential limit of the form lim n\u2192a lim M\u2192a, the first theoretical result in the following Theorem is stated enabling the construction of parametric prediction intervals. Theorem 1. Assume the regression model (1) together with the conditions (A1), (A2) and (A4). Then the estimator n \ufffd\n\ufffd with \u01ebi,n,M := Yi \u2212mOOB n,M (Xi) is consistent, i.e.\nas (n, M) seq \u2192\u221e.\nIf in addition (A3) holds, then the finite-M bias of the residual variance estimator \ufffd\u03c32 n,M can be bounded by E[\ufffd\u03c32 n,M \u2212\ufffd\u03c32 n,\u221e] \u22648 M (||m||\u221e+ \u03c32(1 + 4 log(n))).\n\ufffd\u03c32 n,Mcorrect = \ufffd\ufffd\ufffd\ufffd\ufffd\u03c32 n,M \u22128 M \ufffd ( max 1\u2264i\u2264n |mOOB n,M (Xi)|)2 + \ufffd\u03c32 n,M(1 + 4 log(n)) \ufffd\nThe proposed finite-M corrected residual variance estimator \ufffd\u03c32 n,M for \u03c32 is more conservative than \ufffd\u03c32 n,M due to the substraction of an upper bound. In practice, it can result in the underestimation of the true residual variance, while \ufffd\u03c32 n,M usually overestimates the true residual variance. The over- and underestimation of the corresponding residual variance estimators can be used to construct an additional estimator that smooths out this effect. Corollary 1. Let \u03bb1 \u2208(0, 1) and define \u03bb2 = 1 \u2212\u03bb1. Assume that regression model (1) is valid together with the conditions (A1) - (A4). Then the estimator\n\ufffd\u03c32 n,M;W = \u03bb1\ufffd\u03c32 n,MCorrect + \u03bb2\ufffd\u03c32 n,M\nis L1-consistent, as (n, M) seq \u2192\u221e.\nOne refers to the above residual variance estimator as the weighted, finite-M-corrected residual variance estimator. Combining these findings leads to the proposal of four parametric prediction intervals of type-I using the Random Forest Out-of-Bag principle.\n(5)\n \ufffd  \ufffd is an asymptotically valid type-I interval, if E[|m(X1)|2] < \u221eholds. If instead (A1) \u2212(A4) holds, then the intervals\nCn,1\u2212\u03b1,M := [mn,M(X0) \u2212z1\u2212\u03b1/2 \u00b7 \ufffd\u03c32 n,M, mn,M(X0) + z1\u2212\u03b1/2 \u00b7 \ufffd\u03c32 n,M, ], CCor n,1\u2212\u03b1,M := [mn,M(X0) \u2212z1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n,MCorrect, mn,M(X0) + z1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n,MCorrect] and CW n,1\u2212\u03b1,M := [mn,M(X0) \u2212z1\u2212\u03b1/2 \u00b7 \ufffd\u03c32 n,M;W , mn,M(X0) + z1\u2212\u03b1/2 \u00b7 \ufffd\u03c32 n,M;W ]\nRemark 1. All three parametric prediction intervals Cn,1\u2212\u03b1,\u221e, Cn,1\u2212\u03b1,M and CW n,1\u2212\u03b1,M can be extended to type-III and type-IV prediction intervals, if assumption (A4) is replaced by a stronger argument such as pointwise consistency, i.e. lim n\u2192\u221eE[(mn,\u221e(X0) \u2212m(X0))2|X0 = x0] = 0, depending on the conditioning as given in Definition 1.\nIt is interesting to know whether the general assumptions in (P1) - (P3) can be extended to other Machine-Learning methods than the Random Forest method. But before shortly introducing some considered boosting methods in this paper, the gap of using the Random Forest method in quantile regression is closed. A theoretical work of this can be found in [22] and is available in R through the package quantregForest.\n. Quantile Regression Forest. The key idea is to approximate the conditional distribution function F(y|X = x) = P[Y \u2264y|X = x], which itself can be rewritten into E[1{Y \u2264y}|X = x] as an analgon to m(x) = E[Y |X = x]. The target variable then changes to \u02dcYi = 1{Yi \u2264y} and a Random Forest method is constructed on the initial Dn. The obtained weights from the trained Random Forest are used to approximate F(y|X = x) with the modified response variables { \u02dcYi}i. The Random Forest approximation \ufffdFn(y|X = x) of F(y|X = x) is then used to compute quantiles for {Y (x0)} by considering\n\ufffdQn(x0, \u03b1) = inf{y : \ufffdFn(y|X0 = x0) \u2265\u03b1}.\n\ufffd \ufffd This would result into a type-III prediction interval of the form [ \ufffdQn(x0, \u03b1/2), \ufffdQn(x0, 1\u2212\u03b1/2)], whose validity is proven to hold asymptotically under rather strong assumptions such as the Lipschitz-continuity of the regression function m. Practical drawbacks are given by additional assumptions on the Random Forest method used in obtaining the quantiles \ufffdQn(x0, \u03b1), which are practically not given when using Breiman\u2019s Random Forest method. These include assumptions on the tree construction process such as the regularization of the node-size in each tree.\n(6)\n2. Stochastic Gradient Boosting Method. The aim of boosting is to combine weak learners such as decision trees in an additive fashion in order to obtain a strong learner, see e.g. [27]. In its core, it conducts the gradient descent method on the tree parameter space to find the best parameter settings for fitting the next tree in an additive aggregation style. Within the regression context, the used squared error loss as the optimizing functions yields to residuals from the previous iteration step being fitted to the considered covariates. The randomness applies at every iteration step, where the gradient is not computed on the whole set Dn, but on a random subset. Consistency results such as in (P1) and (P3) on this type of algorithm are not directly given, but for the general boosting method without random sampling in each iteration (see e.g. [28]). Therein, regression functions are assumed to be linear making theoretical transfer for obtaining prediction intervals more difficult. 3. XGBoost. The algorithm belongs to the general class of boosting methods developed in [29], but showed competitive and favourable prediction results for a variety of data examples. Several additional features such as scalable end-to-end tree boosting system with a modified tree penalization, a proportional shrinking of leaf nodes as well as additional randomization parameters have been implemented for the general purpose to increase prediction accuracy compared to the traditional gradient boosting machine, while maintaining low space and time complexity. In this work, the R-package xgboost is used. The verification of the conditions (P1) - (P3) for the stochastic gradient boosting method and the XGoost method is not part of this work due to the extensive theoretical work involved in establishing consistency sults for these type of learners. Instead, the parametric and non-parametric prediction intervals of the form ven in (2) and (3) are used by substituting the corresponding learning method in \ufffdmn and therefore also in n,\u03b1 with the stochastic gradient tree booster resp. the XGBoost.\n# 5. Simulation Design\nIt is interesting to know how the different prediction intervals behave in practical problems. For this eason, an extensive simulation study is conducted in a Monte-Carlo based fashion focusing on three aspects egarding prediction uncertainty: i) Correct (asymptotic) coverage rates for type-I, type-II, type-III and type-IV prediction intervals, while an emphasis should be placed on the type-IV interval for practical usage. ii) Obtaining narrow intervals through the computation of interval lengths for the corresponding type of prediction interval. iii) Exhausting interval performance in terms of coverage rates and interval lengths by deviating from the theoretical assumptions given in (A1) - (A4). Especially considering residual distributions that are non-normal with potential heavy tails and correlated features.\nThe signal-to-noise ratio can be considered as a potential source of distortion regarding interval quality. This, because prediction interval coverage and length can be affected by the signal strength coming from the regression function and the additional noise arising from the residuals through its variance \u03c32. This source of distortion in Random Forest models have been identified in [9], for example. It is theoretically given by\nThe signal-to-noise ratio can be considered as a potential source of distortion regarding interval quality. This, because prediction interval coverage and length can be affected by the signal strength coming from the regression function and the additional noise arising from the residuals through its variance \u03c32. This source of distortion in Random Forest models have been identified in [9], for example. It is theoretically given by SN = V ar(m(X)) \u03c32 , (7) and is controlled in this study through the consideration of SN \u2208{0.5, 1, 3}. One refers to noisy data, if SN < 1, while small noise data is present, if SN > 1. Various regression functions have been considered for a p = 10 dimensional covariate space. Letting \u03b20 = [2, 4, 2, \u22123, 1, 7, \u22124, 0, 0, 0]\u22a4be the regression coefficient, the following regression functions are then taken into account for i = 1, . . . , n: 1. The linear case described by the relation m(xi) = x\u22a4 i \u03b20. 2. The polynomial case given by m(xi) = p\ufffd j=1 \u03b20,jxj i,j. 3. The trigonometric case using m(xi) = 2 \u00b7 sin(x\u22a4 i \u03b20 + 2). 4. The non-continuous case through the consideration of m(xi) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b20,1xi,1 + \u03b20,2xi,2 + \u03b20,3xi,3, if xi,3 > 0.5 \u03b20,4xi,4 + \u03b20,5xi,5 + 3 if xi,3 \u22640.5.\nand is controlled in this study through the consideration of SN \u2208{0.5, 1, 3}. One refers to noisy data, if SN < 1, while small noise data is present, if SN > 1. Various regression functions have been considered for a p = 10 dimensional covariate space. Letting \u03b20 = [2, 4, 2, \u22123, 1, 7, \u22124, 0, 0, 0]\u22a4be the regression coefficient, the following regression functions are then taken into account for i = 1, . . . , n:\nm(xi) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b20,1xi,1 + \u03b20,2xi,2 + \u03b20,3xi,3, if xi,3 > 0.5 \u03b20,4xi,4 + \u03b20,5xi,5 + 3 if xi,3 \u22640.5.\nNote that the features {Xi}n i=1 are iid generated using a multivariate normal distribution and the inverse rule to transform the range back to the hyper-rectangular unit sphere [0, 1]p. This way, it is allowed to have potential dependencies among the features. Hence, one first generate \ufffd X1 = [ \ufffd X11, . . . , Xp1]\u22a4\u223cNp(0, \u03a3) and transform each feature back to the unit sphere through X1 = [F1( \ufffd X11), . . . , Fp( \ufffd Xp1)]\u22a4\u2208[0, 1]p. Fj denotes the normal distribution function with parameters [\u00b5X, \u03c32 X]\u22a4= [0, \u03a3jj]\u22a4. Various covariance structures are considered allowing the analysis of potential dependence effects on covariates. This includes positive autoregressive, negative-autoregressive, compound symmetric, linear decreasing Toeplitz and identity structures, where Ip denotes the identity matrix and Jp = 1p1\u22a4 p the matrix of ones:\nNote that the features {Xi}n i=1 are iid generated using a multivariate normal distribution and the inverse rule to transform the range back to the hyper-rectangular unit sphere [0, 1]p. This way, it is allowed to have potential dependencies among the features. Hence, one first generate \ufffd X1 = [ \ufffd X11, . . . , Xp1]\u22a4\u223cNp(0, \u03a3) and transform each feature back to the unit sphere through X1 = [F1( \ufffd X11), . . . , Fp( \ufffd Xp1)]\u22a4\u2208[0, 1]p. Fj denotes the normal distribution function with parameters [\u00b5X, \u03c32 X]\u22a4= [0, \u03a3jj]\u22a4. Various covariance structures are considered allowing the analysis of potential dependence effects on covariates. This includes positive autoregressive, negative-autoregressive, compound symmetric, linear decreasing Toeplitz and identity structures, where Ip denotes the identity matrix and Jp = 1p1\u22a4 p the matrix of ones: \u03a31 = {(\u22120.5)|i\u2212j|}1\u2264i,j\u2264p, \u03a33 = Ip + Jp, \u03a32 = {0.5|i\u2212j|}1\u2264i,j\u2264p, \u03a34 = {1 \u22121/p|i \u2212j|}1\u2264i,j\u2264p, \u03a35 = Ip. (8) Regarding the residual distribution, four different distributions are taken into consideration such as the normal, the t-, the exponential-, the log-normal distribution. Their parameters are determined by fixing the signal-to-noise ratio in (7) to its respective range {0.5, 1, 5}. For details on the parameter derivations based on the SN, see the supplementary material. Uisng MC = 1, 000 Monte-Carlo iterations, depending on the type of interval as given in Definition 1, the coverage rate p\u03b1 for an interval I(i) n,\u03b1 in the i-th simulation run is then estimated by\nRegarding the residual distribution, four different distributions are taken into consideration such as the normal, the t-, the exponential-, the log-normal distribution. Their parameters are determined by fixing the signal-to-noise ratio in (7) to its respective range {0.5, 1, 5}. For details on the parameter derivations based on the SN, see the supplementary material. Uisng MC = 1, 000 Monte-Carlo iterations, depending on the type of interval as given in Definition 1, the coverage rate p\u03b1 for an interval I(i) n,\u03b1 in the i-th simulation run is then estimated by\n(7)\n(8)\n(9)\nFixing the nominal coverage rate at \u03b1 = 0.05 and considering sample sizes n \u2208{100, 500, 1000}, simulation based results are presented in the next section. Note that the presented boxplots are taken over the used covariance structures and the considered residual distributions. For the weighted, finite-M corrected residual variance estimator used in the prediction interval CW n,1\u2212\u03b1,M, an equal weighting scheme is considered leading to \u03bb1 = \u03bb2 = 0.5. In addition to the used Machine Learning methods, the prediction interval obtained from a linear model is taken into consideration in order to obtain a trivial benchmark. The interval is given by\nFixing the nominal coverage rate at \u03b1 = 0.05 and considering sample sizes n \u2208{100, 500, 1000}, simulation based results are presented in the next section. Note that the presented boxplots are taken over the used covariance structures and the considered residual distributions. For the weighted, finite-M corrected residual variance estimator used in the prediction interval CW n,1\u2212\u03b1,M, an equal weighting scheme is considered leading to \u03bb1 = \u03bb2 = 0.5. In addition to the used Machine Learning methods, the prediction interval obtained from a linear model is taken into consideration in order to obtain a trivial benchmark. The interval is given by \ufffd x\u22a4 0 \ufffd\u03b2n \u2212tn\u2212rk(X),1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n \ufffd 1 + x\u22a4 0 (X\u22a4X)\u22121x0 \ufffd , x\u22a4 0 \ufffd\u03b2n + tn\u2212rk(X),1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n \ufffd 1 + x\u22a4 0 (X\u22a4X)\u22121x0 \ufffd\ufffd , (10)\nFixing the nominal coverage rate at \u03b1 = 0.05 and considering sample sizes n \u2208{100, 500, 1000}, simulation based results are presented in the next section. Note that the presented boxplots are taken over the used covariance structures and the considered residual distributions. For the weighted, finite-M corrected residual variance estimator used in the prediction interval CW n,1\u2212\u03b1,M, an equal weighting scheme is considered leading to \u03bb1 = \u03bb2 = 0.5. In addition to the used Machine Learning methods, the prediction interval obtained from a linear model is taken into consideration in order to obtain a trivial benchmark. The interval is given by \ufffd x\u22a4 0 \ufffd\u03b2n \u2212tn\u2212rk(X),1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n \ufffd 1 + x\u22a4 0 (X\u22a4X)\u22121x0 \ufffd , x\u22a4 0 \ufffd\u03b2n + tn\u2212rk(X),1\u2212\u03b1/2 \u00b7 \ufffd\u03c3n \ufffd 1 + x\u22a4 0 (X\u22a4X)\u22121x0 \ufffd\ufffd , (10) where \ufffd\u03c3n = 1 n\u2212rk(X)Y [In \u2212X(X\u22a4X)\u22121X\u22a4]Y , Y = [Y1, . . . , Yn]\u22a4and X is the design matrix of the covariates {Xi}n i=1 restructed in an n\u00d7 (p+ 1) matrix allowing the inclusion of a model intercept. Under the assumption of a linear model and (A3) together with det(X\u22a4X) \u0338= 0, the linear prediction interval should be a valid type-IV interval.\nwhere \ufffd\u03c3n = 1 n\u2212rk(X)Y [In \u2212X(X\u22a4X)\u22121X\u22a4]Y , Y = [Y1, . . . , Yn]\u22a4and X is the design matrix of the covariates {Xi}n i=1 restructed in an n\u00d7 (p+ 1) matrix allowing the inclusion of a model intercept. Under the assumption of a linear model and (A3) together with det(X\u22a4X) \u0338= 0, the linear prediction interval should be a valid type-IV interval.\n# 6. Results\nThe simulation results for a signal-to-noise ratio of SN = 1 are presented. The other choices of SN can be found in the Supplement on the Simulation Section. Note that the simulation results of the Stochastic Gradient Boosting Method and the XGBoost method are not presented. This, because of their relatively poor performance regarding coverage rates. On average, they resulted into rates being approximately 0.03 while interval length was comparably large. However, the results based on the derived parametric Random Forest prediction intervals are completely different. Starting with the coverage rate of the type-I prediction interval given in Figure 1 with SN = 1, the nonparametric Random Forest prediction interval based on empirical quantiles (NP-RF-EQ) and the parametric Random Forest prediction interval with the usual sampling variance (P-RF) yielded even better coverage rates under the linear model than the interval in (10). The parametric Random Forest prediction interval with finite-M-correction (P-RF-MCor) was more conservative under the type-I prediction interval scheme leading to less accurate, but shorter prediction intervals. The weighted, finite-M corrected prediction interval (P-RF-W) smoothed out the effect by being more accurate than P-RF-MCor resulting to almost perfect coverage rates on average for the trigonometric regression function. As preliminary suggested, the performance of P-RF method in terms of accurate coverage was slightly more liberal, but still very competitive. A potential reason for this is the slight overestimation of the residual variance due to the finite choice of M. The P-RF-W method balanced out this effect. In type-IV prediction intervals, one will see that this effect turns out to be even stronger. Comparing the interval coverage among the Random-Forest methods, NP-RFEQ increased in coverage quality, especially when the sample size increased across the different regression function. For small to moderate sample sizes, the P-RF interval was on average more accurate. In summary, both, the NP-RF-EQ interval and the P-RF interval yielded under a neutral noise setting (SN = 1) compet-\n\ufffd\ufffd (10)\nitive coverage results. The Quantile Regression Forest (QRF), however, had difficulties in obtaining correct coverage rates, especially for small samples and non-linear regression functions, being sometimes even worse than the interval of the linear model. Interesting is also the effect of non-normal residuals on the coverage rate. Both, the NP-RF-EQ and the P-RF method yielded more robust results under the type-I scheme. The P-RF-MCor and the QRF method, however, were more volatile. The reason for the P-RF-MCor method can be the obtained error bound itself, which is based on the normal assumption.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4878/48786ef9-5877-4d1e-8cf2-dd1ce60ee120.png\" style=\"width: 50%;\"></div>\nFigure 1: Simulation results on the coverage rate for the type-I prediction interval using a signal-to-noise ratio of SN = 1 with MC = 1, 000 Monte-Carlo iterations. The arranged boxplots in every sample size segment corresponds to the following methods: (from left to right) Linear Model, Quantile Regression Forest, Random-Forest with Empirical Quantiles, Random Forest with simple residual variance estimator as in Cn,1\u2212\u03b1,M , Random-Forest with finite-M-corrected estimator as in CCor n,1\u2212\u03b1,M , Random Forest with weighted residual variance as in CW n,1\u2212\u03b1,M .\nFigure 1: Simulation results on the coverage rate for the type-I prediction interval using a signal-to-noise ratio of SN = 1 with MC = 1, 000 Monte-Carlo iterations. The arranged boxplots in every sample size segment corresponds to the following methods: (from left to right) Linear Model, Quantile Regression Forest, Random-Forest with Empirical Quantiles, Random Forest with simple residual variance estimator as in Cn,1\u2212\u03b1,M , Random-Forest with finite-M-corrected estimator as in CCor n,1\u2212\u03b1,M , Random Forest with weighted residual variance as in CW n,1\u2212\u03b1,M . Regarding type-I prediction interval length, the P-RF-MCor and the P-RF-W resulted into the most\nRegarding type-I prediction interval length, the P-RF-MCor and the P-RF-W resulted into the most narrow intervals, while the NP-RF-EQ interval and the P-RF were slightly larger being on average similar in length. This effect can be extracted from Figure 2. Due to the more stringent assumption on the residua distribution given in (A3) for the parametric intervals, the interval length got reduced accordingly compared\nto the NP-RF-EQ prediction interval, making all three parametric intervals more competitive, especially the P-RF method. The prediction interval of a linear model and the QRF method indicated much wider interval lengths. Taking both, the coverage rate and the interval length under the SN = 1 scenario together, the P-RF method was competitive leading to accurate coverage rates while having narrow interval lengths.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f61/8f61ab7f-5d6a-46b7-b39d-08d8d1ff32d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Simulation results on the interval length for the type-I prediction interval using a signal-to-noise ratio of SN = 1 with MC = 1, 000 Monte-Carlo iterations. The arranged boxplots in every sample size segment corresponds to the following methods: (from left to right) Linear Model, Quantile Regression Forest, Random-Forest with Empirical Quantiles, Random Forest with simple residual variance estimator as in Cn,1\u2212\u03b1,M , Random-Forest with finite-M-corrected estimator as in CCor n,1\u2212\u03b1,M , Random Forest with weighted residual variance as in CW n,1\u2212\u03b1,M .</div>\nReturning to the stronger type-IV prediction interval given in Figure 3, the proposed P-RF-MCor prediction interval yielded strong coverage rates lying slightly below 0.95 on average. When combining it with the P-RF interval yielding to the weighted, parametric interval P-RF-W, almost accurate coverage results could be obtained throughout the different sample sizes and the considered regression methods. Therefore, the P-RF, the P-RF-MCor and the P-RF-W method yielded the most competitive coverage results for type-IV prediction intervals, while the P-RF-W was even more accurate under this scheme. The QRF interval turned\nout to be more accurate under the type-IV prediction interval than under the type-I interval, but indicated a higher coverage volatility being thus more sensitive to non-normal residuals and correlated features than their paremetric counterparts P-RF, P-RF-MCor and P-RF-W. The non-parametric Random Forest interval NP-RF-EQ performed comparable to the P-RF interval, but was more liberal than the latter. The result indicate that the correction for a finite choice of M is not neglectable and can significantly improve type-IV prediction interval coverage.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c593/c59326d0-a333-4912-8fdc-c002aa9dc509.png\" style=\"width: 50%;\"></div>\nFigure 3: Simulation results on the coverage rate for the type-IV prediction interval using a signal-to-noise ratio of SN = 1 with MC = 1, 000 Monte-Carlo iterations. The arranged boxplots in every sample size segment corresponds to the following methods: (from left to right) Linear Model, Quantile Regression Forest, Random-Forest with Empirical Quantiles, Random Forest with simple residual variance estimator as in Cn,1\u2212\u03b1,M , Random-Forest with finite-M-corrected estimator as in CCor n,1\u2212\u03b1,M , Random Forest with weighted residual variance as in CW n,1\u2212\u03b1,M . The type-IV interval length results are summarized in Figure 4 for a signal-to-noise ratio of SN = 1. Also here, the P-RF-MCor and the P-RF-W interval resulted into the most narrow interval in almost all considered\nFigure 3: Simulation results on the coverage rate for the type-IV prediction interval using a signal-to-noise ratio of SN = 1 with MC = 1, 000 Monte-Carlo iterations. The arranged boxplots in every sample size segment corresponds to the following methods: (from left to right) Linear Model, Quantile Regression Forest, Random-Forest with Empirical Quantiles, Random Forest with simple residual variance estimator as in Cn,1\u2212\u03b1,M , Random-Forest with finite-M-corrected estimator as in CCor n,1\u2212\u03b1,M , Random Forest with weighted residual variance as in CW n,1\u2212\u03b1,M .\nThe type-IV interval length results are summarized in Figure 4 for a signal-to-noise ratio of SN = 1. Also here, the P-RF-MCor and the P-RF-W interval resulted into the most narrow interval in almost all considered scenario. The P-RF method also indicated narrow interval lengths, but the correction due to a finite choice of M indicated a positive trend in prediction interval quality. Regarding prediction interval length the P-RF method performed comparable to the NP-RF-EQ method. Combining the simulated coverage rates and the\ntype-IV prediction interval length, the P-RF-W interval belonged to the most competitive one yielding almost accurate coverage rates and comparably narrow interval lengths. Its unweighted counterpart, the P-RF-MCor method, was in total competitive, too, being even more narrow.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/16c6/16c68ef4-a5df-4b76-8dde-af46502a34d9.png\" style=\"width: 50%;\"></div>\nFigure 4: Simulation results on the interval length for the type-IV prediction interval using a signal-to-noise ratio of SN = 1 with MC = 1, 000 Monte-Carlo iterations. The arranged boxplots in every sample size segment corresponds to the following methods: (from left to right) Linear Model, Quantile Regression Forest, Random-Forest with Empirical Quantiles, Random Forest with simple residual variance estimator as in Cn,1\u2212\u03b1,M , Random-Forest with finite-M-corrected estimator as in CCor n,1\u2212\u03b1,M , Random Forest with weighted residual variance as in CW n,1\u2212\u03b1,M .\nThe results regarding type-II and type-III prediction intervals for SN = 1 are moved to the supplement. For both types of intervals, especially the type-III prediction interval, the P-RF-W method yielded on average very competitive coverage results, while also being comparably narrow. Under the type-II scheme, the method was also competitive, but for an increased sample size. The NP-RF-EQ interval, however, performed worse than their parametric counterparts, especially when the sample size was comparably small. The effect reduced with an increased sample size, but was more sensitive towards the chosen regression function. Similarly to the previous results, the QRF interval was more volatile, especially under the type-III scheme. Regarding interval length similar results as under the type-IV and type-I scheme could be observed. Especially the\nP-RF-MCor and the P-RF-W intervals yielded the tightest intervals. Combigning these finidings with the type-III coverage rates, the P-RF-W method resulted into competitive interval quality. The simulation study also revealed that the signal-to-noise ratio is not neglectable. For small values of SN, such as SN = 0.5, which indicates the presence of noisy data due to the relation V ar(m(X)) < \u03c32, the prediction interval quality in terms of coverage and length turned even more accurate for the P-RF-W interval for all prediction interval types. Especially the more conservative type-IV version indicated preferable results for the P-RF-W and the P-RF-MCor method. The results can be found in the Simulation Section of the Supplement. Regarding the performance of the NP-RF-EQ interval, the quality in terms of length and accurate coverage suffered from noisy data. An increased sample size for this method turned prediction interval performance into a positive direction for all type of intervals according to Definition 1. For an increased signal-to-noise ratio such as SN = 3, the interval quality of P-RF-MCor and P-RF-W suffered under a more conservative estimation of the residual variance leading to a more conservative estimation of the coverage rate for the type-I, type-II and type-III prediction interval. This can also be seen from the simulation results on the prediction interval length. Therein, the P-RF-MCor method yielded the shortest intervals on average penalizing the estimation of the residual variance stronger than their counterpart. The reason lies in the relation that V ar(m(X)) > \u03c32 making the effect of a finite-M bias on the final output prediction stronger and impacting therefore prediction interval quality stronger. However, for these types of intervals (type-I, type-II and type-III), the parametric and non-penalizing method P-RF and NP-RF-EQ under SN > 1 yielded the most favourable results. However, regarding the type-IV prediction interval, the P-RF-W method remained among the best method under the aspect of correct coverage rates and comparably narrow interval length. In this case, the NP-RF-EQ method and the P-RF method performed comparably well, resulting into interval lengths being similar.\n# 7. Conclusions\nThe construction of trustworthy and interpretable models is a general issue in statistical machine learning. Beside accurate point predictions the quantification of uncertainty is of specific importance. In particular, the construction of statistically valid prediction intervals for the quantification of uncertainty regarding future point prediction contributes to the problem of constructing trustworthy machines. Focusing on the Random Forest method and other machine learning algorithms, formal conditions have been set to guarantee correct (asymptotic) coverage rates under parametric interval types. Therein, different types of prediction intervals have been considered mainly differing in the conditioning of the underlying probability measure. While typeI and type-II prediction intervals also average over unseen prediction points, their practical interpretability and usability are more restrictive. Therefore, type-III and type-IV prediction intervals, which keep unseen observations fixed, are easier to interpret and of practical importance. Based on this distinction, the problem of obtaining correct prediction intervals reduced to the issue of consistently estimating residual variance in regression learning methods. Therein, several estimators have been proposed and central sources of distortions have been identified such as the bias due to the finite choice of decision trees in the forest (finite-M-bias)\nand the signal-to-noise ratio (SN). The proposed parametric prediction intervals delivered accurate coverage rates and narrow interval lengths. Especially the interval based on a weighted residual variance scheme resulted into favorable coverage rates in obtaining correct point-wise prediction intervals, such as the type-III and type-IV interval. For noisy (SN > 0.5) and neutral noise containing data (SN = 1), the derived parametric intervals such as the weighted finite-M-corrected version or the interval with a simple Random Forest based residual variance estimator, delivered competitive results. They outperformed the Random Forest prediction interval based on empirical residuals (see [23]) and the Quantile Regression Forest (see [22]). For larger signal-tonoise ratios (SN = 3), the finite-M corrected version penalized residual variance estimators stronger leading to more conservative coverage rates for type-I, type-II and type-III prediction intervals. In this case, the uncorrected Random Forest based parametric prediction interval was more accurate. However, under the type-IV prediction interval, the weighted finite-M corrected version delivered in almost all scenarios accurate coverage rates while maintaining narrow interval length even under non-normal residual distributions and dependent covariate structures. Therefore, the strong type-IV prediction interval quality substantially increased when using the weighted, finite-M corrected version with equal weights. The results also leave optimistic space to even enhanced results for the type-I and type-II prediction interval, if data dependent weighting as a future research field is implemented for the weighted, finite-M corrected Random Forest prediction interval. Other machine learning methods such as prediction intervals based on the stochastic gradient tree boosting method or the XGBoost method were not able to deliver accurate coverage rates in the simulation study. Future research work will cover the involvement of missing values in the covariates and its impact in accurate prediction interval coverage and length.\n# Acknowledgement\nThe author\u2019s work is funded by the Ministry of of Culture and Science of the state of NRW (MKW NRW) through the research grand programme KI-Starter. The authors gratefully acknowledge the computing time provided on the Linux HPC cluster at Technical University Dortmund (LiDO3), partially funded in the course of the Large-Scale Equipment Initiative by the German Research Foundation (DFG) as project 271512359. Moreover, the author likes to thank Markus Pauly for valuable thoughts and vital discussions on Random Forest related issues in uncertainty quantification.\n[1] P. L. B\u00fchlmann, Bagging, subagging and bragging for improving some prediction algorithms, in: Research Report/Seminar f\u00fcr Statistik, Eidgen\u00f6ssische Technische Hochschule (ETH), Vol. 113, Seminar f\u00fcr Statistik, Eidgen\u00f6ssische Technische Hochschule (ETH), Z\u00fcrich, 2003. [2] A. M. Prasad, L. R. Iverson, A. Liaw, Newer Classification and Regression Tree Techniques: Bagging and Random Forests for Ecological Prediction, Ecosystems 9 (2) (2006) 181\u2013199. [3] P. Jiang, H. Wu, W. Wang, W. Ma, X. Sun, Z. Lu, MiPred: classification of real and pseudo microRNA precursors using random forest prediction model with combined features, Nucleic Acids Research 35 (2) (2007) W339\u2013W344. [4] B. T. Pham, D. T. Bui, I. Prakash, Bagging based Support Vector Machines for spatial prediction of landslides, Environmental Earth Sciences 77 (4) (2018) 1\u201317. [5] Z. Wang, Y. Wang, R. Zeng, R. S. Srinivasan, S. Ahrentzen, Random Forest based hourly building energy prediction, Energy and Buildings 171 (2018) 11\u201325. [6] L. Breiman, Random Forests, Machine Learning 45 (1) (2001) 5\u201332. [7] R. D\u00edaz-Uriarte, S. A. De Andres, Gene Selection and Classification of Microarray Data using Random Forest, BMC Bioinformatics 7 (3) (2006). doi:https://doi.org/10.1186/1471-2105-7-3. [8] R. Genuer, J.-M. Poggi, C. Tuleau-Malot, Variable Selection using Random Forests, Pattern Recognition Letters 31 (14) (2010) 2225\u20132236. [9] B. Ramosaj, M. Pauly, Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models, arXiv preprint arXiv:1912.03306 (2019). 10] S. Wager, S. Athey, Estimation and Inference of Heterogeneous Treatment Effects using Random Forests, Journal of the American Statistical Association 113 (523) (2018) 1228\u20131242. 11] E. Scornet, G. Biau, J.-P. Vert, Consistency of Random Forests, The Annals of Statistics 43 (4) (2015) 1716\u20131741. 12] L. Mentch, G. Hooker, Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests, The Journal of Machine Learning Research 17 (1) (2016) 841\u2013881. 13] Z. Zhou, L. Mentch, G. Hooker, V -statistics and Variance Estimation, arXiv preprint arXiv:1912.01089 (2019). 14] S. Wager, T. Hastie, B. Efron, Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife, Journal of Machine Learning Research 15 (1) (2014) 1625\u20131651.\n[15] A. Adadi, M. Berrada, Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI), IEEE Access 6 (2018) 52138\u201352160. [16] D. B. Dunson, Statistics in the big data era: Failures of the machine, Statistics & Probability Letters 136 (2018) 4\u20139. [17] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A Survey of Methods for Explaining Black Box Models, ACM Computing Surveys (CSUR) 51 (5) (2018) 1\u201342. [18] M. Christoph, Interpretable Machine Learning, Lulu.com, 2020. [19] J. W. Coulston, C. E. Blinn, V. A. Thomas, R. H. Wynne, Approximating Prediction Uncertainty for Random Forest Regression Models, Photogrammetric Engineering & Remote Sensing 82 (3) (2016) 189\u2013197. [20] A. Calvi\u00f1o, On Random-Forest-Based Prediction Intervals, in: International Conference on Intelligent Data Engineering and Automated Learning, Springer, 2020, pp. 172\u2013184. [21] M.-H. Roy, D. Larocque, Prediction Intervals with Random Forests, Statistical Methods in Medical Research 29 (1) (2020) 205\u2013229. [22] N. Meinshausen, G. Ridgeway, Quantile Regression Forests., Journal of Machine Learning Research 7 (6) (2006). [23] H. Zhang, J. Zimmerman, D. Nettleton, D. J. Nordman, Random Forest Prediction Intervals, The American Statistician (2019). [24] B. Ramosaj, M. Pauly, Consistent estimation of residual variance with random forest Out-Of-Bag errors, Statistics & Probability Letters 151 (2019) 49\u201357. [25] B. Ramosaj, Analyzing Consistency and Statistical Inference in Random Forest Models., Dissertation. Repositorium der TU Dortmund (2020). [26] J. Lei, M. G\u2019Sell, A. Rinaldo, R. J. Tibshirani, L. Wasserman, Distribution-Free Predictive Inference for Regression, Journal of the American Statistical Association 113 (523) (2018) 1094\u20131111. [27] J. H. Friedman, Stochastic Gradient Boosting, Computational Statistics & Data Analysis 38 (4) (2002) 367\u2013378. [28] P. Buehlmann, Boosting for High-Dimensional Linear Models, The Annals of Statistics 34 (2) (2006) 559\u2013583. [29] T. Chen, C. Guestrin, XGBoost: A Scalable Tree Boosting System, in: KDD \u201916: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 785\u2013794.\n# Supplementary Material to: Intepretable Machines: Constructing Valid Prediction Intervals with Random Forests.\nBurim Ramosaj\u2217 Faculty of Statistics Institute of Mathematical Statistics and Applications in Industry Technical University of Dortmund 44227 Dortmund, Germany\n 9 Mar 2021\nContents\n# 1 Theoretical Results - Proofs\nAdditional Simulation Results 6 2.1 Parameter Specification Residual Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Simulation Results for SN = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2.1 Type-II Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2.2 Type-III Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Simulation Results for SN = 0.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3.1 Type-I Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3.2 Type-II Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3.3 Type-III Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3.4 Type-IV Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.4 Simulation Results for SN = 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4.1 Type-I Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4.2 Type-II Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.4.3 Type-III Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.4.4 Type-IV Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2\nAdditional Simulation Results\n6\n2.1\nParameter Specification Residual Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nSimulation Results for SN = 1\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2.1\nType-II Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2.2\nType-III Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nSimulation Results for SN = 0.5\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.1\nType-I Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.2\nType-II Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3.3\nType-III Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.3.4\nType-IV Prediction Intervals\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.4\nSimulation Results for SN = 3\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.1\nType-I Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.2\nType-II Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.4.3\nType-III Prediction Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n2.4.4\nType-IV Prediction Intervals\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n# 2 Additional Simulation Results\n\u2217Corresponding Author: Burim Ramosaj Email address: burim.ramosaj@tu-dortmund.de\nPreprint submitted to Journal of LATEX Templates\nIn this section, the proofs of Theorem 1, Corollary 1 and Corollary 2 of the main article are presented. In addition, the following Proposition is stated, which treats the Out-of-Bag Random Forest estimator with a theoretically infinite number of decision trees. It illustrates its formal expression and existence. Proposition 1. Assume the same regression model (1) as in the main article and consider Breiman\u2019s Random Forest Regression learner. Then one has for every fixed i \u2208{1, . . ., n}\nIn addition, the following Proposition is stated, which treats the Out-of-Bag Random Forest estimator with a theoretically infinite number of decision trees. It illustrates its formal expression and existence. Proposition 1. Assume the same regression model (1) as in the main article and consider Breiman\u2019s Random Forest Regression learner. Then one has for every fixed i \u2208{1, . . ., n} mOOB n,\u221e(Xi) = E[mn,1(Xi)|\u0398(1) i,1 = 0, Dn] =: E\u0398[mn,1(Xi)|\u0398(1) i,t = 0]. Proof of Proposition 1. Let i \u2208{1, . . ., n} be fixed. The Random Forest method makes use of bagging and feature subspacing at every node of the tree. Therefore, defining the generic random vector \u0398 responsible for the bagging and feature subspacing process, one can assume that the latter is actually a sequence of iid random vectors {\u0398t}M t=1. It can be further decomposed to \u0398t = [\u0398(1)\u22a4 t , \u0398(2)\u22a4 t ]\u22a4, where \u0398(1) t = [\u0398(1) 1,t, . . . , \u0398(1) n,t] is the random vector indicating whether observation k \u2208{1, . . . , n} in tree t \u2208{1, . . ., M} has been selected (\u0398(1) k,t = 1) or not (\u0398(1) k,t = 0). \u0398(2) t is the vectorized matrix responsible for the feature subspacing procedure at every node of the tree. Note that n\ufffd k=1 \u0398(1) k,t = an, i.e. the sample size selected for every tree in the bagging step. Define the random variable Zi = Zi(M) as the number of decision trees not selecting the i\u2212th observation. Then, one has Zi \u223cBin(M, pi), where\nmOOB n,\u221e(Xi) = E[mn,1(Xi)|\u0398(1) i,1 = 0, Dn] =: E\u0398[mn,1(Xi)|\u0398(1) i,t = 0].\nProof of Proposition 1. Let i \u2208{1, . . ., n} be fixed. The Random Forest method makes use of bagging and feature subspacing at every node of the tree. Therefore, defining the generic random vector \u0398 responsible for the bagging and feature subspacing process, one can assume that the latter is actually a sequence of iid random vectors {\u0398t}M t=1. It can be further decomposed to \u0398t = [\u0398(1)\u22a4 t , \u0398(2)\u22a4 t ]\u22a4, where \u0398(1) t = [\u0398(1) 1,t, . . . , \u0398(1) n,t] is the random vector indicating whether observation k \u2208{1, . . . , n} in tree t \u2208{1, . . ., M} has been selected (\u0398(1) k,t = 1) or not (\u0398(1) k,t = 0). \u0398(2) t is the vectorized matrix responsible for the feature subspacing procedure at every node of the tree. Note that n\ufffd k=1 \u0398(1) k,t = an, i.e. the sample size selected for every tree in the bagging step. Define the random variable Zi = Zi(M) as the number of decision trees not selecting the i\u2212th observation. Then, one has Zi \u223cBin(M, pi), where\n\uf8f4 \uf8f3 Due to the strong law of large numbers, one has for fixed n, but M \u2192\u221ein almost sure sense that Zi(M)/M \u2212\u2192E[1{i-th observation not selected}] = pi \u2208(0, \u221e). Then, the finite-M Out-of-Bag Random Forest regression estimator turns out to take the following form for M \u2192\u221e:\nThe convergence to the conditional expectation guarantees its existence as well. In the following, Theorem 1 of the main article is proved.\nIn the following, Theorem 1 of the main article is proved.\n(1)\n\nProof of Theorem 1 of the main article. Let Y = m(X) + \u01eb such that ||m||\u221e=: K < \u221e. Random Forest regression learners can be rewritten as the weighted sum of the response variable {Yi}n i=1. Hence, considering the finite-M Random Forest estimate based on Out-of-Bag samples, one obtains\nm(OOB) n,M (Xi; \u03981, . . . , \u0398M) = n \ufffd k=1 W OOB n,k (Xi; \u03981, . . . , \u0398M) \u00b7 Yk,\nwhere W OOB n,k (Xi; \u03981, . . . , \u0398M) = 1 Zi(M) Zi(M) \ufffd t=1 Wn,k(Xi; \u0398t) and Wn,k(Xi; \u0398t) = 1{Xk\u2208AOOB n (Xi;\u0398t)} Nn(AOOB n (Xi;\u0398t)) . Note that Nn(A) denotes the sample points falling in cell A, which has been constructed using the mean variance reduction split-criterion. W.l.o.g., it is assumed in this representation that the first Zi(M) trees have not used the observation [X\u22a4 i , Yi]\u22a4. Using the fact that the weights {Wn,k(\u00b7; \u03981, . . . , \u0398M)}n k=1 are non-negative and sum up to 1, one can find an upper bound for the Out-of-Bag Random Forest estimate with finite-M:\net m(OOB) n,M (Xi) = m(OOB) n,M (Xi; \u03981, . . . , \u0398M) and use the result in (3). Due to assumption (A2), one has |m(Xi) \u2212m(OOB) n,M (Xi)| \u2264|m(Xi)| + |m(OOB) n,M (Xi)| < K + fn < \u221e. (4\n|m(Xi) \u2212m(OOB) n,M (Xi)| \u2264|m(Xi)| + |m(OOB) n,M (Xi)| < K + fn < \u221e.\nTherefore, one obtains |m(Xi)\u2212m(OOB) n,M (Xi)|2 < (K+fn)2 due to the monotonicity of the quadratic function on the non-negative real line. Furthermore, note that the estimator \ufffd\u03c32 n,M can be decomposed the following way:\nConsidering the first part of the decomposition in (5), one can obtain\nI(1) n,M := E \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd 1 n n \ufffd k=1 (m(Xk) \u2212m(OOB) n,M (Xk))2 \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd \u22641 n n \ufffd k= E[(m(Xk) \u2212m(OOB) n,M (Xk))2 = E[(m(X1) \u2212m(OOB) n,M (X1))2],\nwhere the last equality follows from the identical distribution of the sequence {(m(Xk) \u2212mOOB n,M (Xk))2}n k=1. The obtained upper bound in (4) is independent of M enabling the usage of Lebesgue\u2019s dominated convergence theorem in inequality (6). Therefore, one obtains\nI(1) n,\u221e:= lim M\u2192\u221eI(1) n,M = E[ lim M\u2192\u221e(m(X1) \u2212m(OOB) n,M (X1))2] = E[(m(X1) \u2212m(OOB) n,\u221e (X1))2].\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\nFrom [1], Corollary 1, it is known that assumption (A4) implies the L2-consistency of the infinite Out-o Bag Random Forest. Hence, one has from the previous equation\nleading to I(1) n,M \u2212\u21920, as (n, M) seq \u2192\u221e. Regarding the second part of equation (5), one can obtain\nleading to I(1) n,M \u2212\u21920, as (n, M) seq \u2192\u221e.\nI(2) n,M := E \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd 2 n n \ufffd k=1 (m(Xk) \u2212m(OOB) n,M (Xk)) \u00b7 \u01ebk \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd \u22642 n n \ufffd k=1 E[|(m(Xk) \u2212m(OOB) n,M (Xk))\u01ebk|]\nas (n, M) seq \u2192\u221e. The first inequality results from applying the triangular inequality, while the second inequality from the Cauchy-Schwarz inequality. Note that the second equality uses the identical distribution of the sequence {(m(Xk) \u2212m(OOB) n,M (Xk))}n k=1 Regarding the third part, one can make use of Vitali\u2019s convergence theorem. To apply the latter, note that 1 n n\ufffd k=1 \u01eb2 k converges to \u03c32 \u2208(0, \u221e) almost surely due to the iid structure in the considered regression model. This also implies convergence in probability. Letting A be any measurable set, it then follows that\n\ufffd\ufffd \ufffd\ufffd and therefore, that 1 n n\ufffd k=1 \u01eb2 k is uniformly integrable. Vitali\u2019s convergence theorem guarantees now that\n\ufffd\ufffd \ufffd\ufffd nd therefore, that 1 n n\ufffd k=1 \u01eb2 k is uniformly integrable. Vitali\u2019s convergence theorem guarantees now that\n\ufffd\ufffd \ufffd\ufffd as n \u2192\u221e. Since the latter is independent of M, it also holds for (n, M) seq \u2192\u221e. Combining the results in (6), (9) and (12), one can conclude with (5) that \ufffd\u03c32 n,M is L1-consistent for (n, M) seq \u2192\u221e.\n\ufffd\ufffd \ufffd\ufffd as n \u2192\u221e. Since the latter is independent of M, it also holds for (n, M) seq \u2192\u221e. Combining the results in (6), (9) and (12), one can conclude with (5) that \ufffd\u03c32 n,M is L1-consistent for (n, M) seq \u2192\u221e. If in addition (A3) holds, i.e. the sequence {\u01ebk}n k=1 is Gaussian with finite variance \u03c32 \u2208(0, \u221e), one can make use of Theorem 3.3 in [2], especially the following inequality: 0 \u2264E[(m(X1) \u2212m(OOB) n,M (X1))2] \u2212E[(m(X1) \u2212m(OOB) n,\u221e (X1))2] \u22648 M (||m||2 \u221e+ \u03c32(1 + 4 log(n))) (13)\n \ufffd If in addition (A3) holds, i.e. the sequence {\u01ebk}n k=1 is Gaussian with finite variance \u03c32 \u2208(0, \u221e), one can make use of Theorem 3.3 in [2], especially the following inequality: 0 \u2264E[(m(X1) \u2212m(OOB) n,M (X1))2] \u2212E[(m(X1) \u2212m(OOB) n,\u221e (X1))2] \u22648 M (||m||2 \u221e+ \u03c32(1 + 4 log(n))) (13)\n \ufffd If in addition (A3) holds, i.e. the sequence {\u01ebk}n k=1 is Gaussian with finite variance \u03c32 \u2208(0, \u221e), one can make use of Theorem 3.3 in [2], especially the following inequality:\n(8)\n(9)\n(10)\n(11)\n(12)\n(13)\nThen, regarding the finite-M bias, one can obtain\nE[\ufffd\u03c32 n,M \u2212\ufffd\u03c32 n,\u221e] = 1 n n \ufffd k=1 E \ufffd (\ufffd\u01ebOOB k,n,M)2 \u2212(\ufffd\u01ebOOB k,n,\u221e)2\ufffd\nThe second equality follows from the identical distribution of {(\ufffd\u01ebOOB k,n,M)2 \u2212(\ufffd\u01ebOOB k,n,\u221e)2}n k=1, while the third equality from the fact that Y1 = m(X1) + \u01eb1 together with the independence of \u01eb1 and m(OOB) n,M (X1) resp. \u01eb1 and m(OOB) n,\u221e (X1). The inequality is a direct result of (13).\nUsing the inequality in (3), one can deduce that the upper bound fn is independent of i \u2208{1, . . ., n} Therefore, one also has \ufffd max 1\u2264i\u2264n |m(OOB) n,M (Xi)| \ufffd2 < f 2 n < \u221e. Hence, one can deduce almost surely that\n0 \u2264Cn,M := 8 M \ufffd\ufffd max 1\u2264i\u2264n |m(OOB) n,M (Xi)| \ufffd2 + \ufffd\u03c32 n,M(1 + 4 log(n)) \ufffd \u22648f 2 n M + 8\ufffd\u03c32 n,M(1 + 4 log(n)) M .\n \ufffd Note that 8f 2 n/M L1 \u2212\u21920, as (n, M) seq \u2192\u221e. Since the estimator \ufffd\u03c32 n,M is L1-consistent, i.e. \ufffd\u03c32 n,M L1 \u2212\u2192\u03c32 \u2208 (0, \u221e), it can be immediately deduced that 8\ufffd\u03c32 n,M(1+4 log(n)) M is vanishing in L1-sense for (n, M) seq \u2192\u221e. Therefore, one has Cn,M L1 \u2212\u21920, as (n, M) seq \u2192\u221e. Similarly, this leads to (\ufffd\u03c32 n,M \u2212Cn,M) \u2212\u2192\u03c32 in L1sense, as (n, M) seq \u2192\u221e. Finally, one obtains the L1-convergence of \ufffd\u03c32 n,MCorrect due to the lower triangular inequality:\n \ufffd Note that 8f 2 n/M L1 \u2212\u21920, as (n, M) seq \u2192\u221e. Since the estimator \ufffd\u03c32 n,M is L1-consistent, i.e. \ufffd\u03c32 n,M L1 \u2212\u2192\u03c32 \u2208 (0, \u221e), it can be immediately deduced that 8\ufffd\u03c32 n,M(1+4 log(n)) M is vanishing in L1-sense for (n, M) seq \u2192\u221e. Therefore, one has Cn,M L1 \u2212\u21920, as (n, M) seq \u2192\u221e. Similarly, this leads to (\ufffd\u03c32 n,M \u2212Cn,M) \u2212\u2192\u03c32 in L1sense, as (n, M) seq \u2192\u221e. Finally, one obtains the L1-convergence of \ufffd\u03c32 n,MCorrect due to the lower triangular inequality: E[|\ufffd\u03c32 n,MCorrect \u2212\u03c32|] = E[|(|\ufffd\u03c32 n,M \u2212Cn,M|) \u2212\u03c32|] \u2264E[|\ufffd\u03c32 n,M \u2212\u03c32 \u2212Cn,M|] \u2264E[|\ufffd\u03c32 n,M \u2212\u03c32|] + E[|Cn,M|] \u2212\u21920, (15) as (n, M) seq \u2192\u221e.\nE[|\ufffd\u03c32 n,MCorrect \u2212\u03c32|] = E[|(|\ufffd\u03c32 n,M \u2212Cn,M|) \u2212\u03c32|] \u2264E[|\ufffd\u03c32 n,M \u2212\u03c32 \u2212Cn,M|] \u2264E[|\ufffd\u03c32 n,M \u2212\u03c32|] + E[|Cn,M|] \u2212\u21920,\nas (n, M) seq \u2192\u221e.\nProof of Corollary 1 of the main article. Since assumptions (A1) - (A4) are valid under the regression model (1) in the main article, one can apply Theorem 1 and conclude that\nE[|\u03bb1\ufffd\u03c32 n,MCorrect + \u03bb2\ufffd\u03c32 n,M \u2212\u03c32|] \u2264E[|\u03bb1\ufffd\u03c32 n,MCorrect \u2212\u03bb1\u03c32|] + E[|\u03bb2\ufffd\u03c32 n,M \u2212\u03bb2\u03c32|] = \u03bb1E[|\ufffd\u03c32 n,MCorrect \u2212\u03c32|] + \u03bb2E[|\ufffd\u03c32 n,M \u2212\u03c32|] \u2212\u2192\u03bb1\u03c32 + \u03bb2\u03c32 = \u03c32,\nas (n, M) seq \u2192\u221e, since \u03bb1 + \u03bb2 = 1 and \u03bb1, \u03bb2 > 0.\n(14)\n(15)\n\nProof of Corollary 2 of the main article. Let us start by considering the interval Cn,1\u2212\u03b1,\u221e. Assumption (A4) implies condition (P1), while (A3) implies condition (P2). Condition (P3) is also fulfilled due to Theorem 1 in [1], since the model assumptions in the main article and conditions (A1), (A3) and (A4) together with E[|m(X1)|2] < \u221efulfill the required conditions of the Theorem, which guarantees the L1-consistency of \ufffd\u03c32 n and therefore consistency in stochastic sense (P\u2212consistency).\nLet us start by considering the interval Cn,1\u2212\u03b1,\u221e. Assumption (A4) implies condition (P1), while (A3) implies condition (P2). Condition (P3) is also fulfilled due to Theorem 1 in [1], since the model assumptions in the main article and conditions (A1), (A3) and (A4) together with E[|m(X1)|2] < \u221efulfill the required conditions of the Theorem, which guarantees the L1-consistency of \ufffd\u03c32 n and therefore consistency in stochastic sense (P\u2212cons",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of the lack of interpretability in machine learning algorithms, particularly focusing on the Random Forest method. Previous methods have provided accurate point predictions but have been deficient in offering uncertainty estimates associated with these predictions. The need for valid prediction intervals is emphasized to enhance the interpretability and trustworthiness of machine learning models.",
        "problem": {
            "definition": "The core problem is the challenge of quantifying uncertainty in point predictions made by machine learning models, specifically the need to construct valid prediction intervals that provide a range of certainty for future observations.",
            "key obstacle": "Existing methods for obtaining prediction intervals often lack theoretical guarantees for their correct coverage probability, making them unreliable in practical applications."
        },
        "idea": {
            "intuition": "The idea stems from the observation that while Random Forests yield accurate predictions, they fail to deliver reliable uncertainty estimates. This gap motivates the development of a method to construct valid prediction intervals based on the Random Forest approach.",
            "opinion": "The proposed idea involves using the Out-of-Bag procedure of Random Forests to derive both parametric and nonparametric prediction intervals, ensuring that these intervals are statistically valid and interpretable.",
            "innovation": "The innovation lies in providing theoretical guarantees for the correct coverage of the proposed prediction intervals, which distinguishes this method from existing approaches that often do not meet such standards."
        },
        "method": {
            "method name": "Random Forest Out-of-Bag Prediction Intervals",
            "method abbreviation": "RF-OOB-PI",
            "method definition": "This method constructs prediction intervals for Random Forest point predictions by utilizing the Out-of-Bag samples to estimate uncertainty, thereby providing valid coverage probabilities.",
            "method description": "The method involves calculating prediction intervals based on residual variance estimates derived from the Out-of-Bag predictions of the Random Forest model.",
            "method steps": [
                "1. Train a Random Forest model using a training dataset.",
                "2. For each observation, identify the Out-of-Bag samples.",
                "3. Estimate the residual variance using the Out-of-Bag predictions.",
                "4. Construct prediction intervals based on the estimated residual variance."
            ],
            "principle": "The effectiveness of this method is rooted in the statistical properties of the Random Forest model, particularly its ability to provide consistent estimates of residual variance, which is crucial for constructing valid prediction intervals."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using Monte Carlo simulations to evaluate the performance of the proposed prediction intervals across various scenarios, including different sample sizes and residual distributions.",
            "evaluation method": "The evaluation involved assessing the correct coverage rates, interval widths, and competitiveness of the proposed intervals against existing methods through extensive simulation studies."
        },
        "conclusion": "The study concludes that the proposed Random Forest Out-of-Bag prediction intervals provide reliable uncertainty estimates with correct coverage probabilities, thus enhancing the interpretability and trustworthiness of machine learning predictions. The method outperforms existing approaches, particularly in scenarios with non-normal residual distributions and small sample sizes.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include its ability to provide valid prediction intervals with theoretical guarantees for coverage, making it a robust choice for uncertainty quantification in machine learning.",
            "limitation": "One limitation noted is the reliance on the assumptions regarding the underlying statistical properties of the data, which may not hold in all practical scenarios.",
            "future work": "Future research will focus on exploring the application of this method to other machine learning techniques and addressing the challenges posed by missing values in the covariates."
        },
        "other info": {
            "funding": "The author\u2019s work is funded by the Ministry of Culture and Science of the state of NRW (MKW NRW) through the research grant programme KI-Starter.",
            "acknowledgements": "The author acknowledges the computing time provided on the Linux HPC cluster at Technical University Dortmund and thanks Markus Pauly for valuable discussions on Random Forest related issues."
        }
    },
    "mount_outline": [
        {
            "section number": "6",
            "key information": "The proposed Random Forest Out-of-Bag prediction intervals provide reliable uncertainty estimates with correct coverage probabilities, thus enhancing the interpretability and trustworthiness of machine learning predictions."
        },
        {
            "section number": "6.1",
            "key information": "The method constructs prediction intervals for Random Forest point predictions by utilizing the Out-of-Bag samples to estimate uncertainty, thereby providing valid coverage probabilities."
        },
        {
            "section number": "6.2",
            "key information": "The innovation lies in providing theoretical guarantees for the correct coverage of the proposed prediction intervals, which distinguishes this method from existing approaches that often do not meet such standards."
        },
        {
            "section number": "7.1",
            "key information": "Existing methods for obtaining prediction intervals often lack theoretical guarantees for their correct coverage probability, making them unreliable in practical applications."
        },
        {
            "section number": "7.2",
            "key information": "Future research will focus on exploring the application of this method to other machine learning techniques and addressing the challenges posed by missing values in the covariates."
        }
    ],
    "similarity_score": 0.6432794565167171,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Interpretable Machines_ Constructing Valid Prediction Intervals with Random Forests.json"
}