{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.16228",
    "title": "Deformable Audio Transformer for Audio Event Detection",
    "abstract": " ABSTRACT\n8 Jan 2024\nTransformers have achieved promising results on a variety of tasks. However, the quadratic complexity in self-attention computation has limited the applications, especially in lowresource settings and mobile or edge devices. Existing works have proposed to exploit hand-crafted attention patterns to reduce computation complexity. However, such hand-crafted patterns are data-agnostic and may not be optimal. Hence, it is likely that relevant keys or values are being reduced, while less important ones are still preserved. Based on this key insight, we propose a novel deformable audio Transformer for audio recognition, named DATAR, where a deformable attention equipping with a pyramid transformer backbone is constructed and learnable. Such an architecture has been proven effective in prediction tasks, e.g., event classification. Moreover, we identify that the deformable attention map computation may over-simplify the input feature, which can be further enhanced. Hence, we introduce a learnable input adaptor to alleviate this issue, and DATAR achieves state-of-the-art performance. Index Terms\u2014 Deformable Audio Transformer, Audio Adaptor, Event Classification\narXiv:2312.16228v2\n# 1. INTRODUCTION\nTransformer [1] is firstly introduced to address natural language processing tasks, e.g., machine translation. Recently, it has shown great potential in the field of audio and speech recognition [2, 3, 4]. Transformer-based models, i.e., self-attention, are good at modeling long-range dependencies, which are proven to achieve superior performance in the regime of a large amount of learnable model parameters and training data. However, the quadratic computational complexity of the Transformer to the number of input tokens limits the applications, especially in low-resource settings. Specifically, the superfluous number of keys to attend per query yields high computational costs and slow convergence. To address the issue of excessive attention computation, [5, 6,",
    "bib_name": "zhu2024deformableaudiotransformeraudio",
    "md_text": "# DATAR: DEFORMABLE AUDIO TRANSFORMER FOR AUDIO EVENT RECOGNITION\nAddress - Line 1 Address - Line 2 Address - Line 3\n# ABSTRACT\n8 Jan 2024\nTransformers have achieved promising results on a variety of tasks. However, the quadratic complexity in self-attention computation has limited the applications, especially in lowresource settings and mobile or edge devices. Existing works have proposed to exploit hand-crafted attention patterns to reduce computation complexity. However, such hand-crafted patterns are data-agnostic and may not be optimal. Hence, it is likely that relevant keys or values are being reduced, while less important ones are still preserved. Based on this key insight, we propose a novel deformable audio Transformer for audio recognition, named DATAR, where a deformable attention equipping with a pyramid transformer backbone is constructed and learnable. Such an architecture has been proven effective in prediction tasks, e.g., event classification. Moreover, we identify that the deformable attention map computation may over-simplify the input feature, which can be further enhanced. Hence, we introduce a learnable input adaptor to alleviate this issue, and DATAR achieves state-of-the-art performance. Index Terms\u2014 Deformable Audio Transformer, Audio Adaptor, Event Classification\narXiv:2312.16228v2\n# 1. INTRODUCTION\nTransformer [1] is firstly introduced to address natural language processing tasks, e.g., machine translation. Recently, it has shown great potential in the field of audio and speech recognition [2, 3, 4]. Transformer-based models, i.e., self-attention, are good at modeling long-range dependencies, which are proven to achieve superior performance in the regime of a large amount of learnable model parameters and training data. However, the quadratic computational complexity of the Transformer to the number of input tokens limits the applications, especially in low-resource settings. Specifically, the superfluous number of keys to attend per query yields high computational costs and slow convergence. To address the issue of excessive attention computation, [5, 6, 7, 8] have leveraged carefully designed efficient attention patterns to alleviate the computation complexity. However, since the hand-crafted attention patterns are data-agnostic and may not be optimal, it probably leads to that relevant keys or\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7adf/7adfb786-d411-4758-bca1-3ba23586a075.png\" style=\"width: 50%;\"></div>\nFig. 1. Visualization of the effectiveness of proposed learnable input adaptor, which enhances relatively important parts of the original signal, a log-compressed mel-scaled spectrogram, to become more distinguishable, colored by green dash ovals.\nvalues being dropped, while less important ones are still preserved. To tackle the issue of hand-crafted attention patterns, [9] exploits an efficient deformable attention to generate the candidate key or value set for a given query flexibly. Moreover, the mechanism makes the model be able to adapt to each individual input. Recently, deformable attention-based networks have yielded promising results on many challenging tasks [10, 11, 12]. Hence, this motivates us to explore the deformable attention mechanism in audio transformers. In this work, a deformable audio transformer, DATAR, is proposed in Fig. 2. DATAR is equipped with a powerful pyramid backbone based on the deformable attention. Such a backbone is popular in prediction tasks, e.g., image classification [12]. Based on the observation in [13, 14], global attention typically results in almost the same attention patterns for various queries. Hence, a trainable audio offset generator (AOG) is introduced to learn a few groups of query-agnostic offsets to shift keys and values to important regions. Such a design not only helps hold a linear space complexity, but also introduces a deformable attention pattern to transformer backbones. Specifically, for each self-attention module, reference\npoints are first generated as uniform grids, which are the same across the input data. Then, the introduced AOG takes input as the query features and generates the corresponding offsets for all the reference points. Hence, the candidates of keys or values are shifted towards important regions. This augments the self-attention module with higher efficiency and flexibility to capture more informative features. Furthermore, we identify that the training of DATAR faces an accuracy improvement bottleneck, which is most likely due to that the deformable attention map calculation oversimplifies the input feature, leading to information loss [12]. We address this challenge by introducing a learnable input adaptor that adds learnable signals to the original input for more accurate deformable audio attention map computation. As these learned signals make the important parts of the original input become more distinguishable, the effectiveness of deformable audio attention is improved in Fig. 1.\n# 2. RELATED WORK\nTransformers [1] have been validated in the fields of natural language processing and computer vision. Recently, [15, 16, 4] introduce the Transformer into audio and speech processing and achieve the state-of-the-art performance. AST [15] proposes a convolution-free audio spectrogram Transformer, which is directly applied to an audio spectrogram and capable of capturing long-range global context even in the lowest layers. However, it requires large GPU memory and long training time, which limits the model\u2019s scalability in audio related tasks [16]. MAST and HTS-AT [17, 16, 18, 19] introduce a hierarchical structure into the AST for audio event detection. Inspired by the two pathways in the human auditory system, [4] introduces a two-stream ConvNet for audio recognition, i.e., fusing slow and fast streams with multi-level lateral connections. The slow pathway has a higher channel capacity, while the fast pathway has fewer channels and operates at a fine-grained temporal resolution. Different from the AST, [20, 3, 2] combine the Transformer architecture and ConvNet for audio processing. [20, 3] stack a Transformer on top of a ConvNet. [2] combines a Transformer and a ConvNet in each model block. Other efforts combine ConvNets with simpler attention modules [21, 22, 23]. The main model structure of DATAR is convolution-free. The main challenge of self-attention is the quadratic computation complexity [1, 24]. [25, 26, 7, 27, 8, 28, 29] have proposed various efficient methods to address this issue. The improvements focus on learning multiscale features for prediction tasks or efficient attention mechanisms. These efficient attention mechanisms include global tokens [25, 30, 31], windowed attention [26, 7], dynamic token sizes [32], and focal attention [28]. Deformable convolution [9, 33] is a powerful technique to attend to flexible locations conditioned on input data. Recently, [10, 34, 35] have applied it to vision Transformers (ViT). Deformable DETR [10] improves the con-\nvergence speed of DETR [36] by selecting a few keys for each query on the top of a ConvNet backbone. [34, 35] introduce deformable modules to refine visual tokens. [34] introduces a spatial sampling module before a ViT backbone to improve visual tokens. [35] proposes to refine patches across stages by deformable patch embeddings. We are the first to exploit deformable attention in audio event classification.\n# 3. METHODOLOGY\nThe main idea of deformable attention is to flexibly generate discriminative key and value points for each query token, compared with a regular grid sampling strategy in a conventional Transformer. This learned deformable sampling strategy yields a more useful and discriminative dot-product matrix and output. To ease the computational complexity of deformation generation, we leverage the redundancy in the signal, and the offset network is conducted in 4\u00d7 subsampled tokens by a convolutional neural network followed by bilinear interpolation.\n# 3.1. Deformable Audio Transformer\nLet X \u2208Rh\u00d7T be an audio spectrogram as input for DATAR, where h is the number of triangular mel-frequency bins, and T is the temporal length. After the patch embedding, which can be a convolutional block conducted in the audio spectrogram, we obtain the embedding token matrix A \u2208RN\u00d7C, where C is the embedding dimension and N is the number of tokens. A multi-head self-attention (MHSA) block with M heads is formulated:\n\ufffd \ufffd where \u03c3(\u00b7) denotes the softmax function, and d = C/M is the dimension of each head. z(m) denotes the embedding output from the m-th attention head, q(m), k(m), v(m) \u2208RN\u00d7d denote query, key, and value embeddings respectively. Wq, Wk, Wv, Wo \u2208RC\u00d7C are the projection matrices. To build up a Transformer block, an MLP block with two linear layers and a GELU activation is usually adopted to provide nonlinearity. With layer normalization [37] (LN) and identity shortcuts, the l-th Transformer block is formulated as:\nz\u2032 l = MHSA(LN(zl\u22121)) + zl\u22121, zl = MLP(LN(z\u2032 l)) + z\u2032 l.\n(1)\nDeformable attention module Given the input feature map X \u2208Rh\u00d7T , we generate a uniform grid of points p \u2208RhG\u00d7TG\u00d72 as the references in Fig. 2. Specifically, the grid size is down-sampled from the input feature map size by a factor r, hG = h/r, TG = T/r. The values of reference points are linearly spaced 2D coordinates\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6233/62332c1e-4bbf-40f4-ae0a-a24966ef8555.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Illustration of deformable audio Transformer.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a0b8/a0b87e55-0e9b-4527-8531-5969a7bbd7bd.png\" style=\"width: 50%;\"></div>\nFig. 3. Illustration of spectrogram deforming processing. Audio offset generator in \u00a73.1 calculates offsets used in deformable attention. When applying the deformable attention on the audio spectrum, for each query token, we select the most informative spectrum patch to conduct the self-attention. Based on the query location, DATAR learns the offset to localize the target spectrum patches.\n{(0, 0), \u00b7 \u00b7 \u00b7 , (hG \u22121, TG \u22121)}. Then we normalize them to the range [\u22121, 1] according to the grid shape hG \u00d7 TG, in which (\u22121, \u22121) indicates the top-left corner and (1, 1) indicates the bottom-right corner. Without loss of generality, we still use X as input for each block and omit the patch embedding and reshaping operators. To obtain the offset for each reference point, the feature maps are projected linearly to the query tokens q = XWq, and then fed into a lightweight subnetwork \u03b8offset(\u00b7) to generate the offsets \u2206p = \u03b8offset(q). To stabilize the training process, we scale the amplitude of \u2206p by a predefined factor s to prevent too large offset, i.e., \u2206p \u2190s tanh(\u2206p). Then the features are sampled at the locations of deformed points as keys and values, followed by projection matrices:\nq = XWq, \u02dck = \u02dcXWk, \u02dcv = \u02dcXWv, with \u2206p = \u03b8offset(q), \u02dcX = \u03d5(X; p + \u2206p),\n(2)\nwhere \u02dck and \u02dcv represent the deformed key and value embeddings respectively. Specifically, we set the sampling function\n\u03d5(z; (px, py)) = \ufffd (rx,ry) g(px, rx)g(py, ry)z[ry, rx, :],\nwhere g(a, b) = max(0, 1 \u2212|a \u2212b|) and (rx, ry) indexes all the locations on z \u2208Rh\u00d7T \u00d7C. As g would be non-zero only on the four integral points closest to (px, py), it simplifies Eq. (3) to a weighted average of the four locations. We perform multi-head attention on q, \u02dck, \u02dcv and adopt relative position offsets R. The output of an attention head is formulated as:\n(4)\n\ufffd \ufffd where \u03d5( \u02c6B; R) \u2208RhT \u00d7hGTG corresponds to the position embedding following previous work [7]. Audio offset generator A subnetwork is adopted for offset generation, which consumes the query features and outputs the offset values for reference points. Considering that each reference point covers a local s \u00d7 s region where s is the largest value for offset, the generation network should also have the perception of the local features to learn reasonable offsets. Therefore, we implement the subnetwork as two convolution modules with a nonlinear activation in Fig. 3. The input features are first passed through a 5 \u00d7 5 depth-wise convolution to capture local features. Then, a GELU activation and a 1 \u00d7 1 convolution is adopted to get the 2D offsets. It is also worth noticing that the bias in 1 \u00d7 1 convolution is dropped to alleviate the compulsive shift for all locations. Deformable relative position bias Relative position bias encodes the relative positions between every pair of query and key, which augments the vanilla attention with spatial information. Considering a feature map with shape h \u00d7 T, its relative coordinate displacements lie in the range of [\u2212h, h] and [\u2212T, T] at two dimensions, respectively. In Swin Transformer [7], a relative position bias table \u02c6B \u2208R(2h\u22121)\u00d7(2T \u22121) is constructed to obtain the relative position bias by indexing the table with the relative displacements in two directions. Since our deformable attention has continuous positions of keys, we compute the relative displacements in the normalized range [\u22121, 1], and then interpolate \u03d5( \u02c6B; R) in the parameterized bias table \u02c6B \u2208R(2h\u22121)\u00d7(2T \u22121) by the continuous relative displacements in order to cover all possible offset values.\n# 3.2. Learnable Input Adaptor\nTo further increase the accuracy, we apply a 2D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (C, h, T) and output can be defined as:\nout(Cj) = input(Cj) + \u03bb(b(Cj) + C\u22121 \ufffd k=0 W(Cj, k) \u2217input(k)), (5)\n + C\u22121 \ufffd k=0 W(Cj, k) \u2217input(k)),\n(5)\nwhere Cj is the channel index, \u03bb is used to tune the strength of adaptor, b and W are learnable parameters to enhance input signals, \u2217is a 2D cross-correlation operator, C denotes the number of channels, h is the number of triangular melfrequency bins, and T is the temporal length, input(k) denotes the k-th channel of input.\n# 4. EXPERIMENTS\nWe experiment with three audio event classification datasets \u2013 Kinetics-Sounds [38, 39], Epic-Kitchens-100 [40, 41, 42], and VGGSound [43]. The input is TorchAudio-based fbank, which is a log-compressed mel-scaled spectrogram and the same as AST. The frame length is 1,024 and the number of triangular mel-frequency bins is 128, which are the same as AST. The sampling frequency is 43,000 to cover 10 seconds of audio. We use cross-entropy loss. On Epic-Kitchens-100, we employ two classification heads for verb and noun. Kinetics-Sounds [39, 38] is a commonly used subset of the Kinetics-400 dataset [39], which is composed of 10-second audio data from YouTube. The dataset collection protocol described in [44] is followed. 22, 914 valid training audio data and 1, 585 valid test audio data are collected. Epic-Kitchens100 [42] consists of 90,000 variable length egocentric clips spanning 100 hours capturing daily kitchen activities. The dataset formulates each action into a verb and a noun. There are 67,217 training and 9,668 test samples after we remove categories with no training sample, which results in 97 verbs and 293 nouns. VGGSound [43] consists of about 200, 000 10-second video clips and 309 categories ranging from human actions and sound-emitting objects to human-object interactions. After removing invalid audio data, 159, 223 valid training audio data and 12, 790 valid test audio data are collected. Hyperparameters In all the experiments, we follow AST [21] setting in which model is pre-trained on ImageNet-1k. For hyperparameters in DATAR, we follow DAT [12] and use ImageNet-1K publicly available pretrained weights. AdamW is used with the learning rate of 0.00001. The numbers of epochs are set as 300, 100, and 50 for Kinetics-Sounds, EpicKitchens-100, and VGGSound. We employ the code of AST to obtain the results on these datasets. Comparison with state-of-the-arts According to Table 1, 2, and 3, the comparative results show that the proposed method is effective, and it outperforms the previous state-of-the-arts by a large margin. Specifically, the DATAR outperforms the previous best methods by 18.9%, 3.2% and 0.9% in terms of top-1 accuracy on the three datasets. The main reason is that, the introduced deformable audio attention and the learnable input adaptor help the model effectively capture the informative audio signals which are used to boost the model performance. For computation complexity, our work achieves better accuracy and consumes 93.3G MACs, while AST uses 103.4G MACs on VGGSound. Ablation study To validate the effectiveness of the proposed\nModel\nTop-1 acc\nTop-5 acc\nAST [45]\n52.6\n71.5\nDATAR (deformable+adaptor)\n71.5\n90.5\n<div style=\"text-align: center;\">Table 1. Comparison with state-of-the-arts on KineticsSounds.</div>\nTable 1. Comparison with state-of-the-arts on KineticsSounds.\nModel\nTop-1 acc\nTop-5 acc\nAST [45]\n52.3\n78.1\nChen et al. [43]\n48.8\n76.5\nAudioSlowFast [4]\n50.1\n77.9\nDATAR (deformable+adaptor)\n55.5\n80.2\n<div style=\"text-align: center;\">Table 2. Comparison with state-of-the-arts on VGGSound.</div>\nModel\nVerb\nNoun\nAction\nAST [45]\n44.3\n22.4\n13.0\nDamen et al. [40]\n42.1\n21.5\n14.8\nAudioSlowFast [4]\n46.5\n22.8\n15.4\nDATAR (deformable+adaptor)\n48.7\n22.9\n16.3\nTable 3. Comparison with state-of-the-arts on Epic-100.\nTable 3. Comparison with state-of-the-arts on Epic-100.\nModel\nTop-1\nTop-5\nwithout deformable\n69.4\n86.6\nwith deformable\n70.5\n89.9\ndeformable + N(0, 0.005)\n70.2\n90.1\ndeformable + L(0, 0.005)\n70.5\n89.6\ndeform.+adaptor: \u03bb = 0.2\n70.5\n90.2\ndeform.+adaptor: \u03bb = 0.005\n71.5\n90.5\nTable 4. Ablation study on Kinetics-Sounds.\nTable 4. Ablation study on Kinetics-Sounds.\ndeformable audio attention and the input adaptor, we conduct the ablation study of \u2018without deformable\u2019, \u2018with deformable\u2019, adding Gaussian perturbation to the input instead of learnable adaptor, adding Laplacian perturbation, different strengths, 0.2, 0.005, of \u03bb in Table 4. Deformable audio attention helps improve the model performance, i.e., 1.1% improvement in terms of top-1 accuracy on Kinetics-Sounds dataset. Based on Table 4, the proposed input adaptor is effective and improves the baseline model (DATAR with deformable) by 1% in top-1 accuracy. Compared with Gaussian and Laplacian perturbations, the proposed DATAR with the learnable input adaptor surpasses the two perturbation based methods by 1.3% and 1%. This demonstrates the effectiveness of learnable input adaptor.\nWe propose a novel deformable audio transformer for audio event classification, DATAR, which consists of deformable audio attention and input adaptor. Moreover, a learnable input adaptor is introduced to alleviate the issue of over-simplifying the input feature of deformable attention. Extensive results demonstrate the effectiveness of the proposed approach.\n# 6. REFERENCES\n[1] Ashish Vaswani et al., \u201cAttention is all you need,\u201d in NeurIPS, 2017, vol. 30. [2] Anmol Gulati et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020. [3] Qiuqiang Kong et al., \u201cSound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization,\u201d IEEE/ACM TASLP, vol. 28, pp. 2450\u20132460, 2020. [4] Evangelos Kazakos et al., \u201cSlow-fast auditory streams for audio recognition,\u201d in ICASSP. IEEE, 2021. [5] Manzil Zaheer et al., \u201cBig bird: Transformers for longer sequences,\u201d in NeurIPS, 2020, vol. 33, pp. 17283\u201317297. [6] Iz Beltagy et al., \u201cLongformer: The long-document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [7] Ze Liu et al., \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of ICCV, 2021. [8] Wenhai Wang et al., \u201cPyramid vision transformer: A versatile backbone for dense prediction without convolutions,\u201d in Proceedings of ICCV, 2021, pp. 568\u2013578. [9] Jifeng Dai et al., \u201cDeformable convolutional networks,\u201d in Proceedings of ICCV, 2017, pp. 764\u2013773. 10] Xizhou Zhu et al., \u201cDeformable DETR: Deformable Transformers for End-to-End Object Detection,\u201d in ICLR, 2021. 11] Jue Wang and Lorenzo Torresani, \u201cDeformable video transformer,\u201d in Proceedings of CVPR, 2022, pp. 14053\u201314062. 12] Zhuofan Xia et al., \u201cVision transformer with deformable attention,\u201d in Proceedings of CVPR, 2022. 13] Yue Cao et al., \u201cGCNet: Non-local networks meet squeezeexcitation networks and beyond,\u201d in Proceedings of ICCV Workshop, 2019. 14] Daquan Zhou et al., \u201cDeepViT: Towards deeper vision transformer,\u201d arXiv preprint arXiv:2103.11886, 2021. 15] Yuan Gong, Yu-An Chung, and James Glass, \u201cAST: Audio spectrogram transformer,\u201d in Interspeech, 2021. 16] Ke Chen et al., \u201cHTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection,\u201d in ICASSP. IEEE, 2022, pp. 646\u2013650. 17] Wentao Zhu and Mohamed Omar, \u201cMultiscale audio spectrogram transformer for efficient audio classification,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135. 18] Wentao Zhu, \u201cEfficient multiscale multimodal bottleneck transformer for audio-video classifications,\u201d 2024.\n[19] Wentao Zhu, \u201cEfficient selective audio masked multimodal bottleneck transformer for audio-video classification,\u201d 2024. [20] Koichi Miyazaki et al., \u201cConvolution augmented transformer for semi-supervised sound event detection,\u201d in DCASE, 2020. [21] Qiuqiang Kong et al., \u201cPANNS: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE/ACM TASLP, vol. 28, pp. 2880\u20132894, 2020. [22] Yuan Gong, Yu-An Chung, and James Glass, \u201cPSLA: Improving audio tagging with pretraining, sampling, labeling, and aggregation,\u201d IEEE/ACM TASLP, vol. 29, pp. 3292\u20133306, 2021. [23] Oleg Rybakov et al., \u201cStreaming keyword spotting on mobile devices,\u201d in Proc. Interspeech, 2020, pp. 2277\u20132281. [24] Alexey Dosovitskiy et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in International Conference on Learning Representations, 2021. [25] Chun-Fu Chen et al., \u201cRegionViT: Regional-to-Local Attention for Vision Transformers,\u201d in ICLR, 2022. [26] Xiaoyi Dong et al., \u201cCSWin transformer: A general vision transformer backbone with cross-shaped windows,\u201d in CVPR, 2022, pp. 12124\u201312134. [27] Xuran Pan et al., \u201cOn the integration of self-attention and convolution,\u201d in CVPR, 2022, pp. 815\u2013825. [28] Jianwei Yang et al., \u201cFocal self-attention for local-global interactions in vision transformers,\u201d arXiv preprint arXiv:2107.00641, 2021. [29] Pengchuan Zhang et al., \u201cMulti-scale vision longformer: A new vision transformer for high-resolution image encoding,\u201d in ICCV, 2021, pp. 2998\u20133008. [30] Andrew Jaegle et al., \u201cPerceiver: General perception with iterative attention,\u201d in ICML. PMLR, 2021, pp. 4651\u20134664. [31] Song Bai et al., \u201cVisual parser: Representing part-whole hierarchies with transformers,\u201d arXiv preprint arXiv:2107.05790, 2021. [32] Yulin Wang et al., \u201cNot all images are worth 16x16 words: Dynamic transformers for efficient image recognition,\u201d NeurIPS, vol. 34, pp. 11960\u201311973, 2021. [33] Xizhou Zhu et al., \u201cDeformable ConvNets v2: More deformable, better results,\u201d in CVPR, 2019, pp. 9308\u20139316. [34] Xiaoyu Yue et al., \u201cVision transformer with progressive sampling,\u201d in ICCV, 2021, pp. 387\u2013396. [35] Zhiyang Chen, Yousong Zhu, Chaoyang Zhao, Guosheng Hu, Wei Zeng, Jinqiao Wang, and Ming Tang, \u201cDpt: Deformable patch-based transformer for visual recognition,\u201d in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 2899\u20132907. [36] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko, \u201cEnd-toend object detection with transformers,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 213\u2013229. [37] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton, \u201cLayer normalization,\u201d arXiv preprint arXiv:1607.06450, 2016. [38] Relja Arandjelovic and Andrew Zisserman, \u201cLook, listen and learn,\u201d in Proceedings of ICCV, 2017, pp. 609\u2013617.\n[39] Will Kay et al., \u201cThe kinetics human action video dataset,\u201d arXiv preprint arXiv:1705.06950, 2017. [40] Dima Damen et al., \u201cRescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100,\u201d IJCV, 2021. [41] Dima Damen, Hazel Doughty, et al., \u201cScaling egocentric vision: The EPIC-KITCHENS dataset,\u201d in ECCV, 2018, pp. 720\u2013736. [42] Dima Damen et al., \u201cThe EPIC-KITCHENS Dataset: Collection, Challenges and Baselines,\u201d IEEE TPAMI, 2021. [43] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman, \u201cVGGSound: A large-scale audio-visual dataset,\u201d in ICASSP. IEEE, 2020. [44] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer, \u201cAudiovisual slowfast networks for video recognition,\u201d arXiv preprint arXiv:2001.08740, 2020. [45] Arsha Nagrani et al., \u201cAttention bottlenecks for multimodal fusion,\u201d in NeurIPS, 2021.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of excessive computational complexity in transformer models, particularly in audio recognition tasks, which limits their application in low-resource settings. Previous methods have attempted to reduce complexity using hand-crafted attention patterns that are often data-agnostic and suboptimal, leading to the loss of relevant information.",
        "problem": {
            "definition": "The problem defined in this paper is the quadratic computational complexity of transformers when processing input tokens, which hinders their efficiency and effectiveness in audio event recognition.",
            "key obstacle": "The main obstacle is that existing methods rely on hand-crafted attention patterns that do not adapt to the specific data, potentially discarding important features while retaining irrelevant ones."
        },
        "idea": {
            "intuition": "The intuition behind the proposed method is to create a flexible attention mechanism that can adaptively focus on the most informative parts of the input audio data, thereby improving the model's performance.",
            "opinion": "The proposed idea, DATAR, introduces a deformable attention mechanism combined with a pyramid transformer backbone, which learns to optimize the attention process for audio recognition tasks.",
            "innovation": "The key innovation of DATAR lies in its learnable deformable attention mechanism, which dynamically adjusts the attention focus based on the input data, as opposed to relying on static, hand-crafted patterns."
        },
        "method": {
            "method name": "Deformable Audio Transformer",
            "method abbreviation": "DATAR",
            "method definition": "DATAR is a transformer model that employs deformable attention to dynamically select key and value points for each input query, enhancing the efficiency and effectiveness of audio event recognition.",
            "method description": "The core of DATAR involves a deformable attention mechanism that adaptsively samples relevant features from the input audio spectrogram.",
            "method steps": [
                "Input audio spectrogram is processed into embedding tokens.",
                "A uniform grid of reference points is generated.",
                "Offsets are computed for the reference points to focus on informative regions.",
                "Deformed keys and values are sampled based on the computed offsets.",
                "Multi-head self-attention is applied to the deformed keys and values."
            ],
            "principle": "The effectiveness of DATAR stems from its ability to generate a more discriminative dot-product matrix by flexibly adapting the attention focus based on the input data, thus capturing more informative features."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three audio event classification datasets: Kinetics-Sounds, Epic-Kitchens-100, and VGGSound, using log-compressed mel-scaled spectrograms as input.",
            "evaluation method": "Performance was assessed using cross-entropy loss, with measures taken for top-1 and top-5 accuracy across the datasets."
        },
        "conclusion": "The proposed DATAR method demonstrates significant improvements in audio event classification tasks, outperforming state-of-the-art methods by substantial margins, attributed to the effectiveness of the deformable attention and learnable input adaptor.",
        "discussion": {
            "advantage": "The primary advantage of DATAR is its ability to effectively capture important audio signals through its deformable attention mechanism, leading to improved model performance.",
            "limitation": "A limitation of the method is that the deformable attention map computation may oversimplify the input features, potentially leading to information loss.",
            "future work": "Future work could focus on further refining the deformable attention mechanism and exploring additional ways to enhance the learnable input adaptor to mitigate oversimplification."
        },
        "other info": [
            {
                "info1": "The method achieves state-of-the-art performance on multiple audio classification benchmarks."
            },
            {
                "info2": {
                    "info2.1": "The model requires 93.3G MACs for computation, which is more efficient compared to previous methods."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "3",
            "key information": "The proposed method, DATAR, introduces a deformable attention mechanism combined with a pyramid transformer backbone, which learns to optimize the attention process for audio recognition tasks."
        },
        {
            "section number": "3.1",
            "key information": "The key innovation of DATAR lies in its learnable deformable attention mechanism, which dynamically adjusts the attention focus based on the input data, as opposed to relying on static, hand-crafted patterns."
        },
        {
            "section number": "3.2",
            "key information": "Performance was assessed using cross-entropy loss, with measures taken for top-1 and top-5 accuracy across the datasets."
        },
        {
            "section number": "3.3",
            "key information": "The problem defined in this paper is the quadratic computational complexity of transformers when processing input tokens, which hinders their efficiency and effectiveness in audio event recognition."
        },
        {
            "section number": "8",
            "key information": "A limitation of the method is that the deformable attention map computation may oversimplify the input features, potentially leading to information loss."
        },
        {
            "section number": "8.2",
            "key information": "Future work could focus on further refining the deformable attention mechanism and exploring additional ways to enhance the learnable input adaptor to mitigate oversimplification."
        }
    ],
    "similarity_score": 0.5582842721131274,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1730_natur/papers/Deformable Audio Transformer for Audio Event Detection.json"
}