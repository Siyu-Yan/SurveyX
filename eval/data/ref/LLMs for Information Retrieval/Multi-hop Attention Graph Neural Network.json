{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2009.14332",
    "title": "Multi-hop Attention Graph Neural Network",
    "abstract": " Abstract\n25 Aug 20\nSelf-attention mechanism in graph neural networks (GNNs) led to state-of-the-art performance on many graph representation learning tasks. Currently, at every layer, a node computes attention independently for each of its graph neighbors. However, such attention mechanism is limited as it does not consider nodes that are not connected by an edge but can provide important network context information. Here we propose Multi-hop Attention Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context information into every layer of GNN attention computation. MAGNA diffuses the attention scores across the network, which increases the \u201creceptive field\u201d for every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of not-connected nodes. We demonstrate theoretically and experimentally that MAGNA captures large-scale structural information in every layer, and has a low-pass effect that eliminates noisy high-frequency information from the graph. Experimental results on node classification as well as knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results: MAGNA achieves up to 5.7% relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains strong performance on a large-scale Open Graph Benchmark dataset. Finally, on knowledge graph completion MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four different performance metrics.\n25\n[cs.LG]\n# 1 Introduction\nSelf-attention [Bahdanau et al., 2015; Vaswani et al., 2017] has pushed the state-of-the-art in many domains including graph presentation learning [Devlin et al., 2019]. Graph Attention Network (GAT) [Veli\u02c7ckovi\u00b4c et al., 2018] and related models [Li et al., 2018; Wang et al., 2019a; Liu et al., 2019;\n\u2217Contact Author, xjtuwgt@gmail.com\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-ext",
    "bib_name": "wang2021multihopattentiongraphneural",
    "md_text": "# Multi-hop Attention Graph Neural Networks\n# Guangtao Wang1\u2217, Rex Ying2 , Jing Huang1 and Jure Leskovec2\n2Computer Science, Stanford University guangtao.wang@jd.com, rexying@stanford.edu, jing.huang@jd.com, jure@cs.stanford\nComputer Science, Stanford University guangtao.wang@jd.com, rexying@stanford.edu, jing.huang@jd.com, jure@cs.stanford.edu\nguangtao.wang@jd.com, rexying@stanford.edu, jing.huang@jd.com, jure@cs.stanford.edu\n# Abstract\n25 Aug 20\nSelf-attention mechanism in graph neural networks (GNNs) led to state-of-the-art performance on many graph representation learning tasks. Currently, at every layer, a node computes attention independently for each of its graph neighbors. However, such attention mechanism is limited as it does not consider nodes that are not connected by an edge but can provide important network context information. Here we propose Multi-hop Attention Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context information into every layer of GNN attention computation. MAGNA diffuses the attention scores across the network, which increases the \u201creceptive field\u201d for every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of not-connected nodes. We demonstrate theoretically and experimentally that MAGNA captures large-scale structural information in every layer, and has a low-pass effect that eliminates noisy high-frequency information from the graph. Experimental results on node classification as well as knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results: MAGNA achieves up to 5.7% relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains strong performance on a large-scale Open Graph Benchmark dataset. Finally, on knowledge graph completion MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four different performance metrics.\n25\n[cs.LG]\n# 1 Introduction\nSelf-attention [Bahdanau et al., 2015; Vaswani et al., 2017] has pushed the state-of-the-art in many domains including graph presentation learning [Devlin et al., 2019]. Graph Attention Network (GAT) [Veli\u02c7ckovi\u00b4c et al., 2018] and related models [Li et al., 2018; Wang et al., 2019a; Liu et al., 2019;\n\u2217Contact Author, xjtuwgt@gmail.com\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8b1/c8b1f70a-9b7e-43ad-b254-096becc8dcb6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65e1/65e187ad-8341-439c-b76c-54f2d0a890e6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">%\u2032&,' = 2 [%.,', %&,. , [%&,']) %\u2032.,/ = 2([%',/, %.,'])</div>\n<div style=\"text-align: center;\">%&,' = )(+&, +')  %.,/ = 0</div>\nFigure 1: Multi-hop attention diffusion. Consider making a prediction at nodes A and D. Left: A single GAT layer computes attention scores \u03b1 between directly connected pairs of nodes (i.e., edges) and thus \u03b1D,C = 0. Furthermore, the attention \u03b1A,B between A and B only depends on node representations of A and B. Right: A single MAGNA layer: (1) captures the information of D\u2019s two-hop neighbor node C via multi-hop attention \u03b1\u2032 D,C; and (2) enhances graph structure learning by considering all paths between nodes via diffused attention, which is based on powers of graph adjacency matrix. MAGNA makes use of node D\u2019s features for attention computation between A and B. This means that two-hop attention in MAGNA is context (node D) dependent.\nOono and Suzuki, 2020] developed attention mechanism for Graph Neural Networks (GNNs), which compute attention scores between nodes connected by an edge, allowing the model to attend to messages of node\u2019s neighbors. However, such attention computation on pairs of nodes connected by edges implies that a node can only attend to its immediate neighbors to compute its (next layer) representation. This implies that receptive field of a single GNN layer is restricted to one-hop network neighborhoods. Although stacking multiple GAT layers could in principle enlarge the receptive field and learn non-neighboring interactions, such deep GAT architectures suffer from the oversmoothing problem [Wang et al., 2019a; Liu et al., 2019; Oono and Suzuki, 2020] and do not perform well. Furthermore, edge attention in a single GAT layer is based solely on representations of the two nodes at the edge endpoints, and does not depend on their graph neighborhood context. In other words, the one-hop attention mechanism in GATs limits their ability to explore the relationship between the broader graph structure. While previous works [Xu et al., 2018; Klicpera et al., 2019b] have shown advantages in performing multi-hop message-passing in a single layer, these ap-\nrating multi-hop neighboring context into the attention computation in graph neural networks remains to be explored. Here we present Multi-hop Attention Graph Neural Network (MAGNA), an effective multi-hop self-attention mechanism for graph structured data. MAGNA uses a novel graph attention diffusion layer (Figure 1), where we first compute attention weights on edges (represented by solid arrows), and then compute self-attention weights (dotted arrows) between disconnected pairs of nodes through an attention diffusion process using the attention weights on the edges. Our model has two main advantages: (1) MAGNA captures long-range interactions between nodes that are not directly connected but may be multiple hops away. Thus the model enables effective long-range message passing, from important nodes multiple hops away. (2) The attention computation in MAGNA is context-dependent. The attention value in GATs [Veli\u02c7ckovi\u00b4c et al., 2018] only depends on node representations of the previous layer, and is zero between nonconnected pairs of nodes. In contrast, for any pair of nodes within a chosen multi-hop neighborhood, MAGNA computes attention by aggregating the attention scores over all the possible paths (length \u22651) connecting the two nodes. Mathematically we show that MAGNA places a Personalized Page Rank (PPR) prior on the attention values. We further apply spectral graph analysis to show that MAGNA emphasizes on large-scale graph structure and lowering highfrequency noise in graphs. Specifically, MAGNA enlarges the lower Laplacian eigen-values, which correspond to the large-scale structure in the graph, and suppresses the higher Laplacian eigen-values which correspond to more noisy and fine-grained information in the graph. We experiment on standard datasets for semi-supervised node classification as well as knowledge graph completion. Experiments show that MAGNA achieves state-of-the-art results: MAGNA achieves up to 5.7% relative error reduction over previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains better performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion, MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four metrics, with the largest gain of 7.1% in the metric of Hit at 1. Furthermore, we show that MAGNA with just 3 layers and 6 hop wide attention per layer significantly out-performs GAT with 18 layers, even though both architectures have the same receptive field. Moreover, our ablation study reveals the synergistic effect of the essential components of MAGNA, including layer normalization and multi-hop diffused attention. We further observe that compared to GAT, the attention values learned by MAGNA have higher diversity, indicating the ability to better pay attention to important nodes.\n# 2 Multi-hop Attention Graph Neural Network (MAGNA)\nWe first discuss the background and explain the novel multihop attention diffusion module and the MAGNA architecture.\n# 2.1 Preliminaries\nLet G = (V, E) be a given graph, where V is the set of Nn nodes, E \u2286V\u00d7V is the set of Ne edges connecting M pairs of nodes in V. Each node v \u2208V and each edge e \u2208E are associated with their type mapping functions: \u03c6 : V \u2192T and \u03c8 : E \u2192R. Here T and R denote the sets of node types (labels) and edge/relation types. Our framework supports learning on heterogeneous graphs with multiple elements in R. A general Graph Neural Network (GNN) approach learns an embedding that maps nodes and/or edge types into a continuous vector space. Let X \u2208RNn\u00d7dn and R \u2208RNr\u00d7dr be the node embedding and edge/relation type embedding, where Nn = |V|, Nr = |R|, dn and dr represent the embedding dimension of node and edge/relation types, each row xi = X[i :] represents the embedding of node vi (1 \u2264i \u2264 Nn), and rj = R[j :] represents the embedding of relation rj (1 \u2264j \u2264Nr). MAGNA builds on GNNs, while bringing together the benefits of Graph Attention and Diffusion techniques.\n# 2.2 Multi-hop Attention Diffusion\nWe first introduce attention diffusion to compute the multihop attention directly, which operates on the MAGNA\u2019s attention scores at each layer. The input to the attention diffusion operator is a set of triples (vi, rk, vj), where vi, vj are nodes and rk is the edge type. MAGNA first computes the attention scores on all edges. The attention diffusion module then computes the attention values between pairs of nodes that are not directly connected by an edge, based on the edge attention scores, via a diffusion process. The attention diffusion module can then be used as a component in MAGNA architecture, which we will further elaborate in Section 2.3.\nWe first introduce attention diffusion to compute the multihop attention directly, which operates on the MAGNA\u2019s attention scores at each layer. The input to the attention diffusion operator is a set of triples (vi, rk, vj), where vi, vj are nodes and rk is the edge type. MAGNA first computes the attention scores on all edges. The attention diffusion module then computes the attention values between pairs of nodes that are not directly connected by an edge, based on the edge attention scores, via a diffusion process. The attention diffusion module can then be used as a component in MAGNA architecture, which we will further elaborate in Section 2.3. Edge Attention Computation At each layer l, a vector message is computed for each triple (vi, rk, vj). To compute the representation of vj at layer l+1, all messages from triples incident to vj are aggregated into a single message, which is then used to update vl+1 j . In the first stage, the attention score s of an edge (vi, rk, vj) is computed by the following:\n(1)\nW (l) r \u2208\n    where \u03b4 = LeakyReLU, W (l) h , W (l) t \u2208Rd(l)\u00d7d(l), W (l) r \u2208 Rd(l)\u00d7dr and v(l) a \u2208R1\u00d73d(l) are the trainable weights shared by l-th layer. h(l) i \u2208Rd(l) represents the embedding of node i at l-th layer, and h(0) i = xi. rk is the trainable relation embedding of the k-th relation type rk (1 \u2264k \u2264Nr), and a\u2225b denotes concatenation of embedding vectors a and b. For graphs with no relation type, we treat as a degenerate categorical distribution with 1 category1. Applying Eq. 1 on each edge of the graph G, we obtain an attention score matrix S(l): \ufffd\n(2)\n1In this case, we can view that there is only one \u201cpseudo\u201d relation type (category), i.e., Nr = 1\nSubsequently we obtain the attention matrix A(l) by performing row-wised softmax over the score matrix S(l): A(l) = softmax(S(l)). A(l) ij denotes the attention value at layer l when aggregating message from node j to node i. Attention Diffusion for Multi-hop Neighbors In the second stage, we further enable attention between nodes that are not directly connected in the network. We achieve this via the following attention diffusion procedure. The procedure computes the attention scores of multi-hop neighbors via graph diffusion based on the powers of the 1-hop attention matrix A:\n(3)\nwhere \u03b8i is the attention decay factor and \u03b8i > \u03b8i+1. The powers of attention matrix, Ai, give us the number of relation paths from node h to node t of length up to i, increasing the receptive field of the attention (Figure 1). Importantly, the mechanism allows the attention between two nodes to not only depend on their previous layer representations, but also taking into account of the paths between the nodes, effectively creating attention shortcuts between nodes that are not directly connected (Figure 1). Attention through each path is also weighted differently, depending on \u03b8 and the path length. In our implementation we utilize the geometric distribution \u03b8i = \u03b1(1 \u2212\u03b1)i, where \u03b1 \u2208(0, 1]. The choice is based on the inductive bias that nodes further away should be weighted less in message aggregation, and nodes with different relation path lengths to the target node are sequentially weighted in an independent manner. In addition, notice that if we define \u03b80 = \u03b1 \u2208(0, 1], A0 = I, then Eq. 3 gives the Personalized Page Rank (PPR) procedure on the graph with the attention matrix A and teleport probability \u03b1. Hence the diffused attention weights, Aij, can be seen as the influence of node j to node i. We further elaborate the significance of this observation in Section 4.3. We can also view Aij as the attention value of node j to i since \ufffdNn j=1 Aij = 1.2 We then define the graph attention diffusion based feature aggregation as\nAttDiff(G, H(l), \u0398) = AH(l),\n(4)\nwhere \u0398 is the set of parameters for computing attention. Thanks to the diffusion process defined in Eq. 3, MAGNA uses the same number of parameters as if we were only computing attention between nodes connected via edges. This ensures runtime efficiency (refer to Appendix3 A for complexity analysis) and model generalization.\nwhere \u0398 is the set of parameters for computing attention. Thanks to the diffusion process defined in Eq. 3, MAGNA uses the same number of parameters as if we were only computing attention between nodes connected via edges. This ensures runtime efficiency (refer to Appendix3 A for complexity analysis) and model generalization. Approximate Computation for Attention Diffusion For large graphs computing the exact attention diffusion matrix A using Eq. 3 may be prohibitively expensive, due to computing the powers of the attention matrix [Klicpera et al., 2019a]. To resolve this bottleneck, we proceed as follows: Let H(l) be the input entity embedding of the l-th layer (H(0) = X)\nApproximate Computation for Attention Diffusion For large graphs computing the exact attention diffusion matrix A using Eq. 3 may be prohibitively expensive, due to computing the powers of the attention matrix [Klicpera et al., 2019a]. To resolve this bottleneck, we proceed as follows: Let H(l) be the input entity embedding of the l-th layer (H(0) = X)\n2Obtained by the definition A(l) = softmax(S(l)) and Eq. 3. 3Appendix can be found via https://arxiv.org/abs/2009.14332\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c66b/c66b8fd4-a6e4-49dd-ac45-824710b15959.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Inputs: Initial node and  relation embeddings !\", $</div>\nFigure 2: MAGNA Architecture. Each MAGNA block consists of attention computation, attention diffusion, layer normalization, feed forward layers, and 2 residual connections for each block. MAGNA blocks can be stacked to constitute a deep model. As illustrated on the right, context-dependent attention is achieved via the attention diffusion process. Here vi, vj, vp, vq \u2208V are nodes in the graph.\nand \u03b8i = \u03b1(1 \u2212\u03b1)i. Since MAGNA only requires aggregation via AH(l), we can approximate AH(l) by defining a sequence Z(K) which converges to the true value of AH(l) (Proposition 1) as K \u2192\u221e:\n\u2192\u221e A In the Appendix we give the proof which relies on the expansion of Eq. 5. Using the above approximation, the complexity of attention computation with diffusion is still O(|E|), with a constant factor corresponding to the number of hops K. In practice, we find that choosing the values of K such that 3 \u2264K \u226410 results in good model performance. Many real-world graphs exhibit small-world property, in which case even a smaller value of K is sufficient. For graphs with larger diameter, we choose larger K, and lower the value of \u03b1.\n# 2.3 Multi-hop Attention based GNN Architecture\nFigure 2 provides an architecture overview of the MAGNA Block that can be stacked multiple times. Multi-head Graph Attention Diffusion Layer Multi-head attention [Vaswani et al., 2017; Veli\u02c7ckovi\u00b4c et al., 2018] is used to allow the model to jointly attend to information from different representation sub-spaces at different viewpoints. In Eq. 6, the attention diffusion for each head i is computed separately with Eq. 4, and aggregated:\nFigure 2 provides an architecture overview of the MAGNA Block that can be stacked multiple times.\n(6)\n\ufffd\ufffd\ufffd\ufffd \ufffd headi = AttDiff(G, \u02dc H(l), \u0398i), \u02dc H(l) = LN(H(l)),\nwhere \u2225denotes concatenation and \u0398i are the parameters in Eq. 1 for the i-th head (1 \u2264i \u2264M), Wo represents a parameter matrix, and LN = LayerNorm. Since we calculate the attention diffusion in a recursive way in Eq. 5, we add layer normalization which helpful to stabilize the recurrent computation procedure [Ba et al., 2016]. Deep Aggregation Moreover our MAGNA block contains a fully connected feed-forward sub-layer, which consists of a two-layer feed-forward network. We also add the layer normalization and residual connection in both sub-layers, allowing for a more expressive aggregation step for each block [Xiong et al., 2020]:\n(7)\n\ufffd \ufffd MAGNA generalizes GAT MAGNA extends GAT via the diffusion process. The feature aggregation in GAT is H(l+1) = \u03c3(AH(l)W (l)), where \u03c3 represents the activation function. We can divide GAT layer into two components as follows:\n(8)\n\ufffd\ufffd\ufffd\ufffd \ufffd \ufffd\ufffd \ufffd In component (1), MAGNA removes the restriction of attending to direct neighbors, without requiring additional parameters as A is induced from A. For component (2) MAGNA uses layer normalization and deep aggregation to achieve higher expressive power compared to elu nonlinearity in GAT.\n# 3 Analysis of Graph Attention Diffusion\nIn this section, we investigate the benefits of MAGNA from the viewpoint of discrete signal processing on graphs [Sandryhaila and Moura, 2013b]. Our first result demonstrates that MAGNA can better capture large-scale structural information. Our second result explores the relation between MAGNA and Personalized PageRank (PPR).\n# 3.1 Spectral Properties of Graph Attention Diffusion\n# 3.1 Spectral Properties of Graph Attention Diffusion\nWe view the attention matrix A of GAT, and A of MAGNA as weighted adjacency matrices, and apply Graph Fourier transform and spectral analysis (details in Appendix) to show the effect of MAGNA as a graph low-pass filter, being able to more effectively capture large-scale structure in graphs. By Eq. 3, the sum of each row of either A or A is 1. Hence the normalized graph Laplacians are \u02c6Lsym = I \u2212A and Lsym = I \u2212A for A and A respectively. We can get the following proposition: Proposition 2. Let \u02c6\u03bbg i and \u03bbg i be the i-th eigeinvalues of \u02c6Lsym and Lsym.\n(9)\nRefer to Appendix for the proof. We additionally have \u03bbg i \u2208[0, 2] (proved by [Ng et al., 2002]). Eq. 9 shows that\nwhen \u03bbg i is small such that \u03b1 1\u2212\u03b1 + \u03bbg i < 1, then \u02c6\u03bbg i > \u03bbg i , whereas for large \u03bbg i , \u02c6\u03bbg i < \u03bbg i . This relation indicates that the use of A increases smaller eigenvalues and decreases larger eigenvalues4. See Section 4.3 for its empirical evidence. The low-pass effect increases with smaller \u03b1. The eigenvalues of the low-frequency signals describe the large-scale structure in the graph [Ng et al., 2002] and have been shown to be crucial in graph tasks [Klicpera et al., 2019b]. As \u03bbg i \u2208[0, 2] [Ng et al., 2002] and \u03b1 1\u2212\u03b1 > 0, the reciprocal format in Eq. 9 will amplify the ratio of lower eigenvalues to the sum of all eigenvalues. In contrast, high eigenvalues corresponding to noise are suppressed.\n# 3.2 Personalized PageRank Meets Graph Attention Diffusion\nWe can also view the attention matrix A as a random walk matrix on graph G since \ufffdNn j=1 Ai,j = 1 and Ai,j > 0. If we perform Personalized PageRank (PPR) with parameter \u03b1 \u2208 (0, 1] on G with transition matrix A, the fully Personalized PageRank [Lofgren, 2015] is defined as:\n(10)\nUsing the power series expansion for the matrix inverse, we obtain\n(11)\nComparing to the diffusion Equation 3 with \u03b8i = \u03b1(1\u2212\u03b1)i, we have the following proposition. Proposition 3. Graph attention diffusion defines a personalized page rank with parameter \u03b1 \u2208(0, 1] on G with transition matrix A, i.e., A = Appr. The parameter \u03b1 in MAGNA is equivalent to the teleport probability of PPR. PPR provides a good relevance score between nodes in a weighted graph (the weights from the attention matrix A). In summary, MAGNA places a PPR prior over node pairwise attention scores: the diffused attention between node i and j depends on the attention scores on the edges of all paths between i and j.\n# 4 Experiments\nWe evaluate MAGNA on two classical tasks5: (1) on node classification we achieve an average of 5.7% relative error reduction; (2) on knowledge graph completion we achieve 7.1% relative improvement in the Hit@1 metric.6\n# 4.1 Task 1: Node Classification\nDatasets We employ four benchmark datasets for node classification: (1) standard citation network benchmarks Cora, Citeseer and Pubmed [Sen et al., 2008; Kipf and\n4The eigenvalues of A and A correspond to the same eigenvectors, as shown in Proposition 2 in Appendix. 5All datasets are public. And our implementation is available at https://github.com/xjtuwgt/GNN-MAGNA 6Please see the definitions of these two tasks in Appendix.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f03/7f035846-5324-48d1-9fb2-e32048047996.png\" style=\"width: 50%;\"></div>\nModels\nCora\nCiteseer\nPubmed\nBaselines\nGCN [Kipf and Welling, 2017]\n81.5\n70.3\n79.0\nCheby [Defferrard et al., 2016]\n81.2\n69.8\n74.4\nDualGCN [Zhuang and Ma, 2018]\n83.5\n72.6\n80.0\nJKNet [Xu et al., 2018]\u22c6\n81.1\n69.8\n78.1\nLGCN [Gao et al., 2018]\n83.3 \u00b1 0.5\n73.0 \u00b1 0.6\n79.5 \u00b1 0.2\nDiff-GCN [Klicpera et al., 2019b]\n83.6 \u00b1 0.2\n73.4 \u00b1 0.3\n79.6 \u00b1 0.4\nAPPNP [Klicpera et al., 2019a]\n84.3 \u00b1 0.2\n71.1 \u00b1 0.4\n79.7 \u00b1 0.3\ng-U-Nets [Gao and Ji, 2019]\n84.4 \u00b1 0.6\n73.2 \u00b1 0.5\n79.6 \u00b1 0.2\nGAT [Veli\u02c7ckovi\u00b4c et al., 2018]\n83.0 \u00b1 0.7\n72.5 \u00b1 0.7\n79.0 \u00b1 0.3\nAbl.\nNo LayerNorm\n83.8 \u00b1 0.6\n71.1 \u00b1 0.5\n79.8 \u00b1 0.2\nNo Diffusion\n83.0 \u00b1 0.4\n71.6 \u00b1 0.4\n79.3 \u00b1 0.3\nNo Feed-Forward\u22c4\n84.9 \u00b1 0.4\n72.2 \u00b1 0.3\n80.9 \u00b1 0.3\nNo (LayerNorm + Feed-Forward)\n84.3 \u00b1 0.6\n72.6 \u00b1 0.4\n79.6 \u00b1 0.4\nMAGNA\n85.4 \u00b1 0.6\n73.7 \u00b1 0.5\n81.4 \u00b1 0.2\n \u00b1  \u00b1  \u00b1 \u22c6: based on the implementation in https://github.com/DropEdge/DropEdge; \u22c4: replace the feed forward layer with elu used in GAT.\nTable 1: Node classification accuracy on Cora, Citeseer, Pubmed. MAGNA achieves state-of-the-art.\nTable 1: Node classification accuracy on Cora, Citeseer, Pubmed. MAGNA achieves state-of-the-art.\nWelling, 2017]; and (2) a benchmark dataset ogbn-arxiv on 170k nodes and 1.2m edges from the Open Graph Benchmark [Weihua Hu, 2020]. We follow the standard data splits for all datasets. Further information about these datasets is summarized in the Appendix. Baselines We compare against a comprehensive suite of state-of-the-art GNN methods including: GCNs [Kipf and Welling, 2017], Chebyshev filter based GCNs [Defferrard et al., 2016], DualGCN [Zhuang and Ma, 2018], JKNet [Xu et al., 2018], LGCN [Gao et al., 2018], Diffusion-GCN (Diff-GCN) [Klicpera et al., 2019b], APPNP [Klicpera et al., 2019a], Graph U-Nets (g-U-Nets) [Gao and Ji, 2019], and GAT [Veli\u02c7ckovi\u00b4c et al., 2018]. Experimental Setup For datasets Cora, Citeseer and Pubmed, we use 6 MAGNA blocks with hidden dimension 512 and 8 attention heads. For the large-scale ogbn-arxiv dataset, we use 2 MAGNA blocks with hidden dimension 128 and 8 attention heads. Refer to Appendix for detailed description of all hyper-parameters and evaluation settings. Results MAGNA achieves the best on all datasets (Tables 1 and 2) 7, out-performing mutlihop baselines such as Diffusion GCN, APPNP and JKNet. The baseline performance and their embedding dimensions are from the previous papers. Appendix Table 6 further demonstrates that large 512 dimension embedding only benefits the expressive MAGNA, whereas GAT and Diffusion GCN performance degrades. Ablation study We report (Table 1) the model performance after removing each component of MAGNA (layer normalization, attention diffusion and feed forward layers) from every MAGNA layer. Note that the model is equivalent to GAT without these three components. We observe that diffusion and layer normalization play a crucial role in improving the node classification performance for all datasets. Since MAGNA computes attention diffusion in a recursive manner, layer normalization is crucial in ensuring training stability [Ba et al., 2016]. Meanwhile, comparing to GAT (see the next-to-last row of Table 1), attention diffusion allows multihop attention in every layer to benefit node classification.\nAblation study We report (Table 1) the model performance after removing each component of MAGNA (layer normalization, attention diffusion and feed forward layers) from every MAGNA layer. Note that the model is equivalent to GAT without these three components. We observe that diffusion and layer normalization play a crucial role in improving the node classification performance for all datasets. Since MAGNA computes attention diffusion in a recursive manner, layer normalization is crucial in ensuring training stability [Ba et al., 2016]. Meanwhile, comparing to GAT (see the next-to-last row of Table 1), attention diffusion allows multihop attention in every layer to benefit node classification.\n7We also compared to GAT and Diffusion-GCN (with LayerNorm and feed-forward Layer) over random splits in Appendix.\n7We also compared to GAT and Diffusion-GCN (with LayerNorm and feed-forward Layer) over random splits in Appendix.\n# 4.2 Task 2: Knowledge Graph Completion\nDatasets We evaluate MAGNA on standard benchmark knowledge graphs: WN18RR [Dettmers et al., 2018] and FB15K-237 [Toutanova and Chen, 2015]. See the statistics of these KGs in Appendix. Baselines We compare MAGNA with state-of-the-art baselines, including (1) translational distance based models: TransE [Bordes et al., 2013] and its latest extension RotatE [Sun et al., 2019], OTE [Tang et al., 2020] and ROTH [Chami et al., 2020]; (2) semantic matching based models: ComplEx [Trouillon et al., 2016], QuatE [Zhang et al., 2019], CoKE [Wang et al., 2019b], ConvE [Dettmers et al., 2018], DistMult [Yang et al., 2015], TuckER [Balazevic et al., 2019] and AutoSF [Zhang et al., 2020b]; (3) GNN-based models: R-GCN [Schlichtkrull et al., 2018], SACN [Shang et al., 2019] and A2N [Bansal et al., 2019]. Experimental Setup We use the multi-layer MAGNA as encoder for both FB15k-237 and WN18RR. We randomly initialize the entity embedding and relation embedding as the input of the encoders, and set the dimensionality of the initialized entity/relation vector as 100 used in DistMult [Yang et al., 2015]. We select other MAGNA model hype-parameters, including number of layers, hidden dimension, head number, top-k, learning rate, hop number, teleport probability \u03b1 and dropout ratios (see the settings of these parameter in Appendix), by a random search during the training. Training procedure We use the standard training procedure used in previous KG embedding models [Balazevic et al., 2019; Dettmers et al., 2018] (Appendix for details). We follow an encoder-decoder framework: The encoder applies the proposed MAGNA model to compute the entity embeddings. The decoder makes link prediction given the embeddings. To show the power of MAGNA, we employ a simple decoder DistMult [Yang et al., 2015]. Evaluation We use the standard split for the benchmarks, and the standard testing procedure of predicting tail (head) entity given the head (tail) entity and relation type. We exactly follow the evaluation used by all previous works, namely the Mean Reciprocal Rank (MRR), Mean Rank (MR), and hit rate at K (H@K). See Appendix for a detailed description of this standard setup. Results MAGNA achieves new state-of-the-art in knowledge graph completion on all four metrics (Table 3). MAGNA compares favourably to both the most recent shallow embedding methods (QuatE), and deep embedding methods (SACN). Note that with the same decoder (DistMult), MAGNA using its own embeddings achieves drastic improvements over using the corresponding DistMult embeddings.\n# 4.3 MAGNA Model Analysis\nHere we present (1) spectral analysis results, (2) robustness to hyper-parameter changes, and (3) attention distribution analysis to show the strengths of MAGNA. Spectral Analysis: Why MAGNA works for node classification? We compute the eigenvalues of the graph Laplacian\n<div style=\"text-align: center;\">Table 2: Node classification accuracy on the OGB Arxiv dataset.</div>\nModels\nWN18RR\nFB15k-237\nMR\nMRR\nH@1\nH@3\nH@10\nMR\nMRR\nH@1\nH@3\nH@10\nTransE [Bordes et al., 2013]\n3384\n.226\n-\n-\n.501\n357\n.294\n-\n-\n.465\nRotatE [Sun et al., 2019]\n3340\n.476\n.428\n.492\n.571\n177\n.338\n.241\n.375\n.533\nOTE [Tang et al., 2020]\n-\n.491\n.442\n.511\n.583\n-\n.361\n.267\n.396\n.550\nROTH [Chami et al., 2020]\n-\n.496\n.449\n.514\n.586\n-\n.344\n.246\n.380\n.535\nComplEx [Trouillon et al., 2016]\n5261\n.44\n.41\n.46\n.51\n339\n.247\n.158\n.275\n.428\nQuatE [Zhang et al., 2019]\n2314\n.488\n.438\n.508\n.582\n-\n.366\n.271\n.401\n.556\nCoKE [Wang et al., 2019b]\n-\n.475\n.437\n.490\n.552\n-\n.361\n.269\n.398\n.547\nConvE [Dettmers et al., 2018]\n4187\n.43\n.40\n.44\n.52\n244\n.325\n.237\n.356\n.501\nDistMult [Yang et al., 2015]\n5110\n.43\n.39\n.44\n.49\n254\n.241\n.155\n.263\n.419\nTuckER [Balazevic et al., 2019]\n-\n.470\n.443\n.482\n.526\n-\n.358\n.266\n.392\n.544\nAutoSF [Zhang et al., 2020b]\n-\n.490\n.451\n-\n.567\n-\n.360\n.267\n-\n.552\nR-GCN [Schlichtkrull et al., 2018]\n-\n-\n-\n-\n-\n-\n.249\n.151\n.264\n.417\nSACN [Shang et al., 2019]\n-\n.47\n.43\n.48\n.54\n-\n.35\n.26\n.39\n.54\nA2N [Bansal et al., 2019]\n-\n.45\n.42\n.46\n.51\n-\n.317\n.232\n.348\n.486\nMAGNA + DistMult\n2545\n.502\n.459\n.519\n.589\n138\n.369\n.275\n.409\n.563\nTable 3: KG Completion on WN18RR and FB15k-237. MAGNA achieves state of the art.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/84d8/84d8f46d-cec8-47ec-8b7b-d1d3556bb49d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Analysis of MAGNA on Cora. (a) Influence of MAGNA on Laplacian eigenvalues. (b) Effect of depth on performance. (c) Effec of hop number K on performance. (d) Effect of teleport probability \u03b1.</div>\nof the attention matrix A, \u02c6\u03bbg i , and compare to that of the diffused matrix A, \u03bbg i . Figure 3 (a) shows the ratio \u02c6\u03bbg i /\u03bbg i on the Cora dataset. Low eigenvalues corresponding to large-scale structure in the graph are amplified (up to a factor of 8), while high eigenvalues corresponding to eigenvectors with noisy information are suppressed [Klicpera et al., 2019b].\nof the attention matrix A, \u02c6\u03bbg i , and compare to that of the diffused matrix A, \u03bbg i . Figure 3 (a) shows the ratio \u02c6\u03bbg i /\u03bbg i on the Cora dataset. Low eigenvalues corresponding to large-scale structure in the graph are amplified (up to a factor of 8), while high eigenvalues corresponding to eigenvectors with noisy information are suppressed [Klicpera et al., 2019b]. MAGNA Model Depth Here we conduct experiments by varying the number of GCN, Diffusion-GCN (PPR based) GAT and our MAGNA layers to be 3, 6, 12, 18 and 24 for node classification on Cora. Results in Fig. 3 (b) show that deep GCN, Diffusion-GCN and GAT (even with residual connection) suffer from degrading performance, due to the over-smoothing problem [Li et al., 2018; Wang et al., 2019a]. In contrast, the MAGNA model achieves consistent best results even with 18 layers, making deep MAGNA model robust and expressive. Notice that GAT with 18 layers cannot out-perform MAGNA with 3 layers and K=6 hops, although they have the same receptive field. Effect of K and \u03b1 Figs. 3 (c) and (d) report the effect of hop number K and teleport probability \u03b1 on model performance. We observe significant increase in performance when\nEffect of K and \u03b1 Figs. 3 (c) and (d) report the effect of hop number K and teleport probability \u03b1 on model performance. We observe significant increase in performance when\nconsidering multi-hop neighbors information (K > 1). However, increasing the hop number K has a diminishing returns, for K \u22656. Moreover, we find that the optimal K is correlated with the largest node average shortest path distance (e.g., 5.27 for Cora). This provides a guideline for choosing the best K. We also observe that the accuracy drops significantly for larger \u03b1 > 0.25. This is because small \u03b1 increases the lowpass effect (Fig. 3 (a)). However, \u03b1 being too small causes the model to only focus on the most large-scale graph structure and have lower performance.\nAttention Distribution Last we also analyze the learned attention scores of GAT and MAGNA. We first define a discrepancy metric over the attention matrix A for node vi as \u2206i = \u2225A[i,:]\u2212Ui\u2225 degree(vi) [Shanthamallu et al., 2020], where Ui is the uniform distribution score for the node vi. \u2206i gives a measure of how much the learnt attention deviates from an uninformative uniform distribution. Large \u2206i indicates more meaningful attention scores. Fig. 4 shows the distribution of the discrepancy metric of the attention matrix of the 1st head w.r.t. the first layer of MAGNA and GAT. Observe that at-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b303/b30392f9-7745-444f-b683-bc4af6d1d361.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Attention weight distribution on Cora.</div>\ntention scores learned in MAGNA have much larger discrepancy. This shows that MAGNA is more powerful than GAT in distinguishing important nodes and assigns attention scores accordingly.\n# 5 Related Work\nOur proposed MAGNA belongs to the family of Graph Neural Network (GNN) models [Battaglia et al., 2018; Wu et al., 2020; Kipf and Welling, 2017; Hamilton et al., 2017], while taking advantage of graph attention and diffusion techniques. Graph Attention Neural Networks (GATs) generalize attention operation to graph data. GATs allow for assigning different importance to nodes of the same neighborhood at the feature aggregation step [Veli\u02c7ckovi\u00b4c et al., 2018]. Based on such framework, different attention-based GNNs have been proposed, including GaAN [Zhang et al., 2018], AGNN [Thekumparampil et al., 2018], GeniePath [Liu et al., 2019]. However, these models only consider direct neighbors for each layer of feature aggregation, and suffer from oversmoothing when they go deep [Wang et al., 2019a]. Diffusion based Graph Neural Network Recently Graph Diffusion Convolution [Klicpera et al., 2019b; Klicpera et al., 2019a] proposes to aggregate information from a larger (multi-hop) neighborhood at each layer, by sparsifying a generalized form of graph diffusion. This idea was also explored in [Liao et al., 2019; Luan et al., 2019; Xhonneux et al., 2020; Klicpera et al., 2019a] for multi-scale Graph Convolutional Networks. However, these methods do not incorporate attention mechanism which is crucial to model performance, and do not make use of edge embeddings (e.g., Knowledge graph) [Klicpera et al., 2019b]. Our approach defines a novel multi-hop context-dependent self-attention GNN which resolves the over-smoothing issue of GAT architectures [Wang et al., 2019a]. [Isufi et al., 2020; Cucurull et al., 2018; Feng et al., 2019] also extends attention mechanism for multihop information aggregation, but they require different set of parameters to compute the attention to neighbors of different hops, making these approaches much more expensive compared to MAGNA, and were not extended to the knowledge graph settings.\n# 6 Conclusion\nWe proposed Multi-hop Attention Graph Neural Network (MAGNA), which brings together benefits of graph atten-\ntion and diffusion techniques in a single layer through attention diffusion, layer normalization and deep aggregation. MAGNA enables context-dependent attention between any pair of nodes in the graph in a single layer, enhances largescale structural information, and learns more informative attention distribution. MAGNA improves over all state-of-theart methods on the standard tasks of node classification and knowledge graph completion.\n# References\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [Bahdanau et al., 2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. [Balazevic et al., 2019] Ivana Balazevic, Carl Allen, and Timothy Hospedales. Tucker: Tensor factorization for knowledge graph completion. In EMNLP, 2019. [Bansal et al., 2019] Trapit Bansal, Da-Cheng Juan, Sujith Ravi, and Andrew McCallum. A2n: Attending to neighbors for knowledge graph inference. In ACL, 2019. [Battaglia et al., 2018] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [Bergstra and Bengio, 2012] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, pages 281\u2013305, 2012. [Bordes et al., 2013] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In NeurIPS, 2013. [Chami et al., 2020] Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher R\u00b4e. Low-dimensional hyperbolic knowledge graph embeddings. In ACL, 2020. [Cucurull et al., 2018] Guillem Cucurull, Konrad Wagstyl, Arantxa Casanova, Petar Veli\u02c7ckovi\u00b4c, Estrid Jakobsen, Michal Drozdzal, Adriana Romero, Alan Evans, and Yoshua Bengio. Convolutional neural networks for mesh-based parcellation of the cerebral cortex. In MIDL, 2018. [Defferrard et al., 2016] Micha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS, 2016. [Dettmers et al., 2018] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In AAAI, 2018. [Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. [Feng et al., 2019] Chenyuan Feng, Zuozhu Liu, Shaowei Lin, and Tony QS Quek. Attention-based graph convolutional network for recommendation system. In ICASSP, 2019. [Gao and Ji, 2019] Hongyang Gao and Shuiwang Ji. Graph u-nets. In ICML, 2019. [Gao et al., 2018] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In KDD, 2018. [Hamilton et al., 2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, 2017. [Isufi et al., 2020] Elvin Isufi, Fernando Gama, and Alejandro Ribeiro. Edgenets: Edge varying graph neural networks. arXiv preprint arXiv:2001.07620, 2020.\n[Kipf and Welling, 2017] Thomas N Kipf and Max Welling. Semisupervised classification with graph convolutional networks. In ICLR, 2017. [Klicpera et al., 2019a] Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00a8unnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [Klicpera et al., 2019b] Johannes Klicpera, Stefan Wei\u00dfenberger, and Stephan G\u00a8unnemann. Diffusion improves graph learning. In NeurIPS, 2019. [Li et al., 2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semisupervised learning. In AAAI, 2018. [Liao et al., 2019] Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. Lanczosnet: Multi-scale deep graph convolutional networks. In ICLR, 2019. [Liu et al., 2019] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath: Graph neural networks with adaptive receptive paths. In AAAI, 2019. [Liu et al., 2020] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In KDD, 2020. [Lofgren, 2015] Peter Lofgren. Efficient Algorithms for Personalized PageRank. PhD thesis, Stanford University, 2015. [Luan et al., 2019] Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-scale deep graph convolutional networks. In NeurIPS, 2019. [Mohar et al., 1991] Bojan Mohar, Y Alavi, G Chartrand, and OR Oellermann. The laplacian spectrum of graphs. Graph theory, combinatorics, and applications, pages 871\u2013898, 1991. [Ng et al., 2002] Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In NeurIPS, 2002. [Nguyen et al., 2020] Xuan-Phi Nguyen, Shafiq Joty, Steven CH Hoi, and Richard Socher. Tree-structured attention with hierarchical accumulation. In ICLR, 2020. [Oono and Suzuki, 2020] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In ICLR, 2020. [Sandryhaila and Moura, 2013a] Aliaksei Sandryhaila and Jos\u00b4e MF Moura. Discrete signal processing on graphs. TSP, pages 1644\u2013 1656, 2013. [Sandryhaila and Moura, 2013b] Aliaksei Sandryhaila and Jos\u00b4e MF Moura. Discrete signal processing on graphs: Graph fourier transform. In ICASSP, 2013. [Schlichtkrull et al., 2018] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In ESWC, 2018. [Sen et al., 2008] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, pages 93\u2013106, 2008. [Shang et al., 2019] Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, and Bowen Zhou. End-to-end structure-aware convolutional networks for knowledge base completion. In AAAI, 2019. [Shanthamallu et al., 2020] Uday Shankar Shanthamallu, Jayaraman J Thiagarajan, and Andreas Spanias. A regularized attention mechanism for graph attention networks. In ICASSP, 2020. [Sun et al., 2019] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In ICLR, 2019. [Tang et al., 2020] Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, and Bowen Zhou. Orthogonal relation transforms with graph context modeling for knowledge graph embedding. In ACL, 2020. [Thekumparampil et al., 2018] Kiran K Thekumparampil, Chong\nneural network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018. [Toutanova and Chen, 2015] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In CVSC-WS, 2015. [Tremblay et al., 2018] Nicolas Tremblay, Paulo Gonc\u00b8alves, and Pierre Borgnat. Design of graph filters and filterbanks. In Cooperative and Graph Signal Processing, pages 299\u2013324. Elsevier, 2018. [Trouillon et al., 2016] Th\u00b4eo Trouillon, Johannes Welbl, Sebastian Riedel, \u00b4Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In ICML, 2016. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [Veli\u02c7ckovi\u00b4c et al., 2018] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. [Wang et al., 2019a] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Improving graph attention networks with large margin-based constraints. In NeurIPS-WS, 2019. [Wang et al., 2019b] Quan Wang, Pingping Huang, Haifeng Wang, Songtai Dai, Wenbin Jiang, et al. Coke: Contextualized knowledge graph embedding. arXiv preprint arXiv:1911.02168, 2019. [Wang et al., 2019c] Yaushian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating tree structures into selfattention. In EMNLP, 2019. [Weihua Hu, 2020] etc. Weihua Hu, Matthias Fey. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS, 2020. [Wu et al., 2020] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. IEEE Trans Neural Netw Learn Syst, 2020. [Xhonneux et al., 2020] Louis-Pascal A. C. Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In ICML, 2020. [Xiong et al., 2020] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In ICML, 2020. [Xu et al., 2018] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [Yang et al., 2015] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In ICLR, 2015. [Zhang et al., 2018] Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. In UAI, 2018. [Zhang et al., 2019] Shuai Zhang, Yi Tay, Lina Yao, and Qi Liu. Quaternion knowledge graph embedding. In NeurIPS, 2019. [Zhang et al., 2020a] Jiawei Zhang, Haopeng Zhang, Li Sun, and Congying Xia. Graph-bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140, 2020. [Zhang et al., 2020b] Yongqi Zhang, Quanming Yao, Wenyuan Dai, and Lei Chen. Autosf: Searching scoring functions for knowledge graph embedding. In ICDE, 2020. [Zhuang and Ma, 2018] Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised classification. In WWW, 2018.\n# A Attention Diffusion Approximation Proposition\n# A Attention Diffusion Approximation\nAs mentioned in Section 2.2, we use the following equation and proposition to efficiently approximate the attention diffused feature aggregation AH(l).\n(12)\nProof. Let K > 0 be the total number of iterations (i.e., hop number of graph attention diffusion) and we approximate \u02c6 H(l) by Z(K). After K-th iteration, we can get\n(13)\n\ufffd The term (1 \u2212\u03b1)KAK converges to 0 as \u03b1 \u2208(0, 1] and AK i,j \u2208(0, 1] when K \u2192\u221e, and thus limK\u2192\u221eZ(K) = (\ufffd\u221e i=0 \u03b1(1 \u2212\u03b1)iAi)H(l) = AH(l). Complexity analysis. Suppose the graph contains E edges, using the above approximation, there are O(|E|) message communications in total, with a constant factor corresponding to the number of hops K. In practice, we find that choosing the values of K such that 3 \u2264K \u226410 results in good model performance.\n\ufffd The term (1 \u2212\u03b1)KAK converges to 0 as \u03b1 \u2208(0, 1] an AK i,j \u2208(0, 1] when K \u2192\u221e, and thus limK\u2192\u221eZ(K)  (\ufffd\u221e i=0 \u03b1(1 \u2212\u03b1)iAi)H(l) = AH(l).\n\n\ufffd Complexity analysis. Suppose the graph contains E edges, using the above approximation, there are O(|E|) message communications in total, with a constant factor corresponding to the number of hops K. In practice, we find that choosing the values of K such that 3 \u2264K \u226410 results in good model performance.\n# B Connection to Transformer\nGiven a sequence of tokens, the Transformer architecture makes uses of multi-head attention between all pairs of tokens, and can be viewed as performing message-passing on a fully connected graph between all tokens. A na\u00a8\u0131ve application of Transformer on graphs would require computation of all pairwise attention values. Such approach, however, would not make effective use of the graph structure, and could not scale to large graphs. In contrast, Graph Attention Network [Veli\u02c7ckovi\u00b4c et al., 2018] leverages the graph structure and only computes attention values and perform message passing between direct neighbors. However, it has a limited receptive field (restricted to one-hop neighborhood) and the attention score (Figure 1) that is independent of the multi-hop context for prediction. Transformer consists of self-attention layer followed by feed-forward layer. We can organize the self-attention layer in transformer as the following:\nAttention(Q, K, V ) = softmax(QKT \u221a d ) \ufffd \ufffd\ufffd \ufffd Attention matrix V\nAttention(Q, K, V ) = softmax(QK \u221a d ) V\n(14)\n\ufffd \ufffd\ufffd \ufffd where Q = K = V . The softmax part can be demonstrated as an attention matrix computed by scaled dot-product attention over a complete graph8 with self-loop. Computation of attention over complete graph is expensive, Transformers are usually limited by a fixed-length context (e.g., 512 in\n8All nodes are connected with each other.\nBERT [Devlin et al., 2019]) in the setting of language modeling, and thus cannot handle large graphs. Therefore direct application of the transformer model cannot capture the graph structure in a scalable way. In the past, graph structure is usually encoded implicitly by special position embeddings [Zhang et al., 2020a] or welldesigned attention computation [Wang et al., 2019c; Nguyen et al., 2020]. However, none of the methods can compute attention between any pair of nodes at each layer. In contrast, essentially MAGNA places a prior over the attention values via Personalized PageRank, allowing it to compute the attention between any pair of two nodes via attention diffusion, without any impact on its scalability. In particular, MAGNA can handle large graphs as they are usually quite sparse and the graph diameter is usually quite smaller than graph size in practice, resulting in very efficient attention diffusion computation.\n# C Spectral Analysis Background and Proof for Proposition 2\nGraph Fourier Transform. Suppose AN\u00d7N represents the attention matrix of graph G with Ai,j \u22650, and \ufffdN j=1 Ai,j = 1. Let A = V \u039bV \u22121 be the Jordan\u2019s decomposition of graph attention matrix A, where V is the square N \u00d7 N matrix whose i-th column is the eigenvector vi of A, and \u039b is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, i.e., \u039bi,i = \u03bbi. Then, for a given vector x, its Graph Fourier Transform [Sandryhaila and Moura, 2013b] is defined as\n(15)\nwhere V \u22121 is denoted as graph Fourier transform matrix. The Inverse Graph Fourier Transform is defined as x = V \u02c6x, which reconstructs the signal from its spectrum. Based on the graph Fourier transform, we can define a graph convolution operation on G as x \u2297G y = V (V \u22121x \u2299V \u22121y), where \u2299 denotes the element-wise product. Graph Attention Diffusion Acts as A Polynomial Graph Filter. A graph filter [Tremblay et al., 2018; Sandryhaila and Moura, 2013b; Sandryhaila and Moura, 2013a] h acts on x as h(A)x = V h(\u039b)V \u22121, where h(\u039b) = diag(h(\u03bb1) \u00b7 \u00b7 \u00b7 h(\u03bbN)). A common choice for h in the literature is a polynomial filter of order M, since it is linear and shift invariant [Sandryhaila and Moura, 2013b; Sandryhaila and Moura, 2013a].\n(16)\nComparing to the graph attention diffusion A = \ufffd\u221e i=0 \u03b1(1\u2212 \u03b1)iAi, if we set \u03b2\u2113= \u03b1(1 \u2212\u03b1)\u2113, we can view graph attention diffusion as a polynomial filter. Spectral Analysis. The eigenvectors of the power matrix A2 are same as A, since A2 = (V \u039bV \u22121)(V \u039bV \u22121) = V \u039b(V \u22121V )\u039bV \u22121 = V \u039b2V \u22121. By that analogy, we can get that An = V \u039bnV \u22121. Therefore, the summation of the power series of A has the same eigenvectors as A. Therefore by properties of eigenvectors and Equation 3, we obtain:\nProposition 5. The set of eigenvectors for A and A are the same. Lemma 1. Let \u03bbi and \u02c6\u03bbi be the i-th eigenvalues of A and A, respectively. Then, we have\nLemma 1. Let \u03bbi and \u02c6\u03bbi be the i-th eigenvalues of A and A, respectively. Then, we have\n(17)\nProof. The symmetric normalized graph Laplacian of G is Lsym = I \u2212D\u22121 2 AD\u22121 2 , where D = diag([d1, d2, \u00b7 \u00b7 \u00b7 , dN]), and di = \ufffdAi,j j=1 . As A is the attention matrix of graph G, di = 1 and thus D = I. Therefore, Lsym = I \u2212A. Let \u03bbi be the eigenvalues of A, the eigenvalues of the symmetric normalized Laplacian of G Lsym is \u00af\u03bbi = 1 \u2212\u03bbi. Meanwhile, for every eigenvalue \u00af\u03bbi of the normalized graph Laplacian Lsym, we have 0 \u2264\u00af\u03bbi \u22642 [Mohar et al., 1991], and thus \u22121 \u2264\u03bbi \u22641. As 0 < \u03b1 < 1 and thus |(1 \u2212\u03b1)\u03bbi| \u2264(1 \u2212\u03b1) < 1. Therefore, ((1 \u2212\u03b1)\u03bbi)K \u21920 when K \u2192\u221e, and \u02c6\u03bbi = limK\u2192\u221e \ufffdK \u2113=0 \u03b1(1 \u2212\u03b1)\u2113\u03bb\u2113 i = limK\u2192\u221e \u03b1(1\u2212((1\u2212\u03b1)\u03bbi)K) 1\u2212(1\u2212\u03b1)\u03bbi = \u03b1 1\u2212(1\u2212\u03b1)\u03bbi .\n\nSection 3 further defines the eigenvalues of the Laplacian matrices, \u02c6\u03bbg i and \u03bbg i respectively. They satisfy: \u02c6\u03bbg i = 1 \u2212\u02c6\u03bbi and \u03bbg i = 1 \u2212\u03bbi, and \u03bbg i \u2208[0, 2] (proved by [Ng et al., 2002]).\n# D Graph Learning Tasks\nNode classification and knowledge graph link prediction are two representative and common tasks in graph learning. We first define the task of node classification:\nDefinition 1. Node classification Suppose that X \u2208RNn\u00d7d represents the node input features, where each row xi = Xi: is a d-dimensional vector of attribute values of node vi \u2208V (1 \u2264i \u2264N). Vl \u2282V consists of a set of labeled nodes, and the labels are from T , node classification is to learn the map function f : (X, G) \u2192T , which predicts the labels of the remaining un-labeled nodes V/Vl. Knowledge graph (KG) is a heterogeneous graph describing entities and their typed relations to each other. KG is defined by a set of entities (nodes) vi \u2208V, and a set of relations (edges) e = (vi, rk, vj) connecting nodes vi and vj via relation rk. We then define the task of knowledge graph completion: Definition 2. KG completion refers to the task of predicting an entity that has a specific relation with another given entity [Bordes et al., 2013], i.e., predicting head h given a pair of relation and entity (r, t) or predicting tail t given a pair of head and relation (h, r).\n# E Dataset Statistics\nNode classification. We show the dataset statistics of the node classification benchmark datasets in Table 4. Knowledge Graph Link Prediction. We show the dataset statistics of the knowledge graph benchmarks in Table 5.\n# F Knowledge Graph Training and Evaluation\nTraining. The standard knowledge graph completion task training procedure is as follows. We add the reverse-direction triple (t, r\u22121, h) for each triple (h, r, t) to construct an undirected knowledge graph G. Following the training procedure introduced in [Balazevic et al., 2019; Dettmers et al., 2018], we use 1-N scoring, i.e. we simultaneously score entityrelation pairs (h, r) and (t, r\u22121) with all entities, respectively. We explore KL diversity loss with label smoothing as the optimization function. Inference time procedure. For each test triplet (h, r, t), the head h is removed and replaced by each of the entities appearing in KG. Afterward, we remove from the corrupted triplets all the ones that appear either in the training, validation or test set. Finally, we score these corrupted triplets by the link prediction models and then sorted by descending order; the rank of (h, r, t) is finally scored. This whole procedure is repeated while removing the tail t instead of h. And averaged metrics are reported. We report mean reciprocal rank (MRR), mean rank (MR) and the proportion of correct triplets in the top K ranks (Hits@K) for K = 1, 3 and 10. Lower values of MR and larger values of MRR and Hits@K mean better performance.\n# G Results\nComparison to Diffusion GCN. Diffusion-GCN [Klicpera et al., 2019b] is also based on Personal Page-Rank (PPR) propagation. Specifically, it performs propagation over the adjacent matrix. Comparing to MAGNA, there is no LayerNorm and Feed Forward layers in standard Diffusion GCN. To clarify how much of the gain in performance depends on the page-rank-based propagation compared to the attention propagation in MAGNA, we add the two modules: LayerNorm and Feed Forward layer, into Diffusion-GCN, and conduct experiments over Cora, Citeseer and Pubmed, respectively. Table 6 shows the comparison results. From this table, we observe that adding layer normalization and feed forward layer gets the similar GNN structure to MAGNA, but does not benefit for node classification. And this implies that the PPR based attention propagation in MAGNA is more effective than PPR propagation over adjacent matrix in Diffusion GCN.\n# Comparison over Random Splitting\nsplit nodes in each graph for training, validation and testing following the same ratio in the standard data splits. We conduct experiments with MAGNA, Diffusion GCN (PPR) and GAT over 10 different random splits over Cora, Citeseer and Pubmed, respectively. For each algorithm, we apply random search [Bergstra and Bengio, 2012] for hype-parameter tuning. We report the average classification accuracy with standard deviation over 10 different splits in Table 7. From this table, we observe that MAGNA gets the best average accuracy over all data sets. However, it is noted that the standard deviation is quite large comparing to the that from the standard split in Table 6. This means that the performance is quite sensitive to the graph split. This observation is consistent wit the conclusion in [Weihua Hu, 2020]: random splitting is often problematic in graph learning as they are impractical in\nName\nNodes\nEdges\nClasses\nFeatures\nTrain/Dev/Test\nCora\n2,708\n5,429\n7\n1,433\n140/500/1,000\nCiteseer\n3,327\n4,732\n6\n3,703\n120/500/1,000\nPubmed\n19,717\n88,651\n3\n500\n60/500/1,000\nogbn-arxiv\u2020\n169,343\n1,166,243\n40\n128\n90,941/29,799/48,603\n \n The data is available at https://ogb.stanford.edu/docs/nodeprop/.\nDataset\n#Entities\n#Relations\n#Train\n#Dev\n#Test\n#Avg. Degree\nWN18RR\n40,943\n11\n86,835\n3034\n3134\n2.19\nFB15k-237\n14,541\n237\n272,115\n17,535\n20,466\n18.17\nel\nCora\nCiteseer\nPubmed\nusion-GCN (PPR) [Klicpera et al., 2019b]\n83.6 \u00b1 0.2\n73.4 \u00b1 0.3\n78.7 \u00b1 0.4\u22c6\nusion-GCN (PPR) + LayerNorm + FF\u22c4\n83.4 \u00b1 0.4\n72.3 \u00b1 0.4\n78.1\u00b1 0.5\n (hidden dimension = 512)\u2217\n83.1 \u00b1 0.5\n71.3 \u00b1 0.4\n78.3\u00b1 0.4\nGNA\n85.4 \u00b1 0.6\n73.7 \u00b1 0.5\n81.4 \u00b1 0.2\nFF: Feed Forward (FF) layer, and our implementation is based on Diffusion GCN in\ntorch geometric (https://github.com/rusty1s/pytorch geometric).\nThe best number derived from Diffusion GCN with \u201cPPR\u201d. This is different from the\nmber in Table 1 of main body, which comes from Diffusion GCN with \u201cHeat\u201d.\nThe results of GAT with the same hidden dimension in MAGNA.\ne 6: Comparison of Diffusion GCN (PPR), GAT (with hidden\nension = 512) and MAGNA for Node classification accuracy on\na, Citeseer, Pubmed.\nHyper-parameters\nSearch Space\nType\nHidden Dimension\n512\nFixed\u22a2\nHead Number\n8\nFixed\nLayer Number\n6\nFixed\nLearning rate\n[5 \u00d7 10\u22125, 10\u22123]\nRange\u22c6\nHop Number\n[2, 3, \u00b7 \u00b7 \u00b7 , 10]\nChoice\u22c4\nTeleport probability \u03b1\n[0.05, 0.6]\nRange\nDropout (attention, feature)\n[0.1, 0.6]\nRange\nWeight Decay\n[10\u22126, 10\u22125]\nRange\nOptimizer\nAdam\nFixed\n<div style=\"text-align: center;\"> \u00b1  \u00b1  \u00b1 \u22c4: FF: Feed Forward (FF) layer, and our implementation is based on Diffusion GCN in pytorch geometric (https://github.com/rusty1s/pytorch geometric). \u22c6: The best number derived from Diffusion GCN with \u201cPPR\u201d. This is different from the number in Table 1 of main body, which comes from Diffusion GCN with \u201cHeat\u201d. \u2217: The results of GAT with the same hidden dimension in MAGNA.</div>\nTable 6: Comparison of Diffusion GCN (PPR), GAT (with hidden dimension = 512) and MAGNA for Node classification accuracy on Cora, Citeseer, Pubmed.\nModel\nCora\nCiteseer\nPubmed\nDiffusion GCN (PPR) [Klicpera et al., 2019b]\n83.6 \u00b1 1.8\n71.7 \u00b1 1.1\n79.6 \u00b1 2.5\nDiffusion GCN (PPR) + (LayerNorm & FF)\n83.3 \u00b1 1.9\n69.7 \u00b1 1.2\n79.7 \u00b1 2.3\nGAT [Veli\u02c7ckovi\u00b4c et al., 2018]\n82.2 \u00b1 1.4\n71.0 \u00b1 1.2\n78.3 \u00b1 3.0\nGAT + (LayerNorm & FF)\n82.5 \u00b1 2.2\n69.5 \u00b1 1.1\n78.2 \u00b1 2.0\nMAGNA\n83.6 \u00b1 1.2\n72.1 \u00b1 1.2\n80.3 \u00b1 2.4\nTable 7: Results of MAGNA on Citeseer, Cora and Pubmed with random splitting. The baselines are Diffusion GCN (PPR) and GAT.\nTable 7: Results of MAGNA on Citeseer, Cora and Pubmed with random splitting. The baselines are Diffusion GCN (PPR) and GAT. real scenarios, and can lead to large performance variation. Therefore, in our main results, we use the standard split and also record standard deviation to demonstrate significance of the results.\nreal scenarios, and can lead to large performance variation. Therefore, in our main results, we use the standard split and also record standard deviation to demonstrate significance of the results.\n# H Hyper-parameter Settings\nHyper-parameter settings for node classification. The best models are selected according to the classification accuracy on the validation set by early stopping with window size 200. For each data set, the hyper-parameters are determined by a random search [Bergstra and Bengio, 2012], including learning rate, hop number, teleport probability \u03b1 and dropout ratios. The hyper-parameter search space is show in Tables 8 (for Cora, Citeseer and Pubmed) and 9 (for ogbn-arxiv). Hyper-parameter settings for link prediction on KG. For each KG, the hyper-parameters are determined by a random search [Bergstra and Bengio, 2012], including number of layers, learning rate, hidden dimension, batch-size, head number, hop number, teleport probability \u03b1 and dropout ratios. The hyper-parameter search space is show in Table 10.\nHyper-parameters\nSearch Space\nType\nHidden Dimension\n512\nFixed\u22a2\nHead Number\n8\nFixed\nLayer Number\n6\nFixed\nLearning rate\n[5 \u00d7 10\u22125, 10\u22123]\nRange\u22c6\nHop Number\n[2, 3, \u00b7 \u00b7 \u00b7 , 10]\nChoice\u22c4\nTeleport probability \u03b1\n[0.05, 0.6]\nRange\nDropout (attention, feature)\n[0.1, 0.6]\nRange\nWeight Decay\n[10\u22126, 10\u22125]\nRange\nOptimizer\nAdam\nFixed\n\u22a2Fixed: a constant value;: Range: a value range with lower bound\n\u22a2Fixed: a constant value; \u22c6: Range: a value range with lower bound and higher bound; \u22c4: Choice: a set of values.\n\u22a2Fixed: a constant value; \u22c6: Range: a value range with lower bound and higher bound; \u22c4: Choice: a set of values.\nTable 8: Hyper-parameter search space used for node classification on Cora, Citeseer and Pubmed\nHyper-parameters\nSearch Space\nType\nHidden Dimension\n128\nFixed\nHead Number\n8\nFixed\nLayer Number\n2\nFixed\nLearning rate\n[0.001, 0.01]\nRange\nHop Number\n[3, 4, 5, 6]\nChoice\nTeleport probability \u03b1\n[0.05, 0.6]\nRange\nDropout (attention, feature)\n[0.1, 0.6]\nRange\nWeight Decay\n[10\u22125, 10\u22124]\nRange\nOptimizer\nAdam\nFixed\nTable 9: Hyper-parameter search space for node classification on ogbn-arxiv\nHyper-parameters\nSearch Space\nType\nInitial Entity/Relation Dimension\n100\nFixed\nNumber of layers\n[2, 3]\nChoice\nLearning rate\n[10\u22124, 5 \u00d7 10\u22123]\nRange\nHidden Dimension\n[256, 512, 768]\nChoice\nBatch size\n[1024, 2048, 3072]\nChoice\nHead Number\n[4, 8]\nChoice\nHop Number\n[2, 3, 4, 5, 6]\nChoice\nTeleport probability \u03b1\n[0.05, 0.6]\nRange\nDropout (attention, feature)\n[0.1, 0.6]\nRange\nWeight Decay\n[10\u221210, 10\u22128]\nRange\nOptimizer\nAdam\nFixed\nTable 10: Hyper-parameter search space for link prediction on KG\n",
    "paper_type": "method",
    "attri": {
        "background": "Self-attention mechanisms in graph neural networks (GNNs) have led to state-of-the-art performance in various graph representation learning tasks. However, existing methods compute attention scores only between directly connected nodes, limiting their ability to capture broader network context. This paper introduces the Multi-hop Attention Graph Neural Network (MAGNA), which incorporates multi-hop context information into GNN attention computations, addressing the limitations of previous approaches.",
        "problem": {
            "definition": "The problem addressed by this paper is the limitation of existing GNNs that can only compute attention between directly connected nodes, which restricts their receptive field and ability to learn from broader graph structures.",
            "key obstacle": "The core obstacle is the oversmoothing problem that arises when stacking multiple GNN layers, which hinders effective learning of non-neighboring interactions in deep architectures."
        },
        "idea": {
            "intuition": "The idea for MAGNA was inspired by the observation that important contextual information can be derived from nodes that are not directly connected, which can enhance the learning process in GNNs.",
            "opinion": "MAGNA proposes a novel multi-hop self-attention mechanism that allows for attention computation between any pair of nodes in the graph, regardless of direct connections, by utilizing a diffusion process.",
            "innovation": "The key innovation of MAGNA lies in its ability to compute attention scores through a diffusion process that aggregates information from multiple hops, effectively capturing long-range interactions and enhancing the model's expressive power."
        },
        "method": {
            "method name": "Multi-hop Attention Graph Neural Network",
            "method abbreviation": "MAGNA",
            "method definition": "MAGNA is a graph neural network that employs a multi-hop attention mechanism to compute context-dependent attention scores between nodes in a graph, allowing for effective long-range message passing.",
            "method description": "MAGNA utilizes a novel attention diffusion process to enhance the receptive field of GNNs and improve learning from graph structures.",
            "method steps": "1. Compute initial attention scores on edges. 2. Diffuse attention scores to compute multi-hop attention values between non-connected nodes. 3. Aggregate messages using the diffused attention scores to update node representations.",
            "principle": "The effectiveness of MAGNA is supported by the Personalized Page Rank (PPR) prior on attention values, which ensures that attention scores reflect the influence of all possible paths connecting node pairs."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on standard datasets for node classification (Cora, Citeseer, Pubmed) and knowledge graph completion (WN18RR, FB15k-237), comparing MAGNA against several baseline methods.",
            "evaluation method": "Performance was assessed using accuracy for node classification and metrics such as Mean Reciprocal Rank (MRR) and Hit rates for knowledge graph completion. Results were statistically analyzed to ensure significance."
        },
        "conclusion": "MAGNA demonstrates significant improvements over state-of-the-art methods in both node classification and knowledge graph completion tasks, showcasing its ability to effectively learn from complex graph structures through enhanced attention mechanisms.",
        "discussion": {
            "advantage": "MAGNA's primary advantages include its ability to capture long-range node interactions and its context-dependent attention computation, which leads to better performance in various graph learning tasks.",
            "limitation": "One limitation of MAGNA is its dependence on hyper-parameter tuning, such as the choice of teleport probability and hop number, which can affect performance and may require careful optimization.",
            "future work": "Future research could explore further refinements to the attention diffusion process, investigate its applicability to other graph-based tasks, and develop methods to automatically tune hyper-parameters."
        },
        "other info": {
            "info1": "MAGNA achieves up to 5.7% relative error reduction on Cora, Citeseer, and Pubmed datasets.",
            "info2": {
                "info2.1": "MAGNA outperforms GAT architectures with fewer layers, demonstrating its efficiency.",
                "info2.2": "The model shows a low-pass effect, effectively filtering out noisy high-frequency information."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem addressed by this paper is the limitation of existing GNNs that can only compute attention between directly connected nodes, which restricts their receptive field and ability to learn from broader graph structures."
        },
        {
            "section number": "3.4",
            "key information": "MAGNA demonstrates significant improvements over state-of-the-art methods in both node classification and knowledge graph completion tasks, showcasing its ability to effectively learn from complex graph structures through enhanced attention mechanisms."
        },
        {
            "section number": "4.1",
            "key information": "MAGNA is a graph neural network that employs a multi-hop attention mechanism to compute context-dependent attention scores between nodes in a graph, allowing for effective long-range message passing."
        },
        {
            "section number": "4.2",
            "key information": "MAGNA's primary advantages include its ability to capture long-range node interactions and its context-dependent attention computation, which leads to better performance in various graph learning tasks."
        },
        {
            "section number": "7.3",
            "key information": "One limitation of MAGNA is its dependence on hyper-parameter tuning, such as the choice of teleport probability and hop number, which can affect performance and may require careful optimization."
        }
    ],
    "similarity_score": 0.5764744288809052,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-1934_natur/papers/Multi-hop Attention Graph Neural Network.json"
}