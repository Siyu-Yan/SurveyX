{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.02210",
    "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
    "abstract": " ABSTRACT\n3 Oct 2024\nWhile in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as indiscriminate miscalibration. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.\n# INTRODUCTION\nLarge Language Models (LLMs) have exhibited emergent capabilities such as advanced creative writing, summarization, translation, arithmetic and commonsense reasoning, etc (Wei et al., 2022; Brown et al., 2020). One of the most fascinating aspects of LLMs is their in-context learning capabilities. In particular, this involves adding pairs of demonstration examples to the prompt, and has been shown to significantly enhance the performance of LLMs (Brown et al., 2020). This capability offers users the significant advantage of utilizing LLMs without the need to train or fine-tune their own models. As a result, there is a growing need for research into calibration techniques (Zhou et al., 2023; Zhao et al., 2021; Jiang et al., 2023; 2021) to ensure the reliability of model outputs as well and improve performance. The concept of calibration in modern deep learning models was first introduced in (Guo et al., 2017) where the authors also proposed metrics (such as Expected Calibration Errors, reliability diagrams, etc) and methods (such as temperature scaling) for characterizing a",
    "bib_name": "cheng2024calibratediscriminateimproveincontext",
    "md_text": "# CALIBRATE TO DISCRIMINATE: IMPROVE IN-CONTEXT LEARNING WITH LABEL-FREE COMPARATIVE INFERENCE\nCALIBRATE TO DISCRIMINATE: IMPROVE IN-CONTEXT LEARNING WITH LABEL-FREE COMPARATIVE INFERENCE\n# Wei Cheng\u2217, Tianlu Wang\u2217, Yanmin Ji, Fan Yang, Keren Tan, Yiyu Zheng Meta Platforms, Inc. {weicheng112, tianluwang}@meta.com\nWei Cheng\u2217, Tianlu Wang\u2217, Yanmin Ji, Fan Yang, Keren Tan, Yiyu Zheng Meta Platforms, Inc. {weicheng112, tianluwang}@meta.com\nWei Cheng\u2217, Tianlu Wang\u2217, Yanmin Ji, Fan Yang, Keren Tan, Yiyu Zhen Meta Platforms, Inc. {weicheng112, tianluwang}@meta.com\n# ABSTRACT\n3 Oct 2024\nWhile in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as indiscriminate miscalibration. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.\n# INTRODUCTION\nLarge Language Models (LLMs) have exhibited emergent capabilities such as advanced creative writing, summarization, translation, arithmetic and commonsense reasoning, etc (Wei et al., 2022; Brown et al., 2020). One of the most fascinating aspects of LLMs is their in-context learning capabilities. In particular, this involves adding pairs of demonstration examples to the prompt, and has been shown to significantly enhance the performance of LLMs (Brown et al., 2020). This capability offers users the significant advantage of utilizing LLMs without the need to train or fine-tune their own models. As a result, there is a growing need for research into calibration techniques (Zhou et al., 2023; Zhao et al., 2021; Jiang et al., 2023; 2021) to ensure the reliability of model outputs as well and improve performance. The concept of calibration in modern deep learning models was first introduced in (Guo et al., 2017) where the authors also proposed metrics (such as Expected Calibration Errors, reliability diagrams, etc) and methods (such as temperature scaling) for characterizing and mitigating miscalibration issues. The miscalibration of LLMs has also been studied in recent works (Xiong et al., 2023; Tian et al., 2023; Liusie et al., 2024; Zhao et al., 2023; Geng et al., 2023; Kamath et al., 2020). In this work, we show that LLMs with zero-shot and few-shot prompting exhibit a unique miscalibration issue on classification tasks, which we refer to as indiscriminate miscalibration. This phenomenon occurs when models assign equal confidence to correct and incorrect predictions. We show that this phenomenon cannot be quantitatively measured by Expected Calibration Errors (ECE). One alternative metric that can help catch this phenomenon is using Macro-average Calibration Error (MacroCE) proposed in Si et al. (2022). However, it may not depict the phenomenon thoroughly as it only computes the means of the distributions. We further propose a metric to help describe the phenomenon in more details. We hypothesize that this indiscriminate miscalibration occurs because the model treats each sample independently and has not been trained on the corresponding dataset. As a result, the predicted probabilities are not comparable across samples, which can further negatively impact prediction accuracy. To this end, we propose a label-free in-context comparative inference method that adds unlabeled samples to the prompt. This encourages the model to adjust and calibrate its predictions without requiring labels for the demonstration examples. The added examples serve as a proxy to help the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c67/1c674519-6597-474d-91af-a65c4d3e824b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\nFigure 1: Simulated reliability diagrams show different miscalibration behaviors but having the same ECE and accuracy. (a) an indiscriminate miscalibration behavior which is also observed in zero-shot and few-shot prompting in our experiments; (b) a regular miscalibration behavior closer to the original calibration paper (Guo et al., 2017).\nmodel better understand the test examples and the corresponding task. We delve deeper into the principles of our method and develop an aggregation step for improved calibration and a post-hoc calibration method that can further enhance performance. Through experiments on multiple datasets, we demonstrate significant gains in performance measured by F1 scores, accuracy, and traditional calibration metrics. We also show that our label-free in-context comparative inference method helps to alleviate indiscriminate miscalibration, enabling models to assign higher confidence to correct predictions and lower confidence to incorrect ones.\n# 2 RELATED WORK\n# 2.1 CONFIDENCE AND CALIBRATION\nConfidence calibration on modern neural networks has been discussed in (Guo et al., 2017) where the authors showed that large vision models are poorly calibrated. and proposed the Expected Calibration Errors (ECEs) that have been widely used in the literature. A more recent paper reviews some drawbacks of ECEs and proposed Instance-level Calibration Error (ICE) and Macro-average Calibration Error (MacroCE) (Si et al., 2022). Recent studies have shown great interests of model uncertainty and calibration in language models as well(Xiong et al., 2023; Tian et al., 2023; Liusie et al., 2024; Zhao et al., 2023; Geng et al., 2023; Kamath et al., 2020). As one of the most popular method for prompt engineering with LLM, in-context few-shot prompting (Brown et al., 2020) has the output instability issue, which was revealed by Zhao et al. (2021). They proposed a simple method to estimate and adjust majority label bias, recency bias, and common token biases introduced by in-context learning. Jiang et al. (2021) also showed that LLMs can be overconfident about their answers and calibration purposed fine-tuning or post-hoc calibration method can be used to improve the performance. Other similar methods such as batch calibration (Zhou et al., 2023) has been proposed as well. More recent studies show that few-shot prompting, finetuning or chain-ofthoughts can all suffer miscalibration issues (Zhang et al., 2023). In a low shot setting (e.g. < 4 demonstration examples), model prediction accuracy and calibration error can both increase and the trade-off can be improved with larger models or more shots (e.g. > 8 shots). Moreover, instruction fine-tuned LLMs exhibit the same miscalibration issue(Jiang et al., 2023) as well.\n# 2.2 IN-CONTEXT LEARNING\nIn few-shot prompting (Brown et al., 2020) setting, one provides input-label demonstration pairs in the prompt to improve LLMs\u2019 understanding and generation capability. Many methods have been proposed to improve ICL and understand its behaviors. Wang et al. (2023) shows that the label\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f254/f25409c4-0fd2-4237-a7c1-27ba6170c40a.png\" style=\"width: 50%;\"></div>\nFigure 2: Reliability diagrams averaged across 5 datasets. The confidence matches the accuracy (y-axis) for a perfectly calibrated model. Hence, the red gaps indicate the severity of miscalibrations for each confidence bin. The first and second rows show the indiscriminate miscalibration behavior of large language models under in-context learning (0-shot and 10-shot) setting where accuracy are similar regardless whether confidences are high or low. In certain cases, lower confidences can give even higher accuracy. Under comparative inference setting (e.g.third row), such issue is alleviated and significantly improved with aggregated comparative inference (e.g.last row).\nwords and contextual words can impact model generations. Min et al. (2022) shows that providing a few examples of the label space, the distribution of the input text, or the format of the sequence can be more important than ground truth labels. Ensemble methods such as self-consistency (Wang et al., 2022) over a diverse set of reasoning paths can improve performance as well. In a more recent study, comparison ability (Liusie et al., 2024) without labels was also demonstrated as one of the emergent capability of LLMs (Wei et al., 2022).\n# 3 BACKGROUND\n# 3.1 NOTATIONS\nDenote a classifier as f\u03b8(X) which is parameterized by \u03b8 and takes input X \u2208X. f\u03b8(X) is optimized on the training data Dtrain = {x1, y1, ..., xN, yN} with label Y \u2208Y = {1, 2, ..., K}. We take a calibration point of view and assess whether the label probability generated by a language model can accurately express its confidence level such that the probability estimate \u02c6P(Y |X) can reflect the actual probability of the prediction being correct (Guo et al., 2017; Jiang et al., 2021; Liusie et al., 2024). Here we use \u02c6P to represents the probability estimate, which is a vector of probabilities over K labels. The label estimate is derived by taking the argmax such that \u02c6Y = argmaxY \u02c6P(Y |X) (Guo et al., 2017; Liusie et al., 2024). For simplicity, we further define p\u2217= maxY \u02c6P(Y |X) = \u02c6P( \u02c6Y |X) to represent the probability estimate of the predicted label which is used to represent the confidence (Guo et al., 2017) of the label estimate.\n# CONFIDENCE ESTIMATE FOR LLMS\nFor a LLM, we extract the token probability for each class Y \u2208Y{1, 2, ..., K} as the probability estimates \u02c6P(Y |X). For example, in a binary classification setting, we have the label domain to be Y = {0, 1} where 1 indicates \u2018Yes\u2019, and 0 indicates\u2018No\u2019. The probability estimate \u02c6P(Y = 1|X) is extracted by computing the likelihood of generating label \u2018Yes\u2019. More specifically, if the generated sentence is \u2018The answer is: Yes.\u2019 and the word \u2018Yes\u2019 is tokenized as \u2018Yes\u2019. Then we use the probability of token \u2018Yes\u2019 after \u2018The answer is:\u2019. In the case where a label word has multiple ways of being tokenized, we sum over all possible tokens. 1 Now that we have extracted label probability estimate, we then compute the Expected Calibrated Error (ECE) (Guo et al., 2017), which has been widely used to to characterize the calibration level of a model\u2019s confidence (Geng et al., 2023). It is defined as the following,\nThe lower the ECE, the more calibrated the model is. Similarly to many other observations (Xiong et al., 2023; Tian et al., 2023; Liusie et al., 2024; Zhao et al., 2023; Geng et al., 2023; Kamath et al., 2020), we found that the LLMs\u2019 probability estimates can be miscalibrated (Figure 2, Table 3). However, while ECE can be generally used to measure whether a model is calibrated, it fails to distinguish different miscalibration behaviors. For example in Figure 1, two reliability diagrams have same ECE but have clearly different miscalibration behaviors which we will elaborate more in the next section.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/555d/555d55d1-6ded-413a-9227-0366ecabdc4f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\nFigure 3: Quantifying indiscriminate calibration using Indiscriminate Ratio (MacroCE) and Discriminate KL (DKL) divergence. The metrics aim to capture the difference between the probability distributions of correct and incorrect predictions. Numbers are averaged across 5 dataset. Bar or Shaded area describes the standard deviations across datasets.A smaller MacroCE (or a larger DKL) indicates a more discriminate calibrated model. Comparative inference helps alleviate indiscriminate miscalibration and the aggregation method can further improve it.\n<div style=\"text-align: center;\">Figure 3: Quantifying indiscriminate calibration using Indiscriminate Ratio (MacroCE) and Discriminate KL (DKL) divergence. The metrics aim to capture the difference between the probability distributions of correct and incorrect predictions. Numbers are averaged across 5 dataset. Bar or Shaded area describes the standard deviations across datasets.A smaller MacroCE (or a larger DKL) indicates a more discriminate calibrated model. Comparative inference helps alleviate indiscriminate miscalibration and the aggregation method can further improve it.</div>\nIn contrast to other models\u2019 miscalibration behavior revealed in Guo et al. (2017), LLM\u2019s miscalibration can be special such that it\u2019s overconfident and the phenomenon has been reported in several previous studies (Jiang et al., 2021; Tian et al., 2023). In our experiments, we found that regardless whether the label estimate is correct or not, LLM tends to give equal confidence (e.g.probability estimate) to its predictions when LM is not trained on the task (e.g. in zero-shot setting). The confidence is not necessarily high (e.g. overconfident). In certain scenarios, LM can also be underconfident (e.g. low confidence with high accuracy). More specifically, as shown in Figure 1 (a), the model gives same accuracy for all confidence levels, this is also discussed in Jiang et al. (2021). We report this phenomenon using several classification tasks as an example, shown in Figure 2. We refer to such behavior as indiscriminate miscalibration, a special case of miscalibration , that can\u2019t be captured by ECEs. In conventional miscalibration scenario such as the simulated scenario\n1Suppose the label word is \u2018Positive\u2019. And \u2018Positive\u2019 can be tokenized as i) \u2018Pos\u2019 + \u2018itive\u2019 and ii) \u2018Positiv We sum over both \u2018Positive\u2019 and \u2018Pos\u2019 tokens to better represent the probability estimate. As in this conte generating \u2018Pos\u2019 is almost 100% indicating the next token is \u2018itive\u2019.\n<div style=\"text-align: center;\">(b)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a9b/0a9b8be4-eae0-40ff-94ff-d7f89fa12372.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Inference performances vary significantly at different positions in the comparative inference setting, examples from the TREC task (Li & Roth, 2002; Hovy et al., 2001). In the zero-shot setting, the performance decays at the second and third positions.</div>\nin Figure 1 (b) or the ones reported in Guo et al. (2017), though in each confidence level bins, its actual accuracy has gaps towards the expected accuracy, the model generally has higher confidence in predictions with high accuracy and lower confidence in predictions with low accuracy. It still possess the ability to give different confidence levels toward possible correct predictions and incorrect predictions. Such ability is more strictly defined as Relative Confidence (Geng et al., 2023; Kamath et al., 2020; El-Yaniv et al., 2010; Wightman et al., 2023), the ability to rank samples, distinguishing correct predictions from incorrect ones. In Si et al. (2022), authors show that ECE is incapable for describe this behavior and they proposed Instance-level Calibration Error (ICE) as the following,\nHere, we use \u02c6P( \u02c6yi|xi) to replace the confidence in the original paper. While ICE tries to differentiate correct and wrong predictions, miscalibration issue can be disguised when the accuracy is high. To address this, Macro-average Calibration Error (MacroCE) is further proposed to balance the correct predictions and incorrect ones. MacroCE is defined as,\nwhere ICEpos is the ICE score computed over correct predictions and ICEneg is the one with incorrect predictions. On the other hand, MacroCE computes the mean of the difference between confidence and accuracy by averaging over samples. It may not capture certain different miscalibration distribution patterns. Imagine two cases (case A and and B), both cases have same means for correct prediction confidences and incorrect prediction confidences. The MacroCE would be similar for case A and B. However, when the variances are different, these cases should show different indiscriminate levels. We illustrate one visual example in the Appendix. To measure the difference between correct predictions and incorrect predictions on a more granular level such as using information of variances, we also define the Discriminate KL divergence (DKL) as the following,\n|||| \u0338 Unlike MacroCE, DKL measures the distribution differences. Larger DKL indicates more discriminate confidences levels between correct predictions and incorrect predictions.We report the results in Figure 3b. Similarly, we observed that our proposed method helps improving DKL, i.e. more discriminate confidence level.\n# 4 METHOD\nAs shown above, when we use LLMs with zero-shot and few-shot prompting, it has trouble in differentiating confidences for correct and incorrect predictions. Our hypothesis is that each sample\n(1)\nis treated independently and LLMs are not able to compare or rank them without trained on them even with few-shot prompting. Thus we propose a label-free method by asking LM to compare a sample of interest X = xi with other unlabeled samples jointly to adjust for more calibrated results.\n# 4.1 COMPARATIVE INFERENCE\nConsidering three samples xi, xj, xk, we are interested in performing a comparison to derive the labels probability estimate \u02c6P(yi, yj, yk|xi, xj, xk; C), where C represents a prompting instruction asking for a comparison. The following template is a prompt example,\n(Task Definition) A news description topic can be one of the\nfollowing four types. ###Types: World, Sports, Business, Sci/Tech.\n(Inputs Samples) For the following 3 news descriptions: ###News\nDescription 1: XXXX. ###News Description 2: XXXX.\n###News\nDescription 3: XXXX.\n(Comparison prompt) By comparing them, we know the most suitable\ntypes for each of these 3 news descriptions, respectively, are:\nNotice that even we are using words like \u2018comparing\u2019, the goal is not to ask LLMs to say if sample 1 is \u2018more positive\u2019 than sample 2 and sample 3 as that may change the original task meaning. It\u2019s more important to present multiple unlabeled samples in the context and ask LLMs to predict labels for each of them jointly.\n4.1.1 ASYMMETRIC PROBABILITIES\n# 4.1.1 ASYMMETRIC PROBABILITIES\nThe first thing to notice is because of the auto-regressive nature of large language models, we have \u02c6P(yi, yj|xi, xj; C) \u0338= \u02c6P(yj, yi|xj, xi; C). More specifically, if xi appears before xj in the prompt, where one would expect \u02c6yi to be generated before \u02c6yj, which means the \u02c6yj is generated conditioned on \u02c6yi. And it can be shown with the following,\nThis could lead to bias to the samples generated in later orders. Indeed, ordering of words in the prompts can impact performance significantly (Lu et al., 2021; Min et al., 2022). In our experiments, we observed a degradation of inference performance on later input samples (Figure 4). Besides, the post-processing and probabilities extractions of inputs after the first prediction can be tricky. Hence, we focus on the prediction of the first input throughout the paper, i.e. \u02c6P(yi|xi, xj, ...; C) and always put the sample of interest as the first sample to predict. The rest input samples only serve as a reference comparison inputs.\n# 4.1.2 COMPARISON BIAS\nIn an ideal setting, we can derive the probability estimates leveraging all the possible input samples \u02c6P(yi|xi, Dx; C). However, this is impossible as that will make the prompt too long to be processed by LLMs. Thus, we make the following assumptions,\nwhere {x}j is a sequence of samples from Dx; \u03f5j is the bias introduced when comparing with {x}j in the prompt instead of Dx (e.g. similar to the contextual bias (Zhao et al., 2021). We further hypothesize the biases introduced by different comparison samples can be averaged out (e.g expectation is zero) so we can approximate it in practice by the following,\n(2)\n(3)\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3fc4/3fc40a47-a874-4f30-8163-548cbe8bbb8f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Performance (ECE and F1 scores) improve as we aggregate more comparative inference results. Results are averaged across 5 dataset. Notably, the ECE decrease (the lower the better) drastically with aggregations under our assumption eq.4.</div>\nwhere J is a finite number of sets of comparisons. This is simply aggregating multiple comparative inference probabilities with different comparison samples. We expect with aggregations, calibration error will go down drastically. As one could see in Figure 5, we can see ECE drastically decreased with aggregation. F1 score and accuracy is also improved but relatively less aggressive. Though there still exist other biases (e.g. contextual, label, etc) that can be further calibrated. However, this indeed will increase inference cost by the number of aggregation times.\n4.2\n4.2 POST-HOC CALIBRATION\nThere exist many ways in the literature for calibrating probabilities such as Histogram binning (Zadrozny & Elkan, 2001), Isotonic regression(Zadrozny & Elkan, 2002), Platt scaling (Platt et al., 1999), etc. One simplest approach is temperature scaling (Guo et al., 2017) where we apply weighted softmax on logits or probabilities to recalibrate probabilities. While such methods can decrease ECE, it does not help with improving model\u2019s classification performance. Extensions of Platt scaling towards multi-class classification is referred as vector scaling and matrix scaling, where one would optimize the following linear layers on top of the probability estimates,\n# \u02dcP(yi|xi) = softmax(W \u02c6P(yi|xi) + b),\nwhere \u02dcP(yi|xi) is the calibrated probabilities. In the vector scaling case, W is restricted to a diagonal matrix. And W and b are optimized using a validation dataset. Inspired by these methods, we propose to further calibrate our probability estimates by applying such affine transformations but on top of different comparison referencs. More specifically, we retrieve the recalibrated probabilities by,\nwhere Wj, bj can be estimated via argmax{\u2212yklogP \u2217(yk|xk, {x}j; C))} with a set of {xk, yk}. This is similar to the principle of contextual calibration used in Guo et al. (2017); Zhao et al. (2021). The difference is that previous studies focused calibrating biases introduced from labels, or other prompt contextual information whereas we are focusing on the comparison samples.\n(5)\n# 5 RESULTS\n5.1 EXPERIMENTS SETUP\n# 5.1 EXPERIMENTS SETUP\nModels and Datasets We conducted experiments using the Llama(Touvron et al., 2023) family models: Llama2-7b-chat, Llama2-13b-chat, Llama2-70b-chat , Llama3-8b-instruct, Llama3-70binstruct. Our experiments focused on classification problems. We picked 5 classic tasks covering different domains: AGNews (topic classification) (Zhang et al., 2015), trec (topic classifications) (Voorhees & Tice, 2000; Li & Roth, 2002; Hovy et al., 2001), Financial datasets (sentiment analysis) (Malo et al., 2014), emotion dataset (emotion detection) (Saravia et al., 2018) and hateful speech detection (Toxicity detection) (de Gibert et al., 2018). Due to limited capacity, we capped the number of testing samples at 500. For each method, we run 10 replicates with different seeds.\nICL\nLF-ICL\nModel\n0 shot\n3 shot\n10 shot\n0 shot\n0 shot-agg\n3 shot\n10 shot\nF1\u2191\nL2-7B\n0.37\n0.54\n0.46\n0.56\n0.59\n0.375\n0.45\nL2-13B\n0.48\n0.61\n0.58\n0.58\n0.65\n0.53\n0.61\nL2-70B\n0.60\n0.72\n0.74\n0.66\n0.72\n0.71\n0.75\nL3-8B\n0.52\n0.63\n0.63\n0.70\n0.74\n0.71\n0.77\nL3-70B\n0.71\n0.78\n0.80\n0.76\n0.77\n0.80\n0.81\nECE\u2193\nL2-7B\n0.35\n0.36\n0.46\n0.29\n0.22\n0.45\n0.39\nL2-13B\n0.35\n0.32\n0.36\n0.29\n0.13\n0.30\n0.24\nL2-70B\n0.16\n0.21\n0.20\n0.17\n0.13\n0.21\n0.19\nL3-8B\n0.29\n0.29\n0.30\n0.16\n0.09\n0.21\n0.16\nL3-70B\n0.17\n0.15\n0.19\n0.14\n0.09\n0.17\n0.16\nTable 1: F1 scores and ECE averaged across 5 dataset with 10 runs for all models. LF indicates labelfree comparative inference; \u2018agg\u2019 indicates aggregate 10 different comparative inference results. Note that for all LF-ICL experiments, we don\u2019t provide labels. Higher F1 score and lower ECE are achived by LF-ICL.\nMethodology Our experimental results are mainly composed with two parts: i) methods for manipulating prompts and ii) further post-hoc calibrations where we train an additional linear layer with less than hundreds of parameters. For i), the baseline is the independent inference where each prompt contains only one input sample without demonstrations (zero-shot). We provide our sample prompts in the Appendix. We include few-shot prompting (Brown et al., 2020) methods with either 3-shot or 10-shot. For our Label-Free (denoted as LF in figures and tables) comparative inference method, we use 2 additional input samples randomly picked from the training dataset as comparison references. As discussed in section 4.1, we always placed the sample of interest at the first place among all 3 samples and focused on the results of the first answer generated by LLMs. Our method also uses aggregations where we used different comparison references in the prompt and retrieved averaged results from up to 10 comparative inference results. LF method can be used with few-shot together, which is also included. For ii) post-hoc calibration methods, we mainly considered the vector scaling and matrix scaling methods that have been widely used in Guo et al. (2017); Zhao et al. (2021); Zhou et al. (2023); Zhang et al. (2023).\nLABEL-FREE COMPARATIVE INFERENCE IMPROVES CLASSIFICATIO\nFor classification performance metrics, we consider accuracy and F1 scores. We empirically found these two are consistent. As most of the datasets are imbalanced, we reported F1 scores averaged across 5 datasets in the main text and the corresponding accuracy results in the Appendix. From F1 scores in Table 3 and simiarly accuracy from the Appendix, comparative inference without labels can outperform few-shot prompting (e.g. Llam2-7b and Llama2-70b), while both are significantly better than the zero-shot. We found that both Llama3-8b and Llama3-70b behave even better in comparative inference setting with the TREC and Emotion tasks. Interestingly, these two tasks have 6 different labels which are the most across 5 dataset. However, in the ICL few-shot\nMatrix Scaling\nVector Scaling\nModel\n0\n3\n10\n0-LF\n10-LF\n0\n3\n10\n0-LF\n10-LF\nF1\u2191\nL2-7B\n0.57\n0.60\n0.50\n0.69\n0.69\n0.54\n0.53\n0.47\n0.67\n0.70\nL2-13B\n0.54\n0.67\n0.57\n0.71\n0.75\n0.52\n0.65\n0.57\n0.73\n0.76\nL2-70B\n0.54\n0.74\n0.70\n0.77\n0.79\n0.51\n0.68\n0.71\n0.77\n0.80\nL3-8B\n0.63\n0.74\n0.74\n0.71\n0.79\n0.64\n0.74\n0.74\n0.71\n0.78\nL3-70B\n0.74\n0.79\n0.80\n0.79\n0.83\n0.70\n0.78\n0.80\n0.79\n0.82\nECE\u2193\nL2-7B\n0.17\n0.21\n0.29\n0.23\n0.22\n0.17\n0.20\n0.29\n0.23\n0.19\nL2-13B\n0.24\n0.20\n0.29\n0.25\n0.20\n0.23\n0.23\n0.28\n0.20\n0.18\nL2-70B\n0.28\n0.20\n0.20\n0.19\n0.18\n0.28\n0.17\n0.21\n0.18\n0.17\nL3-8B\n0.20\n0.17\n0.18\n0.24\n0.19\n0.16\n0.18\n0.18\n0.21\n0.18\nL3-70B\n0.17\n0.16\n0.16\n0.18\n0.16\n0.17\n0.15\n0.15\n0.17\n0.16\nTable 2: F1 and ECE scores averaged across 5 datasets with 10 replicates for all models with posthoc calibration. We experiment with different shot number: 0, 3, 10. LF represents label-free incontext learning. All inference results indicates are based on post-hoc calibration with 10 inference results.\n# setting, both models did not perform well in terms of the calibrations.\nsetting, both models did not perform well in terms of the calibration\n.3 ALLEVIATE INDISCRIMINATE MISCALIBRATION WITH COMPARATIVE INFERENCE\nBy applying comparative inference, we observe a significant alleviation of the indiscriminate miscalibration issue. More specifically:i) Expected Calibration Error (ECE) results are generally improved, with the aggregation method (section 4.1.2) consistently performing the best (Table 3). ii). The MacroCE score decreased (Figure 3a) and KL divergence increased with comparative inference across models and datasets (Figure 3b). iii). These behaviors are represented in the reliability diagrams (Figure 2) as well. Additionally, we found that few-shot prompting can potentially worsen ECEs, depending on the specific models used(Table 3, Figure 2). As we discussed in section 4.1.2, we expect aggregation to improve calibration issues more compared to classification performances given our assumption. And we do observe such behaviors as shown by Figure 2, Figure 3 and also by ECEs in Table 3. Aggregation can also improve F1 and accuracy as this can be seen as a way of ensemble, similarly to other methods such as Wang et al. (2022). In our experiment, the performance is even better than comparative + ICL for Llama2-7b and Llama2-13b. We can also expect combining ICL, aggregation and our methods can further enhance performances (Table 3). However, this will require more tailored labels, and inference costs.\n# ONS AND BEST PRACTICES FOR EFFECTIVE COMPARATIVE INFERENC\nFirst observation is that we can combine LF method with few-shot to further improve performances. We found performances enhanced across the board except for the Llama2-7b based on averged F1 score (Table 3). We hypothesize this is because Llama2-7b is not capable to leverage both comparative inference and ICL at the same time. Secondly, we found that LF comparative inference with ICL are mostly effective with 10-shots setting whereas the independent method perform well with 3-shot for smaller models but better with 10-shot for larger or more capable models (e.g. Llama3-8b or 70b models). For examples, 10-shot comparative inference setting is significantly better than other baselines for Llama2-7b and Llama2-13b models on the TREC dataset.\nAs described in section 4.2, vector scaling has K parameters while our post-hoc method will have J \u00d7 K parameters. In our experiments, K is generally < 10 and we restrict J = 10, which will restricted the total number of trainable parameters to less than hundreds for vector scaling and thousands for matrix scaling. In addition, the optimizations of W and b are conducted using 200\nvalidation samples for all datasets. Similarly, we reported F1 scores and ECE scores averaged across 5 datasets for each model in Table 4. We show the breakdowns in the Appendix.\nvalidation samples for all datasets. Similarly, we reported F1 scores and ECE scores averaged across 5 datasets for each model in Table 4. We show the breakdowns in the Appendix. We found that comparative inference with post-hoc calibration is consistently better than baseline post-hoc calibration methods or the baselines with few-shot prompting for Llama2 family in terms of F1 scores. For Llama3, baseline with few-shot prompting is better, which is likely due to the fact that Llama3 models have strong capability on few-shot prompting. We further show that comparative inference with post-hoc calibration can be combined with few-shot prompting and this will further boost the performance and is consistently better than all other methods in terms of F1 scores. ECEs on average are improved compared to baselines but for many settings are not better than certain methods in the previous setction. As indeed, post-hoc calibration has been recently used to improve classification performances (Zhao et al., 2021; Zhou et al., 2023) instead of focusing on conventional calibration issues.\n# 6 DISCUSSION\nIn this paper, we study a special miscalibration behavior of large language models when being used for zero-shot and few-shot prompting, which we refer to as indiscriminate miscalibration. We propose metrics to quantify the severity of this issue and develop a label-free in-context comparative inference method to mitigate it. We show our label-free method can achieve better classification performance as well as more calibrated predictions on multiple datasets.\n# 7 LIMITATIONS\nThe primary limitation is that this study only considered classification tasks. Secondly, we considered 5 models but are all from Llama families. While these 5 models are widely recognized for their performance in natural language processing tasks, the inclusion may introduce biases inherent to this particular model family. Future research can benefit from exploring a broader range of tasks and considering models from other families to provide a more comprehensive understanding of their capabilities and limitations. Lastly, we didn\u2019t study finetuned model, which can be one of our future directions.\n# REFERENCES\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Ona de Gibert, Naiara Perez, Aitor Garc\u00b4\u0131a-Pablos, and Montse Cuadros. Hate Speech Dataset from a White Supremacy Forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pp. 11\u201320, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5102. URL https://www.aclweb.org/anthology/ W18-5102. Ran El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. A survey of language model confidence estimation and calibration. arXiv preprint arXiv:2311.08298, 2023. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 1321\u20131330. PMLR, 2017. Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. URL https://www.aclweb.org/ anthology/H01-1069. Mingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, and Jimmy Ba. Calibrating language models via augmented prompt ensembles. 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Ona de Gibert, Naiara Perez, Aitor Garc\u00b4\u0131a-Pablos, and Montse Cuadros. Hate Speech Dataset from a White Supremacy Forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pp. 11\u201320, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5102. URL https://www.aclweb.org/anthology/ W18-5102. Ran El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. A survey of language model confidence estimation and calibration. arXiv preprint arXiv:2311.08298, 2023. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pp. 1321\u20131330. PMLR, 2017. Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. URL https://www.aclweb.org/ anthology/H01-1069. Mingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, and Jimmy Ba. Calibrating language models via augmented prompt ensembles. 2023.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962\u2013977, 2021. Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. arXiv preprint arXiv:2006.09462, 2020. Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. URL https://www.aclweb.org/ anthology/C02-1150. Adian Liusie, Potsawee Manakul, and Mark Gales. Llm comparative assessment: Zero-shot nlg evaluation through pairwise comparisons using large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 139\u2013151, 2024. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65, 2014. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3687\u20133697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1404. URL https://www.aclweb.org/anthology/D18-1404. Chenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-Graber. Re-examining calibration: The case of question answering. arXiv preprint arXiv:2205.12507, 2022. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 200\u2013207, 2000. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. arXiv preprint arXiv:2305.14160, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.\nGwenyth Portillo Wightman, Alexandra DeLucia, and Mark Dredze. Strength in numbers: Estimating confidence of large language models by prompt agreement. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pp. 326\u2013362, 2023. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023. Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Icml, volume 1, pp. 609\u2013616, 2001. Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 694\u2013699, 2002. Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, and Sham Kakade. A study on the calibration of in-context learning. arXiv preprint arXiv:2312.04021, 2023. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. Theodore Zhao, Mu Wei, J Samuel Preston, and Hoifung Poon. Automatic calibration and error correction for large language models via pareto optimal self-supervision. arXiv preprint arXiv:2306.16564, 2023. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pp. 12697\u201312706. PMLR, 2021. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. arXiv preprint arXiv:2309.17249, 2023.\n# 8 APPENDIX\n# 8.1 PROMPT EXAMPLES\nZero-shot Prompt\n# Zero-shot Prompt\nA question can be one of the following six types. ###Types: Abbreviation, Entity, Description and abstract concept, Human being, Location, Numeric value. For the given question, ###Question: For the given question, ###Question: What is an atom ? The most suitable type for the given question is:\nA question can be one of the following six types. ###Types: Abbreviation, Entity, Description and abstract concept, Human being, Location, Numeric value. For instance, for the following example questions###Example Question: 1: What is the name of the managing director of Apricot Computer ?. ###Example Question: 2: When did Muhammad live ?. ###Example Question: 3: How many people lived in Nebraska in the mid 1900s ?. The most suitable types should be:1). Human being. 2). Numeric value. 3). Numeric value. For the given question, ###Question: Why does the moon turn orange ? The most suitable type for the given question is:\nA question can be one of the following six types. ###Types: Abbreviation, Entity, Description and abstract concept, Human being, Location, Numeric value.For the following 3 questions: ###Question 1:How far is it from Denver to Aspen ? ###Question 2:Where is the Kentucky Horse Park ? ###Question 3:What is the first personal computer company ? By comparing them, we know the most suitable types for each of these 3 questions, respectively, are:\nThree-shot Comparative Prompt\nA question can be one of the following six types. ###Types: Abbreviation, Entity, Description and abstract concept, Human being, Location, Numeric value. For instance, for the following example questions###Example Question 1: What is the name of the managing director of Apricot Computer ?. ###Example Question 2: When did Muhammad live ?. ###Example Question 3: How many people lived in Nebraska in the mid 1900s ?. The most suitable types should be:1). Human being. 2). Numeric value. 3). Numeric value. For the following 3 questions: ###Question 1:What county is Modesto , California in ? ###Question 2:Where is the Kentucky Horse Park ? ###Question 3:What is the first personal computer company ? By comparing them, we know the most suitable types for each of these 3 questions, respectively, are:\n# 8.2 ACCURACY RESULTS\n<div style=\"text-align: center;\">8.2 ACCURACY RESULTS</div>\nICL\nLF-ICL\nModel\n0 shot\n3 shot\n10 shot\n0 shot\n0 shot-agg\n3 shot\n10 shot\nAccuracy\u2191\nL2-7B\n0.41\n0.57\n0.49\n0.57\n0.59\n0.46\n0.51\nL2-13B\n0.49\n0.61\n0.59\n0.59\n0.64\n0.57\n0.64\nL2-70B\n0.52\n0.63\n0.63\n0.71\n0.75\n0.71\n0.77\nL3-8B\n0.61\n0.72\n0.74\n0.67\n0.73\n0.71\n0.75\nL3-70B\n0.72\n0.77\n0.8\n0.77\n0.78\n0.8\n0.81\nTable 3: Accuracy averaged across 5 dataset with 10 replicates for all models. 0-shot indicates baseline independent inference and the corresponding first row includes In-Context Learning with independent inference results; \u2018LF\u2019 indicates label-free comparative inference 0-shot setting and the corresponding second row includes the In-Context Learning with comparative inference results; \u2018agg\u2019 indicates aggregate 10 different comparative inference results.\nMatrix Scaling\nVector Scaling\nModel\n0\nshot\n3\nshot\n10\nshot\n0\nshot-\nLF\n10\nshot-\nLF\n0\nshot\n3\nshot\n10\nshot\n0\nshot-\nLF\n10\nshot-\nLF\nAccuracy\u2191\nL2-7B\n0.6\n0.64\n0.55\n0.7\n0.71\n0.57\n0.6\n0.53\n0.69\n0.71\nL2-13B\n0.56\n0.69\n0.63\n0.72\n0.76\n0.54\n0.68\n0.62\n0.74\n0.77\nL2-70B\n0.55\n0.76\n0.72\n0.78\n0.8\n0.53\n0.71\n0.71\n0.78\n0.8\nL3-8B\n0.65\n0.75\n0.75\n0.72\n0.79\n0.66\n0.75\n0.74\n0.73\n0.79\nL3-70B\n0.76\n0.8\n0.81\n0.8\n0.83\n0.74\n0.79\n0.81\n0.8\n0.83\nTable 4: Accuracy averaged across 5 datasets with 10 replicates for all models with post-hoc calibration. n-shot: baseline in-context learning; LFrepresents label-free in-context learning. All inference results indicates are based on post-hoc calibration with 10 inference results.\n8.3 SPECIAL SCENARIO WITH INDISCRIMINATIVE MISCALIBRATIO\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b0b/1b0b9535-b23b-4ed1-89bf-df59f71d214d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">We simulate a scenario where correct prediction probabilities are sampled from a distribution with mean of 0.65 and incorrect prediction probabilities are sampled from a distribution with mean o 0.45. In the left figure, distributions have smaller variances while in the right figure, variances ar larger. In this scenario, MacroCE will give the same value for both cases while \u2018qualitatively\u2019, lef side is more \u2018discriminative\u2019 and has larged DKL value.</div>\nINDIVIDUAL DATASET RESULTS\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/feec/feec94e5-0c7f-45c2-8d40-f992f8fec0d0.png\" style=\"width: 50%;\"></div>\nFigure 6: Trec Dataset Results. (a). Reliability Diagram. (b). DKL Divergence and MACROCE. (c). Comparative results with F1 scores and ECEs. (d). Post-hoc calibrated results. *Notice that for the aggregation method of the individual data result, we only have one round experiment so we only reported means without variances.\n<div style=\"text-align: center;\">Figure 6: Trec Dataset Results. (a). Reliability Diagram. (b). DKL Divergence and MACROCE. (c). Comparative results with F1 scores and ECEs. (d). Post-hoc calibrated results. *Notice that for the aggregation method of the individual data result, we only have one round experiment so we only reported means without variances.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/397c/397c2c99-358d-41ca-8458-8668eda232c6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: AGnews Dataset Results. (a). Reliability Diagram. (b). DKL Divergence and MACROCE. (c). Comparative results with F1 scores and ECEs. (d). Post-hoc calibrated results. *Notice that for the aggregation method of the individual data result, we only have one round experiment so we only reported means without variances.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ed89/ed895db1-bad4-4991-baf5-cfb05bba12ee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Financial Dataset Results. (a). Reliability Diagram. (b). DKL Divergence and MACROCE. (c). Comparative results with F1 scores and ECEs. (d). Post-hoc calibrated results. *Notice that for the aggregation method of the individual data result, we only have one round experiment so we only reported means without variances.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a04d/a04d55ef-3ed5-4277-8b98-9d2cdc690152.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Emotion Dataset Results. (a). Reliability Diagram. (b). DKL Divergence and MACROCE. (c). Comparative results with F1 scores and ECEs. (d). Post-hoc calibrated results. *Notice that for the aggregation method of the individual data result, we only have one round experiment so we only reported means without variances.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfa1/dfa1de65-deee-4467-a895-bb514a874a88.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Hatespeech Dataset Results. (a). Reliability Diagram. (b). DKL Divergence and MACROCE. (c). Comparative results with F1 scores and ECEs. (d). Post-hoc calibrated results. *Notice that for the aggregation method of the individual data result, we only have one round experiment so we only reported means without variances.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of indiscriminate miscalibration behavior observed in large language models (LLMs) during in-context learning, where both correct and incorrect predictions are assigned the same level of confidence. Traditional calibration metrics, such as Expected Calibration Errors (ECEs), fail to capture this behavior, highlighting the need for new methods to ensure reliable model outputs.",
        "problem": {
            "definition": "The problem is the miscalibration of predictions in LLMs, particularly in zero-shot and few-shot prompting scenarios, where models assign equal confidence to both correct and incorrect predictions, leading to inaccurate classification outcomes.",
            "key obstacle": "The core obstacle is that existing calibration metrics do not effectively measure the severity of indiscriminate miscalibration, preventing accurate assessment and improvement of model predictions."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that the model's inability to compare and rank predictions arises from treating samples independently, without training on the corresponding dataset.",
            "opinion": "The proposed label-free in-context comparative inference method aims to alleviate miscalibrations by incorporating unlabeled samples into the prompt, encouraging the model to adjust its predictions based on comparative context.",
            "innovation": "This method differs from existing approaches by not requiring labeled examples for demonstration and instead using unlabeled samples for comparative inference, significantly improving prediction accuracy and calibration."
        },
        "method": {
            "method name": "Label-Free In-Context Comparative Inference",
            "method abbreviation": "LF-ICL",
            "method definition": "LF-ICL is a method that enhances the calibration of predictions made by LLMs by prompting them to compare an unlabeled sample of interest with other unlabeled samples.",
            "method description": "The core of the method involves using comparative prompts to derive label probability estimates for multiple samples simultaneously, thereby improving calibration.",
            "method steps": [
                "Select a sample of interest and two additional unlabeled samples.",
                "Construct a prompt that asks the model to compare the samples.",
                "Obtain probability estimates for each sample based on the comparative context.",
                "Aggregate results from multiple comparative inferences to enhance calibration."
            ],
            "principle": "The effectiveness of this method lies in its ability to leverage comparative context, allowing the model to better understand the relationships between samples and adjust its confidence levels accordingly."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the Llama family models on five classification tasks: AGNews, TREC, Financial datasets, Emotion dataset, and Hateful speech detection, with a maximum of 500 testing samples per method.",
            "evaluation method": "The performance of the method was assessed using F1 scores and Expected Calibration Errors (ECE), with results averaged across 10 runs for each model."
        },
        "conclusion": "The results demonstrate that the label-free comparative inference method significantly improves both classification performance and calibration of predictions in large language models across multiple datasets.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved accuracy and calibrated predictions compared to traditional prompting methods, addressing the unique miscalibration behavior of LLMs.",
            "limitation": "The primary limitation of this study is its focus solely on classification tasks using Llama family models, which may introduce biases and limit the generalizability of the findings.",
            "future work": "Future research should explore the application of this method to a wider range of tasks and model families, as well as investigate the potential benefits of fine-tuning models."
        },
        "other info": {
            "additional notes": "This study emphasizes the importance of calibration in LLMs and suggests that combining label-free comparative inference with few-shot prompting could further enhance performance."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of indiscriminate miscalibration behavior observed in large language models (LLMs) during in-context learning, where both correct and incorrect predictions are assigned the same level of confidence."
        },
        {
            "section number": "1.2",
            "key information": "The problem is the miscalibration of predictions in LLMs, particularly in zero-shot and few-shot prompting scenarios, leading to inaccurate classification outcomes."
        },
        {
            "section number": "3.1",
            "key information": "The proposed label-free in-context comparative inference method (LF-ICL) aims to enhance the calibration of predictions made by LLMs by prompting them to compare an unlabeled sample of interest with other unlabeled samples."
        },
        {
            "section number": "3.2",
            "key information": "The effectiveness of the LF-ICL method lies in its ability to leverage comparative context, allowing the model to better understand the relationships between samples and adjust its confidence levels accordingly."
        },
        {
            "section number": "4.1",
            "key information": "The key advantages of the proposed approach include improved accuracy and calibrated predictions compared to traditional prompting methods."
        },
        {
            "section number": "6.1",
            "key information": "The primary limitation of this study is its focus solely on classification tasks using Llama family models, which may introduce biases and limit the generalizability of the findings."
        },
        {
            "section number": "6.4",
            "key information": "Future research should explore the application of the label-free comparative inference method to a wider range of tasks and model families."
        }
    ],
    "similarity_score": 0.7029321485678451,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Calibrate to Discriminate_ Improve In-Context Learning with Label-Free Comparative Inference.json"
}