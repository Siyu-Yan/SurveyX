{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.04211",
    "title": "MMREC: LLM Based Multi-Modal Recommender System",
    "abstract": "The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. This surge in content presents unique challenges for designing effective recommender systems. Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences. This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model. Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information. This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations.",
    "bib_name": "tian2024mmrecllmbasedmultimodal",
    "md_text": "# MMREC: LLM Based Multi-Modal Recommender System\nJiahao Tian\u2217 jtian83@gatech.edu Georgia Institute of Technology Atlanta, Georgia, USA\nZhenkai Wang\u2217 kay.zhenkai.wang@utexas.edu The University of Texas at Austin Austin, Texas, USA\nThe importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. This surge in content presents unique challenges for designing effective recommender systems. Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences. This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model. Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information. This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations.\nKeywords: Multi-Modality, Large Language Models, Recommender System, Deep Learning Recommendation Model, Personalization, Imbalanced Dataset Modeling\n# 1 Introduction\nRecommender Systems have become an integral component of modern digital ecosystems, playing a pivotal role in personalizing user experiences across various domains such as e-commerce, streaming services, social media, and more [16, 34, 35, 57, 64]. These systems aim to predict the preferences of users and suggest items that align with their tastes,\nJinman Zhao\u2217 jinman.zhao@mail.utoronto.ca Univeristy of Toronto Toronto, Ontario, Canada\nZhicheng Ding\u2217 zhicheng.ding@columbia.edu Columbia University New York, NY, USA\nthereby enhancing user engagement and satisfaction. The foundations of the Recommender System can be traced back to collaborative filtering techniques, which leverage useritem interaction data to identify patterns and make recommendations. Over time, these systems have evolved to incorporate more sophisticated approaches, including contentbased filtering, hybrid methods, and context-aware recommendations, to address user data\u2019s growing complexity and scale. The development of machine learning and deep learning has revolutionized nearly every field [1, 19, 24, 26, 42, 43, 59], including recommender systems. These systems now benefit from large-scale models that can leverage vast amounts of data to extract complex relationships. Deep learning techniques, such as neural collaborative filtering (NCF), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), have been employed to enhance the accuracy and robustness of Recommender System [6, 9, 21, 56, 65]. These models benefit from their ability to automatically learn feature representations from raw data, eliminating the need for manual feature engineering and improving predictive performance. Moreover, the use of attention mechanisms and transformer architectures has further advanced the capabilities of deep learning-based recommenders by allowing them to capture sequential and contextual information better [35, 64]. Recently, Large Language Models (LLMs) like GPT-4 have shown immense potential in understanding and generating human-like text. In recommender systems, vast amounts of natural language data, such as user reviews and product information, are rich in valuable insights [31, 63]. Before the era of LLMs, Pretrained Language Model such as BERT [11] was heavily used for processing text information . However, LLMs have demonstrated the potential to outperform BERT in terms of reasoning and understanding human language. By leveraging the processed information provided by LLMs, recommender systems can achieve a deeper understanding of user intents and preferences, leading to more personalized and contextually relevant recommendations. In this paper,\nwe propose a novel LLM-enhanced deep learning framework with the following contributions:\n\u2022 We developed a framework that efficiently extracts multi-modal information, such as text and images, from LLMs \u2022 Information from different modalities is unified in a latent space, simplifying the learning process for the ranking model. \u2022 We demonstrated how the use of multi-modal information can further enhance the discriminative power of the model, especially for improving false positive rate in the case of the imbalanced dataset.\nThe structure of the paper is as follows: Section 2 introduces the latest developments in recommender systems and LLMs. Section 3 briefly discusses the data used. Section 4 presents our proposed framework and its key components. Section 5 details the experimental setup and analysis. Finally, Section 6 provides concluding remarks.\n# 2 Related Work\n# 2 Related Work 2.1 Recommender System\n# 2.1 Recommender System\nEarlier work on Recommender Systems(RS) did not involve the extensive use of deep learning as seen in current approaches. For specifics, one can refer to [18], which includes over 100 techniques from before 2017. RS can be broadly categorized into personalized [53, 61, 62] and group-based [27, 36, 38, 58, 60] systems. Collaborative Filtering (CF) stands out as a prevalent technique. CF predicts a user\u2019s preferences or opinions by leveraging the collective insights from a large user base. Notable implementations include memory-based CF approaches such as those presented in [5] and [3], which utilize vector representations. Another prominent category is Model-based CF, which forecasts user preferences by analyzing the relationships between users and items. In recent years, the integration of graph neural networks like CNN [2], GCN [23], GraphSAGE [17], and others have significantly enhanced model-based CF methods. These models have been extensively applied across various domains, with notable success in music, Point of Interest (POI), and book recommendations. For instance, the JODIE [28] model has been influential in music recommendation, while Multi-GCCF [39], and LightGCN [20] have shown promising results in POI and book recommendation scenarios. Among these, LightGCN has emerged as a classic model in the RS field. The effectiveness of review text in RS has been a subject of debate. For example, [7] argued that not all parts of reviews hold equal importance, leading them to propose an Aspect-based Neural Recommender (ANR) that focuses on more granular feature representations of items. Similarly, [29] employed capsule neural networks to extract specific viewpoints and aspects from user and item reviews. Furthermore, [54] developed a dual-encoder system using\nCNNs, one for encoding news and the other for learning user profiles based on their interaction with clicked news.\n# 2.2 Large Language Models Reasoning\nLLMs have demonstrated remarkable reasoning capabilities, particularly in benchmarks such as arithmetic [8, 32] and commonsense [41]. These models have showcased an ability to understand and perform complex calculations, as well as to apply general knowledge about the world in a way that mimics human-like understanding. Many works show the power of prompting during reasoning with LLMs such as few-shot learning [4], emotional prompt [30] and Chain-ofThought [25, 52]. Recently, there has been a trend towards using LLMs for traditional tasks. For instance, [44] employs in-context learning on GPT-3 for Relation Extraction(RE), achieving stateof-the-art (SOTA) performance on multiple test sets. [47] adapts LLMs to the Named Entity Recognition (NER) task, aiming to bridge the gap between sequence labeling and text generation. This adaptation demonstrates how LLMs can be fine-tuned or prompted in innovative ways to handle tasks traditionally outside their direct training objectives. [55] investigate the capabilities of LLMs in zero-shot information extraction scenarios, specifically examining the performance of ChatGPT in the NER task. By focusing on zero-shot learning, the study investigates ChatGPT\u2019s ability to identify and classify named entities within text and without any task-specific training data or fine-tuning. [51] conducted the ability of LLMs to generate new financial signals. LLMs have also been employed for other tasks such as text summarization [14] and sentiment analysis [40].\n# 2.3 LLM for Recommender Systems\nRecent efforts in the domain of recommender systems have increasingly focused on the utilization of Language Models [13, 22, 50]. [12] utilizes LLMs as the interface for recommender systems, facilitating multi-round recommendations. This enhances both the interactivity and the explainability of the system. [46] proposed a three-step prompting strategy that substantially surpasses traditional simple prompting techniques in zero-shot settings. [48] preprocess users\u2019 instructions and traditional feedback, such as clicks, using an instructor module to generate tailored guidance. [10] conduct an evaluation to assess off-the-shelf LLMs for RS, analyzing them from point-wise, pair-wise, and list-wise perspectives.\n# 3 Data\n# 3 Data 3.1 Source\n# 3.1 Source\nIn this study, we utilized a comprehensive dataset tailored for restaurant reviews analysis. This data is published in Kaggle1 and it is collected from Google reviews [45]. This dataset\n1https://www.kaggle.com/\nmprises user-generated reviews for various restaurants. ch entry includes: \u2022 a unique user ID \u2022 the business ID of the restaurant being reviewed \u2022 the review rating (ranging from 1 to 5, we consider a rating below greater or equal to 4 as a positive rating) \u2022 the actual text of the review \u2022 images uploaded by the user that are associated with the review.\n# 3.2 Splitting\nOur primary objective in this study is to analyze the impact of LLM summarization capabilities on feature engineering, rather than addressing the cold start problem. Therefore, during data pre-processing, we exclude any business ID or user ID that has only one associated review. This ensures that after splitting the dataset into training and test sets, no user or business in the test set will face the cold start issue, allowing us to directly apply features derived from the training set to the test set reviews. After eliminating these data points, we set the train-test split ratio to 3:1. We then perform random sampling with the condition that every user and business ID in the test set must also exist in the training set. Eventually, the train set contains 50468 positive reviews and 6537 negative reviews, test set contains 15370 positive reviews and 2032 negative reviews.\n# 4 Methodology\nOur proposed model leverages deep learning techniques and the advanced reasoning capabilities provided by large language models (LLMs) to enhance the performance of the ranking model. We hypothesize that the summarization power of LLMs can significantly improve the discriminative capabilities of the ranking model, leading to more accurate and relevant recommendations. In this section, we introduce the various components of our proposed LLM-enhanced Deep Learning Recommendation Model (DLRM). We detail the architecture, data preprocessing steps, feature engineering techniques, and the training process. Additionally, we discuss the integration of LLMs into the DLRM framework, highlighting how their contextual understanding and summarization abilities contribute to improved model performance.\n# 4.1 DLRM and the base model\nDLRM is a robust framework that leverages deep learning techniques for recommendation tasks [33]. DLRM has proven to be highly effective in personalization and recommendation scenarios, such as click-through rate (CTR) prediction [15, 37]. At its core, DLRM comprises three main components: the Bottom Multi-Layer Perceptron (MLP), the Feature Interaction module, and the Top MLP. The Bottom MLP is designed to process and extract signals from dense or\ncontinuous features, learning essential patterns and representations. The Feature Interaction module then takes the embeddings of sparse or categorical features, along with the output of the Bottom MLP, to capture and model the interactions between all features comprehensively. Specifically, the output from the Bottom MLP and sparse embeddings are concatenated, and the inner product is calculated across all pairwise dimensions. Finally, the Top MLP combines the outputs from the Feature Interaction module and the Bottom MLP to make the final prediction, such as the click-through probability in our case. As shown in Figure 1, we process the textual information contained in user reviews by converting each text review into an embedding using the sentence transformer. Specifically, we use MiniLM-L6-v2 model for all our experiments [49]. We then take the element-wise average across all dimensions to create user or business features. Similarly, each image contained in the user reviews or associated with a particular business is transformed into continuous data using ResNet50. Specifically, we extract the second-to-last layer to represent the images, capturing rich feature representations.\n# 4.2 LLM summarization\nTo leverage the summarizing power of large language modls (LLMs), we propose various methods to enhance the feaures fed into our model. In this section, we explain how the LLM-enhanced DLRM differs from the base model presented bove. \u2022 Dense Features: In addition to the continuous features used in the base model (e.g., the number of reviews a restaurant received, average rating), we utilize the LLM\u2019s reasoning ability to extract pricing information from user reviews. This enriched feature set provides a more comprehensive understanding of the restaurant being scored. \u2022 Sparse Features: Instead of applying an element-wise average across embeddings from all textual reviews or images, we use the LLM to process and summarize the most important information from all reviews, obtaining an embedding for this summary alone. For images, we leverage the LLM\u2019s multimodal capabilities to interpret and summarize the information contained in the images, converting them into textual descriptions. This textual information is then processed in a similar manner to the reviews. This approach offers several advantages: \u2022 Reduction of Noise: By summarizing the most important information, we ensure that only relevant data is fed into the model, preventing irrelevant or noisy information from diluting important signals. \u2022 Unified Embedding Technique: Since images are converted into textual descriptions, both reviews and images use the same embedding technique to transform\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c4f3/c4f38715-8b41-4465-aa35-51f22cf82733.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. Base model for the restaurant recommendation task</div>\nthem into continuous data. This ensures that features from different modalities are projected into the same latent space, enhancing the model\u2019s ability to understand and utilize the combined information effectively.\nBesides the differences mentioned above, as shown in Figure 2, we also introduce an additional sparse feature into the model. Using the LLM, we categorize each restaurant into one of 11 categories. This categorical feature is then fed into the feature interaction module after the embedding layer.\n# 4.3 Dimension reduction\nTo mitigate the risk of overfitting caused by the high-dimensio outputs of both the sentence transformer (384 dimensions) and ResNet50 (2048 dimensions), we propose an upstream model for dimensionality reduction. This approach preserves meaningful information while addressing the potential increase in model parameters. Our method involves the following steps:\nImportantly, we apply the same MLP used in training to the embeddings during the testing phase. This approach ensures effective dimensionality reduction while retaining the most crucial information for prediction. By implementing this technique, we balance model complexity and predictive power, enhancing the overall performance of our recommender system.\n# 5 Experiement\n# 5 Experiement 5.1 Parameter and Configuration\n# 5.1 Parameter and Configuration\nTo evaluate the model\u2019s performance under different conditions, we experiment with various dropout rates, different weighted loss function and baseline vs proposed model.\n\u2022 Dropout: is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero during training, specifically [0.1, 0.3, 0.5] \u2022 weighted loss: This dataset is highly imbalanced with 1/8 of the datapoint is associated with false labels and rest are postive samples. In order to balance the impact of each class on the loss function, we also experiment with different weighted loss functions. Weights are normalized after calculated by following formulas: \u2013 Basic:\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0692/0692860f-5fd0-4646-8c1b-ea466c3f994e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. LLM enhanced DLRM for the restaurant recommendation task</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d2bb/d2bb6d00-a0b5-41b4-8292-c42998455f68.png\" style=\"width: 50%;\"></div>\nFigure 3. LLM user summary example.\n\u2013 Square root:\nEach training starts with learning rate as 0.01, utilizing Adaptive Moment Estimation for adaptive learning rate adjustment. In each epoch, we evaluate the model performance against the training set and test set and update the best model if the model has a better false positive rate against the test set. We start applying early stops after finishing the first 300 epochs. Training stops when there is no continuous improvement of the false positive rate in the last 50 epochs. We repeated each parameter set evaluation five times.\n# 5.2 Data Pre-processing\nOur baseline model utilizes dense features and embedding features to predict the review rating.In constructing the following features, we exclude the current review to avoid bias. During training, for each data point (business_id, user_id, current review), we generate business features based on reviews from other users. This ensures that the current user\u2019s review does not influence the feature construction. Similarly, we generate user features based on reviews users provided to other businesses. During testing, we use the entire training dataset to construct features for the testing data points.\nTable 1. Performance of best model against training set (wgted loss is the way of calculating weighted loss, fp rate means false positive rate\naccuracy\nfp rate\nloss\nmodel\nwgted loss\ndropout\nproposed\nbasic\n0.10\n91.62%\n2.02%\n2.7\n0.30\n88.11%\n4.14%\n4.08\n0.50\n86.06%\n7.06%\n5.28\nsqrt root\n0.10\n95.95%\n4.36%\n4.36\n0.30\n94.14%\n13.69%\n6.35\n0.50\n91.95%\n18.87%\n9.19\nbaseline\nbasic\n0.10\n92.51%\n26.58%\n6.35\n0.30\n92.39%\n27.82%\n6.82\n0.50\n92.23%\n28.27%\n6.97\nsqrt root\n0.10\n94.12%\n29.01%\n10.46\n0.30\n93.97%\n31.02%\n11.07\n0.50\n94.06%\n33.49%\n11.54\n<div style=\"text-align: center;\">Table 2. Performance of best model against testing set</div>\naccuracy\nfp rate\nloss\nmodel\nwgted loss\ndropout\nproposed\nbasic\n0.10\n85.27%\n27.22%\n50.18\n0.30\n84.02%\n20.84%\n30.66\n0.50\n83.48%\n18.16%\n15.96\nsqrt root\n0.10\n88.05%\n38.68%\n66.51\n0.30\n89.15%\n36.42%\n49.68\n0.50\n89.02%\n31.82%\n23.76\nbaseline\nbasic\n0.10\n91.36%\n31.80%\n9.32\n0.30\n91.89%\n31.74%\n7.66\n0.50\n91.83%\n31.79%\n7.64\nsqrt root\n0.10\n92.96%\n35.26%\n14.57\n0.30\n93.26%\n35.66%\n12.62\n0.50\n93.47%\n37.58%\n12.59\n\u2013 Number of Reviews for the Business: The total number of reviews received by current business. \u2013 Average Rating from the User: The average rating given by the user across all their reviews. \u2013 Average Rating for the Business: The average rating received by the business across all reviews. \u2022 Embedding Features: \u2013 User Review Text Feature: All reviews written by the user are collected and converted into 384dimensional embedding vectors using the sentence transformer. An average pooling operation is then applied to these vectors to produce a single 1x384 vector representing the user\u2019s review text feature. \u2013 Business Review Text Feature: Similarly, all reviews received by the business are collected and\nconverted into multiple 384-dimensional embedding vectors, then average pooling is used to generate a single 384-dimensional embedding vector. \u2013 Review Image Feature: Firstly, we use each image as input for a pre-trained reset-50 model, then extract features from the second-to-last layer. Each image generates a 1x2048-dimensional embedding vector.All image embedding vectors for a business are collected, and average pooling is applied to obtain a single embedding vector. Finally, we concatenate the business review text feature, user review text feature, and review image feature into a single big vector. This vector is then passed through a upstream model to reduce its dimension to 32.\nCompared to the baseline model, we add new dense feature: Price Tag, and a sparse Feature: Restaurant Category, into the proposed model. In addition, we replace all embedding features with new ones generated with the help of LLM and multi-modal model.\n\u2022 All following prompt engineering are using the GPT 3.5-turbo-1106 model. \u2022 Dense Features: \u2013 Price Tag Feature: Use LLMs to analyze reviews for a business. We use the prompt: \u201cCan you tell me if the price is over-rice, fair price, low price from reviews for this restaurant. Give me just the category \u201d. There can be a case that there is no clear indication of the price level in review. Therefore after some data post-processing, we generate a price tag categorized as fair price, overpriced, cheap price, or none for each restaurant \u2022 Sparse Features: \u2013 Restaurant Category Feature: Similar to the price tag feature, we use prompt \u201ccan you tell me what kind of restaurant this is from these reviews for the restaurant. Return me in this format:\u2019type\u2019\u201d. This prompt helps generate the best description of the restaurant category based on reviews. Given the limitation of the current LLM model, the result is not always a single restaurant type, usually a brief summary of the food style presented in reviews. Therefore, we generate each restaurant a list of subtypes. There are 179 distinct types and the maximum number of subtypes for a restaurant is 11. In the pre-processing step, all subtype tensors were padded to the length of 11, and the padding value is set to 179 (i.e. embedding table contains 180 distinct values and the last one is padding idx). \u2022 Embedding Features with LLM: \u2013 With the help of LLM, we are able to get a summary of review with prompts shown in Fig 3 to summarize all reviews written by the same user,\nthen convert the summary into an embedding vector representing the user. Similarly, we can get the embedding vector for a single business. We concatenate the business review summary features and user review summary features into one big vector. This vector is then passed through an upstream model to reduce its dimensionality of vector to 32. \u2013 In addition, we rely on a multi-modal model (BLIP 2) to produce textual information from images through unconditional image captioning, producing objective descriptions for the images as shown in Fig 4. Each image is converted to one description sentence and then is transformed into anembedding vector. Average pooling is used to combine these vectors to create a comprehensive image feature representation for the restaurant. The averaged vector is then passed through an upstream model to reduce its dimensionality to 32. \u2013 By concatenating the text vector and image vector together, we form the embedding features for the proposed model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9562/9562eea5-b646-4384-9d75-a3571d094ef4.png\" style=\"width: 50%;\"></div>\nFigure 4. Using BLIP-2 to perform unconditional image captioning on restaurant review image\n<div style=\"text-align: center;\">Figure 4. Using BLIP-2 to perform unconditional image captioning on restaurant review image</div>\n# 5.3 Result\n\u2022 Compared to the baseline model, the proposed model achieves a much better false positive rate in both the train set and the test set. According to Table 1, The best model against the train set is the proposed model with basic weighted loss and 0.1 dropout rate, the false positive rate is around 2%. \u2022 The model with the highest accuracy is the baseline model with a 0.5 dropout rate and square root weight. It achieves 93.47% accuracy with a 37.58% false positive rate against the test set as shown in Table 2. \u2022 Regarding the performance of the false positive rate against the test set, see Table 2, the best model turns\nout to be the proposed model with basic weighted loss and 0.5 dropout rate. It achieves a 18.16% false positive rate while accuracy on the test set still reaches 83.48% we obtain a 19.4% improvement in the false positive rate at the expense of 10% decrease in accuracy\n# 5.4 Analysis\nWe observed that when square root weighting is applied, there is a tendency for the false positive rate to increase. This is likely due to the model\u2019s focus on minimizing the overall error, potentially at the expense of higher precision. In the proposed model, we observe a clear over-fitting with a lower dropout rate, which, paradoxically, results in increased accuracy and a reduced false positive rate. Although the model exhibits over-fitting, the aim of this paper is not to fine-tune the model to achieve optimal performance but to leverage the Large Language Model (LLM) summarization capabilities. In the context of ranking and recommendation, it is crucial to avoid recommending items that do not match customers\u2019 preferences, which means a high false positive rate is unacceptable. In most practical applications, RS shows users a list of top N items. A lower false positive rate ensures that the recommendations are more aligned with the users\u2019 tastes and top 1 accuracy becomes less important. Usually, top N accuracy is high enough. In our experiment, the baseline model tends to label data as positive which results in higher accuracy due to the imbalanced dataset, and performs worse in identifying false samples. Therefore, instead of focusing on accuracy, we consider the proposed model to be better than the baseline model given its superior performance on a low false positive rate. The significant reduction in the false positive rate observed in the proposed model can be attributed to the powerful ability of Large Language Models (LLMs) in summarizing reviews. LLMs, trained on vast and diverse corpora, possess extensive knowledge and nuanced understanding of language, enabling them to distill complex information effectively. When tasked with generating features like price levels or restaurant types from multiple reviews, LLMs can synthesize and summarize data, extracting relevant details and insights that might be challenging to identify through traditional methods. This process not only enhances the original information by integrating the contextual and experiential knowledge embedded in the reviews but also enriches the dataset with features that are informed by the broader understanding LLMs have acquired during their own training. In addition, LLMs are good at extracting and emphasizing critical and repeated information in various reviews. By using prompts to summarize reviews, LLM helps provide a more accurate and representative summary of the sentiments and opinions expressed by users. This summarization power ensures that the model captures the essential features that\ndistinguish different users and restaurants, leading to more precise recommendations. In contrast, the baseline model employs a simpler approach by averaging the embedding vectors of all reviews. This method tends to dilute the information because it treats all reviews equally, regardless of their quality or relevance. Consequently, the baseline model is prone to incorporating a lot of noise into the embeddings, which can obscure the critical information needed to make accurate predictions. This noise makes it difficult for the model to effectively differentiate between user preferences and restaurant characteristics, resulting in a higher false positive rate. In addition, the proposed model leverages the multimodal model (BLIP2) and its description ability to identify multiple food items listed in review images, helping the model to discriminate between various users and restaurants. By contrast, the baseline model relies on the image classification model(resnet), which has limitations in identifying multiple objects within the image, especially when dealing with multiple types. Image classification models alone fail to capture the semantic meaning within images, making the extracted signals less powerful when fed to the recommender.\n# 6 Conclusion\nIn this paper, we have proposed an innovative framework that harnesses the reasoning and summarization capabilities of LLMs to process multi-modal information effectively. Our research demonstrates the significant potential of integrating multi-modal data to enhance the performance of deep learning-based recommender systems, particularly in scenarios involving imbalanced datasets. The utilization of LLMs enables the projection of features from diverse modalities into a unified latent space, facilitating more efficient model learning and convergence. Specifically, our approach leverages LLMs to: \u2022 Summarize user review texts, capturing nuanced user behaviors and preferences. \u2022 Generate descriptive text for images, extracting implicit knowledge about businesses and products.\nThis novel method allows for the transformation of imagebased information into textual data, which can then be processed using the same text encoder employed for user reviews. Consequently, both image-derived and text-based features are represented in the same latent space when fed into the model, ensuring a more cohesive and comprehensive input. Our findings indicate that the incorporation of these LLMgenerated signals leads to substantial improvements in model performance. We hypothesize that this enhancement is particularly pronounced due to two factors: \u2022 The ability to extract valuable insights from negative reviews, which often contain critical information for recommendation systems.\n\u2022 The summarization capabilities of LLMs, which ensure that essential information is distilled and preserved, rather than diluted during the averaging process typically employed in traditional approaches.\n# References\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Hyeon-woo An and Nammee Moon. 2022. Design of recommendation system for tourist spot using sentiment analysis based on CNN-LSTM. Journal of Ambient Intelligence and Humanized Computing 13, 3 (2022), 1653\u20131663. [3] Oren Barkan, Roy Hirsch, Ori Katz, Avi Caciularu, and Noam Koenigstein. 2021. Anchor-based collaborative filtering. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2877\u20132881. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. https://proceedings.neurips.cc/paper_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf [5] Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, and Yi-Hsuan Yang. 2019. Collaborative similarity embedding for recommender systems. In The World Wide Web Conference. 2637\u20132643. [6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7\u201310. [7] Jin Yao Chin, Kaiqi Zhao, Shafiq Joty, and Gao Cong. 2018. ANR: Aspect-based neural recommender. In Proceedings of the 27th ACM International conference on information and knowledge management. 147\u2013156. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168 [cs.LG] [9] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems. 191\u2013198. 10] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering chatgpt\u2019s capabilities in recommender systems. In Proceedings of the 17th ACM Conference on Recommender Systems. 1126\u20131132. 11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4171\u20134186. https://doi.org/10.18653/v1/N19-1423 12] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable\nllms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023). [13] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems. 299\u2013315. [14] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. News Summarization and Evaluation in the Era of GPT-3. arXiv:2209.12356 [cs.CL] [15] Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang He. 2021. An embedding learning framework for numerical features in ctr prediction. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2910\u20132918. [16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30 (2017). [18] Khalid Haruna, Maizatul Akmar Ismail, Suhendroyono Suhendroyono, Damiasih Damiasih, Adi Cilik Pierewan, Haruna Chiroma, and Tutut Herawan. 2017. Context-Aware Recommender System: A Review of Recent Developmental Process and Future Research Direction. Applied Sciences 7, 12 (2017). https://doi.org/10.3390/app7121211 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings in deep residual networks. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14. Springer, 630\u2013645. [20] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639\u2013648. [21] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web. 173\u2013182. [22] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 585\u2013 593. [23] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [24] Andrew Koch, Jiahao Tian, and Michael D Porter. 2020. Criminal Consistency and Distinctiveness. In 2020 Systems and Information Engineering Design Symposium (SIEDS). IEEE, 1\u20133. [25] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 22199\u2013 22213. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf [26] Mikhail V Koroteev. 2021. BERT: a review of applications in natural language processing and understanding. arXiv preprint arXiv:2103.11943 (2021). [27] Chintoo Kumar, C Ravindranath Chowdary, and Deepika Shukla. 2022. Automatically detecting groups using locality-sensitive hashing in group recommendations. Information Sciences 601 (2022), 207\u2013223. [28] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic embedding trajectory in temporal interaction networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 1269\u20131278.\n[29] Chenliang Li, Cong Quan, Li Peng, Yunwei Qi, Yuming Deng, and Libing Wu. 2019. A capsule network for recommendation and explaining what you like and dislike. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. 275\u2013284. [30] Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023. Large Language Models Understand and Can be Enhanced by Emotional Stimuli. arXiv:2307.11760 [cs.CL] [31] Seth Siyuan Li and Elena Karahanna. 2015. Online recommendation systems in a B2C E-commerce context: a review and future directions. Journal of the association for information systems 16, 2 (2015), 2. [32] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 158\u2013167. https://doi.org/10.18653/v1/P17-1015 [33] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019). [34] Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. Pinnerformer: Sequence modeling for user representation at pinterest. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining. 3702\u20133712. [35] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2685\u20132692. [36] Ryoma Sato. 2022. Enumerating fair packages for group recommendations. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. 870\u2013878. [37] Qingquan Song, Dehua Cheng, Hanning Zhou, Jiyan Yang, Yuandong Tian, and Xia Hu. 2020. Towards automated neural interaction discovery for click-through rate prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 945\u2013955. [38] Maria Stratigi, Evaggelia Pitoura, Jyrki Nummenmaa, and Kostas Stefanidis. 2022. Sequential group recommendations based on satisfaction and disagreement scores. Journal of Intelligent Information Systems (2022), 1\u201328. [39] Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming Tang, and Xiuqiang He. 2019. Multi-graph convolution collaborative filtering. In 2019 IEEE International Conference on Data Mining (ICDM). IEEE, 1306\u20131311. [40] Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu, Jiwei Li, Tianwei Zhang, and Guoyin Wang. 2023. Sentiment Analysis through LLM Negotiations. arXiv:2311.01876 [cs.CL] [41] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4149\u20134158. https://doi.org/10.18653/v1/N19-1421 [42] Jiahao Tian and Michael D Porter. 2022. Changing presidential approval: Detecting and understanding change points in interval censored polling data. Stat 11, 1 (2022), e463.\n[43] Jiahao Tian and Michael D Porter. 2024. Time of week intensity estimation from partly interval censored data with applications to police patrol planning. Journal of Applied Statistics (2024), 1\u201319. [44] Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. GPT-RE: In-context Learning for Relation Extraction using Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 3534\u20133547. https://doi.org/10. 18653/v1/2023.emnlp-main.214 [45] Haowen Wang. 2023. Google Restaurants Rating [recommendation system]. https://www.kaggle.com/datasets/hwwang98/googlerestaurants Accessed: 2024-05-20. [46] Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153 (2023). [47] Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. GPT-NER: Named Entity Recognition via Large Language Models. arXiv:2304.10428 [cs.CL] [48] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua. 2023. Generative recommendation: Towards next-generation recommender paradigm. arXiv preprint arXiv:2304.03516 (2023). [49] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for TaskAgnostic Compression of Pre-Trained Transformers. In Proceedings of the 34th International Conference on Neural Information Processing Systems. 5776\u20135788. [50] Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022. Towards unified conversational recommender systems via knowledgeenhanced prompt learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1929\u20131937. [51] Yining Wang, Jinman Zhao, and Yuri Lawryshyn. [n. d.]. GPT-Signal: Generative AI for Semi-automated Feature Engineering in the Alpha Research Process.. In Proceedings of the Joint Workshop of the 8th Financial Technology and Natural Language Processing, and the 1st Workshop on Agent AI for Scenario Planning @ IJCAI 2024. [52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824\u201324837. [53] Chao Wu, Sannyuya Liu, Zeyu Zeng, Mao Chen, Adi Alhudhaif, Xiangyang Tang, Fayadh Alenezi, Norah Alnaim, and Xicheng Peng. 2022. Knowledge graph-based multi-context-aware recommendation algorithm. Information Sciences 595 (2022), 179\u2013194. [54] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. NPA: neural news recommendation with personalized attention. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 2576\u20132584. [55] Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, and Hongwei Wang. 2023. Empirical Study of Zero-Shot NER with ChatGPT. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 7935\u2013 7956. https://doi.org/10.18653/v1/2023.emnlp-main.493 [56] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Jiajie Xu, Victor S Sheng S. Sheng, Zhiming Cui, Xiaofang Zhou, and Hui Xiong. 2019. Recurrent convolutional neural network for sequential recommendation. In The world wide web conference. 3398\u20133404. [57] Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng, Xiang Li, and Wei Lin. 2023. Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm. In Proceedings of the 17th ACM Conference on Recommender Systems. 599\u2013601. [58] Shuxun Zan, Yujie Zhang, Xiangwu Meng, Pengtao Lv, and Yulu Du. 2021. UDA: A user-difference attention for group recommendation.\nInformation Sciences 571 (2021), 401\u2013417. [59] Qinglong Zhang and Yu-Bin Yang. 2021. Rest: An efficient transformer for visual recognition. Advances in neural information processing systems 34 (2021), 15475\u201315485. [60] Song Zhang, Nan Zheng, and Danli Wang. 2022. GBERT: Pre-training user representations for ephemeral group recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2631\u20132639. [61] Rongmei Zhao, Shenggen Ju, Jian Peng, Ning Yang, Fanli Yan, and Siyu Sun. 2022. Two-level graph path reasoning for conversational recommendation with user realistic preference. In proceedings of the 31st ACM international conference on information & knowledge management. 2701\u20132710. [62] Jiayin Zheng, Juanyun Mai, and Yanlong Wen. 2022. Explainable session-based recommendation with meta-path guided instances and self-attention mechanism. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2555\u20132559. [63] Lei Zheng, Vahid Noroozi, and Philip S Yu. 2017. Joint deep modeling of users and items using reviews for recommendation. In Proceedings of the tenth ACM international conference on web search and data mining. 425\u2013434. [64] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1059\u20131068. [65] Xiaokang Zhou, Yue Li, and Wei Liang. 2020. CNN-RNN based intelligent recommendation for online medical pre-diagnosis support. IEEE/ACM Transactions on Computational Biology and Bioinformatics 18, 3 (2020), 912\u2013921.\n",
    "paper_type": "method",
    "attri": {
        "background": "The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. This surge in content presents unique challenges for designing effective recommender systems. Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences. This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques.",
        "problem": {
            "definition": "The problem addressed in this paper is the inefficiency of existing recommender systems in effectively utilizing multi-modal information, specifically natural language data and images, to improve recommendation accuracy and relevance.",
            "key obstacle": "The main difficulty lies in the traditional methods' inability to integrate and process diverse modalities effectively, which limits their capacity to generate personalized and contextually relevant recommendations."
        },
        "idea": {
            "intuition": "The inspiration for the proposed idea stems from the observation that LLMs can better understand and utilize natural language data, which is abundant in recommendation contexts.",
            "opinion": "The proposed idea is to develop a framework that combines LLMs with deep learning techniques to enhance the performance of recommender systems by integrating multi-modal information.",
            "innovation": "The primary innovation of this method is the unification of diverse modalities into a latent space, which simplifies the learning process for the ranking model and enhances its discriminative power."
        },
        "method": {
            "method name": "LLM-enhanced Deep Learning Recommendation Model (DLRM)",
            "method abbreviation": "DLRM",
            "method definition": "DLRM is a framework that leverages deep learning techniques and LLMs to process multi-modal information for improving recommendation accuracy.",
            "method description": "The method focuses on utilizing LLMs to extract and integrate features from both text and images, projecting them into a unified latent space.",
            "method steps": [
                "Extract textual features from user reviews using LLMs.",
                "Transform images into textual descriptions and extract features.",
                "Combine features from both modalities into a single representation.",
                "Train the ranking model using the unified feature set."
            ],
            "principle": "This method is effective because it allows for a more nuanced understanding of user preferences by leveraging the summarization and reasoning capabilities of LLMs, which can distill complex information from diverse data sources."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved a dataset of restaurant reviews collected from Google reviews on Kaggle, with a train-test split ratio of 3:1, ensuring that no user or business in the test set faced cold start issues.",
            "evaluation method": "The performance of the proposed model was assessed by comparing it against baseline methods using various metrics, including accuracy and false positive rates, across different configurations of dropout rates and weighted loss functions."
        },
        "conclusion": "The research demonstrates the significant potential of integrating multi-modal data and LLMs to enhance the performance of deep learning-based recommender systems, particularly in imbalanced datasets. The findings indicate that LLM-generated features lead to substantial improvements in model performance, ensuring that essential information is preserved.",
        "discussion": {
            "advantage": "Key advantages of the proposed approach include improved accuracy in recommendations and reduced false positive rates due to effective multi-modal feature integration.",
            "limitation": "One limitation is the potential for overfitting, particularly with lower dropout rates, which could lead to less generalizable models.",
            "future work": "Future research could focus on refining the model to further reduce overfitting and exploring additional modalities or data sources to enhance recommendation accuracy."
        },
        "other info": {
            "keywords": [
                "Multi-Modality",
                "Large Language Models",
                "Recommender System",
                "Deep Learning Recommendation Model",
                "Personalization",
                "Imbalanced Dataset Modeling"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily."
        },
        {
            "section number": "1.2",
            "key information": "The proposed idea is to develop a framework that combines LLMs with deep learning techniques to enhance the performance of recommender systems by integrating multi-modal information."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed in this paper is the inefficiency of existing recommender systems in effectively utilizing multi-modal information, specifically natural language data and images, to improve recommendation accuracy and relevance."
        },
        {
            "section number": "2.3",
            "key information": "The primary innovation of this method is the unification of diverse modalities into a latent space, which simplifies the learning process for the ranking model and enhances its discriminative power."
        },
        {
            "section number": "3.2",
            "key information": "This method focuses on utilizing LLMs to extract and integrate features from both text and images, projecting them into a unified latent space."
        },
        {
            "section number": "4.1",
            "key information": "LLM-enhanced Deep Learning Recommendation Model (DLRM) leverages deep learning techniques and LLMs to process multi-modal information for improving recommendation accuracy."
        },
        {
            "section number": "8.1",
            "key information": "Key advantages of the proposed approach include improved accuracy in recommendations and reduced false positive rates due to effective multi-modal feature integration."
        },
        {
            "section number": "10.2",
            "key information": "Future research could focus on refining the model to further reduce overfitting and exploring additional modalities or data sources to enhance recommendation accuracy."
        }
    ],
    "similarity_score": 0.7946556434892033,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/MMREC_ LLM Based Multi-Modal Recommender System.json"
}