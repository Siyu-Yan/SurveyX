{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2111.07615",
    "title": "Optimism and Delays in Episodic Reinforcement Learning",
    "abstract": "There are many algorithms for regret minimisation in episodic reinforcement learning. This problem is well-understood from a theoretical perspective, providing that the sequences of states, actions and rewards associated with each episode are available to the algorithm updating the policy immediately after every interaction with the environment. However, feedback is almost always delayed in practice. In this paper, we study the impact of delayed feedback in episodic reinforcement learning from a theoretical perspective and propose two general-purpose approaches to handling the delays. The first involves updating as soon as new information becomes available, whereas the second waits before using newly observed information to update the policy. For the class of optimistic algorithms and either approach, we show that the regret increases by an additive term involving the number of states, actions, episode length, the expected delay and an algorithm-dependent constant. We empirically investigate the impact of various delay distributions on the regret of optimistic algorithms to validate our theoretical results.",
    "bib_name": "howson2023optimismdelaysepisodicreinforcement",
    "md_text": "Abstract\nThere are many algorithms for regret minimisation in episodic reinforcement learning. This problem is well-understood from a theoretical perspective, providing that the sequences of states, actions and rewards associated with each episode are available to the algorithm updating the policy immediately after every interaction with the environment. However, feedback is almost always delayed in practice. In this paper, we study the impact of delayed feedback in episodic reinforcement learning from a theoretical perspective and propose two general-purpose approaches to handling the delays. The first involves updating as soon as new information becomes available, whereas the second waits before using newly observed information to update the policy. For the class of optimistic algorithms and either approach, we show that the regret increases by an additive term involving the number of states, actions, episode length, the expected delay and an algorithm-dependent constant. We empirically investigate the impact of various delay distributions on the regret of optimistic algorithms to validate our theoretical results.\n# 1 Introduction\nEpisodic Reinforcement Learning (RL) considers the problem of an agent learning how to act in an unknow environment to maximise its cumulative reward. The problem formulation is broad enough to capture the na ture of sequential decision-making in many real-world scenarios as it permits complex dependencies betwee actions, rewards and future environmental states. Despite the complexity of the learning problem, there ar many provably efficient algorithms for this problem setting (Jaksch et al., 2010; Filippi et al., 2010; Fru et al., 2020; Azar et al., 2017; Dann et al., 2017).\nThese existing algorithms focus on the traditional model where one assumes that the algorithm updating the policy observes the sequence of states, actions and rewards at the end of every episode. Unfortunately, this immediate feedback assumption is unrealistic in almost all practical applications. In healthcare, for example, feedback relating to a patient on a particular treatment protocol is not observable to the policy maker until they return to the clinic at a scheduled time point in the future. In e-commerce, one observes a conversion at some unknown time long after a sequence of recommendations. Yet another example is wearable technology. Here, the heavy computation involved in policy updating must occur on a separate machine, forcing the communication of information, which naturally introduces a delay between the agent collecting feedback and the policy updater. In any of these scenarios, the algorithm must continue operating, despite lacking information from its past choices.\nThe above examples illustrate that delayed feedback is a fundamental challenge in real world reinforcemen learning. Unfortunately, there is little theoretical understanding of the impact of delays in episodic reinforce ment learning in the existing literature. We seek to fill this gap in the literature in this paper.\n# 1.1 Related Work\nRecently, the topic of delays has attracted a lot of attention in the bandit setting (Agarwal and Duchi, 2011; Dudik et al., 2011; Joulani et al., 2013; Mandel et al., 2015; Vernade et al., 2017; Pike-Burke et al., 2018;\nSarah Filippi\nZhou et al., 2019; Manegueu et al., 2020; Vernade et al., 2020). Here, the feedback is the reward associated with the chosen action in each round. Perhaps the most appealing approach in the multi-armed bandit setting is the queuing technique, which shows that the delays cause an additive penalty involving the expected delay for any base algorithm (Joulani et al., 2013; Mandel et al., 2015). The high-level idea is to build a metaalgorithm that creates a simulated non-delayed environment for any base algorithm designed for immediate feedback, such as UCB1 or KL-UCB. They achieve this by introducing a mechanism that stores the rewards for each action in separate queues and having the base algorithm interact with these rather than the actual environment. Unfortunately, the queuing technique does not readily extend to the delayed feedback setting in RL, as forming the queues would require knowledge of the state and action seen in each step of an episode; this information is delayed in our setting. Joulani et al. (2013) present another meta-algorithm for adversarial multi-armed bandits with delayed rewards that is trivial to adapt to our setting. They propose creating a new instance of the chosen base algorithm whenever there is no feedback, allowing one to bound the regret of each instance separately using standard techniques. More precisely, this involves maintaining \u03c4max + 1 versions of the algorithm, where \u03c4 \u2264\u03c4max almost surely (Joulani et al., 2013). Thus, the regret of taking this approach is multiplicative, as the maximal delay scales the regret of the base algorithm. Previous work in RL has considered constant delays in observing the current state in Markov Decision Processes (MDPs) (Katsikopoulos and Engelbrecht, 2003). More recent work considers delayed feedback in adversarial MDPs (Lancewicki et al., 2021). They developed an algorithm that computes stochastic policies based on policy optimisation. The regret of this algorithm depends on the sum of the delays, the number of states and the number of steps per episode. For stochastic MDPs, they state a regret bound of the form H3/2S \u221a AT + H2S\u03c4max, where H is the number of decisions the learner must make per episode, T = KH is the total number of decisions made across all K episodes, S is the number of states in the environment, A is the number of actions and \u03c4k \u2264\u03c4max. However, the leading order term in their regret bound is loose for many base algorithms. Their approach also requires a-priori knowledge of the maximal delay to define a phase of explicit exploration; this quantity is often unknown in many practical applications. Further, the base algorithm accrues linear regret in this exploration phase, and the maximal delay can be prohibitively large. We propose two approaches that avoid such prior knowledge and can leverage new information in the early episodes much faster, leading to tighter algorithm-specific theoretical results and better empirical performance. In addition to the improved theoretical results, we relax the assumption that the delay distribution has a finite and known maximum, and instead only require that the delays have a finite expectation that we assume is unknown.\nPrevious work in RL has considered constant delays in observing the current state in Markov Decision Processes (MDPs) (Katsikopoulos and Engelbrecht, 2003). More recent work considers delayed feedback in adversarial MDPs (Lancewicki et al., 2021). They developed an algorithm that computes stochastic policies based on policy optimisation. The regret of this algorithm depends on the sum of the delays, the number of states and the number of steps per episode. For stochastic MDPs, they state a regret bound of the form H3/2S \u221a AT + H2S\u03c4max, where H is the number of decisions the learner must make per episode, T = KH is the total number of decisions made across all K episodes, S is the number of states in the environment, A is the number of actions and \u03c4k \u2264\u03c4max. However, the leading order term in their regret bound is loose for many base algorithms. Their approach also requires a-priori knowledge of the maximal delay to define a phase of explicit exploration; this quantity is often unknown in many practical applications. Further, the base algorithm accrues linear regret in this exploration phase, and the maximal delay can be prohibitively large. We propose two approaches that avoid such prior knowledge and can leverage new information in the early episodes much faster, leading to tighter algorithm-specific theoretical results and better empirical performance. In addition to the improved theoretical results, we relax the assumption that the delay distribution has a finite and known maximum, and instead only require that the delays have a finite expectation that we assume is unknown.\n# 1.2 Contributions\nThe delayed feedback model studied in this paper poses several theoretical challenges that do not arise in the standard episodic reinforcement learning problem, such as delayed updates and disentangling the delays from the difficulty of the learning problem in the theoretical analysis.\nWe introduce two novel meta-algorithms to overcome these challenges, namely active and lazy updating. Both take any algorithm as input and transform it into an algorithm that can handle delayed feedback. Henceforth, we refer to the input algorithm as the base algorithm. Using these meta-algorithms, we obtain high probability regret bounds for any optimistic model-based base algorithm in the delayed feedback setting. For both active and lazy updating, the penalty for delayed feedback is an additive term involving the expected delay. Although they obtain similar theoretical results, active and lazy updating employ different algorithmic ideas to separate the delays from the learning problem in the theoretical analysis. The active updating meta-algorithm uses the base algorithm to update the policy as soon as it observes feedback from the environment. Deriving theoretical guarantees for active updating involves tackling the delays head-on, as the delays force the policy to remain constant across numerous episodes. Consequently, the learner can repeatedly make sub-optimal decisions. To quantify the impact of delayed feedback, we introduce several techniques that carefully separate the difficulty of the learning problem from the delays.\nThe lazy meta-algorithm works slightly differently. Instead of updating immediately, it waits for the amount of feedback to surpass some threshold before updating the policy. One can control this threshold, and therefore the frequency of policy updates, through a hyperparameter \u03b1. By waiting to update, lazy creates a simulated non-delayed version of the environment for the input algorithm, allowing us to handle the delays separately from the difficulty of the learning problem.\n# 2 Preliminaries\nWe consider the task of learning to act optimally in an unknown episodic finite-horizon Markov Decision Process, EFH-MDP. An EFH-MDP is formalised as a quintuple: M = (S, A, H, P, R). Here, S is the set of states, A is the set of actions, H is the horizon and gives the number of steps per episode, P = {Ph(\u00b7|s, a)}h,s,a is the set of probability distributions over the next state and R = {Rh(s, a)}h,s,a is the set of reward functions. For conciseness, we assume that the reward function is known, deterministic and bounded between zero and one for all state-action-step triples.1 In the episodic reinforcement learning problem, the base algorithm interacts with an MDP in a sequence of episodes: k = 1, 2, . . . , K. We denote the set of episodes by: [K] = {1, 2, . . . K}; a convention that we adopt for sets of integers. In this paper, we consider base algorithms that compute a deterministic policy \u03c0k : S \u00d7 [H] \u2192A at the start of each episode k \u2208[K]. It is known that in finite horizon stochastic MDPs, if an optimal policy exists, there is a deterministic optimal policy (Puterman, 1994). Once the base algorithm has computed a policy, an agent uses said policy to sample feedback from the environment by: selecting an action, ak h = \u03c0k(sk h, h); receiving a reward, rk h = Rh(sk h, ak h); and transitioning to the next state, sk h+1 \u223cPh(\u00b7|sk h, ak h); for each h = 1, \u00b7 \u00b7 \u00b7 , H. The feedback associated with the h-th step of the k-th episode is given by:\nD  { } We measure the quality of a policy, \u03c0, using the value function, which is the expected return at the end of the episode from the current step, given the current state:\n\ufffd \ufffd\ufffd Further, we denote the optimal value function by: V \u2217 h (s) = max\u03c0{V \u03c0 h (s)}, which gives the maximum expected return over deterministic policies \u2200(s, h) \u2208S \u00d7 [H]. When evaluating reinforcement learnin algorithms, it is common to use regret:\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd Throughout, T = KH denotes the total number of steps. Domingues et al. (2020) show that the lowe bound for the regret in the standard episodic reinforcement learning setting with stage-dependent transition is: \u2126(H \u221a SAT).\n# 2.1 Regret Minimisation in Model-Based RL\nMany provably efficient algorithms exist for learning in EFH-MDPs when feedback is immediate. In th paper, we focus on the large class of optimistic model-based reinforcement learning algorithms. These alg rithms maintain estimators of the transition probabilities for each (s, a, s\u2032) \u2208S \u00d7 A \u00d7 S:\n(1)\n(2)\n(3)\nThere are two main ways of ensuring optimism using model-based algorithms. The first is the modeloptimistic approach, which maintains a confidence set around \u02c6Pkh that contains Ph with high probability (Jaksch et al., 2010; Filippi et al., 2010; Fruit et al., 2020). The second is the value-optimistic approach, which involves directly upper bounding the optimal value function with high probability by adding a bonus to the value function of a policy under the estimated transition density \u02c6Pkh (Dann et al., 2017; Azar et al., 2017). Recent work has shown that all model-based optimistic algorithms have a value-optimistic representation, meaning they all compute a value function of the following form (Neu and Pike-Burke, 2020):\nwhere H\u2032 = H \u2212h and\nis the exploration bonus and x \u2227y = min{x, y}. Here, B1 and B2 are algorithm-dependent quantities which may depend on S, A, H, log(T) or the empirical variance of the optimistic value function. A suitably chosen exploration bonus ensures the computed value function is optimistic with high probability. For our theoretical results to hold, we require the following assumption on the base algorithm.\nAssumption 1. The exploration bonus upper bounds the estimation error with high probability. Mathematically: \u03b2+ kh(s, a) \u2265\u27e8( \u02c6Pkh \u2212Ph)(\u00b7|s, a), V \u2217 h+1(\u00b7)\u27e9for all time-steps, with probability 1 \u2212\u03b4.\nAll value-optimistic algorithms explicitly use the estimation error to derive suitable bonuses. Further, modeloptimistic algorithms compute bonuses satisfying this assumption implicitly (Neu and Pike-Burke, 2020). Therefore, Assumption 1 allows us to capture a wide range of model-based algorithms. For our analysis, it will be helpful to define an algorithm-dependent variable C, which indicates whether the algorithm\u2019s bonuses satisfy the following inequality:\n\u03b2+ kh(s, a)) < \ufffd\ufffd \u02c6Pkh \u2212Ph \ufffd (\u00b7 |s, a) , \u02dcV \u03c0k h+1(\u00b7) \ufffd\n\ufffd\ufffd \ufffd \ufffd for all s, a, h, k with probability 1 \u2212\u03b4. Intuitively, C = 1 corresponds to a bonuses that sits somewhere between the estimation error and the difference between the expectation of the optimistic value function under the estimated and true transition function. Since these bonuses must sit within a specific (potentially narrow) interval, they are tighter. However, as we will see later, such bonuses come at the expense of lowerorder terms. UBEV and UCBVI are algorithms where C = 1. Whereas UCRL2, UCRL2B, KL-UCRL and \u03c72-UCRL are algorithms with C = 0.\n# 3 Delayed Feedback\nUnder stochastic delays, the feedback from an episode does not return to the base algorithm immediately after the interaction. Instead, it returns at some unknown time in the future, k + \u03c4k. Here, \u03c4k denotes the random delay between the agent playing the kth episode and the base algorithm receiving the corresponding feedback. Throughout this paper, we make the following assumption about the delays:\n(4)\n(5)\n(6)\nAssumption 2. The delays are positive, independent and identically distributed random variables with a finite expected value, E[\u03c4k] < \u221e. The introduction of delays causes the feedback associated with an episode to return at some unknown time in the future, k + \u03c4k. As a result, the base algorithm cannot update its policy using feedback from episode k at the start of episode k + 1. Instead, it can only use feedback it has observed, e.g. the feedback associated with episodes i : i + \u03c4i < k + 1. When working with delayed feedback in RL, it is helpful to introduce the observed and missing visitation counters:\nThe introduction of delays causes the feedback associated with an episode to return at some unknown time in the future, k + \u03c4k. As a result, the base algorithm cannot update its policy using feedback from episode k at the start of episode k + 1. Instead, it can only use feedback it has observed, e.g. the feedback associated with episodes i : i + \u03c4i < k + 1.\nThese are related to the total visitation counter by\nWhen the feedback is delayed, optimistic algorithms can only compute their bonuses and any required estimators using the observed visitation counter. The corresponding value functions are still optimistic, but they contract to the optimal value function more slowly since Nkh(s, a) \u2265N \u2032 kh(s, a).\n# 3.1 Bounding the Missing Episodes\nIn our analysis, it is helpful to bound the number of missing episodes to get an upper bound on the amount of information missing for each state-action-step. This is done in the following lemma.\nLemma 1. Let Sk = \ufffdk\u22121 i=1 1{i + \u03c4i \u2265k}, where \u03c41, \u03c42, \u00b7 \u00b7 \u00b7 \u03c4k\u22121 \u223cf\u03c4(\u00b7) are independent and identically distributed random variables with finite expected value. We define\nto be the failure event for a single k. Then, P(F\u03c4) = P(\u222a\u221e k=1F \u03c4 k ) \u2264\u03b4\u2032.\nProof. Firstly, notice that Sk is a sum of Bernoulli random variables, meaning it is subgaussian. Therefore, one can apply Bernstein\u2019s inequality to obtain the following upper bound that holds with probability 1 \u2212\u03b4\u2032:.\nThe remainder of the proof follows from noticing that E[Sk] \u2264\ufffd\u221e i=0 P(\u03c4 > i), which is the tail probability function of the delay distribution and is equal to the expected delay. Similarly, one can show that Var (Sk) \u2264 E[Sk] \u2264E[\u03c4]. Substituting these values into the above inequality gives the result. See Appendix A.1 for a full proof.\n \u2264 full proof.\nwhich holds for all k \u2208[K] with probability 1\u2212\u03b4\u2032. Essentially, \u03c8\u03c4 K allows us to bound the amount of missing information in any given episode due to the delays.\n(8)\n(9)\n# 4 Meta-Algorithms For Delayed Feedback\nHere, we describe two flexible approaches that allow any base algorithm to handle delayed feedback. A tionally, we prove regret guarantees for both procedures, providing the base algorithm satisfies Assum 1. Regardless of the approach, we utilise the following regret decomposition for optimistic base algori that holds for both the delayed and non-delayed settings.\n\ufffd \ufffd where L = log \ufffd S2AH\u03c02/6\u03b4\u2032\ufffd and C indicates whether the bonuses of the algorithm satisfy Equation (6). Proof. See Appendix B.1.\n# 4.1 Active Updating\nThe first meta-algorithm we propose is active updating, which leverages new information by updating as soon as it becomes available. The remainder of this subsection focuses on bounding the regret for model-based optimistic algorithms using active updating, whose pseudo-code is outlined in Algorithm 1.\nAlgorithm 1 Active Updating\nInput. Base(N \u2032, M \u2032) (any base algorithm).\nInitialise. N \u2032 = {N \u2032\nh(s, a) = 0}h,s,a and M \u2032 = {M \u2032\nh(s, a, s\u2032) = 0}h,s,a with\nM \u2032\nh(s, a, s\u2032) :=\n\ufffd\ni:i+\u03c4i<k\n1{(si\nh, ai\nh, si\nh+1) = (s, a, s\u2032)}\nCompute policy: \u03c01 = Base(N \u2032, M \u2032)\nfor k = 1 to K do\nif \u2203i : k \u22122 < i + \u03c4i \u2264k \u22121 then\nUpdate the counters: N \u2032 and M \u2032.\nUpdate the policy: \u03c0k = Base(N \u2032, M \u2032)\nelse\nReuse previous policy: \u03c0k = \u03c0k\u22121\nend if\nAn agent samples an episode using policy \u03c0k.\nend for\nBase(N \u2032, M \u2032) is the only input parameter for our algorithm and is the base algorithm. One could view it as a function that takes in the observed number of visits (N \u2032) and transitions (M \u2032), among other algorithmdependent hyperparameters, and returns a policy. For the class of optimistic algorithms, the additional hyperparameter is the confidence level, \u03b4. Theorem 1 (Active Updating). Under Assumption 1 and 2, with probability 1 \u2212\u03b4, the regret of any modelbased algorithm under delayed feedback:\nBase(N \u2032, M \u2032) is the only input parameter for our algorithm and is the base algorithm. One could view it as a function that takes in the observed number of visits (N \u2032) and transitions (M \u2032), among other algorithmdependent hyperparameters, and returns a policy. For the class of optimistic algorithms, the additional hyperparameter is the confidence level, \u03b4.\nRK \u2272B \u221a HSAT + max \ufffd B, B2, CH2S \ufffd HSA E [\u03c4]\n\ufffd \ufffd where \u2272suppresses numeric constants, poly-log and lower order terms, and B \u2265B1 is a upper bound on the leading-order term in the numerator of the exploration bonus that is a function of H and S, and holds for all (k, h) \u2208[K] \u00d7 [H]. Proof. From Lemma 2, it is clear that we must bound the summation of the bonuses to bound the regret. When there are no delays, one can utilise the fact that the visitation count for (s, a, h) at the start of episode\n\ufffd \ufffd where \u2272suppresses numeric constants, poly-log and lower order terms, and B \u2265B1 is a upper bound on the leading-order term in the numerator of the exploration bonus that is a function of H and S, and holds for all (k, h) \u2208[K] \u00d7 [H].\nk + 1 increases by one if the agent observed (s, a, h) in the k-th episode to bound this term. However, this is no longer the case under delayed feedback. Therefore, we introduce the following lemma to bound the delay-dependent visitation counter.\nLemma 3. Let Zp T = \ufffdK k=1 \ufffdH h=1 1/(N \u2032 kh(sk h, ak h))p. Then,\n\ufffd \ufffd Zp T \u2264 \ufffd 4 \u221a HSAT + 3HSA\u03c8\u03c4 K if p = 1 2 2HSA log (8T) + HSA\u03c8\u03c4 K log(16\u03c8\u03c4 K) if p = 1\nwith probability 1 \u2212\u03b4\u2032.\nProof. To prove the claim, we relate the sum involving the observed visitation counters to a sum involving the total visitation counters. To do so, we artificially introduce it into the summation by multiplying by one:\nThe term in the numerator of the first line is equivalent to the total visitation counter by the equivalence relation given in Equation (9). One can handle the first term using standard results from the immediate feedback setting. The remainder of the proof follows from carefully splitting the second term in the sum on the second line into two disjoint sets. Namely, we split the summation using two indicators: 1{N \u2032 kh(s, a) \u2265\u03c8\u03c4 K} and 1{N \u2032 kh(s, a) < \u03c8\u03c4 K}. After a little algebra, we find that we are able to apply results from the immediate feedback setting, which gives the final result. See Appendix A.2 for further details. For many algorithms, B1 depends polynomially on quantities related to the environment, e.g. H and S. For such algorithms, a direct application of Lemma 3 is able to separate the expected delay from the total number of decisions. This is in line with the intuition that the impact of delays are negligible once we have a reasonable model of the environment. However, for algorithms such as for UCRL2B, \u03c72-UCRL and UCBVI (Fruit et al., 2020; Neu and Pike-Burke, 2020; Azar et al., 2017):\n \ufffd Typically, one uses an application of Cauchy-Schwarz to separate the terms involving the variance from th involving the counters, which gives:\nLemma 3 shows that doing so would lead to the delays multiplying the leading order term, as the summation of the variances found underneath the square root is of order HT and multiplies the HSA\u03c8\u03c4 K that arises from bounding the summation of the observed visitation counter. Setting B = H/2 gives us an upper bound for these types of bonuses and avoids this multiplicative dependence.\n\ufffd \ufffd \ufffd \ufffd \ufffd Directly applying Lemma 3 gives the following upper bound on (10): (10) \u22644B \u221a HSAT + 3BHSA\u03c8\u03c4 K + 2 \ufffd B2 + 3CH2SL \ufffd HSA (log(8T) + \u03c8\u03c4 K log (16\u03c8\u03c4 K)) Substituting the above upper bound of the terms in Lemma 2 and setting \u03b4 = 5\u03b4\u2032 gives the stated result.\n(10)\nTable 4.3 in Section 4.3 presents regret bounds for various optimistic algorithms using active updating under delayed feedback that fit into our framework. Further discussion of the results can be found in Section 4.3.\n# 4.2 Lazy Updating\nInstead of updating the policy via the base algorithm as soon as new feedback becomes observable, we now consider waiting. We name the meta-algorithm that employs this technique lazy updating. Algorithm 2 presents the pseudo-code for this meta-algorithm.\nAlgorithm 2 Lazy Updating\nInput. Base(N \u2032, M \u2032, \u00b7 \u00b7 \u00b7 ) (any base algorithm) and \u03b1 (activity parameter).\nInitialise epoch: j = 1 and kj = 1.\nInitialise counters: N \u2032\nkh(s, a) = M \u2032\nkh(s, a, s\u2032) = 0.\nCompute policy: \u03c0kj = Base(N \u2032\nkh, M \u2032\nkh)\nfor k = 1 to K do\nUpdate counters, e.g. Equation (7).\nif \u2203(s, a, h) : N \u2032\nkh(s, a) \u2265(1 + 1/\u03b1)N \u2032\nkjh(s, a) then\nUpdate epoch: j = j + 1, kj = k\nUpdate epoch counter: Nkj(s, a) = N \u2032\nkh(s, a)\nUpdate the policy: \u03c0kj = Base(N \u2032\nkjh, M \u2032\nkjh)\nend if\nAn agent samples an episode using policy \u03c0kj.\nend for\nLazy updating works in batches of episodes which we call epochs and denote by j = 1, 2, \u00b7 \u00b7 \u00b7 , J. At the start of the j-th epoch, lazy updating uses the base algorithm to compute a policy using all the available information. The meta-algorithm uses this policy in every episode until the next epoch begins. Therefore, each epoch is just a set of episodes where the lazy updating algorithm uses the same policy. A new epoch begins as soon as there is an (s, a, h) whose observed visitation counter reaches 1 + 1/\u03b1 times the observed visits at the start of the epoch, where \u03b1 \u2208[1, \u221e). Note that \u03b1 = 1 corresponds to the wellknown doubling trick from Jaksch et al. (2010), and \u03b1 > 1 represents more frequent updating. Once the observed visitation counter triggers this condition, a new epoch begins, and the meta-algorithm uses the base algorithm to update the policy. Formally, we start epoch j + 1 in episode kj+1, which occurs when:\nLazy updating works in batches of episodes which we call epochs and denote by j = 1, 2, \u00b7 \u00b7 \u00b7 , J. At the start of the j-th epoch, lazy updating uses the base algorithm to compute a policy using all the available information. The meta-algorithm uses this policy in every episode until the next epoch begins. Therefore, each epoch is just a set of episodes where the lazy updating algorithm uses the same policy.\nA new epoch begins as soon as there is an (s, a, h) whose observed visitation counter reaches 1 + 1/\u03b1 times the observed visits at the start of the epoch, where \u03b1 \u2208[1, \u221e). Note that \u03b1 = 1 corresponds to the wellknown doubling trick from Jaksch et al. (2010), and \u03b1 > 1 represents more frequent updating. Once the observed visitation counter triggers this condition, a new epoch begins, and the meta-algorithm uses the base algorithm to update the policy. Formally, we start epoch j + 1 in episode kj+1, which occurs when:\nwhere\ncounts the observed number of visits between episodes k and l for l > k. Intuitively, this updating schem forces the number of samples needed for any particular (s, a, h) to trigger an update to increase exponentiall quickly, meaning that the total number of epochs should grow logarithmically in K. Lemma 4 confirms tha this is indeed the case.\ncounts the observed number of visits between episodes k and l for l > k. Intuitively, this updating scheme forces the number of samples needed for any particular (s, a, h) to trigger an update to increase exponentially quickly, meaning that the total number of epochs should grow logarithmically in K. Lemma 4 confirms that this is indeed the case. Lemma 4. For K \u2265SA and \u03b1 \u22651, Algorithm 2 ensures that the number of epochs has the following upper\nProof. See Appendix A.3 for further details.\n(11)\n(12)\nIn contrast to active updating, we will later see that the lazy updating scheme lets us bound the summation of the bonuses independently of the delays. This property means we can avoid upper bounding the numerator of the exploration bonus, B1, and get tighter leading order terms in the regret bound of the chosen base algorithm. In the regret analysis, we will utilise the following extension of the classic result by Jaksch et al. (2010) that illustrates the delay-independence of the bonuses.\nIn contrast to active updating, we will later see that the lazy updating scheme lets us bound the summation of the bonuses independently of the delays. This property means we can avoid upper bounding the numerator of the exploration bonus, B1, and get tighter leading order terms in the regret bound of the chosen base algorithm. In the regret analysis, we will utilise the following extension of the classic result by Jaksch et al. (2010) that illustrates the delay-independence of the bonuses. Lemma 5. If n0, n1, \u00b7 \u00b7 \u00b7 , nJ are an arbitrary sequence of real-valued numbers satisfying n0 := 0 and 0 \u2264nj \u22641 Nj\u22121 with Nj\u22121 = max{1, \ufffdj\u22121  ni} for all j \u2264J, then\nLemma 5. If n0, n1, \u00b7 \u00b7 \u00b7 , nJ are an arbitrary sequence of real-valued numbers satisfying n0 := 0 an 0 \u2264nj \u22641 \u03b1Nj\u22121 with Nj\u22121 = max{1, \ufffdj\u22121 i=0 ni} for all j \u2264J, then\nProof. We prove the claim for each case using an inductive argument similar to Jaksch et al. (2010). See Appendix A.3. Using Lemmas 4 and 5, we can derive regret bounds for any optimistic base algorithm that satisfies Assumption 1. Theorem 2. Let K \u2265SA and \u03b1 \u22651. Under Assumption 1 and 2, with probability 1 \u2212\u03b4, the regret of any model-based algorithm under delayed feedback is upper bounded by:\nwhere \u02c6RK(Base) is an upper bound on the regret of the chosen base algorithm under immediate feedback. Proof. By optimism and utilising the fact that epochs are disjoint sets of episodes, with probability 1 \u2212\u03b4\u2032:\nwhere the final inequality follows from separating the episodes where we update and bounding their contribution to the regret by HJ. Handling the remaining summation in the regret bound requires a little more care, which we do by splitting the remaining sum into two sets; episodes with short and long delays. An episode has a short delay if it is played and observed in the same epoch, 1{k+\u03c4k < kj+1}. Otherwise, it has a long delay, 1{k+\u03c4k \u2265kj+1}. One can show that the regret of episodes with long delays has the following upper bound:\nwhere the final inequality follows from separating the episodes where we update and bounding their cont bution to the regret by HJ.\nHandling the remaining summation in the regret bound requires a little more care, which we do by splitting the remaining sum into two sets; episodes with short and long delays. An episode has a short delay if it is played and observed in the same epoch, 1{k+\u03c4k < kj+1}. Otherwise, it has a long delay, 1{k+\u03c4k \u2265kj+1}. One can show that the regret of episodes with long delays has the following upper bound:\nAforementioned, Sk \u2264\u03c8\u03c4 K for all k \u2264K with probability 1 \u2212\u03b4\u2032. Therefore, we can upper bound the regret of episodes with long delays by HJ\u03c8\u03c4 K. All that remains is bounding the regret of episodes with short delays. Applying Lemma 2 to these episodes and re-arranging gives:2\n2Here, we have omitted lower order terms for brevity.\nwhere we have omitted the state-action-step triples that caused the update from the summation. By construction, all the state-action-step triples satisfy the conditions of Lemma 5. Applying this result to the summation of the bonuses and combining the contributions of the other terms gives the result. See Appendix A.4 for a full proof of the claim.\n# 4.3 Discussion\nTable 4.3 presents a selection of algorithms that fit into our framework and their accompanying theoretical guarantees when using the active and lazy updating meta-algorithms to handle delayed feedback. In particular, we see that acting in delayed environments causes an additive increase in regret for almost all combinations of optimistic base algorithms and meta-algorithms considered. This result mirrors what is seen in the bandit setting where algorithms incur an additive regret penalty involving E[\u03c4] (Joulani et al., 2013).\nBase Algorithm\nC\n\u02c6RK(Base)\nActive Updating\nLazy Updating\nUBEV (Dann et al., 2017)\n1\nH3/2\u221a\nSAT\n\u02c6RK(Base) + H3S2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\nUCBVI-CH (Azar et al., 2017)\n1\nH3/2\u221a\nSAT\n\u02c6RK(Base) + H3S2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\nUCRL2 (Jaksch et al., 2010)\n0\nH3/2S\n\u221a\nAT\n\u02c6RK(Base) + H2S3/2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\nKL-UCRL (Filippi et al., 2010)\n0\nH3/2S\n\u221a\nAT\n\u02c6RK(Base) + H2S3/2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\nUCRL2B (Fruit et al., 2020)\n0\nH\n\u221a\nS\u0393AT\n\u221a\nH \u02c6RK(Base) + H2S2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\n\u03c72-UCRL (Neu and Pike-Burke, 2020)\n0\nHS\n\u221a\nAT\n\u221a\nH \u02c6RK(Base) + H2S2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\nUCBVI-BF (Azar et al., 2017)\n1\nH\n\u221a\nSAT\n\u221a\nH \u02c6RK(Base) + H3S2AE[\u03c4]\n(1 + 1\n\u03b1) \u02c6RK(Base) + H2SAE[\u03c4]\nlog(1+ 1\n\u03b1 )\nTable 1: A selection of algorithms that fit into our framework and their regret bounds under delayed feedbac Here, \u0393 \u2264S denotes a uniform upper bound on the number of reachable states.\nFor active updating and some base algorithms, we found that the additive delay dependence comes at the price of a penalty to the leading order term in the regret bound. Namely, an extra \u221a H. This extra penalty multiplying the leading order term is a feature of the theoretical analysis. Another important factor influencing the impact of the delays when using active updating is the parameter C. The penalty for delayed feedback is higher when C = 1. The worsened delay dependence for these algorithms is due to the introduction of lower-order terms in the probabilistic analysis under immediate feedback, which allows for tighter bonuses. Unfortunately, these lower-order terms become dependent on the delays in our setting and thus lead to a worse delay dependence.\nprice of a penalty to the leading order term in the regret bound. Namely, an extra \u221a H. This extra penalty multiplying the leading order term is a feature of the theoretical analysis. Another important factor influencing the impact of the delays when using active updating is the parameter C. The penalty for delayed feedback is higher when C = 1. The worsened delay dependence for these algorithms is due to the introduction of lower-order terms in the probabilistic analysis under immediate feedback, which allows for tighter bonuses. Unfortunately, these lower-order terms become dependent on the delays in our setting and thus lead to a worse delay dependence. To rectify the undesirable penalty to the leading order terms and the dependence on C, we developed an alternative approach called lazy updating, which achieves the same additive delay dependence for all algorithms that fit into our framework with only a logarithmic penalty to the leading order term in the regret bound of the base algorithm under immediate feedback. This approach works by introducing an additional hyperparameter that controls how frequently the base algorithm updates its policy. We denote this hyperparameter by \u03b1 and name it the activity parameter. Theorem 2 indicates that there is a trade-off when selecting \u03b1. On the one hand, we would like to choose a large value of \u03b1 to minimise the penalty to the leading order term, which is arises from the slower updating. On the other hand, the penalty introduced by the delays is a strictly increasing function of \u03b1, making large values undesirable. As \u03b1 \u2192\u221e, lazy updating tends to active updating; at this limiting value, lazy updating will update as soon as it receives new feedback, just like active updating. Thus, the empirical performance of lazy updating should get closer to active updating as \u03b1 increases. In Section 5, we demonstrate that this is the case and show that it is possible to get most of the benefits of active updating with a relatively modest value of \u03b1, which has better worst-case regret bounds in the delayed feedback setting. Comparatively, our work significantly improves the regret bounds for many algorithms in the delayed feedback setting. Lancewicki et al. (2021) presents regret bounds for stochastic MDPs of the form H3/2S \u221a AT +\nH2S\u03c4max for all optimistic algorithms. Except for UCRL2 and KL-UCRL, the leading order term in their regret bound is loose in either H, S or both. Conversely, the leading order terms in our regret bounds are tight for all algorithms when utilising lazy updating and are only loose by a factor of \u221a H for a few algorithms when utilising active updating. Furthermore, E[\u03c4] \u226a\u03c4max in almost all scenarios. As a result, our regret bounds have a tighter delay dependence. Our algorithms also remove the need for a-priori knowledge of the maximal delay.\nHS\u03c4max for all optimistic algorithms. Except for UCRL2 and KL-UCRL, the leading order term in their regret bound is loose in either H, S or both. Conversely, the leading order terms in our regret bounds are tight for all algorithms when utilising lazy updating and are only loose by a factor of \u221a H for a few algorithms when utilising active updating. Furthermore, E[\u03c4] \u226a\u03c4max in almost all scenarios. As a result, our regret bounds have a tighter delay dependence. Our algorithms also remove the need for a-priori knowledge of the maximal delay. The setting of delayed feedback also generalises the case where only the rewards are delayed. Thus, our theoretical results also hold for this setting if we directly apply active or lazy updating. However, one could do better in this case by realising that it is only the delays impacting the rewards, meaning it is only necessary to apply the meta-algorithms to the estimation of the rewards. We expect the additive penalty to be HSAE[\u03c4]. Indeed, the improved delay-dependence is due to the fact that learning the expected reward function is an easier task than learning the transitions. We prove that this is indeed the case for UCRL2 algorithm of Jaksch et al. (2010) in Appendix B.2.\nThe setting of delayed feedback also generalises the case where only the rewards are delayed. Thus, our theoretical results also hold for this setting if we directly apply active or lazy updating. However, one could do better in this case by realising that it is only the delays impacting the rewards, meaning it is only necessary to apply the meta-algorithms to the estimation of the rewards. We expect the additive penalty to be HSAE[\u03c4]. Indeed, the improved delay-dependence is due to the fact that learning the expected reward function is an easier task than learning the transitions. We prove that this is indeed the case for UCRL2 algorithm of Jaksch et al. (2010) in Appendix B.2.\n# 5 Experimental Results\nIn this section, we investigate the impact of delayed feedback on the regret of active and lazy updating in the chain environment of Osband and Van Roy (2017). Briefly, this environment consists of a sequence of S states arranged side-by-side. The learner starts in the left-most state and has to decide between A = 2 actions, head left or right. Each episode consists of H = S decisions and the only state with a reward is the right-most state. Thus, the optimal policy is to head right at every step. Heading left is always successful. However, heading right is successful with probability 1\u22121/S. If unsuccessful, the learner moves one state to the left. Notably, any inefficient exploration strategy will take at least 2S episodes to learn the optimal policy (Osband and Van Roy, 2017).\nWe consider chains with H = S \u2208{5, 10, 20, 30} and use UCBVI-BF as the base algorithm in all of our experiments as it has the best regret guarantees under immediate feedback. For our lazy updating approach, we selected several values for the activity hyperparameter, \u03b1 \u2208{1, 10, 100}. In all our experiments, we set the confidence parameter of the base algorithm so that the regret bounds hold with probability 0.95. Additionally, we compare our meta-algorithms to the explicit exploration procedure proposed by Lancewicki et al. (2021). Their procedure requires prior knowledge of the maximum delay, which we provide by generating all the delays before the first episode and taking the maximum. In practice, the maximum delay is often unknown and possibly infinite, making this approach infeasible.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/56c3/56c3e8eb-1cdb-4d80-9564-79340b9e6fee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Cumulative Regret (S = 30, E[\u03c4] = 100).</div>\nFigure 1 displays the results for our experiments in the chain environment with S = 30 and E[\u03c4] = 100. The results for the other chain lengths and expected values are in Appendix C. Empirically, active updating achieves the best performance of all three meta-algorithms. However, our experimental results suggest that it is possible to get near identical performance with lazy updating by setting \u03b1 to be a large enough constant. Both active and lazy updating offer superior performance to the explicit exploration approach of Lancewicki et al. (2021) in all of our experiments, despite their meta-algorithm having prior knowledge of the delays. In some cases, our meta-algorithms have converged to the optimal policy before the explicit exploration procedure finishes; e.g. see Appendix C.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c5ed/c5edcc0c-0afb-48d0-94f9-98fc9f5dfac9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Delay Dependence (S = 30).</div>\nNext, we turn to considering the impact of different delay distributions on the regret of our meta-algorithms. Empirically, Figure 2 shows that the regret penalty of delays at the end of the final episode is linear in the expected delay for active updating and lazy updating, as our theory predicts. For lazy updating, the gradient of this linear relationship decreases with \u03b1, which is to be expected based on the log(1 + 1/\u03b1) term in the denominator of the delay-dependent terms in our regret bounds. Interestingly, lazy updating with \u03b1 = 1 is the most robust to the delay distribution. We believe that this is due to forcing the base algorithm to wait for long periods of time between updates. Intuitively, if the epochs are long enough, most information within an epoch will be received before an update, leading to little loss of information. Investigating this further is an interesting avenue for future work.\n# 6 Conclusion\nIn this paper, we provide two generic meta-algorithms that can extend any episodic reinforcement learning base algorithm to the setting of delayed feedback. Under mild assumptions on the algorithm and the delays, we show that both maintain the sub-linear theoretical guarantees of the chosen base algorithm and provide good empirical performance, regardless of the delay distribution. These first positive results for stochastically delayed feedback in episodic reinforcement learning prove that the penalty for delays is an additive term involving the expected delay that is independent of the number of episodes. This additive penalty matches what is seen in the multi-armed bandit setting, despite the additional complexities of the reinforcement learning problem.\nOur framework is broad enough to cover the theoretically successful class of optimistic model-based algorithms, and many existing algorithms fit into our framework. However, we believe that both updating procedures could be used for a wider class of base algorithms. For example, model-free optimistic algorithms and posterior sampling (Jin et al., 2018; Osband and Van Roy, 2017). Extending our analyses to cover these algorithms is left to future work.\n# A Missing Proofs\n# A.1 Bounding the Missing Episodes\nAn important aspect in our proofs is to bound the amount of missing information. Since we see only one stateaction pair per step of an episode, an upper bound on the missing visitation counter is simply the number of missing episodes. Lemma 1 bounds the number of missing episodes with high probability and only requires the delays have a finite expected value. Lemma 1. Let Sk = \ufffdk\u22121 i=1 1{i + \u03c4i \u2265k}, where \u03c41, \u03c42, \u00b7 \u00b7 \u00b7 \u03c4k\u22121 \u223cf\u03c4(\u00b7) are independent and identically distributed random variables with finite expected value. We define\nBy Bernstein\u2019s inequality, we have that:\nRearranging the above reveals that:\nBy Boole\u2019s inequality, we have that:\nas required.\n# A.2 Missing Proofs for Active Updating\nLemma 2 (the regret decomposition) and Equation (5) (the form of the exploration bonuses) reveal that the summation of the counters is an important quantity in determining the regret of an optimistic algorithm. Whenever \u03c4k = 0 for all k \u2264K, e.g. immediate feedback, we can use standard results that utilise the fact the counters increase by one between successive plays of a state-action pair at a given step. Lemma 6. Let Zp n = \ufffdN n=0 1/(1 \u2228n)p. Then, Zp n has the following upper bound:\nfor p = 1/2 and p = 1.\nfor p = 1/2 and p = 1. Proof. Removing the first two terms from the summation and upper bounding the remaining terms by integral gives:\nProof. Removing the first two terms from the summation and upper bounding the remaining terms by an integral gives:\nZp n = 2 + N \ufffd n=2 1 np \u22642 + \ufffdN 1 1 np dn \u22642 + \ufffd 2 \u221a N \u22122 if p \u22081 2 log (N) if p = 1 \u2264 \ufffd 2 \u221a N if p \u22081 2 log (8N) if p = 1\nas required.\nWhen \u03c4k is random, the observed visitation counter need not increase by one between successive plays of the same state-action-step. Instead, the counter only increases by one (or more in some cases) after a random number of episodes. In the worst-case scenario, the counter will remain constant between playing and observing the feedback associated with a specific state-action-step. Thus, the standard techniques no longer apply, and we must find another way to bound the summation of counters than can remain unchanged for numerous episodes due to the delays. We do this by relating the summation involving the observed visitation counter to one involving the total visitation counter, thereby splitting the terms affected by the delays from those that are not.\nLemma 3. Let Zp T = \ufffdK k=1 \ufffdH h=1 1/(N \u2032 kh(sk h, ak h))p. Then,\n\ufffd \ufffd Zp T \u2264 \ufffd 4 \u221a HSAT + 3HSA\u03c8\u03c4 K if p = 1 2 2HSA log (8T) + HSA\u03c8\u03c4 K log(16\u03c8\u03c4 K) if p = 1\nwith probability 1 \u2212\u03b4\u2032.\nProof. Unless otherwise stated, we let: Nkh(s, a) = 1 \u2228Nkh(s, a) and N \u2032 kh(s, a) = 1 \u2228N \u2032 kh(s, a) for notational convenience. First, we use the relationships between the observed, missing and total visitation counters to split the summation into two parts. To do so, in a similar manner to Lancewicki et al. (2021), we start by artificially introducing the total visitation counter:\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd From Equation (9), Nkh(s, a) = N \u2032 kh(s, a) + N \u2032\u2032 kh(s, a), for any (s, a, h) \u2208S \u00d7 A \u00d7 [H]. Consequently\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7fca/7fca5c7b-b08d-4384-b4c9-4a13f5d6a169.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">16</div>\nsince (1 + x)p \u22641 + xp for p = 1/2 and p = 1 and any x > 0. Term (i) is the summation of the total visitation counter. Thus, Lemma 6 applies. Bounding (ii) requires more care, as it involves the observed and missing visitation counters. Recall that the algorithm plays one state-action pair at each step in every episode. Thus, the missing visitation counter is upper bounded by the number of missing episodes: N \u2032\u2032 kh(s, a) \u2264Sk. Lemma 1 bounds the number of missing episodes: with probability 1 \u2212\u03b4\u2032, Sk \u2264\u03c8\u03c4 K across all k \u2208Z+. Splitting (ii) using the observed visitation counts and the upper bound on Sk gives:\nsince (1 + x)p \u22641 + xp for p = 1/2 and p = 1 and any x > 0. Term (i) is the summation of the total visitation counter. Thus, Lemma 6 applies.\nBounding (ii) requires more care, as it involves the observed and missing visitation counters. Recall that the algorithm plays one state-action pair at each step in every episode. Thus, the missing visitation counter is upper bounded by the number of missing episodes: N \u2032\u2032 kh(s, a) \u2264Sk. Lemma 1 bounds the number of missing episodes: with probability 1 \u2212\u03b4\u2032, Sk \u2264\u03c8\u03c4 K across all k \u2208Z+. Splitting (ii) using the observed visitation counts and the upper bound on Sk gives:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/96eb/96ebc55e-3e5a-4308-b56c-308912944a33.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd \ufffd The last inequality follows since for the first sum, N \u2032 kh(s, a) \u2265\u03c8\u03c4 K.</div>\nClearly, (ii.a) \u2264(i), as it is a summation over a subset of all the episodes. Using (9), it is possible to rewrite the indicator in the remaining term as: 1{Nkh(s, a) \u2212N \u2032\u2032 kh(s, a) \u2264\u03c8\u03c4 K}, for any (s, a, h) \u2208S \u00d7 A \u00d7 [H]. Further, N \u2032\u2032 kh(s, a) \u2264\u03c8\u03c4 K and N \u2032 kh(s, a) \u22651. Therefore,\nLemma 6 gives an upper bound of \ufffdN n=0 1/(1 \u2228n)p. Summing this upper bound over all state-action-step triples gives:\nTherefore:\nZp T \u22642A + B.2 \ufffd\n\u2264 \u2264 \ufffd 4 \u221a HSAT + 3HSA\u03c8\u03c4 K if p = 1 2 HSA (2 log (8T) + \u03c8\u03c4 K log (16\u03c8\u03c4 K)) if p = 1\nas required.\n# A.3 Missing Proofs for Lazy Updating\nWhen using active updating, we prove that the bound on the counts depends on the delay. However, we can mitigate this delay-dependence by taking a slower approach to updating, providing that the number of epochs is bounded and the counts between epochs satisfy certain constraints outlined in Section 4.2. Lemma 4. For K \u2265SA and \u03b1 \u22651, Algorithm 2 ensures that the number of epochs has the following upper bound:\nProof. In this proof, we extend arguments from the standard doubling trick of Jaksch et al. (2010) so that the learner can update more frequently. Firstly, we recall the definition of the observed visitation counter:4\nand the updating rule for j \u22651:\nkj+1 = arg min k>kj \ufffd \u2203s, a, h : N \u2032 k(s, a, h) \u2265 \ufffd 1 + 1 \u03b1 \ufffd N \u2032 kj(s, a, h) \ufffd\nNow, we define a counter that counts the observed number of visits between two episode\nDirect computation allows us to relate the observed visitation counter at the start of the (j + 1)-th epoch to the sum of the observed visitation counts within each of the previous epochs:\nwhere the second equality follows from the fact that an epoch is a disjoint set of episodes and the final equali follows from the definition of the between episodes visitation counter. From the above, it is easy to see tha\nThus, we can re-write the updating rule using the within episode counter as\nproviding that we have seen the state-action-step at least once.5 Therefore, at the end of each epoch there is a state-action-step with nkj+1 kj (s, a, h) \u2265N \u2032 kj(s, a, h)/\u03b1.\nSuppose N \u2032 (K+1)h(s, a) > 0 for a fixed (s, a, h) \u2208S \u00d7 A \u00d7 [H]. Define J(s, a, h) as the number of epochs with nkj+1 kj (s, a, h) \u2265N \u2032 kjh(s, a)/\u03b1. Or, equivalently, it is the number of epochs with Nkj+1 (s, a, h) \u2265 (1 + 1/\u03b1)N \u2032 kj(s, a, h). Then,\n\u22651 +\nThe first inequality follows from focusing only on the epochs where we update due to (s, a, h), where the +1 accounts for the first update due to the observing the given state-action-step triple. The second inequality follows from the condition in the subscript of the summation, e.g. we are updating due to (s, a, h). The final inequality follows from the definition of how we trigger updates and because we update J(s, a, h) times due to (s, a, h). Since \u03b1 \u2208[1, \u221e), Lemma 7 applies. Rearranging terms reveals that:\nTherefore, for N \u2032 K+1(s, a, h) > 0:\nN \u2032 K+1 (s, a, h) \u22651 \u22121 \u03b1 \ufffd 1 + 1 \u03b1 \ufffd + 1 \u03b1 \ufffd 1 + 1 \u03b1 \ufffdJ(s,a,h)+1 > 1 \u03b1 \ufffd 1 + 1 \u03b1 \ufffdJ(s,a,h)+1 \u22121 \u03b1 \ufffd 1 + 1 \u03b1 \ufffd\nIf N \u2032 K+1(s, a, h) = 0 it follows we never update due to this state-action-step triple, which means t J(s, a, h) = 0 too. Plugging this into the above expression reveals that:\nThus, for all possible values of the observed visitation counter, we have that:\nUsing the above inequality, we have that\nN \u2032 kj(s, a, h)\nwhere the final line follows from the fact that J \u2264HSA + \ufffd s,a,h J(s, a, h) because we may or may not visit every state-action-step. Rearranging this gives:\nTaking logs of both sides and rearranging one last time gives:\nas required.\nLemma 5. If n0, n1, \u00b7 \u00b7 \u00b7 , nJ are an arbitrary sequence of real-valued numbers satisfying n0 := 0 a 0 \u2264nj \u22641 \u03b1Nj\u22121 with Nj\u22121 = max{1, \ufffdj\u22121 i=0 ni} for all j \u2264J, then\nProof. We prove the claim via induction in a similar manner to Jaksch et al. (2010). First, consider the case where p = 1/2. Suppose\nThen,\n\ufffd because NJ \u22651. The above is our base case and covers us as long as \ufffdJ\u22121 j=1 nj \u22641 e.g., when J = 1 due to n0 := 1. Now, we assume the above holds for \ufffdJ\u22121 j=1 nj > 1:\nFinally, we prove the claim holds for J:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2d22/2d22d4bb-d7a8-43f3-ad95-16f8b7af090c.png\" style=\"width: 50%;\"></div>\n\ufffd where the final inequality follows from the fact that \ufffdJ\u22121 j=1 nj > 1 =\u21d2NJ = nJ + NJ\u22121. All that remain is selecting c. Using the quadratic formula to find the roots of c2 \u22122c \u22121 = 0, one can deduce that selecting\nsatisfies c \u22651 + 1/\u03b1 and\ngiving the required result. All that remains is to prove the claim for p = 1. Similarly to before, suppose:\nbecause NJ \u22651. The above is our base case and covers us as long as \ufffdJ\u22121 j=1 nj \u22641 e.g., when J = 1 due to n0 := 1. Now, we assume the above holds for \ufffdJ\u22121 j=1 nj > 1:\n(As nJ \u2208[0, NJ\u22121/\u03b1) (As \u03b1 \u22651)\n(Pick c : c2 \u22651 + 2c)\nhere the final inequality follows from the fact that nj/Nj\u22121 \u2208[0, 1] for all j \u2264J.\nwhere the final inequality follows from the fact that nj/Nj\u22121 \u2208[0, 1] for all j \u2264J. Lemma 7. Let \u03b1 \u2208[1, \u221e). Then\nLemma 7. Let \u03b1 \u2208[1, \u221e). Then\noof. Trivially, the statement is true for n = 0, because (1 + 1/\u03b1)0 = 1 and (1 + 1/\u03b1)1 \u22121/\u03b1 = 1. Thus,  proceed by induction. Suppose\nroof. Trivially, the statement is true for n = 0, because (1 + 1/\u03b1)0 = 1 and (1 + 1/\u03b1)1 \u22121/\u03b1 = 1. Thus, e proceed by induction. Suppose\nProof. Trivially, the statement is true for n = 0, because (1 + 1/\u03b1)0 = 1 and (1 + 1/\u03b1)1 \u22121/\u03b1 = 1. Thu we proceed by induction. Suppose\nfor some n. Then\n<div style=\"text-align: center;\">Thus, the claim holds for n + 1, which proves the lemma for all n \u22650.</div>\n\ufffd s,a,h J \ufffd j=1 nkj+1 kj+1,h(s, a) N \u2032 kjh (s, a) \u2264 \ufffd 1 + 1 \u03b1 \ufffd HSA + \ufffd 1 + 1 \u03b1 \ufffd HSA log \ufffdK SA \ufffd \u22642 \ufffd 1 + 1 \u03b1 \ufffd HSA \ufffdK SA \ufffd\nwhere the final inequality holds for K/SA \u2265exp(1).\n(Induction Hypothesis)\n\n(Since 2 \u22651 + 1/\u03b1)\nProof. To prove the result, we extend the summation to include the state-action-step triples in episode kj tha did not trigger the update rule:\nfor K/SA \u2265exp(1), as required.\n# A.4 Proof of Regret Bound for Lazy Updating\nTheorem 2. Let K \u2265SA and \u03b1 \u22651. Under Assumption 1 and 2, with probability 1 \u2212\u03b4, the regret of a model-based algorithm under delayed feedback is upper bounded by:\nwhere \u02c6RK(Base) is an upper bound on the regret of the chosen base algorithm under immediate feedback. Proof. Let \u02dc\u2206k h(s) = \u02dcV \u03c0k h (s) \u2212V \u03c0k h (s) denote the difference between the optimistic and actual value of policy \u03c0k from state s and step h. By definition, the regret of any episodic reinforcement learning algorithm is given by:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c21/5c210f9f-9f0d-4622-89e4-a6b617bea394.png\" style=\"width: 50%;\"></div>\n(Lemma 5)\n\n(i) episodes where we perform a policy update, (ii) episodes played in the j-th epoch but observed in epoch j\u2032 > j, (iii) episodes played in the j-th epoch and observed in the j-th epoch.\nFirst, we focus on the episodes where we perform a policy update, e.g. (i). Recall that Lemma 4 tells us the total number of updates is logarithmic in the number of episodes. Further, the rewards are bounded between zero and one, meaning the regret of any episode is at most H. Combining these two results gives a trivial bound on regret of this term: (i) \u2264HJ.\nFinally, we handle the episodes that are played and observed in the same epoch e.g., term (iii). Lemma 2 allows us to make a start on bounding this term:\n(iii)\nThus, bounding (iii) now amounts to finding an upper bounds for (iii.a) and (iii.b). Since kj does not feature in either summation, we know that\n(iii.b)\nfor all (s, a, h) \u2208S \u00d7 A \u00d7 [H] and k\u2032 \u2265kj + 1. By introducing a summation over all the states-actions and steps, we can easily bound (iii.a) via Lemma 8:\nBounding (iii.b) requires some care due to the various forms of B1 e.g., those that remain constant and those that utilise variance reduction techniques. By Lemma 5, it is clear that the summation of the visitation counters no longer depends on the delay. Therefore, we begin by an application of Cauchy-Schwarz (CS) to separate the numerator of the exploration bonus from the summation of the visitation counters:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/770e/770e1b3b-def6-4430-9bbb-e223fb526587.png\" style=\"width: 50%;\"></div>\n(Eq. (12))\n(By Lemma 8)\nThe penultimate line in the above is simply the sum of the bonuses for the chosen base algorithm under immediate feedback scaled by a logarithmic factor, which is introduced by the slower updating. For B1 \u2248B e.g., the upper bound only involves inflating terms inside logarithms, one can upper bound the summation under the square-root by TB2, which is tight up to logarithmic factors. When B1 involves some form of empirical variance term, one can use the techniques outlined by Neu and Pike-Burke (2020); Azar et al. (2017); Fruit et al. (2020) to bound the summation under the square-root by \u2248HT; once again this too is tight up to logarithmic factors. More simply, the epochs form a simulated non-delayed version of the environment for the base algorithm. Therefore, (iii.b) can be replaced with the upper bound of the regret in the non-delayed environment multiplied by the extra logarithmic factors that arise from the slower updating, because the summation of the bonuses are the leading term in the regret bound. Bringing everything together gives:\n# B Additional Theoretical Results\nHere, we present a brief overview of the results that unify model-optimistic and value-optimistic model based episodic reinforcement learning algorithms (Neu and Pike-Burke, 2020). The class of model-optimistic algorithms explicitly define the following failure event for some divergence D( \u02c6Pkh(\u00b7|s, a), Ph(\u00b7|s, a):\n\ufffd \ufffd \ufffd \ufffd which holds across all episodes with probability \u03b4\u2032. Indeed, D must satisfy some conditions. Namely, D must be jointly convex in its arguments so that Pkh (defined below) is convex, and it must be positive homogeneous.6 Outside the failure event, with probability 1 \u2212\u03b4\u2032, the divergence between the empirical and actual transition density of the hth step at the start of the kth episode is therefore, at most: D( \u02c6Pkh(\u00b7|s, a), Ph(\u00b7|s, a)) \u2264\u03f5p kh(s, a). Using \u03f5p kh(s, a) as the maximum divergence allows for the construction of the following plausible set:\n(Lemma 8)\n\u03b2\u2217 kh (s, a) = max \u02dc Ph(\u00b7|s,a)\u2208\u2206 \ufffd\ufffd \u02dcV , \u02dcPh (\u00b7|s, a) \u2212\u02c6Ph (\u00b7|s, a) \ufffd\ufffd \u03b2\u2212 kh (s, a) = max \u02dc Ph(\u00b7|s,a)\u2208\u2206 \ufffd\ufffd \u2212\u02dcV , \u02dcPh (\u00b7|s, a) \u2212\u02c6Ph (\u00b7|s, a) \ufffd\ufffd \u03b2kh (s, a) \u2265max \ufffd \u03b2\u2217 kh (s, a) , \u03b2\u2212 kh (s, a) \ufffd\n\u00b7|\u2208 \ufffd\ufffd \u03b2kh (s, a) \u2265max \ufffd \u03b2\u2217 kh (s, a) , \u03b2\u2212 kh (s, a) \ufffd\n\ufffd \ufffd by introducing a Lagrange multiplier. For a derivation of the bonuses associated with each divergence, we refer the reader to Appendix A.5 of Neu and Pike-Burke (2020).\n# B.1 Missing Proofs for the Regret Decomposition\nIn this subsection, we utilise the fact that all model-based algorithms compute an optimistic value function of the form (4) to derive an adaptable regret decomposition. The decomposition is adaptable in the sense it allows for tighter delay-dependence when the bonuses satisfy a symmetry-like property.\nThroughout, we assume that the model-based algorithm is optimistic with high probability. That is, \u02dcV \u03c0k h (s) V \u2217 h (s) \u2265V \u03c0k h (s) with high probability at least 1 \u2212\u03b4\u2032. Further, C is defined as the event where:\n\ufffd \ufffd where L = log \ufffd S2AH\u03c02/6\u03b4\u2032\ufffd and C indicates whether the bonuses of the algorithm satisfy Equatio Proof. By definition, the regret of any episodic reinforcement learning algorithm is given by:\nwhere\n\ufffd \ufffd which we do by induction. Recall that: \u02dcV \u03c0k H+1 = V \u2217 H+1 = V \u03c0k H+1 = \u20d70. Therefore, the statement holds when j = H, because: \u02dc\u2206k H+1 = 0. Now assume the statement holds for h = j + 1. Then,\n\ufffd Therefore, we are now able to upper bound the regret as follows:\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd Similarly, |\u00af\u03b6k h+1| \u22642 and Esk h+1\u223cPh(\u00b7 | sk h,ak h) \ufffd\u00af\u03b6k h+1 | Fkh \u222a{sk h, ak h} , sk h+1 \u2208Gkh \ufffd = 0. Therefore, \u03b6k h+1 and \u00af\u03b6k h+1 are martingale differences, which are easily bounded using Azuma-Hoeffding:\nTherefore, with probability 1 \u22124\u03b4\u2032:\nas required.\nLemma 9. Let C be an algorithm dependent-constant indicating whether it is model-optimistic or valu optimistic. Under Assumption 1, the regret of any optimistic model-based algorithm from the h-th step of  k-th episode upper bounded by:\n\ufffd \ufffd \uf8f0 \uf8ed where L = log(S2AH\u03c02/6\u03b4\u2032) and\n \u2212 Proof. By Proposition 2 of Neu and Pike-Burke (2020) and by definition of the value-optimistic algorithm we have that: \u02dc\u2206k h(sk h) = \u02dcV \u03c0k h \ufffd sk h \ufffd \u2212V \u03c0k h \ufffd sk h \ufffd = \u03b2+ kh \ufffd sk h, ak h \ufffd + \ufffd\u02c6Pkh \ufffd \u00b7 | sk h, ak h \ufffd\u02dcV \u03c0k h+1 \ufffd \u2212 \ufffd Ph \ufffd \u00b7 | sk h, ak h \ufffd V \u03c0k h+1 \ufffd = \u03b2+ kh \ufffd sk h, ak h \ufffd + \ufffd\u02c6Pkh \ufffd \u00b7 | sk h, ak h \ufffd \u2212Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dcV \u03c0k h+1 \ufffd + \ufffd Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dcV \u03c0k h+1 \u2212V \u03c0k h+1 \ufffd\n(with probability at least 1 \u2212\u03b4\u2032)\n(with probability at least 1 \u2212\u03b4\u2032)\n\n= \u03b2+ kh \ufffd sk h, ak h \ufffd + \ufffd\u02c6Pkh \ufffd \u00b7 | sk h, ak h \ufffd \u2212Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dcV \u03c0k h+1 \ufffd + \ufffd Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dc\u2206k h+1 \ufffd \u2264\u03b2kh \ufffd sk h, ak h \ufffd + \ufffd\u02c6Pkh \ufffd \u00b7 | sk h, ak h \ufffd \u2212Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dcV \u03c0k h+1 \ufffd + \ufffd Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dc\u2206k h+1 \ufffd = \u02dc\u2206k h+1 \ufffd sk h+1 \ufffd + \u03b2kh \ufffd sk h, ak h \ufffd + \ufffd\u02c6Pkh \ufffd \u00b7 | sk h, ak h \ufffd \u2212Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dcV \u03c0k h+1 \ufffd + \ufffd Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dc\u2206k h+1 \ufffd \u2212\u02dc\u2206k h+1 \ufffd sk h+1 \ufffd\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd where the inequality follows from the fact that \u03b2kh(s, a)+ \u2264\u03b2kh(s, a). For model-optimistic algorithms from the definition of the bonuses, we have that:\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd However, this term cannot be bound as easily for the value-optimistic algorithms. But, Assumption 1 allows us to show that, with probability 1 \u2212\u03b4\u2032:\nas required.\nLemma 10. Let \u03b3kh(sk h, ak h) := \u27e8\u02c6Pkh \ufffd \u00b7 | sk h, ak h \ufffd \u2212Ph \ufffd \u00b7 | sk h, ak h \ufffd , \u02dc\u2206k h+1\u27e9. Then, with probability at leas 1 \u2212\u03b4\u2032:\n\n\ufffd \ufffd where L = log(S2AH\u03c02/6\u03b4\u2032) and\nGkh := {s\u2032 : Ph \ufffd s\u2032 | sk h, ak h \ufffd N \u2032 kh \ufffd sk h, ak h \ufffd \u22654H2L}\n\uf8ed \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \uf8f8 By definition, Ph(s\u2032|s, a) < 4H2L/N \u2032 kh(s, a) whenever s\u2032 \u0338\u2208Gkh, which follows simply from rearrang terms in the definition of Gkh. Therefore,\n\ufffd Now, we focus on the s\u2032 \u2208Gkh.\ncompleting the proof.\n# B.2 Missing Theoretical Results for Delayed Rewards\nIn this section, we describe how to use active or lazy updating in the setting where only the rewards return in delay. We assume the rewards are stochastic and their expected values are unknown.\nIn the setting of delayed rewards, the agent returns the state-action pairs {sk h, ak h}H h=1 at the end of episode k, immediately. However, the rewards {rk h}H h=1 return with a random delay \u03c4k. Since it is only the rewards that return in delay, we can estimate the transitions at the start of each episode, as usual. Thus, we apply active or lazy updating to the estimation of the expected reward function only.\n\u02c6rkh (s, a) = 1 N \u2032 kh (s, a) k\u22121 \ufffd i=1 ri h1{si h = s, ai h = a, i + \u03c4i < k}\nFor lazy updating, this amounts to waiting until the observed number of rewards for a state-action-step triple have doubled before starting a new epoch. When estimating the expected reward function for jth epoch, the base algorithm will use all the available rewards:\nUsing Hoeffding\u2019s inequality, one can construct confidence sets around the above estimators and derive another estimator that is optimistic, with high probability. We derive the width of the confidence set in the proof below. Theorem 3. Let RP K denote the regret of UCRL2 from estimating the transition densities under immediate feedback. Then, with probability 1 \u2212\u03b4, the regret of UCRL2 under delayed reward is:\nfor active updating.\nProof. First, since the rewards are stochastic and their expected values are unknown, we must derive an estimator. Naturally, we use only the observed information to compute the expected value, as it is an unbiased estimator:\n\ufffd Now, assume that the rewards are bounded in [0, 1]. Using Hoeffding\u2019s inequality, we can define an addition failure event to account for the fact that we are estimating the expected reward function:\nwhich upper bounds the true expected reward function with probability 1 \u2212\u03b4\u2032 across immediate feedback setting, the failure event for the transition densities is:\nwhere\n<div style=\"text-align: center;\">\u02dcPkh (\u00b7|s, a) \u2208{Q \u2208\u2206: \u2225Q \u2212Ph (\u00b7|s, a)\u2225\u2264\u03f5p kh (s, a)} By optimism, and due to UCRL2 having C = 1: with probability 1 \u22122\u03b4\u2032:</div>\nThe penultimate inequality follows from Lemma 6. Further,\n\ufffd is the regret of the base algorithm (UCRL2) in an immediate feedback environment with know reward fun tions. Now, to prove the statements of the corollary, we must bound the summation of the estimation err for the rewards. Doing so is just a matter of applying Lemma 3:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/345f/345f89e8-21ab-476f-bfaf-7a35779013b8.png\" style=\"width: 50%;\"></div>\n\ufffd \ufffd Substituting the above into Equation (14) and omitting poly-logarithmic factors gives the stated result.\n# C Additional Experimental Results\nHere, we present additional experimental results for the chain environments with H = S \u2208{5, 10, 20} and E[\u03c4] \u2208{100, 300, 500}. In all combinations of chain length and expected delay, our updating procedures give\n(14)\nbetter empirical performance, especially for the delay distributions with higher variances. For all expecte delays, active updating gives the best performance. However, our experiments indicate that lazy updatin with \u03b1 = 10 or 100 is comparable, as one would expect based on the intuition that it is an approximation t active updating that converges in the limit as \u03b1 \u2192\u221e.\n# C.1 Chain Environment with H = S = 5\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c5c7/c5c77419-80ea-4edf-b719-f9f7b74e669c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Cumulative Regret (S = 5, E[\u03c4] = 100).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca60/ca6079ed-3480-4bdc-96d6-9a25ee68cc37.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4ce/b4ce471c-227f-400f-a3d1-d23975fc1121.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Cumulative Regret (S = 5, E[\u03c4] = 300).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c202/c2020cb8-d7aa-4e03-ab70-196aa5cd6915.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Cumulative Regret (S = 5, E[\u03c4] = 500).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99ef/99ef5e74-3b5e-4ec1-8050-e54d78a308b6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Delay Dependence (S = 5)</div>\n<div style=\"text-align: center;\">C.2 Chain Environment with H = S = 10</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1b6/b1b6a89f-38d8-4817-b678-6d5b3353cda9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Cumulative Regret (S = 10, E[\u03c4] = 100).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a4b5/a4b5df25-6b76-4fb6-beda-b2fc6c76116a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Cumulative Regret (S = 10, E[\u03c4] = 300).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/42b1/42b12e52-6fd2-4ad4-a5d4-3296ed188d03.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Cumulative Regret (S = 10, E[\u03c4] = 500).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0059/0059b85a-f3d8-4bc1-b2a1-6d3770e162e4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Delay Dependence (S = 10)</div>\n# C.3 Chain Environment with H = S = 20\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e6a0/e6a01b20-a2d7-405a-8bf3-ab5c17a1a7d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Cumulative Regret (S = 20, E[\u03c4] = 100).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14f8/14f852bd-ce5f-4eab-9ac5-bd86480154ae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Cumulative Regret (S = 20, E[\u03c4] = 300).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0299/0299d282-4b2e-4e9e-8dba-2cceb260e671.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Cumulative Regret (S = 20, E[\u03c4] = 500).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/019a/019a9342-b61f-44fb-a81c-08ec85c80851.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Delay Dependence (S = 20)</div>\n# C.4 Chain Environment with H = S = 30\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b30b/b30b9363-f0cd-4716-a0e7-ad9a0903dcb6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: Cumulative Regret (S = 30, E[\u03c4] = 300).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a8ed/a8ed65aa-48ba-4852-a4d1-e06b182e95ab.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: Cumulative Regret (S = 30, E[\u03c4] = 500).</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of delayed feedback in episodic reinforcement learning, highlighting the limitations of existing methods that assume immediate feedback, which is often not the case in practical applications such as healthcare and e-commerce. The authors aim to fill the theoretical gap in understanding the impact of delayed feedback and propose new methods to handle it effectively.",
        "problem": {
            "definition": "The problem is the lack of theoretical understanding regarding the impact of delayed feedback in episodic reinforcement learning, which affects the algorithm's ability to update its policy based on past interactions with the environment.",
            "key obstacle": "The core obstacle is that existing algorithms are designed under the assumption of immediate feedback, which is unrealistic in many real-world scenarios, leading to potential inefficiencies and increased regret."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by the observation that algorithms must continue operating without immediate feedback, thus necessitating a method to effectively manage delayed information.",
            "opinion": "The authors propose two meta-algorithms, active updating and lazy updating, to handle delayed feedback in a way that maintains the performance of existing algorithms.",
            "innovation": "The primary innovation is the introduction of these two approaches that allow for policy updates either immediately upon receiving feedback or after a certain threshold of feedback has been reached, improving the theoretical performance of optimistic algorithms under delayed feedback."
        },
        "method": {
            "method name": "Active Updating and Lazy Updating",
            "method abbreviation": "AU and LU",
            "method definition": "Active updating immediately incorporates new feedback into the policy, while lazy updating waits until sufficient feedback is accumulated before updating.",
            "method description": "Both methods aim to enhance the performance of existing episodic reinforcement learning algorithms in the presence of delayed feedback.",
            "method steps": [
                "For active updating, update the policy as soon as feedback is available.",
                "For lazy updating, wait until the observed visitation counter reaches a specified threshold before updating the policy."
            ],
            "principle": "These methods are effective because they allow the algorithm to better manage the uncertainty introduced by delayed feedback, ensuring that the learning process remains efficient despite the lack of immediate information."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in a chain environment with various configurations of states and expected delays, comparing the performance of the proposed methods against existing algorithms that require prior knowledge of the maximum delay.",
            "evaluation method": "The performance was assessed by measuring cumulative regret over multiple episodes and analyzing the impact of different delay distributions on the regret of the algorithms."
        },
        "conclusion": "The paper concludes that both active and lazy updating methods successfully extend existing episodic reinforcement learning algorithms to handle delayed feedback, maintaining sub-linear regret bounds and demonstrating good empirical performance across various delay distributions.",
        "discussion": {
            "advantage": "The key advantages of the proposed methods include their ability to effectively manage delayed feedback without requiring prior knowledge of maximum delays, leading to improved theoretical guarantees and empirical performance.",
            "limitation": "A limitation is that the performance of the methods can still be influenced by the choice of hyperparameters, such as the activity parameter in lazy updating, which may require careful tuning.",
            "future work": "Future research could explore the application of these methods to a broader class of reinforcement learning algorithms and investigate the impact of different types of delay distributions."
        },
        "other info": {
            "info1": "The proposed methods are applicable to both model-based and model-free algorithms.",
            "info2": {
                "info2.1": "The results indicate a trade-off between the frequency of updates and the associated regret, with potential for further optimization.",
                "info2.2": "The findings suggest that lazy updating can achieve near-optimal performance with a modest choice of the activity parameter."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1",
            "key information": "This paper addresses the issue of delayed feedback in episodic reinforcement learning, highlighting the limitations of existing methods that assume immediate feedback, which is often not the case in practical applications such as healthcare and e-commerce."
        },
        {
            "section number": "2",
            "key information": "The problem is the lack of theoretical understanding regarding the impact of delayed feedback in episodic reinforcement learning, which affects the algorithm's ability to update its policy based on past interactions with the environment."
        },
        {
            "section number": "3.1",
            "key information": "The core obstacle is that existing algorithms are designed under the assumption of immediate feedback, which is unrealistic in many real-world scenarios, leading to potential inefficiencies and increased regret."
        },
        {
            "section number": "4.1",
            "key information": "The proposed idea is inspired by the observation that algorithms must continue operating without immediate feedback, thus necessitating a method to effectively manage delayed information."
        },
        {
            "section number": "5.1",
            "key information": "Active updating immediately incorporates new feedback into the policy, while lazy updating waits until sufficient feedback is accumulated before updating."
        },
        {
            "section number": "6.1",
            "key information": "The primary innovation is the introduction of two approaches, active updating and lazy updating, that allow for policy updates either immediately upon receiving feedback or after a certain threshold of feedback has been reached."
        },
        {
            "section number": "7.2",
            "key information": "The key advantages of the proposed methods include their ability to effectively manage delayed feedback without requiring prior knowledge of maximum delays, leading to improved theoretical guarantees and empirical performance."
        },
        {
            "section number": "8.1",
            "key information": "Future research could explore the application of these methods to a broader class of reinforcement learning algorithms and investigate the impact of different types of delay distributions."
        }
    ],
    "similarity_score": 0.5298912861329503,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Optimism and Delays in Episodic Reinforcement Learning.json"
}