{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2008.02396",
    "title": "Learning Illumination from Diverse Portraits",
    "abstract": "We present a learning-based technique for estimating high dynamic range (HDR), omnidirectional illumination from a single low dynamic range (LDR) portrait image captured under arbitrary indoor or outdoor lighting conditions. We train our model using portrait photos paired with their ground truth environmental illumination. We generate a rich set of such photos by using a light stage to record the reflectance field and alpha matte of 70 diverse subjects in various expressions. We then relight the subjects using image-based relighting with a database of one million HDR lighting environments, compositing the relit subjects onto paired high-resolution background imagery recorded during the lighting acquisition. We train the lighting estimation model using rendering-based loss functions and add a multi-scale adversarial loss to estimate plausible high frequency lighting detail. We show that our technique outperforms the state-of-the-art technique for portrait-based lighting estimation, and we also show that our method reliably handles the inherent ambiguity between overall lighting strength and surface albedo, recovering a similar scale of illumination for subjects with diverse skin tones. We demonstrate that our method allows virtual objects and digital characters to be added to a portrait photograph with consistent illumination. Our lighting inference runs in real-time on a smartphone, enabling realistic rendering and compositing of virtual objects into live video for augmented reality applications.",
    "bib_name": "legendre2020learningilluminationdiverseportraits",
    "md_text": "# Learning Illumination from Diverse Portraits\nCHLOE LEGENDRE, Google Research WAN-CHUN MA, ROHIT PANDEY, SEAN FANELLO, CHRISTOPH RHEMANN, JASON DOURGARIAN, and JAY BUSCH, Google PAUL DEBEVEC, Google Research\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1d0/e1d0684e-5487-4af5-88a4-15e19a6c69fa.png\" style=\"width: 50%;\"></div>\nFig. 1. Our network estimates HDR omnidirectional lighting from an LDR portrait image. (a) Input portrait image generated using a photographed reflectance basis. (b) Ground truth and estimated lighting, shown on difuse, glossy, and mirror spheres. (c) Original and novel subjects lit consistently by the estimated lighting, using image-based relighting. (d) The novel subject lit with the original subject\u2019s ground truth lighting. For both subjects, the appearance under the estimated lighting closely matches the appearance under the original lighting.\n<div style=\"text-align: center;\">Fig. 1. Our network estimates HDR omnidirectional lighting from an LDR portrait image. (a) Input portrait image generated using a photographed reflec basis. (b) Ground truth and estimated lighting, shown on difuse, glossy, and mirror spheres. (c) Original and novel subjects lit consistently by the estim lighting, using image-based relighting. (d) The novel subject lit with the original subject\u2019s ground truth lighting. For both subjects, the appearance unde estimated lighting closely matches the appearance under the original lighting.</div>\nWe present a learning-based technique for estimating high dynamic range (HDR), omnidirectional illumination from a single low dynamic range (LDR) portrait image captured under arbitrary indoor or outdoor lighting conditions. We train our model using portrait photos paired with their ground truth environmental illumination. We generate a rich set of such photos by using a light stage to record the refectance feld and alpha mate of 70 diverse subjects in various expressions. We then relight the subjects using image-based relighting with a database of one million HDR lighting environments, compositing the relit subjects onto paired high-resolution background imagery recorded during the lighting acquisition. We train the lighting estimation model using rendering-based loss functions and add a multi-scale adversarial loss to estimate plausible high frequency lighting detail. We show that our technique outperforms the state-of-the-art technique for portrait-based lighting estimation, and we also show that our method reliably handles the inherent ambiguity between overall lighting strength and surface albedo, recovering a similar scale of illumination for subjects with diverse skin tones. We demonstrate that our method allows virtual objects and digital characters to be added to a portrait photograph with consistent illumination. Our lighting inference runs in real-time on a smartphone, enabling realistic rendering and compositing of virtual objects into live video for augmented reality applications.\narXiv:2008.02396v1\n# INTRODUCTION\nIn both portrait photography and flm production, lighting greatly infuences the look and feel of a given shot. Photographers and cinematographers dramatically light their subjects to communicate a particular aesthetic sensibility and emotional tone. While flms using visual efects techniques ofen blend recorded camera footage with computer-generated, rendered content, the realism of such composites depends on the consistency between the real-world lighting and that used to render the virtual content. Tus, visual efects practitioners work painstakingly to capture and reproduce real-world illumination inside virtual sets. Debevec (1998) introduced one\nsuch technique for real-world lighting capture, recording the color and intensity of omnidirectional illumination by photographing a mirror sphere using multiple exposures. Tis produced an HDR \u201dimage-based lighting\u201d (IBL) environment (Debevec 2006), used for realistically rendering virtual content into real-world photographs. Augmented reality (AR) shares with post-production visual efects the goal of realistically blending virtual content and real-world imagery. Face-based AR applications are ubiquitous, with widespread adoption in both social media and video conferencing applications. However, in real-time AR, lighting measurements from specialized capture hardware are unavailable, as acquisition is impractical for casual mobile phone or headset users. Similarly, in visual efects, on-set lighting measurements are not always available, yet lighting artists must still reason about illumination using cues in the scene. If the footage includes faces, their task is somewhat less challenging, as faces include a diversity of surface normals and refect light somewhat predictably. Prior work has leveraged the strong geometry and refectance priors from faces to solve for lighting from portraits. In the years since Marschner and Greenberg (1997) introduced portrait \u201dinverse lighting,\u201d most such techniques (Egger et al. 2018; KemelmacherShlizerman and Basri 2010; Knorr and Kurz 2014; Sengupta et al. 2018; Shim 2012; Shu et al. 2017b; Tewari et al. 2018; Tewari et al. 2017; Zhou et al. 2018) have sought to recover both facial geometry and a low frequency approximation of distant scene lighting, usually represented using up to a 2nd order spherical harmonic (SH) basis. Te justifcation for this approximation is that skin refectance is predominantly difuse (Lambertian) and thus acts as a low-pass flter on the incident illumination. For difuse materials, irradiance indeed lies very close to a 9D subspace well-represented by this basis (Basri and Jacobs 2003; Ramamoorthi and Hanrahan 2001a).\nHowever, to the skilled portrait observer, the lighting at capturetime reveals itself not only through the skin\u2019s difuse refection, but also through the directions and extent of cast shadows and the intensity and locations of specular highlights. Inspired by these cues, we train a neural network to perform inverse lighting from portraits, estimating omnidirectional HDR illumination without assuming any specifc skin refectance model. Our technique yields higher frequency lighting that can be used to convincingly render novel subjects into real-world portraits, with applications in both visual efects and AR when of-line lighting measurements are unavailable. Furthermore, our lighting inference runs in real-time on a smartphone, enabling such applications.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4952/495292b4-1dd9-4e09-86a8-fa075031e87e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. We train a convolutional neural network to regress from a facecropped input image to omnidirectional, HDR illumination.</div>\nWe train our lighting estimation model in a supervised manner using a dataset of portraits and their corresponding ground truth illumination. To generate this dataset, we photograph 70 diverse subjects in a light stage system as illuminated by 331 directional light sources forming a basis on a sphere, such that the captured subject can be relit to appear as they would in any scene with imagebased relighting (Debevec et al. 2000). Although a few databases of real-world lighting environments captured using traditional HDR panoramic photography techniques are publicly available, e.g. the Laval indoor and outdoor datasets with 2,000 and 12,000 scenes respectively (Gardner et al. 2017; Lalonde and Mathews 2014), we extend the LDR data collection technique of LeGendre et al. (2019) to instead capture on the order of 1 million indoor and outdoor lighting environments, promoting them to HDR via a novel non-negative least squares solver formulation before using them for relighting. A few recent works have similarly sought to recover illumination from portraits without relying on a low-frequency lighting basis, including the deep learning methods of Sun et al. (2019) for arbitrary scenes and Calian et al. (2018) for outdoor scenes containing the sun. We show that our method out-performs both of these methods, and generalizes to arbitrary indoor or outdoor scenes. Any atempt at lighting estimation is complicated by the inherent ambiguity between surface refectance (albedo) and light source strength (Belhumeur et al. 1999). Stated otherwise, a pixel\u2019s shading is rendered unchanged if its albedo is halved while light source intensity doubles. Statistical priors for facial albedo have been leveraged to resolve this ambiguity (Calian et al. 2018; Egger et al. 2018; Tewari et al. 2017), but, to the best of our knowledge, we are the frst to explicitly evaluate the performance of our model on a wide variety of subjects with diferent skin tones. In contrast, Sun et al. (2019) report lighting accuracy with a scale-invariant metric, while Calian et al. (2018) show visual results for synthetically rendered and photographed faces where the subjects are predominantly light\nin skin tone. We show that for a given lighting condition, our model can recover lighting at a similar scale for a variety of diverse subjects. In summary, our contributions are the following:\n\u2022 A deep learning method to estimate HDR illumination from LDR images of faces in both indoor and outdoor scenes. Our technique outperforms the previous state-of-the-art. \u2022 A frst-of-its-kind analysis that shows that our HDR lighting estimation technique reliably handles the ambiguity between light source strength and surface albedo, recovering similar illumination for subjects with diverse skin tones.\n# 2 RELATED WORK\nIn this section we summarize work related to lighting capture, inverse rendering from faces, and the related topics of portrait relighting and unconstrained lighting estimation.\nLighting measurement techniques. Afer Debevec (1998) introduced image-based lighting from high dynamic range panoramas, subsequent work proposed more general acquisition techniques including recording the extreme dynamic range of sunny daylight with a fsheye lens (Stumpfel et al. 2004) and recording HDR video with a mirror sphere (Unger et al. 2006; Waese and Debevec 2002). Debevec et al. (2012) and Reinhard et al. (2010) presented more practical techniques to recover the full dynamic range of daylight by augmenting the typical mirror sphere capture with simultaneous photography of a difuse, gray sphere that allowed for saturated light source intensity recovery. We extend these techniques to promote one million real-world, clipped panoramas to HDR.\nInverse Rendering. Te joint recovery of scene geometry, material refectance, and illumination given only an image, thereby inverting the image formation or rendering process, is a long-studied problem in computer vision (Lombardi and Nishino 2016; Ramamoorthi and Hanrahan 2001b; Yu et al. 1999). Similarly, the topic of \u201dintrinsic image\u201d decomposition has received considerable atention, recovering shading and refectance, rather than geometry and illumination (Barrow et al. 1978; Land and McCann 1971). \u201dShape from Shading\u201d methods aim to recover geometry under known illumination (Horn 1970), while another variant jointly recovers \u201dShape, Illumination, and Refectance from Shading\u201d (Barron and Malik 2014). Recently, signifcant progress has been made in the domain of inverse rendering from portrait images or videos, with the goal of recovering a 3D face model with illumination and/or refectance (Egger et al. 2018; Kemelmacher-Shlizerman and Basri 2010; Sengupta et al. 2018; Tewari et al. 2018; Tewari et al. 2017; Tran et al. 2019; Tran and Liu 2019; Yamaguchi et al. 2018). Many of these techniques rely on geometry estimation via fting or learning a 3D morphable model (Blanz and Veter 1999), and they model skin refectance as Lambertian and scene illumination using a low-frequency 2nd order SH basis. In contrast, our goal is to recover higher frequency illumination useful for rendering virtual objects with diverse refectance characteristics beyond Lambertian.\nInverse Lighting from Faces. Marschner and Greenberg (1997) introduced the problem of \u201dinverse lighting,\u201d estimating the directional distribution and intensity of incident illumination falling on a rigid object with measured geometry and refectance, demonstrating lighting estimation from portraits as one such example. With the appropriate lighting basis and refectance assumption, the problem was reduced to inverting a linear system of equations. Te linearity of light transport was similarly leveraged in follow-up work to estimate lighting from faces (Shahlaei and Blanz 2015; Shim 2012), including for real-time AR (Knorr and Kurz 2014), but these approaches estimated either a small number of point light sources or again used a low frequency 2nd order SH lighting basis. Specular refections from the eyes of portrait subjects have been leveraged to estimate higher frequency illumination, but as the refections of bright light sources are likely to be clipped, the recovery of the full dynamic range of natural illumination is challenging to recover from a single exposure image (Nishino and Nayar 2004). Several new deep learning techniques for inverse lighting from faces have been proposed. Zhou et al. (2018) estimated 2nd order SH illumination from portraits. For higher frequency lighting estimates, Yi et al. (2018) recovered illumination by frst estimating specular highlights and ray-tracing them into a panorama of lighting directions. However, this model produced HDR IBL maps that are mostly empty (black), with only dominant light source colors and intensities represented. In contrast, we estimate plausible omnidirectional illumination. Calian et al. (2018) trained an autoencoder on a large database of outdoor panoramas to estimate lighting from LDR portraits captured outdoors, combining classical inverse lighting and deep learning. While this method produced impressive results for outdoor scenes with natural illumination, it is not applicable to indoor scenes or outdoor scenes containing other sources of illumination. Our model, in contrast, generalizes to arbitrary setings. Critically, neither Yi et al. (2018) nor Calian et al. (2018) evaluated model performance on subjects with diverse skin tones, which we feel is an important variation axis for lighting estimation error analysis. Both works presented qualitative results only for photographed subjects and rendered computer-generated models with fair skin.\nPortrait Relighting. Marchner and Greenberg (1997) also proposed portrait relighting and portrait lighting transfer, showing that the lighting from one portrait subject could be used to approximately relight another subject, such that the two could be convincingly composited together into one photograph. Recent works solved this problem either with a mass transport (Shu et al. 2017a) or deep learning (Sun et al. 2019; Zhou et al. 2019) approach. Sun et al. (2019) estimated illumination while training a portrait relighting network. Lighting estimates from this technique proved superior compared with two other state-of-the-art methods (Barron and Malik 2014; Sengupta et al. 2018). Similarly to Sun et al. (2019), we generate photo-realistic, synthetic training data using a set of refectance basis images captured in an omnidirectional lighting system, or light stage, relying on the technique of image-based relighting (Debevec et al. 2000; Nimerof et al. 1995) to synthesize portraits lit by novel sources of illumination. However, in contrast to Sun et al. (2019), we extend a recent environmental lighting\ncapture technique (LeGendre et al. 2019) to expand the number of lighting environments used for training data, employ a set of loss functions designed specifcally for lighting estimation, and use a lightweight model to achieve lighting inference at real-time frame rates on a mobile device. Even when trained on the same dataset, we show that our lighting estimation model outperforms that of Sun et al. (2019), the previous state-of-the-art for lighting estimation from portraits.\nLighting Estimation. Given the prominence of virtual object compositing in both visual efects and AR, it is unsurprising that lighting estimation from arbitrary scenes (not from portraits) is also an active research area. Several works have sought to recover outdoor, natural illumination from an unconstrained input image (Hold-Geofroy et al. 2019, 2017; Lalonde et al. 2009; Lalonde and Mathews 2014; Zhang et al. 2019). Several deep learning based methods have recently tackled indoor lighting estimation from unconstrained images (Gardner et al. 2017; Garon et al. 2019; Song and Funkhouser 2019). Cheng et al. (2018) estimated lighting using a deep learning technique given two opposing views of a panorama. For AR applications, LeGendre at al. (2019) captured millions of LDR images of three difuse, glossy, and mirror reference spheres as they appeared in arbitrary indoor and outdoor scenes, using this dataset to train a model to regress to omnidirectional HDR lighting from an unconstrained image. We leverage this lighting data collection technique but extend it to explicitly promote the captured data to HDR so that it can be used for image-based relighting, required for generating our synthetic portraits. LeGendre et al. (2019) trained their model using a combination of rendering-based and adversarial losses, which we extend to the multi-scale domain for superior performance.\n# 3 METHOD\n# 3.1 Training Data Acquisition and Processing\nTo train a model to estimate lighting from portrait photographs in a supervised manner, we require many portraits labeled with ground truth illumination. Since no such real-world, dataset exists, we synthesize portraits using the data-driven technique of image-based relighting, shown by Debevec et al. (2000) to produce photo-realistic relighting results for human faces, appropriately capturing complex light transport phenomena for human skin and hair e.g. sub-surface and asperity scatering and Fresnel refections. Noting the difculty of generating labeled imagery for the problem of inverse lighting from faces, many prior works have instead relied on renderings of 3D models of faces (Calian et al. 2018; Yi et al. 2018; Zhou et al. 2018), which ofen fail to represent these complex phenomena.\nRefectance Field Capture. Debevec et al. (2000) introduced the 4D refectance feld R(\u03b8,\u03d5,x,y) to denote a subject lit from any lighting direction (\u03b8,\u03d5) for image pixels (x,y) and showed that taking the dot product of this refectance feld with an HDR lighting environment similarly parameterized by (\u03b8,\u03d5) relights the subject to appear as they would in that scene. To photograph a subject\u2019s refectance feld, we use a computer-controllable sphere of 331 white LED light sources, similar to that of Sun et al. (2019), with lights spaced 12\u25e6 apart at the equator. Te refectance feld is formed from a set of refectance basis images (see Fig. 3), photographing the subject as\neach of the directional LED light sources is individually turned on one-at-a-time within the spherical rig. We capture these \u201dOne-LightAt-a-Time\u201d (OLAT) images for multiple camera viewpoints, shown in Fig. 4. In total we capture 331 OLAT images for each subject using six color Ximea machine vision cameras with 12 megapixel resolution, placed 1.7 meters from the subject. Te cameras are positioned roughly in front of the subject, with fve cameras with 35 mm lenses capturing the upper body of the subject from diferent angles, and one additional camera with a 50 mm lens capturing a close-up image of the face with tighter framing.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3355/335554ea-4d7e-4e09-b041-5b93046473ea.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3. \u201dOne-Light-at-a-Time\u201d images: 24 of the 331 lighting directions</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ccc8/ccc868c2-86e7-4a64-8380-da3fda6f88d6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4. Our six camera viewpoints for an example lighting direction.</div>\nWe capture refectance felds for 70 diverse subjects, each performing nine diferent facial expressions and wearing diferent accessories, yielding about 630 sets of OLAT sequences from six diferent camera viewpoints, for a total of 3780 unique OLAT sequences. In addition to age and gender diversity, we were careful to photograph subjects spanning a wide range of skin tones, as seen in Fig. 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb3a/bb3a93b5-771c-412c-a35f-ea08cb18d071.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. Representative portraits of the 70 recorded subjects.</div>\nSince acquiring a full OLAT sequence for a subject takes six seconds, there can be subject motion over the sequence. We therefore employ an optical fow technique (Anderson et al. 2016) to align the images, interspersing at every 11th OLAT frame one extra \u201dtracking\u201d frame with even, consistent illumination to ensure the brightness constancy constraint for optical fow is met, as in Wenger et al. (2005). Tis step preserves the sharpness of image features when performing the relighting operation, which linearly combines aligned OLAT images.\nHDR Lighting Environment Capture. To relight our subjects with photographed refectance felds, we require a large database of HDR lighting environments, where no light sources are clipped. While there are a few such datasets containing on the order of thousands of indoor panoramas (Gardner et al. 2017) or the upper hemisphere of outdoor panoramas (Lalonde and Mathews 2014), deep learning models are typically enhanced with a greater volume of training data. Tus, we extend the video-rate capture technique of LeGendre et al. (2019) to collect on the order of 1 million indoor and outdoor lighting environments. Tis work captured background images augmented by a set of difuse, mate silver, and mirrored reference spheres held in the lower part of the frame as in Fig. 6. Tese three spheres reveal diferent cues about the scene illumination. Te mirror ball refects omnidirectional high frequency lighting, but bright light sources will be clipped, altering both their intensity and color. Te near-Lambertian BRDF of the difuse ball, in contrast, acts as a low-pass flter on the incident illumination, capturing a blurred but relatively complete record of total scene irradiance. Without explicitly promoting these LDR sphere appearances to a record of HDR environmental illumination, LeGendre et al. (2019) regressed from the unconstrained background images to HDR lighting using an in-network, diferentiable rendering step, predicting illumination to match the clipped, LDR ground truth sphere appearances. In contrast, we require a true HDR record of the scene illumination to use for relighting our subjects, so, unlike LeGendre et al. (2019), we must explicitly promote the three sphere appearances into an estimate of their corresponding HDR lighting environment. Promoting LDR Sphere Images to HDR Lighting. Given captured im-\nPromoting LDR Sphere Images to HDR Lighting. Given captured images of the three refective spheres, perhaps with clipped pixels, we\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/51d4/51d47a3e-0165-44bb-b55d-74c281a7be73.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6. Background images with ground truth lighting recorded by difuse, mate silver, and mirrored spheres as in LeGendre et al. (2019).</div>\nwish to solve for HDR lighting that could have plausibly produced these three sphere appearances. We frst record the refectance feld for the difuse and mate silver spheres, again using the light stage system. We convert their refectance basis images into the same relative radiometric space, normalizing based on the incident light source color. We then project the refectance basis images into the mirror ball mapping (Reinhard et al. 2010) (Lambert azimuthal equal-area projection), accumulating energy from the input images for each new lighting direction (\u03b8,\u03d5) on a 32 \u00d7 32 image of a mirror sphere as in LeGendre et al. (2019), forming the refectance feld R(\u03b8,\u03d5,x,y), or, sliced into individual pixels, Rx,y(\u03b8,\u03d5). For lighting directions (\u03b8,\u03d5) in the captured mirror ball image without clipping for color channel c, we recover the scene lighting Lc(\u03b8,\u03d5) by simply scaling the mirror ball image pixel values by the inverse of the measured mirror ball refectivity (82.7%). For lighting directions (\u03b8,\u03d5) with clipped pixels in the original mirror ball image, we set the pixel values to 1.0, scale this by the inverse of the measured refectivity forming Lc(\u03b8,\u03d5), and then subsequently solve for a residual missing lighting intensity Uc(\u03b8,\u03d5) using a non-negative least squares solver formulation. Given an original image pixel value px,y,c,k for BRDF index k (e.g. difuse or mate silver), and color channel c, and the measured refectance feld Rx,y,c,k(\u03b8,\u03d5), due to the superposition principle of light, we can write: \ufffd\n(1)\n\ufffd Tis generates a set of m linear equations for each BRDF k and color channel c, equal to the number of sphere pixels in the refectance basis images, with n unknown residual light intensities. For lighting directions without clipping, we know that Uc(\u03b8,\u03d5) = 0. For each color channel, with km > n, we can solve for the unknown Uc(\u03b8,\u03d5) values using non-negative least squares, ensuring light is only added, not removed. In practice, we exclude clipped pixelspx,y,c,k from the solve. Prior methods have recovered clipped light source intensities by comparing the pixel values from a photographed difuse sphere with the difuse convolution of a clipped panorama (Debevec et al. 2012; Reinhard et al. 2010), but, to the best of our knowledge, we are the frst to use photographed refectance bases and multiple BRDFs. In Fig. 7 upper rows, we show input sphere images extracted from LDR imagery (\u201dground truth\u201d), and in lower rows, we show the three spheres rendered using Eqn. 1, lit with the HDR illumination recovered from the solver. We observed when solving for Uc(\u03b8,\u03d5) treating each color channel independently, brightly-hued red, green, and blue light sources were produced, ofen at geometrically-nearby lighting directions,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/00cc/00ccb6ca-842b-4bd2-8a3d-5fecef3a8347.png\" style=\"width: 50%;\"></div>\nFig. 7. Upper: ground truth LDR sphere images (inputs to the LDR to HDR linear solver). Lower: spheres rendered using the recovered HDR illumination, using image-based relighting and the captured reflectance basis.\nrather than a single light source with greater intensity in all three colors channels. To recover results with more plausible, neutrallycolored light sources, we add a cross color channel regularization based on the insight that the color of the photographed difuse grey ball reveals the average color balance (Rav\u0434,Gav\u0434, Bav\u0434) of the bright light sources in the scene. We thus add to our system of equations a new set of linear equations with weight \u03bb = 0.5:\n[()()] Tese regularization terms penalize the recovery of strongly-hued light sources of a diferent color balance than the target difuse ball. Debevec et al. (2012) noted that a regularization term could be added to encourage similar intensities for geometrically-nearby lighting directions, but this would not necessarily prevent the recovery of strongly-hued lights. We recover Uc(\u03b8,\u03d5) using the Ceres solver (Agarwal et al. [n. d.]), promoting our 1 million captured sphere appearances to HDR illumination. As the LDR images from this video-rate data collection method are 8-bit and encoded as sRGB, possibly with local tone-mapping, we frst linearize the sphere images assuming \u03b3 = 2.2, as required for our linear system formulation. Portrait Synthesis. Using our photographed refectance felds for each subject and our HDR-promoted lighting, we generate relit portraits with ground truth illumination to serve as training data. We again convert the refectance basis images into the same relative radiometric space, calibrating based on the incident light source color. As our lighting environments are represented as 32\u00d732 mirror ball images, we project the refectance felds onto this basis, again accumulating energy from the input images for each new lighting direction (\u03b8,\u03d5) as in LeGendre et al. (2019). Each new basis image is a linear combination of the original 331 OLAT images. Te lighting capture technique also yields a high-resolution background image corresponding to the three sphere appearances. Since such images on their own contain useful cues for extracting lighting estimates (Gardner et al. 2017; Hold-Geofroy et al. 2017), we composite our relit subjects onto the these backgrounds rather than onto a black frame as in Sun et al. (2019), as shown in Fig. 8, producing images which mostly appear to be natural photographs taken out in the wild. Since the background images are 8-bit sRGB, we clip and\napply this transfer function to the relit subject images prior to compositing. As in-the-wild portraits are likely to contain clipped pixels (especially for 8-bit live video for mobile AR), we discard HDR data for our relit subjects to match the expected inference-time inputs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b062/b0624296-0b1b-4063-bba2-389f071d613d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) background (b) alpha mate (c) relit subject (d) composited</div>\n<div style=\"text-align: center;\">Fig. 8. (a) A background with paired HDR illumination, shown via the inset spheres (upper right). (b) Alpha mate from our system. (c) Subject relit with the illumination from a. (d) Subject relit and composited into a.</div>\nFace Localization. Although background imagery may provide contextual cues that aid in lighting estimation, we do not wish to waste our network\u2019s capacity learning a face detector. Instead, we compute a face bounding box for each input, and during training and inference we crop each image, expanding the bounding box by 25%. During training we add slight crop region variations, randomly changing their position and extent. In our implementation, we use the BlazeFace detector of Bazarevsky et al. (2019), but any could be used. In Fig. 9 we show example cropped inputs to our model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45af/45af4892-053e-4c9a-bef7-92e967cb8e8b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9. Example synthetic training portraits, cropped to the bounding box of the detected face. Upper right corners: ground truth HDR illumination for each training example (not included as input during training).</div>\n# 3.2 Network Architecture\nTe input to our model is an sRGB encoded LDR image, with the crop of the detected face region of each image resized to an input resolution of 256 \u00d7 256 and normalized to the range of [\u22120.5, 0.5]. We use an encoder-decoder architecture with a latent vector of size 1024 at the botleneck, representing log-space HDR illumination, as the sun can be several orders of magnitude brighter than the sky (Stumpfel et al. 2004). Te encoder consists of fve 3 \u00d7 3 convolutions each followed by a blur-pooling operation (Zhang 2019), with successive flter depths of 16, 32, 64, 128, and 256, followed by one last convolution with a flter size of 8 \u00d7 8 and depth 256, and fnally a fully-connected layer. Te decoder consists of three sets of 3 \u00d7 3 convolutions of flter depths 64, 32, and 16, each followed by a bilinear-upsampling operation. Te fnal output of the network is a 32 \u00d7 32 HDR image of a mirror ball representing log-space omnidirectional illumination. We also use an auxiliary discriminator architecture to add an adversarial loss term, enforcing estimation of plausible high frequency illumination (see Sec. 3.3). Tis network takes as input clipped images of ground truth and predicted illumination from the main model, and tries to discriminate between the real and generated examples. Te discriminator encoder consists of three 3 \u00d7 3 convolutions each followed by a max-pooling operation, with successive flter depths of 64, 128, and 256, followed by a fully connected layer of size 1024 before the fnal output layer. As our main network\u2019s decoder includes several upsampling operations, our network is implicitly learning information at multiple scales. We leverage this multi-scale output to provide inputs to the discriminator not just of the full-resolution 32 \u00d7 32 clipped lighting image, but also of a lighting image at each scale: 4\u00d74, 8\u00d78, and 16\u00d716, using the multi-scale gradient technique of MSG-GAN (Karnewar and Wang 2020). As the lower-resolution feature maps produced by our generator network have more than 3 channels, we add a convolution operation at each scale as extra branches of the network, producing multiple scales of 3-channel lighting images to supply to the discriminator.\n# 3.3 Loss Function\nMulti-scale Image-Based Relighting Rendering Loss. LeGendre et al. (2019) describe a diferentiable image-based relighting rendering loss, used for training a network to estimate HDR lighting \u02c6L from an unconstrained image. Tis approach minimizes the reconstruction loss between the ground truth sphere images I for multiple BRDFs and the corresponding network-rendered spheres \u02c6I, lit with the predicted illumination. We use this technique to train our model for inverse lighting from portraits, relying on these sphere renderings to learn illumination useful for rendering virtual objects of a variety of BRDFs. We produce sphere renderings \u02c6I in-network using imagebased relighting and photographed refectance felds for each sphere of BRDF index k (mirror, mate silver, or difuse), and color channel c, with \u02c6Lc(\u03b8,\u03d5) as the intensity of light for the direction (\u03b8,\u03d5): \ufffd\nAs in LeGendre et al. (2019), our network similarly outputs a log space image Q of HDR illumination, with pixel values Qc(\u03b8,\u03d5), so sphere images are rendered as:\nWith binary mask \u02c6M to mask out the corners of each sphere, \u03b3 = 2.2 for gamma-encoding, \u03bbk as an optional weight for each BRDF, and a diferentiable sof-clipping function \u039b as in LeGendre et al. (2019), the fnal LDR image reconstruction loss Lrec comparing ground truth images Ik and network-rendered images \u02c6Ik is:\n(6)\n\ufffd \ufffd Rather than use the LDR sphere images captured in the video-rate data collection as the reference images Ik, we instead render the spheres with the HDR lighting recovered from the linear solver of Sec. 3.1, gamma-encoding the renderings with \u03b3 = 2.2. Tis ensures that the same lighting is used to render the \u201dground truth\u201d spheres as the input portraits, preventing the propagation of residual error from the HDR lighting recovery to our model training phase. We fnally add extra convolution branches to convert the multiscale feature maps of the decoder into 3-channel images representing log-space HDR lighting at successive scales. We then extend the rendering loss function of LeGendre et al. (2019) (Eqn. 6) to the multiscale domain, rendering mirror, mate silver, and difuse spheres during training in sizes 4 \u00d7 4, 8 \u00d7 8, 16 \u00d7 16, and 32 \u00d7 32. With scale index represented by s, and an optional weight for each as \u03bbs, our multi-scale image reconstruction loss is writen as: \ufffd \ufffd\n(7)\n\ufffd\ufffd \ufffd\ufffd Adversarial Loss. Recent work in unconstrained lighting estimation has shown that adversarial loss terms improve the recovery of high-frequency information compared with using only image reconstruction losses (LeGendre et al. 2019; Song and Funkhouser 2019). Tus, we add an adversarial loss term with weight \u03bbadv as in LeGendre et al. (2019). However, in contrast to this technique, we use a multi-scale GAN architecture that fows gradients from the discriminator to the generator network at multiple scales (Karnewar and Wang 2020), providing the discriminator with diferent sizes of both real and generated clipped mirror ball images.\n# 3.4 Implementation Details\nWe use Tensorfow and the ADAM (Kinga and Ba 2015) optimizer with \u03b21 = 0.9, \u03b22 = 0.999, a learning rate of 0.00015 for the generator network, and, as is common, one 100\u00d7 lower for the discriminator network, alternating between training the generator and discriminator. We set \u03bbk = 0.2, 0.6, 0.2 for the mirror, difuse, and mate silver BRDFs respectively, set \u03bbs = 1 to weight all image scales equally, set \u03bbadv = 0.004, and use a batch size of 32. As the number of lighting environments is orders of magnitude larger than the number of subjects, we found that early stopping at 1.2 epochs appears to prevent over-fting to subjects in the training set. We use the ReLU activation function for the generator network and the ELU activation function (Clevert et al. 2016) for the discriminator. To augment our dataset, we fip both the input images and lighting environments across the vertical axis.\nDatasets. We split our 70 subjects into two groups: 63 for training and 7 for evaluation, ensuring that all expressions and camera views for a given subject belong to the same subset. We manually select the 7 subjects to include both skin tone and gender diversity. In total, for each of our 1 million lighting environments, we randomly select 8 OLAT sequences to relight from the training set (across subjects, facial expressions, and camera views), generating a training dataset of 8 million portraits with ground truth illumination (examples in Fig. 9). Using the same method, we capture lighting environments in both indoor and outdoor locations unseen in training to use for our evaluation, pairing these only with the evaluation subjects.\n# 4 EVALUATION\nIn this section, we compare against prior techniques, perform an ablation study to investigate the performance gains for various sub-components, and measure performance across our diverse evaluation subjects. We also use our lighting estimates to render and composite virtual objects into real-world imagery.\n# 4.1 Comparisons\nAccurately estimated lighting should correctly render objects with arbitrary refectance properties, so we test our model\u2019s performance using Lrec. Tis metric compares the appearance of three spheres (difuse, mate silver, and mirror) as rendered with the ground truth versus estimated illumination. In Table 1, we compare our model against Sun et al. (2019), Calian et al. (2018), and a 2nd order SH decomposition of the ground truth lighting. We use our own implementation for Sun et al. (2019), training the model on our data for a fair comparison. As in the original implementation, we train the model with random crops from portraits composited over black backgrounds (not real-world imagery). As the method includes loss terms on both relit portraits and lighting, we generate 4 million portrait pairs from our original images and train the joint portrait relighting / lighting estimation model. To compare with Calian et al. (2018), the authors generously computed outdoor lighting for a set of portraits. However, the scale of their lighting depends on an albedo prior ft to a diferent dataset. So, for a best case comparison, we re-scale the author-provided illumination such that the total scene radiance matches that of the ground truth. Finally, we compare against the 2nd order SH decomposition, as this represents the best case scenario for any monocular face reconstruction technique that models illumination with this low frequency basis. For the LDR image reconstruction losses, our model out-performs Sun et al. (2019) and Calian et al. (2018) for the difuse and mate silver spheres. However, Sun et al. (2019) out-performs ours for the mirror sphere, as it its log-space loss on lighting is similar to Lrec for the mirror ball (but in HDR). As expected, the 2nd order SH approximation of the ground truth illumination out-performs our model for Lrec for the difuse ball, since a low frequency representation of illumination sufces for rendering Lambertian materials. However, our model out-performs the 2nd order SH decomposition for Lrec for both the mate silver and mirror balls, with non-Lambertian BRDFs. Tis suggests that lighting produced by our model is beter suited for rendering diverse materials.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0354/0354ce89-d65b-4677-96d4-e7372e3a1e4d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 10. Comparison sphere renderings (difuse, mate silver, and mirror) for evaluation subjects and indoor and outdoor lighting environments. We compare our method against \u201dSingle Image Portrait Relighting\u201d (SIPR) (Sun et al. 2019), the second order SH decomposition of the ground truth illumination, and for outdoor scenes, a radiance-scaled version of \u201dFrom Faces to Outdoor Light Probes\u201d (FFOLP) (Calian et al. 2018). Our model more faithfully recovers the total cene radiance compared with SIPR, and, unlike the SH decomposition, is useful for rendering materials with BRDFs beyond Lambertian.</div>\nTable 1. Comparison among methods: Average L1 loss by BRDF [difuse (d), mirror (m), and mate silver (s) spheres (in columns)], for evaluation portraits. We compare ground truth sphere images with those rendered using the HDR lighting inference, for unseen indoor (UI) and outdoor (UO) locations. (*n = 237 for Calian et al. (2018) due to face tracking failures.)\nL1(d)\nL1(s)\nL1(m)\nn = 270*\nUI\nUO\nUI\nUO\nUI\nUO\nOur model\n0.069\n0.056\n0.087\n0.072\n0.181\n0.157\n2nd order SH of GT\n0.016\n0.015\n0.120\n0.109\n0.306\n0.247\nSun et al. (2019)\n0.145\n0.120\n0.113\n0.100\n0.154\n0.139\nCalian et al. (2018)\n\u2013\n0.158\n\u2013\n0.163\n\u2013\n0.215\nIn Table 2, we compare the relative radiance for each color channel for our model and that of Sun et al. (2019), computed as the sum of the pixels of the predicted illumination subtracted from the ground truth illumination, divided by the sum of the ground truth. We show that on average, the illumination recovered by the method of Sun et al. (2019) is missing 41% of the scene radiance. In contrast, for this randomly selected evaluation subset, our method adds on average 9% to the total scene radiance. As our rendering-based loss terms include matching the appearance of a difuse ball, which is similar to a difuse convolution of the HDR lighting environment, our method is able to more faithfully recover the total scene radiance.\n<div style=\"text-align: center;\">Table 2. Average relative radiance diference [(GT - Pred) / GT] for estimated lighting, comparing our method and Sun et al. (2019).</div>\nRed Channel\nGreen Channel\nBlue Channel\nn = 270\nUI\nUO\nUI\nUO\nUI\nUO\nOur model\n-9.04%\n-6.22%\n-6.22%\n-6.10%\n-7.66%\n-17.88%\nSun et al. (2019)\n34.53%\n41.79%\n38.31%\n44.55%\n39.73%\n48.19%\nIn Fig. 10 we show qualitative results, rendering the three spheres using illumination produced using our method, that of Sun et al.\n(2019) labeled as \u201dSIPR\u201d, that of a 2nd order SH decomposition, and that of Calian et al. (2018) for outdoor scenes, labeled as \u201dFFOLP\u201d. Te missing scene radiance from the method of Sun et al. (2019) is apparent looking at the difuse sphere renderings, which are considerably darker than ground truth for this method. While the 2nd order SH approximation of the ground truth lighting produces difuse sphere renderings nearly identical to the ground truth, Fig. 10 again shows how this approximation is ill-suited to rendering nonLambertian materials. For the method of Calian et al. (2018), the sun direction is misrepresented as our evaluation lighting environments include a diversity of camera elevations, with the horizon line not exclusively along the equator of the mirror sphere. In Fig. 11, we show an example where the illumination is estimated from a synthetic LDR portrait of a given subject (Fig. 11a), with the estimated and ground truth illumination in Fig. 11b. We then use both the estimated illumination from our model and the 2nd order SH approximation of the ground truth to light the same subject, shown in Fig. 11c and d respectively. For lighting environments with high frequency information (rows 1, 2, and 4 in Fig. 11), our lighting estimates produce portraits that more faithfully match the input images. Tese results highlight the limitation inherent in the Lambertian skin refectance assumption.\n# 4.2 Ablation Study\nIn Table 3, we report Lrec for each BRDF when evaluating each component of our system. We compare a baseline model using the single-scale losses (LeGendre et al. 2019) to our proposed model trained with multi-scale losses (Lms-rec and MSG-GAN). Te multiscale loss modestly decreases Lrec for both the difuse and mate silver spheres, while increasing that of the mirror sphere. Tis increase is expected, as the adversarial loss for the mirror ball pulls the estimate away from an overly-blurred image that minimizes\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f383/f383c345-de18-44a6-90be-1264e204496c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 11. (a) Inputs to our model, generated using image-based relighting and a photographed reflectance basis for each evaluation subject. (b) Lef: ground truth (GT) lighting used to generate a; Right: lighting estimated from a using our method. (c) The same subject lit with the predicted lighting. (d) The same subject lit with the 2nd order SH decomposition of the GT lighting. (e) A new subject lit with the GT lighting. (f) The new subject lit with the illumination estimated from a using our method. (g) The new subject lit with the 2nd order SH decomposition of the GT lighting. Our method produces lighting environments that can be used to realistically render virtual subjects into existing scenes, while the 2nd order SH lighting leads to an overly difuse skin appearance.</div>\nLrec. In Fig. 12, we show the visual impact of the multi-scale loss term, which synthesizes more high frequency details.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d583/d583733a-96cf-429e-adf9-a8bb77df6f17.png\" style=\"width: 50%;\"></div>\nFig. 12. Our multi-scale losses increase the sharpness of features in the recovered illumination, as shown in the mirror ball images (botom rows), compared with baseline. Upper-right grid shown at +1 stop for display. In Table 3, we also compare our baseline model, trained on images cropped using a face detector, to a model trained on random crops as in Sun et al. (2019), labeled \u201dNo Face Detector.\u201d Te face detector\n<div style=\"text-align: center;\">Fig. 12. Our multi-scale losses increase the sharpness of features in the recovered illumination, as shown in the mirror ball images (botom rows), compared with baseline. Upper-right grid shown at +1 stop for display. In Table 3, we also compare our baseline model, trained on images cropped using a face detector, to a model trained on random crops as in Sun et al. (2019), labeled \u201dNo Face Detector.\u201d Te face detector</div>\nTable 3. Average L1 loss by BRDF: difuse (d), mirror (m), and mate silver (s) spheres (in columns), for lighting estimated from portraits of our evaluation subjects, using our technique with and without diferent features.\n<div style=\"text-align: center;\">Table 3. Average L1 loss by BRDF: difuse (d), mirror (m), and mate silver (s) spheres (in columns), for lighting estimated from portraits of our evaluation subjects, using our technique with and without diferent features.</div>\nL1(d)\nL1(s)\nL1(m)\nModel, n = 3968\nUI\nUO\nUI\nUO\nUI\nUO\nProposed (no Multi-scale Losses)\n0.054\n0.050\n0.076\n0.069\n0.144\n0.128\nNo Face Detector\n0.055\n0.051\n0.080\n0.075\n0.151\n0.136\nNo Background Imagery\n0.057\n0.053\n0.078\n0.072\n0.147\n0.133\nProposed with Multi-scale Losses\n0.050\n0.047\n0.072\n0.067\n0.156\n0.141\nlog-L2 Loss (as in Sun et al. (2019))\n0.151\n0.133\n0.114\n0.103\n0.152\n0.132\nNo Face (LeGendre et al. (2019))\n0.136\n0.135\n0.144\n0.137\n0.174\n0.166\nimparts some modest improvement. Additionally, we compare our baseline model, trained on portraits composited onto real-world background imagery matching the ground truth illumination, to one trained without backgrounds, with subjects composited instead over black as in Sun et al. (2019). (Te evaluation in this case is also performed on subjects against black backgrounds). Te backgrounds also impart some modest improvement. We further show that our baseline model outperforms a model trained using the log-L2 loss on HDR lighting of Sun et al. (2019). As this loss function does\nnot include a rendering step, this is somewhat expected. Finally, we compare against a model trained using only random crops of the background imagery, without portraits, using the single-scale loss terms. Tis table entry, labeled as \u201dNo Face,\u201d is equivalent to LeGendre et al. (2019), but trained on our background images and with our network architecture. As expected, the presence of faces in the input images signifcantly improves model performance.\n# 4.3 Lighting Consistency for Diverse Skin Tones\nIn Table 4, we report Lrec for each of the three spheres individually, for 496 test examples in unseen indoor and outdoor lighting environments for each evaluation subject. Each example set includes diverse camera viewpoints, facial expressions, and hats/accessories. In Fig. 13, we plot the data of Table 4 to visualize that while there are some slight variations in Lrec across subjects, the model\u2019s performance appears similar across diverse skin tones.\nTable 4. Average L1 loss by BRDF: difuse (d), mirror (m), and mate silver (s) spheres (in columns), for lighting estimated from portraits of our evaluation subjects, numbered 1-7 (see Fig. 13). This table corresponds with Fig. 13.\nL1(d)\nL1(s)\nL1(m)\nn = 496\nUI\nUO\nUI\nUO\nUI\nUO\nSubject 1\n0.050\n0.052\n0.074\n0.071\n0.161\n0.154\nSubject 2\n0.063\n0.065\n0.084\n0.081\n0.169\n0.162\nSubject 3\n0.049\n0.051\n0.073\n0.072\n0.160\n0.154\nSubject 4\n0.048\n0.049\n0.073\n0.072\n0.155\n0.149\nSubject 5\n0.040\n0.041\n0.066\n0.066\n0.152\n0.147\nSubject 6\n0.042\n0.043\n0.063\n0.063\n0.148\n0.142\nSubject 7\n0.051\n0.050\n0.071\n0.070\n0.153\n0.146\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1845/18457b12-d810-4899-a261-2e86794b4854.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 13. Average Lrec for individual evaluation subjects, with n = 496 each for unseen indoor and outdoor scenes. This plot corresponds with Table. 4. Our model\u2019s performance is similar for subjects of diverse skin tones.</div>\nWhile Lrec is a useful metric, its absolute value operation masks the sign of residual error. To see whether radiance is missing or added to the predicted lighting for each subject, we also show the total relative radiance diference [(GT-Pred.)/GT] for each color channel for each subject in Fig. 14. Te trend lines in Fig. 14 show that for evaluation subjects with smaller albedo values (measured as an average of each subject\u2019s forehead region), some energy in the estimated lighting is missing relative to the ground truth, with the inverse true for subjects with larger albedo values. For both indoor and outdoor scenes, this relative radiance diference is on\naverage \u00b120% for evaluation subjects with very dark or very light skin tones, respectively, and smaller for subjects with medium skin tones. Nonetheless, as our evaluation subject with the lightest skin tone has an albedo value almost 3.5\u00d7 that of our evaluation subject with the darkest skin tone, the network has mostly learned the correct scale of illumination across diverse subjects. In Fig. 15, we show examples where our model recovers similar lighting for diferent LDR input portraits of our evaluation subjects, where each is lit with the same ground truth illumination. In Fig. 11, we show that for a given input portrait (Fig. 11a), and lighting estimated from this portrait using our method Fig. 11b), we can accurately light a subject of a diferent skin tone (Fig. 11f) without adjusting the scale of the illumination and composite them into the original image, closely matching that subject\u2019s ground truth appearance (Fig. 11e). An additional such example is shown in Fig. 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2113/21139d1a-fc40-4937-b1e3-49032015a460.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Sampled Forehead Albedo</div>\nFig. 14. y axis: Average relative error in total radiance [(GT-Pred.)/GT] for each color channel for each of our evaluation subjects (n = 496 each for unseen indoor and outdoor scenes). x axis: Each subject\u2019s average RGB albedo, sampled from the forehead under a unit sphere of illumination.\n# .4 Lighting Consistency across Head Poses\nWe did not observe any marked diferences in the lighting estimated for a given subject for diferent head poses or facial expressions. In Fig. 16, we show that similar illumination is recovered for diferent camera views and expressions for one of the evaluation subjects.\n# 4.5 Real-World Results\nIn Fig. 20 we show lighting estimation from real-world portraits in-the-wild, for a diverse set of subjects, including one wearing a costume with face-paint. While ground truth illumination is not available, the sphere renderings produced using our lighting inference look qualitatively plausible. Tese results suggest that our model has generalized well to arbitrary portraits.\nMobile Augmented Reality. Our lighting inference runs in realtime on a mobile device (CPU: 27.5 fps, GPU: 94.3 fps on a Google Pixel 4 smartphone), enabling real-time rendering and compositing of virtual objects for smartphone AR applications. We show our inference running in real-time in our supplemental video.\nMobile Augmented Reality. Our lighting inference runs in realtime on a mobile device (CPU: 27.5 fps, GPU: 94.3 fps on a Google Pixel 4 smartphone), enabling real-time rendering and compositing of virtual objects for smartphone AR applications. We show our inference running in real-time in our supplemental video. Digital Double Actor Replacement. In Fig. 17, we estimate lighting from in-the-wild portraits (a), and then light a virtual character to composite into the original scene with consistent illumination.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4daa/4daa5678-c8b4-4bf5-a885-15a21ee9e776.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 15. Lef: spheres rendered with the ground truth illumination. Remaining columns: spheres rendered with the illumination produced using our technique, for input portraits of diferent subjects all lit with the same ground truth illumination. Our model recovers lighting at a similar scale for LDR input portraits of subjects with a variety of skin tones.</div>\nPost-Production Virtual Object Compositing. In Fig. 18 we render and composite a set of shiny virtual balloons into a \u201dselfe\u201d portrait, using lighting estimates produced by our method. We show a version with motion in our supplemental video.\n# 6 LIMITATIONS\nAs our method relies on a face detector, it fails if no face is detected. Fig. 19 shows two other failure modes: an example where a saturated pink hat not observed in training leads to an erroneous lighting estimate, and an example where the illumination color is incorrectly estimated for an input environment with unnatural color balance. Tis input example was generated by scaling the red channel of the ground truth illumination by a factor of 3. Future work could address the frst limitation with additional training data spanning a\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c4cd/c4cd8e07-b9b7-433f-ba2c-8b5cd7e2e61c.png\" style=\"width: 50%;\"></div>\nFig. 16. Lef: spheres rendered with the ground truth illumination. Remaining columns: spheres rendered with the illumination produced using our technique, for input portraits of the same subject with diferent head poses and expressions, lit with the same illumination. Our method recovers similar lighting across facial expressions and head poses.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40cc/40cc5ce6-ffbb-4e91-b90b-4d67d56bf2fb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">n-the-wild portrait (b) lighting (c) digital character lit with b</div>\n<div style=\"text-align: center;\">(b) lighting (c) digital character lit with b</div>\n<div style=\"text-align: center;\">Fig. 17. (a) In-the-wild input portraits. (b) Lighting estimated by our technique. (c) A digital human character rendered with the predicted illumination, composited into the original scene. Digital character model by Ian Spriggs, rendered in V-Ray with the VRayAlSurfaceSkin shader.</div>\nbroader range of accessories, while the second limitation could be addressed with data augmentation via adjusting the white balance of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1576/15768545-ad4a-41a6-8b5e-9242612ee4c1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 18. Virtual balloons composited into \u201dselfie\u201d portraits using lighting estimated by our technique.</div>\nthe ground truth illumination. Finally, our lighting model assumes distant illumination, so our method is not able to recover complex local lighting efects.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7b0f/7b0f8ea1-faad-4a05-b034-3f1d2afa1789.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 19. Two example failure cases. Lef: A brightly-hued hat not observed in training. Right: Input lighting environment with non-natural color balance.</div>\n# 7 CONCLUSION\nWe have presented a learning-based technique for estimating omnidirectional HDR illumination from a single LDR portrait image. Our model was trained using a photo-realistic, synthetically-rendered dataset of portraits with ground truth illumination generated using refectance felds captured in a light stage, along with more than one million lighting environments captured using an LDR videorate technique, which we promoted to HDR using a novel linear solver formulation. We showed that our method out-performs both the previous state-of-the-art in portrait-based lighting estimation, and, for non-Lambertian materials, a low-frequency, second order spherical harmonics decomposition of the ground truth illumination. We are also, to the best of our knowledge, the frst to explicitly evaluate our lighting estimation technique for subjects of diverse skin tones, while demonstrating recovery of a similar scale of illumination for diferent subjects. Our technique runs in real-time on a mobile device, suggesting its usefulness for improving the photorealism of face-based augmented reality applications. We further demonstrated our method\u2019s utility for post-production visual efects, showing that digital characters can be composited into real-world photographs with consistent illumination learned by our model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bcc4/bcc40e13-4e35-4a0a-9796-72e937d948ed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Input</div>\n<div style=\"text-align: center;\">Pred.</div>\nHyunjung Shim. 2012. Faces as light probes for relighting. Optical Engineering 51, 7 (2012), 077002. Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli, Sylvain Paris, and Dimitris Samaras. 2017a. Portrait lighting transfer using a mass transport approach. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1. Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, and Dimitris Samaras. 2017b. Neural face editing with intrinsic image disentangling. In Proceedings of the IEEE Conference on Computer Vision and Patern Recognition. 5541\u20135550. Shuran Song and Tomas Funkhouser. 2019. Neural illumination: Lighting prediction for indoor environments. In Proceedings of the IEEE Conference on Computer Vision and Patern Recognition. 6918\u20136926. Jessi Stumpfel, Chris Tchou, Andrew Jones, Tim Hawkins, Andreas Wenger, and Paul Debevec. 2004. Direct HDR capture of the sun and sky. In Proceedings of the 3rd international conference on Computer graphics, virtual reality, visualisation and interaction in Africa. ACM, 145\u2013149. Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyfe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. 2019. Single Image Portrait Relighting. ACM Trans. Graph. 38, 4, Article Article 79 (July 2019), 12 pages. htps://doi.org/10.1145/3306346.3323008 A. Tewari, M. Zollh\u00a8ofer, P. Garrido, F. Bernard, H. Kim, P. P\u00b4erez, and C. Teobalt. 2018. Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz. In Proceedings of Computer Vision and Patern Recognition (CVPR 2018). Ayush Tewari, Michael Zollh\u00a8ofer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard, Patrick P\u00b4erez, and Christian Teobalt. 2017. Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. In Te IEEE International Conference on Computer Vision (ICCV), Vol. 2. 5. Andrei Tkachenka, Gregory Karpiak, Andrey Vakunov, Yury Kartynnik, Artsiom Ablavatski, Valentin Bazarevsky, and Siargey Pisarchyk. 2019. Real-time Hair Segmentation and Recoloring on Mobile GPUs. arXiv preprint arXiv:1907.06740 (2019). Luan Tran, Feng Liu, and Xiaoming Liu. 2019. Towards high-fdelity nonlinear 3D face morphable model. In Proceedings of the IEEE Conference on Computer Vision and Patern Recognition. 1126\u20131135. Luan Tran and Xiaoming Liu. 2019. On learning 3d face morphable model from inthe-wild images. IEEE transactions on patern analysis and machine intelligence (2019). Jonas Unger, Stefan Gustavson, and Anders Ynnerman. 2006. Densely Sampled Light Probe Sequences for Spatially Variant Image Based Lighting. In Proceedings of\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of estimating high dynamic range (HDR) omnidirectional illumination from a single low dynamic range (LDR) portrait image, which is crucial for realistic rendering in portrait photography and augmented reality applications. Previous methods have struggled with the inherent ambiguity between lighting strength and surface albedo, necessitating a new approach to enhance accuracy and applicability.",
        "problem": {
            "definition": "The problem is to accurately estimate HDR illumination from LDR portrait images without relying on specific skin reflectance models, which has been a challenge in existing methods.",
            "key obstacle": "The main difficulty lies in the ambiguity between surface reflectance (albedo) and light source strength, which complicates the accurate recovery of lighting from portraits."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that lighting cues can be derived from the geometry and reflectance properties of faces, which can be leveraged to infer illumination without strict assumptions about skin reflectance.",
            "opinion": "The proposed method involves training a neural network to perform inverse lighting from portraits, estimating omnidirectional HDR illumination effectively.",
            "innovation": "The key innovation is the ability to recover high-frequency lighting details that surpass the low-frequency approximations used in previous methods, allowing for more realistic rendering of virtual objects."
        },
        "method": {
            "method name": "Learning-based HDR Illumination Estimation",
            "method abbreviation": "LHE",
            "method definition": "A deep learning technique that estimates HDR omnidirectional illumination from LDR portrait images, trained on a dataset of paired images and ground truth illumination.",
            "method description": "The method utilizes a neural network to infer lighting from portrait images, producing high-quality HDR outputs suitable for various applications.",
            "method steps": [
                "Capture reflectance fields for diverse subjects under controlled lighting conditions.",
                "Generate a large dataset of portraits with corresponding ground truth illumination.",
                "Train a neural network with rendering-based loss functions and adversarial losses.",
                "Evaluate the model's performance on unseen data to ensure generalization."
            ],
            "principle": "The effectiveness of this method is grounded in its ability to leverage facial geometry and reflectance properties to resolve ambiguities in lighting estimation, capturing both diffuse and specular reflections."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved using a dataset of 1 million HDR lighting environments and 70 diverse subjects captured under controlled conditions to train and evaluate the model.",
            "evaluation method": "Performance was assessed using reconstruction losses comparing rendered images of spheres under estimated and ground truth lighting, alongside qualitative assessments of rendered subjects."
        },
        "conclusion": "The method demonstrates superior performance in estimating HDR illumination from LDR portraits, effectively handling diverse skin tones and enabling real-time applications on mobile devices, thereby enhancing the realism of augmented reality and visual effects.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to accurately recover high-frequency illumination details, improving the realism of rendered virtual objects compared to existing methods.",
            "limitation": "A noted limitation is the reliance on face detection; if no face is detected, the method fails. Additionally, complex local lighting effects are not captured due to the model's assumptions.",
            "future work": "Future research could focus on expanding the training dataset to include a wider variety of accessories and lighting conditions, as well as enhancing the model's capability to handle non-standard lighting scenarios."
        },
        "other info": {
            "real-time performance": "The lighting inference runs at 27.5 fps on CPU and 94.3 fps on GPU on a Google Pixel 4 smartphone, enabling practical applications in augmented reality.",
            "training data": "The dataset includes 8 million portraits generated from 1 million lighting environments, ensuring a robust training framework."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem is to accurately estimate HDR illumination from LDR portrait images without relying on specific skin reflectance models, which has been a challenge in existing methods."
        },
        {
            "section number": "2.2",
            "key information": "The primary advantage of the proposed approach is its ability to accurately recover high-frequency illumination details, improving the realism of rendered virtual objects compared to existing methods."
        },
        {
            "section number": "5.1",
            "key information": "The method utilizes a neural network to infer lighting from portrait images, producing high-quality HDR outputs suitable for various applications."
        },
        {
            "section number": "5.4",
            "key information": "A noted limitation is the reliance on face detection; if no face is detected, the method fails. Additionally, complex local lighting effects are not captured due to the model's assumptions."
        },
        {
            "section number": "6.5",
            "key information": "The lighting inference runs at 27.5 fps on CPU and 94.3 fps on GPU on a Google Pixel 4 smartphone, enabling practical applications in augmented reality."
        },
        {
            "section number": "7.1",
            "key information": "The main difficulty lies in the ambiguity between surface reflectance (albedo) and light source strength, which complicates the accurate recovery of lighting from portraits."
        }
    ],
    "similarity_score": 0.5728224991174083,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Learning Illumination from Diverse Portraits.json"
}