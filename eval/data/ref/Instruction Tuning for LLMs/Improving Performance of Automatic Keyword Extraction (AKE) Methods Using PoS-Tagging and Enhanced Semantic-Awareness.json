{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2211.05031",
    "title": "Improving Performance of Automatic Keyword Extraction (AKE) Methods Using PoS-Tagging and Enhanced Semantic-Awareness",
    "abstract": "Automatic keyword extraction (AKE) has gained more importance with the increasing amount of digital textual data that modern computing systems process. It has various applications in information retrieval (IR) and natural language processing (NLP), including text summarisation, topic analysis and document indexing. This paper proposes a simple but effective post-processing-based universal approach to improve the performance of any AKE methods, via an enhanced level of semantic-awareness supported by PoS-tagging. To demonstrate the performance of the proposed approach, we considered word types retrieved from a PoS-tagging step and two representative sources of semantic information \u2013 specialised terms defined in one or more context-dependent thesauri, and named entities in Wikipedia. The above three steps can be simply added to the end of any AKE methods as part of a post-processor, which simply re-evaluate all candidate keywords following some context-specific and semantic-aware criteria. For five stateof-the-art (SOTA) AKE methods, our experimental results with 17 selected datasets showed that the proposed approach improved their performances both consistently (up to 100% in terms of improved cases) and significantly (between 10.2% and 53.8%, with an average of 25.8%, in terms of F1-score and across all five methods), especially when all the three enhancement steps",
    "bib_name": "altuncu2022improvingperformanceautomatickeyword",
    "md_text": "# Improving Performance of Automatic Keyword Extraction (AKE) Methods Using PoS-Tagging and Enhanced Semantic-Awareness\nEnes Altuncua,\u2217, Jason R.C. Nursea, Yang Xub, Jie Guob, Shujun Lia\naInstitute of Cyber Security for Society (iCSS) & School of Computing, University of Kent, Canterbury, CT2 7NP, UK bShanghai Jiao Tong University, Shanghai, China\n# Abstract\nAutomatic keyword extraction (AKE) has gained more importance with the increasing amount of digital textual data that modern computing systems process. It has various applications in information retrieval (IR) and natural language processing (NLP), including text summarisation, topic analysis and document indexing. This paper proposes a simple but effective post-processing-based universal approach to improve the performance of any AKE methods, via an enhanced level of semantic-awareness supported by PoS-tagging. To demonstrate the performance of the proposed approach, we considered word types retrieved from a PoS-tagging step and two representative sources of semantic information \u2013 specialised terms defined in one or more context-dependent thesauri, and named entities in Wikipedia. The above three steps can be simply added to the end of any AKE methods as part of a post-processor, which simply re-evaluate all candidate keywords following some context-specific and semantic-aware criteria. For five stateof-the-art (SOTA) AKE methods, our experimental results with 17 selected datasets showed that the proposed approach improved their performances both consistently (up to 100% in terms of improved cases) and significantly (between 10.2% and 53.8%, with an average of 25.8%, in terms of F1-score and across all five methods), especially when all the three enhancement steps\nare used. Our results have profound implications considering the ease to apply our proposed approach to any AKE methods and to further extend it. Keywords: keyword extraction, pos-tagging, semantic-awareness, context-awareness\n# 1. Introduction\nKeyword extraction (KE), also known as keyphrase or key term extraction, is an information extraction task that aims to identify a number of words/phrases that best summarise the nature or the context of a piece of text. It has several applications in information retrieval (IR) and natural language processing (NLP), including text summarisation, topic analysis and document indexing. Considering the vast amount of text-based documents online in today\u2019s digital society, it is very useful to be able to extract keywords from online documents automatically to support large-scale textual analysis. Therefore, for many years the research community has been investigating automatic keyword extraction (AKE) methods, especially with the recent advancements in artificial intelligence (AI) and NLP. Despite these efforts, however, AKE has been shown to be a challenging task and AKE methods with very high performance are still to be found (Papagiannopoulou and Tsoumakas, 2020). Two main challenges are the lack of a precise definition of the AKE task and the lack of consistent performance evaluation metrics and benchmarks (Merrouni et al., 2020). Since there is no consensus on the definition and characteristics of a keyword, KE datasets created by researchers have different characteristics, e.g., the minimum/average/maximum numbers of keywords, if absent keywords (human-labeled keywords that do not appear in the text) are allowed, and what part-of-speech (PoS) tags such as verbs are accepted as valid keywords. This makes performance evaluation and comparison of AKE methods more difficult. Based on whether a labelled training set is used, AKE methods reported in the literature can be grouped into unsupervised and supervised methods. Unsupervised methods include statistical, graph-based, embeddingbased and/or language model-based methods, while supervised ones use either traditional or deep machine learning models (Papagiannopoulou and Tsoumakas, 2020). Surprisingly, most AKE methods have not considered semantic information to align the returned keywords with the semantic context of the input document.\nIn this work, to fill the above-mentioned gap on the lack of use of semantic information in the state-of-the-art (SOTA) AKE methods, we propose a universal performance improvement approach for any AKE methods, as a post-processor that can consider semantic information more explicitly, with the support of PoS-tagging. To start with, we conducted an analysis of human-created \u2018gold standard\u2019 keywords in 17 KE datasets to better understand some relevant characteristics of such keywords, focusing on PoS-tag patterns, n-gram sizes, and the possible consideration of semantic information by human labellers when extracting keywords. Our proposed approach is demonstrated using the following three postprocessing steps that can be freely combined: (1) keeping candidate keywords with a desired PoS-tag only; (2) matching candidate keywords with one or more context-specific thesauri containing more semantically relevant terms; and (3) prioritising candidate keywords that appear as a valid Wikipedia named entity. We applied different combinations of the above three postprocessing steps to five SOTA AKE methods, YAKE! (Campos et al., 2020), KP-Miner (El-Beltagy and Rafea, 2009), RaKUn (\u02c7Skrlj et al., 2019), LexRank (Ushio et al., 2021), and SIFRank+ (Sun et al., 2020), and compared the performances of the original methods with those of the enhanced versions. The experimental results with the 17 KE datasets showed that our proposed postprocessing steps helped improve the performances of all the five SOTA AKE methods both consistently (up to 100% in terms of improved cases) and significantly (between 10.2% and 53.8%, with an average of 25.8%, in terms of F1-score and across all five methods), particularly when all the three steps are combined. The source code and the complete experimental results of our work (including some details that are not reported in this paper due to the space limit) will be released after the paper is accepted for publication. The rest of the paper is organised as follows. Section 2 briefly surveys AKE methods in the literature. The analysis of the human-created keywords in 17 KE datasets is given in Section 3. In Section 4, we present the methodology of our study. Section 5 explains the experimental setup for evaluation as well as the results. Finally, the paper is concluded with some further discussions in Section 6, and an overall summary in Section 7.\n# 2. Related Work\n# 2.1. Unsupervised AKE Methods\nSome unsupervised AKE methods have been proposed, including statistical, graph-based, embedding-based and language model-based methods (Papagiannopoulou and Tsoumakas, 2020). Statistical AKE methods rely on some selected statistical metrics, e.g., term frequency, relevance to context, and co-occurrences, for ranking candidate keywords. One of the mostly used metrics is TF-IDF (Jones, 1972), which combines two aspects of a term: term frequency within the input article, and the inverse document frequency across several domains. One AKE method using TF-IDF is KP-Miner (El-Beltagy and Rafea, 2009), which also considers other metrics such as word length and word position. A more recent method in this category is YAKE! (Campos et al., 2020), which leverages a range of statistical metrics, such as casing, word position, word frequency, word relatedness to context and how often a term appears in different sentences. Finally, LexSpec (Ushio et al., 2021) makes use of lexical specificity \u2013 a statistical metric to select the most representative keywords from a given text based on the hypergeometric distribution. Graph-based AKE methods consider candidate keywords as nodes in a directed graph, often with weighted edges reflecting syntactic/semantic relatedness of different keywords. They leverage graph-based methods, such as PageRank (Brin and Page, 1998), for ranking the nodes of the graph in terms of their overall importance. The earliest AKE method in this category is TextRank (Mihalcea and Tarau, 2004), which uses an unweighted graph of candidate keywords after filtering the ones that are not a noun or an adjective, and uses PageRank for ranking the nodes. As an extension to TextRank, SingleRank (Wan and Xiao, 2008) adds edge weights to the graph, which reflect the number of co-occurrences of the candidate keywords represented by any pair of two connected nodes. Another graph-based AKE method is RAKE (Rose et al., 2010), which builds a word-word co-occurrence graph, and assigns a score for each candidate by using word frequency and word degree. A more recent graph-based AKE method is RaKUn (\u02c7Skrlj et al., 2019), which introduces meta-vertices by aggregating similar vertices, and employs load centrality metrics for candidate ranking. Finally, LexRank (Ushio et al., 2021) and TFIDFRank (Ushio et al., 2021) are two different enhanced versions of SingleRank, which use lexical specificity and TF-IDF, respectively.\nEmbedding-based AKE methods utilise word representation techniques, such as Doc2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). An example method in this category is EmbedRank (Bennani-Smires et al., 2018), which uses sentence embeddings, and ranks candidate keywords in terms of cosine similarity. A more recent method is SIFRank (Sun et al., 2020), which combines sentence embedding model SIF and autoregressive pre-trained language model ELMo, and it was upgraded to SIFRank+ by position-biased weight to improve its performance for long documents. Lastly, MDERank (Zhang et al., 2022) considers the similarity between the embeddings of the source document and its masked version for candidate ranking. Apart from the AKE methods mentioned above, there also exist a number of AKE methods based on other techniques. Rabby et al. (2020) proposed TeKET, a domain- and language-independent AKE technique utilising a binary tree for extracting final keywords from candidate ones. As another example, Liu et al. (2009) introduced an AKE algorithm based on term clustering considering semantic relatedness to identify the exemplar terms. The identified exemplar terms are, then, used to extract keywords.\n# 2.2. Supervised AKE Methods\nAlthough unsupervised methods are preferred for AKE, supervised methods have also been proposed. One of the earliest methods is KEA (Witten et al., 2005), which calculates TF-IDF scores and the position of the first occurrence of each candidate, and employs the Naive Bayes learning algorithm to decide if a candidate should be selected. More recently, there is a growing interest to use deep learning for AKE. For example, Basaldella et al. (2018) proposed an AKE method based on Bi-LSTM, which is capable of exploiting the context of each candidate word. Another AKE method, TNTKID (Martinc et al., 2021), leverages transformers, and allows users to train their own language model on a domain-specific corpus. A third example is TANN (Wang et al., 2018), an AKE method based on a topic-based artificial neural network model, which aims to improve the performance of AKE by transferring knowledge from a resource-rich source domain to an unlabelled or an insufficiently labelled target domain.\n# 2.3. PoS-Tagging and Semantics in AKE\nMany AKE methods have considered how to extract more semantically meaningful keywords. For this purpose, PoS-tagging has been used so that extracted keywords are restricted to a pre-defined set of PoS-tag patterns,\ne.g., noun phrases only (Hulth, 2003; Pay, 2016; Zervanou, 2010). Some methods utilise external knowledge to provide useful contextual information for extracting more semantically sensible keywords. For instance, Li and Wang (2014) proposed a TextRank-based AKE method that benefits from domain knowledge by using author-assigned keywords of scientific publications, and Gazendam et al. (2010) proposed to use semantic relations between thesaurus terms for ranking candidate keywords without a reference corpus (Gazendam et al., 2010). Thesaurus relations have also been combined with machine learning techniques to improve performance of AKE methods (Hulth et al., 2001; Medelyan and Witten, 2006). Some AKE methods also make use of Wikipedia, a useful source of semantic information. Shi et al. (2008) utilised Wikipedia to extract semantic features of candidate keywords. Their method constructs a semantic graph connecting candidate keywords to document topics based on the hierarchical relations extracted from Wikipedia, and semantic feature weights are assigned to candidate keywords with a link analysis algorithm. WikiRank is another AKE method leveraging Wikipedia (Yu and Ng, 2018). It employs the TAGME annotator (Ferragina and Scaiella, 2010) to link meaningful word sequences in the input document to concepts in Wikipedia, and constructs a semantic graph. Then, it transforms the KE task to an optimisation problem on the graph, and tries to obtain the optimal keyword set that has the best coverage of the identified concepts. Finally, several embedding-based AKE methods utilise Wikipedia for pre-training and/or fine-tuning their underlying embedding methods (Bennani-Smires et al., 2018; Papagiannopoulou and Tsoumakas, 2018). Compared with existing AKE methods that have considered PoS-tagging or semantic information more explicitly, our proposed approach is more universal and can be applied to any AKE methods as a post-processor, which simply re-evaluate candidate keywords generated by an AKE method before top n keywords are returned. Our approach is easily generalisable and can be used flexibly to eliminate candidate keywords that are unlikely to be a keyword, and prioritise ones that are more likely to be a keyword.\n# 3. Analysis of Human-Created Keywords\nOur proposed approach was motivated by some of our observations regarding how human labellers extracted \u201cgolden\u201d (i.e., ground truth) keywords in 17 KE datasets. Such observations also helped us to determine\nsome fine details of our proposed approach. In the following, we describe the 17 datasets we used and the key observations.\n# 3.1. Datasets Inspected\nConsidering the subjectivity of the keyword extraction task, there is no standard approach to follow in constructing keyword extraction datasets (Zesch and Gurevych, 2009). This brings an extreme diversity to the datasets constructed so far, which makes testing a keyword extraction algorithm thoroughly harder. Therefore, to achieve a better understanding of humancreated keywords, we aimed at collecting a large number of datasets used in the literature. In total, we selected 17 datasets covering multiple contexts, including agriculture, computer science and health, and several types of documents, such as scientific papers, news, theses and abstracts. Further details regarding the datasets can be seen in Table 1.\n# 3.2. Observations: PoS-Tag Patterns\nThere have been a lot of research on linguistic properties of different multi-word expression types, such as collocations (Smadja, 1993) and technical terms (Justeson and Katz, 1995). However, these are unable to properly explain the linguistic properties of keywords used in AKE research because of the lack of linguistic standards for human-created keywords. Therefore, firstly, we reviewed the structure of human-created keywords in the 17 datasets, in terms of the used PoS-tag patterns. For this purpose, we used the NLTK (Bird, 2006) library\u2019s PoS tagger and computed the distribution of different PoS-tag patterns. As shown in Table 2, nine of the top ten PoS-tag patterns correspond to either noun or gerund phrases. The only non-noun/gerund pattern in the top ten PoS-tag patterns is a single adjective (JJ), with an average percentage of 6.85%. The top ten PoS-tag patterns count 80% of all patterns. These observations imply that leveraging knowledge about how human labellers define keywords based on PoS-tag patterns for a specific domain can potentially help improve the performance of any AKE methods for the corresponding domain.\n# 3.3. Observations: n-Gram Size\nAKE methods generally include a parameter for the maximum n-gram size, corresponding to the maximum number of words a keyword is allowed to contain. Although it is well-known that multi-word expressions (MWEs) are more likely to be of length two to three in English (Choueka, 1988),\nDataset\nContent Context\nSize\nAvg. #(Keys) Abs. Keys\nAnnotators1\nKPCrowd (Marujo\net al., 2013)\nNews\nMisc.\n500\n48.92\n13.5%\nReaders\nciteulike180 (Medelyan\net al., 2009)\nPaper\nMisc.\n183\n18.42\n32.2%\nReaders\nDUC-2001 (Wan and\nXiao, 2008)\nNews\nMisc.\n308\n8.1\n3.7%\nReaders\nfao30 (Medelyan and\nWitten, 2008)\nPaper\nAgr.\n30\n33.23\n41.7%\nExperts\nfao780 (Medelyan and\nWitten, 2008)\nPaper\nAgr.\n779\n8.97\n36.1%\nExperts\nInspec (Hulth, 2003)\nAbstract\nCS\n2,000\n14.62\n37.7%\nExperts\nKDD (Das Gollapalli\nand Caragea, 2014)\nAbstract\nCS\n755\n5.07\n53.2%\nAuthors\nKPTimes\n(test) (Gallina et al.,\n2019)\nNews\nMisc.\n20,000\n5.0\n54.7%\nEditors\nKrapivin2009 (Krapivin\net al., 2009)\nPaper\nCS\n2,304\n6.34\n15.3%\nAuthors\nNguyen2007 (Nguyen\nand Kan, 2007)\nPaper\nCS\n209\n11.33\n17.8%\nAuthors & Readers\nPubMed (Gay et al.,\n2005)\nPaper\nHealth\n500\n15.24\n60.2%\nAuthors\nSchutz2008 (Schutz,\n2008)\nPaper\nHealth\n1,231\n44.69\n13.6%\nAuthors\nSemEval2010 (Kim\net al., 2010)\nPaper\nCS\n243\n16.47\n11.3%\nAuthors & Readers\nSemEval2017 (Augen-\nstein et al.,\n2017)\nParagr\nMisc.\n493\n18.19\n0.0%\nExperts & Readers\ntheses1002\nThesis\nMisc.\n100\n7.67\n47.6%\nUnknown\nwiki20 (Medelyan\net al., 2008)\nReport\nCS\n20\n36.50\n51.2%\nReaders\nWWW (Das Gollapalli\nand Caragea, 2014)\nAbstracts\nCS\n1,330\n5.80\n55.0%\nAuthors\n \nit is less clear how human labellers of the 17 datasets were instructed to consider the n-gram size. Therefore, we analysed the golden keywords across the 17 datasets to see how human labellers decided the n-gram sizes. On average, bigrams (n = 2) constitute 45.55% of the golden keywords in the 17 datasets, while this rate is 36.45% for unigrams (n = 1) and 12.73% for trigrams (n = 3). In addition, the percentages for keywords with n \u22654 are considerably low \u2013 5.12% on average. More detailed statistics can be seen in Table 3. These results show that human labellers largely used two or three as the maximum n-gram size, covering 82.01% and 94.74% of the golden\n<div style=\"text-align: center;\">Table 2: Percentages of top 10 PoS-tag patterns across 17 datasets. PoS tags: NN \u2013 noun (singular), NNS \u2013 noun (plural), JJ \u2013 adjective, VBG \u2013 verb gerund.</div>\nDataset\nNN\nNN NN JJ NN NNS\nJJ\nJJ NNS NN NNS JJ NN NN VBG NN NN NN\nKPCrowd\n31.38\n2.18\n3.29\n11.65 10.13\n0.95\n0.95\n0.26\n5.27\n0.17\nciteulike180\n48.71\n7.03\n4.78\n12.93 12.74\n1.61\n1.56\n0.15\n1.95\n0.05\nDUC-2001\n19.13\n15.90\n15.28 10.49 1.80\n8.73\n10.16\n3.65\n0.28\n1.52\nfao30\n32.60\n14.68\n7.92\n15.84 5.06\n6.62\n9.35\n0.00\n0.78\n0.26\nfao780\n29.56\n14.11\n9.11\n15.18 3.78\n6.02\n10.88\n0.06\n1.21\n0.04\nInspec\n19.05\n12.57\n12.49\n6.64\n3.85\n8.11\n5.95\n4.35\n1.11\n2.50\nKDD\n27.93\n13.49\n9.06\n5.89\n9.25\n5.13\n3.55\n2.22\n4.81\n0.76\nKPTimes\n15.32\n16.65\n15.67\n4.27\n2.83\n8.62\n6.26\n2.92\n1.76\n1.51\nKrapivin2009 35.15\n4.70\n4.06\n14.14 5.67\n2.17\n1.47\n0.27\n0.95\n0.17\nNguyen2007\n20.85\n19.83\n11.31\n4.84\n2.53\n4.79\n3.37\n3.06\n1.51\n2.66\nPubMed\n30.88\n9.23\n3.87\n15.43 12.01\n3.51\n5.50\n0.77\n0.56\n2.03\nSchutz2008\n30.15\n6.20\n10.61 18.63 10.91\n5.04\n3.19\n1.61\n0.31\n0.66\nSemEval2010 19.45\n21.74\n21.54\n0.08\n3.20\n0.17\n0.06\n6.40\n0.42\n3.15\nSemEval2017 14.57\n8.73\n9.00\n7.23\n2.12\n5.95\n4.46\n3.31\n0.66\n1.62\ntheses100\n27.88\n8.55\n5.39\n9.48 15.24\n6.13\n4.28\n0.00\n1.30\n0.19\nwiki20\n41.91\n18.65\n11.06\n1.49\n6.60\n0.50\n1.82\n2.81\n2.81\n0.99\nWWW\n32.33\n13.44\n8.98\n5.41\n8.74\n3.88\n3.88\n1.63\n2.86\n1.05\nAverage (%) 28.05\n12.22\n9.61\n9.39\n6.85\n4.59\n4.51\n1.97\n1.68\n1.13\nkeywords across the different datasets, respectively. The results are aligned with those in the research literature on MWEs. Based on such observations, we can see that AKE methods could benefit from focusing more on keywords with a shorter word length.\n# 3.4. Observations: Semantic Information\nFinally, we analysed the human-created keywords to see if human labellers explicitly or implicitly relied on semantic information to select keywords. We first calculated the percentage of golden keywords that are covered by Wikipedia across all the datasets. This quantitative analysis indicated that, on average, 64.39% of the golden keywords are Wikipedia named entities, i.e., titles of Wikipedia articles. This interesting (previously unreported) finding justifies that Wikipedia can be a very useful knowledge base for AKE algorithms as it covers so many golden keywords chosen by human labellers for all the 17 datasets we chose. Although unexpected, this finding can be explained by the diversity and richness of the content of Wikipedia. More detailed results of the analysis can be seen in Table 4.\n<div style=\"text-align: center;\">Table 3: n-gram distributions of the 17 datasets</div>\nDataset\nn = 1\nn = 2\nn = 3\nn \u22654\nn = 1, 2\n1 \u2264n \u22643\nKPCrowd\n73.78\n18.47\n4.90\n2.83\n92.25\n97.15\nciteulike180\n77.10\n19.98\n2.79\n0.09\n97.08\n99.87\nDUC-2001\n17.32\n61.29\n17.73\n3.66\n78.61\n96.34\nfao30\n43.02\n52.74\n3.41\n0.83\n95.76\n99.17\nfao780\n42.32\n53.72\n3.62\n0.34\n96.04\n99.66\nInspec\n16.44\n53.68\n23.05\n6.84\n70.12\n93.17\nKDD\n25.48\n56.32\n13.97\n4.24\n81.80\n95.77\nKPTimes\n46.68\n34.39\n12.55\n6.38\n81.07\n93.62\nKrapivin2009\n18.95\n61.61\n15.74\n3.70\n80.56\n96.30\nNguyen2007\n27.53\n49.96\n15.42\n6.97\n77.49\n92.91\nPubMed\n35.79\n43.74\n15.90\n4.58\n79.53\n95.43\nSchutz2008\n57.83\n30.22\n8.15\n1.67\n88.05\n96.20\nSemEval2010\n20.05\n52.97\n20.66\n6.31\n73.02\n93.68\nSemEval2017\n25.23\n33.74\n17.19\n23.84\n58.97\n76.16\ntheses100\n31.63\n50.37\n11.09\n6.90\n82.00\n93.09\nwiki20\n26.20\n53.52\n18.17\n2.11\n79.72\n97.89\nWWW\n34.36\n47.71\n12.15\n5.78\n82.07\n94.22\nAverage (%)\n36.45\n45.55\n12.73\n5.12\n82.01\n94.74\n<div style=\"text-align: center;\">Table 4: The percentages of golden keywords covered by Wikipedi</div>\nDataset\n%\nDataset\n%\nKPCrowd\n71.77\nNguyen2007\n52.19\nciteulike180\n83.78\nPubMed\n81.28\nDUC-2001\n51.05\nSchutz2008\n67.43\nfao30\n80.97\nSemEval2010\n41.27\nfao780\n79.00\nSemEval2017\n31.02\nInspec\n39.08\ntheses100\n68.82\nKDD\n62.92\nwiki20\n89.01\nKPTimes\n79.09\nWWW\n63.83\nKrapivin2009\n52.12\nIn addition to Wikipedia named entities, we also manually inspected many golden keywords and observed that many collected datasets contain domain-specific golden keywords. This observation indicates that consider ing domain-specific terms can potentially help improve performance of AKE methods, too.\n# 4. Methodology\nOur proposed approach is based on three post-processing steps that can be applied to any baseline AKE methods: 1) filtering candidate keywords with unlikely PoS-tag patterns, 2) using one or more context-aware (i.e., domain specific) thesauri to prioritise important candidate keywords for the target domain, and 3) prioritising candidate keywords that are Wikipedia named entities. In the following, we explain the three steps with more details.\n# 4.1. Filtering Specific PoS-Tag Patterns\nAs mentioned in Section 2.3, PoS-tagging has been extensively used in AKE methods to consider morpho-syntactic features. Motivated by the observations in Section 3.2, we attempted to leverage a PoS-tagger to filter out candidate keywords labelled with unlikely PoS-tag patterns. More precisely, candidate keywords that do not conform with any of the following PoS-tag patterns were discarded: (i) simple nouns and noun phrases \u2013 one or more nouns (optionally with one or more adjectives appearing before the first noun); (ii) two or more simple nouns and/or noun phrases connected by one or more prepositions or conjunctions1; and (iii) a single adjective. In the PoS-tag patterns mentioned above, nouns and adjectives mean any PoS tags that can provide the corresponding functionality in a sentence. Therefore, nouns also include gerunds, and adjectives also include past participle verbs. Considering the most common PoS-tag patterns mentioned in Section 3.2, our proposed PoS-tag patterns correspond to over 90% of the patterns observed across all the 17 datasets. We used NLTK to extract PoS tags for each term in the input documents. For pattern matching, we took the advantage of regular expressions. Since regular expressions return the longest possible matches, we extracted the shorter matches from the longest ones separately. Note that the proposed PoS-tag patterns can be further changed to reflect any domain-specific needs, e.g., we observed that gerunds are quite uncommon in health domain so they can be removed if preferred.\n# 4.2. Context-Aware Thesauri\nContext means any kind of domain, topic or field that has its own set of terms semantically specific to itself. While the set of terms specific to a context can be covered in a more structured vocabulary, such as a thesaurus or an ontology, a simple word list can often be sufficient for the purpose of AKE. As reported in Section 3.4, many keywords are related to the context of the input text, contextual consideration can be quite useful for AKE. Therefore, we propose to make use of external resources to inform AKE methods more about semantically useful keywords for the relevant domain. More specifically, we proposed to integrate one or more domain-specific thesauri, which contain terms specific to a target context, and to prioritise candidate keywords included in such thesauri. At the implementation level, we introduce a weight for each candidate keyword and increase the weight of any candidate keyword appearing in one of the thesauri. In our experiments, we doubled the weights of candidate keywords in a thesaurus, but the actual weight increase can be a parameter, which can be empirically determined based on some training data or qualitative evidence observed. To determine if a candidate keyword exists in a given thesaurus, we applied exact matching with lemmatisation. Although using stemming with exact matching is a more common practice in AKE (Papagiannopoulou and Tsoumakas, 2020), we preferred to use lemmatisation due to its context-awareness. In our experiments, we focused on thesauri with a single context, but using multiple contexts in a single thesaurus is of course also possible. Regarding integrating relevant thesauri, we considered two different approaches explained below. Manual Context Consideration:. This approach is more useful when documents processed by an AKE method are known to belong to a specific context. It utilises one or more thesauri containing a list of terms relevant to the context, which are given a higher weight for prioritisation by the AKE method. In our experiments, we assigned a single domain-specific thesaurus to each of the datasets to represent the relevant context. Note that it is possible that multiple contexts and multiple thesauri are used in some applications of AKE.\nAutomatic Context Identification:. Considering the wide range of applications in which AKE methods can be utilised, manually providing a thesaurus for each input document may not be very usable. Therefore, we also studied how to identify the context of an input document automatically, which can\nallow assigning a different context and a corresponding thesaurus automatically. This can be achieved by building a machine learning based classifier, which produce a class label representing the context or a context-specific thesaurus of a given document or its abstract. Once the classifier predicts the context of an input abstract, we identify a thesaurus corresponding to the context, as defined in a context-to-thesaurus look-up table, to inform the AKE method. Unlike the manual approach, the automatic identification allows us to use a different thesaurus for each document in the dataset, therefore can be applied to many real-world scenarios where the documents processed can belong to multiple contexts.\n# 4.3. Wikipedia Named Entities\nBased on the Wikipedia-related observations reported in Section 3.4, we propose to use Wikipedia as a context-independent thesaurus to improve performance of any AKE methods working in any context(s). Similar to how a thesaurus can be used, we prioritise the candidate keywords covered by Wikipedia as an entry by increasing their weight, and we apply exact matching with lemmatisation to identify if a candidate keyword is a Wikipedia named entity. Since Wikipedia also contains a vast amount of entries with too general semantic meanings, e.g., unigrams such as \u2018father\u2019, \u2018school\u2019 and \u2018table\u2019 that are less likely to be a useful keyword for any AKE task, we utilised the NLTK\u2019s words corpus (i.e., a wordlist including common English dictionary words) to identify such unigrams and remove them from the desired Wikipedia entities. For the Wikipedia named entities, we used the 2021-10-01 version of the English Wikipedia dump2, containing only page titles. We firstly cleaned the dump data by removing the disambiguation tags3 added next to the title by Wikipedia. Then, we normalised the data with lemmatisation and lower-casing by following the common practice.\n# 5. Experiments and Results\n# 5.1. Evaluation Metrics\nAs the evaluation metrics, we used precision, recall and F1-score at top ten keywords, which have been commonly used in AKE evaluation (Pa-\npagiannopoulou and Tsoumakas, 2020). Furthermore, we adopted microaveraging, and exact matching with stemming when calculating the scores.\n# 5.2. Selecting Baseline Methods\n5.2. Selecting Baseline Methods To show the effectiveness and generalisability of the proposed methods, we first attempted to identify some representative AKE algorithms with different key characteristics for our experiments. We reviewed existing AKE algorithms in terms of multiple aspects, e.g., recency, ease of reconfiguration, and if they already use one or more of our proposed methods by any means, as shown in Table 5. These methods are considered more representative because they have open-source implementations, are applicable to any document type, were validated on a number of datasets, and do not require training (i.e., unsupervised so that it is easier to use and less likely to have generalisation problems)4. Among these methods, we selected two statistical methods, i.e., KP-Miner and YAKE!, two graph-based methods, i.e., RaKUn and LexRank, and an embedding-based method, i.e., SIFRank+, as baseline methods for our experiments. Since SIFRank+ is very computationally costly, we used only seven of the datasets, containing shorter documents (i.e., KPCrowd, DUC-2001, Inspec, KDD, KPTimes, SemEval2017 and WWW) for its evaluation when our methods were applied. For the implementations of the selected AKE methods, we utilised the PKE (Boudin, 2016) library for KP-Miner, and the original implementations of the other four. We used the default parameters for all the methods, except the maximum n-gram size parameter. Considering the n-gram size across the datasets being mostly limited up to 3, as mentioned in Section 3.3, we set the maximum n-gram size to be 3.\n# 5.3. PoS-Tag Patterns\nAs the first step, we applied our PoS-tagging-based post-processing approach to the selected AKE methods, and evaluated on all the datasets. Results show that the proposed approach improved all the methods except SIFRank+ on the average, in terms of precision, recall, and F1-score. While KP-Miner achieved a better performance for 14 of the 17 datasets with an average of 6.08% in F1-score, RaKUn was improved by a cross-dataset average of 4.46% for 14 of the 17 datasets. We observed the most change in\nTable 5: An overview of some existing open-source unsupervised AKE methods, showing a number of key characteristics.\nMethod\nEasy to\nPoS-tagging\nThesaurus\nWikipedia\nReconfigure\nStatistical Methods\nKP-Miner (El-Beltagy and Rafea, 2009)\n\u2713\n\u2013\n\u2013\n\u2013\nYAKE! (Campos et al., 2020)\n\u2713\n\u2013\n\u2013\n\u2013\nLexSpec (Ushio et al., 2021)\n\u2713\n\u2713\n\u2013\n\u2013\nGraph-based Methods\nTextRank (Mihalcea and Tarau, 2004)\n\u2713\n\u2713\n\u2013\n\u2013\nSingleRank (Wan and Xiao, 2008)\n\u2713\n\u2713\n\u2013\n\u2013\nRAKE (Rose et al., 2010)\n\u2713\n\u2013\n\u2013\n\u2013\nRaKUn (\u02c7Skrlj et al., 2019)\n\u2713\n\u2013\n\u2013\n\u2013\nLexRank (Ushio et al., 2021)\n\u2713\n\u2713\n\u2013\n\u2013\nTFIDFRank (Ushio et al., 2021)\n\u2713\n\u2713\n\u2013\n\u2013\nEmbeddings-based Methods\nEmbedRank (Bennani-Smires et al., 2018)\n\u2713\n\u2713\n\u2013\n\u2713\nSIFRank (Sun et al., 2020)\n\u2713\n\u2713\n\u2013\n\u2713\nSIFRank+ (Sun et al., 2020)\n\u2713\n\u2713\n\u2013\n\u2713\nMDERank (Zhang et al., 2022)\n\u2013\n\u2713\n\u2013\n\u2713\nthe performance of YAKE! \u2013 it was improved in 16 of the 17 datasets by a cross-dataset average of 18.05%. We believe this is because YAKE! does not benefit from linguistic features as a more language-independent (multilingual) approach. Finally, we observed a limited improvement in the scores of LexRank (0.84% on average) for 12 of the 17 datasets, and a slight decrease in the performance of SIFRank+, which is likely due to the fact that these two methods already use a PoS-tagging-based filtering. The obtained scores for YAKE! and SIFRank+ are shown in Tables 6 and 7, respectively, as examples. These results provide new evidence for the effectiveness of PoStagging in AKE algorithms, and imply that there is still room to improve the use of PoS-tagging in many AKE methods. Finally, we studied how tailoring the selected PoS-tag patterns according to domain-specific needs may affect the performance of AKE methods. To this end, we considered the example given in Section 4.1, i.e., the observation that gerunds are rarely seen as a keyword in the health domain. We selected the health datasets (i.e., PubMed and Schutz2008) from our collection and applied the tailored PoS-tag-based filtering that disregards gerunds. For this experiment, we used YAKE! since it is more sensitive to linguistic-based improvements as a language-independent algorithm. As shown in Table 8, the tailored filtering approach provided some small improvement to our original\n<div style=\"text-align: center;\">Table 6: Comparison of the precision, recall, and F1-score of the original YAKE! and the one utilising PoS-tagging, at 10 extracted keywords</div>\nDataset\nYAKE!\nYAKE!+PoS\nP%\nR%\nF1%\nP%\nR%\nF1%\nKPCrowd\n24.20\n4.92\n8.17\n33.98\n6.90\n11.47\nciteulike180\n23.11\n13.27\n16.86\n25.68\n14.74\n18.73\nDUC-2001\n12.01\n14.87\n13.29\n17.44\n21.58\n19.29\nfao30\n22.00\n6.83\n10.42\n25.33\n7.86\n12.00\nfao780\n11.93\n14.95\n13.27\n13.18\n16.52\n14.67\nInspec\n19.82\n14.05\n16.44\n24.57\n17.41\n20.38\nKDD\n6.01\n14.68\n8.53\n5.83\n14.23\n8.27\nKPTimes\n7.97\n15.83\n10.61\n11.37\n22.58\n15.12\nKrapivin2009\n9.54\n17.88\n12.44\n9.93\n18.61\n12.95\nNguyen2007\n19.00\n15.82\n17.26\n19.19\n15.98\n17.43\nPubMed\n7.28\n5.11\n6.01\n8.66\n6.08\n7.15\nSchutz2008\n37.29\n8.06\n13.26\n47.63\n10.30\n16.93\nSemEval2010\n20.37\n13.08\n15.93\n20.82\n13.37\n16.28\nSemEval2017\n20.61\n11.91\n15.10\n29.41\n17.00\n21.55\ntheses100\n9.40\n14.09\n11.28\n10.50\n15.74\n12.60\nwiki20\n19.50\n5.49\n8.57\n22.00\n6.20\n9.67\nWWW\n6.49\n13.47\n8.76\n6.58\n13.66\n8.88\nAvg. Score (%)\n16.27\n12.02\n12.13\n19.54\n14.04\n14.32\nImprovement (%)\n20.10\n16.81\n18.05\n<div style=\"text-align: center;\">Table 7: Comparison of the precision, recall, and F1-score of the original SIFRank+ and the one utilising PoS-tagging, at 10 extracted keywords</div>\nTable 7: Comparison of the precision, recall, and F1-score of the original SIFRan the one utilising PoS-tagging, at 10 extracted keywords\nDataset\nSIFRank+\nSIFRank+ + PoS\nP%\nR%\nF1%\nP%\nR%\nF1%\nKPCrowd\n26.08\n5.30\n8.81\n26.20\n5.32\n8.85\nDUC-2001\n28.34\n35.09\n31.36\n27.86\n34.49\n30.82\nInspec\n35.68\n25.29\n29.60\n35.10\n24.88\n29.12\nKDD\n5.68\n13.87\n8.06\n4.42\n10.80\n6.28\nKPTimes\n7.92\n15.74\n10.54\n7.74\n15.37\n10.30\nSemEval2017\n41.66\n24.08\n30.52\n40.16\n23.21\n29.42\nWWW\n6.59\n13.69\n8.90\n5.26\n10.93\n7.10\nAvg. Score (%)\n21.71\n19.01\n18.26\n20.96\n17.86\n17.41\nImprovement (%)\n-3.45\n-6.05\n-4.65\nfiltering proposal in terms of precision, recall, and F1-score. The limited improvement is likely due to the small percentage of gerunds as candidate keywords.\n# 5.4. Context-Aware Thesauri\nFor this step, we selected 10 datasets mentioned in Section 3.1 that have a particular context. The included contexts (and datasets) are agri-\n<div style=\"text-align: center;\">Table 8: Comparison of the precision, recall, and F1-score of YAKE! when the original (PoS) and the tailored (PoS*) filtering approaches are used, at 10 extracted keywords</div>\nDataset\nYAKE!+PoS\nYAKE!+PoS*\nP%\nR%\nF1%\nP%\nR%\nF1%\nPubMed\n8.66\n6.08\n7.15\n8.70\n6.11\n7.18\nSchutz2008\n47.63\n10.30\n16.93\n47.80\n10.34\n17.00\nAvg. Score (%)\n28.15\n8.19\n12.04\n28.25\n8.23\n12.09\nImprovement (%)\n0.36\n0.49\n0.42\nculture (fao30 and fao780), health (PubMed) and computer science (Inspec, Krapivin2009, Nguyen2007, SemEval2010, KDD, Wiki20 and WWW). In addition, we constructed another context-specific dataset, KPTimes-Econ, by extracting economy-related news from the KPTimes dataset, which includes 3,258 news articles. For extracting economy-related news articles, we have looked for the records involving the term \u201ceconomy\u201d in the keyword and/or categories field(s). Based on the 11 datasets, we collected a thesaurus (or something similar, e.g., dictionary, ontology, or wordlist) for each context. More specifically, we used the following thesauri: (i) AGROVOC 2021-07 (Caracciolo et al., 2013) \u2013 a multilingual controlled vocabulary constructed by the Food and Agriculture Organization of the United Nations (FAO), with 844,000 agriculture-related terms including 50,163 English ones; (ii) Medical Subject Headings (MeSH) 2021 (Lipscomb, 2000) \u2013 a thesaurus covering biomedical and health-related terms produced by the National Library of Medicine (NLM), with over 1.4 million terms in English; (iii) Computer Science Ontology (CSO) v3.3 (Salatino et al., 2018) \u2013 a large-scale computer science ontology automatically produced by Klink-2 (Osborne and Motta, 2015) algorithm from 16 million computer science publications, with 14,000 terms; and (iv) STW v9.10 (Kempf and Neubert, 2016) \u2013 a bilingual thesaurus (in English and German) for economics produced by the Leibniz Information Center for Economics (ZBW), with over 20,000 terms including 6,217 English ones. For the initial step aiming to experiment with manual integration, we fed each of the baseline methods with each of the datasets and their corresponding thesaurus depending on the context. As in the previous experiment, SIFRank+ was evaluated on only the datasets with shorter documents, i.e., Inspec, KDD, WWW, and KP-Times-Econ in this case. The experiments showed that the manual integration of context-aware thesaurus improved all\nfive AKE methods in terms of precision, recall, and F1-score significantly for all the datasets. The improvement in F1-score was observed to be 29.03%, 23.88%, 12.85%, 13.19%, and 7.09% for RaKUn, LexRank, YAKE!, KPMiner, and SIFRank+, respectively. Table 9 and Table 10 show more detailed results of the experiment for LexRank and SIFRank+, respectively. The obtained results in this experiment indicates solid evidence of the effectiveness of using context-aware thesaurus to improve the performance of AKE methods.\nTable 9: Comparison of precision, recall, and F1-score of the original LexRank and its enhanced versions with manual (M) and automatic (A) thesaurus integration, at 10 extracted keywords\nDataset\nContext\nLexRank\nLexRank+T (M)\nLexRank+T (A)\nP%\nR%\nF1%\nP%\nR%\nF1%\nP%\nR%\nF1%\nfao30\nAgr.\n20.33\n6.31\n9.63\n30.33\n9.41\n14.36\n\u2014\n\u2014\n\u2014\nfao780\nAgr.\n8.55\n10.72\n9.51\n13.04 16.35 14.51\n\u2014\n\u2014\n\u2014\nInspec\nCS\n30.49 21.61 25.29 31.10 22.04 25.79\n30.97\n21.95\n25.69\nKDD\nCS\n6.07\n14.81\n8.61\n6.23\n15.20\n8.83\n6.25\n15.26\n8.87\nKrapivin2009\nCS\n7.01\n13.14\n9.15\n8.79\n16.48 11.47\n8.74\n16.37\n11.39\nNguyen2007\nCS\n13.25 11.04 12.04 15.69 13.07 14.26\n15.45\n12.87\n14.04\nSemEval2010\nCS\n13.13\n8.43\n10.27 15.10\n9.70\n11.81 15.10\n9.70\n11.81\nwiki20\nCS\n14.00\n3.94\n6.15\n23.00\n6.48\n10.11 23.00\n6.48\n10.11\nWWW\nCS\n6.66\n13.83\n8.99\n6.95\n14.43\n9.38\n6.93\n14.40\n9.36\nPubMed\nHealth\n4.22\n2.96\n3.48\n8.98\n6.31\n7.41\n8.92\n6.26\n7.36\nSchutz2008\nHealth\n28.32\n6.12\n10.07 34.35\n7.43\n12.21\n34.00\n7.35\n12.09\nKPTimes-Econ\nEcon.\n3.27\n7.03\n4.46\n4.09\n8.80\n5.59\n4.09\n8.79\n5.58\nAvg. Score (%)\n12.94\n9.99\n9.80\n16.47 12.14 12.14\n15.35\n11.94\n11.63\nImprovement (%)\n27.28\n21.52\n23.88\n21.44\n16.03\n18.07\nTable 10: Comparison of precision, recall, and F1-score of the original SIFRank+ and its enhanced versions with manual (M) and automatic (A) thesaurus integration, at 10\nTable 10: Comparison of precision, recall, and F1-score of the original SIFRank+ an its enhanced versions with manual (M) and automatic (A) thesaurus integration, at 1 extracted keywords\nDataset\nContext\nSIFRank+\nSIFRank+ + T (M) SIFRank+ + T (A)\nP%\nR% F1%\nP%\nR%\nF1%\nP%\nR%\nF1%\nInspec\nCS\n35.68 25.29 29.60 36.62 25.95\n30.37\n36.03 25.53\n29.88\nKDD\nCS\n5.68 13.87 8.06\n5.97\n14.58\n8.48\n5.95 14.52\n8.44\nWWW\nCS\n6.59 13.69 8.90\n7.32\n15.19\n9.88\n7.27 15.10\n9.81\nKPTimes-Econ\nEcon.\n3.49\n7.50\n4.76\n4.56\n9.81\n6.23\n4.56 9.81\n6.23\nAvg. Score (%)\n12.86 15.09 12.83 13.62 16.38\n13.74\n13.45 16.24\n13.59\nImprovement (%)\n5.91\n8.55\n7.09\n4.59\n7.62\n5.92\nprocess. In our experiments, especially for datasets covering mainly scientific papers, we built a classifier for classifying a given article\u2019s title and abstract into the main discipline the article belongs to. The classifier was trained on samples extracted from the arXiv.org dataset5 containing metadata of over 1.7M preprints in multiple disciplines. Before the training process, we filtered the arXiv.org dataset by the main discipline reflected by its categories field so as to include the following three disciplines: 1) cs (Computer Science, e.g., cs.AI), 2) bio (Biology, e.g., q-bio), and 3) fin (Finance, e.g., q-fin.CP) and econ (Economics, e.g., econ.EM). After this filtering process, we obtained a dataset of 583,796 samples (551,443 computer science, 20,110 biology, and 12,243 finance/economics samples). Since the resulting dataset is highly imbalanced, we applied random downsampling to equate the number of samples from each discipline to the size of the smallest class, 12,243, which made the final size of our training set 36,729. In our classifier, we utilised the TF-IDF vectoriser for feature extraction. We chose to use the calibrated linear support vector classifier (SVC) with the default parameters and the one-vs-rest setting, rather than a multi-class classification method or more advanced feature extraction methods such as BERT, to show that even a lightweight classifier is sufficient for the task of automatic context detection. The classifier was evaluated with a stratified 5-fold cross-validation. The testing accuracies6 of computer science, biology and finance/economics models were 93.2%, 94.9%, and 97.0%, respectively. The classifier can also be extended to support multiple contexts for a single article, although in our experiments we considered the case of a single context per article for the sake of simplicity and clarity. We used the Scikit-learn library (Pedregosa et al., 2011) to implement all of the mentioned components. Since the training set of the classifier does not cover agriculture preprints, and we were unable to find a proper agriculture dataset for training, we excluded the agriculture context and the corresponding datasets, fao30 and fao780, for this part of the experiments. The results of the experiments performed with our classifier indicated that the automatic thesaurus integration approach achieved as good as the manual integration approach with a negligible performance decrease. More precisely, the F1-score was improved by an\naverage of 23.23%, 18.07%, 9.60%, 11.27%, and 5.92% for RaKUn, LexRank, YAKE!, KP-Miner, and SIFRank+, respectively, compared to the baseline scores. Table 9 and Table 10 show more detailed results of the experiment for LexRank and SIFRank+, respectively. The obtained results imply that automatic integration can be generalised to cover more contexts and thesauri, which can be quite useful in real-world AKE applications.\n# 5.5. Wikipedia Named Entities\nFor this part of the experiments, we used the entire set of datasets as we did in Section 3.2. The results of the experiment indicated that leveraging Wikipedia named entities improved the performance of KP-Miner and RaKUn for 16 of the datasets, and the performance of YAKE! and LexRank for all the datasets, in terms of all the evaluation metrics. Furthermore, the average improvement rates of the F1-score were observed as 18.83%, 11.11%, 10.96%, and 10.11% for RaKUn, LexRank, YAKE!, and KP-Miner, respectively. However, we observed a slight decrease in the average F1-score of SIFRank+, although it improved for most (5 out of 7) of the datasets, which may be explained by its underlying sentence embedding approach, SIF (Arora et al., 2017), which already leverages Wikipedia for pre-training and fine-tuning. Table 11 and Table 12 show more detailed results for RaKUn and SIFRank+ as examples.\n# 5.6. Combining Post-Processing Steps\nIn the final part of our experiments, we tried combining multiple postprocessing steps to improve the performance further. With this respect, we tried to apply all the combinations of the three proposed enhancements. The generated heatmaps from the F1@10 scores and the percentages of improved cases with different combinations for each baseline method can be seen in Figure 1. The results show that the best F1 scores for YAKE!, RaKUn, and KP-Miner were obtained when all the proposed post-processing steps were applied. For LexRank and SIFRank+, however, the best combination was integrating context-aware thesaurus and Wikipedia since they already benefited from PoS-tagging-based filtering. In addition, the applied postprocessing steps improved the baselines significantly \u2013 the improvement rate reached up to 23.7% for YAKE!, 21.3% for KP-Miner, 53.8% for RaKUn, 20.1% for LexRank, and 10.2% for SIFRank+. Finally, the improvements were consistent \u2013 at least one combination of the post-processing steps was observed for each method, resulting in a higher performance across all the\n<div style=\"text-align: center;\">Table 11: Comparison of precision, recall, and F1-score of the original RaKUn and its enhanced versions with Wikipedia, at 10 extracted keywords</div>\nDataset\nRaKUn\nRaKUn+Wiki\nP%\nR%\nF1%\nP%\nR%\nF1%\nKPCrowd\n42.52\n8.64\n14.36\n42.64\n8.66\n14.40\nciteulike180\n16.56\n9.50\n12.08\n17.92\n10.29\n13.07\nDUC-2001\n5.68\n7.03\n6.29\n6.17\n7.64\n6.82\nfao30\n15.00\n4.65\n7.10\n18.67\n5.79\n8.84\nfao780\n6.50\n8.14\n7.23\n7.64\n9.57\n8.50\nInspec\n6.54\n4.64\n5.43\n6.74\n4.77\n5.59\nKDD\n3.66\n8.92\n5.19\n3.63\n8.86\n5.15\nKPTimes\n8.07\n16.03\n10.74\n8.15\n16.18\n10.84\nKrapivin2009\n2.77\n5.20\n3.62\n4.94\n9.26\n6.44\nNguyen2007\n6.79\n5.66\n6.17\n9.67\n8.05\n8.78\nPubMed\n4.30\n3.02\n3.55\n6.58\n4.62\n5.43\nSchutz2008\n33.14\n7.16\n11.78\n40.09\n8.67\n14.25\nSemEval2010\n6.75\n4.33\n5.28\n10.04\n6.45\n7.85\nSemEval2017\n11.42\n6.60\n8.37\n11.74\n6.79\n8.60\ntheses100\n3.90\n5.85\n4.68\n4.80\n7.20\n5.76\nwiki20\n9.50\n2.68\n4.18\n19.50\n5.49\n8.57\nWWW\n4.32\n8.98\n5.84\n4.39\n9.12\n5.93\nAvg. Score (%)\n11.02\n6.88\n7.17\n13.14\n8.08\n8.52\nImprovement (%)\n19.24\n17.44\n18.83\n<div style=\"text-align: center;\">Table 12: Comparison of the precision, recall, and F1-score of the original SIFRank+ and the one utilising Wikipedia named entities, at 10 extracted keywords</div>\nTable 12: Comparison of the precision, recall, and F1-score of the original SIFRa the one utilising Wikipedia named entities, at 10 extracted keywords\nDataset\nSIFRank+\nSIFRank+ + Wiki\nP%\nR%\nF1%\nP%\nR%\nF1%\nKPCrowd\n26.08\n5.30\n8.81\n27.46\n5.58\n9.27\nDUC-2001\n28.34\n35.09\n31.36\n22.82\n28.26\n25.25\nInspec\n35.68\n25.29\n29.60\n36.60\n25.94\n30.36\nKDD\n5.68\n13.87\n8.06\n6.11\n14.90\n8.66\nKPTimes\n7.92\n15.74\n10.54\n9.22\n18.31\n12.26\nSemEval2017\n41.66\n24.08\n30.52\n41.34\n23.89\n30.28\nWWW\n6.59\n13.69\n8.90\n7.50\n15.57\n10.12\nAvg. Score (%)\n21.71\n19.01\n18.26\n21.58\n18.92\n18.03\nImprovement (%)\n-0.60\n-0.47\n-1.26\ndatasets. The results showed that even for more modern AKE methods there is still room for improvement using simple post-processing steps like those proposed in this paper.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b59c/b59c9691-e4ee-4ab5-a14a-303d165f38f6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) SIFRank+</div>\nFigure 1: Average improvements in F1 scores across all the datasets (upper side), an percentages of the improved cases across all the datasets (bottom side), for different AK methods. (B: Baseline, P: PoS-tagging, T: Thesaurus integration, W: Wikipedia integra tion)\n# 6. Further Discussions\nThe proposed post-processing steps in this study were applied to five SOTA AKE methods, showing their universality to work with any AKE methods. PoS-tagging can be easily integrated to AKE methods to implement a filtering mechanism. However, it should be separately considered for each dataset since AKE datasets lack linguistic standards for golden keywords. This can significantly increase the accuracy of the AKE methods\nbenefiting from PoS-tagging. Thesaurus and Wikipedia integration can also be applied to any AKE method without much effort. Considering that a text document can cover multiple contexts, the results we reported can be further improved by integrating multiple contexts. This can be achieved by utilising a multi-label classifier. Since one-vs-rest classifiers can be used for multi-label classification, our classifier can be refined to cover multiple contexts. In addition, more advanced models, such as BERT, can be utilised to develop a more accurate classifier. It is also worth noting that two of the proposed post-processing steps in this study were selected as representative examples of semantic elements. Other semantic elements can also be used to further improve the performance of AKE methods. Although our experiments on the proposed post-processing steps are based on English NLP tools, they can also be applied to multilingual AKE methods, e.g., YAKE!, for any language. The language of input documents can be identified automatically with a language identifier, which can achieve a high accuracy for many languages (Jauhiainen et al., 2019). Then, the corresponding PoS-tagger and Wikipedia data can be utilised, although the set of acceptable PoS-tag patterns will need updating according to the identified language. Nevertheless, utilising a context-aware thesaurus could be tricky for some languages especially small ones as there might be no thesaurus relevant to the context of the document in the identified language. This study has a number of limitations that can be addressed in future works. Firstly, the selected baseline AKE methods are just examples of SOTA methods so may not be sufficiently representative. As our focus was improving AKE methods in general, we did not aim to achieve the best scores among the studies on AKE. As a result, this study is limited to opensource, unsupervised, and general-purpose AKE methods. In addition, this study leveraged multiple elements for English language, and used English datasets for evaluation. Therefore, it disregarded non-English settings, which are needed especially for multilingual AKE methods, such as YAKE!. Besides, the proposed mechanisms have been applied separately throughout the experiments. Therefore, the results could be improved further if different mechanisms benefit from each other (e.g., applying PoS-tag-based filtering to Wikipedia integration mechanism to disregard the Wikipedia named entities that cannot be a keyword). Finally, a better matching strategy considering word ambiguities can be developed for checking if a candidate keyword appears in a thesaurus or Wikipedia, with the help of techniques such as word sense disambiguation.\n# 7. Conclusion\nAKE has a more important role in IR and NLP with the increasingly vast amount of digital textual data that modern systems process. In this paper, we aimed to show that an enhanced level of semantic-awareness supported by PoS-tagging can improve AKE algorithms. We selected five algorithms as the baseline methods upon experiments comparing several state-of-the-art AKE methods. Then, we used PoS-tagging, integrated thesauri and Wikipedia named entities for improving the baselines. Our experiments on 17 English datasets indicated that the three proposed mechanisms improved the baseline algorithms significantly and consistently.\n# Acknowledgements\nAcknowledgements We would like to thank Ricardo Campos for clarification and additional information about the YAKE! algorithm. The first author E. Altuncu was supported by funding from the Ministry of National Education, Republic of Turkey, under the grant number MoNE-YLSY-2018.\n# References\nArora, S., Liang, Y., Ma, T., 2017. A simple but tough-to-beat baseline for sentence embeddings, in: Proceedings of the 2017 International Conference on Learning Representations (ICLR \u201917), pp. 1\u201316. URL: https://open review.net/forum?id=SyK00v5xx.\nMartinc, M., \u02c7Skrlj, B., Pollak, S., 2021. TNT-KID: Transformer-based neural tagger for keyword identification. Natural Language Engineering , 1\u2013 40doi:10.1017/S1351324921000127.\nMarujo, L., Gershman, A., Carbonell, J., Frederking, R., Neto, J.P., 2013. Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization. arXiv:1306.4886 [cs.CL]. doi:10.48550/arXiv.1306.4886.\nSalatino, A.A., Thanapalasingam, T., Mannocci, A., Osborne, F., Motta, E., 2018. The Computer Science Ontology: A large-scale taxonomy of research areas, in: The Semantic Web: Proceedings of the 17th International Semantic Web Conference (ISWC \u201918), Part II, Springer. pp. 187\u2013205. doi:10.1007/978-3-030-00668-6 12.\nSchutz, A.T., 2008. Keyphrase Extraction from Single Documents in the Open Domain Exploiting Linguistic and Statistical Methods. Master\u2019s thesis. National University of Ireland, Galway. URL: https://citeseer x.ist.psu.edu/viewdoc/download?doi=10.1.1.394.5372&rep=rep1&t ype=pdf. Shi, T., Jiao, S., Hou, J., Li, M., 2008. Improving keyphrase extraction using wikipedia semantics, in: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application, IEEE. pp. 42\u201346. doi:10.1109/IITA.2008.211. Smadja, F., 1993. Retrieving collocations from text: Xtract. Computational Linguistics 19, 143\u2013178. URL: https://aclanthology.org/J93-1007. pdf. Sun, Y., Qiu, H., Zheng, Y., Wang, Z., Zhang, C., 2020. SIFRank: A new baseline for unsupervised keyphrase extraction based on pre-trained language model. IEEE Access 8, 10896\u201310906. doi:10.1109/ACCESS.202 0.2965087. Ushio, A., Liberatore, F., Camacho-Collados, J., 2021. Back to the basics: A quantitative analysis of statistical and graph-based term weighting schemes for keyword extraction, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP \u201921), ACL. pp. 8089\u20138103. doi:10.18653/v1/2021.emnlp-main.638. Wan, X., Xiao, J., 2008. Single document keyphrase extraction using neighborhood knowledge, in: Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2, AAAI. pp. 855\u2013860. URL: https://www.aaai.org/Papers/AAAI/2008/AAAI08-136.pdf. Wang, Y., Liu, Q., Qin, C., Xu, T., Wang, Y., Chen, E., Xiong, H., 2018. Exploiting topic-based adversarial neural network for cross-domain keyphrase extraction, in: Proceedings of the 2018 IEEE International Conference on Data Mining (ICDM \u201918), IEEE. pp. 597\u2013606. doi:10.1109/ICDM.2018. 00075. Witten, I.H., Paynter, G.W., Frank, E., Gutwin, C., Nevill-Manning, C.G.,\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving Automatic Keyword Extraction (AKE) methods by integrating semantic awareness through PoS-tagging, filling the gap in existing methods that do not adequately utilize semantic information.",
        "problem": {
            "definition": "The problem is the inconsistent performance of AKE methods, which struggle to effectively extract keywords that accurately represent the content of documents due to a lack of semantic context.",
            "key obstacle": "The main challenge is the absence of a precise definition of keywords and the lack of consistent evaluation metrics across different AKE methods, making performance comparisons difficult."
        },
        "idea": {
            "intuition": "The idea originated from observing that human-labeled keywords often follow specific PoS-tag patterns and semantic contexts, suggesting that these elements can enhance AKE methods.",
            "opinion": "The proposed method involves a post-processing approach that applies PoS-tagging and integrates context-aware thesauri and Wikipedia named entities to improve keyword extraction.",
            "innovation": "This method is innovative as it provides a universal enhancement applicable to any AKE method, distinguishing it from existing methods that do not utilize semantic information effectively."
        },
        "method": {
            "method name": "Post-Processing Keyword Extraction Enhancement",
            "method abbreviation": "PKEE",
            "method definition": "PKEE is a universal approach that enhances AKE methods by applying semantic-awareness through PoS-tagging and the integration of context-specific thesauri and Wikipedia.",
            "method description": "The core of the method involves filtering candidate keywords based on PoS-tags, prioritizing keywords from relevant thesauri, and enhancing keyword selection using Wikipedia named entities.",
            "method steps": [
                "Filter candidate keywords based on specified PoS-tag patterns.",
                "Utilize context-specific thesauri to prioritize candidate keywords.",
                "Enhance keyword selection by identifying Wikipedia named entities."
            ],
            "principle": "The effectiveness of this method lies in its ability to align extracted keywords with the semantic context of the input document, leveraging linguistic features and external knowledge."
        },
        "experiments": {
            "experiments setting": "The experiments were conducted on 17 datasets across various domains, comparing the performance of five state-of-the-art AKE methods before and after applying the proposed enhancements.",
            "evaluation method": "The performance was evaluated using precision, recall, and F1-score metrics, focusing on the top ten extracted keywords."
        },
        "conclusion": "The experiments demonstrated that the proposed enhancements significantly improved the performance of AKE methods, with average improvements in F1-score ranging from 10.2% to 53.8%, confirming the effectiveness of integrating semantic-awareness into AKE.",
        "discussion": {
            "advantage": "The key advantages include improved accuracy in keyword extraction, the ability to apply the method universally across different AKE algorithms, and the enhancement of existing methods without requiring extensive retraining.",
            "limitation": "The limitations include the reliance on the quality of the thesauri and Wikipedia data, and the potential for decreased performance in methods already optimized for PoS-tagging.",
            "future work": "Future research could explore the integration of additional semantic elements, the application of the method to multilingual AKE, and the development of more sophisticated context identification techniques."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge the support from the Ministry of National Education, Republic of Turkey, and thank Ricardo Campos for his contributions regarding the YAKE! algorithm."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The importance of integrating semantic awareness through PoS-tagging in Automatic Keyword Extraction (AKE) methods significantly advances the accuracy of keyword extraction in natural language processing."
        },
        {
            "section number": "2.2",
            "key information": "Fine-tuning is relevant in the context of adapting AKE methods to effectively utilize semantic context, enhancing their performance."
        },
        {
            "section number": "3.2",
            "key information": "The proposed method, Post-Processing Keyword Extraction Enhancement (PKEE), improves model performance by applying semantic-awareness through PoS-tagging and context-specific thesauri."
        },
        {
            "section number": "3.3",
            "key information": "A key challenge in AKE methods is the inconsistent performance due to a lack of semantic context, which the proposed method aims to address."
        },
        {
            "section number": "4.2",
            "key information": "The integration of semantic context in AKE methods influences the effectiveness and accuracy of keyword extraction, showcasing the importance of prompt engineering in enhancing AKE."
        },
        {
            "section number": "8.1",
            "key information": "Current challenges include the reliance on the quality of thesauri and Wikipedia data, which could hinder the performance of the proposed enhancements."
        },
        {
            "section number": "8.4",
            "key information": "Future research could explore the application of PKEE to multilingual AKE and the integration of additional semantic elements to enhance keyword extraction further."
        }
    ],
    "similarity_score": 0.5578588157816465,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0858_fine-/papers/Improving Performance of Automatic Keyword Extraction (AKE) Methods Using PoS-Tagging and Enhanced Semantic-Awareness.json"
}