{
    "from": "google",
    "scholar_id": "2MR7UtYO5cQJ",
    "detail_id": null,
    "title": "Can large language models explain themselves? a study of llm-generated self-explanations",
    "abstract": "\nAbstract\u2014Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce \u201chelpful\u201d responses, they can and often will produce explanations along with the response, which we call selfexplanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentimentladen words such as \u201cfantastic\u201d and \u201cmemorable\u201d in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT\u2019s selfexplanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs. Index Terms\u2014Machine Learning, Natural Language Processing, Interpretability, Large Language Model (LLM)\nI. INTRODUCTION\nOver the past couple years, instruction-tuned [30] large language models (LLMs) have revolutionized the field of natural language processing (NLP) with its superior performance on a wide variety of tasks and benchmarks such as senti",
    "bib_name": "huang2023can",
    "md_text": "# Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations\nyuan Huang\u2217 Siddarth Mamidanna\u2217 Shreedhar Jangam Yilun Zhou Leilani H. Gilpin  Santa Cruz UC Santa Cruz UC Santa Cruz MIT CSAIL UC Santa Cruz {shuan101, spmamida, sjangam}@ucsc.edu, yilun@csail.mit.edu, lgilpin@ucsc.edu\nAbstract\u2014Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce \u201chelpful\u201d responses, they can and often will produce explanations along with the response, which we call selfexplanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentimentladen words such as \u201cfantastic\u201d and \u201cmemorable\u201d in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT\u2019s selfexplanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs. Index Terms\u2014Machine Learning, Natural Language Processing, Interpretability, Large Language Model (LLM)\nI. INTRODUCTION\nOver the past couple years, instruction-tuned [30] large language models (LLMs) have revolutionized the field of natural language processing (NLP) with its superior performance on a wide variety of tasks and benchmarks such as sentiment analysis [44], mathematical reasoning [14] and summarization [48]. Commercial models such as ChatGPT and GPT-4 by OpenAI, Bard by Google and Claude by Anthropic have been used to (partially) automate many human labor-intensive processes such as education [22], trip planning [46] and customer service [15]. A notable feature of these models is the ability to provide (seemingly) reasonable explanations. For example, when asked to solve a math problem, they often include detailed derivation steps even when not explicitly instructed to do so. Similarly, when asked to analyze the sentiment of a movie or book review, they often spontaneously explain their decision with supporting evidence, such as by highlighting sentiment-laden words or explaining the sarcasm in the text.\nIs this review positive? Why or why not?  \u201cA fantastic movie directed by the famous  Lucas Johnson, who has a track-record of  producing terrific novel adaptions.\u201d\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d20/7d205132-eb3e-4f3f-a4e3-f1a94f600c48.png\" style=\"width: 50%;\"></div>\nFig. 1. An overview of our investigation. Current conversational LLMs can explain their answers (e.g., by highlighting important words in the input), often automatically or at least when asked to. How should we think of these self-explanations? In this paper, we study them in relationship to traditional model interpretability techniques such as occlusion saliency and LIME, and on various metrics such as comprehensiveness, sufficiency and rank agreement. Our findings suggest that we may need to rethink the model interpretability pipeline for analyzing these models.\nHow should we think about these explanations? In particular, do these explanations faithfully support the model\u2019s prediction (e.g., the final answer of the math problem or the positive/negative judgment in sentiment analysis), or do they simply give a sense of transparency while being decoupled from the prediction? To what extent are they similar to or different from the explanations generated from traditional techniques [e.g. 24, 25, 28, 34, 37, 49], which are commonly used to study predecessor models such as BERT [12] and GPT2 [33]? For any \u201cnegative results\u201d about the effectiveness of these explanations, should we attribute them to the model\u2019s inability to generate the explanations, or the limitations in our evaluations [13, 23] about them? In this paper, we seek answers to the above questions, using OpenAI\u2019s ChatGPT as the model, feature attributions as the explanation type and sentiment analysis as the task. All three design choices are made based on popularity in the respective aspect: ChatGPT is arguably the most widely used instructiontuned LLM, feature attribution (also known as saliency map or feature importance) explanation is by far the most popular\ntype of explanations studied in the interpretability literature, and sentiment analysis is often the first task to consider when benchmarking these explanations due to the intuitive alignment between strong sentiment words (e.g, \u201cfantastic\u201d or \u201cterrible\u201d) with feature attribution scores. In the experiment, we study two paradigms of generating the explanations: generating the explanation first before the prediction (explain-then-predict or E-P), versus generating the prediction first and then explaining it (predict-and-explain or P-E) [7]. In each paradigm, we construct two methods to eliciting the explanations: asking the model to generate a full list of feature attribution explanations containing the importance score of every word, and asking the model to just return the top-k most important words. We then compare them with two classic interpretability methods, occlusion [25] and LIME [34]. The comparison is done on two fronts, a suite of faithfulness evaluation metrics [51] and a set of disagreement measurements among explanations [23]. Quantitatively, the model\u2019s self-explanations perform on par with traditional methods via faithfulness evaluations, but at the same time differ greatly from them (on various agreement metrics). More importantly, we find that the classic interpretability pipeline of defining and evaluating model explanations may be fundamentally ill-suited for these LLMs with quite human-like reasoning ability. Specifically, the model does not produce sufficiently fine-grained model predictions for us to meaningfully compute and evaluate explanations, in ways that we have been done for predecessor models such as BERT [12]. Nonetheless, we believe that this should be considered as a limitation on the current interpretability practice, rather than the model, as many such computations (e.g., identifying the effect of removing a single word in a long sentence) are ill-posed for humans as well. Overall, we hope that this work could bring formal investigations of LLM-generated self-explanations to the attention of the community, and inspire more work towards better understanding and using them.\n# II. RELATED WORK\nOur research lies at the intersection of three areas within interpretability research: feature attribution explanations, evaluations of feature attribution explanations and LLM-generated self-explanations. The former two areas are well-studied, so we give emphasis to works that are directly used in our paper. For broader contexts and more comprehensive coverage, we refer the reader to the textbook by Molnar [29]. The last area is relatively new, and while, to the best of our knowledge, we are the first to rigorously study LLM-generated feature attribution explanations, we briefly discuss some related ideas.\n# A. Feature Attribution Explanations\nAt a high level, feature attribution explanations assign attribution scores to input features [52]. There are in general two ways of defining input features. The first is based on the model\u2019s response to infinitesimal perturbation of the input feature value, with representative methods such as the gradient saliency [37], smooth gradient [39] and integrated gradient\n[41]. For NLP models, the perturbation is often done in the embedding space [24]. However, since ChatGPT is a closedsource model that does not provide input gradients, none of these explanations are possible to calculate. Instead, we focus on the second approach: defining feature importance based on a model\u2019s response to inputs with various features removed. The simplest definition is the occlusion saliency [25, 49], with the importance of a feature (i.e., word) defined as the change in the model prediction on the sentence with that word removed. To capture non-linear interactions among different words, LIME [34] and SHAP [28] remove multiple features at the same time and define feature importance as linear regression coefficients or the game-theoretic Shapley value [35] respectively. The word removal strategy can be influential, with Pham et al. [31] finding that some strategies favor some evaluations. In this paper, we adopt the mainstream approach of word deletion (i.e., removing the word \u201cThis\u201d from the sentence \u201cThis is a great movie\u201d results in \u201cis a great movie\u201d), and leave the exploration of other strategies (e.g., mask infilling) to future work.\n# B. Evaluations of Feature Attribution Explanations\nOur main contribution of this paper is an investigation into the relative strengths and weaknesses of LLM-generated self-explanations compared to traditional explanations such as occlusion and LIME. In this paper, we evaluate both their faithfulness and agreement with each other. For faithfulness evaluations, we employ several popular metrics, including comprehensiveness [13], sufficiency [13], decision flip rate under the removal of the most important token/word [10], fraction of tokens need to be removed for decision flip [36] and deletion rank correlation [3]. They all subscribe to a fundamental principle \u2013 modifying a significant feature, as identified by the explanation, should markedly influence the prediction \u2013 but implement it differently. Furthermore, recognizing that there is no \u201cground truth\u201d model reasoning available (due to the black-box nature of these models), one type of evaluation modifies the dataset and retrains the model [1, 2, 6, 47, 52], such that the model has to use a specific type of reasoning for high accuracy, and then evaluates whether the saliency maps of such high-accuracy models align with the expectation. Given the high cost and limited flexibility (API access only) of finetuning, we leave these evaluations to future work. We also study the correlation between the LLM selfexplanation and traditional techniques using the set of disagreement metrics introduced by Krishna et al. [23]. Additional manifestation of the disagreement problem includes the finding that the explanations themselves are often unstable [16] and that they are very sensitive to hyperparameter choices [5].\n# C. LLM-Generated Self-Explanations\nGenerative models, those that can generate texts such as the GPT family of models [32, 33], offer a distinct way of explaining them \u2013 by simply generating the explanations directly. Camburu et al. [7] trained models on supervised\npairs of labels and explanations and considered two orderings of prediction and explanation: making a prediction and then explaining it, and generating an explanation and using it to make a prediction. We study both approaches in our paper. In addition, Camburu et al. [7] found that training the model to generate the explanation, both before and after the prediction, results in a slight accuracy drop, casting doubt on the inherent usefulness of the explanation to the prediction. For ChatGPT and later models, chain-of-thought generation [45], which can be considered as the model\u2019s explanation, is often helpful for the accuracy of the final answer, especially on complex reasoning tasks such as solving math problems [18]. This evidence implies that the model could meaningfully reason from such explanations. By contrast, [8] studied properties of self-generated counterfactual explanations, proposed several metrics and found that these explanations are often misleading and lead to the wrong mental model of human inspectors.\nOur main contribution in this paper is a suite of systematic analyses of LLM-generated self-explanations in the sentiment analysis domain. We first covers the basics of auto-regressive LLMs (such as ChatGPT and similar conversational models), and then describe our prompting strategy and considerations. After that, we introduce the two traditional interpretability methods that we compare to, and also evaluation metrics.\n# A. Auto-regressive LLMs\nAuto-regressive LLMs, as pioneered by GPT-1 [32] and GPT-2 [33], operate by successively generating the next word (or token) given the history (called context window), until a special stop token is generated. The generation can be\nTABLE I THE PROMPT AND AN EXAMPLE QUERY OF THE E-P SETUP. BLACK TEXT IN NORMAL FONT IS PROVIDED LITERALLY. Gray text in italics IS CHANGED FOR EVERY QUERY. BLUE TEXT IN BOLDFACE IS THE MODEL\u2019S OUTPUT.\nRole\nText\nSystem\nYou are a creative and intelligent movie review analyst, whose purpose is to aid in sentiment analysis of movie reviews. You will receive\na review, and you must analyze the importance of each word and punctuation in Python tuple format: (<word or punctuation>, <float\nimportance>). Each word or punctuation is separated by a space. The importance should be a decimal number to three decimal places\nranging from -1 to 1, with -1 implying a negative sentiment and 1 implying a positive sentiment. Provide a list of (<word or punctuation>,\n<float importance>) for each and every word and punctuation in the sentence in a format of Python list of tuples. Then classify the review\nas either 1 (positive) or 0 (negative), as well as your confidence in the score you chose and output the classification and confidence in the\nformat (<int classification>, <float confidence>). The confidence should be a decimal number between 0 and 1, with 0 being the lowest\nconfidence and 1 being the highest confidence.\nIt does not matter whether or not the sentence makes sense. Do your best given the sentence.\nThe movie review will be encapsulated within <review> tags. However, these tags are not considered part of the actual content of\nthe movie review.\nExample output:\n[(<word or punctuation>, <float importance>), (<word or punctuation>, <float importance>), ... ]\n(<int classification>, <float confidence>)\nUser\n<review> Offers that rare combination of entertainment and education . <review>\nAssistant\n[(\u2018Offers\u2019, 0.500), (\u2018that\u2019, 0.000), (\u2018rare\u2019, 0.500), (\u2018combination\u2019, 0.000), (\u2018of\u2019, 0.000), (\u2018entertainment\u2019, 0.750), (\u2018and\u2019, 0.000),\n(\u2018education\u2019, 0.750), (\u2018.\u2019, 0.000)]\n(1, 1.000)\nstochastic or deterministic, and various decoding strategies could be used to improve the quality of the generation [19]. In all of our experiments, we use deterministic greedy decoding (i.e., with temperature = 0). Thus, the generation process can be modeled as the function\n(1)\nwhere xc is the sequence of context words, and xg is the sequence of generated words. A conversational LLM (i.e., an LLM that can engage in human conversations) typically has three message types: an initial \u201csystem\u201d message to set the general tone and instruction of the conversation, followed by rounds of interleaving \u201cuser\u201d and \u201cassistant\u201d messages. While the exact details of ChatGPT is unknown due to its proprietary nature, similar open-source models such as Alpaca [42] and Vicuna [9] all adopt the same practice to represent the message history: by concatenating individual message prepended with the role indicator. For example, if the model (i.e., assistant) needs to answer the user\u2019s second question below:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab97/ab97226a-8980-4600-be8b-0369705712eb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">the context xc is constructed to be the following text:</div>\nwhich is a concatenation of all of the past messages.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/60ce/60ce9ecb-f83e-41c0-ad30-f6b99ba3756d.png\" style=\"width: 50%;\"></div>\n# \u2018A\u2019, 0.2), (\u2018film\u2019, 0.5), (\u2018that\u2019, 0.1), (\u2018takes\u2019, 0.3), (\u2018you\u2019, 0.4), (\u2018inside\u2019, 0.6), (\u2018the\u2019, 0.1), (\u2018rhythms\u2019, 0.7), (\u2018of\u2019, 0.1), (\u2018its\u2019, 0.1), subject\u2019, 0.5), (\u2018:\u2019, 0.1), (\u2018You\u2019, 0.4), (\u2018experience\u2019, 0.6), (\u2018it\u2019, 0.3), (\u2018as\u2019, 0.2), (\u2018watch\u2019, 0.4), (\u2018.\u2019, 0.1)]\n# B. Prompting\nOur central goal is to have the model generate a prediction on the sentiment of the input sentence and a feature attribution explanation for the prediction. We include both explanation pipeline setups studied by Camburu et al. [7], explain-then-predict (E-P), in which the model first generates the explanation and then arrive at a prediction based on it, and predict-and-explain (P-E), in which the model first generates the prediction and then explains it. Our prompts for E-P and P-E setups are shown in Table I and II respectively. In the design of both prompts, we have several considerations. First, in order to minimize the number of turns in the conversation, we instruct the model to output both the explanation and prediction at the same time, but in clearly defined ordering. Second, in some of our experiments, we need to feed the model with partial inputs (i.e., sentences with some words removed) such as \u201cThis is a really great movie with beautiful music and intricate plot\u201d \u21d2\u201cThis a really movie with music and intricate\u201d. We empirically find that models without any guidance sometimes will refuse to\nTABLE III A CHATGPT RESPONSE WITH HUMAN \u201cIN THE WILD,\u201d WHERE CHATGPT AUTOMATICALLY GIVES THE EXPLANATION IN THE FORM OF A FEW HIGHLIGHTED PHRASES.\nRole\nText\nUser\nCan you tell me whether the movie review is positive or negative?\nThe gorgeously elaborate continuation of \"The Lord of the Rings\" trilogy is so huge that a column of words can not adequately describe\nco-writer/director Peter Jackson \u2019s expanded vision of J.R.R. Tolkien \u2019s Middle-earth .\nSystem\nBased on the given text, it appears to be a positive movie review. The phrases \"gorgeously elaborate,\" \"expanded vision,\" and \"Middle-earth\"\nsuggest admiration and appreciation for the film.\nprovide an answer, so we intentionally ask them to give their best guess despite input ambiguity. Finally, we initially had significant difficulty making the model to output the word attribution values in a consistent format, but in the end find that the \u201cPython\u201d format works the best for the model to follow. Despite the success in generating syntactically correct outputs, a concern with this Python format guiding is that the unnaturalness of output (compared to, e.g., Table III) may result in the explanation values also being less faithful to the model\u2019s true reasoning. Thus, to remove any effect of the unnatural prompt, and recognizing that humans, to which these models are aligned [30], are not be able to produce detailed attribution scores, we additionally consider two top-k versions of the E-P and P-E setups. We construct these prompts to be as natural as possible while ensuring that the result remains easy to parse, and the final prompts are shown in Table IV and V respectively. The number k is chosen dynamically according to each input sentence as min(1, \u230aL/5\u230b), where L is the number of words in the sentence. For each prompt, the model generates a decimal point num-\nRole\nText\nSystem\nAs a movie review analyst, your role is to analyze the sentiment of movie reviews and provide insights on the importance of each word\nand punctuation in determining the overall positivity level. Your task is to identify the top 3 most significant words, ranked from the\nmost positive sentiment to the least positive sentiment. Additionally, you need to determine whether the movie review is positive or\nnegative along with your confidence in your prediction. A positive review is represented by the number 1, while a negative review will\nbe represented by the number 0. The confidence should be a decimal score between 0 and 1, with 0 being the lowest confidence and 1\nbeing the highest confidence. Please note that the coherence of the sentence is not relevant; your focus should be on analyzing the sentiment.\nThe movie review will be enclosed within <review> tags, but these tags should not be included in the evaluation of the review\u2019s\ncontent.\nOnly output the list of 3 words in the form of a comma separated list, with the prediction(as a number) and confidence following\nafter, nothing more.\nUser\n<review> A film that takes you inside the rhythms of its subject : You experience it as you watch . <review>\nAssistant\nrhythms, experience, watch, 1, 0.9\nRole Text System As a movie review analyst, your role is to analyze the sentiment of movie reviews and provide insights on the importance of each word and punctuation in determining the overall positivity level. Your task is to identify the top 3 most significant words, ranked from the most positive sentiment to the least positive sentiment. Additionally, you need to determine whether the movie review is positive or negative along with your confidence in your prediction. A positive review is represented by the number 1, while a negative review will be represented by the number 0. The confidence should be a decimal score between 0 and 1, with 0 being the lowest confidence and 1 being the highest confidence. Please note that the coherence of the sentence is not relevant; your focus should be on analyzing the sentiment. The movie review will be enclosed within <review> tags, but these tags should not be included in the evaluation of the review\u2019s content. Only output the list of 3 words in the form of a comma separated list, with the prediction(as a number) and confidence following after, nothing more. User <review> A film that takes you inside the rhythms of its subject : You experience it as you watch . <review> Assistant rhythms, experience, watch, 1, 0.9\nber for the prediction confidence. Since we do not have model weight access, we cannot compute the logit or probability of any generated token. Thus, we take this decimal point (in the range of 0.0 to 1.0) as the model\u2019s prediction, which is used to evaluate the model\u2019s accuracy.\n# C. Traditional Interpretability Methods\nWe benchmark the LLM-generated self-explanations with traditional interpretability methods that are often used to explain classifiers. As mentioned in Section II-A, gradientbased attribution values cannot be used due to the lack of input gradient access. Instead, we consider two perturbationbased methods, occlusion [25] and LIME [34]. The occlusion method defines the attribution value on word w as\n(2)\nwhere f(\u00b7) is the model prediction (as the generated decimal point number in the range of 0.0 to 1.0) for the input sentence, x is the original sentence, and x\u2212w is the input sentence with\nMPT AND AN EXAMPLE QUERY OF THE P-E TOP-k SETUP. BLACK TEXT IN NORMAL FONT IS PROVIDED LITERALLY. Gray text in italics IS CHANGED FOR EVERY QUERY. BLUE TEXT IN BOLDFACE IS THE MODEL\u2019S OUTPUT.\nRole\nText\nSystem\nAs a movie review analyst, your role is to analyze the sentiment of movie reviews and provide insights on the importance of each\nword and punctuation in determining the overall positivity level. Your task is to determine whether the movie review is positive or\nnegative along with your confidence in your prediction. A positive review is represented by the number 1, while a negative review will\nbe represented by the number 0. The confidence should be a decimal score between 0 and 1, with 0 being the lowest confidence and 1\nbeing the highest confidence. In addition, you need to identify the top 3 most significant words, ranked from the most positive sentiment\nto the least positive sentiment. Please note that the coherence of the sentence is not relevant; your focus should be on analyzing the sentiment.\nThe movie review will be enclosed within <review> tags, but these tags should not be included in the evaluation of the review\u2019s\ncontent.\nOnly output the prediction(as a number) and confidence, with the list of 3 words in the form of a comma separated list following\nafter, nothing more.\nUser\n<review> A film that takes you inside the rhythms of its subject : You experience it as you watch . <review>\nAssistant\n1, 0.8, rhythms, experience, watch\nthe word w removed. Intuitively, the occlusion value captures the effect of a single word removal on model prediction. By comparison, LIME recognizes that the effect of word removal may not be linearly additive (i.e., the total prediction change from removing two words is not the sum of prediction changes from removing each word), and thus proposes to remove random subsets of words, computes the model prediction change, and attributes the change to individual words by solving a linear regression problem.\n# D. Evaluation Metrics\nFor faithfulness evaluations of the self-explanations versus occlusion and LIME, we use five evaluation metrics: comprehensiveness [13], sufficiency [13], decision flip rate under the removal of most important token (DFMIT) [10], minimum fraction of tokens needing to be removed in order to cause decision flip (DFFrac) [36] and rank correlation with model prediction after word deletion (RankDel) [3]. Given a candidate attribution explanation to be evaluated, these metrics produce\na score based on the model predictions (and their changes) on (1) the original input and (2) the input with some words removed according to the explanation. We follow the definition and implementation by Zhou and Shah [51]. As before, the model prediction is taken from the decimal point number in the model\u2019s textual output. During the evaluation, when multiple words have the same saliency value (a feature unique to these LLM-based explanations, elaborated in Section IV-E), we perform random tie-breaking among them to determine the word removal order. To evaluate the top-k explanations (which does not have any ranking among the top-k words or among the remaining words), we adapt comprehensiveness, sufficiency and DFMIT to their top-k versions. Specifically, let x be the original input, x\u2212k be the input with the top-k words removed, and xk be the input with only top-k words kept. Then we define\n(3) (4) (5)\nwhere the indicator function in DFMIT@k evaluates to 1 if removing all k words together flips the label, and 0 otherwise. For non-top-k explanations, we can also evaluate them on these metrics by considering the k features with highest explanation values (random tie-breaking if necessary) as the top-k features. Besides these faithfulness evaluations (i.e., computing a metric score for an individual explanation), we also evaluate their (dis)agreement, motivated by Krishna et al. [23] and directly using their definitions.\n# E. A Caveat on Model Sameness\nSince the model is auto-regressive in nature, its output is affected by two factors: the model weight and the context (i.e., the prompt). Thus, given the same model (e.g., ChatGPT), prompting it in different ways (e.g., via different system messages) effectively leads to different models. Thus, in our\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03ff/03ff87ab-fd43-4a27-ae3c-08678cc995f1.png\" style=\"width: 50%;\"></div>\nexperiment, we need to treat the E-P and P-E prompt as two different models, and the generated self-explanations are not comparable to each other. In addition, in order to make meaningful comparisons between self-explanation and occlusion/LIME, we need the latter to be generated with the same system prompt, too. Concretely, as an example, for the sentence \u201cI really like this movie\u201d, to obtain the occlusion value for the word \u201clike\u201d that is comparable to E-P self-explanation, we need to compute the model prediction difference on the input \u201cI really like this movie\u201d vs. \u201cI really this movie\u201d, both using the prompt as shown in Table I, even though we do not need the explanation value for the latter sentence. The same reasoning applies on the evaluation side, where in order to retrieve the model prediction values, we need to use the original prompt and ignore the explanation output byproduct. This also means that the top-k E-P and P-E explanations are not those for the E-P and P-E models. However, there is no way around this issue: in order for the model to generate the top-k explanation, it has to be instructed in that way, which effectively results in a different model. Thus, in our study, we ignore the difference, and consider the topk explanation effectively as an explanation generated by an external explainer, for the target E-P or P-E model.\nWe conduct experiments on the Stanford Sentiment Treebank (SST) dataset [40], which is a collection of short- to medium-length movie reviews, with their associated labels. While the labels are provided as a continuous value between 0.0 (totally negative) and 1.0 (totally positive), we binarize the label at 0.5. Due to the cost and rate limit of the OpenAI\u2019s API access which makes the full set of experiments on a single instance taking around half an hour, we select a random subset of 100 sentences from the test set for our investigations. With this setup, Figure 2 presents the explanations for two sentences. We see that the LLM-generated self-explanations\nPrediction Only\n92%\nE-P\n85%\nP-E\n88%\nE-P top-k\n80%\nP-E top-k\n83%\nare intuitively reasonable in highlighting words of strong intrinsic sentiment values such as \u201csuperbly\u201d and \u201cvoid\u201d. In addition, there are other features, such as that the explanation values are quite sparse and distributed on a few \u201clevels,\u201d which are discussed further in Section IV-E.\n# B. Model Accuracy\nTable VI summarizes the classification accuracy of each model. First, we note that the classification accuracy of E-P is lower than that of P-E, at 85% vs 88%. This difference suggests that generating an explanation before making the prediction is not always helpful, unlike the benefits observed in chain-of-thought prompting [45]. We believe that this difference is due to the nature of the tasks: the \u201creasoning\u201d in sentiment analysis cannot be adequately represented by attribution scores assigned to each word, and hence forcing the model to make a prediction based on preceding explanation (i.e., the E-P setup) hurts the performance. Furthermore, both models have worse performance compared to the model that does not need to generate any explanation, which has an accuracy of 92%, with the system prompt shown below.\nYou are a creative and intelligent movie review analyst, whose purpose is to aid in sentiment analysis of movie reviews. A review will be provided to you, and you must classify the review as either 1 (positive) or 0 (negative), as well as your confidence in the score you chose. The confidence should be a decimal number between 0 and 1, with 0 being the lowest confidence and 1 being the highest confidence. Output this in the Python tuple format (<int classification>, <float confidence>).\nYou are a creative and intelligent movie review analyst, whose purpose is to aid in sentiment analysis of movie reviews. A review will be provided to you, and you must classify the review as either 1 (positive) or 0 (negative), as well as your confidence in the score you chose. The confidence should be a decimal number between 0 and 1, with 0 being the lowest confidence and 1 being the highest confidence. Output this in the Python tuple format (<int classification>, <float confidence>). The movie review will be surrounded by <review> tags. Example output: (<int classification>, <float confidence>)\nThis result is reminiscent of that by Camburu et al. [7], where asking the model to provide explanations hurts the performance, despite with different mechanisms: prompting in our work and supervised training in the other work. Last, the prediction is even less accurate when it needs to be made based on a few important words (in the E-P top-k setting), or when it needs to be justified later with a few words (in the P-E top-k setting). All thse model accuracy results imply that feature attribution explanations may not be the best form of explanation for sentiment analysis, which forces the\n<div style=\"text-align: center;\">TABLE VII COMPARISON BETWEEN THE SELF-EXPLANATION GENERATED BY CHATGPT AND TRADITIONAL EXPLANATIONS COMPUTED ON IT.</div>\nComp \u2191\nSuff \u2193\nDFMIT \u2191\nDFFrac \u2193\nRankDel \u2191\nE-P (Accuracy: 85%)\nOcclusion\n0.15\n0.26\n0.18\n0.57\n0.00\nLIME\n0.17\n0.22\n0.13\n0.50\n-0.02\nSELFEXP\n0.19\n0.25\n0.16\n0.55\n-0.03\nP-E (Accuracy: 88%)\nOcclusion\n0.20\n0.23\n0.14\n0.64\n-0.02\nLIME\n0.27\n0.20\n0.10\n0.56\n0.02\nSELFEXP\n0.27\n0.22\n0.07\n0.56\n-0.01\nmodel into an uncomfortable accuracy-interpretability tradeoff [17].\n# C. Faithfulness Evaluation Results\nTable VII presents the performance of SELFEXP for E-P and P-E models, compared to occlusion and LIME generated for each model respectively. We make several remarks below.\nFirst, we note the lack of a clear winner across the different metrics. Unexpectedly, we see that the occlusion saliency performs the best on DFMIT, because its definition (i.e., the importance of a word defined as the model prediction change under word removal) inherently makes its most important word being the most likely to flip the model prediction when removed \u2013 an observation of solvability made by Zhou and Shah [51]. We should also expect it to achieve the first place on RankDel, but all the numbers are very close to 0 and, as we will show in Section IV-E, the insensitivity of model prediction under single word removal makes this metric highly prone to the effect of stochasticity in tie-breaking. Among the remaining metrics, occlusion is the best on none, which is not surprising as it could not capture the non-linear effects of feature removals. Such non-linear effects are considered by LIME, which performs quite evenly with SELFEXP. Nonetheless, running LIME is highly expensive: since it needs to solve a linear regression problem, the default number of perturbations by the Python lime package1 is 5,000, meaning that we need to query the OpenAI API 5,000 times in order to generate the explanation for a single sentence. In practice, we adaptively scale this number to be 10 times the number of tokens in the sentence with an average of 183 perturbations per explanation, but this still is quite expensive, taking approximately 20 minutes per explanation. Thus, given that SELFEXP performs on par with LIME, we see that it could be low-cost substitute of LIME, as this selfexplanation is generated along with the prediction. As mentioned in the Section III-B, while the Pythonlike format guiding helps the model to generate syntactically valid explanation and prediction, we are concerned that the explanation quality may suffer. Thus, we also prompt the\n<div style=\"text-align: center;\">TABLE VIII COMPARISON OF ALL METHODS WITH THE TOP-k EXPLANATIONS ON TOP-k-ADAPTED METRICS.</div>\nComp@k \u2191\nSuff@k \u2193\nDFMIT@k \u2191\nE-P (Accuracy: 80%)\nOcclusion\n0.16\n0.34\n0.29\nLIME\n0.17\n0.28\n0.40\nSELFEXP\n0.11\n0.32\n0.32\nTOPK\n0.10\n0.27\n0.34\nP-E (Accuracy: 83%)\nOcclusion\n0.11\n0.29\n0.17\nLIME\n0.18\n0.26\n0.31\nSELFEXP\n0.11\n0.30\n0.29\nTOPK\n0.11\n0.25\n0.31\nmodel to generate the top-k most important words, and without attribution values attached to them. Table VIII shows three metrics that are compatible with the top-k explanation, and to compare it with other (fully formed) explanations, we extract the top-k words from the latter. As we can see, the top-k explanation is not distinctively better \u2013 while it achieves the best result on some metrics, it (or any other explanation) is by no means the clear winner.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/879a/879a6c1d-46ea-493e-b87f-956f38cef110.png\" style=\"width: 50%;\"></div>\nThus, given that explanations in the wild are often in the top-k format (c.f., Table III), the results suggest caution to us when interpreting these explanations.\n# D. Agreement Evaluation Results\nFigure 3 shows the agreement among various explanations, for both the E-P and P-E setups. Similar to the findings by [23], high disagreement among every pair of explanations is exhibited. However, the faithfulness evaluation does not identify any method as a top performer across the board, but instead all methods perform very closely to each other. This is in stark contrast with traditional interpretability methods generated for classifiers, where the performance difference among different methods is usually much larger (c.f., results by Zhou and Shah [51]). In other words, while Krishna et al. [23] found that explanations that perform very differently on faithfulness metrics (for classifier models) have high disagreement, we find that explanations that perform similarly on faithfulness metrics also have high disagreement (for the ChatGPT model). There could be two possible reasons for our finding. The first one is that our evaluations do not have the discriminative power to differentiate the good explanations from the bad ones. Some evidence supporting this claim is presented in Section\nIV-E2 below. The second reason is that there could be some explanations that achieve distinctively higher faithfulness metric values, which understandably also has high disagreement with all of the current ones. In other words, we are only obtaining mediocre explanations with occlusion, LIME and self-explanation, while missing out on the \u201coptimal one.\u201d We leave further investigation of this topic to future work.\n# E. Qualitative Analyses\nDuring our experiments, we notice several differences between the explanation generation and evaluation for ChatGPT versus those for other models. These differences often stem from fundamental differences between ChatGPT and other models, and serve as important caveats when generating and understanding these explanations. 1) Level of Saliency Values: In the SELFEXP attribution values, it is very common for them to be on a few levels, whose values are often \u201cwell-rounded\u201d decimal numbers such as 0.5 and 0.75, rather than arbitrary ones such as 0.6423 and 0.1982 (often produced by traditional techniques for traditional models). One example is shown in Table I and Table II, and additional instances are presented in Table IX. Quantitatively, on average each sentence has 18.2 words, but there are only 2.72 levels of unique explanations per sentence for the E-P model and 5.69 for the P-E model.\nE-P Prompting\n<review> Ford deserves to be remembered at Oscar time for crafting\nthis wonderful portrait of a conflicted soldier . <review>\n[(\u2018Ford\u2019, 0.500), (\u2018deserves\u2019, 0.500), (\u2018to\u2019, 0.000), (\u2018be\u2019, 0.000), (\u2018re-\nmembered\u2019, 0.500), (\u2018at\u2019, 0.000), (\u2018Oscar\u2019, 0.000), (\u2018time\u2019, 0.000),\n(\u2018for\u2019, 0.000), (\u2018crafting\u2019, 0.500), (\u2018this\u2019, 0.000), (\u2018wonderful\u2019,\n1.000), (\u2018portrait\u2019, 0.500), (\u2018of\u2019, 0.000), (\u2018a\u2019, 0.000), (\u2018conflicted\u2019,\n-0.500), (\u2018soldier\u2019, 0.000), (\u2018.\u2019, 0.000)]\n(1, 0.800)\n<review> It \u2019s never a good sign when a film \u2019s star spends the entirety\nof the film in a coma . <review>\n[(\u2018It\u2019, 0.5), (\u201c\u2019s\u201d, 0.5), (\u2018never\u2019, -0.5), (\u2018a\u2019, 0.0), (\u2018good\u2019, 0.8), (\u2018sign\u2019,\n0.6), (\u2018when\u2019, 0.0), (\u2018a\u2019, 0.0), (\u2018film\u2019, 0.0), (\u201c\u2019s\u201d, 0.5), (\u2018star\u2019, 0.4),\n(\u2018spends\u2019, 0.2), (\u2018the\u2019, 0.0), (\u2018entirety\u2019, 0.0), (\u2018of\u2019, 0.0), (\u2018the\u2019, 0.0),\n(\u2018film\u2019, 0.0), (\u2018in\u2019, 0.0), (\u2018a\u2019, 0.0), (\u2018coma\u2019, -0.7), (\u2018.\u2019, 0.0)]\n(0, 0.700)\nP-E Prompting\n<review> Ford deserves to be remembered at Oscar time for crafting\nthis wonderful portrait of a conflicted soldier . <review>\n(1, 0.9)\n[(\u2018Ford\u2019, 0.2), (\u2018deserves\u2019, 0.8), (\u2018to\u2019, 0.1), (\u2018be\u2019, 0.1), (\u2018remem-\nbered\u2019, 0.7), (\u2018at\u2019, 0.1), (\u2018Oscar\u2019, 0.5), (\u2018time\u2019, 0.2), (\u2018for\u2019, 0.1),\n(\u2018crafting\u2019, 0.6), (\u2018this\u2019, 0.1), (\u2018wonderful\u2019, 0.9), (\u2018portrait\u2019, 0.7),\n(\u2018of\u2019, 0.1), (\u2018a\u2019, 0.1), (\u2018conflicted\u2019, 0.8), (\u2018soldier\u2019, 0.7), (\u2018.\u2019, 0.1)]\n<review> Even die-hard fans of Japanese animation ... will find this\none a challenge . <review>\n(0, 0.8)\n[(\u2018Even\u2019, 0.2), (\u2018die-hard\u2019, 0.1), (\u2018fans\u2019, 0.3), (\u2018of\u2019, 0.0), (\u2018Japanese\u2019,\n0.5), (\u2018animation\u2019, 0.6), (\u2018...\u2019, 0.0), (\u2018will\u2019, 0.0), (\u2018find\u2019, 0.0), (\u2018this\u2019,\n0.0), (\u2018one\u2019, 0.0), (\u2018a\u2019, 0.0), (\u2018challenge\u2019, 0.4), (\u2018.\u2019, 0.0)]\nWe believe that this phenomenon occurs due to the humanlike behavior and reasoning capability of ChatGPT. In its attempt to mimic a human (which is the central goal of the reinforcement learning from human feedback (RLHF) alignment [30]), it outputs explanation values that are plausibly produced by humans, who will not give very fine-grained saliency values (e.g., 0.6423 and 0.1982) as these values are often beyond the human\u2019s reasoning ability. As recent papers have noted [50, 53], model explanations are often less understandable to humans than desirable in that humans may not be able to draw the correct high-level conclusions about the model\u2019s reasoning from individual (i.e., local) explanations. One potential culprit could be the traditionally very fine-grained and varied saliency values, even if they faithfully reflect the model\u2019s reasoning. Thus, the ChatGPT self-explanation may offer a different kind of faithfulnessunderstandability trade-off, where the explanations are intentionally vague but easier for human consumption. Testing this hypothesis requires a formal evaluation of understandability with human subjects, which we leave to future work. Orthogonally, comparing the explanations generated by aligned and unaligned models could identify the impact of alignment, though getting access to the latter model may prove difficult as companies are often reluctant to release these models due to safety concerns [20]. We attempted to combat the issue with few-shot prompting, where we provide some explanation values according to our intuitions on some examples before asking the model to generate its own explanation and prediction. However, the results are less than satisfactory. Few-shot prompting does solve the issue of saliency values being too \u201crounded:\u201d when provided with more fine-grained values (e.g. 0.8694 and 0.1285) as attribution scores, the generated explanations also tend to be less rounded. However, still not too many saliency levels are present. In addition, ChatGPT tends to \u201cmemorize\u201d the provided attribution values for specific words. For example, when the word \u201cthe\u201d has saliency value of 0.163 in the exemplar, the model is likely to assign the same value of 0.163 to other instances of \u201cthe\u201d in the query sentence. A full example demonstrating both phenomena is presented in Table X. As a result of the weaknesses of the few-shot prompting, we opted to base our experiments on the more stable, humanlike responses from the zero-shot prompting setup. Overall, our findings suggest that the model is learning to mimic the human-provided explanation, which is a fundamentally flawed behavior: we want the model\u2019s explanation to reflect its own reasoning process, rather than reflecting the reasoning process of another decision maker (human or model). 2) Prediction Value Roundedness and Insensitivity: A key component in both the definition and evaluation of model explanations is the model\u2019s prediction on partial sentences with certain words removed. Recall that since we do not have access to token probability output from ChatGPT, we define the model prediction as the textual output of the confidence score as a floating point number. Since this textual output is also shaped by alignment, we have similar findings as before.\nTABLE X AN EXAMPLE FEW-SHOT LEARNING CONVERSATION. ALL TEXTS UP TO AND INCLUDING THE LAST USER MESSAGE ARE HUMAN GENERATED AND PROVIDED AS INSTRUCTIONS AND FEW-SHOT EXEMPLARS. ONLY THE LAST ASSISTANT MESSAGE IN BOLDFACE IS MODEL-GENERATED. GREEN AND PURPLE COLORS SHOW THAT THE MODEL STILL PRODUCES ATTRIBUTION VALUES ON FEW SALIENCY LEVELS, THOUGH THEY ARE NO LONGER WELL-ROUNDED. BLUE COLOR SHOWS THAT THE MODEL MEMORIZES THE PROVIDED (WORD, ATTRIBUTION VALUE) PAIR.\nRole\nText\nSystem\nYou are a movie review bot. Your job is to analyze movie reviews and classify them as either 1 (positive) or 0 (negative), as well as your\nconfidence in the score you chose. Please also provide the degree of importance of each word in the sentence in your explanation in the\nformat (\"word\", importance). For example, the expected response for the sentence \"This movie was very good\" is: \"Classification: 1, 0.99\nconfidence. [(\"This\", 0.085), (\"movie\", 0.149), (\"was\", -0.177), (\"very\", 0.691), (\"good\", 0.997)]. \" It does not matter whether or not the\nsentence makes sense. Do your best given the sentence.\nIt is imperative that you respond in the desired format regardless of the validity of the sentence.\nUser\nReggio \u2019s trippy , ambitious downer can also sometimes come across like nothing more than a glorified Nike ad .\nAssistant\nClassification: 0, 0.82 confidence. [(\"Reggio\", 0.254), (\"\u2019s\", 0.192), (\"trippy\", -0.392), (\",\", -0.045), (\"ambitious\", 0.498), (\"downer\", -0.602),\n(\"can\", 0.195), (\"also\", 0.075), (\"sometimes\", 0.285), (\"come\", 0.043), (\"across\", 0.177), (\"like\", 0.101), (\"nothing\", -0.255), (\"more\", -0.101),\n(\"than\", 0.121), (\"a\", 0.004), (\"glorified\", 0.384), (\"Nike\", -0.369), (\"ad\", -0.739), (\".\", 0.007)]\nUser\nThere is not a single movie that could have been better than this .\nAssistant\nClassification: 1, 0.90 confidence. [(\"There\", 0.004), (\"is\", 0.114), (\"not\", -0.787), (\"a\", 0.119), (\"single\", 0.239, (\"movie\", 0.395, (\"that\",\n0.043), (\"could\", 0.294), (\"have\", 0.155), (\"been\", 0.020), (\"better\", 0.859), (\"than\", 0.122), (\"this\", 0.500), (\".\", 0.001)]\nUser\nIt was a great movie overall , but the ending was a bit lackluster .\nAssistant\nClassification: 1, 0.75 confidence. [(\"It\", 0.174), (\"was\", -0.101), (\"a\", 0.122), (\"great\", 0.825), (\"movie\", 0.608), (\"overall\", 0.390), (\",\",\n-0.009), (\"but\", -0.134), (\"the\", 0.033), (\"ending\", -0.635), (\"was\", -0.145), (\"a\", 0.103), (\"bit\", -0.396), (\"lackluster\", -0.859), (\".\", -0.003)]\nUser\nThe film provides some great insight into the neurotic mindset of all comics even those who have reached the absolute top of the game .\nAssistant\nClassification: 1, 0.98 confidence. [(\"The\", 0.033), (\"film\", 0.607), (\"provides\", 0.346), (\"some\", 0.091), (\"great\", 0.825), (\"insight\",\n0.537), (\"into\", 0.091), (\"the\", 0.033), (\"neurotic\", -0.498), (\"mindset\", -0.498), (\"of\", 0.033), (\"all\", 0.033), (\"comics\", 0.537),\n(\"even\", 0.033), (\"those\", 0.033), (\"who\", 0.033), (\"have\", 0.033), (\"reached\", 0.033), (\"the\", 0.033), (\"absolute\", 0.033), (\"top\",\n0.033), (\"of\", 0.033), (\"the\", 0.033), (\"game\", 0.033), (\".\", 0.000)]\nSpecifically, we notice that the model\u2019s prediction values are often \u201cwell-rounded\u201d and being one of several distinct values, as shown in Table XI. Furthermore, the removal of a few words often does not change the model\u2019s prediction, as Table XII shows. This phenomenon has two causes: first, the model could likely infer the missing words and treat them as minor typographical errors; second, the model is aligned to human thinking, which habitually ignores these errors when making predictions. This model behavior has two implications. First, the occlu-\n# TABLE XI\nSentence\nPred\nGuided more by intellect than heart , his story flattens instead\nof sharpens .\n0.70\nThese are names to remember , in order to avoid them in the\nfuture .\n0.50\nA hideous , confusing spectacle , one that may well put the\nnail in the coffin of any future Rice adaptations .\n0.00\nEastwood is an icon of moviemaking , one of the best\nactors , directors and producers around , responsible for some\nexcellent work .\n0.90\nWhile Super Troopers is above Academy standards , its\nquintet of writers could still use some more schooling .\n0.80\nsion saliency for a word is defined as the model prediction difference on inputs with and without the word, which results in the saliency value being exactly 0 for 82.6% of words for the E-P model and 54.5% for the P-E model. More importantly, all evaluations are affected to different extents by this behavior. The most severely affected ones are DFMIT and RankDel, which are computed from the model\nSentence\nPred\nOne of the greatest family-oriented , fantasy-adventure\nmovies ever .\n0.90\nOne of the greatest family-oriented , fantasy-adventure\nmovies .\n0.90\nOne of the family-oriented , fantasy-adventure movies .\n0.80\nOne of the, fantasy-adventure movies .\n0.80\nOne of the , movies .\n0.80\nof the , movies .\n0.50\nof the , movies\n0.50\nof the ,\n0.50\nthe ,\n0.50\n,\n0.50\nprediction on the original sentence and that of one-wordremoved sentence only, similar to occlusion saliency. As a result, all explanations have very low scores on these two metrics, compared to the same evaluation settings on a finetuned RoBERTa model [27] which has much more gradual model prediction changes under word removal (\u223c25% for DFMIT and \u223c0.5% for RankDel on average as reported by [51]). Notably, we did not observe a perfect 1.0 score for occlusion on RankDel, because many saliency values and word removal prediction changes are the same (at which point random tie-breaking is applied). Even with more words being removed, the model\u2019s prediction still stays the same for a long time, which decreases the comprehensiveness score, as the initial few word removals could not significantly decrease the model\u2019s confidence, and increases the sufficiency score, as the initial few word insertions could not significantly increase the model\u2019s confidence.\n# V. CONCLUSION AND FUTURE WORK\nIn this paper, we conducted the first rigorous assessment in LLM\u2019s capability to self-generate feature attribution explanations \u2013 i.e., explaining its own prediction by stating the importance of every word in the input. Using ChatGPT and the SST dataset as our testbed, we construct prompts that reliably generate two types of LLM-generated self-explanations: full feature attribution explanation that assigns every word an importance score and a top-k explanation that only highlights the few most important words. The explanations are generated in two settings: one where they are generated first and then the model predictions (i.e., sentiment assessment) is made based on them, and the other where the prediction is generated first and the explanations are generated to justify the predictions. We compare these explanations to traditional explanation techniques \u2013 occlusion saliency and LIME \u2013 on both faithfulness and agreement metrics. We have two major findings from our investigation. First, none of the explanations, self-generated or not, hold a distinct advantage over the rest according to the faithfulness evaluations, yet they are highly different according to the agreement evaluations. As a result, there could potentially be some explanation which is much better than the current ones, and novel techniques may be needed to reveal them. Second, and possibly of independent interest in the study of LLMs, both the model prediction values and word attribution values are highly well-rounded, taking values such as 0.25, 0.67, 0.75, etc. This finding may also explain the similarity in faithfulness metric values in across different explanations: due to the lack of fine-grained variations in the explanation and prediction values, the evaluation metrics may not have the sufficient power to distinguish the good explanations from the bad ones. These findings also inform directions for future work. First, given that there is no prior work on studying LLM-generated feature attribution explanations, it is likely that our solution is not optimal, and better ways to elicit self-explanations could be developed. Second, we may need to rethink the evaluation\npractice, which has shown to not perform very well. Indeed, if we consider ChatGPT\u2019s reasoning ability as being more similar to humans than to earlier machine learning models, the result is not surprising: a human will not be able to give a precise number for the effect of removing one word in a sentence of twenty words (as used by DFMIT and RankDel metrics), or give a list of the cumulative influence of a word deletion or insertion sequence (as used by comprehensiveness, sufficiency and DFFrac metrics). Thus, alternative evaluation strategies, possibly involving carefully designed human subject studies [e.g. 4] are needed to definitively assess the strengths and weaknesses of these explanations. An orthogonal direction would be to evaluate other LLMs, such as GPT-4, Bard and Claude. A comparative study could reveal important insights in how these models understand themselves (i.e., introspection), as explanations are essentially properties about predictions (e.g., occlusion saliency is the prediction change under single word removal). In addition, we can conduct the analogous studies on other types of explanations, such as counterfactual explanations [43] and concept-based explanations [21, 26]. Finally, now that we have demonstrated that these explanations are feasible options to consider, we need to make sure that they are indeed beneficial and not used for harmful purposes. For example, how easily can they be manipulated [38], and could they hide fairness issues of the model [11]? Identifying and solving any observed issues is a necessary step before the deployment of these explanations.\n# REFERENCES\n[1] Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. Debugging tests for model explanations. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. Post hoc explanations may be ineffective for detecting unknown spurious correlation. In International Conference on Learning Representations (ICLR), 2022. [3] David Alvarez-Melis and Tommi S Jaakkola. On the robustness of interpretability methods. In ICML Workshop on Human Interpretability in Machine Learning, 2018. [4] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. Does the whole exceed its parts? the effect of AI explanations on complementary team performance. In ACM CHI Conference on Human Factors in Computing Systems (CHI), pages 1\u201316, 2021. [5] Naman Bansal, Chirag Agarwal, and Anh Nguyen. SAM: The sensitivity of attribution methods to hyperparameters. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8673\u20138683, 2020. [6] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. A protocol for evaluating the faithfulness of input salience methods for text classification. In Conference on Empirical Methods\nin Natural Language Processing (EMNLP). Association for Computational Linguistics, 2022. [7] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. CoRR, abs/1812.01193, 2018. URL http://arxiv.org/abs/1812.01193. [8] Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves? counterfactual simulatability of natural language explanations. arXiv preprint arXiv:2307.08678, 2023. [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. [10] George Chrysostomou and Nikolaos Aletras. Improving the faithfulness of attention-based explanations with taskspecific information for text classification. In Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 477\u2013488. Association for Computational Linguistics, 2021. [11] Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H. Bach, and Himabindu Lakkaraju. Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations. In AAAI/ACM Conference on AI, Ethics, and Society (AIES), page 203\u2013214. Association for Computing Machinery, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171\u20134186. Association for Computational Linguistics, 2019. [13] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. ERASER: A benchmark to evaluate rationalized NLP models. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 4443\u20134458. Association for Computational Linguistics, 2020. [14] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867, 2023. [15] A Shaji George and AS Hovan George. A review of chatgpt ai\u2019s impact on several business sectors. Partners Universal International Innovation Journal, 1(1):9\u201323, 2023. [16] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In AAAI Conference on Artificial Intelligence (AAAI), volume 33,\npages 3681\u20133688, 2019. [17] David Gunning and David Aha. Darpa\u2019s explainable artificial intelligence (xai) program. AI magazine, 40(2): 44\u201358, 2019. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [19] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations (ICLR), 2019. [20] Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018. [21] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pages 2668\u20132677. PMLR, 2018. [22] Lucas Kohnke, Benjamin Luke Moorhouse, and Di Zou. Chatgpt for language teaching and learning. RELC Journal, page 00336882231162868, 2023. [23] Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu Lakkaraju. The disagreement problem in explainable machine learning: A practitioner\u2019s perspective. CoRR, abs/2202.01602, 2022. URL https://arxiv.org/abs/2202.01602. [24] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in NLP. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 681\u2013691. Association for Computational Linguistics, 2016. [25] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. arXiv:1612.08220, 2016. [26] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In AAAI Conference on Artificial Intelligence (AAAI), 2018. [27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:1907.11692, 2019. [28] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems (NIPS), pages 4765\u2013 4774, 2017. [29] Christoph Molnar. Interpretable Machine Learning. 2 edition, 2022. [30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\nTraining language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. [31] Thang Pham, Trung Bui, Long Mai, and Anh Nguyen. Double trouble: How to not explain a text classifier\u2019s decisions using counterfactuals synthesized by masked language models? In Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (AACL-IJCNLP), pages 12\u201331. Association for Computational Linguistics, 2022. [32] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [33] Alec Radford, Jeffrey Wu, Rewon Child, David \u2019Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9, 2019. [34] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"Why should I trust you?\" explaining the predictions of any classifier. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016. [35] Alvin E Roth. The Shapley Value: Essays in Honor of Lloyd S. Shapley. Cambridge University Press, 1988. [36] Sofia Serrano and Noah A. Smith. Is attention interpretable? In Annual Meeting of the Association for Computational Linguistics (ACL), pages 2931\u20132951. Association for Computational Linguistics, 2019. [37] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv:1312.6034, 2013. [38] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods. In AAAI/ACM Conference on AI, Ethics, and Society (AIES), pages 180\u2013186. Association for Computing Machinery, 2020. [39] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. SmoothGrad: Removing noise by adding noise. arXiv:1706.03825, 2017. [40] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/D13-1170. [41] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning (ICML), pages 3319\u2013\n3328, 2017. [42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. [43] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & Technology, 31:841, 2017. [44] Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. Is chatgpt a good sentiment analyzer? a preliminary study. arXiv preprint arXiv:2304.04339, 2023. [45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https: //arxiv.org/abs/2201.11903. [46] IpKin Anthony Wong, Qi Lilith Lian, and Danni Sun. Autonomous travel decision-making: An early glimpse into chatgpt and generative ai. Journal of Hospitality and Tourism Management, 56:253\u2013263, 2023. [47] Mengjiao Yang and Been Kim. Benchmarking attribution methods with relative feature importance. arXiv:1907.09701, 2019. [48] Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. Exploring the limits of chatgpt for query or aspect-based text summarization. arXiv preprint arXiv:2302.08081, 2023. [49] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (ECCV), pages 818\u2013833. Springer, 2014. [50] Yiming Zheng, Serena Booth, Julie Shah, and Yilun Zhou. The irrationality of neural rationale models. In 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP). Association for Computational Linguistics, 2022. [51] Yilun Zhou and Julie Shah. The solvability of interpretability evaluation metrics. arXiv:2205.08696, 2022. [52] Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution methods correctly attribute features? In AAAI Conference on Artificial Intelligence (AAAI), 2022. [53] Yilun Zhou, Marco Tulio Ribeiro, and Julie Shah. ExSum: From local explanations to model understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational Linguistics, 2022.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The rise of large language models (LLMs) like ChatGPT has transformed natural language processing (NLP), yet there is a need to evaluate their self-explanations in terms of faithfulness and utility in model interpretability.",
            "purpose of benchmark": "The benchmark aims to assess the quality of self-generated explanations by LLMs, particularly in sentiment analysis, comparing them to traditional interpretability methods."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of evaluating the faithfulness and effectiveness of self-explanations generated by LLMs in sentiment analysis tasks.",
            "key obstacle": "Existing benchmarks do not adequately capture the unique nature of LLMs and their self-explanations, leading to potential misinterpretations of model reasoning."
        },
        "idea": {
            "intuition": "The creation of this benchmark stems from the observation that LLMs can produce explanations similar to human reasoning, prompting a need to evaluate their quality and reliability.",
            "opinion": "The authors believe that understanding LLM-generated self-explanations is crucial for advancing interpretability practices in AI.",
            "innovation": "This benchmark introduces a systematic evaluation of LLM-generated self-explanations, contrasting them with traditional methods like occlusion and LIME, highlighting their differences and potential advantages.",
            "benchmark abbreviation": "SELFEXP"
        },
        "dataset": {
            "source": "The dataset is derived from the Stanford Sentiment Treebank (SST), which contains labeled movie reviews.",
            "desc": "The SST dataset includes a collection of short to medium-length movie reviews, with binary sentiment labels derived from continuous scores.",
            "content": "The dataset consists of textual data from movie reviews, which are analyzed for sentiment classification.",
            "size": "100",
            "domain": "Sentiment Analysis",
            "task format": "Sentiment Classification"
        },
        "metrics": {
            "metric name": "Comprehensiveness, Sufficiency",
            "aspect": "The metrics evaluate the faithfulness of the model's self-explanations in relation to their predictions.",
            "principle": "The chosen metrics are based on the premise that a good explanation should significantly influence the model's prediction when key features are altered.",
            "procedure": "Evaluation involves comparing model predictions on original inputs against those with certain features removed, assessing the impact of these changes."
        },
        "experiments": {
            "model": "ChatGPT",
            "procedure": "Models were prompted to generate predictions and explanations in two setups: explain-then-predict (E-P) and predict-and-explain (P-E).",
            "result": "The self-explanations generated by ChatGPT performed comparably to traditional methods in faithfulness evaluations but showed significant differences in agreement metrics.",
            "variability": "Variability in results was accounted for by conducting multiple trials on a random subset of the SST dataset."
        },
        "conclusion": "The findings indicate that while LLM-generated self-explanations do not outperform traditional methods in faithfulness, they provide a low-cost alternative that merits further investigation.",
        "discussion": {
            "advantage": "The benchmark highlights the potential of LLM-generated self-explanations as efficient and human-like alternatives to traditional interpretability methods.",
            "limitation": "The evaluations may not fully capture the nuances of LLM reasoning, suggesting the need for more refined assessment techniques.",
            "future work": "Future research should explore alternative evaluation methods and extend the study to other LLMs to assess their introspective capabilities."
        },
        "other info": {
            "info1": "The benchmark encourages a rethinking of current interpretability practices in light of LLM capabilities.",
            "info2": {
                "info2.1": "The study suggests that traditional metrics may not be adequate for evaluating LLM-generated explanations.",
                "info2.2": "Further work is needed to explore the implications of LLM self-explanations on model accountability and transparency."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The rise of large language models (LLMs) like ChatGPT has transformed natural language processing (NLP), yet there is a need to evaluate their self-explanations in terms of faithfulness and utility in model interpretability."
        },
        {
            "section number": "4.1",
            "key information": "The benchmark introduces a systematic evaluation of LLM-generated self-explanations, contrasting them with traditional methods like occlusion and LIME, highlighting their differences and potential advantages."
        },
        {
            "section number": "5.3",
            "key information": "The evaluations may not fully capture the nuances of LLM reasoning, suggesting the need for more refined assessment techniques."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore alternative evaluation methods and extend the study to other LLMs to assess their introspective capabilities."
        }
    ],
    "similarity_score": 0.7440811173257623,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d20/7d205132-eb3e-4f3f-a4e3-f1a94f600c48.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab97/ab97226a-8980-4600-be8b-0369705712eb.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/60ce/60ce9ecb-f83e-41c0-ad30-f6b99ba3756d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03ff/03ff87ab-fd43-4a27-ae3c-08678cc995f1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/879a/879a6c1d-46ea-493e-b87f-956f38cef110.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Can large language models explain themselves_ a study of llm-generated self-explanations.json"
}