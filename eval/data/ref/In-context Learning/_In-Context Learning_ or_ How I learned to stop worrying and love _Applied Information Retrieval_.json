{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.01116",
    "title": "\"In-Context Learning\" or: How I learned to stop worrying and love \"Applied Information Retrieval\"",
    "abstract": "With the increasing ability of large language models (LLMs), incontext learning (ICL) has evolved as a new paradigm for natural language processing (NLP), where instead of fine-tuning the parameters of an LLM specific to a downstream task with labeled examples, a small number of such examples is appended to a prompt instruction for controlling the decoder\u2019s generation process. ICL, thus, is conceptually similar to a non-parametric approach, such as \ud835\udc58-NN, where the prediction for each instance essentially depends on the local topology, i.e., on a localised set of similar instances and their labels (called few-shot examples). This suggests that a test instance in ICL is analogous to a query in IR, and similar examples in ICL retrieved from a training set relate to a set of documents retrieved from a collection in IR. While standard unsupervised ranking models can be used to retrieve these few-shot examples from a training set, the effectiveness of the examples can potentially be improved by re-defining the notion of relevance specific to its utility for the downstream task, i.e., considering an example to be relevant if including it in the prompt instruction leads to a correct prediction. With this task-specific notion of relevance, it is possible to train a supervised ranking model (e.g., a bi-encoder or cross-encoder), which potentially learns to optimally select the few-shot examples. We believe that the recent advances in neural rankers can potentially find a use case for this task of optimally choosing examples for more effective downstream ICL predictions.",
    "bib_name": "parry2024incontextlearningori",
    "md_text": "# \u201cIn-Context Learning\u201d or: How I learned to stop worrying and love \u201cApplied Information Retrieval\u201d\nk Debasis Ganguly\u2020\u2217 Debasis.Ganguly@glasgow.ac.uk University of Glasgow Glasgow, UK Manish Chandra\u2217 m.chandra.1@research.gla.ac.uk University of Glasgow Glasgow, UK\nAndrew Parry\u2217 a.parry.1@research.gla.ac.uk University of Glasgow Glasgow, UK Debasis Ganguly\u2020\u2217 Debasis.Ganguly@glasgow.ac.uk University of Glasgow Glasgow, UK Manish Ch m.chandra.1@res University of Glasgow\nAndrew Parry\u2217 a.parry.1@research.gla.ac.uk University of Glasgow Glasgow, UK\n# ABSTRACT\nWith the increasing ability of large language models (LLMs), incontext learning (ICL) has evolved as a new paradigm for natural language processing (NLP), where instead of fine-tuning the parameters of an LLM specific to a downstream task with labeled examples, a small number of such examples is appended to a prompt instruction for controlling the decoder\u2019s generation process. ICL, thus, is conceptually similar to a non-parametric approach, such as \ud835\udc58-NN, where the prediction for each instance essentially depends on the local topology, i.e., on a localised set of similar instances and their labels (called few-shot examples). This suggests that a test instance in ICL is analogous to a query in IR, and similar examples in ICL retrieved from a training set relate to a set of documents retrieved from a collection in IR. While standard unsupervised ranking models can be used to retrieve these few-shot examples from a training set, the effectiveness of the examples can potentially be improved by re-defining the notion of relevance specific to its utility for the downstream task, i.e., considering an example to be relevant if including it in the prompt instruction leads to a correct prediction. With this task-specific notion of relevance, it is possible to train a supervised ranking model (e.g., a bi-encoder or cross-encoder), which potentially learns to optimally select the few-shot examples. We believe that the recent advances in neural rankers can potentially find a use case for this task of optimally choosing examples for more effective downstream ICL predictions.\n# CCS CONCEPTS\nCCS CONCEPTS \u2022 Information systems \u2192Information retrieval; \u2022 Computing methodologies \u2192Machine learning; Natural language processing.\n# \u2022 Information systems \u2192Information retrieval; \u2022 Computing methodologies \u2192Machine learning; Natural language processing.\n# KEYWORDS\nLarge Language Models, In-Context Learning, Ranking Models, Query Performance Prediction\n\u2217Author names in alphabetical order by first name. \u2020Corresponding Author\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07...$15.00 https://doi.org/10.1145/3626772.3657842\nACM Reference Format: Andrew Parry, Debasis Ganguly, and Manish Chandra. 2024. \u201cIn-Context Learning\u201d or: How I learned to stop worrying and love \u201cApplied Information Retrieval\u201d. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201924), July 14\u201318, 2024, Washington, DC, USA. ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/3626772.3657842\n# 1 INTRODUCTION\nResearch on large language models [58] (LLMs) is expanding in scope and yielding significant scientific advancements rapidly. These language models are pre-trained on large corpora of documents to capture the inherent semantics of text in a generic task-independent manner. Common pre-training methodologies either involve a maske language model (MLM) that predicts randomly masked tokens from the text [17, 45, 64], or an auto-regressive model or causal language model (CLM) which predicts a token only from its predecessor tokens [5, 63, 82]. While MLM is employed in BERT [17] and its successors, such as RoBERTa [45], BART [39] etc., the latter class of models, i.e., CLM, is applied to train GPT variants [5, 58, 63] and open-source Llama and Mistral variants [35, 79] etc. LLMs, when scaled from millions to billions of parameters, have demonstrated to be adaptable to a broad set of tasks due to instruction tuning [59], in the sense that they are not only able to produce semantically correct and coherent text but are also able to adapt themselves surprisingly well with small changes in contexts supplied as inputs, commonly called prompts [3]. This ability to adapt to unseen data and tasks with only a small number of examples differs from the standard notion of supervised learning, where the parameters of a pre-trained model (e.g., BERT [17]) is then again learned (commonly referred to as \u2018fine-tuning\u2019) from a training set of labelled examples. Instead, in few-shot learning or in-context learning (ICL), a small number of labelled examples from a training set are simply appended to a prompt instruction to control the text generation in a desirable way beneficial to the downstream task [41, 53, 54, 61]. In addition to leveraging ICL for a purely generative task, e.g., question answering or abstractive summarisation [5, 42, 78], a more common use is in a predictive task, such as text classification [46, 52], where each class is specified by a set of words (commonly called a verbaliser [69]), e.g., for a binary sentiment classification task the positive class could be defined by the set {\u2018good\u2019, \u2018great\u2019, \u2018wonderful\u2019... }. Once each class for a predictive task is well-defined, the generated text can be mapped to the most likely class(es) by using the posterior over the vocabulary generated by the decoder. It is to be realised that ICL is somewhat conceptually similar to a non-parametric approach, such as \ud835\udc58-NN, where the prediction\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25c1/25c1beef-7716-44c7-88dd-ec16704f59aa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Downstream Tasks</div>\nFigure 1: A workflow diagram illustrating how three verticals of IR research fit into the workflow of in-context learning (ICL). Section 3 discusses possible ways of adjusting unsupervised and supervised QPP approaches for adapting the number of ICL examples. Section 4 discusses ideas of how to learn the notion of downstream usefulness of examples. Section 5 discusses methodologies related to diversifying examples for ICL.\nfor each instance essentially depends on the local topology, i.e., on a localised set of similar instances and their labels (called fewshot examples) - the only difference of ICL with \ud835\udc58-NN is that the former involves a frozen set of encoder-decoder parameters of an underlying LLM, where ICL generally works well on any domain with only a small number of examples because, unlike supervised models, it does not suffer from overfitting the parameters on a particular set of labelled examples. Instead, the semantics expressed in the examples potentially play a key role in controlling the text generation process to yield the desired output\u2014text generation process to yield the desired output \u2013 either the text itself or a mapping into a class prediction. The usefulness of localised examples, akin to the nearest neighborbased prediction, suggests a strong case of analogy between ICL and ad-hoc IR. More precisely, a test instance in ICL is analogous to a query in IR, and similar examples in ICL retrieved from a training set relate to a set of documents retrieved from a collection in IR. This analogy opens up several interesting research questions in ICL concerning the effective use of IR to improve ICL predictions. In this perspective paper, we discuss particular components of ICL that can be mapped to known and well-researched IR problems. This means that the solutions to these problems that the IR community has researched over decades can potentially be applied to improve\nthe effectiveness of ICL. Moreover, this should also be of interest to an IR researcher to develop new methodologies for classic problems in IR, such as that of document retrieval or query performance prediction (QPP) [14, 65, 73], specifically catered to a downstream predictive task, hence opening up new possibilities of evaluating novel IR methodologies beyond retrieval tasks. We now propose the following three main ways of incorporating the ideas from core IR into ICL. First, during inference in ICL, instead of using a constant number of examples for each instance, a potentially better approach could be to make the number of examples variable. A similar problem in IR is to predict how many documents to retrieve (or equivalently to predict the rank cutoff threshold [2, 4]), which is also closely tied to the problem of query performance prediction (QPP) [14, 65, 73]. In the context of ICL, this means that for some test instances, one can find more \u2018useful\u2019 examples from the training set (an example can be considered useful if including it as a part of the prompt leads an LLM to the correct prediction). In contrast, it is difficult for others to find such useful examples. An ICL method that is aware of the quality of the examples can, hence, potentially adapt itself, e.g., by using a higher number of examples if the example quality is predicted to be poor. Second, we propose to make the underlying metric space \u2013 used to compute the similarities between a test instance and the examples \u2013 learnable. The objective of learning this similarity function would be to rank the \u2018useful\u2019 examples ahead of the not useful ones. Unsupervised retrieval models only consider the similarity between the textual content of a test instance and the training instances. However, a supervised retrieval model learned via a standard ranking objective (e.g., noise-contrastive loss [30]) using triplets \u2013 each triplet constituting a test instance (query), a useful example (relevant document), and a not useful one (non-relevant document) \u2013 may specifically capture the inherent semantics of the utility of examples for a particular test instance. Third, diversity of examples is likely to play a part in the effectiveness of ICL because an example that is different from the previous one selected should be more informative to an LLM decoder to generate relevant words which can then be mapped to the correct class. This can also be traced to facet or aspect-based IR which attempts to make the top retrieved set of documents cater to all the latent aspects of an information need [49, 80]. These three core tasks in IR, namely that of QPP, supervised ranking or learning to rank, and diverse or faceted IR have a long history of thorough investigation, which have constantly pushed the boundaries of the state-of-the-art achievable for these tasks. In this perspective paper, we argue that this knowledge gained by the IR community could be beneficial to further improve the effectiveness of generative AI for text. In the next section (Section 2), we provide a brief technical introduction to the concept of ICL, following which, we structure the remainder of the paper into three sections detailing on how each of the specific IR tasks can be applied to an ICL workflow, i.e., QPP for adaptive ICL (Section 3), learning to rank for learning to order examples in ICL (Section 4), and diversity-based and faceted IR for obtaining more informative examples in ICL (Section 5). Although an exhaustive empirical validation of each of these independent ideas for improving ICL is beyond the scope of a perspective paper, we do, however, include a preliminary evaluation to support the\nuse-case of QPP in ICL (Section 6), where we show that adjusting the number of examples in a data-driven manner does lead to significant improvements. We believe that this focused investigation, along with the other ideas presented, would motivate other NLP researchers to apply black-box established IR methodologies or even IR researchers to adjust the state-of-the-art IR methodologies to specifically cater to the downstream predictive tasks in ICL.\n# 2 IN-CONTEXT LEARNING\nWe first provide a brief technical introduction to In-Context Learning (ICL) before describing how the ICL methodology can be improved by incorporating core ideas from IR.\n# 2.1 A Formal Introduction\nIn-context learning (ICL), unlike supervised learning, does not involve training a set of parameters \ud835\udf03on labeled examples. Rather, the posteriors are now a function of the following: a) text of the input test instance, b) the decoder parameters of a pre-trained large language model (LLM), c) a prompt instruction, and d) optionally, a set of \ud835\udc58input examples (commonly called \ud835\udc58-shot learning). Formally,\n\ud835\udc43(\ud835\udc66|x) = \ud835\udc53(x, P\ud835\udc58(x);\ud835\udf19LLM),\n(1)\nwhere, different from a supervised setup, the function \ud835\udc53does not have a parameterized representation that can be learned using a training set with gradient descent. The function itself depends on the pre-trained parameters \ud835\udf19LLM of an LLM, the current inputs for which a label is to be predicted, and a prompt comprising a set of \ud835\udc58 text units denoted by P\ud835\udc58(x). Since the decoder of an LLM generates a sequence of words of the form of \ud835\udc641, . . . ,\ud835\udc64\ud835\udc41(\ud835\udc41being the maximum length of a sequence), the class posterior likelihoods are computed in the following way. A set of classes (say for a \ud835\udc5d-way classification problem) is mapped to \ud835\udc5ddifferent equivalent sets of words, say \ud835\udc49(\ud835\udc66), where \ud835\udc66\u2208Z\ud835\udc5d \u2013 these sets commonly being called verbalisers [32]. For instance, for a binary classification problem (e.g., that of a movie review as shown in Figure 2), \ud835\udc5d= 2 (i.e., \ud835\udc66\u2208{0, 1}), and a reasonable way to define the verbaliser sets could be via the following words: \ud835\udc49(0) = {\u2018false\u2019, \u2018negative\u2019}, and \ud835\udc49(1) = {\u2018true\u2019, \u2018positive\u2019}. Note that the word \u2018learning\u2019 in ICL is a misnomer because there are no updates to the decoder parameters of an LLM. For more details on ICL, please refer to these excellent surveys [20, 47].\n# 2.2 The role of IR\nOne of the most important components of ICL (as shown in Figure 2) is the search component that outputs a top-\ud835\udc58candidate set of similar instances from the training set, i.e., P\ud835\udc58(x) of Equation 1. Although, in principle, it is possible to include random examples from the training set in the prompt, it has been shown that localised examples (i.e., examples that are topically similar to the current instance) yield better performance [44, 47]. The reason why this works can be traced to the fundamental principle of reproducing kernel Hilbert spaces (RKHS) machine learning \u2013 that a predictor function is an aggregation of parameterised kernel functions pivoted around training data instances [60]. It is thus crucial to retrieve as many relevant examples as possible from the training set while imposing a practical constraint on the\nnumber of such examples for efficiency reasons \u2013 a classic trade-off of recall and precision in IR ad-hoc retrieval; the only difference is that relevance for ICL needs to be defined in terms of the utility or usefulness of an example towards the correct prediction. A similar question explored in IR is where to stop reading a ranked list because there is little utility in retrieving documents due to the low probability of finding relevant documents beyond a certain rank cutoff [2, 4]. What is more challenging is that this rank cut-off depends on the number of relevant documents occurring in the collection for a specific query, that is to say, while some queries with well-defined information needs are associated with a small number of relevant documents satisfying the specific relevance criterion, other queries with broader information needs usually are associated with a higher number of relevant documents [6]. In core IR research, this problem is usually addressed by estimating the retrieval qualities of queries \u2013 the assumption being that wellspecified queries yield better retrieval results (in terms of precision and recall), whereas ill-specified ones suffer from poor retrieval quality due to the apparent ambiguity of information need. This motivation paves the path to the following section, where we discuss how query performance prediction (QPP) can also be beneficial to the related problem of retrieving similar examples in ICL.\n# 3 ADAPTIVE ICL \u21a6\u2192QPP?\n \u21a6\u2192 In this section, we describe an adaptive approach to the selection of examples for ICL. We outline analogous principles from IR literature that can be applied in broader tasks.\n# 3.1 A Variable Number of Examples\nThe observation in IR that different queries exhibit different levels of retrieval performance can be utilised for ICL, where we can draw an analogy that some test instances are associated with better candidates for training examples (i.e., examples which are useful in the sense that including them as a part of the prompt leads to correct predictions), and hence including a small number of them should be adequate. On the other hand, the retrieval quality for some test instances (used as queries for ICL) does not yield good candidates. As a result, one needs to look down the ranked list further to collect useful examples. We call this methodology of using a variable number of demonstrations for ICL inference by the name \u2018Adaptive In-Context Learning\u2019, or AICL for short. The idea of AICL centres around choosing the context P\ud835\udc58(x) in a data-driven manner, i.e., making \ud835\udc58 a function of the data (current instance x) itself. This is somewhat analogous to choosing different values of \ud835\udc58for a \ud835\udc58-NN based nonparametric modeling [87], as shown in Figure 3. The motivation is that classifying some instances would be more difficult than others, in which cases they are potentially to be benefited from a larger value of\ud835\udc58(more context). On the other hand, for relatively easy data instances using too much context may be detrimental for effective prediction. Formally speaking, the difference of AICL with that of ICL (Equation 1) is that the value \ud835\udc58, indicating the size of the neighborhood, is no longer a constant. Instead, we denote it by a parameterised function \ud835\udf05(x) such that\n\ud835\udc43(\ud835\udc66|x) = \ud835\udc53(x, P\ud835\udf05(x) (x);\ud835\udf19LLM),\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a34a/a34aea0b-3f4d-4948-b740-8c8d984be620.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Example workflow of In-Context Learning for sentiment classification. The illustrative example shows a sample test instance for which a single demonstration (as retrieved from the training set) does not result in the correct prediction (prediction shown at the top). The example also shows that increasing the number of demonstrations from one to two results in the correct prediction (shown at the bottom). Demonstrations included within the prompt are shown in blue.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f49/3f49210a-1b07-4ec2-8597-68fe056a4185.png\" style=\"width: 50%;\"></div>\nFigure 3: Motivation behind using a variable sized neighborhood for \ud835\udc58-NN classification [87]: An instance close to a decision boundary (black \u2018?\u2019) is likely to have a higher heterogeneity in its class distribution, thus indicating the necessity of a larger neighborhood for an effective classification.\n<div style=\"text-align: center;\">Figure 3: Motivation behind using a variable sized neighborhood for \ud835\udc58-NN classification [87]: An instance close to a decision boundary (black \u2018?\u2019) is likely to have a higher heterogeneity in its class distribution, thus indicating the necessity of a larger neighborhood for an effective classification.</div>\nwhere \ud835\udf05: x \u21a6\u2192{0, . . . , \ud835\udc40}, \ud835\udc40being an upper bound on the number of example instances. We now suggest how unsupervised or supervised approaches may be applied to choose the rank cutoff \ud835\udf05.\n# 3.2 Unsupervised Rank Cutoff\nAmong unsupervised approaches, two main ideas in IR research can be used to determine the number of examples in ICL.\nScore Distribution-based Models. The first thread of work is based on the hypothesis that the scores of relevant and non-relevant documents follow a different statistical distribution, e.g., Arampatzis et al. propose to use a mixture of Normal-Exponential distributions \u2013 Normal for relevant and Exponential for non-relevant documents \u2013 to model the score distribution of top-ranked documents. The work in [2] uses expectation maximisation (EM) to estimate the parameters of this mixture distribution and thereby predict the most likely cutoff rank beyond which the probability of finding a relevant document is considerably low. Such ideas of utilising the characteristic differences between the score distributions of relevant and non-relevant documents have also been used for query performance prediction (QPP) [12]. While an EM from retrieval scores allows provision for applying a variable number of examples, the following are some ICL-specific challenges that need to be researched.\n\u2022 With the notion of relevance being changed to \u2018downstream utility\u2019, the score distributions of useful and not useful examples may not follow the same mixture distribution of NormalExponential as reported in [2, 12]. It will be an interesting future research direction to investigate the latent relations between the similarity scores and the downstream utility of the examples in the context of ICL. \u2022 With a threshold on the score-distributions, it is difficult to restrict the cutoff to a maximum value, which is essential for ICL due to a maximum limit on the input size to an LLM. \u2022 A score distribution-based approach does not explicitly consider the information from the queries themselves (equivalently, the test instances in ICL).\nWe now describe another thread of work in IR research that may help alleviate the last two limitations.\nQPP-based Models. Different from the rank cut-off strategies, query performance prediction (QPP) models seek to estimate the retrieval quality of a query. As a direct analogy, such methods can be applied to the top-similar examples retrieved in ICL with a different objective of predicting the usefulness of the examples. Most of the classic works in QPP involve unsupervised approaches that make use of the information from the set of top-retrieved documents to estimate how topically distinct are the top-retrieved documents from the rest of the collection \u2013 a large difference indicating potentially better retrieval quality [10]. Various evidences extracted from the top-retrieved documents have been shown to be useful for different post-retrieval QPP estimation methods. This includes i) the KL divergence between the language model of the top-retrieved documents and the collection model in Clarity [10], ii) the aggregated values of the information gains of each top-retrieved document with respect to the collection in WIG (Weighted Information Gain) [88], iii) the skew of the RSVs (Retrieval Status Values) measured with variance in NQC (Normalized Query Commitment) [72], iv) ideas based on the clustering hypothesis for a pairwise document similarity matrix [19], and, more recently, v) the characteristics of the embedded space of documents and queries [21, 66]. A suitable adaptation of these existing techniques can be applied in a two-stage pipeline to determine the number of examples in ICL. As a first step, one can employ a QPP methodology to predict the retrieval quality (in terms of the usefulness) of a set of ordered examples \u2013 a high value likely indicating that the useful examples\nAlgorithm 1: LLM \ud835\udc58-shot predictions\nInput: x \u2013 an instance from the training set\nInput: \ud835\udc58(< \ud835\udc40) \u2013 number of examples (max \ud835\udc40)\nOutput: \u0394\ud835\udc5d\u2013 Softmax posteriors\nbegin\n\ud835\udc41\ud835\udc58(x) \u2190{z1, . . . , z\ud835\udc58}\nInstruction \u2190\u201cPredict the type of \u27e8x\u27e9as one of\n{\u27e8\ud835\udc360\u27e9, . . . , \u27e8\ud835\udc36\ud835\udc5d\u22121\u27e9} given the following example\u201d.\nfor \ud835\udc56\u21901 to \ud835\udc58do\nInstruction.append(\u201cExample: \u27e8z\ud835\udc56\u27e9is a representative\nof class \u27e8\ud835\udc66(z\ud835\udc56)\u27e9\u201d)\n\u0394\ud835\udc5d\u2190LLM(Instruction)\nreturn \u0394\ud835\udc5d\ncan potentially be found at the very top ranks, as a result of which, a small number of examples should potentially work well. On the other hand, a low QPP estimate likely indicates that the very top ranked examples are not likely to be useful for downstream prediction, in which case it should be better to employ a large number of examples. This approach of selecting rank cutoffs (with an upper bound) as a function of the QPP scores has been applied to determine a variable depth of relevance assessments required for a robust retrieval evaluation [25].\n# 3.3 Supervised Rank Cutof\nInstead of devising a heuristic to predict the number of training examples to use for a test instance x, i.e., \ud835\udf05(x), a supervised approach can be applied to solve this as a classification problem, i.e., \ud835\udf05\u2261Softmax(xT\ud835\udf03), where \ud835\udf03is a set of layer(s) of parameters. The underlying hypothesis is that if we provide enough training data constituting the optimal number of examples for a range of topical content, we should be able to learn to predict the likely number of examples to use for unseen text during inference time. To train a classifier that maps a text to a number between 1 to \ud835\udc40(the maximum number of examples), it is necessary to obtain the ground-truth labels, i.e., the optimal number of examples, for each instance in the training set. We propose to obtain this by the following methodology: Given a training set instance x, one can employ a similarity function (e.g., BM25) to retrieve a candidate set of \ud835\udc40examples - {z1, . . . , z\ud835\udc40}. Since x is an instance from the training set, we can utilise its label to check if the \ud835\udc58-shot predictions using an LLM are correct. It may happen that correct predictions are obtained for several values of \ud835\udc58\u2208{1, . . . , \ud835\udc40}. Several strategies can be adapted to define the ground-truth number of examples. For instance, one can stop early and simply select the smallest \ud835\udc58that results in a correct prediction. Alternatively, a potentially more robust procedure would be to exhaustively check through all possible values of \ud835\udc58= 1, . . . , \ud835\udc40, and select the one that results in a correct prediction with the least uncertainty [67, 75]. The workflow of this least uncertainty-based selection of the ground truth for the number of ICL examples is shown in Algorithm 2. Algorithm 1, which is invoked during the ground-truth construction, shows a sample prompt template for text classification. After executing Algorithm 2, we obtain a set of ground-truth labels K which could then be used to train a classifier, parameterised\nAlgorithm 2: Optimal number of examples\nInput: T \u2013 a training set of labelled instances\nOutput: K = \u222ax\u2208T\ud835\udc58\u2217(x) \u2013 Number of examples yielding the most\nconfident and correct predictions for each instance x \u2208T\nbegin\nfor x \u2208T do\nmax_confidence \u21900; \ud835\udc58\u2217\u21901\nfor \ud835\udc57\u21900 to \ud835\udc40do\n\u0394\ud835\udc5d\u2190LLM \ud835\udc58-shot predictions(x, \ud835\udc57)\n// Call\nAlgorithm 1, i.e., try to predict with \ud835\udc57examples\n\u02c6\ud835\udc66(x) \u2190argmax\u0394\ud835\udc5d\n// Get the predicted class\nconfidence\u2190\u0394 \u02c6\ud835\udc66(x)I( \u02c6\ud835\udc66(x) = \ud835\udc66(x))\n// Check if\nthe predicted class is the correct one and record\nthe prediction confidence\nif confidence > max_confidence then\nmax_confidence \u2190confidence // Keep track\nof the least uncertain correct prediction\n\ud835\udc58\u2217\u2190\ud835\udc57\nK \u2190K \u222a\ud835\udc58\u2217\nreturn K\nby \ud835\udf03, via optimising:\n(3)\nwhere L is a standard loss function, e.g., the cross-entropy. During inference, for each x \u2208E (E denoting an evaluation set), we propose to apply the classifier \ud835\udf05: x \u21a6\u2192{1, . . . , \ud835\udc40} \u2013 trained via Equation 3 \u2013 to predict the number of examples, and eventually conduct a \ud835\udf05(x)-shot prediction on x (Equation 2).\n# 3.4 Open Research Questions and Challenges\nTill now in this section, we described how unsupervised and supervised approaches can be applied to dynamically select the number of examples to be used for an ICL-based prediction. In this section, we discuss some research directions that could be explored to adapt ICL in alternative ways to further improve its effectiveness. First, we would like to point out to the existing work on generating query variants, as a part of a data augmentation strategy, to devise alternative formulations of the same or similar information needs. This has been shown to improve the effectiveness of rankers [28], query performance prediction [15, 86] relevance feedback [7], and even act as a tool to measure consistency of IR models [70]. Given the recent success of zero-shot query generation capabilities of LLMs [1, 83], we believe that augmenting a test instance with alternative text representations can be useful to eventually improve retrieval quality (and hence potentially improve the downstream ICL effectiveness). The unsupervised and supervised approaches for predicting the number of examples per query (test instance) may also lead to better ICL effectiveness, as per the existing findings that variants do actually help improve QPP [15, 86]. We thus formulate the following two research questions aligned along this direction. \u2022 RQ-3.1: Can query variants generated by LLMs (or otherwise) improve the prediction of the number of examples to use for each instance? \u2022 RQ-3.2: Can relevance feedback based approaches with or without the use of generated query variants help reorder the top-\ud835\udc58\ninitially retrieved candidate set of examples towards a better prediction of the number of examples? The other direction of work involves a dynamic selection of not just the neighborhood size but also other ICL parameters. For instance, the verbaliser [69] sets can be selected dynamically from a set of alternatives based on the input instance. Further, a prompt can also be selected dynamically - again based on the input instance; an unsupervised approach exploring this idea has already been studied in [75]. Generally speaking, the research question that can potentially be explored is the following. \u2022 RQ-3.3: Can other ICL parameters also be chosen in a data-driven manner to lead to better effectiveness, e.g., the verbaliser, the prompt, or even an LLM itself (akin to a mixture of experts)?\n# 4 RANK ICL EXAMPLES \u21a6\u2192SUPERVISED IR?\n \u21a6\u2192 In this section, we discuss another crucial aspect of ICL that can potentially be improved by developing ranking models specifically suited to a different notion of relevance: ICL downstream task-specific usefulness of examples. The concept of an effective example in core neural IR is well-researched, particularly the notion of \u2018hard\u2019 negatives during fine-tuning [27, 36]. These negatives have improved downstream precision on ranking tasks [84] and, more generally, representation learning [29]. Specific to few-shot learning, Rubin et al. [67] employed a noise contrastive estimation (NCE) loss [30] to train a bi-encoder-based pairwise ranker using SBERT [64] embeddings. For training the ranking model, pairs of instances (relevant and non-relevant examples) were collected in the following way. For each pivot instance x from a training set, the authors employed BM25 to constitute a top-\ud835\udc58candidate set of examples. Each pair (x, z\ud835\udc56) was then tested to check whether a 1-shot prediction with z\ud835\udc56was correct, in which case, z\ud835\udc56was classified as a relevant example for x, or else it was considered as a non-relevant one. Batches comprising relevant and non-relevant pairs were then constituted to train a standard NCE loss. While the work of Rubin et al. [67] is a definitive step towards leveraging a task-specific notion of relevance, the investigation should not be considered complete. Several potentially promising research directions should be explored to improve ICL effectiveness further. We now provide a survey of neural ranking literature introducing core paradigms which may be utilised in example selection. Bi-Encoder architecture. A bi-encoder architecture encodes text into a latent representation that can be compared in a vector space; in the context of a retrieval task, these texts would be queries and documents. While a bi-encoder is implemented either with a Siamese network of shared parameters [64] or as a single encoder [48], the latter has become prevalent in recent years [36, 84]. The performance of neural models in search was significantly improved with the release of BERT [18]. Karpukhin et al. [36] first proposed the use of \u2018hard\u2019 negatives mined from BM25 to improve the precision of BERT-based rankers. Gao et al. [27] then proposed a variant of the NCE objective, \u2018Localised Contrastive Estimation\u2019, in which multiple negatives are sampled for each query to account for the variance in the notion of non-relevance. In doing so, they also showed the effectiveness of hard negatives mined from finetuned rankers. To further improve the quality of negative samples, Xiong et al. [85] proposed that a model could choose negatives\nduring training to allow negatives to become continuously \u2018harder\u2019 as fine-tuning progresses. At a conceptual level, bi-encoders generally represent a text as a single embedding by using the representation of the BERT [CLS] token as a proxy for the entire sequence. Other pooling methods are effective, including maximum sequence similarity [13] and late interaction in which a max pooling is performed over the token-level similarity of each query token to document tokens [37]. More recent works instead use a BERT-style encoder with a shallow decoder, which places greater emphasis on the ability of the encoder during pre-training. This architectural development has yielded not only state-of-the-art recall but new pre-training styles, including lexical grounding [71] and text reconstruction [84]. The separate encoding of queries and documents allows for the offline encoding of documents which can vastly improve online latency. This is often coupled with an approximate nearest neighbour search in a vector space [31, 37]. More specifically, after training a bi-encoder model, the parameters of the trained model act as \u2018embeddings\u2019 for each document in the collection. During inference time, a query is first embedded into a vector. Then an approximate nearest neighbour search, e.g., HNSW [50], is conducted on an indexed representation of these dense document vectors. Therefore, exploring the potential benefits gained from efficient, dense endto-end retrieval of training examples for effective ICL can be an interesting research direction. Cross-Encoder architecture. A cross-encoder instead jointly encodes a query and document at inference time [55], allowing deep interactions between texts that are impossible in a bi-encoder architecture. Empirically, these models are more precise than biencoders at the expense of latency, as representations cannot be pre-computed in a standard setting. Both BERT- and T5-based architectures have been proposed [55, 56]; in the case of a BERT model, a feed-forward classification head is used to output class probabilities of relevance [55]. In the case of a sequence-to-sequence model, token logits are taken as surrogates for class probabilities [56]. Recent developments in LLMs have prompted research in these large decoder-only models as text rankers. A list-wise approach is commonly taken in which a model receives multiple documents for a given query and outputs a permutation of the original ranking [62, 77]. The development of these models is still in its infancy but it offers opportunities to investigate highly precise ranking models potentially in sample mining beyond simple ad-hoc search. Therefore, it is reasonable to assume that employing a crossencoder to learn ranking examples by their downstream usefulness should yield better results than a bi-encoder-based approach. An interesting research direction would hence be to investigate the optimal architecture within an ICL pipeline, considering the efficiency-effectiveness trade-off. Teacher Distillation. Moreover, a rich literature exists on distilling the more computationally expensive cross-encoder models into the simpler bi-encoder, the former acting as a teacher model and the latter as a student [31]. Distilling a teacher model into a bi-encoder one allows provision for end-to-end dense retrieval without requiring any sparse index to retrieve a candidate top-\ud835\udc58. Two core paradigms of distillation are homogeneous architecture and heterogeneous architecture distillation. The former generally\nTeacher Distillation. Moreover, a rich literature exists on distilling the more computationally expensive cross-encoder models into the simpler bi-encoder, the former acting as a teacher model and the latter as a student [31]. Distilling a teacher model into a bi-encoder one allows provision for end-to-end dense retrieval without requiring any sparse index to retrieve a candidate top-\ud835\udc58. Two core paradigms of distillation are homogeneous architecture and heterogeneous architecture distillation. The former generally\nwill distill one model into a newly initialised copy via minimisation of a divergence metric over either the final hidden state [43] or internal states such as attention layers [34]. The latter minimises prediction error between teacher and student models commonly via a mean squared error criterion over triplet residuals (residual between positive and negative example scores), allowing for \u2018crossarchitecture knowledge distillation\u2019 [31] as a scalar relevance score is not architecture dependent. This approach has become a core component of many state-of-the-art dense retrieval models, frequently beginning with a cross-encoder teacher used to mine hard negatives and teacher scores before a second stage distillation is performed using the previous distilled model as a teacher [71, 84]. A parallel area of work gaining traction is knowledge sharing between a retrieval system and a generative model [33, 34, 40]. This paradigm is directly correlated with our perspective with recent work finding success in directly optimising a retriever to maximise downstream QA performance [34]. However, these systems are currently brittle with Cuconasu et al. [11] finding that the addition of irrelevant content before a gold label answer as context to a QA system can improve performance against any intuition, suggesting much work can be done in this area to optimise how we present a model with ICL examples.\n# 4.1 Combined Utility of ICL Examples\nThere exists a fundamental difference between relevance of documents in IR and usefulness of examples in ICL. In IR, a document\u2019s relevance is independent of the relevance of another document, and when combined, the information still remains relevant. The situation is more complex for ICL. More precisely speaking, two labeled instances in ICL that are useful examples by themselves (i.e., when used as a 1-shot demonstration results in a correct prediction) may not be yielding a correct prediction when combined for a 2-shot inference [46]. This is likely to happen because the decoder, on obtaining too much of a context, can be biased towards a specific topical cluster of words corresponding to the incorrect class descriptor. While more investigation is required to analyse the empirical likelihood of this phenomenon of \u2018non-cooperation\u2019 between examples occurring, it is worth exploring what adjustments may be needed at a methodology level to even define an ideal ranking of the training data examples for few-shot ICL. The objective in this case is not just to maximise the number of \u2018relevant documents\u2019 (as per the IR analogy) within the top-\ud835\udc58, but rather to ensure the combined usefulness of the examples. A possible direction towards this could be to adapt a listwise ranking model with this modified notion of combined relevance (usefulness). A more computationally efficient approach would be to operate at the level of pairs, i.e., predict which pairs are concordant and discordant. An element of each pair takes on a Boolean value (either useful as a 1-shot example or not), which means that the number of different ways in which a pair can be either concordant or discordant is the number of possible Boolean functions of 2 variables, which is 16 (two such sample functions are Boolean OR, where if one of the examples is useful - so is the combination, and XNOR where a pair is discordant if either of the examples are useful as 1-shot). Since, in the general case, the number of Boolean functions\nof \ud835\udc5bvariables is 22\ud835\udc5b, listwise training with \ud835\udc5b> 3 will likely be computationally prohibitive. Open research questions. Before concluding this section, we now summarise the importance of the following IR-specific research questions for ranking examples in ICL. \u2022 RQ-4.1: Is ICL sensitive to the choice of a neural retrieval model, i.e., can we get an improvement using a basic Siamese model over SBERT as envisaged in [67]? \u2022 RQ-4.2: How faithful is the assumption that a combination of several 1-shot useful examples remain useful for ICL prediction? \u2022 RQ-4.3: If the answer to RQ-4.2 is negative, then there is a significant scope of improving over standard learning to rank approach by explicitly modeling concordance (or the lack of it) of the usefulness of examples in ICL. How can we adjust ranking models, and how much improvement can we achieve over a baseline of the standard few-shot?\n# 5 INFORMATIVE EXAMPLES \u21a6\u2192FACETED IR?\n \u21a6\u2192 In this section, we discuss the last of our proposed verticals towards an effective ICL workflow as outlined in Figure 2, which is that of seeking to provide relevant but diverse contexts to an LLM. More precisely speaking, topical diversity of the examples should play an important role in preventing a decoder bias towards a single topic. This is more true for text generation tasks, such as non-factoid question answering, where an LLM decoder needs to be aware of the different sub-topics to be able to construct a comprehensive answer. Even for classification tasks, diverse examples are likely to help a decoder consider a majority of likely topics (the verbalisers of which map to descriptors of closely related categories) during inference, thus minimising the risks of misclassification. Faceted search has been well studied in IR. Explained simply, a faceted search system extracts the multiple different aspects of the information need from the top-retrieved set and maps each retrieved document to one of these aspects [9, 22\u201324]. Faceted search is particularly useful for queries with broader information needs, where it can assist a user to reformulate their information need to one of the more specific aspects of the broader one, e.g., transform a query \u2018dehumidifiers\u2019 to \u2018price range of dehumidifiers\u2019 where the intention (information need facet) is to buy one [6]. Faceted search is closely related to the concept of diversified ranking [8], where search systems seek to improve the retrieval effectiveness for all possible aspects of a broader information need, e.g., for the earlier example query on \u2018dehumidifiers\u2019, retrieve documents related to information on both the aspects of price ranges, technical specifications, product reviews, and general knowledge on dehumidifiers. Santos et al. [68] propose to leverage query variants (what the paper calls \u2018sub-queries\u2019) and their top-retrieved lists for constructing a list of documents potentially relevant to each facet of the original query. Close to diversity is the concept of fair search which seeks to mitigate biases towards any particular aspects of information need, and recently neural approaches have become common to balance relevance with fairness [57]. From a search user\u2019s perspective, it has been shown that diversified retrieval systems play an important role in improving the search experience, by providing greater coverage of a topic and mitigating potential bias in search results [51]. Similarly, a greater\ntopical coverage and a less topical bias can potentially lead an LLM decoder towards contexts more useful for a downstream task. In fact, Levy et al. [38] show that diversifying the few-shot examples on the basis of abstract syntax tree (AST) structures improves the downstream task of compositional generalisation. This indeed shows a positive direction of research where the considerable volume of work conducted on faceted search and diversification by the IR community can be useful for ICL. However, similar to relevance, the notion of diversity would also need suitable adjustments for ICL. A suitable notion of diversity should not just consider similarities between the input examples but rather also their class labels and, more importantly, similarities in the ways in which they affect an LLM decoder\u2019s generation path. Two examples which both output similar output trees should not be considered diverse. In principle, one can potentially adapt the classification methodology that we proposed to learn the optimal number of examples based on minimising the prediction uncertainties for the purpose of classifying if a given pair of examples is diverse or not. Furthermore, we posit that neural approaches that take into account both relevance and fairness or diversity (again both in the context of downstream ICL) should find a use-case in ICL to help diversify the useful examples.\n# Open research questions. Based on the discussions in this section, we now outline the following research directions.\n\u2022 RQ-5.2: How can the standard notion of diversity be extended to consider the latent dependence between the input and the output of an LLM decoder aligning towards a specific downstream task? \u2022 RQ-5.3: How may existing IR metrics for diversity (e.g., \ud835\udefc-nDCG [8]) be adapted to measure how effective is the example retrieval for downstream ICL? \u2022 RQ-5.4: How can multi-objective neural ranking models be trained to jointly learn downstream specific usefulness and diversity for ICL?\n# 6 PRELIMINARY EVALUATION\nIn this section, we report the results of our initial investigation, which was conducted to answer a subset of research questions of the first vertical, i.e., to develop an effective adaptive version of ICL that can dynamically select the number of examples.\n# 6.1 Research Questions and Dataset\nResearch Questions Investigated. In Section 3.2, we discussed the possibilities of applying QPP-inspired unsupervised approaches for selecting a cutoff point in the ranked list of examples. On the other hand, in Section 3.3 we proposed a classifier-based approach to learn the optimal number of examples. In our experiments, we compare the supervised approach of Algorithm 2 and an NQC-based unsupervised approach for adaptive \ud835\udc58-shot and compare both with static \ud835\udc58-shot on standard datasets for text classification. Explicitly stated, we investigate the following research question. \u2022 CRQ-1: Does adaptively selecting the number of examples in ICL lead to improved downstream effectiveness?\n\u2022 CRQ-1: Does adaptively selecting the number of examples in ICL lead to improved downstream effectiveness?\n\u2022 CRQ-2: Does an unsupervised approach obtain a reasonable performance as compared to a supervised one? Since our experiments answer the above questions, they are not open, unlike the ones we expounded on in this paper. Therefore, we prefix these questions with a \u2018C\u2019 (closed). Dataset. We conduct experiments on three text classification datasets, namely AGNews [16], Jigsaw Toxic Comment1 and SST2 [74]. Below, we provide more details on each dataset. \u2022 AGNews: AGNews is a topic classification dataset constituting news articles from the Web. Each document in the dataset belongs to one of the following 4 classes: World, Sports, Business, and Sci/Tech. The total number of training instances is 120, 000, while the test set size is 7, 600. Each class contains 30, 000 samples from the train set and 1, 900 instances from the test set. \u2022 Jigsaw Toxic Comments: Due to its societal impact, toxicity prediction is a problem of considerable practical interest. This dataset, released by Jigsaw and Google as a part of a Kaggle competition, comprises of comments extracted from Wikipedia\u2019s talk page, each being annotated by human evaluators across six categories representing toxic behaviors: toxic, \u2018severe toxic\u2019, obscene, threat, insult, and \u2018identity hate\u2019. \u2022 SST2: The Stanford Sentiment Treebank (SST) is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in a language. The corpus consists of 11, 855 sentences extracted from movie reviews. Being parsed with the Stanford parser, it constitutes a total of 215, 154 unique phrases from the parse trees, each annotated by 3 human judges. The SST2 (also called SST-binary) dataset is a subset of SST, specifically prepared for the task of binary classification. More precisely, neutral sentences from SST were discarded, and two level, each for the negative and the positive classes were merged thus yielding two classes in total.\n# 6.2 Methods and Parameters\n# Our proposed methods for Adaptive ICL (AICL). As novel methods for adaptive ICL, we employ the following:\nOur proposed methods for Adaptive ICL (AICL). As novel methods for adaptive ICL, we employ the following:\n\u2022 The supervised strategy of Algorithm 2, which we call supervised adaptive ICL (SAICL).\n A QPP-based unsupervised strategy (as per the generic direction outlined in Section 3.2), where we compute the rank cutoff in a relatively simple way, stated as follows. First, given a top-\ud835\udc40set of candidate examples, we compute a normalised value of the NQC estimator [72] (we employ a max normalisation, the normalisation constant being the max NQC value from the training set). We then quantise the normalised values into \ud835\udc40equi-spaced intervals ranging from 0 to the max NQC value. As per the hypothesis that a higher NQC value indicates a better retrieval quality, we employ the inverse linear relation and end up selecting a value close to 0 for higher NQC, and a value close to \ud835\udc40for smaller ones. We call this method QPP-AICL.\nBaselines. As baselines to compare SAICL and QPP-AICL against, we employ the following:\n\u2022 0-shot: This approach simply inputs an instruction without supplying any examples. \u2022 Static ICL (SICL): This refers to the standard method of supplying a fixed number of semantically-similar examples as input, similar to [44]. This is different from AICL in that the number of examples in the prompt is always fixed, however, the examples themselves vary for different test inputs based on semantic similarity. For a fair comparison with AICL methods, we report the results obtained with three different values of \ud835\udc58: 1, \u2308\ud835\udc40 2 \u2309and \ud835\udc40 representing the most conservative (in terms of the input size), average, and least conservative situations. In our case, \ud835\udc40= 5, which means that our standard ICL experiments operate with the 1-shot, 3-shot and 5-shot settings.\nModel and hyper-parameter settings. Among a relatively large number of available choices for available LLMs \u2013 either opensource models or black-box cloud APIs \u2013 we, in particular, conduct our experiments on GPT-J [76]. GPT-J is an open-source GPT-3-like model trained on the Pile dataset [26]. GPT-J-6B yields performance comparable to the 6.7 billion parameter GPT-3 (Curie) on a variety of tasks [81]. The maximum context length (in terms of number of tokens) of GPT-J is 2048. In our experiments, we vary \ud835\udc40- the maximum number of examples, from 1 to 5 (for static ICL this is denoted by \ud835\udc58). For a fair comparison, we use the identical prompt template (as shown in Algorithm 1) and greedy decoding with the same verbalizers across all methods employed in our experiments.\n# 6.3 Results\nTable 1 shows the results (in terms of macro-averaged precision, recall and F1) obtained by the different ICL strategies. It can be seen that SAICL turns out to be the best among the competing approaches. The reason it outperforms the best baseline (static ICL) is that SAICL is able to effectively adapt the number of examples to use, thereby preventing itself from the degradation effects of non-relevant (not useful) examples. In effect, it learns a latent relationship between the topical content and the quantity of context required to guide the decoder\u2019s output in the right direction effectively. Moreover, SAICL is able to operate more effectively with smaller input sizes (see the average value of \ud835\udc58and also the average size of the input in terms of the number of tokens), which means that it is computationally faster as compared to static ICL (SICL). Our observations reveal that CRQ-1 is answered in the affirmative, i.e., an adaptive selection of the number of examples in ICL does improve downstream effectiveness and efficiency. The results with the unsupervised QPP-based approach (QPPAICL) turned out to be worse than the baseline of static ICL. From a broader perspective, this points to an important finding - that offthe-shelf IR approaches without modifications specifically suited to the underlying characteristics of the downstream tasks in ICL may not directly yield improvements in the effectiveness of ICL. For instance, NQC seeks to estimate relevance of documents, and as we have argued before, that relevance has a different interpretation for the ICL examples. Although the observations with QPP-AICL answers CRQ-2 in negative, i.e., an unsupervised approach for an adaptive selection of ICL examples is substantially worse than a supervised one, they do suggest that methodologies developed by\nTable 1: Macro-averaged precision, recall and F1-scores for different in-context learning (ICL) methodologies. The column \ud835\udc58denotes the number of few-shot examples. For AICL approaches, this column denotes the average number of examples used for the respective method. \u2018AIS\u2019 denotes the average input size measured in terms of the number of tokens rounded off to the nearest integer.\nEvaluation\nDataset\nMethod\n\ud835\udc58\nPrecision\nRecall\nF-score\nAIS\nAGNews\n0-shot\n0\n0.6569\n0.5932\n0.5849\n60\nSICL\n1\n0.9015\n0.9017\n0.9016\n125\nSICL\n3\n0.9008\n0.8997\n0.8989\n252\nSICL\n5\n0.8963\n0.8930\n0.8917\n380\nQPP-AICL\n3\n0.8545\n0.8499\n0.8486\n220\nSAICL\n1.87\n0.9080\n0.9096\n0.9067\n175\nToxicity\n0-shot\n0\n0.5689\n0.6238\n0.5769\n103\nSICL\n1\n0.5760\n0.6989\n0.5505\n195\nSICL\n3\n0.6092\n0.7180\n0.6254\n335\nSICL\n5\n0.6078\n0.7248\n0.6217\n431\nQPP-AICL\n3\n0.5906\n0.6942\n0.5977\n289\nSAICL\n3.46\n0.6194\n0.6983\n0.6303\n359\nSST2\n0-shot\n0\n0.7503\n0.5022\n0.3379\n30\nSICL\n1\n0.8703\n0.8703\n0.8703\n61\nSICL\n3\n0.9140\n0.9137\n0.9137\n121\nSICL\n5\n0.9245\n0.9230\n0.9230\n181\nQPP-AICL\n3\n0.8556\n0.8479\n0.8470\n106\nSAICL\n4.12\n0.9302\n0.9304\n0.9302\n154\nresearchers in the future for answering any of the open research questions discussed in this paper should be fundamentally grounded in modeling the notion of relevance (usefulness of examples) in a robust and effective manner.\n# 7 CONCLUSION\nIn this perspective paper, we discuss how some of the recent developments in generative AI (specifically in-context learning or ICL) can provide a scope to IR/NLP researchers to revisit some of the well-researched IR topics in a new light, where the notion of relevance of a document to an information need changes to that of usefulness of a few-shot example for a downstream AI task, e.g., text classification, question answering etc. More specifically, we suggest three main verticals in which this research can be structured - each offering a set of open questions related to core IR research. The first vertical aims at adaptively adjusting an ICL workflow, e.g., choosing the number of examples to be used in a data-driven manner. Initial empirical investigations reported in this perspective paper shows that this direction is promising. The second vertical mainly covers devising novel ranking models to better distinguish (and thereby retrieve at better ranks) a useful few-shot context from a noisy one. Finally, the third vertical concerns an investigation of topical diversity in the few-shot examples for better downstream prediction. We believe that the research questions that we have proposed in this paper will benefit the research community to exploit this synergy between ICL and IR, and eventually guide the development of new algorithms and techniques.\n# REFERENCES\n[1] Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul Thomas. 2023. Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR \u201923). Association for Computing Machinery, New York, NY, USA, 1869\u20131873. https://doi.org/10.1145/3539618.3591960 [2] Avi Arampatzis, Jaap Kamps, and Stephen Robertson. 2009. Where to Stop Reading a Ranked List? Threshold Optimization Using Truncated Score Distributions. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Boston, MA, USA) (SIGIR \u201909). Association for Computing Machinery, New York, NY, USA, 524\u2013531. https://doi.org/10.1145/1571941.1572031 [3] Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher R\u00e9. 2022. Ask Me Anything: A simple strategy for prompting language models. arXiv:2210.02441 [cs.CL] [4] Dara Bahri, Yi Tay, Che Zheng, Donald Metzler, and Andrew Tomkins. 2020. Choppy: Cut Transformer for Ranked List Truncation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 1513\u20131516. https://doi.org/10.1145/3397271.3401188 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901. [6] Ben Carterette, Evangelos Kanoulas, Mark M. Hall, and Paul D. Clough. [n. d.]. Overview of the TREC 2014 Session Track. In Proc. of TREC 2014. [7] Anirban Chakraborty, Debasis Ganguly, and Owen Conlan. 2020. Retrievability based Document Selection for Relevance Feedback with Automatically Generated Query Variants. In CIKM. ACM, 125\u2013134. [8] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan B\u00fcttcher, and Ian MacKinnon. 2008. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Singapore, Singapore) (SIGIR \u201908). Association for Computing Machinery, New York, NY, USA, 659\u2013666. https://doi.org/10.1145/1390334.1390446 [9] Charles L. A. Clarke, Nick Craswell, and Ian Soboroff. 2009. Overview of the TREC 2009 Web Track. In Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 17-20, 2009 (NIST Special Publication, Vol. 500-278), Ellen M. Voorhees and Lori P. Buckland (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec18/ papers/WEB09.OVERVIEW.pdf [10] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002. Predicting Query Performance. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201902). Association for Computing Machinery, New York, NY, USA, 299\u2013306. [11] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. arXiv:2401.14887 [cs.IR] [12] Ronan Cummins. 2014. Document Score Distribution Models for Query Performance Inference and Prediction. ACM Trans. Inf. Syst. 32, 1, Article 2 (2014), 28 pages. [13] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Paris, France) (SIGIR\u201919). Association for Computing Machinery, New York, NY, USA, 985\u2013988. https://doi.org/10.1145/3331184.3331303 [14] Suchana Datta, Debasis Ganguly, Derek Greene, and Mandar Mitra. 2022. DeepQPP: A Pairwise Interaction-based Deep Learning Model for Supervised Query Performance Prediction. In WSDM \u201922: The Fifteenth ACM International Conference on Web Search and Data Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022, K. Selcuk Candan, Huan Liu, Leman Akoglu, Xin Luna Dong, and Jiliang Tang (Eds.). ACM, 201\u2013209. https://doi.org/10.1145/3488560.3498491 [15] Suchana Datta, Debasis Ganguly, Mandar Mitra, and Derek Greene. 2023. A Relative Information Gain-based Query Performance Prediction Framework with Generated Query Variants. ACM Trans. Inf. Syst. 41, 2 (2023), 38:1\u201338:31. [16] Gianna M. Del Corso, Antonio Gull\u00ed, and Francesco Romani. 2005. Ranking a Stream of News. In Proceedings of the 14th International Conference on World Wide Web (Chiba, Japan) (WWW \u201905). Association for Computing Machinery, New York, NY, USA, 97\u2013106. https://doi.org/10.1145/1060745.1060764 [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171\u20134186.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171\u20134186. https://doi.org/10.18653/v1/N19-1423 [19] Fernando Diaz. 2007. Performance Prediction Using Spatial Autocorrelation. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201907). Association for Computing Machinery, New York, NY, USA, 583\u2013590. [20] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. arXiv:2301.00234 [cs.CL] [21] Guglielmo Faggioli, Nicola Ferro, Cristina Muntean, Raffaele Perego, and Nicola Tonellotto. 2023. A Geometric Framework for Query Performance Prediction in Conversational Search. In Proceedings of 46th international ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2023 July 23\u201327, 2023, Taipei, Taiwan. ACM. https://doi.org/10.1145/3539618.3591625 [22] Debasis Ganguly, Manisha Ganguly, Johannes Leveling, and Gareth J. F. Jones. 2013. TopicVis: a GUI for topic-based feedback and navigation. In SIGIR. ACM, 1103\u20131104. [23] Debasis Ganguly and Gareth J. F. Jones. 2018. A non-parametric topical relevance model. Inf. Retr. J. 21, 5 (2018), 449\u2013479. [24] Debasis Ganguly, Johannes Leveling, and Gareth J. F. Jones. 2013. An LDAsmoothed relevance model for document expansion: a case study for spoken document retrieval. In SIGIR. ACM, 1057\u20131060. [25] Debasis Ganguly and Emine Yilmaz. 2023. Query-specific Variable Depth Pooling via Query Performance Prediction. In SIGIR. ACM, 2303\u20132307. [26] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027 (2020). [27] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline. CoRR abs/2101.08751 (2021). arXiv:2101.08751 https://arxiv.org/abs/2101.08751 [28] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot Dense Retrieval without Relevance Labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1762\u20131777. https://doi.org/10.18653/V1/2023.ACL-LONG.99 [29] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 6894\u20136910. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.552 [30] Michael Gutmann and Aapo Hyv\u00e4rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 9), Yee Whye Teh and Mike Titterington (Eds.). PMLR, Chia Laguna Resort, Sardinia, Italy, 297\u2013304. https: //proceedings.mlr.press/v9/gutmann10a.html [31] Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and Allan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation. CoRR abs/2010.02666 (2020). arXiv:2010.02666 https://arxiv.org/abs/2010.02666 [32] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong Sun. 2021. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. arXiv preprint arXiv:2108.02035 (2021). [33] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, Paola Merlo, J\u00f6rg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, 874\u2013880. https://doi.org/10.18653/V1/2021.EACL-MAIN.74 [34] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models. J. Mach. Learn. Res. 24 (2023), 251:1\u2013251:43. http://jmlr.org/papers/v24/ 23-0037.html [35] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]\n[36] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 6769\u20136781. https://doi.org/10.18653/ V1/2020.EMNLP-MAIN.550 [37] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 39\u201348. https://doi.org/10.1145/3397271.3401075 [38] Itay Levy, Ben Bogin, and Jonathan Berant. 2023. Diverse Demonstrations Improve In-context Compositional Generalization. arXiv:2212.06800 [cs.CL] [39] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-main.703 [40] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html [41] Minghan Li, Xueguang Ma, and Jimmy Lin. 2022. An Encoder Attribution Analysis for Dense Passage Retriever in Open-Domain Question Answering. In Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022). Association for Computational Linguistics, Seattle, U.S.A., 1\u201311. https: //doi.org/10.18653/v1/2022.trustnlp-1.1 [42] Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu Chen. 2023. Fewshot In-context Learning on Knowledge Base Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 6966\u20136980. https: //doi.org/10.18653/v1/2023.acl-long.385 [43] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021. In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP, RepL4NLP@ACL-IJCNLP 2021, Online, August 6, 2021, Anna Rogers, Iacer Calixto, Ivan Vulic, Naomi Saphra, Nora Kassner, Oana-Maria Camburu, Trapit Bansal, and Vered Shwartz (Eds.). Association for Computational Linguistics, 163\u2013173. https://doi.org/10.18653/V1/2021.REPL4NLP-1.17 [44] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, Eneko Agirre, Marianna Apidianaki, and Ivan Vuli\u0107 (Eds.). Association for Computational Linguistics, Dublin, Ireland and Online, 100\u2013114. https://doi.org/10.18653/v1/2022.deelio1.10 [45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. http://arxiv.org/abs/1907.11692 [46] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 8086\u20138098. https://doi.org/10.18653/v1/2022.acllong.556 [47] Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, and Mehran Kazemi. 2024. Incontext Learning with Retrieved Demonstrations for Language Models: A Survey. arXiv:2401.11624 [cs.CL] [48] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR: Contextualized Embeddings for Document Ranking. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019, Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 1101\u20131104. https://doi.org/10.1145/3331184.3331317 [49] Mohammed Najah Mahdi, Abdul Rahim Ahmad, Roslan Ismail, Hayder Natiq, and Mohammed Abdulameer Mohammed. 2020. Solution for Information Overload Using Faceted Search\u2013A Review. IEEE Access 8 (2020), 119554\u2013119585. https:\n//doi.org/10.1109/ACCESS.2020.3005536 [50] Yury A. Malkov and Dmitry A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. IEEE Trans. Pattern Anal. Mach. Intell. 42, 4 (2020), 824\u2013836. https://doi.org/10.1109/ TPAMI.2018.2889473 [51] David Maxwell, Leif Azzopardi, and Yashar Moshfeghi. 2019. The impact of result diversification on search behaviour and performance. Inf. Retr. J. 22, 5 (2019), 422\u2013446. [52] Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. 2023. In-Context Learning for Text Classification with Many Labels. In Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, Dieuwke Hupkes, Verna Dankers, Khuyagbaatar Batsuren, Koustuv Sinha, Amirhossein Kazemnejad, Christos Christodoulopoulos, Ryan Cotterell, and Elia Bruni (Eds.). Association for Computational Linguistics, Singapore, 173\u2013184. https://doi.org/10.18653/v1/ 2023.genbench-1.14 [53] Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large Language Model Augmented Narrative Driven Recommendations. arXiv:2306.02250 [cs.IR] [54] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021. Large Dual Encoders Are Generalizable Retrievers. arXiv:2112.07899 [cs.IR] [55] Rodrigo Frassetto Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. CoRR abs/1901.04085 (2019). arXiv:1901.04085 http://arxiv.org/abs/1901. 04085 [56] Rodrigo Frassetto Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 708\u2013718. https: //doi.org/10.18653/V1/2020.FINDINGS-EMNLP.63 [57] Harrie Oosterhuis. 2021. Computationally Efficient Optimization of Plackett-Luce Ranking Models for Relevance and Fairness. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (, Virtual Event, Canada,) (SIGIR \u201921). Association for Computing Machinery, New York, NY, USA, 1023\u20131032. https://doi.org/10.1145/3404835.3462830 [58] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [59] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731-Paper-Conference.pdf [60] Vern I. Paulsen and Mrinal Raghupathi. 2016. An Introduction to the Theory of Reproducing Kernel Hilbert Spaces. Cambridge University Press. [61] Ronak Pradeep, Kai Hui, Jai Gupta, Adam D. Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, and Vinh Q. Tran. 2023. How Does Generative Retrieval Scale to Millions of Passages? arXiv:2305.11841 [cs.IR] [62] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze! CoRR abs/2312.02724 (2023). https://doi.org/10.48550/ARXIV.2312.02724 arXiv:2312.02724 [63] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [64] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv:1908.10084 [cs.CL] [65] Haggai Roitman, Yosi Mass, Guy Feigenblat, and Roee Shraga. 2020. Query Performance Prediction for Multifield Document Retrieval. In Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval (Virtual Event, Norway) (ICTIR \u201920). Association for Computing Machinery, New York, NY, USA, 49\u201352. https://doi.org/10.1145/3409256.3409821 [66] Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, and Gareth J.F. Jones. 2019. Estimating Gaussian mixture models in the local neighbourhood of embedded word vectors for query performance prediction. Information Processing and Management 56, 3 (2019), 1026 \u2013 1045. [67] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve Prompts for In-Context Learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 2655\u20132671. https://doi.org/10.18653/v1/2022.naacl-main.191 [68] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting query reformulations for web search result diversification. In Proceedings of the 19th International Conference on World Wide Web (Raleigh, North Carolina, USA) (WWW \u201910). Association for Computing Machinery, New York, NY, USA, 881\u2013890. https://doi.org/10.1145/1772690.1772780\n[69] Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting Cloze-Questions for FewShot Text Classification and Natural Language Inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 255\u2013269. https://doi.org/10. 18653/v1/2021.eacl-main.20 [70] Procheta Sen, Sourav Saha, Debasis Ganguly, Manisha Verma, and Dwaipayan Roy. 2022. Measuring and Comparing the Consistency of IR Models for Query Pairs with Similar and Different Information Needs. In CIKM. ACM, 4449\u20134453. [71] Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao, Linjun Yang, and Daxin Jiang. 2023. LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https: //openreview.net/pdf?id=PfpEtB3-csK [72] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. 2012. Predicting Query Performance by Query-Drift Estimation. ACM Trans. Inf. Syst. 30, 2, Article 11 (2012), 35 pages. [73] Ashutosh Singh, Debasis Ganguly, Suchana Datta, and Craig MacDonald. 2023. Unsupervised Query Performance Prediction for Neural Models with Pairwise Rank Preferences. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.). ACM, 2486\u20132490. https://doi. org/10.1145/3539618.3592082 [74] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, 1631\u20131642. https://aclanthology.org/D13-1170 [75] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 819\u2013862. https://doi.org/10.18653/v1/2022.acl-long.60 [76] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864 [cs.CL] [77] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 14918\u201314937. https://aclanthology.org/2023.emnlp-main.923 [78] Yuting Tang, Ratish Puduppully, Zhengyuan Liu, and Nancy Chen. 2023. Incontext Learning of Large Language Models for Controlled Dialogue Summarization",
    "paper_type": "method",
    "attri": {
        "background": "With the increasing ability of large language models (LLMs), in-context learning (ICL) has evolved as a new paradigm for natural language processing (NLP), where instead of fine-tuning the parameters of an LLM specific to a downstream task with labeled examples, a small number of such examples is appended to a prompt instruction for controlling the decoder\u2019s generation process. This suggests that a test instance in ICL is analogous to a query in information retrieval (IR), and similar examples in ICL retrieved from a training set relate to a set of documents retrieved from a collection in IR.",
        "problem": {
            "definition": "The problem addressed in this paper is the challenge of effectively selecting few-shot examples for ICL to enhance the performance of LLMs in downstream tasks.",
            "key obstacle": "The main difficulty lies in the existing methods that do not adequately define relevance in terms of the utility of examples for specific tasks, leading to suboptimal selection of few-shot examples."
        },
        "idea": {
            "intuition": "The idea stems from the observation that the effectiveness of few-shot examples can be improved by redefining the notion of relevance based on their utility for downstream tasks.",
            "opinion": "The proposed idea involves training a supervised ranking model to optimally select few-shot examples that enhance the performance of ICL.",
            "innovation": "The key innovation is the introduction of a task-specific notion of relevance that focuses on the utility of examples for correct predictions, contrasting with traditional relevance definitions in IR."
        },
        "method": {
            "method name": "Adaptive In-Context Learning (AICL)",
            "method abbreviation": "AICL",
            "method definition": "AICL is a method that adapts the number of few-shot examples used in ICL based on the quality of the examples and the nature of the test instance.",
            "method description": "AICL dynamically selects a variable number of examples for ICL, making the selection process data-driven.",
            "method steps": [
                "Identify the test instance for which predictions are needed.",
                "Retrieve a candidate set of examples from the training set.",
                "Evaluate the quality of examples using a ranking model.",
                "Select an optimal number of examples based on their utility for the specific instance.",
                "Append the selected examples to the prompt and generate predictions."
            ],
            "principle": "The effectiveness of AICL lies in its ability to tailor the number of examples based on their predicted utility, thus preventing degradation from irrelevant examples."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three text classification datasets: AGNews, Jigsaw Toxic Comments, and SST2, comparing AICL with static ICL methods.",
            "evaluation method": "The performance was assessed using macro-averaged precision, recall, and F1 scores, comparing adaptive and static methods across varying numbers of examples."
        },
        "conclusion": "The initial findings indicate that adaptively selecting the number of examples in ICL significantly improves downstream effectiveness and efficiency compared to static methods.",
        "discussion": {
            "advantage": "AICL demonstrates improved performance by adapting the number of examples based on the specific needs of the test instance, leading to more relevant context for predictions.",
            "limitation": "The unsupervised approach for selecting examples performed worse than the supervised method, indicating that off-the-shelf IR techniques may not directly translate to improvements in ICL.",
            "future work": "Future research should focus on refining the definition of usefulness in ICL and exploring additional methods for example selection and ranking."
        },
        "other info": {
            "authors": [
                {
                    "name": "Debasis Ganguly",
                    "affiliation": "University of Glasgow",
                    "email": "Debasis.Ganguly@glasgow.ac.uk"
                },
                {
                    "name": "Manish Chandra",
                    "affiliation": "University of Glasgow",
                    "email": "m.chandra.1@research.gla.ac.uk"
                },
                {
                    "name": "Andrew Parry",
                    "affiliation": "University of Glasgow",
                    "email": "a.parry.1@research.gla.ac.uk"
                }
            ],
            "conference": "SIGIR '24, July 14\u201318, 2024, Washington, DC, USA",
            "keywords": [
                "Large Language Models",
                "In-Context Learning",
                "Ranking Models",
                "Query Performance Prediction"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) has evolved as a new paradigm for natural language processing (NLP), where a small number of examples is appended to a prompt instruction for controlling the decoder\u2019s generation process."
        },
        {
            "section number": "1.2",
            "key information": "The problem addressed in this paper is the challenge of effectively selecting few-shot examples for ICL to enhance the performance of LLMs in downstream tasks."
        },
        {
            "section number": "3.1",
            "key information": "Adaptive In-Context Learning (AICL) is a method that adapts the number of few-shot examples used in ICL based on the quality of the examples and the nature of the test instance."
        },
        {
            "section number": "3.2",
            "key information": "The proposed idea involves training a supervised ranking model to optimally select few-shot examples that enhance the performance of ICL."
        },
        {
            "section number": "4.1",
            "key information": "AICL dynamically selects a variable number of examples for ICL, making the selection process data-driven, which influences the outcomes of in-context learning."
        },
        {
            "section number": "6.1",
            "key information": "The unsupervised approach for selecting examples performed worse than the supervised method, indicating that off-the-shelf IR techniques may not directly translate to improvements in ICL."
        },
        {
            "section number": "7",
            "key information": "The initial findings indicate that adaptively selecting the number of examples in ICL significantly improves downstream effectiveness and efficiency compared to static methods."
        }
    ],
    "similarity_score": 0.7679862443683607,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/_In-Context Learning_ or_ How I learned to stop worrying and love _Applied Information Retrieval_.json"
}