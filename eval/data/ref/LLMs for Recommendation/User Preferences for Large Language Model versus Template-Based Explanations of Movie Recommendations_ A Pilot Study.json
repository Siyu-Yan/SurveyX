{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.06297",
    "title": "User Preferences for Large Language Model versus Template-Based Explanations of Movie Recommendations: A Pilot Study",
    "abstract": "Recommender systems have become integral to our digital experiences, from online shopping to streaming platforms. Still, the rationale behind their suggestions often remains opaque to users. While some systems employ a graph-based approach, offering inherent explainability through paths associating recommended items and seed items, non-experts could not easily understand these explanations. A popular alternative is to convert graph-based explanations into textual ones using a template and an algorithm, which we denote here as ''template-based'' explanations. Yet, these can sometimes come across as impersonal or uninspiring. A novel method would be to employ large language models (LLMs) for this purpose, which we denote as ''LLM-based''. To assess the effectiveness of LLMs in generating more resonant explanations, we conducted a pilot study with 25 participants. They were presented with three explanations: (1) traditional template-based, (2) LLM-based rephrasing of the template output, and (3) purely LLM-based explanations derived from the graph-based explanations. Although subject to high variance, preliminary findings suggest that LLM-based explanations may provide a richer and more engaging user experience, further aligning with user expectations. This study sheds light on the potential limitations of current explanation methods and offers promising directions for leveraging large language models to improve user satisfaction and trust in recommender systems.",
    "bib_name": "albert2024userpreferenceslargelanguage",
    "md_text": "# User Preferences for Large Language Model versu Template-Based Explanations of Movie Recommendations: A Pilot Study\nJulien Albert UNamur .albert@unamur.be Martin Balfroid UNamur martin.balfroid@unamur.be Miriam Doh ULB-UMONS miriam.doh@ulb.be Jeremie Bogaert UCLouvain jeremie.bogaert@uclouvain.b\nJulien Albert\njulien.albert@unamur.be martin.balfroid@unamur.be\nca ons.be Liesbet De Vos UNamur liesbet.devos@unamur.be Bryan Renard Multitel-UNamur renard@multitel.be bryan.renard@unamur.be Vincent Stragier UMONS vincent.stragier@umons.ac.be Emmanuel Jean Multitel jean@multitel.be\nAbstract\u2014Recommender systems have become integral to our digital experiences, from online shopping to streaming platforms. Still, the rationale behind their suggestions often remains opaque to users. While some systems employ a graph-based approach, offering inherent explainability through paths associating recommended items and seed items, non-experts could not easily understand these explanations. A popular alternative is to convert graph-based explanations into textual ones using a template and an algorithm, which we denote here as \u201ctemplate-based\u201d explanations. Yet, these can sometimes come across as impersonal or uninspiring. A novel method would be to employ large language models (LLMs) for this purpose, which we denote as \u201cLLM-based\u201d. To assess the effectiveness of LLMs in generating more resonant explanations, we conducted a pilot study with 25 participants. They were presented with three explanations: (1) traditional template-based, (2) LLM-based rephrasing of the template output, and (3) purely LLM-based explanations derived from the graph-based explanations. Although subject to high variance, preliminary findings suggest that LLM-based explanations may provide a richer and more engaging user experience, further aligning with user expectations. This study sheds light on the potential limitations of current explanation methods and offers promising directions for leveraging large language models to improve user satisfaction and trust in recommender systems. Index Terms\u2014Large Language Models, Recommender Systems, Explainability, GD6\narXiv:2409.06297v1\nI. INTRODUCTION\nMost of us wonder daily why platforms like Facebook and YouTube recommend specific people or videos to us. The lack of transparency in these recommendations often leaves us without a clear explanation. This can degrade user confidence, recommendation acceptance and, more generally, the user experience [1]. To address those important concerns, a growing field of research focuses on making recommendation systems more transparent and explainable [1]\u2013[3]. A promising approach is to use large language models (LLMs) to generate\njeremie.bogaert@uclouvain.be\nexplanations1 for recommendations. LLMs are initially pretrained on extensive corpora, allowing them to perform a versatile range of natural language processing (NLP) tasks [4]. The generated text is typically well-written and clear, making it easy for humans to understand. Motivated by these perspectives, we put them to the test in the generation of explanations for recommendations during the TRAIL\u201923 Workshop2. Concretely, we defined two goals to address during the workshop. The first is implementing working examples of recommendation explanations generated with LLMs using various recommendation methods and LLM models. This way, we could assess the technical possibilities and limitations of LLMs. The second goal is to evaluate explanations generated by different LLM models and recommendation methods to understand their qualities and their limitations in this context. To achieve this goal, we designed a user-based evaluation method to assess explanations w.r.t. different explanatory goals and subjective properties [1].\n# II. TECHNICAL EXPLORATION AND IMPLEMENTATION\nAs shown in Fig. 1, we propose a pipeline that takes user preferences (i.e., past interactions with items) as input, and generates explained recommendations as output. The most important design choice is to separate the recommendation and explanation processes, only using LLMs to explain items previously recommended by a standalone recommendation method. We choose to use classic recommendation to ensure valid recommendation, as hallucination is an important issue with LLMs [5]. Moreover, this choice allows us to isolate the explanation task, empowering us to compare explanations\n1In the context of this abstract, when we refer to the generation of LLMsbased recommendations explanations, we actually mean using an LLMs to rephrase an explanation or interpret a graph representation of an explainable recommendation. 2https://trail.ac/en/trail-summer-workshops/ the-trail-summer-workshop-2023/, more details in the Appendix\ncreated by a baseline explanation method, with explanations written by LLMs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/437e/437e0406-9b0f-43a1-8fc1-fa8c1b04b69a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Pipeline used to guide our experiments. Methods and models used for the evaluation part are in fuchsia.</div>\nRegarding the recommendation methods, we focused on graph-based methods. More specifically, we used Personalized PageRank [6] and RippleNet [7], both of which generate explanations based on a graph of the past interactions between users and items. This graph is augmented with knowledge about the movie domain, to further guide the recommendation system. Those methods also provide explanations for recommendations in the form of paths from the seed items to the recommended ones. The datasets used for experimentation were Movielens1M3 and MindReader4. For the user-based evaluation, we only used Movielens-1M in combination with the Personalized PageRank. We are interested in textual explanations, since they convey rich information to the user [1]. The main existing approaches are template-based and generation-based [3]. As a baseline, we use a template-based approach, which transcripts path-based explanations into text. We compare this baseline method to LLM-based methods for generating explanations, inspired by the literature on the topic, e.g., PEPLER [8]. Large Language Models (LLMs) are now some of the world\u2019s most famous NLP models due to the publicity made by OpenAI with ChatGPT, which uses GPT-3.5-turbo and GPT-4 (SOTA). They can perform various NLP tasks. Current LLMs use a decoder-only architecture based on the transformer\u2019s architecture [9]. They are trained to give a probability of distribution over the vocabulary of tokens, allowing to predict the next token. The tokens are subparts of sentences, and the vocabulary of tokens, fixed and based on the training data, is often built using byte pair encoding (BPE) [10], [11]. To produce sequences of tokens, we used greedy decoding with Llama 2 70B Chat and the default technique (which we don\u2019t know of) when using GPT-4. Greedy decoding only considers the most probable token at each generation step, which is timeefficient, unlike other techniques. We decided on using greedy decoding for our exploration study but plan to investigate other strategies in the future. We considered two methods for generating explanations for movie recommendations. We aimed to measure how effec-\n3https://grouplens.org/datasets/movielens/ 4https://mindreader.tech/dataset/\ntively each approach could deliver concise yet informative explanations to users that align with their expectations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/28a9/28a94f22-67d7-4710-aa22-5ff5253f3444.png\" style=\"width: 50%;\"></div>\nFig. 2. Illustration of the three types of explanations compared in the user evaluation. Three types of explanations were finally chosen for the userbased evaluation (as shown in Fig. 2): 1) Template-based: our baseline method, which uses a template to generate explanations algorithmically based on the edges and nodes of the explanation paths; 2) LLM-based: which uses LLMs to generate the explanation. We explored two variations: a) LLM-based rephrasing: rephrase the template-based explanation; b) LLM-based graph-to-text: the model deduces the reasoning behind the recommendation given a knowledge graph as context. Between the two LLM variants, only the context varies, either the template-based explanation or the graph. The definition of the task is, therefore, the same for both: to explain why a particular film has been recommended. To ensure a fairly consistent format across each generation, we constrained the LLM\u2019s behaviour [12] by specifying that only one paragraph should be used and that it should be written in layman\u2019s terms. Otherwise, the model tended to ramble and use technical terms that could confuse the user.\n# III. USER-BASED EVALUATION\n# A. Methodology\nPart of our project\u2019s goal was to perform a user-based evaluation of the three types of explanations generated by our pipeline. We drew inspiration from [14] to craft the structure for our evaluation procedure (Fig. 4), albeit with slight modifications due to the inclusion of LLM-generated explanations. We decided to focus on the following key aspects: 1) Assessing user expectations of recommendation explanations using the seven goals from [1], also used by [14]. 2) Presenting a recommended item to the user alongside multiple alternative explanations (based on a watching profile selected by the user beforehand).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65d8/65d8bc6d-07cf-4ec8-ab23-49b9511b3720.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b587/b587fafd-4668-4df4-8170-0e3e25dd0396.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) LLM-based rephrasing</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25f6/25f6e4b3-5899-46d6-81a2-de491a52ec68.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) LLM-based graph-to-text</div>\nFig. 3. Here is an example of the same recommendation presented in the same format as the prompt in Liu et al. [13]. Black-colored text outlines the task, red-colored text highlights the formatting guidelines, and blue-colored text is either the given template or the graph.\n3) Requesting users to assess the explanations based on their general preference and measure the extent to which each explanation satisfies the seven goals. 4) Gathering qualitative insights via open question on user expectations and explanation assessments.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3fa/b3fa6486-cda3-450c-85df-964fe3eb5ebe.png\" style=\"width: 50%;\"></div>\nAfterward, we conducted a dry run to validate the clarity of the questionnaire and assess its length to prevent evaluator fatigue. Subsequently, we rolled out the questionnaire to multiple evaluators to gather their responses.\n# B. Results\nWe conducted 25 user tests with TRAIL\u201923 Workshop participants (researchers in AI). The small number of participants means that no statistically robust conclusions can be drawn, but certain trends can be observed. Concerning the user\u2019s expectations about explanations, we observe no difference in importance for the seven goals investigated. However, concerning user assessment of the generated explanations (see Fig. 5), we observe that the explanation generated by the LLM from a knowledge graph performs best w.r.t. the 7 goals. And this result is confirmed\nby the participants\u2019 general assessment of the explanations. According to the participants, this explanation type is mainly preferred because it\u2019s often more detailed and more pleasant to read. However, beyond the small sample size (n = 25), it is important to point out the significant variance in these last results. This indicates strong differences between participants in the way they perceive explanations, which is a result that should be investigated further.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/384e/384e2e17-8a6d-4c4c-acae-40799d9af220.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Fig. 5. User assessment of explanations w.r.t. the 7 goals. about the recommendation explanations.</div>\nMoreover, we observed that LLMs often introduce additional details. Although most of them seem correct, they do not come from the knowledge graph and are therefore not verified. This may be due to the model\u2019s ability to draw on cultural references [12], in this case, movie titles. If this is undesirable, a workaround is to use numbered labels instead of movie titles. This will limit the model only to use in-context information.\n# IV. FUTURE WORKS\nIn the future, we would like to explore various models. In particular, we would like to focus on smaller LLMs, to understand how model size affects efficiency. We expect that, due to their size, smaller models may struggle to generate explanations based on the knowledge graph, but may be sufficient for rephrasing the template-based explanations. This is particularly relevant considering the substantial computational requirements of larger LLMs. Furthermore, fine-tuning could be explored, as well as advanced prompting techniques like chain-of-thought [15] and self-consistency [16] (which may appeal to users wanting more detailed reasoning). Another interesting approach might be to specify the explanation generation task by memetic proxy [12], i.e., to use the model\u2019s ability to draw on cultural references, metaphors, analogies, role-playing, and so on. Finally, instead of only relying on user-based evaluations, we aim to use mixed-methods evaluation to draw a com-\nplete picture of LLM\u2019s explanation generation capabilities for recommendations. This evaluation would combine heuristicsbased methods (based on classical metrics for text quality like BLEU [17] and ROUGE [18] scores), explanation quality metrics (e.g., [8]), and user-based methods. Such user-based methods could include qualitative (e.g., interviews) and quantitative (e.g., online survey) methods to assess explanations w.r.t. different explanatory goals and subjective properties [1].\n# ACKNOWLEDGMENT\nACKNOWLEDGMENT\nThis research was partially supported by the ARIAC project (No. 2010235), funded by the Service Public de Wallonie (SPW Recherche). This research used resources of the \u201cPlateforme Technologique de Calcul Intensif (PTCI)\u201d (http://www.ptci.unamur.be) located at the University of Namur, Belgium, which is supported by the FNRS-FRFC, the Walloon Region, and the University of Namur (Conventions No. 2.5020.11, GEQ U.G006.15, 1610468, RW/GEQ2016 et U.G011.22). The PTCI is member of the \u201cConsortium des \u00b4Equipements de Calcul Intensif (C\u00b4ECI)\u201d (https://www. ceci-hpc.be). Vincent Stragier is funded through a PhD grant from the \u0152uvre f\u00b4ed\u00b4erale Les Amis des Aveugles et Malvoyants ASBL- The Friends of the Blind and Visually Impaired Federal Charity-, Ghlin, Belgium and the Loterie Nationale, Rue Belliard 25-33, 1040 Brussels, Belgium. Vincent Stragier is partially supported by the FNRS-FRS. Bryan Renard is funded by the Public Service of Wallonia (Economy, Employment and Research), under the FoodWal agreement n\u00b02210182 from the Win4Excellence project of the Wallonia Recovery Plan.\n# REFERENCES\n[1] N. Tintarev and J. Masthoff, \u201cExplaining Recommendations: Design and Evaluation,\u201d in Recommender Systems Handbook, pp. 353\u2013382, Boston, MA: Springer US, 2015. [2] A. Papadimitriou, P. Symeonidis, and Y. Manolopoulos, \u201cA generalized taxonomy of explanations styles for traditional and social recommender systems,\u201d Data Mining and Knowledge Discovery, vol. 24, pp. 555\u2013583, may 2012. [3] Y. Zhang and X. Chen, \u201cExplainable Recommendation: A Survey and New Perspectives,\u201d Foundations and Trends\u00ae in Information Retrieval, vol. 14, no. 1, pp. 1\u2013101, 2020. [4] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., \u201cOn the opportunities and risks of foundation models,\u201d arXiv e-prints, pp. arXiv\u2013 2108, 2021. [5] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, \u201cSurvey of hallucination in natural language generation,\u201d ACM Comput. Surv., vol. 55, mar 2023. [6] T. Haveliwala, \u201cTopic-sensitive pagerank: a context-sensitive ranking algorithm for web search,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 784\u2013796, 2003. [7] H. Wang, F. Zhang, J. Wang, M. Zhao, W. Li, X. Xie, and M. Guo, \u201cRipplenet: Propagating user preferences on the knowledge graph for recommender systems,\u201d in Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM \u201918, (New York, NY, USA), p. 417\u2013426, Association for Computing Machinery, 2018. [8] L. Li, Y. Zhang, and L. Chen, \u201cPersonalized Prompt Learning for Explainable Recommendation,\u201d ACM Transactions on Information Systems, vol. 41, pp. 103:1\u2013103:26, Mar. 2023. [9] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu, \u201cHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,\u201d Apr. 2023.\n[10] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, \u201cImproving Language Understanding by Generative Pre-Training,\u201d 2018. [11] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models,\u201d July 2023. [12] L. Reynolds and K. McDonell, \u201cPrompt programming for large language models: Beyond the few-shot paradigm,\u201d in Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1\u2013 7, 2021. [13] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, \u201cIs chatgpt a good recommender? a preliminary study,\u201d arXiv preprint arXiv:2304.10149, 2023. [14] K. Balog and F. Radlinski, \u201cMeasuring recommendation explanation quality: The conflicting goals of explanations,\u201d in Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, pp. 329\u2013338, 2020. [15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al., \u201cChain-of-thought prompting elicits reasoning in large language models,\u201d in Advances in Neural Information Processing Systems, 2022. [16] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, \u201cSelf-consistency improves chain of thought reasoning in language models,\u201d arXiv preprint arXiv:2203.11171, 2022. [17] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine translation,\u201d in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311\u2013318, 2002. [18] C.-Y. Lin, \u201cRouge: A package for automatic evaluation of summaries,\u201d in Text summarization branches out, pp. 74\u201381, 2004.\n# A. Authors\u2019 Biographies\n1) Julien Albert: After an initial career as a librarian, Julien Albert embarked on a career change and obtained a master\u2019s degree in computer science from UNamur in 2020. He then worked for one year at UNamur on the EFFaTA-MeM research project, which aims to develop innovative tools for text analysis. In September 2021, he began a Ph.D. in computer science at UNamur under the supervision of Professors Beno\u02c6\u0131t Frenay and Bruno Dumas. His research area is explainability in artificial intelligence. His approach involves placing the user at the centre of concerns by combining explainability techniques from machine learning with methods developed in human-computer interaction. 2) Martin Balfroid: Martin Balfroid is a PhD student at the University of Namur, his research investigates AI-in-the-loop approaches to improve software engineering. He earned his master\u2019s in Computer Science, focusing on Data Science, in June 2022. The results of his master\u2019s thesis were published at the 2nd Software Testing Education Workshop. Martin began his PhD in July 2022 with funding from the ARIAC project and is supervised by Assistant Professors Beno\u02c6\u0131t Vanderose and Xavier Devroey. 3) Miriam Doh: Miriam Doh obtained a master\u2019s degree in Information and Communication Engineering from the University of Trento (UniTn) in Italy in 2021. Her master\u2019s thesis focused on the application of genetic algorithms to social networks, with a focus on studying the problem of community segregation in metropolitan areas. After completing her degree, she began a joint PhD program between ULB and UMONS on the intersection of Deep Learning and Computer Vision, with a particular emphasis on Explainable AI (XAI). Her research project is dedicated to exploring the integration of Cognitive Psychology principles to advance Explainable and Trustworthy Artificial Intelligence, particularly within the context of Face Analysis applications. 4) Jeremie Bogaert: J\u00b4er\u00b4emie Bogaert obtained his master\u2019s degree in computer science engineering with a focus on artificial intelligence from UCLouvain in 2021. His master\u2019s thesis explored the limitations of deep fake news generation models and their detection using machine learning models and human readers. He started his doctoral thesis at UCLouvain in September 2021 and is currently working on the interaction between interpretable machine learning models and human readers for the detection of deep fake news. He has interest in studying the explainability of NLP models in general. 5) Luca La Fisca: Luca La Fisca is currently a PhD student with a keen interest in neural engineering. His primary research focus revolves around advancing tools for a deeper understanding of the intricacies of the human brain. Luca\u2019s doctoral thesis specifically delves into the realm of ElectroEncephalogram (EEG) analysis. He is particularly fascinated by the interpretation of latent space to unveil critical interactions among brain regions during the execution of specific tasks,\nwith a primary emphasis on visual tasks. Additionally, Luca harbours a strong interest in the field of neurofeedback. Within the ARIAC project, Luca La Fisca is actively involved in Work Package 1, which centres on the interactions between humans and artificial intelligence. His contributions span various aspects, including interactive and human-in-theloop algorithms, user assistance in AI-in-the-loop scenarios, consensus mechanisms, handling imperfect multi-expert labels, and the development of explainable AI solutions. 6) Liesbet De Vos: Liesbet De Vos obtained a master\u2019s in Linguistics at the Catholic University of Leuven in 2021. Fascinated by computational linguistics, she completed her studies with an advanced master\u2019s in Artificial Intelligence at the Catholic University of Leuven, which she completed in 2022. Liesbet continues to nurture her passion for language during a PhD at the University of Namur, where she focuses on building hybrid AI systems that learn to use language through the same mechanisms as humans. In her thesis, she aims to extend the computational construction grammar framework to the visual modality so that it can adequately represent and learn the linguistic structure of sign languages. Within the ARIAC project, Liesbet actively contributes to Work Package 2, which revolves around trust mechanisms for artificial intelligence. 7) Bryan Renard: Bryan Renard obtained a master\u2019s degree in theoretical physics from UNamur in 2022. He then changed his career path and is now a dedicated PhD student whose research interests span several exciting domains within the field of artificial intelligence. His primary focus is the application of artificial intelligence in the realm of proteins, exploring innovative ways to harness AI (especially LLMs) for proteinrelated research. Additionally, Bryan is passionate about selfsupervised learning, particularly in the context of Automatic Speech Recognition (ASR). His thesis is jointly conducted by UNamur and Multitel. It is funded by the FoodWal portfolio from the Public Service of Wallonia (Economy, Employment, and Research), more particularly within the PEPTIBoost project. As a part of the ARIAC project, Bryan Renard plays an integral role in Work Package 4, which revolves around optimizing AI implementations. His contributions encompass a wide range of topics, including transfer learning, High-Performance Computing (HPC) and self-supervised learning techniques. 8) Vincent Stragier: Vincent Stragier is a PhD student at the University of Mons (UMONS). He is working on an interactive assistant for visually impaired and blind people within the ISIA Lab, a department of the Faculty of Engineering. His research interests are mainly focused on NLP, large language models and computer vision related topics. In 2021, he obtains his master\u2019s degree in electrical Engineering, specialized in Signals, Systems and BioEngineering from the Faculty of Engineering in Mons. In 2020, he works on an epilepsy detection pipeline base on an XGBoost classifier built by the CETIC, where he is Engineer Intern at the time. During his studies, he participates in the electronic student association, electroLAB, and the Erasmus Student Network of\nMons, ESNMons. In his free time, he likes taking photographs, fixing various things (hardware and software related), and learning new skills. 9) Emmanuel Jean: In 2009, Emmanuel Jean earned a dual degree in electrical engineering from the Faculty of Engineering at the University of Mons and Supelec-Paris. Subsequently, he joined the Signal Processing and Embedded Systems department at Multitel, where he actively participated in various regional and European projects involving vocal technologies and multimodal Human-Computer Interaction (HCI). In 2012, he furthered his education by obtaining a Bachelor\u2019s degree in Management Sciences from the Louvain School of Management at UCL-Mons. Since 2017, his professional focus has shifted towards diverse projects centred around Deep Learning applied to temporal signals, including audio, speech, and vibrations. His current research interests revolve around the development of weakly supervised machine learning techniques and the deployment of reliable artificial intelligence systems.\n# B. ARIAC and TRAIL\nTRAIL and the ARIAC research project are part of the regional DigitalWallonia4.ai program, which aims to accelerate the development of artificial intelligence technologies in Wallonia. TRAIL (TRusted AI Labs) provides actors in the socioeconomic landscape with R&D expertise and AI technological bricks developed by the 5 French-speaking universities and 4 approved research centres active in AI. To achieve this, the SPW-EER has allocated a budget of C32 million for the ARIAC research project led by the TRAIL consortium. This initiative is part of the 4th axis of the regional DigitalWallonia4.ai programme: \u201cResearch, innovation and partnerships\u201d. The ambition is to pool research in artificial intelligence in the Wallonia-Brussels Federation and is concretely reflected through the research project \u201cARIAC by DigitalWallonia4.ai\u201d, based on an agreement between the Walloon Region (SPW Research) and the actors forming the TRAIL consortium. The ARIAC project (\u201cApplications and Research for Trusted Artificial Intelligence\u201d in English or \u201cApplications et Recherche pour une Intelligence Artificielle de Confiance\u201d in French) is spread over 6 years and is articulated around 5 WP (Work Package): \u2022 human-AI interaction, \u2022 trust mechanisms for AI, \u2022 model-AI integration, \u2022 optimized implementations of AI, \u2022 TRAIL Factory.\n",
    "paper_type": "method",
    "attri": {
        "background": "Recommender systems are essential in digital experiences, yet their suggestions often lack transparency, leading to diminished user confidence. Previous methods, such as graph-based approaches, provide some explainability, but they can be complex for non-experts. Template-based explanations offer a clearer alternative, but may feel impersonal. This paper explores the use of large language models (LLMs) to create more engaging explanations.",
        "problem": {
            "definition": "The problem addressed is the lack of transparency and user engagement in explanations provided by recommender systems, which can hinder user trust and satisfaction.",
            "key obstacle": "The main challenge is that existing explanation methods, including template-based approaches, do not resonate with users or adequately clarify the reasoning behind recommendations."
        },
        "idea": {
            "intuition": "The idea was inspired by the potential of LLMs to generate clear and engaging text, enhancing user understanding and satisfaction.",
            "opinion": "The proposed approach involves using LLMs to generate explanations for recommendations, aiming to improve user experience compared to traditional methods.",
            "innovation": "The key innovation lies in the use of LLMs to generate explanations that are not only clear and informative but also resonate more deeply with users, contrasting with the more mechanical outputs of template-based methods."
        },
        "method": {
            "method name": "LLM-based Explanation Generation",
            "method abbreviation": "LLM-ExGen",
            "method definition": "This method employs large language models to generate textual explanations for movie recommendations based on user preferences and past interactions.",
            "method description": "The core of the method involves using LLMs to transform graph-based recommendation data into user-friendly textual explanations.",
            "method steps": [
                "Input user preferences and past interactions.",
                "Generate recommendations using a standalone recommendation method.",
                "Use LLMs to create explanations based on the generated recommendations."
            ],
            "principle": "The effectiveness of this method is rooted in LLMs' ability to produce coherent, contextually relevant text, which enhances user understanding and engagement."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved 25 participants who evaluated three types of explanations: traditional template-based, LLM-based rephrasing of template outputs, and purely LLM-based explanations derived from graph-based data.",
            "evaluation method": "Participants assessed the explanations based on their clarity, engagement, and alignment with user expectations, providing both qualitative and quantitative feedback."
        },
        "conclusion": "Preliminary findings indicate that LLM-based explanations may enhance user experience and satisfaction compared to traditional methods, although further research is needed to confirm these trends due to the small sample size and high variance in responses.",
        "discussion": {
            "advantage": "The proposed LLM-based approach offers a more engaging and detailed user experience, potentially increasing user trust and satisfaction with recommendations.",
            "limitation": "The method may introduce unverified details, leading to inaccuracies in the explanations, and the small sample size limits the robustness of the findings.",
            "future work": "Future research will explore the effectiveness of smaller LLMs, advanced prompting techniques, and mixed-methods evaluations to assess explanation quality comprehensively."
        },
        "other info": {
            "acknowledgment": "This research was supported by the ARIAC project funded by the Service Public de Wallonie, and utilized resources from the Plateforme Technologique de Calcul Intensif at the University of Namur.",
            "additional details": {
                "dataset": [
                    "Movielens-1M",
                    "MindReader"
                ],
                "baseline method": "Template-based explanation method."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommender systems are essential in digital experiences, yet their suggestions often lack transparency, leading to diminished user confidence."
        },
        {
            "section number": "1.2",
            "key information": "This paper explores the use of large language models (LLMs) to create more engaging explanations."
        },
        {
            "section number": "1.3",
            "key information": "The proposed approach involves using LLMs to generate explanations for recommendations, aiming to improve user experience compared to traditional methods."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed is the lack of transparency and user engagement in explanations provided by recommender systems, which can hinder user trust and satisfaction."
        },
        {
            "section number": "2.3",
            "key information": "The key innovation lies in the use of LLMs to generate explanations that are not only clear and informative but also resonate more deeply with users."
        },
        {
            "section number": "3.2",
            "key information": "The core of the method involves using LLMs to transform graph-based recommendation data into user-friendly textual explanations."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of this method is rooted in LLMs' ability to produce coherent, contextually relevant text, which enhances user understanding and engagement."
        },
        {
            "section number": "5.1",
            "key information": "Participants assessed the explanations based on their clarity, engagement, and alignment with user expectations."
        },
        {
            "section number": "10.1",
            "key information": "The method may introduce unverified details, leading to inaccuracies in the explanations, and the small sample size limits the robustness of the findings."
        },
        {
            "section number": "11",
            "key information": "Preliminary findings indicate that LLM-based explanations may enhance user experience and satisfaction compared to traditional methods."
        }
    ],
    "similarity_score": 0.7634603150129042,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/User Preferences for Large Language Model versus Template-Based Explanations of Movie Recommendations_ A Pilot Study.json"
}