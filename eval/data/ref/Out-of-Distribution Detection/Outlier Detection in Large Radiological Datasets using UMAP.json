{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.21263",
    "title": "Outlier Detection in Large Radiological Datasets using UMAP",
    "abstract": "The success of machine learning algorithms heavily relies on the quality of samples and the accuracy of their corresponding labels. However, building and maintaining large, high-quality datasets is an enormous task. This is especially true for biomedical data and for meta-sets that are compiled from smaller ones, as variations in image quality, labeling, reports, and archiving can lead to errors, inconsistencies, and repeated samples. Here, we show that the uniform manifold approximation and projection (UMAP) algorithm can find these anomalies essentially by forming independent clusters that are distinct from the main (\u201cgood\u201d) data but similar to other points with the same error type. As a representative example, we apply UMAP to discover outliers in the publicly available ChestX-ray14, CheXpert, and MURA datasets. While the results are archival and retrospective and focus on radiological images, the graph-based methods work for any data type and will prove equally beneficial for curation at the time of dataset creation.",
    "bib_name": "islam2024outlierdetectionlargeradiological",
    "md_text": "OUTLIER DETECTION IN LARGE RADIOLOGICAL DATASETS USING UMAP\n# Mohammad Tariqul Islam and Jason W. Fleischer\u2217 Department of Electrical and Computer Engineering\nartment of Electrical and Computer Engineering Princeton University Princeton, NJ 08544, USA {mtislam,jasonf}@princeton.edu\n Aug 202\n Aug\n# ABSTRACT\nThe success of machine learning algorithms heavily relies on the quality of samples and the accuracy of their corresponding labels. However, building and maintaining large, high-quality datasets is an enormous task. This is especially true for biomedical data and for meta-sets that are compiled from smaller ones, as variations in image quality, labeling, reports, and archiving can lead to errors, inconsistencies, and repeated samples. Here, we show that the uniform manifold approximation and projection (UMAP) algorithm can find these anomalies essentially by forming independent clusters that are distinct from the main (\u201cgood\u201d) data but similar to other points with the same error type. As a representative example, we apply UMAP to discover outliers in the publicly available ChestX-ray14, CheXpert, and MURA datasets. While the results are archival and retrospective and focus on radiological images, the graph-based methods work for any data type and will prove equally beneficial for curation at the time of dataset creation.\narXiv:2407.21263v2\nKeywords x-ray \u00b7 data visualization \u00b7 data curation \u00b7 neighbor embeddin\n# 1 Introduction\nA prominent reason behind the current success of machine learning-based disease detection is the availability of large medical datasets. However, for the machine learning models to be reliable, quality datasets representative of the target population need to be ensured [1]. The labels in these datasets are often generated from human annotations using automated extraction or entity detection tools. However, these annotations (and their structured archiving) from automated tools can have errors due to faulty perceptions, interpretations, and human errors [2]. Even if the error rate of the annotator is less than 4%, this can lead to millions of annotation errors per year [3]. This parallels the 3.3% error rate in large computer vision dataset [4]. Thus, there needs to be a better way to identify such errors before they are included in a dataset. For images, the search can be performed visually. However, examining individual images is a daunting task that requires many human hours. A popular alternative is neighbor embedding [5], which can produce a two-dimensional (2-D) cluster plot that can be analyzed visually quickly. (This class of methods is also known as nonlinear dimensionality reduction as the 2-D plot preserves the pairwise similarity, i.e., graph structure, of the original high-dimensional space.) Widely used neighbor embedding algorithms are t-distributed stochastic neighbor embedding (t-SNE) [6] and uniform manifold approximation and projection (UMAP) [7]. UMAP was introduced relatively recently and has become very popular, as this method draws concepts from rich algebraic and topological structures and is computationally fast. In this paper, we design a UMAP-based visual analytic method for extracting outlier images from large x-ray datasets. We validate our method by analyzing three publicly available and widely used medical image datasets. We show that the method can successfully cluster image features and produce interpretable visualization. We also discover labeling\nA prominent reason behind the current success of machine learning-based disease detection is the availability of large medical datasets. However, for the machine learning models to be reliable, quality datasets representative of the target population need to be ensured [1]. The labels in these datasets are often generated from human annotations using automated extraction or entity detection tools. However, these annotations (and their structured archiving) from automated tools can have errors due to faulty perceptions, interpretations, and human errors [2]. Even if the error rate of the annotator is less than 4%, this can lead to millions of annotation errors per year [3]. This parallels the 3.3% error rate in large computer vision dataset [4]. Thus, there needs to be a better way to identify such errors before they are included in a dataset. For images, the search can be performed visually. However, examining individual images is a daunting task that requires many human hours. A popular alternative is neighbor embedding [5], which can produce a two-dimensional (2-D) cluster plot that can be analyzed visually quickly. (This class of methods is also known as nonlinear dimensionality reduction as the 2-D plot preserves the pairwise similarity, i.e., graph structure, of the original high-dimensional space.) Widely used neighbor embedding algorithms are t-distributed stochastic neighbor embedding (t-SNE) [6] and uniform\nA prominent reason behind the current success of machine learning-based disease detection is the availability of large medical datasets. However, for the machine learning models to be reliable, quality datasets representative of the target population need to be ensured [1]. The labels in these datasets are often generated from human annotations using automated extraction or entity detection tools. However, these annotations (and their structured archiving) from automated tools can have errors due to faulty perceptions, interpretations, and human errors [2]. Even if the error rate o the annotator is less than 4%, this can lead to millions of annotation errors per year [3]. This parallels the 3.3% erro rate in large computer vision dataset [4]. Thus, there needs to be a better way to identify such errors before they are included in a dataset.\n-2024 Workshop on Topology- and Graph-Informed Imaging Informatics (TGI3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb23/bb230086-0461-4743-9504-d439c97ed36d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Schematic of the outlier search algorithm. Image features extracted from a DenseNet-121 neural network a projected onto a low-dimensional space (2-D plane) using UMAP.</div>\nerrors and erroneous images that have slipped through the verification process done prior to dissemination. Codes to reproduce the results are available at https://github.com/tariqul-islam/Outlier_Detection_UMAP.\n# 2 Related Works\nIn the literature, the term outlier is often used interchangeably with abnormality and anomaly [8]. Here, we define outliers as images that do not have sufficient signal for final decision-making or do not belong in the dataset due to specification. Generally, outlier detection methods assume an underlying distribution and often model it as normal distribution [9,10], i.e., a data point is an outlier if it is far away from the mean of the fitted distribution. Fritsch et al. [8] used the minimum covariance determinant estimator and its extensions to find outliers (due to motion or registration issues) in neuroimaging data by analyzing principal components. Gang et al. [11] used a t-SNE plot to find outliers from binary lung masks in terms of size variation and segmentation error. Fleischer and Islam [12] employed UMAP on chest x-rays for phenotyping COVID-19 response.\n# 3 System Overview\nFollowing preprocessing (discussed in A), the major parts of the framework are feature extraction and dimensionality reduction (Fig. 1). To extract features from these images, we employed DenseNet-121 [13] trained on ImageNet [14], a widely used deep neural network architecture designed to efficiently propagate features from earlier layers of a network to deeper layers. Importantly, neural algorithms are usually robust to many variabilities in images by design, and thus can accommodate standard images and outliers on equal terms. Medical images usually vary in resolution, have different contrast, brightness, and alignment, and often suffer from registration issues. In our framework, the features have been extracted from the final layer (before the softmax layer) of the network, where the features are generally most discriminating. Since we are not using a radiologically pre-trained model, these features generally will not be able to identify individual diseases. Rather, we employ other related labels (e.g., x-ray views, study labels) to examine the datasets. After extracting the features, we employ UMAP [7], to obtain a 2-D approximation of the high-dimensional features.\n# 4 Datasets\nWe evaluate our approach on three publicly available datasets: ChestX-ray14 [15], CheXpert [16], and Musculoskeletal Radiographs (MURA) [17]. ChestX-ray14 contains 112,120 frontal chest x-ray images from 30,805 unique patients.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/254b/254b9a3c-6d7c-4ff2-8923-b61ccfc7e73c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">b Existence of lateral x-rays</div>\n<div style=\"text-align: center;\">Existence of lateral x-rays</div>\nFigure 2: Outlier detection in the ChestX-ray14 dataset. (a) 2-D embedding. Labeled clusters from (a) are: (b) Latera x-rays which were not supposed to be in the dataset, (c) PA x-rays with borders, (d) AP x-rays with borders, and (e cluster from a single patient.\nImages are from posterior-anterior (PA) and anterior-posterior (AP) views. CheXpert dataset contains 224,316 chest x-rays (PA, AP, and Lateral) from 65,240 patients. We used 223,414 JPEG formatted x-rays from the training set of the dataset. MURA dataset contains 40,561 musculoskeletal x-rays from 14,863 studies. Like CheXpert, we used 36,808 x-rays from the training set and utilized study labels, e.g., finger, wrist, hand, forearm, elbow, humerus, and shoulder, for analysis.\n# 5 Experiments\nIn this section, we start by analyzing embeddings of chest x-rays (ChestX-ray14 and CheXpert datasets) and find different types of outliers. Then, we expand on variations of our method by altering the neural architecture, pre-training dataset, and embedding algorithms. Finally, we discuss outliers in musculoskeletal x-rays of the MURA dataset.\n# 5.1 Analyzing Chest X-ray Datasets\n# 5.1.1 Lateral X-rays in ChestX-ray14:\nOur 2-D embedding (Fig. 2 (a)) shows two large clusters of PA and AP views, corresponding to the primary topology of the DenseNet features. The embedding also includes a few satellite clusters around these large ones. These satellite clusters around the larger ones occur because the nearest neighbor graph creates a loop (or isolated sub-graph) of common features that are distinct from the rest of the data. In most cases, each of the satellite clusters of x-rays is from a single patient with a unique signature. However, if any specific image features (such as similar artifacts in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f2c/3f2c4985-ecbd-469c-ac2e-6b7334af7229.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Outlier detection in the CheXpert dataset. (a) 2-D Embedding. Example images with (b) block artifacts,  noise, (d) improper dynamic range, (e) vertical artifacts, and (f) alignment issues.</div>\nmultiple images) are present in x-rays of different patients, these can create satellite clusters as well. Another interesting structure in Fig. 2 (a) is the protruding region from the AP cluster. Representative examples of anomalous clusters are shown in Figs. 2 (b)-(e). The most surprising finding is the existence of some lateral x-rays in the dataset (Fig. 2 (b)), as this dataset is supposed to be composed of frontal chest x-rays only. We found 92 lateral x-rays using our method. The protruding region from the AP cluster marked c and d (in Fig. 2 (a)), consisting of x-rays with dark borders of PA and AP views, respectively. Finally, cluster (e) shown in Fig. 2 (e) groups 46 x-rays from patient ID 9845, and a single x-ray from patient ID 12562.\n# 5.1.2 Corrupted Images in CheXpert:\nFigure 3 (a) shows the 2-D embedding of CheXpert dataset. As before, the large PA and AP clusters form the bulk of the mapping. The lateral x-rays also form a separate large cluster. A few of the large satellite clusters (b-f) have been marked by red circles in Fig. 3 (a). Four images from each of the clusters are plotted in Fig. 3 (b)-(e). Figure 3 (b) depicts images with block artifacts, e.g., from poor JPEG compression or accidental splicing. We found 107 such images in this cluster. Figure 3 (c) depicts images that are just noise (19 images). Figure 3 (d) shows images with block artifacts and dynamic range issues (53 images). Figure 3 (e) shows x-rays with vertical artifacts (88 images). Finally, Fig. 3 (f) shows rotated images. This cluster is placed near the large cluster of lateral (L) x-rays. Thus, DenseNet considers rotated x-rays to be more similar to lateral images than upright frontal x-rays.\n# 5.2 Effect of Different Pre-trained Networks\nIn the previous section, we used DenseNet-121 trained on ImageNet data for feature extraction (Fig. 1). However, using ImageNet may introduce domain shift, since the dataset is not specific to medical images. To assess, we looked at embeddings from different models trained on ImageNet and chest x-rays (Fig. 4). ImageNet models (Fig. 4 (a,b)) create similar clusters of PA, AP, and L x-rays, whereas Chest-Xray14 models (Fig. 4 (c,d)) merge the PA and AP clusters. The x-ray model embeddings also contain fewer satellite clusters, and their distinctness is lost. Thus, outlier detection fails. Additionally, we highlight the placement of images with vertical artifacts from the cluster (e) of Fig. 3 in the alternate models (yellow points in Fig. 4). While both ImageNet models successfully separate these images (with a few scattered around), all the chest x-ray models fail (where the outliers are inside the large cluster and scattered throughout). A major difference between models is that ImageNet models use softmax (n-ary) cross entropy loss (for non-overlapping labels), but chest-x-ray models use binary cross entropy loss (for overlapping labels). To resolve this discrepancy, we trained chest x-ray models using non-overlapping labels of the ChestX-ray-14 dataset (for details, see Appendix B).\nIn the previous section, we used DenseNet-121 trained on ImageNet data for feature extraction (Fig. 1). However, using ImageNet may introduce domain shift, since the dataset is not specific to medical images. To assess, we looked at embeddings from different models trained on ImageNet and chest x-rays (Fig. 4).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5673/56735565-2e16-4ab2-bd9a-9d6338a55783.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Embedding of CheXpert dataset using different pre-trained models. DenseNet-121 and ResNet-50 trained o ImageNet (left two) and ChestX-ray14 (right tow) datasets. Each yellow point represents an image with vertical artifac (from cluster e in Fig. 3 (a)) indicating chest x-ray pre-trained models fail to identify these as outliers.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f36/7f36608e-52a2-4047-ab5f-7cbe86bdb472.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Pre-training DenseNet-121 models using non-overlapping labels from ChestX-ray14 dataset and embedding of CheXpert dataset using UMAP. Models are trained using (a) binary cross entropy loss and (b) softmax cross entropy loss. Each yellow point represents an image with vertical artifacts (from cluster e in Fig. 3 (a)).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e682/e6827654-a2a1-4f1d-99f6-82d0e08dd2a3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Embedding of CheXpert dataset using several dimensionality reduction algorithms. (a) PCA, (b) t-SNE, (c) t-SNE (exaggerated), (d) TriMap, and (e) PaCMAP. Each yellow point represents an image with vertical artifacts (from cluster e in Fig. 3 (a)).</div>\n<div style=\"text-align: center;\">Figure 6: Embedding of CheXpert dataset using several dimensionality reduction algorithms. (a) PCA, (b) t-SNE, (c t-SNE (exaggerated), (d) TriMap, and (e) PaCMAP. Each yellow point represents an image with vertical artifacts (from cluster e in Fig. 3 (a)).</div>\nThe resulting embeddings (Fig. 5) show a similar characteristic of the chest x-ray models with overlapping labels (Fig. 4 (c,d)), in that the individual views are weakly separated while the specific outlier images fail to form the satellite clusters. For example, the outliers with vertical artifacts scatter within the 2D mapping. The networks consider them as any other x-ray image and ignore the artifacts. This result strengthens the idea that training on ImageNet (or a more general computer vision task) that has broader exposure benefits the discovery of outlier images.\n# 5.3 Comparing Various Embedding Algorithms\nTo assess the effectiveness of UMAP, we compared it with a few other dimensionality reduction techniques, specifically principal component analysis (PCA), t-SNE, TriMap [18], and PaCMAP [19].\nTo assess the effectiveness of UMAP, we compared it with a few other dimensionality reduction techniques, specifically principal component analysis (PCA), t-SNE, TriMap [18], and PaCMAP [19]. The PCA embedding in Fig. 6 (a) shows the top two directions of largest variances. There are two clusters but the AP and PA views overlap. The nonlinear dimensionality reduction algorithms - t-SNE, UMAP, and variants - discover more features and make the clusters more distinct.\nThe PCA embedding in Fig. 6 (a) shows the top two directions of largest variances. There are two clusters but the AP and PA views overlap. The nonlinear dimensionality reduction algorithms - t-SNE, UMAP, and variants - discover more features and make the clusters more distinct.\nThe default t-SNE is tuned to preserve the neighborhood as best as possible. This often causes the individual clusters to spread out and be less compact (Fig. 6 (b)). This behavior is apparent in the PA, AP, and lateral x-ray clusters: the separation among them is minimal, and there is little room for the satellite clusters. However, t-SNE can be tuned to produce a more UMAP-like output. Following the findings of Linderman et al. [20] and Bohm et al. [21], we used an exaggeration factor of 4, which means we applied four times more attractive force than the repulsive force throughou the optimization procedure. The standard early exaggeration factor of 12 was also applied at the start of the optimization\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/04b3/04b32dd8-6cdd-4fad-90a3-1c065f31db57.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Embedding of \u2018finger\u2019 x-rays from MURA dataset and 100 chest x-rays from CheXpert dataset using UMAP. (a) Scatter plot of the embedding. The cluster of chest x-rays is marked using a red rectangle. (b) Scatter plot in the red rectangle. (c) two x-rays labeled \u2018finger\u2019 are actually chest x-rays. (d) Typical finger x-rays from the MURA dataset.</div>\nThe resulting plot in Fig. 6 (c) largely resolves the PA and AP clusters, but there are fewer satellite clusters than the UMAP output. Both PaCMAP [19] and TriMap [18] aim to preserve the global structure of the data while keeping the clustering properties of UMAP. PaCMAP modifies the pairwise relation of UMAP by considering different neighborhoods at different scales (near and far), while TriMap achieves this with the triplet constraint. These methods obtain the clustering of PA, AP, and lateral views (Figs. 6 (d,e)), but, similar to exaggerated t-SNE, the satellite clusters are largely absent. The yellow dots show the placement of x-rays with vertical artifacts from cluster (e) of Fig. 3 for the different embedding algorithms. In all the alternate embedding algorithms, these images are scattered within the other x-ray images. The outlier images fails to be identified and labeled separately, which demonstrates a superior performance of UMAP.\n# 5.4 Extracting Mislabeled X-rays from MURA\nSince the MURA dataset consists of x-rays from different parts of the arm and the shoulder, there is a natural ambiguity in the labels, e.g., both wrist and hand x-rays may contain the hand of a person, and shoulder x-rays may contain part of the chest. In such cases, finding mislabeled x-rays by embedding all the images may be sub-optimal. To find outliers more directly, we searched for misclassified images by explicitly using labels of the dataset. The method has two parts: 1) introduce target images with a specific label (preferably from a different dataset than the MURA one); and 2) perform neighbor embedding on the joint dataset. For example, to look for possible chest x-rays that are falsely classified as finger x-rays, we added 100 chest x-rays from the CheXpert dataset to the 5,106 finger x-rays of MURA. We then applied the UMAP to the composite set (Fig. 7 (a)). As shown in Figs. 7 (b-d), the seeded chest x-rays acted as an attractor for mislabeled images in MURA, with x-rays labeled \u2018finger\u2019 now appearing in the (new) chest cluster. Interestingly, both of these x-rays were from patient 04547 (another 3 from this patient were labeled correctly). We describe an additional experiment in Appendix D.\n# 6 Conclusion\nNeighbor embedding algorithms can be an effective tool for summarizing datasets and identifying outlier images. The principle of the method is that the outliers are different from the main data but can have similarities among themselves. Thus, the different outlier types form distinct clusters in the embeddings. Our experiments, using a DenseNet-121 feature extractor and UMAP neighbor embedding method on the ChestX-ray14, CheXpert, and MURA datasets, distinguished different radiological views of chest x-rays, classified differences, and identified wrongly labeled or\ncorrupted images. We further found specific types of outliers by seeding the dataset with target images and performing neighbor embedding. While this study performed retrospective analysis of large x-ray datasets, outlier curation can be achieved during the initial assembly of the dataset as well. For suspected outliers, the method of seeding data with known labels can be applied. To streamline the process, appropriate reference datasets may be created beforehand. Undoubtedly, cleaner input data will result in cleaner output data. For larger datasets, more accurate results and faster embedding may be achieved by dividing them into smaller subsets and applying better alignment techniques [22]. Finally, since the methods are graph-based and agnostic to the underlying data type, all of the methods here can be applied to arbitrary datasets, including and especially those that are mixed modality.\n# Acknowledgement\nThe authors gratefully acknowledge financial support from the Schmidt DataX Fund at Princeton University made possible through a major gift from the Schmidt Futures Foundation.\n# Disclosure\nThe authors have no competing interests to declare that are relevant to the content of this article.\nThe authors have no competing interests to declare that are relevant to the content of this article.\n# References\n[1] Kun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. Artificial intelligence in healthcare. Nature biomedical engineering, 2(10):719\u2013731, 2018. [2] Stephen Waite, Jinel Scott, Brian Gale, Travis Fuchs, Srinivas Kolla, and Deborah Reede. Interpretive error in radiology. American Journal of Roentgenology, 208(4):739\u2013749, 2017. [3] Michael A Bruno, Eric A Walker, and Hani H Abujudeh. Understanding and confronting our mistakes: the epidemiology of error in radiology and strategies for error reduction. Radiographics, 35(6):1668\u20131676, 2015. [4] Curtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks. arXiv preprint arXiv:2103.14749, 2021. [5] Geoffrey Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in Neural Information Processing Systems, volume 15, pages 833\u2013840, 2002. [6] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008. [7] Leland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [8] Virgile Fritsch, Ga\u00ebl Varoquaux, Benjamin Thyreau, Jean-Baptiste Poline, and Bertrand Thirion. Detecting outliers in high-dimensional neuroimaging datasets with robust covariance estimators. Medical image analysis, 16(7):1359\u20131370, 2012. [9] Victoria Hodge and Jim Austin. A survey of outlier detection methodologies. Artificial intelligence review, 22(2):85\u2013126, 2004. 10] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. ADBench: Anomaly detection benchmark. Advances in Neural Information Processing Systems, 35:32142\u201332159, 2022. 11] Peng Gang, Wang Zhen, Wei Zeng, Yuri Gordienko, Yuriy Kochura, Oleg Alienin, Oleksandr Rokovyi, and Sergii Stirenko. Dimensionality reduction in deep learning for chest x-ray analysis of lung cancer. In 2018 tenth international conference on advanced computational intelligence (ICACI), pages 878\u2013883. IEEE, 2018. 12] Jason Fleischer and Mohammad Tariqul Islam. Late breaking abstract-identifying and phenotyping COVID-19 patients using machine learning on chest x-rays. European Respiratory Journal, 2020. 13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017. 14] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n[15] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2097\u20132106, 2017. [16] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590\u2013597, 2019. [17] Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang, Kaylie Zhu, Dillon Laird, Robyn L Ball, et al. MURA Dataset: Towards radiologist-level abnormality detection in musculoskeletal radiographs. In Medical Imaging with Deep Learning, 2018. [18] Ehsan Amid and Manfred K Warmuth. TriMap: Large-scale dimensionality reduction using triplets. arXiv preprint arXiv:1910.00204, 2019. [19] Yingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding how dimension reduction tools work: an empirical approach to deciphering t-SNE, UMAP, TriMAP, and PaCMAP for data visualization. The Journal of Machine Learning Research, 22(1):9129\u20139201, 2021. [20] George C Linderman and Stefan Steinerberger. Clustering with t-SNE, provably. SIAM Journal on Mathematics of Data Science, 1(2):313\u2013332, 2019. [21] Jan Niklas B\u00f6hm, Philipp Berens, and Dmitry Kobak. Attraction-repulsion spectrum in neighbor embeddings. The Journal of Machine Learning Research, 23(1):4118\u20134149, 2022. [22] Mohammad Tariqul Islam and Jason W Fleischer. Manifold-aligned neighbor embedding. In ICLR 2022 Workshop on Geometrical and Topological Representation Learning, 2022. [23] Richard E. Woods and Rafael C. Gonzalez. Digital image processing, 2008. [24] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [25] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 806\u2013813, 2014. [26] Joseph Paul Cohen, Joseph D Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al. Torchxrayvision: A library of chest x-ray datasets and models. In International Conference on Medical Imaging with Deep Learning, pages 231\u2013249. PMLR, 2022. [27] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017.\n# A Implementation Details\n# A.1 Image Pre-processing\nWe apply the following transformations: histogram equalization [23], resizing the images, center cropping, and normalization. Images are resized such that the lowest dimension contains 256 pixels and then center-cropped to a 224 \u00d7 224 dimensional image for feature extraction. Then the images are normalized according to the specification of ImageNet: mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225) of red, green, and blue channels, respectively.\n# A.2 Feature Extraction\nWe use a DenseNet-121 architecture [13] pre-trained on the ImageNet dataset [14] from the PyTorch deep learning library [24]. (DenseNet is a deep neural network with many inter-layer connections designed to reduce the numerical instabilities that originate due to the depth of the network. The usage of neural network outputs as features is an effective baseline in machine learning algorithms [25] and is extensively used in medical image analysis [26].) Here, we remove the classification (softmax) layer from the neural network and use the output as features.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0e78/0e78d3c7-24be-45f3-8341-38158cce2768.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">DenseNet-121 (TorchXRayVision)</div>\n# Figure 8: Embedding of CheXpert dataset using publicly available DenseNet-121 model pre-trained on ChestX-ray14 dataset (from TorchXRayVision library [26])\n# A.3 Parameter Settings of Embeddings\nIn general, we kept the number of nearest neighbors k to be low, as increasing k increases the computational budget exponentially.\nIn general, we kept the number of nearest neighbors k to be low, as increasing k increases the com exponentially.\nexponentially. ChestX-ray14 - Fig. 2: For ChestX-ray14 we used k = 50. The minimum distance parameter md was set to 0.1. We experimented with smaller md values to increase the separation of the AP and PA x-rays, but it had little effect. The embedding was optimized for 200 epochs. CheXpert - Figs. 3, 4, and 5: The embedding was obtained by using k = 10. We used a smaller minimum distance md = 0.001, as we found that this value provided a better separability of the large clusters. We ran the optimization for 300 epochs. Figure 6: (b) t-SNE: early exaggerated for 250 steps, then 500 regular steps, (c) t-SNE: early exaggerated for 250 steps, then 500 exaggerated steps, (d) TriMap: 1000 steps with k = 10, compactness parameter = 0.1, and (e) PaCMAP: 450 steps with k = 10. MURA - Figs. 7, 9, and 11: Similar to CheXpert, we used k = 10, md = 0.001, and ran the optimization for 300 epochs.\nMURA - Figs. 7, 9, and 11: Similar to CheXpert, we used k = 10, md = 0.001, and ran the optimization for 30 epochs.\n# B Pre-training Models using ChestX-ray14 Dataset\nWe trained several chest x-ray models using the ChestX-ray14 dataset. We trained traditional x-ray models using binary cross entropy loss for every disease (one vs all model) by following [27] which matches with the settings of the ImageNet pre-training. Additionally, to test whether the loss function and multilabel training have major effects on the resulting embeddings, we constructed a dataset by removing overlapping labels (Table 1) and trained two models using binary and softmax cross-entropy loss, respectively. For each mode, we optimized for 400,000 iterations using Adam (learning rate = 0.001) with a batch size of 30 (2 images per disease label).\n# B.1 Comparison to Publicly Available Chest X-ray Models\nWe compare our DenseNet-121 model to that from TorchXRayVision library [26]. It differs from ours in that the image pixels are normalized to [\u22121024, 1024], images are converted to (or read as) grayscale, and \u2018No Finding\u2019 labe is untrained. This model (Fig. 8) agrees with our results and ignores the artifacts of the outlier images, showing embeddings are indifferent to alternate preprocessing scheme.\n# C Additional Discussion on MURA\nFig. 9 shows the embedding of 36,808 musculoskeletal radiographs from the MURA dataset. There is decent separation among the x-rays in terms of the labels, but there is also a considerable overlap. For example, finger, wrist, and hand x-rays overlap, as finger x-rays include parts of the wrist and hand, and vice versa. Similarly, wrist and forearm clusters\n<div style=\"text-align: center;\">Table 1: ChestX-ray14 labels and samples.</div>\nTable 1: ChestX-ray14 labels and samples.\nLabel\nNo. of labels\nNo. of non-overlapping labels\nAtelectasis\n11559\n4215\nCardiomegaly\n2776\n1093\nEffusion\n13317\n3955\nInfiltration\n19894\n9547\nMass\n5782\n2139\nNodule\n6331\n2705\nPneumonia\n1431\n322\nPneumothorax\n5302\n2194\nConsolidation\n4667\n1310\nEdema\n2303\n628\nEmphysema\n2516\n892\nFibrosis\n1686\n727\nPleural Thickening\n3385\n1126\nHernia\n227\n110\nNo Finding\n60361\n60361\nTotal Images\n112,120\n91324\n<div style=\"text-align: center;\">Embedding of MURA dataset</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a8e/0a8e59ec-fb3f-4c00-b91e-b1fd1836ad46.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Embedding of MURA dataset.</div>\nare often merged, since x-ray of the forearm tend to capture a portion of the wrist, and vice versa. The same happens for humerus and shoulder. The images within each cluster thus share similar acquisition views, aspect ratios, and specific features (e.g., circular window function, stitching of multiple x-rays in one image). Unlike ChexPert case, analysis of this mapping did not reveal any satellite clusters with corrupted images. Based on this, we decided to focus on individual labels and extract images with specific labels. The chest x-rays from the CheXpert dataset used to produce Fig. 7 are shown in Fig. 10.\n# D Extracting Mislabeled X-rays from MURA\nIn a different experiment on the MURA dataset, we used \u2018finger\u2019 and \u2018shoulder\u2019 x-rays (Fig. 11). The broad features of the finger and shoulder are easily separable with a few misclassified points. Analyzing \u2018finger\u2019 x-rays misclassified in \u2018shoulder\u2019 clusters, we can find the two chest x-rays labeled as finger (which we found in previous experiment as well) and two images that are just noise/non-x-ray images (Fig. 11 (b)). The latter belongs to patient ID 04687, which uncovers two more outliers (one is an x-ray of keys). The misclassified \u2018shoulder\u2019 labels in the \u2018finger\u2019 cluster reveal three leg x-rays (Fig. 11 (c)).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b54/0b545128-5e83-4cdd-9440-a456004ed14b.png\" style=\"width: 50%;\"></div>\nFigure 10: Chest x-rays chosen from CheXpert dataset to produce the embedding in Fig 7.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2a9e/2a9ecc4c-1772-40d8-957e-9c067c38eafd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Embedding of \u2018finger\u2019 and \u2018shoulder\u2019 x-rays from MURA dataset using UMAP. (a) 2-D scatterplot of the embedding. (b) Chest x-ray and non-x-ray images were discovered which are labeled as \u2018finger\u2019 x-rays. (c) Leg x-rays</div>\n<div style=\"text-align: center;\">Figure 11: Embedding of \u2018finger\u2019 and \u2018shoulder\u2019 x-rays from MURA dataset using UMAP. (a) 2-D scatterplot of the embedding. (b) Chest x-ray and non-x-ray images were discovered which are labeled as \u2018finger\u2019 x-rays. (c) Leg x-rays labeled as \u2018shoulder\u2019 x-rays.</div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The success of machine learning algorithms in disease detection relies heavily on the availability of large, high-quality medical datasets. However, the creation and maintenance of such datasets is challenging due to variations in image quality, labeling errors, and inconsistencies that can lead to significant annotation errors.",
            "purpose of benchmark": "The benchmark aims to provide a method for identifying outlier images in large radiological datasets, facilitating better data curation and enhancing the reliability of machine learning models."
        },
        "problem": {
            "definition": "The benchmark is designed to address the challenge of identifying outlier images in medical datasets that may not conform to expected standards, which can lead to erroneous conclusions in machine learning applications.",
            "key obstacle": "Existing benchmarks often fail to adequately account for the complexities and variations in medical images, leading to a lack of reliable methods for detecting outliers."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the need to improve the identification of mislabeled or corrupted images in radiological datasets, which can significantly impact model performance.",
            "opinion": "The authors believe that this benchmark is crucial for improving the quality of medical datasets, ultimately leading to better diagnostic tools and outcomes.",
            "innovation": "This benchmark introduces the use of UMAP for visualizing and detecting outliers in radiological datasets, offering a novel approach compared to traditional methods.",
            "benchmark abbreviation": "UMAP"
        },
        "dataset": {
            "source": "The dataset consists of publicly available medical imaging datasets, including ChestX-ray14, CheXpert, and MURA, which were used to evaluate the proposed method.",
            "desc": "The benchmark includes 112,120 images from ChestX-ray14, 224,316 images from CheXpert, and 40,561 images from MURA, providing a diverse set of radiological images for analysis.",
            "content": "The dataset contains various types of x-ray images, specifically frontal chest x-rays and musculoskeletal x-rays, which are essential for the outlier detection tasks.",
            "size": "376,997",
            "domain": "Radiology",
            "task format": "Outlier Detection"
        },
        "metrics": {
            "metric name": "Clustering Quality, Outlier Detection Accuracy",
            "aspect": "The metrics measure the effectiveness of the UMAP algorithm in clustering similar images and accurately identifying outliers.",
            "principle": "The choice of metrics is based on their ability to reflect the clustering performance and the accuracy of outlier detection within the high-dimensional image space.",
            "procedure": "Model performance is evaluated by applying UMAP to the datasets and analyzing the resulting clusters for distinctiveness and accuracy in identifying outliers."
        },
        "experiments": {
            "model": "The benchmark tested DenseNet-121 as the feature extractor for the image datasets.",
            "procedure": "Models were trained using various parameter settings, including different loss functions and embedding techniques, to assess their effectiveness in detecting outliers.",
            "result": "The experiments demonstrated that the UMAP-based method successfully identified multiple types of outliers across the different datasets, with statistically significant results.",
            "variability": "Variability in results was accounted for by conducting multiple trials and analyzing different subsets of the datasets."
        },
        "conclusion": "The study concluded that UMAP is an effective method for identifying outlier images in large radiological datasets, improving the quality of data used in machine learning applications.",
        "discussion": {
            "advantage": "The benchmark provides a robust framework for outlier detection, enhancing the reliability of medical datasets and supporting better diagnostic processes.",
            "limitation": "Potential limitations include the reliance on high-quality initial datasets and the need for careful parameter tuning to optimize UMAP performance.",
            "future work": "Future research may focus on refining the outlier detection process, exploring additional datasets, and developing methods for real-time outlier detection during dataset creation."
        },
        "other info": {
            "info1": "Codes to reproduce the results are available at https://github.com/tariqul-islam/Outlier_Detection_UMAP.",
            "info2": {
                "info2.1": "The methods outlined can be applied to various data types beyond medical imaging.",
                "info2.2": "The approach is particularly beneficial for large-scale datasets, where manual inspection is impractical."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The benchmark is designed to address the challenge of identifying outlier images in medical datasets that may not conform to expected standards, which can lead to erroneous conclusions in machine learning applications."
        },
        {
            "section number": "2.3",
            "key information": "The benchmark aims to provide a method for identifying outlier images in large radiological datasets, facilitating better data curation and enhancing the reliability of machine learning models."
        },
        {
            "section number": "3.1",
            "key information": "This benchmark introduces the use of UMAP for visualizing and detecting outliers in radiological datasets, offering a novel approach compared to traditional methods."
        },
        {
            "section number": "3.2",
            "key information": "The metrics measure the effectiveness of the UMAP algorithm in clustering similar images and accurately identifying outliers."
        },
        {
            "section number": "5.1",
            "key information": "The benchmark provides a robust framework for outlier detection, enhancing the reliability of medical datasets and supporting better diagnostic processes."
        },
        {
            "section number": "7.1",
            "key information": "Potential limitations include the reliance on high-quality initial datasets and the need for careful parameter tuning to optimize UMAP performance."
        }
    ],
    "similarity_score": 0.6520452232807574,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Outlier Detection in Large Radiological Datasets using UMAP.json"
}