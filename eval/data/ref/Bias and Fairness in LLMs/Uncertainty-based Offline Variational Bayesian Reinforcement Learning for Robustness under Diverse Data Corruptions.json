{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.00465",
    "title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions",
    "abstract": "Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.",
    "bib_name": "yang2024uncertaintybasedofflinevariationalbayesian",
    "md_text": "# Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions\nRui Yang1,2, Jie Wang1,2\u2217, Guoping Wu1, Bin Li1 1University of Science and Technology of China 2MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition {yr0013, guoping}@mail.ustc.edu.cn {jiewangx, binli}@ustc.edu.cn\n Nov 2024\n# Abstract\nReal-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.\narXiv:2411.00465v1\n# 1 Introduction\nOffline reinforcement learning (RL) aims to learn an effective policy from a fixed dataset without direct interaction with the environment [1, 2]. This paradigm has recently attracted much attention in scenarios where real-time data collection is expensive, risky, or impractical, such as in healthcare [3], autonomous driving [4], and industrial automation [5]. Due to the restriction of the dataset, offline RL confronts the challenge of distribution shift between the policy represented in the offline dataset and the policy being learned, which often leads to the overestimation for out-of-distribution (OOD) actions [1, 6, 7]. To address this challenge, one of the promising approaches introduce uncertainty estimation techniques, such as using the ensemble of action-value functions or Bayesian inference to measure the uncertainty of the dynamics model [8\u201311] or the action-value function [12\u201315] regarding the rewards and transition dynamics. Therefore, they can constrain the learned policy to remain close to the policy represented in the dataset, guiding the policy to be robust against OOD actions.\n\u2217Corresponding author. Email: jiewangx@ustc.edu.cn.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6a4c/6a4c8828-b411-44aa-ad99-28315209d255.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Graphical model of decision-making process. Nodes connected by solid lines denote data points in the offline dataset, while the Q values (i.e., action values) connected by dashed lines are not part of the dataset. These Q values are often objectives that offline algorithms aim to approximate.</div>\nNevertheless, in the real world, the dataset collected by sensors or humans may be subject to extensive and diverse corruptions [16\u201318], e.g., random noise from sensor failures or adversarial attacks during RLHF data collection. Offline RL methods often assume that the dataset is clean and representative of the environment. Thus, when the data is corrupted, the methods experience performance degradation in the clean environment, as they often constrain policies close to the corrupted data distribution. Despite advances in robust offline RL [2], these approaches struggle to address the challenges posed by diverse data corruptions [18]. Specifically, many previous methods on robust offline RL aim to enhance the testing-time robustness, learning from clean datasets and defending against attacks during testing [19\u201321]. However, they cannot exhibit robust performance using offline dataset with perturbations while evaluating the agent in a clean environment. Some related works for data corruptions (also known as corruption-robust offline RL methods) introduce statistical robustness and stability certification to improve performance, but they primarily focus on enhancing robustness against adversarial attacks [16, 22, 23]. Other approaches focus on the robustness against both random noise and adversarial attacks, but they often aim to address only corruptions in states, rewards, or transition dynamics [24, 17]. Based on these methods, recent work [18] extends the data corruptions to all four elements in the dataset, including states, actions, rewards, and dynamics. This work demonstrates the superiority of the supervised policy learning scheme [25, 26] for the data corruption of each element in the dataset. However, as it does not take into account the uncertainty in decisionmaking caused by the simultaneous presence of diverse corrupted data, this work still encounters difficulties in learning robust agents, limiting its applications in real-world scenarios. In this paper, we propose to use offline data as the observations, thus leveraging their correlations to capture the uncertainty induced by all corrupted data. Considering that (1) diverse corruptions may introduce uncertainties into all elements in the offline dataset, and (2) each element is correlated with the action values (see dashed lines in Figure 1), there is high uncertainty in approximating the action-value function by using various corrupted data. To address this high uncertainty, we propose to leverage all elements in the dataset as observations, based on the graphical model in Figure 1. By using the high correlations between these observations and the action values [27], we can accurately identify the uncertainty of the action-value function. Motivated by this idea, we propose a robust variational Bayesian inference for offline RL (TRACER) to capture the uncertainty via offline data against all types of data corruptions. Specifically, TRACER first models all data corruptions as uncertainty in the action-value function. Then, to capture such uncertainty, it introduces variational Bayesian inference [28], which uses all offline data as observations to approximate the posterior distribution of the action-value function. Moreover, the corrupted observed data often induce higher uncertainty than clean data, resulting in higher entropy in the distribution of action-value function. Thus, TRACER can use the entropy as an uncertainty measure to effectively distinguish corrupted data from clean data. Based on the entropy-based uncertainty measure, it can regulate the loss associated with corrupted data in approximating the action-value distribution. This approach effectively reduces the influence of corrupted samples, enhancing robustness and performance in clean environments. This study introduces Bayesian inference into offline RL for data corruptions. It significantly captures the uncertainty caused by diverse corrupted data, thereby improving both robustness and performance in offline RL. Moreover, it is important to note that, unlike traditional Bayesian online and offline RL methods that only model uncertainty from rewards and dynamics [29\u201335], our approach identifies the\nuncertainty of the action-value function regarding states, actions, rewards, and dynamics under data corruptions. We summarize our contributions as follows. \u2022 To the best of our knowledge, this study introduces Bayesian inference into corruption-robust offline RL for the first time. By leveraging all offline data as observations, it can capture uncertainty in the action-value function caused by diverse corrupted data. \u2022 By introducing an entropy-based uncertainty measure, TRACER can distinguish corrupted from clean data, thereby regulating the loss associated with corrupted samples to reduce its influence for robustness. \u2022 Experiment results show that TRACER significantly outperforms several state-of-the-art offline RL methods across a range of both individual and simultaneous data corruptions.\n# 2 Preliminaries\nBayesian RL. We consider a Markov decision process (MDP), denoted by a tuple M = (S, A, R, P, P0, \u03b3), where S is the state space, A is the action space, R is the reward space, P(\u00b7|s, a) \u2208P(S) is the transition probability distribution over next states conditioned on a stateaction pair (s, a), P0(\u00b7) \u2208P(S) is the probability distribution of initial states, and \u03b3 \u2208[0, 1) is the discount factor. Note that P(S) and P(A) denote the sets of probability distributions on subsets of S and A, respectively. For simplicity, throughout the paper, we use uppercase letters to refer to random variables and lowercase letters to denote values taken by the random variables. Specifically, R(s, a) denotes the random variable of one-step reward following the distribution \u03c1(r|s, a), and r(s, a) represents a value of this random variable. We assume that the random variable of one-step rewards and their expectations are bounded by Rmax and rmax for any (s, a) \u2208S \u00d7 A, respectively. Our goal is to learn a policy that maximizes the expected discounted cumulative return: \ufffd \ufffd\n\ufffd Based on the return, we can define the value function as V \u03c0(s) = E\u03c0,\u03c1,P [\ufffd\u221e t=0 \u03b3tR(st, at)|s0 = s the action-value function as Q\u03c0(s, a) = ER\u223c\u03c1(\u00b7|s,a),s\u2032\u223cP (\u00b7|s,a) [R(s, a) + \u03b3V \u03c0(s\u2032)], and the actio value distribution [36] as\nD\u03c0(s, a) = \u221e \ufffd \u03b3tR(st, at|s0 = s, a0 = a), with st+1 \u223cP(\u00b7|st, at), at+1 \u223c\u03c0(\u00b7|st+1).\nD\u03c0(s, a) = \u221e \ufffd \u03b3tR(st, at|s0 = s, a0 = a), with st+1 \u223cP(\u00b7|st, at), at+1 \u223c\u03c0(\u00b7|st+1).\n\ufffd Note that V \u03c0(s) = Ea\u223c\u03c0 [Q\u03c0(s, a)] = Ea\u223c\u03c0,\u03c1 [D\u03c0(s, a)].\nVariational Inference. Variational inference is a powerful method for approximating complex posterior distributions, which is effective for RL to handle the parameter uncertainty and deal with modelling errors [36]. Given an observation X and latent variables Z, Bayesian inference aims to compute the posterior distribution p(Z|X). Direct computation of this posterior is often intractable due to the high-dimensional integrals involved. To approximate the true posterior, Bayesian inference introduces a parameterized distribution q(Z; \u03d5) and minimizes the Kullback-Leibler (KL) divergence DKL(q(Z; \u03d5)\u2225p(Z|X)). Note that minimizing the KL divergence is equivalent to maximizing the evidence lower bound (ELBO) [37, 38]: ELBO(\u03d5) = Eq(Z;\u03d5)[log p(X, Z) \u2212log q(Z; \u03d5)]. Offline RL under Diverse Data Corruptions. In the real world, the data collected by sensors or humans may be subject to diverse corruption due to sensor failures or malicious attacks. Let b and B denotes the uncorrupted and corrupted dataset with samples {(si t, ai t, ri t, si t+1)}N i=1, respectively. Each data in B may be corrupted. We assume that an uncorrupted state follows a state distribution pb(\u00b7), a corrupted state follows pB(\u00b7), an uncorrupted action follows a behavior policy \u03c0b(\u00b7|si t), a corrupted action is sampled from \u03c0B(\u00b7|si t), a corrupted reward is sampled from \u03c1B(\u00b7|si t, ai t), and a corrupted next state is drawn from PB(\u00b7|si t, ai t). We also denote the uncorrupted and corrupted empirical state-action distributions as pb(si t, ai t) and pB(si t, ai t), respectively. Moreover, we introduce the notations [18, 39] as follows. \u02dcT Q(s, a) = \u02dcr(s, a) + Es\u2032\u223cPB(\u00b7|s,a) [V (s\u2032)] , \u02dcr(s, a) = Er\u223c\u03c1B(\u00b7|s,a)[r], (2) \u02dcT D(s, a) : D= R(s, a) + \u03b3D (s\u2032, a\u2032) , s\u2032 \u223cPB(\u00b7|s, a), a\u2032 \u223c\u03c0B(\u00b7|s), (3)\nVariational Inference. Variational inference is a powerful method for approximating complex posterior distributions, which is effective for RL to handle the parameter uncertainty and deal with modelling errors [36]. Given an observation X and latent variables Z, Bayesian inference aims to compute the posterior distribution p(Z|X). Direct computation of this posterior is often intractable due to the high-dimensional integrals involved. To approximate the true posterior, Bayesian inference introduces a parameterized distribution q(Z; \u03d5) and minimizes the Kullback-Leibler (KL) divergence DKL(q(Z; \u03d5)\u2225p(Z|X)). Note that minimizing the KL divergence is equivalent to maximizing the evidence lower bound (ELBO) [37, 38]: ELBO(\u03d5) = Eq(Z;\u03d5)[log p(X, Z) \u2212log q(Z; \u03d5)].\n(1)\n(2) (3)\n(2)\nfor any (s, a) \u2208S \u00d7 A and Q : S \u00d7 A \ufffd\u2192[0, rmax/(1 \u2212\u03b3)], where X : D= Y denotes equality of probability laws, that is the random variable X is distributed according to the same law as Y . To address the diverse data corruptions, based on IQL [26], RIQL [18] introduces quantile estimators with an ensemble of action-value functions {Q\u03b8i(s, a)}K i=1 and employs a Huber regression [40]: LQ (\u03b8i) = E(s,a,r,s\u2032)\u223cB [l\u03ba H (r + \u03b3V\u03c8 (s\u2032) \u2212Q\u03b8i(s, a))] , l\u03ba H(x) = \ufffd1 2\u03bax2, if |x| \u2264\u03ba |x| \u22121 2\u03ba, if |x| > \u03ba , (4) LV (\u03c8) = E(s,a)\u223cB [L\u03bd 2 (Q\u03b1(s, a) \u2212V\u03c8(s))] , L\u03bd 2(x) = |\u03bd \u2212I(x < 0)| \u00b7 x2. (5) Note that l\u03ba H is the Huber loss, and Q\u03b1 is the \u03b1-quantile value among {Q\u03b8i(s, a)}K i=1. RIQL then follows IQL [26] to learn the policy using weighted imitation learning with a hyperparameter \u03b2: L\u03c0(\u03d5) = E(s,a)\u223cB [exp(\u03b2 \u00b7 A\u03b1(s, a)) log \u03c0\u03d5(a|s)] , A\u03b1(s, a) = Q\u03b1(s, a) \u2212V\u03c8(s). (6)\nfor any (s, a) \u2208S \u00d7 A and Q : S \u00d7 A \ufffd\u2192[0, rmax/(1 \u2212\u03b3)], where X : D= Y denotes equality of probability laws, that is the random variable X is distributed according to the same law as Y . To address the diverse data corruptions, based on IQL [26], RIQL [18] introduces quantile estimators with an ensemble of action-value functions {Q\u03b8i(s, a)}K i=1 and employs a Huber regression [40]:\nprobability laws, that is the random variable X is distributed according to the same law as Y . To address the diverse data corruptions, based on IQL [26], RIQL [18] introduces quantile estimators with an ensemble of action-value functions {Q\u03b8i(s, a)}K i=1 and employs a Huber regression [40]: \ufffd\nLV (\u03c8) = E(s,a)\u223cB [L\u03bd 2 (Q\u03b1(s, a) \u2212V\u03c8(s))] , L\u03bd 2(x) = |\u03bd \u2212I(x < 0)| \u00b7 x2. (5) Note that l\u03ba H is the Huber loss, and Q\u03b1 is the \u03b1-quantile value among {Q\u03b8i(s, a)}K i=1. RIQL then follows IQL [26] to learn the policy using weighted imitation learning with a hyperparameter \u03b2: L\u03c0(\u03d5) = E(s,a)\u223cB [exp(\u03b2 \u00b7 A\u03b1(s, a)) log \u03c0\u03d5(a|s)] , A\u03b1(s, a) = Q\u03b1(s, a) \u2212V\u03c8(s). (6)\nNote that l\u03ba H is the Huber loss, and Q\u03b1 is the \u03b1-quantile value among {Q\u03b8i(s, a)}K i=1. RIQL then follows IQL [26] to learn the policy using weighted imitation learning with a hyperparameter \u03b2:\n# 3 Algorithm\nWe first introduce the Bayesian inference for capturing the uncertainty caused by diverse corrupted data in Section 3.1. Then, we provide our algorithm TRACER with the entropy-based uncertainty measure in Section 3.2. Moreover, we provide the theoretical analysis for robustness, the architecture, and the detailed implementation of TRACER in Appendices A.1, B.1, and B.2, respectively.\n# 1 Variational Inference for Uncertainty induced by Corrupted Data\nWe focus on corruption-robust offline RL to learn an agent under diverse data corruptions, i.e., random or adversarial attacks on four elements of the dataset. We propose to use all elements as observations, leveraging the data correlations to simultaneously address the uncertainty. By introducing Bayesian inference framework, our aim is to approximate the posterior distribution of the action-value function. At the beginning, based on the relationships between the action values and the four elements (i.e., states, actions, rewards, next states) in the offline dataset as shown in Figure 1, we define D\u03b8 = D\u03b8(S, A, R) \u223cp\u03b8(\u00b7|S, A, R), parameterized by \u03b8. Building on the action-value distribution, we can explore how to estimate the posterior of D\u03b8 using the elements available in the offline data. Firstly, we start from the actions {ai t}N i=1 following the corrupted distribution \u03c0B and use them as observations to approximate the posterior of the action-value distribution under a variational inference. As the actions are correlated with the action values and all other elements in the dataset, the likelihood is p\u03c6a(A|D, S, R, S\u2032), parameterized by \u03c6a. Then, under the variational inference framework, we maximize the posterior and derive to minimize the loss function based on ELBO: LD|A(\u03b8, \u03c6a) = EB,p\u03b8 \ufffd DKL \ufffd p\u03c6a(A|D\u03b8, S, R, S\u2032) \u2225\u03c0B(A|S) \ufffd \u2212EA\u223cp\u03c6a [log p\u03b8(D\u03b8|S, A, R)] \ufffd , (7) where S, R, and S\u2032 follow the offline data distributions pB, \u03c1B, and PB, respectively. Secondly, we apply the rewards {ri t}N i=1 drawn from the corrupted reward distribution \u03c1B as the observations. Considering that the rewards are related to the states, actions, and action values, we model the likelihood as p\u03c6r(R|D, S, A), parameterized by \u03c6r. Therefore, we can derive a loss function by following Equation (7): LD|R(\u03b8, \u03c6r) = EB,p\u03b8 \ufffd DKL \ufffd p\u03c6r(R|D\u03b8, S, A) \u2225\u03c1B(R|S, A) \ufffd \u2212ER\u223cp\u03c6r [log p\u03b8(D\u03b8|S, A, R)] \ufffd , (8) where S and A follow the offline data distributions pB and \u03c0B, respectively. Finally, we employ the state {si t}N i=1 in the offline dataset following the corrupted distribution pB as the observations. Due to the relation of the states, we can model the likelihood as p\u03c6s(S|D, A, R), parameterized by \u03c6r. We then have the loss function: LD|S(\u03b8, \u03c6s) = EB,p\u03b8 \ufffd DKL \ufffd p\u03c6s(S|D\u03b8, A, R) \u2225pB(S) \ufffd \u2212ES\u223cp\u03c6s [log p\u03b8(D\u03b8|S, A, R)] \ufffd , (9) where A and R follow the offline data distributions \u03c0B and \u03c1B, respectively. We present the detailed derivation process in Appendix A.2. The goal of first terms in Equations (7), (8), and (9) is to estimate \u03c0B(A|S), \u03c1B(R|S, A), and pB(S) using p\u03c6a(A|D\u03b8, S, R, S\u2032), p\u03c6r(R|D\u03b8, S, A), and p\u03c6s(S|D\u03b8, A, R), respectively. As we do not\n (4)\n(5)\n(6)\n\ufffd (7)\nhave the explicit expression of distributions \u03c0B, \u03c1B, and pB, we cannot directly compute the KL divergence in these first terms. To address this issue, based on the generalized Bayesian inference [41], we can exchange two distributions in the KL divergence. Then, we model all the aformentioned distributions as Gaussian distributions, and use the mean \u00b5\u03c6 and standard deviation \u03a3\u03c6 to represent the corresponding p\u03c6. For implementation, we directly employ MLPs to output each (\u00b5\u03c6, \u03a3\u03c6) using the corresponding conditions of p\u03c6. Then, based on the KL divergence between two Gaussian distributions, we can derive the loss function as follows.\nLfirst(\u03b8, \u03c6s, \u03c6a, \u03c6r) = 1 2E(s,a,r)\u223cB,D\u03b8\u223cp\u03b8 \ufffd (\u00b5\u03c6a \u2212a)T \u03a3\u22121 \u03c6a (\u00b5\u03c6a \u2212a) + (\u00b5\u03c6r \u2212r)T \u03a3\u22121 \u03c6r (\u00b5\u03c6r \u2212 \ufffd\n\ufffd Moreover, the goal of second terms in Equations (7), (8), and (9) is to maximize the likelihoods of D\u03b8 given samples \u02c6s \u223cp\u03c6s, \u02c6a \u223cp\u03c6a, or \u02c6r \u223cp\u03c6r. Thus, with (s, a, r) \u223cB, we propose minimizing the distance between D\u03b8(\u02c6s, a, r) and D(s, a, r), D\u03b8(s, \u02c6a, r) and D(s, a, r), and D\u03b8(s, a, \u02c6r) and D(s, a, r), where \u02c6s \u223cp\u03c6s, \u02c6a \u223cp\u03c6a, and \u02c6r \u223cp\u03c6r. Then, based on [41], we can derive the following loss with any metric \u2113to maximize the log probabilities: \ufffd \ufffd \ufffd\nLsecond(\u03b8, \u03c6s, \u03c6a, \u03c6r) = E(s,a,r)\u223cB,\u02c6s\u223cp\u03c6s,\u02c6a\u223cp\u03c6a,\u02c6r\u223cp\u03c6r ,D\u223cp \ufffd \u2113 \ufffd D(s, a, r), D\u03b8(s, \u02c6a, r) \ufffd + \u2113 \ufffd D(s, a, r), D\u03b8(s, a, \u02c6r) \ufffd + \u2113 \ufffd D(s, a, r), D\u03b8(\u02c6s, a, r) \ufffd\ufffd .\n# \ufffd \ufffd \ufffd 2 Corruption-Robust Algorithm with the Entropy-based Uncertain\nWe focus on developing tractable loss functions for implementation in this subsection. Learning the Action-Value Distribution based on Temporal Difference (TD). Based on [42, 43 we introduce the quantile regression [44] to approximate the action-value distribution in the offlin dataset B using an ensemble model {D\u03b8i}K i=1. We use Equation (4) to derive the loss as: \uf8ee \uf8f9\n developing tractable loss functions for implementation in this subsection\nLearning the Action-Value Distribution based on Temporal Difference (TD). Based on [42, 43], we introduce the quantile regression [44] to approximate the action-value distribution in the offline dataset B using an ensemble model {D\u03b8i}K i=1. We use Equation (4) to derive the loss as: \uf8ee \uf8f9\nwhere \u03c1\u03ba \u03c4 (\u03b4) = |\u03c4 \u2212I {\u03b4 < 0}| \u00b7 l\u03ba H (\u03b4) with the threshold \u03ba, Z denotes the value distribution, \u03b4\u03c4,\u03c4 \u2032 \u03b8i is the sampled TD error based on the parameters \u03b8i, \u03c4 and \u03c4 \u2032 are two samples drawn from a uniform distribution U([0, 1]), D\u03c4 \u03b8(s, a, r) := F \u22121 D\u03b8(s,a,r)(\u03c4) is the sample drawn from p\u03b8(\u00b7|s, a, r), Z\u03c4(s) := F \u22121 Z(s)(\u03c4) is sampled from p(\u00b7|s), F \u22121 X (\u03c4) is the inverse cumulative distribution function (also known as quantile function) [45] at \u03c4 for the random variable X, and N and N \u2032 represent the respective number of iid samples \u03c4 and \u03c4 \u2032. Notably, based on [43], we have Q\u03b8i(s, a) = \ufffdN n=1 D\u03c4n \u03b8i (s, a, r). In addition, if we learn the value distribution Z, the action-value distribution can extract the information from the next states based on Equation (12), which is effective for capturing the uncertainty. On the contrary, if we directly use the next states in the offline dataset as the observations, in practice, the parameterized model of the action-value distribution needs to take (s, a, r, s\u2032, a\u2032, r\u2032, s\u2032\u2032) as the input data. Thus, the model can compute the action values and values for the sampled TD error in Equation (12). To avoid the changes in the input data caused by directly using next states as observations in Bayesian inference, we draw inspiration from IQL and RIQL to learn a parameterized value distribution. Based on Equations (5) and (12), we derive a new objective as:\n\ufffd where D\u03c4 \u03b1 is the \u03b1-quantile value among {D\u03c4 \u03b8i(s, a)}K i=1, and V\u03c8(s) = \ufffdN n=1 Z\u03c4n \u03c8 (s). More details are shown in Appendix B.2. Furthermore, we provide the theoretical analysis in Appendix A.1 to give a value bound between the value distributions under clean and corrupted data. Updating the Action-Value Distribution based on Variational Inference for Robustness. We discuss\nwhere D\u03c4 \u03b1 is the \u03b1-quantile value among {D\u03c4 \u03b8i(s, a)}K i=1, and V\u03c8(s) = \ufffdN n=1 Z\u03c4n \u03c8 (s). More detail are shown in Appendix B.2. Furthermore, we provide the theoretical analysis in Appendix A.1 to give a value bound between the value distributions under clean and corrupted data. Updating the Action-Value Distribution based on Variational Inference for Robustness. We discus the detailed implementation of Equations (10) and (11) based on Equations (12) and (13). As the data\n  Updating the Action-Value Distribution based on Variational Inference for Robustness. We discuss the detailed implementation of Equations (10) and (11) based on Equations (12) and (13). As the data\n\u2212r)\n(10)\n(12)\n(13)\ncorruptions may introduce heavy-tailed targets [18], we apply the Huber loss to replace all quadratic loss in Equation (10) and the metric \u2113in Equation (11), mitigating the issue caused by heavy-tailed targets [46] for robustness. We rewrite Equation (11) as follows.\n+ l\u03ba H \ufffd D\u03c4n(s, a, r), D\u03c4n \u03b8i (s, a, \u02c6r) \ufffd + l\u03ba H \ufffd D\u03c4n(s, a, r), D\u03c4n \u03b8i (\u02c6s, a, r) \ufffd \ufffd\nThus, we have the whole loss function LD|S,A,R = Lfirst(\u03b8i, \u03c6s, \u03c6a, \u03c6r) + Lsecond(\u03b8i, \u03c6s, \u03c6a, \u03c6r) in the generalized variational inference framework. Moreover, based on the assumption of heavy-tailed noise in [18], we have a upper bound of action-value distribution by using the Huber regression loss. Entropy-based Uncertainty Measure for Regulating the Loss associated with Corrupted Data. To further address the challenge posed by diverse data corruptions, we consider the problem: how to exploit uncertainty to further enhance robustness? Considering that our goal is to improve performance in clean environments, we propose to reduce the influence of corrupted data, focusing on using clean data to learn agents. Therefore, we provide a two-step plan: (1) distinguishing corrupted data from clean data; (2) regulating the loss associated with corrupted data to reduce its influence, thus enhancing the performance in clean environments. For (1), as the Shannon entropy for the measures of aleatoric and epistemic uncertainties provides important insight [47\u201349], and the corrupted data often results in higher uncertainty and entropy of the action-value distribution than the clean data, we use entropy [50] to quantify uncertainties of corrupted and clean data. Furthermore, by considering that the exponential function can amplify the numerical difference in entropy between corrupted and clean data, we propose the use of exponential entropy [51]\u2014a metric of extent of a distribution\u2014to design our uncertainty measure. Specifically, based on Equation 12, we can use the quantile points {\u03c4n}N n=1 to learn the corresponding quantile function values {D\u03c4n}N n=1 drawn from the action-value distribution p\u03b8. We sort the quantile points and their corresponding function values in ascending order based on the values. Thus, we have the sorted sets {\u03c2n}N n=1, {D\u03c2n}N n=1, and the estimated PDF values {\u03c2n}N n=1, where \u03c21 = \u03c21 and \u03c2n = \u03c2n \u2212\u03c2n\u22121 for 1 < n \u2264N. Then, we can further estimate differential entropy following [52] (see Appendix A.3 for a detailed derivation).\nwhere \u02c6\u03c2n denotes (\u03c2n\u22121 + \u03c2n)/2 for 1 < n \u2264N, and D \u03c2n denotes D\u03c2n \u2212D\u03c2n\u22121 for 1 < n \u2264N. For (2), TRACER employs the reciprocal value of exponential entropy 1/ exp(H(p\u03b8i)) to weight the corresponding loss of \u03b8i in our proposed whole loss function LD|S,A,R. Therefore, during the learning process, TRACER can regulate the loss associated with corrupted data and focus on minimizing the loss associated with clean data, enhancing robustness and performance in clean environments. Note that we normalize entropy values by dividing the mean of samples (i.e., quantile function values) drawn from action-value distributions for each batch. In Figure 3, we show the relationship of entropy values of corrupted and clean data estimated by Equation (15) during the learning process. The results illustrate the effectiveness of the entropy-weighted technique for data corruptions. Updating the Policy based on the Action-Value Distribution. We directly applies the weighted imitation learning technique in Equation (6) to learn the policy. As Q\u03b1(s, a) is the \u03b1-quantile value among {Q\u03b8i(s, a)}K i=1 = \ufffd\ufffdN n=1 D\u03c4n \u03b8i (s, a, r) \ufffdK i=1 and V\u03c8(s) = \ufffdN n=1 Z\u03c4n \u03c8 (s), we have\n (14)\n(15)\n(16)\n<div style=\"text-align: center;\">and standard errors under random and adversarial simultaneous corruptio</div>\n<div style=\"text-align: center;\">e 1: Average scores and standard errors under random and adversarial sim</div>\nEnv\nCorrupt\nBC\nEDAC\nMSG\nUWMSG\nCQL\nIQL\nRIQL\nTRACER (ours)\nHalfcheetah\nrandom\n23.17 \u00b1 0.43\n1.70 \u00b1 0.80\n9.97 \u00b1 3.44\n8.31 \u00b1 1.25\n14.25 \u00b1 1.39\n24.82 \u00b1 0.57\n29.94 \u00b1 1.00\n33.04 \u00b1 0.42\nadvers\n16.37 \u00b1 0.32\n0.90 \u00b1 0.30\n3.60 \u00b1 0.89\n3.13 \u00b1 0.85\n5.61 \u00b1 2.21\n11.06 \u00b1 0.45\n17.85 \u00b1 1.39\n19.72 \u00b1 2.80\nWalker2d\nrandom\n13.77 \u00b1 1.05\n\u22120.13 \u00b1 0.01\n\u22120.15 \u00b1 0.11\n4.36 \u00b1 1.95\n0.63 \u00b1 0.36\n12.35 \u00b1 2.03\n17.42 \u00b1 2.95\n23.62 \u00b1 2.33\nadvers\n6.75 \u00b1 0.33\n\u22120.17 \u00b1 0.01\n3.77 \u00b1 1.09\n4.19 \u00b1 2.82\n4.23 \u00b1 1.35\n16.61 \u00b1 2.73\n9.20 \u00b1 1.40\n17.21 \u00b1 1.62\nHopper\nrandom\n18.49 \u00b1 0.52\n0.80 \u00b1 0.01\n15.84 \u00b1 2.47\n12.22 \u00b1 2.11\n3.16 \u00b1 1.07\n25.28 \u00b1 15.34\n22.50 \u00b1 10.01\n28.83 \u00b1 7.06\nadvers\n17.34 \u00b1 1.00\n0.80 \u00b1 0.01\n12.14 \u00b1 0.71\n10.43 \u00b1 0.94\n0.10 \u00b1 0.34\n19.56 \u00b1 1.08\n24.71 \u00b1 6.20\n24.80 \u00b1 7.14\nAverage score\n15.98\n0.65\n7.53\n7.11\n4.66\n18.28\n20.27\n24.54\n<div style=\"text-align: center;\">Table 2: Average score under diverse random corruptions.</div>\nEnv\nCorrupted Element\nBC\nEDAC\nMSG\nUWMSG\nCQL\nIQL\nRIQL\nTRACER (ours)\nHalfcheetah\nobservation\n33.4 \u00b1 1.8\n2.1 \u00b1 0.5\n\u22120.2 \u00b1 2.2\n2.9 \u00b1 0.1\n9.0 \u00b1 7.5\n21.4 \u00b1 1.9\n27.3 \u00b1 2.4\n34.2 \u00b1 0.9\naction\n36.2 \u00b1 0.3\n47.4 \u00b1 1.3\n52.0 \u00b1 0.9\n56.0 \u00b1 0.4\n19.9 \u00b1 21.3\n42.2 \u00b1 1.9\n42.9 \u00b1 0.6\n42.9 \u00b1 0.6\nreward\n35.8 \u00b1 0.9\n38.6 \u00b1 0.3\n17.5 \u00b1 16.4\n35.6 \u00b1 0.4\n32.6 \u00b1 19.6\n42.3 \u00b1 0.4\n43.6 \u00b1 0.6\n40.0 \u00b1 1.1\ndynamics\n35.8 \u00b1 0.9\n1.5 \u00b1 0.2\n1.7 \u00b1 0.4\n2.9 \u00b1 0.1\n29.2 \u00b1 4.0\n36.7 \u00b1 1.8\n43.1 \u00b1 0.2\n43.8 \u00b1 3.0\nWalker2d\nobservation\n9.6 \u00b1 3.9\n\u22120.2 \u00b1 0.3\n\u22120.4 \u00b1 0.1\n6.2 \u00b1 0.5\n19.4 \u00b1 1.6\n27.2 \u00b1 5.1\n28.4 \u00b1 7.7\n32.7 \u00b1 2.8\naction\n18.1 \u00b1 2.1\n83.2 \u00b1 1.9\n25.3 \u00b1 10.6\n31.5 \u00b1 10.6\n62.7 \u00b1 7.2\n71.3 \u00b1 7.8\n84.6 \u00b1 3.3\n86.7 \u00b1 6.2\nreward\n16.0 \u00b1 7.4\n4.3 \u00b1 3.6\n18.4 \u00b1 9.5\n62.0 \u00b1 3.7\n69.4 \u00b1 7.4\n65.3 \u00b1 8.4\n83.2 \u00b1 2.6\n85.5 \u00b1 3.1\ndynamics\n16.0 \u00b1 7.4\n\u22120.1 \u00b1 0.0\n7.4 \u00b1 3.7\n0.2 \u00b1 0.0\n\u22120.2 \u00b1 0.1\n17.7 \u00b1 7.3\n78.2 \u00b1 1.8\n75.9 \u00b1 1.8\nHopper\nobservation\n21.5 \u00b1 2.9\n1.0 \u00b1 0.5\n6.9 \u00b1 5.0\n12.8 \u00b1 0.4\n42.8 \u00b1 7.0\n52.0 \u00b1 16.6\n62.4 \u00b1 1.8\n62.7 \u00b1 8.2\naction\n22.8 \u00b1 7.0\n100.8 \u00b1 0.5\n37.6 \u00b1 6.5\n53.4 \u00b1 5.4\n69.8 \u00b1 4.5\n76.3 \u00b1 15.4\n90.6 \u00b1 5.6\n92.8 \u00b1 2.5\nreward\n19.5 \u00b1 3.4\n2.6 \u00b1 0.7\n24.9 \u00b1 4.3\n60.8 \u00b1 7.5\n70.8 \u00b1 8.9\n69.7 \u00b1 18.8\n84.8 \u00b1 13.1\n85.7 \u00b1 1.4\ndynamics\n19.5 \u00b1 3.4\n0.8 \u00b1 0.0\n12.4 \u00b1 4.9\n6.1 \u00b1 1.3\n0.8 \u00b1 0.0\n1.3 \u00b1 0.5\n51.5 \u00b1 8.1\n49.8 \u00b1 5.3\nAverage score\n23.7\n23.5\n17.0\n27.5\n35.5\n43.6\n60.0\n61.1\n<div style=\"text-align: center;\">Table 3: Average score under diverse adversarial corruptions.</div>\nEnv\nCorrupted Element\nBC\nEDAC\nMSG\nUWMSG\nCQL\nIQL\nRIQL\nTRACER (ours)\nHalfcheetah\nobservation\n34.5 \u00b1 1.5\n1.1 \u00b1 0.3\n1.1 \u00b1 0.2\n1.9 \u00b1 0.1\n5.0 \u00b1 11.6\n32.6 \u00b1 2.7\n35.7 \u00b1 4.2\n36.8 \u00b1 2.1\naction\n14.0 \u00b1 1.1\n32.7 \u00b1 0.7\n37.3 \u00b1 0.7\n36.2 \u00b1 1.0\n\u22122.3 \u00b1 1.2\n27.5 \u00b1 0.3\n31.7 \u00b1 1.7\n33.4 \u00b1 1.2\nreward\n35.8 \u00b1 0.9\n40.3 \u00b1 0.5\n47.7 \u00b1 0.4\n43.8 \u00b1 0.3\n\u22121.7 \u00b1 0.3\n42.6 \u00b1 0.4\n44.1 \u00b1 0.8\n41.9 \u00b1 0.2\ndynamics\n35.8 \u00b1 0.9\n\u22121.3 \u00b1 0.1\n\u22121.5 \u00b1 0.0\n5.0 \u00b1 2.2\n\u22121.6 \u00b1 0.0\n26.7 \u00b1 0.7\n35.8 \u00b1 2.1\n36.2 \u00b1 1.2\nWalker2d\nobservation\n12.7 \u00b1 5.9\n\u22120.0 \u00b1 0.1\n2.9 \u00b1 2.7\n6.3 \u00b1 0.7\n61.8 \u00b1 7.4\n37.7 \u00b1 13.0\n70.0 \u00b1 5.3\n70.0 \u00b1 6.7\naction\n5.4 \u00b1 0.4\n41.9 \u00b1 24.0\n5.4 \u00b1 0.9\n5.9 \u00b1 0.4\n27.0 \u00b1 7.5\n27.5 \u00b1 0.6\n66.1 \u00b1 4.6\n69.3 \u00b1 4.6\nreward\n16.0 \u00b1 7.4\n57.3 \u00b1 33.2\n9.6 \u00b1 4.9\n35.1 \u00b1 10.5\n67.0 \u00b1 6.1\n73.5 \u00b1 4.9\n85.0 \u00b1 1.5\n88.9 \u00b1 4.7\ndynamics\n16.0 \u00b1 7.4\n4.3 \u00b1 0.9\n0.1 \u00b1 0.2\n1.8 \u00b1 0.2\n3.9 \u00b1 1.4\n\u22120.1 \u00b1 0.1\n60.6 \u00b1 21.8\n64.0 \u00b1 16.5\nHopper\nobservation\n21.6 \u00b1 7.1\n36.2 \u00b1 16.2\n16.0 \u00b1 2.8\n15.0 \u00b1 1.3\n78.0 \u00b1 6.5\n32.8 \u00b1 6.4\n50.8 \u00b1 7.6\n64.5 \u00b1 3.7\naction\n15.5 \u00b1 2.2\n25.7 \u00b1 3.8\n23.0 \u00b1 2.1\n27.7 \u00b1 1.3\n32.2 \u00b1 7.6\n37.9 \u00b1 4.8\n63.6 \u00b1 7.3\n67.2 \u00b1 3.8\nreward\n19.5 \u00b1 3.4\n21.2 \u00b1 1.9\n22.6 \u00b1 2.8\n30.3 \u00b1 4.2\n49.6 \u00b1 12.3\n57.3 \u00b1 9.7\n65.8 \u00b1 9.8\n64.3 \u00b1 1.5\ndynamics\n19.5 \u00b1 3.4\n0.6 \u00b1 0.0\n0.6 \u00b1 0.0\n0.7 \u00b1 0.0\n0.6 \u00b1 0.0\n1.3 \u00b1 1.1\n65.7 \u00b1 21.1\n61.1 \u00b1 6.2\nAverage score\n20.5\n21.7\n13.7\n17.5\n26.6\n33.1\n56.2\n58.1\n# 4 Experiments\nIn this section, we show the effectiveness of TRACER across various simulation tasks using diverse corrupted offline datasets. Firstly, we provide our experiment setting, focusing on the corruption settings for offline datasets. Then, we illustrate how TRACER significantly outperforms previous stateof-the-art approaches under a range of both individual and simultaneous data corruptions. Finally, we conduct validation experiments and ablation studies to show the effectiveness of TRACER.\n# 4.1 Experiment Setting\nBuilding upon RIQL [18], we use two hyperparameters, i.e., corruption rate c \u2208[0, 1] and corruption scale \u03f5, to control the corruption level. Then, we introduce the random corruption and adversarial corruption in four elements (i.e., states, actions, rewards, next states) of offline datasets. The implementation of random corruption is to add random noise to elements of a c portion of the offline datasets, and the implementation of adversarial corruption follows the Projected Gradient Descent attack [53, 54] using pretrained value functions. Note that unlike other adversarial corruptions, the adversarial reward corruption multiplies \u2212\u03f5 to the clean rewards instead of using gradient optimization. We also introduce the random or adversarial simultaneous corruption, which refers to random or adversarial corruption simultaneously present in four elements of the offline datasets. We apply the corruption rate c = 0.3 and corruption scale \u03f5 = 1.0 in our experiments.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e38/6e38b167-3599-4859-acb0-683a6abd1f7a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: In the left, we report the means and standard deviations on CARLA under random simultaneous corruptions. In the right, we report the results with random simultaneous corruptions against different corruption levels.</div>\nFigure 2: In the left, we report the means and standard deviations on CARLA under random simultaneous corruptions. In the right, we report the results with random simultaneous corruptions against different corruption levels.\nWe conduct experiments on D4RL benchmark [55]. Referring to RIQL, we train all agents for 3000 epochs on the \u2019medium-replay-v2\u2019 dataset, which closely mirrors real-world applications as it is collected during the training of a SAC agent. Then, we evaluate agents in clean environments, reporting the average normalized performance over four random seeds. See Appendix C for detailed information. The algorithms we compare include: (1) CQL [7] and IQL [26], offline RL algorithms using a twin Q networks. (2) EDAC [56] and MSG [57], offline RL algorithms using ensemble Q networks (number of ensembles > 2). (3) UWMSG [17] and RIQL, state-of-the-arts in corruption-robust offline RL. Note that EDAC, MSG, and UWMSG are all uncertainty-based offline RL algorithms.\n# 4.2 Main results under Diverse Data Corruptions\nWe conduct experiments on MuJoCo [58] (see Tables 1, 2, and 3, which highlight the highest results) and CARLA [59] (see the left of Figure 2) tasks from D4RL under diverse corruptions to show the superiority of TRACER. In Table 1, we report all results under random or adversarial simultaneous data corruptions. These results show that TRACER significantly outperforms other algorithms in all tasks, achieving an average score improvement of +21.1%. In the left of Figure 2, results on \u2019CARLA-lane_v0\u2019 under random simultaneous corruptions also illustrate the superiority of TRACER. See Appendix C.2 for details.\nRandom Corruptions. We report the results under random simultaneous data corruptions of all algorithms in Table 1. Such results demonstrate that TRACER achieves an average score gain of +22.4% under the setting of random simultaneous corruptions. Based on the results, it is clear that many offline RL algorithms, such as EDAC, MSG, and CQL, suffer the performance degradation under data corruptions. Since UWMSG is designed to defend the corruptions in rewards and dynamics, its performance degrades when faced with the stronger random simultaneous corruption. Moreover, we report results across a range of individual random data corruptions in Table 2, where TRACER outperforms previous algorithms in 7 out of 12 settings. We then explore hyperparameter tuning on Hopper task and further improve TRACER\u2019s results, demonstrating its potential for performance gains. We provide details in Appendix C.3.1.\nAdversarial Corruptions. We construct experiments under adversarial simultaneous corruptions to evaluate the robustness of TRACER. The results in Table 1 show that TRACER surpasses others by a significant margin, achieving an average score improvement of +19.3%. In these simultaneous corruption, many algorithms experience more severe performance degradation compared to the random simultaneous corruption, which indicates that adversarial attacks are more damaging to the reliability of algorithms than random noise. Despite these challenges, TRACER consistently achieves significant performance gains over other methods. Moreover, we provide the results across a range of individual adversarial data corruptions in Table 3, where TRACER outperforms previous algorithms in 7 out of 12 settings. We also explore hyperparameter tuning on Hopper task and further improve TRACER\u2019s results, demonstrating its potential for performance gains. See Appendix C.3.1 for details.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1349/1349425f-b600-4469-80f9-6e95b26791df.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: In the first column, we report the mean and standard deviation to show the superiority of using the entropy-based uncertainty measure. In the second and third columns, we report the results over three seeds to show the higher entropy of corrupted data compared to clean data during training.</div>\n# 4.3 Evaluation of TRACER under Various Corruption Levels\nBuilding upon RIQL, we further extend our experiments to include Mujoco datasets with various corruption levels, using different corruption rates c and scales \u03f5. We report the average scores and standard deviations over four random seeds in the right of Figure 2, using batch sizes of 256. Results in the right of Figure 2 demonstrate that TRACER significantly outperforms baseline algorithms in all tasks under random simultaneous corruptions with various corruption levels. It achieves an average score improvement of +33.6%. Moreover, as the corruption levels increase, the slight decrease in TRACER\u2019s results indicates that while TRACER is robust to simultaneous corruptions, its performance depends on the extent of corrupted data it encounters. We also evaluate TRACER in different scales of corrupted data and provide the results in Appendix C.4.1.\n# 4.4 Evaluation of the Entropy-based Uncertainty Measure\nWe evaluate the entropy-based uncertainty measure in action-value distributions to show: (1) is the uncertainty caused by corrupted data higher than that of clean data? (2) is regulating the loss associated with corrupted data effective in improving performance?\nFor (1), we first introduce labels indicating whether the data is corrupted. Importantly, these labels are not used by agents during the training process. Then, we estimate entropy values of labelled corrupted and clean data in each batch based on Equation (15). Thus, we can compare entropy values to compute results, showing how many times the entropy of the corrupted data is higher than that of clean data. Specifically, we evaluate the accuracy every 50 epochs over 3000 epochs. For each evaluation, we sample 500 batches to compute the average entropy of corrupted and clean data. Each batch consists of 32 clean and 32 corrupted data. We illustrate the curves over three seeds in the second and third columns of Figure 3, where each point shows how many of the 500 batches have higher entropy for corrupted data than that of clean data. Figure 3 indicates an oscillating upward trend of TRACER\u2019s measurement accuracy using entropy (TRACER Using Entro) under simultaneous corruptions, demonstrating that using the entropy-based uncertainty measure can effectively distinguish corrupted data from clean data. These curves also reveal that even in the absence of any constraints on entropy (TRACER NOT using Entro), the entropy associated with corrupted data tends to exceed that of clean data. For (2), in the first column of Figure 3, these results demonstrate that TRACER using the entropybased uncertainty measure can effectively reduce the influence of corrupted data, thereby enhancing robustness and performance against all corruptions. We provide detailed information for this evaluation in Appendix C.4.2.\n# 5 Related Work\nRobust RL. Robust RL can be categorized into two types: testing-time robust RL and training-time robust RL. Testing-time robust RL [19, 20] refers to training a policy on clean data and ensuring its robustness by testing in an environment with random noise or adversarial attacks. Training-time robust RL [16, 17] aims to learn a robust policy in the presence of random noise or adversarial attacks during training and evaluate the policy in a clean environment. In this paper, we focus on training-time robust RL under the offline setting, where the offline training data is subject to various data corruptions, also known as corruption-robust offline RL. Corruption-Robust RL. Some theoretical work on corruption-robust online RL [60\u201363] aims to analyze the sub-optimal bounds of learned policies under data corruptions. However, these studies primarily address simple bandits or tabular MDPs and focus on the reward corruption. Some further work [64, 65] extends the modeling problem to more general MDPs and begins to investigate the corruption in transition dynamics. It is worth noting that corruption-robust offline RL has not been widely studied. UWMSG [17] designs a value-based uncertainty-weighting technique, thus using the weight to mitigate the impact of corrupted data. RIQL [18] further extends the data corruptions to all four elements in the offline dataset, including states, actions, rewards, and next states (dynamics). It then introduces quantile estimators with an ensemble of action-value functions and employs a Huber regression based on IQL [26], alleviating the performance degradation caused by corrupted data. Bayesian RL. Bayesian RL integrates the Bayesian inference with RL to create a framework for decision-making under uncertainty [28]. It is important to highlight that Bayesian RL is divided into two categories for different uncertainties: the parameter uncertainty in the learning of models [66, 67] and the inherent uncertainty from the data/environment in the distribution over returns [68, 69]. In this paper, we focus on capturing the latter. For the latter uncertainty, in model-based Bayesian RL, many approaches [68, 70, 71] explicitly model the transition dynamics and using Bayesian inference to update the model. It is useful when dealing with complex environments for sample efficiency. In model-free Bayesian RL, value-based methods [69, 72] use the reward information to construct the posterior distribution of the action-value function. Besides, policy gradient methods [73, 74] use information of the return to construct the posterior distribution of the policy. They directly apply Bayesian inference to the value function or policy without explicitly modeling transition dynamics. Offline Bayesian RL. offline Bayesian RL integrates Bayesian inference with offline RL to tackle the challenges of learning robust policies from static datasets without further interactions with the environment. Many approaches [75\u201377] use Bayesian inference to model the transition dynamics or guide action selection for adaptive policy updates, thereby avoiding overly conservative estimates in the offline setting. Furthermore, recent work [78] applies variational Bayesian inference to learn the model of transition dynamics, mitigating the distribution shift in offline RL.\nOffline Bayesian RL. offline Bayesian RL integrates Bayesian inference with offline RL to tackle the challenges of learning robust policies from static datasets without further interactions with the environment. Many approaches [75\u201377] use Bayesian inference to model the transition dynamics or guide action selection for adaptive policy updates, thereby avoiding overly conservative estimates in the offline setting. Furthermore, recent work [78] applies variational Bayesian inference to learn the model of transition dynamics, mitigating the distribution shift in offline RL.\n# 6 Conclusion\nIn this paper, we investigate and demonstrate the robustness and effectiveness of introducing Bayesian inference into offline RL to address the challenges posed by data corruptions. By leveraging Bayesian techniques, our proposed approach TRACER captures the uncertainty caused by diverse corrupted data. Moreover, the use of entropy-based uncertainty measure in TRACER can distinguish corrupted data from clean data. Thus, TRACER can regulate the loss associated with corrupted data to reduce its influence, improving performance in clean environments. Our extensive experiments demonstrate the potential of Bayesian methods in developing reliable decision-making. Regarding the limitations of TRACER, although it achieves significant performance improvement under diverse data corruptions, future work could explore more complex and realistic data corruption scenarios and related challenges, such as the noise in the preference data for RLHF and adversarial attacks on safety-critical driving decisions. Moreover, we look forward to the continued development and optimization of uncertainty-based corrupted-robust offline RL, which could further enhance the effectiveness of TRACER and similar approaches for increasingly complex real-world scenarios.\nWe would like to thank all the anonymous reviewers for their insightful comments. This work was supported in part by National Key R&D Program of China under contract 2022ZD0119801, National Nature Science Foundations of China grants U23A20388 and 62021001, and DiDi GAIA Collaborative Research Funds.\n# References\n[1] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2052\u20132062. PMLR, 2019. [2] Rafael Figueiredo Prudencio, Marcos R. O. A. M\u00e1ximo, and Esther Luna Colombini. A survey on offline reinforcement learning: Taxonomy, review, and open problems. CoRR, abs/2203.01387, 2022. [3] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In Ken Jung, Serena Yeung, Mark P. Sendak, Michael W. Sjoding, and Rajesh Ranganath, editors, Proceedings of the Machine Learning for Healthcare Conference, volume 149 of Proceedings of Machine Learning Research, pages 2\u201335. PMLR, 2021. [4] Christopher Diehl, Timo Sievernich, Martin Kr\u00fcger, Frank Hoffmann, and Torsten Bertram. Uncertaintyaware model-based offline reinforcement learning for automated driving. IEEE Robotics Autom. Lett., 8(2):1167\u20131174, 2023. [5] Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, and Sergey Levine. Offline meta-reinforcement learning for industrial insertion. In 2022 International Conference on Robotics and Automation, pages 6386\u20136393. IEEE, 2022. [6] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 11761\u201311771, 2019. [7] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. [8] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: model-based offline policy optimization. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020. [9] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020. 10] Porter Jenkins, Hua Wei, J. Stockton Jenkins, and Zhenhui Li. Bayesian model-based offline reinforcement learning for product allocation. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 12531\u2013 12537. AAAI Press, 2022. 11] Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, and Junshan Zhang. Model-based offline metareinforcement learning with regularization. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022. 12] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 7436\u20137447, 2021. 13] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11319\u201311328. PMLR, 2021.\n[14] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022. [15] Filippo Valdettaro and A. Aldo Faisal. Towards offline reinforcement learning with pessimistic value priors. In Fabio Cuzzolin and Maryam Sultana, editors, Epistemic Uncertainty in Artificial Intelligence - First International Workshop, volume 14523 of Lecture Notes in Computer Science, pages 89\u2013100. Springer, 2024. [16] Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Corruption-robust offline reinforcement learning. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 5757\u20135773. PMLR, 2022. [17] Chenlu Ye, Rui Yang, Quanquan Gu, and Tong Zhang. Corruption-robust offline reinforcement learning with general function approximation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023. [18] Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, and Tong Zhang. Towards robust offline reinforcement learning under diverse data corruption. In The Eleventh International Conference on Learning Representations, 2023. [19] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust reinforcement learning using offline data. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. [20] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: robust offline reinforcement learning via conservative smoothing. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. [21] Jose H. Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023. [22] Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and control. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 14543\u201314553, 2019. [23] Fan Wu, Linyi Li, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, and Bo Li. COPA: certifying robust policies for offline reinforcement learning against poisoning attacks. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022. [24] Zhihe Yang and Yunjian Xu. Dmbp: Diffusion model based predictor for robust offline reinforcement learning against state observation perturbations. In The Twelfth International Conference on Learning Representations, 2023. [25] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019. [26] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In The Tenth International Conference on Learning Representations. OpenReview.net, 2022. [27] Rens van de Schoot, Sarah Depaoli, Ruth King, Bianca Kramer, Kaspar M\u00e4rtens, Mahlet G Tadesse, Marina Vannucci, Andrew Gelman, Duco Veen, Joukje Willemsen, et al. Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1):1, 2021. [28] Box George EP and Tiao George C. Bayesian inference in statistical analysis. John Wiley & Sons, 2011. [29] Esther Derman, Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor. A bayesian approach to robust reinforcement learning. In Amir Globerson and Ricardo Silva, editors, Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, volume 115 of Proceedings of Machine Learning Research, pages 648\u2013658. AUAI Press, 2019. [30] Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner, and Shimon Whiteson. VIREL: A variational inference framework for reinforcement learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7120\u20137134, 2019.\n[31] Mattie Fellows, Kristian Hartikainen, and Shimon Whiteson. Bayesian bellman operators. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 13641\u201313656, 2021. [32] Brendan O\u2019Donoghue. Variational bayesian reinforcement learning with regret bounds. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 28208\u201328221, 2021. [33] Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability challenges and effective data collection strategies. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 4607\u20134618, 2021. [34] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL policies should be trained to be adaptive. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7513\u20137530. PMLR, 2022. [35] Yuhao Wang and Enlu Zhou. Bayesian risk-averse q-learning with streaming observations. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023. [36] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement learning: A survey. Found. Trends Mach. Learn., 8(5-6):359\u2013483, 2015. [37] Carl Doersch. Tutorial on variational autoencoders. CoRR, abs/1606.05908, 2016. [38] Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach. Learn., 12(4):307\u2013392, 2019. [39] Marc G. Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 449\u2013458. PMLR, 2017. [40] Kotz Samuel and Johnson Norman L. Breakthroughs in statistics: methodology and distribution. Springer Science & Business Media, 2012. [41] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized variational inference. CoRR, abs/1904.02063, 2019. [42] Will Dabney, Mark Rowland, Marc G. Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning with quantile regression. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 2892\u20132901. AAAI Press, 2018. [43] Will Dabney, Georg Ostrovski, David Silver, and R\u00e9mi Munos. Implicit quantile networks for distributional reinforcement learning. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1104\u20131113. PMLR, 2018. [44] Koenker Roger and Hallock Kevin F. Quantile regression. Journal of economic perspectives, 15(4):143\u2013156, 2001. [45] M\u00fcller Alfred. Integral probability metrics and their generating classes of functions. Advances in applied probability, 29(2):429\u2013443, 1997. [46] Abhishek Roy, Krishnakumar Balasubramanian, and Murat A. Erdogdu. On empirical risk minimization with dependent and heavy-tailed data. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, pages 8913\u20138926, 2021. [47] Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, and Insup Lee. Credal bayesian deep learning. arXiv e-prints, pages arXiv\u20132302, 2023. [48] Michele Caprio, Yusuf Sale, Eyke H\u00fcllermeier, and Insup Lee. A novel bayes\u2019 theorem for upper probabilities. In Fabio Cuzzolin and Maryam Sultana, editors, Epistemic Uncertainty in Artificial Intelligence - First International Workshop, volume 14523 of Lecture Notes in Computer Science, pages 1\u201312. Springer, 2024. [49] Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk Jin Jang, Ivan Ruchkin, Oleg Sokolsky, and Insup Lee. Distributionally robust statistical verification with imprecise neural networks. CoRR, abs/2308.14815, 2023.\n[50] Jim W. Hall. Uncertainty-based sensitivity indices for imprecise probability distributions. Reliab. Eng. Syst. Saf., 91(10-11):1443\u20131451, 2006. [51] L Lorne Campbell. Exponential entropy as a measure of extent of a distribution. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 5(3):217\u2013225, 1966. [52] Sheri Edwards. Thomas m. cover and joy a. thomas, elements of information theory (2nd ed.), john wiley & sons, inc. (2006). Inf. Process. Manag., 44(1):400\u2013401, 2008. [53] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, 2018. [54] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane S. Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33, 2020. [55] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep data-driven reinforcement learning. CoRR, 2020. [56] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34, 2021. [57] Seyed Kamyar Seyed Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating uncertainties for offline RL through ensembles, and why their independence matters. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. [58] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pages 5026\u20135033. IEEE, 2012. [59] Alexey Dosovitskiy, Germ\u00e1n Ros, Felipe Codevilla, Antonio M. L\u00f3pez, and Vladlen Koltun. CARLA: an open urban driving simulator. In 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 1\u201316. PMLR, 2017. [60] Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial markov decision processes with bandit feedback and unknown transition. In Proceedings of the 37th International Conference on Machine Learning, 2020. [61] Thodoris Lykouris, Vahab S. Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, 2018. [62] Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, 2019. [63] Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with adversarial corruptions. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory, 2019. [64] Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, 2023. [65] Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration in episodic reinforcement learning. In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning Theory, 2021. [66] Matthieu Geist and Olivier Pietquin. Kalman temporal differences. J. Artif. Intell. Res., 39:483\u2013532, 2010. [67] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Christopher J. C. Burges, L\u00e9on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3003\u20133011, 2013.\n[68] Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A model-based and data-efficient approach to policy search. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning, 2011. [69] Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In Jack Mostow and Chuck Rich, editors, Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, 1998. [70] Gal Yarin, McAllister Rowan, and Rasmussen Carl Edward. Improving pilco with bayesian neural network dynamics models. In Data-efficient machine learning workshop, ICML, volume 4, page 25, 2016. [71] Zhihai Wang, Jie Wang, Qi Zhou, Bin Li, and Houqiang Li. Sample-efficient reinforcement learning via conservative model-based actor-critic. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 8612\u20138620. AAAI Press, 2022. [72] Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with gaussian processes. In Luc De Raedt and Stefan Wrobel, editors, Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), 2005. [73] Mohammad Ghavamzadeh and Yaakov Engel. Bayesian policy gradient algorithms. In Bernhard Sch\u00f6lkopf, John C. Platt, and Thomas Hofmann, editors, Advances in Neural Information Processing Systems 19, 2006. [74] Mohammad Ghavamzadeh, Yaakov Engel, and Michal Valko. Bayesian policy gradient and actor-critic algorithms. J. Mach. Learn. Res., 2016. [75] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline RL policies should be trained to be adaptive. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, 2022. [76] Hao Hu, Yiqin Yang, Jianing Ye, Ziqing Mai, Yujing Hu, Tangjie Lv, Changjie Fan, Qianchuan Zhao, and Chongjie Zhang. Bayesian offline-to-online reinforcement learning : A realist approach, 2024. [77] Yuhao Wang and Enlu Zhou. Bayesian risk-averse q-learning with streaming observations. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36, 2023. [78] Toru Hishinuma and Kei Senda. Importance-weighted variational inference model estimation for offline bayesian model-based reinforcement learning. IEEE Access, 2023. [79] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Claude Sammut and Achim G. Hoffmann, editors, Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), pages 267\u2013274. Morgan Kaufmann, 2002. [80] Vallender SS. Calculation of the wasserstein distance between probability distributions on the line. Theory of Probability & Its Applications, 18:784\u2013786, 1974. [81] Anqi Li, Dipendra Misra, Andrey Kolobov, and Ching-An Cheng. Survival instinct in offline reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, 2023. [82] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems 34, pages 15084\u201315097, 2021. [83] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018. [84] Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In International Conference on Machine Learning, pages 5556\u20135566. PMLR, 2020. [85] Zhihai Wang, Taoxing Pan, Qi Zhou, and Jie Wang. Efficient exploration in resource-restricted reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10279\u201310287, 2023.\n[86] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019. [87] Daniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257\u2013263, 2023. [88] Chris Gamble and Jim Gao. Safety-first ai for autonomous data centre cooling and industrial control. DeepMind, August, 17, 2018. [89] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys (CSUR), 55(1):1\u201336, 2021. [90] Zhihai Wang, Jie Wang, Dongsheng Zuo, Yunjie Ji, Xinli Xia, Yuzhe Ma, Jianye Hao, Mingxuan Yuan, Yongdong Zhang, and Feng Wu. A hierarchical adaptive multi-task reinforcement learning framework for multiplier circuit design. In Forty-first International Conference on Machine Learning. PMLR, 2024. [91] Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu. Learning cut selection for mixed-integer linear programming via hierarchical sequence model. In The Eleventh International Conference on Learning Representations, 2023. [92] Jie Wang, Zhihai Wang, Xijun Li, Yufei Kuang, Zhihao Shi, Fangzhou Zhu, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu. Learning to cut via hierarchical sequence/set model for efficient mixedinteger programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u201317, 2024. [93] Zhihai Wang, Zijie Geng, Zhaojie Tu, Jie Wang, Yuxi Qian, Zhexuan Xu, Ziyan Liu, Siyuan Xu, Zhentao Tang, Shixiong Kai, et al. Benchmarking end-to-end performance of ai-based chip placement algorithms. arXiv preprint arXiv:2407.15026, 2024.\n# Appendix / supplemental material\n# A Proofs\nWe provide a proof for the upper bound of value distributions and derivations of loss functions. See Table 4 for all notations.\n# A.1 Proof for Value Bound\n# To prove an upper bound of the value distribution, we introduce an assumption from [18] below. Assumption A.1. [18] Let \u03b6 = \ufffdN i=1 (2\u03b6i + log \u03b6\u2032 i) denote the cumulative corruption level, wher \u03b6i and \u03b6\u2032 i are defined as\nTo prove an upper bound of the value distribution, we introduce an assumption from [18] below. Assumption A.1. [18] Let \u03b6 = \ufffdN i=1 (2\u03b6i + log \u03b6\u2032 i) denote the cumulative corruption level, where \u03b6i and \u03b6\u2032 i are defined as\n\ufffd\ufffd\ufffdT V (si) \u2212\u02dcT V (si) \ufffd\ufffd\ufffd \u221e\u2264\u03b6i, max \ufffd\u03c0B (a | si) \u03c0b (a | si) , \u03c0b (a | si) \u03c0B (a | si) \ufffd \u2264\u03b6\u2032 i, \u2200a \u2208A.\n\ufffd \ufffd Here \u2225\u00b7 \u2225\u221emeans taking supremum over V : S \ufffd\u2192[0, rmax/(1 \u2212\u03b3)]\nNote that {\u03b6\u2032 i}i = 1N can quantify the corruption level of states and actions, and {\u03b6i}i = 1N can quantify the corruption level of rewards and next states (transition dynamics). Then, we provide the following assumption. Assumption A.2. There exists an M > 0 such that\nmax{d\u03c0E(s, a) pb(s, a) , d\u02dc\u03c0E(s, a) pB(s, a) , d\u03c0E(s) d\u02dc\u03c0E(s), d\u03c0IQL(s) d\u03c0E(s) , d\u02dc\u03c0IQL(s) d\u02dc\u03c0E(s) } \u2264M, \u2200(s, a) \u2208S \u00d7 A,\nwhere \u03c0E(a | s) \u221d\u03c0b(a | s) \u00b7 exp (\u03b2 \u00b7 [T Q\u2217\u2212V \u2217] (s, a)) is the policy of clean data, \u03c0IQL in Equation (17) is the policy following IQL\u2019s supervised policy learning scheme under clean data, \u02dc\u03c0IQL = arg min \u03c0 Es\u223cB [KL (\u02dc\u03c0E(\u00b7 | s), \u03c0(\u00b7 | s))]is the policy under corrupted data, and \u02dc\u03c0E(a | s) \u221d \u03c0B(a | s) \u00b7 exp \ufffd \u03b2 \u00b7 \ufffd \u02dcT Q\u2217\u2212V \u2217\ufffd (s, a) \ufffd is the policy of corrupted data.\nwhere \u03c0E(a | s) \u221d\u03c0b(a | s) \u00b7 exp (\u03b2 \u00b7 [T Q\u2217\u2212V \u2217] (s, a)) is the policy of clean data, \u03c0IQL  Equation (17) is the policy following IQL\u2019s supervised policy learning scheme under clean da \u02dc\u03c0IQL = arg min \u03c0 Es\u223cB [KL (\u02dc\u03c0E(\u00b7 | s), \u03c0(\u00b7 | s))]is the policy under corrupted data, and \u02dc\u03c0E(a | s)  \ufffd \ufffd \ufffd \ufffd\n\u03c0IQL = arg max \u03c0 E(s,a)\u223cb [exp (\u03b2 \u00b7 [T Q\u2217\u2212V \u2217] (s, a)) log \u03c0(a | s) = arg min \u03c0 Es\u223cb [DKL (\u03c0E(\u00b7 | s), \u03c0(\u00b7 | s))] .\nAs TRACER directly applies the weighted imitation learning technique from IQL to learn the policy, we can use \u02dc\u03c0IQL as the policy learned by TRACER under data corruptions, akin to RIQL Assumption A.2 requires that each pair, including the policy \u03c0E and the clean data b, the policy \u02dc\u03c0E and the corrupted dataset B, \u03c0E and \u02dc\u03c0E, \u03c0E and \u03c0IQL, and \u02dc\u03c0E and \u02dc\u03c0IQL, has good coverage. I is similar to the coverage condition in [18]. Based on Assumptions A.1 and A.2, we can derive the following theorem to show the robustness of our approach using the supervised policy learning scheme and learning the action-value distribution.\nLemma A.3. (Performance Difference) For any \u02dc\u03c0 and \u03c0, we have\nZ \u02dc\u03c0(s) \u2212Z\u03c0(s) = 1 1 \u2212\u03b3 E(s,a)\u223cd\u02dc\u03c0,\u02dc\u03c0 [D\u03c0(s, a, r) \u2212Z\u03c0(s)] .\n(17) (18)\n(17)\n(18)\nZ \u02dc\u03c0(s) = \u221e \ufffd t=0 \u03b3tE(St,At)\u223cP,\u02dc\u03c0 [R(St, At)|S0 = s] = \u221e \ufffd t=0 \u03b3tE(St,At)\u223cP,\u02dc\u03c0 [R(St, At) + Z\u03c0(St) \u2212Z\u03c0(St)|S0 = s] = \u221e \ufffd t=0 \u03b3tE(St,At,St+1)\u223cP,\u02dc\u03c0 [R(St, At) + \u03b3Z\u03c0(St+1) \u2212Z\u03c0(St)|S0 = s] + Z\u03c0(s) = Z\u03c0(s) + \u221e \ufffd t=0 \u03b3tE(St,At)\u223cP,\u02dc\u03c0 [D\u03c0(St, At, Rt) \u2212Z\u03c0(St)|S0 = s] = Z\u03c0(s) + 1 1 \u2212\u03b3 E(s,a)\u223cd\u02dc\u03c0,\u02dc\u03c0 [D\u03c0(s, a, r) \u2212Z\u03c0(s)] .\nwhere q\u03c0 is the value distribution, W1(\u00b7, \u00b7) is the Wasserstein distance [80] for measuring the distribution difference, \u03f51 = Es\u223cb [DKL (\u03c0E(\u00b7|s) \u2225\u03c0IQL(\u00b7|s))], and \u03f52 = Es\u223cB [DKL (\u02dc\u03c0E(\u00b7|s) \u2225\u02dc\u03c0IQL(\u00b7|s))].\n\ufffd \u2208 \ufffd \ufffd  || \u2212|| \ufffd The value distribution q\u03c0, also denoted by p(\u00b7|s) in Section 3.2 of the main text, is an expactation of action value distribution Ea\u223c\u03c0,r\u223c\u03c1[p\u03b8(\u00b7|s, a, r)]. Note that as TRACER directly applies the weighted imitation learning technique from IQL to learn the policy (see Section 3.2), we can use \u02dc\u03c0IQL as the policy learned by TRACER under data corruptions, akin to RIQL. Let q\u02dc\u03c0IQL and q\u03c0IQL be the value distributions of learned policies following IQL\u2019s supervised policy learning scheme under corrupted and clean data, respectively. Z \u02dc\u03c0IQL \u223cq\u02dc\u03c0IQL and Z\u03c0IQL \u223cq\u03c0IQL. Let p\u03c0E be the value distribution of the policy of clean",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of data corruptions in offline reinforcement learning (RL), highlighting the limitations of existing methods that struggle to learn robust agents under high uncertainty caused by diverse corrupted data. Previous approaches often assume clean datasets, leading to performance degradation when faced with corrupted data. A breakthrough is necessary to improve robustness in real-world applications.",
        "problem": {
            "definition": "The problem this paper aims to solve is the inability of existing offline RL methods to effectively learn robust policies when the training data is subject to various types of corruptions, affecting states, actions, rewards, and dynamics.",
            "key obstacle": "The main challenge is that current methods do not adequately account for the uncertainty introduced by these corruptions, which can significantly degrade performance in clean environments."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to leverage all available offline data to capture the uncertainty introduced by data corruptions, enhancing the learning process in offline RL.",
            "opinion": "The proposed method, TRACER, introduces variational Bayesian inference into offline RL to model the uncertainty associated with corrupted data, allowing for better policy learning.",
            "innovation": "TRACER is innovative as it uses Bayesian inference for the first time in corruption-robust offline RL, capturing uncertainties in the action-value function and regulating the influence of corrupted data through an entropy-based uncertainty measure."
        },
        "method": {
            "method name": "TRACER",
            "method abbreviation": "TRACER",
            "method definition": "TRACER is a robust variational Bayesian inference method for offline reinforcement learning that captures uncertainties caused by diverse data corruptions.",
            "method description": "The core of TRACER involves modeling data corruptions as uncertainty in the action-value function and using Bayesian inference to approximate the posterior distribution of this function.",
            "method steps": [
                "Model all corruptions as uncertainty in the action-value function.",
                "Use offline data as observations to approximate the posterior distribution.",
                "Employ an entropy-based uncertainty measure to distinguish between corrupted and clean data.",
                "Regulate the loss associated with corrupted data to enhance robustness."
            ],
            "principle": "The effectiveness of TRACER lies in its ability to capture and manage the uncertainty introduced by data corruptions, allowing for more reliable policy learning in offline settings."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the D4RL benchmark on MuJoCo and CARLA tasks, comparing TRACER with several state-of-the-art offline RL algorithms under various corruption scenarios.",
            "evaluation method": "Performance was measured by evaluating the learned policies in clean environments after training on corrupted datasets, with metrics including average scores and standard deviations across multiple runs."
        },
        "conclusion": "The experiments demonstrate that TRACER significantly outperforms existing methods in both individual and simultaneous data corruption scenarios, validating the approach's robustness and effectiveness in offline reinforcement learning.",
        "discussion": {
            "advantage": "TRACER effectively captures uncertainties from corrupted data, improving robustness and performance in clean environments compared to traditional offline RL methods.",
            "limitation": "While TRACER shows significant improvements, it may still face challenges in more complex and realistic corruption scenarios that were not fully addressed in this work.",
            "future work": "Future research could explore more sophisticated data corruption models and enhance the framework to handle a wider range of real-world applications, including safety-critical environments."
        },
        "other info": {
            "support": "This work was supported by the National Key R&D Program of China and various National Nature Science Foundations of China grants."
        }
    },
    "mount_outline": [
        {
            "section number": "3.1",
            "key information": "The problem this paper aims to solve is the inability of existing offline RL methods to effectively learn robust policies when the training data is subject to various types of corruptions, affecting states, actions, rewards, and dynamics."
        },
        {
            "section number": "3.2",
            "key information": "Previous approaches often assume clean datasets, leading to performance degradation when faced with corrupted data."
        },
        {
            "section number": "4.1",
            "key information": "The proposed method, TRACER, introduces variational Bayesian inference into offline RL to model the uncertainty associated with corrupted data, allowing for better policy learning."
        },
        {
            "section number": "5.3",
            "key information": "TRACER is innovative as it uses Bayesian inference for the first time in corruption-robust offline RL, capturing uncertainties in the action-value function and regulating the influence of corrupted data through an entropy-based uncertainty measure."
        },
        {
            "section number": "6.1",
            "key information": "TRACER is a robust variational Bayesian inference method for offline reinforcement learning that captures uncertainties caused by diverse data corruptions."
        },
        {
            "section number": "8.1",
            "key information": "Future research could explore more sophisticated data corruption models and enhance the framework to handle a wider range of real-world applications, including safety-critical environments."
        }
    ],
    "similarity_score": 0.5276508516662095,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions.json"
}