{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.18126",
    "title": "Curriculum Demonstration Selection for In-Context Learning",
    "abstract": "Large Language Models (LLMs) have shown strong in-context learning (ICL) abilities with a few demonstrations. However, one critical challenge is how to select demonstrations to elicit the full potential of LLMs. In this paper, we propose Curriculum Demonstration Selection (CDS), a novel demonstration selection method for ICL. Instead of merely using similarity, CDS additionally partitions samples by their complexity measurements. Following curriculum learning, CDS then selects demonstrations from easy to difficult. Thus the selected demonstrations cover a wide range of difficulty levels, enabling LLMs to learn from varied complexities within the training set. Experiments demonstrate that our CDS consistently outperforms baseline methods, achieving notable improvements across nine LLMs on three benchmarks. Moreover, CDS proves especially effective in enhancing LLM performance in solving challenging problems.",
    "bib_name": "vu2024curriculumdemonstrationselectionincontext",
    "md_text": "# Curriculum Demonstration Selection for In-Context Learning\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/013b/013bc3d6-1b51-4b77-a835-6577be786f1a.png\" style=\"width: 50%;\"></div>\n27 Nov 2024\n]  27 Nov 20\n# Abstract\nAbstract\nLarge Language Models (LLMs) have shown strong in-context learning (ICL) abilities with a few demonstrations. However, one critical challenge is how to select demonstrations to elicit the full potential of LLMs. In this paper, we propose Curriculum Demonstration Selection (CDS), a novel demonstration selection method for ICL. Instead of merely using similarity, CDS additionally partitions samples by their complexity measurements. Following curriculum learning, CDS then selects demonstrations from easy to difficult. Thus the selected demonstrations cover a wide range of difficulty levels, enabling LLMs to learn from varied complexities within the training set. Experiments demonstrate that our CDS consistently outperforms baseline methods, achieving notable improvements across nine LLMs on three benchmarks. Moreover, CDS proves especially effective in enhancing LLM performance in solving challenging problems.\narXiv:2411.1812\narXiv:2\n# Keywords\nACM proceedings, LATEX, text tagging\nACM Reference Format: Duc Anh Vu, Nguyen Tran Cong Duy, Xiaobao Wu, Hoang Minh Nhat, Du Mingzhe, Nguyen Thanh Thong, and Anh Tuan Luu. 2025. Curriculum Demonstration Selection for In-Context Learning. In Proceedings of ACM \u2217Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SAC\u201925, March 31 \u2013April 4, 2025, Sicily, Italy \u00a9 2025 ACM. ACM ISBN 979-8-4007-0629-5/25/03 https://doi.org/xx.xxx/xxx_x\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SAC\u201925, March 31 \u2013April 4, 2025, Sicily, Italy \u00a9 2025 ACM. ACM ISBN 979-8-4007-0629-5/25/03 https://doi.org/xx.xxx/xxx_x\nSAC Conference (SAC\u201925). ACM, New York, NY, USA, Article 4, 8 pages. https://doi.org/xx.xxx/xxx_x\n# 1 Introduction\nLarge language models (LLMs) have made significant strides in natural language processing (NLP) [34, 44, 49], excelling in tasks such as natural language understanding [3, 14, 16, 47], text generation [30, 31, 35, 51] and reasoning [20, 32, 33, 36]. Despite these advances, the full potential of LLMs remains limited by how they generalize across tasks of varying complexity. As LLMs tackle increasingly diverse and complex real-world challenges, optimizing their ability to handle varying task difficulties is crucial. In-Context Learning (ICL) has emerged as a powerful paradigm in which LLMs perform tasks by leveraging examples embedded within the input prompt, without the need for parameter updates. The success of ICL, however, is highly sensitive to the choice of demonstrations included in the prompt. Studies have shown that selecting the right set of demonstrations can significantly influence model performance [6, 25]. Despite this, existing methods for demonstration selection often rely on random selection or heuristic approaches that may lead to inconsistent and suboptimal outcomes, particularly in more complex tasks. Although various approaches have been proposed to improve demonstration selection, including unsupervised [25, 29, 41, 48] and supervised methods [28, 38, 45, 52], these techniques often fail to account for the varying difficulty of the demonstrations. As a result, LLMs are limited in their ability to generalize effectively across tasks that span different levels of complexity. This shortcoming becomes especially problematic when models must solve a wide range of problems, from simple to highly intricate. To address this gap, we draw inspiration from curriculum learning, where learners gradually progress from simpler to more challenging tasks. We propose Curriculum Demonstration Selection (CDS), a method that systematically selects demonstrations based on their complexity to ensure a balanced representation. It first\nsplits the dataset into distinct difficulty levels, then retrieves demonstrations from each group. By adopting this curriculum-based approach, CDS enables LLMs to incrementally refine their understanding, which improves their performance on both simple and complex tasks. Our experiments demonstrate that CDS consistently outperforms baseline methods, particularly on more difficult problems where traditional selection techniques often fall short. The contributions of this paper are threefold: (i) We introduce Curriculum Demonstration Selection (CDS), a novel method for optimizing demonstration selection in ICL, leveraging curriculum learning to enhance LLM performance. (ii) We empirically demonstrate the effectiveness of CDS across multiple benchmarks, showing significant improvements over existing methods. (iii) We provide insights into the behavior of LLMs when exposed to demonstrations of varying complexity, highlighting the potential of CDS to boost challenging problem-solving capabilities.\n# 2 Related Work\n# 2.1 Demonstration Selection\nIn-context learning (ICL) has garnered increasing attention due to its effectiveness in leveraging demonstrations for LLMs without necessitating parameter updates. A critical challenge in ICL lies in selecting optimal demonstrations, as research has shown that they can significantly influence performance [6]. Several strategies have been explored for demonstration selection. Early research use random selection [3, 22] or human-crafted examples [19, 46]. While straightforward to implement, these methods often result in inconsistent performance, as some demonstrations may not provide effective guidance to the model. To mitigate this issue, Liu et al. [25] proposed a retrieval-based method where examples are selected based on their semantic similarity to the query. Another variant [45] uses a latent variable model to explain the effectiveness of certain demonstrations, showing that semantically similar examples can enhance generalization across tasks. Beyond semantic similarity, other strategies have emerged. Complexity-based selection [10] emphasizes selecting demonstrations that involve more reasoning steps or higher complexity. Furthermore, Zhao et al. [53] presents kNN-ICL, a method that integrates ICL with a nearest-neighbor search to simplify prompt engineering and enhance compositional task-oriented parsing. In addition, there are more advanced selection processes based on metrics such as informativeness [24], perplexity [11] and concept learning [45].\n# 2.2 Curriculum Learning\nThe concept of curriculum learning [2] has inspired extensive research across numerous NLP tasks including ICL. Drozdov et al. [7] prioritized harder demonstrations, estimated by Demonstration Query Likelihood (DQL). The most challenging samples, represented by low-probability queries, are selected assuming they contribute to larger updates in the learning process, emulating how gradient updates work during training. Liu et al. [27] introduced In-Context Curriculum Learning (ICCL), which advocates ordering already selected demonstrations from simple to complex. Additionally, other approaches to demonstration selection in ICL have emerged that align with curriculum learning principles. For\nexample, Sorensen et al. [41] proposed an information-theoretic framework designed to select demonstrations that maximize information gain, a strategy that can be interpreted as a form of curriculum learning where examples are chosen for their informativeness rather than difficulty. Despite these advancements, most existing methods focus primarily on selecting demonstrations similar to the test instance, according to predefined metrics, and overlook the importance of incorporating a set of demonstrations with varying complexity levels. This lack of diversity may hinder the model\u2019s ability to generalize effectively across a broader spectrum of complexities.\n# 3 Methodology\nWe introduce Curriculum Demonstration Selection (CDS), a novel strategy for selecting examples that draws inspiration from curriculum learning. CDS ensures that selected demonstrations cover a wide spectrum of difficulty levels, enabling LLMs to progressively learn from varied complexities within the training set.\n# 3.1 Problem Formulation\nGiven an LLM \ud835\udf03and a set of \ud835\udc5btraining pairs {\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56}\ud835\udc5b \ud835\udc56=1, our objective is to select \ud835\udc58demonstrations from this set to guide the model \ud835\udf03in solving a specific task \ud835\udc46. We hypothesize that demonstrations with diverse complexity levels will enhance the model\u2019s performance. Therefore, CDS aims to acquire demonstrations from different difficulty levels.\n# 3.2 Curriculum Construction\nTo construct the curriculum, we rely on human-annotated complexity measures. Complexity is assessed based on factors such as grade level, with higher grade levels indicating more challenging examples. In cases where examples share the same complexity level, we further differentiate their difficulty using human performance metrics, such as task completion rates (e.g., acceptance rates in coding tasks). The dataset is divided into \ud835\udc58distinct difficulty partitions, where \ud835\udc58is determined by the distribution of complexity scores in the dataset. This partitioning enables the construction of a curriculum that spans a broad spectrum of example complexities, ensuring that the LLM is exposed to both simpler and more complex tasks.\n# 3.3 Demonstration Retrieval\nOnce the dataset is partitioned into \ud835\udc58difficulty levels, the CDS algorithm, outlined in Algorithm 1, selects one demonstration from each partition. This process ensures that the selected demonstrations are representative of different complexities, allowing the LLM to leverage a balanced set of examples. The retrieval process can either be similarity-based or random. For similarity-based retrieval, we use CLS embeddings from a pre-trained Transformers model to represent the sentences, following KATE [25]. The algorithm then selects demonstrations that are most similar to the test question from each difficulty level by calculating the negative Euclidean distance. We then shuffle the demonstration order to prevent overfitting to specific patterns and ensure unbiasedness.\nMATH\nARC-c\nAlgebra\nGeometry\nPrecalculus\nNumber Theory\nProbability\nAvg.\nLlama 2 7B\n4.58\u00b10.33\n1.11\u00b10.1\n0.92\u00b10.15\n3.27\u00b10.23\n2.18\u00b10.26\n3.48\u00b10.17\n57.37\u00b10.43\n+ KATE\n4.93\u00b10.27\n2.02\u00b10.69\n2.26\u00b10.31\n4.26\u00b10.36\n2.88\u00b10.4\n4.09\u00b10.08\n57.54\u00b10.43\n+ CDS (ours)\n5.66\u00b10.29\n2.16\u00b10.26\n2.08\u00b10.09\n4.57\u00b10.68\n3.59\u00b10.17\n4.62\u00b10.24\n57.99\u00b10.26\nLlama 2 13B\n6.7\u00b10.06\n2.92\u00b10.59\n1.04\u00b10.09\n3.46\u00b10.78\n3.09\u00b10.2\n5.03\u00b10.09\n65.02\u00b10.12\n+ KATE\n7.69\u00b10.19\n2.99\u00b10.86\n2.08\u00b10.38\n4.57\u00b10.23\n4.64\u00b10.17\n6.0\u00b10.19\n66.24\u00b10.32\n+ CDS (ours)\n7.94\u00b10.1\n3.2\u00b10.1\n2.32\u00b10.17\n5.0\u00b10.4\n4.92\u00b10.5\n6.27\u00b10.06\n66.47\u00b10.64\nLlama 3 8B\n25.91\u00b10.25\n12.18\u00b11.45\n8.18\u00b10.53\n17.47\u00b10.86\n16.60\u00b10.95\n20.87\u00b10.38\n80.29\u00b10.28\n+ KATE\n27.14\u00b10.18\n13.15\u00b10.59\n9.65\u00b10.23\n16.36\u00b10.09\n20.39\u00b10.78\n22.09\u00b10.11\n81.11\u00b10.2\n+ CDS (ours)\n27.09\u00b10.52\n15.31\u00b10.71\n10.13\u00b10.71\n17.96\u00b10.76\n21.31\u00b10.17\n22.57\u00b10.28\n81.0\u00b10.15\nMistral 7B\n12.71\u00b10.26\n5.15\u00b11.0\n4.7\u00b10.17\n5.74\u00b10.76\n7.88\u00b10.43\n9.9\u00b10.03\n75.37\u00b10.35\n+ KATE\n14.9\u00b10.3\n6.61\u00b10.35\n6.41\u00b10.3\n6.23\u00b10.71\n7.74\u00b10.26\n11.57\u00b10.12\n76.59\u00b10.15\n+ CDS (ours)\n14.89\u00b10.03\n6.26\u00b10.3\n6.47\u00b10.09\n6.42\u00b10.87\n8.65\u00b10.52\n11.64\u00b10.02\n76.71\u00b10.25\nQwen 7B\n13.75\u00b10.14\n5.43\u00b11.02\n5.56\u00b11.27\n6.54\u00b10.71\n8.79\u00b10.1\n10.81\u00b10.21\n48.66\u00b10.79\n+ KATE\n15.25\u00b10.25\n6.96\u00b10.43\n6.41\u00b10.79\n7.9\u00b10.75\n9.14\u00b10.53\n12.12\u00b10.25\n44.62\u00b10.24\n+ CDS (ours)\n15.54\u00b10.26\n7.24\u00b10.94\n5.92\u00b10.46\n8.64\u00b11.06\n9.7\u00b10.6\n12.39\u00b10.04\n49.74\u00b10.48\nTable 1: Result on MATH and ARC-c dataset. Numbers in bold indicate the best scores among all methods.\n\u00b1 \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 Table 1: Result on MATH and ARC-c dataset. Numbers in bold indicate the best scores among all methods.\n\u00b1 \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 Table 1: Result on MATH and ARC-c dataset. Numbers in bold indicate the best scores among all methods.\n\u00b1 \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 ult on MATH and ARC-c dataset. Numbers in bold indicate the best scores among all methods.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9ca/b9caef9c-063a-47c9-b4ff-9e2d0aeb6ae8.png\" style=\"width: 50%;\"></div>\nAlgorithm 1 Curriculum Demonstration Selection (CDS)\n1: Input: LLM\ud835\udf03, Training set\ud835\udc47, Testing set \ud835\udc38, Difficulty Measurer\n\ud835\udc37, Retrieval function \ud835\udc45, number of demonstrations \ud835\udc58\n2: Output: Selected demonstrations for each test instance\n3: {\ud835\udc47\ud835\udc56}\ud835\udc58\n\ud835\udc56=1 = \ud835\udc37(\ud835\udc47) \u22b2Partition the training set \ud835\udc47into \ud835\udc58subsets by\ndifficulty using \ud835\udc37\n4: for each \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61instance in \ud835\udc38do\n5:\n\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b_\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59\u2190empty list\n6:\nfor \ud835\udc56= 1 to \ud835\udc58do\n7:\n\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b_\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59.add(\ud835\udc45(\ud835\udc47\ud835\udc56))\n8:\nend for\n9:\n\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b_\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59.shuffle()\n10:\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b= \ud835\udf03(\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b_\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59)\n11: end for\n# 4 Experiments\n# 4 Experiments 4.1 Experimental Setup\n# 4.1 Experimental Setup\n\u2022 Math reasoning. Mathematical reasoning plays a crucial role in evaluating LLMs\u2019 performance. Numerous studies have focused on leveraging LLMs on math-related tasks such as mathematical question generation [21, 35] and problem solving [4, 12, 54]. In this study, we utilize the MATH dataset [15], a highly challenging benchmark consisting of 7,500 training examples and 5,000 testing examples, spanning topics such as prealgebra, algebra, number theory, counting and probability, geometry, intermediate algebra, and precalculus. The MATH dataset, sourced from competitions such as the AMC 10, AMC 12, and AIME, categorizes problems into five distinct complexity levels. We leverage this metadata to construct the curriculum for our CDS method, ensuring that demonstrations reflect a range of difficulty levels.\n\u2022 Commonsense reasoning. Commonsense reasoning, essential for natural language understanding, tests a model\u2019s ability to infer everyday scenarios by integrating observational and prior knowledge [39, 43]. To evaluate LLM performance in this area, we use the ARC-Challenge dataset [5], which contains 2,590 multiplechoice science questions targeting students from grade 3 to grade 9. For CDS, we rank the difficulty by the grade level data, ensuring a balanced representation across complexity levels. \u2022 Code generation. These tasks have become pivotal in assessing LLMs\u2019 capabilities, covering areas from code correction [40, 50] to code summarization [42] and efficiency optimization [8]. For this task, we employ the Mercury dataset, a specialized benchmark designed for evaluating code generation correctness and efficiency. Mercury consists of 256 evaluation tasks sourced from public programming problems on Leetcode1, categorized into Easy, Medium, and Hard difficulty levels. To further refine difficulty classification, we use acceptance rates as an additional metric, with lower acceptance rates indicating higher complexity within the same difficulty category. 4.1.2 Models. For the math and commonsense reasoning tasks, we test five widely used open-source LLMs: Llama-2 (7B and 13B) [44], Llama-3 (8B) [9], Mistral-7B [17], and Qwen-7B [1]. For the code generation task, we follow the experimental settings of Mercury [8], evaluating four code-specialized LLMs: CodeLlama (7B and 13B) [37], StarCoder (3B) [23], and DeepSeek-Coder (6.7B) [13], with model sizes ranging from 3B to 13B parameters. 4.1.3 Implementation Details. We employ greedy decoding across all models for math and commonsense reasoning tasks. Prompts are constructed using the Chain-of-Thought (CoT) technique [46],\n\u2022 Commonsense reasoning. Commonsense reasoning, essential for natural language understanding, tests a model\u2019s ability to infer everyday scenarios by integrating observational and prior knowledge [39, 43]. To evaluate LLM performance in this area, we use the ARC-Challenge dataset [5], which contains 2,590 multiplechoice science questions targeting students from grade 3 to grade 9. For CDS, we rank the difficulty by the grade level data, ensuring a balanced representation across complexity levels.\n\u2022 Code generation. These tasks have become pivotal in assessing LLMs\u2019 capabilities, covering areas from code correction [40, 50] to code summarization [42] and efficiency optimization [8]. For this task, we employ the Mercury dataset, a specialized benchmark designed for evaluating code generation correctness and efficiency. Mercury consists of 256 evaluation tasks sourced from public programming problems on Leetcode1, categorized into Easy, Medium, and Hard difficulty levels. To further refine difficulty classification, we use acceptance rates as an additional metric, with lower acceptance rates indicating higher complexity within the same difficulty category.\n4.1.2 Models. For the math and commonsense reasoning tasks, we test five widely used open-source LLMs: Llama-2 (7B and 13B) [44], Llama-3 (8B) [9], Mistral-7B [17], and Qwen-7B [1]. For the code generation task, we follow the experimental settings of Mercury [8], evaluating four code-specialized LLMs: CodeLlama (7B and 13B) [37], StarCoder (3B) [23], and DeepSeek-Coder (6.7B) [13], with model sizes ranging from 3B to 13B parameters.\n4.1.3 Implementation Details. We employ greedy decoding across all models for math and commonsense reasoning tasks. Prompts are constructed using the Chain-of-Thought (CoT) technique [46], which encourages step-by-step reasoning before arriving at the final answer. The final prediction is extracted from the last line of the\n1https://leetcode.com/problemset/algorithms/\nmodel\u2019s output. In the Mercury dataset, we adhere to the settings of Du et al. [8], with a decoding temperature of 0.2. All experiments use \ud835\udc58= 5 demonstrations, and each model is evaluated using three different random seeds on the test sets. Example prompts in our experiments are provided in Appendix A.\n4.1.4 Baselines. We compare CDS with two baseline methods: \u2022 Uniform: We uniformly select \ud835\udc58demonstrations from training set \ud835\udc47for each test example. \u2022 Similarity (KATE [25]): This method utilizes RoBERTa-Large [26] to obtain CLS embeddings, followed by cosine similarity computation between candidate demonstrations and the test examples. The \ud835\udc58most similar demonstrations are then selected based on negative Euclidean distance.\n4.1.5 Evaluation metrics. For the math and commonsense reasoning tasks, accuracy is used as the evaluation metric. A prediction is considered correct if it exactly matches the ground truth, either as a value (MATH) or a selected option (ARC-Challenge). For the code generation task, we use two key metrics from the Mercury dataset. The first is Functional Correctness (Pass), which evaluates whether the generated code successfully passes all test cases associated with a given task. The Pass score represents the percentage of tasks for which the code is fully functional. The second metric, Beyond, evaluates not only the correctness of the generated code but also its efficiency. This metric ranks the runtime performance of the model-generated code against historical solutions, providing a percentile score that reflects how efficiently the code executes.\n# 4.2 Results\n4.2.1 LLM Reasoning. We compared five widely-used open-source LLMs on mathematical and commonsense reasoning tasks, where the results clearly demonstrate the superiority of CDS over other methods, as seen in Table 1. In the mathematical reasoning benchmark, CDS consistently outperforms both random selection and the KATE baseline. Across all mathematical topics in the MATH dataset, CDS delivers stronger performance, with algebra and number theory benefiting the most, while geometry and precalculus show moderate improvements. Notably, probability problem solve rates increase substantially with CDS. Larger models, particularly Llama 2 13B, experience the greatest performance gains, underscoring the scalability of CDS in enhancing model capabilities, in line with the Scaling Laws proposed by Kaplan et al. [18]. Similarly, on the ARC-c benchmark, CDS outperforms both random selection and KATE. While KATE provides some improvement over random selection, its performance is inconsistent across tasks. For example, KATE underperforms the random baseline by approximately 9% with Qwen 7B. In contrast, CDS exhibits greater stability, consistently yielding superior results across the majority of cases. This highlights CDS as an effective and reliable method for in-context learning in LLMs.\n4.2.2 Code Generation. CDS also demonstrates significant improvements in code generation tasks, as evaluated on the Mercury dataset in Table 2. Specifically, CDS significantly outperforms the random baseline in both the Pass and Beyond metrics for all LLMs. Furthermore, CDS consistently provides improvement over the KATE baseline on the Pass metric across all models and shows\nModel\nPass\nBeyond\nCodeLlama-7b\n28.12\u00b10.68\n21.75\u00b10.65\n+ KATE\n36.85\u00b10.9\n28.34\u00b11.8\n+ CDS (ours)\n36.98\u00b11.93\n28.1\u00b12.0\nCodeLlama-13b\n33.98\u00b11.03\n26.86\u00b10.9\n+ KATE\n39.45\u00b10.01\n32.24\u00b10.27\n+ CDS (ours)\n41.54\u00b10.45\n33.18\u00b10.76\ndeepseek-coder-6.7b\n43.36\u00b10.68\n33.95\u00b10.52\n+ KATE\n64.19\u00b10.9\n52.24\u00b10.11\n+ CDS (ours)\n64.71\u00b10.9\n51.51\u00b11.38\nstarcoder2-3b\n50.26\u00b11.13\n39.51\u00b10.67\n+ KATE\n57.42\u00b11.35\n46.58\u00b13.28\n+ CDS (ours)\n58.98\u00b10.01\n47.15\u00b11.06\n\u00b1 \u00b1 Table 2: Results on Mercury. Numbers in bold indicate the best scores among all methods.\n\u00b1 \u00b1 Table 2: Results on Mercury. Numbers in bold indicate the best scores among all methods.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31d9/31d9a287-7989-4863-89a5-d9d09a475818.png\" style=\"width: 50%;\"></div>\nFigure 1: Comparison of CDS with random demonstration retrieval and with similarity retrieval on MATH benchmark across five LLMs.\ncomparable results on the Beyond metric. These results highlight the effectiveness of CDS on the code generation task.\n# comparable results on the Beyond metric. These results highlight the effectiveness of CDS on the code generation task.\n4.2.3 Random retrieval v.s. Similarity retrieval. We examined the effects of different retrieval functions, namely random retrieval and similarity retrieval, on CDS performance. As shown in Figure 1, both retrieval methods outperform the random selection baseline on the MATH dataset, with similarity-based retrieval consistently outperforming random retrieval. This result suggests that similaritybased retrieval enhances the effectiveness of CDS, providing better demonstration selection.\n4.2.4 Solve rates on harder problems. Both the MATH and ARC-c datasets show a clear positive trend, with harder problems benefiting the most from CDS, as illustrated in Figure 2. On the MATH dataset, the performance improvement grows from 2% on easier tasks to 6% on harder ones. The ARC-c shows a similar, albeit more modest, trend, with improvements ranging from negative values on easy tasks to approximately 1% on medium and hard tasks. This\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0e14/0e141981-6790-4d8c-aeb0-e053b382b156.png\" style=\"width: 50%;\"></div>\nFigure 2: Average improvement across five models in three difficulty levels.\n<div style=\"text-align: center;\">Figure 2: Average improvement across five models in three difficulty levels.</div>\nhighlights the remarkable efficacy of CDS in addressing more complex problems, likely due to its strategy of selecting demonstrations across a wide range of difficulty levels, helping LLMs adapt better to challenging tasks. 4.2.5 Effect of demonstrations\u2019 order. As demonstrated in Table 3, ordering the demonstrations from easy to hard (E2H) has no significant effect on performance compared to random shuffling, indicating that the order in which demonstrations are presented does not impact the overall effectiveness of CDS. This finding aligns with previous research by Wang et al. [45], further supporting the robustness of CDS in enhancing LLM performance, regardless of demonstration order.\nhighlights the remarkable efficacy of CDS in addressing more complex problems, likely due to its strategy of selecting demonstrations across a wide range of difficulty levels, helping LLMs adapt better to challenging tasks.\n4.2.5 Effect of demonstrations\u2019 order. As demonstrated in Table 3, ordering the demonstrations from easy to hard (E2H) has no significant effect on performance compared to random shuffling, indicating that the order in which demonstrations are presented does not impact the overall effectiveness of CDS. This finding aligns with previous research by Wang et al. [45], further supporting the robustness of CDS in enhancing LLM performance, regardless of demonstration order.\nAlg.\nGeo.\nCal.\nNum. Theory\nProb.\nAvg.\nE2H\n5.47\n2.51\n2.20\n4.63\n3.37\n4.54\nRand.\n5.66\u00b10.29\n2.16\u00b10.26\n2.08\u00b10.09\n4.57\u00b10.68\n3.59\u00b10.17\n4.62\u00b10.24\nTable 3: Accuracy of our method with (E2H) and without (Rand.) reordering. Numbers are obtained with the Llama-2 7B model on MATH dataset.\n# 5 Conclusions\nIn this paper, we introduced Curriculum Demonstration Selection (CDS), a novel approach aimed to enhance the performance of LLMs in ICL. By leveraging the principles of curriculum learning, CDS effectively organizes demonstrations according to their complexity, allowing LLMs to learn from simpler to more complex tasks. Our extensive experiments across three benchmarks \u2014 mathematical reasoning, commonsense reasoning, and code generation \u2014 consistently demonstrated that CDS outperforms traditional methods, including both random selection and similarity-based approaches like KATE. Additionally, CDS shows significant potential in solving more complex and challenging problems, highlighting its robustness in optimizing demonstration selection for ICL. Overall,\nCDS presents a significant step forward in optimizing demonstration selection for ICL, providing a more structured and effective method that enhances both the accuracy and efficiency of LLMs. This work opens new avenues for improving LLM in numerous problem-solving tasks.\n# 6 Limitations\nOur work does have some limitations. First, we employed a fixed five-shot setting in all experiments, providing the model with five demonstrations per task. While this is a common approach in incontext learning, it may not fully capture the potential of CDS when applied to different shot settings. The decision to use five demonstrations balanced computational costs and performance consistency across benchmarks. Future research could explore how varying the number of demonstrations influences CDS effectiveness, particularly for more complex tasks that may require additional context for optimal performance. Second, the curriculum structure in CDS relies on predefined metadata, such as grade levels in math reasoning, to determine task difficulty. While this allows for efficient curriculum construction, it assumes that such metadata is available and accurately reflects task complexity. In some datasets or domains, difficulty rankings may not be available or may not accurately represent the challenges posed by the tasks. In these cases, CDS would require alternative methods for estimating task complexity, such as model-based estimates or task-specific heuristics, to maintain its effectiveness. Lastly, while our evaluation focused on three benchmarks, namely mathematical reasoning, commonsense reasoning, and code generation, the generalizability of CDS to other types of tasks remains an open question. Future work could expand the scope of CDS evaluation to include a broader range of benchmarks, helping to identify its strengths and limitations across various domains and task types. This will provide deeper insights into the applicability of CDS and its potential to further improve LLM performance in diverse contexts.\n# References\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). [2] Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In International Conference on Machine Learning. https: //api.semanticscholar.org/CorpusID:873046 [3] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [4] Zui Chen, Yezeng Chen, Jiaqi Han, Zhijie Huang, Ji Qi, and Yi Zhou. 2024. An Empirical Study of Data Ability Boundary in LLMs\u2019 Math Reasoning. arXiv preprint arXiv:2403.00799 (2024). [5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018). [6] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022). [7] Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew McCallum, Donald Metzler, and Kai Hui. 2023. PaRaDe: Passage Ranking using Demonstrations with LLMs. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 14242\u201314252. https://doi.org/10.18653/v1/2023.findingsemnlp.950 [8] Mingzhe Du, Anh Tuan Luu, Bin Ji, and See-Kiong Ng. 2024. Mercury: An efficiency benchmark for llm code synthesis. arXiv preprint arXiv:2402.07844 (2024).\n[9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [10] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations. [11] Hila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and Luke Zettlemoyer. 2023. Demystifying Prompts in Language Models via Perplexity Estimation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10136\u201310148. https://doi.org/10.18653/v1/2023.findings-emnlp.679 [12] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452 (2023). [13] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming\u2013The Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024). [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021). [16] Nhat Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, and Anh Tuan Luu. 2024. ToXCL: A Unified Framework for Toxic Speech Detection and Explanation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 6460\u20136472. https://doi.org/10. 18653/v1/2024.naacl-long.359 [17] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. ArXiv abs/2001.08361 (2020). https://api. semanticscholar.org/CorpusID:210861095 [19] Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. 2023. LAMBADA: Backward Chaining for Automated Reasoning in Natural Language. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 6547\u20136568. https://doi.org/10.18653/v1/2023.acl-long.361 [20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199\u201322213. [21] Ghader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and Salam Al-Emari. 2019. A Systematic Review of Automatic Question Generation for Educational Purposes. International Journal of Artificial Intelligence in Education 30 (2019), 121\u2013204. https://api.semanticscholar.org/CorpusID:208212657 [22] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems 35 (2022), 3843\u20133857. [23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023). [24] Xiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. arXiv preprint arXiv:2302.13539 (2023). [25] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, Eneko Agirre, Marianna Apidianaki, and Ivan Vuli\u0107 (Eds.). Association for Computational Linguistics, Dublin, Ireland and Online, 100\u2013114. https://doi.org/10.18653/v1/2022.deelio1.10 [26] Yinhan Liu. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [27] Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. 2024. Let\u2019s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning. arXiv preprint arXiv:2402.10738 (2024). [28] Thong Nguyen, Yi Bin, Xiaobao Wu, Xinshuai Dong, Zhiyuan Hu, Khoi Le, CongDuy Nguyen, See-Kiong Ng, and Luu Anh Tuan. 2025. Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning. In European Conference on Computer Vision. Springer, 77\u201398.\n[29] Thong Nguyen and Anh Tuan Luu. 2021. Contrastive learning for neural topic model. Advances in neural information processing systems 34 (2021), 11974\u201311986. [30] Thong Nguyen, Anh Tuan Luu, Truc Lu, and Tho Quan. 2021. Enriching and controlling global semantics for text summarization. arXiv preprint arXiv:2109.10616 (2021). [31] Thong Thanh Nguyen and Anh Tuan Luu. 2022. Improving neural cross-lingual abstractive summarization via employing optimal transport distance for knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 11103\u201311111. [32] Fengjun Pan, Xiaobao Wu, Zongrui Li, and Luu Anh Tuan. 2024. Are LLMs Good Zero-Shot Fallacy Classifiers?. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 14338\u201314364. [33] Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-Checking Complex Claims with Program-Guided Reasoning. In Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, Toronto, Canada, 6981\u20137004. [34] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 (2023). [35] Phuoc Van Long Pham, Anh Vu Duc, Nhat Minh Hoang, Xuan Long Do, and Anh Tuan Luu. 2024. ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions. In Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing (Avila, Spain) (SAC \u201924). Association for Computing Machinery, New York, NY, USA, 65\u201373. https://doi.org/10. 1145/3605098.3636030 [36] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey. arXiv preprint arXiv:2212.09597 (2022). [37] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [38] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve Prompts for In-Context Learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 2655\u20132671. https://doi.org/10.18653/v1/2022.naacl-main.191 [39] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM 64, 9 (2021), 99\u2013106. [40] Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. 2023. An analysis of the automatic bug fixing performance of chatgpt. In 2023 IEEE/ACM International Workshop on Automated Program Repair (APR). IEEE, 23\u201330. [41] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 819\u2013862. https://doi.org/10.18653/v1/2022.acl-long.60 [42] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, et al. 2023. Automatic code summarization via chatgpt: How far are we? arXiv preprint arXiv:2305.12865 (2023). [43] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4149\u20134158. https://doi.org/10.18653/v1/N19-1421 [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [45] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2024. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems 36 (2024). [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 24824\u201324837.\n[47] Xiaobao Wu, Liangming Pan, William Yang Wang, and Luu Anh Tuan. 2024. AKEW: Assessing Knowledge Editing in the Wild. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 15118\u201315133. [48] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. SelfAdaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 1423\u20131436. https://doi.org/10. 18653/v1/2023.acl-long.79 [49] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daum\u00e9 III and Aarti Singh (Eds.). PMLR, 11328\u201311339. https://proceedings.mlr.press/v119/zhang20ae.html [50] Quanjun Zhang, Tongke Zhang, Juan Zhai, Chunrong Fang, Bowen Yu, Weisong Sun, and Zhenyu Chen. 2023. A critical review of large language model on software engineering: An example from chatgpt and automated program repair. arXiv preprint arXiv:2310.08879 (2023). [51] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2024. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics 12 (2024), 39\u201357. [52] Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active Example Selection for In-Context Learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 9134\u20139148. https://doi.org/10.18653/v1/2022.emnlp-main.622 [53] Wenting Zhao, Ye Liu, Yao Wan, Yibo Wang, Qingyang Wu, Zhongfen Deng, Jiangshu Du, Shuaiqi Liu, Yunlong Xu, and Philip Yu. 2024. \ud835\udc58NN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 326\u2013337. https://doi.org/10.18653/v1/2024.naacl-long.19 [54] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921 (2023).\n# A Prompts used for few-shot learning\n### Question: A string \ud835\udc60is called good if there are no two different characters in \ud835\udc60that have the same frequency. Given a string \ud835\udc60, return the minimum number of characters you need to delete to make \ud835\udc60good. The frequency of a character in a string is the number of times it appears in the string. For example, in the string \"aab\", the frequency of \u2019a\u2019 is 2, while the frequency of \u2019b\u2019 is 1. \\textbf{Example 1:} \\textbf{Input:} \\(s = \\text{\u00e4ab\u00a8}\\) \\\\ \\textbf{Output:} 0 \\\\ \\textbf{Explanation:} \\(s\\) is already good. \\textbf{Example 2:} \\textbf{Input:} \\(s = \\textbf{\u00e4aabbbcc\u00a8}\\) \\\\ \\textbfOutput: 2 \\\\ \\textbfExplanation: You can delete two \u2019b\u2019s resulting in the good string \"aaabcc\". Another way is to delete one \u2019b\u2019 and one \u2019c\u2019, resulting in the good string \"aaabbc\". \\textbf{Example 3:} \\textbf{Input:}(s = \\text{\u00a8ceabaacb\u00a8}) \\textbf{Output:} 2 \\textbf{Explanation:} You can delete both \u2019c\u2019s, resulting in the good string \"eabaab\". Note that we only care about characters that are still in the string at the end (i.e., frequency of 0 is ignored). \\textbf{Constraints:} \\begin{itemize} \\item \\(1 \\leq s.length \\leq 105 \\item \\(s\\) contains only lowercase English letters. \\end{itemize} ### Code Prompt: class Solution(object): def minDeletions(self, s): ### Code Completion: import heapq class Solution(object): def minDeletions(self, s): lst = list(s) counter = {} for char in lst: if char not in counter: counter[char] = 0 counter[char] += 1 q = [] ans = 0 for char in counter: heapq.heappush(q, (-counter[char], char)) del counter while q: curr_ct, curr = heapq.heappop(q) if q: if q[0][0] == curr_ct: curr_ct += 1 ans += 1 if curr_ct < 0: heapq.heappush(q, (curr_ct, curr)) return ans ### Question: \u00abQ2\u00bb ### Code Prompt: \u00abS2\u00bb ### Code Completion: \u00abE2\u00bb ### Question: \u00abQ3\u00bb ### Code Prompt: \u00abS3\u00bb ### Code Completion: \u00abE3\u00bb ### Question: \u00abQ4\u00bb ### Code Prompt: \u00abS4\u00bb ### Code Completion: \u00abE4\u00bb ### Question: \u00abQ5\u00bb ### Code Prompt: \u00abS5\u00bb ### Code Completion: \u00abE5\u00bb ### Question: \u00abTest Question\u00bb ### Code Prompt: \u00abTest Code Prompt\u00bb e 4: Example of the prompt used in the experimen\nTable 4: Example of the prompt used in the experiments on the Mercury dataset.\n### Question: The least common multiple of two integers is 36 and 6 is their greatest common divisor. What is the product of the two numbers? ### Solution: Let $a$ and $b$ be the two integers. We can use the identity $\\gcd(a,b) \\cdot \\mathop \\textlcm[a,b] = ab$. Substituting gives that the answer is $36 \\cdot 6 = \\boxed{216}$. ### Extracted Answer: 216 ### Question: In an office at various times during the day, the boss gives the secretary a letter to type, each time putting the letter on top of the pile in the secretary\u2019s in-box. When there is time, the secretary takes the top letter off the pile and types it. There are nine letters to be typed during the day, and the boss delivers them in the order $1, 2, 3, 4, 5, 6, 7, 8, 9$. While leaving for lunch, the secretary tells a colleague that letter 8 has already been typed, but says nothing else about the morning\u2019s typing. The colleague wonders which of the nine letters remain to be typed after lunch and in what order they will be typed. Based upon the above information, how many such after-lunch typing orders are possible? (That there are no letters left to be typed is one of the possibilities.) Re-stating the problem for clarity, let $S$ be a set arranged in increasing order. At any time an element can be appended to the end of $S$, or the last element of $S$ can be removed. The question asks for the number of different orders in which the all of the remaining elements of $S$ can be removed, given that $8$ had been removed already. ### Solution: Since $8$ had already been added to the pile, the numbers $1 \\ldots 7$ had already been added at some time to the pile; 9 might or might not have been added yet. So currently $S$ is a subset of $\\{1, 2, \\ldots 7\\}$, possibly with $9$ at the end. Given that $S$ has $k$ elements, there are $k+1$ intervals for $9$ to be inserted, or $9$ might have already been placed, giving $k+2$ different possibilities. Thus, the answer is $\\sum_{k=0}\u02c6{7} \\7 \\choose k}(k+2)$ $= 1 \\cdot 2 + 7 \\cdot 3 + 21 \\cdot 4 + 35 \\cdot 5 + 35 \\cdot 6 + 21 \\cdot 7 + 7 \\cdot 8 + 1 \\cdot 9$ $= \\boxed{704}$. ### Extracted Answer: 704 ### Question: \u00abQ3\u00bb ### Solution: \u00abS3\u00bb ### Extracted Answer: \u00abE3\u00bb ### Question: \u00abQ4\u00bb ### Solution: \u00abS4\u00bb ### Extracted Answer: \u00abE4\u00bb ### Question: \u00abQ5\u00bb ### Solution: \u00abS5\u00bb ### Extracted Answer: \u00abE5\u00bb ### Question: \u00abTest Question\u00bb\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/281f/281f1698-04da-443b-a366-5e218a031acf.png\" style=\"width: 50%;\"></div>\nTable 5: Example of the prompt used in the experiments on the MATH dataset.\nTable 5: Example of the prompt used in the experiments on the MATH dataset.\n",
    "paper_type": "method",
    "attri": {
        "background": "Large Language Models (LLMs) have demonstrated significant in-context learning (ICL) capabilities, yet their performance is hindered by the challenge of selecting appropriate demonstrations. Existing methods often rely on random or heuristic selection, leading to inconsistent outcomes, particularly in complex tasks. This paper introduces a new approach called Curriculum Demonstration Selection (CDS), inspired by curriculum learning, to effectively select demonstrations based on their complexity and improve LLM performance across varying task difficulties.",
        "problem": {
            "definition": "The paper aims to address the issue of selecting demonstrations for LLMs in ICL that optimize their performance across tasks of varying complexity.",
            "key obstacle": "Existing demonstration selection methods often fail to account for the varying difficulty levels of tasks, leading to suboptimal model performance, especially in complex scenarios."
        },
        "idea": {
            "intuition": "The idea behind CDS is inspired by curriculum learning, where learners progress from simpler to more challenging tasks, allowing for gradual skill development.",
            "opinion": "CDS systematically selects demonstrations based on their complexity, ensuring a balanced representation that enhances LLM learning from both simple and complex tasks.",
            "innovation": "Unlike traditional methods that focus primarily on similarity or random selection, CDS incorporates complexity measures to select demonstrations, allowing for a more structured and effective learning process."
        },
        "method": {
            "method name": "Curriculum Demonstration Selection",
            "method abbreviation": "CDS",
            "method definition": "CDS is a method that selects demonstrations for LLMs based on their complexity, following a curriculum learning approach.",
            "method description": "CDS partitions the dataset into difficulty levels and selects demonstrations from each level to provide a comprehensive learning experience.",
            "method steps": "1. Partition the dataset into distinct difficulty levels based on complexity measures. 2. Retrieve one demonstration from each difficulty partition. 3. Shuffle the selected demonstrations to ensure unbiasedness.",
            "principle": "CDS is effective because it allows LLMs to learn incrementally from a diverse set of demonstrations, which enhances their ability to generalize across varying task complexities."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized the MATH dataset for mathematical reasoning, the ARC-Challenge dataset for commonsense reasoning, and the Mercury dataset for code generation, comparing CDS against baseline methods.",
            "evaluation method": "Performance was assessed based on accuracy for reasoning tasks and functional correctness for code generation, with results analyzed across multiple LLMs."
        },
        "conclusion": "The results demonstrate that CDS significantly enhances LLM performance in ICL by optimizing demonstration selection based on complexity, outperforming traditional methods and showing particular efficacy in solving complex problems.",
        "discussion": {
            "advantage": "CDS provides a structured approach to demonstration selection that improves model performance and consistency across various tasks.",
            "limitation": "The method relies on predefined complexity measures, which may not be available for all datasets, potentially limiting its applicability.",
            "future work": "Future research could explore the impact of varying the number of demonstrations and extending CDS to a broader range of tasks to evaluate its generalizability."
        },
        "other info": {
            "arXiv link": "arXiv:2411.1812",
            "keywords": [
                "Curriculum Learning",
                "In-Context Learning",
                "Large Language Models",
                "Demonstration Selection"
            ],
            "conference": "SAC\u201925, March 31 \u2013April 4, 2025, Sicily, Italy"
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The paper introduces a new approach called Curriculum Demonstration Selection (CDS), inspired by curriculum learning, to effectively select demonstrations based on their complexity and improve LLM performance across varying task difficulties."
        },
        {
            "section number": "3.1",
            "key information": "CDS systematically selects demonstrations based on their complexity, ensuring a balanced representation that enhances LLM learning from both simple and complex tasks."
        },
        {
            "section number": "3.2",
            "key information": "The principle of CDS is effective because it allows LLMs to learn incrementally from a diverse set of demonstrations, enhancing their ability to generalize across varying task complexities."
        },
        {
            "section number": "6.1",
            "key information": "The method relies on predefined complexity measures, which may not be available for all datasets, potentially limiting its applicability."
        },
        {
            "section number": "5.1",
            "key information": "The experiments utilized the MATH dataset for mathematical reasoning, the ARC-Challenge dataset for commonsense reasoning, and the Mercury dataset for code generation, comparing CDS against baseline methods."
        }
    ],
    "similarity_score": 0.7323317031415651,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Curriculum Demonstration Selection for In-Context Learning.json"
}