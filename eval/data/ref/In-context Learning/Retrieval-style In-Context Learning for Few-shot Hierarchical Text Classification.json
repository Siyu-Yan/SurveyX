{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.17534",
    "title": "Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification",
    "abstract": "Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.",
    "bib_name": "chen2024retrievalstyleincontextlearningfewshot",
    "md_text": "# Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification\n# Huiyao Chen1,\u2217, Yu Zhao2,\u2217, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang1,\u2020 , Min Zhang1\nHuiyao Chen1,\u2217, Yu Zhao2,\u2217, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang1,\u2020 , Min Zhang1\nnstitute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), China 2College of Intelligence and Computing, Tianjin University, China chenhy1018@gmail.com, zhaoyucs@tju.edu.cn, chenzulong198867@163.com mason.zms@gmail.com, zhangmin2021@hit.edu.cn\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/38b1/38b10584-4066-4893-90d5-2b4c3829d504.png\" style=\"width: 50%;\"></div>\nAbstract\nHierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.\narXiv:2406.17534v2\n# 1 Introduction\nHierarchical text classification (HTC), a specialized branch of multilabel text classification, involves the systematic arrangement and categorization of textual data throughout a tiered label structure. The output labels are organized in a parentchild hierarchy, with the higher-level labels encompassing broader concepts, and the child labels delineating more specific subtopics or attributes. In recent years, HTC has gained significant attention, due to its applicability across a variety\n\u2217Equal contribution \u2020 The corresponding author\nFigure 1: The problems of ICL-based few-shot HTC and our solutions. MLM, CLS and DCL denote Mask Language Modeling, Layer-wise CLaSsification and Divergent Contrastive Learning, which are the three objectives for indexer training.\nof fields, including recommendation systems (Sun et al., 2023; Agrawal et al., 2013), document categorization (Peng et al., 2016; Kowsari et al., 2017) and information retrieval (Sinha et al., 2018). In standard supervised HTC, there is an underlying assumption of abundant training samples (Zhao et al., 2023; Im et al., 2023; Song et al., 2023), which is often unattainable and expensive to construct manually. Moreover, HTC datasets are characterized by a complex hierarchical label structure, with leaf labels typically following a Zipfian distribution, resulting in very few data instances for these labels. As a result, the few-shot setting is more realistic, and has gained increasing interest recently (Ji et al., 2023; Bhambhoria et al., 2023; Wang et al., 2023b). Nevertheless, existing works often struggle with unsatisfactory performance in this setting. For example, BERT with the vanilla fine-tuning strategy performs extremely poorly in few-shot HTC. Recently, large language models (LLMs) have achieved notable success on various NLP tasks (Wang et al., 2023a; Drozdov et al., 2023; Zeng et al., 2023), which have significantly enhanced the efficacy of in-context learning (ICL) with relevant demonstrations in the few-shot setting\n(Shome and Yadav, 2023; Dai et al., 2023; Zhang et al., 2023). However, the application of ICL on HTC faces unique challenges, diverging from traditional text classification scenarios. These challenges are primarily due to two distinct characteristics of HTC, as delineated in Figure 1. Firstly, HTC features a deep hierarchical labeling structure and expansive label sets, resulting in large label sets in ICL, which adversely impacts its performance. Secondly, as the hierarchy deepens, the semantic similarity between adjacent labels increases (Stein et al., 2019), making it very challenging to select relevant demonstrations that guide the learning process efficiently. In this work, we introduce the first ICL-based framework for few-shot HTC. Specifically, we use a LLM as the foundation model for inference, and provide demonstrations to guide HTC label generation through ICL. Our success depends on finding suitable demonstrations for a given input. In order to achieve this, we build a retrieval database that can find the most-relevant demonstrations for the input. Further, in order to avoid providing an enormous set of multi-layer contextual HTC labels all at once, as is required for ICL, we suggest an iterative policy to infer the labels layer-by-layer, reducing the number of candidate labels greatly. The quality of our retrieval database is highly critical. The key idea is to obtain the HTC labelaware representations for input texts, which are then used for subsequent retrieving. Given an input, we define prompt templates for each-layer HTC, concatenating them with the raw input to form a new text. The hidden vectors of prompts are exploited as label-aware representations. We perform continual training to learn our representations using three types of objectives: the masked language model (MLM), the multi-label classification (MLC) for HTC particularly and a welldesigned divergent contrastive learning (DCL) objective. The DCL is especially useful for the semantically-closed HTC labels (e.g., adjacent labels from the same parent). In addition, we can incorporate label descriptions naturally by DCL, which can benefit ICL-based HTC much. We conduct experiments on two classic English hierarchical classification datasets, WOS (Reuters, 2012) and DBpedia (Sinha et al., 2018), as well as a Chinese patent dataset1, and measure model per-\n1This dataset is not allowed to be publicly released due to local law.\nformance by both micro-F1 and macro-F1 metrics consistent with previous work. Results show that our method is highly effective, giving improved results compared with a series of baselines for the few-shot HTC with different shots. We can also achieve state-of-the-art (SOTA) results on these HTC datasets. Further, we perform thorough qualitative analysis in order to gain a comprehensive understanding of our methodology. In summary, our contributions can be summarized as follows: \u2022 We present the first ICL-based work for HTC backended with LLMs, utilizing the knowledge from the existing demonstrations during the LLM inference. \u2022 We propose a novel retrieve-style framework for ICL of HTC, which can help to find highly-relevant demonstrations to support HTC label prediction. \u2022 We conduct extensive experiments to compare our method with several representative baselines, and results show that our method can advance the SOTA of HTC further. Our code is publicly available at https://github.com/DreamH1gh/TACL2024 to facilitate future research.\n# 2 Related Work\nHTC is initially presented by (Koller and Sahami, 1997), and neural network models have achieved great advances in this task. Previous approaches treat HTC as multi-label classification and adopt traditional text classification methods for HTC problems (Aly et al., 2019; Zhang et al., 2022b). The majority of recent HTC studies pay the focus on finding ways to insert the hierarchical label knowledge into the model (Chen et al., 2021; Mao et al., 2019; Sinha et al., 2018). Several works attempt to solve HTC problems by modeling the hierarchical labels as a graph or a tree structure (Zhou et al., 2020; Tai et al., 2015) while other researchers try to apply meta-learning (Wu et al., 2019) or reinforcement learning (Mao et al., 2019) to leverage HTC label structure. However, existing methods mainly concentrate on encoding the holistic label structure, ignoring the classification of nuanced long-tail terminal labels. There have been efforts prove that retrieval augmented methods could help classification task with only few-shot samples (Chen et al., 2022; Zhang et al., 2022a; Yu et al., 2023; Xiong et al.,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e60/2e606d9e-af54-44f7-bc66-533db9143800.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The architecture of retrieval-style in-context learning for HTC. The [Pj] term is a soft prompt templa token to learn the j-th hierarchical layer label index representation.</div>\n2023), which could be a solution of long-tail challenge. Given this insight, we make an attempt to convert the HTC task to a retrieval form. Moreover, with the development of LLMs, recent work explores solutions that tackle traditional NLP tasks with the ICL paradigm and achieve surprising effectiveness (Shome and Yadav, 2023; Fei et al., 2023; Min et al., 2022; Liu et al., 2022). But ICL strongly relies on the demonstration selecting (Gao et al., 2021; Zhao et al., 2021; Rubin et al., 2022; Zhang et al., 2022c; Li et al., 2023). Many studies explore adjusting demonstrations for better performance through instruction formatting (Zhou et al., 2023), examples ordering (Liu et al., 2021) and demonstration filtering (Sorensen et al., 2022). In our work, we combine the ICL-based framework with retrieval for HTC, selecting demonstrations that involve both the language knowledge of LLM and advantages of retrieval.\n# 3 Method\nProblem Formulation In HTC tasks, the structure of labels H = (Y, E) is often predefined as a tree, where Y = {Y1, Y2, . . . , YL} is a set of nodes (labels) and E indicates the parent-child hierarchical edges (connections) between the labels. It is worth noting that in the label structure, every node, except for the root, has one and only one parent. Generally speaking, HTC tasks select the label path in H for a given text x. We define that x = w1w2 \u00b7 \u00b7 \u00b7 wn is a text and y = {y1, y2, . . . , yC} \u2286Y is the corresponding hierarchical labels which follow H, where C denotes\nthe maximum label depth.\nProposed Framework Figure 2 illustrates our ICL-based framework for HTC. We first train a PLM-based indexer and build a retrieval database containing reference samples (the training data). After that, we perform a similarity search in the retrieval database with the text to be inferred. Finally, we construct an ICL prompt with highly similar demonstrations for prediction. We will introduce our ICL prompt policy for HTC (\u00a7 3.1), and then detail the retrieval database construction (\u00a7 3.2) and demonstration retrieval methods (\u00a7 3.3).\n# 3.1 In-Context Learning\nIn order to integrate label structural information into ICL, we propose an iterative approach by decoupling the label structure H. We decompose the label structure into several subclusters, each corresponding to a parent-child set. Then, we employ an iterative method to produce the sub-labels layer by layer until we arrive at the leaf labels. As shown in Figure 2, we perform iterative inference at each hierarchy level. Based on the Top K similar demonstrations, the prompt contains K identical structured text blocks. Each block contains three parts: Text, Current Label, and Candidate Label Set. Text is the demonstration content. Current Label is the predicted label of the previous hierarchy level2.\n2Current Label is Root when predicting the first hierarchy level\nWhen the LLM is used for inference in classification tasks, the entire set of labels is always presented. The inference result is drawn from this large label set. In contrast, our method supplies a pruned subset of labels as a concise candidate label set. Candidate Label Set is the intersection of the child nodes of the current label and the selected K demonstration labels, which maximizes the use of demonstration information and avoids the impact of erroneous labels. The predicted label of the next hierarchy level is required to be selected from the candidate label set.\n# 3.2 Retrieval Database Construction\nAfter determining the ICL prompt policy, it is crucial to obtain demonstrations related to the test text, which will provide effective guidance for LLM inferences. Firstly, we train a HTC indexer to generate index vectors for each training sample. We employ a pretrained text encoder as the indexer and use a prompt template to elicit multi-layer representations as index vectors. To make the index vectors discriminative, the indexer is trained via DCL based on label descriptions.\nlize the language knowledge embedded in pretrained text encoders and leverage interdependencies among hierarchical labels, we propose the construction of a concise prompt template prior to raw input x. The new text is formatted as: \u00afx=[P1] [P2] . . . [PC] x. Here, the [Pj] term is a soft prompt template token to learn the j-th hierarchical layer label index representation. Then, we input \u00afx into the encoder of PLM to obtain the hidden states:\n(1)\nThus, we can obtain the index vectors m1 \u00b7 \u00b7 \u00b7 mC consisting of hidden state embeddings for all fixed-position [P], where we consider mj as the index vector of the j-th hierarchical level corresponding to x.\nLabel Description. In order to reduce the ambiguity errors caused by insufficient label information, we explore diverse approaches that aim to provide more informative and representative label information for HTC task. First, we propagate the textual information of all label nodes to their corresponding leaf nodes, obtaining the textual information with the entire label path. As shown in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4357/4357de74-8093-46f1-b4f0-49a7bde49349.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Label description generation.</div>\nFigure 3, for the original leaf label \u201cspeech recognition\u201d, its label path is \u201cspeech recognition of speech of AI\u201d. However, due to the close semantic proximity of adjacent leaf node labels, the generated label path may still be insufficient or ambiguous. For example, \u201cspeech recognition of speech of AI\u201d and \u201cspeech synthesis of speech of AI\u201d may still be difficult to distinguish. To address this issue, we use the LLM to expand and enhance the label path l of x by leveraging the knowledge contained within the LLM:\nwhere d is the description of the label path l and Describe denotes the prompt used to generate the description. By utilizing expanded and enhanced label descriptions, we could obtain a more detailed explanation of the label.\nIndexer Training. For indexer training, we apply the objectives of mask language modeling Lmlm, and layer-wise classification Lcls. Lmlm is used to predict the words that fill the random mask tokens in the inputs. Lcls is to predict HTC labels through each hierarchical layer index vectors. Additionally, we propose DCL for indexer training, which uses label text information to select positive and negative samples. For x, positive samples are chosen from sentences with the same label as x. Additionally, the corresponding label description d could be treated as a positive sample. Negative samples consist of two parts. First, based on the similarity between d and descriptions of other labels, negative examples are sampled from highly similar label categories. Similarly, their corresponding label descriptions could be also treated as negative samples. In addition, a few randomly selected sentences from other labels are used as negative samples of x. Thus, compared to traditional random sampling methods, our negative sample selection approach opts for more\ninstances with semantically similar labels as hard negative samples. Then the index vectors among the positive samples are pulled together and the negative ones are pushed apart. Taking x as an example, denote B = {x, x+, x\u2212 1 , . . . , x\u2212 n } as a group of input data. The contrastive loss can be calculated as:\n(3)\n\ufffd where cos(\u00b7, \u00b7) is the cosine similarity, \u03c4 is the contrastive learning temperature. In comparison to calculating the contrastive loss in a random sampling batch, our DCL pays more attention to samples whose labels are less similar to the x. The final objective is set in the multi-task form:\n(4)\nAfter the training step, we store index vectors m1 \u00b7 \u00b7 \u00b7 mC of each training instance to construct the retrieval database.\n# 3.3 Demonstration Retrieval\nWith the database and indexer in hand, we can process predictions in retrieval form. For the test text x\u2032, we also use the trained indexer to obtain the hierarchical index vectors m\u2032 1 \u00b7 \u00b7 \u00b7 m\u2032 C. Then, we select similar instances from the retrieval database by calculating similarity between their index vectors. For each training instance x, we have C index vectors m1 \u00b7 \u00b7 \u00b7 mC in retrieval database. The similarity between x and x\u2032 can be calculated as:\n(5)\n\ufffd where the first factor is utilized to adjust the weights of similarity between different hierarchical layer, while ensuring that \ufffdC j 2j\u22121 2C\u22121 = 1. As the hierarchy deepens, the impact of index vector similarity gradually increases. Then, we choose the Top K most similar instances from the database as demonstrations. It is worth noting that we filter out instances with the same label here, to ensure that the labels of the Top K instances are different, providing relatively diverse instances for ICL.\n# 4 Experiments\n# 4 Experiments 4.1 Settings\nDataset and Evaluation Metrics. Our experiments are evaluated on three datasets: Web-ofScience (WOS) (Reuters, 2012), DBpedia (Sinha\n<div style=\"text-align: center;\">WOS DBpedia Patent</div>\n#levels\n2\n3\n4\n#Number of documents\n46,985\n381,025\n30,104\n#Level 1 Categories\n7\n9\n10\n#Level 2 Categories\n134\n70\n17\n#Level 3 Categories\nNA\n219\n105\n#Level 4 Categories\nNA\nNA\n305\n#Mean label length\n1.8\n1.7\n4.4\n#Max label length\n3\n7\n14\n#Mean document length\n200.7\n106.9\n335.1\n#Max document length\n1262\n881\n1669\nTable 1: Overview of HTC datasets.\net al., 2018) and Patent. WOS and DBpedia are both widely used English datasets for HTC and Patent which we collected consists of 30,104 Chinese patent records. We evaluate the effectiveness of our proposed method on both English and Chinese datasets. All of them are for single-path HTC. The statistics are illustrated in Table 1. Following the previous work, we report experimental results with Micro-F1 and Macro-F1.\nModel Details. We utilize bert-base-uncased3 (Devlin et al., 2019) as the base indexer for WOS and DBpedia datasets, while for Patent dataset, we employ chinese-bert-wwm-ext4 (Cui et al., 2019, 2020). Regarding LLM, we select vicuna-7b-v1.516k5 (Zheng et al., 2023) and gpt-3.5-turbo-06136, which performs well on English for WOS and DBpedia datasets, and ChatGLM-6B7 (Zeng et al., 2022; Du et al., 2022), the top-performing opensource Chinese language model for Patent (due to legal restrictions, we can only evaluate it on opensource models). Our model is implemented with the OpenPrompt toolkit (Ding et al., 2022).\nModel Details. We utilize bert-base-uncased3 (Devlin et al., 2019) as the base indexer for WOS and DBpedia datasets, while for Patent dataset, we employ chinese-bert-wwm-ext4 (Cui et al., 2019, 2020). Regarding LLM, we select vicuna-7b-v1.516k5 (Zheng et al., 2023) and gpt-3.5-turbo-06136, which performs well on English for WOS and DBpedia datasets, and ChatGLM-6B7 (Zeng et al., 2022; Du et al., 2022), the top-performing opensource Chinese language model for Patent (due to legal restrictions, we can only evaluate it on opensource models). Our model is implemented with the OpenPrompt toolkit (Ding et al., 2022). Experimental Settings. As mentioned in the introduction, we try to validate the effectiveness of our proposed method on the few-shot classes in the long-tail phenomenon. Specifically, we focus on the few-shot setting, where only Q samples per label path are available for training and use the same seeds as Ji et al. (2023), as shown in Algorithm 1. We conduct experiments based on\n3https://huggingface.co/bert-base-uncased 4https://huggingface.co/hfl/chinese-bert-wwm-ext 5https://github.com/lm-sys/FastChat 6https://openai.com/blog/chatgpt. ICL inference in experiments is based on this model unless otherwise specified. 7https://github.com/THUDM/ChatGLM-6B\nQ \u2208{1, 2, 4, 8, 16}. The batch size of our proposed method is 1. It is composed of a training sample, a randomly selected positive sample from the same label, 4 randomly selected negative samples from the Top4 labels based on label description similarity, and 10 randomly selected negative samples from other labels. For all datasets, the learning rate is 5\u221710\u22125 and we train the model for 20 epochs and apply the Adam optimizer (Kingma and Ba, 2015) with a linearly decaying schedule with warmup steps at 0. The temperature of Vicuna, GPT3.5 and ChatGLM are both 0.2. \u03b1 in Equation 4 is 1 and \u03b2 is 0.01.\nBaselines. In this work, we select several recent works as baselines:\n\u2022 BERT with vanilla fine-tuning transforms HTC into a multi-label classification task. It is a standard method for HTC.\n\u2022 HiMatch (Chen et al., 2021) learns the representation of text and labels separately and then defines different optimization objectives based on them to improve HTC.\n\u2022 HGCLR (Wang et al., 2022a) incorporates the hierarchical label structure directly into the text encoder and obtains the hierarchyaware text representation for HTC.\n\u2022 HPT (Wang et al., 2022b) leverages a dynamic virtual template with soft-prompt label words and a zero-bounded multi-label crossentropy loss, ingeniously aligning the goals of HTC and MLM.\n\u2022 HierVerb (Ji et al., 2023) treats HTC as a multi-label problem at different levels, utilizing vectors as constrained by the hierarchical structure, effectively integrating knowledge of hierarchical labels.\n EPR (Rubin et al., 2022) estimates the output probability based on the input and a candidate training example prompt, separating examples as positive and negative and allowing effective retrieval of training examples as prompts during testing.\n\u2022 REGEN (Yu et al., 2023) employs a retrieval model and a classification model, utilizing class-specific verbalizers and a general unlabeled corpus to enhance semantic understanding. Notably, REGEN incorporates supplementary unsupervised data8.\nRetrieval employs our retrieval method to select the label associated with the text in the retrieval database that has the highest similarity score as the label for the test text. Retrieval-style ICL involves the selection of the top three (K=3) documents with distinct labels from the retrieval database. Subsequently, these documents and labels are utilized as demonstrations to construct the prompt, and our iterative method is applied to hierarchical label inference. It is worth mentioning that in our LLM generative approach, if the generated label is not present in the candidate label set, the label corresponding to the retrieval text with the highest similarity is selected as its inference result.\n# 4.2 Main Results\nThe main results are shown in Table 2 and Table 3. It can be observed that our retrieval-based approach achieved the best results across almost all settings. Also, we find that our method is less affected by random seeds, resulting in more stable and robust performance, which further demonstrated the effectiveness of our approach.\n8We employ full dataset texts, excluding the training data, as unsupervised data.\n<div style=\"text-align: center;\">WOS(Depth 2)</div>\nQ\nMethod\nWOS(Depth 2)\nDBpedia(Depth 3)\nMicro-F1\nMacro-F1\nMicro-F1\nMacro-F1\n1\nBERT \u2020\n2.99 \u00b1 20.85(5.12)\n0.16 \u00b1 0.10 (0.24)\n14.43 \u00b1 13.34 (24.27)\n0.29 \u00b1 0.01 (0.32)\nHiMatch \u2020\n43.44 \u00b1 8.09 (48.26)\n7.71 \u00b1 4.90 (9.32)\n-\n-\nHGCLR \u2020\n9.77 \u00b1 11.77(16.32)\n0.59 \u00b1 0.10 (0.63)\n15.73 \u00b1 31.07 (25.13)\n0.28 \u00b1 0.10 (0.31)\nHPT \u2020\n50.05 \u00b1 6.80 (50.96)\n25.69 \u00b1 3.31 (27.76)\n72.52 \u00b1 10.20 (73.47)\n31.01 \u00b1 2.61 (32.50)\nHierVerb \u2020\n58.95 \u00b1 6.38 (61.76)\n44.96 \u00b1 4.86 (48.19)\n91.81 \u00b1 0.07 (91.95)\n85.32 \u00b1 0.04 (85.44)\nEPR\n31.77 \u00b1 3.15 (35.31)\n6.61 \u00b1 2.70 (9.66)\n16.58 \u00b1 8.94 (25.60)\n7.41 \u00b1 4.13 (11.91)\nREGEN\n5.62 \u00b1 2.98 (8.70)\n2.59 \u00b1 2.45 (4.71)\n18.70 \u00b1 8.19 (27.33)\n8.17 \u00b1 3.87 (12.20)\nRetrieval\n63.46 \u00b1 2.30 (65.99)\n50.24\u00b1 2.21 (52.66)\n93.68 \u00b1 0.05 (93.74)\n88.41 \u00b1 0.23 (88.67)\nRetrieval-style ICL\n68.91 \u00b1 0.48 (69.38)\n57.41 \u00b1 0.40 (57.82)\n94.54 \u00b1 0.03 (94.58)\n89.75 \u00b1 0.09 (94.83)\n2\nBERT \u2020\n46.31 \u00b1 0.65 (46.85)\n5.11 \u00b1 1.31 (5.51)\n87.02 \u00b1 3.89 (88.20)\n69.05 \u00b1 26.81(73.28)\nHiMatch \u2020\n46.41 \u00b1 1.31 (47.23)\n18.97 \u00b1 0.65 (21.06)\n-\n-\nHGCLR \u2020\n45.11 \u00b1 5.02 (47.56)\n5.80 \u00b1 11.63 (9.63)\n87.79 \u00b1 0.40 (88.42)\n71.46 \u00b1 0.17 (71.78)\nHPT \u2020\n57.45 \u00b1 1.89 (58.99)\n35.97 \u00b1 11.89 (39.94)\n90.32 \u00b1 0.64 (91.11)\n81.12 \u00b1 1.33 (82.42)\nHierVerb \u2020\n66.08 \u00b1 4.19 (68.01)\n54.04 \u00b1 3.24 (56.69)\n93.71 \u00b1 0.01 (93.87)\n88.96 \u00b1 0.02 (89.02)\nEPR\n36.04 \u00b1 2.97 (39.11)\n16.28 \u00b1 1.94 (18.32)\n21.89 \u00b1 5.02 (27.02)\n15.96 \u00b1 2.96 (19.02)\nREGEN\n49.55 \u00b1 2.88 (52.64)\n12.12 \u00b1 3.54 (15.91)\n87.91 \u00b1 2.44 (90.57)\n71.80 \u00b1 2.41 (74.35)\nRetrieval\n69.85 \u00b1 0.63 (70.58)\n58.64 \u00b1 0.58 (59.25)\n94.12 \u00b1 0.18 (94.32)\n89.33 \u00b1 0.19 (89.54)\nRetrieval-style ICL\n71.68 \u00b1 0.09 (71.76)\n61.99 \u00b1 0.10 (62.08)\n94.87 \u00b1 0.10 (94.97)\n90.82 \u00b1 0.08 (90.89)\n4\nBERT \u2020\n56.00 \u00b1 4.25 (57.18)\n31.04 \u00b1 16.65(33.77)\n92.94 \u00b1 0.66 (93.38)\n84.63 \u00b1 0.17 (85.47)\nHiMatch \u2020\n57.43 \u00b1 0.01 (57.43)\n39.04 \u00b1 0.01 (39.04)\n-\n-\nHGCLR \u2020\n56.80 \u00b1 4.24 (57.96)\n32.34 \u00b1 15.39(33.76)\n93.14 \u00b1 0.01 (93.22)\n84.74 \u00b1 0.11 (85.11)\nHPT \u2020\n65.57 \u00b1 1.69 (67.06)\n45.89 \u00b1 9.78 (49.42)\n94.34 \u00b1 0.28 (94.83)\n90.09 \u00b1 0.87 (91.12)\nHierVerb \u2020\n72.58 \u00b1 0.83 (73.64)\n63.12 \u00b1 1.48 (64.47)\n94.75 \u00b1 0.13 (95.13)\n90.77 \u00b1 0.33 (91.43)\nEPR\n38.42 \u00b1 0.91 (39.36)\n19.94 \u00b1 1.32 (21.31)\n27.94 \u00b1 1.47 (29.56)\n18.31 \u00b1 1.70 (20.09)\nREGEN\n58.75 \u00b1 2.04 (60.71)\n33.20 \u00b1 2.01 (35.40)\n94.11 \u00b1 0.79 (95.01)\n86.76 \u00b1 1.04 (87.92)\nRetrieval\n75.37 \u00b1 0.70 (76.08)\n65.94\u00b1 0.57 (66.41)\n95.15 \u00b1 0.07 (95.23)\n91.26 \u00b1 0.14 (91.38)\nRetrieval-style ICL\n75.62 \u00b1 0.15 (75.78)\n66.34 \u00b1 0.09 (66.41)\n95.26 \u00b1 0.07 (95.23)\n91.42 \u00b1 0.05 (91.47)\n8\nBERT \u2020\n66.24 \u00b1 1.96 (67.53)\n50.21 \u00b1 5.05 (52.60)\n94.39 \u00b1 0.06 (94.57)\n87.63 \u00b1 0.28 (87.78)\nHiMatch \u2020\n69.92 \u00b1 0.01 (70.23)\n57.47 \u00b1 0.01 (57.78)\n-\n-\nHGCLR \u2020\n68.34 \u00b1 0.96 (69.22)\n54.41 \u00b1 2.97 (55.99)\n94.70 \u00b1 0.05 (94.94)\n88.04 \u00b1 0.25 (88.61)\nHPT \u2020\n76.22 \u00b1 0.99 (77.23)\n67.20 \u00b1 1.89 (68.63)\n95.49 \u00b1 0.01 (95.57)\n92.35 \u00b1 0.03 (92.52)\nHierVerb \u2020\n78.12 \u00b1 0.55 (78.87)\n69.98\u00b1 0.91 (71.04)\n95.69 \u00b1 0.01 (95.70)\n92.44 \u00b1 0.01 (92.51)\nEPR\n41.35 \u00b1 0.43 (41.83)\n22.19 \u00b1 0.32 (22.57)\n44.95 \u00b1 0.43 (45.42)\n31.13 \u00b1 0.38 (31.56)\nREGEN\n67.91 \u00b1 1.47 (69.54)\n55.39 \u00b1 1.86 (57.32)\n95.24 \u00b1 0.12 (95.38)\n90.56 \u00b1 0.39 (90.99)\nRetrieval\n79.04 \u00b1 0.48 (79.53)\n70.59 \u00b1 0.52 (71.04)\n95.71 \u00b1 0.06 (95.78)\n92.50 \u00b1 0.02 (92.52)\nRetrieval-style ICL\n76.93 \u00b1 0.05 (76.98)\n67.54 \u00b1 0.04 (67.57)\n95.43 \u00b1 0.01 (95.44)\n91.85 \u00b1 0.01 (91.86)\n16\nBERT \u2020\n75.52 \u00b1 0.32 (76.07)\n65.85 \u00b1 1.28 (66.96)\n95.31 \u00b1 0.01 (95.37)\n89.16 \u00b1 0.07 (89.35)\nHiMatch \u2020\n77.67 \u00b1 0.01 (78.24)\n68.70 \u00b1 0.01 (69.58)\n-\n-\nHGCLR \u2020\n76.93 \u00b1 0.52 (77.46)\n67.92 \u00b1 1.21 (68.66)\n95.49 \u00b1 0.04 (95.63)\n89.41 \u00b1 0.09 (89.71)\nHPT \u2020\n79.85 \u00b1 0.41 (80.58)\n72.02 \u00b1 1.40 (73.31)\n96.13 \u00b1 0.01 (96.21)\n93.34 \u00b1 0.02 (93.45)\nHierVerb \u2020\n80.93 \u00b1 0.10 (81.26)\n73.80 \u00b1 0.12 (74.19)\n96.17 \u00b1 0.01 (96.21)\n93.28 \u00b1 0.06 (93.49)\nEPR\n44.57 \u00b1 0.09 (44.70)\n24.50 \u00b1 0.18 (24.74)\n52.68 \u00b1 0.04 (52.71)\n42.76 \u00b1 0.03 (42.78)\nREGEN\n77.64 \u00b1 1.04 (78.70)\n69.91 \u00b1 1.68 (71.68)\n95.88 \u00b1 0.03 (95.91)\n91.73 \u00b1 0.07 (91.80)\nRetrieval\n81.12 \u00b1 0.26 (81.38)\n73.72\u00b1 0.17 (73.82)\n96.22 \u00b1 0.04 (96.27)\n93.37 \u00b1 0.02 (93.46)\nRetrieval-style ICL\n78.62 \u00b1 0.03 (78.65)\n69.56 \u00b1 0.03 (69.59)\n95.56 \u00b1 0.00 (95.56)\n92.04 \u00b1 0.00 (92.04)\nTable 2: Micro-F1 and Macro-F1 scores on two English datasets. We reported the average, standard deviatio and best results across three random seeds. Bold: the best result. Underlined: the second highest. \u2020: the dire utilization of results from Ji et al. (2023).\nSpecifically, as Q increases, all methods improve continuously. However, our retrieval-based method consistently performs the best, and its advantages become even more pronounced in extremely low-resource settings. In the 1-shot setting, compared with the previous state-of-the-art model, Retrieval shows an average of 4.51% micro, 5.28% macro-F1 absolute improvement on WOS, 1.87% micro, 3.09% macro-F1 absolute on DBPedia, and 5.88% micro-F1, 4.85% macro-F1 absolute improvement on Patent. As the hierarchy depth increases, we find that /texttt[Retrieval] exhibits an advantage even in the 16-shot setting. We think it is because label text descriptions better dif-\n<div style=\"text-align: center;\">DBpedia(Depth 3)</div>\nferentiate categories, especially in the deeper HTC dataset. Despite the fact that our method is still the most effective, we observe that all methods in Patent don\u2019t perform well due to the deep hierarchy and the large number of labels. Furthermore, by examining the results on the Patent dataset, we observe that all methods exhibit similar trends to those on the English dataset, which also confirms the effectiveness of hierarchical classification approaches for Chinese HTC tasks. In the 1-shot, 2shot, and 4-shot settings, Retrieval-style ICL achieves outstanding performance. The EPR also uses a retrieval strategy. Following Rubin et al. (2022), we replicate its model on\nQ\nMethod\nPatent(Depth 4)\nMicro-F1\nMacro-F1\n1\nBERT\n27.04 \u00b1 1.48 (28.37)\n2.40 \u00b1 0.23 (2.67)\nHGCLR\n28.99 \u00b1 1.12 (29.89)\n2.94 \u00b1 0.24 (3.13)\nHPT\n35.22 \u00b1 1.07 (36.26)\n4.22 \u00b1 0.68 (4.87)\nHierVerb\n41.83 \u00b1 0.60 (42.52)\n5.91 \u00b1 0.65 (6.53)\nRetrieval\n47.71 \u00b1 0.41 (48.15)\n10.76 \u00b1 0.25 (11.02)\nRetrieval-style ICL\n52.23 \u00b1 0.23 (52.45)\n15.62 \u00b1 0.17 (15.78)\n2\nBERT\n36.07 \u00b1 0.24 (36.88)\n6.41 \u00b1 0.77 (6.92)\nHGCLR\n36.73 \u00b1 1.13 (38.03)\n6.82 \u00b1 0.21 (7.06)\nHPT\n42.61 \u00b1 0.75 (43.43)\n10.53 \u00b1 0.30 (10.87)\nHierVerb\n48.42 \u00b1 0.39 (48.74)\n12.97 \u00b1 0.39 (13.24)\nRetrieval\n51.63 \u00b1 0.34 (51.91)\n15.12 \u00b1 0.23 (15.32)\nRetrieval-style ICL\n56.84 \u00b1 0.11 (56.93)\n20.07 \u00b1 0.10 (20.17)\n4\nBERT\n49.41 \u00b1 0.98 (50.24)\n9.64 \u00b1 0.56 (10.13)\nHGCLR\n50.24 \u00b1 0.36 (50.63)\n11.40 \u00b1 0.29 (11.67)\nHPT\n53.91 \u00b1 0.44 (54.29)\n18.45 \u00b1 0.40 (18.79)\nHierVerb\n57.58 \u00b1 0.83 (58.64)\n23.28 \u00b1 0.39 (23.63)\nRetrieval\n60.53 \u00b1 0.36 (60.82)\n25.65 \u00b1 0.29 (25.95)\nRetrieval-style ICL\n59.35 \u00b1 0.10 (59.45)\n24.15 \u00b1 0.08 (24.25)\n8\nBERT\n62.10 \u00b1 1.34 (63.29)\n26.85 \u00b1 0.97 (27.75)\nHGCLR\n64.69 \u00b1 0.30 (65.01)\n27.69 \u00b1 0.47 (28.21)\nHPT\n67.35 \u00b1 0.13 (67.45)\n28.39 \u00b1 0.08 (28.46)\nHierVerb\n68.74 \u00b1 0.12 (68.82)\n29.93 \u00b1 0.07 (30.01)\nRetrieval\n69.44 \u00b1 0.10 (69.53)\n30.32 \u00b1 0.07 (30.38)\nRetrieval-style ICL\n65.81 \u00b1 0.05 (65.86)\n27.86 \u00b1 0.04 (27.89)\n16\nBERT\n70.97 \u00b1 0.36 (71.32)\n30.90 \u00b1 0.39 (31.34)\nHGCLR\n71.44 \u00b1 0.38 (71.74)\n31.87 \u00b1 0.05 (31.85)\nHPT\n73.23 \u00b1 0.17 (73.37)\n33.44 \u00b1 0.17 (33.60)\nHierVerb\n75.72 \u00b1 0.11 (75.85)\n34.75 \u00b1 0.10 (34.86)\nRetrieval\n75.94 \u00b1 0.11 (76.06)\n34.95 \u00b1 0.05 (35.00)\nRetrieval-style ICL\n68.73 \u00b1 0.03 (68.76)\n29.82 \u00b1 0.03 (29.85)\n<div style=\"text-align: center;\">Patent(Depth 4)</div>\nTable 3: Micro-F1 and Macro-F1 scores on the Chinese Patent dataset. We reported the average, standard deviation, and best results across three random seeds. Bold: the best result. Underlined: the second highest.\nthe HTC task9, and it demonstrated inferior performance compared to our method. There are probably two factors causing this performance gap. On the one hand, EPR uses the poorly performing GPT-neo 2.7B as the scoring and inference LM and does not fine-tune it during the training process. Especially in the few shot setting, the ability of scoring LM itself has a significant impact on the experimental results. On the other hand, EPR is not proposed for HTC. Therefore, it does not utilize the information of hierarchical relationship between labels, which leads to a mismatch between the retrieved samples and the target sample. Furthermore, based on our observations, we find that the performance of EPR on the simpler dataset DBpedia is even inferior to that on WOS when compared to other methods. This discrepancy could be attributed to the deeper hierarchy of\n9The scoring LM utilized by EPR is GPT-neo, which does not perform well on Chinese. Therefore, we only present experimental results conducted on English datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c01/4c01f9e4-2fe5-400f-b9c4-dfe1cf32d730.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Macro-F1</div>\nDBpedia, which leads to a larger number of labels and increases retrieval difficulty. In contrast, our proposed method incorporates classification objective loss and leverages hierarchical label information, which remains unaffected by these challenges and ensures more robust performance. Observing Table 2, it can be observed that REGEN results, particularly in terms of Macro-F1, exhibit performance gaps compared to other HTC methods. This discrepancy stems from the fact that REGEN does not utilize label hierarchy information and focuses only on predicting leaf nodes. It reflects the significance of considering the label hierarchy structure of HTC.\n# 4.3 Analysis\nThe impact of label descriptions on retrieval. We conduct experiments on different types of label texts: (1) original leaf label text, (2) all text on the label path, and (3) label descriptions generated by LLM. The results are shown in Figure 4. We find that on WOS, (1)>(2)>(3), while on DBpedia and Patent datasets, (1)<(2)<(3). We analyze that it may be due to the shallow hierarchy and small number of labels in the WOS dataset. The label text itself has a high degree of discrimination, so adding additional information leads to a decrease. In contrast, the deeper hierarchy and larger number of labels in the DBpedia and Patent datasets require more information to distinguish the semantic meaning of label text. The experiment proves that label text improves retrieval re-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03a5/03a568bb-1e8e-4b7b-8dc0-ee43dcaa4549.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Results of different contrastive learning strategy on WOS dataset. The x-axis denotes the shot number Q and the y-axis denotes the F1 score.</div>\nFigure 5: Results of different contrastive learning strategy on WOS dataset. The x-axis denotes the shot number Q and the y-axis denotes the F1 score.\nsults. However, which type of label text to use needs to be selected according to the dataset.\nComparison with different contrastive learning strategies. To demonstrate the effectiveness of our divergent contrastive learning, we illustrate the results with three more straightforward losses. CL Hierarchical denotes we calculate Lcon for each hierarchical label representation with random sampling among a batch. CL Leaf Only refers we only calculate loss between leaf label representations with random sampling among a batch. w/o CL means training without Lcon in Equation 4. As shown in Figure 5, our divergent contrastive learning outperforms the others through all the shot numbers. Previous research has shown that contrastive learning is an effective option for training dense retrievers (Jin et al., 2023; Xiong et al., 2021). w/o CL has the lowest performance compared to other contrastive learning methods. As opposed to CL Leaf Only that treats HTC as a flat classification, CL Hierarchical models the label path information. Our divergent contrast learning selects more hard negative samples based on label similarity, further spatially pulling apart the vector distribution of samples with similar labels.\nretrieval-based methods. Previous research on HTC has mainly used classification-based methods, which train classifiers to predict the probability distribution of each label. In contrast, our proposed retrieval-based method predicts labels by calculating similarity with a retrieval database to obtain the most similar text and corresponding la-\nQ\nClassification\nRetrieval\n1\nMicro-F1\n63.25 \u00b1 2.17 (65.61)\n63.46 \u00b1 2.30 (65.99)\nMacro-F1\n49.91 \u00b1 2.43 (52.65)\n50.24 \u00b1 2.21 (52.66)\n2\nMicro-F1\n69.09 \u00b1 0.57 (69.74)\n69.85 \u00b1 0.63 (70.58)\nMacro-F1\n58.49 \u00b1 0.46 (59.04)\n58.64 \u00b1 0.58 (59.25)\n4\nMicro-F1\n74.48 \u00b1 0.74 (75.34)\n75.37 \u00b1 0.70 (76.08)\nMacro-F1\n65.78 \u00b1 0.60 (66.36)\n65.94 \u00b1 0.57 (66.41)\n8\nMicro-F1\n78.36 \u00b1 0.15 (78.48)\n79.04 \u00b1 0.48 (79.53)\nMacro-F1\n70.55 \u00b1 0.34 (70.93)\n70.59 \u00b1 0.52 (71.04)\n16\nMicro-F1\n80.92 \u00b1 0.21 (81.06)\n81.12 \u00b1 0.26 (81.38)\nMacro-F1\n73.88 \u00b1 0.21 (74.08)\n73.72 \u00b1 0.17 (73.82)\nTable 4: The results of classification-based and retrieval-based methods on WOS dataset. We reported the average, standard deviation, and best results across three random seeds. Bold: the best result.\nMethod\nLevel\nDBpedia\nQ=1\nQ=16\nMicro-F1\nMacro-F1\nMicro-F1\nMacro-F1\nBERT\n1\n17.60\n5.13\n98.42\n94.55\n2\n14.02\n0.31\n93.81\n90.69\n3\n11.17\n0.10\n90.13\n85.92\nEPR\n1\n23.51\n11.51\n71.40\n69.51\n2\n8.59\n8.38\n53.71\n48.74\n3\n7.06\n6.51\n43.67\n39.71\nRetrieval\n1\n98.34\n95.08\n98.75\n96.58\n2\n93.05\n89.85\n94.93\n91.17\n3\n89.14\n87.62\n90.66\n87.01\nTable 5: Results at different hierarchy levels on DBpedia dataset.\nbels. Therefore, we replaced our retrieval prediction with classifier prediction while keeping other settings consistent, and compared classificationbased and retrieval-based methods. The results are shown in Table 4. We find that under the few-shot setting, the retrieval-based method outperformed the classification-based method, although the gap gradually decreased with an increasing number of training samples. We speculate that in settings with a small number of samples, the classifier may not be well-trained, while index vectors generated during the retrieval process have better semantic representations due to the rich semantic knowledge of pre-trained models. The retrievalbased method that utilizes similarity matching can achieve relatively better performance, especially in terms of Micro-F1. Concurrently, we compare the classic classification method BERT, the retrieval-based method EPR, and our method on DBpedia which has a deeper hierarchy. Table 5 illustrates the perfor-\nQ\nWOS\nMicro-F1\nMacro-F1\n16\nBERT\n70.42 \u00b1 3.43 (74.07)\n57.38 \u00b1 7.98 (65.36)\nHiMatch\n72.67 \u00b1 4.97 (77.64)\n61.80 \u00b1 7.89 (69.80)\nHGCLR\n71.93 \u00b1 4.48 (76.41)\n60.72 \u00b1 5.83 (67.63)\nHPT\n73.85 \u00b1 4.33 (78.18)\n65.02 \u00b1 6.70 (73.39)\nHierVerb\n75.63 \u00b1 3.80 (79.62)\n64.77 \u00b1 7.60 (73.39)\nEPR\n39.66 \u00b1 5.17 (44.90)\n15.67 \u00b1 7.10 (23.13)\nRetrieval\n79.42 \u00b1 3.82 (83.81)\n69.02 \u00b1 5.56 (74.92)\nTable 6: Results of randomly sampled 16-shot setting on WOS.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/36a9/36a9277e-ab77-42a8-80bf-17d6f19be32d.png\" style=\"width: 50%;\"></div>\n# Impact of imbalanced few-shot sampling\nsample few-shot training set with Algorithm 1, where we enforce balanced control over each type of samples. We now explore the impact of a bias training set and replace the step 13 to 17 in Algorithm 1 to:\nSampleN = Random(0, Min(C[yi], Q)) S = S \u222aRandom Sample(C[yi], SampleN).\nThe report the results on WOS dataset in Table 6. We observe that the average F1 values of all the methods decrease and the random sampling approach makes a wider range of results. Our method could keep in lead and delivers a more stable performance.\nComparison with zero-shot setting on LLM. The quality of instances in the ICL prompt directly affects the results of ICL inference. We compared\nQ\nModel\nMicro-F1\nMacro-F1\n1\nBERT (Vanilla FT)\n2.99 \u00b1 20.85 (5.12)\n0.16 \u00b1 0.10 (0.24)\nLlama-7B (Seq2Seq FT)\n42.76 \u00b1 1.30 (44.10)\n31.89 \u00b1 1.24 (33.20)\nRetrieval (Top1)\n63.46 \u00b1 2.30 (65.99)\n50.24 \u00b1 2.21 (52.66)\nLlama-7B (Top3) ICL\n65.61 \u00b1 0.17 (65.80)\n36.50 \u00b1 0.48 (37.01)\nChatGPT (Top3) ICL\n68.91 \u00b1 0.48 (69.38)\n57.41 \u00b1 0.40 (57.82)\n2\nBERT (Vanilla FT)\n46.31 \u00b1 0.65 (46.85)\n5.11 \u00b1 1.31 (5.51)\nLlama-7B (Seq2Seq FT)\n45.66 \u00b1 0.10 (45.77)\n41.26 \u00b1 0.11 (41.39)\nRetrieval (Top1)\n69.85 \u00b1 0.63 (70.58)\n58.64 \u00b1 0.58 (59.25)\nLlama-7B (Top3) ICL\n67.09 \u00b1 0.19 (67.29)\n54.70 \u00b1 0.30 (55.03)\nChatGPT (Top3) ICL\n71.68 \u00b1 0.09 (71.76)\n61.99 \u00b1 0.10 (62.08)\n4\nBERT (Vanilla FT)\n56.00 \u00b1 4.25 (57.18)\n31.04 \u00b1 16.65(33.77)\nLlama-7B (Seq2Seq FT)\n59.02 \u00b1 0.08 (59.10)\n52.63 \u00b1 0.07 (52.66)\nRetrieval (Top1)\n75.37 \u00b1 0.70 (76.08)\n65.94 \u00b1 0.57 (66.41)\nLlama-7B (Top3) ICL\n71.68 \u00b1 0.09 (71.77)\n60.22 \u00b1 0.04 (60.26)\nChatGPT (Top3) ICL\n75.62 \u00b1 0.15 (75.78)\n66.34 \u00b1 0.09 (66.41)\n8\nBERT (Vanilla FT)\n66.24 \u00b1 1.96 (67.53)\n50.21 \u00b1 5.05 (52.60)\nLlama-7B (Seq2Seq FT)\n69.78 \u00b1 0.05 (69.83)\n63.22 \u00b1 0.04 (63.26)\nRetrieval (Top1)\n79.04 \u00b1 0.48 (79.53)\n70.59 \u00b1 0.52 (71.04)\nLlama-7B (Top3) ICL\n75.03 \u00b1 0.04 (75.07)\n63.58 \u00b1 0.03 (63.61)\nChatGPT (Top3) ICL\n76.93 \u00b1 0.05 (76.98)\n67.54 \u00b1 0.04 (67.57)\n16\nBERT (Vanilla FT)\n75.52 \u00b1 0.32 (76.07)\n65.85 \u00b1 1.28 (66.96)\nLlama-7B (Seq2Seq FT)\n78.42 \u00b1 0.19 (78.66)\n70.09 \u00b1 0.06 (70.15)\nRetrieval (Top1)\n81.12 \u00b1 0.26 (81.38)\n73.72 \u00b1 0.17 (73.82)\nLlama-7B (Top3) ICL\n76.43 \u00b1 0.05 (76.48)\n65.56 \u00b1 0.04 (65.60)\nChatGPT (Top3) ICL\n8.62 \u00b1 0.03 (78.65)\n69.56 \u00b1 0.03 (69.59)\nTable 7: The Micro-F1 and the Macro-F1 scores of the Llama model on WOS dataset. We reported the average, standard deviation, and best results across three random seeds. Bold: best result.\nthe impact of retrieval on ICL inference under different settings, and the results are shown in Figure 6. We distinguish between different methods using different lines, where the soild line represents the retrieval-based methods and the dashed line represents the LLM-based methods. LLM w/o ICL refers to the situation where no examples are provided, and only the test document and the label set are given to the large model for inference. In other words, under the zero-shot setting, the inference relies entirely on the strong ability of LLM. Due to the large and complex label space, it is difficult to input it to the large model for inference at once. Therefore, LLM w/o ICL also uses iterative inference, sequentially inputting the label corresponding sub-clusters. We find that even under zero-shot setting, the large model still demonstrate strong performance, with 55.40% Micro-F1 and 44.79% Macro-F1, which even outperform classification results of vanilla fine-tuned BERT under 4-shot.\nels. We also apply a powerful open access LLM base model Llama-7B for comparison. The results are shown in Table 7. Llama-7B (Seq2Seq FT) means we fine-tune the pre-trained Llama on our few-shot training set to generate hierar-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b84a/b84aaedd-259c-4a8f-adc8-0905342148cd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: A case that LLM fails to choose the correct label although the retrieved Top1 result is right.</div>\nchical labels with a sequence-to-sequence target. Llama-7B (Top3) ICL means we use the ICL with our retrieved top 3 demonstrations on the fixed Llama model without fine-tune. The intricate architecture and extensive parameters of Llama-7B contribute to its superior performance over BERT (110M) in fine-tuning scenarios. In contrast, our retrieval model, built upon the BERT architecture with 110M parameters, consistently outperforms the fine-tuned results of Llama-7B. In extremely few shot settings (such as Q= 1, 2, 4), applying ICL on LLM with our retrieved results leads to further performance improvement, with more powerful models like ChatGPT typically demonstrating superior results. When Q grows, our retrieval methods could outperform LLM-based ICL. Llama-7B (Top3) ICL shows only marginal improvement compared to the Retrieval (Top1) result in 1-shot setting, implying that the degree of enhancement in ICL inference results is contingent upon the performance strength of the LLM.\nIn extremely few shot settings (such as Q= 1, 2, 4), applying ICL on LLM with our retrieved results leads to further performance improvement, with more powerful models like ChatGPT typically demonstrating superior results. When Q grows, our retrieval methods could outperform LLM-based ICL. Llama-7B (Top3) ICL shows only marginal improvement compared to the Retrieval (Top1) result in 1-shot setting, implying that the degree of enhancement in ICL inference results is contingent upon the performance strength of the LLM.\nThe improvement limitation of ICL inference. We present the Top1 and Top3 retrieval results10 in Figure 6. For the retrieval-style ICL method, if we only provide the Top1 example retrieved, the ICL inference result will be consistent with Top1 retrieval result. Therefore, we present the results of Retrieval Top2 + ICL and Retrieval Top3 + ICL in Figure 6. We show the results of constructing the candidate set without employing the filtering strategy, labeled as Retrieval Top3 w/o filter + ICL. When Q = 1, the Top3 retrieved labels are unique, rendering the results identical to Retrieval Top3 + ICL. When Q = 2, the Top3 retrieved labels typically encompass only two categories, leading to a scenario where one label (often the Top1 label) appears twice in the demonstration selections, introducing a bias in the inference process. As a result, Retrieval Top3 w/o filter + ICL slightly underperforms compared to Retrieval Top2 + ICL. When Q \u22654, the Top3 retrieved labels usually belong to a single category, aligning the outcomes with those of Retrieval Top1. Ideally, the ICL method can select the label closest to the gold-standard label from the candidate label set based on the provided examples. Taking Top3 as an example, the Retrieval Top3 + ICL curve should be close to the Retrieval Top3 curve. In fact, the result curves of ICL are all round Retrieval Top1, and the curve of Retrieval Top2 + ICL is closer to Retrieval Top1 than Retrieval Top3 + ICL. We analyze several possible reasons as follows: (1) Firstly, the LLM is not fine-tuned, and its understanding of labels may be inconsistent with the training set. (2) The effect of ICL is limited. The quality of retrieval examples is getting strong, resulting in increasingly similar candidate labels provided, which may increase the difficulty of the LLM inference. Figure 7 shows a case that LLM fails to choose the correct label, although the retrieved Top1 result is right. (3) Although LLM has demonstrated strong ability, there is still room for improvement. Using a more powerful LLM may yield better results. It can be concluded that our retrieval-style ICL method is far superior to direct inference using LLM, and can improve the performance on retrieval-based inference under ex10Top3 selects the label with the highest overlap with the\n10Top3 selects the label with the highest overlap with the gold-standard label among the top3 retrieved labels as the predicted label result.\nQ\nWOS\nMicro-F1\nMacro-F1\n0\nLLM + iterative\n55.40\n44.79\n-w/o iterative\n26.70\n16.44\n1\nRandom Samples + ICL\n56.42\n45.24\nRetrieval (Top3) + ICL\n68.91\n57.41\n-w/o iterative\n68.52\n57.06\n-w/o similar samples\n64.75\n52.27\n-w/o pruning\n60.37\n47.43\n-w/o candidate label set\n52.35\n32.66\nTable 8: Results of different prompt settings on WOS. w/o means \u2019without\u2019. Bold: best result.\ntremely low resources. However, enhancing the retrieval results cannot continuously improve the performance of ICL.\nThe impact of different prompts on LLM inference. The differences in prompts directly affect the results of LLM inference. We conduct ablation experiments on the prompts we proposed to verify the rationality of our iterative prompts, and the results are shown in Table 8. Under the zero-shot setting, when compared to directly inputting all hierarchical label paths to the language model, the iterative method improves the Micro-F1 by 28.70% and the Macro-F1 by 28.35%. It helps alleviate the negative impact of excessively long prompts during the inference process. This indicates that the iterative method is particularly effective for handling HTC tasks. Taking 1-shot as an example, we randomly select three samples in training dataset to form the prompt and use all labels from the target hierarchy layer as the candidate label set for iterative prediction. Interestingly, even with prompts constructed from random samples, the results obtained through ICL outperform LLM + iterative. This finding emphasizes the effectiveness of the ICL approach in generating inference results that closely match the desired format. Then, we use our Top3 retrieval reulst as demonstrations and conduct four ablation comparisons. The first one is to remove the iterative operation, which means that the candidate label set consists of label paths, and all hierarchical labels are predicted at once. The second one is to remove all similar samples, and only provide the current label and candidate label set of the test document. The third one is to remove the pruning operation, which means that the candidate label set consists\nAnnotation Methods\nMicro-F1\nAvg. Time (s)\n(1) direct classification\n67.25\n37.2\n(2) with label description\n73.00\n45.8\n(3) 16-shot retrieval-assisted\n86.44\n9.1\nTable 9: Statistical results of different annotation methods on WOS. Avg. Time indicates the average time (s) spent annotating each instance.\nof all child labels of the current label. The last one is to remove the candidate label set and let the LLM select the most similar text, and use the label of the similar test as the label of the test document. The results prove that our prompts is reasonable, and each part of the prompt has a positive effect on inference11.\nthermore, we recruit non-experts to annotate a portion of the test dataset, aiming to ascertain the expected upper-bound performance. We conduct experimental analyses on the WOS dataset as examples. We recruit college students with proficient English and conduct a simple annotation test, selecting nine annotators with comparable levels of annotation skill and efficiency, who are then divided into groups of three for subsequent annotation tasks. We randomly select 200 instances from the WOS test dataset for annotation. Three annotation methods are employed: (1) providing only the full list of labels; (2) based on (1), supplying an explanation for each label; (3) based on (2), offering the Top3 similar examples assisted by a retriever model trained on the 16-shot setting for annotation. Each annotation method is carried out by three annotators, with the final annotation results produced by voting. The statistical results of different annotation methods are presented in Table 9. Annotation method (1) represents the upper-bound result based on human knowledge under the 0-shot setting. A comparison reveals that after providing label descriptions, the Micro-F1 increases by 5.75%, but the average annotation time also lengthens due to the provision of more information. When assisted by the Top3 examples provided by a retriever trained in the 16-shot setting, the Micro-F1 significantly improves by 13.44%, and the average annotation time is reduced to only\n11All details of prompts will be publicly available, thus enhancing the reproducibility of our work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5b3d/5b3d0979-f1c6-46e5-870e-793d29a68700.png\" style=\"width: 50%;\"></div>\none-fifth of that for method (2), as many clearly incorrect labels are eliminated, reducing the difficulty of annotation. This indicates that our retrieval method can assist human annotation, effectively improving the quality of human annotations and reducing the time required.\nVisualization of index vector. Finally, we use T-SNE (Van der Maaten and Hinton, 2008) to visualize the changes in [P] of the WOS test dataset before and after training, as shown in Figure 8. We find that index vectors exhibit clear hierarchical clustering characteristics, further demonstrating the effectiveness of our method.\n# 5 Conclusion\nIn this paper, we proposed a retrieval-style ICL framework for few-shot HTC. We uniquely identify the most relevant demonstrations from a retrieval database to improve ICL performance and meanwhile designed an iterative policy to inference hierarchical labels sequentially, significantly reducing the number of candidate labels. The retrieval database is achieved by using a HTC labelaware representation for any given input, enabling the differentiation of semantically-closed labels (especially the leaf adjacent labels). The representation learning is implemented by continual training on a PLM with three carefully-designed objectives including MLM, layer-wise classification, and a novel DCL objective. We conducted experiments on three benchmark\ndatasets to evaluate our method. The results show that our method is highly effective, which is able to gain large improvements among a serious of baselines. Finally, our method can bring the stateof-the-art results in few-shot HTC on the three datasets. Further, we performed comprehensive analysis for deep understanding of our method, spreading various important factors. This work still includes several unresolved problems, which might be addressed in the future. Firstly, LLMs are currently confined to expanding text via label descriptions and their application to full training set expansion has not been effective. In order to fully utilize LLMs in text expansion, we need further optimization. Second, the performance gap between supervised methods and our ICL-based approach appears to diminish with increasing training dataset size, suggesting the need for further analysis.\n# Acknowledgements\nWe thank the anonymous reviewers for their helpful comments. This work is supported by the National Natural Science Foundation of China (No. 62176180).\n# References\nRahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. 2013. Multi-label learning with millions of labels: recommending advertiser bid phrases for web pages. In 22nd International World Wide Web Conference, WWW \u201913, pages 13\u201324.\nRami Aly, Steffen Remus, and Chris Biemann. 2019. Hierarchical multi-label classification of text with capsule networks. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28 - August 2, 2019, Volume 2: Student Research Workshop, pages 323\u2013330. Association for Computational Linguistics.\nRohan Bhambhoria, Lei Chen, and Xiaodan Zhu. 2023. A simple and effective framework for strict zero-shot hierarchical classification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1782\u20131792. Association for Computational Linguistics.\naibin Chen, Qianli Ma, Zhenxi Lin, and Jiangyue Yan. 2021. Hierarchy-aware label semantics matching network for hierarchical text classification. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4370\u20134379, Online. Association for Computational Linguistics.\nJunfan Chen, Richong Zhang, Yongyi Mao, and Jie Xu. 2022. Contrastnet: A contrastive learning framework for few-shot text classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10492\u2013 10500.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pre-trained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 657\u2013 668, Online. Association for Computational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT learn in-context? language models secretly perform gradient descent as metaoptimizers. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4005\u20134019. Association for Computational Linguistics.\nacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u2013 4186. Association for Computational Linguistics.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022. OpenPrompt: An opensource framework for prompt-learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 105\u2013113, Dublin, Ireland. Association for Computational Linguistics.\nAndrew Drozdov, Nathanael Sch\u00a8arli, Ekin Aky\u00a8urek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2023. Compositional semantic parsing with large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335.\nYu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023. Mitigating label biases for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 14014\u201314031. Association for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 3816\u20133830. Association for Computational Linguistics.\nSanghun Im, Gibaeg Kim, Heung-Seon Oh, Seongung Jo, and Donghwan Kim. 2023. Hierarchical text classification as sub-hierarchy sequence generation. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative\nApplications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 12933\u201312941. AAAI Press. Ke Ji, Yixin Lian, Jingsheng Gao, and Baoyuan Wang. 2023. Hierarchical verbalizer for fewshot hierarchical text classification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2918\u20132933, Toronto, Canada. Association for Computational Linguistics. Qiao Jin, Andrew Shin, and Zhiyong Lu. 2023. LADER: log-augmented dense retrieval for biomedical literature search. In Proceedings of SIGIR 2023, pages 2092\u20132097. ACM. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Daphne Koller and Mehran Sahami. 1997. Hierarchically classifying documents using very few words. In Proceedings of the Fourteenth International Conference on Machine Learning (ICML 1997), Nashville, Tennessee, USA, July 8-12, 1997, pages 170\u2013178. Morgan Kaufmann. Kamran Kowsari, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, Matthew S. Gerber, and Laura E. Barnes. 2017. Hdltex: Hierarchical deep learning for text classification. In 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017, Cancun, Mexico, December 18-21, 2017, pages 364\u2013371. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4644\u20134668, Toronto, Canada. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.\nachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pages 100\u2013114. Association for Computational Linguistics.\nLaurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(11).\nuning Mao, Jingjing Tian, Jiawei Han, and Xiang Ren. 2019. Hierarchical text classification with reinforced label assignment. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 445\u2013455. Association for Computational Linguistics.\newon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048\u201311064. Association for Computational Linguistics.\nShengwen Peng, Ronghui You, Hongning Wang, Chengxiang Zhai, Hiroshi Mamitsuka, and Shanfeng Zhu. 2016. Deepmesh: deep semantic representation for improving large-scale mesh indexing. Bioinform., 32(12):70\u201379.\nThomson Reuters. 2012. Web of science.\nhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for incontext learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u2013 2671, Seattle, United States. Association for Computational Linguistics.\nDebaditya Shome and Kuldeep Yadav. 2023. Exnet: Efficient in-context learning for dataless text classification. CoRR, abs/2305.14622.\nKoustuv Sinha, Yue Dong, Jackie Chi Kit Cheung, and Derek Ruths. 2018. A hierarchical neural attention-based text classifier. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 817\u2013 823, Brussels, Belgium. Association for Computational Linguistics.\nKai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long shortterm memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31,\n2015, Beijing, China, Volume 1: Long Papers, pages 1556\u20131566. The Association for Computer Linguistics.\nYue Wang, Dan Qiao, Juntao Li, Jinxiong Chang, Qishen Zhang, Zhongyi Liu, Guannan Zhang, and Min Zhang. 2023b. Towards better hierarchical text classification with data generation. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 7722\u20137739. Association for Computational Linguistics.\nZihan Wang, Peiyi Wang, Lianzhe Huang, Xin Sun, and Houfeng Wang. 2022a. Incorporating hierarchy into text encoder: a contrastive learning approach for hierarchical text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7109\u20137119, Dublin, Ireland. Association for Computational Linguistics.\nihan Wang, Peiyi Wang, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui, and Houfeng Wang. 2022b. HPT: Hierarchy-aware prompt tuning for hierarchical text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3740\u20133751, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\nYue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, and Chao Zhang. 2023. Regen: Zero-shot text classification via training data generation with progressive dense retrieval. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 11782\u201311805. Association for Computational Linguistics.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.\nJiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2023. TIM: teaching large language models to translate with comparison. CoRR, abs/2307.04408.\nHaoxing Zhang, Xiaofeng Zhang, Haibo Huang, and Lei Yu. 2022a. Prompt-based metalearning for few-shot text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1342\u20131357, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nRuohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, and Likun Lei. 2023. Long-tailed extreme multi-label text classification by the retrieval of generated pseudo label descriptions. In Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 1062\u20131076. Association for Computational Linguistics.\nXinyi Zhang, Jiahao Xu, Charlie Soh, and Lihui Chen. 2022b. LA-HCN: label-based attention for hierarchical multi-label text classification neural network. Expert Syst. Appl., 187:115922.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022c. Active example selection for incontext learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u20139148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nFei Zhao, Zhen Wu, Liang He, and Xin-Yu Dai. 2023. Label-correction capsule network for hierarchical text classification. IEEE ACM Trans. Audio Speech Lang. Process., 31:2158\u20132168.\ne Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang, Pengjun Xie, and Gongshen Liu. 2020. Hierarchy-aware global model for hierarchical text classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1106\u20131117. Association for Computational Linguistics.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n",
    "paper_type": "method",
    "attri": {
        "background": "Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. Existing methods often struggle with unsatisfactory performance in this setting, particularly due to the expansive hierarchical label sets and extremely ambiguous labels. This paper introduces the first in-context learning (ICL)-based framework with large language models (LLMs) for few-shot HTC, which leverages a retrieval database to identify relevant demonstrations and an iterative policy to manage multi-layer hierarchical labels.",
        "problem": {
            "definition": "The problem addressed in this paper is the inefficiency of existing methods in few-shot hierarchical text classification, which is hindered by the complexity of hierarchical label structures and the scarcity of training samples.",
            "key obstacle": "The main obstacle is the expansive hierarchical label sets and the semantic similarity between adjacent labels, which complicates the selection of relevant demonstrations for effective learning."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that retrieval-based methods could enhance classification tasks with limited samples, especially in the context of hierarchical classification.",
            "opinion": "The proposed idea involves using a retrieval-style framework to enhance in-context learning for few-shot hierarchical text classification by identifying the most relevant demonstrations.",
            "innovation": "The primary innovation of this method is the integration of retrieval mechanisms with ICL, allowing for more effective label prediction by reducing the number of candidate labels through an iterative inference process."
        },
        "method": {
            "method name": "Retrieval-style In-Context Learning",
            "method abbreviation": "RICL",
            "method definition": "RICL is a framework that utilizes retrieval techniques to enhance in-context learning for hierarchical text classification by identifying relevant demonstrations and managing label inference iteratively.",
            "method description": "The core of RICL involves constructing a retrieval database to find the most relevant demonstrations and applying an iterative policy to predict hierarchical labels layer by layer.",
            "method steps": [
                "Train a pretrained language model-based indexer and build a retrieval database.",
                "Perform similarity searches in the retrieval database for the input text.",
                "Construct ICL prompts using the top K similar demonstrations for hierarchical label prediction."
            ],
            "principle": "RICL is effective because it leverages the rich semantic knowledge embedded in pretrained models and reduces the complexity of label selection through an iterative approach."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three benchmark datasets: Web-of-Science (WOS), DBpedia, and a Chinese patent dataset, using micro-F1 and macro-F1 metrics for evaluation.",
            "evaluation method": "The performance of the proposed method was assessed by comparing it against several baseline methods, measuring the effectiveness of the retrieval-style ICL approach in few-shot settings."
        },
        "conclusion": "The proposed retrieval-style ICL framework for few-shot HTC shows significant improvements over existing methods, achieving state-of-the-art results on benchmark datasets and demonstrating the effectiveness of combining retrieval techniques with in-context learning.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved performance in few-shot settings, stability across different random seeds, and the ability to effectively manage hierarchical label inference.",
            "limitation": "Limitations include the reliance on the quality of the retrieval database and the potential for performance gaps when training datasets are large.",
            "future work": "Future research should focus on optimizing the use of LLMs for text expansion and exploring methods to further bridge the performance gap between supervised and ICL-based approaches."
        },
        "other info": {
            "acknowledgements": "This work is supported by the National Natural Science Foundation of China (No. 62176180).",
            "dataset_info": {
                "WOS": {
                    "levels": 2,
                    "number_of_documents": 46985
                },
                "DBpedia": {
                    "levels": 3,
                    "number_of_documents": 381025
                },
                "Patent": {
                    "levels": 4,
                    "number_of_documents": 30104
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is introduced as a framework for few-shot hierarchical text classification (HTC) leveraging large language models (LLMs)."
        },
        {
            "section number": "1.3",
            "key information": "This paper presents the first ICL-based framework for few-shot HTC, emphasizing the role of LLMs in enhancing classification tasks with limited samples."
        },
        {
            "section number": "3",
            "key information": "The proposed retrieval-style ICL framework utilizes retrieval techniques to enhance in-context learning for hierarchical text classification."
        },
        {
            "section number": "3.1",
            "key information": "The method manages label inference iteratively, which allows LLMs to adapt to the complexity of hierarchical label structures."
        },
        {
            "section number": "4",
            "key information": "The paper discusses how the design of retrieval-based prompts can significantly influence the outcomes of in-context learning for hierarchical classification."
        },
        {
            "section number": "6",
            "key information": "Limitations include the reliance on the quality of the retrieval database and potential performance gaps when training datasets are large."
        },
        {
            "section number": "7",
            "key information": "The proposed retrieval-style ICL framework shows significant improvements over existing methods, achieving state-of-the-art results and demonstrating the effectiveness of combining retrieval techniques with in-context learning."
        }
    ],
    "similarity_score": 0.7027281644996307,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification.json"
}