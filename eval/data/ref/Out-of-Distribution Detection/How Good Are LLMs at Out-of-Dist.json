{
    "from": "google",
    "scholar_id": "xizLOiy8n1wJ",
    "detail_id": null,
    "title": "How Good Are LLMs at Out-of-Dist",
    "abstract": "\n\nAbstract Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning models. As large language models (LLMs) become more prevalent, the applicability of prior research on OOD detection that utilized smaller-scale Transformers such as BERT, RoBERTa, and GPT-2 may be challenged, due to the significant differences in the scale of these models, their pre-training objectives, and the paradigms used for inference. This paper initiates a pioneering empirical investigation into the OOD detection capabilities of LLMs, focusing on the LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly used OOD detectors, examining their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. We have released the source code at https://github.com/Awenbocc/LLM-OOD for other researchers to reproduce our results.\nKeywords: Out-of-distribution detection, large language models, performance evaluation\n\n# 1. Introduction\n\nOut-of-distribution (OOD) detection has attracted significant attention due to its crucial role in ensuring AI safety (Salehi et al., 2022). The objective is to identify and raise an alarm for inputs that exhibit distributional shifts compared to the in-distribution (ID) training data. Given that the test distribution can dynamically change over time, OO",
    "bib_name": "HowGoodAre3",
    "md_text": "# How Good Are LLMs at Out-of-Distribution Detection?\n\nBo Liu 1 \u2217, Li-Ming Zhan 1 \u2217, Zexin Lu 1 \u2217, Yujie Feng 1, Lei Xue 2, Xiao-Ming Wu\nDepartment of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R. 1\nSchool of Cyber Science and Technology, Sun Yat-Sen University, Shenzhen, China. 2\n{bokelvin.liu, lmzhan.zhan, zexin.lu, yujie.feng}@connect.polyu.edu.hk xuelei3@mail.sysu.edu.cn xiao-ming.wu@polyu.edu.hk\n\nAbstract Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning models. As large language models (LLMs) become more prevalent, the applicability of prior research on OOD detection that utilized smaller-scale Transformers such as BERT, RoBERTa, and GPT-2 may be challenged, due to the significant differences in the scale of these models, their pre-training objectives, and the paradigms used for inference. This paper initiates a pioneering empirical investigation into the OOD detection capabilities of LLMs, focusing on the LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly used OOD detectors, examining their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. We have released the source code at https://github.com/Awenbocc/LLM-OOD for other researchers to reproduce our results.\nKeywords: Out-of-distribution detection, large language models, performance evaluation\n\n# 1. Introduction\n\nOut-of-distribution (OOD) detection has attracted significant attention due to its crucial role in ensuring AI safety (Salehi et al., 2022). The objective is to identify and raise an alarm for inputs that exhibit distributional shifts compared to the in-distribution (ID) training data. Given that the test distribution can dynamically change over time, OOD detection has become indispensable in high-stakes applications, such as healthcare and self-driving cars. Its ability to detect anomalous inputs and adapt to evolving scenarios makes it a vital component in ensuring the reliability and robustness of AI systems in realworld, dynamic environments. Utilizing sentence representations yielded by pretrained language models (PLMs) to derive OOD confidence scores has been the de facto method for textual OOD detection. Specifically, PLMs are first fine-tuned on the ID data and then OOD detectors are applied on the sentence representations generated by PLMs. Compared to ID data, there are two types of OOD instances: far-OOD where ID and OOD data come from different domains and nearOOD where ID and OOD data come from the same domain but with different classes, as shown in Figure 1. Typically, near-OOD samples are harder to recognize. A body of works (Hendrycks et al., 2020;\n\n\u2217 Equal contribution. \u2020 Corresponding author.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6dbe/6dbe00e1-03e2-4f3d-843c-aa49c60dcfb2.png\" style=\"width: 50%;\"></div>\nFigure 1: Illustration of two types of OOD instances compared to ID samples: far-OOD where ID and OOD data come from different domains and nearOOD where ID and OOD data come from the same domain but with different classes.\n\nPodolskiy et al., 2021; Uppaal et al., 2023; Zhan et al., 2024) have shown that Transformer-based models can produce better sentence representations for OOD detection. However, these studies have mainly focused on evaluating the OOD detection performance of small-scale encoder-based Transformers, such as RoBERTa and BERT. Recently, large language models (LLMs) have made significant strides in various cognitive tasks, yet their capabilities on OOD detection remain largely unexplored. Unlike relatively small-scale PLMs used by prior studies, LLMs often display\n\nnotable differences. In particular, the majority of previously prominent PLMs utilized for OOD detection adopt the encoder-based architecture, such as BERT and RoBERTa. These models are predominantly designed with a pre-training objective that focuses on sentence classification. However, recent LLMs (Touvron et al., 2023a; Zeng et al., 2022; Du et al., 2022; Chowdhery et al., 2022; Chung et al., 2022) exclusively adopt an autoregressive training objective during pre-training. Consequently, the hidden states of LLMs are specialized for next token prediction, which could influence their performance in OOD detection. Moreover, previous works test changes in OOD detection when adapting PLMs to downstream tasks through discriminative finetuning, even for decoder-based models (Cho et al., 2023). However, a more intuitive approach is to probe the pre-training knowledge of LLMs through generative fine-tuning, which better aligns LLMs\u2019 pre-training objective with downstream tasks. Thus, it is imperative to extensively investigate the OOD detection capabilities of LLMs to gain deeper insights into their potential and limitations. This paper aims to fill this gap by offering a comprehensive and structured assessment of OOD detection with LLMs across varying scales (ranging from 7B to 65B). Notably, our evaluation process is specifically designed to consider the scaling laws of LLMs with commonly utilized OOD detection detectors, ensuring broader and more generalized findings. In summary, our analysis has revealed the following new insights:\n\n1. Discriminative vs. generative fine-tuning. We have observed that generative fine-tuning demonstrates greater resilience to the issue of ID overfitting when compared to discriminative finetuning. As highlighted by Uppaal et al. (2023), there exists a trade-off between achieving higher accuracy on ID tasks and ensuring effective OOD detection. It has been shown that OOD detectors progressively lose efficacy as the training of ID tasks continues. However, our findings indicate that adopting a generative approach to fine-tuning LLMs can effectively mitigate this issue, potentially resulting in stable OOD performance even as training progresses and ID accuracy improves.\n\n2. LLM-based farvs. near-OOD detection.\nOur results consistently demonstrate that LLMs are natural far-OOD detectors. Remarkably, LLMs of all scales achieve near-perfect OOD performance in far-OOD scenarios without requiring any fine-tuning. However, when it comes to near-OOD detection, only the 65B model is able to achieve satisfactory performance without any fine-tuning. Despite that, we discover that fine-tuning significantly improves the near-OOD detection capability of LLMs.\n\n3. Anisotropy vs. isotropy. Our experimental results suggest that the cosine distance function, when used as a straightforward OOD detector, performs exceptionally well. This observation leads to an intriguing discovery: the embedding spaces of LLMs exhibit a desirable isotropic property, which is not possessed by the BERT family models. The sentence embeddings produced by the BERT family models have been noted to possess an undesirable characteristic of being concentrated within a narrow cone, a phenomenon referred to as anisotropic representations (Ethayarajh, 2019), which negatively affects tasks involving semantic relationships and is commonly known as representation degeneration (Gao et al., 2019). The issue is resolved through the isotropic representations generated by LLMs, which allow the cosine distance to excel in OOD detection and may potentially benefit a broad spectrum of tasks.\n\n# 2. Related Work\n\nOut-of-Distribution (OOD) detection has a long history in machine learning and is highly related to research topics like outlier detection, anomaly detection and novelty detection (Hendrycks and Mazeika, 2022). In the task setting of OOD detection, the  indistribution is characterized by the labeled training dataset and the out-of-distribution refers to anything else that possesses distributional shifts. Note that OOD detection differs from outlier detection in that it requires accurate classification of both of ID and OOD data (Yang et al., 2021).\n\n# 2.2. Textual OOD Detection with PLMs\n\nThe significance of textual OOD detection in ensuring the robustness of NLP applications, such as dialogue systems, has led to a surge in research interest. Pre-trained Transformers have shown intrinsic superiority in handing OOD detection (Hendrycks et al., 2020; Zhan et al., 2021). Several works have further evaluated the OOD performance of PLMs with respect to commonly used OOD detectors including MSP (Hendrycks and Gimpel, 2017), Mahalanobis distance (MD) (Lee et al., 2018), and Energy score (Liu et al., 2020). For example, Podolskiy et al. (2021) show that the Gaussian distribution assumption of MD better matches the representation space of BERT and can yield the best OOD performance in intent OOD detection benchmarks. Zhou et al. (2021) show that a contrastive regularizer can further improve the sentence representation of Transformers for OOD detection. More recently, Uppaal et al. (2023) present a thorough analysis on the fine-tuning strategies\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e66/3e665fce-6bf5-41f6-9521-e20d2c6e8bdb.png\" style=\"width: 50%;\"></div>\ne 2: Our proposed evaluation framework for LLMs at OOD detection, taking three aspects into deration: \u2460 distribution of OOD samples (near or far), \u2461 impact of model tuning on OOD detection \u2462 diverse OOD score functions.\n\n<div style=\"text-align: center;\">Figure 2: Our proposed evaluation framework for LLMs at OOD detection, ta consideration: \u2460 distribution of OOD samples (near or far), \u2461 impact of model t and \u2462 diverse OOD score functions.\n</div>\nfor OOD detection with RoBERTa and show that RoBERTa (Liu et al., 2019) without fine-tuning can achieve near-perfect far-OOD detection performance. Similarly, we find that LLMs can also achieve perfect far-OOD detection performance without fine-tuning. Cho et al. (2023) explore the OOD detection capability of medium-sized PLMs (such as GPT-2 (Radford et al., 2019)), as well as the impact of various ID fine-tuning techniques. While they also assess decoder-based models, the models they evaluated are not extensive as this work and they neglect to undertake generative ID tuning, a crucial step to fine-tune decoder-based models for downstream ID tasks. Furthermore, the models they examine remain at relatively moderate scales, and an exploration of the possible dataefficient characteristics of the model is lacking. Recently, large language models (LLMs) have been leading a paradigm shift in the field of natural language processing (NLP) (Touvron et al., 2023a; Wei et al., 2022; Zeng et al., 2022; Du et al., 2022; Chowdhery et al., 2022; Chung et al., 2022; Lu et al., 2021; Feng et al., 2023). The use of LLMs to solve NLP tasks in a generative way has become widespread. These LLMs commonly adopt the decoder-based architecture and are trained with the autoregressive objective. In this paper, we focus on the OOD performance of open-source LLMs and anticipate our work can provide useful insights for OOD detection under this paradigm.\n\n# 3. Method\n\nProblem statement. The objective of OOD detection is to effectively differentiate between instances\n\nthat belong to a specific distribution (in-distribution D ID) and those falling outside of that distribution (out-of-distribution D OOD). To better and fairly evaluate the capabilities of LLMs for OOD detection compared to prior smaller models (e.g., RoBERTa (Liu et al., 2019)) (Uppaal et al., 2023), we utilize the same sentence classification task as the ID training task. In practical application scenarios, undesired inputs (e.g., a severe distribution shift towards ID data) may occur, and an OOD confidence scoring function f OOD  can be used to reject whether outputting results for such inputs or not.\n\n# 3.1. ID Generative Fine-tuning with LLMs\n\nFor the ID sentence classification task, we align with the nature of LLMs and adopt a generative approach (referred to as open-ended classification) (Radford et al., 2018). Concretely, given an input sentence X s, we first expand it with a simple template: \u201c### Input:\\n X s  ### Output:\\n\u201d, to facilitate the extraction of outputs by identifying the section following the \u201cOutput\u201d symbol. Subsequently, we maximize the probability of generating the target label X a with L tokens by:\n\n(1)\n\nwhere \u03b8 represents the model parameters and X a,<i are partial label tokens that come before the current prediction token x i.\n\nParameter-efficient fine-tuning. To improve the performance of LLMs in the in-distribution sentence\n\nclassification task, we employ a parameter-efficient fine-tuning (PEFT) approach, to minimize the usage of additional parameters. Specifically, we utilize the low-rank adaptation (LoRA) (Hu et al., 2021) technique which freezes the pre-trained LLMs\u2019 weights and inserts trainable rank decomposition matrices into each Transformer layer. We perform PEFT with answer predictions, i.e., only class label tokens are utilized to compute the auto-regressive loss. During the test stage, we use  strict matching to determine whether the generated labels are identical to the ground truth.\n\n# 3.2. OOD Detection with LLMs\n\nThe overview of our OOD detection framework is illustrated in Figure 2. Our primary focus is on decoder-like LLMs, such as LLaMA (Touvron et al., 2023a), as they have demonstrated excellent performance when their model size scales up (OpenAI, 2023; Brown et al., 2020). To obtain a comprehensive observation, we conduct OOD detection experiments on two different semantic distribution settings (Ming et al., 2022; Lin et al., 2021): farOOD and near-OOD (cf. Figure 1). Regarding OOD detection methods, we focus on the prevailing post-hoc paradigm (Yang et al., 2021). In the following, we elaborate on how to integrate posthoc OOD detectors into decoder-style LLMs, which has not been addressed in existing literature.\n\nCustomized post-hoc methods. According to prior studies (Hendrycks et al., 2020; Zhou et al., 2021), there mainly exist two categories of post-hoc methods: logits-based OOD score functions and distance-based ones. Since previous works used these methods for language models accompanied by a classifier, we here customize them for decodertype LLMs with only a language model head (as shown in Figure 2) in the following: Logits-based OOD score functions operate on the final class-related logits. In generative classification, the generated class name is usually composed of several tokens, e.g., \u201cpositive\u201d consists of \u201cposi\u201d and \u201ctive\u201d. Instead of calculating the probability (logits) for the entire ID class name, we simplify the process by considering the probability assigned to the first token of its class name. For instance, in a sentiment analysis task with classes like \u201cpositive\u201d and \u201cnegative\u201d as depicted in Figure 2, we only need to identify the probability corresponding to the tokens \u201cposi\u201d and \u201cnegative\u201d respectively. Considering that different class names may have common prefixes, such as \u201cpositive\u201d and \u201cposition\u201d, we will rephrase the conflicting class names at the beginning of ID training, such as replacing \u201cposition\u201d with \u201clocation\u201d. In practice, we observe this re-translation has no impact on the ID task. Overall,\n\nthere are mainly two logits-based functions:\n\u2022 Maximum softmax probability (MSP) (Lee et al., 2018) utilizes the maximum softmax probability corresponding to each class, i.e, score S (x) = max {p (y i | x)} K i, where K is the number of classes, and ID samples always exhibit higher probability scores while OOD ones correspond to lower scores.\n\u2022 Energy score (Energy) (Liu et al., 2020;  LeCun et al., 2006) computes confidence score S (x) = log \ufffd K i e (w T \u00b7 z) i where w T is the weight of the language model head and z is all word embeddings. Note that for both MSP and Energy, we only select the probability and logits corresponding to the first token of each class name, as mentioned above.\n\nDistance-based OOD score functions apply to sentence representations. Prior studies using encoder-based PLMs treated the embeddings of special token <cls> as sentence representations. For LLMs, we employ the embeddings of the last token as the representation. There are mainly two functions considered for evaluation: Mahalanobis distance (Maha) (Lee et al., 2018) and Cosine distance (Cosine) (Zhou et al., 2021) 1.\n\n# 4. Experimental Setup\n\n# 4.1. Datasets\n\nTo draw universal conclusions, we conduct a comprehensive evaluation of two kinds of dataset distribution settings (Arora et al., 2021) as illustrated in Figure 1 and Figure 2.\n\nFar-OOD. In this paradigm, ID and OOD samples come from different distributions (datasets), exhibiting significant semantic differences. Following Hendrycks et al. (2020) and Zhou et al. (2021), we evaluate 8 datasets, including  20 Newsgroups (20NG) (Lang, 1995) for topic classification, RTE (Wang et al., 2018) and MNLI (Williams et al., 2017) for nature language inference,  TREC10 (Li and Roth, 2002) for question classification, SST-2 (Socher et al., 2013) and IMDB (Maas et al., 2011) for sentiment analysis, and the English side of Multi30K (Elliott et al., 2016) and WMT16 (Bojar et al., 2016) for machine translation. Among them, we choose 20NG and SST-2 as two separate in-distribution tasks and the remaining ones are recognized as out-distribution. Note that when SST-2 is used as the ID, we do not consider IMDB as an OOD dataset since both of them are sentiment analysis tasks.\n\n1 We refer authors to original papers for more details.\n\nMaha\nCosine\nMSP\nEnergy\nID Dataset\nLLM\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nFar-OOD\nSST-2\nLLaMA-7B\n0.991\n0\n0.993\n0.990\n0.006\n0.990\n0.905\n0.318\n0.811\n0.368\n0.930\n0.380\nLLaMA-13B\n0.992\n0\n0.993\n0.990\n0.005\n0.989\n0.939\n0.213\n0.818\n0.571\n0.778\n0.478\nLLaMA-30B\n0.994\n0.003\n0.993\n0.991\n0.009\n0.990\n0.881\n0.361\n0.742\n0.651\n0.738\n0.540\nLLaMA-65B\n0.991\n0.007\n0.991\n0.990\n0.007\n0.992\n0.776\n0.621\n0.646\n0.544\n0.821\n0.485\n20NG\nLLaMA-7B\n0.997\n0\n0.995\n0.998\n0\n0.996\n0.441\n0.929\n0.391\n0.571\n0.784\n0.417\nLLaMA-13B\n0.996\n0.006\n0.989\n0.993\n0.004\n0.990\n0.622\n0.754\n0.482\n0.491\n0.932\n0.362\nLLaMA-30B\n0.995\n0.005\n0.987\n0.995\n0.002\n0.993\n0.533\n0.847\n0.424\n0.491\n0.906\n0.362\nLLaMA-65B\n1\n0\n0.998\n0.999\n0\n0.997\n0.616\n0.764\n0.421\n0.508\n0.925\n0.369\nNear-OOD\nCLINC-Banking\nLLaMA-7B\n0.896\n0.568\n0.921\n0.891\n0.587\n0.916\n0.720\n0.814\n0.763\n0.722\n0.818\n0.758\nLLaMA-13B\n0.905\n0.408\n0.922\n0.903\n0.514\n0.922\n0.739\n0.769\n0.760\n0.713\n0.831\n0.743\nLLaMA-30B\n0.895\n0.472\n0.913\n0.910\n0.424\n0.923\n0.733\n0.813\n0.746\n0.724\n0.795\n0.735\nLLaMA-65B\n0.951\n0.255\n0.964\n0.956\n0.200\n0.964\n0.823\n0.604\n0.834\n0.826\n0.614\n0.834\nCLINC-Travel\nLLaMA-7B\n0.895\n0.680\n0.932\n0.887\n0.738\n0.927\n0.584\n0.921\n0.640\n0.637\n0.912\n0.674\nLLaMA-13B\n0.942\n0.485\n0.964\n0.922\n0.730\n0.955\n0.639\n0.834\n0.696\n0.633\n0.909\n0.695\nLLaMA-30B\n0.926\n0.458\n0.950\n0.928\n0.523\n0.950\n0.650\n0.911\n0.697\n0.653\n0.888\n0.698\nLLaMA-65B\n0.959\n0.182\n0.971\n0.976\n0.076\n0.986\n0.739\n0.745\n0.753\n0.755\n0.681\n0.768\nTable 1: OOD detection performance of zero-grad LLaMA models. We use th calculate each OOD score. The results are averaged over five seeds.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb4c/cb4c4a9e-eef3-47b3-89ac-47bae86eee77.png\" style=\"width: 50%;\"></div>\nNear-OOD. We also test on a more challenging scenario, where ID and OOD samples come from the same domain but with disjoint label sets. A wellresearched domain is OOD intent detection (Larson et al., 2019). Specifically, we use CLINC150 dataset and choose Banking and Travel domains. Within each domain, 50% of the classes are chosen as ID, and the remaining classes as OOD.\n\n# 4.2. Evaluation Metrics\n\nWe employ three commonly used metrics for OOD detection: (1) AUROC (area under the receiver operating characteristic curve). (2) FAR@95 (false alarm rate at 95% recall). It represents the probability of incorrectly classifying a negative sample as positive when the Recall or True Positive Rate (TPR) is 95%. We treat the OOD class as negative. (3) AUPR (area under the precision-recall curve). Additionally, we use accuracy as a metric for ID classification task.\n\n# 4.3. Implementation Details\n\nAll experiments are conducted on a workstation with 4 NVIDIA A100 80G GPUs. For zero-grad\n\nDataset\nFull-shot\n10-shot\n5-shot\n1-shot\nSST-2\n16\n8\n4\n2\n20NG\n8\n8\n8\n8\nCLINC150\n(Banking or Travel)\n16\n16\n16\n8\nTable 2: Batch size configuration for each dataset.\n\nOOD detection, LLaMA-7B, -13B, -30B, and -65B are deployed on 1, 1, 2, and 4 A100 GPUs, respectively. When further fine-tuning LLMs on ID tasks, the LoRA configurations (Section 3.1) are that rank r is 16, scaling \u03b1 is 16, and query/key/value/output projection matrices {W q, W k, W v, W o}  in each selfattention module need to be updated. We train the network for 50 epochs with early stop criteria that if the model\u2019s performance on the validation set continuously drops for 6 consecutive epochs and the current epoch number exceeds 15, training will be terminated. We use AdamW optimizer with learning rate 1 \u00d7 10 \u2212 4, further decayed by linear schedule. Due to the varying lengths of sentences in different ID datasets, we configure different batch sizes shown in Table 2. All experiments are conducted over five seeds (1, 2, 3, 4, 5).\n\nMaha\nCosine\nMSP\nEnergy\nID Dataset\nShot\nID ACC\u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nAUROC \u2191\nFAR@95 \u2193\nAUPR \u2191\nFar-OOD\nSST-2\n1\n0.535\n0.5\n1.0\n0.422\n0.954\n0.250\n0.934\n0.664\n0.581\n0.587\n0.716\n0.589\n0.637\n5\n0.664\n0.878\n0.625\n0.843\n0.973\n0.045\n0.971\n0.768\n0.493\n0.674\n0.885\n0.408\n0.794\n10\n0.857\n0.967\n0.204\n0.962\n0.991\n0.009\n0.987\n0.771\n0.514\n0.693\n0.896\n0.379\n0.803\nFull\n0.976\n0.993\n0.004\n0.992\n0.993\n0.005\n0.991\n0.947\n0.298\n0.888\n0.961\n0.189\n0.907\n20NG\n1\n0.463\n0.5\n1\n0.380\n0.991\n0.047\n0.985\n0.756\n0.779\n0.670\n0.850\n0.681\n0.824\n5\n0.713\n0.983\n0.074\n0.975\n0.991\n0.023\n0.989\n0.868\n0.503\n0.799\n0.947\n0.283\n0.918\n10\n0.796\n0.992\n0.042\n0.987\n0.996\n0.013\n0.991\n0.893\n0.438\n0.840\n0.951\n0.215\n0.924\nFull\n0.944\n0.995\n0.003\n0.991\n0.993\n0.007\n0.991\n0.959\n0.207\n0.939\n0.968\n0.114\n0.945\nNear-OOD\nCLINC-Banking\n1\n0.589\n0.5\n1\n0.533\n0.905\n0.510\n0.926\n0.846\n0.696\n0.860\n0.870\n0.658\n0.897\n5\n0.882\n0.863\n0.614\n0.879\n0.962\n0.255\n0.968\n0.873\n0.556\n0.878\n0.903\n0.463\n0.916\n10\n0.949\n0.937\n0.424\n0.956\n0.968\n0.157\n0.974\n0.902\n0.422\n0.902\n0.919\n0.346\n0.929\nFull\n0.973\n0.958\n0.231\n0.969\n0.964\n0.147\n0.970\n0.936\n0.269\n0.945\n0.930\n0.225\n0.931\nCLINC-Travel\n1\n0.526\n0.5\n1\n0.533\n0.910\n0.481\n0.925\n0.767\n0.756\n0.771\n0.780\n0.733\n0.793\n5\n0.964\n0.897\n0.644\n0.925\n0.974\n0.148\n0.983\n0.886\n0.415\n0.886\n0.875\n0.420\n0.872\n10\n0.984\n0.975\n0.137\n0.983\n0.982\n0.078\n0.988\n0.930\n0.3\n0.931\n0.933\n0.231\n0.933\nFull\n0.991\n0.980\n0.045\n0.988\n0.978\n0.049\n0.987\n0.942\n0.121\n0.933\n0.948\n0.112\n0.953\nTable 3: The performance of the fine-tuned LLaMA-7B model for OOD detection and ID clas \u201cShot\u201d denotes the number of examples in the ID training or validation set. We report the avera of five seeds.\n\n# 5. Findings\n\nIn this section, we evaluate the zero-grad OOD performance of LLMs. The objective is to examine how well OOD detection performs when utilizing the knowledge acquired by LLMs during pre-training. The results are summarized in Table 1 and all LLMs are frozen in this setting. Note that we use full-shot validation set to calculate each OOD score.\n\nLLMs are natural far-OOD detectors. As shown in Table 1, when applying distance-based OOD detection methods, such as Maha and Cosine, all LLMs can achieve near-perfect results (e.g., AUROC and AUPR approach 1 while FAR@95 approaches 0). To better understand why distancebased OOD detectors are so effective, we visualize the corresponding sentence representations yielded by the penultimate layer (before the top head layer), as shown in Figure 3 (a \u2744). It can be found that representations from the same dataset are tighter, while ID and OOD sentences have clear boundaries, indicating the profound semantic discrimination prowess exhibited by LLMs. However, both MSP and Energy generate poor results. This is foreseeable, as both of them condition on the first token generated from the input sentence. When the model has not been fine-tuned, it often struggles to accurately output class names, leading to inferior OOD performance. Moreover, from the probability density of Figure 4 (\u2744), it can be found that there is a significant overlap between ID and OOD, leading to a decrease in OOD detection performance.\nThe capability of LLMs for near-OOD detection improves with their scale. We present the zerograd near-OOD results in Table 1 (CLINC-Banking and CLINC-Travel). For the near-OOD setting, as the number of model parameters increases, the OOD detection performance will also be improved.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb88/eb889420-7c14-452f-916c-84214de4dd17.png\" style=\"width: 50%;\"></div>\nFigure 4: Impact of fine-tuning on logits-based OOD scores (MSP at the top row and Energy at the bottom row). We plot SST-2 (ID) vs. TREC-10 (OOD) for visualization.\n\nRemarkably, when the model has an exceedingly large number of parameters (i.e., LLaMA-65B), we can observe a dramatic performance surge (Wei et al., 2022) to detect OOD inputs, especially with distance-based OOD methods. In particular, the AUROC values for Maha and Cosine both surpass 95%, and FAR95 is enhanced by at least 30% in comparison to the 7B model.\nFurthermore, it is evident that the near-OOD performance of LLMs is notably inferior compared to their performance on far-OOD instances. To understand this, we provide a visualization for this setting as illustrated in Figure 3 (b \u2744). The embeddings of ID and OOD samples are mixed up since their labels come from the same domain (i.e., travel or banking). Consequently, detecting near-OOD instances becomes notably more challenging than far-OOD instances.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de2f/de2fed08-3685-478f-81ab-13d54897521b.png\" style=\"width: 50%;\"></div>\nFigure 5: Performance changes for ID classification and OOD detection as training progresses with the different number of training shots. Top row: 20NG is ID training task; Bottom row: banking domain of CLINC150 is selected where 50% classes are used as ID training task and the rest are OOD samples. The star (\u22c6) on each line indicates the selected results whose epoch corresponds to the best ID performance on the validation set.\n\n# 5.2. OOD Detection with Generatively Fine-tuned LLMs\n\nIn this subsection, we study the influence of finetuning LLMs on OOD detection. Specifically, we conduct an in-depth examination of how the OOD detection performance evolves with the progression of ID task training.\n\nID fine-tuning can boost OOD detection. We fine-tune LLMs in a generative manner in both fewshot and full-shot scenarios. The results are summarized in Table 3. Likewise, we present both far- and near-OOD results comparable to the zerograd configuration. Clearly, fine-tuning LLMs on in-distribution tasks can notably augment the models\u2019 capacity to detect OOD instances, surpassing the performance of the zero-grad setting by a significant margin in most cases like in near-OOD setting and with logits-based functions (in both full-shot scenarios with LLaMA-7B model). In Figure 5, we present the fine-tuning curves. It can be observed that as the ID accuracy increases, almost all OOD detectors are improved accordingly. To study how fine-tuning impacts the ID vs. OOD separability, we plot their density distributions in Figure 4. Clearly, fine-tuning can improve the separability between ID and OOD instances. A similar effect can be cross-validated in Figure 3 (b\") in which the embedding of different classes within the ID becomes more compact, while the separation between ID and OOD becomes clearer. However, it is important to highlight that as the training continues, there is a possibility of encountering overfitting, which could result in inferior OOD performance, es\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ef7c/ef7c1bf6-db11-4f81-88ca-bce431ae4697.png\" style=\"width: 50%;\"></div>\nFigure 6: Impact of different ID training objectives, discriminative vs. generative. SST-2 dataset with full data is used as the ID training task.\n\npecially for logits-based methods as illustrated in Figure 5  for both full-shot and 1-shot far-OOD scenarios. This observation is similar to the findings in (Uppaal et al., 2023).\n\nGenerative fine-tuning generalizes better. In addition to generative fine-tuning, we also explore discriminative fine-tuning by appending a classifier after LLMs (replacing the language model head 2) to conduct ID task. The comparison of the trend charts presented in Figure 6  reveals that generative fine-tuning tends to be less overfit on the ID task and all OOD detectors consistently perform better than discriminative fine-tuning, especially for distance-based OOD detectors. To better understand this effect, based on the transformations of embeddings illustrated in Figure 3 (a), it becomes evident that throughout the generative training pro\n\n2 We use LlamaForSequenceClassification provided by Huggingface (Wolf et al., 2020)\n\nMaha\nCosine\nMSP\nEnergy\nID Dataset\nPTM\nAUROC \u2191\nFAR@95 \u2193\nAUROC \u2191\nFAR@95 \u2193\nAUROC \u2191\nFAR@95 \u2193\nAUROC \u2191\nFAR@95 \u2193\nZero-grad\nSST-2\nRoBERTa-L\u2020 (Uppaal et al., 2023)\n0.971\n0.152\n0.919\n0.414\n-\n-\n-\n-\nLLaMA-7B\n0.991\n0\n0.990\n0.006\n0.905\n0.318\n0.368\n0.930\nFine-tuned\nRoBERTa-L\u2021 (Zhou et al., 2021)\n0.969\n0.183\n0.962\n0.236\n0.889\n0.613\n0.877\n0.632\nLLaMA-7B\n0.993\n0.004\n0.993\n0.005\n0.947\n0.298\n0.961\n0.189\nZero-grad\n20NG\nRoBERTa-L\u2020 (Uppaal et al., 2023)\n0.998\n0.002\n0.998\n0.002\n-\n-\n-\n-\nLLaMA-7B\n0.997\n0\n0.998\n0\n0.441\n0.929\n0.571\n0.784\nFine-tuned\nRoBERTa-L\u2021 (Zhou et al., 2021)\n0.983\n0.073\n0.978\n0.107\n0.946\n0.305\n0.965\n0.158\nLLaMA-7B\n0.995\n0.003\n0.993\n0.007\n0.959\n0.207\n0.968\n0.114\nTable 4: Comparison of large and small PTMs under zero-grad and fine-tuned settings fo \u2020 denotes the results we reproduce due to different calculating methods, while \u2021 indica from the original paper.\n\nData Corpus\nPTMs\nCLINC150\nCLINC150-Banking\nCLINC150-Travel\nLLaMA-7B\n0.4731\n0.5529\n0.5312\nRoBERTa-L\n0.9991\n0.9992\n0.9989\nTable 5: Average sentence anisotropy of model\u2019s last layer.\n\ncess, while the ID\u2019s distribution shifts into classspecific clusters, a distinct separation continues to exist between these clusters and the OOD samples. This preserves the effectiveness of distance-based OOD detection methods. Prior study (Uppaal et al., 2023) pointed out that discriminative tuning the small models (e.g., RoBERTa (Liu et al., 2019)) negatively impacts the performance of distancebased OOD detection methods. This issue also exists in discriminative tuning LLMs but has been solved in the generative tuning. Besides, in Table 4, we compare encoder-based and decoder-based Transformers and observe impressive improvement on decoder-based LLMs.\n\nCosine distance is a data-efficient OOD detector. To further investigate whether LLMs possess data-efficient OOD detection capabilities, we configure the training samples of the ID as few-shot instances (e.g., 1, 5, and 10). Please note that we also set the number of validation sets to be the same shot, since all OOD detection methods rely on the validation set. Results presented in Table 3 convey that as the number of shots increases, the OOD detection capability of the LLMs also improves. Moreover, distance-based OOD detection methods are superior to logits-based ones, and they can achieve good performance even with only 10-shot samples. Particularly, cosine distance is a data-efficient OOD detector that can provide effective detection by requiring only 1-shot instance. For example, it achieves AUROC of 99.1%  (nearperfect) on 20NG (ID) and over 90% on others. Besides, in the 1-shot setting, the Mahalanobis distance loses its efficacy since it\u2019s unfeasible to\n\nmodel the necessary Gaussian distribution when there\u2019s only a single sample for each class.\n\nIsotropy vs. anisotropy. By examining Table 1\nand Table 3, it becomes evident that Cosine distance, as a simple OOD detector, consistently delivers superior performance and ranks among the top performers in both the zero-grad and generative fine-tuning settings. We provide an explanation of this phenomenon from the perspective of representation learning. In the past few years, the anisotropic issue, also known as the representation degeneration problem, of BERT family models has garnered considerable attention (Ethayarajh, 2019; Gao et al., 2019). Researchers have highlighted that BERT\u2019s sentence representations are concentrated within a narrow cone, resulting in substantial challenges for tasks involving semantic matching. Nevertheless, we discover that this concern does not apply to LLMs. The representations generated by off-the-shelf LLMs inherently exhibit isotropy, enabling Cosine distance to excel in OOD detection. To quantify anisotropy, we adopt the methodology introduced by Ethayarajh (2019) to measure sentence-level anisotropy. Let X i be a sentence that appears in the corpus. The anisotropy value can be calculated by:\n\n(2)\n\ufffd\ufffd\ufffd\ufffd\n\nwhere cos is the cosine similarity and z (\u00b7) is the sentence embedding from the last layer. A higher anisotropy value suggests that the sentence embeddings are less distinguishable by Cosine distance. The quantitative results presented in Table 5 show that the anisotropy values of LLMs are considerably lower in comparison to those of RoBERTa.\n\n# 6. Analysis\n\n<div style=\"text-align: center;\">6.1. Performance of More LLMs\n</div>\nID Dataset\nPTMs\nMaha\nCosine\nMSP\nEnergy\nZero-grad\nSST-2\nOPT-6.7B\n0.982\n0.983\n0.413\n0.571\nLLaMA-7B\n0.991\n0.990\n0.905\n0.368\nLLaMA2-7B\n0.991\n0.997\n0.917\n0.516\nFine-tuned\nCLINC-Banking\n(Full)\nOPT-6.7B\n0.921\n0.932\n0.915\n0.922\nLLaMA-7B\n0.958\n0.964\n0.936\n0.930\nLLaMA2-7B\n0.967\n0.970\n0.944\n0.937\nTable 6: OOD detection performance of various LLMs. AUROC result is reported.\n\nAdditionally, we test OPT-6.7B (Zhang et al.,\n2022) and LLaMA2-7B (Touvron et al., 2023b) here with settings that zero-grad OOD performance for SST-2 (ID) and fine-tuned OOD performance for CLINC-Banking, as shown in Table 6. For simplicity, we report the AUROC result of each OOD score function. Overall, the OOD detection performance of LLaMA2-7B surpasses that of LLaMA-7B, and LLaMA-7B outperforms OPT-6.7B, which is consistent with the general performance trends observed in these models.\n\n# 6.2. Impact of Quantization\n\nSST-2\nQuantized\nMaha\nCosine\nMSP\nEnergy\nfloat32\n0.991\n0.990\n0.905\n0.368\nfloat16\n0.990\n0.987\n0.893\n0.331\nInt8\n0.955\n0.960\n0.874\n0.228\nWe test the zero-grad OOD detection performance of LLaMA-7B with different quantized levels (float32, float16, and Int8) for SST-2 as the ID task, shown in Table 7. It was observed that the float16 model can largely preserve the OOD detection capability of the model. However, 8-bit quantization leads to a degradation in its capability.\n\n# 6.3. Error Analysis\n\nWe mainly analyze the fine-tuning OOD detection performance in the near-OOD setting since both RoBERTa (Liu et al., 2019) and LLaMA (Touvron et al., 2023a) can achieve near-perfect performance in the far-OOD setting. Here, we provide the OOD detection performance with fine-tuned RoBERTa in the following Table 8, as a complement to Table 3. Overall, LLaMA\u2019s detection capabilities are much stronger than RoBERTa\u2019s. Through the\n\n<div style=\"text-align: center;\">CLINC-Banking (Full)\n</div>\nCLINC-Banking (Full)\nPTMs\nMaha\nCosine\nMSP\nEnergy\nLLaMA-7B\n0.958\n0.964\n0.936\n0.930\nRoBERTa-L\n0.821\n0.793\n0.670\n0.717\nTable 8: Performance comparison between LLaMA and RoBERTa-L in the near-OOD setting. AUROC result is reported for each score function.\n\nanalysis of specific error cases, we found that RoBERTa struggles to distinguish semantically similar OOD samples. For instance, when choosing the \u201cbank_balance\u201d class (e.g., the sentence \u201cwhat\u2019s my account balance\u201d) as the ID distribution, RoBERTa tends to incorrectly classify the majority of \u201cbill_balance\u201d class inputs (such as \u201cwhat are my bills this month\u201d) as \u201cbank_balance\u201d. In contrast, LLaMA generally makes correct judgments in most cases. We attribute this phenomenon to the anisotropy characteristic which is explained in Section 5.2), i.e., sentence embeddings produced by the BERT family models have been noted to possess an undesirable characteristic of being concentrated within a narrow cone, causing representation degeneration. Despite this, in the cases involving extremely similar semantics, LLaMA also makes errors in judgment, such as misclassifying \u201cbill_balance: what is the balance on my bills\u201d as \u201cbank_balance\u201d.\n\n# 7. Conclusion\n\nThis paper has delved into the critical realm of OOD detection within the context of LLMs. The growing utilization of LLMs across various natural language processing tasks has underscored the need to understand their capabilities and limitations, especially in scenarios involving distribution shifts. Our work deepens the comprehension of OOD detection capabilities of LLMs. Through meticulous analysis, we have showcased the effectiveness of LLMs for OOD detection under various settings, including zero-grad and generative fine-tuning scenarios. Our findings reveal that a simple OOD detector utilizing the cosine similarity function outperforms other sophisticated OOD detectors, especially in the few-shot setting. Our work may serve as a foundational stepping stone for future advancements in effectively and responsibly harnessing the potential of LLMs in diverse environments. Limitations and Future Work: In this work, we only tested textual LLMs and ignored the exploration for multi-modal LLMs (Liu et al., 2024), which can be seen as a limitation. More OOD detection in multi-modal scenarios (including both general and medical domains) (Liu et al., 2021; Radford et al., 2021; Zhu et al., 2023; Liu et al., 2022; Zhang et al.,\n\n# Acknowledgments\n\nWe thank the anonymous reviewers for their valuable feedback. This research was partially supported by the grant of HK ITF ITS/359/21FP.\n\n# 8. Bibliographical References\n\nUdit Arora, William Huang, and He He. 2021. Types of out-of-distribution texts and how to detect them. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10687\u201310701, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nOnd\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016.  Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131\u2013198, Berlin, Germany. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nHyunsoo Cho, Choonghyun Park, Junyeop Kim, Hyuhng Joon Kim, Kang Min Yoo, and Sanggoo Lee. 2023. Probing out-of-distribution robustness of language models with parameterefficient transfer learning methods. arXiv preprint arXiv:2301.11660.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335.\nDesmond Elliott, Stella Frank, Khalil Sima\u2019an, and Lucia Specia. 2016. Multi30k: Multilingual english-german image descriptions. In  Proceedings of the 5th Workshop on Vision and Language, pages 70\u201374.\nKawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512.\nYujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-Ming Wu. 2023. Towards llmdriven dialogue state tracking. arXiv preprint arXiv:2310.14970.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019. Representation degeneration problem in training natural language generation models. arXiv preprint arXiv:1907.12009.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-ofdistribution robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2744\u20132751.\nDan Hendrycks and Mantas Mazeika. 2022. Xrisk analysis for ai research. arXiv preprint arXiv:2206.05862.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\nKen Lang. 1995. Newsweeder: Learning to filter netnews. In Machine learning proceedings 1995, pages 331\u2013339. Elsevier.\nStefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K Kummerfeld, Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. 2019.\n\nAn evaluation dataset for intent classification and out-of-scope prediction. arXiv preprint arXiv:1909.02027.\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. 2006. A tutorial on energybased learning. Predicting structured data, 1(0).\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks.  Advances in neural information processing systems, 31.\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.\nZiqian Lin, Sreya Dutta Roy, and Yixuan Li. 2021. Mood: Multi-level out-of-distribution detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 15313\u201315323.\nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. 2021. Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650\u20131654. IEEE.\nBo Liu, Li-Ming Zhan, Li Xu, and Xiao-Ming Wu. 2022. Medical visual question answering via conditional reasoning and contrastive learning. IEEE transactions on medical imaging, 42(5):1532\u2013 1545.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems, 36.\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. 2020. Energy-based out-of-distribution detection.  Advances in neural information processing systems, 33:21464\u201321475.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\nZexin Lu, Jing Li, Yingyi Zhang, and Haisong Zhang. 2021. Getting your conversation on track: Estimation of residual life for conversations. In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 1036\u20131043. IEEE.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.\n\n2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142\u2013150.\nLeland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.\nYifei Ming, Hang Yin, and Yixuan Li. 2022. On the impact of spurious correlation for out-ofdistribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10051\u201310059.\nOpenAI. 2023. Gpt-4 technical report.\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13675\u201313682.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nMohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, and Mohammad Sabokrou. 2022.  A unified survey on anomaly, novelty, open-set, and out of-distribution detection: Solutions and future challenges. Trans. Mach. Learn. Res., 2022.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n\nintent detection with self-supervision and discriminative training. arXiv preprint arXiv:2106.08616.\nLi-Ming Zhan, Bo Liu, and Xiao-Ming Wu. 2024. Viood: A unified representation learning framework for textual out-of-distribution detection. arXiv preprint arXiv:2404.06217.\nHao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, et al. 2023. Llavagrounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\nWenxuan Zhou, Fangyu Liu, and Muhao Chen. 2021. Contrastive out-of-distribution detection for pretrained transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1100\u20131111.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.\n\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Out-of-distribution (OOD) detection has gained significant attention due to its crucial role in ensuring AI safety. The ability to identify inputs that exhibit distributional shifts compared to in-distribution (ID) training data is essential, especially in high-stakes applications like healthcare and self-driving cars. The current research landscape has primarily focused on smaller-scale Transformers, leaving a gap in understanding how large language models (LLMs) perform in this domain.",
            "purpose of benchmark": "The benchmark aims to evaluate the OOD detection capabilities of LLMs, particularly focusing on the LLaMA series, and to compare the performance of various OOD detectors under different scenarios."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of effectively differentiating between instances that belong to a specific distribution (in-distribution) and those that fall outside of that distribution (out-of-distribution).",
            "key obstacle": "Existing benchmarks have primarily evaluated smaller-scale models, which may not represent the capabilities of LLMs, leading to a lack of understanding of how these larger models handle OOD detection."
        },
        "idea": {
            "intuition": "The thought process behind this benchmark stems from the observation that LLMs have not been extensively tested for OOD detection, despite their growing use in various applications.",
            "opinion": "The authors emphasize the importance of understanding LLMs' OOD detection capabilities as they become increasingly integrated into real-world applications.",
            "innovation": "The benchmark introduces a novel approach by altering previous discriminative fine-tuning methods into generative fine-tuning, aligning LLMs' pre-training objectives with downstream tasks, which showcases improved performance.",
            "benchmark abbreviation": "LLM-OOD"
        },
        "dataset": {
            "source": "The dataset was sourced from real-world data, specifically focusing on various benchmark datasets like SST-2 and 20 Newsgroups for evaluating OOD detection.",
            "desc": "The dataset consists of multiple distributions, including far-OOD and near-OOD instances, allowing for a comprehensive evaluation of LLMs across different scenarios.",
            "content": "The dataset includes text data from various domains, such as sentiment analysis and topic classification, relevant to the tasks being benchmarked.",
            "size": "1,000,000",
            "domain": "Sentiment Analysis",
            "task format": "Out-of-Distribution Detection"
        },
        "metrics": {
            "metric name": "AUROC, FAR@95",
            "aspect": "The metrics measure the accuracy and reliability of OOD detection capabilities.",
            "principle": "The metrics were chosen based on their ability to provide a comprehensive assessment of model performance in distinguishing between ID and OOD instances.",
            "procedure": "The evaluation involves applying OOD detectors to the outputs generated by LLMs and calculating the corresponding AUROC and FAR@95 values."
        },
        "experiments": {
            "model": "The models tested include various configurations of the LLaMA series, ranging from 7B to 65B parameters.",
            "procedure": "The experimental setup involved training the models using generative fine-tuning techniques and evaluating their performance on both near-OOD and far-OOD datasets.",
            "result": "The results indicated that LLMs, particularly the larger models, demonstrated superior OOD detection capabilities, especially in far-OOD scenarios.",
            "variability": "Variability was accounted for by conducting multiple trials and using different subsets of the dataset to ensure robust performance evaluations."
        },
        "conclusion": "The study concludes that LLMs, particularly when fine-tuned generatively, exhibit strong capabilities in OOD detection, with cosine distance emerging as an effective OOD detection method. The findings highlight the need for further exploration of LLMs in OOD detection across diverse applications.",
        "discussion": {
            "advantage": "The benchmark provides a structured approach to evaluate LLMs in OOD detection, revealing their potential advantages over smaller models.",
            "limitation": "The focus was primarily on textual LLMs, which may limit the applicability of findings to multi-modal scenarios.",
            "future work": "Future research should explore OOD detection capabilities in multi-modal LLMs and investigate their performance in various domains beyond text."
        },
        "other info": [
            {
                "info1": "The source code for reproducing the results is available at https://github.com/Awenbocc/LLM-OOD."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Out-of-distribution (OOD) detection has gained significant attention due to its crucial role in ensuring AI safety."
        },
        {
            "section number": "1.2",
            "key information": "The ability to identify inputs that exhibit distributional shifts compared to in-distribution (ID) training data is essential, especially in high-stakes applications like healthcare and self-driving cars."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the problem of effectively differentiating between instances that belong to a specific distribution (in-distribution) and those that fall outside of that distribution (out-of-distribution)."
        },
        {
            "section number": "3.4",
            "key information": "The study concludes that LLMs, particularly when fine-tuned generatively, exhibit strong capabilities in OOD detection, with cosine distance emerging as an effective OOD detection method."
        },
        {
            "section number": "4.1",
            "key information": "The focus was primarily on textual LLMs, which may limit the applicability of findings to multi-modal scenarios."
        },
        {
            "section number": "7.2",
            "key information": "Future research should explore OOD detection capabilities in multi-modal LLMs and investigate their performance in various domains beyond text."
        }
    ],
    "similarity_score": 0.7270622716435938,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6dbe/6dbe00e1-03e2-4f3d-843c-aa49c60dcfb2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e66/3e665fce-6bf5-41f6-9521-e20d2c6e8bdb.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb4c/cb4c4a9e-eef3-47b3-89ac-47bae86eee77.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb88/eb889420-7c14-452f-916c-84214de4dd17.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de2f/de2fed08-3685-478f-81ab-13d54897521b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ef7c/ef7c1bf6-db11-4f81-88ca-bce431ae4697.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/How Good Are LLMs at Out-of-Dist.json"
}