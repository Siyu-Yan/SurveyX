{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.06216",
    "title": "Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large Language Model-based Recommendation",
    "abstract": "Large language models (LLMs) have gained much attention in the recommendation community; some studies have observed that LLMs, fine-tuned by the cross-entropy loss with a full softmax, could achieve state-of-the-art performance already. However, these claims are drawn from unobjective and unfair comparisons. In view of the substantial quantity of items in reality, conventional recommenders typically adopt a pointwise/pairwise loss function instead for training. This substitute however causes severe performance degradation, leading to under-estimation of conventional methods and over-confidence in the ranking capability of LLMs. In this work, we theoretically justify the superiority of crossentropy, and showcase that it can be adequately replaced by some elementary approximations with certain necessary modifications. The remarkable results across three public datasets corroborate that even in a practical sense, existing LLM-based methods are not as effective as claimed for next-item recommendation. We hope that these theoretical understandings in conjunction with the empirical results will facilitate an objective evaluation of LLMbased recommendation in the future. Our code is available at https: //github.com/MTandHJ/CE-SCE-LLMRec.",
    "bib_name": "xu2024understandingrolecrossentropyloss",
    "md_text": "# Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large Language Model-based Recommendation\nZhangchi Zhu\u2217 East China Normal University Shanghai, China zczhu@stu.ecnu.edu.cn Jun Wang East China Normal University Shanghai, China wongjun@gmail.com\nCong Xu\u2217 East China Normal University Shanghai, China congxueric@gmail.com Zhangchi Zhu\u2217 East China Normal University Shanghai, China zczhu@stu.ecnu.edu.cn\nCong Xu\u2217 East China Normal University Shanghai, China congxueric@gmail.com\nJianyong Wang Tsinghua University Beijing, China jianyong@tsinghua.edu.cn\n22 Feb 2024\nABSTRACT\n# ABSTRACT\nLarge language models (LLMs) have gained much attention in the recommendation community; some studies have observed that LLMs, fine-tuned by the cross-entropy loss with a full softmax, could achieve state-of-the-art performance already. However, these claims are drawn from unobjective and unfair comparisons. In view of the substantial quantity of items in reality, conventional recommenders typically adopt a pointwise/pairwise loss function instead for training. This substitute however causes severe performance degradation, leading to under-estimation of conventional methods and over-confidence in the ranking capability of LLMs. In this work, we theoretically justify the superiority of crossentropy, and showcase that it can be adequately replaced by some elementary approximations with certain necessary modifications. The remarkable results across three public datasets corroborate that even in a practical sense, existing LLM-based methods are not as effective as claimed for next-item recommendation. We hope that these theoretical understandings in conjunction with the empirical results will facilitate an objective evaluation of LLMbased recommendation in the future. Our code is available at https: //github.com/MTandHJ/CE-SCE-LLMRec.\narXiv:2402.06216v2\narXiv:240\n# CCS CONCEPTS\n# \u2022 Information systems \u2192Recommender systems; \u2022 Computer systems organization \u2192Neural networks.\n# KEYWORDS\nACM Reference Format: Cong Xu, Zhangchi Zhu, Jun Wang, Jianyong Wang, and Wei Zhang. 2024. Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large\nACM Reference Format: Cong Xu, Zhangchi Zhu, Jun Wang, Jianyong Wang, and Wei Zhang. 2024. Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large \u2217Equal contribution\n\u2217Equal contribution\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, July 2017, Washington, DC, USA \u00a9 2024 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, July 2017, Washington, DC, USA \u00a9 2024 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a1e/7a1eb2e6-98e3-4522-ba2c-da63ed9e5956.png\" style=\"width: 50%;\"></div>\nFigure 1: Recommendation performance comparisons. The marker size depicts the number of model parameters: 60M for P5 (CID + IID) [19], 7B for LlamaRec [53] and E4SRec [28], and merely \u22641M for SASRec [21].\nLanguage Model-based Recommendation . In Proceedings of ACM Conference (Conference\u201917). ACM, New York, NY, USA, 16 pages. https://doi.org/ XXXXXXX.XXXXXXX\n# 1 INTRODUCTION\nWith the growth of the Internet, the amount of information being generated every moment is far beyond human discernment. Recommender systems are thus developed to help humans quickly and accurately ascertain the items of interest, and have played important roles in diverse applications, including e-commerce [55], online news [13], and education [50]. Due to inherent differences in data types and recommendation goals, different tasks are typically handled using various techniques and separate models. For example, graph neural networks [23] have dominated collaborative filtering [15, 32], while Transformer [48] becomes increasingly popular in sequential recommendation [21, 44]. Recently, the prosperity of Large Language Models (LLMs) [35, 38, 46, 47] suggests a promising direction towards universal recommenders [12, 27]. Equipped with carefully designed prompts, they show great potential in explainable and cross-domain recommendations [9, 10]. Nevertheless, there still exist non-negligible gaps [1, 22] between LLMs and conventional methods unless domainspecific knowledge is injected. Some researches have observed \u2018compelling\u2019 results after fine-tuning [28, 53], and hastily affirmed LLM-based recommenders\u2019 ranking capability.\nHowever, the comparisons therein are not objective and fair enough, leading to under-estimation of conventional recommenders and over-confidence in LLMs. Recall that the next-token prediction objective used for LLM pre-training (and fine-tuning), by its nature, is a cross-entropy loss that needs a full softmax over the entire corpus. In view of the substantial quantity of items in reality, conventional methods typically adopt a pointwise/pairwise loss function (e.g., BCE and BPR). This compromise however causes significant performance degradation. As shown in Figure 1, SASRec trained with cross-entropy outperforms LLMs by a large margin, while falling behind with BCE or BPR. Such superior results relying on cross-entropy however cannot serve as direct evidence to challenge the ranking capability of existing LLM-based recommenders, since the full softmax is intractable to calculate in practice. In this work, we re-emphasize the ability to optimize ranking metrics for a desired recommendation loss, and then unveil the corresponding limitations of some approximations to cross-entropy. To achieve effective and practical approximations, we introduce some novel alternatives with theoretical analysis. In summary, the innovative insights and technical contributions are as follows:\n Minimizing cross-entropy is equivalent to maximizing a lower bound of Normalized Discounted Cumulative Gain (NDCG) and Reciprocal Rank (RR). One can thus expect that the ranking capability would be gradually enhanced as crossentropy is optimized during training. We further show that dynamic truncation on the normalizing term yields a tighter bound and potentially better performance. This fact highlights the importance of optimizing these ranking metrics, and the crossentropy loss is arguably adequate for this purpose. The challenge that remains unsolved is how to realize the approximation in an effective and practical manner, so the comparison with LLMbased recommenders is meaningful in reality. After revisiting the limitations of some well-known approximations, a rather simple solution will be presented.\nsetting fails to optimize a meaningful bound in the early stages of training. Before the advent of subword segmentation algorithms [25], the training of neural language models also struggles to circumvent an explicit normalizing over the entire vocabulary. Mnih et al. [34] thus resorted to a simplified NCE that fixes the normalizing term estimate as a constant value of 1. This suggestion however introduces training difficulties in recommendation: sampling more negative samples should accelerate the training yet the opposite occurs. This intriguing phenomenon is attributed to the weak connection between NCE and NDCG (RR). Because NCE grows exponentially fast w.r.t. the number of positively scored items, a meaningless bound is encountered in the early training stages. This conclusion suggests adjusting the estimate of the normalizing term to a slightly larger value, which shows promising empirical performance but lacks consistent applicability. Next, we introduce a more reliable loss.  Scaling up the sampled normalizing term provides an effective and practical approximation to cross-entropy. Since the normalizing term of cross-entropy is intractable in reality, a direct way is to approximate it by (uniformly) sampling part of items (a.k.a. sampled softmax loss [49]). To further mitigate\nthe magnitude loss caused by sampling, we multiply it by an additional weight so the sampled term is scaled up. Indeed, this modification can also be understood as a special case of importance sampling [2], in which the proposal distribution assigns a higher probability mass to the target item. Unlike NCE, this Scaled Cross-Entropy (dubbed SCE) yields a bound mainly determined by the current rank of the target item, making it meaningful even in the early training stages. Empirically, sampling a very few negative samples per iteration is sufficient to achieve comparable results to using cross-entropy with a full softmax. Based on these approximations for cross-entropy, we conduct a comprehensive investigation to assess the true ranking capability of both conventional and LLM-based recommenders. The experimental results presented in Section 5 suggest the over-confidence in existing LLM-based methods. Even without considering the model sizes, they are still far inferior to conventional methods in next-item recommendation. Apart from the potential of explainability and cross-domain transferability, further investigation and exploration are necessary to assess the true ranking capability of LLM-based recommenders.\nthe magnitude loss caused by sampling, we multiply it by an additional weight so the sampled term is scaled up. Indeed, this modification can also be understood as a special case of importance sampling [2], in which the proposal distribution assigns a higher probability mass to the target item. Unlike NCE, this Scaled Cross-Entropy (dubbed SCE) yields a bound mainly determined by the current rank of the target item, making it meaningful even in the early training stages. Empirically, sampling a very few negative samples per iteration is sufficient to achieve comparable results to using cross-entropy with a full softmax.\nBased on these approximations for cross-entropy, we conduct a comprehensive investigation to assess the true ranking capability of both conventional and LLM-based recommenders. The experimental results presented in Section 5 suggest the over-confidence in existing LLM-based methods. Even without considering the model sizes, they are still far inferior to conventional methods in next-item recommendation. Apart from the potential of explainability and cross-domain transferability, further investigation and exploration are necessary to assess the true ranking capability of LLM-based recommenders.\n# 2 RELATED WORK\nRecommender systems are developed to enable users to quickly and accurately ascertain relevant items. The primary principle is to learn underlying interests from user information, especially historical interactions. Collaborative filtering [16, 40] performs personalized recommendation by mapping users and items into the same latent space in which interacted pairs are close. Beyond static user representations, sequential recommendation [26, 42] focuses on capturing dynamic interests from item sequences. Early efforts such as GRU4Rec [17] and Caser [45] respectively apply recurrent neural networks (RNNs) and convolutional neural networks (CNNs) to sequence modeling. Recently, Transformer [8, 48] becomes increasingly popular in recommendation due to its parallel efficiency and superior performance. For example, SASRec [21] and BERT4Rec [44] respectively employ unidirectional and bidirectional self-attention. Differently, Zhou et al. [56] present FMLP-Rec to denoise the item sequences through learnable filters so that stateof-the-art performance can be obtained by mere MLP modules. LLM for recommendation has gained a lot of attention recently because: 1) The next-token generation feature is technically easy to extend to the next-item recommendation (i.e., sequential recommendation); 2) The immense success of LLM in natural language processing promises the development of universal recommenders. Some studies [7, 10] have demonstrated the powerful zero/fewshot ability of LLMs (e.g., GPT [35]), especially their potential in explainable and cross-domain recommendations [9, 10]. Nevertheless, there is a consensus [1, 22, 54] that without domain-specific knowledge learned by fine-tuning, LLM-based recommenders still stay far behind conventional models. As an early effort, P5 [12] unifies multiple recommendation tasks into a sequence-to-sequence paradigm. Based on the foundation model of T5 [38], each task can be activated through some specific prompts. Hua et al. [19] takes a further step beyond P5 by examining the impact of various ID indexing methods, and a combination of collaborative and independent indexing stands out. Recently,\nmore LLM recommenders [28, 30, 37, 53] based on Llama [46] or Llama2 [47] are developed. For example, LlamaRec [53] proposes a two-stage framework based on Llama2 to rerank the candidates retrieved by conventional models. To enable LLM to correctly identify items, E4SRec [28] incorporates ID embeddings trained by conventional sequential models through a linear adaptor, and applies LORA [18] for parameter-efficient fine-tuning. Cross-entropy and its approximations [2, 3, 14, 31, 40] have been extensively studied. The most related works are: 1) Bruch et al. [4] theoretically connected cross-entropy to some ranking metrics; 2) Wu et al. [49] further found its desirable property in alleviating popularity bias; and recently, 3) Klenitskiy et al. [24] and Petrov et al. [36] respectively applied cross-entropy and a generalized BCE loss to eliminate the performance gap between SASRec and BERT4Rec. Differently, we are to 1) understand the superiority of cross-entropy as well as the limitations of its approximations; 2) identify a viable approximation according to these findings; and 3) facilitate an objective evaluation of LLM-based recommendation by acknowledging the true capability of conventional models.\n# 3 PRELIMINARIES\nGiven a query \ud835\udc5ethat encompasses some user information, a recommender system aims to retrieve some items \ud835\udc63\u2208I that would be of interest to the user. In sequential recommendation, the recommender predicts the next item \ud835\udc63\ud835\udc61+1 based on historical interactions \ud835\udc5e= [\ud835\udc631, \ud835\udc632, . . . , \ud835\udc63\ud835\udc61]. The crucial component is to develop a scoring function \ud835\udc60\ud835\udc5e\ud835\udc63:= \ud835\udc60\ud835\udf03(\ud835\udc5e, \ud835\udc63) to accurately model the relevance of a query \ud835\udc5eto a candidate item \ud835\udc63. A common paradigm is to map them into the same latent space through some models parameterized via \ud835\udf03, followed by an inner product operation for similarity calculation. Then, top-ranked items based on these scores will be prioritized for recommendation. Typically the desired recommender is trained to minimize an objective function over all observed interactions D:\n# min \ud835\udf03 E(\ud835\udc5e,\ud835\udc63+)\u223cD [\u2113(\ud835\udc5e, \ud835\udc63+;\ud835\udf03)],\n# where \ud835\udc63+ indicates the target item for the query \ud835\udc5e, and the loss function \u2113considered in this paper is in the form of\n\u2113(\ud835\udc5e, \ud835\udc63+;\ud835\udf03) := \u2212log exp(\ud835\udc60\ud835\udf03(\ud835\udc5e, \ud835\udc63+)) \ud835\udc4d\ud835\udf03(\ud835\udc5e) = \u2212\ud835\udc60\ud835\udf03(\ud835\udc5e, \ud835\udc63+) + log\ud835\udc4d\ud835\udf03(\ud835\udc5e). (1)\nHere \ud835\udc4d\ud835\udf03(\ud835\udc5e) depicts the \u2018normalizing\u2019 term specified to the query \ud835\udc5e. For clarity, we will omit \ud835\udc5eand \ud835\udf03hereafter if no ambiguity is raised. It is worth noting that the reformulation of Eq. (1) makes it easy to understand the subtle differences between a wide range of loss functions. Table 1 covers a selection of approximations: Binary Cross-Entropy (BCE) and Bayesian Personalized Ranking (BPR) [40] are widely used in recommendation for their low costs; Importance Sampling (IS) [2, 20], Noise Contrastive Estimation (NCE) [14, 34], and NEGative sampling (NEG) [33] are the cornerstones of the subsequent methods [3, 6, 11, 29, 43, 51, 52]. Additional details regarding their mechanisms are provided in Appendix B. Note that in this work we only involve some elementary approximations, as the primary purpose is not to develop a complex loss. For more advanced loss functions, please refer to [5].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba49/ba4971a9-a1a5-4a08-9303-9a9d50de1926.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Performance comparison based on tighter bounds for NDCG. The dashed line represents the results trained by CE (namely the case of \ud835\udf02\u2192+\u221e).</div>\n# 4 THE ROLE OF CROSS-ENTROPY LOSS IN OPTIMIZING RANKING CAPABILITY\nBCE and BPR are commonly employed in training recommender systems due to their high efficiency. However, the substantial performance gaps shown in Figure 1 suggest their poor alignment with cross-entropy. Needless to say, claims based on the comparison with these inferior alternatives are unconvincing. In this section, we are to showcase the substitutability of cross-entropy by 1) highlighting the importance of implicitly optimizing the ranking metrics for a recommendation loss; 2) introducing some practical modifications to boost the effectiveness of some elementary approximations. Due to space constraints, the corresponding proofs are deferred to Appendix C. SASRec [21], one of the most prominent sequential models, will serve as the baseline to empirically elucidate the conclusions in this part. All results are summarized based on 5 independent runs.\n# 4.1 Cross-Entropy for Some Ranking Metrics\nThe capability to prioritize items aligning with the user\u2019s interests is essential for recommender systems. Denoted by \ud835\udc5f+ := \ud835\udc5f(\ud835\udc63+) = |{\ud835\udc63\u2208 I : \ud835\udc60\ud835\udc63\u2265\ud835\udc60\ud835\udc63+}| the predicted rank of the target item \ud835\udc63+, the metric of Normalized Discounted Cumulative Gain (NDCG)1 is often employed to assess the sorting quality. For next-item recommendation considered in this paper, NDCG is simplified to\n( +) It increases as the target item \ud835\udc63+ is ranked higher, and reaches the maximum when \ud835\udc63+ is ranked first (i.e., \ud835\udc5f+ = 1). Consequently, the average quality computed over the entire test set serves as an indicator of the ranking capability. Notably, Reciprocal Rank (RR) is another popular ranking metric, and we leave the definition and results in Appendix since the corresponding findings are very similar to those of NDCG. The following proposition suggests that cross-entropy is a soft proxy to these ranking metrics.\nProposition 4.1. For a target item \ud835\udc63+ which is ranked as \ud835\udc5f+, the following inequality holds true for any \ud835\udc5b\u2265\ud835\udc5f+\n(2)\n1In practice, it is deemed meaningless when \ud835\udc5f+ exceeds a pre-specified threshold \ud835\udc58 (e.g., \ud835\udc58= 1, 5, 10). Hence, the widely adopted NDCG@\ud835\udc58metric is modified to assign zero reward to these poor ranking results.\n1In practice, it is deemed meaningless when \ud835\udc5f+ exceeds a pre-specified threshold \ud835\udc58 (e.g., \ud835\udc58= 1, 5, 10). Hence, the widely adopted NDCG@\ud835\udc58metric is modified to assign zero reward to these poor ranking results.\n<div style=\"text-align: center;\">able 1: Cross-entropy loss and its approximations. The bounding probabilities are obtained in the case of uniform sampling ore conclusions in terms of Reciprocal Rank (RR) can be found in Appendix C.</div>\nTable 1: Cross-entropy loss and its approximations. The bounding probabilities are obtained in the ca More conclusions in terms of Reciprocal Rank (RR) can be found in Appendix C.\nLoss\nFormulation\n\u2018Normalizing\u2019 term \ud835\udc4d\nComplexity\nP\ufffd\u2212log NDCG(\ud835\udc5f+) \u2264\u2113\u2217\n\ufffd\u2265\n\u2113CE\n\u2212log\nexp(\ud835\udc60\ud835\udc63+ )\n\ufffd\n\ud835\udc63\u2208I exp(\ud835\udc60\ud835\udc63)\n\ufffd\n\ud835\udc63\u2208I exp(\ud835\udc60\ud835\udc63)\nO(|I|\ud835\udc51)\n1\n\u2113BCE\n\u2212log\ud835\udf0e(\ud835\udc60\ud835\udc63+) \u2212log(1 \u2212\ud835\udf0e(\ud835\udc60\ud835\udc63\u2212))\n(1 + exp(\ud835\udc60\ud835\udc63+))(1 + exp(\ud835\udc60\ud835\udc63\u2212))\nO(\ud835\udc51)\n-\n\u2113BPR\n\u2212log\ud835\udf0e(\ud835\udc60\ud835\udc63+ \u2212\ud835\udc60\ud835\udc63\u2212)\nexp(\ud835\udc60\ud835\udc63+) + exp(\ud835\udc60\ud835\udc63\u2212)\nO(\ud835\udc51)\n-\n\u2113NCE\n\u2212log\ud835\udf0e(\ud835\udc60\u2032\ud835\udc63+) \u2212\ufffd\ud835\udc3e\n\ud835\udc56=1 log(1 \u2212\ud835\udf0e(\ud835\udc60\u2032\ud835\udc63\ud835\udc56))\n(1 + exp(\ud835\udc60\u2032\ud835\udc63+)) \ufffd\ud835\udc3e\n\ud835\udc56=1(1 + exp(\ud835\udc60\u2032\ud835\udc63\ud835\udc56))\nO(\ud835\udc3e\ud835\udc51)\n1 \u2212\ud835\udc5a\ufffd1 \u2212|S\u2032+|/|I|\ufffd\u230a\ud835\udc3e/\ud835\udc5a\u230b\n\u2113NEG\n\u2212log\ud835\udf0e(\ud835\udc60\ud835\udc63+) \u2212\ufffd\ud835\udc3e\n\ud835\udc56=1 log(1 \u2212\ud835\udf0e(\ud835\udc60\ud835\udc63\ud835\udc56))\n(1 + exp(\ud835\udc60\ud835\udc63+)) \ufffd\ud835\udc3e\n\ud835\udc56=1(1 + exp(\ud835\udc60\ud835\udc63\ud835\udc56))\nO(\ud835\udc3e\ud835\udc51)\n1 \u2212\ud835\udc5a\ufffd1 \u2212|S+|/|I|\ufffd\u230a\ud835\udc3e/\ud835\udc5a\u230b\n\u2113IS\n\u2212log\nexp(\ud835\udc60\ud835\udc63+ \u2212log\ud835\udc44(\ud835\udc63+))\n\ufffd\ud835\udc3e\n\ud835\udc56=1 exp(\ud835\udc60\ud835\udc63\ud835\udc56\u2212log\ud835\udc44(\ud835\udc63\ud835\udc56))\n\ufffd\ud835\udc3e\n\ud835\udc56=1 exp(\ud835\udc60\ud835\udc63\ud835\udc56\u2212log\ud835\udc44(\ud835\udc63\ud835\udc56) + log\ud835\udc44(\ud835\udc63+))\nO(\ud835\udc3e\ud835\udc51)\n1 \u22122\ud835\udc5a\ufffd1 \u2212\ud835\udc5f+/|I|\ufffd\u230a\ud835\udc3e/2\ud835\udc5a\u230b\n\u2113SCE\n\u2212log\nexp(\ud835\udc60\ud835\udc63+ )\nexp(\ud835\udc60\ud835\udc63+ )+\ud835\udefc\ufffd\ud835\udc3e\n\ud835\udc56=1 exp(\ud835\udc60\ud835\udc63\ud835\udc56)\nexp(\ud835\udc60\ud835\udc63+) + \ud835\udefc\ufffd\ud835\udc3e\n\ud835\udc56=1 exp(\ud835\udc60\ud835\udc63\ud835\udc56)\nO(\ud835\udc3e\ud835\udc51)\n1 \u22121\n\ud835\udefc2\ud835\udc5a\ufffd1 \u2212\ud835\udc5f+/|I|\ufffd\u230a\ud835\udefc\ud835\udc3e/2\ud835\udc5a\u230b\nWe can draw from Proposition 4.1 that \u2212log NDCG(\ud835\udc5f+) would be strictly bounded by CE-like losses, as long as all items ranked before \ud835\udc63+ are retained in the normalizing term. In other words, minimizing these CE-like losses is equivalent to maximizing a lower bound of NDCG. Because cross-entropy is a special case that retains all items (i.e., \ud835\udc5b= |I|), we readily have the following corollary:\nCorollary 4.2 ([4]). Minimizing the cross-entropy loss \u2113CE is equivalent to maximizing a lower bound of normalized discounted cumulative gain.\nTherefore, satisfactory ranking capability can be expected if \u2113CE for all queries are minimized. Since the superiority of cross-entropy possibly stems from its connection to some ranking metrics, one may hypothesize that optimizing a tighter bound with a smaller value of \ud835\udc5b\u226a|I| allows greater performance gains. However, the condition \ud835\udc5b\u2265\ud835\udc5f+ cannot be consistently satisfied via a constant value of \ud835\udc5bsince \ud835\udc5f+ dynamically changes during training. Alternatively, an adaptive truncation can be employed for this purpose: \u2211\ufe01\n(3)\nNote that this \ud835\udf02-truncated loss retains only items whose scores are not lower than \ud835\udc60+ \u2212\ud835\udf02|\ud835\udc60+|, so a tighter bound will be obtained as \ud835\udf02 drops to 0. Specifically, this \ud835\udf02-truncated loss becomes \u2113CE-\ud835\udc5f+ (i.e., the tightest case) when \ud835\udf02= 0, and approaches \u2113CE when \ud835\udf02\u2192+\u221e. Figure 2 illustrates how NDCG@10 varies as \ud835\udf02gradually increases from 0.1 to 5. There are two key observations: 1. SASRec performs worst in the tightest case of \ud835\udf02\u22480. This can be attributed to the instability of the \ud835\udf02-truncated loss. On the one hand, \u2113CE-\ud835\udf02will rapidly collapse to 0 for those easily recognized targets, in which case all other items are excluded in the normalizing term except the target itself. On the other hand, due to this strict truncation, only a few negative items are encountered during training, and thus over-fitting is more likely to occur. 2. Once \ud835\udf02is large enough to overcome the training instability, SASRec begins to enjoy the benefits from tightness and achieves its best performance around \ud835\udf02\u22480.7. Further increasing \ud835\udf02however\nleads to a similar effect as cross-entropy, thereby along with a slightly degenerate performance due to the suboptimal tightness. In spite of the minor performance gains, the complexity of these tighter bounds is still equal to or even higher than that of cross entropy. The challenge that remains unsolved is how to realize the approximation in an effective and practical manner. To this end, we will introduce two practical alternatives to cross-entropy, one based on noise contrastive estimation [14, 34], and the other based on sampled softmax loss [49]. Their different ways of approximating the cross-entropy loss lead to distinct properties during training. Some specific modifications focusing on the normalizing term estimates are then developed to enhance their ability to optimize NDCG and RR.\n# 4.2 Revisiting Noise Contrastive Estimation\nNoise Contrastive Estimation (NCE) is widely used in training neural language models for bypassing an explicit normalizing over the entire vocabulary. It requires the model to discriminate the target from an easy-to-sample noise distribution. In the case of the uniform sampling, it can be formulated as follows \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd where \ud835\udc60\u2032\ud835\udc63= \ud835\udc60\ud835\udc63\u2212\ud835\udc50\u2212log \ud835\udc3e |I| . In the original implementation of NCE [14], \ud835\udc50is a trainable parameter as an estimate of log\ud835\udc4dCE. However, this strategy is infeasible for conditional probability models like language models and sequential recommendation, where they need to determine one \ud835\udc50\ud835\udc5efor each text (query). Mnih et al. [34] therefore fixed \ud835\udc50\u22611 for all texts during training, and NEGative sampling (NEG) used in Word2Vec [33] further simplifies it by replacing \ud835\udc60\u2032 with \ud835\udc60directly; that is, \ufffd \ufffd\n\ufffd\ufffd \ufffd\ufffd However, we observe that both NCE (\ud835\udc50= 1) and NEG introduce training difficulties as the number of negative samples increases. The NDCG@10 metric shown in Figure 3 remains almost constant at the beginning of training, and consumes more iterations to converge as \ud835\udc3eincreases. This contradicts the usual understanding\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f3b3/f3b35129-dce4-43bb-bcb2-11e295057171.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">CG@10 performance of NCE and NEG across different number of negati</div>\n(4)\nwe have\n# we have\n(5)\nFrom Theorem 4.3, we have the following conclusions: 1. Notice that Eq. (4) for NCE (NEG) is mainly determined by the size of |\ud835\udc46\u2032+| (|\ud835\udc46+|) rather than the current rank \ud835\udc5f+. As a result, NCE and NEG can easily bound NDCG as long as the number of items with non-negative scores is large enough. This is more common in the early stages of training, in which case item representations are poorly distributed in the latent space. In other words, the bounds at the beginning are too weak to be meaningful for improving the model ranking capability. The so-called training difficulties are actually a stage in narrowing the gap. 2. Figure 3 also suggests that the standstill duration of NCE is significantly longer than that of NEG, for example, 150 epochs versus 70 epochs on Beauty if 500 negative samples are sampled for training. Note that NEG can be regarded as a special case of NCE by fixing \ud835\udc50= log(|I|/\ud835\udc3e), a value higher than 1 if the experimental settings described in Figure 3 are applied. As such, according to Theorem 4.3, NCE with \ud835\udc50= 1 will suffer from a weaker bound than NEG, thereby more iterations are required for convergence. Overall, NCE and NEG converge slower as \ud835\udc3eincreases because of the exponential growth of their normalizing terms w.r.t. the sizes of |S\u2032+| and |S+|. One feasible modification is to adopt a moderately large\ud835\udc50so that the sizes remain acceptable even if numerous negative items are sampled. As shown in Table 2, the number of epochs required for convergence decreases as the value of \ud835\udc50increases from 1 to 10. But a larger value once again hinders the training process.\n<div style=\"text-align: center;\">(b) MovieLens-1M</div>\nTable 2: The convergence epoch and the highest NDCG@10 metric achieved among the 500 epochs (with \ud835\udc3e= 500).\n<div style=\"text-align: center;\">Table 2: The convergence epoch and the highest NDCG@1 metric achieved among the 500 epochs (with \ud835\udc3e= 500).</div>\nBeauty\nMovieLens-1M\nNDCG@10\nEpoch\nNDCG@10\nEpoch\nNCE (\ud835\udc50= 1)\n0.0547\n400-470\n0.1817\n280-440\nNCE (\ud835\udc50= 5)\n0.0545\n195-280\n0.1812\n100-200\nNCE (\ud835\udc50= 10)\n0.0571\n95-115\n0.1817\n80-160\nNCE (\ud835\udc50= 50)\n0.0410\n\u2265500\n0.1816\n\u2265420\nNCE (\ud835\udc50= 100)\n0.0171\n\u2265500\n0.1676\n\u2265500\nRecall that \ud835\udc50is an estimate of log\ud835\udc4dCE. Setting \ud835\udc50\u226550 implies a hypothesis of \ud835\udc4dCE \u2265\ud835\udc5250, which is obviously difficult to achieve for most models. As a rule of thumb, the hyper-parameter \ud835\udc50should be chosen carefully, and \ud835\udc50= 10 appears a good choice according to the experiments in Section 5. Next we will introduce a more reliable variant of the sampled softmax loss [49]. As opposed to NCE and NEG, it yields a bound determined by the current rank \ud835\udc5f+.\n# 4.3 Scaling Up the Sampled Normalizing Term\nSince the normalizing term of cross-entropy is intractable in reality, a direct way is to approximate it by (uniformly) sampling part of items from I:\nThe resulting Scaled Cross-Entropy (SCE) becomes\n# The resulting Scaled Cross-Entropy (SCE) becomes\n\u2113SCE := \u2212\ud835\udc60\ud835\udc63+ + log \u02c6\ud835\udc4d(\ud835\udefc).\nNote that here we scale up the sampled normalizing term by a prespecific weight \ud835\udefc. For the case of \ud835\udefc= 1, this approximation (a.k.a. sampled softmax loss [49]) implies a (\ud835\udc3e+1)-class classification task, and has been used in previous studies [24, 49]. But it is worth noting that they have inherent differences. We modify it via a weight \ud835\udefcto remedy the magnitude loss resulted from sampling, so the scaled loss is more likely to bound NDCG and RR:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ebe/3ebe2b5e-593b-4c47-8ea3-4cf40e30f730.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: NDCG@10 performance under various weight \ud835\udefc.</div>\nTheorem 4.4. Under the same conditions as stated in Theorem 4.3, the inequality (5) holds for SCE with a probability of at least\n(7)\n\ufffd\ufffd In addition to the promising bounding probability achieved through the weight \ud835\udefc, we can also observe from Theorem 4.4 that Eq. (7) is directly governed by the current rank \ud835\udc5f+. In contrast to NCE (\ud835\udc50= 1) and NEG, SCE is expected to be meaningful even in the early stages of training. Certainly, SCE is not perfect: the scaling operation inevitably raises concerns about the high variance problem. As depicted in Figure 4, if negative samples are very rare, a larger weight of \ud835\udefctends to worsen the ranking capability. Fortunately, the high variance problem appears less significant as \ud835\udc3eslightly increases (e.g., \ud835\udc3e\u226550 for Beauty and \ud835\udc3e\u226510 for MovieLens-1M). Notably, sampling 100 negative samples for \ud835\udefc= 100 produces comparable performance to using 500 negative samples for \ud835\udefc= 1 on the Beauty dataset. Connection to importance sampling. While SCE does not make sense at first glance, it is indeed closely related to importance sampling [2, 41], a widely used technique for cross-entropy approximation. Given a proposal distribution \ud835\udc44over all items, it corrects the approximation as follows\n\ud835\udc63\ud835\udc56\u223c\ud835\udc44, \ud835\udc56= 1, 2, . . . , \ud835\udc3e.\n\ufffd It allows for an unbiased estimation if the proposal distribution \ud835\udc44 is precisely identical to the underlying data distribution. But for the conditional probability model like sequential recommendation, achieving the optimum requires additional overhead for each query. Hence, some heuristic designs based on popularity sampling [6, 29] are adopted more often in practice. We point out that SCE is morphologically equivalent to these designs with \ufffd\n(8)\nConsequently scaling up the sampled normalizing term can be considered as assigning a higher probability mass to the target item \ud835\udc63+. This partially explains why SCE is effective: a well-trained scoring function should skew towards the target item. Another subtle difference is that the normalizing term for importance sampling may not include the target term, while SCE always preserves it. This is practically necessary to avoid an unstable training process.\n<div style=\"text-align: center;\">Table 3: Dataset statistics.</div>\nDataset\n#Users\n#Items\n#Interactions\nDensity\nAvg. Length\nBeauty\n22,363\n12,101\n198,502\n0.07%\n8.9\nMovieLens-1M\n6,040\n3,416\n999,611\n4.84%\n165.5\nYelp\n30,431\n20,033\n316,354\n0.05%\n10.4\nTable 4: Model statistics. The number of parameters is estimated based on the Beauty dataset.\nModel\nFoundation Model\nArchitecture\nEmbedding Size\n#Params\nP5(CID+IID) [19]\nT5\nTransformer\n512\n60M\nPOD [27]\nT5\nTransformer\n512\n60M\nLlamaRec [53]\nLlama2\nTransformer\n4096\n7B\nE4SRec [28]\nLlama2\nTransformer\n4096\n7B\nGRU4Rec [17]\n-\nRNN\n64\n0.80M\nCaser [45]\n-\nCNN\n64\n3.80M\nSASRec [21]\n-\nTransformer\n64\n0.83M\nBERT4Rec [44]\n-\nTransformer\n64\n1.76M\nFMLP-Rec [56]\n-\nMLP\n64\n0.92M\n# 4.4 Computational Complexity\nThe major cost of cross-entropy lies in the inner product and softmax operations. It has a complexity of O(|I|\ud835\udc51), where \ud835\udc51denotes the embedding size before similarity calculation. In contrast, the approximations require a lower cost O(\ud835\udc3e\ud835\udc51), correspondingly an additional overhead O(\ud835\udc3e) for uniform sampling. Overall, it is profitable if \ud835\udc3e\u226a|I|.\n# 5 EXPERIMENTS\nIn this section, we are to reveal the true ranking capability of conventional recommenders by using the modified Noise Contrastive Estimation (NCE) and the proposed Scaled Cross-Entropy (SCE). Thus, the current advancements made by LLM-based recommenders can also be assessed objectively.\n# 5.1 Experimental Setup\nThis part introduces the datasets, evaluation metrics, baselines, and implementation details.\nThis part introduces the datasets, evaluation metrics, baselines, and implementation details. Datasets. To ensure the reliability of the conclusions, we select three public datasets from different scenarios, including the Beauty, MovieLens-1M, and Yelp datasets. Beauty is an e-commerce dataset extracted from Amazon reviews known for high sparsity; MovieLens-1M is a movie dataset with a much longer sequence length; Yelp collects abundant meta-data suitable for multi-task training. Following [12, 56], we filter out users and items with less than 5 interactions, and the validation set and test set are split in a leave-one-out fashion, namely the last interaction for testing and the penultimate one for validation. The dataset statistics are presented in Table 3. Evaluation metrics. For each user, the scores returned by the recommender will be sorted in descending order to generate candidate lists. In addition to the aforementioned NDCG@\ud835\udc58(Normalized Discounted Cumulative Gain), HR@\ud835\udc58(Hit Rate) that quantifies the proportion of successful hits among the top-\ud835\udc58recommended candidates will also be included in this paper due to its widespread\n<div style=\"text-align: center;\">Table 5: Overall performance comparison on the Beauty, MovieLens-1M, and Yelp datasets. The best results of each block are marked in bold. \u2018\u25b2% over CE/LLM\u2019 represents the relative gap between respective best results.</div>\nBeauty\nMovieLens-1M\nYelp\nHR@5\nHR@10\nNDCG@5\nNDCG@10\nHR@5\nHR@10\nNDCG@5\nNDCG@10\nHR@5\nHR@10\nNDCG@5\nNDCG@10\nLLM\nPOD\n0.0185\n0.0245\n0.0125\n0.0146\n0.0422\n0.0528\n0.0291\n0.0326\n0.0476\n0.0564\n0.0330\n0.0358\nP5 (CID+IID)\n0.0569\n0.0791\n0.0403\n0.0474\n0.2225\n0.3131\n0.1570\n0.1861\n0.0289\n0.0453\n0.0200\n0.0252\nLlamaRec\n0.0591\n0.0862\n0.0405\n0.0492\n0.1757\n0.2836\n0.1113\n0.1461\n0.0416\n0.0605\n0.0306\n0.0367\nE4SRec\n0.0527\n0.0753\n0.0376\n0.0448\n0.1871\n0.2765\n0.1234\n0.1522\n0.0309\n0.0473\n0.0207\n0.0260\nCE\nGRU4Rec\n0.0474\n0.0690\n0.0329\n0.0398\n0.2247\n0.3201\n0.1542\n0.1850\n0.0275\n0.0463\n0.0171\n0.0231\nCaser\n0.0435\n0.0614\n0.0303\n0.0361\n0.2181\n0.3049\n0.1520\n0.1800\n0.0283\n0.0383\n0.0211\n0.0243\nSASRec\n0.0713\n0.0986\n0.0510\n0.0597\n0.2221\n0.3131\n0.1518\n0.1812\n0.0476\n0.0696\n0.0345\n0.0415\nBERT4Rec\n0.0509\n0.0747\n0.0347\n0.0423\n0.1978\n0.2922\n0.1330\n0.1634\n0.0355\n0.0540\n0.0243\n0.0303\nFMLP-Rec\n0.0717\n0.0988\n0.0507\n0.0594\n0.2287\n0.3243\n0.1585\n0.1893\n0.0512\n0.0759\n0.0364\n0.0444\n\u25b2% over LLM\n21.4%\n14.6%\n25.7%\n21.3%\n2.8%\n3.6%\n0.9%\n1.7%\n7.5%\n25.6%\n10.3%\n21.0%\nBCE\nGRU4Rec\n0.0214\n0.0376\n0.0134\n0.0186\n0.1595\n0.2490\n0.1023\n0.1310\n0.0157\n0.0273\n0.0098\n0.0135\nCaser\n0.0282\n0.0434\n0.0185\n0.0234\n0.1639\n0.2476\n0.1078\n0.1348\n0.0304\n0.0428\n0.0224\n0.0264\nSASRec\n0.0429\n0.0671\n0.0275\n0.0353\n0.1594\n0.2492\n0.1040\n0.1329\n0.0325\n0.0501\n0.0225\n0.0281\nBERT4Rec\n0.0245\n0.0415\n0.0152\n0.0207\n0.1241\n0.2021\n0.0789\n0.1039\n0.0223\n0.0379\n0.0138\n0.0188\nFMLP-Rec\n0.0460\n0.0710\n0.0301\n0.0381\n0.1800\n0.2722\n0.1173\n0.1469\n0.0460\n0.0651\n0.0330\n0.0391\n\u25b2% over CE\n-35.9%\n-28.1%\n-41.0%\n-36.2%\n-21.3%\n-16.1%\n-26.0%\n-22.4%\n-10.0%\n-14.3%\n-9.4%\n-11.9%\n\u25b2% over LLM\n-22.1%\n-17.7%\n-25.8%\n-22.6%\n-19.1%\n-13.1%\n-25.3%\n-21.1%\n-3.3%\n7.6%\n-0.1%\n6.5%\nNCE\nGRU4Rec\n0.0434\n0.0652\n0.0288\n0.0359\n0.2273\n0.3184\n0.1541\n0.1834\n0.0241\n0.0418\n0.0148\n0.0205\nCaser\n0.0377\n0.0567\n0.0253\n0.0314\n0.2213\n0.3106\n0.1523\n0.1811\n0.0296\n0.0405\n0.0220\n0.0255\nSASRec\n0.0686\n0.0961\n0.0485\n0.0573\n0.2177\n0.3135\n0.1479\n0.1788\n0.0471\n0.0682\n0.0344\n0.0412\nBERT4Rec\n0.0487\n0.0734\n0.0324\n0.0404\n0.1960\n0.2933\n0.1311\n0.1624\n0.0389\n0.0574\n0.0271\n0.0330\nFMLP-Rec\n0.0693\n0.0964\n0.0491\n0.0578\n0.2291\n0.3279\n0.1567\n0.1885\n0.0512\n0.0760\n0.0364\n0.0444\n\u25b2% over CE\n-3.4%\n-2.4%\n-3.6%\n-3.2%\n0.2%\n1.1%\n-1.1%\n-0.4%\n0.0%\n0.1%\n0.0%\n0.1%\n\u25b2% over LLM\n17.3%\n11.8%\n21.3%\n17.4%\n2.9%\n4.7%\n-0.2%\n1.3%\n7.5%\n25.7%\n10.3%\n21.1%\nSCE\nGRU4Rec\n0.0489\n0.0694\n0.0344\n0.0410\n0.2309\n0.3248\n0.1587\n0.1891\n0.0290\n0.0487\n0.0183\n0.0246\nCaser\n0.0456\n0.0628\n0.0322\n0.0377\n0.2274\n0.3135\n0.1586\n0.1864\n0.0293\n0.0404\n0.0218\n0.0253\nSASRec\n0.0698\n0.0968\n0.0500\n0.0587\n0.2273\n0.3186\n0.1567\n0.1862\n0.0472\n0.0693\n0.0339\n0.0410\nBERT4Rec\n0.0540\n0.0776\n0.0372\n0.0449\n0.2078\n0.3014\n0.1405\n0.1707\n0.0414\n0.0612\n0.0283\n0.0346\nFMLP-Rec\n0.0703\n0.0979\n0.0502\n0.0591\n0.2372\n0.3284\n0.1648\n0.1942\n0.0517\n0.0779\n0.0357\n0.0441\n\u25b2% over CE\n-2.0%\n-0.9%\n-1.5%\n-1.1%\n3.7%\n1.3%\n4.0%\n2.6%\n1.0%\n2.6%\n-2.0%\n-0.6%\n\u25b2% over LLM\n19.0%\n13.6%\n23.8%\n20.0%\n6.6%\n4.9%\n5.0%\n4.4%\n8.6%\n28.8%\n8.1%\n20.2%\nuse in other studies [12, 21, 56]. Besides, we also provide the Mean Reciprocal Rank (MRR) results in Appendix D. Baselines. Although LLM itself has surprising zero-shot recommendation ability, there still exist non-negligible gaps [1, 22] unless domain-specific knowledge is injected. Hence, only LLMbased recommenders enhanced by fine-tuning will be compared in this paper: P5 (CID+IID) [19], POD [27], LlamaRec [53], and E4SRec [28]. Specifically, the first two methods take T5 as the foundation model, while the last two methods fine-tune Llama2 for efficient sequential recommendation. Additionally, five sequential models including FMLP-Rec [56], Caser [45], GRU4Rec [17], SASRec [21], and BERT4Rec [44], are considered here to unveil the true capability of conventional methods. They cover various architectures so as to comprehensively validate the effectiveness of the proposed approximation methods. Table 4 presents an overview of the model statistics. Notice that for LlamaRec and E4SRec we employ Llama2-7B instead of Llama2-13B as the foundation model in order to improve training efficiency. This results in minor performance differences in practice. Implementation details. According to the discussion in Section 4, we set \ud835\udc50= 10 for NCE and \ud835\udefc= 100 for SCE, and less than 5% of all items will be sampled for both approximation objectives.\nSpecifically, \ud835\udc3e= 500 on the Beauty dataset, and \ud835\udc3e= 100 on the MovieLens-1M and Yelp datasets. We find that training 200 epochs is sufficient for cross-entropy to converge, while sometimes NCE and SCE need 300 epochs. Other hyper-parameters of NCE and SCE completely follow cross-entropy. Because the data preprocessing scripts provided with POD may lead to information leakage [39], we assign random integer IDs to items rather than sequentially incrementing integer IDs. The maximum sequence length and embedding size have a direct impact on representation capability and inference efficiency, so we will discuss them separately in Section 5.3.\n# 5.2 Overall Performance Evaluation\nIn this part, we fulfill our primary objective by presenting the comparison between LLM-based recommenders and conventional recommenders. Considering the expensive training cost of LLMbased models, their results reported in Table 5 are based on a single run, while the results of other conventional methods are averaged over 5 independent runs. Conventional methods using cross-entropy outperform LLM-based recommenders. Let us focus on the first three blocks in Table 5, where the use of cross-entropy greatly improves the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/13c2/13c29147-0373-493b-b863-0ff9dc5b7615.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) NDCG@10</div>\n<div style=\"text-align: center;\">Figure 5: Relative gaps between SCE and NCE.</div>\nconventional methods\u2019 recommendation performance. In particular, SASRec and FMLP-Rec demonstrate superior performance compared to LLMs, but fall significantly behind if replacing crossentropy with BCE. Hence, previous affirmative arguments about the LLMs\u2019 recommendation performance are rooted in unobjective and unfair comparisons, wherein BCE or BPR are commonly used for training conventional models. Moreover, the inflated model size (from 60M of P5 to 7B of LlamaRec) only yields negligible improvements in some of the metrics. The rich world knowledge and powerful reasoning ability seem to be of limited use here due to the emphasis on personalization in sequential recommendation. In conclusion, despite after fine-tuning, LLM-based recommenders still fail to surpass state-of-the-art conventional models. Comparable effectiveness can be achieved using practical approximations. Conducting a full softmax over all items for cross-entropy may be infeasible in practice. Fortunately, the last two blocks in Table 5 shows the substitutability of cross-entropy by applying the modified NCE or the proposed SCE. To showcase the effectiveness of these substitutes, we intentionally sample a rather conservative number of negative samples, and thus there remains a slight gap compared to cross-entropy. Nevertheless, the superior results of NCE and SCE re-emphasize the clear gap that exists between LLM-based and conventional recommenders. SCE is more consistent and reliable than NCE. As discussed in Section 4.2, NCE is sensitive to the choice of \ud835\udc50: extremely small or large values might impede learning and degrade performance. The inconsistent performance gains from NCE can also verify this conclusion. Figure 5 clearly demonstrates that NCE can contribute competitive performance to SASRec and FMLP-Rec, but underperforms SCE across a variety of models (e.g., GRU4Rec and BERT4Rec) and datasets (e.g., Beauty and MovieLens-1M). For Caser on Beauty and GRU4Rec on Yelp, replacing SCE with NCE results in a performance degradation of even \u226515%.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d58e/d58eea4f-aee2-4681-8b15-7e3f23eb5d64.png\" style=\"width: 50%;\"></div>\nFigure 6: P5 (CID+IID) versus SASRec(+) across various maximum sequence length \ud835\udc3f. SASRec+ uses a sliding window similar to P5 to augment each sequence.\n<div style=\"text-align: center;\">Figure 6: P5 (CID+IID) versus SASRec(+) across various maximum sequence length \ud835\udc3f. SASRec+ uses a sliding window similar to P5 to augment each sequence.</div>\n<div style=\"text-align: center;\">Table 6: The impact of embedding size \ud835\udc51.</div>\nBeauty\nMovieLens-1M\n\ud835\udc51\nHR@10\nNDCG@10\nHR@10\nNDCG@10\nP5(CID+IID)\n512\n0.0791\n0.0474\n0.3131\n0.1861\nSASRec+\n64\n0.0937\n0.0574\n0.3271\n0.1915\nSASRec+\n512\n0.0963\n0.0589\n0.3360\n0.1999\nAdditional experiments on BPR loss and MRR metrics are shown in the Appendix. They draw the same conclusions as above.\n# 5.3 Other Factors for Objective Evaluation\nDue to the difference in model design, it is challenging to conduct evaluations on a completely identical testbed. To clarify the reliability of the results in Table 5, we further investigate two key factors: maximum sequence length2 \ud835\udc3fand embedding size \ud835\udc51. According to the conclusions above, SCE is employed to train SASRec in the following. Results for CE and NCE can be found in Appendix E. Maximum sequence length. Following the routine of previous studies [21, 44], conventional models like SASRec are allowed to make predictions based on the last \ud835\udc3f= 200 interactions on MovieLens-1M and \ud835\udc3f= 50 on other datasets. In contrast, LLMbased recommenders are confined to \ud835\udc3f= 20 on all three datasets for training efficiency. We argue that this difference does not make the conclusion differ because as can be seen in Figure 6, P5 does not exhibit much better performance with access to more historical interactions. Notably, SASRec\u2019s performance is stable on Beauty, but on MovieLens-1M, it deteriorates significantly when \ud835\udc3fgets smaller. This phenomenon primarily arises from the fact that the original implementation of SASRec only utilizes the most recent \ud835\udc3finteractions for training, whereas for P5 each sequence is divided into multiple segments. Consequently, P5 is able to access far more interactions than SASRec during training, especially on the MovieLens-1M dataset known for long sequence length. If we apply a similar strategy that augments each sequence using a sliding window, the resulting SASRec+ then performs consistently across diverse \ud835\udc3f.\n2The maximum sequence length refers to the maximum number of historical interactions used for next-item prediction. Other tokens like prompts are not included.\nnding the Role of Cross-Entropy Loss in Fairly Evaluating Large Language Model-based Recommend\nEmbedding size. Models with a larger embedding size have stronger representation capability and thus potentially better recommendation performance. According to the discussion above, we examine its impact under the same setting of \ud835\udc3f= 20. In Table 6, when the embedding size of SASRec+ is increased from 64 to 512, the obtained performance gains are marginal. In view of its costly overhead, such improvement is not attractive in practice. This also implies that existing LLM-based recommenders are over-parameterized in terms of ranking capability.\nIn this work, we bridge the theoretical and empirical performance gaps between cross-entropy and some of its approximations through a modified noise contrastive estimation loss and an effective scaled cross-entropy loss. Based on these practical approximations, we showcase that existing LLM-based recommenders are not as effective as claimed. The innovative understandings and extensive experiments can be expected to facilitate an objective evaluation of LLM-based recommendation in the future.\n# REFERENCES\n[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An effective and efficient tuning framework to align large language model with recommendation. In ACM Conference on Recommender Systems (RecSys). ACM, 1007\u20131014. [2] Yoshua Bengio and Jean-S\u00e9bastien Sen\u00e9cal. 2008. Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks (TNNLS) 19, 4 (2008), 713\u2013722. [3] Guy Blanc and Steffen Rendle. 2018. Adaptive sampled softmax with kernel based sampling. In International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research, Vol. 80). PMLR, 589\u2013598. [4] Sebastian Bruch, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2019. An analysis of the softmax cross entropy loss for learning-to-rank with binary relevance. In ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR). ACM, 75\u201378. [5] Chong Chen, Weizhi Ma, Min Zhang, Chenyang Wang, Yiqun Liu, and Shaoping Ma. 2023. Revisiting negative sampling vs. non-sampling in implicit recommendation. ACM Transactions on Information Systems (TOIS) 41, 1 (2023), 1\u201325. [6] Jin Chen, Defu Lian, Binbin Jin, Kai Zheng, and Enhong Chen. 2022. Learning recommenders for implicit feedback with importance resampling. In ACM Web Conference (WWW). ACM, 1997\u20132005. [7] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. In ACM Conference on Recommender Systems (RecSys). ACM, 1126\u20131132. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational Linguistics, 4171\u20134186. [9] Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, and Fei Sun. 2023. A large language model enhanced conversational recommender system. arXiv preprint arXiv:2308.06212 (2023). [10] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023). [11] Yingbo Gao, David Thulke, Alexander Gerstenberger, Khoa Viet Tran, Ralf Schl\u00fcter, and Hermann Ney. 2021. On sampling-based training criteria for neural language modeling. In Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 1877\u20131881. [12] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (RLP): A unified pretrain, personalized prompt & predict paradigm (P5). In ACM Conference on Recommender Systems (RecSys). ACM, 299\u2013315. [13] Shansan Gong and Kenny Q. Zhu. 2022. Positive, negative and neutral: Modeling implicit feedback in session-based news recommendation. In International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1185\u20131195. [14] Michael Gutmann and Aapo Hyv\u00e4rinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In International Conference on Artificial Intelligence and Statistics (AISTATS). JMLR Workshop and Conference Proceedings, 297\u2013304. [15] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and powering graph convolution network for recommendation. In International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). ACM, 639\u2013648. [16] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In International Conference on World Wide Web (WWW). ACM, 173\u2013182. [17] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based recommendations with recurrent neural networks. In International Conference on Learning Representations (ICLR). [18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR). [19] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How to index item ids for recommendation foundation models. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP). ACM, 195\u2013204. [20] S\u00e9bastien Jean, KyungHyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In Association for Computational Linguistics (ACL). Association for Computer Linguistics, 1\u201310. [21] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-attentive sequential recommendation. In IEEE International Conference on Data Mining (ICDM). IEEE Computer Society, 197\u2013206.\n[22] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs understand user preferences? Evaluating llms on user rating prediction. arXiv preprint arXiv:2305.06474 (2023). [23] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations (ICLR). [24] Anton Klenitskiy and Alexey Vasilev. 2023. Turning dross into gold loss: is BERT4Rec really better than SASRec?. In ACM Conference on Recommender Systems (RecSys). ACM, 1120\u20131125. [25] Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, 66\u201375. [26] Jiacheng Li, Yujie Wang, and Julian J. McAuley. 2020. Time interval aware selfattention for sequential recommendation. In International Conference on Web Search and Data Mining (WSDM). ACM, 322\u2013330. [27] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. In ACM International Conference on Information and Knowledge Management (CIKM). ACM, 1348\u20131357. [28] Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023. E4SRec: An elegant effective efficient extensible solution of large language models for sequential recommendation. arXiv preprint arXiv:2312.02443 (2023). [29] Defu Lian, Qi Liu, and Enhong Chen. 2020. Personalized ranking with importance sampling. In ACM Web Conference (WWW). ACM / IW3C2, 1093\u20131103. [30] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, and Xiangnan He. 2023. LLaRA: Aligning large language models with sequential recommenders. arXiv preprint arXiv:2312.02445 (2023). [31] Zhuang Ma and Michael Collins. 2018. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 3698\u20133707. [32] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: Ultra simplification of graph convolutional networks for recommendation. In ACM International Conference on Information and Knowledge Management (CIKM). ACM, 1253\u20131262. [33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems (NeurIPS) 26 (2013), 3111\u20133119. [34] Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In International Conference on Machine Learning (ICML). [35] OpenAI. 2023. GPT models documentation. https://platform.openai.com/docs/ models/overview. [36] Aleksandr Vladimirovich Petrov and Craig MacDonald. 2023. gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. In Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023. ACM, 116\u2013128. [37] Junyan Qiu, Haitao Wang, Zhaolin Hong, Yiping Yang, Qiang Liu, and Xingxing Wang. 2023. ControlRec: Bridging the semantic gap between language model and personalized recommendation. arXiv preprint arXiv:2311.16441 (2023). [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR) 21 (2020), 140:1\u2013140:67. [39] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al. 2023. Recommender systems with generative retrieval. arXiv preprint arXiv:2305.05065 (2023). [40] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Conference on Uncertainty in Artificial Intelligence (UAI). AUAI Press, 452\u2013461. [41] Christian P Robert, George Casella, and George Casella. 1999. Monte Carlo statistical methods. Vol. 2. Springer. [42] Guy Shani, David Heckerman, Ronen I Brafman, and Craig Boutilier. 2005. An mdp-based recommender system. Journal of Machine Learning Research (JMLR) 6, 9 (2005), 1265\u20131295. [43] Wentao Shi, Jiawei Chen, Fuli Feng, Jizhi Zhang, Junkang Wu, Chongming Gao, and Xiangnan He. 2023. On the theories behind hard negative sampling for recommendation. In ACM Web Conference (WWW). ACM, 812\u2013822. [44] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In ACM International Conference on Information and Knowledge Management (CIKM). ACM, 1441\u20131450. [45] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In ACM International Conference on Web Search and Data Mining (WSDM). ACM, 565\u2013573.\n[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foundation language models. CoRR abs/2302.13971 (2023). [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR abs/2307.09288 (2023). [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS). 5998\u20136008. [49] Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu, and Xiangnan He. 2023. On the effectiveness of sampled softmax loss for item recommendation. ACM Transactions on Information Systems (TOIS) (2023). https://doi.org/10.1145/3637061 [50] Siyu Wu, Jun Wang, and Wei Zhang. 2024. Contrastive Personalized Exercise Recommendation With Reinforcement Learning. IEEE Transactions on Learning Technologies (TLT) 17 (2024), 691\u2013703. [51] Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H. Chi. 2020. Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations. In ACM Web Conference (WWW). ACM / IW3C2, 441\u2013447. [52] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed H. Chi. 2019. Sampling-bias-corrected neural modeling for large corpus item recommendations. In ACM Conference on Recommender Systems (RecSys). ACM, 269\u2013277. [53] Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023. LlamaRec: Two-stage recommendation using large language models for ranking. arXiv preprint arXiv:2311.02089 (2023). [54] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023). [55] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). 1059\u20131068. [56] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-enhanced MLP is all you need for sequential recommendation. In ACM Web Conference (WWW). ACM, 2388\u20132399.\n# Contents\n1 1 2 3\nAbstract 1 Introduction 2 Related Work 3 Preliminaries 4 The Role of Cross-Entropy Loss in Optimizing Ranking Capability 4.1 Cross-Entropy for Some Ranking Metrics 4.2 Revisiting Noise Contrastive Estimation 4.3 Scaling Up the Sampled Normalizing Term 4.4 Computational Complexity 5 Experiments 5.1 Experimental Setup 5.2 Overall Performance Evaluation 5.3 Other Factors for Objective Evaluation 6 Conclusion References Contents A Experimental setup B Overview of Loss Function C Proofs C.1 Proof of Proposition 4.1 C.2 Proof of Theorem 4.3 and 4.4 C.3 Proof of Eq. (8) C.4 Proofs Regarding Reciprocal Rank (RR) D Empirical results on Reciprocal Rank (RR)\nMaximum Sequence Length\n# A EXPERIMENTAL SETUP\nBaselines. Although LLM itself has surprising zero-shot recommendation ability, there still exist non-negligible gaps [1, 22] unless domain-specific knowledge is injected. Hence, only LLM-based recommenders enhanced by fine-tuning will be compared in this paper: \u2022 P5 (CID+IID) [19] unifies multiple tasks (e.g., sequential recommendation and rating prediction) into a sequence-to-sequence paradigm. The use of collaborative and independent indexing together creates LLM-compatible item IDs. \u2022 POD [27] bridges IDs and words by distilling long discrete prompts into a few continuous prompts. It also suggests a task-alternated training strategy for efficiency. \u2022 LlamaRec [53] aims to address the slow inference process caused by autoregressive generation. Given the candidates retrieved by conventional models, it subsequently reranks them based on the foundation model of Llama2. \u2022 E4SRec [28] incorporates ID embeddings trained by conventional sequential models through a linear adaptor, and applies LORA [18] for parameter-efficient fine-tuning. Additionally, five sequential models, covering MLP, CNN, RNN, and Transformer architectures, are considered here to uncover the true capability of conventional methods. \u2022 GRU4Rec [17] applies RNN to recommendation with specific modifications made to cope with data sparsity. \u2022 Caser [45] treats the embedding matrix as an \u2018image\u2019, and captures local patterns by utilizing conventional filters. \u2022 SASRec [21] and BERT4Rec [44] are two pioneering works equipped with unidirectional and bidirectional self-attention, respectively. By their nature, SASRec predicts the next item based on previously interacted items, while BERT4Rec is optimized through a cloze task. \u2022 FMLP-Rec [56] denoises item sequences in the frequency domain. Although FMLP-Rec consists solely of MLPs, it exhibits superior performance compared to Transformer-based models. An overview of the model statistics can be found in Table 4. Note that for LlamaRec and E4SRec we employ Llama2-7B instead of Llama2-13B as the foundation model for training efficiency. This results in minor performance differences. Implementation details. The implementation of LLM-based recommenders is due to their source code, while for conventional models the code is available at https://anonymous.4open.science/r/ 1025. Due to the difference in model design, it is challenging to conduct evaluations on a completely identical testbed. Therefore, we follow the routine of previous studies [21, 44], where the maximum sequence length \ud835\udc3f= 200 on MovieLens-1M and \ud835\udc3f= 50 on Beauty and Yelp. In contrast, for LLM-based models, \ud835\udc3f= 20 on all three datasets. Empirically, this difference does not make the conclusion differ. Training strategies. For SASRec and BERT4Rec, each item sequence is trained once per epoch. As a result, only the most\nrecent \ud835\udc3fhistorical interactions are accessed during the training process, which will negatively impact performance when the sequence lengths are typically longer than \ud835\udc3f. Some approaches such as LLM-based approaches and other conventional methods thus divides each sequence into multiple sub-sequences.\n# B OVERVIEW OF LOSS FUNCTION\nCross-Entropy (CE), also known as the negative log-likelihood (NLL) loss, can be formulated as follows:\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd This is also the de facto objective commonly used in the pre-training (fine-tuning) of LLMs. Binary Cross-Entropy (BCE). BCE samples one negative item \ud835\udc63\u2212for each target \ud835\udc63+, which necessitates the recommender to possess excellent pointwise scoring capability:\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Here \ud835\udf0e: R \u2192[0, 1] denotes the sigmoid function. Bayesian Personalized Ranking (BPR) [40] also samples one negative item \ud835\udc63\u2212for each target \ud835\udc63+, but it intends to maximize the probability that \ud835\udc63+ will be chosen in preference to \ud835\udc63\u2212:\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd Importance Sampling (IS) [2, 20] is a widely used technique for CE approximation. It is capable of correcting the approximation error via a proposal distribution \ud835\udc44:\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd In addition to uniform distribution, the distribution derived from popularity metrics [29] is also a commonly used choice. Noise Contrastive Estimation (NCE) [14, 34] requires the model to discriminate the target \ud835\udc63+ from an easy-to-sample noise\ndistribution:\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd In the case of uniform sampling, \ud835\udc60\u2032\ud835\udc63= \ud835\udc60\ud835\udc63\u2212\ud835\udc50\u2212log \ud835\udc3e |I| , where \ud835\udc50is a trainable parameter as an estimate of log\ud835\udc4dCE. NEGative sampling (NEG) [33] is a special case of NCE by fixing \ud835\udc50= log |I| \ud835\udc3e:\n# C PROOFS\nThis section completes the proofs regarding the connection between the aforementioned loss functions and ranking metrics. Before delving into the proofs, it is important to note that in this paper we focus on the metrics specifically for next-item recommendation as it is most consistent with LLM\u2019s next-token generation feature.\n# C.1 Proof of Proposition 4.1\nProposition C.1. For a target item \ud835\udc63+ which is ranked as \ud835\udc5f+, the following inequality holds true for any \ud835\udc5b\u2265\ud835\udc5f+\nwhere\n(10)\nProof. Notice that log2(1 + \ud835\udc65) \u2264\ud835\udc65holds true for any \ud835\udc65\u22651. Hence, we have\nNDCG(\ud835\udc5f+) = 1 log2(1 + \ud835\udc5f+) \u22651 \ud835\udc5f+ = 1 1 + \ufffd \ud835\udc63\u2260\ud835\udc63+ \ud835\udeff(\ud835\udc60\ud835\udc63> \ud835\udc60\ud835\udc63+) = 1 1 + \ufffd \ud835\udc5f(\ud835\udc63)<\ud835\udc5f+ \ud835\udeff(\ud835\udc60\ud835\udc63> \ud835\udc60\ud835\udc63+) \u2265 1 1 + \ufffd \ud835\udc5f\ud835\udc63<\ud835\udc5f+ exp(\ud835\udc60\ud835\udc63\u2212\ud835\udc60\ud835\udc63+) = exp(\ud835\udc60\ud835\udc63+) exp(\ud835\udc60\ud835\udc63+) + \ufffd \ud835\udc5f(\ud835\udc63)<\ud835\udc5f+ exp(\ud835\udc60\ud835\udc63) \u2265 exp(\ud835\udc60\ud835\udc63+) \ufffd \ud835\udc5f(\ud835\udc63)\u2264\ud835\udc5bexp(\ud835\udc60\ud835\udc63) ,\nwhere \ud835\udeff(condition) = 1 if the given condition is true else 0, and the second-to-last inequality holds because exp(\ud835\udc60\ud835\udc63\u2212\ud835\udc60\ud835\udc63+) \u22651 when \ud835\udc60\ud835\udc63> \ud835\udc60\ud835\udc63+.\n\u25a1\n# C.2 Proof of Theorem 4.3 and 4.4\nFirst, let us introduce some lemmas that give lower bounds of the loss functions. Lemma C.2. Let \ud835\udf091 be the number of sampled items with nonnegative corrected scores; that is,\nFirst, let us introduce some lemmas that give lower bounds of the loss functions.\nLemma C.2. Let \ud835\udf091 be the number of sampled items with nonnegative corrected scores; that is, \ufffd\ufffd \ufffd\ufffd\n(11)\nThen, we have\n  Proof. According to the definition of NCE, it follows that \ufffd \ufffd\nLemma C.3. Let \ud835\udf092 be the number of sampled items with nonnegative scores; that is, \ufffd\ufffd \ufffd\ufffd\n(13)\nThen, we have\n(14)\nLemma C.4. Let \ud835\udf093 be the number of sampled items with scores not lower than that of the target; that is \ufffd\ufffd \ufffd\ufffd\n(15)\nThen, we have\nProof. According to the definition of SCE, it follows that\nLemma C.5. Let \ud835\udf094 be the number of sampled items with scores higher than that of the target; that is \ufffd \ufffd\n(17)\nThen, we have3\n(18)\nProof. According to the definition of importance sampling, it follows that\n\u25a1\nLemma C.6. Let \ud835\udf09\u223cB(\ud835\udc3e, \ud835\udc5d) denote a random variable representing the number of successes over \ud835\udc3ebinomial trials with a probability of \ud835\udc5d. Then, we have\n(19)\nProof. 4 Divide the \ud835\udc3eindependent binomial trials into \ud835\udc5adisjoint groups, each containing at least \u230a\ud835\udc3e/\ud835\udc5a\u230btrials. If \ud835\udf09< \ud835\udc5a, then one of the groups must have no successes observed; formally, we have\n(20)\n(21)\n(22)\nHence, the proof is completed by noting the fact that P(\ud835\udf09\u2265\ud835\udc5a) = 1 \u2212P(\ud835\udf09< \ud835\udc5a).\nHence, the proof is completed by noting the fact that\n(23)\nTheorem C.7. Let \ud835\udc63+ be a target item which is ranked as \ud835\udc5f+ \u2264 22\ud835\udc5a\u22121 for some \ud835\udc5a\u2208N, and S+ := {\ud835\udc63\u2208I : \ud835\udc60\ud835\udc63\u22650}, S\u2032 + := {\ud835\udc63\u2208I : \ud835\udc60\u2032 \ud835\udc63\u22650}.\nTheorem C.7. Let \ud835\udc63+ be a target item which is ranked as \ud835\udc5f+ \u2264 22\ud835\udc5a\u22121 for some \ud835\udc5a\u2208N, and\nS+ := {\ud835\udc63\u2208I : \ud835\udc60\ud835\udc63\u22650}, S\u2032 + := {\ud835\udc63\u2208I : \ud835\udc60\u2032 \ud835\udc63\u22650}.\n4The proof follows from the response posted at the URL: https://math.stackexchange. com/questions/3626472/upper-bound-on-binomial-distribution.\nIf we uniformly sample \ud835\udc3eitems for training, then with a probability of at least \ufffd\ufffd\n(24)\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 we have\n\u2212log NDCG(\ud835\udc5f+) \u2264\u2113\u2217.\n(25)\nTherefore, Eq. (25) holds true for NCE as long as \ud835\udf091 \u2265\ud835\udc5a. Formally, we have \ufffd \ufffd\ufffd\ufffd\n(26)\n\ufffd \ufffd\ufffd\ufffd Also notice that uniformly sampling from I yields a probability of \ud835\udc5d= |S\u2032+|/|I| such that the corrected score of the sampled item is non-negative. Therefore, based on Lemma C.6, we have \ufffd\ufffd\nP\ufffd\ud835\udf091 \u2265\ud835\udc5a\ufffd\u22651 \u2212\ud835\udc5a(1 \u2212|S\u2032 +|/|I|)\u230a\ud835\udc3e/\ud835\udc5a\u230b.\n(27)\n\ufffd\ufffd Case 2. The proof of NEG is completely the same as that of NCE. Case 3. Analogously, Lemma C.4 implies that \ufffd \ufffd\n(28)\n(29)\n(30)\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd Also notice that uniformly sampling from I yields a probability of \ud835\udc5d= \ud835\udc5f+/|I| such that the score of the sampled item is not lower than that of the target (i.e., the top-\ud835\udc5f+ ranked items). Therefore, based on Lemma C.6, we have\n(31) (32)\n(31)\n(32)\nCase 3. The proof of importance sampling is similar to that of SCE. The proof has been completed now.\n# C.3 Proof of Eq. (8)\nHere, we show that \u2113SCE is morphologically equivalent to \u2113IS if \ufffd\nProof. Under the condition of Eq. (C.3), we have\n\u2113SCE\n \ufffd which is morphologically equivalent to \u2113IS with \ud835\udc3e+ 1 items in the normalizing term.\n# C.4 Proofs Regarding Reciprocal Rank (RR)\nReciprocal Rank (RR) is another popular metric used to measure the ranking capability, which is defined as follows\n(33)\nWe provide some theoretical conclusions here and leave the empirical results in next section. Firstly, we connect RR to the crossentropy as Proposition 4.1 does for NDCG.\nProposition C.8. For a target item \ud835\udc63+ which is ranked as \ud835\udc5f+, the following inequality holds true for any \ud835\udc5b\u2265\ud835\udc5f+\n(34)\nwhere\nNext, we establish a connection between RR and NCE, NEG, SCE, and importance sampling, similar to what Theorem C.7 does for NDCG. Theorem C.9. Let \ud835\udc63+ be a target item which is ranked as \ud835\udc5f+ \u22642\ud835\udc5a for some \ud835\udc5a\u2208N, and S+ := {\ud835\udc63\u2208I : \ud835\udc60\ud835\udc63\u22650}, S\u2032 + := {\ud835\udc63\u2208I : \ud835\udc60\u2032 \ud835\udc63\u22650}.\nNext, we establish a connection between RR and NCE, NEG, SCE, and importance sampling, similar to what Theorem C.7 does for NDCG.\nS+ := {\ud835\udc63\u2208I : \ud835\udc60\ud835\udc63\u22650}, S\u2032 + := {\ud835\udc63\u2208I : \ud835\udc60\u2032 \ud835\udc63\u22650}.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0250/02505226-c90b-4a29-bef5-6320bb836a01.png\" style=\"width: 50%;\"></div>\nFigure 7: Performance comparison based on tighter bounds for MRR. The dashed line represents the results trained by CE (namely the case of \ud835\udf02\u2192+\u221e).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f746/f746c23d-2db6-4835-9070-a8bac33d181a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: MRR@10 performance under various weight \ud835\udefc.</div>\nIf we uniformly sample \ud835\udc3eitems for training, then with a probability of at least \uf8f1 \ufffd\ufffd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 we have\n\ufffd \u2212log RR(\ud835\udc5f+) \u2264\u2113\u2217.\n(36)\nProof. As \ud835\udc5f+ \u22642\ud835\udc5afor some \ud835\udc5a\u2208N, we immediately have\nThe conclusions then can be proved in the same way as Theorem C.7. \u25a1\n\u25a1\nRemark 1. It is worth noting that the inequality for RR is achieved at a stricter condition compared to NDCG. For the same rank \ud835\udc5f+, NDCG allows for a smaller value of\ud835\udc5a, thereby yielding slightly higher bounding probabilities. However, this nuance does not undermine the fact that the same conclusions can be drawn from the two metrics. The empirical observations in the next section can be used to verify this.\n# D EMPIRICAL RESULTS ON RECIPROCAL RANK (RR)\n# D EMPIRICAL RESULTS ON RECIPROCAL\nIn this part, we provide the empirical results on Reciprocal Rank (RR), which are very similar to those of NDCG. Note that Mean Reciprocal Rank (MRR) reported below represents the average performance over all users.\nTable 7: MRR@5 and MRR@10 comparison",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of evaluating large language model (LLM)-based recommendation systems, specifically focusing on the role of cross-entropy loss in providing fair comparisons with conventional recommenders. The authors highlight that previous studies often make unobjective comparisons, leading to inflated perceptions of LLM performance.",
        "problem": {
            "definition": "The problem lies in the unfair evaluation of LLM-based recommenders due to the inappropriate use of pointwise/pairwise loss functions instead of cross-entropy loss, which results in performance underestimation of conventional methods.",
            "key obstacle": "The main challenge is the intractability of calculating the full softmax required for cross-entropy loss in practical scenarios, which complicates meaningful comparisons between LLM-based and conventional recommenders."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that cross-entropy loss, while theoretically superior, is often replaced by simpler loss functions that do not optimize ranking metrics effectively.",
            "opinion": "The authors propose that cross-entropy loss should be the standard for evaluating recommendation systems, as it aligns better with ranking metrics like NDCG and RR.",
            "innovation": "The main innovation is the introduction of practical approximations of cross-entropy loss, specifically Scaled Cross-Entropy (SCE) and modified Noise Contrastive Estimation (NCE), which maintain performance while being computationally feasible."
        },
        "Theory": {
            "perspective": "The perspective is that minimizing cross-entropy loss is equivalent to maximizing a lower bound of ranking metrics such as NDCG and RR, thus providing a theoretical justification for its use.",
            "opinion": "The authors assume that effective recommendation systems should focus on optimizing ranking metrics directly rather than relying on simpler loss functions that do not capture the complexities of item ranking.",
            "proof": "The authors provide theoretical proofs demonstrating that the optimization of cross-entropy loss directly correlates with improvements in ranking capabilities, establishing a foundational link between loss functions and performance metrics."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three public datasets: Beauty, MovieLens-1M, and Yelp. Various models including LLM-based recommenders (P5, LlamaRec) and conventional models (SASRec, BERT4Rec) were compared.",
            "evaluation method": "The evaluation involved measuring performance using metrics such as HR@k, NDCG@k, and MRR, with results averaged over multiple runs for conventional models and single runs for LLM-based models."
        },
        "conclusion": "The findings indicate that LLM-based recommenders do not outperform conventional methods when evaluated fairly using cross-entropy loss. The modified NCE and SCE methods show that conventional methods can achieve comparable or superior performance with less computational complexity.",
        "discussion": {
            "advantage": "The paper provides a clearer understanding of the evaluation landscape for recommendation systems, emphasizing the importance of using appropriate loss functions for fair comparisons.",
            "limitation": "The proposed methods may still face challenges in scalability and applicability across different recommendation scenarios, particularly with extremely large item sets.",
            "future work": "Future research could explore further refinements to approximation methods for cross-entropy loss, as well as investigate the integration of domain-specific knowledge into LLM-based recommenders to enhance their performance."
        },
        "other info": [
            {
                "info1": "The code for the proposed methods is available at https://github.com/MTandHJ/CE-SCE-LLMRec.",
                "info2": {
                    "info2.1": "The paper includes comprehensive experimental results comparing various models across multiple datasets.",
                    "info2.2": "The authors suggest that while LLMs have potential, their current implementations may not yet leverage their full capabilities in the context of personalized recommendations."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The paper emphasizes the importance of using cross-entropy loss for evaluating LLM-based recommenders, aligning with the capabilities of LLMs in processing and understanding data."
        },
        {
            "section number": "4.2",
            "key information": "The authors propose that cross-entropy loss should be the standard for evaluating recommendation systems, as it aligns better with ranking metrics like NDCG and RR."
        },
        {
            "section number": "10.1",
            "key information": "The paper discusses the existing challenges in evaluating LLM-based recommenders, particularly the intractability of calculating the full softmax required for cross-entropy loss."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore refinements to approximation methods for cross-entropy loss and investigate the integration of domain-specific knowledge into LLM-based recommenders."
        },
        {
            "section number": "3.2",
            "key information": "The paper introduces practical approximations of cross-entropy loss, specifically Scaled Cross-Entropy (SCE) and modified Noise Contrastive Estimation (NCE), which enhance AI-driven approaches."
        }
    ],
    "similarity_score": 0.7729079267661366,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Understanding the Role of Cross-Entropy Loss in Fairly Evaluating Large Language Model-based Recommendation.json"
}