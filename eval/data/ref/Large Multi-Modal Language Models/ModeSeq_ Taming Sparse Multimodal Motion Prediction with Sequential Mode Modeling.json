{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.11911",
    "title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
    "abstract": "Anticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multimodal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works predominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and misaligned mode confidence. While some approaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding multiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about multimodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-TakeAll (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or rulebased trajectory selection, ModeSeq considerably improves the diversity of multimodal output while attaining satisfactory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain.",
    "bib_name": "zhou2024modeseqtamingsparsemultimodal",
    "md_text": "# ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling\nZikang Zhou1 Hengjian Zhou2 Haibo Hu1 Zihao Wen Jianping Wang1 Yung-Hui Li3 Yu-Kai Huang4\nZikang Zhou1 Hengjian Zhou2 Haibo Hu1 Zihao Wen1 Jianping Wang1 Yung-Hui Li3 Yu-Kai Huang4 1City University of Hong Kong 2Zhejiang University 3Hon Hai Research Institute 4Carnegie Mellon U\n17 Nov 2024\n# Abstract\nAnticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multimodal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works predominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and misaligned mode confidence. While some approaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding multiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about multimodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-TakeAll (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or rulebased trajectory selection, ModeSeq considerably improves the diversity of multimodal output while attaining satisfactory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain.\n# 1. Introduction\nHandling the intricate uncertainty presented in the real world is one of the major hurdles in autonomous driving. One aspect of the uncertainty lies in the multimodal behavior of traffic agents, i.e., multiple instantiations of an agent\u2019s\nFigure 1. A comparison between parallel and sequential mode modeling. While parallel mode modeling (Fig. 1a) decodes multimodal trajectories in one shot, our sequential mode modeling (Fig. 1b) reasons about multiple plausible futures step by step, which captures the relationships between modes to avoid producing indistinguishable trajectories and confidence scores. future may be compatible with a given observation of the past. Without characterizing the multimodal distribution of agent motions, autonomous vehicles may fail to interact with the surroundings in a safe and human-like manner. For this reason, advanced decision-making systems demand a motion predictor to forecast several plausible and representative trajectories of critical agents [8, 16]. Although multimodality has long been the central topic studied in motion prediction, this problem has not been fundamentally solved owing to the unavailability of multimodal ground truth, i.e., only one possibility is observable in real-world driving data. To struggle with this dilemma, most existing works adopt the winner-take-all (WTA) strategy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under this strategy, only the best among all predicted trajectories will receive supervision signals for regression, while all the remaining will be masked in the training loss. Despite being the current standard practice in the research community, the WTA solution has been found to cause mode collapse easily and produce indistinguishable trajectories [24, 34, 45], further confusing the learning of mode scoring [18]. As a remedy, some recent research intends to cover the groundtruth mode by generating a massive number of trajectory candidates [27, 39, 46], from which the most representative ones are heuristically selected based on post-processing methods such as non-maximum suppression (NMS). How-\never, such a post-processing step requires carefully tuning the hyperparameters, e.g., the thresholds in NMS. Even if the hyperparameters were well-tuned, they might not fit various scenarios with diverse road conditions, leading to inferior generalization. Moreover, performing dense mode prediction followed by rule-based post-processing may significantly sacrifice trajectory accuracy in practice [39], given that correctly extracting the best trajectories from a large set of candidates is non-trivial. The limitations of mainstream methods prompt us to seek an end-to-end solution that directly produces a sparse set of diverse, high-quality, and representative agent trajectories, eliminating the need for dense mode prediction and heuristic mode selection. To begin with, we identify a commonality of existing multimodal motion predictors, that all trajectory modes are decoded in one shot, which we dub parallel mode modeling as depicted in Fig. 1a. Despite its efficiency, this paradigm neglects the relationship between the predicted modes, hindering models from decoding diverse multimodal output. For anchor-free methods with parallel mode modeling [27, 28, 46, 53, 54], the distinction between the decoded trajectories depends entirely on the difference in sub-network parameters, which cannot be guaranteed under the unstable WTA training. For this reason, some of these solutions turn to dense mode prediction and rely on post-processing steps to diversify the output [27, 46]. While anchor-based approaches offload the duties of ensuring diversity onto the input anchors [4, 29, 39, 51], determining a sparse set of anchors that can adapt to specific scenarios is challenging, compelling all these approaches to employ excessive anchors for dense mode prediction. Under the paradigm of parallel mode modeling, producing multimodal trajectories with sparse mode prediction faces significant obstacles. To tackle these challenges, this paper explores sequential mode modeling (ModeSeq), a completely different pathway toward sparse multimodal motion prediction. As illustrated in Fig. 1b, we attempt to construct a chain of modes when decoding the future from the scene embeddings, producing only one plausible trajectory and the corresponding confidence at each decoding step. Compared with parallel prediction, our approach puts more effort into capturing the correlation between modes, asking the model to tell what the next mode should be and how much confidence it has conditioned on the mode embeddings at previous steps. By giving the model a chance to look at the prior modes and learning the factorized joint latent space of multiple futures, we tremendously boost the capability of reasoning about multimodality and characterizing the full distribution without the reliance on dense mode prediction, postprocessing tricks, and all manner of anchors. To strengthen the capacity of models under our new paradigm, we develop an iterative refinement framework similar to DETR-like de-\ncoders [3, 39, 52, 54], which is powered by reordering the mode embeddings in between decoding layers. Moreover, leveraging the order of modes in the sequence, we further propose the Early-Match-Take-All (EMTA) training strategy, which can encourage the decoder to match the ground truth as early as possible. Meanwhile, our EMTA scheme also enforces the decoder to vacate the duplicated modes to cover the missing futures at the cost of negligible degradation in trajectory accuracy, thereby achieving better mode coverage and easing the learning of confidence scoring. We validate ModeSeq on the Waymo Open Motion Dataset [9] and the Argoverse 2 Motion Forecasting Dataset [49], where we achieve more balanced performance in terms of mode coverage, mode scoring, and trajectory accuracy, compared with representative motion forecasting methods such as QCNet [54] and MTR [39]. Furthermore, our approach naturally emerges with the capability of mode extrapolation thanks to sequential mode modeling, which enables predicting a dynamic number of modes on demand.\nMultimodality has been a dark cloud in the field of motion prediction. Early works employ generative models to sample multimodal trajectories [11, 13, 14, 32, 44], but they are susceptible to mode collapse. Modern motion predictors [4, 6, 17, 27, 39, 46, 53, 54] mostly follow the paradigm of multiple choice learning [15], where multiple trajectory modes are produced directly from mixture density networks [2]. Due to the lack of multimodal ground truth, these methods adopt the WTA training strategy [15], which is unstable and fails to deal with mode collapse fundamentally [24, 34, 45]. To mitigate this issue, a line of research performs dense mode prediction, i.e., decoding excessive trajectory candidates for better mode coverage [27, 39, 46]. Among these works, some equip the decoder with anchors [4, 29, 39, 51] to achieve more stable training. However, dense mode prediction necessitates the rule-based selection of the most representative trajectories from a large set of candidates, risking the precision of trajectories and the generalization ability across a wide range of scenarios. This paper provides new insights into multimodal problems by introducing the paradigm of sequential mode modeling and the EMTA training strategy, pursuing an end-to-end solution that produces a sparse set of diverse, high-quality, and representative trajectories directly. Sequential modeling has found many applications in motion prediction and traffic modeling. On the one hand, applying sequential modeling to the time dimension results in trajectory encoders and decoders based on recurrent networks [1, 5, 7, 11, 12, 14, 26, 31, 32, 35, 44, 46] or Transformers [10, 21, 27, 28, 30, 36, 47, 50, 53\u201355], which can facilitate the learning of temporal dynamics. In particular, recent advances in motion generation [30, 36, 55] have\nshown that factorizing the joint distribution of multi-agent time-series in a social autoregressive manner [1, 32, 44] can better characterize the evolution of traffic scenarios. On the other hand, some works utilize sequential modeling in the agent dimension for multi-agent motion prediction [33, 43]. For example, M2I [43] uses heuristic methods to label influencers and reactors from pairs of agents, followed by predicting the marginal distribution of the influencers and the conditional distribution of the reactors. FJMP [33] extends M2I to model the joint distribution of an arbitrary number of agents, where the joint future trajectories of agents are factorized using a directed acyclic graph. Our work is the first attempt that employs sequential prediction in the mode dimension, which enhances the understanding of multimodal behavior by capturing the correlation between modes.\n# 3. Methodology\n# 3.1. Problem Formulation\nDenote S as the input of motion prediction models, which encompasses the map elements represented as M polygonal instances and the T-step historical trajectories of A traffic agents (e.g., vehicles, pedestrians, and cyclists) in the scene. The models are tasked with forecasting K plausible trajectory modes per agent of interest, each comprising \u02c6T waypoints and an associated confidence score. These trajectories are desired to be representative, reflecting distinct behavior modes of agents and properly measuring the likelihood of each mode via the estimated confidence. Typical motion predictors employ the encoder-decoder architecture, where an encoder computes the embedding \u03a8 from the scene input, based on which a decoder learns the embeddings {mi,k}k\u2208{1,...,K} of the i-th agent\u2019s future modes. Without loss of generality, the following simplifies mi,k as mk to discuss the prediction for a single agent, which can be extended for multiple agents by repeating the same decoding process. Given mk, a prediction head then outputs a trajectory \u02c6yk = [\u02c6y1 k, . . . , \u02c6y \u02c6T k ] and a confidence score \u02c6\u03d5k via simple modules such as multi-layer perceptrons (MLPs). The whole pipeline can be summarized as \uf8f1\n# \uf8f4 \uf8f3 3.2. Motivation\nPrior works formulate multimodal decoding as a problem of set prediction [3, 48]. For instance, most cuttingedge methods employ DETR-like decoders [3] to produce the joint embeddings of multiple modes from learnable or anchor-based queries that are permutation-equivariant [27, 39, 46, 54]. Ideally, the joint mode embeddings should be supervised by the ground-truth multimodal distribution,\nakin to the application of set prediction in object detectors [3, 20, 25, 56] where each object query receives supervision signals via optimal bipartite matching. However, real-world driving data contains only one instantiation of scene evolution, which compels motion prediction solutions to adopt the winner-take-all (WTA) matching [15]. As a result, only the mode embeddings of the best-predicted trajectories get optimized, which will easily degenerate the models into learning multiple mode embeddings independently. The inability to jointly optimize all modes explains why DETR-like motion decoders fail to avoid duplicated trajectories without the use of non-maximum suppression (NMS), given that gathering multiple predicted trajectories around the most probable regions is expected to achieve lower training loss when only one ground-truth future is presented during training. To facilitate reasoning about multimodality in the absence of multimodal ground truth, our ModeSeq framework requires the decoder to conduct chain-based factorization on the joint embeddings, which is demonstrated as follows:\n\ufffd \ufffd With such a factorization that converts the unordered set of modes into a sequence, the correlation between modes can be naturally strengthened, as the mode to be decoded depends on the ones that appear previously. Further equipping this framework with appropriate model implementation and training scheme has the potential to offer better mode coverage and scoring without severe sacrifice in trajectory accuracy, which we introduce in the following sections.\n# 3.3. Scene Encoding\nSince this work focuses on the decoding of multimodal trajectories, we simply adopt QCNet [54] as the scene encoder, which is one of the de facto best practices in industry and academia due to its symmetric modeling in space and time leveraging relative positional embeddings [37]. This encoder, on the one hand, exploits a hierarchical map encoding module based on map-map self-attention to produce the map embedding of shape [M, D], with D referring to the hidden size. On the other hand, the encoder consists of Transformer modules that factorize the space and time axes, including temporal self-attention, agent-map crossattention, and agent-agent self-attention. These three types of attention are grouped and interleaved twice to yield the agent embedding of shape [A, T, D], which constitutes the final scene embeddings together with the map embedding. In principle, any scene encoding method can fit into our ModeSeq framework with reasonable efforts.\n# 3.4. Single-Layer Mode Sequence\nThis section illustrates the detailed structure of a single ModeSeq layer, which consists of a Memory Transformer\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3325/3325a84c-7900-4209-bf98-8ed414a52b8c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. Overview of the ModeSeq framework. Left: We stack multiple ModeSeq layers with mode rearrangement in between to iteratively refine the multimodal output under the Early-Match-Take-All (EMTA) training strategy. Right: Each ModeSeq layer consists of a Memory Transformer module for capturing mode-wise dependencies and a Context Transformer module for retrieving the scene embeddings produced by the encoder, operating in a recurrent fashion to decode a sequence of trajectory modes.</div>\nmodule and a Context Transformer module. Stacking multiple ModeSeq layers can further improve the performance via iterative refinement, which will be discussed in Sec. 3.5. Routine. We introduce the decoding procedure of the \u2113th ModeSeq layer, with the right-hand side of Fig. 2 depicting the first layer (i.e., \u2113= 1). The layer is designed to recurrently output a sequence of mode embeddings [m(\u2113) 1 , . . . , m(\u2113) K ], where we slightly complicate the notation of modes with a superscript that identifies the layer index. The input of the layer includes the scene embedding \u03a8 yielded by the encoder and the mode embeddings produced by the (\u2113\u22121)-th decoding layer, i.e., [m(\u2113\u22121) 1 , . . . , m(\u2113\u22121) K ]. Since the latter does not apply to the first layer, we introduce an embedding e \u2208RD to serve as the output of the \u201c0-th layer\u201d, which is randomly initialized at the beginning of training. The same learnable e is shared across the K input embeddings of the first layer:\n(3)\nBefore starting the decoding, we create an empty sequence \u2126(\u2113) 0 = [ ] with the subscript and superscript indicating the 0-th decoding step and the \u2113-th layer, respectively. This sequence will be used to keep track of the mode embeddings produced at various steps. At the t-th decoding step, we employ a Memory Transformer module and a Context Transformer module to update m(\u2113\u22121) t to become m(\u2113) t , leveraging the information in the memory bank \u2126(\u2113) t\u22121 and the scene embedding \u03a8. The output mode embedding m(\u2113) t is then pushed to the end of the sequence \u2126(\u2113) t\u22121 to obtain \u2126(\u2113) t = [m(\u2113) 1 , . . . , m(\u2113) t ], which will serve as the input at the (t + 1)-th decoding step. After going through K decoding steps, we use a prediction head to transform each of the mode embeddings stored in \u2126(\u2113) K into a specific trajectory and a corresponding confidence score via MLPs. The fol-\nlowing paragraphs detail the modules constituting a ModeSeq layer and discuss the differences between our approach and other alternatives. Memory Transformer. The Memory Transformer takes charge of modeling the sequential dependencies of trajectory modes. At the t-th decoding step of the \u2113-th layer, this module takes as input \u2126(\u2113) t\u22121 and m(\u2113\u22121) t , the memory bank for the current layer and the t-th mode embedding derived from the last layer. Since we desire the generation of the t-th mode embedding m(\u2113) t to be aware of the existence of the preceding modes, we treat m(\u2113\u22121) t as the query of the Transformer module, which retrieves the memory bank in a cross-attention manner:\n\u2212 \ufffd (4)\n\ufffd \ufffd In this way, the information in \u2126(\u2113) t\u22121 is assimilated into m(\u2113\u22121) t to produce \u02c6m(\u2113) t , which is a query feature conditioned on the modes up to the (t \u22121)-th decoding step. Context Transformer. To derive scene-compliant modes, we must provide the query feature with a specific scene context. To this end, we use the Context Transformer module to refine the conditional query \u02c6m(\u2113) t with the scene embeddings output by the encoder. Specifically, the t-th mode embedding m(\u2113) t is computed by enriching \u02c6m(\u2113) t with \u03a8 using cross-attention:\nConsidering the high complexity of performing global attention, we decompose the Context Transformer into three separate modules in practice, including mode-time crossattention, mode-map cross-attention, and mode-agent crossattention, each of which takes as input only a subset of the\nembeddings contained in \u03a8. First, the mode-time crossattention fuses the query feature with the historical encoding belonging to the agent of interest, enabling the query to adapt to the specific agent. Second, we aggregate the map information surrounding the agent of interest into the query feature leveraging the mode-map cross-attention, which contributes to the map compliance of the forecasting results. Finally, utilizing the mode-agent cross-attention module to fuse the neighboring agents\u2019 latest embeddings promotes the model\u2019s social awareness. After going through these three modules, the conditional query \u02c6m(\u2113) t eventually becomes m(\u2113) t , which is now context-aware. Prediction Head. Given the conditional, context-aware mode embedding m(\u2113) t , we use an MLP head to output the t-th trajectory \u02c6y(\u2113) t and another to estimate the corresponding confidence score \u02c6\u03d5(\u2113) t : \uf8f1 \ufffd \ufffd\n(6)\n\uf8f4 \uf8f3 \ufffd \ufffd Comparison with DETR-Like Decoders. In contrast to motion decoders [27, 39, 46, 54] inspired by DETR [3], where the relationships between modes are completely neglected [27, 46] or weakly modeled by mode-mode selfattention [39, 54], modeling modes as a sequence strengthens mode-wise relational reasoning thanks to the conditional dependence in generating multimodal embeddings, which is beneficial to eliminating duplicated trajectories. Furthermore, it is worth noting that DETR-like approaches can only decode a fixed number of modes, as the number of learnable/static anchors cannot be changed once specified at the start of training. By contrast, our ModeSeq framework supports decoding more/less modes at test time, which can be simply achieved by changing the number of decoding steps. This characteristic can be helpful since the degree of uncertainty varies by scenario. Comparison with Typical Recurrent Networks. The ModeSeq layer can be viewed as a sort of recurrent network [5, 12] due to its parameter sharing across all decoding steps, though its core components are modernized with Transformers to achieve impressive performance. While typical recurrent networks compress the memory into a single hidden state, which is often lossy, the Memory Transformer inside a ModeSeq layer allows for direct access to all prior mode embeddings, naturally scaling the capacity of the memory as the number of modes grows.\nModeSeq layer can be viewed as a sort of recurrent network [5, 12] due to its parameter sharing across all decoding steps, though its core components are modernized with Transformers to achieve impressive performance. While typical recurrent networks compress the memory into a single hidden state, which is often lossy, the Memory Transformer inside a ModeSeq layer allows for direct access to all prior mode embeddings, naturally scaling the capacity of the memory as the number of modes grows.\n# 3.5. Multi-Layer Mode Sequences\nSingle-layer mode sequences may have limited capability of learning high-quality mode representations. In particular, if the layer happens to produce unrealistic or less likely modes at the first few decoding steps, the learning of the later modes may be unexpectedly disturbed. Inspired by\nDETR [3], we develop an iterative refinement framework by stacking multiple ModeSeq layers and applying training losses to the output of each layer. As shown in the left part of Fig. 2, all layers except for the first one take as input the mode embeddings output from the last round of decoding, refining the features with the scene context. Crucially, we introduce the operation of mode rearrangement in between layers, which corrects the order of the embeddings in the mode sequence to encourage decoding trajectory modes with monotonically decreasing confidence scores. Mode Rearrangement. Before transitioning from the \u2113th to the (\u2113+ 1)-th ModeSeq layer, we sort the mode embeddings stored in the memory bank \u2126(\u2113) K according to the descending order of the confidence scores predicted from them. The sorted mode embeddings will then be sequentially input to the (\u2113+1)-th ModeSeq layer for recurrent decoding. Through iterative refinement with mode rearrangement, the trajectories and the order of modes become more scene-compliant and more reasonable, respectively.\n# 3.6. Early-Match-Take-All Training\nThe WTA training strategy [15] is blamed for producing overlapped trajectories and indistinguishable confidence scores [18, 24, 34, 45]. Fortunately, our approach has the opportunity to opt for a more advanced training method thanks to the paradigm of sequential mode modeling. In this section, we propose the EMTA loss, which leverages the order of modes to define the positive and negative samples toward better mode coverage and confidence scoring without significantly sacrificing trajectory accuracy. Typical WTA loss optimizes only the trajectory with the minimum displacement error with respect to the ground truth. In comparison, our EMTA loss optimizes the matched trajectory decoded at the earliest recurrent step. For example, if both the second and the third trajectories match the ground truth, only the second one will be optimized, regardless of which one has the minimum error. To this end, we search over the K predictions to acquire the collection of mode indices associated with matched trajectories:\n\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd where G(\u2113) denotes the set of qualified mode indices in the \u2113-th ModeSeq layer, 1{\u00b7} represents the indicator function, and IsMatch(\u00b7, \u00b7) defines the criterion for a match given the ground-truth trajectory y. The implementation of IsMatch(\u00b7, \u00b7) can be flexible, depending on the trajectory accuracy demanded by practitioners. For instance, on the Waymo Open Motion Dataset [9], we decide whether a predicted trajectory is a match based on the velocity-aware distance thresholds defined in the Miss Rate metric of the benchmark; while on the Argoverse 2 Motion Forecasting Dataset [49], a matched trajectory is expected to have less\nDataset\nMethod\nEnsemble\nLidar\nSoft mAP6 \u2191\nmAP6 \u2191\nMR6 \u2193\nminADE6 \u2193\nminFDE6 \u2193\nVal\nMTR v3 [38]\n\u00d7\n\u2713\n-\n0.4593\n0.1175\n0.5791\n1.1809\nMTR++ [40]\n\u00d7\n\u00d7\n-\n0.4382\n0.1337\n0.6031\n1.2135\nQCNet [54]\n\u00d7\n\u00d7\n0.4508\n0.4452\n0.1254\n0.5122\n1.0225\nModeSeq (Ours)\n\u00d7\n\u00d7\n0.4562\n0.4507\n0.1206\n0.5237\n1.0681\nTest\nMTR v3 [38]\n\u2713\n\u2713\n0.4967\n0.4859\n0.1098\n0.5554\n1.1062\nModeSeq (Ours)\n\u2713\n\u00d7\n0.4737\n0.4665\n0.1204\n0.5680\n1.1766\nRMP Ensemble [42]\n\u2713\n\u00d7\n0.4726\n0.4553\n0.1113\n0.5596\n1.1272\n1. Quantitative results on the 2024 Waymo Open Dataset Motion Prediction Benchmark.\nMethod\nEnsemble b-minFDE6 \u2193MR6 \u2193minADE6 \u2193minFDE6 \u2193\nMTR [39]\n\u2713\n1.98\n0.15\n0.73\n1.44\nMTR++ [40]\n\u2713\n1.88\n0.14\n0.71\n1.37\nQCNet [54]\n\u00d7\n1.91\n0.16\n0.65\n1.29\nModeSeq (Ours)\n\u00d7\n1.87\n0.14\n0.63\n1.26\nTable 2. Quantitative results on the 2024 Argoverse 2 SingleAgent Motion Forecasting Benchmark. than 2-meter final displacement error. Given G(\u2113), we determine the unique positive sample\u2019s index \u02c6k(\u2113) as follows, with all the remaining modes treated as negative samples: \uf8f1\n(8)\n\uf8f3 \ufffd \ufffd where |\u00b7| denotes the cardinality of a set, and Dist(\u00b7, \u00b7) measures the average displacement error between trajectories. This strategy for label assignment encourages the model to decode matched trajectories as early as possible by treating the earliest instead of the best matches as positive samples. Meanwhile, it drives the later matches, if any, away from the ground truth by assigning negative labels to them. On the other hand, if none of the predictions match, which commonly happens at the early stage of training, we will fall back to the regular WTA scheme to ease the difficulty in optimization. Following label assignment, we use the Laplace negative log-likelihood [53, 54] as the regression loss, optimizing the trajectories of the positive samples. Besides, we use the Binary Focal Loss [19] to optimize the confidence scores according to the labels assigned. We also try a variant of confidence loss, where we introduce the definition of ignored samples to mask the loss of the modes decoded earlier than the positive samples, which is shown to be effective in the absence of mode rearrangement.\n# 4. Experiments\n# 4. Experiments 4.1. Experimental Setup\n# 4.1. Experimental Setup\nDatasets. We conduct experiments on the Waymo Open Motion Dataset (WOMD) [9] and the Argoverse 2 Motion Forecasting Dataset [49]. The WOMD contains 486995/44097/44920 training/validation/testing samples, where the history of 1.1 seconds is provided as the context and the 8-second future trajectories of up to 8 agents are required to predict. The Argoverse 2 dataset comprises 199908/24988/24984 samples with 5-second obser-\nvation windows and 6-second prediction horizons for training/validation/testing. Metrics. Following the standard of the benchmarks [9, 49], we constrain models to output at most K = 6 trajectories. We use Miss Rate (MRK) to measure mode coverage, which counts the fraction of cases in which the model fails to produce any trajectories that match the ground truth within the required thresholds. Built upon the definition of a match, mAPK and Soft mAPK assess the precision of the confidence scores by computing the P/R curves and averaging the precision values over various confidence thresholds. To further evaluate trajectory quality, we use minimum Average Displacement Error (minADEK) and minimum Final Displacement Error (minFDEK) as indicators, which calculate the distance between the ground truth and the best-predicted trajectories as an average over the whole horizon and at the final time step, respectively. Besides, the b-minFDEK concerns the joint performance of trajectories and confidences by summing the minFDEK and Brier scores of the best-predicted trajectories. Implementation Details. We develop models with a hidden size of 128. The decoder stacks 6 layers for iterative refinement, with each layer executing 6 steps to obtain exactly 6 modes as required by the benchmarks [9, 49]. On the WOMD [9], we use the AdamW optimizer [23] to train models for 30 epochs on the training set with a batch size of 32, a weight decay rate of 0.1, and a dropout rate of 0.1. On Argoverse 2 [49], we use a similar training configuration except that the number of epochs is extended to 64. The initial learning rate is set to 5\u00d710\u22124, which is decayed to 0 at the end of training following the cosine annealing schedule [22]. Unless specified, the ablation studies are based on experiments on the WOMD with 20% of the training data.\n# 4.2. Comparison with State of the Art\nWe compare our approach with QCNet [54] and the MTR series [38\u201340], which are currently the most effective sparse and dense multimodal prediction solutions across the WOMD and the Argoverse 2 dataset. As demonstrated in Tab. 1, ModeSeq achieves the best scoring performance among the Lidar-free methods on the validation split of the WOMD, though it lags behind the mAP6 performance of MTR v3 [38], a model that augments the input information with raw sensor data. As a sparse mode predictor, ModeSeq\nDecoder\nTraining Strategy\nIgnored Samples\nSoft mAP6 \u2191\nmAP6 \u2191\nMR6 \u2193\nminADE6 \u2193\nminFDE6 \nDETR w/ Refinement\nWTA\nNone\n0.4096\n0.4050\n0.1536\n0.5660\n1.1716\nOther Matches\n0.4150\n0.4103\n0.1502\n0.5619\n1.1621\nModeSeq (Ours)\nWTA\nNone\n0.4138\n0.4093\n0.1502\n0.5563\n1.1498\nOther Matches\n0.4207\n0.4161\n0.1503\n0.5556\n1.1501\nEMTA\nNone\n0.4231\n0.4196\n0.1457\n0.5700\n1.1851\nOther Matches\n0.4098\n0.4060\n0.1496\n0.5817\n1.2207\nTable 3. Effects of sequential mode modeling and Early-Match-Take-All training on the validation set of the WOMD.\nMode Rearrangement\nIgnored Samples\nSoft mAP6 \u2191\nmAP6 \u2191\nMR6 \u2193\nminADE6 \u2193\nminFDE6 \u2193\n\u00d7\nNone\n0.4112\n0.4077\n0.1548\n0.5884\n1.2389\nEarly Mismatches\n0.4141\n0.4109\n0.1489\n0.5749\n1.2066\n\u2713\nNone\n0.4231\n0.4196\n0.1457\n0.5700\n1.1851\nEarly Mismatches\n0.4161\n0.4129\n0.1461\n0.5751\n1.2041\nTable 4. Effects of mode rearrangement on the validation set of the WOMD\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d34d/d34d4aef-2467-4cd1-bc38-4ff452ad0b3e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3. The performance after each decoding layer on the validation set of the WOMD.</div>\nundoubtedly outperforms MTR++ [40] in terms of MR6, minADE6, and minFDE6 by a large margin. Compared with QCNet [54], ModeSeq attains better Soft mAP6, mAP6, and MR6 at the cost of slight degradation on minADE6 and minFDE6, confirming that our approach can improve the mode coverage and confidence scoring of sparse predictors without significant sacrifice in trajectory accuracy. As of the time we submitted the results to the benchmark, the ensemble version of ModeSeq ranked first among Lidar-free approaches on the test set of the WOMD. Our approach also exhibits promising performance on the Argoverse 2 dataset, where our ensemble-free model surpasses QCNet and the MTR series on all critical metrics as shown in Tab. 2.\n# 4.3. Ablation Study\nEffects of Sequential Mode Modeling. In Tab. 3, we examine the effectiveness of sequential mode modeling by comparing ModeSeq with the sparse DETR-like decoder enhanced with iterative refinement [3], both employing the same QCNet encoder [54] for fair comparisons. The results demonstrate that ModeSeq outperforms the baseline on all metrics when using the same training strategy. Interestingly, ignoring the confidence loss of the suboptimal modes that match the ground truth can improve the performance of both methods under the WTA training. The reason behind this is that treating the other matched modes as negative samples\nwill confuse the optimization process, given that the best and the other matches usually have similar mode representations while they are assigned as opposite samples. Effects of EMTA Training. We also investigate the role of EMTA training in Tab. 3. After replacing the WTA loss with our EMTA scheme, the results on Soft mAP6, mAP6, and MR6 are considerably improved, which demonstrates the benefits of EMTA training in terms of mode coverage and confidence scoring. On the other hand, the performance on minADE6 and minFDE6 slightly deteriorates since the EMTA loss has relaxed the requirement for trajectory accuracy, but the degree of deterioration falls within an acceptable extent, leading to more balanced performance taken overall. Moreover, contrary to the conclusion drawn from the WTA baselines, treating other matches as ignored samples is detrimental under the EMTA strategy. This is because the joint effects of sequential mode decoding and EMTA training have broken the symmetry of mode modeling and label assignment, allowing us to assign the other matches as negative samples to drive them away from the ground truth for covering other likely modes. Effects of Iterative Refinement. To understand the effects of iterative refinement under our framework, we take the output from different decoding layers for evaluation. As shown in Fig. 3, the performance on Soft mAP6 and MR6 is generally improved as the depth increases, totaling a substantial enhancement between the first and the last layer. One of the reasons why iterative refinement works well can be attributed to the operation of mode rearrangement in between layers, which we explain in the following. Effects of Mode Rearrangement. We study the effects of mode rearrangement in Tab. 4. Comparing the first and third rows of the table, we can see that reordering the mode embeddings before further refinement can remarkably promote the forecasting capability. To gain deeper insights into the results, we develop a variant of label assignment, where the modes decoded earlier than the first match are deemed ignored samples. We found this strategy to outperform the de-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5112/51128a64-c7f9-4bf2-9cd6-37cd8493a505.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) #Mode@Training=3, #Mode@Inference=3 (b) #Mode@Training=6, #Mode@Inference=6 (c) #M Figure 4. Visualization on the WOMD. The agents in purple are predicted with blue trajectories, w</div>\n<div style=\"text-align: center;\">ode@Training=3, #Mode@Inference=3 (b) #Mode@Training=6, #Mode@Inference=6 (c) #Mode@Training=6, #Mode@Inference=24 4. Visualization on the WOMD. The agents in purple are predicted with blue trajectories, with the opacity indicating confidence.</div>\n<div style=\"text-align: center;\">gure 4. Visualization on the WOMD. The agents in purple are predicted with blu</div>\nfault one in the absence of mode rearrangement, while the conclusion reverses when we reorder the modes in between layers. This phenomenon can be explained by the fact that bad modes may appear in the first few decoding steps of the shallow layers, which can negatively impact the learning of the subsequent modes. By manually putting the less confident modes to the end of the sequence, we enable the model to prioritize the refinement of the more probable trajectories in the next layer. Without rearrangement, we have to intentionally assign monotonically decreasing labels by blocking the training loss of the early mismatches, aiming at implicitly guiding the model to output more confident modes first. Capability of Representative Mode Learning. We demonstrate ModeSeq\u2019s ability to produce representative modes in Tab. 5. While training models to decode merely 3 modes necessarily leads to worse performance, the 3-mode variant of ModeSeq achieves the same level of performance on Soft mAP6 and mAP6 compared with the 6-mode model. By comparison, QCNet [54] fails to achieve comparable results if only using 3 mode queries during training. Capability of Mode Extrapolation. We ask the model trained by generating 6 modes to execute more decoding steps at test time. As depicted in Fig. 5, ModeSeq achieves lower prediction error with the increase of the decoded modes, emerging with the capability of mode extrapolation thanks to sequential modeling. This characteristic enables handling various degrees of uncertainty across scenarios.\n# 4.4. Qualitative Results\nThe qualitative results produced by ModeSeq are presented in Fig. 4. Figure 4a demonstrates that our model can generate representative trajectories when being trained to decode only 3 modes. Comparing Fig. 4c with Fig. 4b, we can see that the 6-mode model successfully extrapolates diverse yet realistic modes when executing 24 decoding steps during inference, showcasing the extrapolation ability of ModeSeq.\n# 5. Conclusion\nThis paper introduces ModeSeq, a modeling framework that achieves sparse multimodal motion prediction via sequen-\nModel\n#Mode\nSoft mAP6 \u2191mAP6 \u2191MR6 \u2193\nTraining Inference\nQCNet [54]\n3\n3\n0.4214\n0.4163\n0.2007\n6\n6\n0.4508\n0.4452\n0.1254\nModeSeq (Ours)\n3\n3\n0.4509\n0.4479\n0.1967\n6\n6\n0.4562\n0.4507\n0.1206\nTable 5. Capability of generating representative modes with precise confidence scores. Models are trained on 100% training data and evaluated on the validation split of the WOMD.\nTable 5. Capability of generating representative modes with precise confidence scores. Models are trained on 100% training data and evaluated on the validation split of the WOMD.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b49/2b49d3b8-58c5-46ff-aca8-0f30d25984c8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1634/16343384-61fa-45d4-ab9c-adee78d4f631.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7abc/7abc57ff-8a05-4e6f-a782-044a04aad1ea.png\" style=\"width: 50%;\"></div>\nFigure 5. The results of generating more than 6 modes on the validation set of the WOMD. tial mode modeling. The framework comprises a mechanism of sequential multimodal decoding, an architecture of iterative refinement with mode rearrangement, and a training strategy of Early-Match-Take-All label assignment. As an alternative to the unordered multimodal decoding and the winner-take-all training strategy, ModeSeq achieves stateof-the-art results on motion prediction benchmarks and exhibits the characteristic of mode extrapolation, creating a new path to solving multimodal problems. Limitations. Our approach is still flawed in some respects. First, the sequential generation of modes is less efficient than one-shot predictions, which necessitates an improvement in efficiency. Second, although our approach supports multi-agent forecasting in parallel, we only explore marginal multi-agent prediction, lacking validation on joint prediction. Future research may involve extending ModeSeq into a joint multi-agent model. Acknowledgement. This project is supported by a grant from Hong Kong Research Grant Council under GRF project 11216323 and CRF C1042-23G.\n[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, 2016. 2, 3 [2] Christopher M Bishop. Mixture density networks. 1994. 2 [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020. 2, 3, 5, 7 [4] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In CoRL, 2019. 1, 2 [5] Kyunghyun Cho, Bart van Merrienboer, C\u00b8 aglar G\u00a8ulc\u00b8ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014. 2, 5 [6] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous driving using deep convolutional networks. In ICRA, 2019. 1, 2 [7] Nachiket Deo and Mohan M Trivedi. Convolutional social pooling for vehicle trajectory prediction. In CVPRW, 2018. 2 [8] Wenchao Ding, Lu Zhang, Jing Chen, and Shaojie Shen. Epsilon: An efficient planning system for automated vehicles in highly interactive environments. T-RO, 2021. 1 [9] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aur\u00b4elien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In ICCV, 2021. 2, 5, 6, 1 10] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for trajectory forecasting. In ICPR, 2020. 2 11] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In CVPR, 2018. 2 12] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural Computation, 1997. 2, 5 13] Joey Hong, Benjamin Sapp, and James Philbin. Rules of the road: Predicting driving behavior with a convolutional model of semantic interactions. In CVPR, 2019. 2 14] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In CVPR, 2017. 2 15] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep ensembles. In NIPS, 2016. 1, 2, 3, 5\n[16] Tong Li, Lu Zhang, Sikang Liu, and Shaojie Shen. Marc: Multipolicy and risk-aware contingency planning for autonomous driving. RA-L, 2023. 1 [17] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning lane graph representations for motion forecasting. In ECCV, 2020. 1, 2 [18] Longzhong Lin, Xuewu Lin, Tianwei Lin, Lichao Huang, Rong Xiong, and Yue Wang. Eda: Evolving and distinct anchors for multimodal motion prediction. In AAAI, 2024. 1, 5 [19] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00b4ar. Focal loss for dense object detection. In CVPR, 2017. 6 [20] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. In ICLR, 2022. 3 [21] Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, and Bolei Zhou. Multimodal motion prediction with stacked transformers. In CVPR, 2021. 2 [22] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. 6 [23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6 [24] Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. In CVPR, 2019. 1, 2, 5 [25] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In ICCV, 2021. 3 [26] Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guillaume Sandou, Dominique Beauvois, and Guillermo Pita Gil. Multi-head attention for multi-modal joint vehicle motion forecasting. In ICRA, 2020. 2 [27] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient attention networks. In ICRA, 2023. 1, 2, 3, 5 [28] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, David Weiss, Ben Sapp, Zhifeng Chen, and Jonathon Shlens. Scene transformer: A unified architecture for predicting multiple agent trajectories. In ICLR, 2022. 2 [29] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton, Oscar Beijbom, and Eric M Wolff. Covernet: Multimodal behavior prediction using trajectory sets. In CVPR, 2020. 2 [30] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish: Traffic modeling as next-token prediction. In ICLR, 2024. 2 [31] Nicholas Rhinehart, Kris M Kitani, and Paul Vernaza. R2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting. In ECCV, 2018. 2 [32] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals in visual multi-agent settings. In ICCV, 2019. 2, 3\n[33] Luke Rowe, Martin Ethier, Eli-Henry Dykhne, and Krzysztof Czarnecki. Fjmp: Factorized joint multi-agent motion prediction over learned directed acyclic interaction graphs. In CVPR, 2023. 3 [34] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximilian Baust, Federico Tombari, Nassir Navab, and Gregory D Hager. Learning in an uncertain world: Representing ambiguity through multiple hypotheses. In ICCV, 2017. 1, 2, 5 [35] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In ECCV, 2020. 2 [36] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In ICCV, 2023. 2 [37] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Selfattention with relative position representations. In NAACL, 2018. 3 [38] Chen Shi, Shaoshuai Shi, and Li Jiang. Mtr v3: 1st place solution for 2024 waymo open dataset challenge - motion prediction. In CVPR 2024 Workshop on Autonomous Driving, 2024. 6 [39] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion transformer with global intention localization and local movement refinement. In NeurIPS, 2022. 1, 2, 3, 5, 6 [40] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr++: Multi-agent motion prediction with symmetric scene modeling and guided intention querying. TPAMI, 2024. 6, 7 [41] Roman Solovyev, Weimin Wang, and Tatiana Gabruseva. Weighted boxes fusion: Ensembling boxes from different object detection models. Image and Vision Computing, 2021. 1 [42] Jiawei Sun, Jiahui Li, Tingchen Liu, Chengran Yuan, Shuo Sun, Zefan Huang, Anthony Wong, Keng Peng Tee, and Marcelo H Ang Jr. Rmp-yolo: A robust motion predictor for partially observable scenarios even if you only look once. arXiv preprint arXiv:2409.11696, 2024. 6 [43] Qiao Sun, Xin Huang, Junru Gu, Brian C Williams, and Hang Zhao. M2i: From factored marginal trajectory prediction to interactive prediction. In CVPR, 2022. 3 [44] Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple futures prediction. In NeurIPS, 2019. 2, 3 [45] Luca Anthony Thiede and Pratik Prabhanjan Brahma. Analyzing the variety loss in the context of probabilistic trajectory prediction. In ICCV, 2019. 1, 2, 5 [46] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, and Benjamin Sapp. Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction. In ICRA, 2022. 1, 2, 3, 5 [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 2 [48] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. In ICLR, 2016. 3\n[49] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In NeurIPS Datasets and Benchmarks, 2021. 2, 5, 6, 1 [50] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Spatio-temporal graph transformer networks for pedestrian trajectory prediction. In ECCV, 2020. 2 [51] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li, and Dragomir Anguelov. Tnt: Target-driven trajectory prediction. In CoRL, 2020. 2 [52] Yang Zhou, Hao Shao, Letian Wang, Steven L Waslander, Hongsheng Li, and Yu Liu. Smartrefine: A scenario-adaptive refinement framework for efficient motion prediction. In CVPR, 2024. 2 [53] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie Lu. Hivt: Hierarchical vector transformer for multi-agent motion prediction. In CVPR, 2022. 1, 2, 6 [54] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai Huang. Query-centric trajectory prediction. In CVPR, 2023. 1, 2, 3, 5, 6, 7, 8 [55] Zikang Zhou, Haibo Hu, Xinhong Chen, Jianping Wang, Nan Guan, Kui Wu, Yung-Hui Li, Yu-Kai Huang, and Chun Jason Xue. Behaviorgpt: Smart agent simulation for autonomous driving with next-patch prediction. In NeurIPS, 2024. 2 [56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021. 3\n# ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling\nSupplementary Material\n# 6. Definition of a Match\nThe Argoverse 2 Motion Forecasting Benchmark [49] desires the predictions\u2019 displacement error at the 60-th time step to be less than 2 meters. By linearly scaling the 2-meter threshold across time steps, we obtain a distance threshold \u0393(t) for each time step t:\n(9)\nOur EMTA training loss requires a matched trajectory to fall within the given threshold of the ground truth at every future time step. On the Waymo Open Motion Dataset (WOMD) [9], the thresholds are divided into lateral and longitudinal ones, which are adaptive to the current velocity of the agent of interest. To begin with, the benchmark defines a scaling factor with respect to the velocity v:\n(10)\n\uf8f4 \uf8f3 Utilizing the scaling factor, we define the lateral threshold \u0393lat(v, t) as\nif 1 \u2264t \u226430 ;\n(11)\n\uf8f3 Similarly, we set the longitudinal threshold \u0393lon(v, t) to be twice as large as the lateral one:\nif 1 \u2264t \u226430 ;\n(12)\n\uf8f3 Regarding the experiments on the WOMD, we demand a matched trajectory to have errors below both the lateral and longitudinal thresholds at every future time step.\n# 7. Ensemble Method on the WOMD\nInspired by Weighted Boxes Fusion (WBF) [41], we propose Weighted Trajectory Fusion to aggregate multimodal trajectories produced by multiple models. Our ensemble method is almost the same as WBF, except we are fusing trajectories according to distance thresholds rather than\nModeSeq (Ours)\nQCNet [54]\n#Mode = 3\n#Mode = 6\n#Mode = 3\n#Mode = 6\nLatency (ms)\n86\u00b19\n143\u00b110\n63\u00b111\n69\u00b116\nTable 6. Comparisons on the inference latency averaged over the validation set of the WOMD.\nbounding boxes according to IOU thresholds. Our ensemble method can improve mAP6/Soft mAP6/MR6 by sacrificing minADE6/minFDE6, which indicates that the performance on various metrics sometimes disagrees. The critical hyperparameters in Weighted Trajectory Fusion are the distance thresholds used for trajectory clustering. We choose the velocity-aware thresholds defined in Eq. (11) and Eq. (12) as the base thresholds. On top of this, we multiply the base thresholds with the scaling factors of 1.5, 1.4, and 1.4 for vehicles, pedestrians, and cyclists, respectively.\n# 8. Inference Latency\nAs stated in the main paper, a weakness of our current approach lies in the inference latency, which is about twice as high as that of QCNet [54] if predicting 6 modes according to the measurement in Tab. 6. However, in many realworld use cases, the number of modes is refrained from being more than 3. As shown in Tab. 6, the gap in inference latency between a 3-mode ModeSeq and a 3-mode QCNet is much smaller. Given our approach\u2019s capability of producing representative trajectories with fewer modes, we believe sequential mode modeling has the potential to be deployed on board. Our future work will focus on further reducing the inference cost by improving the architecture or distilling a small model from a large one, which is necessary for facilitating real-world applications of our approach.\n# 9. More Qualitative Results\nFigure 6 supplements the results in Fig. 4 to demonstrate our approach\u2019s ability to produce representative trajectories and extrapolate more modes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/782e/782e0d64-ec59-4dbd-8584-12f8e1bee138.png\" style=\"width: 50%;\"></div>\nagents in purple are predicted with blue trajectories, with the opacity indicating c\non on the WOMD. The agents in purple are predicted with blue trajectories, with\n(c) #Mode@Training=6, #Mode@Inference=24\n<div style=\"text-align: center;\">(c) #Mode@Training=6, #Mode@Inference=24</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of multimodal motion prediction for traffic agents, which is complicated by the lack of multimodal ground truth data. Previous methods have primarily relied on a winner-take-all training strategy, leading to limited trajectory diversity and misaligned mode confidence. The need for a new approach is underscored by the shortcomings of existing methods, which often require post-processing stages that compromise trajectory accuracy.",
        "problem": {
            "definition": "The problem at hand is to effectively predict multiple plausible future trajectories for traffic agents based on historical motion data, given the inherent multimodality of such predictions.",
            "key obstacle": "The primary challenge is the unavailability of multimodal ground truth, which leads existing methods to adopt strategies that can easily cause mode collapse and produce indistinguishable trajectories."
        },
        "idea": {
            "intuition": "The idea behind ModeSeq is inspired by the need to model modes as sequences, allowing for a more explicit capture of the correlations between different modes of motion.",
            "opinion": "ModeSeq proposes a novel approach to multimodal motion prediction by requiring motion decoders to infer the next mode step by step, rather than generating multiple trajectories in a single shot.",
            "innovation": "The key innovation of ModeSeq lies in its sequential mode modeling framework, which significantly enhances the ability to reason about multimodality and improves trajectory diversity without relying on dense mode prediction."
        },
        "method": {
            "method name": "ModeSeq",
            "method abbreviation": "MS",
            "method definition": "ModeSeq is a framework for multimodal motion prediction that models modes as sequences, allowing for iterative refinement and improved correlation between predicted modes.",
            "method description": "The core of ModeSeq involves sequentially decoding multiple plausible trajectories while capturing the relationships between them.",
            "method steps": [
                "Input historical trajectories and scene embeddings.",
                "Iteratively decode the next mode based on previous modes.",
                "Utilize a prediction head to output trajectories and confidence scores.",
                "Apply Early-Match-Take-All (EMTA) training to optimize the model."
            ],
            "principle": "The effectiveness of ModeSeq stems from its ability to model the sequential dependencies between modes, which allows for better coverage of the multimodal distribution without the pitfalls of traditional methods."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on the Waymo Open Motion Dataset and the Argoverse 2 Motion Forecasting Dataset, with specific metrics for assessing mode coverage, scoring, and trajectory accuracy.",
            "evaluation method": "The performance of ModeSeq was evaluated using metrics such as Miss Rate, minimum Average Displacement Error, and minimum Final Displacement Error, comparing against baseline methods like QCNet and MTR."
        },
        "conclusion": "ModeSeq shows promising results in multimodal motion prediction, achieving state-of-the-art performance on benchmark datasets while demonstrating the ability to extrapolate modes, thus providing a new direction for addressing multimodal challenges.",
        "discussion": {
            "advantage": "The main advantages of ModeSeq include improved trajectory diversity, better mode scoring, and the ability to handle uncertain future scenarios without reliance on complex post-processing.",
            "limitation": "The sequential nature of the method may lead to increased inference latency compared to one-shot predictions, and the current implementation focuses on marginal multi-agent prediction without exploring joint predictions.",
            "future work": "Future research could focus on enhancing the efficiency of the method and extending it to joint multi-agent motion prediction, as well as further refining the architecture to reduce inference costs."
        },
        "other info": {
            "acknowledgement": "This project is supported by a grant from Hong Kong Research Grant Council under GRF project 11216323 and CRF C1042-23G."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem at hand is to effectively predict multiple plausible future trajectories for traffic agents based on historical motion data, given the inherent multimodality of such predictions."
        },
        {
            "section number": "2.2",
            "key information": "The primary challenge is the unavailability of multimodal ground truth, which leads existing methods to adopt strategies that can easily cause mode collapse and produce indistinguishable trajectories."
        },
        {
            "section number": "3.1",
            "key information": "ModeSeq is a framework for multimodal motion prediction that models modes as sequences, allowing for iterative refinement and improved correlation between predicted modes."
        },
        {
            "section number": "3.5",
            "key information": "ModeSeq shows promising results in multimodal motion prediction, achieving state-of-the-art performance on benchmark datasets while demonstrating the ability to extrapolate modes, thus providing a new direction for addressing multimodal challenges."
        },
        {
            "section number": "4.1",
            "key information": "The core of ModeSeq involves sequentially decoding multiple plausible trajectories while capturing the relationships between them."
        },
        {
            "section number": "4.2",
            "key information": "The main advantages of ModeSeq include improved trajectory diversity, better mode scoring, and the ability to handle uncertain future scenarios without reliance on complex post-processing."
        },
        {
            "section number": "7.1",
            "key information": "The sequential nature of the method may lead to increased inference latency compared to one-shot predictions."
        }
    ],
    "similarity_score": 0.5833195661333185,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/ModeSeq_ Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling.json"
}