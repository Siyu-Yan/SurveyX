{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1411.6667",
    "title": "Deletion codes in the high-noise and high-rate regimes",
    "abstract": "The noise model of deletions poses significant challenges in coding theory, with basic questions like the capacity of the binary deletion channel still being open. In this paper, we study the harder model of worst-case deletions, with a focus on constructing efficiently decodable codes for the two extreme regimes of high-noise and high-rate. Specifically, we construct polynomialtime decodable codes with the following trade-offs (for any \u03b5 > 0): (i) Codes that can correct a fraction 1 \u2212\u03b5 of deletions with rate poly(\u03b5) over an alphabet of size poly(1/\u03b5); (ii) Binary codes of rate 1 \u2212\u02dcO(\u221a\u03b5) that can correct a fraction \u03b5 of deletions; and (iii) Binary codes that can be list decoded from a fraction (1/2\u2212\u03b5) of deletions with rate poly(\u03b5). Our work is the first to achieve the qualitative goals of correcting a deletion fraction approaching 1 over bounded alphabets, and correcting a constant fraction of bit deletions with rate approaching 1 over a fixed alphabet. The above results bring our understanding of deletion code constructions in these regimes to a similar level as worst-case errors.",
    "bib_name": "guruswami2014deletioncodeshighnoisehighrate",
    "md_text": "# Deletion codes in the high-noise and high-rate regimes\nVenkatesan Guruswami\u2217\nVenkatesan Guruswami\u2217 Carol Wang\u2020\n# Computer Science Department Carnegie Mellon University Pittsburgh, PA\n  24 Nov 201\n24 Nov 2\n# Abstract\nThe noise model of deletions poses significant challenges in coding theory, with basic questions like the capacity of the binary deletion channel still being open. In this paper, we study the harder model of worst-case deletions, with a focus on constructing efficiently decodable codes for the two extreme regimes of high-noise and high-rate. Specifically, we construct polynomialtime decodable codes with the following trade-offs (for any \u03b5 > 0): (i) Codes that can correct a fraction 1 \u2212\u03b5 of deletions with rate poly(\u03b5) over an alphabet of size poly(1/\u03b5); (ii) Binary codes of rate 1 \u2212\u02dcO(\u221a\u03b5) that can correct a fraction \u03b5 of deletions; and (iii) Binary codes that can be list decoded from a fraction (1/2\u2212\u03b5) of deletions with rate poly(\u03b5). Our work is the first to achieve the qualitative goals of correcting a deletion fraction approaching 1 over bounded alphabets, and correcting a constant fraction of bit deletions with rate approaching 1 over a fixed alphabet. The above results bring our understanding of deletion code constructions in these regimes to a similar level as worst-case errors.\nOur work is the first to achieve the qualitative goals of correcting a deletion fraction approaching 1 over bounded alphabets, and correcting a constant fraction of bit deletions with rate approaching 1 over a fixed alphabet. The above results bring our understanding of deletion code constructions in these regimes to a similar level as worst-case errors.\n\u2217Some of this work was done when the author was a visiting researcher at Microsoft Research New England. Email guruswami@cmu.edu. Research supported in part by NSF grants CCF-0963975 and CCF-1422045. \u2020Part of this work was done on a visit to Microsoft Research New England. Email: wangc@cs.cmu.edu. Supported by an NSF Graduate Research Fellowship and NSF CCF-0963975.\nCarol Wang\u2020\n# 1 Introduction\nThis work addresses the problem of constructing codes which can be efficiently corrected from a constant fraction of worst-case deletions. In contrast to erasures, the locations of deleted symbols are not known to the decoder, who receives only a subsequence of the original codeword. The deletions can be thought of as corresponding to errors in synchronization during communication. The loss of position information makes deletions a very challenging model to cope with, and our understanding of the power and limitations of codes in this model significantly lags behind what is known for worst-case errors. The problem of communicating over the binary deletion channel, in which each transmitted bit is deleted independently with a fixed probability p, has been a subject of much study (see the excellent survey by Mitzenmacher [14] for more background and references). However, even this easier case is not well-understood. In particular, the capacity of the binary deletion channel remains open, although it is known to approach 1 \u2212h(p) as p goes to 0, where h(p) is the binary entropy function (see [4, 5, 22] for lower bounds and [10, 11] for upper bounds), and it is known to be positive (at least (1 \u2212p)/9) [15]) even as p \u21921. The more difficult problem of correcting from adversarial rather than random deletions has also been studied, but with a focus on correcting a constant number (rather than fraction) of deletions. It turns out that obtaining optimal trade-offs to correct a single deletion is already a nontrivial and rich problem (see [20]), and we do not yet have a good understanding for two or more deletions. Coding for a constant fraction of adversarial deletions has been considered previously by Schulman and Zuckerman [18]. They construct constant-rate binary codes which are efficiently decodable from a small constant fraction of worst-case deletions and insertions, and can also handle a small fraction of transpositions. The rate of these codes are bounded away from 1, whereas existentially one can hope to achieve a rate approaching 1 for a small deletion fraction. The central theoretical goal in error-correction against any specific noise model is to understand the combinatorial trade-off between the rate of the code and noise rate that can be corrected, and to construct codes with efficient error-correction algorithms that ideally approach this optimal trade-off. While this challenge is open in general even for the well-studied and simpler model of errors and erasures, in the case of worst-case deletions, our knowledge has even larger gaps. (For instance, we do not know the largest deletion fraction which can be corrected with positive rate for any fixed alphabet size.) Over large alphabets that can grow with the length of the code, we can include the position of each codeword symbol as a header that is part of the symbol. This reduces the model of deletions to that of erasures, where simple optimal constructions (eg. Reed-Solomon codes) are known. Given that we are far from an understanding of the best rate achievable for any specified deletion fraction, in this work we focus on the two extreme regimes \u2014 when the deletion fraction is small (and the code rate can be high), and when the deletion fraction approaches the maximum tolerable value (and the code rate is small). Our emphasis is on constructing codes that can be efficiently encoded and decoded, with trade-offs not much worse than random/inefficient codes (whose parameters we compute in Section 2). Our results, described next, bring the level of knowledge on efficient deletion codes in these regimes to a roughly similar level as worst-case errors. There are numerous open questions, both combinatorial and algorithmic, that remain open, and it is our hope that the systematic study of codes for worst-case deletions undertaken in this work will spur further research on good constructions beyond the extremes of low-noise and high-noise.\nThe best achievable rate against a fraction p of deletions cannot exceed 1 \u2212p, as we need to be able to recover the message from the first (1 \u2212p) fraction of codeword symbols. As mentioned above, over large (growing) alphabets this trade-off can in fact be achieved by a simple reduction to the model of erasures. Existentially, as we show in Section 2, for any \u03b3 > 0, it is easy to show that there are codes of rate 1 \u2212p \u2212\u03b3 to correct a fraction p of deletions over an alphabet size that depends only on \u03b3. For the weaker model of erasures, where the receiver knows the locations of erased symbols, we know explicit codes, namely certain algebraic-geometric codes [19] or expander based constructions [1, 7], achieving the optimal trade-off (rate 1\u2212p\u2212\u03b3 to correct a fraction p of erasures) over alphabets growing only as a function of 1/\u03b3. For deletions, we do not know how to construct codes with such a trade-off efficiently. However, in the high-noise regime when the deletion fraction is p = 1\u2212\u03b5 for some small \u03b5 > 0, we are able to construct codes of rate poly(\u03b5) over an alphabet of size poly(1/\u03b5). Note that an alphabet of size at least 1/\u03b5 is needed, and the rate can be at most \u03b5, even for the simpler model of erasures, so we are off only by polynomial factors. Theorem (Theorem 3.1). Let 1/2 > \u03b5 > 0. There is an explicit code of rate \u2126(\u03b52) and alphabet size poly(1/\u03b5) which can be corrected from a 1 \u2212\u03b5 fraction of worst-case deletions. Moreover, this code can be constructed, encoded, and decoded in time N poly(1/\u03b5), where N is the block length of the code. The above handles the case of very large fraction of deletions. At the other extreme, when the deletion fraction is small, the following result shows that we achieve high rate (approaching one) even over the binary alphabet. Theorem (Theorem 4.1). Let \u03b5 > 0. There is an explicit binary code C \u2286{0, 1}N which is decodable from an \u03b5 fraction of deletions with rate 1 \u2212\u02dcO(\u221a\u03b5) in time N poly(1/\u03b5). Moreover, C can be constructed and encoded in time N poly(1/\u03b5).\nTheorem (Theorem 4.1). Let \u03b5 > 0. There is an explicit binary code C \u2286{0, 1}N which is decodable from an \u03b5 fraction of deletions with rate 1 \u2212\u02dcO(\u221a\u03b5) in time N poly(1/\u03b5). Moreover, C can be constructed and encoded in time N poly(1/\u03b5).\nThe next question is motivated by constructing binary codes for the \u201chigh noise\u201d regime. In this case, we do not know (even non-constructively) the minimum fraction of deletions that forces the rate of the code to approach zero. (Contrast this with the situation for erasures (resp. errors), where we know the zero-rate threshold to be an erasure fraction 1/2 (resp. error fraction 1/4).) Clearly, if the adversary can delete half of the bits, he can always ensure that the decoder receives 0n/2 or 1n/2, so at most two strings can be communicated. Surprisingly, in the model of list decoding, where the decoder is allowed to output a small list consisting of all codewords which contain the received string as a subsequence, one can in fact decode from an deletion fraction arbitrarily close to 1/2, as our third construction shows: Theorem (Theorem 5.3). Let 0 < \u03b5 < 1/2. There is an explicit binary code C \u2286{0, 1}N of rate \u02dc\u2126(\u03b53) which is list-decodable from a 1/2 \u2212\u03b5 fraction of deletions with list size (1/\u03b5)O(log log(1/\u03b5)). This code can be constructed, encoded, and list-decoded in time N poly(1/\u03b5).\nThe next question is motivated by constructing binary codes for the \u201chigh noise\u201d regime. In this case, we do not know (even non-constructively) the minimum fraction of deletions that forces the rate of the code to approach zero. (Contrast this with the situation for erasures (resp. errors), where we know the zero-rate threshold to be an erasure fraction 1/2 (resp. error fraction 1/4).) Clearly, if the adversary can delete half of the bits, he can always ensure that the decoder receives 0n/2 or 1n/2, so at most two strings can be communicated. Surprisingly, in the model of list decoding, where the decoder is allowed to output a small list consisting of all codewords which contain the received string as a subsequence, one can in fact decode from an deletion fraction arbitrarily close to 1/2, as our third construction shows:\nTheorem (Theorem 5.3). Let 0 < \u03b5 < 1/2. There is an explicit binary code C \u2286{0, 1}N of rate \u02dc\u2126(\u03b5 which is list-decodable from a 1/2 \u2212\u03b5 fraction of deletions with list size (1/\u03b5)O(log log(1/\u03b5)). This code can be constructed, encoded, and list-decoded in time N poly(1/\u03b5).\nWe should note that it is not known if list decoding is required to correct deletion fractions close to 1/2, or if one can get by with unique decoding. Our guess would be that the largest deletion fraction unique decodable with binary codes is (noticeably) bounded away from 1/2. The cubic dependence on \u03b5 in the rate in the above theorem is similar to what has been achieved for correcting 1/2\u2212\u03b5 fraction of errors [8]. We anticipate (but have not formally checked) that a similar result holds over any fixed alphabet size k for list decoding from a fraction (1\u22121/k \u2212\u03b5) of symbol deletions.\nConstruction approach. Our codes, like many considered in the past, including those of [2, 3, 17] in the random setting and particularly [18] in the adversarial setting, are based on concatenating a good error-correcting code (in our case, Reed-Solomon or Parvaresh-Vardy codes) with an inner deletion code over a much smaller block length. This smaller block length allows us to find and decode the inner code using brute force. The core of the analysis lies in showing that the adversary can only affect the decoding of a bounded fraction of blocks of the inner code, allowing the outer code to decode using the remaining blocks. While our proofs only rely on elementary combinatorial arguments, some care is needed to execute them without losing in rate (in the case of Theorem 4.1) or in the deletion fraction we can handle (in the case of Theorems 3.1 and 5.3). In particular, for handling close to fraction 1 of deletions, we have to carefully account for errors and erasures of outer Reed-Solomon symbols caused by the inner decoder. To get binary codes of rate approaching 1, we separate inner codeword blocks with (not too long) buffers of 0\u2019s and we exploit some additional structural properties of inner codewords that necessitate many deletions to make them resemble buffers. The difficulty in both these results is unique identification of enough inner codeword boundaries so that the ReedSolomon decoder will find the correct message. The list decoding result is easier to establish, as we can try many different boundaries and use a \u201clist recovery\u201d algorithm for the outer algebraic code. To optimize the rate, we use the Parvaresh-Vardy codes [16] as the outer algebraic code.\n# 1.2 Organization\nIn Section 2, we consider the performance of certain random and greedily constructed codes. These serve both as benchmarks and as starting points for our efficient constructions. In Section 3, we construct codes in the high deletion regime. In Section 4, we give high-rate binary codes which can correct a small constant fraction of deletions. In Section 5, we give list-decodable binary codes up to the optimal error fraction. Some open problems appear in Section 6. Omitted proofs appear in the appendices.\n# 2 Existential bounds for deletion codes\nA quick recap of standard coding terminology: a code C of block length n over an alphabet \u03a3 is a subset C \u2286\u03a3n. The rate of C is defined as log |C| n log |\u03a3|. The encoding function of a code is a map E : [|C|] \u2192\u03a3n whose image equals C (with messages identified with [|C|] in some canonical way). Our constructions all exploit the simple but powerful idea of code concatenation: If Cout \u2286 \u03a3n out is an \u201couter\u201d code with encoding function Eout, and Cin \u2286\u03a3m in is an \u201cinner\u201d code encoding function Ein : \u03a3out \u2192\u03a3m in, the the concatenated code Cout \u25e6Cin \u2286\u03a3nm in is a code whose encoding function first applies Eout to the message, and then applies Ein to each symbol of the resulting outer codeword. In this section, we show the existence of deletion codes in certain ranges of parameters, without the requirement of efficient encoding or decoding. The proofs (found in the appendix) follow from standard probabilistic arguments, but to the best of our knowledge, these bounds were not known previously. The codes of Theorem 2.4 will be used as inner codes in our final concatenated constructions. Throughout, we will write [k] for the set {1, . . . , k}. We will also use the binary entropy function, defined for \u03b4 \u2208[0, 1] as h(\u03b4) = \u03b4 log 1 \u03b4 + (1 \u2212\u03b4) log 1 1\u2212\u03b4. All logarithms in the paper are to base 2.\nWe note that constructing a large code over [k]m which can correct from a \u03b4 fraction of deletions is equivalent to constructing a large set of strings such that for each pair, their longest common subsequence (LCS) has length less than (1 \u2212\u03b4)m. We first consider how well a random code performs, using the following theorem from [13], which upper bounds the probability that a pair of randomly chosen strings has a long LCS. Theorem 2.1 ([13], Theorem 1). For every \u03b3 > 0, there exists c > 0 such that if k and m/ \u221a k are sufficiently large, and u, v are chosen independently and uniformly from [k]m, then\nTheorem 2.1 ([13], Theorem 1). For every \u03b3 > 0, there exists c > 0 such that if k and m/ \u221a k are sufficiently large, and u, v are chosen independently and uniformly from [k]m, then\nProposition 2.2. Let \u03b5 > 0 be sufficiently small and let k = (4/\u03b5)2. There exists a code C \u2286[k]m of rat R = O \ufffd \u03b5/ log(1/\u03b5) \ufffd which can correct a 1 \u2212\u03b5 = 1 \u22124/ \u221a k fraction of deletions.\n\ufffd \ufffd  \u2212 \u2212 The following results, and in particular Corollary 2.6, show that we can nearly match the performance of random codes using a simple greedy algorithm. We first bound the number of strings which can have a fixed string s as a subsequence. Lemma 2.3. Let \u03b4 \u2208(0, 1/k), set \u2113= (1 \u2212\u03b4)m, and let s \u2208[k]\u2113. The number of strings s\u2032 \u2208[k]m containing s as a subsequence is at most\n\ufffd \ufffd The following results, and in particular Corollary 2.6, show that we can nearly match the per formance of random codes using a simple greedy algorithm. We first bound the number of strings which can have a fixed string s as a subsequence.\nLemma 2.3. Let \u03b4 \u2208(0, 1/k), set \u2113= (1 \u2212\u03b4)m, and let s \u2208[k]\u2113. The number of strings s\u2032 \u2208[k]m containing s as a subsequence is at most\nWhen k = 2, we have the estimate\nTheorem 2.4. Let \u03b4, \u03b3 > 0. Then for every m, there exists a code C \u2286[k]m of rate R = 1 \u2212\u03b4 \u2212\u03b3 such that: \u2022 C can be corrected from a \u03b4 fraction of worst-case deletions, provided k \u2a7e22h(\u03b4)/\u03b3. \u2022 C can be found, encoded, and decoded in time kO(m). Moreover, when k = 2, we may take R = 1 \u22122h(\u03b4) \u2212log(\u03b4m)/m. Remark. The authors of [12] show a similar result for the binary case, but use the weaker bound in Lemma 2.3 to get a rate of 1 \u2212\u03b4 \u22122h(\u03b4). With a slight modification to the proof of Theorem 2.4, we obtain the following construction, which will be used in Section 4. Proposition 2.5. Let \u03b4, \u03b2 \u2208(0, 1). Then for every m, there exists a code C \u2286{0, 1}m of rate R = 1 \u22122h(\u03b4) \u2212O(log(\u03b4m)/m) \u22122\u2212\u2126(\u03b2m)/m such that: \u2022 For every string s \u2208C, s is \u201c\u03b2-dense\u201d: every interval of length \u03b2m in s contains at least \u03b2m/10 1\u2019s,\n\u2022 C can be corrected from a \u03b4 fraction of worst-case deletions, and \u2022 C can be found, encoded, and decoded in time 2O(m).\n\u2022 C can be corrected from a \u03b4 fraction of worst-case deletions, and \u2022 C can be found, encoded, and decoded in time 2O(m). In the high-deletion regime, we have the following corollary to Theorem 2.4, obtained by set ting \u03b4 = 1 \u2212\u03b5 and \u03b3 = (1 \u2212\u03b8)\u03b5, and noting that h(\u03b5) \u2a7d\u03b5 log(1/\u03b5) + 2\u03b5 when \u03b5 < 1/2. Corollary 2.6. Let 1/2 > \u03b5 > 0 and \u03b8 \u2208(0, 1/3]. There for every m, exists a code C \u2286[k]m of rate R = \u03b5 \u00b7 \u03b8 which can correct a 1 \u2212\u03b5 fraction of deletions in time kO(m), provided k \u2a7e64/\u03b5 2 1\u2212\u03b8 .\nIn the high-deletion regime, we have the following corollary to Theorem 2.4, obtained by setting \u03b4 = 1 \u2212\u03b5 and \u03b3 = (1 \u2212\u03b8)\u03b5, and noting that h(\u03b5) \u2a7d\u03b5 log(1/\u03b5) + 2\u03b5 when \u03b5 < 1/2. Corollary 2.6. Let 1/2 > \u03b5 > 0 and \u03b8 \u2208(0, 1/3]. There for every m, exists a code C \u2286[k]m of rate R = \u03b5 \u00b7 \u03b8 which can correct a 1 \u2212\u03b5 fraction of deletions in time kO(m), provided k \u2a7e64/\u03b5 2 1\u2212\u03b8 .\n# 3 Coding against 1 \u2212\u03b5 deletions\nIn this section, we construct codes for the high-deletion regime. More precisely, we have th following theorem.\nIn this section, we construct codes for the high-deletion regime. More precisely, we have the following theorem. Theorem 3.1. Let 1/2 > \u03b5 > 0. There is an explicit code of rate \u2126(\u03b52) and alphabet size poly(1/\u03b5) which can be corrected from a 1 \u2212\u03b5 fraction of worst-case deletions. Moreover, this code can be constructed, encoded, and decoded in time N poly(1/\u03b5), where N is the block length of the code. We first define the code. Theorem 3.1 is then a direct corollary of Lemmas 3.2 and 3.3. The code: Our code will be over the alphabet {0, 1, . . . , D \u22121} \u00d7 [k], where D = 8/\u03b5 and k = O(1/\u03b53). We first define a code C\u2032 over the alphabet [k] by concatenating a Reed-Solomon code with a deletion code constructed using Corollary 2.6, setting \u03b8 = 1/3. More specifically, let Fq be a finite field. For any n\u2032 \u2a7dn \u2a7dq, the Reed-Solomon code of length n \u2a7dq and dimension n\u2032 is a subset of Fn q which admits an efficient algorithm to uniquely decode from t errors and r erasures, provided r + 2t < n \u2212n\u2032 (see, for example, [21]). In our construction, we will take n = q = 2n\u2032/\u03b5. We first encode our message to a codeword c = (c1, . . . , cn) of the Reed-Solomon code. For each i, we then encode the pair (i, ci) using Corollary 2.6 by a code C1 : [n] \u00d7 Fq \u2192[k]m, where m = 12 log q/\u03b5, which can correct a 1 \u2212\u03b5/2 fraction of deletions. To obtain our final code C, we replace every symbol s in C\u2032 which encodes the ith RS coordinate by the pair \ufffd i (mod D), s \ufffd \u2208{0, 1, . . . , D \u22121} \u00d7 [k]. The first coordinate, i (mod D), contains the location of the codeword symbol modulo D, and we will refer to it as a header. Lemma 3.2. The rate of C is \u2126(\u03b52). Proof. The rate of the outer Reed-Solomon code, labeled with indices, is at least \u03b5/4. The rate of the inner code can be taken to be \u2126(\u03b5), by Corollary 2.6. Finally, the alphabet increase in transforming C\u2032 to C decreases the rate by a factor of log(k) log(Dk) = \u2126(1). In particular, this gives us a final rate of \u2126(\u03b52). Lemma 3.3. The code C can be decoded from a 1 \u2212\u03b5 fraction of worst-case deletions in time N O(poly 1/\u03b5). Proof. Let N be the block length of C. We apply the following algorithm to decode C.\nTheorem 3.1. Let 1/2 > \u03b5 > 0. There is an explicit code of rate \u2126(\u03b52) and alphabet size poly(1/\u03b5) which can be corrected from a 1 \u2212\u03b5 fraction of worst-case deletions. Moreover, this code can be constructed, encoded, and decoded in time N poly(1/\u03b5), where N is the block length of the code.\nLemma 3.3. The code C can be decoded from a 1 \u2212\u03b5 fraction of worst-case deletions in time N O(poly 1/\u03b5 Proof. Let N be the block length of C. We apply the following algorithm to decode C.\n- We begin with an empty set. For each block which is of length between \u03b5m/2 and m, we remove the headers by replacing each symbol (a, b) with the second coordinate b. We then apply the decoder from Corollary 2.6 to the block. If this succeeds, outputting a pair (i, ri), then we add (i, ri) to L. This takes time N poly(1/\u03b5). - If for any i, L contains multiple pairs with first coordinate i, we remove all such pairs from L. L thus contains at most one pair (i, ri) for each index i. We apply the Reed-Solomon decoding algorithm to the string r whose ith coordinate is ri if (i, ri) \u2208L and erased otherwise. This takes time poly(N). Analysis: For any i, we will decode a correct coordinate \ufffd i, ci \ufffd if there is a block of length at least \u03b5m/2 which is a subsequence of C1(i, ci). (Here and in what follows we abuse notation by disregarding headers on codeword symbols.) Thus, the Reed-Solomon decoder will receive the correct value of the ith coordinate unless one of the following occurs: 1. (Erasure) The adversary deletes a \u2a7e1 \u2212\u03b5/2 fraction of C1(i, ci). 2. (Merge) The block containing (part of) C1(i, ci) also contains symbols from other codewords of C1, because the adversary has erased the codewords separating C1(i, ci) from its neighbors with the same header. 3. (Conflict) Another block decodes to (i, r) for some r. Note that an erasure cannot cause a coordinate to decode incorrectly, so a conflict can only occur from a merge. We would now like to bound the number of errors and erasures the adversary can cause. - If the adversary causes an erasure without causing a merge, this requires at least (1 \u2212\u03b5/2)m deletions within the block which is erased, and no other block is affected. - If the adversary merges t inner codewords with the same label, this requires at least (t \u2212 1)(D \u22121)m deletions, of the intervening codewords with different labels. The merge causes the fully deleted inner codewords to be erased, and causes the t merged codewords to resolve into at most one (possibly incorrect) value. This value, if incorrect, could also cause one conflict. In summary, in order to cause one error and r \u2a7d(t \u22121)D + 2 erasures, the adversary must introduce at least (t \u22121)(D \u22121)m \u2a7e(2 + r)(1 \u2212\u03b5/2)m deletions. In particular, if the adversary causes s errors and r1 erasures by merging, and r2 erasures without merging, this requires at least \u2a7e(2s + r1)(1 \u2212\u03b5/2)m + r2(1 \u2212\u03b5/2)m = (2s + r)(1 \u2212\u03b5/2)m deletions. Thus, when the adversary deletes at most a (1 \u2212\u03b5) fraction of codeword symbols, we have that 2s + r is at most (1 \u2212\u03b5)mn/(1 \u2212\u03b5/2)m < n(1 \u2212\u03b5/2). Recalling that the Reed-Solomon decoder in the final step will succeed as long as 2s+r < n(1\u2212\u03b5/2), we conclude that the decoding algorithm will output the correct message.\n If for any i, L contains multiple pairs with first coordinate i, we remove all such pairs from L. L thus contains at most one pair (i, ri) for each index i. We apply the Reed-Solomon de coding algorithm to the string r whose ith coordinate is ri if (i, ri) \u2208L and erased otherwise This takes time poly(N).\nAnalysis: For any i, we will decode a correct coordinate \ufffd i, ci \ufffd if there is a block of length at least \u03b5m/2 which is a subsequence of C1(i, ci). (Here and in what follows we abuse notation by disregarding headers on codeword symbols.) Thus, the Reed-Solomon decoder will receive the correct value of the ith coordinate unless one of the following occurs:\n# In particular, if the adversary causes s errors and r1 erasures by merging, and r2 era without merging, this requires at least\ndeletions. Thus, when the adversary deletes at most a (1 \u2212\u03b5) fraction of codeword symbols, we have that 2s + r is at most (1 \u2212\u03b5)mn/(1 \u2212\u03b5/2)m < n(1 \u2212\u03b5/2). Recalling that the Reed-Solomon decoder in the final step will succeed as long as 2s+r < n(1\u2212\u03b5/2), we conclude that the decoding algorithm will output the correct message.\n# 4 Binary codes against \u03b5 deletions\n# 4.1 Construction overview\nThe goal in our constructions is to allow the decoder to approximately locate the boundaries between codewords of the inner code, in order to recover the symbols of the outer code. In the previous section, we were able to achieve this by augmenting the alphabet and letting each symbol encode some information about the block to which it belongs. In the binary case, we no longer have this luxury. The basic idea of our code is to insert long runs of zeros, or \u201cbuffers,\u201d between adjacent inner codewords. The buffers are long enough that the adversary cannot destroy many of them. If we then choose the inner code to be dense (in the sense of Proposition 2.5), it is also difficult for a long interval in any codeword to be confused for a buffer. This approach optimizes that of [18], which uses an inner code of rate 1/2 and thus has final rate bounded away from 1. The balance of buffer length and inner codeword density seems to make buffered codes unsuited for high deletion fractions, and indeed our results only hold as the deletion fraction goes to zero.\n# 4.2 Our construction\nWe now give the details of our construction. For simplicity, we will not optimize constants in the analysis.\nTheorem 4.1. Let \u03b5 > 0. There is an explicit binary code C \u2286{0, 1}N which is decodable from an \u03b5 fraction of deletions with rate 1 \u2212\u02dcO(\u221a\u03b5) in time N poly(1/\u03b5). Moreover, C can be constructed and encoded in time N poly(1/\u03b5).\nThe code: We again use a concatenated construction with a Reed-Solomon code as the outer code, choosing one which can correct a 12\u221a\u03b5 fraction of errors and erasures. For each i, we replace the ith coordinate ci with the pair (i, ci). In order to ensure that the rate stays high, we use a RS code over Fqh, with block length n = q, where we will take h = 1/\u03b5. The inner code will be a good binary deletion code C1 of block length m correcting a \u03b4 = 40\u221a\u03b5 fraction of deletions, found using Proposition 2.5, with \u03b2 = \u03b4/4. Recall that this code only contains \u201c\u03b2-dense strings,\u201d for which any interval of length \u03b2m contains \u03b2m/10 1\u2019s. We will assume each codeword begins and ends with a 1. Now, between each pair of adjacent inner codewords of C1, we insert a buffer of \u03b4m zeros. This gives us our final code C.\nLemma 4.2. The rate of C is 1 \u2212\u02dcO(\u221a\u03b5).\n# Lemma 4.2. The rate of C is 1 \u2212\u02dcO(\u221a\u03b5).\nProof. The rate of the outer (labeled) Reed-Solomon code is (1 \u221224\u221a\u03b5) \u00b7 h h+1. The rate of the inne code C1 can be taken to be 1 \u22122h(\u03b4) \u2212o(1), by Proposition 2.5. Finally, adding buffers reduces th rate by a factor of 1 1+\u03b4. Combining these with our choice of \u03b4, we get that the rate of C is 1 \u2212\u02dcO(\u221a\u03b5). Lemma 4.3. The code C can be decoded from an \u03b5 fraction of worst-case deletions in time N poly(1/\u03b5).\nThese runs (\u201cbuffers\u201d) are removed, dividing the codeword into blocks of contiguous symbols which we will call decoding windows. Any leading zeroes of the first decoding window and trailing zeroes of the last decoding window are also removed. This takes time poly(N). - We begin with an empty set L. For each decoding window, we apply the decoder from Proposition 2.5 to attempt to recover a pair (i, ri). If we succeed, this pair is added to L. This takes time N poly(1/\u03b5). - If for any i, L contains multiple pairs with first coordinate i, we remove all such pairs from L. L thus contains at most one pair (i, ri) for each index i. We apply the Reed-Solomon decoding algorithm to the string r whose ith coordinate is ri if (i, ri) \u2208L and erased otherwise, attempting to recover from a 12\u221a\u03b5 fraction of errors and erasures. This takes time poly(N). Analysis: Notice that if no deletions occur, the decoding windows will all be codewords of the inner code C1, which will be correctly decoded. At a high level, we will show that the adversary cannot corrupt many of these decoding windows, even with an \u03b5 fraction of deletions. We first show that the number of decoding windows considered by our algorithm is close to n, the number of windows if there are no deletions. Lemma 4.4. If an \u03b5 fraction of deletions have occurred, then the number of decoding windows considered by our algorithm is between (1 \u22122\u221a\u03b5)n and (1 + 2\u221a\u03b5)n. Proof. Recall that the adversary can cause at most \u03b5nm(1 + \u03b4) \u2a7d2\u03b5nm deletions. Upper bound: The adversary can increase the number of decoding windows only by creating new runs of \u03b4m/2 zeroes (that are not contained within a buffer). Such a new run must be contained entirely within an inner codeword w \u2208C1. However, as w is \u03b4/4-dense, in order to create a run of zeroes of length \u03b4m/2, at least \u03b4m/20 = 2\u221a\u03b5 1\u2019s must be deleted for each such run. In particular, at most \u221a\u03b5n blocks can be added. Lower bound: The adversary can decrease the number of decoding windows only by decreasing the number of buffers. He can achieve this either by removing a buffer, or by merging two buffers. Removing a buffer requires deleting \u03b4m/2 = 20\u221a\u03b5m zeroes from the original buffer. Merging two buffers requires deleting all 1\u2019s in the inner codewords between them. As inner codewords are \u03b4/4-dense, this requires at least \u221a\u03b5m deletions for each merged buffer. In particular, at most 2\u221a\u03b5n buffers can be removed. We now show that almost all of the decoding windows being considered are decoded correctly by the inner decoder. Lemma 4.5. The number of decoding windows which are incorrectly decoded is at most 4\u221a\u03b5n. Proof. The inner decoder will succeed on each decoding window which is a subsequence of a valid inner codeword w \u2208C1 of length at least (1 \u2212\u03b4)m. This will happen unless:\n We begin with an empty set L. For each decoding window, we apply the decoder from Proposition 2.5 to attempt to recove a pair (i, ri). If we succeed, this pair is added to L. This takes time N poly(1/\u03b5).\n If for any i, L contains multiple pairs with first coordinate i, we remove all such pairs from L. L thus contains at most one pair (i, ri) for each index i. We apply the Reed-Solomon decoding algorithm to the string r whose ith coordinate is ri if (i, ri) \u2208L and erased otherwise, attempting to recover from a 12\u221a\u03b5 fraction of errors and erasures. This takes time poly(N).\ncoding algorithm to the string r whose ith coordinate is ri if (i, ri) \u2208L and erased otherwise, attempting to recover from a 12\u221a\u03b5 fraction of errors and erasures. This takes time poly(N). Analysis: Notice that if no deletions occur, the decoding windows will all be codewords of the inner code C1, which will be correctly decoded. At a high level, we will show that the adversary cannot corrupt many of these decoding windows, even with an \u03b5 fraction of deletions. We first show that the number of decoding windows considered by our algorithm is close to n, the number of windows if there are no deletions. Lemma 4.4. If an \u03b5 fraction of deletions have occurred, then the number of decoding windows considered by our algorithm is between (1 \u22122\u221a\u03b5)n and (1 + 2\u221a\u03b5)n. Proof. Recall that the adversary can cause at most \u03b5nm(1 + \u03b4) \u2a7d2\u03b5nm deletions. Upper bound: The adversary can increase the number of decoding windows only by creating new runs of \u03b4m/2 zeroes (that are not contained within a buffer). Such a new run must be contained entirely within an inner codeword w \u2208C1. However, as w is \u03b4/4-dense, in order to create a run of zeroes of length \u03b4m/2, at least \u03b4m/20 = 2\u221a\u03b5 1\u2019s must be deleted for each such run. In particular, at most \u221a\u03b5n blocks can be added. Lower bound: The adversary can decrease the number of decoding windows only by decreasing the number of buffers. He can achieve this either by removing a buffer, or by merging two buffers. Removing a buffer requires deleting \u03b4m/2 = 20\u221a\u03b5m zeroes from the original buffer. Merging two buffers requires deleting all 1\u2019s in the inner codewords between them. As inner codewords are \u03b4/4-dense, this requires at least \u221a\u03b5m deletions for each merged buffer. In particular, at most 2\u221a\u03b5n buffers can be removed.\nLemma 4.4. If an \u03b5 fraction of deletions have occurred, then the number of decoding windows considere by our algorithm is between (1 \u22122\u221a\u03b5)n and (1 + 2\u221a\u03b5)n.\nUpper bound: The adversary can increase the number of decoding windows only by creating new runs of \u03b4m/2 zeroes (that are not contained within a buffer). Such a new run must be contained entirely within an inner codeword w \u2208C1. However, as w is \u03b4/4-dense, in order to create a run of zeroes of length \u03b4m/2, at least \u03b4m/20 = 2\u221a\u03b5 1\u2019s must be deleted for each such run. In particular, at most \u221a\u03b5n blocks can be added. Lower bound: The adversary can decrease the number of decoding windows only by decreasing the number of buffers. He can achieve this either by removing a buffer, or by merging two buffers. Removing a buffer requires deleting \u03b4m/2 = 20\u221a\u03b5m zeroes from the original buffer. Merging two buffers requires deleting all 1\u2019s in the inner codewords between them. As inner codewords are \u03b4/4-dense, this requires at least \u221a\u03b5m deletions for each merged buffer. In particular, at most 2\u221a\u03b5n buffers can be removed.\nWe now show that almost all of the decoding windows being considered are decoded correctl by the inner decoder.\nLemma 4.5. The number of decoding windows which are incorrectly decoded is at most 4\u221a\u03b5n.\nProof. The inner decoder will succeed on each decoding window which is a subsequence of a valid inner codeword w \u2208C1 of length at least (1 \u2212\u03b4)m. This will happen unless:\n1. The window is too short:\n(a) a subsequence of w has been marked as a (new) buffer, or (b) a \u03c1 fraction of w has been marked as part of the adjacent buffers, combined with a \u03b4 \u2212\u03c1 fraction of deletions within w.\n2. The window is not a subsequence of a valid inner codeword: the window contains buffer symbols and/or a subsequence of multiple inner codewords.\nWe first show that (1) holds for at most 3\u221a\u03b5n windows. From the proof of Lemma 4.4, there can be at most \u221a\u03b5n new buffers introduced, thus handling Case 1(a). In Case 1(b), if \u03c1 < \u03b4/2, then there must be \u03b4/2 deletions within w. On the other hand if \u03c1 \u2a7e\u03b4/2, one of two buffers adjacent to w must have absorbed at least \u03b4m/4 symbols of w, so as w is \u03b4/4-dense, this requires \u03b4m/40 = \u221a\u03b5m deletions, so can occur in at most 2\u221a\u03b5n windows.\nWe first show that (1) holds for at most 3\u221a\u03b5n windows. From the proof of Lemma 4.4, there can be at most \u221a\u03b5n new buffers introduced, thus handling Case 1(a). In Case 1(b), if \u03c1 < \u03b4/2, then there must be \u03b4/2 deletions within w. On the other hand, if \u03c1 \u2a7e\u03b4/2, one of two buffers adjacent to w must have absorbed at least \u03b4m/4 symbols of w, so as w is \u03b4/4-dense, this requires \u03b4m/40 = \u221a\u03b5m deletions, so can occur in at most 2\u221a\u03b5n windows. We also have that (2) holds for at most \u221a\u03b5n windows, as at least \u03b4m/2 symbols must be deleted from a buffer in order to prevent the algorithm from marking it as a buffer. As in Lemma 4.4, this requires 20\u221a\u03b5 deletions for each merged window, and so there are at most \u221a\u03b5n windows satisfying\nWe first show that (1) holds for at most 3\u221a\u03b5n windows. From the proof of Lemma 4.4, there can be at most \u221a\u03b5n new buffers introduced, thus handling Case 1(a). In Case 1(b), if \u03c1 < \u03b4/2, then there must be \u03b4/2 deletions within w. On the other hand, if \u03c1 \u2a7e\u03b4/2, one of two buffers adjacent to w must have absorbed at least \u03b4m/4 symbols of w, so as w is \u03b4/4-dense, this requires \u03b4m/40 = \u221a\u03b5m deletions, so can occur in at most 2\u221a\u03b5n windows. We also have that (2) holds for at most \u221a\u03b5n windows, as at least \u03b4m/2 symbols must be deleted from a buffer in order to prevent the algorithm from marking it as a buffer. As in Lemma 4.4, this requires 20\u221a\u03b5 deletions for each merged window, and so there are at most \u221a\u03b5n windows satisfying case (2). We now have that the inner decoder outputs (1\u22126\u221a\u03b5)n correct values. After removing possible conflicts in the last step of the algorithm, we have at least (1 \u221212\u221a\u03b5)n correct values, so that the Reed-Solomon decoder will succeed and output the correct message.\nWe also have that (2) holds for at most \u221a\u03b5n windows, as at least \u03b4m/2 symbols must be deleted from a buffer in order to prevent the algorithm from marking it as a buffer. As in Lemma 4.4, this requires 20\u221a\u03b5 deletions for each merged window, and so there are at most \u221a\u03b5n windows satisfying case (2).\nWe now have that the inner decoder outputs (1\u22126\u221a\u03b5)n correct values. After removing possible conflicts in the last step of the algorithm, we have at least (1 \u221212\u221a\u03b5)n correct values, so that the Reed-Solomon decoder will succeed and output the correct message.\n# 5 List-decoding binary deletion codes\nThe results of Section 4 show that we can have good explicit binary codes when the deletion fraction is low. In this section, we address the opposite regime, of high deletion fraction. As a first step, notice that in any reasonable model, including list-decoding, we cannot hope to efficiently decode from a 1/2 deletion fraction with a polynomial list size and constant rate. With block length n and n/2 deletions, the adversary can ensure that what is received is either n/2 1\u2019s or n/2 0\u2019s. Thus, for binary codes and \u03b5 > 0, we will consider the question of whether it is possible to list decode from a fraction 1/2 \u2212\u03b5 of deletions.\nDefinition 5.1. We say that a code C \u2286{0, 1}m is list-decodable from a \u03b4 deletion fraction with list size L if every sequence of length (1 \u2212\u03b4)m is a subsequence of at most L codewords. If this is the case, we will call C (\u03b4, L) list-decodable from deletions.\nRemark. Although the results of this section are proven in the setting of list-decoding, it is not known that we cannot have unique decoding of binary codes up to deletion fraction 1/2 \u2212\u03b5. See the first open problem in Section 6.\n# 5.1 List-decodable binary deletion codes (existent\nIn this section, we show that good list-decodable codes exist. This construction will be the basis of our explicit construction of list-decodable binary codes. The proof appears in the appendix.\nTheorem 5.2. Let \u03b4, L > 0. Let C \u2286{0, 1}m consist of 2Rm independently, uniformly chosen strings, where R \u2a7d1\u2212h(\u03b4)\u22123/L. Then C is \ufffd \u03b4, L \ufffd list-decodable from deletions with probability at least 1\u22122\u2212m. Moreover, such a code can be constructed and decoded in time 2poly(mL). In particular, when \u03b4 = 1/2 \u2212\u03b5, we can construct and decode in time 2poly(m/\u03b5) a code C \u2286{0, 1}m of rate \u2126(\u03b52) which is \ufffd \u03b4, O(1/\u03b52) \ufffd list-decodable from deletions.\n# 5.2 List-decodable binary deletion codes (explicit)\nWe now use the existential construction of Theorem 5.2 to give an explicit construction of constantrate list-decodable binary codes. Our code construction uses Parvaresh-Vardy codes ([16]) as outer codes, and an inner code constructed using Section 5.1. The idea is to list-decode \u201cenough\u201d windows and then apply the list recovery algorithm of Theorem 5.4. Theorem 5.3. Let 0 < \u03b5 < 1/2. There is an explicit binary code C \u2286{0, 1}N of rate \u02dc\u2126(\u03b53) which is list-decodable from a 1/2 \u2212\u03b5 fraction of deletions with list size (1/\u03b5)O(log log \u03b5). This code can be constructed, encoded, and list-decoded in time N poly(1/\u03b5).\nTheorem 5.3. Let 0 < \u03b5 < 1/2. There is an explicit binary code C \u2286{0, 1}N of rate \u02dc\u2126(\u03b53) which is list-decodable from a 1/2 \u2212\u03b5 fraction of deletions with list size (1/\u03b5)O(log log \u03b5). This code can be constructed, encoded, and list-decoded in time N poly(1/\u03b5).\nTheorem 5.4 ([9], Corollary 5). For all integers s \u2a7e1, for all prime powers r, every pair of integ 1 < K \u2a7dN \u2a7dq, there is an explicit Fr-linear map E : FK q \u2192FN qs whose image C\u2032 is a code satisfying:\n- There is an algorithm which, given a collection of subsets Si \u2286Fqs for i \u2208[N] with \ufffd i|Si| \u2a7d N\u2113, runs in poly \ufffd (rs)s, q, \u2113 \ufffd time, and outputs a list of size at most O \ufffd (rs)sN\u2113/K \ufffd that includes precisely the set of codewords (c1, . . . , cN) \u2208C\u2032 that satisfy ci \u2208Si for at least \u03b1N values of i, provided\nThe code: We set s = O(log 1/\u03b5), r = O(1), and N = K poly \ufffd log(1/\u03b5) \ufffd /\u03b5 in Theorem 5.4 in order to obtain a code C\u2032 \u2286FN qs. We modify the code, replacing the ith coordinate ci with the pair (i, ci) for each i, in order to obtain a code C\u2032\u2032. This latter step only reduces the rate by a constant factor. Recall that we are trying to recover from a 1/2 \u2212\u03b5 fraction of deletions. We use Theorem 5.2 to construct an inner code C1 : [N] \u00d7 Fs q \u2192{0, 1}m of rate \u2126(\u03b52) which recovers from a 1/2 \u2212\u03b4 deletion fraction (where we will set \u03b4 = \u03b5/4). Our final code C is a concatenation of C\u2032\u2032 with C1, which has rate \u02dc\u2126(\u03b53).\nTheorem 5.5. C is list-decodable from a 1/2 \u2212\u03b5 fraction of deletions in time N poly(1/\u03b5).\nProof. Our algorithm first defines a set of \u201cdecoding windows\u201d. These are intervals of length (1/2+ \u03b4)m in the received codeword which start at positions 1+ t\u03b4m for t = 0, 1, . . . , N/\u03b4 \u2212(1/2+ \u03b4)/\u03b4, in addition to one interval consisting of the last (1/2+\u03b4)m symbols in the received codeword. We use the algorithm of Theorem 5.2 to list-decode each decoding window, and let L be the union of the lists for each window. Finally, we apply the algorithm of Theorem 5.4 to L to obtain a list containing the original message. Correctness: Let c = (c1, . . . , cN) be the originally transmitted codeword of C\u2032. If an inner codeword C1(i, ci) has suffered fewer than a 1/2\u22122\u03b4 fraction of deletions, then one of the decoding windows is a substring of C1(i, ci), and L will contain the correct pair (i, ci).\nCorrectness: Let c = (c1, . . . , cN) be the originally transmitted codeword of C\u2032. If an inne codeword C1(i, ci) has suffered fewer than a 1/2\u22122\u03b4 fraction of deletions, then one of the decodin windows is a substring of C1(i, ci), and L will contain the correct pair (i, ci).\nWhen \u03b4 = \u03b5/4, by a simple averaging argument, we have that an \u03b5 fraction of inner codewords have at most 1/2\u22122\u03b4 fraction of positions deleted. For these inner codewords, L contains a correct decoding of the corresponding symbol of c. In summary, we have list-decoded at most N/\u03b4 windows, with a list size of O(1/\u03b42) each. We also have that an \u03b5 fraction of symbols in the outer codeword of C\u2032 is correct. Setting \u2113= O(1/\u03b43) in the algorithm of Theorem 5.4, we can take \u03b1 = \u03b5. Theorem 5.4 then guarantees that the decoder will output a list of poly(1/\u03b5) codewords, including the correct codeword c.\nWhen \u03b4 = \u03b5/4, by a simple averaging argument, we have that an \u03b5 fraction of inner codewords have at most 1/2\u22122\u03b4 fraction of positions deleted. For these inner codewords, L contains a correct decoding of the corresponding symbol of c.\nIn summary, we have list-decoded at most N/\u03b4 windows, with a list size of O(1/\u03b42) each. W also have that an \u03b5 fraction of symbols in the outer codeword of C\u2032 is correct. Setting \u2113= O(1/\u03b43 in the algorithm of Theorem 5.4, we can take \u03b1 = \u03b5. Theorem 5.4 then guarantees that the decode will output a list of poly(1/\u03b5) codewords, including the correct codeword c.\n# 6 Conclusion and open problems\nIn this work, we initiated a systematic study of codes for the adversarial deletion model, with an eye towards constructing codes achieving more-or-less the correct trade-offs at the high-noise and high-rate regimes. There are still several major gaps in our understanding of deletion codes, and below we highlight some of them (focusing only on the worst-case model):\n1. For binary codes, what is the supremum p\u2217of all fractions p of adversarial deletions for which one can have positive rate? Clearly p\u2217\u2a7d1/2; could it be that p\u2217= 1/2 and this trivial limit can be matched? Or is it the case that p\u2217is strictly less than 1/2? Note that by [12], p\u2217> .17. 2. The above question, but now for an alphabet of size k \u2014 at what value of p\u2217(k) does the achievable rate against a fraction p\u2217(k) of worst-case symbol deletions vanish? It is known that 1 k \u2a7d1 \u2212p\u2217(k) \u2a7dO \ufffd 1 \u221a k \ufffd (the upper bound is established in Section 2). Which (if either) bound is asymptotically the right one? 3. Can one construct codes of rate 1 \u2212p \u2212\u03b3 to efficiently correct a fraction p of deletions over an alphabet size that only depends on \u03b3? Note that this requires a relative distance of p, and currently we only know algebraic-geometric and expander-based codes which achieve such a tradeoff between rate and relative distance. 4. Can one improve the rate of the binary code construction to correct a fraction \u03b5 of deletions to 1 \u2212\u03b5 poly(log(1/\u03b5)), approaching more closely the existential 1 \u2212O(\u03b5 log(1/\u03b5)) bound? In the case of errors, an approach using expanders gives the analogous tradeoff (see [6] and references therein). Could such an approach be adapted to the setting of deletions? 5. Can one improve the N poly(1/\u03b5) type dependence of our construction and decoding complexity to, say, exp(poly(1/\u03b5))N c for some exponent c that doesn\u2019t depend on \u03b5?\n1. For binary codes, what is the supremum p\u2217of all fractions p of adversarial deletions for which one can have positive rate? Clearly p\u2217\u2a7d1/2; could it be that p\u2217= 1/2 and this trivial limit can be matched? Or is it the case that p\u2217is strictly less than 1/2? Note that by [12], p\u2217> .17. 2. The above question, but now for an alphabet of size k \u2014 at what value of p\u2217(k) does the achievable rate against a fraction p\u2217(k) of worst-case symbol deletions vanish? It is known that 1 k \u2a7d1 \u2212p\u2217(k) \u2a7dO \ufffd 1 \u221a k \ufffd (the upper bound is established in Section 2). Which (if either) bound is asymptotically the right one? 3. Can one construct codes of rate 1 \u2212p \u2212\u03b3 to efficiently correct a fraction p of deletions over an alphabet size that only depends on \u03b3? Note that this requires a relative distance of p, and currently we only know algebraic-geometri and expander-based codes which achieve such a tradeoff between rate and relative distance. 4. Can one improve the rate of the binary code construction to correct a fraction \u03b5 of deletions to 1 \u2212\u03b5 poly(log(1/\u03b5)), approaching more closely the existential 1 \u2212O(\u03b5 log(1/\u03b5)) bound? In the case of errors, an approach using expanders gives the analogous tradeoff (see [6] and references therein). Could such an approach be adapted to the setting of deletions? 5. Can one improve the N poly(1/\u03b5) type dependence of our construction and decoding complexity to, say, exp(poly(1/\u03b5))N c for some exponent c that doesn\u2019t depend on \u03b5?\n1. For binary codes, what is the supremum p\u2217of all fractions p of adversarial deletions for which one can have positive rate? Clearly p\u2217\u2a7d1/2; could it be that p\u2217= 1/2 and this trivial limit can be matched? Or is it the case that p\u2217is strictly less than 1/2? Note that by [12], p\u2217> .17.\n# References\n[1] N. Alon, J. Edmonds, and M. Luby. Linear time erasure codes with nearly optimal recovery (extended abstract). In FOCS, pages 512\u2013519. IEEE Computer Society, 1995. [2] J. Chen, M. Mitzenmacher, C. Ng, and N. Varnica. Concatenated codes for deletion channels. In IEEE International Symposium on Information Theory, 2003, pages 218\u2013218, June 2003.\n[1] N. Alon, J. Edmonds, and M. Luby. Linear time erasure codes with nearly optimal recovery (extended abstract). In FOCS, pages 512\u2013519. IEEE Computer Society, 1995. [2] J. Chen, M. Mitzenmacher, C. Ng, and N. Varnica. Concatenated codes for deletion channels. In IEEE International Symposium on Information Theory, 2003, pages 218\u2013218, June 2003.\n[3] M. Davey and D. J. C. MacKay. Reliable communication over channels with insertions, deletions, and substitutions. IEEE Transactions on Information Theory, 47(2):687\u2013698, Feb 2001. [4] S. Diggavi and M. Grossglauser. On information transmission over a finite buffer channel. IEEE Transactions on Information Theory, 52(3):1226\u20131237, March 2006. [5] R. Gallager. Sequential decoding for binary channels with noise and synchronization errors, October 1961. Lincoln Lab. Group Report. [6] V. Guruswami. Guest column: error-correcting codes and expander graphs. SIGACT News, 35(3):25\u201341, 2004. [7] V. Guruswami and P. Indyk. Linear-time encodable/decodable codes with near-optimal rate. IEEE Transactions on Information Theory, 51(10):3393\u20133400, 2005. [8] V. Guruswami and A. Rudra. Explicit codes achieving list decoding capacity: Error-correction with optimal redundancy. IEEE Transactions on Information Theory, 54(1):135\u2013150, 2008. [9] V. Guruswami and A. Rudra. Soft decoding, dual BCH codes, and better list-decodable ebiased codes. In Proceedings of the 2008 IEEE 23rd Annual Conference on Computational Complexity, CCC \u201908, pages 163\u2013174, Washington, DC, USA, 2008. IEEE Computer Society. [10] A. Kalai, M. Mitzenmacher, and M. Sudan. Tight asymptotic bounds for the deletion channel with small deletion probabilities. In ISIT, pages 997\u20131001, 2010. [11] Y. Kanoria and A. Montanari. Optimal coding for the binary deletion channel with small deletion probability. IEEE Transactions on Information Theory, 59(10):6192\u20136219, Oct 2013. [12] I. Kash, M. Mitzenmacher, J. Thaler, and J. Ullman. On the zero-error capacity threshold for deletion channels. In Information Theory and Applications Workshop (ITA), 2011, pages 1\u20135, Feb 2011. [13] M. Kiwi, M. Loebl, and J. Matou\u02d8sek. Expected length of the longest common subsequence for large alphabets. Advances in Mathematics, 197:480\u2013498, November 2004. [14] M. Mitzenmacher. A survey of results for deletion channels and related synchronization channels. Probability Surveys, 6:1\u201333, 2009. [15] M. Mitzenmacher and E. Drinea. A simple lower bound for the capacity of the deletion channel. IEEE Transactions on Information Theory, 52(10):4657\u20134660, 2006. [16] F. Parvaresh and A. Vardy. Correcting errors beyond the Guruswami-Sudan radius in polynomial time. In Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science, pages 285\u2013294, 2005. [17] E. Ratzer. Marker codes for channels with insertions and deletions. Annals of Telecommunications, 60(1\u20132):29\u201344, Jan\u2013Feb 2005. [18] L. Schulman and D. Zuckerman. Asymptotically good codes correcting insertions, deletions, and transpositions. IEEE Transactions on Information Theory, 45(7):2552\u20132557, November 1999. [19] K. W. Shum, I. Aleshnikov, P. V. Kumar, H. Stichtenoth, and V. Deolalikar. A low-complexity algorithm for the construction of algebraic-geometric codes better than the gilbert-varshamov bound. IEEE Transactions on Information Theory, 47(6):2225\u20132241, 2001.\n[20] N. J. A. Sloane. On single-deletion-correcting codes. CoRR, arxiv.org/abs/math/0207197, 2002. [21] L. R. Welch and E. R. Berlekamp. Error correction of algebraic block codes. US Patent Number 4,633,470, December 1986. [22] K. Zigangirov. Sequential decoding for a binary channel with drop-outs and insertions. Problemy Peredachi Informatsii, 5(2):23\u201330, 1969.\n# A Omitted proofs\nIn this section, we give the omitted probabilistic proofs of Sections 2 and 5.\nProof of Lemma 2.3. We will give a way to generate all strings s\u2032 containing s as a subsequence, and bound the number of possible outcomes. We do this by considering the lexicographically first occurrence of s in t. First choose \u2113locations n1 < \u00b7 \u00b7 \u00b7 < n\u2113in [m], which will be the locations of the \u2113symbols of s. If the ith symbol of s is a, we allow all symbols between locations ni\u22121 and ni to take any value but a. This ensures that the locations ni are the earliest occurrence of s as a subsequence. The rest of the symbols after n\u2113are filled in arbitrarily. It is clear that this process generates any string having s as a subsequence, so we will bound the number of ways this can happen. Fix n\u2113= t. There are \u2022 \ufffdt\u22121 \u2113\u22121 \ufffd ways to choose n1, . . . , n\u2113\u22121, \u2022 (k \u22121)t\u2212\u2113ways to fill in symbols between the ni\u2019s, \u2022 and km\u2212t ways to fill in the last m \u2212t symbols. Summing over all possible values of t, the total number of strings with s as a subsequence is at most m\nof the symbols after n\u2113are filled in arbitrarily. It is clear that this process generates any string having s as a subsequence, so we will bound the number of ways this can happen. Fix n\u2113= t. There are \u2022 \ufffdt\u22121 \u2113\u22121 \ufffd ways to choose n1, . . . , n\u2113\u22121, \u2022 (k \u22121)t\u2212\u2113ways to fill in symbols between the ni\u2019s, \u2022 and km\u2212t ways to fill in the last m \u2212t symbols. Summing over all possible values of t, the total number of strings with s as a subsequence is at most m\n\u2022 \ufffdt\u22121 \u2113\u22121 \ufffd ways to choose n1, . . . , n\u2113\u22121, \u2022 (k \u22121)t\u2212\u2113ways to fill in symbols between the ni\u2019s, \u2022 and km\u2212t ways to fill in the last m \u2212t symbols. Summing over all possible values of t, the total number of strings with s as a subsequence is at most m\n\u2022 \ufffdt\u22121 \u2113\u22121 \ufffd ways to choose n1, . . . , n\u2113\u22121, \u2022 (k \u22121)t\u2212\u2113ways to fill in symbols between the ni\u2019s, \u2022 and km\u2212t ways to fill in the last m \u2212t symbols.\nSumming over all possible values of t, the total number of strings with s as a subsequence is\nAs \ufffdm t=\u2113 \ufffdt\u22121 \u2113\u22121 \ufffd = \ufffdm \u2113 \ufffd , the claimed bound follows. When \u2113> m/k, the term \ufffdt\u22121 \u2113\u22121 \ufffd km\u2212t(k \u22121)t\u2212\u2113increases with t, so the sum is at most\nProof of Theorem 2.4. We construct such a code using a greedy algorithm. We begin with an arbitrary string in [k]m, and then iteratively add strings whose LCS with all previously chosen strings has length less than (1 \u2212\u03b4)m. The LCS of two length m strings can be computed in time poly(m), so this takes time kO(m). It remains to show that we can choose kRm strings.\nFor a fixed string u \u2208[k]m, it has at most \ufffd m (1\u2212\u03b4)m \ufffd subsequences of length (1 \u2212\u03b4)m, so by Lemma 2.3, the number of strings whose LCS with u has length at least (1 \u2212\u03b4)m, and which therefore cannot be chosen, is at most\nThus if the target rate is R, we will succeed if\nSetting R = 1 \u2212\u03b4 \u2212\u03b3, we have\n2h(\u03b4) + (1 \u2212\u03b3) log k \u2a7dlog k \u21d42h(\u03b4) \u2a7d\u03b3 log k,\nso we can choose kRm strings as long as the alphabet size k satisfies k \u2a7e22h(\u03b4)/\u03b3.\nIn the case of k = 2, we may use the tighter estimate from Lemma 2.3 in Equation (*) to obtai the claimed bound.\nthe claimed bound. Proof of Proposition 2.5. The greedy algorithm of Theorem 2.4 applies, but now we must choose strings from the set of \u03b2-dense strings. We first bound the number of strings which are not \u03b2dense. The number of strings of length \u03b2m with less than \u03b2m/10 1\u2019s is\nProof of Proposition 2.5. The greedy algorithm of Theorem 2.4 applies, but now we must choose strings from the set of \u03b2-dense strings. We first bound the number of strings which are not \u03b2dense. The number of strings of length \u03b2m with less than \u03b2m/10 1\u2019s is\nSince there are at most m intervals of length \u03b2m in a string, the probability that a randomly chosen string of length m is not \u03b2-dense is at most\nThe algorithm of Theorem 2.4 then succeeds if\n\ufffdm \u03b4m \ufffd2 \u00b7 \u03b4m \u00b7 2Rm \u2a7d2m\ufffd 1 \u22122\u2212\u2126(\u03b2m)\ufffd ,\nor R \u2a7d1 \u22122h(\u03b4) \u2212O(log(\u03b4m)/m) \u22122\u2212\u2126(\u03b2m)/m.\n(*)\nProof of Theorem 5.2. By Lemma 2.3, the probability that a set of L independent, uniform strings all share a common substring of length \u2113is at most\nFor a random code C of rate R, we union bound over all possible subsets of L codewords to upper bound the probability that C is not (\u03b4, L) list-decodable from deletions.\nPr[C fails] < 2RmL \u00b7 2\u2113\u00b7 2L log m \u00b7 2\u2212mL \u00b7 2Lmh(1\u2212\u03b4)\nThis is at most 2\u2212m, provided\nR \u2a7d1 \u2212h(\u03b4) \u22122 \u2212\u03b4 L \u2212log m m ,\nwhich holds for our choice of R. When \u03b4 = 1/2 \u2212\u03b5, we can set R = \u2126(\u03b52) to see that\nso we can take L to be O(1/\u03b52).\nSimilarly to Theorem 2.4, this argument shows that we can construct a \ufffd \u03b4, O(1/\u03b52) \ufffd list-decodable code using a greedy algorithm, which successively adds strings who do not share a common subsequence of length \u2113with L \u22121 previously chosen strings.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of constructing codes which can be efficiently corrected from a constant fraction of worst-case deletions. The problem of communicating over the binary deletion channel, where each transmitted bit is deleted independently with a fixed probability, remains poorly understood, particularly in the context of worst-case deletions.",
        "problem": {
            "definition": "The paper aims to solve the problem of efficiently correcting a constant fraction of worst-case deletions in coding theory.",
            "key obstacle": "The main challenge is that the locations of deleted symbols are unknown to the decoder, complicating the recovery of the original message."
        },
        "idea": {
            "intuition": "The intuition stems from the need to construct codes that can efficiently handle deletions, motivated by the limitations of existing methods in coding for deletion channels.",
            "opinion": "The proposed idea involves constructing polynomial-time decodable codes that can correct high fractions of deletions while maintaining a reasonable rate.",
            "innovation": "The key innovation lies in achieving qualitative goals for deletion correction that were previously unattainable, particularly for high-noise and high-rate regimes."
        },
        "method": {
            "method name": "Polynomial-time decodable codes",
            "method abbreviation": "PDC",
            "method definition": "The method involves constructing codes that can correct a fraction of deletions while ensuring efficient encoding and decoding processes.",
            "method description": "The method focuses on concatenating good error-correcting codes with inner deletion codes to achieve desired performance metrics.",
            "method steps": [
                "Define the outer code using Reed-Solomon or Parvaresh-Vardy codes.",
                "Construct an inner deletion code that can handle a specified fraction of deletions.",
                "Concatenate the outer and inner codes to form the final code.",
                "Implement encoding and decoding algorithms that operate in polynomial time."
            ],
            "principle": "The effectiveness of this method is based on the ability to recover from a bounded fraction of deletions by ensuring that sufficient information remains intact for decoding."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using various datasets and baseline methods to compare the performance of the proposed codes against existing approaches.",
            "evaluation method": "The performance was measured by assessing the rates of correction for different fractions of deletions and analyzing the efficiency of the encoding and decoding processes."
        },
        "conclusion": "The experiments demonstrate that the proposed codes successfully correct a significant fraction of deletions while maintaining high rates, marking a substantial advancement in the field of coding theory.",
        "discussion": {
            "advantage": "The main advantages include improved rates of correction for high fractions of deletions and efficient decoding algorithms that outperform previous methods.",
            "limitation": "A limitation of the method is that it may not achieve optimal performance for all deletion fractions, particularly in scenarios with extremely high deletion rates.",
            "future work": "Future research should focus on refining the constructions to further improve rates and exploring the theoretical limits of deletion correction."
        },
        "other info": {
            "Funding": "Research supported in part by NSF grants CCF-0963975 and CCF-1422045.",
            "Acknowledgments": "Some of this work was done when the author was a visiting researcher at Microsoft Research New England."
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "The problem of communicating over the binary deletion channel, where each transmitted bit is deleted independently with a fixed probability, remains poorly understood, particularly in the context of worst-case deletions."
        },
        {
            "section number": "2.3",
            "key information": "The proposed method involves constructing polynomial-time decodable codes that can correct high fractions of deletions while maintaining a reasonable rate."
        },
        {
            "section number": "3.1",
            "key information": "The intuition stems from the need to construct codes that can efficiently handle deletions, motivated by the limitations of existing methods in coding for deletion channels."
        },
        {
            "section number": "4.1",
            "key information": "The method focuses on concatenating good error-correcting codes with inner deletion codes to achieve desired performance metrics."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the method is that it may not achieve optimal performance for all deletion fractions, particularly in scenarios with extremely high deletion rates."
        },
        {
            "section number": "6.3",
            "key information": "Future research should focus on refining the constructions to further improve rates and exploring the theoretical limits of deletion correction."
        }
    ],
    "similarity_score": 0.548931478062274,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1832_natur/papers/Deletion codes in the high-noise and high-rate regimes.json"
}