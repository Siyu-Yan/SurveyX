{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1906.06273",
    "title": "Epistemic Risk-Sensitive Reinforcement Learning",
    "abstract": "We develop a framework for interacting with uncertain environments in reinforcement learning (RL) by leveraging preferences in the form of utility functions. We claim that there is value in considering different risk measures during learning. In this framework, the preference for risk can be tuned by variation of the parameter \u03b2 and the resulting behavior can be risk-averse, risk-neutral or risk-taking depending on the parameter choice. We evaluate our framework for learning problems with model uncertainty. We measure and control for epistemic risk using dynamic programming (DP) and policy gradient-based algorithms. The risk-averse behavior is then compared with the behavior of the optimal risk-neutral policy in environments with epistemic risk.",
    "bib_name": "eriksson2019epistemicrisksensitivereinforcementlearning",
    "md_text": "# Epistemic Risk-Sensitive Reinforcement Learning\nHannes Eriksson 1 2 Christos Dimitrakakis 1\n# Abstract\nWe develop a framework for interacting with uncertain environments in reinforcement learning (RL) by leveraging preferences in the form of utility functions. We claim that there is value in considering different risk measures during learning. In this framework, the preference for risk can be tuned by variation of the parameter \u03b2 and the resulting behavior can be risk-averse, risk-neutral or risk-taking depending on the parameter choice. We evaluate our framework for learning problems with model uncertainty. We measure and control for epistemic risk using dynamic programming (DP) and policy gradient-based algorithms. The risk-averse behavior is then compared with the behavior of the optimal risk-neutral policy in environments with epistemic risk.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9af9/9af96d2e-9bb7-4a9f-a1e0-568fd1b1f0f5.png\" style=\"width: 50%;\"></div>\narXiv:1906.06273v1\n# 1. Introduction\nIn this work, we consider the problem of reinforcement learning (RL) for risk-sensitive policies under epistemic uncertainty, i.e. the uncertainty due to the agent not knowing how the environment responds to the agent\u2019s actions. This is in contrast to typical approaches in risk-sensitive decision making, which have focused on the aleatory uncertainty due to inherent stochasticity in the environment. Modelling risk-sensitivity in this way makes more sense for applications such as autonomous driving, where systems are nearly deterministic, and most uncertainty is due to the lack of information about the environment. In this paper, we consider epistemic uncertainty and risk sensitivity both within a Bayesian utilitarian framework. We develop novel algorithms for policy optimisation in this setting, and compare their performance quantitatively with policies optimised for the risk-neutral setting.\nReinforcement learning (RL) is a sequential decisionmaking problem under uncertainty. Typically, this is for-\n1Department of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, Sweden 2Zenuity AB, Gothenburg, Sweden. Correspondence to: Hannes Eriksson <hannese@chalmers.se>.\nmulated as the maximisation of an expected return, where the return R is defined as the sum of scalar rewards obtained over time R \u225c\ufffdT t=1 rt to some potentially infinite or random horizon T. As is common in RL, we assume that agents acting in a discrete Markov decision process (MDP). For any given finite MDP \u00b5 \u2208M, the optimal risk-neutral policy \u03c0\u2217(\u00b5) \u2208arg max\u03c0 E\u03c0 \u00b5[R] be found efficiently via dynamic programming algorithms. Because the learning problem specifies that \u00b5 is unknown, the optimal policy under epistemic uncertainty must take into account expected information gain. In the Bayesian setting, we maintain a subjective belief in the form of a probability distribution \u03be over MDPs M and the optimal solution is given by:\n\ufffd This problem is generally intractable, as the optimisation is performed over adaptive policies, which is exponentially large in the problem horizon. A risk-neutral agent would only wish to maximise expected return. However, we are interested in the case where the agent is risk-sensitive, and in particular with respect to uncertainty about \u00b5.\nContribution. Typically, risk sensitivity in reinforcement learning has addressed risk due to the inherent environment stochasticity (aleatory) and that due to uncertainty (epistemic) with different mechanisms. We instead wish to consider both under a coherent utility maximising framework, where the convexity of the utility with respect to the return R determines whether our behaviour will be risk-seeking or risk-averse. By applying this utility function on the actual return or the expected return given the MDP we can easily trade off between aleatory and epistemic risk-sensitivity.\n# 1.1. Related work\nDefining risk sensitivity with respect to the return R can be done in a number of ways. In this paper, we focus on the expected utility formulation, whereby the utility is defined as a function of the return: concave functions lead to risk aversion and convex to risk seeking. In the context of reinforcement learning, this approach was first proposed by Mihatsch & Neuneier (2002), who derived efficient temporal-difference algorithms for exponential utility functions. However, the authors considered risk only with\nrespect to the MDP stochasticity.\nWorks on Robust MDPs have traditionally dealt with uncertainty due to epistemic risk. In Givan et al. (2000) the authors extend the exact MDP setting by allowing bounded transition probabilities and rewards. They introduce optimal pessimistic and optimistic policy updates for this setting. Mannor et al. (2012) further extend this by allowing the parameter uncertainties to be coupled which allows for less conservative solutions. Tamar et al. (2014) use RL and approximate dynamic programming (ADP) to scale the Robust MDP framework to problems with larger state spaces using function approximators. Another way of dealing with risk is conditional valueat-risk (CVaR) (Rockafellar et al., 2000; Sivaramakrishnan & Stamicar, 2017) measures. Compared to more traditional risk measures such as Mean-Variance tradeoff (Markowitz, 1952; Tamar et al., 2012) or expected utility framework (Friedman & Savage, 1948), CVaR allows us to control for tail risk. CVaR has been used for risksensitive MDPs in Chow & Ghavamzadeh (2014); Chow et al. (2015b;a); Stanko (2018). A Bayesian setting is also considered by Depeweg et al. (2018), who focus on a risk decomposition in aleatory and epistemic components. The authors model the underlying dynamics with a neural network. Under the risk-sensitive framework, they aim to trade-off expected performance with variation in performance. The risk-sensitive criterion they aim to maximise is E \ufffd\ufffd t rt \ufffd + \u03b2\u03c3 \ufffd\ufffd t rt \ufffd where \u03b2 controls for the level of risk-aversion. Thus, they are essentially considering risk only with respect to individual rewards. Our paper instead considers the risk with respect to the total return, which we believe is a more interesting setting for long-term planning problems under uncertainty.\nWorks on Robust MDPs have traditionally dealt with uncertainty due to epistemic risk. In Givan et al. (2000) the authors extend the exact MDP setting by allowing bounded transition probabilities and rewards. They introduce optimal pessimistic and optimistic policy updates for this setting. Mannor et al. (2012) further extend this by allowing the parameter uncertainties to be coupled which allows for less conservative solutions. Tamar et al. (2014) use RL and approximate dynamic programming (ADP) to scale the Robust MDP framework to problems with larger state spaces using function approximators.\nAnother way of dealing with risk is conditional valueat-risk (CVaR) (Rockafellar et al., 2000; Sivaramakrishnan & Stamicar, 2017) measures. Compared to more traditional risk measures such as Mean-Variance tradeoff (Markowitz, 1952; Tamar et al., 2012) or expected utility framework (Friedman & Savage, 1948), CVaR allows us to control for tail risk. CVaR has been used for risksensitive MDPs in Chow & Ghavamzadeh (2014); Chow et al. (2015b;a); Stanko (2018).\nA Bayesian setting is also considered by Depeweg et al. (2018), who focus on a risk decomposition in aleatory and epistemic components. The authors model the underlying dynamics with a neural network. Under the risk-sensitive framework, they aim to trade-off expected performance with variation in performance. The risk-sensitive criterion they aim to maximise is E \ufffd\ufffd t rt \ufffd + \u03b2\u03c3 \ufffd\ufffd t rt \ufffd where \u03b2 controls for the level of risk-aversion. Thus, they are essentially considering risk only with respect to individual rewards. Our paper instead considers the risk with respect to the total return, which we believe is a more interesting setting for long-term planning problems under uncertainty.\n# 2. Optimal policies for epistemic risk\nUnder the expected utility hypothesis, risk sensitivity can be modelled (Friedman & Savage, 1948) through a concave or convex utility function U : R \u2192R of the return R. Then, for a given model \u00b5, the optimal U-sensitive policy with respect to aleatory risk is the solution to max\u03c0 E\u03c0 \u00b5[U(R)]. In the case where we are uncertain about what is the true MDP, we can express it through belief \u03be over models \u00b5. Then optimal policy is the solution to\n# \u03c0A(U, \u03be) \u225carg max \u03c0 \ufffd M E\u03c0 \u00b5[U(R)] d\u03be(\u00b5).\n(2)\nHowever, this is only risk-sensitive with respect to the stochasticity of the underlying MDPs. We believe that epistemic risk, i.e. the risk due to our uncertainty about the model is more pertinent for reinforcement learning. The\noptimal epistemic risk sensitive policy maximises:\n# optimal epistemic risk sensitive policy maximises:\n(3)\nWhen U is the identity function, both solutions are riskneutral. In this paper, we shall consider functions of the form U(x) = \u03b2\u22121e\u03b2x, so that \u03b2 > 0 is risk-seeking and \u03b2 < 0 risk-averse.\nWe consider two algorithms for this problem. The first, based on an approximate dynamic programming algorithm for Bayesian reinforcement learning introduced in (Dimitrakakis, 2011), is introduced in Section 2.2. The second, based on the policy gradient framework (Sutton et al., 2000), allows us to extend the previous algorithm to larger MDPs and allows for learning of stochastic policies. The priors used in the experiments are detailed in Section 3.\n# 2.1. Utility functions.\nPrevious works (Mihatsch & Neuneier, 2002; Howard & Matheson, 1972) on utility functions for risk-sensitive reinforcement learning used an exponential utility function of the form U(x) = \u03b2\u22121e\u03b2x. The \u03b2 parameter could then be used to control the shape of the utility function and determine how risk-sensitive we want to be with respect to x. This is the utility function we will use in this paper.1 Mihatsch & Neuneier (2002) consider not only exponential utility functions, but a special form of them, that is:\n(4)\nMihatsch & Neuneier (2002); Coraluppi & Marcus (1999); Marcus et al. (1997) argue that this utility function has some interesting properties. In particular, maximising leads to maximising:\nFrom this, we get that the behavior of our policy \u03c0E(U) as \u03b2 \u21920 is the same as for the risk-neutral case. For \u03b2 < 0 we get risk-averse behavior and for \u03b2 > 0 risk-taking behavior. In our work, we are working in a Bayesian setting. Consequently, we introduce a belief \u03be over models and define the expected utility of a policy \u03c0 related to the model uncertainty as follows:\n(6)\n\ufffd M \ufffd \ufffd 1Other choices are U(x) = x\u03b2 or U(x) = log(x), however they both come with significant drawbacks, such as handling negative returns R.\nAlgorithm 1 Epistemic Risk Sensitive Backwards Induction\nInput: M (set of MDPs), \u03be (current posterior)\nrepeat\nfor \u00b5 \u2208M s \u2208S, a \u2208A do\nQ\u00b5(s, a) = R\u00b5(s, a) + \u03b3 \ufffd\ns\u2032 T ss\u2032\n\u00b5\nV\u00b5(s\u2032)\nend for\nfor s \u2208S do\nfor a \u2208A do\nQ\u03be(s, a) = \ufffd\n\u00b5 \u03be(\u00b5)U[(Q\u00b5(s, a)]\nend for\n\u03c0(s) = arg maxa Q\u03be(s, a).\nfor \u00b5 \u2208M do\nV\u00b5(s) = Q\u00b5(s, \u03c0(s)).\nend for\nend for\nuntil convergence\nreturn \u03c0\n# 2.2. Epistemic risk sensitive backward induction\nAlgorithm 1 is an Approximate Dynamic Programming (ADP) (Powell, 2007) algorithm for optimising policies in our setting. While the algorithm is given for a belief over a finite set of MDPs, it can be easily extended to arbitrary \u03be through simple Monte-Carlo sampling, as in Dimitrakakis (2011). The algorithm essentially maintains a separate Q\u00b5-value function for every MDP \u00b5. At every step, it finds the best local policy \u03c0 with respect to the utility function U. Then the value function V\u00b5 of each MDP reflects the value of \u03c0 within that MDP.\nAlgorithm 1 is an Approximate Dynamic Programming (ADP) (Powell, 2007) algorithm for optimising policies in our setting. While the algorithm is given for a belief over a finite set of MDPs, it can be easily extended to arbitrary \u03be through simple Monte-Carlo sampling, as in Dimitrakakis (2011).\nThe algorithm essentially maintains a separate Q\u00b5-value function for every MDP \u00b5. At every step, it finds the best local policy \u03c0 with respect to the utility function U. Then the value function V\u00b5 of each MDP reflects the value of \u03c0 within that MDP.\nThis algorithm will be used as a baseline for the experiments with risk aversion. We know that as the epistemic uncertainty vanishes; if there is only one underlying true model, then the optimal policy for the epistemic risk-sensitive algorithm will be the same as the optimal policy for the epistemic risk-neutral algorithm. It is important to point out that this is only true when our belief is concentrated around the true MDP. Behavior during learning of the MDP could be very different. For cases with multiple models, the epistemic uncertainty will always exist and there are no such guarantees.\n# 2.3. Bayesian policy gradient\nA common method for model-free reinforcement learning is policy gradient (Sutton et al., 2000). It is also very useful in model-based settings, and specifically for the Bayesian reinforcement learning problem, where sampling from the posterior allows us to construct efficient stochastic gradient algorithms. Bayesian policy gradient (BPG) methods have been explored for these contexts before, Ghavamzadeh & Engel (2006) uses BPG to plan how to maximise information gain given our current belief \u03bet for the risk-neutral\nsetting. In our specific setting, we are interested in maximising (3), where we use utility function (6) to model risk sensitivity. Our choice of policy parametrisation is a softmax policy with non-linear features. The probability of selecting action a in state s, given current parameters \u03b8, is \u03c0\u03b8(a|s) = e\u03c6(s,a,\u03b8) \ufffd a\u2032\u2208A e\u03c6(s,a\u2032,\u03b8) , where the features \u03c6(s, a, \u03b8) are calculated by a feedforward neural network with one hidden layer. Full parameter details of the neural networks are given in Section 3. We choose to introduce a policy parametrisation over (3). This gives us (7).\n(7)\nCombining our choice of utility function in (6) and (7) gives us our new objective function (8).\nOur goal is now to find the set of parameters \u03b8 so as to maximise information gain. Taking the gradient of (8) gives (9) which leads to (10) after a straightforward derivation.\n(9)\n(10)\n\ufffd The RHS of the numerator is the classical policy gradient (Sutton et al., 2000). We can replace the integrals in (10) with a sum by sampling models from our belief \u03be. Note that this has to be done independently for the numerator and the denominator to avoid bias. To get each of the separate E\u00b5 \u03c0\u03b8[R] we make use of rollouts. Each expected return term is estimated independently with their own set of rollouts.\nWe now have an estimate for the gradient and can move our policy parameters accordingly to act optimally given our belief \u03bet.\nThe procedure of calculating the gradient in (10) is given in Algorithm 2. The algorithm builds upon the classic episodical REINFORCE algorithm (Williams, 1992). Basing the algorithm on an episodical update makes sense in this case since it decreases the number of rollouts we have to do. One\nAlgorithm 2 Epistemic Risk Sensitive Policy Gradient\nInput: Policy parametrisation \u03b8t, \u03bet (current posterior).\nrepeat\nSimulate to get \u03b8t+1\nfor i = 1 to N do\n\u00b5(1), \u00b5(2) \u223c(Mt, Rt)\nfor j = 1 to M do\n\u03c4 (1)\n\u00b5(1), \u03c4 (2)\n\u00b5(1) \u223c\u03c0\u03b8, \u00b5(1)\n\u03c4 (3)\n\u00b5(2) \u223c\u03c0\u03b8, \u00b5(2)\nend for\nend for\n\u03b8t+1 \u2190\u03b8t \u2212\n\ufffdN\ni=0 exp\n\ufffd\n\u03b2\u03c4\u00b5i\n(1)\n\ufffd\n\u03c4\u00b5i\n(2)\u2207\u03b8 log \u03c0\u03b8(a|s)\n\ufffdN\ni=0 exp\n\ufffd\n\u03b2\u03c4\u00b5i\n(3)\n\ufffd\nDeploy \u03c0\u03b8t+1\n\u03c4 \u223c\u00b5, \u03c0\u03b8t+1\n\u03bet+1 \u2190\u03bet, \u03c4\nuntil convergence\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b74d/b74dc352-334c-4560-9297-a5c4609cc5ac.png\" style=\"width: 50%;\"></div>\ndrawback is that it restricts updates episode by episode and so the information gained in the current episode cannot be used until after the episode ends. Monte-Carlo methods such as REINFORCE also have a few other drawbacks such as high variance and low convergence, problems that could be addressed by extending the current framework to an actorcritic based one, as in Barto et al. (1983); Ghavamzadeh & Engel (2007); Ghavamzadeh et al. (2016). The algorithm consists of two stages. One planning stage which is used to identify the best policy parameters \u03b8t given our current belief \u03bet. This is done through model-based simulation by sampling MDPs from our belief. After the rollouts have been collected the neural network is optimised and a new policy \u03c0\u03b8t+1 is attained. We then commit to this policy for one episode and move on to the next stage of the algorithm. The second stage of the algorithm uses the new policy to act in the real environment. Trajectories \u03c4 = (s0, a0, r1, s1, ..., sT \u22121, aT \u22121, rT , sT ) are collected from this environment and used to update our belief over transitions and reward functions.\ndrawback is that it restricts updates episode by episode and so the information gained in the current episode cannot be used until after the episode ends. Monte-Carlo methods such as REINFORCE also have a few other drawbacks such as high variance and low convergence, problems that could be addressed by extending the current framework to an actorcritic based one, as in Barto et al. (1983); Ghavamzadeh & Engel (2007); Ghavamzadeh et al. (2016).\n# 2.4. Bayesian Epistemic CVaR\nAnother common approach of handling risk-aversion is to optimize with respect to a CVaR objective, (Chow & Ghavamzadeh, 2014; Chow et al., 2015b; Tamar et al., 2015; Stanko, 2018). In this paper, we investigate Bayesian epistemic CVaR, first studied by Chen et al. (2018) in the context of investment strategies. In their case, they use it to do posterior inference of a SV-ALD model to efficiently estimate risk. We define Bayesian epistemic CVaR as follows, defining the set of MDPs where we get at least z utility as the\nfollowing;\n(11)\nThe \u03bd\u03c0 \u03be (\u03b1) is value-at-risk in the Bayesian setting for a given quantile \u03b1.\n(13)\n(14)\nConcisely stated; we want to maximize our performance for the \u03b1 least likely MDPs according to our belief \u03be. The intuition behind this is that we want to be risk-averse with respect to what we are the most uncertain about.\n# 3. Experiments\nIn this paper, we conduct two kinds of experiments. Firstly, a classical learning problem on a Gridworld. This is discrete, so we can use Algorithm 1 to find a near-optimal deterministic policy. Similar problems have been studied before in the field of robust MDPs and there are already solutions for finite state and action-space problems. As this is a discrete state-action space, the belief maintained over the MDP transitions is in the form of a Dirichlet-product prior and Belief over rewards is in the form of a NormalGamma prior. Further experimental details about this problem can be found in Section 3.1 and Appendix A.1. Secondly, we consider a continuous state-space problem in Section 3.2. We do not see a trivial extension of previous works in robust MDPs to the case with infinite state-space. Previous works such as Tamar et al. (2014) scale earlier works on robust MDP to large state-space problems but not to continuous. The problem has been studied to a great extent in the field of risk-aversion, see Tamar et al. (2012; 2015); Chow & Ghavamzadeh (2014) and Appendix A.2. For this problem, we maintain function priors in the form of Gaussian Processes on the reward functions and transition kernels.\nIn this paper, we conduct two kinds of experiments. Firstly, a classical learning problem on a Gridworld. This is discrete, so we can use Algorithm 1 to find a near-optimal deterministic policy. Similar problems have been studied before in the field of robust MDPs and there are already solutions for finite state and action-space problems. As this is a discrete state-action space, the belief maintained over the MDP transitions is in the form of a Dirichlet-product prior and Belief over rewards is in the form of a NormalGamma prior. Further experimental details about this problem can be found in Section 3.1 and Appendix A.1.\nSecondly, we consider a continuous state-space problem in Section 3.2. We do not see a trivial extension of previous works in robust MDPs to the case with infinite state-space. Previous works such as Tamar et al. (2014) scale earlier works on robust MDP to large state-space problems but not to continuous. The problem has been studied to a great extent in the field of risk-aversion, see Tamar et al. (2012; 2015); Chow & Ghavamzadeh (2014) and Appendix A.2. For this problem, we maintain function priors in the form of Gaussian Processes on the reward functions and transition kernels.\n# 3.1. Gridworld experiment\nShown in Figure 1 are the results of Algorithm 1 ran for different values of \u03b2. We aimed to test both risk-aversion and risk-taking behavior through the choice of this parameter. The regret is averaged over \u2248100 independent experiment\nFigure 1. Experiment detailing the results of the run of ADP Algorithm 1 for varying choices of \u03b2. (a) The top plot of the grid is the regret over time with respect to the optimal deterministic risk-neutral policy. (b) The bottom plot shows the distribution of falls throughout over all experiments.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e2e/2e2e433a-4aa8-4df6-988b-e3d8ae7b2ea1.png\" style=\"width: 50%;\"></div>\nruns. We can see that the regret increases for more risktaking policies in the top plot. In this environment the risk-neutral and the risk-averse behavior is almost identical, with only a minor difference in #Falls. The risk-taking policy is in one sense, over-exploring while the risk-averse policy in general, would be more inclined to do exploitation. Note that in this experiment the only epistemic uncertainty comes from that the agent does not know the true MDP \u00b5. This uncertainty will go down over time as more transitions are observed.\n# 3.2. Option pricing experiment\nThe result of runs in this environment is depicted in Figure 2. The blue colored line will serve as a reference and has been verified to have an almost optimal risk-neutral policy. It also has the most observed data (millions of episodes) compared to the other algorithms (hundreds of thousands of episodes). We do see some notable difference in behavior between the policies and can identify that the algorithm with CVaR objective and the algorithm with \u03b2 = \u22120.001 are overly cautious. On the other hand for \u03b2 = {\u22120.01, \u22120.1} and BPG we see behavior similar to that of regular PG.\n# 4. Conclusions\nWe have introduced a framework that allows us to control for how risk-sensitive we want to be with respect to model uncertainty in the Bayesian RL setting. In Section 3.1 we detail the performance of Algorithm 1, an ADP algorithm that has the ability to control for risk-sensitiveness in envi-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f648/f6480631-41a5-434e-be6a-a5c7cd1fd387.png\" style=\"width: 50%;\"></div>\nronments with discrete state and action-spaces. The results point towards risk-averse policies lead to less exploratory policies; that is, we would only expect to explore additionally if we are convinced there is much more to be gained by doing so, compared to a risk-neutral policy. Figure 2 shows an extension of this, using Policy gradient algorithms to more complicated environments with continuous statespace.\nA few hurdles noted is the speed of Algorithm 2. In order to get a rich representation of the model uncertainty, a lot of MDPs should be sampled. However, sampling MDPs in this setup is quite expensive. Other belief models could be considered instead of GP if we were to scale this up to real problems.\nFuture work. We can see an adaptive agent that could change its risk-sensitive parameter \u03b2 with time. For problems where we know uncertainties will resolve later into the future, this could be one approach.\nThere could be an avenue to explore using utility functions to induce more exploratory behavior in complicated environments. Current methods use concepts such as curiosity (Houthooft et al., 2016; Pathak et al., 2017) for reward shaping or entropy regularization (Williams & Peng, 1991; Mnih et al., 2016; O\u2019Donoghue et al., 2016) to enforce additional exploration. However, under the expected utility framework we could perhaps get this for free by changing\nthe utility function.\nthe utility function. Decreasing variance and speeding up Algorithm 2 is of utmost importance for this framework to be used for more complex problems. As we touched upon in Section 2.3 a straight-forward improvement would be to move towards the Bayesian actor-critic framework introduced in Ghavamzadeh & Engel (2007).\n# Acknowledgement\nThis work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The simulations were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at Chalmers Centre for Computational Science and Engineering (C3SE).\n# References\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834\u2013846, 1983. Chen, L., Zerilli, P., and Baum, C. F. Leverage effects and stochastic volatility in spot oil returns: A bayesian approach with var and cvar applications. Energy Economics, 2018. Chow, Y. and Ghavamzadeh, M. Algorithms for cvar optimization in mdps. In Advances in neural information processing systems, pp. 3509\u20133517, 2014. Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. Risk-constrained reinforcement learning with percentile risk criteria, 2015a. Chow, Y., Tamar, A., Mannor, S., and Pavone, M. Risksensitive and robust decision-making: a cvar optimization approach. In Advances in Neural Information Processing Systems, pp. 1522\u20131530, 2015b. Coraluppi, S. P. and Marcus, S. I. Risk-sensitive and minimax control of discrete-time, finite-state markov decision processes. Automatica, 35(2):301\u2013309, 1999. Depeweg, S., Hernandez-Lobato, J.-M., Doshi-Velez, F., and Udluft, S. Decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning. In International Conference on Machine Learning, pp. 1192\u2013 1201, 2018. Dimitrakakis, C. Robust bayesian reinforcement learning\nCoraluppi, S. P. and Marcus, S. I. Risk-sensitive and minimax control of discrete-time, finite-state markov decision processes. Automatica, 35(2):301\u2013309, 1999.\nDepeweg, S., Hernandez-Lobato, J.-M., Doshi-Velez, F., and Udluft, S. Decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning. In International Conference on Machine Learning, pp. 1192\u2013 1201, 2018.\nDimitrakakis, C. Robust bayesian reinforcement learning through tight lower bounds. In European Workshop on Reinforcement Learning (EWRL 2011), pp. 177\u2013188, 2011.\nFriedman, M. and Savage, L. J. The Utility Analysis of Choices Involving Risk. The Journal of Political Economy, 56(4):279, 1948. Ghavamzadeh, M. and Engel, Y. Bayesian policy gradient algorithms. In NIPS 2006, 2006. Ghavamzadeh, M. and Engel, Y. Bayesian actor-critic algorithms. In Proceedings of the 24th international conference on Machine learning, pp. 297\u2013304. ACM, 2007. Ghavamzadeh, M., Engel, Y., and Valko, M. Bayesian policy gradient and actor-critic algorithms. The Journal of Machine Learning Research, 17(1):2319\u20132371, 2016. Givan, R., Leach, S., and Dean, T. Bounded-parameter markov decision processes. Artificial Intelligence, 122 (1-2):71\u2013109, 2000. Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 1109\u20131117, 2016. Howard, R. A. and Matheson, J. E. Risk-sensitive markov decision processes. Management science, 18(7):356\u2013369, 1972. Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt, T., Lefrancq, A., Orseau, L., and Legg, S. Ai safety gridworlds. arXiv preprint arXiv:1711.09883, 2017. Mannor, S., Mebel, O., and Xu, H. Lightning does not strike twice: Robust mdps with coupled uncertainty. arXiv preprint arXiv:1206.4643, 2012. Marcus, S. I., Fern\u00b4andez-Gaucherand, E., Hern\u00b4andezHernandez, D., Coraluppi, S., and Fard, P. Risk sensitive markov decision processes. In Systems and control in the twenty-first century, pp. 263\u2013279. Springer, 1997. Markowitz, H. Portfolio selection. The journal of finance, 7 (1):77\u201391, 1952. Mihatsch, O. and Neuneier, R. Risk-sensitive reinforcement learning. Machine learning, 49(2-3):267\u2013290, 2002. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928\u2013 1937, 2016. Murphy, K. Conjugate bayesian analysis of the gaussian distribution. Technical report, UBC, 2007. O\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. Combining policy gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. Combining policy gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16\u201317, 2017. Powell, W. B. Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703. John Wiley & Sons, 2007. Rockafellar, R. T., Uryasev, S., et al. Optimization of conditional value-at-risk. Journal of risk, 2:21\u201342, 2000. Sivaramakrishnan, K. and Stamicar, R. A cvar scenariobased framework: Minimizing downside risk of multiasset class portfolios. The Journal of Portfolio Management, 44:114\u2013129, 12 2017. doi: 10.3905/jpm.2018.44.2. 114. Stanko, S. Risk-averse distributional reinforcement learning, 2018. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057\u20131063, 2000. Tamar, A., Di Castro, D., and Mannor, S. Policy gradients with variance related risk criteria. In Proceedings of the twenty-ninth international conference on machine learning, pp. 387\u2013396, 2012. Tamar, A., Mannor, S., and Xu, H. Scaling up robust mdps using function approximation. In International Conference on Machine Learning, pp. 181\u2013189, 2014. Tamar, A., Glassner, Y., and Mannor, S. Optimizing the cvar via sampling. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16\u201317, 2017. Powell, W. B. Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703. John Wiley & Sons, 2007. Rockafellar, R. T., Uryasev, S., et al. Optimization of conditional value-at-risk. Journal of risk, 2:21\u201342, 2000. Sivaramakrishnan, K. and Stamicar, R. A cvar scenariobased framework: Minimizing downside risk of multiasset class portfolios. The Journal of Portfolio Management, 44:114\u2013129, 12 2017. doi: 10.3905/jpm.2018.44.2. 114. Stanko, S. Risk-averse distributional reinforcement learning, 2018. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057\u20131063, 2000. Tamar, A., Di Castro, D., and Mannor, S. Policy gradients with variance related risk criteria. In Proceedings of the twenty-ninth international conference on machine learning, pp. 387\u2013396, 2012. Tamar, A., Mannor, S., and Xu, H. Scaling up robust mdps using function approximation. In International Conference on Machine Learning, pp. 181\u2013189, 2014. Tamar, A., Glassner, Y., and Mannor, S. Optimizing the cvar via sampling. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.\nSivaramakrishnan, K. and Stamicar, R. A cvar scenariobased framework: Minimizing downside risk of multiasset class portfolios. The Journal of Portfolio Management, 44:114\u2013129, 12 2017. doi: 10.3905/jpm.2018.44.2. 114.\nWilliams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.\n# A. Environment descriptions A.1. Gridworld\n# A. Environment descriptions\nA visual representation of the environment can be seen in Leike et al. (2017). The experiments are run with slightly different parameters, which are the following; Let \u03b3 = 0.99 be the discount factor, and T = 100 be the maximum allowed steps within an episode. The allowed actions correspond to the four cardinal directions\n{North, East, South, West}. The environment consists of a GOAL tile, which terminates the episode and gives a reward of rg = 0.0. The environment also has WATER tiles which, upon entering, terminates the episode and gives a reward of rw = \u221210.0. Every other action in any other state gives a reward of r0 = \u22120.1. To ensure there is always epistemic uncertainty, we introduce another almost identical environment but with a goal at another location. We measure performance in Regret with respect to the optimal deterministic risk-neutral policy. The regret of an algorithm A with respect to the optimal deterministic riskneutral policy is given by the following;\nWe measure performance in Regret with respect to the optimal deterministic risk-neutral policy. The regret of an algorithm A with respect to the optimal deterministic riskneutral policy is given by the following;\nWe let our prior belief over the MDP transitions be T 0 \u00b5 (s\u2032|s, a) = Dir(\u03b10), \u03b10 = 0.5. We can then, given observed transitions in the environment, update T T \u00b5 (s\u2032|s, a) = Dir( \u03b10+ns\u2032 s,a \ufffd s\u2032\u2032 ns\u2032\u2032 s,a ), that is, we count the number of transitions (s, a) \u2192s\u2032 and (s, a) \u2192\u00b7. From these counts we can compute \u03b1t which is our posterior on the transition (s, a) \u2192s\u2032 at time t. We also hold a belief over the reward matrices R\u00b5(s, a). A common prior is to use is a NormalGamma-prior (Murphy, 2007) over each r\u00b5(s, a). The NormalGamma distribution is the conjugate prior to the Normal distribution with unknown parameters \u00b5, \u03bb. The model is given as p(\u00b5, \u03bb|D) = NG(\u00b5, \u03bb|\u00b5n, \u03ban, \u03b1n, \u03b2n). So our belief over rewards R\u00b5(s, a) = NG(\u00b5, \u03bb|\u00b5n, \u03ban, \u03b1n, \u03b2n).\nWe let our prior belief over the MDP transitions be T 0 \u00b5 (s\u2032|s, a) = Dir(\u03b10), \u03b10 = 0.5. We can then, given observed transitions in the environment, update T T \u00b5 (s\u2032|s, a) = Dir( \u03b10+ns\u2032 s,a \ufffd s\u2032\u2032 ns\u2032\u2032 s,a ), that is, we count the number of transitions (s, a) \u2192s\u2032 and (s, a) \u2192\u00b7. From these counts we can compute \u03b1t which is our posterior on the transition (s, a) \u2192s\u2032 at time t.\nWe also hold a belief over the reward matrices R\u00b5(s, a). A common prior is to use is a NormalGamma-prior (Murphy, 2007) over each r\u00b5(s, a). The NormalGamma distribution is the conjugate prior to the Normal distribution with unknown parameters \u00b5, \u03bb. The model is given as p(\u00b5, \u03bb|D) = NG(\u00b5, \u03bb|\u00b5n, \u03ban, \u03b1n, \u03b2n). So our belief over rewards R\u00b5(s, a) = NG(\u00b5, \u03bb|\u00b5n, \u03ban, \u03b1n, \u03b2n).\n# A.2. Options\nA common experiment (Chow & Ghavamzadeh, 2014; Tamar et al., 2012), used to test risk-averse algorithms in a continuous environment is the Option Pricing experiment. It can have many forms and classically we aim to minimise expected cost. In this work, we consider the reward perspective instead and the goal is to maximise the expected return. The environment is as follows; At any time t the agent represents an investor with the opportunity of buying an option which gives an immediate reward of rt. The reward of this option varies with time and with probability p, rt+1 \u2190\u03b3furt, and with probability (1\u2212p), rt+1 \u2190\u03b3fdrt. The goal is then for the agent to chose a suitable time to stop. Upon choosing to buy the option, the episode ends and the agent returns to x0. Every time step the agent decides to wait will also incur a small negative reward ph which represents the opportunity cost for not using its resources. If the agent chooses to wait until the horizon T, it is then forced to take the option for its current value.\nWe use the following parameters in our experiment; x0 = [1; 0], ph = \u22120.1, T = 20, \u03b3 = 0.95, fu = 2.0, fd = 0.5, p = [0.45; 0.65; 0.85], K = 5.0.\nAlgorithm 2 requires us to be able to sample MDPs from the environment and to be able to do rollouts in them. There are no obvious candidates of priors over continuous statespace transitions and reward functions. In this work, we chose to use Gaussian Processes as priors over functions. We use multiple Gaussian Processes to maintain our belief over transition kernels and reward functions. We update our belief with transitions and reward from the true underlying models. We use a Dirichlet prior over models and update it when we receive information on which model we acted in. Given data from rollouts sampled from our belief, we can use any standard Policy gradient-like algorithm to update our policy \u03c0\u03b8t. In this paper, we chose to focus on an episodic Policy gradient algorithm similar to the REINFORCE framework. We parametrise our policy by a one hidden layer neural network with parameters \u03b8.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of reinforcement learning (RL) for risk-sensitive policies under epistemic uncertainty, contrasting with typical approaches that focus on aleatory uncertainty. The authors argue that a new framework is necessary to effectively model risk-sensitivity in environments where uncertainty arises from the agent's lack of knowledge about the environment's responses.",
        "problem": {
            "definition": "The problem is defined as the challenge of optimizing reinforcement learning policies in the presence of epistemic uncertainty, where the model of the environment is not fully known.",
            "key obstacle": "The core obstacle is the intractability of optimizing policies over adaptive strategies in the face of uncertainty about the true model of the environment."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to integrate risk sensitivity in reinforcement learning under epistemic uncertainty, where traditional methods have focused primarily on aleatory risks.",
            "opinion": "The proposed idea involves developing a framework that allows for the tuning of risk preferences in RL through utility functions, enabling agents to adopt risk-averse, risk-neutral, or risk-taking behaviors based on the parameter \u03b2.",
            "innovation": "The primary innovation is the introduction of a coherent utility maximizing framework that simultaneously addresses both aleatory and epistemic risks, allowing for a more nuanced approach to risk-sensitive decision-making in RL."
        },
        "method": {
            "method name": "Epistemic Risk-Sensitive Reinforcement Learning",
            "method abbreviation": "ERS-RL",
            "method definition": "A framework that incorporates preferences in the form of utility functions to model risk sensitivity in reinforcement learning under epistemic uncertainty.",
            "method description": "The method allows for the tuning of risk preferences during the learning process, enabling different behaviors based on the level of risk aversion or seeking.",
            "method steps": [
                "Define a belief distribution over possible MDPs.",
                "Utilize dynamic programming and policy gradient algorithms to optimize policies.",
                "Implement risk-sensitive utility functions to guide decision-making.",
                "Evaluate the performance of the proposed policies against risk-neutral benchmarks."
            ],
            "principle": "The method is effective because it leverages utility functions to quantify and manage the trade-offs between expected returns and risks associated with model uncertainty."
        },
        "experiments": {
            "evaluation setting": "Two types of experiments were conducted: a Gridworld learning problem with discrete states and actions, and a continuous state-space problem using Gaussian Processes for function priors.",
            "evaluation method": "Performance was assessed by comparing the regret of the proposed risk-sensitive policies against optimal risk-neutral policies in controlled environments."
        },
        "conclusion": "The framework introduced in this paper allows for effective control of risk-sensitivity in reinforcement learning, demonstrating that risk-averse policies can lead to less exploratory behaviors, while also offering a pathway for future research into adaptive risk-sensitive agents.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to flexibly adjust risk preferences in RL, enhancing the agent's decision-making capabilities in uncertain environments.",
            "limitation": "A noted limitation is the computational expense associated with sampling multiple MDPs to accurately represent model uncertainty, which can hinder scalability.",
            "future work": "Future research directions include developing adaptive agents that can modify their risk sensitivity over time and exploring the use of utility functions to promote exploratory behavior in complex environments."
        },
        "other info": {
            "acknowledgment": "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.",
            "experimental details": {
                "Gridworld": {
                    "discount factor": 0.99,
                    "maximum steps": 100,
                    "allowed actions": [
                        "North",
                        "East",
                        "South",
                        "West"
                    ]
                },
                "Option Pricing": {
                    "initial state": [
                        1,
                        0
                    ],
                    "opportunity cost": -0.1,
                    "horizon": 20,
                    "discount factor": 0.95
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The fundamental principles of AI safety include the need to model risk-sensitivity in environments where uncertainty arises from the agent's lack of knowledge about the environment's responses."
        },
        {
            "section number": "3.1",
            "key information": "The main challenges faced in AI safety and evaluation include the intractability of optimizing policies over adaptive strategies in the face of uncertainty about the true model of the environment."
        },
        {
            "section number": "3.2",
            "key information": "Existing frameworks for safe AI development can be informed by the proposed Epistemic Risk-Sensitive Reinforcement Learning method, which incorporates preferences in the form of utility functions to model risk sensitivity."
        },
        {
            "section number": "4.1",
            "key information": "The latest advancements in generative AI technologies can include frameworks that allow for the tuning of risk preferences in reinforcement learning, enabling agents to adopt various risk behaviors."
        },
        {
            "section number": "5.1",
            "key information": "Quantitative metrics for evaluating language model performance can be extended to assess the performance of risk-sensitive policies against optimal risk-neutral benchmarks."
        },
        {
            "section number": "7.1",
            "key information": "Potential future developments in AI safety research include exploring the use of utility functions to promote exploratory behavior in complex environments."
        }
    ],
    "similarity_score": 0.566994317748395,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-1050_,AI_s/papers/Epistemic Risk-Sensitive Reinforcement Learning.json"
}