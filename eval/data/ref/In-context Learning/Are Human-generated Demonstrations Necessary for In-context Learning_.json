{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.14681",
    "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
    "abstract": " ABSTRACT\nABSTRACT\nDespite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether humangenerated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from humancrafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data. Code is available at https://github.com/ruili33/SEC.1\narXiv:2309.14681v4\narXiv:2309.14\nLarge language models (LLMs) (Zeng et al., 2022; Chowdhery et al., 2022; Wang et al., 2022; Zhang et al., 2023; Touvron et al., 2023; OpenAI, 2023) have shown the ability to learn in context (Brown et al., 2020; Dong et al., 2022; Qin et al., 2023; Sun et al., 2023a): given a few annotated examples as demonstrations, LLMs are able to generate for a new test input (Brown et al., 2020). The standard paradigm of In-context Learning (ICL) still suffers",
    "bib_name": "li2024humangenerateddemonstrationsnecessaryincontext",
    "md_text": "# ARE HUMAN-GENERATED DEMONSTRATIONS NECESSARY FOR IN-CONTEXT LEARNING?\nRui Li1, Guoyin Wang2, Jiwei Li3 1University of Science and Technology of China 2Bytedance 3Zhejiang University\n# ABSTRACT\nABSTRACT\nDespite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether humangenerated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from humancrafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data. Code is available at https://github.com/ruili33/SEC.1\narXiv:2309.14681v4\narXiv:2309.14\nLarge language models (LLMs) (Zeng et al., 2022; Chowdhery et al., 2022; Wang et al., 2022; Zhang et al., 2023; Touvron et al., 2023; OpenAI, 2023) have shown the ability to learn in context (Brown et al., 2020; Dong et al., 2022; Qin et al., 2023; Sun et al., 2023a): given a few annotated examples as demonstrations, LLMs are able to generate for a new test input (Brown et al., 2020). The standard paradigm of In-context Learning (ICL) still suffers from the following conspicuous disadvantages: (1) the final performance is extremely sensitive to the selected demonstrations (Liu et al., 2022; Lu et al., 2023), and to date, there is no widely-agreed criterion for the perfect demonstration selection; (2) crafting demonstrations can be work-intensive, troublesome or even prohibitive: in many ICL scenarios, demonstrations contain not only inputs and corresponding labels, but also the reasoning process (Wei et al., 2022b; Sun et al., 2023b; Yao et al., 2023) generated by annotators. For many tasks (e.g., summarization), it is non-trivial for humans to articulate the reasoning process behind the decision. An important question arises, do we really need humans to provide LLMs with the demonstrations, or can LLMs generate demonstrations on their own? When comparing the ICL approach to a student\u2019s interaction with a tutor, within the ICL framework, the tutor initiates the process by offering the student a set of analogous instances as suggestive prompts, based on which the student gives his answer. There is definitely an alternative paradigm to ICL, where a competent student rely solely on their own memory to find analogous examples, arriving at their answers independently, eliminating the need for any guidance or examples from the tutor.\nEmail: rui li@mail.ustc.edu.cn, guoyin.wang@bytedance.com, jiwei li@zju.edu.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b455/b455ad68-e2fd-4a32-bd26-6c73e0f681a8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Comparison between vanilla ICL and vanilla SEC. Different parts of the prompt and results are highlighted with different colors for emphasis.</div>\nIn this paper, we propose the self-contemplation prompting strategy (SEC for short), a paradigm alternative to ICL. The key point of SEC is that, instead of using hand-crafted examples as demonstrations, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. This is akin to the above process where the student relies solely on their own memory to find analogous examples, rather than examples from the tutor. SEC effectively address the drawbacks of ICL: it will not only spare us the laborious efforts in demonstration crafting, but more importantly, eliminate instability of human crafted prompts. SEC is a flexible framework that not only can be easily combined with existing strengthening strategies for ICL, but with notably greater ease: for example, for the chain-of-thought (CoT) strategy where demonstrations consist of the reasoning process, in SEC, we can prompt LLMs to first automatically create not only inputs and labels, but also the associative reasoning process. In doing so, the efforts of crafting rationales in ICL can be conserved. The demonstrations of vanilla SEC and CoT-SEC are shown in Figure 1 (b) and 2 (b).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f08/9f086636-5793-4619-a31e-6c718d7e8a33.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Comparison between CoT-ICL and CoT-SEC. Different parts of the prompt and results are highlighted with different colors for emphasis.</div>\nWe conduct experiments across multiple LLMs and a wide range of tasks, including arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks. Notably, With no access to ANY training example or human intervention, SEC achieves comparable results to ICL across all benchmarks in both few-shot and CoT scenarios, including MATH (33.5% vs 31.2%) (Hendrycks et al., 2021), MMLU (71.4 % vs 70.4 %) (Hendrycks et al.) and HumanEval (76.2 % vs 73.2 %) (Chen et al., 2021). This result demonstrates that contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for generating illustrative examples, obviating the need for external training data. From a broader perspective, SEC is a zero-shot learning paradigm with no training data, while ICL is still a supervised learning paradigm in nature. Building upon the observation that zero-shot SEC\nperforms comparable to the supervised ICL using domain-specific data, we demonstrate that with the generalization ability of LLMs to date, there is potential that supervised training data may be dispensable in the future. We wish SEC would open doors for further research towards this direction.\n# 2 SELF-CONTEMPLATION PROMPTING\n# 2.1 PRELIMINARIES\nVanilla ICL In the vanilla ICL strategy, LLMs are first given a few human-crafted labelled (fewshot) examples as demonstrations, where each demonstration is an input-output pair. The demonstrations are followed by the test input, and LLMs are prompted to generate the label for the test input based on the given demonstrations. Different from the pre-training-and-finetuning strategy (Devlin et al., 2019), ICL enables the model to make predictions via a single API call. CoT-ICL To bolster the performance on reasoning-intensive tasks, the chain-of-thought (CoT) Prompting strategy (Wei et al., 2022b) incorporates the step-by-step reasoning process into the prompt for LLMs, as illustrated in the pink part in Figure 2. The CoT strategy can be combined with the vanilla ICL, where each demonstration consists of not only an input and an output label, but also the reasoning process to obtain the label.\nConsidering the difficulty and unreliability in human-generated few-shot prompts, we propose selfcontemplation prompting (SEC), a prompting strategy that relies entirely on LLMs to generate fewshot examples tailored to each input test sample. We describe SEC in both vanilla few-shot learning scenario (Vanilla SEC) and chain-of-thought scenario (CoT-SEC) in order below:\nThe SEC demonstration generation prompt for the vanilla few-shot scenario consists of the following components:\n\u2022 Test input (text highlighted in green): at the beginning of the prompt, we directly provide the test example. \u2022 Instruction for the few-shot demonstration generation (text highlighted in yellow): an explicit instruction to ask LLMs to generate demonstrations based on the test input. \u2022 Output format instruction (text highlighted in purple): explicitly defines the output format to facilitate answer extraction from the generated text sequence.\nThen, we deploy the paradigm of vanilla ICL based on model-generated demonstrations. The difference between Vanilla SEC and vanilla ICL is that the former asks LLMs to generate demonstrations while the latter uses human-crafted demonstrations.\n# 2.2.2 COT-SEC\nSEC can be adapted to the CoT strategy with ease. The prompt for SEC demonstration generation in CoT-SEC still consists of three components, i.e., test input, instruction for the few-shot demonstration generation and output format instruction. The difference is that, in the instruction for the few-shot demonstration generation, LLMs are asked to generate demonstrations with not only inputs and labels, but also the reasoning process. The difference between CoT-SEC and CoT-ICL is that the former asks LLMs to generate demonstrations with reasoning process while the latter uses human-crafted demonstrations with reasoning process. Over ICL, the advantages of SEC is as follows: \u2022 No need for hand-crafted demonstrations: since the demonstrations are generated by LLMs\n\u2022 No need for hand-crafted demonstrations: since the demonstrations are generated by LLMs on their own, SEC saves human efforts for demonstration crafting, along with the intricate process for demonstration selection and ordering.\n\u2022 Demonstrations tailored to the test input: the demonstrations are generated given the input sample. Therefore, they are customized to suit each test example. In experiments, we find that this strategy serves a similar purpose to the KNN demonstration in KNN search, leading to more competitive performance on some datasets (details in Sention 3 and Appendix B.7).\n# 3 EXPERIMENTS\n# 3.1 TASKS AND DATASETS\nWe evaluate SEC in the following tasks and datasets (details in Appendix A.1): Arithmetic Reasoning: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021); Commonsense Reasoning: AI2 Reasoning Challenge (ARC) (Clark et al., 2018); Multi-task Language Understanding: MMLU (Hendrycks et al.), C-Eval (Huang et al., 2023); Code Generation: HumanEval (Chen et al., 2021). We use the exact match accuracy as the evaluation metric for the GSM8K and Math dataset. For the GSM8K dataset, we extract the first numerical object in the answer string and convert it to an integer. For the Math dataset, we combine the normalization function in Wei et al. (2022b) and Hendrycks et al. (2021) to reach our normalization function. For HumanEval, we directly use the code in HumanEval Github repository1 (Chen et al., 2021) for answer cleaning and evaluation. The details of extracting few-shot demonstrations are in Appendix A.5.\nMATH\nGSM8K\nARC\nMMLU\nC-Eval\nHumanEval\nNumber of Shots\n4\n5\n5\n4\n4\n4\nTable 1: The number of shots used in the main experiments.\nTable 1: The number of shots used in the main experiments.\n# 3.2 BASELINES\nWe compared SEC to the zero-shot strategy and the ICL (Brown et al., 2020) strategy in both the vanilla and chain-of-thoughts (Wei et al., 2022b) scenarios. To ensure apple-to-apple comparisons, the numbers of human-crafted and LLM-generated demonstrations are the same. The number of shots for different tasks and tasks are shown in Table 1. For all our baselines, we adopt ChatGPT (gpt-3.5-turbo), GPT4 (OpenAI, 2023) and Llama2 34B (Touvron et al., 2023) as the model backbone, details in Appendix A.2. If not specified otherwise, we are using GPT-3.5 for our experiments.\n(GPT-3.5)\nArithmetic\nCommon\nMulti-task NLU\nCode\nMATH\nGSM8K\nARC\nMMLU\nC-Eval\nHumanEval\nPublished Results\nVanilla ICL\n-\n57.1a\n85.2a\n70.0a\n[51.0c]\n[48.1a]\nCoT-ICL\n-\n74.9b\n-\n67.3b\n54.6c\n-\nOur Results\nZero-shot\n16.6\n31.4\n80.1\n64.7\n51.0\n48.8\nZero-shot CoT\n31.7\n73.4\n84.1\n60.5\n50.5\n-\nVanilla ICL\n20.3\n57.1a\n86.5\n70.4\n55.0\n73.8\nVanilla SEC\n18.1\n65.4\n85.9\n68.3\n54.0\n75.6\nCoT-ICL\n31.2\n77.4\n87.9\n69.6\n53.1\n-\nCoT-SEC\n33.5\n77.0\n86.9\n71.4\n54.6\n-\nTable 2: Comparison between SEC and baselines on GPT-3.5. 2\n1https://github.com/openai/human-eval\n(GPT-4)\nArithmetic\nCommon\nMulti-task NLU\nCode\nMATH\nGSM8K\nARC\nMMLU\nC-Eval\nHumanEval\nPublished Results\nVanilla ICL\n-\n-\n96.3a\n86.4a\n[66.4c]\n[67.0a]\nCoT-ICL\n42.6b\n92.0b\n-\n86.4b\n68.7c\n-\nOur Results\nZero-shot\n26.4\n68.3\n88.5\n82.0\n64.8\n67.0a\nZero-shot CoT\n32.6\n86.7\n90.2\n82.2\n64.4\n-\nVanilla ICL\n31.2\n91.5\n94.4\n86.6\n67.7\n83.5\nVanilla SEC\n35.0\n91.7\n94.7\n86.1\n68.1\n83.0\nCoT-ICL\n42.3\n92.0a\n95.1\n86.0\n67.0\n-\nCoT-SEC\n41.9\n92.1\n96.2\n86.5\n67.8\n-\nTable 3: Comparison between SEC and baselines on GPT-4.2\n# 3.3 RESULTS\nTable 2 Table 3, and Table 9 in Appendix A.3 summarizes the performance of SEC across 6 benchmarks on GPT3.5, GPT4, Llama2 34B. Overall, SEC achieves significantly better performances than zero-shot prompting, and comparable performances to few-shot ICL and CoT-ICL in the corresponding setups. Despite the fact that the final decision of SEC relies on demonstrations, SEC is a zero-shot (and unsupervised) model in nature due to the fact that these demonstrations are generated by the LLM itself. SEC bridges the gap between zero-shot prompting and few-shot ICL (Kojima et al.; Brown et al., 2020), through automatic generating few-shot demonstrations. This demonstrates that, for many tasks, contemporary LLMs are competent enough to depend on their own capacity for decision making, removing the need for external training data. Arithmetic Reasoning Primarily, surprisingly, in MATH, SEC significantly outperforms ICL in both GPT-3.5 CoT and GPT-4.0 answer-only scenarios, despite the absence of training datasets. This is because demonstrations in SEC are generated tailored to each test case. In contrast, ICL employs identical few-shot examples for the entire dataset instead of customizing for distinct test cases. Figure 3 illustrates a breakdown of the results on the MATH dataset, categorized by subtopics. We discovered that CoT-SEC outperforms CoT-ICL in 5 subtopics other than Geometry. Another observation is that CoT-SEC consistently outperforms vanilla SEC in all 6 subtopics, even in Algebra and Precalculus, where CoT-ICL underperforms vanilla ICL.\nMulti-task Language Understanding The efficacy of SEC is further proved by its competitive performance on the Multi-task NLU task, which covers a broad spectrum of over 50 domains and disciplines. Moreover, SEC\u2019s competitive performance on C-Eval shows its capability in the cross-linguistic scenario. The breakdown of results on MMLU are shown in Appendix A.4. Code Generation SEC significantly outperforms the zero-shot baseline and vanilla ICL baseline for the code generation task. These results not only demonstrate the effectiveness of SEC, but also question the value of annotated training data with the present of LLMs to date. Our experiments arguably demonstrate that\n2Any result encompassed within brackets signifies data derived from zero-shot prompting. The sup scripts are used to indicate results that have been cited from previous studies: a(OpenAI, 2023), b(Fu et  2023),c(Huang et al., 2023).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4be3/4be305ce-3fcb-48b8-9e96-d5fbe2c23efd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Experiment results on the MATH dataset by subtopic.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2045/20454d30-0ce1-4d9d-b9a8-5723e0966f1f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Effects of the number of examples on the GSM8K and HumanEval datasets. Solid data points represent the data points adopted in the main results.</div>\nLLMs with the scale of GPT3.5 and Llama2 34B inherently possess the ability to achieve performance comparable to few-shot prompting in both vanilla few-shot and CoT scenarios, which indicates the potential that supervised training data may not be indispensable in the future.\n# 4 ABLATION STUDIES\nNumber of Shots We investigate the effect of the number of shots on both SEC and ICL. Figure 4 shows the results from the GSM8K dataset and HumanEval dataset. Our analysis discerns marked differences between the characteristics of few-shot demonstrations in SEC and those manually crafted. Within the context of the two datasets examined, SEC often reaches its optimal performance with fewer shots (e.g., 2 shots) than ICL. The explanation is as follows: since SEC is able to generate demonstrations tailored to the input, there is no need to provide diverse demonstrations to make the prompt applicable to various types of test input. Therefore, fewer demonstrations are needed for SEC.\n1st shot\n2nd shot\n3rd shot\n4th shot\nCanonical Solution\nAvg. Lines\n7.3\n6.9\n6.8\n7.1\n7.8\nTable 4: The average number of lines in model-generated demonstrations and canonical solutions The average length represents the complexity of the code to some extent.\nOne specific issue that stands out is that, on HumanEval, we observe as the number of shots increases, the performance of SEC slightly decreases. To investigate, we provide a comparison of the complexity between model-generated few-shot demonstrations and canonical solutions. The complexity is measured by the length, i.e., the number of lines of the answer. The results are shown in Table 4. It is evident that the complexity of the few-shot demonstrations generated by the model is significantly smaller than the complexity of the canonical solutions, which could lead the model to misjudge the complexity of the task in some test samples. The detailed analysis will be shown in Appendix B.5. Comparing SEC with ICL using LLMs with different capacities To investigate the effect of model capability on the performance of SEC, we conduct an experiment across all four prompting strategies using three models from the GPT3.5 family (details in Appendix B.6). From the results shown in Figure 5, we can conclude that SEC underperforms ICL when the model is not strong enough. This may be due\nOne specific issue that stands out is that, on HumanEval, we observe as the number of shots increases, the performance of SEC slightly decreases. To investigate, we provide a comparison of the complexity between model-generated few-shot demonstrations and canonical solutions. The complexity is measured by the length, i.e., the number of lines of the answer. The results are shown in Table 4. It is evident that the complexity of the few-shot demonstrations generated by the model is significantly smaller than the complexity of the canonical solutions, which could lead the model to misjudge the complexity of the task in some test samples. The detailed analysis will be shown in Appendix B.5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d828/d82892ef-cb29-4f92-81d8-dbfcdc0cbe56.png\" style=\"width: 50%;\"></div>\nModel Figure 5: The performance of all four prompting strategies on three models in the GPT3.5 family on GSM8K. SEC is an emergent ability.\n<div style=\"text-align: center;\">Model Figure 5: The performance of all four prompting strategies on three models in the GPT3.5 family on GSM8K. SEC is an emergent ability.</div>\nCorrectness\nAll Correct\nMinor Error\nMajor Error\nAll Incorrect\nCorrect\n13/20\n1/20\n3/20\n3/20\nIncorrect\n2/20\n5/20\n7/20\n6/20\nTable 5: Correctness of five few-shot demonstrations for 20 correct and 20 incorrect final model predictions in the GSM8K dataset. Minor Error means 1-2 incorrect examples, and Major Error means 3-4 incorrect examples.\nto the fact that weaker models struggle to follow the instructions and generate poor-quality few-shot examples, making SEC not stand up favorably against ICL when deployed on smaller models. Error Analysis in GSM8K We manually inspected 20 correct and 20 incorrect model predictions from GSM8K, assessing the correctness of their few-shot demonstrations. The results are summarized in Table 5. We found that, in GSM8K, the correct rate of few-shot demonstrations for incorrect predictions is significantly lower than that for correct samples (10% vs 65%). Therefore, the errors of the final prediction can to some extent be attributed to the low quality of the few-shot demonstrations, and we leave it to future work to refine the model-generated demonstrations. Please refer to Appendix B.1 for more details about the error analysis. Why incorrect few-shot demonstrations could lead to correct final predictions, while correct few-shot demonstrations could also lead to incorrect predictions? The errors in the few-shot demonstrations generated by LLM can be classified into four main categories: answer extraction errors, computation errors, question errors and logical errors, which are in Appendix B.2. For incorrect few-shot demonstrations that lead to correct results, these errors in few-shot demonstrations often belong to answer extraction errors, computation errors, question errors rather than fundamental errors in the reasoning process (logical errors). For correct few-shot demonstrations that eventually lead to incorrect results, typically, although the few-shot demonstrations are correct, they don\u2019t align closely enough with the test question, hindering the model to extract and apply the pertinent knowledge from the demonstrations. Occasionally, the generated demonstrations may be overly simplistic, leading the model to misjudge the intricacies of the test question. Detailed examples and discussions are available in Appendix B.3. The performance differences between CoT-SEC and CoT-ICL in GSM8K. We show the results of examining the accuracy of 1319 test samples within GSM8k under both CoT-SEC and CoT-ICL in Figure 6. Though the overall performance of these two methods is very similar, their performances on specific individual problems are different: approximately 22% of the samples had opposite correctness between SEC and ICL strategies, in stark contrast to the 11.8% where both failed. In Appendix B.4, we will preliminarily investigate the characteristics of these differences. This difference further highlights that these two prompting strategies each have their respective areas of expertise.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/888f/888f86fd-fead-41f6-b6f0-c516c49e3398.png\" style=\"width: 50%;\"></div>\nLabel in the Figure\nCoT-SEC\nCoT-ICL\nA\nCorrect \u2713\nCorrect \u2713\nB\nCorrect \u2713\nIncorrect \u2717\nC\nIncorrect \u2717\nCorrect \u2713\nD\nIncorrect \u2717\nIncorrect \u2717\nFigure 6: Results of 1319 test samples in the GSM8k dataset under both CoT-SEC and CoT-ICL.\nFigure 6: Results of 1319 test samples in the GSM8k dataset under both CoT-SEC and CoT-ICL.\nComparision between SEC and Auto-CoT The performance of SEC and Auto-CoT (Zhang et al., 2022b) are summarized in Table 6. CoT-SEC\u2019s performance is comparable to Auto-CoT, even without access to the full test dataset and additional clustering.\nMethod\nGSM8k ARC\nZero Shot CoT\n73.4\n84.1\nCoT-ICL\n77.4\n87.9\nAuto-CoT\n77.5\n87.8\nCoT-SEC\n77.0\n86.9\nTable 6: Comparision between CoT-SEC and Auto-CoT\nTable 6: Comparision between CoT-SEC and Auto-CoT\n# 5 RELATED WORK\nIn-context Learning To enhance the performance of ICL, prior research explores the optimization of the selection and sequencing of few-shot exemples (Rubin et al., 2021; Zhang et al., 2022a; Wu et al., 2022; Lu et al., 2021; Fu et al., 2022; Zhou et al., 2022b; Su et al., 2022) such as kNN Prompting (Xu et al., 2023). SEC and kNN Prompting share the idea of using demonstrations tailored to each test question. Incorporating reasoning process and augmenting information (Lampinen et al., 2022) has also been proposed, e.g., CoT Prompting (Wei et al., 2022b), CARP (Sun et al., 2023b), Least-to-Most Prompting (Zhou et al., 2022a) and adding task-specific instructions (Mishra et al., 2022; Wei et al., 2022a; Sanh et al., 2022). Considering the cost in manually crafting prompts, many automatic prompting strategies have been proposed (Sorensen et al., 2022; Shin et al., 2020). Kim et al. (2022) utilize PLMs to automatically generate demonstrations. Li et al. (2022) proposes Self-Prompting framework, which first generates a training corpus and then selects few-shot examples for each test sample by clustering. Compared to Li et al. (2022), our method provides a more flexible way to generate demonstrations without either generating numerous training samples in advance or further clustering and selection. Besides, while Li et al. (2022) focused their research on QA alone, we extended SEC to many new tasks. Besides, recent work has discussed the instability in ICL. Specifically, the selection and shuffling of few-shot examples and the structure of the prompt could cause a drastic fluctuation of the accuracy (Zhao et al., 2021; Lu et al., 2022; Liu et al., 2022; Lu et al., 2023). However, SEC mitigates this issue, since the few-shot examples are only conditioned on LLMs free from any external interference. Chain-of-thoughts Prompting Wei et al. (2022b) proposed CoT prompting, a prompting strategy integrating few-shot training examples with intermediate reasoning process. Zero-shot CoT (Kojima et al.) utilized a simple prompt, \u201dLet\u2019s think step by step\u201d, to elicit the rationale in the output and achieved encouraging results. Following Kojima et al., we add a specific CoT instruction to generate rationales. Our finding supports Kojima et al. that LLMs are decent zero-shot reasoners. Zhang et al. (2022b) proposed Auto-CoT, an automatic prompting strategy leveraging zero-shot CoT to generate rationales. The key difference between Zhang et al. (2022b) and our\u2019s work is that Zhang et al. (2022b) requires the access to a whole test set and involves intensive querying and clustering and , whereas SEC only requires two queries for each test sample.\n# 6 CONCLUSION\nIn this paper, we introduced self-contemplation prompting as a simple, resource-efficient and broadly applicable prompting strategy for LLMs to strengthen their zero-shot ability. This new paradigm addresses some of the issues associated with supervised ICL methods, such as the lack of manually annotated data and the instability of the performance. Our method provides a more comprehensive and consistent evaluation framework to LLMs. Our experiments show that SEC performs comparable to ICL in both answer only and CoT scenario. To the best of our knowledge, SEC achieves the strongest zero-shot performance on a variety of tasks. This extraordinary performance indicates the promise that annotated data might be superfluous given the generalization of LLMs. Furthermore, the difference between the ability of CoT-SEC and CoT-ICL may indicate the promise of further integration of these strategies.\n# LIMITATIONS\nConsidering that SEC employs demonstrations that generated from LLMs, it may experience performance degradation in scenarios where the model is not strong enough, or the test data is not sufficiently represented in the training set. To investigate this issue, we design a novel test set containing 200 3-digit base-5 addition problems which appears rarely in everyday language and on web pages. We test SEC and baseline methods on this dataset. The results, as summarized in Table 7, indicate that SEC tends to exhibit a slight decline in performance on these tasks compared to ICL methods.\nMethod\nAccurary\nZero-shot\n26.5\nZero-shot CoT\n19.0\nVanilla ICL\n28.0\nCoT-ICL\n27.0\nVanilla SEC\n27.0\nCoT-SEC\n24.0\nTable 7: Performance of SEC and baseline methods on 3-digit base-5 addition problems\nThis work was supported by National Key R&D Program of China (No. 2022ZD0119101). We extend our sincerest gratitude to the reviewers, the Area Chairs, the Program Committee, and the Senior Area Chairs for their invaluable insights and suggestions that significantly contributed to the improvement of this manuscript. Their expertise and thoughtful critiques have been instrumental in refining our research and ensuring its quality. Additionally, we would like to thank all individuals who offered their feedback and recommendations throughout the development of this work.\n# REFERENCES\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Evelyn Fix and Joseph Lawson Hodges. Discriminatory analysis. nonparametric discrimination: Consistency properties. International Statistical Review/Revue Internationale de Statistique, 57 (3):238\u2013247, 1989. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models\u2019 reasoning performance. arXiv preprint arXiv:2305.17306, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In ICML 2022 Workshop on Knowledge Retrieval and Language Models. Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7. Junlong Li, Zhuosheng Zhang, and Hai Zhao. Self-prompting large language models for opendomain qa. arXiv preprint arXiv:2212.08635, 2022. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, 2022. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=DHyHRBwJUTN. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, 2022. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022. OpenAI. Gpt-4 technical report, 2023. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In ICLR 2022-Tenth International Conference on Learning Representations, 2022. Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u20134235, 2020. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 819\u2013862, 2022. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022. Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, et al. Pushing the limits of chatgpt on nlp tasks. arXiv preprint arXiv:2306.09719, 2023a. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. Text classification via large language models. arXiv preprint arXiv:2305.08377, 2023b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, and Yu Sun. Pre-trained language models and their applications. Engineering, 2022. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a. URL https://openreview.net/ forum?id=gEZrGCozdqR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL https://openreview. net/forum?id=_VjQlMeSB_J. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning. arXiv preprint arXiv:2212.10375, 2022. Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. k nn prompting: Beyond-context learning with calibration-free nearest neighbor inference. In The Eleventh International Conference on Learning Representations, 2023. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model, 2022.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr. Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486, 2022a. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022b. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021. Denny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022a. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr. Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486, 2022a. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022b. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021. Denny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022a. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b.\n# A DETAILS OF EXPERIMENTAL SETUP\n# A.1 TASKS AND DATASETS\n<div style=\"text-align: center;\">We tested our method on six datasets covering the fields of arithmetic reasoning, commonsense reasoning, and code generation. A summary of our evaluation datasets is shown in Table 8.</div>\nWe tested our method on six datasets covering the fields of arithmetic reasoning, commonsen reasoning, and code generation. A summary of our evaluation datasets is shown in Table 8.\nDataset\nSplit\nExample\nDomain\nMetric\nGSM8K (Cobbe et al., 2021)\nTest\n1319\nArithmetic Reasoning\nacc\nMATH (Hendrycks et al.,\n2021)\nTest\n5000\nArithmetic Reasoning\nacc\nAI2 Reasoning Challenge\n(ARC) (Clark et al., 2018)\nTest\n1172\nCommonsense Reasoning\nacc\nMMLU (Hendrycks et al.)\nTest\n13985\nMulti-task Language Understanding\nacc\nC-Eval (Huang et al., 2023)\nTest\n12342\nMulti-task Language Understanding\nin Chinese\nacc\nHumanEval (Chen et al., 2021)\nTest\n164\nCode Generation\nacc\nTable 8: A summary of our evaluation datasets.\nHere is a detailed description of datasets selection: Arithmetic Reasoning: For arithmetic reasoning, we use GSM8K (grade-school math problems) (Cobbe et al., 2021) and MATH (high school level math competition problems) (Hendrycks et al., 2021). Also, MMLU (Hendrycks et al.) and C-Eval (Huang et al., 2023) contain mathematical questions. Therefore, we also provide the scores of these two datasets in their mathematics problem. Commonsense Reasoning: Here we utilize AI2 Reasoning Challenge (ARC) (Clark et al., 2018), a grade-school level, multiple-choice science questions dataset. Multi-task Language Understanding: For these tasks, we use MMLU (multi-choice questions in 57 domains) (Hendrycks et al.) and C-Eval (multi-choice questions across 52 diverse disciplines in China and four difficulty levels) (Huang et al., 2023). Code Generation: We use HumanEval (Chen et al., 2021), a collection of 164 hand-written programming problems.\n# A.2 MODELS\nFor our main results, we adopt ChatGPT (gpt-3.5-turbo)3, the most capable model in GPT-3.5 family4, GPT4 (OpenAI, 2023) and Llama2 34B (Touvron et al., 2023). And for some ablation research, we also use text-davinci-003 and text-davinci-002 in GPT-3.5 family5. The key difference between text-davinci-003 and text-davinci-002 is that text-davinci-003 was trained of HFRL, but text-davinci-002 was only trained with supervised finetuning.\n# A.3 DETAILS OF RESULTS ON LLAMA2\n<div style=\"text-align: center;\">The results of SEC and baseline methods on Llama2 (Touvron et al., 2023) are summarized in Table 9. SEC reaches comparable to ICL methods and significantly outperforms zero-shot baselines.</div>\n(Llama2 34B)\nArithmetic\nCommon\nMulti-task NLU\nCode\nMATH\nGSM8K\nARC\nMMLU\nC-Eval\nHumanEval\nPublished Results\nVanilla ICL\n6.2a\n42.2a\n54.5a\n62.6a\n-\n[22.6a]\nOur Results\nZero-shot\n3.5\n29.7\n54.4\n56.1\n36.5\n19.5\nZero-shot CoT\n3.9\n34.5\n58.6\n56.5\n36.3\n-\nVanilla ICL\n6.4\n42.0\n67.2\n62.3\n38.5\n22.5\nVanilla SEC\n5.8\n41.2\n65.9\n61.1\n39.0\n21.4\nCoT-ICL\n7.4\n44.5\n68.7\n61.8\n38.9\n-\nCoT-SEC\n7.5\n45.6\n67.5\n62.0\n39.9\n-\n# A.4 DETAILS OF RESULTS ON MMLU\nFigure 7 provides a comprehensive breakdown of the results for the MMLU dataset on GPT3.5. Notably, in domains like physics and chemistry, CoT-SEC substantially outperforms our baselines, while in history and politics, it still lag behind ICL. This disparity aligns with the observed differences in capabilities between these two prompting strategies, as discovered in Section 4.\n# A.5 DETAILS OF FEW-SHOT EXTRACTION\nFor more efficient extraction of few-shot demonstrations of the LLM output, we specify a desire output format in the prompt, as shown in Figure 1. Then, we parsed the output according to th format.\nFor more efficient extraction of few-shot demonstrations of the LLM output, we specify a desired output format in the prompt, as shown in Figure 1. Then, we parsed the output according to the format. In reality, the answer in the demonstration generated by LLMs may not always align with the answer format in the datasets. For example, LLMs might anser by the content of options rather than the option labels for a multi-choice question. Acknowledging these discrepancies, we perform answer extraction and answer validation for the generated answers to ensure that the answers are in the desired format. Initially, we follow the rules mentioned in Section 3 to extract and clean the answer for each few-shot demonstration. 6 Then, each cleaned answer is validated to confirm its consistency with the predetermined format. For all cases that did not pass the verification, we slightly alter the\nFor more efficient extraction of few-shot demonstrations of the LLM output, we specify a desired output format in the prompt, as shown in Figure 1. Then, we parsed the output according to the format.\nIn reality, the answer in the demonstration generated by LLMs may not always align with the answer format in the datasets. For example, LLMs might anser by the content of options rather than the option labels for a multi-choice question. Acknowledging these discrepancies, we perform answer extraction and answer validation for the generated answers to ensure that the answers are in the desired format. Initially, we follow the rules mentioned in Section 3 to extract and clean the answer for each few-shot demonstration. 6 Then, each cleaned answer is validated to confirm its consistency with the predetermined format. For all cases that did not pass the verification, we slightly alter the\n3In our main experiments, we use gpt-3.5-turbo-0301 checkpoint, which will be deprecated later. However, we have discovered that gpt-3.5-turbo-0301 is highly likely to have undergone a model change on June 27th, the day OpenAI replaced the gpt-3.5-turbo checkpoint. 4https://platform.openai.com/docs/models/gpt-3-5 5https://platform.openai.com/docs/models/gpt-3-5 6In the GSM8k dataset, it is a double-edged sword that normalizing all answers to integers to match the characteristics of the dataset\u2019s answers. Details in Appendix A.8.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c84/3c846356-9d19-4761-9121-20e126a25028.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Experiment results on the MMLU dataset by subcategories.</div>\nprompt while setting the temperature to 1 to add randomness. Then we have the LLM generate the demonstration again until it passes the validation.\n# A.6 PROMPTS\n# A.6.1 THE SOURCE OF FEW-SHOT DEMONSTRATIONS IN ICL\nVanilla ICL We used question answer pairs in the prompt in Minerva (Lewkowycz et al., 2022) for the MATH dataset, and the first five examples in the validation set for the ARC dataset. The few-shot demonstrations for MMLU and C-Eval datasets are provided along the dataset. As for the HumanEval dataset, since there are no publicly available few-shot demonstrations and only a test set is provided, we select the first 4 test samples as the few-shot demonstrations for the remaining part of the test set, while choosing the last 4 test samples for the first 4 test samples.\nCoT-ICL We used the prompt in chain-of-thought Hub (Fu et al., 2023) for the GSM8K dataset, Minerva (Lewkowycz et al., 2022) for the MATH dataset. For the ARC dataset, we used the rationale generated by GPT4 model via ChatGPT official website8 for the first five examples in the validation set. The few-shot CoT prompts for MMLU and C-Eval datasets are provided along the dataset. Since the HumanEval dataset directly prompts the model to generate a function body, it\u2019s weird to add CoT in a Python function. Thus we don\u2019t provide CoT results on HumanEval.\nTable 10 summarizes the instruction used for the few-shot demonstration generation across all experiments.\nA.6.3 OUTPUT FORMAT INSTRUCTION\nTable 11 summarizes the output format instruction across all experiments.\n7Any result encompassed within brackets signifies data derived from zero-shot prompting. The superscript are used to indicate results that have been cited from previous studies: a(Touvron et al., 2023). 8https://chat.openai.com/?model=gpt-4 Using this feature requires subscribing to ChatGPT Plus.\n7Any result encompassed within brackets signifies data derived from zero-shot prompting. The superscripts are used to indicate results that have been cited from previous studies: a(Touvron et al., 2023). 8https://chat.openai.com/?model=gpt-4 Using this feature requires subscribing to ChatGPT Plus.\nVanilla SEC\nCoT-SEC\nMATH\nPlease\nfollwing\nthe\nabove\nquestion, generate five simi-\nlar questions with its answer\nin Latex.\nPlease follwing the above question, gener-\nate 5 similar questions with its step by step\nsolution and answer in Latex.\nGSM8K\nPlease generate five similar\nquestions with integer answer\nPlease generate five similar questions with\nstep by step reasoning process and an inte-\nger answer\nARC\nFollwing the above question,\ngenerate five similar multiple\nchoice science questions with\nits choice labels, choice text\nand an answer label\nFollwing the above question, generate ten\nsimilar multiple choice science questions\nwith its choice labels, choice text, the ex-\nplanation of the right answer and an answer\nlabel\nMMLU\nPlease\nfollwing\nthe\nabove\nquestion, create five similar\nmultiple choice {} questions\nwith its choice labels, choice\ntext and an answer label (A or\nB or C or D). Note that each\nquestion should be complete\nand can be answered indepen-\ndently.\nFollwing the above question, generate five\nsimilar multiple choice {} questions with\nits choice labels, choice text, the step by\nstep reason to choose your answer and an\nanswer label (A or B or C or D). Note that\neach question should be complete and can\nbe answered independently.\nC-Eval\n\u8bf7\u6a21\u4eff\u4e0a\u9762\u7684\u95ee\u9898\uff0c\u751f\n\u62105\u4e2a\u76f8\u4f3c\u7684\u5173\u4e8e{}\u8003\u8bd5\u7684\n\u5355\u9879\u9009\u62e9\u9898\uff0c\u548c\u8fd9\u4e2a\u8fd9\u4e2a\n\u9898\u76ee\u7684\u9009\u9879\u6807\u7b7e\uff0c\u9009\u9879\u5185\n\u5bb9\u548c\u7b54\u6848\u3002\u6bcf\u4e2a\u95ee\u9898\u5fc5\u987b\n\u662f\u5b8c\u6574\u4e14\u80fd\u88ab\u72ec\u7acb\u56de\u7b54\n\u7684\u3002\n\u8bf7\u6a21\u4eff\u4e0a\u9762\u7684\u95ee\u9898\uff0c\u751f\u62105\u4e2a\u76f8\u4f3c\u7684\u5173\n\u4e8e{}\u8003\u8bd5\u7684\u5355\u9879\u9009\u62e9\u9898\uff0c\u548c\u8fd9\u4e2a\u8fd9\u4e2a\u9898\n\u76ee\u7684\u9009\u9879\u6807\u7b7e\uff0c\u9009\u9879\u5185\u5bb9\uff0c\u9010\u6b65\u7684\u63a8\u5bfc\n\u51fa\u7b54\u6848\u7684\u8fc7\u7a0b\u548c\u6b63\u786e\u7b54\u6848\u3002\u6bcf\u4e2a\u95ee\u9898\u5fc5\n\u987b\u662f\u5b8c\u6574\u4e14\u80fd\u88ab\u72ec\u7acb\u56de\u7b54\u7684\u3002\nHumanEval\nFollowing the example, gen-\nerate 5 similar function head-\ners with its function body in\npython.\n-\nTable 10: The instruction used for the few-shot demonstration generation across all 6 datasets. T {} in MMLU and C-Eval dataset will fill in specific subcategories.\nTable 10: The instruction used for the few-shot demonstration generation across all 6 datasets. The {} in MMLU and C-Eval dataset will fill in specific subcategories.\nVanilla SEC\nCoT-SEC\nMATH\nPlease output in the following\nform\n[[Question1:question\\n\nAnswer1:$\nanswer\nin\nLa-\ntex$ ]]\\n \\n \\n \\n [[Ques-\ntion5:question\\n\nAnswer5:$\nanswer in Latex$ ]]\\n \\n.\nPlease output in the following form [[Ques-\ntion1:question\\n Sol1:[step by step solu-\ntion]\\n Answer1:$ answer in Latex$ ]]\\n\\n\n... \\n\\n [[Question5:question\\n Sol5:[step\nby step solution]\\n Answer5:$ answer in\nLatex$ ]]\\n\\n.\nGSM8K\nin the following form Q1:{\nquestion} \\n A1:{ answer} \\n\n...\\n Q5:{ question} \\n A5:{\nanswer} \\n.\nin the following form Q1:{ question} \\n\nS1:{ solution} \\n A1:{ answer} \\n\\n\n...\\n\\n Q5:{ question} \\n S5:{ solution}\n\\n A5:{ answer} \\n\\n.\nARC\nin\nthe\nfollowing\nform\n[[Question1:{\nquestion}\n\\n\nLabel1:[choice\nlabels]\\n\nText1:[choice text]\\n Ans1:{\nanswer label} ]]\\n \\n ...\\n\n\\n\n[[Question5:{\nquestion}\n\\n\nLabel5:[choice\nlabels]\\n\nText5:[choice text]\\n Ans5:{\nanswer label} ]]\\n \\n .\nin\nthe\nfollowing\nform\n[[Question1:{\nquestion}\n\\n\nLabel1:[choice\nlabels]\\n\nText1:[choice text]\\n Sol1:{ the explana-\ntion of the right answer} \\n Ans1:{ answer\nlabel} ]]\\n \\n ... \\n \\n [[Question10:{\nquestion}\n\\n\nLabel10:[choice\nlabels]\\n\nText10:[choice text]\\n Sol10:{ the explana-\ntion of the right answer} \\n Ans10:{ answer\nlabel} ]]\\n \\n .\nMMLU\nPlease output in the following\nform [[Question1:[question]\\n\nLabel1:[choice\nlabels]\\n\nText1:[choice text]\\n Ans1:[A\nor B or C or D]]]\\n \\n ... \\n\n\\n\n[[Question5:[question]\\n\nLabel5:[choice\nlabels]\\n\nText5:[choice text]\\n Ans5:[A\nor B or C or D]]]\\n \\n .\nPlease\noutput\nin\nthe\nfollowing\nform\n[[Question1:[question]\\n Label1:[choice la-\nbels]\\n Text1:[choice text]\\n Sol1:[reason\nto choose your answer]\\n Ans1:[A or\nB or C or D]]]\\n \\n ... \\n \\n [[Ques-\ntion5:[question]\\n Label5:[choice labels]\\n\nText5:[choice\ntext]\\n\nSol5:[reason\nto\nchoose your answer]\\n Ans5:[A or B or C\nor D]]]\\n \\n .\nC-Eval\n\u8bf7\u4ee5\u4ee5\u4e0b\u683c\u5f0f\u8f93\u51fa\u7b54\n\u6848\uff1a[[\u9898\u76ee1:[\u9898\u76ee]\\n\n\u9009\n\u9879\u6807\u7b7e1:[\u9009\u9879\u6807\u7b7e]\\n \u9009\n\u9879\u5185\u5bb91:[\u9009\u9879\u5185\u5bb9]\\n \u7b54\n\u68481:[A\u6216B\u6216C\u6216D]]]\\n\n\\n\n...\\n\n\\n\n[[\u9898\u76ee5:[\u9898\u76ee]\\n\n\u9009\u9879\u6807\u7b7e5:[\u9009\u9879\u6807\u7b7e]\\n \u9009\n\u9879\u5185\u5bb95:[\u9009\u9879\u5185\u5bb9]\\n \u7b54\n\u68485:[A\u6216B\u6216C\u6216D]]]\u3002\n\u8bf7\u4ee5\u4ee5\u4e0b\u683c\u5f0f\u8f93\u51fa\u7b54\u6848\uff1a[[\u9898\u76ee1:[\u9898\n\u76ee]\\n \u9009\u9879\u6807\u7b7e1:[\u9009\u9879\u6807\u7b7e]\\n \u9009\u9879\u5185\n\u5bb91:[\u9009\u9879\u5185\u5bb9]\\n \u89e3\u67901\uff1a[\u9010\u6b65\u7684\u63a8\u5bfc\u51fa\n\u7b54\u6848\u7684\u8fc7\u7a0b]\\n \u7b54\u68481:[A\u6216B\u6216C\u6216D]]]\\n\n\\n ... \\n \\n [[\u9898\u76ee5:[\u9898\u76ee]\\n \u9009\u9879\u6807\n\u7b7e5:[\u9009\u9879\u6807\u7b7e]\\n \u9009\u9879\u5185\u5bb95:[\u9009\u9879\u5185\n\u5bb9]\\n \u89e3\u67905\uff1a[\u9010\u6b65\u7684\u63a8\u5bfc\u51fa\u7b54\u6848\u7684\u8fc7\n\u7a0b]\\n \u7b54\u68485:[A\u6216B\u6216C\u6216D]]]\u3002\nHumanEval\nPlease output in the following\nform: [[[Function Header1]]:\\n\n[[Function\nBody1]]:\n]\\n\n\\n\n... \\n\n\\n\n[[[Function\nHeader5]]:\\n\n[[Function\nBody5]]: ]\\n \\n .\n-\nTable 11: Output Format Instruction across all 6 datasets. The output format instruction contains\nTable 11: Output Format Instruction across all 6 datasets. The output format instruction contains numerous special symbols to facilitate better answer extraction, but these symbols may not be well presented in LATEX formatting.\nTable 11: Output Format Instruction across all 6 datasets. The output format instruction contains numerous special symbols to facilitate better answer extraction, but these symbols may not be well presented in LATEX formatting.\nFor better understanding, we have simplified SEC in Figure 1 and Figure 2 of Section 1. We mainly simplified the output format in the few-shot demonstration generation, as the original format is designed for automatic recognition but might not be intuitive for understanding. Therefore, we transformed it into a more comprehensible format. The following Figure 8 and Figure 9 show the unsimplified illustration of Vanilla SEC and CoT-SEC.\n# Unsimplified Illustration of Vanilla SEC\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1dd2/1dd28c9d-cc68-41b5-ba09-9b1294cda202.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: The unsimplified illustration of Vanilla SEC.</div>\nA.8 THE DETAILED DISCUSSION ON ANSWER NORMALIZATION IN THE GSM8K\nNot only might LLM\u2019s outputs fail to conform to the output format, but the questions generated by LLM may also not adhere to our instructions. This phenomenon is especially pronounced in the GSM8K dataset. The dataset\u2019s answers are represented as integers, but the questions generated by LLM may not always manipulate the data in a manner that results in an integer answer. Thus, We rounded the answers to integers so that they could meet the presentation format of the answer in the dataset.\n<div style=\"text-align: center;\">Unsimplified Illustration of CoT-SEC (a): SEC Demonstration Generation</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e986/e986e606-0c5f-41d0-9575-b7237069bace.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: The unsimplified illustration of CoT-SEC</div>\nHowever, the approach of directly rounding the answer may have negative effects on the performance of our method, despite the advantages mentioned above. Since the rounded answer is no longer the exact answer to the original question, there is a risk of misleading the model. Despite the risks, we still decided to use it in the end, considering the advantages mentioned earlier and the convenience of normalization. The final experimental results were very satisfactory.\nA.9 DETAILS REGARDING THE HANDLING OF CASES THAT DO NOT CONFORM TO THE SPECIFIED OUTPUT FORMAT\nDETAILS REGARDING THE HANDLING OF CASES THAT DO NOT CONFO SPECIFIED OUTPUT FORMAT\n# A.9 DETAILS REGARDING THE HANDLING OF CASES THAT DO NOT CONFORM TO THE SPECIFIED OUTPUT FORMAT\nAs mentioned earlier, in CoT-SEC, out of the 5000 samples in the MATH dataset, 6 samples were unable to generate few-shot demonstrations that conform to the specified output format. Meanwhile, in vanilla SEC, all 5000 samples generated few-shot demonstrations that meet the specified output format. Therefore, we decided to use the demonstrations in vanilla SEC on these 6 samples instead of CoT-SEC. Given that the overall performance of vanilla SEC is significantly lower than that of\n# CoT-SEC, we believe that this approach is unlikely to overestimate the actual results of CoT-SEC. a\nCoT-SEC, we believe that this approach is unlikely to overestimate the actual results of CoT-SEC. a\n# B DETAILS OF ABLATION STUDY\nB DETAILS OF ABLATION STUDY\nB.1 DETAILS OF ERROR ANALYSIS IN GSM8K\nCorrectness of Reasoning Process in Model-generated Few-shot Demonstrations In almost all the examples we examined, the correctness of the reasoning process aligns with the correctness of the answers. We observed that in only two test samples (out of 40), the reasoning process was correct, but the answer and reasoning process did not match, which is incorrect. Additionally, there are cases where the answer is correct but the final extracted answer is incorrect due to insufficient accuracy in answer cleaning and extraction.\nError Type We categorized the errors of the final predictions (not few-shot demonstrations) into three types: answer extraction errors, computation errors, and logical errors. In our 20 incorrect examples, only 1 sample belongs to answer extraction errors, 2 belong to computation errors, and the rest are logical errors, which are relatively more difficult to correct and identify.\n# NITION AND DELINEATION OF ERRORS OF FEW-SHOT EXAMPLES\nDEFINITION AND DELINEATION OF ERRORS OF FEW-SHOT EXAMPL\nThe errors in the few-shot examples generated by LLM can be classified into four main categories: Answer extraction errors, Computation errors, Question errors and Logical errors.\n\u2022 Answer extraction errors: This type of error occurs when the model\u2019s solution is correct, and the answer obtained from the solution is also correct. However, after going through answer cleaning and normalization, the answer is no longer correct. Please note that the cases where incorrect answers result from the integer normalization process are also considered answer extraction errors. That is why such errors still occur with a relatively high frequency even when we have a well-established answer extraction and normalization pipeline. \u2022 Computation errors: These errors occur when the model performs calculations incorrectly, but the mathematical expressions or equations used in the process are correct. \u2022 Question errors: This type of error occurs when the question lacks necessary conditions, has ambiguity, or contains contradictions, resulting in an inability to answer. \u2022 Logical errors: Logical errors happen when the model follows an incorrect reasoning process, leading to an incorrect final answer. Generally, errors that do not fall into the first three categories are classified as logical errors.\nWe categorize Logical errors as fundamental errors, as they represent errors in the logical aspects of the model\u2019s reasoning. The other three types of errors are categorized as non-fundamental errors, as they don\u2019t involve errors at the logical level but rather concern shallower aspects like calculations, formatting, and so on.\nB.3 WHY INCORRECT FEW-SHOT DEMONSTRATIONS COULD LEAD TO CORRECT RESULTS AND CORRECT FEW-SHOT DEMONSTRATIONS COULD LEAD TO INCORRECT RESULTS?\nWe have thoroughly researched the detailed proportion of fundamental errors in few-shot examples that leading to correct results. All but 4 cases of the first row (where the final results of SEC are correct) which belong to Major Error or All Incorrect in Table 5 and Table 12 can be changed to Minor Error or All Correct by ignoring non-fundamental errors. Furthermore, among the remaining 4 samples, 2 of them are cases where the answer happens to be correct, meaning the reasoning process is incorrect, but it leads to the correct answer in the end. So, among the 40 correct samples we inspected, only 2 samples (5%) that led to the correct final answer and rationale had more than two few-shot demonstrations with a fundamentally flawed reasoning process and answer. Section 4 mentioned several typical scenarios where incorrect few-shot demonstrations led to correct results. For these cases, we provide specific examples: answer extraction errors (see Figure 13),\ncomputation errors (see Figure 14), question errors (see Figure 15) and instances where the answer happened to be correct despite a flawed reasoning process (see Figure 16).\ncomputation errors (see Figure 14), question errors (see Figure 15) and instances where the answer happened to be correct despite a flawed reasoning process (see Figure 16). In addition, we also provide examples where correct few-shot demonstrations led to incorrect results. In most of these cases, the few-shot demonstrations were not sufficiently similar to the test question (see Figure 17) or were too easy, leading the model to underestimate the difficulty of the final question (see Figure 18).\nIn addition, we also provide examples where correct few-shot demonstrations led to incorrect results. In most of these cases, the few-shot demonstrations were not sufficiently similar to the test question (see Figure 17) or were too easy, leading the model to underestimate the difficulty of the final question (see Figure 18).\nB.4 PERFORMANCE DIFFERENCES BETWEEN COT-SEC AND COT-ICL IN THE GSM8K\nRFORMANCE DIFFERENCES BETWEEN COT-SEC AND COT-ICL IN TH\n# B.4 PERFORMANCE DIFFERENCES BETWEEN COT-SEC AND COT-ICL IN THE GS DATASET\nB.4 PERFORMANCE DIFFERENCES BETWEEN COT-SEC AND COT-ICL IN THE GSM8K DATASET\nIn order to conduct a more in-depth study of the difference, following the manual inspection of correct and incorrect cases in the GSM8K dataset, we also randomly sampled 20 samples where CoT-SEC was correct but CoT-ICL was incorrect and 20 samples where CoT-SEC was incorrect but CoT-ICL was correct. Then we further investigate the model-generated few-shot demonstrations of these 40 samples. The results are summarized in Table 12.\nCoT-SEC\nCoT-ICL\nAll Correct\nMinor Er-\nror\nMajor Er-\nror\nAll Error\nCorrect\nIncorrect\n5/20\n6/20\n6/20\n3/20\nIncorrect\nCorrect\n2/20\n5/20\n6/20\n7/20\nTable 12: Correctness of five few-shot demonstrations for 20 samples where CoT-SEC was correct but CoT-ICL was incorrect, and 20 samples where CoT-SEC was incorrect but CoT-ICL was correct in the GSM8K dataset. Minor Error means 1-2 incorrect examples, while Major Error means 3-4 incorrect examples.\nIt is reasonable that the quality of the generated few-shot demonstrations where CoT-SEC was correct but CoT-ICL was incorrect (shown in Table 12) is lower than the average of samples where CoT-SEC was correct (shown in Table 5), since these examples are relatively more challenging for LLMs. And the quality of the generated few-shot demonstrations where CoT-SEC was incorrect but CoT-ICL was correct (shown in Table 12) is similar to where CoT-SEC was incorrect (shown in Table 5).\nIt is reasonable that the quality of the generated few-shot demonstrations where CoT-SEC was correct but CoT-ICL was incorrect (shown in Table 12) is lower than the average of samples where CoT-SEC was correct (shown in Table 5), since these examples are relatively more challenging for LLMs. And the quality of the generated few-shot demonstrations where CoT-SEC was incorrect but CoT-ICL was correct (shown in Table 12) is similar to where CoT-SEC was incorrect (shown in Table 5). We provided examples where CoT-SEC was correct but CoT-ICL was incorrect (as shown in Figure 19), and examples where CoT-SEC was incorrect but CoT-ICL was correct (as shown in Figure 20).\nWe provided examples where CoT-SEC was correct but CoT-ICL was incorrect (as shown in Figure 19), and examples where CoT-SEC was incorrect but CoT-ICL was correct (as shown in Figure 20).\n# B.5 ERROR ANALYSIS IN HUMANEVAL\nTo further investigate the role and unique characteristics of model-generated few-shot demonstrations, we conducted manual inspections on samples that exhibited changes in correctness (from correct to incorrect or incorrect to correct) when transitioning from k=1 to k=2 and from k=2 to k=3. We examined these samples from four aspects:\n1. The correctness of model-generated few-shot demonstrations. 2. The homogeneity of the model-generated few-shot demonstrations across different test samples. 3. The type of error of the final output: Errors are categorized into two types: Logical Error and Grammar Error. Grammar Error only requires grammatical correction to become the correct answer, while Logical Error cannot be transformed into the correct answer with grammar correction alone. 4. The complexity of the final output: Following the previous standard for measuring complexity, we adopt the number of lines in the answer.\nFor detailed results of the error analysis, please refer to Table 13, Table 14. From Table 13, we could discover that the accuracy rate is quite high and approximately the sa for each transition scenario. It shows that LLMs could produce few-shot code examples with h\naccuracy, and the correctness of the generated code examples does not significantly impact the transition. Table 14 demonstrates that most of the errors of the final results in the transition belong to\naccuracy, and the correctness of the generated code examples does not significantly impact the transition. Table 14 demonstrates that most of the errors of the final results in the transition belong to Logical Error.\nTransition\nAccuracy of Model-generated demonstrations\nNumber of Shots\nCorrectness\n1st shot\n2nd shot\n3rd shot\n1 shot \u21922 shots\nT \u2192F\n6/7\n7/7\n-\n1 shot \u21922 shots\nF \u2192T\n11/13\n10/13\n-\n2 shots \u21923 shots\nT \u2192F\n12/13\n9/13\n11/13\n2 shots \u21923 shots\nF \u2192T\n3/3\n3/3\n3/3\nTable 13: The correctness of few-shot demonstrations generated by LLM in the HumanEval dataset where the correctness of the final output changes. Transition means the type of change these test samples experienced. For example, 1 shot \u21922 shots and T \u2192F means the final output of these examples is correct in 1 shot scenario while it is not correct in 2 shots scenario. The accuracy is represented in a/b, where a represents the number of correct cases, and b represents the total number of cases. Since in 1 shot \u21922 shots scenario only the first two shots have an impact on the final correctness change, we only examined the first two shots.\nTransition\nError Type of the Incorrect Result\nAvg. Lines\nNumber of Shots\nCorrectness\nLogical Error\nGrammar Error\nBefore\nAfter\n1 shot \u21922 shots\nT \u2192F\n6/7\n1/7\n23.1\n12.5\n1 shot \u21922 shots\nF \u2192T\n9/13\n4/13\n15.4\n14.1\n2 shots \u21923 shots\nT \u2192F\n12/13\n1/13\n12.8\n13.0\n2 shots \u21923 shots\nF \u2192T\n3/3\n0/3\n9.7\n9.7\nTable 14: The error type and length (measured by average lines) of the final results in the HumanEval dataset where the correctness of the final output changes. The definition of transition is mentioned in the caption of Table 13. The error type is also represented in a/b, where a represents the number of cases that belong to this particular error type, and b represents the total number of cases. In Avg. Lines, Before means before the transition while After means after the transition. For example, in 1 shot \u21922 shots and T \u2192F scenario, before means in 1 shot while after means in 2 shots.\nTable 14: The error type and length (measured by average lines) of the final results in the HumanEval dataset where the correctness of the final output changes. The definition of transition is mentioned in the caption of Table 13. The error type is also represented in a/b, where a represents the number of cases that belong to this particular error type, and b represents the total number of cases. In Avg. Lines, Before means before the transition while After means after the transition. For example, in 1 shot \u21922 shots and T \u2192F scenario, before means in 1 shot while after means in 2 shots. Based on the data from Table 14, we can infer that during the transition from 1 shot to 2 shots, the reason for the change from correct to incorrect results might be that the complexity of the modelgenerated few-shot demonstrations are relatively low, leading the model to oversimplify the problem (avg. lines changes from 23.1 to 12.5). Then, we speculate that this trend of excessive simplification will continue to strengthen as the number of shots increases, although this change is not intuitively shown by the average line count. While the increase in performance with an increase in the number of shots is intuitive, this finding provides more insights into the reason that, after reaching a certain number of shots (1 or 2 shots), the performance actually decreases with an increase in shots.\nBased on the data from Table 14, we can infer that during the transition from 1 shot to 2 shots, the reason for the change from correct to incorrect results might be that the complexity of the modelgenerated few-shot demonstrations are relatively low, leading the model to oversimplify the problem (avg. lines changes from 23.1 to 12.5). Then, we speculate that this trend of excessive simplification will continue to strengthen as the number of shots increases, although this change is not intuitively shown by the average line count. While the increase in performance with an increase in the number of shots is intuitive, this finding provides more insights into the reason that, after reaching a certain number of shots (1 or 2 shots), the performance actually decreases with an increase in shots.\nAlso, we made an astonishing additional discovery that in the few-shot demonstrations generated from 53.1% (17 out of 32) of the test samples we examined, the first two shots include one that is the same as the question in Figure 10. This indicates that in the HumanEval dataset, the model-generated few-shot demonstrations are not as strongly related to the input questions as we initially anticipated. On the contrary, the few-shot demonstrations generated from different test samples are highly homogenized. This phenomenon is very different from what we observed in the GSM8K dataset. Also, the frequent occurrence of such simple questions (in Figure 10) in model-generated few-shot examples further indicates tha\nodel-generated few-shot examples further indicates that the model-generated demonstrations are o simplistic and underestimate the complexity of the problems.\nodel-generated few-shot examples further indicates that the model-generated demonstrations are o simplistic and underestimate the complexity of the problems.\nPrompt\ndef reverse_list(lst: List[int]) -> List[int]:\n    \"\"\" Reverse a given list of integers\n    >>> reverse_list([1, 2, 3, 4, 5])\n    [5, 4, 3, 2, 1]\n    >>> reverse_list([10, 20, 30])\n    [30, 20, 10]\n    \"\"\"\nSolution\n    return lst[::-1]\nFigure 10: From 53.1% (17 out of 32) of the test samples we examined, the question in the figure appears in the first two shots of the few-shot demonstrations generated.\n# B.6 PERFORMANCE OF GSM8K ON DIFFERENT MODELS\nDuring the experiment on text-davinci-002 and text-davinci-003, we discover that the model fails to generate five demonstrations that all follow our output format. Frequently, the model only generates one demonstration, or it deviates from our output format starting from the second or third example. Therefore, we tentatively infer that these two models lack the capability to generate multiple good few-shot demonstrations, especially text-davinci-002. Thus, we only adopt 1 shot in these two models while adopting 5 shots in gpt-3.5-turbo.\n# B.7 THE SIMILARITY BETWEEN FEW-SHOT EXAMPLES AND TEST QUESTION.\nB.7 THE SIMILARITY BETWEEN FEW-SHOT EXAMPLES AND TEST QUESTION.\nWe calculated the similarity between few-shot demonstrations from SEC and ICL, compared to the original questions. Our methodology involved utilizing the BERTScore (Zhang* et al., 2020) (f1 score) to measure the similarity between question pairs. 9 The results of all 6 datasets are shown in Figure 11. Also, we summarized the BERTScore of each shot in the GSM8K dataset in Figure 12.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d799/d799f375-47b6-442d-8b34-55d918d5171e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: The BERTScore between the few-shot demonstrations from SEC as well as ICL and the original questions of all 6 datasets. Figure 12: The BERTScore of each shot in the GSM8K dataset.</div>\nThe comparative analysis distinctly showcases a heightened similarity between the SEC and test questions, in contrast to ICL. This phenomenon further demonstrates the resemblance between SEC and knn prompting (Xu et al., 2023) which selected the few-shot examples based on k Nearest Neighbors (Fix & Hodges, 1989). Besides, Figure 12 shows that in SEC, the BERTScore decreases as the index of the few-shot demonstrations increases. It is reasonable because as the index increases, they are further away from the test questions, making it harder for the model to reference them effectively. From the perspective of similarity, the quality of demonstrations decreases as the index increases.\n9If the question used in vanilla ICL and CoT-ICL differ, we averaged the BERTScore from both promptin strategies to obtain the final similarity of ICL.\n9If the question used in vanilla ICL and CoT-ICL differ, we averaged the BERTScore from both prompting trategies to obtain the final similarity of ICL.\nTest Question\nJordan wanted to surprise her mom with a homemade birthday cake. From reading the instruc-\ntions, she knew it would take 20 minutes to make the cake batter and 30 minutes to bake the\ncake. The cake would require 2 hours to cool and an additional 10 minutes to frost the cake. If\nshe plans to make the cake all on the same day, what is the latest time of day that Jordan can start\nmaking the cake to be ready to serve it at 5:00 pm?\nQuestion of Few-shot Demonstrations\nRationale of Few-shot Demonstrations\nExtracted\nAnswer\nof\nthe\nOutput\nSamantha wants to make a lasagna for\ndinner.\nThe recipe requires 30 min-\nutes to prepare the ingredients, 1 hour to\nbake the lasagna, and 10 minutes to let\nit cool. If she wants to serve the lasagna\nat 7:00 pm, what is the latest time she\ncan start making it?\nTo serve the lasagna at 7:00 pm, Saman-\ntha needs to account for the 30 minutes\nof preparation time, 1 hour of baking\ntime, and 10 minutes of cooling time.\nTherefore, she needs to start making the\nlasagna at 5:20 pm (7:00 pm - 1 hour -\n10 minutes - 30 minutes). \u2713\n20 \u2717\nJohn wants to make a pot of chili for a\nparty. The recipe requires 20 minutes to\nprepare the ingredients, 2 hours to cook\nthe chili, and 15 minutes to let it cool.\nIf the party starts at 6:00 pm, what is\nthe latest time John can start making the\nchili?\nTo serve the chili at 6:00 pm, John needs\nto account for the 20 minutes of prepara-\ntion time, 2 hours of cooking time, and\n15 minutes of cooling time. Therefore,\nhe needs to start making the chili at 3:25\npm (6:00 pm - 2 hours - 15 minutes - 20\nminutes). \u2713\n25 \u2717\nEmily wants to bake a batch of cookies\nfor her friends. The recipe requires 15\nminutes to prepare the dough, 12 min-\nutes to bake the cookies, and 5 minutes\nto let them cool. If she wants to serve\nthe cookies at 4:00 pm, what is the lat-\nest time she can start making them?\nTo serve the cookies",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of the necessity of human-generated demonstrations in In-context Learning (ICL) for large language models (LLMs). Previous methods face challenges such as sensitivity to selected demonstrations and the labor-intensive nature of crafting these demonstrations. The authors propose a new method to alleviate these issues.",
        "problem": {
            "definition": "The problem is the dependence on human-generated demonstrations for effective In-context Learning (ICL) in large language models, which can lead to inconsistencies and inefficiencies.",
            "key obstacle": "The main challenge is the laborious process of crafting demonstrations and the instability of performance due to the sensitivity of models to the selected demonstrations."
        },
        "idea": {
            "intuition": "The idea stems from questioning whether LLMs can generate their own demonstrations instead of relying on human-crafted examples, thereby improving efficiency and consistency.",
            "opinion": "The proposed self-contemplation prompting strategy (SEC) allows LLMs to create their own demonstrations based on the test input, which could enhance their performance.",
            "innovation": "The primary innovation of SEC is that it eliminates the need for human-crafted demonstrations, allowing LLMs to generate tailored examples autonomously."
        },
        "method": {
            "method name": "Self-Contemplation Prompting",
            "method abbreviation": "SEC",
            "method definition": "SEC is a prompting strategy that enables LLMs to autonomously generate demonstrations for In-context Learning, thereby reducing reliance on human input.",
            "method description": "SEC allows LLMs to generate their own demonstrations based on the test input, improving the flexibility and applicability of the learning process.",
            "method steps": [
                "Provide the test input.",
                "Instruct the LLM to generate demonstrations based on the test input.",
                "Define the output format for the generated demonstrations."
            ],
            "principle": "SEC is effective because it tailors the generated demonstrations to the specific test case, thus enhancing the relevance and applicability of the examples."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted across multiple tasks including arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation, using datasets like GSM8K, MATH, ARC, MMLU, C-Eval, and HumanEval.",
            "evaluation method": "The performance of SEC was compared against baseline methods, measuring accuracy across various tasks and analyzing the generated demonstrations."
        },
        "conclusion": "The experiments demonstrate that SEC significantly outperforms zero-shot strategies and achieves comparable results to ICL with human-crafted demonstrations, suggesting that LLMs can effectively rely on their own capabilities for many tasks.",
        "discussion": {
            "advantage": "The key advantages of SEC include reduced labor in demonstration crafting, improved stability of results, and the ability to customize demonstrations to specific test inputs.",
            "limitation": "A limitation of SEC is its potential performance degradation in scenarios where the model is not sufficiently strong or the test data is poorly represented.",
            "future work": "Future research could focus on refining the model-generated demonstrations and exploring the integration of SEC with other prompting strategies."
        },
        "other info": {
            "info1": "The method is adaptable to both standard ICL and chain-of-thought prompting.",
            "info2": {
                "info2.1": "Code for the SEC method is available at https://github.com/ruili33/SEC.",
                "info2.2": "The study was supported by the National Key R&D Program of China."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the necessity of human-generated demonstrations in In-context Learning (ICL) for large language models (LLMs), highlighting foundational concepts related to the dependence on these demonstrations."
        },
        {
            "section number": "1.2",
            "key information": "The significance of in-context learning is underscored by the paper's focus on the challenges posed by reliance on human-generated demonstrations, which can lead to inconsistencies and inefficiencies."
        },
        {
            "section number": "3.1",
            "key information": "The proposed self-contemplation prompting strategy (SEC) allows LLMs to create their own demonstrations based on test input, improving adaptation and robustness in in-context learning scenarios."
        },
        {
            "section number": "3.3",
            "key information": "The paper analyzes the SEC method, which enables LLMs to autonomously generate demonstrations, thus optimizing the selection process for in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts in the SEC method significantly influences the outcomes of in-context learning by allowing LLMs to tailor demonstrations to specific test cases."
        },
        {
            "section number": "6.1",
            "key information": "The paper discusses limitations of SEC, including potential performance degradation in scenarios where the model is not sufficiently strong or the test data is poorly represented, relating to model bias and context sensitivity."
        },
        {
            "section number": "7",
            "key information": "The conclusion emphasizes that SEC significantly outperforms zero-shot strategies and achieves comparable results to ICL with human-crafted demonstrations, suggesting future directions for research in model-generated demonstrations."
        }
    ],
    "similarity_score": 0.7386589143768977,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Are Human-generated Demonstrations Necessary for In-context Learning_.json"
}