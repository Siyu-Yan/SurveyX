{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.12391",
    "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
    "abstract": "This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.",
    "bib_name": "li2024llminferenceservingsurvey",
    "md_text": "# LLM Inference Serving: Survey of Recent Advances and Opportunities\nBaolin Li\u2217, Yankai Jiang\u2217, Vijay Gadepally\u2020, Devesh Tiwari\u2217 \u2217Northeastern University, \u2020 MIT\nAbstract\u2014This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.\n# I. INTRODUCTION\nI. INTRODUCTION\nLarge language models (LLMs) have rapidly gained immense popularity since the release of ChatGPT. However, deploying and scaling these powerful AI models in production environments has presented significant challenges. The substantial computational and memory demands of LLMs often necessitate the use of high-performance GPU servers, yet even these resources can be strained by the sheer size of the models and the lengthy text sequences they process. The growing demand for LLM-powered applications has fueled a surge of research into LLM serving systems. In this paper, we present a comprehensive survey of these systems, focusing on advancements since 2023. While previous LLM system research existed, the landscape has dramatically shifted within the last year. Nearly every major system conference now features dedicated sessions on LLMs, with a particular emphasis on serving systems due to their widespread deployment and the importance of low-latency performance for user experience. The sheer volume of research published in such a short timeframe makes it difficult for LLM practitioners to stay abreast of developments and identify the most promising approaches for real-world deployment. This survey aims to provide a clear overview of the current state of the art, highlighting key areas of innovation and practical considerations for production environments. In this survey, we have meticulously selected all the highquality research papers focused exclusively on LLM serving systems, published between January 2023 and June 2024. Our selection criteria prioritized publications from prestigious machine learning (ML) and system venues (e.g., ASPLOS, MLSys, OSDI), as well as impactful arXiv submissions from established industry and academic research groups. Notably, we exclude studies that modify LLM decoding algorithms (e.g., multiple decoding head [1], lookahead decoding [2], key token selection [3]) and solely focus on system-level\nenhancements that maintain the integrity of standard LLM decoding processes. While a few prior LLM inference system surveys exist [4], [5], [6], these generally cover a broader scope and do not specifically emphasize system research. Additionally, many of the papers discussed in those surveys involve decoding algorithm modifications that can affect model accuracy. Our survey, in contrast, explicitly focuses on system-level solutions that do not alter the core LLM decoding mechanisms. Moreover, our survey encompasses a significant body of research published after the release of these earlier surveys, thus providing a more comprehensive and up-to-date overview of the field. We have organized the recent advances in LLM serving systems into four distinct categories, each with its own set of challenges and opportunities, which we will delve into in the following sections. KV cache and memory management. Efficient memory management is crucial to handle the dynamic growth of KV caches, which store previous key-value pairs to accelerate LLM inference. Recent research explores non-contiguous memory allocation, distributed management, and intelligent caching strategies to optimize memory utilization. Compression techniques are also being investigated to reduce the overall memory footprint, ultimately enhancing LLM performance and scalability by allowing for longer context lengths and lower memory overhead. LLM computation optimization. Efforts to optimize LLM computation focus on request batching to maximize resource utilization. Additionally, disaggregating the inference process into prefill and decode phases enables independent optimization and hardware specialization. Model parallelism, employing various techniques, facilitates efficient execution across multiple GPUs. These strategies collectively enhance LLM execution efficiency and hardware utilization. Cloud LLM deployment. Cloud platforms provide a scalable and cost-effective foundation for LLM inference. However, challenges remain in optimizing costs and resource utilization. Research is addressing this through techniques such as spot instance management, serverless optimizations, intelligent resource allocation, and power management. Additionally, strategies like cloud task co-location and token delivery optimization enhance user experience and overall cloud efficiency. Emerging research fields. Emerging areas in LLM serving include retrieval-augmented generation (RAG) and mixtureof-experts (MoE) inference. RAG faces challenges related to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2c1f/2c1fb7a2-9ff4-45f0-aa75-487623a06073.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: Transformer-based LLM architecture including both the multi-head attention mechanism and feed-forward network.</div>\nthe computational overhead of increased input lengths due to retrieved documents, while MoE inference grapples with efficient communication and load balancing between distributed experts. Other research efforts address ethical concerns in LLM serving, such as fairness and environmental sustainability, for which we provide a comprehensive list of relevant studies.\n# II. BACKGROUND\nMainstream LLMs are built on multiple transformer blocks [7]. Each identical transformer primarily consists of self-attention-based Multi-head Attention (MHA) operations and Feed-Forward Networks (FFN). Initially, the transformer applies three weight matrices (W Q, W K, W V ) to the input X (encoded representation of input text sequence) to compute queries Q, keys K, and values V . Then, the Self-attention is calculated as:\nThis is the calculation of one attention head (Hi), and multiple heads are concatenated and linearly projected into the final attention result:\nHi = Attention(XW Q i , XW K i , XW Q i ) Multi-Head Attention = Concat(H1, H2, ..., Hh)W O\nMHA makes transformers focus on different parts of the sequence in different representational spaces. Next, following the MHA block, the normalized output is fed into a positionwise FFN, which consists of two linear transformations with a ReLU activation. FFN(x) = max(0, xW1 + b1)W2 + b2\nThe FFN can be applied separately to each position, further refining the information captured by the MHA block. The output will have the same dimension as the input X. Fig. 1 provides a visualization of the LLM architecture.\n# B. Overview of LLM Inference\nLLM inference generates output tokens autoregressively [8] based on the initial input sequences P, referred to as Prompts. This process is divided into two major phases: the prefill phase and the decoding phase. The prefill phase is essential for setting up the model to generate text efficiently, while the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4032/4032a317-6c57-4e92-9ded-b59eafe0dea1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2: Prefill and decoding phase in the LLM inference.</div>\ndecoding phase handles the generation of subsequent tokens. We visualize this process in Fig. 2. The prefill phase starts with a tokenized and encoded representation of the prompt going through layers of the transformers. Note that the generated key-value (KV ) pairs of all transformer blocks are cached during the prefill phase, referred to as KV cache [9]. It ensures that the model can generate tokens more efficiently without recomputing the KV vectors of all previous tokens. Let the input prompt P = [p1, p2, ..., pn], during the prefill phase, a new token is generated, denoted as Pn+1, and the new K and V are cached as [(k1, v1), (k2, v2), ..., (kn, vn)]. The decoding phase is where the model generates new tokens autoregressively. The LLM predicts the next token, appends the newly generated token pn+1 to the original prompt P, and updates the KV cache. Note that the KV cache grows linearly with the number of tokens generated. The autoregressive LLM inference process is outlined in Algorithm 1.\nAlgorithm 1 Autoregressive LLM Inference\nInput P: encoded input sequence [p1, p2, ..., pn]\nOutput X: generated new sequence [].\n1: Forward Pass ([p1, p2, ..., pn])\n2: Store the KV cache: [(k1, v1), (k2, v2), ..., (kn, vn)]\n3: for i from 1 to M do\n4:\nPredict the next token pn+i using the KV cache.\n5:\nStore (kn+i, vn+i) to the KV cache.\n6:\nX \u2190X \u222a{pn+i}\n7:\nif pn+i is EOS token or len(X)>max length then\n8:\nbreak\nIII. MEMORY MANAGEMENT AND CACHING In this section, we explore memory management techniques to mitigate memory footprint and access overhead during LLM inference. While model parameters remain constant and intermediate activations are relatively small, the KV cache \u2013 used to store attention information \u2013 grows substantially with the number of generated tokens. Therefore, recent research has focused on efficient KV cache management to enable larger batch sizes and longer context processing.\n# A. Efficient Management of KV Cache\nPagedAttention [10] identifies that the KV cache dynamically grows and shrinks over time as the model generates new tokens, but the request generation lifetime and length are not known a priori. Thus, it proposes to manage the KV cache as non-contiguous memory blocks. Compared to\ncontiguous KV cache, non-contiguous KV cache management significantly reduces the memory waste on pre-allocation and fragmentation. Due to its efficient memory management using pages, PagedAttention has become an industry norm in LLM serving frameworks, supported by TGI [11], vLLM [10] and TensorRT-LLM [12]. Despite its success, researchers still identify its weakness as PagedAttention requires rewriting attention kernels to accommodate the non-contiguous memory blocks, its memory manager adds software complexity and redundancy, and introduces performance overhead. Recently, vAttention [13] was proposed to retain the KV cache in contiguous virtual memory. It leverages pre-existing low-level system calls for demand paging, which is a standard operating system feature to reduce the software complexity. vAttention overlaps memory allocation with computation, pre-allocates memory ahead of time, and defers memory reclamation to hide the latency of memory allocation and improve the overall performance of the system. Besides system memory management, other efforts have addressed application-specific KV cache efficiency. Prompt Cache [14] designs specific prompt schema for users to submit their requests, so that attention states from these pre-defined modules (e.g., system prompt) can be reused across multiple prompts. AttentionStore [15] identifies that human interactions with applications such as ChatGPT are mostly multi-turn conversations. However, LLM engines would discard the KV cache when the user session becomes inactive to free up HBM space for other active sessions and re-compute the whole KV cache again when the session becomes active, leading to extra pre-filling costs. AttentionStore utilizes slower-mediums (e.g., CPU memory and disk), overlaps KV cache loading with computation, and designs intelligent pre-fetching and eviction policies.\n# B. Support for Long-Context Applications\nServing long-context LLM applications is particularly challenging as the size of the KV cache scales with the number of tokens. The limited memory limits LLM\u2019s ability to handle long sequences, demanding more memory-efficient solutions. Ring attention [16] is a novel distributed approach that leverages blockwise computation of attention and feedforward of long sequences across multiple devices. It efficiently overlaps KV cache communication with computation and extends the context length by the device count times. Infinite-LLM [17] is another distributed solution, it breaks down KV cache into smaller manageable units called rBlocks across GPUs/CPUs, and efficiently manages them with dynamic memory sharing and coordination. MemServe [18] unifies handling of interrequest and intra-request optimizations for LLM serving by introducing MemPool, a distributed memory pool to manage KV cache across all cluster memory and employs a global scheduler to maximize KV cache reuse. When the context grows larger than the GPU memory limit, most systems offload the KV cache to the CPU. InfiniGen [19] is a solution that speculates the important KV cache entries\nby rehearsing the attention computation of the current layer in the preceding layer and prefetches only the essential entries to the GPU, thereby reducing the data transfer overhead. LoongServe [20] introduces a new parallelism paradigm called Elastic Sequence Parallelism (ESP) to dynamically adapt to resource usage variance between requests and phases (prefilling and decoding) of a request. It reduces KV cache migration overhead and KV cache fragmentation when serving long sequences.\n# C. Compression of KV Cache\nDue to the large memory footprint of LLM serving, some systems have resorted to compressing the KV cache. On top of memory aggregation and communication scheduling, FlexGen [21] uses fine-grained groupwise quantization to compress the weights and KV cache to 4 bits. KIVI [22] analyzes the element distribution of the LLM KV cache and applies asymmetric quantization of the Key and Value cache. KIVI quantizes the key cache per-channel (grouping elements along the channel dimension) and the value cache per-token to achieve minimum quantization error. Gear [23] achieves near-lossless high-ratio KV cache compression by quantizing the majority of entries of similar magnitudes and employs a low-rank matrix to approximate the quantization error. MiniCache [24] observes that the KV cache states exhibit high similarity between adjacent layers in the middleto-deep portion of LLMs. Based on this insight, MiniCache leverages this high similarity to merge them into a shared representation to reduce redundancy, while also identifying and retaining distinct states that are crucial for maintaining the model\u2019s performance, preventing information loss during compression.\n# IV. COMPUTATION TASK SCHEDULING\nBesides memory and KV cache management, the computation of LLM also presents significant system challenges. Due to the sequential dependency between tokens during the autoregressive generation, LLM can only generate one token at a time for each request. Thus, LLM inference workloads are less resource-efficient than training workloads on GPU hardware that is designed for massively parallel execution. Following this incentive, we investigate system solutions that optimize the scheduling of computation tasks during the inference process.\n# A. Request Batching\nWhen a single request cannot efficiently utilize the GPU, it is intuitive to batch multiple inference requests together to boost the occupancy of GPU cores. However, as responses to different prompts can have significantly variable lengths, when batched together, the shorter responses are forced to wait for the longer ones to complete, resulting in computational waste. Response Length Perception and Sequence Scheduling [25] instructs the LLM to predict the response length before starting to generate the actual response, and batches queries with similar predicted response lengths to reduce computational\nwaste. A similar approach, S3 [26], finetunes a Distillbert model for sequence length prediction. Upon mispredictions, it preempts sequences that exceed their allocated memory and retrain the predictor to learn from its mistakes. Generation length prediction based batching is less practical due to the strong reliance on the predictor. Orca [27] proposes continuous batching at the token level rather than the request level. It continuously schedules new requests into the batch as soon as a request in the current batch completes. Continuous batching now has become an industry standard in LLM serving frameworks, incorporated into the software of TGI, vLLM, and TensorRT-LLM. Based on continuous batching, DeepSpeedFastGen [28] proposes a dynamic SplitFuse mechanism that decomposes long prompts into smaller chunks scheduled across multiple iterations and composes short prompts together to maintain the inference running at high throughput region (bounded by GPU compute not memory bandwidth). A similar idea was explored in Sarathi-Serve [29], which splits prefill requests into smaller chunks and schedules them alongside ongoing decode requests without causing stalls (stall-free batching). This allows new requests to join a running batch without pausing ongoing decodes, leading to minimal pipeline bubbles.\n# B. Disaggregated Inference\nLLM inference goes through a prefill stage to process the prompt, populate the KV cache, and start the decoding stage to generate tokens (Sec. II). Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. However, these two phases display distinct characteristics and can interfere with each other when requests at the prefill stage are batched with requests at the decoding stage. TetriInfer [30] separates prefill and decode instances, allowing each phase to run independently and preventing interference between batch-like prefill jobs and latency-critical decode tasks. It employs a two-level scheduling algorithm that incorporates predicted resource usage to avoid scheduling hotspots during the decode phase, ensuring efficient resource allocation and minimizing contention. Splitwise [31] extensively characterizes the differences in the execution and utilization patterns of the prefill and decoding stage on different generations of GPUs (heterogeneous hardware). Splitwise proposes to split these two phases into separate machines, allowing for specialized hardware for each phase to achieve better utilization, reduce hardware ownership costs, and save energy. DistServe [32] designs a placement algorithm to schedule the prefill and decoding stage computation tasks. In clusters with high-speed cross-node networks, DistServe optimizes parallelism configurations for prefill and decoding instances independently to achieve the best per-GPU goodput; In clusters with limited cross-node bandwidth, it ensures that prefill and decoding instances of the same stage are co-located within a single node and optimizes parallelism configurations within the node.\n# C. Model Parallelism\nLLMs can have hundreds of billions of parameters, requiring model parallel execution on multiple GPUs. Pope et al. [9] develop an analytical model for inference efficiency, enabling the selection of optimal multi-dimensional partitioning techniques tailored for TPU v4 slices based on specific application needs. HeteGen [33] introduces a framework for heterogeneous parallel computing using CPUs and GPUs. It employs a heterogeneous parallel computing algorithm to distribute computation within its hybrid heterogeneous parallelism framework and enables asynchronous overlap to mitigate I/O bottlenecks between the CPU and GPU. ExeGPT [34] can find an optimal schedule control variable of the batch size and tensor parallelism degree that maximizes inference throughput while adhering to a given latency limit. It leverages the distribution of input and output sequence lengths to allocate resources efficiently and determine the best parallelism configuration. Helix [35] is designed to partition an LLM across heterogeneous GPUs and different types of network connections. It formulates its model partition scenario as a max-flow problem of a directed, weighted graph whose nodes represent GPU instances and edges capture both GPU and network heterogeneity through their capacities in the maxflow problem.\n# V. LLMS IN THE CLOUD\nV. LLMS IN THE CLOUD\nLLM deployments are computationally intensive and often require significant infrastructure to run effectively. Cloud platforms offer a scalable and cost-effective solution for deploying LLMs, eliminating the need for expensive hardware investments. The flexibility of cloud deployment allows organizations to easily adjust resources as needed, ensuring optimal performance and minimizing downtime. However, the significant costs associated with cloud computing resources and the challenge of ensuring their efficient utilization can be major obstacles for LLM service providers.\n# A. Cloud Deployment Cost\nModern clouds offer a variety of spot instances (e.g., AWS EC2 Spot Instance, Azure Spot Virtual Machines, Google Cloud Spot VMs). These instances run on spare capacity and are offered at highly discounted prices, but may be preempted at any time when other instances need the capacity. SpotServe [36] addresses the challenges of using these instances for LLM serving, such as how to quickly adapt to changes in available instances and how to minimize the cost of migrating instances when interruptions occur. It also introduces a stateful inference recovery mechanism for inference engines to commit their progress at the token level and efficiently resume interrupted requests. Serverless is a recently emerged cloud computing paradigm, where inference service users can submit their model to the cloud and the cloud provider takes care of all infrastructure provision and scaling with varying inference request load, and saves unused hardware costs for customers. A major challenge in serverless is mitigating cold start, where a service\ninstance would be shut down after not being accessed for some time, and once a new request arrives, it would experience a latency spike associated with re-initializing the service instance. ServerlessLLM [37] addresses these latency issues by utilizing the underutilized storage and memory resources available on GPU servers. It introduces a new checkpoint format and loading system to speed up LLM model loading, a live migration mechanism to avoid interrupting ongoing inferences, and a locality-aware server allocation strategy to minimize LLM inference cold start latency. Cloud providers often offer a wide range of heterogeneous instance selections labeled at different prices. M\u00b4elange [38] is a cloud resource allocation framework that considers three key LLM service characteristics: request size, request rate, and service-level objective. It automatically navigates through the GPU option space to determine the most cost-efficient heterogeneous GPU allocation for a given LLM service. With the resources allocated and model hosted on the GPUs, Llumnix [39] is a dynamic scheduling system for LLM serving that addresses the challenges of heterogeneous and unpredictable requests by rescheduling them across multiple model instances at runtime \u2013 similar to how OS context switches across cores. Llumnix introduces an efficient live migration mechanism for requests and their in-memory states, minimizing downtime during rescheduling, and employs a dynamic scheduling policy that unifies various rescheduling scenarios, such as load balancing, de-fragmentation, prioritization, and auto-scaling. This efficiency has resulted in significant cost savings while achieving similar tail latency.\n# B. Cloud Efficiency\nA key bottleneck resource in cloud datacenters is power, which LLMs are quickly saturating due to their growing computation demand. POLCA [40] characterizes the power consumption patterns of LLMs in the cloud and finds that while training LLMs demands a lot of power and can strain the data center\u2019s power infrastructure, inference tasks offer more flexibility for power management due to their less predictable power demands. POLCA devises a framework to manage power in LLM inference clusters by dynamically applying techniques such as GPU frequency locking and power capping. PerLLM [41] takes the LLM inference to an edge-cloud collaboration scenario, where it leverages the strengths of edge computing (low latency, reduced energy costs) and cloud computing (high processing power) to handle LLM inference tasks efficiently. PerLLM employs a Constraint Satisfaction Upper Confidence Bound (CS-UCB) algorithm to optimize service scheduling and resource allocation while adhering to constraints like processing time, bandwidth, and computing power \u2013 achieving energy LLM efficiency. Workloads often get co-located in the cloud environment. FlexLLM [42] is a system designed to efficiently service LLM inference and parameter-efficient fine-tuning (PEFT) requests in the same iteration. LLM inference, which involves generating text token by token, is primarily limited by memory bandwidth due to the need to access all model parameters\nfor each token generation. In contrast, PEFT, which processes all tokens of a request simultaneously, is mainly constrained by compute resources, such as the tensor cores on GPUs. FlexLLM introduces a token-level fine-tuning mechanism that breaks down the fine-tuning process into smaller, more manageable token-level computations to minimize memory usage and inference latency, making co-serving feasible. As LLM inference follows token-by-token generation, users also read the response word-by-word. Andes [43] defines a user experience metric of Quality of Experience (QoE) for text streaming services. It is formulated by comparing the actual token delivery timeline (TDT) of a request with its expected TDT. The expected TDT is determined by the expected time to first token (TTFT) and the expected token delivery speed (TDS), which can vary depending on factors like the user\u2019s typical reading speed. The intuition is generating text too fast (than user reading speed) does not yield QoE benefits, wasting cloud resources. Andes addresses this by strategically allocating GPU resources among multiple requests to optimize QoE. It employs a dynamic priority-based preemptive scheduler that operates at the token level, prioritizing urgent requests and preempting those that have been sufficiently served. Andes improves average QoE and can handle higher request rates while maintaining similar token generation throughput.\n# VI. EMERGING RESEARCH FIELDS\nVI. EMERGING RESEARCH FIELDS\nA. Retrieval Augmented Generation\n# A. Retrieval Augmented Generation\nRetrieval-Augmented Generation (RAG) [44] is a technique that enhances LLMs by incorporating external information sources. It addresses the limitations of LLMs in retaining factual knowledge and their tendency to generate inaccurate or fabricated information (hallucinations). RAG operates in two stages: retrieval and generation. During retrieval, the system identifies the most relevant contexts from an external knowledge base or corpus based on the given query. Once the relevant contexts are retrieved, they are integrated into the LLM\u2019s generation process in different processes including concatenation (where the retrieved contexts are simply appended to the query) and cross-attention (where the LLM attends to the retrieved contexts during generation). Sparse RAG [45] observes that RAG can be computationally expensive due to the increased input length from retrieved documents. It first encodes retrieved documents in parallel to eliminate latency caused by long-range attention, then selectively decodes the output by attending only to highly relevant caches chosen via prompting the LLM with special control tokens. RAGCache [46] caches intermediate states of external knowledge with a knowledge tree to organize and store intermediate states. The cached knowledge can be shared across multiple queries to reduce the redundant computation. Another knowledge caching technique is CacheBlend [47], which selectively recomputes a small portion of the KV cache based on the preceding text in the input.\nB. Mixture-of-Experts Inference The mixture of Experts (MoE) is used in LLMs to improve efficiency and performance. It divides the model into specialized sub-networks, called \u201cexperts\u201d, each focusing on a specific task. A \u201cgating\u201d network then directs input to the most suitable expert. In the inference process of an MoE transformer, the input is first passed through a gating network. This network determines which expert, or a combination of experts, is best suited to process the specific input. MoE\u2019s sparsely activated subset of experts avoids the large computational need to process the entire model for every inference. MoE Communication. Lina [48] is a system designed to address the all-to-all communication bottleneck in distributed MoE. The all-to-all communication occurs when distributed MoE sends tokens to their selected experts for processing and then sends the results back to the original devices. During inference, Lina dynamically schedules resources based on expert popularity, balancing the transfer size and bandwidth of all-to-all communication across devices. ExFlow [49] is an optimization technique to accelerate the inference of distributed MoE. It leverages the inter-layer expert affinity, which is the correlation between expert selection across different MoE layers. By placing experts on corresponding GPUs based on their affinity, ExFlow reduces cross-GPU routing latency and improves inference throughput. Expert offloading. SiDA-MoE [50] (Sparsity-inspired DataAware) leverages both main memory and GPU memory by exploiting the inherent sparsity of expert activation in MoE models. SiDA-MoE includes two parallel threads: an inference thread and a hash-building thread. The hash-building thread predicts which experts will be activated for each token at each layer, storing these predictions in a hash table. The inference thread then uses this information to dynamically load activated experts onto the GPU and offload inactive experts to main memory, maximizing GPU memory utilization. MoE-Infinity [51] takes a different approach toward expert offloading. The system leverages the observation that MoE models exhibit sparse activation and temporal locality during inference, meaning only a few experts are repeatedly activated for processing a specific sequence. MoE-Infinity traces expert activation at the sequence level, enabling it to predict which experts will be needed and prefetch them accordingly. MoE Efficiency. Fiddler [52] is a system designed to efficiently run these models on a limited number of GPUs, even when the model\u2019s size would typically exceed the GPU\u2019s memory capacity. Fiddler strategically distributes the model\u2019s components. Non-expert layers, which are used frequently, are kept on the GPU. A subset of expert layers, chosen based on how often they\u2019re used, are also placed on the GPU. The rest remain in the CPU\u2019s memory. Huang et al. [53] introduce three optimization techniques to address the MoE inference inefficiencies. (i) Dynamic gating allows the number of tokens processed by each expert to vary, which avoids the over-provisioning of resources in static gating and reduces computational waste, communication overhead, and memory\nconsumption. (ii) Expert buffering leverages the observation that expert activation is often sparse and exhibits temporal locality. By caching frequently used (hot) experts in GPU memory and buffering less active experts in CPU memory, expert buffering reduces the static memory allocation on GPU. (iii) Imbalanced token assignments to experts can lead to bottlenecks and performance degradation. Expert load balancing ensures a more even distribution of workload across devices.\nC. Miscellaneous Fields\nEthics and environmental sustainability. Sheng et. al [54] ensure fairness in serving LLMs by introducing a Virtual Token Counter (VTC). VTC defines LLM serving fairness based on a cost function that accounts for the number of input and output tokens processed. It achieves fairness by tracking the services received by each client and prioritizing those with the least service, while also considering the varying costs of processing input and output tokens. Sprout [55] addresses the environmental sustainability of LLMs and designs a framework to reduce the carbon footprint of LLM inference services. Sprout introduces \u201cgeneration directives\u201d to guide the autoregressive generation process, balancing the need for sustainability with the demand for high-quality generation. Inference pipeline optimization. FlashDecoding++ [56] conducts inference engine performance optimization, addressing several issues in softmax synchronization, GPU kernel, and dataflow. For example, the decoding phase performs linear GEMM operations with flat shapes where the batch size dimension involved in the multiplication is much smaller than the others. FlashDecoding++ accelerates flat GEMM with double buffering that overlaps computation and data transfer and hides the memory latency in loading input matrices. Parrot [57] is designed to optimize the performance of LLM-based applications that involve multiple LLM requests with complex workflows. Parrot performs data flow analysis and uncovers correlations across multiple LLM requests, and introduces a series of optimizations to improve performance. FlashAttention-3 [58] is a method to speed up attention for large language models and long-context applications. It introduces techniques like warp specialization and asynchronous block-wise operations to optimize GPU utilization. FlashAttention-3 achieves significant speedup on Hopper GPUs compared to its predecessor and reduces numerical errors in FP8 computations. Frugal inference. FrugalGPT [59] proposes several solutions to reduce the inference cost, such as prompt caching and LLM cascading which uses a sequence of LLMs, starting with cheaper ones and moving to more expensive ones only if necessary. SpecInfer [60] applies speculative decoding using smaller, speculative models to predict the LLM\u2019s output, reducing the computational resources. These predictions are organized into a tree structure, and their accuracy is verified in parallel against the LLM. RouteLLM [61] dynamically selects between a stronger and a weaker LLM during inference to optimize the balance between cost and response quality.\n# VII. CONCLUSION\nThis survey has presented a comprehensive overview of recent advancements in LLM serving systems, emphasizing the importance of system-level solutions for enhancing performance and efficiency. We have highlighted key innovations for deploying and scaling LLMs, paving the way for the future development of LLM serving systems.\n# ACKNOWLEDGMENTS\nThis material is based upon work supported by the Assistant Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001, and United States Air Force Research Laboratory Cooperative Agreement Number FA8750-19-2-1000. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Assistant Secretary of Defense for Research and Engineering, or the United States Air Force. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.\n# REFERENCES\n[1] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao, \u201cMedusa: Simple llm inference acceleration framework with multiple decoding heads,\u201d arXiv preprint arXiv:2401.10774, 2024. [2] Y. Fu, P. Bailis, I. Stoica, and H. Zhang, \u201cBreak the sequential dependency of llm inference using lookahead decoding,\u201d arXiv preprint arXiv:2402.02057, 2024. [3] M. Adnan, A. Arunkumar, G. Jain, P. Nair, I. Soloveychik, and P. Kamath, \u201cKeyformer: Kv cache reduction through key tokens selection for efficient generative inference,\u201d Proceedings of Machine Learning and Systems, vol. 6, pp. 114\u2013127, 2024. [4] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen, and Z. Jia, \u201cTowards efficient generative large language model serving: A survey from algorithms to systems,\u201d arXiv preprint arXiv:2312.15234, 2023. [5] Z. Yuan, Y. Shang, Y. Zhou, Z. Dong, C. Xue, B. Wu, Z. Li, Q. Gu, Y. J. Lee, Y. Yan et al., \u201cLlm inference unveiled: Survey and roofline model insights,\u201d arXiv preprint arXiv:2402.16363, 2024. [6] Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang, Z. Yuan, X. Li et al., \u201cA survey on efficient inference for large language models,\u201d arXiv preprint arXiv:2404.14294, 2024. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017. [8] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., \u201cImproving language understanding by generative pre-training,\u201d 2018. [9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, \u201cEfficiently scaling transformer inference,\u201d Proceedings of Machine Learning and Systems, vol. 5, pp. 606\u2013624, 2023. 10] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, \u201cEfficient memory management for large language model serving with pagedattention,\u201d in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611\u2013626. 11] \u201cText generation inference: A rust, python and grpc server for text generation inference.\u201d [Online]. Available: https://github.com/ huggingface/text-generation-inference 12] \u201cTensorrt-llm: A tensorrt toolbox for optimized large language model inference.\u201d [Online]. Available: https://github.com/NVIDIA/ TensorRT-LLM 13] R. Prabhu, A. Nayak, J. Mohan, R. Ramjee, and A. Panwar, \u201cvattention: Dynamic memory management for serving llms without pagedattention,\u201d arXiv preprint arXiv:2405.04437, 2024.\n[14] I. Gim, G. Chen, S.-s. Lee, N. Sarda, A. Khandelwal, and L. Zhong, \u201cPrompt cache: Modular attention reuse for low-latency inference,\u201d Proceedings of Machine Learning and Systems, vol. 6, pp. 325\u2013338, 2024. [15] B. Gao, Z. He, P. Sharma, Q. Kang, D. Jevdjic, J. Deng, X. Yang, Z. Yu, and P. Zuo, \u201cCost-Efficient large language model serving for multi-turn conversations with CachedAttention,\u201d in 2024 USENIX Annual Technical Conference (USENIX ATC 24). Santa Clara, CA: USENIX Association, Jul. 2024, pp. 111\u2013126. [Online]. Available: https://www.usenix.org/conference/atc24/presentation/gao-bin-cost [16] H. Liu, M. Zaharia, and P. Abbeel, \u201cRing attention with blockwise transformers for near-infinite context,\u201d arXiv preprint arXiv:2310.01889, 2023. [17] B. Lin, T. Peng, C. Zhang, M. Sun, L. Li, H. Zhao, W. Xiao, Q. Xu, X. Qiu, S. Li et al., \u201cInfinite-llm: Efficient llm service for long context with distattention and distributed kvcache,\u201d arXiv preprint arXiv:2401.02669, 2024. [18] C. Hu, H. Huang, J. Hu, J. Xu, X. Chen, T. Xie, C. Wang, S. Wang, Y. Bao, N. Sun et al., \u201cMemserve: Context caching for disaggregated llm serving with elastic memory pool,\u201d arXiv preprint arXiv:2406.17565, 2024. [19] W. Lee, J. Lee, J. Seo, and J. Sim, \u201cInfinigen: Efficient generative inference of large language models with dynamic kv cache management,\u201d arXiv preprint arXiv:2406.19707, 2024. [20] B. Wu, S. Liu, Y. Zhong, P. Sun, X. Liu, and X. Jin, \u201cLoongserve: Efficiently serving long-context large language models with elastic sequence parallelism,\u201d arXiv preprint arXiv:2404.09526, 2024. [21] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. R\u00b4e, I. Stoica, and C. Zhang, \u201cFlexgen: High-throughput generative inference of large language models with a single gpu,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 31 094\u201331 116. [22] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, and X. Hu, \u201cKivi: A tuning-free asymmetric 2bit quantization for kv cache,\u201d arXiv preprint arXiv:2402.02750, 2024. [23] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, \u201cGear: An efficient kv cache compression recipefor nearlossless generative inference of llm,\u201d arXiv preprint arXiv:2403.05527, 2024. [24] A. Liu, J. Liu, Z. Pan, Y. He, G. Haffari, and B. Zhuang, \u201cMinicache: Kv cache compression in depth dimension for large language models,\u201d arXiv preprint arXiv:2405.14366, 2024. [25] Z. Zheng, X. Ren, F. Xue, Y. Luo, X. Jiang, and Y. You, \u201cResponse length perception and sequence scheduling: An llm-empowered llm inference pipeline,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024. [26] Y. Jin, C.-F. Wu, D. Brooks, and G.-Y. Wei, \u201cS3: Increasing gpu utilization during generative inference for higher throughput,\u201d Advances in Neural Information Processing Systems, vol. 36, pp. 18 015\u201318 027, 2023. [27] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, \u201cOrca: A distributed serving system for {Transformer-Based} generative models,\u201d in 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), 2022, pp. 521\u2013538. [28] C. Holmes, M. Tanaka, M. Wyatt, A. A. Awan, J. Rasley, S. Rajbhandari, R. Y. Aminabadi, H. Qin, A. Bakhtiari, L. Kurilenko et al., \u201cDeepspeedfastgen: High-throughput text generation for llms via mii and deepspeedinference,\u201d arXiv preprint arXiv:2401.08671, 2024. [29] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, A. Tumanov, and R. Ramjee, \u201cTaming throughput-latency tradeoff in llm inference with sarathi-serve,\u201d arXiv preprint arXiv:2403.02310, 2024. [30] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang, S. Wang, Y. Bao et al., \u201cInference without interference: Disaggregate llm inference for mixed downstream workloads,\u201d arXiv preprint arXiv:2401.11181, 2024. [31] P. Patel, E. Choukse, C. Zhang, \u00b4I. Goiri, A. Shah, S. Maleki, and R. Bianchini, \u201cSplitwise: Efficient generative llm inference using phase splitting,\u201d arXiv preprint arXiv:2311.18677, 2023. [32] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, \u201cDistserve: Disaggregating prefill and decoding for goodput-optimized large language model serving,\u201d arXiv preprint arXiv:2401.09670, 2024. [33] Z. XUANLEI, B. Jia, H. Zhou, Z. Liu, S. Cheng, and Y. You, \u201cHetegen: Efficient heterogeneous parallel inference for large language models on resource-constrained devices,\u201d Proceedings of Machine Learning and Systems, vol. 6, pp. 162\u2013172, 2024.\n[34] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.-s. Chang, and J. Seo, \u201cExegpt: Constraint-aware resource scheduling for llm inference,\u201d in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, 2024, pp. 369\u2013384. [35] Y. Mei, Y. Zhuang, X. Miao, J. Yang, Z. Jia, and R. Vinayak, \u201cHelix: Distributed serving of large language models via max-flow on heterogeneous gpus,\u201d arXiv preprint arXiv:2406.01566, 2024. [36] X. Miao, C. Shi, J. Duan, X. Xi, D. Lin, B. Cui, and Z. Jia, \u201cSpotserve: Serving generative large language models on preemptible instances,\u201d in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, 2024, pp. 1112\u20131127. [37] Y. Fu, L. Xue, Y. Huang, A.-O. Brabete, D. Ustiugov, Y. Patel, and L. Mai, \u201cServerlessllm: Locality-enhanced serverless inference for large language models,\u201d arXiv preprint arXiv:2401.14351, 2024. [38] T. Griggs, X. Liu, J. Yu, D. Kim, W.-L. Chiang, A. Cheung, and I. Stoica, \u201cM\\\u2019elange: Cost efficient large language model serving by exploiting gpu heterogeneity,\u201d arXiv preprint arXiv:2404.14527, 2024. [39] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and W. Lin, \u201cLlumnix: Dynamic scheduling for large language model serving,\u201d arXiv preprint arXiv:2406.03243, 2024. [40] P. Patel, E. Choukse, C. Zhang, \u00b4I. Goiri, B. Warrier, N. Mahalingam, and R. Bianchini, \u201cCharacterizing power management opportunities for llms in the cloud,\u201d in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 2024, pp. 207\u2013222. [41] Z. Yang, Y. Yang, C. Zhao, Q. Guo, W. He, and W. Ji, \u201cPerllm: Personalized inference scheduling with edge-cloud collaboration for diverse llm services,\u201d arXiv preprint arXiv:2405.14636, 2024. [42] X. Miao, G. Oliaro, X. Cheng, M. Wu, C. Unger, and Z. Jia, \u201cFlexllm: A system for co-serving large language model inference and parameterefficient finetuning,\u201d arXiv preprint arXiv:2402.18789, 2024. [43] J. Liu, Z. Wu, J.-W. Chung, F. Lai, M. Lee, and M. Chowdhury, \u201cAndes: Defining and enhancing quality-of-experience in llm-based text streaming services,\u201d arXiv preprint arXiv:2404.16283, 2024. [44] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt\u00a8aschel et al., \u201cRetrievalaugmented generation for knowledge-intensive nlp tasks,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 9459\u20139474, 2020. [45] Y. Zhu, J.-C. Gu, C. Sikora, H. Ko, Y. Liu, C.-C. Lin, L. Shu, L. Luo, L. Meng, B. Liu et al., \u201cAccelerating inference of retrievalaugmented generation via sparse context selection,\u201d arXiv preprint arXiv:2405.16178, 2024. [46] C. Jin, Z. Zhang, X. Jiang, F. Liu, X. Liu, X. Liu, and X. Jin, \u201cRagcache: Efficient knowledge caching for retrieval-augmented generation,\u201d arXiv preprint arXiv:2404.12457, 2024. [47] J. Yao, H. Li, Y. Liu, S. Ray, Y. Cheng, Q. Zhang, K. Du, S. Lu, and J. Jiang, \u201cCacheblend: Fast large language model serving with cached knowledge fusion,\u201d arXiv preprint arXiv:2405.16444, 2024. [48] J. Li, Y. Jiang, Y. Zhu, C. Wang, and H. Xu, \u201cAccelerating distributed {MoE} training and inference with lina,\u201d in 2023 USENIX Annual Technical Conference (USENIX ATC 23), 2023, pp. 945\u2013959. [49] J. Yao, Q. Anthony, A. Shafi, H. Subramoni, and D. K. D. Panda, \u201cExploiting inter-layer expert affinity for accelerating mixture-of-experts model inference,\u201d in 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 2024, pp. 915\u2013925. [50] Z. Du, S. Li, Y. Wu, X. Jiang, J. Sun, Q. Zheng, Y. Wu, A. Li, H. Li, and Y. Chen, \u201cSida: Sparsity-inspired data-aware serving for efficient and scalable large mixture-of-experts models,\u201d Proceedings of Machine Learning and Systems, vol. 6, pp. 224\u2013238, 2024. [51] L. Xue, Y. Fu, Z. Lu, L. Mai, and M. Marina, \u201cMoe-infinity: Activationaware expert offloading for efficient moe serving,\u201d arXiv preprint arXiv:2401.14361, 2024. [52] K. Kamahori, Y. Gu, K. Zhu, and B. Kasikci, \u201cFiddler: Cpu-gpu orchestration for fast inference of mixture-of-experts models,\u201d arXiv preprint arXiv:2402.07033, 2024. [53] H. Huang, N. Ardalani, A. Sun, L. Ke, H.-H. S. Lee, A. Sridhar, S. Bhosale, C.-J. Wu, and B. Lee, \u201cTowards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference,\u201d arXiv preprint arXiv:2303.06182, 2023. [54] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, D. Zhuo, J. E. Gonzalez, and I. Stoica, \u201cFairness in serving large language models,\u201d arXiv preprint arXiv:2401.00588, 2023.\n[55] B. Li, Y. Jiang, V. Gadepally, and D. Tiwari, \u201cToward sustainable genai using generation directives for carbon-friendly large language model inference,\u201d arXiv preprint arXiv:2403.12900, 2024. [56] K. Hong, G. Dai, J. Xu, Q. Mao, X. Li, J. Liu, Y. Dong, Y. Wang et al., \u201cFlashdecoding++: Faster large language model inference with asynchronization, flat gemm optimization, and heuristics,\u201d Proceedings of Machine Learning and Systems, vol. 6, pp. 148\u2013161, 2024. [57] C. Lin, Z. Han, C. Zhang, Y. Yang, F. Yang, C. Chen, and L. Qiu, \u201cParrot: Efficient serving of llm-based applications with semantic variable,\u201d arXiv preprint arXiv:2405.19888, 2024. [58] J. Shah, G. Bikshandi, Y. Zhang, V. Thakkar, P. Ramani, and T. Dao, \u201cFlashattention-3: Fast and accurate attention with asynchrony and lowprecision,\u201d arXiv preprint arXiv:2407.08608, 2024. [59] L. Chen, M. Zaharia, and J. Zou, \u201cFrugalgpt: How to use large language models while reducing cost and improving performance,\u201d arXiv preprint arXiv:2305.05176, 2023. [60] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang, R. Y. Y. Wong, A. Zhu, L. Yang, X. Shi et al., \u201cSpecinfer: Accelerating large language model serving with tree-based speculative inference and verification,\u201d in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 2024, pp. 932\u2013949. [61] I. Ong, A. Almahairi, V. Wu, W.-L. Chiang, T. Wu, J. E. Gonzalez, M. W. Kadous, and I. Stoica, \u201cRoutellm: Learning to route llms with preference data,\u201d arXiv preprint arXiv:2406.18665, 2024.\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to provide a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on system-level enhancements that improve performance and efficiency without altering core LLM decoding mechanisms.",
            "scope": "The survey covers system-level solutions for LLM serving published between January 2023 and June 2024, excluding studies that modify LLM decoding algorithms. It emphasizes innovations in memory management, computation optimization, cloud deployment, and emerging research fields."
        },
        "problem": {
            "definition": "The core issue explored is the challenge of deploying and scaling LLMs in production environments, particularly regarding their substantial computational and memory demands.",
            "key obstacle": "Researchers face significant challenges in optimizing resource utilization, managing memory efficiently, and ensuring low-latency performance for user experience."
        },
        "architecture": {
            "perspective": "The survey categorizes existing research into four distinct categories: KV cache and memory management, LLM computation optimization, cloud LLM deployment, and emerging research fields such as retrieval-augmented generation and mixture-of-experts inference.",
            "fields": [
                {
                    "name": "KV Cache and Memory Management",
                    "explanation": "Focuses on efficient memory management strategies for handling dynamic growth of KV caches."
                },
                {
                    "name": "LLM Computation Optimization",
                    "explanation": "Explores methods to optimize LLM computation through request batching and model parallelism."
                },
                {
                    "name": "Cloud LLM Deployment",
                    "explanation": "Addresses challenges and techniques for deploying LLMs in cloud environments."
                },
                {
                    "name": "Emerging Research Fields",
                    "explanation": "Investigates new areas such as retrieval-augmented generation and mixture-of-experts inference."
                }
            ]
        },
        "conclusion": {
            "comparisions": "The survey compares various LLM serving methods based on their effectiveness in optimizing performance, resource utilization, and scalability.",
            "results": "The key takeaways highlight the importance of system-level solutions for enhancing LLM serving capabilities, paving the way for future developments in this area."
        },
        "discussion": {
            "advantage": "Existing research has achieved significant advancements in memory management, computation optimization, and cloud deployment strategies for LLMs.",
            "limitation": "Current studies often fall short in addressing the complexities of resource allocation and the impact of model size on inference performance.",
            "gaps": "There are unanswered questions regarding the long-term sustainability and ethical implications of LLM deployments.",
            "future work": "Future research should focus on optimizing resource utilization in cloud environments, addressing ethical concerns, and exploring novel architectures for LLM serving."
        },
        "other info": {
            "acknowledgments": "This material is based upon work supported by the Assistant Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001.",
            "references_count": 61
        }
    },
    "mount_outline": [
        {
            "section number": "2.3",
            "key information": "The survey categorizes existing research into four distinct categories: KV cache and memory management, LLM computation optimization, cloud LLM deployment, and emerging research fields such as retrieval-augmented generation and mixture-of-experts inference."
        },
        {
            "section number": "4.1",
            "key information": "The core issue explored is the challenge of deploying and scaling LLMs in production environments, particularly regarding their substantial computational and memory demands."
        },
        {
            "section number": "8.1",
            "key information": "Addresses challenges and techniques for deploying LLMs in cloud environments."
        },
        {
            "section number": "10.3",
            "key information": "There are unanswered questions regarding the long-term sustainability and ethical implications of LLM deployments."
        },
        {
            "section number": "1.2",
            "key information": "The survey aims to provide a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on system-level enhancements that improve performance and efficiency without altering core LLM decoding mechanisms."
        }
    ],
    "similarity_score": 0.756277785133913,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLM Inference Serving_ Survey of Recent Advances and Opportunities.json"
}