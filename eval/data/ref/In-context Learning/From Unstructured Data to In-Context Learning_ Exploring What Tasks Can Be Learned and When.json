{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.00131",
    "title": "From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When",
    "abstract": "Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to make predictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities can emerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.",
    "bib_name": "wibisono2024unstructureddataincontextlearning",
    "md_text": "# How In-Context Learning Emerges from Training on Unstructured Data: On the Role of Co-Occurrence, Positional Information, and Noise Structures\nKevin Christian Wibisono University of Michigan, Statistics kwib@umich.edu Yixin Wang University of Michigan, Statistics yixinw@umich.edu\n# June 5, 2024\nAbstract\nLarge language models (LLMs) like transformers have impressive in-context learning (ICL) capabilities; they can generate predictions for new queries based on input-output sequences in prompts without parameter updates. While many theories have attempted to explain ICL, they often focus on structured training data similar to ICL tasks, such as regression. In practice, however, these models are trained in an unsupervised manner on unstructured text data, which bears little resemblance to ICL tasks. To this end, we investigate how ICL emerges from unsupervised training on unstructured data. The key observation is that ICL can arise simply by modeling co-occurrence information using classical language models like continuous bag of words (CBOW), which we theoretically prove and empirically validate. Furthermore, we establish the necessity of positional information and noise structure to generalize ICL to unseen data. Finally, we present instances where ICL fails and provide theoretical explanations; they suggest that the ICL ability of LLMs to identify certain tasks can be sensitive to the structure of the training data.1\nKeywords: in-context learning, language models, continuous bag of words, co-occurrence, positional embeddings, transformers\n# 1 Introduction\nLarge language models (LLMs) such as transformers demonstrate impressive in-context learnin (ICL) abilities [Brown et al., 2020]: without updating parameters, they can identify tasks an generate predictions from prompts containing input-output examples. For example, given th\n1Software that replicates the empirical studies can be found at https://github.com/yixinw-lab icl-unstructured.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bdce/bdcee6e1-1ab7-4b3c-8a23-6505a46f0f5f.png\" style=\"width: 50%;\"></div>\nFigure 1: This paper aims to understand how in-context learning (ICL) emerges from pretraining on unstructured natural language data. In Section 2, we show that ICL can arise merely through modeling co-occurrence information using continuous bag of words (CBOW). Violet represents relationship-specific noise tokens. In Section 3, we establish the necessity of positional information and blocked noise structures for certain ICL tasks. Violet represents noise tokens. In Section 4, we present two scenarios where ICL unexpectedly fails and provide theoretical explanations, highlighting the importance of training data structure in enabling the ICL ability of language models. prompt dog anjing, cat kucing, lion singa, elephant, a well-trained LLM should detect the Englishto-Indonesian pattern and predict gajah, the Indonesian translation for elephant, as the most likely next token. The emergence of ICL is surprising for at least two reasons. First, LLMs are trained from unstructured natural language data in an unsupervised manner through next-token prediction. Second, the training data of LLMs likely does not include sentences that resemble typical ICL prompts, i.e., of the form c1d1 \u00b7 \u00b7 \u00b7 cKdK, where (ck, dk) represents a known input-output pair. Many efforts have sought to understand ICL from various theoretical and empirical perspectives; see related work in Section 5. Some studies (e.g., Ahn et al. [2024], Aky\u00fcrek et al. [2022], Dai et al. [2023], Von Oswald et al. [2023], Zhang et al. [2024]) expanded Garg et al.\u2019s [2022] regression formulation and attributed transformers\u2019 ICL ability to gradient descent. Other studies (e.g., Wang et al. [2023], Zhang et al. [2023], Chiang and Yogatama [2024]) adopted a Bayesian perspective, building upon Xie et al.\u2019s [2021] argument that ICL performs implicit Bayesian inference. While these connections are theoretically intriguing, they do not fully capture the actual ICL phenomenon: ICL arises from training on unstructured natural language data that are distinct from ICL prompts. This work. We study how ICL emerges from pretraining on unstructured natural language data. Throughout the paper, we focus on two types of ICL tasks. The first type involves known inputoutput pairings that frequently occur together in a sentence, such as (country)-(capital) and (English word)-(Indonesian translation). The second type involves recognizable patterns that may not commonly co-occur in a sentence, such as (word)-(first letter). For the first task (see left of Figure 1), we examine cases where the training sentences contain one or two distinct input-output relationship types. We also consider more realistic scenarios where some input-output pairs do not always occur together and two types of relationships can co-occur in a single sentence. We prove that, in most cases, ICL is achievable by only modeling co-occurrence\ninformation using continuous bag of words (CBOW) [Mikolov et al., 2013], a language model from the pre-transformer era. Further, we conduct prompting and synthetic data experiments that support the conclusion that ICL may indeed arise from co-occurrence information. For the second task (see middle of Figure 1), we investigate cases where the training sentences contain one or two distinct patterns, as well as a more realistic scenario where noise tokens are present. We prove that positional information and blocked noise structures (e.g., pqrs in Figure 1) are crucial for the success of ICL. This observation aligns with Chen et al.\u2019s [2024b] empirical finding that parallel structures in pretraining data support ICL. Moreover, we find that learned positional embeddings generally perform better, except in noisy scenarios where the noises are not clustered in blocks. Finally, we present two scenarios where ICL unexpectedly fails regardless of model architectures; see right of Figure 1. In the first scenario (left example), both the training data and test prompts follow repeating patterns across blocks, but the pattern being repeated in the test data differs from that in the training data. In the second scenario (right example), training sentences contain known input-output pairs but only at fixed locations. These findings, along with their empirical and theoretical explanations, underscore that LLMs may require specific structures in the pretraining data to exhibit ICL ability. Summary of contributions. In this paper, we (1) theoretically and empirically show that ICL can arise from merely modeling co-occurrence patterns using CBOW, (2) prove that, in other instances, ICL requires modeling positional information and blocked noise structures, and (3) present scenarios where ICL fails, highlighting the crucial role of training data structure for the emergence of ICL.\ninformation using continuous bag of words (CBOW) [Mikolov et al., 2013], a language model from the pre-transformer era. Further, we conduct prompting and synthetic data experiments that support the conclusion that ICL may indeed arise from co-occurrence information.\n# 2 In-context learning can arise by modeling co-occurrence\n# 2 In-context learning can arise by modeling co-occurren via CBOW\nIn this section, we focus on in-context learning (ICL) tasks involving pairings that commonly co-occur within training sentences. To motivate the discussion, we revisit the (English word)(Indonesian translation) example in Section 1. Below we perform a simple experiment with ChatGPT 3.5 [OpenAI, 2022]. The model is given prompts of the following form:\nProvide the most plausible next token to complete this sentence (only the answer). Even if the sentence does not make sense, please complete it as best as you can: dog anjing, cat kucing, lion singa\nWe take turns replacing [word] with elephant, tiger, soon, and main. For the first two options, ChatGPT 3.5 correctly outputs gajah and harimau, their respective Indonesian translations. However, it does not provide the correct outputs for the latter two: it follows soon with lebih baik beri makanan haiwan! (better feed the animals!) and main with bola (ball).2 A similar pattern is observed with LLaMA 2 [Touvron et al., 2023], which produces the correct translations the first two words but incorrectly continues the last two words with to-be-published and an1, respectively.\n2In Indonesian, main means play, main bola means play soccer, and mainan means toy.\nIf ICL stems from the ability of LLMs to recognize consistent mappings in test prompts, these models should be equally likely to produce the correct answer for any given [word], irrespective of its relevance to the in-context examples. However, this experiment demonstrates that this is not the case; in Section 2.4, we also present two experiments involving countries, US states, and their capital cities. This naturally raises the question: Does/can ICL arise from modeling co-occurrence information using a simple model like continuous bag of words (CBOW) [Mikolov et al., 2013]? ICL via CBOW. We prove that, for certain tasks, ICL is achievable by modeling co-occurrence information between pairs of tokens (regardless of their positions) using CBOW. We utilize a variant of CBOW where each center word is modeled conditional on all other words in a sentence. We associate each word w with their center and context embeddings uw and vw of the same dimension. Given a sentence x1x2 \u00b7 \u00b7 \u00b7 xI, the i-th word is distributed conditional on the other words in the sentence as follows:\nRoadmap of Section 2. In Section 2.1, we begin by considering a simple ICL task of the form ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1, where (ci, di) represents a known pairing (e.g., a country and its capital city) and i1, i2, \u00b7 \u00b7 \u00b7 , i\u2113+1 are all distinct. The focus is to investigate whether a trained CBOW model can correctly output di\u2113. We also explore two other scenarios: ICL tasks of the form ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1 and ci1ei1 \u00b7 \u00b7 \u00b7 ci\u2113ei\u2113ci\u2113+1 in Section 2.2 (two connected relationships), as well as ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113\u22121di\u2113\u22121ci\u2113and ei1fi1 \u00b7 \u00b7 \u00b7 ei\u2113fi\u2113ei\u2113+1 (two disconnected relationships) in Section 2.3. Sections 2.4 and 2.5 conclude with prompting and synthetic data experiments that provide support to the theory.\n# 2.1 In-context learning on single-relationship tasks\nWe investigate ICL in single-relationship tasks that take the form of ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1, where (ci, di)\u2019s denote known pairings such as countries and their capital cities; there is only one type of relationship between ci and di. The vocabulary consists of c1:K, d1:K, r1:L, where r\u2032 is represent other words (e.g., stop words). We first introduce Theorem 1, which states that ICL can arise if each sentence consists of exactly one (ci, di) pair, as long as the number of in-context examples (\u2113) is not too large. To simplify calculations, we replace the cross-entropy loss with the squared loss. This involves removing the softmax activation and comparing the outputs against the one-hot encoding of the target words. The proof of Theorem 1 is in Appendix A. Theorem 1 (ICL on single-relationship tasks). Let K, L \u2265S \u22653. Suppose we have infinitely many training sentences of length S, each comprising exactly one (ci, di) pair and S \u22122 distinct ri\u2019s. We train a CBOW model with the squared loss and a sufficiently large embedding dimension on these sentences. Given a prompt ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1 with distinct ik\u2019s, the model predicts di\u2113+1 if and only if\n<  \u2212 (K + L)(S \u22122)2(S \u22121) + K(S \u22122)(S \u22121)2 \u22122(S \u22122)4.\n<div style=\"text-align: center;\">Table 1: ICL on various single-relationship tasks, averaged over 10 repetitions, demonstrates stable, good performance across embedding dimensions (dE), as Theorem 1 suggests. The corrupted setting also demonstrates excellent ICL ability under certain scenarios.</div>\nClean\nCorrupted\n(p0, p1, p2)\ndE = 10\ndE = 100\ndE = 10\ndE = 100\n(0, 1, 0)\n0\n0\n0\n0\n(0, 0, 1)\n0\n0\n0\n0\n(1/2, 1/2, 0)\n1\n0.99\n0\n0\n(1/2, 0, 1/2)\n1\n1\n1\n1\n(0, 1/2, 1/2)\n1\n1\n0\n0.01\n(1/3, 1/3, 1/3)\n1\n1\n1\n1\nAs an example, when each training sentence contains exactly one country-capital pair (i.e., (ci, di)), Theorem 1 says a trained CBOW model will correctly predict di\u2113+1 (i.e., the capital city of ci\u2113+1) given an ICL prompt of the form ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1, provided that the prompt length (2\u2113+ 1) is not too large. Intuitively, this behavior is due to the presence of ci\u2113+1 in the ICL prompt, which leads the model to correctly predict di\u2113+1 due to the frequent occurrences of the pair (ci\u2113+1, di\u2113+1) in the training data. If we let L \u2192\u221eand fix K and S, the condition in Theorem 1 becomes 2\u2113+ 1 < K(S \u22121)2/(S \u22122)2. This inequality trivially holds if the prompt length is set to be S \u22121 to match the training sentences. Moreover, the same analysis applies when each sentence consists of exactly two (instead of one) different (ci, di) pairs. Again, letting L \u2192\u221eand fixing K and S, the model correctly predicts di\u2113+1 given the same ICL prompt if and only if 2\u2113+ 1 < K(K\u22122)(S\u22121)2 (K\u22122)(S\u22122)(S\u22124)\u2212K. This upper bound is strictly larger than K(S \u22121)2/(S \u22122)2: when each sentence contains exactly two (ci, di) pairs, ICL under the squared loss occurs for longer prompts. Experiments. To empirically verify Theorem 1 and its generalizations, we conduct experiments using the cross-entropy loss with S = 8, K = 10, L = 20, and \u2113= 3. We explore multiple (p0, p1, p2) values, where pk denotes the probability of having exactly k pairs of (ci, di) in the sentence. For each (p0, p1, p2) triple, we introduce a more realistic setting where ci and di do not always appear together by considering its corrupted version. In this setup, each (ci, di) pair has a 25% chance of being replaced with (ci, rj) and a 25% chance of being replaced with (di, rj), for some j \u2208[L]. Table 1 displays the average accuracy for each scenario, calculated over 10 repetitions. Notably, when (p0, p1, p2) is (0, 1, 0) or (0, 0, 1), ICL under the cross-entropy loss achieves zero accuracy, in contrast to perfect accuracy with the squared loss as shown in Theorem 1. We believe this difference in accuracy is an artifact of the loss functions used, although its relevance is limited by the fact that, in reality, it is unlikely for every sentence to contain at least one (ci, di) pair. On the other hand, perfect ICL performance is observed in other settings (e.g., when the training sentences contain either zero, one, or two (ci, di) pairs) in both the clean and corrupted scenarios.\nTable 2: ICL on dual-connected-relationship tasks, averaged over 10 repetitions, achieves perfect accuracy when (p0, p1, p2) \u2208{(1/2, 0/1, 2), (0, 1/2, 1/2), (1/3, 1/3, 1/3)} regardless of architectures and embedding dimensions (dE), as Theorem 2 suggests. When (p0, p1, p2) = (1/2, 1/2, 0), ICL performs better under imbalanced or extreme scenarios and with larger dE.\nBalanced\nImbalanced\nExtreme\n(p0, p1, p2)\ndE = 10\ndE = 100\ndE = 10\ndE = 100\ndE = 10\ndE = 100\n(0, 1, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0, 1)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0.07, 0.10)\n(0, 0)\n(1/2, 1/2, 0)\n(0.53, 0.47)\n(0.51, 0.50)\n(0.69, 0.68)\n(1, 1)\n(0.94, 0.93)\n(1, 1)\n(1/2, 0, 1/2)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(0, 1/2, 1/2)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1/3, 1/3, 1/3)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n# 2.2 In-context learning on dual-connected-relationship tasks\nIn Section 2.1, we discussed the case where the training sentences contain one relationship, namely (ci, di)\u2019s. We now explore ICL on dual-connected-relationship tasks, where the two types of relationships are connected and denoted by (ci, di) and (ci, ei): ci might represent a country, di its capital city, and ei its currency. The vocabulary comprises c1:K, d1:K, e1:K, r1:L, where ri\u2019s represent other words. The corresponding ICL tasks thus take the form ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1 and ci1ei1 \u00b7 \u00b7 \u00b7 ci\u2113ei\u2113ci\u2113+1, where the model is expected to output di\u2113+1 and ei\u2113+1. This involves task selection as the model should use the in-context examples to infer the task. We first present Theorem 2, which states that a trained CBOW model can perform task selection if each sentence contains exactly two distinct (ci, di) pairs or two distinct (ci, ei) pairs with uniform probability. Its proof is in Appendix B. Theorem 2 (Task selection in CBOW). Let K, L \u22652 and S \u22655. Suppose we have infinitely many training sentences of length S, each comprising exactly two distinct (ci, di) pairs or (ci, ei) pairs with uniform probability, and S \u22124 distinct ri\u2019s. We train a CBOW model with the squared loss and a large enough embedding dimension. Given a prompt ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1 (ci1ei1 \u00b7 \u00b7 \u00b7 ci\u2113ei\u2113ci\u2113+1) with distinct ik\u2019s, the model is more likely to predict di\u2113+1 (ei\u2113+1) than ei\u2113+1 (di\u2113+1). Theorem 2 says that, when each training sentence includes two (ci, di) pairs or two (ci, ei) pairs, a trained CBOW model is capable of task selection. To understand this result, consider the ICL prompt of the first type, i.e., ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1. Here, the output is more likely to be di\u2113+1 than ei\u2113+1 since di\u2113+1 co-occurs with the other dij\u2019s in the training data (and ei\u2113+1 does not). In Theorem 2, we require that each sentence contains either two distinct (ci, di) pairs or two distinct (ci, ei) pairs for ICL to emerge. However, this may not be necessary as we empirically show next. Experiments. We use the cross-entropy loss with S = 8, K = 10, L = 60, and \u2113= 3. Each training sentence is equally likely to be a cd sentence (i.e., containing (ci, di) pairs) or a ce sentence (i.e., containing (ci, ei) pairs), but not both. We explore multiple (p0, p1, p2)\u2019s, where pk is the probability of having exactly k pairs of (ci, di) for a cd sentence, or k pairs of (ci, ei) for a ce sentence.\nTable 3: ICL on dual-disconnected-relationship tasks, averaged over 10 repetitions, achieves perfect accuracy when (p0, p1, p2) \u2208{(1/2, 0/1, 2), (0, 1/2, 1/2), (1/3, 1/3, 1/3)} regardless of architectures and embedding dimensions (dE). When (p0, p1, p2) = (1/2, 1/2, 0), ICL already performs well under the balanced scenario.\nBalanced\nImbalanced\nExtreme\n(p0, p1, p2)\ndE = 10\ndE = 100\ndE = 10\ndE = 100\ndE = 10\ndE = 100\n(0, 1, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0)\n(0, 0, 1)\n(0, 0)\n(0, 0)\n(0.16, 0.14)\n(0, 0)\n(0.21, 0.29)\n(0, 0)\n(1/2, 1/2, 0)\n(1, 1)\n(0.82, 0.83)\n(0.28, 0.27)\n(0.95, 0.95)\n(0.83, 0.85)\n(0.91, 0.91)\n(1/2, 0, 1/2)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(0, 1/2, 1/2)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1/3, 1/3, 1/3)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\n(1, 1)\nAdditionally, we introduce three different scenarios: balanced, where all L random words are equally likely to occur in both cd and ce sentences; imbalanced, where L/3 words are more likely to occur in cd (ce) sentences; and extreme, where L/3 of the words can only occur in cd (ce) sentences. Table 2 shows the accuracies of both tasks for each scenario, averaged over 10 repetitions. We observe a perfect accuracy when (p0, p1, p2) \u2208{(1/2, 0/1, 2), (0, 1/2, 1/2), (1/3, 1/3, 1/3)} across all embedding dimensions and scenario types. The near-zero accuracy when (p0, p1, p2) or (0, 1, 0) or (0, 0, 1) is again an artifact of the cross-entropy loss, as discussed in Section 2.1. Interestingly, ICL works in the imbalanced and extreme scenarios when (p0, p1, p2) = (1/2, 1/2, 0), where sentences do not contain more than one (ci, di) or (ci, ei) pair. To see this, consider the balanced scenario where each ri is equally probable to appear in both types of sentences. Given a prompt of the form ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1, it is easy to see that the model should output di\u2113+1 or ei\u2113+1 with equal probability. On the other hand, in the imbalanced and extreme scenarios, the signals from the ri\u2019s can allow for task selection, thus contributing to the success of ICL.\n# 2.3 In-context learning on dual-disconnected-relationship tasks\nWe next replicate the experiments in Section 2.2, but with two disconnected relationships (ci, di) and (ei, fi). For example, (ci, di) might represent a country and its capital city and (ei, fi) might represent a company and its CEO. The vocabulary consists of c1:K, d1:K, e1:K, f1:K, r1:L, where ri\u2019s represent other words. Table 3 summarizes the accuracies of the ICL tasks ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1 and ei1fi1 \u00b7 \u00b7 \u00b7 ei\u2113fi\u2113ei\u2113+1 for each scenario, averaged over 10 repetitions. Similar to the connected setting in Section 2.2, we observe a perfect accuracy when (p0, p1, p2) is one of (1/2, 0/1, 2), (0, 1/2, 1/2), or (1/3, 1/3, 1/3) across all embedding dimensions and scenario types. However, when (p0, p1, p2) = (1/2, 1/2, 0), ICL already works well in the balanced scenario. This is because the two relationships are disjoint, thus making task selection easier. In addition, we consider a contaminated version of the training data where cd (ef ) sentences can contain some ei\u2019s and fi\u2019s (ci\u2019s and di\u2019s). We also obtain a perfect accuracy when (p0, p1, p2) \u2208  across all embedding dimensions and scenario types.\nIn addition, we consider a contaminated version of the training data where cd (ef ) sentences can contain some ei\u2019s and fi\u2019s (ci\u2019s and di\u2019s). We also obtain a perfect accuracy when (p0, p1, p2) \u2208 {(1/2, 0/1, 2), (0, 1/2, 1/2), (1/3, 1/3, 1/3)} across all embedding dimensions and scenario types.\nWe perform two experiments involving countries and their capital cities, as well as US states and their capital cities. The prompts follow the format c1d1, c2d2, \u00b7 \u00b7 \u00b7 , c6d6, c7, where ci is a country or US state and di is its capital city. Using the LLaMA 2 model [Touvron et al., 2023], we compare the prediction for each prompt with its corresponding d7. The experimental results support the theory. In the first experiment, we focus on 160 countries with a population exceeding one million in 2022. Among these countries, 31 have capital cities that are not their most populous cities, denoted by type A. The remaining 129 countries fall under type B. Each ICL prompt includes three type A countries among c1, \u00b7 \u00b7 \u00b7 , c6 to emphasize that the desired relationship is (country)-(capital) rather than (country)-(largest city). Subsequently, we randomly generate 1,000 prompts, with 500 having a c7 representing a type A country and 500 having a c7 representing a type B country. The ICL accuracies corresponding to type A and type B prompts are 0.58 and 0.96, respectively. In the second experiment, we consider all 50 states, among which 33 are of type A and 17 are of type B, defined similarly. The ICL accuracies corresponding to type A and type B prompts are found to be 0.69 and 0.84, respectively. From both experiments, we notice that LLaMA 2 performs better on type B prompts (i.e., the capital city as the largest city). This suggests that ICL may arise from co-occurrence information, as larger cities tend to appear more frequently compared to smaller ones.\n# 2.5 Experiments on a synthetic corpus\nWe conduct experiments on a synthetic corpus consisting of (country)-(capital) and (country)-(IOC code) relationships. Each sentence in the corpus is categorized into exactly one of six possible categories: (1) exactly one country-capital pair; (2) exactly two country-capital pairs; (3) exactly one country-IOC pair; (4) exactly two country-IOC pairs; (5) exactly one country without any pair; and (6) no country. In sentences with country-capital pairs, each capital city can appear in any position relative to the country. Conversely, in sentences with country-IOC pairs, each IOC code must directly follow the country. The corpus generation process is as follows: 1. Randomly select 10 countries and obtain their capital cities and IOC codes. 2. Generate 30 sentences containing exactly one country-capital pair (3 for each country). Example: Paramaribo is the vibrant heart of Suriname. 3. Generate 30 sentences containing exactly one country-IOC pair (3 for each country). Example: Gabon (GAB) protects its diverse rainforests and wildlife. 4. Generate 30 sentences containing exactly one country without any pair. Example: The banking sector is central to Liechtenstein\u2019s prosperity. 5. Generate 60 sentences without any country, capital city, or IOC code. Example: Every country has its unique cultural identity and heritage.\n3. Generate 30 sentences containing exactly one country-IOC pair (3 for each country). Example: Gabon (GAB) protects its diverse rainforests and wildlife.\n6. Generate 810 sentences containing exactly two different country-capital pairs by concatenating sentences generated in Step 2. Example: The city of Dushanbe reflects Tajikistan\u2019s vibrant spirit. Roseau is the cultural tapestry of Dominica. 7. Generate 810 sentences containing exactly two different country-IOC pairs by concatenating sentences generated in Step 3. Example: Mayotte (MAY) features lush landscapes and peaks. Turkmenistan (TKM) features the fiery Darvaza Crater.\n7. Generate 810 sentences containing exactly two different country-IOC pairs by concatenating sentences generated in Step 3. Example: Mayotte (MAY) features lush landscapes and peaks. Turkmenistan (TKM) features the fiery Darvaza Crater.\nTwo models are trained on this corpus: a CBOW and a five-layer two-head autoregressive transformer. Both models have an embedding dimension of 100. We then compare the ICL accuracies for both relationships given one to five in-context examples. For the CBOW model, the country-capital accuracies are (0.77, 0.66, 0.67, 0.65, 0.63) and the country-IOC accuracies are (0.08, 0.17, 0.16, 0.23, 0.34). Here, the i-th number corresponds to the accuracy given i in-context examples. For the transformer, the accuracies are (0.05, 0.26, 0.04, 0.02, 0.09) and (0.57, 0.38, 0.91, 0.85, 0.83), respectively. When using the transformer, we find that the accuracies for the country-IOC task are significantly higher compared to those for the country-capital task. This is likely because each IOC code consistently follows the corresponding country in the corpus, similar to ICL prompts. On the other hand, ICL fails to work on the country-capital task, where there is no consistent pattern in how each pair occurs in the corpus. Meanwhile, ICL works decently well on both tasks under the CBOW model.\n# 3 The essential role of positional information in enabling in-context learning\nIn this section, we examine another common example of in-context learning (ICL), where the task involves predicting the first (or second) token given a sequence of tokens. To understand the significance of positional information (unlike the tasks in Section 2), we consider a simpler task: modeling sequences of tokens in the form xi1xi2xi3xi1. Theorem 3 underscores the necessity of incorporating positional information to correctly predict xi1 from xi1xi2xi3 in a single-layer model, and provides a construction of a basic attention-based model capable of achieving zero loss and perfect accuracy on this task. Its proof is in Appendix C. Theorem 3 (Necessity of modeling positions). Let the vocabulary be V = {1, 2, \u00b7 \u00b7 \u00b7 , |V |} and the training sequences take the form xi1xi2xi3xi1, where xi1 \u0338= xi2 \u0338= xi3 \u0338= xi1 are chosen uniformly at random from V. Consider a one-layer model that predicts the last xi1 via a learned function f({xi1, xi2}, xi3) using the cross-entropy loss. In this case, it is not possible to achieve pefect accuracy or zero loss. On the other hand, we can achieve zero loss (and thus perfect accuracy) by incorporating positional information, i.e., via a learned function \u02dcf({(xi1, 1), (xi2, 2)}, (xi3, 3)). Here, f({xi1, xi2}, xi3) represents a scenario where the model lacks positional information (e.g., f is a one-layer autoregressive transformer without positional embeddings). In this scenario, the output of this function is identical for inputs xi1xi2xi3 and xi2xi1xi3, which leads to the\n<div style=\"text-align: center;\">Table 4: Prediction accuracy with single/multi-layer models. For successful ICL, it is crucial that the first token of sentences in the training set covers the entire vocabulary (Both). In this case, positional embeddings are essential, especially when using a one-layer model.</div>\nTable 4: Prediction accuracy with single/multi-layer models. For successful ICL, it is crucial that the first token of sentences in the training set covers the entire vocabulary (Both). In this case, positional embeddings are essential, especially when using a one-layer model.\nBoth\nEither\nPos. emb.\n1-layer\n5-layer\n1-layer\n5-layer\nLearned\n1\n1\n0\n0\nSinusoidal\n1\n1\n0\n0\nNo pos. emb.\n0.30\n0.89\n0\n0\nimpossibility of attaining zero loss. In contrast, \u02dcf({(xi1, 1), (xi2, 2)}, (xi3, 3)) refers to a scenario where the model has access to positional information. We provide a construction of \u02dcf that achieves zero loss in Appendix C.\nExperiments. We validate Theorem 3 by training transformers to autoregressively learn sequences of the form xi1xi2xi3xi1, and assessing their accuracy in predicting the last token on a separate test data of the same pattern. We use |V | = 20 and an embedding dimension of 10. We consider these settings: (i) number of layers: 1, 5; (ii) positional embeddings: learned, sinusoidal, no positional embeddings; and (iii) train-test split: each token in the vocabulary is the first token in both the training and test sets (Both), each token in the vocabulary is the first token in either set, but not both (Either). Table 4 summarizes the results. Two main findings emerge: (1) for the model to successfully generalize to unseen sentences, each token in V should be present as the first token in both the training and test sets; (2) positional embeddings are crucial when using only one attention layer. Multiple layers. With multi-layer models, positional information can be encoded without explicit positional embeddings. This is summarized in Proposition 4, whose proof is in Appendix D. Proposition 4 (Multi-layer models can encode positions). Consider the sentence xi1xi2xi3xi1. Using a two-layer autoregressive model, the model\u2019s final output for predicting the last xi1 is given by t(xi1xi2xi3) := g3 ({f1({xi1}), f2({xi1}, xi2)}, f3({xi1, xi2}, xi3)) for some f1, f2, f3, and g3. Proposition 4 shows that we generally have t(xi1xi2xi3) \u0338= t(xi2xi1xi3), unlike in the one-layer case. Consequently, high accuracy is achievable without positional embeddings, as shown in Table 4. Roadmap of Section 3. In the rest of this section, we consider settings where each sentence contains repeating patterns. Section 3.1 focuses on a simple scenario where training sentences follow the form abacdc, where a \u0338= b and c \u0338= d, or a noisy variation of it. The ICL prompts maintain the same pattern but use different combinations of ab and cd from those in the training data. The goal is to understand what types of training data facilitate ICL in clean or noisy scenarios. Section 3.2 explores a more realistic case where two possible patterns are present: repeating the first letter () and repeating the second letter ().\nProposition 4 shows that we generally have t(xi1xi2xi3) \u0338= t(xi2xi1xi3), unlike in the one-layer case. Consequently, high accuracy is achievable without positional embeddings, as shown in Table 4.\nRoadmap of Section 3. In the rest of this section, we consider settings where each sentence contains repeating patterns. Section 3.1 focuses on a simple scenario where training sentences follow the form abacdc, where a \u0338= b and c \u0338= d, or a noisy variation of it. The ICL prompts maintain the same pattern but use different combinations of ab and cd from those in the training data. The goal is to understand what types of training data facilitate ICL in clean or noisy scenarios. Section 3.2 explores a more realistic case where two possible patterns are present: repeating the first letter (abca) and repeating the second letter (abcb).\n# .1 In-context learning on single-pattern tas\nIn this section, we examine the case where the training sentences follow a specific pattern of the form abacdc. Also, we analyze the impact of adding noise tokens to the training sentences on the ICL ability of autoregressive models. To formalize the discussion, let the vocabulary be V \u222aN, where N represents the noise tokens. Define S = {(a, b) | a, b \u2208V, a \u0338= b} and partition S into S1 and S2, where {c[1] | c \u2208S1} = {c[1] | c \u2208S2} = V and c[i] denotes the i-th element of c, to ensure training sentences are distinct from the ICL prompts. Consider three different scenarios: 1. Clean: Training data follow the form abacdc where ab, cd \u2208S1. ICL prompts follow the form abacd where ab, cd \u2208S2. 2. One-noisy: Training data follow the form abacdc where ab, cd \u2208S1, with one noise token n \u2208N randomly inserted anywhere except the last position (to ensure ICL prompts do not resemble the training data). ICL prompts follow the form abacd where ab, cd \u2208S2. 3. Block-noisy: Training data follow the form abacdc where ab, cd \u2208S1, with three consecutive noise tokens n1, n2, n3 \u2208N randomly inserted while preserving the aba and cdc blocks. ICL prompts follow the form abacdcef where ab, cd, ef \u2208S2. We set the vocabulary size |V | = 20, the number of noise tokens N = 20, and use only one attention layer as additional layers do not improve performance. Table 5 reveals interesting phenomena. Firstly, under the clean data scenario, ICL performs exceptionally well, with an observed performance increase with learned positional embeddings and a larger embedding dimension. However, ICL is notably challenging under the one-noisy scenario. In the block-noisy scenario, learned positional embeddings are crucial for satisfactory ICL performance. Theorem 5 formalizes these findings. Theorem 5 (Blocked noise structure facilitates ICL). Consider a sufficiently large autoregressive position-aware model that can achieve the minimum possible theoretical loss. Training this model in the one-noisy (block-noisy) scenario results in zero (perfect) ICL accuracy. The proof is in Appendix E. Theorem 5 says that ICL works perfectly under the block-noisy scenario, yet fails to work under the one-noisy scenario. However, as shown in Table 5, the use of sinusoidal positional embeddings significantly enhances prediction accuracy in the one-noisy scenario. This may be due to the fact that sinusoidal embeddings can encode relative positional information [Vaswani et al., 2017]. For example, training sentences of the form nabacdc, where n \u2208N, may help in predicting the most likely token following the ICL prompt abacd.\n# 3.2 In-context learning on dual-pattern tasks\nWe next examine the case where both the training data and ICL prompts contain two different patterns occurring with equal probability: abcadefd and abcbdefe, where a \u0338= b \u0338= c \u0338= a and d \u0338= e \u0338= f \u0338= d. We consider the clean and block-noisy scenarios, defined similarly as in Section 3.1, and set |V | = N = 20. Table 6 outlines the ICL performance for both scenario types across different model configurations. Unlike the single-pattern scenario, there is an improvement in performance with five layers compared to one layer, particularly with learned positional embeddings.\nTable 5: ICL on single-pattern tasks, averaged over 10 repetitions, achieves near-perfect accuracy in the clean data scenario regardless of architectures and embedding dimension (dE). The onenoisy scenario is the most challenging, with sinusoidal embeddings giving a higher accuracy. In the block-noisy scenario, learned positional embeddings result in significantly better ICL performance.\ndE = 10\ndE = 100\nPos. emb.\nClean\nOne-noisy\nBlock-noisy\nClean\nOne-noisy\nBlock-noisy\nLearned\n0.97\n0.00\n0.95\n1.00\n0.00\n1.00\nSinusoidal\n0.66\n0.10\n0.01\n0.96\n0.00\n0.55\nTable 6: ICL on dual-pattern tasks, averaged over 10 repetitions, achieves notably better accuracy using learned than sinusoidal embeddings. Near-perfect accuracy is attained in the clean scenario by a 5-layer transformer with an embedding dimension (dE) of 100 and learned positional embed dings. The block-noisy scenario is challenging; the same model attains the best performance.\ndE = 10\ndE = 100\nPos. emb.\nClean\nBlock-noisy\nClean\nBlock-noisy\n1-layer\nLearned\n(0.33, 0.33)\n(0.15, 0.16)\n(0.51, 0.49)\n(0.49, 0.50)\nSinusoidal\n(0.12, 0.66)\n(0.03, 0.03)\n(0.51, 0.48)\n(0.06, 0.10)\n5-layer\nLearned\n(0.39, 0.39)\n(0.23, 0.22)\n(0.97, 0.98)\n(0.87, 0.70)\nSinusoidal\n(0.32, 0.34)\n(0.04, 0.04)\n(0.83, 0.82)\n(0.04, 0.07)\nThis phenomenon is related to the notion of induction heads, where at least two layers may be necessary to distinguish the two patterns [Olsson et al., 2022]. This is reflected in Figure 2, which compares the accuracy trajectories of one-layer and five-layer models. While the five-layer setup effectively differentiates the two patterns, the one-layer configuration fails to do so. Meanwhile, in both clean and block-noisy scenarios, learned positional embeddings lead to notably higher accuracies as compared to sinusoidal ones, similar to the single-pattern case.\n# 4 Scenarios where in-context learning unexpectedly fails\nIn this section, we consider two scenarios where in-context learning (ICL) unexpectedly fails, irrespective of architectures. In Section 4.1, both the training data and test prompts follow repeating patterns across blocks, but the pattern in the test data differs from that in the training data. In Section 4.2, the training sentences contain known input-output pairs but only at fixed locations. Section 4.3 concludes with a synthetic data experiment that provides support to the theory.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/075f/075f77a4-1212-4626-9570-1d673229c75a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: One-layer models fail to differentiate the two patterns in Section 3.2, as evidenced by the accuracy trajectory graph on the left. On the other hand, five-layer models are capable of doing so.</div>\n# Failed scenario 1: Sentences with repeating\nIn this scenario, the training data comprises sentences in the form of abacdcefe, where a \u0338= b, c \u0338= d, and e \u0338= f. Each sentence is composed of three blocks, and each block consists of three tokens with the same pattern. For the ICL task, we consider predicting f from the prompt abbcddef, where a \u0338= b, c \u0338= d, and e \u0338= f. As each training sentence contains a repeated pattern, we expect a well-trained model to output f to maintain the pattern seen in the in-context examples: abb and cdd. However, as depicted in Table 7, all models fail to recognize the repeated patterns and predict the correct token. We next formalize a generalization of this scenario. Let the vocabulary be V = {1, 2, \u00b7 \u00b7 \u00b7 , |V |}, and define S = {(a, b) | a, b \u2208V, a \u0338= b}. To ensure training sentences are distinct from the ICL prompts, we first partition S into S1 and S2, where {c[1] | c \u2208S1} = {c[1] | c \u2208S2} = V. Here, c[i] denotes the i-th element of c. Suppose we autoregressively train a sufficiently large position-aware model so that it is possible to achieve the minimum possible theoretical loss. The training sentences take the form x11x12x11x21x22x21 \u00b7 \u00b7 \u00b7 xN1xN2xN1, where xi1 \u0338= xi2 and (xi1, xi2) is independently selected from S1 for every i \u2208[N]. Theorem 6, whose proof is in to Appendix F, states that ICL fails to hold regardless of the number of in-context examples. Theorem 6 (Failure of ICL: Different repeated patterns). Consider the generalized scenario in Section 4.1. For any 1 \u2264\u2113\u2264N, given an in-context prompt of the form x11x12x12x21x22x22 \u00b7 \u00b7 \u00b7 x\u21131x\u21132 where xi1 \u0338= xi2 and (xi1, xi2) \u2208S2 for every i \u2208[\u2113], the model predicts x\u21131 instead of x\u21132. Theorem 6 and Table 7 demonstrate that ICL achieves zero accuracy irrespective of the number of in-context examples (\u2113\u22121). This finding provides insight into the generalization capacity of autoregressive models in the context of ICL. Simply put, if the pattern in the in-context examples differs significantly from any pattern in the training data, ICL may not occur.\n<div style=\"text-align: center;\">Table 7: ICL in failed scenarios, averaged over 10 repetitions, achieves zero accuracy for a architecture and embedding dimension (dE).</div>\nFailed scenario 1\nFailed scenario 2\nPos. emb.\ndE = 10\ndE = 100\ndE = 10\ndE = 100\n1-layer\nLearned\n0.00\n0.00\n0.01\n0.00\nSinusoidal\n0.01\n0.00\n0.00\n0.00\n5-layer\nLearned\n0.00\n0.00\n0.00\n0.00\nSinusoidal\n0.00\n0.00\n0.00\n0.00\n# 4.2 Failed scenario 2: Sentences with known pairs but only at fixed\n# 4.2 Failed scenario 2: Sentences with known pairs but only  locations\nWe revisit the paired relationship scenario discussed in Section 2. The training data now comprises sentences of the form of aipqrsbi, where (ai, bi) represents a known pairing and p, q, r, s represent other words. For the ICL task, we consider predicting bi3 from the prompt ai1bi1ai2bi2ai3, where i1 \u0338= i2 \u0338= i3 \u0338= i1. As each training sentence contains an (ai, bi) pair always at a fixed location, we expect a well-trained model to output bi3 to maintain the pattern in the in-context examples: ai1bi1 and ai2bi2. However, none of the models can identify the repeated patterns and predict the correct token, as shown in Table 7. We next formalize a generalization of this scenario. Let the vocabulary be {(ai, bi)}i\u2208[I] \u222aV, where V = {1, 2, \u00b7 \u00b7 \u00b7 , |V |} represent other words. As in Section 4.1, we autoregressively train a sufficiently large position-aware model that can achieve the minimum possible theoretical loss. The training sentences take the form aiv1v2 \u00b7 \u00b7 \u00b7 v2kbi, where i and v1:2k are independently chosen from [I] and V, respectively, uniformly at random. Theorem 7, whose proof is in Appendix G, states that ICL fails to occur regardless of the number of in-context examples. Theorem 7 (Failure of ICL: Different pattern structures). Consider the generalized scenario in Section 4.2. For any 1 \u2264\u2113\u2264k + 1, given an in-context prompt of the form ai1bi1ai2bi2 \u00b7 \u00b7 \u00b7 ai\u2113with distinct ij\u2019s, the model never predicts bi\u2113. Theorem 7 highlights the finding that the success of ICL relies heavily on how the patterns appear in the training data. In this scenario, the (ai, bi) pairs consistently appear at the beginning and end of each training sentence, and we anticipate the model to recognize this relationship for ICL to occur. However, as shown in Theorem 7 and Table 7, this is not the case.\n# 4.3 Experiment on a synthetic corpus\nWe conduct an experiment on a synthetic corpus consisting of (country)-(capital) relationships. Each sentence in the corpus is categorized into exactly one of four possible categories: (1) exactly one country-capital pair; (2) exactly two country-capital pairs; (3) exactly one country without any pair; and (4) no country. In sentences with exactly one country-capital pairs, each capital appears in the first position, each country appears in the last position, and every sentence consists of six words. The corpus generation process is as follows:\n2. Generate 130 sentences containing exactly one country-capital pair (13 for each country). Example: Paramaribo stands as capital of Suriname. 3. Generate 30 sentences containing exactly one country without any pair. Example: The banking sector is central to Liechtenstein\u2019s prosperity. 4. Generate 60 sentences without any country, capital city, or IOC code. Example: Every country has its unique cultural identity and heritage. 5. Generate 1,000 sentences containing exactly two different country-capital pairs by concatenating sentences generated in Step 2. Example: Brazil functions as heart of Brasilia. Turkmenistan operates as center for Ashgabat. e train a five-layer two-head autoregressive transformer on this corpus, with an embedding mension of 100. Similar to Section 2.5, we assess the ICL accuracies using prompts involving untries and their capitals. We discover that the ICL accuracies are zero regardless of the number\nWe train a five-layer two-head autoregressive transformer on this corpus, with an embeddin dimension of 100. Similar to Section 2.5, we assess the ICL accuracies using prompts involvin countries and their capitals. We discover that the ICL accuracies are zero regardless of the numbe of in-context examples (one to five), thus supporting the theory.\n# 5 Related work\nLarge language models (LLMs), such as transformers, are widely recognized for their outstanding performance in in-context learning (ICL) [Brown et al., 2020]. ICL refers to the capability of LLMs to discern specific tasks and generate predictions based on input-output pairs (known as prompts) without needing any parameter updates. A multitude of studies have been dedicated to exploring this intriguing phenomenon from various theoretical and empirical perspectives. In this section, we provide a brief summary of some of these studies.\nwe provide a brief summary of some of these studies. Some studies adopted a Bayesian approach to studying ICL. Xie et al. [2021] posited that ICL can be viewed as implicit Bayesian inference. They demonstrated that LLMs can infer a latent document-level concept for next-token prediction during pretraining and a shared latent concept across input-output pairs in an ICL prompt, under the assumption that documents are generated from hidden Markov models (HMMs). Wang et al. [2023] and Zhang et al. [2023] expanded on this idea by exploring more realistic latent variable models beyond HMMs. Wang et al. [2023] argued that large language models function as latent variable models, with latent variables containing taskrelated information being implicitly inferred. Zhang et al. [2023] showed that without updating the neural network parameters, ICL can be interpreted as Bayesian model averaging parameterized by the attention mechanism. Panwar et al. [2023] provided empirical evidence that transformers behave like Bayesian predictors when performing ICL with linear and non-linear function classes. Dalal and Misra [2024] proposed a Bayesian learning framework to understand ICL through the lens of text generation models represented by multinomial transition probability matrices. Chiang and Yogatama [2024] proposed the pelican soup framework to explain ICL without relying on latent variable models. This framework incorporates concepts such as a common sense knowledge base, natural language classification, and meaning association, enabling the establishment of a loss bound for ICL that depends on the number of in-context examples.\nGarg et al. [2022] formulated ICL as learning a specific function class F from prompts of the form (x1, f(x1), . . . , xn, f(xn), xn+1) and their corresponding responses f(xn+1). Here, f \u2208F, where F is a function class. In this context, ICL refers to the capability of a transformer to output a number close to g(yn+1) given a prompt of the form (y1, g(y1), . . . , yn, g(xn), yn+1), where g \u2208F. Many studies adopted this regression formulation of ICL, with some linking ICL to gradient descent. Aky\u00fcrek et al. [2022], Von Oswald et al. [2023], and Dai et al. [2023] proved that transformers are capable of implementing gradient descent, which results in their ICL ability. Bai et al. [2023] established generalization bounds for ICL and proved that transformers can perform algorithm selection like statisticians. Zhang et al. [2024] showed that the gradient flow dynamics of transformers converge to a global minimum that enables ICL. Huang et al. [2023] investigated the learning dynamics of single-layer softmax transformers trained via gradient descent to perform ICL on linear functions. Ahn et al. [2024] explored the optimization landscape of transformers and proved that the optimal parameters coincide with an iteration of preconditioned gradient descent. In a related exploration, Li et al. [2023a] showed that softmax regression models learned through gradient descent are similar to transformers. Ren and Liu [2023] related ICL with softmax transformers to contrastive learning, where the inference process of ICL can be viewed as a form of gradient descent. Mahankali et al. [2023] proved that minimizing the pretraining loss is equivalent to a step of gradient descent in single-layer linear transformers. Vladymyrov et al. [2024] established that linear transformers execute a variant of preconditioned gradient descent by maintaining implicit linear models. On the other hand, some studies argued that the ICL ability of transformers cannot be attributed to gradient descent. Fu et al. [2023] showed that ICL for linear regression tasks arises from higher-order optimization techniques like iterative Newton\u2019s method rather than gradient descent. Wibisono and Wang [2023] demonstrated that transformers can perform ICL on unstructured data that lack explicit input-output pairings, with softmax attention playing an important role especially when using a single attention layer. Shen et al. [2023] provided empirical evidence that the equivalence between gradient descent and ICL might not be applicable in real-world scenarios. Numerous studies focused on the pretraining aspects (e.g., data distribution and task diversity) of ICL. Min et al. [2022] showed that the input-label mapping in the in-context examples does not significantly affect ICL performance. Chan et al. [2022] demonstrated that the ICL capabilities of transformers depend on the training data distributions and model features. Kossen et al. [2024] established that ICL considers in-context label information and is capable of learning entirely new tasks in-context. Li and Qiu [2023] introduced an iterative algorithm designed to enhance ICL performance by selecting a small set of informative examples that effectively characterize the ICL task. Qin et al. [2023] proposed a method based on zero-shot chain-of-thought reasoning for selecting ICL examples, emphasizing the importance of choosing diverse examples that are strongly correlated with the test sample. Han et al. [2023b] studied ICL by identifying a small subset of the pretraining data that support ICL via gradient-based methods. They discovered that this supportive pretraining data typically consist of more uncommon tokens and challenging examples, characterized by a small information gain from long-range context. Peng et al. [2024] proposed a selection method for ICL demonstrations that are both data-dependent and modeldependent. Van et al. [2024] introduced a demonstration selection method that enhances ICL performance by analyzing the influences of training samples using influence functions.\nIn a similar vein, Wu et al. [2023] demonstrated that pretraining single-layer linear attention models for ICL on linear regression with a Gaussian prior can be effectively accomplished with a minimal number of independent tasks, regardless of task dimension. Ravent\u00f3s et al. [2023] emphasized a task diversity threshold that differentiates the conditions under which transformers can successfully address unseen tasks. Yadlowsky et al. [2023] attributed the impressive ICL capabilities of transformers to the diversity and range of data mixtures in their pretraining, rather than their inductive biases for generalizing to new tasks. Ding et al. [2024] compared the ICL performance of transformers trained with prefixLM (where in-context samples can attend to all tokens) versus causalLM (where in-context samples cannot attend to subsequent tokens), finding that the latter resulted in poorer ICL performance. Chen et al. [2024b] discovered that the ICL capabilities of language models rely on the presence of pairs of phrases with similar structures within the same sentence. Zhao et al. [2024] proposed a calibration scheme that modifies model parameters by adding random noises, resulting in fairer and more confident predictions. Abbas et al. [2024] demonstrated that the ICL predictions from transformer-based models often exhibit low confidence, as indicated by high Shannon entropy. To address this issue, they introduced a straightforward method that linearly calibrates output probabilities, independent of the model\u2019s weights or architecture. Other studies analyzed ICL from a learning theory perspective. Hahn and Goyal [2023] proposed an information-theoretic bound that explains how ICL emerges from next-token prediction. Wies et al. [2023] derived a PAC-type framework for ICL and finite-sample complexity results. Jeon et al. [2024] introduced a novel information-theoretic view of meta-learning (including ICL), allowing for the decomposition of errors into three components. They proved that in ICL, the errors decrease as the number of examples or sequence length increase. Other studies focus on the mechanistic interpretability component of ICL. Olsson et al. [2022] argued that transformers can develop induction heads that are able to complete token sequences such as [A][B] \u00b7 \u00b7 \u00b7 [A] \u2192 [B], leading to impressive ICL performance. Bietti et al. [2023] examined a setup where tokens are generated from either global or context-specific bigram distributions to distinguish between global and in-context learning. They found that global learning occurs rapidly, while in-context learning is achieved gradually through the development of an induction head. Ren et al. [2024] identified semantic induction heads that increase the output logits of tail tokens when attending to head tokens, providing evidence that these heads could play a vital role in the emergence of ICL. Yu and Ananiadou [2024] showed that the ICL ability of transformers arises from the utilization of in-context heads, where each query and key matrix collaborate to learn the similarity between the input text and each demonstration example. A number of works delved into specific data generating processes to provide insight into the emergence of ICL. Bhattamishra et al. [2023] examined the ICL ability of transformers by focusing on discrete functions. Specifically, they showed that transformers perform well on simpler tasks, struggle with more complex tasks, and can learn more efficiently when provided with examples that uniquely identify a task. Guo et al. [2023] investigated ICL in scenarios where each label is influenced by the input through a potentially complex yet constant representation function, coupled with a unique linear function for each instance. Aky\u00fcrek et al. [2024] studied ICL of regular languages produced by random finite automata. They compared numerous neural sequence models and demonstrated that transformers significantly outperform RNN-based models because\nof their ability to develop n-gram heads, which are a generalization of induction heads. Sander et al. [2024] analyzed simple first-order autoregressive processes to gain insight into how transformers perform ICL to predict the next tokens.\nSome studies explored how different components of transformers affect their ICL abilities. Ahuja and Lopez-Paz [2023] compared the ICL performance of transformers and MLP-based architectures under distribution shifts. Their findings demonstrate that while both methods perform well in indistribution ICL, transformers exhibit superior ICL performance when faced with mild distribution shifts. Collins et al. [2024] showed that softmax attention outperforms linear attention in ICL due to its ability to calibrate its attention window to the Lipschitzness of the pretraining tasks. Xing et al. [2024] focused on linear regression tasks to identify transformer components that enable ICL. They found that positional encoding is crucial, along with the use of multiple heads, multiple layers, and larger input dimensions. Cui et al. [2024] proved that multi-head attention outperforms single-head attention in various practical scenarios, including those with noisy labels and correlated features. Chen et al. [2024a] investigated the ICL dynamics of a multi-head softmax attention model applied to multi-task linear regression. They proved the convergence of the gradient flow and observed the emergence of a task allocation phenomenon, where each attention head specializes in a specific task. Finally, several studies proposed various hypotheses on the emergence of ICL and provided theoretical justifications. Li et al. [2023b] viewed ICL as an algorithm learning problem where a transformer implicitly constructs a hypothesis function at inference time. Han et al. [2023a] argued that the ability of transformers to execute ICL is attributable to their capacity to simulate kernel regression. Singh et al. [2023] explored the interaction between ICL and in-weights learning (IWL) using synthetic data designed to support both processes. They observed that ICL initially emerges, followed by a transient phase where it disappears and gives rise to IWL. Yan et al. [2023] studied ICL from the perspective that token co-occurrences play a crucial role in guiding the learning of surface patterns that facilitates ICL. Abernethy et al. [2024] showed that transformers can execute ICL by dividing a prompt into examples and labels, then employing sparse linear regression to deduce input-output relationships and generate predictions. Lin and Lee [2024] developed a probabilistic model that can simultaneously explain both task learning and task retrieval aspects of ICL. Here, task learning refers to the ability of language models to identify a task from in-context examples, while task retrieval pertains to their ability to locate the relevant task within the pretraining data.\n# 6 Discussion\nIn this paper, we investigate how in-context learning (ICL) can emerge from pretraining on unstructured natural language data. We present three main findings, supported by both theory and empirical studies. First, ICL can be achieved by simply modeling co-occurrence using older generations of language models like continuous bag of words (CBOW), when ICL prompts involve pairs that frequently appear together. Second, when ICL prompts involve recognizable patterns that do not always co-occur, positional information and noise structures play crucial roles in enabling ICL. Finally, we highlight the importance of training data structure in ICL by examining two instances where ICL unexpectedly fails.\n# 7 Limitations and future work\nThis study has several limitations. Firstly, the experiments are conducted on a relatively small scale. However, they still provide sufficient evidence to support the theoretical findings. Secondly, the focus of this study is on two specific types of in-context learning (ICL) tasks, as described in Section 1. Lastly, real data sets are not utilized due to the lack of alignment with the study objectives. Despite these limitations, we believe that this work offers valuable insights into the emergence of ICL through training on unstructured natural language data, supported by both theoretical and empirical evidence from experiments involving prompting and synthetic data. Further analyses on other ICL tasks and their reliance on model architecture can be fruitful avenues for future work.\nAcknowledgments. This work was supported in part by the Office of Naval Research under grant number N00014-23-1-2590 and the National Science Foundation under Grant No. 2231174 and No. 2310831.\nM. Abbas, Y. Zhou, P. Ram, N. Baracaldo, H. Samulowitz, T. Salonidis, and T. Chen. Enhancing in-context learning via linear probe calibration. In Artificial Intelligence and Statistics, 2024. J. Abernethy, A. Agarwal, T. V. Marinov, and M. K. Warmuth. A mechanism for sample-efficient in-context learning for sparse retrieval tasks. In Algorithmic Learning Theory, 2024. K. Ahn, X. Cheng, H. Daneshmand, and S. Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Neural Information Processing Systems, 2024. K. Ahuja and D. Lopez-Paz. A closer look at in-context learning under distribution shifts. In Workshop on Efficient Systems for Foundation Models at ICML, 2023. E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is incontext learning? Investigations with linear models. In International Conference on Learning Representations, 2022. E. Aky\u00fcrek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Architectures and algorithms. arXiv preprint arXiv:2401.12973, 2024. Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Neural Information Processing Systems, 2023. S. Bhattamishra, A. Patel, P. Blunsom, and V. Kanade. Understanding in-context learning in transformers and LLMs by learning to learn discrete functions. In International Conference on Learning Representations, 2023. A. Bietti, V. Cabannes, D. Bouchacourt, H. Jegou, and L. Bottou. Birth of a transformer: A memory viewpoint. In Neural Information Processing Systems, 2023. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Neural Information Processing Systems, 2020. S. C. Chan, A. Santoro, A. K. Lampinen, J. X. Wang, A. K. Singh, P. H. Richemond, J. McClelland, and F. Hill. Data distributional properties drive emergent in-context learning in transformers. In Neural Information Processing Systems, 2022. S. Chen, H. Sheen, T. Wang, and Z. Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024a. Y. Chen, C. Zhao, Z. Yu, K. McKeown, and H. He. Parallel structures in pre-training data yield in-context learning. arXiv preprint arXiv:2402.12530, 2024b. T.-R. Chiang and D. Yogatama. Understanding in-context learning with a pelican soup framework. arXiv preprint arXiv:2402.10424, 2024.\nL. Collins, A. Parulekar, A. Mokhtari, S. Sanghavi, and S. Shakkottai. In-context learning with transformers: Softmax attention adapts to function Lipschitzness. arXiv preprint arXiv:2402.11639, 2024. Y. Cui, J. Ren, P. He, J. Tang, and Y. Xing. Superiority of multi-head attention in in-context linear regression. arXiv preprint arXiv:2401.17426, 2024. D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei. Why can GPT learn in-context? Language models secretly perform gradient descent as meta optimizers. In Association for Computational Linguistics, 2023. S. Dalal and V. Misra. The matrix: A Bayesian learning model for LLMs. arXiv preprint arXiv:2402.03175, 2024. N. Ding, T. Levinboim, J. Wu, S. Goodman, and R. Soricut. CausalLM is not optimal for in-context learning. In International Conference on Learning Representations, 2024. D. Fu, T.-Q. Chen, R. Jia, and V. Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. In Workshop on Mathematics of Modern Machine Learning at NeurIPS, 2023. S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? A case study of simple function classes. In Neural Information Processing Systems, 2022. T. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y. Bai. How do transformers learn incontext beyond simple functions? A case study on learning with representations. In International Conference on Learning Representations, 2023. M. Hahn and N. Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv preprint arXiv:2303.07971, 2023. C. Han, Z. Wang, H. Zhao, and H. Ji. Explaining emergent in-context learning as kernel regression. arXiv preprint arXiv:2305.12766, 2023a. X. Han, D. Simig, T. Mihaylov, Y. Tsvetkov, A. Celikyilmaz, and T. Wang. Understanding in-context learning via supportive pretraining data. In Association for Computational Linguistics, 2023b. Y. Huang, Y. Cheng, and Y. Liang. In-context convergence of transformers. In Workshop on Mathematics of Modern Machine Learning at NeurIPS, 2023. H. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024. D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. J. Kossen, Y. Gal, and T. Rainforth. In-context learning learns label relationships but is not conventional learning. In International Conference on Learning Representations, 2024. S. Li, Z. Song, Y. Xia, T. Yu, and T. Zhou. The closeness of in-context learning and weight shifting for softmax regression. arXiv preprint arXiv:2304.13276, 2023a.\nX. Li and X. Qiu. Finding support examples for in-context learning. In Empirical Methods in Natural Language Processing, 2023. Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, 2023b. Z. Lin and K. Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024. A. V. Mahankali, T. Hashimoto, and T. Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In International Conference on Learning Representations, 2023. T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Empirical Methods in Natural Language Processing, 2022. C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. OpenAI. ChatGPT 3.5. https://openai.com/chatgpt, 2022. M. Panwar, K. Ahuja, and N. Goyal. In-context learning through the Bayesian prism. In International Conference on Learning Representations, 2023. K. Peng, L. Ding, Y. Yuan, X. Liu, M. Zhang, Y. Ouyang, and D. Tao. Revisiting demonstration selection strategies in in-context learning. arXiv preprint arXiv:2401.12087, 2024. C. Qin, A. Zhang, A. Dagar, and W. Ye. In-context learning with iterative demonstration selection. arXiv preprint arXiv:2310.09881, 2023. A. Ravent\u00f3s, M. Paul, F. Chen, and S. Ganguli. Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. In Neural Information Processing Systems, 2023. J. Ren, Q. Guo, H. Yan, D. Liu, X. Qiu, and D. Lin. Identifying semantic induction heads to understand in-context learning. arXiv preprint arXiv:2402.13055, 2024. R. Ren and Y. Liu. In-context learning with transformer is really equivalent to a contrastive learning pattern. arXiv preprint arXiv:2310.13220, 2023. M. E. Sander, R. Giryes, T. Suzuki, M. Blondel, and G. Peyr\u00e9. How do transformers perform in-context autoregressive learning? arXiv preprint arXiv:2402.05787, 2024. L. Shen, A. Mishra, and D. Khashabi. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023. A. Singh, S. Chan, T. Moskovitz, E. Grant, A. Saxe, and F. Hill. The transient nature of emergent in-context learning in transformers. In Neural Information Processing Systems, 2023.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. M.-H. Van, X. Wu, et al. In-context learning demonstration selection via influence analysis. arXiv preprint arXiv:2402.11750, 2024. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. M. Vladymyrov, J. von Oswald, M. Sandler, and R. Ge. Linear transformers are versatile in-context learners. arXiv preprint arXiv:2402.14180, 2024. J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2023. X. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Neural Information Processing Systems, 2023. K. C. Wibisono and Y. Wang. On the role of unstructured training data in transformers\u2019 in-context learning capabilities. In Workshop on Mathematics of Modern Machine Learning at NeurIPS, 2023. N. Wies, Y. Levine, and A. Shashua. The learnability of in-context learning. In Neural Information Processing Systems, 2023. J. Wu, D. Zou, Z. Chen, V. Braverman, Q. Gu, and P. Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In International Conference on Learning Representations, 2023. S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit Bayesian inference. In International Conference on Learning Representations, 2021. Y. Xing, X. Lin, N. Suh, Q. Song, and G. Cheng. Benefits of transformer: In-context learning in linear regression tasks with unstructured data. arXiv preprint arXiv:2402.00743, 2024. S. Yadlowsky, L. Doshi, and N. Tripuraneni. Pretraining data mixtures enable narrow model selection capabilities in transformer models. arXiv preprint arXiv:2311.00871, 2023. J. Yan, J. Xu, C. Song, C. Wu, Y. Li, and Y. Zhang. Understanding in-context learning from repetitions. In International Conference on Learning Representations, 2023. Z. Yu and S. Ananiadou. How do large language models learn in-context? Query and key matrices of in-context heads are two towers for metric learning. arXiv preprint arXiv:2402.02872, 2024. R. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 2024. Y. Zhang, F. Zhang, Z. Yang, and Z. Wang. What and how does in-context learning learn? Bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023.\nY. Zhao, Y. Sakai, and N. Inoue. NoisyICL: A little noise in model parameters calibrates in-contex learning. arXiv preprint arXiv:2402.05515, 2024.\n# A Proof of Theorem 1\nProof. Let |V | = 2K + L denote the vocabulary size. Consider a sentence X represented by its one-hot encoding (i.e., X \u2208{0, 1}|V |\u00d7S). For every position i \u2208[S], the loss for predicting the word in the i-th position given all the other words is given by ||AX(1S \u2212ei) \u2212Xei||2 2, where A = U\u22a4V S\u22121 \u2208R|V |\u00d7|V | and ei \u2208RS is a zero vector with 1 on its i-th entry. Here, A is a matrix summarizing the similarity between each pair of words, one as a center word and the other as a context word. Our objective is to find A that minimizes the sum of losses for each position in each sentence. Lemma 8 gives a closed-form expression of the minimizer.\nLemma 8. The minimizer of the overall loss is given by A = B ((S \u22122)B + C)\u22121. Here, B is a matrix whose (i, j)-th entry is p(i, j), the probability that for a given (center, context) pair, the center is i \u2208|V | and the context is j \u2208|V |. Moreover, C is a diagonal matrix whose i-th diagonal entry is p(i) = \ufffd j\u2208|V | p(i, j).\n \ufffd Proof. Let L(X) = \ufffdS i=1 ||AX(1S \u2212ei) \u2212Xei||2 2 denote the sum of the losses corresponding to all tokens in sentence X. By direct calculation,\nNote that \ufffdS i=1(1S \u2212ei)(1S \u2212ei)\u22a4= (S \u22122)1S\u00d7S +IS\u00d7S and \ufffdS i=1 ei(1S \u2212ei)\u22a4= 1S\u00d7S \u2212IS\u00d7S Now, let our sentences be X1, X2, \u00b7 \u00b7 \u00b7 , XN. The minimizer of the overall loss thus satisfies\nWe denote the number of (center, context) pairs across all sentences in which the center is i and the context is j by #(i, j). Moreover, we define #(i) = \ufffd j\u2208|V | #(i, j). It is easy to see that Equation (1) can be rewritten as\nwhere \u02dcB is a matrix such that its (i, j)-th entry is #(i,j) N and \u02dcC is a diagonal matrix such that its i-th diagonal element is #(i) N . As N \u2192\u221e, an application of the law of large numbers yields #(i,j) N \u2192S(S \u22121)p(i, j) almost surely and #(i) N \u2192S(S \u22121)p(i) almost surely, where p(i, j) is the probability that for a given (center, context) pair, the center is i and the context is j, and p(i) = \ufffd j\u2208|V | p(i, j). Thus, as N \u2192\u221e, we have\n(1)\nwhere the equalities in the probabilities are a consequence of the data distribution. For ease of presentation, we denote a square matrix with \u03b1 on the diagonal and \u03b2 off the diagonal as X\u03b1,\u03b2, and a matrix with all entries \u03b3 as Y\u03b3. We then have\n\uf8f0 \uf8fb Now, define a = (S\u22122)p1, b = (S\u22122)p2, c = (S\u22122)p3, d = (S\u22122)p4, e = 2(K\u22121)p1+p3+Lp and f = (L \u22121)p2 + 2Kp4. It is easy to see that\nMoreover, its inverse can be written as\nwhere\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cc1a/cc1a54ff-1134-4119-be45-40435eab3826.png\" style=\"width: 50%;\"></div>\n \u2212 \ufffd \ufffd = \u2212 \uf8eb \uf8ec \uf8ec \uf8ed \u22122a2b(K \u22121)(L \u22121) \u22122a2f(K \u22121) + 2abe(K \u22122)(L \u22121 + 2(a \u2212e)d2KL + be(c + e)(L \u22121) + ef (e \u2212c)(2a \u2212c \u2212e)\u2206 d q6 = \u2212 \ufffd 2a(K\u22121)(b(L\u22122)+f)+b(L\u22122)(c+e)+cf\u22122d2KL+2d2K+ef (b\u2212f)\u2206 \ufffd .  computing A = B((S \u22122)B+C)\u22121, given the following center word em and all possible context words are as follows: \u2022 Center word = ci for any i \u2013 ci : 2(K \u22121)p1q1 + p3q3 + Lp4q4; \u2013 cj : 2(K \u22122)p1q1 + p1q5 + p3q1 + p1q3 + Lp4q4 (j \u0338= i); \u2013 di : 2(K \u22121)p1q1 + p3q5 + Lp4q4; \u2013 dj : 2(K \u22122)p1q1 + p1q3 + p3q1 + p1q5 + Lp4q4 (j \u0338= i); \u2013 rj : 2(K \u22121)p1q4 + p3q4 + p4q6 + (L \u22121)p4q2 (for any j). \u2022 Center word = di for any i \u2013 di : 2(K \u22121)p1q1 + p3q3 + Lp4q4; \u2013 dj : 2(K \u22122)p1q1 + p1q5 + p3q1 + p1q3 + Lp4q4 (j \u0338= i); \u2013 ci : 2(K \u22121)p1q1 + p3q5 + Lp4q4; \u2013 cj : 2(K \u22122)p1q1 + p1q3 + p3q1 + p1q5 + Lp4q4 (j \u0338= i); \u2013 rj : 2(K \u22121)p1q4 + p3q4 + p4q6 + (L \u22121)p4q2 (for any j). \u2022 Center word = ri \u2013 cj : 2(K \u22121)p4q1 + p4q5 + p4q3 + (L \u22121)p2q4 (for any j); \u2013 dj : 2(K \u22121)p4q1 + p4q5 + p4q3 + (L \u22121)p2q4 (for any j); \u2013 ri : 2Kp4q4 + (L \u22121)p2q2; \u2013 rj : 2Kp4q4 + (L \u22122)p2q2 + p2q6 (j \u0338= i).\nRecall that the ICL problem of interest is the following: given context words ci1di1 \u00b7 \u00b7 \u00b7 ci\u2113di\u2113ci\u2113+1, we aim to predict di\u2113+1. Without loss of generality, we can rewrite the problem to predict d\u2113+1 given context words c1d1 \u00b7 \u00b7 \u00b7 c\u2113d\u2113c\u2113+1. We now compute the total similarity for each possible center word, where \u03f5\u22a4\u03b4 indicates the similarity between the word \u03f5 in the center and the word \u03b4 in the context. \u2022 c1 (or any of c2, \u00b7 \u00b7 \u00b7 , c\u2113) : c\u22a4 1 c1 + \u2113c\u22a4 1 c2 + c\u22a4 1 d1 + (\u2113\u22121)c\u22a4 1 d2; \u2022 d1 (or any of d2, \u00b7",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the emergence of in-context learning (ICL) capabilities in large language models (LLMs) trained on unstructured data, which is critical for understanding their performance in practical applications.",
        "problem": {
            "definition": "The primary problem is understanding how ICL arises from unsupervised training on unstructured text data, which typically does not resemble structured input-output tasks.",
            "key obstacle": "A significant challenge is the gap between structured training tasks and the unstructured data used for training, making it difficult to explain ICL's emergence."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that co-occurrence patterns in language can lead to effective learning outcomes, even without structured prompts.",
            "opinion": "The authors propose that ICL can emerge from simple modeling of co-occurrence information rather than complex training setups.",
            "innovation": "The main innovation is demonstrating that ICL can be achieved through classical language models like continuous bag of words (CBOW), contrasting with previous approaches that emphasized more sophisticated models."
        },
        "Theory": {
            "perspective": "The theoretical perspective focuses on the necessity of co-occurrence modeling and the role of positional information and noise structures in ICL.",
            "opinion": "The authors argue that ICL is sensitive to the structure of training data and the relationships between input-output pairs.",
            "proof": "The paper provides theoretical proofs that establish conditions under which ICL can emerge from co-occurrence information and highlights the importance of positional embeddings."
        },
        "experiments": {
            "evaluation setting": "The experiments are conducted using synthetic datasets designed to simulate various ICL tasks involving known input-output relationships.",
            "evaluation method": "The evaluation involves comparing model predictions against ground truth outputs across different configurations, including clean and noisy scenarios."
        },
        "conclusion": "The paper concludes that ICL can emerge from unsupervised training on unstructured data by modeling co-occurrence patterns, but requires attention to data structure for successful generalization.",
        "discussion": {
            "advantage": "The main advantage is providing a clearer understanding of how ICL can be achieved without relying on structured training data.",
            "limitation": "A limitation is that the experiments are conducted on a limited scale, which may not fully capture the complexities of real-world applications.",
            "future work": "Future work could explore a broader range of ICL tasks and investigate the impact of different model architectures on ICL performance."
        },
        "other info": [
            {
                "acknowledgments": "This work was supported in part by the Office of Naval Research and the National Science Foundation."
            },
            {
                "keywords": [
                    "in-context learning",
                    "language models",
                    "continuous bag of words",
                    "co-occurrence",
                    "positional embeddings",
                    "transformers"
                ]
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the emergence of in-context learning (ICL) capabilities in large language models (LLMs) trained on unstructured data, which is critical for understanding their performance in practical applications."
        },
        {
            "section number": "1.2",
            "key information": "Understanding how ICL arises from unsupervised training on unstructured text data is crucial, as it highlights the gap between structured training tasks and the unstructured data used for training."
        },
        {
            "section number": "1.3",
            "key information": "The authors propose that ICL can emerge from simple modeling of co-occurrence information rather than complex training setups, demonstrating the significance of LLMs in this process."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective focuses on the necessity of co-occurrence modeling and the role of positional information and noise structures in ICL."
        },
        {
            "section number": "3.4",
            "key information": "The paper provides theoretical proofs that establish conditions under which ICL can emerge from co-occurrence information and highlights the importance of positional embeddings."
        },
        {
            "section number": "6.1",
            "key information": "The authors argue that ICL is sensitive to the structure of training data and the relationships between input-output pairs, indicating potential model bias."
        },
        {
            "section number": "7",
            "key information": "The paper concludes that ICL can emerge from unsupervised training on unstructured data by modeling co-occurrence patterns, requiring attention to data structure for successful generalization."
        }
    ],
    "similarity_score": 0.7347936990530154,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/From Unstructured Data to In-Context Learning_ Exploring What Tasks Can Be Learned and When.json"
}