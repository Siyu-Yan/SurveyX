{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2306.14561",
    "title": "On the Guarantees of Minimizing Regret in Receding Horizon",
    "abstract": "Towards bridging classical optimal control and online learning, regret minimization has recently been proposed as a control design criterion. This competitive paradigm penalizes the loss relative to the optimal control actions chosen by a clairvoyant policy, and allows tracking the optimal performance in hindsight no matter how disturbances are generated. In this paper, we propose the first receding horizon scheme based on the repeated computation of finite horizon regret-optimal policies, and we establish stability and safety guarantees for the resulting closed-loop system. Our derivations combine novel monotonicity properties of clairvoyant policies with suitable terminal ingredients. We prove that our scheme is recursively feasible, stabilizing, and that it achieves bounded regret relative to the infinite horizon clairvoyant policy. Last, we show that the policy optimization problem can be solved efficiently through convex-concave programming. Our numerical experiments show that minimizing regret can outperform standard receding horizon approaches when the disturbances poorly fit classical design assumptions - even when the finite horizon planning is recomputed less frequently.",
    "bib_name": "martin2023guaranteesminimizingregretreceding",
    "md_text": "# On the Guarantees of Minimizing Regret in Receding Horizo\nAndrea Martin, Luca Furieri, Florian D\u00a8orfler, John Lygeros, and Giancarlo Ferrari-Trecate\nn, Luca Furieri, Florian D\u00a8orfler, John Lygeros, and Giancarlo Fe\nAbstract\u2014Towards bridging classical optimal control and online learning, regret minimization has recently been proposed as a control design criterion. This competitive paradigm penalizes the loss relative to the optimal control actions chosen by a clairvoyant policy, and allows tracking the optimal performance in hindsight no matter how disturbances are generated. In this paper, we propose the first receding horizon scheme based on the repeated computation of finite horizon regret-optimal policies, and we establish stability and safety guarantees for the resulting closed-loop system. Our derivations combine novel monotonicity properties of clairvoyant policies with suitable terminal ingredients. We prove that our scheme is recursively feasible, stabilizing, and that it achieves bounded regret relative to the infinite horizon clairvoyant policy. Last, we show that the policy optimization problem can be solved efficiently through convex-concave programming. Our numerical experiments show that minimizing regret can outperform standard receding horizon approaches when the disturbances poorly fit classical design assumptions \u2013 even when the finite horizon planning is recomputed less frequently.\n[eess.SY\n# I. INTRODUCTION\nMany feedback control methods aim to optimize a performance measure with respect to a specific class of exogenous perturbations. For instance, classical H2 and H\u221econtrol paradigms assume that a stochastic or adversarial disturbance process drives the system dynamics, respectively, and minimize the expected or worst-case control cost accordingly [1]. As these control policies are tailored to the assumed nature of the exogenous perturbations, they may incur significant loss or be conservative if the true disturbances do not fulfill the modeling assumptions [2], [3]. For striking a balance between nominal performance and robustness, several approaches have been proposed, including risk-sensitive control [4], mixed H2/H\u221e[5]\u2013[8], and adversarially robust control [9]. However, these control strategies often settle upon a robustness level selected a priori, with no regard for the disturbance sequence observed during online operation. The resulting policies therefore lack the adaptivity that is required to take full advantage of dynamic environments. In the computer science literature, the design of sequential decision-making algorithms that learn from experience has traditionally been approached from the perspective of regret minimization [10], [11]. This framework encourages agents to dynamically adjust their strategy \u2013 based on information\ndeduced from previous rounds \u2013 by penalizing the loss relative to the optimal policy in hindsight. Regret-based methods offer attractive guarantees in terms of the performance of the best policy an agent could have counterfactually played, which hold independently of the stochastic or adversarial nature of the disturbances. While classical online learning theory has mostly considered memoryless environments (see, for instance, the rich literature on bandit problems [12]), recent years have witnessed increasing interest in applying modern statistical and algorithmic techniques to settings with dynamics. Initiated by [13], several learning algorithms have been proposed for adaptively controlling a linear dynamical system driven by a stochastic [14]\u2013[16] or adversarial [17]\u2013[20] disturbance process. The methods above attain sublinear regret against the classes of state-feedback and disturbance-action policies, that is, the average difference between the cost they incur and that of the best fixed strategy converges to zero over time. On the other hand, policy regret methods offer no guarantee that this static benchmark incurs a low control cost, leading to possibly loose performance certificates [21]. For instance, the authors of [22] have shown that no single static feedback controller can perform well in a scenario where the disturbances alternate between being drawn according to a well-behaved stochastic process and being chosen adversarially. Hence, any online algorithm that tries to learn the best fixed state-feedback law will also incur a high control cost. The drawbacks of policy regret methods have motivated recent literature to design algorithms that compete against a series of time-varying benchmarks [23]. Gradient-based methods that achieve low dynamic regret against the class of disturbance-action policies have been proposed, e.g., in [24] and [25]. A control law that exactly minimizes the loss relative to the globally optimal sequence of control actions in hindsight has been computed in [22], [26] via reductions to classical H\u221esynthesis and Nehari noncausal approximation problems [27]. Algorithms that compete against time-varying benchmarks have been shown to effectively interpolate between the performance of classical H2 and H\u221econtrollers in a variety of applications of interest, ranging from longitudinal motion control of a helicopter to control of a wind energy conversion system [28]. However, the resulting control policies cannot comply with the safety requirements of many practical applications due to a lack of provable robustness guarantees. With the aim of allowing a reliable deployment of regret minimization methods, in earlier work [29] we leveraged the system level synthesis (SLS) framework [30] to formulate safe regret-optimal control problems as semidefinite optimization programs; a similar approach was adopted in [31] to tackle the case where the exogenous perturbations satisfy instanta-\nneous ellipsoidal bounds. The poor scalability of semidefinite optimization, however, hinders the widespread application of the results in [29], [31]\u2013[34]; efficiently minimizing dynamic regret over an infinite horizon \u2013 while guaranteeing stability and safety of the closed-loop system \u2013 remains an open challenge. Contributions: Motivated by the ability of robust model predictive control (MPC) to handle constraints on the physical variables of a system [35], we propose a stabilizing receding horizon scheme based on the repeated computation of finite horizon safe regret-optimal feedback policies. Unlike classical architectures with H2 and H\u221eobjectives, static policies do not attain minimal dynamic regret [26] in this setting. Further, the cost-to-go function \u2013 commonly employed as a control Lyapunov function in proving asymptotic stability of the closedloop system [36] \u2013 cannot be computed as a quadratic function of the state only. To address the issues above, we shape the terminal ingredients based on an auxiliary H\u221epolicy that induces a quadratic upper bound on the regret to go. Moreover, to compete against the optimal benchmark along each planning horizon, we repeatedly compare with finite horizon clairvoyant policies. Yet, we establish regret guarantees of our receding horizon control law relative to the infinite horizon clairvoyant policy. As a result of our analysis, we show that the proposed strategy enjoys recursive feasibility and guarantees \u21132-stability of the closed-loop system. A main challenge has been that the same guarantees cannot be established by reducing the regret minimization to an \u21132-gain attenuation problem through the construction of an auxiliary system [22] combined with receding horizon H\u221econtrol [37], [38]. Indeed, the dynamics of the augmented system resulting from a series of Riccati recursions are time-varying in general, even if the dynamics of the original system are not [22]. Last, we present numerical experiments to assess the efficacy of dynamic regret as a receding horizon design criterion. Organization: Section II formalizes the competitive problem of interest, reviews the system level approach to controller synthesis, and recalls useful properties of the clairvoyant optimal policy. Section III studies regret minimization over a finite horizon, summarizing and extending the results of our previous work [29]. Section IV establishes novel monotonicity properties of the clairvoyant optimal policy, and presents conditions that guarantee recursive feasibility and bounded regret of the proposed receding horizon control scheme. Section V discusses efficient computational methods and collects our numerical results. Finally, Section VI summarizes the contributions and outlines directions for future research. Notation: We denote the sets of natural, integer and real numbers by N, Z and R, respectively. We use R\u22650 to denote the non-negative reals and I[a,b] to denote the set of integers in the interval [a, b] \u2286R. We use lower and upper case letters such as x and A to denote vectors and matrices, respectively, and lower and upper case boldface letters such as x and A to denote finite horizon signals and operators, respectively. We use calligraphic letters such as X to denote sets. We use I to denote the identity matrix when dimensions are\nclear from the context. We denote by \u03bbmax(A) and \u03bbmin(A) the maximum and minimum eigenvalue of the matrix A, respectively. We write A \u227b0 or A \u2ab00 if the symmetric matrix A is positive definite or positive semidefinite, respectively. Inequalities involving vectors are applied element-wise. We use \u2297to denote the Kronecker product. We use A[i,j] to denote the n \u00d7 n submatrix obtained extracting the (i, j) block entry of a block matrix A. Similarly, we write A[i1:i2,j] to denote the (i2\u2212i1+1)n\u00d7n submatrix obtained stacking horizontally the (i, j) block entries of a block matrix A, where the index i ranges from i1 to i2, inclusive.\nWe consider discrete-time linear time-invariant dynamical systems described by the state-space equations\nxt+1 = Axt + But + wt ,\n(1)\nwhere xt \u2208Rn, ut \u2208Rm, and wt \u2208Rn are the system state, the control input, and an exogenous signal, respectively. We do not make any assumptions about the statistical distribution of the disturbance process, and we let the realizations wt be drawn arbitrarily, potentially by an adversary, from a set W. As standard in the robust MPC literature [35], we assume that W is convex, compact and contains the origin in its interior. Our objective is to construct a stabilizing control policy with infinite horizon safety and performance guarantees. To do so, we repeatedly optimize the system behavior over a finite planning horizon of length T \u2208N, and only apply the first s \u2208I[1,T ] control moves in a receding horizon fashion. For compactness, we write the evolution of the system state and input trajectories over T as\nwhere Z denotes the block-downshift operator, that is, \uf8ee \uf8f9\nWe note that this choice ensures that minimum dynamic regret over T is attained in unconstrained scenarios [22], [26], and, as we will discuss in Section V, it also allows for policy optimization via convex programming techniques through a suitable re-parametrization [30], [39]. We then restrict our attention to control laws that are admissible, namely that guarantee compliance with the following safety constraints:\n(3)\nwhere Z \u2286Rn \u00d7 Rm and Zf \u2286Rn are closed convex sets that contain the origin in their interior. We denote the set of all admissible policies \u03c0T = (\u03c00, . . . , \u03c0T \u22121) of the form (2) as \u03a0T (x0). Similarly, we denote the set of initial conditions for which an admissible linear control policy exists as XT = {x \u2208Rn : \u03a0T (x) \u0338= \u2205}. For any possible realization of the disturbance sequence w, we measure the control cost that an admissible policy \u03c0T \u2208 \u03a0T (x0) incurs along the planning horizon by\n(4)\n\ufffd where Q \u2ab00 and R \u227b0 are the state and control stage cost matrices, respectively. Since (4) depends on yet unknown disturbance realizations, computing the cost-minimizing sequence of control actions given causal information becomes an illposed problem. To break this deadlock, classical H2 and H\u221e paradigms assume that w follows a known probability distribution and that w is selected adversarially, respectively [1]. Then, H2 methods optimize the average cost E[JT (\u03c0T , \u03b4)+Vf(xT )], where Vf(xT ) = x\u22a4 T PxT denotes a terminal cost weighted by P \u2ab00, whereas H\u221emethods minimize the performance level \u03b3 \u2208R\u22650 for which the bound\nwith \u03b2(x0, \u03b3) \u2208R\u22650, holds for all w with bounded energy. Instead, at each planning stage, we consider the objective of safely minimizing the worst-case loss relative to the clairvoyant optimal policy \u03c0c T . The latter is defined by\n\u03c0c T = argmin\u03c0\u2208\u03a0c T JT (\u03c0, \u03b4) = argminu(\u03b4) JT (u, \u03b4) , (6) where \u03a0c T denotes the set of all possibly nonlinear and clairvoyant (noncausal) control laws that could be designed if we had complete foreknowledge of the disturbance realizations. As such, policies in \u03a0c T yield an ideal, yet unattainable, closed-loop performance that we strive to reproduce by designing a causal policy. Specifically, given a performance level \u03b3 \u2208 R\u22650, let RegT (\u03c0T , \u03b3, \u03b4) be defined according to\nRegT (\u00b7) = JT (\u03c0T , \u03b4)\u2212JT (\u03c0c T , \u03b4)+Vf(xT )\u2212\u03b32 \u2225w\u22252 2 , (7) Then, we aim at synthesizing an admissible control policy \u03c0T \u2208\u03a0T (x0) and a non-negative real scalar \u03b2(x0, \u03b3) such that the dynamic regret bound\nRegT (\u03c0T , \u03b3, \u03b4) \u2264\u03b2(x0, \u03b3) ,\nholds for all disturbance sequences w \u2208W = WT = W \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 W. Note that, to synthesize a control policy whose dynamic regret has optimal dependence on the energy of the disturbance,1 we aim at fulfilling the regret bound in (8) with performance level \u03b3 \u2208R\u22650 as small as possible. Finally, we point out that the role of the terminal penalty Vf(xT ) will be to compute a quadratic upper bound to the regret-to-go, and to design, jointly with the terminal constraint set Zf, a receding horizon control law with guaranteed recursive feasibility and stability properties.\n# A. System Level Synthesis\nWe now briefly outline the necessary background on the SLS approach to optimal controller synthesis, and refer to [30] for a complete discussion. Akin to the Youla parametrization [40] and disturbance-feedback controllers [39], the SLS approach shifts the synthesis problem from directly designing the controller to shaping the closed-loop maps from the exogenous disturbance to the state and input signals [41]\u2013[43]. Along the planning horizon T, the behavior of the closedloop system under the feedback interconnection u = Kx in (2) can be described through the relations\nx = (I \u2212Z (A + BK))\u22121 \u03b4 := \u03a6x\u03b4 = \u03a60 xx0 + \u03a6w x w , (9) u = K\u03a6x\u03b4 := \u03a6u\u03b4 = \u03a60 ux0 + \u03a6w u w , (10)\n(10)\nwhere \u03a6x = col(\u03a60 x, \u03a6w x ) and \u03a6u = col(\u03a60 u, \u03a6w u ). By inspection, the expressions above are non-convex in K, yet they are linear in the closed-loop responses {\u03a6x, \u03a6u} that the controller K induces. Note also that these operators inherit a lower block-triangular causal structure from their definition. The SLS framework then shows that there exists a feedback controller K such that x = \u03a6x\u03b4 and u = \u03a6u\u03b4 if and only if the closed-loop maps lie in the affine subspace defined by\n(11)\npairs of closed-loop responses {\u03a6x, \u03a6u} that satisfy (11) are said to be achievable [30]. Based on (11), many optimal control problems of practical interest, including classical H2 and H\u221econtrol synthesis [1], can be equivalently posed as an optimization over the convex set of system responses. Thanks to convexity, the optimal closed-loop responses {\u03a6\u22c6 x, \u03a6\u22c6 u} can be computed efficiently, and the corresponding optimal control policy in the form of (2) can then be recovered by K\u22c6= \u03a6\u22c6 u\u03a6\u22c6 x \u22121. In particular, note that the structure that causality imposes on the closed-loop responses ensures that \u03a6x is always invertible. In light of this correspondence, throughout the paper we will interchangeably identify a control policy \u03c0T through the associated feedback gain matrix K or through the closed-loop responses {\u03a6x, \u03a6u} that it induces.\nIn the full-information setting we consider, there exists a unique noncausal control law that outperforms any other controller for every disturbance realization \u2013 the clairvoyant optimal policy \u03c0c T . We formalize this statement in the following lemma, which we prove in the appendix by adapting the derivations of [1], [29]. For compactness, we define Q = blkdiag(IT \u2297Q, 0) and R = IT \u2297R.\n   Lemma 1. For every disturbance realization \u03b4, the clairvoyant optimal policy in (6) is given by:\n(12)\n(13)\n\ufffd \ufffd\ufffd \ufffd Interestingly, the globally optimal clairvoyant control actions can be computed as a linear combination of past, present, and future disturbances [1]. As we have shown in [29, Lem. 2], the optimal control sequence in hindsight (12) can be equivalently computed by solving a quadratic SLS problem, i.e., an H2 problem, that imposes no causal constraints on the structure of closed-loop responses {\u03a6c x, \u03a6c u} associated with the clairvoyant policy \u03c0c T , i.e.,\n(14)\n\ufffd \ufffd Note that, besides allowing for efficient computation of optimal controllers, (14) can be used to include additional convex requirements, such as safety structural constraints, in the definition of the clairvoyant policy [29].\nIII. MINIMAL REGRET IN FINITE HORIZON CONTROL\nTowards establishing guarantees for regret-based receding horizon control, we first enable a tractable formulation of the optimal control problem that we will solve online. Building on our previous work [29], we exploit the system level parametrization of achievable closed-loop responses to characterize a safe regret-optimal policy at performance level \u03b3 as the solution of a convex optimization problem. To do so, we consider the following objective function for (\u03a6x, \u03a6u) satisfying (3) and (11):\n(15)\nwhere, with slight abuse of notation, we rewrite the regret RegT (\u03a6x, \u03a6u, \u03b3, \u03b4) defined in (7) as a function of the closedloop system responses as\nwith S = blkdiag(Q, R), \u00afQ = blkdiag(IT \u2297Q, P), and \u00afS = blkdiag( \u00afQ, R). In particular, note that (16) follows by combining (9) and (10) with (4). Further, observe that (16) is negatively weighted in the disturbance energy, as common in the classical literature on finite horizon H\u221econtrol [44], [45]. Crucially, however, the regret-optimal framework also introduces system-dependent optimal performance weights by subtracting the cost incurred by the clairvoyant optimal policy \u03c0c T . By maximizing over w in (15), we then penalize the worst-case loss relative to \u03c0c T . Intuitively, this metric forces our controller to closely track the performance of \u03c0c T whenever a low cost can be attained, while bearing a higher cost whenever the clairvoyant optimal policy \u03c0c T does so. Loosely speaking, minimizing regret induces adaptivity in the controllers by letting them know what disturbances are worth spending more control effort on. We now establish desirable properties of the objective function in (15). Then, we provide conditions that make the computation of a minimax control policy amenable to standard techniques in convex optimization. For ease of exposition, we defer the discussion on the numerical implementation to Section V. We first show that the worst-case dynamic regret objective Reg\u22c6 T (\u03a6x, \u03a6u, \u03b3, x0) is convex in the closed-loop system responses; the proof is detailed in Appendix B.\nAn admissible control policy that minimizes (15) can be efficiently computed if the function w \ufffd\u2192RegT (\u03a6x, \u03a6u, \u03b3, \u03b4) is concave. In this case, the optimization problem in (15) simply calls for the maximization of a concave function over the convex set W, and a minimax control policy can be expressed as the solution of a tractable convex-concave optimization problem. Let \u03a60 = col(\u03a60 x, \u03a60 u) and \u03a6w = col(\u03a6w x , \u03a6w u ). With this notation in place, we equivalently rewrite (16) as\n\ufffd \ufffd where CT is as in (13). Therefore, for a given performance level \u03b3 \u2208R\u22650, the desired concavity condition is met if there exists a pair of achievable closed-loop responses \u03a6x and \u03a6u that satisfy the quadratic matrix inequality\n(17)\nEquation (17) sets a lower bound on the best achievable performance level \u03b3 that we can attain while maintaining tractability. Note that (17) is necessary and sufficient for concavity if the perturbation w is an arbitrary disturbance sequence with bounded energy, compare also [29, Th. 3]. Instead, even though (17) is not necessary for particular choices of W as those considered in [31], in these cases only upper and lower bounds on the minimum performance\nlevel are currently available. Finally, we also note that (17) guarantees the existence of a saddle point solution for the considered minimax problem [46, Th. 2.3]. Applying the Schur complement to (17) leads to: \ufffd \ufffd\nFor a given initial condition x0 \u2208Rn and planning horizon T \u2208N, we denote the set of all admissible control policies that satisfy the linear matrix inequality (18) with performance level \u03b3 by \u03a0\u03b3 T (x0) \u2286\u03a0T (x0), and the set of initial conditions for which one such policy exists by\n(19)\nIn particular, we remark that, while x0 does not affect (18) directly, the value of the initial state plays a role in the definition of the set \u03a0\u03b3 T (x0) due to the presence of the mixed state and input constraints (3). Further, as at every state we are interested in satisfying (8) with as small \u03b3 as possible, we define the minimum gain function \u03b3\u22c6 T : XT \u2192R\u22650 as\n(20)\n  Then, for any x0 \u2208XT and any \u03b3 \u2265\u03b3\u22c6 T (x0), we consider the following tractable optimization problem:2\n(21)\n\u2208W subject to (11) , (17) ,\nwhere Z compactly denote the set of admissible state and input signals that satisfy (3) along the entire planning horizon. By design, the minimax control policy computed as a solution to (21) complies with the safety constraints (3) at all times, and drives the system to the terminal set Zf in face of the uncertain disturbance realizations. Further, the dynamic regret bound (8) holds with \u03b2(x0, \u03b3) equal to the value function V \u22c6 T (x0, \u03b3). IV. RECEDING HORIZON REGRET-OPTIMAL CONTROL In this section, we turn our attention to the infinite horizon control problem and present our main contributions. We show that, if the terminal ingredients Zf and Vf(xT ) are appropriately chosen, the receding horizon control law \u00b5s T that implements only the first s \u2208I[1,T ] control actions of a minimax policy in (21) is recursively feasible as per Theorem 1 and \u21132-stable. In particular, our method enjoys finite regret with respect to the infinite horizon clairvoyant optimal policy \u03c0c \u221eas per Theorem 2. In other words, if (21) is feasible for (x0, \u03b3), then the closed-loop system is guaranteed to satisfy the constraints at all times, and there exists \u03b3 \u2208R\u22650 and \u03b2 \u2208R\u22650 such that the bound\nIn this section, we turn our attention to the infinite horizon control problem and present our main contributions. We show that, if the terminal ingredients Zf and Vf(xT ) are appropriately chosen, the receding horizon control law \u00b5s T that implements only the first s \u2208I[1,T ] control actions of a minimax policy in (21) is recursively feasible as per Theorem 1 and \u21132-stable. In particular, our method enjoys finite regret with respect to the infinite horizon clairvoyant optimal policy \u03c0c \u221eas per Theorem 2. In other words, if (21) is feasible for (x0, \u03b3), then the closed-loop system is guaranteed to satisfy the constraints at all times, and there exists \u03b3 \u2208R\u22650 and \u03b2 \u2208R\u22650 such that the bound\n2Reformulations that enable efficient implementation via convex optimization techniques are discussed in Section V.\nholds for any infinite disturbance sequence w\u221ewith realizations wt \u2208W. We remark that, unlike in the H\u221econtrol case (5), the disturbance sequence w\u221edirectly affects the performance evaluation through the term J\u221e(\u03c0c \u221e, w\u221e) in (22) \u2013 thus teaching the policy how much effort to spend in trying to counteract each individual disturbance sequence w\u221e. To streamline the presentation of our results, we consider the standard implementation of MPC that corresponds to choosing s = 1, see also [35], [36]; analogous results hold for a general s \u2208IT . Hence, we formally define our receding horizon control law \u00b51 T = \u00b5T : XT \u00d7 R\u22650 \u2192Rm as:\n(23)\n(24)\nUnlike classical H2 and H\u221econtrollers, evaluating the infinite horizon regret value function requires aggregate information about the entire history of disturbance realizations. Hence, showing that the regret-to-go decreases along the trajectories of the closed-loop system (24) and can serve as a Lyapunov function is a challenging task. To get around this problem, Assumption 2 allows us to compute a quadratic upper bound on the infinite horizon value function that only depends on the current state. Once the system has reached the terminal safe set, Assumption 3 guarantees that ut = Kfxt is constraint admissible. Moreover, being Kf optimal in the H\u221e sense, the optimal regret cannot be higher than the worst-case cost incurred by this policy. As we will show, this choice ensures that the finite horizon regret-optimal computation is recursively feasible and stabilizing, at the cost of some conservatism in our theoretical performance bound.\nA. Monotonicity Properties of the Clairvoyant Optimal Policy To prove recursive feasibility of the receding horizon control law (23), we need to ensure that (17) recursively holds when moving from time T to T + 1. To do so, we first study how the cost incurred by \u03c0c T relates to that of \u03c0c T +1 \u2013 that is, the clairvoyant optimal policy over a horizon of T + 1 \u2013 when focusing on subsequences of length T.\nLemma 2. Let CT and CT +1 denote the matrices defining the cost of the clairvoyant optimal policies according to (13) for a planning horizon of length T and T + 1, respectively. Then, the following chain of matrix inequalities holds:\n(26)\nThe proof of Lemma 2 can be found in Appendix C. This result states that, for any \u03b4, the cost (13) incurred by \u03c0c T is bounded between the cost that \u03c0c T +1 incurs on the extended disturbance sequences col(0, \u03b4) and col(\u03b4, 0). We remark that non-causality and optimality for each disturbance sequence \u2013 properties that are unique to clairvoyant policies \u2013 are crucial to establishing (26). Indeed, when considering causal policies defined over different horizons, monotonicity properties as those in (26) can only be derived with respect to a single performance measure, such as the worst-case control cost [38]. Instead, (26) indicate stronger connections, which allow recovering monotonicity properties also known for causal policies as simple corollaries.\nCorollary 1. The clairvoyant worst-case cost function cmax : N \u2192R\u22650 defined as cmax(T) = \u03bbmax(CT ) is monotonically non-decreasing with respect to the planning horizon T. Similarly, the clairvoyant best-case cost function cmin : N \u2192 R\u22650 defined as cmin(T) = \u03bbmin(CT ) is monotonically nonincreasing with respect to the planning horizon T.\nA proof of Corollary 1 is given in Appendix D for completeness. The inequalities presented in this section are key to our subsequent derivations, as, by induction, they allow accounting for the mismatch between the finite horizon clairvoyant policy \u03c0c T , which we repeatedly adopt as control benchmark during online operation, and the infinite horizon clairvoyant policy \u03c0c \u221ewe compare against in (22).\n# B. Recursive Feasibility Properties\nInspired by the proof philosophy in [38] for receding horizon H\u221econtrol and leveraging Lemma 2 to bound the effect of introducing a clairvoyant control benchmark via regret minimization, we proceed to show that, under Assumptions 13, the optimization problem (21) for the closed-loop system (24) can be solved online at all times \u2013 assuming that it is feasible initially. To do so, we first prove that, for any \u03b3 \u2265\u03b3f, the sets X \u03b3 T defined in (19) are monotonically non-decreasing in T with respect to the set inclusion. To simplify the notation, we define:\nH\u03b3 T (\u03c0T ) = \u03b32I \u2212\u03a6\u22a4 w \u00afS\u03a6w \u2ab00 .\n(27)\nThis quadratic matrix inequality is reminiscent of (17) and guarantees concavity of the H\u221eobjective (5). Additionally, we observe that, by definition, it holds that:\nProposition 2. Let Assumptions 1-3 hold with performance level \u03b3f \u2264\u03b3. Then, the following set inclusion property holds:\nThis chain of set inclusions, which we prove in Appendix E, implies that, for any x \u2208XT , the minimum gain function \u03b3\u22c6 T (\u00b7) defined in (20) satisfies the inequality \u03b3\u22c6 T +1(x) \u2264 max{\u03b3f, \u03b3\u22c6 T (x)}. We continue by showing that the proposed receding horizon scheme is recursively feasible; we defer the proof to Appendix F.\nTheorem 1. Let Assumptions 1-3 hold. Then, for every performance level \u03b3 \u2265max{\u03b3\u22c6 T (x0), \u03b3f} and every T \u2208N, the set X \u03b3 T is robust positively invariant under the receding horizon control law (23), i.e., Ax + B\u00b5T (x, \u03b3) + w \u2208X \u03b3 T for all x \u2208X \u03b3 T and all w \u2208W.\n# C. Bounded Regret in Receding Horizon Control\nBuilding on the recursive feasibility properties established in the previous section, we now show that the proposed receding horizon control scheme with terminal ingredients derived from an auxiliary H\u221epolicy is safe, in the sense of (xt, ut) \u2208Z at all times, and achieves finite regret with respect to the infinite horizon clairvoyant optimal policy \u03c0c \u221e.\nThe proof of Theorem 2 is presented in Appendix G. We remark that, for any w\u221esuch that \u2225w\u221e\u22252 < \u221e, bounded dynamic regret as per (22) implies that the realized control cost J\u221e(\u00b5T , w\u221e) is also bounded. To see this, it is sufficient to note that the H\u221econtroller Kf guarantees bounded cost J\u221e(Kf, w\u221e), and that J\u221e(\u03c0c \u221e, w\u221e) \u2264J\u221e(Kf, w\u221e) by definition. We now turn our attention to stability of the closed-loop system (24) as a Corollary of Theorem 2. Corollary 2. Let Assumptions 1-3 hold with performance level \u03b3 \u2265max(\u03b3\u22c6 T (x0), \u03b3f). Then, the origin of the undisturbed closed-loop system xt+1 = Axt + B\u00b5T (xt, \u03b3) is locally exponentially stable with region of attraction X \u03b3 T . Moreover,\nif Q \u227b0, the closed-loop system (24) exhibits a finite \u21132 gain also from disturbances to state and input trajectories, that is:\nProof. Recalling from the proof of Theorem 2 that Vf(\u00b7) is a robust control Lyapunov function in a neighborhood of the origin, the result for the undisturbed system follows by resorting to classical results in MPC, see, for instance, [36, Sect. 3]. Finally, since J\u221e(\u00b5T , w\u221e) \u2265\u03bbmin(Q) \u2225x\u221e\u22252 2 + \u03bbmin(R) \u2225u\u221e\u22252 2 and both \u03bbmin(Q) and \u03bbmin(R) are strictly positive, we conclude that \u2225x\u221e\u22252 2 and \u2225u\u221e\u22252 2 are themselves bounded. Hence, (28) follows.\nThe interplay between regret and classical definitions of stability has been recently studied in [47] in the context of linear dynamical systems subject to adversarial disturbances, and in [48] for deterministic nonlinear systems. Our analysis takes another step towards understanding the relationship between these notions by showing that implementing a regretoptimal control policy in a receding horizon fashion does not compromise the stability of the closed-loop system \u2013 despite the significantly higher performance that can be achieved when disturbances do not follow classical assumptions.\nV. NUMERICAL EXPERIMENTS\nIn this section, we first discuss how the minimax policy optimization problem (21) can be efficiently solved with convex programming techniques. Then, we present numerical results to show how the proposed receding horizon control scheme retains the ability of safe finite horizon regret-optimal policy to interpolate between, or even prevail over, the performance of constrained H2 and H\u221econtrollers [29], [31]. In particular, our numerical experiments showcase that regret-optimal policies can achieve superior control performance despite less frequent solution of the policy optimization problem during online operation.\n# A. Numerically Efficient Implementation\nFor simplicity, we assume that the disturbance set W is a polytope. Then, there exists a matrix Hw and a vector hw such that W = {w : Hww \u2264hw}; we refer the interested reader to [49, Ch. 6] for insights on how to generalize our results to disturbance sets characterized by affine conic inequalities. Similarly, we assume that Z = {z : Hzz \u2264hz}. The following proposition presents an equivalent convex reformulation of the optimal control problem of interest. To ease readability, we defer all proofs to the appendix.\nProposition 3. Consider the evolution of the linear timeinvariant system (1) over a planning horizon of length T \u2208N,\nand the constrained regret-optimal control problem at performance level \u03b3:\n(29a)\nsubject to (11) , (17) , \ufffd \ufffd\ufffd\n(29c)\nThen, (29) is equivalently formulated as the following convex optimization problem:\n(30a)\nsubject to (11) , (29c) , \u03b7 \u22650 , Yij \u22650 ,\n(30b) (30c) (30d)\nwhere \u0398(\u03a6x, \u03a6u, \u03b3, x0) is defined as\n\uf8f0 \uf8fb Note that the semidefinite optimization problem (30) can be efficiently solved using off-the-shelf interior point methods. Moreover, given an optimal solution {\u03a6\u22c6 x, \u03a6\u22c6 u} to (30), the corresponding optimal state-feedback controller can then be reconstructed by K\u22c6= \u03a6\u22c6 u\u03a6\u22c6 x \u22121. Independently of how the disturbances are generated, this optimal safe control policy guarantees that the dynamic regret bound (8) holds with performance level \u03b3. Furthermore, the receding horizon control law that results by repeatedly implementing the first s \u2208I[1,T ] optimal control actions is recursively feasible and satisfies (22) with performance level not greater than max(\u03b3, \u03b3f). To ensure that the regret-optimal policy synthesized at each planning stage through (30) achieves the optimal performance level \u03b3\u22c6 T (xt), one can search over \u03b3 through bisection and solve several corresponding instances of (30). Clearly, this procedure comes with an increased computational cost. To mitigate this aspect, one can further assume that the energy of w is bounded by a constant \u03c3 \u2013 while its specific realization remains arbitrary. Using this assumption, our next proposition suggests a convex reformulation in the line of [31] that jointly synthesizes a regret-optimal policy and an optimal performance level. In the interest of clarity, we point out that our main theoretical results hold irrespective of the bounded energy assumption on w over the interval [0, T \u22121]. Proposition 4. Consider the evolution of the linear timeinvariant system (1) over a planning horizon of length T \u2208N, and the constrained regret-optimal control problem:\nmin \u03a6x,\u03a6u max \u2225w\u22252\u2264\u03c3 JT (\u03c0T , \u03b4) \u2212JT (\u03c0c T , \u03b4) + Vf(xT ) (31 subject to (11) , (17) , (29b) , (29c) .\n(31)\nThen, (31) is equivalent to the following convex optimization problem:\n(32a)\nsubject to (11) , (30b) , (30c) , (29c) ,\n(32b)\nwhere \u039e(\u03a6x, \u03a6u, \u03b3, x0) is defined as\n\uf8f0 B. Numerical Results\nWe consider a linear time-invariant system (1) with\n\uf8f0 \uf8fb \uf8f0 \uf8fb with spectral radius \u03c1 = 0.7.3 We randomly select the initial condition x0 = \ufffd\u22123.08 1.22 \u22120.62\ufffd\u22a4. Further, we define the safe set Z = {(x, u) \u2208R5 : \u2225x\u2225\u221e\u22643.5 , \u2225u\u2225\u221e\u22642}, and we enforce robust constraint satisfaction for all disturbance realizations taking values in W = {w \u2208R3 : \u22121 \u2264\u2225w\u2225\u221e\u2264 1}. We consider a planning horizon of length T = 20 and a simulation of length N = 60. Last, we set the weight matrices to Q = I3 and R = I2 for all experiments. Next, we synthesize different stabilizing receding horizon control policies following the H2, H\u221eand regret minimization philosophies; we discuss implementation details below. 1) Schemes based on H2 and H\u221eobjectives: We compute terminal cost matrices P2 and P\u221efor the expected and worst-case objectives, respectively, by solving the sign-definite algebraic Riccati equation\nand the sign-indefinite algebraic Riccati equation (25), where we iteratively determine the optimal value of \u03b3f via bisection with a tolerance of 0.001. From these two solutions, we derive auxiliary H2 and H\u221eoptimal controllers as K2 = (R + B\u22a4PB)\u22121B\u22a4P2A and K\u221e= \u2212(R + B\u22a4\u00afP\u221eB)\u22121B\u22a4P\u221eA, where \u00afP\u221e= P\u221e+ P\u221e(\u03b32 fI \u2212P\u221e)\u22121P\u221ein accordance to Assumption 2. Concerning the design of the terminal sets Xf,2 and Xf,\u221e, we endow each scheme with a robust positively invariant polyhedral outer-approximation of the minimal robust positively invariant set under the corresponding local controller K2 or K\u221e. To do so, we follow Algorithm 1 in [50], with approximation parameter \u03f5 = 0.5. Importantly, we verify numerically that Xf,2 and Xf,\u221econstitute valid terminal sets, since each of them is constraint admissible under the associated local controller as required by Assumption 3.\nWe remark that, by construction, the receding horizon control laws with H2 and H\u221eobjectives are recursively feasible and render the closed-loop system input-to-state and \u21132 stable, respectively [49, Th. 4.20, Th. 5.15]. 2) Schemes based on regret minimization: In line with Assumptions 2-3, we employ the terminal ingredients P\u221eand Xf,\u221ederived from the auxiliary H\u221econtrol policy previously computed. Moreover, to evaluate the term CT that appears in (32), we first compute the closed-loop responses associated with the finite horizon clairvoyant optimal policy \u03c0c T by solving the quadratic optimization problem (14). In (32b), we use \u03c3 = 1. 3) Performance comparison: We now compare the average control cost incurred by the receding horizon H2, H\u221e, and regret-optimal (R) receding horizon safe control policies. We draw the disturbance realizations according to a variety of stochastic and deterministic profiles, including a scaled Gaussian distribution and moving average sinusoidal signals. For each of these scenarios, we simulate the closed-loop system behavior when the number s of control actions applied before the optimization is repeated varies from 1 to T. We collect our results in Table I;4 note that we only report a single column for the receding horizon scheme with H2 objective, as we observe that the solution rapidly converges to the unconstrained optimal controller K2. We verify that, as predicted by [49, Th. 4.20, Prop. 5.14] for receding horizon H2 and H\u221econtrol, respectively, and by Theorem 1 for the case of regret minimization, all predictive control schemes are recursively feasible. Also, we observe numerically that Xf,\u221e\u2282Xf,2, meaning that the receding horizon H2 policy yields the largest feasible set and domain of attraction. Regarding performance, we observe that using an H2 objective leads to the lowest control costs when the disturbance follows a Gaussian distribution. This is expected since this is precisely the class of disturbances that H2 controllers are designed to counteract. Instead, despite having imposed more challenging terminal constraints, minimizing regret can yield superior performance for other practically relevant disturbance profiles, such as piece-wise constant or sinusoidal signals, that poorly fit classical design assumptions. We found that an advantage of regret-optimal policies facing non-stochastic disturbances is that a consistenly lower control cost is attained despite optimizing less frequently, i.e., when s \u2208I[1,T ] is large. This result may appear counter-intuitive at first; the explanation lies in the inherent capability of regret-optimal policies to adapt to heterogeneous disturbance sequences. Indeed, unlike H2/H\u221e-optimal policies, regretoptimal policies exploit the full history of signals even in the unconstrained case in order to favour adaptivity. In the spirit of event-triggered predictive control [51], we hope that regretoptimal predictive control will especially thrive in applications where communication rates or energy consumption are limited,\nTABLE I: Performance comparison: average control cost per unit of disturbance energy incurred by the H2, H\u221e, and regret-optimal (R) receding horizon safe control policy over 10 different disturbance realizations. For each disturbance profile, the minimum cost incurred by any of these policies is highlighted in green.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2165/2165d333-84ee-4d4a-a668-09eb7c5e7dcf.png\" style=\"width: 50%;\"></div>\nrequiring a significant reduction of the frequency at which the planning optimization problem has to be solved.\nOn the other extreme, when s = 1, we numerically observe that the performance matches that of a receding horizon architecture with H2 objective. This consideration is consistent with the result of [26, Th. 4], which shows that unconstrained regret-optimal policies have a state-feedback law of the H2 controller plus a correction term obtained by linearly combining the observed disturbance realizations. By optimizing at each time-step without storing additional information about the entire sequence of observed disturbance realizations other than that provided by the state vector at the current time instant, the above-mentioned correction term becomes zero. On the other hand, introducing memory can prove challenging as the state augmentation procedure proposed in [22] to solve a finite horizon regret-optimal control problem yields timevarying dynamics governed by a series of Riccati recursions, and we leave this extension as an interesting direction for future research. 4) Robustness of performance comparison: To confirm the robustness of our conclusions, we perform a second set of experiments by focusing on a family of sinusoidal disturbance signals wt. In particular, for i \u2208I3, we construct each individual disturbance component wi t = sin(\u03c9t + \u03c6) by choosing 10 different values for the angular frequency \u03c9 and the phase \u03c6 equally spaced in the intervals [ 3\u03c0 N , 12\u03c0 N ] = [0.1571, 0.6283] and [0, 2\u03c0], respectively. We plot the results obtained by simulating the closed-loop system behavior in Figure 1. We visualize the standard deviation of the energy-normalized average control cost \u00afJ across different angular frequencies or phases by means of shaded tubes centered around the mean values. We observe that changing the angular frequency impacts the realized closed-loop performance more, yet our receding horizon control scheme based on regret minimization can\noutperform classical architectures consistently across nearly all sinusoidal disturbances. Figure 1 further validates that the adaptation capabilities inherent to regret-optimal policies can prove key when the disturbances are non-stochastic. In fact, as s decreases, we note that our scheme again approaches the performance of the receding horizon H2 control law, while the best performance are attained by adopting an H\u221eobjective.\nVI. CONCLUSION\nWe studied control of constrained linear systems with bounded additive disturbances from the perspective of receding horizon regret minimization. By imposing terminal constraints derived from an auxiliary unconstrained H\u221econtrol law, we have formally shown that repeatedly minimizing dynamic regret guarantees recursive feasibility of the predictive control scheme and \u21132 stability of the resulting closed-loop system. In the process of establishing these theoretical certificates, we have derived novel monotonicity properties of the clairvoyant optimal policy; an interesting direction for future research is to combine these with a precise analysis of the regretto-go at each time-step to derive tighter infinite horizon regret guarantees. Additionally, future work includes developing model-free solutions, addressing computational complexity challenges for real-time implementation, deriving analogous results for estimation, and studying the interplay between different competitive metrics for nonlinear systems. bibliographystyleIEEEtran\n# REFERENCES\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/093c/093c0815-01bb-45b0-97de-87265f8a7398.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f748/f7481946-787f-4cee-b6ab-cf525d3d9438.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Effect of varying the phase \u03c6 in [0, 2\u03c0].</div>\nFig. 1: Normalized control cost incurred by the H2, H\u221e, and regret-optimal (R) receding horizon safe control poli correspondence with sinusoidal disturbance profiles with different angular frequencies and phases: shaded areas den standard deviations around mean values.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7dfc/7dfc7cb5-0869-4358-b59b-6e24b1a95493.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Effect of varying the angular frequency \u03c9 in [ 3\u03c0 N , 12\u03c0 N ].</div>\n# APPENDIX\n# A. Proof of Lemma 1\nRecall that F = (I \u2212ZA)\u22121ZB and G = (I \u2212ZA)\u22121 denote the causal response operators that encode the system dynamics as x = Fu+G\u03b4. Then, the control cost an arbitrary decision policy \u03c0T incurs over T steps can be expressed as\nwhere P = R + F\u22a4QF \u227b0. Further, completing the square by observing that\nthanks to the Woodbury matrix identity, the cumulative contro cost in (33) can be equivalently rewritten as\n(34)\nWe then note that the first addend in (34) is non-negative, as P \u227b0 by construction, and that the second term does not depend on the control actions that the decision policy \u03c0T\nselects. Hence, in the absence of safety constraints, the costminimizing control sequence is obtained by setting the former term is equal to zero. The expression (12) for \u03c0c T is then obtained by direct computation. Further, (13) is simply the second addend in (34), which directly yields the control cost of the policy \u03c0c T incurs when the disturbance is \u03b4.\n# B. Proof of Proposition 1\nFor any fixed performance level \u03b3 \u2208 R\u22650 and perturbation sequence w \u2208W, the function (\u03a6x, \u03a6u) \ufffd\u2192 RegT (\u03a6x, \u03a6u, \u03b3, \u03b4) is convex and continuous by inspection, see also [29]. Convexity and lower semicontinuity of the function (\u03a6x, \u03a6u) \ufffd\u2192Reg\u22c6 T (\u03a6x, \u03a6u, \u03b3, x0) then follow, since the point-wise maximum of lower semicontinuous and convex functions is lower semicontinuous and convex [52, Prop. 2.9]. To prove that this function is bounded from below, we recall that Vf(xT ) \u22650 for any xT since P \u2ab00 by definition, and we note that the optimality of the clairvoyant policy \u03c0c T implies that JT (\u03c0T , \u03b4) \u2212JT (\u03c0c T , \u03b4) \u22650 for any policy \u03c0T and disturbance w. Hence, we have that\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd Further, since W contains an open neighborhood of the origin by assumption, we have that\nReg\u22c6 T (\u03a6x, \u03a6u, \u03b3, x0) \u2265RegT (\u03a6x, \u03a6u, \u03b3, x0, 0) \u22650 ,\nwhich concludes the proof of the lower bound. Finally, (15) is also proper, i.e., it only takes finite values for every {\u03a6x, \u03a6u}, since the set W is assumed compact.\n# C. Proof of Lemma 2\nWe first consider the left inequality C[1:T +1,1:T +1] T +1 \u2aafCT . We equivalently rewrite the cost that the clairvoyant policy \u03c0c T +1 incurs over a planning horizon of length T + 1 as\nwhere \u00afw = col(w0, w1, . . . , wT ) collects the realizations of the disturbance process along the extended planning horizon. By inspection, it follows that the addend \u00afw\u22a4C[1:T +1,1:T +1] T +1 \u00afw constitutes the entire cumulative cost incurred by the clairvoyant policy \u03c0c T +1 if the system is initialized at x0 = 0, i.e.,\nOn the other hand, for any \u00afw, we also have that \u00afw\u22a4CT \u00afw = JT (\u03c0c T , w0, w1:T ), i.e., the cost incurred by the clairvoyant policy \u03c0c T over a planning horizon of length T when the system is initialized at x1 = w0 and the disturbance sequence w1:T realizes. Then, to conclude the first part of the proof, we show that\n(35)\nfor any \u00afw. To do so, let us denote by u\u22c6 1:T the sequence of clairvoyant control actions selected in hindsight by the policy \u03c0c T to minimize the left-hand side of (35). Also, let x\u22c6 1:T +1\nbe the corresponding state trajectory, initialized at x\u22c6 1 = w0 Then, the clairvoyant control sequence defined by\n(36)\ncan be implemented by clairvoyant policy \u03c0c T +1, which also has complete foreknowledge of the disturbance realizations w0:T , and results, when the system is initialized at x0 = 0 under \u00afw, in the state trajectory\n(37)\nIt is then straightforward to see that (36) and (37) imply that\nnamely that, when x0 = 0, the feasible control sequence (36) attains over T + 1 a cost equal to JT (\u03c0c T , w0, w1:T ). The left inequality in (26) then follows by suboptimality of \u00afu0:T with respect to \u03c0c T +1(col(0, \u00afw)). We next consider the right inequality CT \u2aafC[0:T,0:T ] T +1 . We start by rewriting the cost incurred over T + 1 by the clairvoyant policy \u03c0c T +1 as\nwhere \u00af\u03b4 = col(x0, w0, . . . , wT \u22121) now collects the system initial condition and all disturbance realizations but the last one. By inspection, whenever wT = 0, the previous expression simplifies to \u00af\u03b4\u22a4C[0:T,0:T ] T +1 \u00af\u03b4, namely\nOn the other hand, for any \u00af\u03b4, we also have that \u00af\u03b4\u22a4CT\u00af\u03b4 = JT (\u03c0c T , x0, w0:T \u22121), i.e., the cost incurred by the clairvoyant policy \u03c0c T over a planning horizon of length T when the system is initialized at x0 \u2208Rn and the disturbance sequence w0:T \u22121 realizes. To conclude the second part of the proof, we show that\n(38a) (38b)\nfor any possible choices of \u00af\u03b4 and wT , and in particular for the case of interest wT = 0. We start by noting that, since the dynamics of the system under control are causal, the cost that any policy incurs during the planning horizon T does not explicitly depend on wT \u2013 see also (34), which depends on w0:T \u22121 but not on wT . We then deduce that the globally optimal sequence of control actions that minimizes (34) also does not depend on wT . Hence, (38a) follows by optimality of \u03c0c T , since knowing the realization of wT in hindsight does not influence the cost-minimizing sequence of control inputs. Finally, the inequality (38b) follows, since the linear quadratic stage cost x\u22a4 T QxT + u\u22a4 T RuT is non-negative by definition.\n# D. Proof of Corollary 1\nLet the plant under control evolve under the clairvoyant optimal policy \u03c0c T , and let \u00af\u03b4 = col(x0, w0, . . . , wT \u22121) denote the disturbance sequence with unit norm that results in the highest control cost, i.e., \u00af\u03b4 = argmax\u2225\u03b4\u22252=1 \u03b4\u22a4CT \u03b4 and \u00af\u03b4\u22a4CT\u00af\u03b4 = \u03bbmax(CT ). Consider now the unitary norm extended perturbation sequence \u02dc\u03b4 defined by \u02dc\u03b4 = col(\u00af\u03b4, 0). Since CT \u2aafC[0:T,0:T ] T +1 in light of Lemma 2, we have that \u03bbmax(CT ) = \u00af\u03b4\u22a4CT\u00af\u03b4 \u2264\u02dc\u03b4\u22a4CT +1\u02dc\u03b4. By suboptimality of \u02dc\u03b4 with respect to argmax\u2225\u03b4\u22252=1 \u03b4\u22a4CT +1\u03b4, we also have that \u02dc\u03b4\u22a4CT +1\u02dc\u03b4 \u2264\u03bbmax(CT +1), and the claimed monotonicity property follows. The second part of the proof regarding the minimum eigenvalue follows similar derivations, and is thus omitted.\n# E. Proof of Proposition 2\nWe prove this chain of set inclusions by induction, and consider the base case Zf \u2286X \u03b3 1 first. For any x0 \u2208Zf, the local control law u0 = Kfx0 can be used to safely maintain the system inside the terminal set while satisfying a quadratic matrix inequality H\u03b3 1(Kf) \u2ab00 of the form (27), as shown in [38, Prop. 3]. Hence, since C[1,1] 1 \u2ab00, we conclude that the same candidate policy satisfies a quadratic matrix inequality R\u03b3 1(Kf) \u2ab00 of the form (17). For the induction step, we show that an arbitrary initial condition x0 \u2208X \u03b3 T satisfies x0 \u2208X \u03b3 T +1. By definition, for any x0 \u2208X \u03b3 T , there exists an admissible control policy at performance level \u03b3 \u03c0T \u2208\u03a0\u03b3 T (x0, \u03b3) that drives the system to a terminal state xT \u2208Zf while satisfying (xt, ut) \u2208Z along the way. If Assumptions 1-3 hold with \u03b3f \u2264\u03b3, then an admissible control \u03c0T +1 \u2208\u03a0T +1(x0) can be constructed leveraging the admissible and robust positively invariant local controller Kf, namely, appending the input uT = \u03c0T (x0, . . . , xT ) = KfxT to the control sequence in \u03c0T . It only remains to show that this proposed candidate policy \u03c0T +1 also satisfies a quadratic matrix inequality of the form (17) with the same performance level \u03b3, i.e., that \u03c0T +1 \u2208\u03a0\u03b3 T +1(x0). To do so, in light of the equivalence between SLS and disturbance feedback parametrizations [41], we recall from [38, Prop. 3] that H\u03b3 T +1(\u03c0T +1), the H\u221e-type quadratic form associated with the proposed candidate policy \u03c0T +1, can be related to H\u03b3 T (\u03c0T ) through\nwhere Af = A + BKf, for appropriately defined matrices X and Y. Observing that, by definition, R\u03b3 T +1(\u03c0T +1) = H\u03b3 T +1(\u03c0T +1) + C[1:T +1,1:T +1] T +1 , we rewrite R\u03b3 T +1(\u03c0T +1) as \ufffd \ufffd\nTherefore, we deduce that R\u03b3 T +1(\u03c0T +1) and R\u03b3 T (\u03c0T ) can be related through\n(39)\nwhere \u2206i = C[1:T +1,1:T +1] T +1 \u2212blkdiag(C[1:T,1:T ] T , 0n\u00d7n) captures the clairvoyant baseline shift that occurs as planning horizons of different length are considered. Proceeding analogously to [38, Prop. 3], one can show that R\u03b3 T (\u03c0T ) \u2ab00 and \u03b32I \u2212P \u227b0, which hold true by assumption, imply that the first addend in (39) is positive semidefinite. Hence, it suffices to show that \u2206i \u2ab00 to conclude that R\u03b3 T +1(\u03c0T +1) \u2ab00. To see this, we observe that for any perturbation sequence w0:T , the quadratic form \u00afw\u22a4\u2206i \u00afw, with \u00afw = col(w0, . . . , wT ), can be equivalently computed as JT +1(\u03c0c T +1, w0, w1:T ) \u2212JT (\u03c0c T , w0, w1:T \u22121). The positive semidefiniteness of \u2206i then follows from (38) in Lemma 2, and the proof is completed.\n# F. Proof of Theorem 1\nIf Assumptions 1-3 hold with performance level \u03b3 \u2265 max{\u03b3\u22c6 T (x0), \u03b3f}, then Proposition 2 guarantees that, for any initial condition x0 \u2208X \u03b3 T , there exists an admissible control policy \u03c0T +1 \u2208\u03a0\u03b3 T +1(x0) \u2286\u03a0T +1(x0) with first control action \u00b5T (x0, \u03b3). Let x1 = Ax0 + B\u00b5T (x0, \u03b3) + w0, and define \u03c0T as the tail of length T extracted from the control policy \u03c0T +1. By construction, \u03c0T \u2208\u03a0T (x1); it thus only remains to show that \u03c0T \u2208\u03a0\u03b3 T (x1), i.e., the performance level \u03b3 is met. To do so, we recall from [38, Lem. 1] that H\u03b3 T (\u03c0T ) in (27) can be related to H\u03b3 T +1(\u03c0T +1) by\n(40)\nfor appropriately defined matrices U and V. We then deduce that R\u03b3 T (\u03c0T ) in (17) and R\u03b3 T +1(\u03c0T +1) can be related by\n(41)\nwhere \u2206f = C[1:T +1,1:T +1] T +1 \u2212blkdiag(0n\u00d7n, C[1:T,1:T ] T ) captures the clairvoyant baseline shift that occurs as planning horizons of different length are considered. By partitioning \u2206f into blocks of appropriate dimensions, we equivalently rewrite (41) as\nwhere \u00afV = V + C[1,2:T +1] T +1 . Note that Lemma 2 implies that the second addend in the expression above is negative semidefinite, since C[2:T +1,2:T +1] T +1 \u2212C[1:T,1:T ] T \u2aaf0. Therefore, we deduce that\nFinally, since all principal sub-matrices of a positive semidefinite matrix are positive semidefinite, we conclude that R\u03b3 T (\u03c0T ) \u2ab00, i.e., that \u03c0T \u2208\u03a0\u03b3 T (x1).\nG. Proof of Theorem 2 For any real-valued scalar map \u03c6 : Rn \u2192R, we define the one-step difference function \u2206\u03c6 : R2n+m \u2192R as\nFurther, inspired by classical literature on H\u221econtrol, we le\n    with \u2225xt\u22252 Q = x\u22a4 t Qxt and \u2225ut\u22252 R = u\u22a4 t Rut, denote a quadratic stage loss that is negatively weighted in the energy of the disturbance realization. With this notation in place, the stage loss that measures the regret relative to the infinite horizon clairvoyant optimal policy \u03c0c \u221ebecomes\n(42)\n  where uc t and xc t denote the clairvoyant optimal control action at time t and the corresponding state value, respectively. If Assumptions 1-3 hold, the auxiliary feedback policy Kf, which is a saddle point solution of an unconstrained H\u221ezerosum game, satisfies [46]:\n(43)\nNote that, for all (xt, ut, wt), we have that \u2113r(xt, ut, wt) \u2264 \u2113(xt, ut, wt) \u2264 \u2113f(xt, ut, wt), where the first inequality follows because the stage cost incurred by the clairvoyant optimal policy is non-negative, and the second because \u03b3 \u2265 max{\u03b3\u22c6 T (x0), \u03b3f} \u2265\u03b3f by assumption. Hence,\nfor all (xt, ut, wt). In particular, choosing ut = Kfxt as in (43) while restricting the domain of the maximization over the compact set W \u2282Rn only, we have that\nfor any xt and for any wt, which in turn implies that Vf is a robust control Lyapunov function in a neighborhood of the origin. Following standard arguments in robust receding horizon control [36, Sect. 4], this fact, together with the assumed robust invariance of Zf under the local control law ut = Kfxt, ensures that\n(44)\nfor all xt \u2208X \u03b3 T and wt \u2208W. Taking the sum of the left-hand side of (44) from time 0 to a generic time q \u2208N\n\ufffd Recalling the definition of \u2113r(\u00b7) in (42) and since V \u22c6 T (xq, \u03b3) is non-negative from Proposition 1, it then follows that\nfor any q \u2208N, i.e., the amplification from the disturbance energy to the dynamic regret is finite and, in particular, is bounded above by \u03b3. Lastly, Theorem 1 ensures that the proposed receding horizon control law is recursively feasible, hence the constraints (xt, ut) \u2208Z are satisfied at all times.\nLet M = C[0,0] T \u2212\u03a6\u22a4 0 \u00afS\u03a60, N = C[0,1:T ] T \u2212\u03a6\u22a4 0 \u00afS\u03a6w, and L = \u03b32I + C[1:T,1:T ] T \u2212\u03a6\u22a4 w \u00afS\u03a6w so that\nRegT (\u00b7) = \u2212 \ufffd x\u22a4 0 Mx0 + 2x\u22a4 0 Nw + w\u22a4Lw \ufffd , Reg\u22c6 T (\u00b7) = \u2212min w\u2208W x\u22a4 0 Mx0 + 2x\u22a4 0 Nw + w\u22a4Lw .\nWe first observe that strong duality holds since W contains the origin in its interior by assumption. Therefore, we can equivalently rewrite Reg\u22c6 T (\u03a6x, \u03a6u, \u03b3, x0) as the optimal value of the following optimization problem\n\u2212max \u03bb\u22650 min w x\u22a4 0 Mx0 + 2x\u22a4 0 Nw + w\u22a4Lw\nor, equivalently, as the optimal value of\n+ min w (2N\u22a4x0 + H\u22a4 w\u03bb)\u22a4w + w\u22a4Lw ,\nto highlight the presence of an unconstrained minimization over the disturbance sequence w. Since (17) ensures that L \u2ab00, we can then write the explicit solution to the inner minimization following [53, Sec. A.5.5]. In particular, it holds that minw (2N\u22a4x0 + H\u22a4 w\u03bb)\u22a4w + w\u22a4Lw equals \u22121 4(2N\u22a4x0 + H\u22a4 w\u03bb)\u22a4L\u2020(2N\u22a4x0 + H\u22a4 w\u03bb) if the range condition\n(45)\nwhere L\u2020 denotes the pseudo-inverse of L, holds; otherwise, the problem is unbounded. By directly enforcing (45) as constraint and introducing the auxiliary epigraph variable \u03c4, we can then express Reg\u22c6 T (\u03a6x, \u03a6u, \u03b3, x0) as\nLetting \u03b7 = 1 2\u03bb and applying the Schur complement, one can reformulate the worst-case regret as\nRecalling the definition of M, N, and L, we rewrite the above matrix inequality constraint as\nwhere O = \ufffd\u00afS 1 2 \u03a60x0 \u00afS 1 2 \u03a6w \ufffd . The desired expression (30d) then follows by applying once more the Schur complement. In particular, note that if (30d) is satisfied, then (17) and (18) are guaranteed to hold since all principle sub-matrices of a positive semidefinite matrix are positive semidefinite. To ensure that the safety constraints are robustly satisfied, we proceed as in [29] and employ strong duality of linear optimization problems to eliminate the universal quantifier from (29b). Specifically, we first note that (29b) holds if and only if Hz\u03a6ww \u2264hz \u2212Hz\u03a60x0 for all w \u2208W. Then, we observe that\nwhere yi denotes the dual vector corresponding with the ith row of the maximization. From this dual reformulation, the set of linear constraints (30b)-(30c) can be derived by concatenating the dual variables that arise from each row of the maximization into the matrix Y.\n# I. Proof of Proposition 4\nLet \u02dcL = C[1:T,1:T ] T \u2212\u03a6\u22a4 w \u00afS\u03a6w so that the inner maximization problem in (31) equivalently reads as\nThen, since w = 0 verifies the Slater\u2019s constraint qualification condition, strong duality holds [53, App. B]. Hence, by taking the dual we rewrite this optimization problem as\nRecalling the definition of M, N, and \u00afL, we finally express the above matrix inequality constraint as\nfrom which the constraint (32b) can be derived by directly applying the Schur complement.\nfrom which the constraint (32b) can be derived by directly applying the Schur complement.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of bridging classical optimal control and online learning through regret minimization, highlighting the limitations of existing control methods that are not adaptable to dynamic environments.",
        "problem": {
            "definition": "The problem focuses on the challenge of optimizing control strategies in the face of bounded additive disturbances without prior knowledge of their statistical distribution.",
            "key obstacle": "Existing control methods often lack the adaptivity needed to respond effectively to disturbances that do not conform to their modeling assumptions."
        },
        "idea": {
            "intuition": "The idea stems from the need to create control policies that can dynamically adjust based on the observed disturbances rather than relying on fixed strategies.",
            "opinion": "The proposed method introduces a receding horizon scheme that computes finite horizon regret-optimal policies to improve performance in uncertain environments.",
            "innovation": "This method differs from traditional approaches by leveraging monotonicity properties of clairvoyant policies and establishing guarantees for stability and safety in the closed-loop system."
        },
        "method": {
            "method name": "Receding Horizon Regret Minimization",
            "method abbreviation": "RHRM",
            "method definition": "A control policy that repeatedly computes finite horizon regret-optimal feedback strategies while ensuring recursive feasibility and stability.",
            "method description": "The method iteratively optimizes the control actions over a finite horizon to minimize regret relative to an optimal benchmark.",
            "method steps": [
                "Define the planning horizon and initialize the system state.",
                "Compute the optimal control actions based on the current state and disturbances.",
                "Apply the first control action and update the state.",
                "Recompute the optimal actions at the next time step."
            ],
            "principle": "The method is effective because it allows for continuous adaptation to the disturbances, ensuring that the control actions are aligned with the optimal performance benchmarks."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on a linear time-invariant system with various disturbance profiles, including Gaussian and sinusoidal signals, across different planning horizons.",
            "evaluation method": "Performance was assessed by comparing the average control cost incurred by the proposed method against traditional H2 and H\u221e control strategies under the same disturbance conditions."
        },
        "conclusion": "The proposed receding horizon regret minimization method demonstrates improved performance and stability in controlling systems with bounded disturbances, providing a robust alternative to classical control approaches.",
        "discussion": {
            "advantage": "The key advantages include superior adaptivity to dynamic disturbances and the ability to achieve lower control costs in scenarios that deviate from classical assumptions.",
            "limitation": "One limitation is the computational complexity associated with real-time implementation, particularly when optimizing over larger planning horizons.",
            "future work": "Future research directions include developing model-free solutions, addressing computational challenges for real-time applications, and exploring the interplay between regret minimization and other competitive metrics."
        },
        "other info": {
            "info1": "The method is built on the principles of robust model predictive control.",
            "info2": {
                "info2.1": "The proposed approach guarantees \u21132-stability of the closed-loop system.",
                "info2.2": "Numerical experiments indicate that the method can outperform conventional control strategies under non-stochastic disturbances."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "The paper highlights the limitations of existing control methods that are not adaptable to dynamic environments, indicating a historical context of challenges in control strategies."
        },
        {
            "section number": "3.1",
            "key information": "The problem focuses on the challenge of optimizing control strategies in the face of bounded additive disturbances without prior knowledge of their statistical distribution."
        },
        {
            "section number": "3.2",
            "key information": "The proposed receding horizon regret minimization method demonstrates improved performance and stability in controlling systems with bounded disturbances, providing a robust alternative to classical control approaches."
        },
        {
            "section number": "4.2",
            "key information": "The proposed method introduces a receding horizon scheme that computes finite horizon regret-optimal policies to improve performance in uncertain environments, addressing ethical considerations of adaptivity in AI systems."
        },
        {
            "section number": "6.1",
            "key information": "The receding horizon regret minimization method ensures recursive feasibility and stability, which are key components in defining responsible AI."
        },
        {
            "section number": "8.1",
            "key information": "Future research directions include developing model-free solutions and addressing computational challenges for real-time applications, which are essential for advancing responsible AI practices."
        }
    ],
    "similarity_score": 0.5336076129759838,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/On the Guarantees of Minimizing Regret in Receding Horizon.json"
}