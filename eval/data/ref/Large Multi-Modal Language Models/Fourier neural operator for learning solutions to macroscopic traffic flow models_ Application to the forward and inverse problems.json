{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2308.07051",
    "title": "Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems",
    "abstract": "Deep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to nonlinear hyperbolic partial differential equations with applications in macroscopic traffic flow models. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic state in a supervised learning setting. We chose a physics-informed Fourier neural operator ($\u03c0$-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefied solutions. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of $2-3$ vehicle queues and $1-2$ traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles $(\\geq 2)$ with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data.",
    "bib_name": "thodi2023fourierneuraloperatorlearning",
    "md_text": "# Fourier neural operator for learning solutions to macroscopic traffic flow model Application to the forward and inverse problems\nonnam Thodia,b, Sai Venkata Ramana Ambadipudib, Saif Eddin Jabaria,\naNew York University Tandon School of Engineering, Brooklyn, NY 12011, U.S.A. bNew York University Abu Dhabi, Saadiyat Island, P.O. Box 129188, Abu Dhabi, U.A.E.\n8 Dec 2023\nDeep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to first-order macroscopic traffic flow models with applications in estimating traffic densities for urban arterials. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic density in a supervised learning setting. We chose a physics-informed Fourier neural operator (\u03c0-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefaction solutions of certain macroscopic traffic flow models. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of 2 \u22123 vehicle queues and 1 \u22122 traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles (\u22652) with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data. Keywords: Fourier neural operator, traffic state estimation, LWR traffic flow model, physics-informed machine learning, inverse problems.\nPhysics-informed machine learning (PIML) methods that integrate data-driven algorithms with physical priors or domain knowledge have recently drawn significant interest in scientific computing applications, and traffic flow is no exception. Some recent applications of PIML in traffic include flow modeling and simulations (Thonnam Thodi, 2023; Thonnam Thodi et al., 2022; Zhang et al., 2022; Mo et al., 2021; Yuan et al., 2021), state estimation (Di et al., 2023; Yang et al., 2023; Thonnam Thodi et al., 2022, 2021; Shi et al., 2021; Huang and Agarwal, 2020; Jabari et al., 2019), traffic predictions (Li et al., 2022; Pereira et al., 2022; Liu et al., 2021), and traffic control (Han et al., 2022; Di and Shi, 2021). Of particular interest are the deep learning methods for solving forward and inverse problems that involve partial differential equations (PDE) that arise as a key component in most real-time traffic simulation and estimation methods (Kessels, 2019). There are two broad categories of deep learning-based PDE solvers studied in the literature: physics-informed neural networks (PINNs) (Raissi et al., 2019) and neural operators (NOs) (Lu et al., 2021; Li et al., 2021). PINNs use deep neural networks to approximate the PDE solution, which is trained to minimize a physics loss \u2212PDE residual evaluated at random collocation points in the domain and a data loss \u2212solution error at the given input or boundary data. On the other hand, NOs have shown that the underlying solution mapping can be learned even without knowledge of the PDE but with historical simulation data in a supervised learning framework. Both methods offer advantages\n\u2217Corresponding author, e-mail: sej7@nyu.edu\nPreprint submitted to Elsevier\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a94/7a942ca4-b1c3-44d6-966b-faa629c63688.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Two types of Riemann density solutions possible for LWR-type traffic flow models. The  (a) ur \u2264ul < ucr, where ucr = maxu f(u) is the critical traffic density.</div>\n<div style=\"text-align: center;\">ig. 1: Two types of Riemann density solutions possible for LWR-type traffic flow models. The solutions correspond to the condition (a) ul \u2264ur  cr and (a) ur \u2264ul < ucr, where ucr = maxu f(u) is the critical traffic density.</div>\nover traditional PDE solvers in terms of handling sparse and heterogeneous input data, grid-independent solutions, and computational cost. Despite the improvements, deep learning-based solvers can still generate unrealistic or nonphysical predictions owing to a lack of physical information about the problem during training and a poor choice of data for training the solvers (Wang et al., 2021). We consider the Lighthill-Whitham and Richards (LWR PDE) traffic flow model (Lighthill and Whitham, 1955; Richards, 1956), \u2202tu + f \u2032(u)\u2202xu = 0, where u denotes traffic density with concave flux function f (the fundamental diagram). The wave motion in the LWR PDE is characterized by finite propagation speeds f \u2032(u). The solutions are characteristic trajectories emanating from the datum (e.g., the initial conditions). As the wave speed f \u2032(u) is densitydependent, the characteristic lines may cross somewhere in the domain within a finite time. At these domain points, the solution is multi-valued, which means that the problem does not have a well-posed solution in the classical sense. Hence, weak solutions formed by a combination of shocks (discontinuities or kinks) and rarefactions (continuously varying densities) are introduced (Whitham, 1999; LeVeque, 1992). An example of shock and rarefaction solutions is illustrated in Figure 1. We are interested in learning solutions to forward and inverse problems that involve the LWR PDE for applications in traffic flow. Numerical solvers for the forward problem are well-studied in the transportation literature: various classical solvers have been tailored to solving the LWR PDE, including finite difference techniques (Michalopoulos et al., 1984), finite elements (Beskos and Michalopoulos, 1984), and finite volume methods (Daganzo, 1994, 1995; Lebacque, 1996). Other classical techniques have been tailored to solving Hamilton-Jacobi and Lagrangian variants of the LWR PDE (Daganzo, 2005; Claudel and Bayen, 2010; Mazar\u00b4e et al., 2011; Leclercq et al., 2007). In the class of inverse problems, we consider traffic state estimation problems, where sparse measurements from point sensors or mobile sensors are available, and the objective is to reconstruct traffic variables throughout the spatio-temporal domain of interest. Examples of classical techniques that were developed for these kinds of inverse problems in the transportation literature include (Blandin et al., 2012; Yuan et al., 2012; Jabari and Liu, 2013; Canepa and Claudel, 2017; Zheng et al., 2018). Recent traffic-related works explored the use of physics-informed neural networks (PINNs) for both the forward and the inverse problems (Di et al., 2023; Shi et al., 2021; Liu et al., 2021; Rempe et al., 2021; Huang and Agarwal, 2022, 2020). However, some limitations are associated with PINNs. PINNs have shown successful results in solving several forward and inverse problems with underlying solutions that are smooth. However, PINNs have only seen limited success in approximating solutions having high irregularities, such as shock solutions of non-linear hyperbolic conservation laws, which include the LWR PDE (Patel et al., 2022; De Ryck et al., 2022; Jagtap et al., 2020). Due to possible non-differentiabilities (\u201ckinks\u201d) in the solution, the partial derivatives \u2202tu and \u2202xu are not well-defined at all points in the computational domain. This implies that the physics loss function used for training neural networks is not well-defined and leads to poor convergence results. Huang and Agarwal (2023) raised this concern while solving the LWR traffic flow model. Even for viscous regularized versions, where a viscosity term is added to smoothen discontinuities and kinks, the derivatives can have high variations near the jumps, and residuals can blow up in those regions. Also, neural networks are theoretically shown to poorly approximate discontinuous or highly varying functions. Typically, the approximation errors for deep neural networks scale in the order \u223cO\ufffd\u03f5\u2212d/n\ufffd, for a desirable approximation error \u03f5 (Yarotsky, 2017). d and n are the dimension and smoothness of the approximating function class; larger n implies a smooth function and vice-versa. This means highly varying functions (i.e., n \u21920) require exponentially large neural networks to ensure a lower error \u03f5.\n<div style=\"text-align: center;\">(b) Rarefaction solution</div>\nA few recent attempts focused on overcoming the above challenges of learning highly-varying solutions. For instance, (Jagtap et al., 2020; Dwivedi et al., 2021) proposes to divide the computational domain into multiple subdomains, each approximated with different neural networks. This allows for parallel computing, where the approximation task is distributed across multiple, presumably smaller-size, neural networks. (Wu et al., 2023; Mao et al., 2020) proposes an adaptive sampling method to choose larger collocation points near the regions of high irregularities, thus giving more weight to the discontinuous region in the loss function. In the context of traffic, (Huang and Agarwal, 2023; Shi et al., 2021) propose to solve the viscous regularized LWR PDE to ensure a well-posed physics loss function. Despite their practical success, these guided improvements have several tuning parameters and fail to offer a universal solution. Other works proposed using alternate, equivalent residuals to form the physics loss function. For instance, (Patel et al., 2022; De Ryck et al., 2022) define residuals using weak entropy conditions and learn weak entropy solutions that capture both shocks and rarefactions (LeVeque, 1992). However, the convergence of these methods has yet to be fully explored and has only seen limited success in practice. This paper approaches this problem using an operator learning framework, where a parametric operator is trained to map any input data to its corresponding LWR PDE solution in a supervised learning setting. By \u201coperator learning\u201d, we mean that the method is primarily data-driven compared to the pointwise PDE residual training in PINNs. The Fourier neural operator (FNO) is one such operator that has shown successful results in learning PDE solutions (Li et al., 2021; Kovachki et al., 2021). To learn solutions of the LWR PDE, we propose a physics-informed variant of the Fourier neural operator (\u03c0-FNO), where an additional physics loss from the integral form of the conservation law is penalized during training. Unlike the differential operator used in (Wang et al., 2021), the equivalent integral form permits discontinuities and non-differentiabilities in the solutions (Patel et al., 2022; De Ryck et al., 2022). The operator is trained offline using data generated from numerical simulations. Once trained, computing the solution with new inputs,i.e., changing initial/boundary conditions, involves a single forward pass through the trained network. The operator learning framework benefits from learning a family of solutions instead of approximating a single solution, as in the PINNs framework. In other words, the PINNs framework requires re-training the neural network for every input data, which is computationally expensive. The operator learning framework alleviates this by offloading the operator optimization to a single offline training stage using a given training dataset. However, this also has limitations. First, how the trained operators generalize, i.e., how well they perform on previously unseen inputs, is not well understood. Second, there is no guidance on how to choose training data for operator learning, resulting in expensive training procedures to produce operators that perform well out of sample. To this end, we provide guidance into training data selection for the LWR PDE: solutions generated using input data (i.e., initial and boundary conditions) sampled from random piecewise-constant functions. This specific choice is motivated by the Riemann solutions to the problem (LeVeque, 1992) and captures the essential characteristics, namely, shocks and rarefactions. Previous studies have employed similar heuristics, e.g., (Wang et al., 2021) used smooth input data sampled from Gaussian processes to generate data for learning smooth PDE solutions. Our proposed choice of training data distribution is also parametric and has a natural complexity measure, i.e., a measure of data richness. This allows us to train the operator with elementary solutions and then test its generalization to increasingly complex inputs. We also present systematic experiments to study the generalization performance of the trained operator. In experiments, we extensively study the solutions learned for initial value problems (periodic boundary conditions), boundary value problems (Dirichlet boundary conditions), and inverse problems (internal boundary conditions), using the LWR traffic flow model (Lighthill and Whitham, 1955; Richards, 1956). In short, the main contributions of this study (and the remaining sections of the paper) are summarized in the following points: \u2022 We explore an operator learning framework for learning solutions to macroscopic traffic flow models with arbitrary input data. A unified framework to handle both forward and inverse problems is presented (Section 2). \u2022 We propose a physics-informed variant of the Fourier neural operator (\u03c0-FNO) for the LWR PDE, where the physics regularizer is derived from its integral form (Section 3). \u2022 We propose a mechanism for selecting training inputs (namely, initial and boundary conditions) that results in superior generalization performance for the trained operator (Section 4). \u2022 Lastly, we present the numerical results for traffic density predictions for urban signalized roads (Section 5) and quantify the out-of-sample error performance with respect to the vehicle queue distributions and the number of traffic signal cycles (Section 6).\n# 2. Problem formulation\n# 2.1. LWR traffic flow problem\nThe LWR model is a continuum description of the flow of vehicles on a road segment. The shock solutions of the LWR represent the sharp changes in traffic conditions, e.g., queue formation and dissipation observed in congested highway traffic and end-of-vehicle queues on urban signalized roads. The LWR model arises as a key component in many real-time traffic state estimation and traffic control algorithms (Daganzo, 1995, 2005; Kessels, 2019). Below are the details of the model. Consider a one-dimensional space-time domain \u2126\u2282R \u00d7 R+. Denote by u(x, t) : \u2126\u2192[0, umax], the density of traffic at position x \u2208R and time t \u2208R+ and is the average number of vehicles per unit road length or the spatial concentration of vehicles. Let q(x, t) : \u2126\u2192[0, qmax] be the traffic flux, which is the number of vehicles crossing a road section per unit of time or the temporal concentration of vehicles. The LWR model describes the evolution of traffic density based on the conservation of vehicles:\nut(x, t) + qx(x, t) = 0, (x, t) \u2208\u2126, u(x, 0) = u0(x), (x, 0) \u2208\u21260, u(x, t) = ub(x, t), (x, t) \u2208\u2126b,\nwhere ut \u2261\u2202u \u2202t and qx \u2261\u2202q \u2202x. Here u0 is the given initial density data defined over the spatial domain \u21260 \u2282\u2126and ub is the given density data defined at the physical road boundaries \u2126b \u2282\u2126. For instance, ub corresponds to traffic measurements obtained from point sensors installed at the entry and exit of the road segment. We assume that the given boundary condition is well-posed (Jabari, 2016). For the LWR traffic flow model, one typically prescribes a concave flux function q = f(u), f(u) : [0, umax] \u2192 [0, qmax], which acts as a closure to the above equation (1). An example flux function is the Greenshield\u2019s fundamental relation (Greenshields, 1935), given by f(u) = uvmax \ufffd1 \u2212u/umax \ufffd, where vmax is the free-flow speed and umax is the jam density. Note f \u2032(u) depends on u, which means (1) is nonlinear for which a simple, shock-free solution does not exist. Thus, we are interested in the weak solution of (1), which is composed of shocks and rarefactions (Whitham, 1999; LeVeque, 1992). We refer to (1) as the forward problem. We also consider an associated inverse problem,\nut(x, t) + qx(x, t) = 0, (x, t) \u2208\u2126, u(x, 0) = u0(x), (x, 0) \u2208\u21260, u(x, t) = up(x, t), (x, t) \u2208\u2126p,\nwhere instead of the regular boundary data ub, we are provided with interior boundary data up(xp, tp) defined at a set of collocation points within the domain\u2019s interior, denoted as (xp, tp) \u2208\u2126p \u2282\u2126. For example, up may represent sparse vehicle trajectory measurements obtained from GPS-equipped or connected vehicles, often referred to as probe vehicles. The points (xp, tp) \u2208\u2126p correspond to the spatial and temporal coordinates of the probe vehicle\u2019s trajectory within the space-time domain. However, the input up is a more general specification, encompassing subsets of the regular boundary data ub mentioned in the forward problem (1). The key factor differentiating up from ub is that the former does not provide a complete boundary specification, resulting in non-unique solutions to the overall problem. To provide a practical example, up can represent probe vehicle trajectory measurements as well as measurements collected from a single road boundary. This scenario is relevant to signalized arterials with point sensors at the stopline only. Due to the incomplete boundary specification, the inverse problem in (2) is not a well-posed problem and is challenging to solve using conventional numerical solvers such as those based on forward Euler schemes (Kessels, 2019). Solving (2) necessitates the use of optimization-based iterative methods to obtain approximate solutions, which largely depends on the nature of input specification up. In light of these challenges, the primary objective of this work is to develop a unified framework for learning the solutions to both the forward and inverse problems (1)-(2), regardless of the input specification. Our overarching aim is to make this framework highly accessible for real-world applications and practitioners, thereby easing its practical implementation.\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3507/350710ff-5797-4045-8af5-c1ed5edab546.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Example 2</div>\nFig. 2: Three different types of input conditions considered in this study and their representations: initial condition \ufffdu0 with domain \u21260, boundary condition \ufffdub with domain \u2126b, and interior condition \ufffdup with domain \u2126p. u is the output that defines the solution in the complete domain \u2126. Two examples are provided for illustrations.\n<div style=\"text-align: center;\">Fig. 2: Three different types of input conditions considered in this study and their representations: initial condition \ufffdu0 with domain \u21260, bou condition \ufffdub with domain \u2126b, and interior condition \ufffdup with domain \u2126p. u is the output that defines the solution in the complete domain \u2126 examples are provided for illustrations.</div>\n# 2.2. Solution using operator learning framework\nIn this section, we frame the solutions to (1)-(2) as an operator learning problem. Denote the complete solution over the domain \u2126as u := \ufffdu(x, t) | (x, t) \u2208\u2126\ufffd, arranged into a space-time matrix of dimensions (m \u00d7 n), which are the number of space and time discretizations. Similarly, denote the initial data, physical boundary data, and internal boundary data using matrices u0, ub, and up, respectively, with dimension same as u. The input conditions (u0, ub and up) are defined only at the respective input points in the domain, denoted by \u21260, \u2126b, and \u2126p (respectively). The locations where the inputs are unavailable i.e., (x, t) \u2208\u2126/ \ufffd\u21260 \u222a\u2126b \u222a\u2126p \ufffd, are defined as null values, e.g., numerically as \u22121. We illustrate these three input data in Figure 2 using two examples. The original problem (1)-(2) is then to reconstruct the complete solution u from one or more input condition(s), collectively denoted as a := \ufffdu0, ub, up \ufffd. We approach this problem with an operator approximation viewpoint, where the goal is to approximate an operator G\u0398 such that G\u0398 : a \ufffd\u2192u. This means any input conditions a can be mapped to its corresponding solution u at the cost of evaluating G\u0398, which serves as a surrogate model. The parameters \u0398 define the functional class of G\u0398 (i.e., a set of mappings a \ufffd\u2192u). Examples of such operators include Fourier neural operator (Li et al., 2021), deep-O-net (Lu et al., 2021; Wang et al., 2021), graph kernel operator (Li et al., 2020), and wavelet neural operator (Tripura and Chakraborty, 2023). The operator learning, i.e., finding the optimal parameters \u0398\u2217, is performed using an empirical risk minimization approach (Murphy, 2012),\n\ufffd  \ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd where L is the error in the predicted solution \ufffdu = G\u0398 \ufffda\ufffdand true solution u. R is a regularizer that incorporates prior knowledge on the operator G\u0398 or the predicted solution \ufffdu. The above minimization is over P(a, u), which is the complete distribution of the input data a and its corresponding solution u. The parameter optimization in (3) is performed only once and done offline. Once optimized, computing solutions for new inputs anew involves a single forward pass G\u0398\u2217\ufffdanew \ufffd. The operator learning framework presented in (3) is generic. There are two elements that primarily dictate its performance: the choice of solution operator G\u0398 and the data distribution P\ufffda, u\ufffd. Whether there exists such an operator G\u0398 that can learn a family of the solutions to LWR-PDE is an open research question. Some theoretical\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/26b6/26b653d3-c598-445e-adc6-13afea12ba8f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">triction on the computational mesh size in the context of finite volume numerical schem</div>\n<div style=\"text-align: center;\"> effect of CFL restriction on the computational mesh size in the context of finite volume </div>\ntudies worked on addressing the approximation errors of neural operators (Chen et al., 2023; Ryck and Mishra, 2022; Kovachki et al., 2021), but their results are restricted to well-posed viscous PDEs. In this study, we use the Fourier neural operator as the solution operator G\u0398 with a slight modification. We present this in more detail in Section 3. Further, the minimization in (3) is over all possible values for (a, u), i.e., the distribution P(a, u). However, we only observe a subset Ptrain(a, u) \u2286P(a, u) of the true distribution P(a, u), in which case the integrand in (3) is replaced by ummation. This makes the problem (3) practically feasible. Thus, part of the operator learning problem also involves finding a suitable subset Ptrain \u2286P so that the operator approximation G\u03b8\u2217is valid globally. We present a clever choice of data distribution suitable for learning the weak solutions of LWR PDE in Section 4. The framework (3) is also independent of how we choose the input data a. Depending on the choice of input data illustrated in Figure 2), we study three different problem settings below. The first two problem settings are forward problems (1), and the third is an inverse problem (2). 1. Initial value problem: The input to these problems is just the initial condition aivp = \ufffdu0 \ufffd. Such problems arise when the boundary is periodic or free, so explicit solution constraints need not be imposed at the boundary. An example is the simulation of traffic flow on a ring road. 2. Boundary value problem: The inputs are the initial and Dirichlet-type boundary conditions abvp = \ufffdu0, ub \ufffd, a traditional setup for solving LWR PDE. We assume boundary conditions ub are well-posed, which means the inputs abvp can uniquely determine a weak solution for (1). These problems arise in simulating traffic flow on an urban road, where the boundary flows are restricted by control measures such as traffic lights. 3. Inverse problem: The inputs are the initial and the internal boundary conditions aip = \ufffdu0, up \ufffd. Unlike the forward problems above, this problem is not well-posed due to an unknown boundary condition ub. In other words, aip cannot uniquely determine the solution for (2). It can have multiple solutions depending on the uncertainty in the boundary conditions, and picking a physical solution would be difficult. These problems arise in practice, where measurements from probe vehicle trajectories up are more ubiquitous than measurements at the boundaries ub. It is important to note that \u2126p need not necessarily cover the entire spatiotemporal domain, which can lead to ambiguities in solutions when employing analytical techniques such as the one in (Canepa and Claudel, 2017). We investigate if an operator learning framework can faithfully reconstruct a physically consistent solution for this type of problem.\n# 2.3. Computational aspects of conventional solvers\nWe conclude this section with a brief note on the computational aspects of conventional solvers. The forward problem (1) is generally solved using finite-volume numerical schemes that approximate an average PDE solution in a discrete space-time domain. An example is the Godunov scheme (LeVeque, 1992; Lebacque, 1996). The computations are of the order O(mn) for the one-dimensional LWR PDE, where m and n are the numbers of discretizations in x and t dimensions. However, these methods are primarily limited by choice of discretization size \u2206t and \u2206x, which need to satisfy the Courant\u2013Friedrichs\u2013Lewy (CFL) condition (Kessels, 2019)\n\u2206t sup u | f \u2032(u)| \u2264\u2206x.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/242a/242a568b-364a-45dd-86a4-09ac7afc0d3f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c951/c95176ec-6d89-4fa4-b6de-b2d42ef6e3f5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7c7/b7c71743-765d-437b-aae7-f7bbca4b8569.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Fourier neural operator G\u0398 \ufffda\ufffd</div>\n\ufffd\ufffd Fig. 4: Architecture of the Fourier neural operator used as the solution operator in the current pap Fourier operator layer. These depictions are adapted and modified from (Li et al., 2021).\n\ufffd\ufffd  \ufffd\ufffd  Fourier neural operator used as the solution operator in the current paper. (a) Complete model architecture. (b) Single ese depictions are adapted and modified from (Li et al., 2021).\n<div style=\"text-align: center;\">\ufffd\ufffd  \ufffd\ufffd Fig. 4: Architecture of the Fourier neural operator used as the solution operator in the current paper. (a) Complete model architecture. (b) Singl Fourier operator layer. These depictions are adapted and modified from (Li et al., 2021).</div>\nThe CFL condition implies that smaller \u2206x requires a smaller \u2206t. Arbitrary discretizations of the domain result in unstable solutions. For instance, if one needs a solution at finer \u2206x but coarser \u2206t, it requires first performing computations at finer \u2206t and then aggregating, which mandates unwarranted computations. We illustrate this limitation in Figure 3. As \u2206x is reduced, the required \u2206t shrinks linearly, but the number of discrete time steps n increases as a power law with exponent \u22121\nThe inverse problem (2) has no straightforward solution (Tarantola, 2005). One approach is to consider it as a constrained least squares problem\nThe inverse problem (2) has no straightforward solution (Tarantola, 2005). One constrained least squares problem\narg min u \ufffd\ufffd\ufffdP\u2126 \ufffdu \u2212a\ufffd\ufffd\ufffd\ufffd2 2 s.t. R\ufffdu\ufffd= 0,\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd where P\u2126, \u201cthe binary mask\u201d, projects u \u2212a to \u2126, i.e., element i of the vector P\u2126(u \u2212a) is (u \u2212a)i if i \u2208\u2126; it equals 0, otherwise. R(u) is the prior knowledge of the solution, such as a differential equation, a total-variation bound, or an entropy condition (Tarantola, 2005). If R is linear, (6) becomes a quadratic program, which can be solved using convex optimization algorithms. If R is nonlinear, as is the case with many differential equations, one resorts to inexact iterative algorithms (Li et al., 2022; Yang et al., 2022). The physics-informed neural network (Raissi et al., 2019) is a special case of (6), which solves the following minimization problem for the LWR model (Di et al., 2023; Shi et al., 2021; Liu et al., 2021)\nmin \u03b8 \ufffd\ufffd\ufffdg\ufffdu\u03b8 \ufffd\ufffd\ufffd\ufffd2 2 + w1 \ufffd\ufffd\ufffdu\u03b8 \ufffdx, 0\ufffd\u2212u0 \ufffd\ufffd\ufffd2 2 + w2 \ufffd\ufffd\ufffdu\u03b8 \ufffdxb, t\ufffd\u2212ub \ufffd\ufffd\ufffd2 2\nmin \u03b8 \ufffd\ufffd\ufffdg\ufffdu\u03b8 \ufffd\ufffd\ufffd\ufffd2 2 + w1 \ufffd\ufffd\ufffdu\u03b8 \ufffdx, 0\ufffd\u2212u0 \ufffd\ufffd\ufffd2 2 + w2 \ufffd\ufffd\ufffdu\u03b8 \ufffdxb, t\ufffd\u2212ub \ufffd\ufffd\ufffd2 2 s.t. g\ufffdu\u03b8 \ufffd= \u2202tu\u03b8 + f \u2032\ufffdu\u03b8 \ufffd\u2202xu\u03b8,\n\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd s.t. g\ufffdu\u03b8 \ufffd= \u2202tu\u03b8 + f \u2032\ufffdu\u03b8 \ufffd\u2202xu\u03b8,\n\ufffd \ufffd\ufffd \ufffd where a neural network u\u03b8 : (x, t) \u2192u(x, t) maps domain points to their corresponding solution and g(u\u03b8) is the physics residual. The problem complexity here is the training convergence of the neural network u\u03b8. In general, the limitation of these approaches to inverse problems (6)-(7) is that optimization is performed for every input data u0 and ub, which is not feasible for online applications.\n# 3. Methodology I: Solution operator\n3.1. Fourier neural operator (FNO) as the solution operator We use the Fourier neural operator (FNO) model (Li et al., 2021) to approximate the solution operator G\u0398. We describe the architecture of FNO in this sub-section. Following the standard notations in (Li et al., 2021), denote the input data a \u2208Rm\u00d7Rn +\u00d7Rda and the solution output u \u2208Rm\u00d7Rn +\u00d7Rdu, where da and du denote the feature dimensions. For the present problem, da = 1 and du = 1. The FNO model is defined as a sequence of nonlinear compositions\nWe use the Fourier neural operator (FNO) model (Li et al., 2021) to approximate the solution operator G\u0398. We describe the architecture of FNO in this sub-section. Following the standard notations in (Li et al., 2021), denote the input data a \u2208Rm\u00d7Rn +\u00d7Rda and the solution output u \u2208Rm\u00d7Rn +\u00d7Rdu, where da and du denote the feature dimensions For the present problem, da = 1 and du = 1. The FNO model is defined as a sequence of nonlinear compositions \ufffdu G \ufffda\ufffd \ufffd Q \u25e6F (L) \u25e6F (L\u22121) \u25e6\u00b7 \u00b7 \u00b7 \u25e6F (2) \u25e6F (1) \u25e6P \ufffd\ufffda\ufffd, (8\n\ufffdu = G\u0398 \ufffda\ufffd = \ufffd Q \u25e6F (L) \u25e6F (L\u22121) \u25e6\u00b7 \u00b7 \u00b7 \u25e6F (2) \u25e6F (1) \u25e6P \ufffd\ufffda\ufffd,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f0b/1f0bbce3-cbc1-4c92-9695-804b555b6647.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Computations in a single Fourier operator F \ufffdz\ufffd</div>\n(5)\n(7)\nwhere P and Q are projection operators, F (l) is a Fourier operator, and l \u2208{1, . . . , L} is the layer index. The operator P lifts the input data a to a high-dimensional latent space z(0) = P\ufffda\ufffd, z(0) \u2208Rm \u00d7Rn \u00d7Rdz, where dz is the feature dimension of the latent space. The latent variable z(0) gets passed through a series of Fourier operators \ufffdz(l+1) = F (l)\ufffdz(l)\ufffd\ufffdL l=1 until reaching the operator Q, which project it back to the output space u = Q\ufffdz(L)\ufffd. P and Q are feed forward neural networks with parameters \u0398P and \u0398Q. The FNO model is shown in Figure 4 (a). Motivated by Green\u2019s function for solutions to linear partial differential equations, the original work (Li et al., 2021) defines the Fourier operator F as\n# F (z) = \u03c3 \ufffd W \u22971 z + IFFT\ufffdR \u22972 FFT(z)\ufffd\ufffd ,\n\ufffd \ufffd\ufffd\ufffd where FFT and IFFT are the discrete Fourier transform and the inverse discrete Fourier transform in two dimensions, space and time in this case. \u03c3 is a nonlinear activation function, acting element-wise. The matrix operations \u22971 and \u22972 are defined as\nW \u22971 z = \ufffd\ufffd\ufffd\ufffdn WT\ufffdz\ufffd n (local operator) R \u22972 \u02dcz = \ufffd\ufffd\ufffd\ufffd\u02dcdz \ufffd dz \ufffdR\ufffd dz, \u02dcdz \u2299\ufffd\u02dcz\ufffd dz, \u02dcdz (kernel operator)\n\ufffd\ufffd where \u02dcz = FFT(z), \u2299is the element-wise matrix multiplication, \u2225i\u00b7 is the concatenation operator in dimension i, and [ ] j is the jth component or dimension. The first operator (local) in (10) maps the latent dimension of z from dz to d\u2032 z at all points (m, n) using a matrix W \u2208Rdz\u00d7d\u2032 z. This local operation is equivalent to a 1-dimensional convolution. The second operator (kernel) in (10) performs an element-wise multiplication of z\u2032 using a tensor R \u2208Cmk\u00d7nk\u00d7dz\u00d7d\u2032 z, where R is defined in the complexspace. Figure 4(b) visually illustrates these computations. W and R are the trainable parameters of a single Fourier operator F . The central idea of the Fourier operator F is the parameterization in the Fourier space. By learning the significant components in the Fourier space, the FNO operator can approximate the solution efficiently. The set\n# \u0398 := \ufffd W(l), R(l)\ufffdL l=1 \u222a \ufffd \u0398P, \u0398Q \ufffd ,\n\ufffd \ufffd \ufffd \ufffd is the complete set of the trainable parameters of the solution operator G\u0398. The operator G\u0398 is differentiable endto-end using automatic differentiation. Hence the parameters \u0398 can be optimized using any gradient descent-based algorithm. The choice of the loss function and how to incorporate physical knowledge in the optimization is described next.\n# 3.2. Physics-informed Fourier neural operator (\u03c0-FNO)\nThe nature of solutions learned by G\u0398 depends on the minimization criterion used in the operator learning framework (3). To enforce physically consistent solutions during training, we choose the prior R in (3) as a physical constraint on the weak solution of LWR PDE. This is discussed below.\nPhysics loss. A straightforward choice for R is the point-wise PDE residual, as in conventional physics-informed neural networks (Raissi et al., 2019). However, as argued in the previous sections, it does not form a well-defined regularizer due to potential non-differentiabilities or kinks in the solution. Instead, we choose the integral form of the conservation law to form R. For a compact sub-domain \u2126sub := \ufffdx1, x2 \ufffd\u222a\ufffdt1, t2 \ufffd\u2286\u2126, the integral form of a one-dimensional scalar conservation law (LeVeque, 1992) is\n\ufffd\ufffd\ufffd\ufffd for all choices of x1, x2, t1, t2 \u2208\u2126, where x1 < x2 and t1 < t2. The integral condition (12) means that the variations in traffic density u over a road section x \u2208[x1, x2] during time t1 to t2 only occur due to a difference in the traffic flux q at the boundaries x1 and x2 within the time interval [t1, t2]. Condition (12) also holds at shock fronts in the solution.\n(9)\n(10)\n(11)\n(12)\nWe enforce condition (12) locally over the discrete domain that is used to define the predicted solution \ufffdu. One ould also use a different discretization (coarser or finer) to evaluate (12). The physics prior then becomes\nR\ufffd\ufffdu\ufffd= \ufffd (x,t) \ufffd\ufffd\ufffd\ufffd\ufffd\ufffdu\ufffdx, t + \u2206t\ufffd\u2212\ufffdu\ufffdx, t\ufffd+ \u2206t \u2206x \ufffd \ufffdq\ufffdx \u2212\u2206x/2, t\ufffd\u2212\ufffdq\ufffdx + \u2206x/2, t\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd where \ufffdq is the flux derived from the predicted solution \ufffdu. Here, \u2206x and \u2206t denote the size of discrete cells, the choice of which follows the Courant\u2013Friedrichs\u2013Lewy condition (LeVeque, 1992), which for our problem is \u2206t \u2264 \u2206x/ supu | f \u2032(u)|. The optimal solution operator G\u0398 should produce predictions \ufffdu that ideally satisfy\nR \ufffd\ufffdu\ufffd= 0.\n \ufffd\ufffd\ufffd Data loss. We choose the data loss L as the normalized l2-norm of the difference in the predicted solution \ufffdu = G\u0398 \ufffda\ufffd and the true solution u,\n \ufffd\ufffd\ufffd Data loss. We choose the data loss L as the normalized l2-norm of the difference in the predi and the true solution u,\n \ufffd\ufffd\ufffd \ufffd\ufffd \ufffd We found minimization using a normalized loss metric to have a stable convergence compared to its unnormalized counterpart. Note that (15) naturally incorporates the loss in predicting the given initial and boundary data. Models. To evaluate the benefits of physics-informing, we study two different models in this work. Following the operator-learning framework (3), we consider a vanilla FNO model for which the solution operator G\u0398 is optimized only using the data loss,\n \ufffd\ufffd\ufffd \ufffd\ufffd \ufffd We found minimization using a normalized loss metric to have a stable convergence compared to its unnormalized counterpart. Note that (15) naturally incorporates the loss in predicting the given initial and boundary data.\nModels. To evaluate the benefits of physics-informing, we study two different models in this work. Following the operator-learning framework (3), we consider a vanilla FNO model for which the solution operator G\u0398 is optimized only using the data loss,\n# \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd and a physics-informed \u03c0-FNO model where the solution operator G\u0398 is optimized using both the data loss and the physics loss,\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd \ufffd\ufffd and a physics-informed \u03c0-FNO model where the solution operator G\u0398 is optimized using physics loss,\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd \ufffd\ufffd \ufffd\ufffd \ufffd\ufffd where D is the training dataset and \u03bb is a relative weight assigned to the physics loss. We train models (16)-(17) for all the problem setups using the same FNO model architecture and training dataset. The hyperparameter \u03bb is tuned independently using a brute-force method. The FNO architecture is chosen by trial and error. The model architecture and hyperparameters used in our experiments are detailed in Appendix A and Appendix B.\n# 3.3. Computational complexity of \u03c0-FNO solver\nThe computational complexity of the \u03c0-FNO operator has two parts. First is optimizing the \u03c0-FNO parameters \u0398 for a given training dataset \u2013 the training stage. The second is the forward evaluation of the operator G\u0398\u2217for new inputs \u2013 the inference stage. The computational effort for the training stage depends on the size of \u03c0-FNO (number of Fourier layers) and training dataset, which we consider less relevant to the present study since training is performed offline with a dedicated computing power. The inference stage or a forward pass is performed in real-time and discussed further below. The key computational elements of a single forward pass of the \u03c0-FNO are composed of two terms: the global operator and the local operator in (9)-(10). The global operator consists of a two-dimensional Fast Fourier transform followed by an inverse Fast Fourier transform, which has a time complexity of O(mn(log m+log n)) for uniform spatial and temporal discretizations. The local operator is an element-wise matrix multiplication which has a complexity O(mn). So, the total time complexity of the \u03c0-FNO model is on the order O(mn(log m + log n) + mn), which exceeds the Godunov scheme by a factor of 1 + log(mn), when using the same spatio-temporal discretization. However,\n(14)\n(15)\n(16)\n(17)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b81f/b81f3e3c-8b2c-46a3-aa5b-a35a96ca0d64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Samples of initial condition</div>\nthis complexity measure holds irrespective of the problem type, i.e., it is the same for both the forward and inverse problems, which is a major advantage for \u03c0-FNO. We experimentally compare the computation times later in the paper. Furthermore, the spectral projection z\u2032 = FFT(z) allows one to consider only the significant Fourier components and neglect the insignificant high-frequency components. We consider the top mk \u226am and nk \u226an components of z, as shown in Figure 4(b) (upper part). Thus, the size of R is fixed as (mk, nk) and independent of the problem size (m, n), which means scalable model complexity. The choice of (mk, nk) depends on the regularity of the function z: low values are preferred for smooth functions, and higher values are preferred for highly-varying functions. Regardless, this factor keeps the model complexity (number of optimization parameters) to be independent of problem size. These advantages are further pronounced for large problem domains.\n# 4. Methodology II: Data distribution\nThe quality of solutions learned by the \u03c0-FNO for both seen and unseen inputs partly depends on the choice of th training dataset. Recall the role of data distribution Ptrain in the operator learning framework (3). Many deep learning based studies lack a systematic procedure for efficiently choosing the training dataset and evaluating the algorithm generalization performance. We aim to address these two interrelated aspects in this section.\n# 4.1. Choice of training data distribution\nThe choice of training data depends on the nature of the solution. For instance, (Wang et al., 2021) proposes to use initial conditions sampled from a Gaussian process to generate data for learning the smooth solutions of parabolic PDEs. Following a similar argument and motivated by the Riemann solutions (LeVeque, 1992; Lebacque, 1996) used as a base solver for LWR PDE, we propose to use piecewise constant functions as input data \ufffdu0, ub \ufffdto generate the solution u to train \u03c0-FNO. The piecewise constant input conditions can capture the basic features of the hyperbolic solutions, namely, shocks and rarefactions. However, the nature of the piecewise constant functions depends on the application. In traffic flow models, the initial data u0(x) is the initial distribution of vehicles on a road section. We assume vehicles distribute themselves spatially as either one, two, or multiple vehicle queues. This can be modeled using step functions with the number of steps as the number of vehicle queues. For a general setting where all vehicles are unevenly distributed, the number of steps equals the total number of vehicles on the road. Similarly, the boundary condition ub(t) specifies the traffic flow at road ends; for e.g., at intersections. On an urban road with traffic lights, we model the effect of red-green vehicles stopping and moving using wavelet-like functions, e.g., Haar wavelets. These wavelet-like functions can capture high-density traffic at the boundary points. Further, boundary data with cycles of non-periodic wavelets can model multiple traffic signal cycles over time.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0143/0143c412-65c8-46b6-8683-c62dc80411f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Samples of boundary conditions</div>\nAlgorithm 1: Simulate multi-step initial condition\ninput\n: number of steps in the initial condition ns \u2208Z+,\ndiscretization size in the x-dimension m \u2208Z+,\nsolution bounds umin, umax \u2208R+.\noutput\n: sample path u0(x) with ns steps\ninitialize: initialize a sample path to a random constant u0(x) = c > 0, ;\nfixed step height sh > 0, ;\nmaximum step width sw = m/ns, ;\nrandom integer generator \ufffd(start, end), ;\nindices i = 0, j = 0.\nfor {1, . . . , ns} steps do\nrandomly locate the next step in the sample path:\nj \u2190min \ufffdm \u22121, \ufffd\ufffdi, i + sw\n\ufffd\ufffd\nupdate the sample path value from the new location:\nu0[ j :] \u2190u0[ j \u22121] + \ufffd(\u2212sh, sh)\ncheck for feasibility constraints:\nu0 \u2190max \ufffdumax, min \ufffdumin, u0\n\ufffd\ufffd\nupdate indices: i \u2190j\n;\n/* min and max act element-wise */\nend\nA few examples of the proposed random piecewise constant functions are illustrated in Figure 5. The width, amplitude, and location of the steps and wavelets used to define initial and boundary data are considered random. We then construct a distribution of such input data, and an input instance is a sample drawn from this distribution. As an example, Figure 5 (a) shows different samples of initial density data with 2 steps (top row) and 3 steps (bottom row). Figure 5 (b) shows different samples of boundary density data with 1 wavelets (top row) and 2 wavelets (bottom row). This way, one generates a distribution of initial and boundary data, where each distribution is parameterized by the number of steps and wavelets, respectively. A general procedure to simulate the random multi-step and multi-wavelet functions is described in Algorithms 1 and 2. Both algorithms proceed by sequentially adding steps (wavelets) of random widths and heights at random locations in the spatial (temporal) domain. For the boundary data, we partition the temporal domain based on the specified number of wavelets, and each wavelet is randomly located within a single partition. Appropriate bounds are enforced to ensure feasible values. Once a random set of input data are generated, we obtain the corresponding solutions using a numerical solver. We use the Godunov numerical scheme to generate the solutions for the LWR traffic flow model (Kessels, 2019).\n# 4.2. Evaluation of generalization performance\nQuantifying the generalization performance is critical for the reliable application of the proposed numerical solver. Generalization performance here refers to the out-of-sample error of the \u03c0-FNO model on previously unseen input conditions. This is typically measured using k-fold cross-validation (Murphy, 2012), which is generally agnostic to the inherent difficulties in the learning problem. For instance, it does not differentiate the difficulty in approximating discontinuous solutions over continuous solutions. We find that it makes more sense to quantify the out-of-sample error as a function of the input conditions of different complexities rather than treating all test samples alike as in k-fold cross-validation. The different families of random input functions proposed in the previous sub-section provide a natural grouping for this evaluation. To this end, we propose a set of systematic experiments to quantify the generalization performance of the \u03c0-FNO solver. The key is to separate out the training and testing domain as a function of input complexity, which we refer to as the number of steps in the initial data and the number of wavelets in the boundary data. We train the solver using data generated from lower complexity input conditions (simple traffic density dynamics) and test its generalization to input conditions of increasing complexity (complex traffic density dynamics). For instance, the \u03c0-FNO model is trained\nAlgorithm 2: Simulate multi-wavelet boundary condition\ninput\n: number of wavelets in the bound. condition nw \u2208Z+,\ndiscretization size in the t-dimension n \u2208Z+,\nsolution bounds umin, umax \u2208R+ .\noutput\n: sample path ub(t) with nw wavelets\ninitialize: initialize a sample path using standard Gaussian ub(t) = c + \ufffd(0, 1, n), ;\nfixed wavelet width sw > 0, ;\nmaximum partition width pw = n/nw, ;\nrandom integer generator \ufffd(start, end), ;\nindices i = 0, j = 0.\nfor k \u2208{1, . . . , nw} wavelets do\npartition the sample path:\nupar\nb\n\u2190ub[kpw : (k + 1)pw]\nrandomly locate wavelet position within the partition:\ni \u2190\ufffd(0, pw/2), j \u2190\ufffd(i, pw)\nassign maximum solution value for the wavelet location:\nupar\nb [i : j] \u2190umax\nupdate the sample path with the updated partition:\nub[kpw : (k + 1)pw] \u2190upar\nb\n;\n/* Notation u[i : j] denotes slicing the array u at indices i and j */\nend\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7c6/b7c67081-44b9-4b99-b77f-8912e33ddf1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6: Different input conditions of increasing complexity used in the training data and testing data. The top row shows the initial conditions, an the bottom row shows the boundary conditions. The subscripts denote the number of steps or wavelets.</div>\nwith solutions initialized using 1 \u22122 stepped initial data (homogeneous vehicle queues) and 1 \u22122 wavelet boundary data (one or two traffic signal cycles). The solver is then tested with solutions of complex dynamics generated from arbitrary initial data (heterogeneous vehicle queues) and multi-wavelet boundary data (multiple red-green traffic signal cycles). We denote a sample path of initial data by i\u03b1 and of boundary data by b\u03b2, where the subscripts \u03b1 and \u03b2 indicate the number of steps and wavelets (respectively). Figure 6 shows sample paths of different initial and boundary data used for training (in blue color) and testing (in orange color). We use initial data of up to three steps (i0, i1, i2 and i3) and boundary data of up to two wavelets (b0, b1 and b2) in the training data. The testing data consists of initial data up to forty steps (i4 - i40) and boundary data up to eight wavelets (b3 - b8) \u2212the physics of the problem limits these values (for a typical urban road). Note the boundary data shown in Figure 6 corresponds to the downstream road boundary. For the upstream road boundary, we\n<div style=\"text-align: center;\">Table 1: Summary of numerical results</div>\nSolver\nValidation error\nTesting error\nMAE1\nRelative \u211322\nMAE1\nRelative \u211322\nIVP\n\u03c0-FNO\n1.202\n0.025\n1.298\n0.033\nFNO\n2.260\n0.049\n2.579\n0.070\nDiff3\n\u221246.8%\n-\n\u221249.7%\n-\nBVP\n\u03c0-FNO\n1.050\n0.044\n1.423\n0.061\nFNO\n1.172\n0.049\n1.596\n0.066\nDiff3\n\u221210.4%\n-\n\u221210.8%\n-\nIP\n\u03c0-FNO\n2.085\n0.073\n2.303\n0.083\nFNO\n2.124\n0.076\n2.333\n0.085\nDiff3\n\u22121.8%\n-\n\u22121.3%\n-\n1Mean absolute error in vehs/km\n\u2212 \u2212 1Mean absolute error in vehs/km 2Relative l2 norm error - unitless metric 3Percent error reduction of \u03c0-FNO relative to FNO\nuse boundary data with no wavelets. The goal is to train \u03c0-FNO with simple solutions and assess the out-of-sample error as input conditions become complex. For the traffic flow example, the input complexity refers to the initial distribution of vehicles on the road (u0) and the influence of traffic lights at the road boundaries (ub). These two input factors put together can generate complex traffic density dynamics u. Figure 6 illustrates these incremental cases for u0 and ub. This approach essentially places a limit on the size of training data for operator learning. The results from this systematic evaluation of \u03c0-FNO are discussed in detail in Section 6.\n# 5. Results I: Numerical experiments\nThis section presents a detailed numerical evaluation of the proposed \u03c0-FNO model for three different problem settings (introduced in Section 2): initial value problems (IVP), boundary value problems (BVP), and inverse problems (IP). Throughout the remainder of the paper, the IVP assumes a traffic setting of a closed-loop traffic system (e.g., a ring road), and BVP and IP assume an arterial road section with a free upstream entrance and signal-controlled downstream exit. All road sections have a unit length of 1 km, and density solutions are predicted for 300 to 600 secs. For all road sections, we assume free flow speed vfree = 60 kmph, jam density kjam = 120 vehs/km, and a maximum flow (saturation flow) qmax = 1800 vehs/hr.\n# 5.1. Evaluation metrics\nWe summarize the solution errors for IVP, BVP, and IP in Table 1. The error metric is the average mean absolute error (MAE) of 50 data samples in units of vehs/km. Each data sample corresponds to the solution predicted using an initial and/or boundary condition. The MAE metric is in comparison to the reference solution obtained from the numerical scheme. A unitless relative \u21132 norm metric is also shown in the table. The validation and testing error corresponds to the data samples with input conditions indicated in Figure 6. The prediction errors for both \u03c0-FNO and FNO models are provided for comparison. We see that the largest MAE incurred for both solvers is \u22643.00 vehs/km (2.50%), an acceptable threshold for practical applications. The error metrics for validation and testing samples are similar, implying that the solvers generalize well to unseen input conditions and do not overfit (Murphy, 2012). Further, \u03c0-FNO incurs a lower average error than FNO, indicating the benefit of physics-informed training. We observe this more in the initial valued problems for which the reduction in the prediction error is close to 50%. The solvers can also learn solutions for the ill-posed inverse problems though the errors are 2\u00d7 higher than those for the forward problems. Also, error metrics for \u03c0-FNO and FNO only differ slightly (\u22482%) for the inverse problems. We will enlighten these results in the following sub-sections.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8123/8123acf6-b0ac-4760-b828-186391bea334.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7: Sample prediction using \u03c0-FNO for the initial value problem (bottom row) for different initial data u0. The reference solution generated from the Godunov scheme is shown (top row) for comparison. Refer to Figure 2 for color coding.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/20c0/20c05bc0-482e-4f44-950a-dda96a5169ed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Solution profiles corresponding to Figure 7 (a)</div>\nFig. 8: Comparison of the solution profiles at the different x and t values for the initial value problem. The profiles are cross-sections of the heatmaps shown in Figure 7.\n# 5.2. Initial value problems (IVP)\nWe first present the solutions predicted by \u03c0-FNO for IVP where only an initial condition aivp dictates the whole solution u. Four sample results using four different initial conditions aivp are shown in Figure 7. The bottom row shows the predicted solution \ufffdu over the complete space-time domain \u2126. The top row shows the reference solution u obtained from the Godunov numerical scheme for comparison. Figure 7 (a) is an example from the training data. Figures 7 (b)-(d) are examples from the testing data. The sub-figure captions denote the respective sample paths used as input data to the solver. We observe that the predicted solution \ufffdu is in good agreement with the true solution u for all the cases. The dissipation and speed of solution discontinuities (shocks), which depend on the initial solution profile, are flawlessly captured in all examples. This suggests a good generalization of \u03c0-FNO to unseen (out-of-sample) initial conditions. This is evident from test samples in Figures 7 (b)-(d). We also see that the predicted solutions honor the periodic nature of the boundary conditions. For instance, the solution waves exiting at x = 1 km emerge exactly at x = 0 km at the next time-step without any delay or artifacts. Further, we do not explicitly enforce the initial condition u0 (which is input to \u03c0-FNO) in the predicted solution \ufffdu. Instead, \u03c0-FNO learns to predict the initial condition u0. For this specific part of the solution, we see that the solver is performing an identity mapping. To verify these findings, compare the solution profiles at x = 0 km and t = 0 min for two examples shown in Figure 8. From Figure 8 (a) and (b) (first row), we observe that the solver accurately replicates the input initial condition and the periodic boundary conditions. The input shock profile u(x, 0) and the boundary solution u(0, t) are very accurate.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/563e/563ea129-f5be-4f06-9e8a-4ea231639b09.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Solution profiles corresponding to Figure 7 (d)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c561/c5611c99-500c-435c-a6ed-b58fce8835dc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9: Sample predictions using \u03c0-FNO for BVP for different initial data u0. The reference solution generated from the Godunov scheme is shown or comparison. The sub-figures (a) is a training instance, and (b)-(d) are testing instances. Refer to Figure 2 for color coding.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d52/4d52ba64-9074-44dd-9bad-e7129e55b397.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 10: Sample predictions using \u03c0-FNO for BVP for different boundary data ub. The reference solution generated from the Godunov scheme is hown for comparison. The sub-figures (a) is a training instance, and (b)-(d) are testing instances. Refer to Figure 2 for color coding.</div>\nFurther, Figure 8 (b) is a test sample. Note that previous studies enforce the initial conditions using a bubble function (hard constraint) or using additional weights on the loss function (soft constraint) (Patel et al., 2022). We found in our experiments that \u03c0-FNO can sufficiently learn the overall solution dynamics without any additional constraints. A limitation of the \u03c0-FNO model noted for IVP is that the solutions get dispersed near the end of the time domain, especially for backward-moving waves.\n# 5.3. Boundary value problems (BVP)\nWe next present the predicted solutions for BVP where both initial and boundary data abvp collectively influence the density solution u. Eight example predictions \ufffdu corresponding to eight different input data are shown in Figure 9 and Figure 10. The examples in Figure 9 are for different initial data u0 while the examples in Figure 10 are for different boundary data ub. Figure 9 (a) and Figure 10 (a) corresponds to training data. Figures 9 (b)-(d) and Figures 10 (b)-(d) are examples from the testing data. The reference solution u from the Godunov scheme is also shown for comparison. We observe that \u03c0-FNO qualitatively reconstructs the solution for all examples shown in Figures 9 and 10. In particular, the reconstruction of the shockwaves separating different traffic regimes (e.g., free-flow to congestion tran-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1a45/1a4511f9-ebbb-4a0d-b03f-0d4c3620cfbd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f0f0/f0f0d99a-0261-4a28-acbc-80a1c00c33ac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ample predictions using \u03c0-FNO for IP for different interior data up (black-dotted curve i Godunov scheme is shown for comparison. Refer to Figure 2 for color coding.</div>\n<div style=\"text-align: center;\"> IP for different interior data up (black-dotted curve in the top row). The reference soluti parison. Refer to Figure 2 for color coding.</div>\nsition and vice-versa), the rarefaction waves that represent vehicle queues\u2019 dissipation, and the minor waves emanating from the boundaries are all close to the ground truth. The shock waves are mostly sharp, and unphysical artifacts in the predictions are minimal. Similar to IVP, \u03c0-FNO learns to generalize to more complex initial and boundary conditions. For instance, Figure 10 (a) is a density solution with two traffic signal cycles (i.e., two wavelets) at the boundary x = 1 km, which is an instance from the training data. Figure 10 (b)-(d) are the solution for four, six, and eight traffic signal cycles that are not seen during training. The solver learned the physical queuing dynamics (patterns of the red band) for arbitrary combinations of the number of signal cycles and cycle lengths. This shows the extrapolation capability of the \u03c0-FNO to boundary conditions of higher complexity. Similar insights on the generalization to different initial conditions can be seen in Figure 9. Similar to IVP, we collectively learn the initial and boundary condition with the complete solution instead of explicitly enforcing them during the model training. To illustrate this, we compare solution profiles at x = 1 km (upstream boundary condition) and t = 0 min (initial condition) in Figure 11. The solution profiles correspond to Figure 10 (b) and Figure 10 (d), both of which are test samples. The \u03c0-FNO model perfectly reconstructs the initial data u(x, 0) and the major wavelets in the boundary data u(1, t). The profile u(x, 0) in Figure 11 (b) shows generalization to randomly varying input data. We also noticed for a few samples that the end wavelets are smeared out for boundary data > b6, as seen in the profile u(0.7, t) in Figure 11 (b).\n# 5.4. Inverse problems (IP)\nWe next present \u03c0-FNO solutions for IP. Figure 12 shows four sample results corresponding to four different input conditions aip. Note that the reference solution u is obtained assuming known initial and physical boundary data\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4094/4094b036-a29c-45f3-86da-2ab959f63675.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ion profiles corresponding to Figure 12 (b) (b) (a) Solution profiles corresponding to Figure 12 (c) 3: Comparison of solution profiles at different x and t values for the inverse problem results shown in Figure 12.</div>\na = (u0, ub) using the Godunov scheme. In contrast, \u03c0-FNO only uses the initial data and the internal boundary data aip = (u0, up) as input. The location of the internal boundary data (vehicle trajectories) \u2126p is shown as the black dotted curve in the reference solution u (top row). Figure 12 (a) is a training sample, and Figure 12 (b)-(d) are testing samples. The solution profiles are shown in Figure 13. From Figure 12, we observe a good agreement between predicted and reference solutions. Despite being ill-posed owing to unknown boundary conditions, \u03c0-FNO uniquely reconstructs all the observable features in the solution, such as the effect of initial conditions, vehicle queue formation at boundaries, and the discontinuities separating different traffic regimes. The generalizability to higher-order complexity in initial and boundary conditions is in-par with the forward problems. See predictions for three, six, and eight wavelet boundary data in Figure 12 (b)-(d). Another observation is that the sparse information from the input data up can infer physically plausible queueing dynamics. As an example, the last vehicle trajectory in Figure 12 (b) cannot dictate how the queue dissipates in the future period, i.e., the boundary separating the orange and blue region. This requires additional input from the upstream boundary (x = 0 m). But, \u03c0-FNO inferred this information from the downstream vehicle trajectory and produced a plausible reconstruction. Another instance is the shock dynamics inferred between the 4th and 5th trajectories in Figure 12 (d). The distribution of input vehicle trajectories \u2126p will affect the reconstructed solution. Nevertheless, \u03c0-FNO produces a feasible solution even with unknown boundary conditions. Finally, it is worth noting that \u03c0-FNO failed to capture the minor waves emanating from boundaries for IP. Compare the dark blue regions in Figure 12. The minor waves are due to small random variations in the boundary condition. This is also evident from the solution profile u(0.0, t) in Figure 13, where the solver fails to exactly reproduce the randomly fluctuating minor waves. This limitation is expected since the input data up does not capture these waves and consequently is not observed in the predicted solution \ufffdu. Recall from Figure 10 that the forward problem uses boundary data as input and \u03c0-FNO faithfully reconstructs these minor waves. Nevertheless, the predicted solution for the inverse problem captures the primary trend and ignores the minor waves as noise.\n# 5.5. Benefits of physics-informing\nIn this section, we evaluate the benefits of physics-informed training on the predicted solution, especially on the behavior of shocks. Figure 14 presents the comparison results for an example IVP, and Figure 15 presents that for an example BVP. We recall that the FNO prediction (middle row) is physics-uninformed, and the \u03c0-FNO prediction (bottom row) is physics-informed. The observed benefits are different for both problems, as discussed below. From Figure 14, we see that the solution predicted by FNO significantly deviates from the true solution as time progresses. See the t-section solution profile u(x, 4) in Figure 14. The profiles indicate that the solution gets smeared to an average value past a certain time, in this case, when t > 3 min. Thus, FNO fails to carry over the solution dynamics to longer time periods, possibly due to insufficient inputs. On the other hand, \u03c0-FNO successfully predicts the complete solution with only a slight deviation from the true solution. This suggests that physics-informed training aids better in learning the long-term dynamics compared to mere data loss. We also found that the FNO solutions are noisy or contain unphysical artifacts that are not interpretable from a physics viewpoint. This occurs even during the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c11/7c1172d9-8007-47d1-9d5a-b2700008c8d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/13a0/13a0a034-987d-4bfe-a2df-1e610611b1cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c15/3c15f467-beca-4543-9912-773d76adbb2b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Example 1</div>\ninitial period t < 3 min, as noticed in the x-section profiles in Figure 14. This noisy behavior is not observed in the \u03c0-FNO predictions. Physics-informing has a different impact on the solutions to BVP, as seen from the comparison results in Figure 15. Both FNO and \u03c0-FNO reasonably reconstruct all the observable features of the solution. The primary difference appears in the behavior of shocks or solution discontinuities. The heatmaps in Figure 15 show that the discontinuities in the FNO solution are smeared-out or noisy. For instance, the blue and red regions in the solution heatmap represent low-density and high-density solutions. The transition from low-density to high-density (thin, light blue region) represents the discontinuous shock solution. The width of this transition region is higher in the FNO solution in comparison to the \u03c0-FNO solution, which implies FNO predicts a smeared solution instead of a sharp shock. This\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cbe3/cbe3126b-be4c-4059-b2e8-f1501eb17159.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Example 2</div>\nDiscretization\nresolution\n(m \u00d7 n)\nForward problem\nInverse problem\nConventional method:\nGodunov scheme (LeVeque, 1992)\n\u03c0-FNO model\n(this paper)\nConventional method:\nExtended KF (Blandin et al., 2012)\n\u03c0-FNO model\n(this paper)\n(600 \u00d7 50)\n0.08 (\u00b1 0.01)\n0.31 (\u00b1 0.04)\n0.55 (\u00b1 0.07)\n0.32 (\u00b1 0.04)\n(1200 \u00d7 100)\n0.25 (\u00b1 0.01)\n0.75 (\u00b1 0.07)\n3.02 (\u00b1 0.32)\n0.75 (\u00b1 0.07)\n(2400 \u00d7 200)\n1.16 (\u00b1 0.13)\n4.20 (\u00b1 0.43)\n13.23 (\u00b1 0.53)\n4.20 (\u00b1 0.43)\n<div style=\"text-align: center;\">Table 3: Summary of generalization error rates</div>\nModels\nInitial condition\nBoundary condition\nInput complexity\nIVP: FNO\nIVP: \u03c0-FNO\n0.09x0.58\n\u03b1\n+ 2.07\n0.08x0.45\n\u03b1\n+ 1.20\n\u2212\nBVP: FNO\nBVP: \u03c0-FNO\n0.10x0.50\n\u03b1\n+ 0.99\n0.08x0.49\n\u03b1\n+ 0.89\n1x\u03b2\u22642.50\n\ufffd1.00 + 0.14x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd1.00 + 0.14x\u03b2 + 0.06(x\u03b2 \u22122.50)1.41\ufffd\n1x\u03b2\u22642.50\n\ufffd0.91 + 0.11x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd0.91 + 0.11x\u03b2 + 0.11(x\u03b2 \u22122.50)1.16\ufffd\nIP: FNO\nIP: \u03c0-FNO\n0.01x0.85\n\u03b1\n+ 1.96\n0.01x0.91\n\u03b1\n+ 1.96\n1x\u03b2\u22642.50\n\ufffd1.92 + 0.08x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd1.92 + 0.08x\u03b2 + 0.19(x\u03b2 \u22122.50)1.07\ufffd\n1x\u03b2\u22642.50\n\ufffd1.95 + 0.04x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd1.95 + 0.04x\u03b2 + 0.18(x\u03b2 \u22122.50)1.17\ufffd\nModel complexity I\u2217\nM1 (mk = 120, nk = 24)\n0.08x0.49\n\u03b1\n+ 0.89\n1x\u03b2\u22642.50\n\ufffd0.92 + 0.11x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd0.92 + 0.11x\u03b2 + 0.11(x\u03b2 \u22122.50)1.16\ufffd\nM2 (mk = 90, nk = 20)\n0.09x0.46\n\u03b1\n+ 1.37\n1x\u03b2\u22642.50\n\ufffd1.39 + 0.12x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd1.39 + 0.12x\u03b2 + 0.10(x\u03b2 \u22122.50)1.32\ufffd\nM3 (mk = 75, nk = 18)\n0.12x0.46\n\u03b1\n+ 1.81\n1x\u03b2\u22642.50\n\ufffd1.84 + 0.15x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd1.84 + 0.15x\u03b2 + 0.12(x\u03b2 \u22122.50)1.04\ufffd\nM4 (mk = 60, nk = 16)\n0.16x0.40\n\u03b1\n+ 2.47\n1x\u03b2\u22642.50\n\ufffd2.52 + 0.16x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd2.52 + 0.16x\u03b2 + 0.16(x\u03b2 \u22122.50)1.14\ufffd\nTraining data I\u2020\nD1 (i0 \u2212i3, b0 \u2212b2)\n0.27x0.25\n\u03b1\n+ 1.57\n1x\u03b2\u22642.50\n\ufffd1.78 + 0.12x\u03b2\n\ufffd+ 1x\u03b2>2.50\n\ufffd1.78 + 0.12x\u03b2 + 0.15(x\u03b2 \u22122.50)1.35\ufffd\nD2 (i0 \u2212i6, b0 \u2212b3)\n0.26x0.43\n\u03b1\n+ 0.91\n1x\u03b2\u22642.50\n\ufffd1.19 + 0.08x\u03b2\n\ufffd+ 1x\u03b2>3.50\n\ufffd1.19 + 0.08x\u03b2 + 0.23(x\u03b2 \u22123.50)0.97\ufffd\nD3 (i0 \u2212i10, b0 \u2212b4)\n0.07x0.62\n\u03b1\n+ 1.07\n1x\u03b2\u22642.50\n\ufffd1.12 + 0.08x\u03b2\n\ufffd+ 1x\u03b2>4.50\n\ufffd1.12 + 0.08x\u03b2 + 0.22(x\u03b2 \u22124.50)0.92\ufffd\nD4 (i0 \u2212i15, b0 \u2212b5)\n0.15x0.39\n\u03b1\n+ 0.85\n1x\u03b2\u22642.50\n\ufffd1.05 + 0.06x\u03b2\n\ufffd+ 1x\u03b2>5.50\n\ufffd1.05 + 0.06x\u03b2 + 0.47(x\u03b2 \u22125.50)0.71\ufffd\n\u2217,\u2020 All models are \u03c0-FNO\n\u2217,\u2020 All models are \u03c0-FNO\nsmeared solution was consistently observed in all the solutions predicted by the FNO model. Further, FNO produces noisy predictions, as evident from the two wavelets (example 1) and three wavelets (example 2) shown in the xsection solution profiles u(1, t) in Figure 15. Whereas physics-informing helps minimize these exogenous noises in the predictions and ensure physically consistent solutions.\n# 5.6. Computation time comparisons\nWe conclude this section with brief comments on the computational run time of the \u03c0-FNO model and its comparison with conventional methods. For the conventional methods, we choose the Godunov numerical method (LeVeque, 1992) for the forward problem and an extended Kalman Filter implementation (Blandin et al., 2012) for the inverse problem. The latter is the fastest known scheme for solving inverse problems in traffic flow using the LWR model. It is not the most accurate method (Blandin et al. (2012) mention better extensions of the Kalman filter), but our purpose is to compare CPU times. The physics-informed \u03c0-FNO model is used for both the forward and inverse problems. We report average CPU times for three different domain discretization leves in Table 2. All methods are evaluated with the same input settings and on the same CPU: an Apple computer with an M1 Mac CPU chipset. From Table 2, we observe that the Godunov scheme outperforms \u03c0-FNO in the forward problem, whereas \u03c0-FNO has the least run time in the inverse problems. This computational run time difference is more pronounced at larger discretizations. We add that \u03c0-FNO run times can be further reduced by efficient parallelization using a GPU chipset. Such parallelization can be easily done with open-source packages such as PyTorch (Paszke et al., 2019) with a few additional lines of code. However, how to parallelize the conventional methods has not been addressed in the literature. This is an avenue that we leave to future research.\n# 6. Results II: Generalization performance\nThe previous section showed the potential of \u03c0-FNO in predicting macroscopic traffic states under different input settings. This section extends our evaluation of this deep learning model by examining their generalization perfor-\nThe previous section showed the potential of \u03c0-FNO in predicting macroscopic traffic states under different input settings. This section extends our evaluation of this deep learning model by examining their generalization perfor-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e4f/6e4f0d6b-6a61-455c-82c0-b41c5d0f7d4c.png\" style=\"width: 50%;\"></div>\nmance, by which we mean the prediction error on previously unseen input conditions (out-of-sample error). We take an empirical approach to assess the generalization performance, as detailed in the following sub-sections. The results from this analysis are crucial for their reliable application in practice.\nWe first study the generalization error rates as a function of input complexity. Recall that input complexity refers to the number of steps in the initial condition (number of different vehicle queues) and the number of wavelets in the boundary condition (number of traffic signal cycles). A higher number of steps and wavelets presumably generate complex solutions, e.g., more interacting shock and rarefaction waves. We train \u03c0-FNO with solutions consisting of up to three steps in the initial data and up to two wavelets in the boundary data. We then assess the solution error as a function of the order of input complexity. The generalization results for IVP are summarized in Figure 16, where we plot the average solution errors for all input conditions i0 - i40. Each data point is the average mean absolute error (MAE) of 50 samples. The vertical dashed line separates the training data (i0 - i3) and testing data (i4 - i40). The blue data points correspond to FNO, and the orange corresponds to \u03c0-FNO. We observe that the out-of-sample error gradually increases as the number of steps in the initial condition increases. The error growth appears sub-linear for both \u03c0-FNO and FNO. To quantify the error growth, we fit a generic power law function defined as\n# y = k0 + k1xk2 \u03b1 , {k1 \u22650, 0 < k2 \u22643, k3 \u22650}, x\u03b1 \u2208\ufffd0, . . . , 40\ufffd\n\ufffd\ufffd and is displayed in Figure 16 as solid curves. The free parameters k0, k1, and k2 characterize the error growth and are optimized using least squares. The exponent k2 (predominantly) determines the rate of growth in the power law function and hence the generalization error of the solver. The fitted growth functions are shown in Table 3 (row: Input complexity - IVP). Table 3 shows that error growth functions for initial value problem are 0.09x0.57 \u03b1 +2.08 (FNO) and 0.08x0.45 \u03b1 +1.20 (\u03c0-FNO). We interpret these functions as follows. The exponent value for both solvers is < 1, which suggests a sub-linear growth in the generalization error with input complexity. An additional unit of step in the initial condition increases the solution error by +0.051x\u22120.43 \u03b1 vehs/km for FNO and +0.036x\u22120.55 \u03b1 vehs/km for \u03c0-FNO. In other words, every additional discontinuity or inhomogeneity in the vehicle queue incurs a positive error in the density prediction. All k0, k1, and k2 values for \u03c0-FNO are lower than FNO, suggesting better generalization performance due to physicsinforming, in line with our previous results. This is also evident from Figure 16, where the growth function for \u03c0-FNO is relatively constant compared to that of FNO. Next, we study the error growths for BVP as a function of initial and boundary conditions shown in Figure 17. Note that for testing different initial conditions i \u2208{i0, . . . , i40}, we use boundary functions from training domain b \u2208{b0, b1, b2}. And, vice-versa when testing different boundary conditions, i.e., for b \u2208{b0, . . . , b8} we choose i \u2208{i0, i1, i2, i3}. Figure 17 (a) shows the error growths for different initial conditions and is similar to that observed for initial value problems in Figure 16. Figure 17 (b) shows the error growth for an increasing number of wavelets in the boundary condition. The vertical dashed line separates the train and test boundary conditions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3617/3617f57a-a9bf-47d6-8cf3-79a79c54577b.png\" style=\"width: 50%;\"></div>\nFig. 17: [Input complexity - BVP] Generalization errors for the boundary value problem. The solid curves capture the e dashed line separates the training and testing domains. (a) Error curve obtained for different initial conditions and  different boundary conditions.\n<div style=\"text-align: center;\">Fig. 17: [Input complexity - BVP] Generalization errors for the boundary value problem. The solid curves capture the error growth, and the vertica dashed line separates the training and testing domains. (a) Error curve obtained for different initial conditions and (b) Error curve obtained for different boundary conditions.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c160/c160c04c-4554-48cb-85aa-7a9af37d23e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig. 18: [Input complexity - IP] Generalization errors for the inverse problem. The solid curves capture the error growth, and the vertical dashe ne separates the training and testing domains. (a) Different initial conditions, and (b) Different boundary conditions.</div>\n\ufffd \ufffd \ufffd\ufffd \ufffd\ufffd where 1x\u03b2\u2264x\u03b20 (1x\u03b2>x\u03b20) is an indicator function which takes the value 1 if x\u03b2 \u2264x\u03b20 (x\u03b2 > x\u03b20) and 0 otherwise. x\u03b20 is a threshold separating training and testing domain. The first part of the piecewise function (linear) captures training error, and the second part (power law) captures testing error. The exponent k3 predominantly dictates the rates of error growth and will be used here to interpret the results. The fitted piecewise curves are summarized in Table 3 (row: Input complexity - BVP). From Figure 17 (b), we see that the error curve steadily increases with the number of wavelets for both \u03c0-FNO and FNO. This is expected since boundary conditions have a more significant impact on the dynamics of traffic density than the initial condition. The error curve for the training boundary conditions (\u2264b2) is relatively flat compared to that obtained for testing boundary conditions (> b2), which is also expected. Further, Table 3 shows that the exponent value k3 for the error curves are k3 = 1.41 (FNO) and k3 = 1.16 (\u03c0-FNO), which means a super-linear growth in the generalization error, although \u03c0-FNO incurs a lower exponent value than FNO. Further, every additional wavelet (traffic signal cycle) in the boundary data increases the generalization error by +0.08(x\u03b2 \u22122.50)0.41 (FNO) and +0.02(x\u03b2 \u22122.50)0.16 (\u03c0-FNO). However, the latter error rate is nearly constant. Nevertheless, these error growths are relatively faster than those observed for IVP. Similar insights can be obtained for the IP shown in Figure 18 and Table 3 (row: Input complexity - IP). We see a sub-linear error growth for initial conditions and super-linear growth for boundary conditions. However, the shape of error curves is different from that observed for forward problems. For instance, the error growth functions for the initial condition are 0.01x0.85 + 0.99 (FNO) and 0.01x0.91 + 1.96 (\u03c0-FNO), which has higher exponents compared to those obtained for forward problems. We also observe a large variation in the errors for the initial conditions in Figure\n18 (a), which could be the reason for such a higher error rate. On the other hand, the error growth for boundary conditions is similar to the forward problem, except for a higher intercept. Also, there is a clear distinction in the error growth for training and testing boundary conditions. Finally, it is also worth noting that \u03c0-FNO and FNO have a similar error growth for the IP.\n# 6.2. Effect of model complexity\nThe previous analysis showed the generalization error curves specific to a chosen \u03c0-FNO model architecture and training data. We further study how these error curves vary with respect to two key factors discussed in the operator learning framework (3), namely, the model complexity G\u0398 (in this sub-section) and training data distribution P train (in the next sub-section). We first study the effect of model complexity on the generalization error of \u03c0-FNO. Model complexity here refers to the learning capacity (approximation power) of the solution operator G\u03b8, which we examine using two approaches. In the first approach, we change the number of Fourier modes retained in x and t directions (mk and nk); refer to Figure 4. A higher choice of mk and nk corresponds effectively to a higher number of parameters and hence higher model complexity. We consider four levels of model complexity, M1 (mk = 128, nk = 24), M2 (mk = 90, nk = 20), M3 (mk = 75, nk = 18) and M4 (mk = 60, nk = 16), and train four different \u03c0-FNO models with the same training data, keeping every other parameter in the problem fixed. We evaluate the solvers with the same testing data. Figure 19 shows the error curves obtained for all four \u03c0-FNO models for BVP. We observe that all four models have similar error curves, both for the initial and boundary conditions. This is also evident from the fitted functions in Table 3 (row: Model complexity I). The predominant error term for the initial condition is \u223cx0.49 \u03b1 (model M1), \u223cx0.46 \u03b1 (model M2), \u223cx0",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of learning solutions to macroscopic traffic flow models, particularly focusing on the Lighthill-Whitham-Richards (LWR) PDE. Traditional methods face limitations in accurately predicting traffic density dynamics, necessitating a new approach that can effectively handle shocks and rarefactions in traffic conditions.",
        "problem": {
            "definition": "The problem involves predicting traffic density dynamics based on sparse and heterogeneous input data, specifically for the LWR traffic flow model, which is characterized by finite propagation speeds and may include non-unique solutions due to shocks and rarefactions.",
            "key obstacle": "Existing methods struggle with handling irregularities in the solutions, particularly where discontinuities occur, leading to poor convergence and unrealistic predictions."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for a method that can learn from historical traffic data while incorporating physical laws governing traffic flow, specifically through the use of a physics-informed neural operator.",
            "opinion": "The proposed solution is a physics-informed Fourier neural operator (\u03c0-FNO) that integrates physics-based regularization into the training process, allowing for better predictions of traffic density dynamics.",
            "innovation": "The \u03c0-FNO differs from existing methods by incorporating a physics regularizer derived from the integral form of the conservation law, enabling it to learn solutions with discontinuities more effectively."
        },
        "method": {
            "method name": "Physics-informed Fourier neural operator",
            "method abbreviation": "\u03c0-FNO",
            "method definition": "The \u03c0-FNO is defined as a parametric operator that maps input traffic data to corresponding solutions of the LWR PDE in a supervised learning framework, incorporating a physics-informed loss during training.",
            "method description": "This method leverages operator learning to predict traffic density dynamics based on initial and boundary conditions, learning from both smooth and complex traffic scenarios.",
            "method steps": [
                "Define the input conditions (initial and boundary data).",
                "Generate training data using numerical simulations of the LWR model.",
                "Train the \u03c0-FNO using the generated data, optimizing the parameters based on both data loss and physics loss.",
                "Evaluate the trained operator on new input conditions to predict traffic density dynamics."
            ],
            "principle": "The effectiveness of the \u03c0-FNO arises from its ability to learn a family of solutions rather than a single pointwise solution, thus generalizing better across varying traffic scenarios."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the LWR traffic flow model with various initial and boundary conditions, including both training and testing datasets with increasing complexity.",
            "evaluation method": "Performance was assessed using mean absolute error (MAE) and relative \u21132 norm error metrics, comparing the \u03c0-FNO predictions against reference solutions obtained from traditional numerical methods."
        },
        "conclusion": "The \u03c0-FNO demonstrates superior performance in predicting traffic density dynamics across various scenarios, significantly reducing prediction errors compared to traditional methods, particularly in handling complex and irregular traffic conditions.",
        "discussion": {
            "advantage": "The primary advantages of the \u03c0-FNO include its ability to handle discontinuities in the solutions and its improved generalization performance due to the incorporation of physics-informed training.",
            "limitation": "Despite its strengths, the \u03c0-FNO may still struggle with minor wave features in the solutions when these features are not adequately represented in the input data.",
            "future work": "Future research should focus on enhancing the model's ability to capture minor wave dynamics and exploring more efficient training data selection methods to further improve generalization."
        },
        "other info": {
            "keywords": [
                "Fourier neural operator",
                "traffic state estimation",
                "LWR traffic flow model",
                "physics-informed machine learning",
                "inverse problems"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of learning solutions to macroscopic traffic flow models, particularly focusing on the Lighthill-Whitham-Richards (LWR) PDE."
        },
        {
            "section number": "1.4",
            "key information": "The proposed solution is a physics-informed Fourier neural operator (\u03c0-FNO) that integrates physics-based regularization into the training process, allowing for better predictions of traffic density dynamics."
        },
        {
            "section number": "2.1",
            "key information": "The \u03c0-FNO is defined as a parametric operator that maps input traffic data to corresponding solutions of the LWR PDE in a supervised learning framework, incorporating a physics-informed loss during training."
        },
        {
            "section number": "2.2",
            "key information": "Existing methods struggle with handling irregularities in the solutions, particularly where discontinuities occur, leading to poor convergence and unrealistic predictions."
        },
        {
            "section number": "5.1",
            "key information": "The effectiveness of the \u03c0-FNO arises from its ability to learn a family of solutions rather than a single pointwise solution, thus generalizing better across varying traffic scenarios."
        },
        {
            "section number": "7.1",
            "key information": "The primary advantages of the \u03c0-FNO include its ability to handle discontinuities in the solutions and its improved generalization performance due to the incorporation of physics-informed training."
        },
        {
            "section number": "7.3",
            "key information": "Future research should focus on enhancing the model's ability to capture minor wave dynamics and exploring more efficient training data selection methods to further improve generalization."
        },
        {
            "section number": "6.6",
            "key information": "The \u03c0-FNO demonstrates superior performance in predicting traffic density dynamics across various scenarios, significantly reducing prediction errors compared to traditional methods, particularly in handling complex and irregular traffic conditions."
        }
    ],
    "similarity_score": 0.5883266788894482,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Fourier neural operator for learning solutions to macroscopic traffic flow models_ Application to the forward and inverse problems.json"
}