{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2107.08800",
    "title": "Deep Learning with Nonsmooth Objectives",
    "abstract": "We explore the potential for using a nonsmooth loss function based on the max-norm in the training of an artificial neural network. We hypothesise that this may lead to superior classification results in some special cases where the training data is either very small or unbalanced.\n  Our numerical experiments performed on a simple artificial neural network with no hidden layers (a setting immediately amenable to standard nonsmooth optimisation techniques) appear to confirm our hypothesis that uniform approximation based approaches may be more suitable for the datasets with reliable training data that either is limited size or biased in terms of relative cluster sizes.",
    "bib_name": "peiris2021deeplearningnonsmoothobjectives",
    "md_text": "# Deep Learning with Nonsmooth Objectives\nVinesha Peiris, Vera Roshchina, Nadezda Sukhorukova\nAbstract\nWe explore the potential for using a nonsmooth loss function based on the max-norm in the training of an artificial neural network. We hypothesise that this may lead to superior classification results in some special cases where the training data is either very small or unbalanced. Our numerical experiments performed on a simple artificial neural network with no hidden layers (a setting immediately amenable to standard nonsmooth optimisation techniques) appear to confirm our hypothesis that uniform approximation based approaches may be more suitable for the datasets with reliable training data that either is limited size or biased in terms of relative cluster sizes. keywords: quasiconvex functions, bisection method for quasiconvex minimisation, deep learning MSC 2010: 90C26, 90C90, 90C47, 65D15, 65K10\n# 1 Introduction\nDeep learning is a popular tool in the modern area of Artificial Intelligence. Deep learning has many practical applications, including data analysis, signal and image processing and many others [11, 16, 24]. Deep learning is based on solid mathematical modelling established in [6, 14,17,19]. These works demonstrate that deep learning solves approximation problems and, in its essence, relies on the results of the celebrated Kolmogorov-Arnold Theorem [1,15]. There is a massive amount of publications and internet resources dedicated to deep learning. One of the most comprehensive and thorough textbook on the modern view on deep learning can be found in [11]. This book also touches upon the optimisation side of the problem. In particular, the goal is to minimise the loss function, which is also the measure of \u201cinaccuracy\u201d. Overall, the goal of deep learning is to optimise weights in the network and therefore, this problem can be treated using modern optimisation tools [11\u201313,24]. It is customary to choose the mean least squares loss function to evaluate and optimise the performance of a neural network against a training set. There are several reasons for the popularity of the least squares formulation. From the optimisation perspective this model involves minimising a smooth quadratic objective function, and therefore basic optimisation techniques such as the gradient descent can be successfully employed. The least squares formulation also fares well with the assumption that the errors in the training set have a normal distribution. The goal of this work is to explore an alternative choice of loss functions and to analyse the impact these choices have on the quality of the training and the choice of the optimisation technique. The idea to use a different objective is not new, and was explored in the literature and applications. For instance, l1-norm may be useful in the settings when sparsity is sought after [2]. Other sources where alternative loss functions are explore are [11,18]. Our main hypothesis is that uniform approximation-based models may work better than least squares-based models whenever the size of the data available for training the model is small\n[cs.LG]\nwhile highly reliable or when the data is unbalanced. This is the case for the kind of datasets where each observation is a results of a very expensive procedure or experiment and therefore the availability of data is limited [23]. At the same time, the reliability of such data is high, since every experiment is carefully designed and analysed. Our preliminary numerical experiments appear to confirm the claim. We use an elementary model of an artificial neural network that has no hidden layers and only one output node. It was fairly easy to implement the necessary nonsmooth optimisation technique for this model to test our assumptions, moreover it is common to use such simple models in research literature: they represent building blocks for more complex models used in practice, while are easier to analyse. Since the proposed objective function is nonsmooth, we couldn\u2019t use standard software for the training, and instead implemented a numerical routine from scratch. The relevant optimisation problem turns out to be quasiconvex, it can be solved by the bisection method, with each step requiring a linear feasibility problem to be solved. In [22] the authors explore the application of quasiconvexity in the case of least squares as well, but their approach is fundamentally different from ours, since our approach uses global characteristics of the functions, rather than local. We have implemented the algorithm in MATLAB and ran our numerical experiments in tandem alongside a standard implementation with the mean squared loss error function. Our experiments confirm that for a very small training set our max-norm model may perform better than the standard mean least squares formulation, moreover, adding an extra step that removes the outliers may improves the classification further. The paper is organised as follows. In Section 2 we explain a basic model of an artificial neural network and fix the notation that we use in subsequent sections. In Section 3 we recap some optimisation background related to quasiconvex functions and the bisection method that are used in Section 4 to perform the numerical experiments. Theorem 1 is the central mathematical result of this paper, connecting neural network models and quasiconvex optimisation. Section 5 reports the results of numerical experiments. Finally, Section 6 provides conclusions and future research directions.\n# 2 Training a Simple Artificial Neural Network\nWe consider a basic model of an artificial neural network, and describe a training algorithm that can be used for the model with no hidden layers that is based on a bisection method for quasiconvex programming. A basic model of a neural network consists of several layers of nodes (artificial neurons), connected by directed edges. The network receives a (numerical) signal to its input layer and calculates the output on each subsequent layer using some real valued functions (propagation functions), effectively calculating a composition of these functions as the signal propagates through this network to the output layer. A classic example of an artificial neural network is a handwritten number recognition system, where the input is a grayscale image of a handwritten symbol, and the output is the digit that this handwritten symbol represents [16]. The propagation functions that pass the signal on to the next layer are usually selected from a parametrised\nWe consider a basic model of an artificial neural network, and describe a training algorithm that can be used for the model with no hidden layers that is based on a bisection method for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c71/9c71efb3-bd1b-4fbd-9d29-a1f84c52bcb8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: A simple neural network without hidden layers</div>\nfamily, with parameters (weights) adjusted during the training of the network. We will use the tuple x = (x1,...,xn) \u2208Rn to represent the input of the neural network (for example, this can be the values of the grayscale pixels in the input image), and y \u2208Rm represents the output that encodes the (classification) information extracted by the neural network (for instance, the digit that the handwritten symbol represents). An elementary example of a neural network consists of one input layer with n nodes and the output layer with m nodes (see Fig. 1). Usually the propagation function is chosen as a composition of some univariate function \u03c3 (activation function that plays the role of the sigmoid from models of biological neural networks) with a parametrised affine mapping. The (multilayered) network is then a composition of linear functions with the univariate function \u03c3. In this work we will focus on a simple model with no hidden layers, and hence the input x is converted into the output y by a single composition of an activation function \u03c3 and some affine (linear) parametrised function, so\nwhere wij \u2208R for i \u2208{1,...,m}, j \u2208{0,...n} are the weights. The free parameter w0j is usually called the bias weight, as the model is contextualised as having an additional set of input nodes called biases that always take the value 1, so that the output function is linear on a lifted space with one additional input variable x0 = 1. Most commonly used activation functions are nondecreasing, and in fact it will be convenient for us to assume that \u03c3 is a continuous strictly increasing function. We can rewrite the model in a matrix notation as y(x) = \u03d5(W,x) = \u03c3( \u00afWx+w0), where\np \u2208Rd, so that\n\uf8f0 \uf8fb \uf8f0 \uf8fb The training of a neural network normally consists of minimising a loss function that captures how closely the generated output describes the desired output values. Given a training set Z = {(\u00afx1, \u00afy1),(\u00afx2, \u00afy2),...,(\u00afxN, \u00afyN)} that consists of pairs of inputs and desired outputs, we would like to choose our parameters W in such a way that the outputs produced by the neural network y(\u00afxi) = \u03d5(W, \u00afxi) = \u03c3( \u00afW \u00afxi + w0) are as close as possible to the desired outputs \u00afyj for j \u2208{1,...,N}. A common loss (error) function used in this context is based on mean least squares formulation, N\nwhere \u2225\u00b7\u22252 is the 2-norm,\n goal is to explore a different choice of the loss function, specific\nL\u221e,\u221e(Z,W) = max i\u2208{1,...,N} max j\u2208{1,...,m}|\u00afyi j \u2212\u03d5j(W, \u00afxi)|,\nwhich is effectively a composition of max-norms. We could have made different choices on both levels, but our interest lies in departing reasonably far from the mean least squares. The nature of this function ensures that we count the contribution from outliers in the dataset, rather than discount them via averaging.\n# 3 Quasiconvex Functions and the Bisection Algorithm\nThe notion of quasiconvexity was originally introduced in the area of financial mathematics [10], where the author studied the behaviour of functions with convex sublevel sets, but the term quasiconvexity was introduced much later. Let D be a convex subset of Rn. A function f : D \u2192R is quasiconvex if and only if its sublevel set\nS\u03b1 = {x \u2208D| f(x) \u2264\u03b1}\nis convex for any \u03b1 \u2208R. It is not difficult to observe that this definition is equivalent to requiring f(\u03bbx+(1\u2212\u03bb)y) \u2264max{ f(x), f(y)} \u2200x,y \u2208D, \u03bb \u2208[0,1]. (2)\nis convex for any \u03b1 \u2208R. It is not difficult to observe that this definition is equivalent to requiring\nis convex for any \u03b1 \u2208R. It is not difficult to observe that this definition is equivalent to requiring\nThis characterisation is convenient for proving a number of important properties of quasiconvex functions, for instance, that the supremum of quasiconvex functions is quasiconvex.\n(1)\n(2)\nA function f : D \u2192R, where D is a convex subset of Rn, is called quasiconcave if \u2212f is quasiconvex. Functions that are both quasiconvex and quasiconcave are called quasiaffine (quasilinear). A quasiconvex function does not need to be continuous, same applies to quasiconvcave and quasiaffine functions. In the case of univariate functions, quasiaffine functions are monotone. There are many studies devoted to quasiconvex optimisation [3,5,7,9,20,21]. In these studies, the notion of quasiconvexity appears as one of the possible generalisations of convexity. It is easy to see (e.g. using the characterisation (2)) that if g : Rn \u2192R is quasiconvex and h : R \u2192R is nondecreasing, then the composition f = h \u25e6g is quasiconvex. It appears that a similar statement is true when g is quasiaffine and h is monotone.\n# Lemma 1 (Composition of monotone and quasiaffine functions) Assume that g : Rn \u2192R is quasiaffine and h : R \u2192R is monotone, then the composition f = h\u25e6g is quasiaffine.\n# osition of monotone and quasiaffine functions) Assume that g \nproof Assume that h is nondecreasing and g is quasiaffine, then f = h \u25e6g is quasiconvex. It remains to prove that \u2212f = \u2212h\u25e6g is quasiconvex. Consider an auxiliary function \u02dch : R \u2192R, such that \u02dch(t) = h(\u2212t) and therefore h(t) = \u02dch(\u2212t). It is clear that if h is nondecreasing then \u02dch is nonincreasing and vice versa. We have\n\u2212f = \u2212h\u25e6g = \u2212\u02dch\u25e6(\u2212g).\nSince \u2212\u02dch is nondecreasing and \u2212g is quasiconvex (g is quasiaffine), then \u2212f is quasiconvex and therefore f is quasiaffine. It is also clear from the proof that h does not have to be non-decreasing, it is enough for h to be monotone. \u25a1 A standard technique for minimising quasiconvex functions is the bisection method (see [3, Section 4.2.5]). If a quasiconvex minimisation problem has an optimal solution and a lower and upper bounds on the optimal value of the objective are known, the method proceeds by bisecting the interval between the lower and upper bounds and solving a feasibility problem to detect whether the sublevel set corresponding to that midpoint is nonempty. If the sublevel set is empty, then the bisector becomes the new lower bound, otherwise the new upper bound is assigned the bisector value, and the process continues.\n# 4 Training the Model\nWe consider a simple network without hidden layers (only input and output layers), furthermore we assume that the output layer consists of a single node. In this case on the input x \u2208Rn the single output neuron produces the output\nwhere \u03c3 is a monotone activation function, and W is reduced to a row vector, so we omit the first index and write wj for w1j with j \u2208{1,...,N}. Since the function \u2211n j=1 w jxi j +w0 is affine,\n(3)\nhence quasiaffine, from Lemma 1 we conclude that the composition \u03d5(x) = \u03c3( \u00afwx + w0) is a quasiaffine function. Since changing the sign or adding a constant preserves the quasiaffinity the loss function (1) can be represented as the maximum of quasiaffine functions,\nL\u221e,\u221e(Z,W) = max i\u2208{1,...,N}max{\u00afyi \u2212\u03d5(W, \u00afxi),\u2212\u00afyi +\u03d5(W, \u00afxi)}\nQiasiaffine functions are quasiconvex, and the maximum of quasiconvex functions is quasiconvex, therefore our loss function is quasiconvex in W. We have proved the following result. Theorem 1 In the case of a simple neural network consisting of an input layer and a single output node, the loss function L\u221e,\u221eis quasiconvex. Remark 1 Observe that Theorem 1 can be generalised to the case when the output contains more than one node: sandwiching one more layer of maxima in (4) again results in a quasiconvex function, as the proof is based on the same argument about a maximum of quasiaffine functions. Our next goal is to develop the implementation of the bisection method for our setting. We would like to solve the minimisation problem\nQiasiaffine functions are quasiconvex, and the maximum of quasiconvex functions is quasiconvex, therefore our loss function is quasiconvex in W. We have proved the following result.\n\ufffd\ufffd \ufffd\ufffd Here w = (w0,w1,...,wn) = (w0, \u00afw) \u2208R\u00d7Rn are the weights to be decided, and Z = {(\u00afxi, \u00afyi)}N i=1, with (\u00afxi, \u00afyi) \u2208Rn \u00d7R, i \u2208{1,...,N} is the training set. We know from Theorem 1 that the max function is quasiconvex, hence we can apply the bisection method to this minimisation problem. It is evident that the objective function is nonnegative, so we can choose l0 = 0 as the lower bound for the optimal value. For the upper bound we can substitute any value of the parameter w in the objective, for instance, w = 0 \u2208Rn+1, then\n\ufffd\ufffd \ufffd\ufffd We know that the optimal value of the objective function is between l0 and u0. Let L1 := l0+u0 2 . On each iteration we solve the feasibility problem\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd If the problem is feasible, we let lk = lk\u22121, uk = Lk. Otherwise we let lk = Lk, uk = uk\u22121. We set a threshold \u03b5 > 0 and continue the process until the gap between the upper and the lower bound is smaller than this value, that is, our stopping criterion is\n(4)\n(5)\nThis is a simple procedure, which terminates in a finite number of steps. The convergence is linear [3] and therefore it is essential to start with the most accurate estimations for u0 and l0 to reduce the number of iterations. The feasibility problem (5) that we are required to solve on every iteration can be equivalently rewritten as\nUnder the assumption that \u03c3 is strictly increasing and hence has an inverse, this can be rewritten as\nNotice that this problem is a linear feasibility problem (in other words, it is a system of linear inequalities with respect to w), hence it can be solved with any standard linear programming technique.\nNotice that this problem is a linear feasibility problem (in other words, it is a system of linear inequalities with respect to w), hence it can be solved with any standard linear programming technique. Remark 2 The bisection method works in the case of non-decreasing functions (for example, classical Rectified Linear Unit (ReLU) activation function is not strictly monotone), since the problem remains quasiconvex. The only difference is that the left- and right-hand side values will correspond to the minima and maxima respectively over the set-valued inverse images of \u03c3. Our assumption of strict monotonicity fares well with the choice of the Leaky Rectified Linear Unit (Leaky ReLU) activation function,\nNotice that this problem is a linear feasibility problem (in other words, it is a system of linear inequalities with respect to w), hence it can be solved with any standard linear programming technique.\nRemark 2 The bisection method works in the case of non-decreasing functions (for example, classical Rectified Linear Unit (ReLU) activation function is not strictly monotone), since the problem remains quasiconvex. The only difference is that the left- and right-hand side values will correspond to the minima and maxima respectively over the set-valued inverse images of \u03c3. Our assumption of strict monotonicity fares well with the choice of the Leaky Rectified Linear Unit (Leaky ReLU) activation function,\nThis is a piecewise linear function with only two pieces, changing the linear slope at the origin:\nA common choice of the parameter is \u03b1 = 0.01, that is\nIt is easy to see that the inverse can be calculated explicitly,\nhence the linear feasibility problem (5) can also be written and coded explicitly.\n(6)\n<div style=\"text-align: center;\">Table 1: Original dataset: classification results</div>\nTable 1: Original dataset: classification results\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n89.7%\n108\n25\n13\n224\nUniform approximation\n60.54%\n77\n56\n90\n147\n# 5 Numerical Experiments\nThe goal of our numerical experiments is to test our hypothesis that using a max-type (uniform) loss function (1) may be beneficial in training an artificial neural network in the setting of a reliable but small training dataset. In our numerical experiments, we model this situation by reducing the size of the training set. We use HandOutlines dataset from [8]. This dataset contains 2 classes, the corresponding training set contains 1000 recordings (362 recordings in Class 1 and 638 recordings in Class 2) and the corresponding test set contains 370 recordings (133 recordings in Class 1 and 237 recordings in Class 2). Each record contains 2709 floating point values. The dataset contains the information about hand outlines of the subjects and their age (image type data). The data set was manually labelled as correct and incorrect hand outlines by three volunteers. If all three volunteers agree that a data point is valid, it is labelled as correct and hence, class 2 contains correctly identified data points whereas class 1 contains incorrectly identified data points. The size of the classes is not equal: Class 2 is almost twice the size of Class 1.\n# 5.1 Experiments with the original dataset\nWe start with the original dataset (1000 points training set and 370 points test set) and compare the classification results obtained by MATLAB Deep learning toolbox and the results obtained via our uniform approximation algorithm (using bisection method, \u03b5 = 10\u22125). We use the default activation functions for MATLAB Deep learning experiments and Leaky ReLu function with \u03b1 = 10\u22122 for uniform approximation. We report the classification accuracy and provide the confusion matrix. The classification accuracy gives the proportion of points that were assigned the correct class, and the confusion matrix gives more details: the diagonal entries give the number of elements from each of the two classes that were classified correctly, while the off-diagonal elements correspond to the number of misclassified points. In general, the rows of the confusion matrix correspond to the actual class and the columns correspond to the predicted class. For example, the element at the position (1,2) correspond to the number of points from Class 1 assigned to Class 2 (misclassified). The results of the experiments are shown in Table 1. From Table 1 one can see that MATLAB toolbox is much more accurate (almost 90%), while the uniform approximation is just above 60%.\n<div style=\"text-align: center;\">Table 2: Original dataset: classification results, training and test sets are swapped</div>\n2: Original dataset: classification results, training and test sets are s\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n84.5%\n222\n140\n15\n623\nUniform approximation\n66.7%\n195\n167\n166\n472\nOur intention is to demonstrate that uniform approximation is a better tool when the size of the corresponding training set is small. To demonstrate this numerically, we swap training and test sets and now the training set contains 370 points, while the test set contains 1000 points. The results are presented in Table 2. One can see from Table 2 that the classification accuracy for MATLAB toolbox decreased, which is not surprising, since the size of the training set is reduced and therefore less information is used to train the models. On the other hand, the classification accuracy in the case of uniform approximation has improved. Uniform approximation, due to its nature, treats smaller (underrepresented) groups as valid points, while least squares approximation tends to \u201caverage\u201d and therefore under-represented groups tend to be \u201cignored\u201d. This is a great advantage when the under-represented groups are outliers, but in many cases these points are valid data. On the other hand, the presence of ouliers may decrease may decrease the accuracy in the case of uniform approximation. Therefore, our hypothesis is that uniform approximation approach is preferable in the following cases.\n2. Presence of under-represented groups of valid data or uneven distribution of data betwee the classes (that is, one class is significantly larger than others).\n Limited size of the available data, where most datapoints are val\nThe last case is very common in applications where each datapoint is a result of a very expensive experiment or procedure [23]. Based on our hypothesis, the improvement in the classification accuracy in the case of uniform approximation is due to the fact that many outliers are now removed. Overall, MATLAB toolbox is still more accurate, but this simple experiment encourages us to proceed with the reduction of the training set.\n# 5.2 Experiments with reduced training set\nur next step is to reduce the training set even more. The experimental setup is as follows. 1. We use a reduced size test set (the exact size is specified in each experiment) to train the model and the original training set (1000 points) to test (we use this set for testing, since it is a larger set);\nTable 3: Reduced dataset: classification results for even number of points from each training set\n<div style=\"text-align: center;\">Table 3: Reduced dataset: classification results for even number of points from each class in the</div>\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n83.9%\n218\n144\n17\n621\nUniform approximation\n70.60%\n188\n174\n120\n518\n# 2. The tolerance \u03b5 in the bisection procedure is \u03b5 = 10\u22125.\n# 2. The tolerance \u03b5 in the bisection procedure is \u03b5 = 10\u22125. Our experiments can be divided into two groups.\n1. Equal vs unequal distribution of points between the classes in the training set. In this group of experiments, we are checking if it is harder to create accurate classification rules for unbalanced training sets. 2. Random vs non-random selection of reduced size training set. Random selection of training data is a common approach, since it reduces the chance of getting an \u201cunusual\u201d piece of data. At the same time, non-random selection of points (for example, top 10%) is helpful when one needs to recreate the experiments on the same piece of data.\n1. Equal vs unequal distribution of points between the classes in the training set. In this group of experiments, we are checking if it is harder to create accurate classification rules for unbalanced training sets.\n# Even number of representatives from each class in the train\nIn this experiment, we use 20 points for training: first 10 points from Class 1 and first 10 points from Class 2 (taken from 370 point set, which is the test set for the original dataset). The results are in Table 3. The results demonstrate that MATLAB toolbox is still more accurate, but the uniform approximati based approach is coming closer. Our next step is to consider situations, where the size of the training set remains at the same level, but one of the classes is underrepresented.\nIn this experiment, we use 20 points for training: first 10 points from Class 1 and first 10 points from Class 2 (taken from 370 point set, which is the test set for the original dataset). The results\n# 2.2 Uneven number of representative from each class in the training set\nTraining set contains 40 points Consider the situation where the training set contains 40 points: 35 points from Class 1 and 5 points from Class 2. The results are in Table 4. The classification accuracy is still higher in the case MATLAB toolbox, but this difference\nTraining set contains 40 points Consider the situation where the training set contains 40 points 35 points from Class 1 and 5 points from Class 2. The results are in Table 4. The classification accuracy is still higher in the case MATLAB toolbox, but this difference is reducing. Consider now a symmetric situation where the training set contains 40 points: 5 points from Class 1 and 35 points from Class 2. The results are in Table 5. In this experiment, the classification accuracy is higher for uniform approximation.\n<div style=\"text-align: center;\">Table 4: Reduced dataset: classification results for uneven number of points from each class in the training set: 35 points in Class 1 and 5 points in Class 2</div>\nt: 35 points in Class 1 and 5 points in Class 2\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n74.6%\n116\n246\n8\n630\nUniform approximation\n66.5%\n128\n234\n101\n537\n<div style=\"text-align: center;\">Table 5: Reduced dataset: classification results for uneven number of points from each class in the training set: 5 ponts in Class 1 and 35 points in Class 2.</div>\nt: 5 ponts in Class 1 and 35 points in Class 2.\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n64.3%\n296\n66\n291\n347\nUniform approximation\n69.5%\n193\n169\n136\n502\n<div style=\"text-align: center;\">Table 6: Reduced dataset: classification results for uneven number of points from each class i the training set: 18 points in Class 1 and 2 points in Class 2</div>\nTable 6: Reduced dataset: classification results for uneven number of points the training set: 18 points in Class 1 and 2 points in Class 2\n: 18 points in Class 1 and 2 points in Class 2\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n68.6%\n65\n297\n17\n621\nUniform approximation\n63.30%\n91\n271\n96\n542\n<div style=\"text-align: center;\">Table 7: Reduced dataset: classification results for uneven number of points from each class  the training set: 2 points in Class 1 and 18 points in Class 2.</div>\n: 2 points in Class 1 and 18 points in Class 2.\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n39.5%\n338\n24\n582\n56\nUniform approximation\n56.60%\n267\n95\n339\n299\n<div style=\"text-align: center;\">Table 8: Reduced dataset: classification results for randomly generated training set of 100 points repeated 10 times, the accuracy is averaged.</div>\nated 10 times, the accuracy is averaged.\nMethod\nTest set classification accuracy\nMATLAB toolbox\n56.94%\nUniform approximation\n73.19%\nTraining set contains 20 points Consider the situation where the training set contains 20 points 18 points from Class 1 and 2 points from Class 2. The results are in Table 6. The classification accuracy is still higher in the case MATLAB toolbox, but this difference is not very larger. Consider now a symmetric situation where the training set contains 20 points: 2 points from Class 1 and 18 points from Class 2. The results are in Table 7. In this experiment, the classification accuracy is low for both approaches, which is not surprising, given the size of the training set and uneven distribution of classes. However, the uniform approximation approach is more accurate.\nConclusions Overall, when the size of the training set is reducing, the uniform approximation approach becomes more efficient than MATLAB toolbox (based on MSE). This observation i especially significant when Class 1 is significantly underrepresented.\n# 5.2.3 Random choice of training set points\nIn this section we present the numerical results when the training set points were chosen randomly from the original test set (370 points), while the testing was performed on the original training set (1000 points). The total size of the training sets are 100, 50 and 20. Since the points are chosen randomly, the distribution of the points between classes is also random. Each experiment was repeated ten times and the reported test set accuracy is the average. This approach is related to the commonly used 10-fold cross-validation approach with some adjustment to our problem.\nRandom training set size is 100 We start with the case where the training set contains 100 poin in total. The results are in Table 8. The classification accuracy is higher for uniform approximation.\nRandom training set size is 50 The training set contains 50 points in total. The results are in Table 9. The classification accuracy is higher for uniform approximation, but the gap is slightly decreasing compared to the experiment with 100 points.\nRandom training set size is 20 The training set contains 20 points in total. The results are in Table 10.\n<div style=\"text-align: center;\">Table 9: Reduced dataset: classification results for randomly generated training set of 50 points repeated 10 times, the accuracy is averaged.</div>\nepeated 10 times, the accuracy is averaged.\nMethod\nTest set classification accuracy\nMATLAB toolbox\n57.08%\nUniform approximation\n71.58%\n<div style=\"text-align: center;\">Table 10: Reduced dataset: classification results for randomly generated training set of 100 points: repeated 10 times, the accuracy is averaged.</div>\nints: repeated 10 times, the accuracy is averaged.\nMethod\nTest set classification accuracy\nMATLAB toolbox\n56.57%\nUniform approximation\n69.21%\nThe classification accuracy is higher for uniform approximation. The gap between uniform approximation and MATLAB is quite significant.\nWhy the reduction of the training set leads to the improvement in the classification accuracy in the case of the uniform approximation-based approach? One possible explanation is that by removing a significant proportion of points, we also remove all (or almost all) outliers. If this is the case, then the removal of outliers make uniform approximation a better choice. Most outliers are recording or instrumental error and should be removed. At the same time, if recording or instrumental are common, it is unrealistic to avoid them is real-life applications. In the next section, we provide the results of numerical experiments where points with high absolute deviation from best uniform approximation are considered as outliers.\n# 4 High absolute deviation point removal from the training s\nIn this section we assume that points with the highest absolute deviation from best uniform approximation are treated as outliers. The procedure contains two main steps (recall that we use the original test set for training and the training set for testing due to their size).\n1. Training set reduction Find best uniform approximation using the original test set (370 point Identify the points whose absolute deviation is maximal or close to maximal with a specified tolerance \u03b5. Refine the training set (370 points) by removing these points (outliers).\n2. Actual training Treat the refined set as the training set and perform test classification on the original training set (1000 points).\nNote that since there are several points with almost the same absolute deviation, it is hard to anticipate the threshold \u03b5 that gives a certain percentage of data reduction. We consider two cases. In the first case, the threshold \u03b5 = 10\u22127: 46 points out of 370 are removed. In the second case, our goal is to remove (approximately) half of the training set points: 181 points are treated as outliers and the remaining 189 points are used for training.\n<div style=\"text-align: center;\">Table 11: Reduced dataset: 324 valid points and 46 outliers.</div>\nTable 11: Reduced dataset: 324 valid points and 46 outliers.\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n57.3%\n81\n281\n143\n495\nUniform approximation\n71%\n216\n146\n144\n494\n<div style=\"text-align: center;\">Table 12: Reduced dataset: 181 outliers and 189 valid points</div>\nTable 12: Reduced dataset: 181 outliers and 189 valid points\nTest set\nMethod\nclassification\nConfusion matrix\naccuracy\nMATLAB toolbox\n57.1%\n92\n270\n159\n479\nUniform approximation\n72.4%\n235\n127\n149\n489\nTraining set size is 324 We start with the case where 46 points are treated as outliers and therefore they are removed. The results are in Table 11. The classification accuracy is significantly higher for uniform approximation. Training set size is 189 In this experiment, 181 points are treated as outliers while 189 points are counted as valid points. The results are in Table 12. The classification accuracy is significantly higher for uniform approximation and the gap is increasing compared to the situation where fewer outliers are removed.\nTraining set size is 324 We start with the case where 46 points are treated as outliers and therefore they are removed. The results are in Table 11. The classification accuracy is significantly higher for uniform approximation.\nTraining set size is 189 In this experiment, 181 points are treated as outliers while 189 points are counted as valid points. The results are in Table 12. The classification accuracy is significantly higher for uniform approximation and the gap is increasing compared to the situation where fewer outliers are removed.\n# 6 Conclusions and Future Research\nThe results of the numerical experiments support our hypothesis that uniform approximationbased approach can be more efficient than mean least squares based approach when the training data is reliable while limited in size. This type of data are common in industrial, medical and research applications, where each datapoint is a result of an expensive experiment, medical procedure or expert evaluation that are not performed routinely. The size of such datasets can be very small, but each recording is carefully performed and therefore is a valid point. Based on our experiments, it may be beneficial to use a uniform approximation based approach, rather than the standard approach for this type of data. In the absence of an expert opinion it may be hard to distinguish between outliers and valid points that are underrepresented in a given dataset. This is one of the (many) reasons for gender, racial and other kinds of bias present in modern automatic decision making processes [4] that rely on the standard artificial neural network approach and implicitly assumes Gaussian data\ndistribution. This may lead to automatic discarding of under-represented data points as errors. This problem is not new, but it may be interesting to explore if replacing the mean least squares classifiers with uniform approximation may lead to useful results. Our work represents an initial step in researching the use of uniform approximation to address problems arising with data classification via standard artificial neural networks when a modest but high quality sample of classified data points is available. The following research themes are of future interest. 1. Extend our approach to artificial neural networks with several hidden layers. 2. Study other classification problems with limited or unbalanced training data. 3. Identify the types of activation functions that are efficient for particular datasets, rather than using \u201cstandard\u201d ReLU activation.\ndistribution. This may lead to automatic discarding of under-represented data points as errors. This problem is not new, but it may be interesting to explore if replacing the mean least squares classifiers with uniform approximation may lead to useful results. Our work represents an initial step in researching the use of uniform approximation to address problems arising with data classification via standard artificial neural networks when a modest but high quality sample of classified data points is available. The following research themes are of future interest.\n3. Identify the types of activation functions that are efficient for particular datasets, rather than using \u201cstandard\u201d ReLU activation.\nFinally, we would like to emphasise that the essence of deep learning is in approximation and optimisation and therefore there is a need for more robust mathematical models to tackle these problems. On the other hand, some fast heuristics may be used in the models where the size of complexity makes it hard to apply mathematical optimisation. Generally speaking, this models are not as reliable as those based on mathematical optimisation, but in some cases they are the only way we can handle such problems.\n# Acknowledgement\nThis research was supported by the Australian Research Council (ARC), Solving hard Chebyshev approximation problems through nonsmooth analysis (Discovery Project DP180100602).\n# References\n[1] Arnold, V.: On functions of three variables. Dokl. Akad. Nauk SSSR 114, 679\u2013681 (1957). English translation: Amer. Math. Soc. Transl., 28 (1963), pp. 51\u201354 [2] Bach, F., Jenatton, R., Mairal, J., Obozinski, G.: Convex Optimization with SparsityInducing Norms, chap. 2, pp. 19\u201353. MIT press (2011) [3] Boyd, S., Vandenberghe, L.: Convex Optimization, seventh edn. Cambridge University Press, New York, USA (2009) [4] Buolamwini, J., Gebru, T.: Gender shades: Intersectional accuracy disparities in commercial gender classification. Proceedings of Machine Learning Research 81, 1\u201315 (2018) [5] Crouzeix, J.P.: Conditions for convexity of quasiconvex functions. Mathematics of Operations Research 5(1), 120\u2013125 (1980) [6] Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems 2, 303\u2013314 (1989)\n[7] Daniilidis, A., Hadjisavvas, N., Martinez-Legaz, J.E.: An appropriate subdifferential for quasiconvex functions. SIAM Journal on Optimization 12(2), 407\u2013420 (2002) [8] Dau, H.A., Keogh, E., Kamgar, K., Yeh, C.C.M., Zhu, Y., Gharghabi, S., Ratanamahatana, C.A., Yanping, Hu, B., Begum, N., Bagnall, A., Mueen, A., Batista, G., Hexagon-ML: The ucr time series classification archive (2018). https://www.cs.ucr.edu/~eamonn/ time_series_data_2018/ [9] Dutta, J., Rubinov, A.M.: Abstract convexity. Handbook of generalized convexity and generalized monotonicity 76, 293\u2013333 (2005) [10] de Finetti, B.: Sulle stratificazioni convesse. Ann. Mat. Pura Appl. pp. 173\u2013183 (1949) [11] Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press (2016). http: //www.deeplearningbook.org [12] Gould, S., Hartley, R., Campbell, D.: Deep declarative networks: A new hope. CoRR abs/1909.04866 (2019). http://arxiv.org/abs/1909.04866 [13] Haeffele, B.D., Vidal, R.: Global optimality in neural network training. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 4390\u20134398. IEEE Computer Society (2017). doi 10.1109/CVPR.2017.467. https://doi.org/10.1109/CVPR.2017.467 [14] Hornik, K.: Approximation capabilities of multilayer feedforward networks. Neural networks 4(2), 251\u2013257 (1991) [15] Kolmogorov, A.N.: On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition. Dokl. Akad. Nauk SSSR 114, 953\u2013956 (1957) [16] LeCun, Y., Cortes, C., Burges, C.C.: The mnist database of handwritten digits (1998). http://yann.lecun.com/exdb/mnist/ [17] Leshno, M., Lin, V.Y., Pinkus, A., Schocken, S.: Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks 6(6), 861\u2013867 (1993). https://doi.org/10.1016/S0893-6080(05)80131-5 [18] Marcotte, P., Savard, G.: Novel approaches to the discrimination problem. Mathematical Methods of Operations Research 36, 517\u2013545 (1992) [19] Pinkus, A.: Approximation theory of the MLP model in neural networks. Acta Numerica 8, 143\u2013195 (1999). doi 10.1017/S0962492900002919 [20] Rubinov, A.M.: Abstract convexity and global optimization, vol. 44. Springer Science & Business Media (2013) [21] Rubinov, A.M., Simsek, B.: Conjugate quasiconvex nonnegative functions. Optimization 35(1), 1\u201322 (1995). doi 10.1080/02331939508844124\n[22] Sinha, V.B., Kudugunta, S., Sankar, A.R., Chavali, S.T., Balasubramanian, V.N.: Dante: Deep alternations for training neural networks. Neural Networks 131, 127\u2013143 (2020). doi https://doi.org/10.1016/j.neunet.2020.07.026. https://www.sciencedirect.com/ science/article/pii/S0893608020302677 [23] Steponavi\u02c7c\u02d9e, I., Hyndman, R., Smith-Miles, K., Villanova, L.: Efficient Identification of the Pareto Optimal Set. In: Pardalos P., Resende M., Vogiatzis C., Walteros J. (eds) Learning and Intelligent Optimization. LION 2014. Lecture Notes in Computer Science, vol. 8427. Springer, Cham (2014) [24] Sun, R.Y.: Optimization for deep learning: An overview. Journal of the Operations Research Society of China 8, 249\u2013294 (2020)\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of using alternative loss functions in deep learning, specifically focusing on the potential benefits of a nonsmooth loss function based on the max-norm. The authors highlight the limitations of traditional least squares loss functions, particularly in scenarios with small or unbalanced training datasets, and propose that uniform approximation-based models may yield better results in such cases.",
        "problem": {
            "definition": "The problem this paper aims to solve is the inefficiency of standard mean least squares loss functions in deep learning when dealing with small or unbalanced datasets, which can lead to suboptimal classification results.",
            "key obstacle": "The main challenge is that existing methods often assume a normal distribution of errors, which is not always applicable, particularly in cases where the dataset is small or where certain classes are underrepresented."
        },
        "idea": {
            "intuition": "The authors were inspired by the observation that traditional loss functions might not adequately handle datasets with limited size or skewed distributions, leading to the hypothesis that a uniform approximation-based approach would perform better.",
            "opinion": "The proposed idea involves utilizing a nonsmooth loss function based on the max-norm, which is believed to be more effective for training neural networks under specific conditions of data reliability and size.",
            "innovation": "The key innovation of this method lies in its departure from the mean least squares formulation, introducing a loss function that emphasizes the worst-case error (max-norm), thus potentially improving performance in scenarios where data is limited or unbalanced."
        },
        "method": {
            "method name": "Max-Norm Loss Function",
            "method abbreviation": "MNLF",
            "method definition": "The Max-Norm Loss Function is defined as the maximum of absolute deviations between the predicted outputs and the actual outputs, aimed at minimizing the worst-case error in classification tasks.",
            "method description": "This method focuses on optimizing a neural network's weights by minimizing a loss function that captures the maximum deviation from the desired outputs, rather than averaging errors.",
            "method steps": [
                "Define the neural network structure with input and output layers.",
                "Implement the max-norm loss function to evaluate performance.",
                "Utilize the bisection method for quasiconvex optimization of the loss function.",
                "Train the model using the defined loss function and assess its performance on test data."
            ],
            "principle": "The effectiveness of the Max-Norm Loss Function stems from its ability to focus on the most significant errors in classification, thus providing a more robust training approach for datasets with limited or skewed data distributions."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the HandOutlines dataset, which includes 1000 training recordings and 370 test recordings, structured to assess the performance of the proposed method against standard implementations.",
            "evaluation method": "The performance was evaluated by comparing classification accuracy and confusion matrices of the proposed max-norm approach against the traditional MATLAB deep learning toolbox using mean least squares."
        },
        "conclusion": "The results suggest that the uniform approximation-based approach can be more effective than traditional mean least squares methods, particularly when the training dataset is small yet reliable. This has significant implications for applications where data is expensive to obtain and often limited in size.",
        "discussion": {
            "advantage": "The primary advantage of the proposed method is its improved handling of underrepresented classes and outliers, allowing for better classification performance in scenarios where traditional approaches struggle.",
            "limitation": "A notable limitation is that while the uniform approximation performs well in certain situations, it may not consistently outperform mean least squares in all contexts, particularly with larger, well-balanced datasets.",
            "future work": "Future research directions include extending the method to more complex neural network architectures, exploring its application in various classification problems with limited data, and identifying optimal activation functions for specific datasets."
        },
        "other info": [
            {
                "acknowledgement": "This research was supported by the Australian Research Council (ARC), Solving hard Chebyshev approximation problems through nonsmooth analysis (Discovery Project DP180100602)."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper addresses the inefficiency of standard mean least squares loss functions in deep learning when dealing with small or unbalanced datasets, which can lead to suboptimal classification results."
        },
        {
            "section number": "2.2",
            "key information": "The proposed method utilizes a nonsmooth loss function based on the max-norm, which is believed to be more effective for training neural networks under specific conditions of data reliability and size."
        },
        {
            "section number": "3.4",
            "key information": "The Max-Norm Loss Function emphasizes the worst-case error (max-norm), potentially improving performance in scenarios where data is limited or unbalanced."
        },
        {
            "section number": "4.1",
            "key information": "The primary advantage of the proposed method is its improved handling of underrepresented classes and outliers, allowing for better classification performance in scenarios where traditional approaches struggle."
        },
        {
            "section number": "6.2",
            "key information": "Future research directions include extending the method to more complex neural network architectures and exploring its application in various classification problems with limited data."
        }
    ],
    "similarity_score": 0.5487680432924106,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0115_inter/papers/Deep Learning with Nonsmooth Objectives.json"
}