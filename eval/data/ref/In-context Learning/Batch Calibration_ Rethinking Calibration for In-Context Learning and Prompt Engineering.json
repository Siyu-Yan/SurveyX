{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.17249",
    "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
    "abstract": "Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.",
    "bib_name": "zhou2024batchcalibrationrethinkingcalibration",
    "md_text": "# BATCH CALIBRATION: RETHINKING CALIBRATION FOR IN-CONTEXT LEARNING AND PROMPT ENGINEERING\nHan Zhou1,2,\u2217 Xingchen Wan1 Lev Proleev1 Diana Mincu1 Jilin Chen1 Katherine Heller1 Subhrajit Roy1 1Google Research 2University of Cambridge {hzhouml,xingchenw,levp,dmincu,jilinc,kheller,subhrajitroy}@goo\n# ABSTRACT\nPrompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.\narXiv:2309.17249v2\n# INTRODUCTION\nPrompting large language models (LLMs) (Chowdhery et al., 2022; Anil et al., 2023) has become an efficient learning paradigm for adapting LLMs to a new task by conditioning on humandesigned instructions. The remarkable in-context learning (ICL) ability of LLMs also leads to efficient few-shot learners that can generalize from few-shot input-label pairs (Brown et al., 2020; Liu et al., 2023). However, the predictions of LLMs are highly sensitive and even biased to the choice of templates (Min et al., 2022b), verbalizers (Holtzman et al., 2021), and demonstrations (Liu et al., 2022a), resulting in barriers for pursuing efficiently adaptable and robust LLM applications.\nExtensive research has been devoted to mitigating these biases, which we explicitly refer to the a-priori propensity of LLMs to predict certain classes over others unfairly. Lu et al. (2022) provide an analysis of the impacts of the order of ICL examples to LLMs and have explored the order selection mechanisms for ICL. On the other hand, Zhao et al. (2021) reveal the bias of language models toward certain answers and propose to calibrate the LLM given content-free tokens. More recently, Fei et al. (2023) detect the domain-label bias, and Han et al. (2023) treat the calibration of LLMs, a technique for mitigating the label bias, as learning a robust decision boundary. Though multiple calibration solutions have been provided, the field currently lacks a unified analysis that systematically distinguishes and explains the unique characteristics, merits, and downsides of each approach.\n\u2217Work done as a Student Researcher at Google.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/63bd/63bd51fb-ef2d-48eb-92cb-1cdc82c42e17.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Batch Calibration (BC) achieves the best performance on 1-shot ICL over calibration baselines on an average of 13 classification tasks on PaLM 2-S and PaLM 2-L (Anil et al., 2023).</div>\nIn this work, we first conduct a comprehensive analysis across existing calibration methods for LLMs. We approach the calibration problem from a distinctive point of view by interpreting the decision boundaries for each calibration method together with the ICL decision boundary. We start observing fatal failure cases for each method by extending them to more challenging and under-explored evaluation tasks. We then conclude the current limitation for each method with a novel interpretation from the decision boundary perspective, pointing to the need for a unified and widely applicable solution for conquering diverse bias sources in the field of LLM efficient learning. Inspired by these findings, we propose Batch Calibration (BC), a zero-shot and inference-only calibration method for prompting and ICL. The central objective of BC is to accurately model the bias from the prompt context (referred to as contextual bias in this paper) by marginalizing the LLM scores in the batched input. The simplicity of the design of BC only brings negligible computation overhead at the output of the LLM. We further extend BC to the black-box few-shot learning (BCL), a practical case where labeled data is available, by introducing a single learnable parameter into BC, which enables it to adapt and learn the contextual bias from the available data resources. We conducted extensive experiments on more than 10 natural language understanding tasks together with image classification tasks. BC stands as the most widely applicable calibration method while achieving state-of-the-art results. With the proposed black-box few-shot BCL framework, we show that further slight gains can be achieved by leveraging more labeled data. We provide further analysis with BC on robustness with templates, ICL choices and orders, and verbalizers, validating that BC can effectively alleviate prompt brittleness and make prompt engineering easier. To summarize, we provide the following contributions: \u2022 We provide a unified and systematic analysis of existing calibration methods through their decision boundaries, investigate the common use of content-free tokens as an estimator of contextual bias, and identify their deficiency with individual case studies. \u2022 We propose Batch Calibration (BC), a zero-shot and inference-only calibration method for ICL, that mitigates the bias from the batch. We further extend BC to learn from few-shot data. \u2022 We show that while conceptually simple, BC attains state-of-the-art performance in both zero-shot and few-shot learning setups over widely selected tasks with PaLM-2 and CLIP models.\n# 2 RELATED WORK\nUnderstanding and Improving ICL. Lu et al. (2022) show the sensitivity of LLMs to ICL examples. This phenomenon is further explained through the effect from pretraining term frequencies (Razeghi et al., 2022) and corpora (Shin et al., 2022). Meanwhile, Xie et al. (2022) explain the ICL process through implicit Bayesian inference, and Wei et al. (2023) show the emergent ability of LLMs by learning new input-label mappings. Various methods have been proposed to optimally select better in-context templates (Sorensen et al., 2022; Pan et al., 2023; Yin et al., 2023) and examples (Rubin et al., 2022; Liu et al., 2022a; Wan et al., 2023b). Specifically, Wan et al. (2023a) introduce a selection criteria based on the consistency, diversity, and repetition of in-context examples. Recently, noisy channel prompting (Min et al., 2022a) and flipped learning (Ye et al., 2023) have been proposed for robust ICL. Learning to assign labels by k-nearest neighbors (Xu et al., 2023) and training decoder networks (Cui et al., 2023) are also effective alternatives for few-shot ICL.\nUnderstanding and Improving ICL. Lu et al. (2022) show the sensitivity of LLMs to ICL examples. This phenomenon is further explained through the effect from pretraining term frequencies (Razeghi et al., 2022) and corpora (Shin et al., 2022). Meanwhile, Xie et al. (2022) explain the ICL process through implicit Bayesian inference, and Wei et al. (2023) show the emergent ability of LLMs by learning new input-label mappings. Various methods have been proposed to optimally select better in-context templates (Sorensen et al., 2022; Pan et al., 2023; Yin et al., 2023) and examples (Rubin et al., 2022; Liu et al., 2022a; Wan et al., 2023b). Specifically, Wan et al. (2023a) introduce a selection criteria based on the consistency, diversity, and repetition of in-context examples. Recently, noisy channel prompting (Min et al., 2022a) and flipped learning (Ye et al., 2023) have been proposed for robust ICL. Learning to assign labels by k-nearest neighbors (Xu et al., 2023) and training decoder networks (Cui et al., 2023) are also effective alternatives for few-shot ICL. Bias in ICL and Calibrating LLMs. Zhao et al. (2021) reveal the instability of LLMs in fewshot learning and demonstrate three bias sources: majority label bias, recency bias, and common token bias, as the bias factors behind the instability. They propose contextual calibration (CC) to mitigate these biases by grounding the prediction based on a content-free token as sample inputs. Si et al. (2023) characterize the feature bias of LLMs, and Wang et al. (2023) introduce the positional bias in candidate choices. Fei et al. (2023) further observe the existence of domain-label bias and propose domain-context calibration (DC) that uses random in-domain tokens for estimating the bias. Meanwhile, Han et al. (2023) analyze the impact of decision boundary for text classification tasks and propose to estimate prototypical clusters by Gaussian mixture models, thereby learning a robust decision boundary. Concurrently with our work, Pezeshkpour & Hruschka (2023) spot the positional bias in multiple-choice questions, and Zheng et al. (2023) propose to debias the positional bias in multiple choices with permutation-based prior estimation. We will discuss quantitatively and provide\nBias in ICL and Calibrating LLMs. Zhao et al. (2021) reveal the instability of LLMs in fewshot learning and demonstrate three bias sources: majority label bias, recency bias, and common token bias, as the bias factors behind the instability. They propose contextual calibration (CC) to mitigate these biases by grounding the prediction based on a content-free token as sample inputs. Si et al. (2023) characterize the feature bias of LLMs, and Wang et al. (2023) introduce the positional bias in candidate choices. Fei et al. (2023) further observe the existence of domain-label bias and propose domain-context calibration (DC) that uses random in-domain tokens for estimating the bias. Meanwhile, Han et al. (2023) analyze the impact of decision boundary for text classification tasks and propose to estimate prototypical clusters by Gaussian mixture models, thereby learning a robust decision boundary. Concurrently with our work, Pezeshkpour & Hruschka (2023) spot the positional bias in multiple-choice questions, and Zheng et al. (2023) propose to debias the positional bias in multiple choices with permutation-based prior estimation. We will discuss quantitatively and provide\na unified analysis of these methods in Sec. 3. As we will show, our proposed method differentiates from these methods as a generalizable solution across challenging classification tasks and modalities\n# 3 A SYSTEMATIC ANALYSIS OF CALIBRATION\n3.1 BIAS IN PROMPTING AND IN-CONTEXT LEARNING (ICL)\nPrompting is an efficient learning paradigm that allows LLMs to perform zero-shot inference by conditioning on a human-designed instruction. Formally, denoting a test query-target pair {xi, yi} and instruction as the context C for a classification task, LLMs make prediction by computing: arg maxy\u2208Y p(y|xi, C), where p \u2208RJ is the logits, and Y denotes the verbalizers that define the label set for J classes. ICL further enables LLM to learn from k input-label pairs (i.e., few-shot setup), s(i) = Template(x(i), y(i)) \u2200i \u2208{1, ..., k}, by concatenating few-shot demonstrations in a pre-defined template as the context, C = Concat(s(i), ..., s(k)). Though ICL has demonstrated strong performance with easy implementations, the prediction of LLMs is shown to be biased towards certain answers due to different elements of p(y|xi, C) (Lu et al., 2022). In the ICL context C, majority label bias and recency label bias (Zhao et al., 2021) can bias the prediction of LLMs toward the most frequent label and the label towards the end of the demonstration, respectively. Among verbalizer tokens yj \u2208Y, LLMs are shown to be inherently biased towards predicting the label-tokens that appear more frequently from pretraining term statistics (Shin et al., 2022; Razeghi et al., 2022). These bias factors significantly degrade the performance of LLMs for robust ICL applications.\n# 3.2 OVERVIEW OF ICL CALIBRATION METHODS.\nAs introduced in Sec. 2, various calibration methods have been proposed to mitigate the issue of bias identified above. In this section, we provide an overview of the state-of-the-art calibration methods. Contextual Calibration (Zhao et al., 2021) (CC). Motivated by a common calibration technique that applies affine transformation on the model outputs (Platt et al., 1999; Guo et al., 2017), Zhao et al. (2021) propose to calibrate the LLM prediction by first measuring the entire test-time distribution \u02c6p by a content-free input. Using \u201cN/A\u201d as a content-free example, the model score distribution is generated by \u02c6pcf := p(y|[N/A], C). CC then generates the calibrated output by transforming the uncalibrated scores p(y|x, C) with W \u2208RJ\u00d7J via Wp(y|x, C), where W = diag(\u02c6pcf)\u22121 offsets the uncalibrated scores with the model score (a contextual prior) triggered by the content-free sample. Domain-Context Calibration (Fei et al., 2023) (DC). Instead of using a single content-free token, Fei et al. (2023) propose DC that estimates a contextual prior \u02c6p(y|C) by using a random in-domain sequence. It randomly sampled L tokens at an average sentence length from an unlabeled text set. Then, it estimates the content-free prediction prior by averaging the model score T times, such that: \u02c6prandom = 1 T \ufffdT t=1 p(y|[RANDOM TEXT]t, C). The final test-time prediction is then calibrated by dividing the estimated prior prediction, or equivalently in logits space, p(y|xi, C) \u2212\u02c6prandom. Prototypical Calibration (Han et al., 2023) (PC). PC learns a decision boundary with Gaussian mixture models (GMMs). It estimates J prototypical clusters for the model output p for J classes: PGMM(p) = \ufffdJ\u22121 j=0 \u03b1jPG(p|\u00b5j, \u03a3j), where PG denotes a multi-variate Gaussian distribution, and the parameters: mixing coefficient \u03b1, mean vector \u00b5, covariance matrix \u03a3 are estimated by the Expectation-Maximization (Moon, 1996). Followed by an automatic label assignment strategy, the predicted label is then computed by arg maxj PG(pj|\u00b5\u2217, \u03a3\u2217) in the inference time. This EMGMM process can require up to T repetitions to stabilize its estimation of clusters where T is a hyperparameter of the algorithm.\nSummarizing the calibration methods with distinctive design principles discussed so far, in Table 1, we present a unified view of the characteristics of each method with their mathematical formulation, computation cost, and strengths & weaknesses. Though each approach demonstrates a clear motivation for calibrating ICL, it is still unclear which method surpasses others in what scenarios.\nTable 1: Calibration methods with their mathematical formulation and their equivalent decision boundary derivations in a two-dimensional problem. The cost for the number of API calls is denoted as #Forward, where 1 counts for the original ICL forward cost, and PC and BC incur no additional API costs. We refer derivations of decision boundaries to Sec. 3.3. The potential failure case for each calibration method in practical scenarios is marked as \u2717.\nMethod\nToken\n#Forward\nComp.\nCost\nCali.\nForm\nLearning\nTerm\nDecision\nBoundary h(p)\nMulti-\nSentence\nMulti-\nClass\nCC\nN/A\n1 + 1\nInverse\nWp + b\nW = diag(\u02c6p)\u22121, b = 0\np0 = \u03b1p1\n\u2717\n\u2713\n\u2713\n\u2713\nDC\nRandom\n20 + 1\nAdd\nWp + b\nW = I, b = \u22121\nT\n\ufffd\nt p(y|textj, C)\np0 = p1 + \u03b1\n\u2717\n\u2713\n\u2713\n\u2713\nPC\n-\n1\nEM-GMM\n-\n\ufffd\nj \u03b1jPG(p|\u00b5j, \u03a3j)\nPG(p|\u00b50, \u03a30) = PG(p|\u00b51, \u03a31)\n\u2713\n\u2713\n\u2713\n\u2717\nBC (Ours)\n-\n1\nAdd\nWp + b\nW = I, b = \u2212Ex\n\ufffd\np(y|x, C)\n\ufffd\np0 = p1 + \u03b1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5b34/5b34bdd3-b58c-4c7c-b07a-5f2c44fd3de8.png\" style=\"width: 50%;\"></div>\nFigure 2: Visualization of the decision boundaries of uncalibrated ICL, and after applying existing calibration methods and the proposed BC (to be introduced in Sec 4) in representative binary classification tasks of SST-2 (top row) (Socher et al., 2013) and QNLI (bottom row) (Wang et al., 2018) on 1-shot PaLM 2-S. We show success and failure cases for each baseline method (CC, DC, and PC), whereas BC is consistently effective. Refer to Appendix \u00a7E for more examples. We proceed with an in-depth analysis of existing methods in representative tasks. We provide a novel view of calibration methods from a multi-variate decision boundary perspective. In pursuit of practical guidelines for ICL calibration, we set out two important research questions behind their design principles: 1) What constitutes a better decision boundary for calibrations? 2) Is content-free prior a good estimator of contextual bias?\nWhat Constitutes a Better Decision Boundary for Calibrations? question, we first derive the decision boundary for each category of calibration methods. We recall that the classification by a LLM is based on arg maxj\u2208{0,...,J\u22121} pj where pj denotes the j-th element of output vector p. Consider binary classification problem for simplicity: the decision boundary h(p) for ICL is given by the line p0 \u2212p1 = 0: the model predicts class 0, y0, if p0 \u2212p1 \u22650, and class 1 otherwise. Consequently, CC and DC that apply an affine transformation at p is equivalent to a linear transformation to the decision boundary. In CC with W = diag(\u02c6p)\u22121, b = 0, the decision boundary can then be derived as p0 \u00d7 1 \u02c6p0 = p1 \u00d7 1 \u02c6p1 \u2192p0 \u2212p1 \u00d7 \u02c6p0 \u02c6p1 = 0, which is a rotation of the ICL\u2019s linear decision boundary around the origin. Similarly, DC with W = I, b = \u22121 T \ufffd t p(y|[RANDOM TEXT]t, C) = \u2212\u02c6p is equivalent to a shift of ICL\u2019s linear decision boundary away from the origin, such that p0 \u2212p1 = (\u02c6p0 \u2212\u02c6p1). It is worth noting that both calibration choices lead to a linear decision boundary, indicating that the calibration problem can be framed as an unsupervised decision boundary learning problem. Based on this intuition, we further derive the decision boundary for PC as PG(p|\u00b50, \u03a30) \u2212PG(p|\u00b51, \u03a31) = 0, which delivers a non-linear boundary between the estimated Gaussian mixtures. We conduct a preliminary experiment to visualize the derived decision bounds from existing calibration methods alongside the ICL baseline. In Fig. 2,\nwe observe that uncalibrated ICL is biased towards predicting negative in the SST-2 task. This biased prediction is then mitigated by each calibration method, where we observe a rotated decision boundary from CC, a shifted boundary from DC, and a non-linear boundary between the GMMs by PC. However, in the QNLI task (bottom row of Fig. 2), we observe failure cases in the calibration baselines, in particular, PC (third figure from the left), where it fails to capture the correct distribution for each class. From Fig. 2 and the additional results in Fig. 10 in Appendix \u00a7E, we find that while theoretically more flexible, the non-linear decision boundaries learned by PC tend to be susceptible to overfitting and may suffer from instability in EM-GMM. We hypothesize that the PC boundary is even more vulnerable to instability for more challenging multi-class tasks due to the increased difficulties of learning clusters and assigning classes correctly. Conversely, we find that linear decision boundaries, as evidenced by CC and DC, can be more robust and generalizable across tasks. We validate this hypothesis by proposing BC with extensive experiments in Sec. 5.2.\nIs Content-free Input a Good Estimator of the Contextual Prior? CC and DC both use a linear decision boundary but differ from each other by leveraging different formats of a content-free input to estimate the contextual prior. However, as we observed in Fig. 2, they both exhibit failure cases in QNLI, a question-answering NLI task. We hypothesize that contrary to the proposals made by CC and DC, relying on content-free tokens for calibration is not always optimal and may even introduce additional bias, depending on the task type. For example, in a textual entailment task involving question-sentence pairs, we empirically observe\nthat an ICL template employed with a content-free token \u2018N/A\u2019 such as \u2018Question: N/A, Sentence: N/A, Answer:\u2019 will result in a biased prediction towards \u2018entailment\u2019, because although \u2018N/A\u2019 is intended to be content-free, the LLM may nevertheless construe \u2018N/A\u2019 in the sentence to be substantively entailed to the \u2018N/A\u2019 in the question due to surface text equivalency. This phenomenon holds true for other multi-text classification tasks, such as paraphrasing and word disambiguation tasks. Consequently, the prior estimated via a single content-free token can lead to further bias. DC introduces multiple randomly sampled tokens to form a content-free input, e.g. \u2018Question: that What old rubisco\u2019s the did Which?\u2019. We suspect a possible reason is that random sequences, when used in conjunction with in-context demonstrations, can be susceptible to spurious relations among them that often lead to unfair priors further skewing the predictions, which is also reflected in Fig. 3, where CC and DC fail to mitigate the contextual bias in the QNLI task. In sum, the empirical observation shows that content-free inputs can be inappropriate prior estimators, especially for multi-sentence classification tasks.\n# 4 BATCH CALIBRATION\nInspired by the previous discussions, we now propose Batch Calibration (BC), a zero-shot, inferenceonly, and generalizable calibration technique with negligible computation cost. We further discuss how BC can be extended to vision-language models as well as the black-box few-shot learning setup. Batch Calibration (BC). Following the discussion in Sec. 3.3, we argue that the most critical component for calibration is to accurately estimate the contextual bias term p(y|C). Both CC and DC, which use content-free and in-domain random tokens as trigger signals, respectively, have failure cases in multi-sentence classification when the estimation of the contextual bias is inaccurate. On the other hand, PC is vulnerable to overfitting and may incorrectly model the mixtures, especially in high-dimensional space. We, therefore, opt for a linear decision boundary for its robustness, and instead of relying on content-free tokens, we propose to estimate the contextual bias for each class p(y = yj|C) from a batch with M samples, {x1, ..., xM}, in a content-based manner by marginalizing the output score over all samples x \u223cP(x) within the batch:\nInspired by the previous discussions, we now propose Batch Calibration (BC), a zero-shot, inferenceonly, and generalizable calibration technique with negligible computation cost. We further discuss how BC can be extended to vision-language models as well as the black-box few-shot learning setup.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7b4e/7b4eef16-3115-434b-b26b-64d308805427.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The distribution of ICL scores after applying CC and DC on QNLI. Due to an unfair content-free prior, the prediction by 1-shot PaLM-2 is biased towards entailment.</div>\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/844b/844bd614-8c24-4719-a086-11d57d3f97d2.png\" style=\"width: 50%;\"></div>\nFigure 4: Illustration of Batch Calibration (BC). Batches of demonstrations with in-context examples and test samples are passed into the LLM. Due to implicit bias sources in the context, the score distribution from the LLM becomes highly biased. BC is a modular and adaptable layer option appended to the output of the LLM/VLM. BC generates calibrated scores according to Eq. 1 & 2. Highlighted symbols indicate the distribution means (visualized for illustration only).\n<div style=\"text-align: center;\">Figure 4: Illustration of Batch Calibration (BC). Batches of demonstrations with in-context examples and test samples are passed into the LLM. Due to implicit bias sources in the context, the score distribution from the LLM becomes highly biased. BC is a modular and adaptable layer option appended to the output of the LLM/VLM. BC generates calibrated scores according to Eq. 1 & 2. Highlighted symbols indicate the distribution means (visualized for illustration only).</div>\nWe then obtain the calibrated probability by dividing the output probability over the contextual prior, which is equivalently by shifting the log-probability by the estimated mean of each class:\n\ufffd \ufffd It is noteworthy that this BC procedure is zero-shot and only involves unlabeled test samples. BC incurs negligible computation costs. We may either compute the correction term \u02c6p(y|C) once all test samples are seen or, alternatively, in an on-the-fly manner that dynamically processes the outputs. To do so, we may use a running estimate of the contextual bias for BC. At the n + 1 mini-batch, the bias term is given by:\nAdjustable Batch Calibration Layer (BCL). While BC is designed to be zero-shot and inference-only, it is also common that some labeled data are available. In this section, we describe a simple, adapted variant of BC that may further refine the calibration and mitigate any estimation errors from the unlabeled data, which we term BCL. Specifically, instead of deducting the bias term \u02c6p from the test data only, we introduce a single additional hyperparameter strength \u03b3 \u2208R:\n|| \u2212| where \u03b3 controls the strength of BC. To select the appropriate \u03b3, we simply perform a grid search by uniformly sampling T different \u03b3 values in [a, b] (we set [a, b] := [\u22125, 5], but any reasonable range may be used). The strength \u03b3 is then learned by \u03b3\u2217= arg max\u03b3\u2208[a,b] R(pBC, \u03b3), where R(\u00b7, \u00b7) is the evaluation function (e.g., accuracy) on the set of labeled data, allowing the amount of calibration to be adjusted from evaluation metrics directly.\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/42dc/42dc6a42-7536-48e1-953f-e63fdd3316b2.png\" style=\"width: 50%;\"></div>\nFigure 5: BC benefits from labeled data: The performance of the adjustable BCL compared to the zero-shot BC with a changing strength. The strength \u03b3 at 0 and 1 represent the uncalibrated ICL and BC, respectively. We highlight the optimal strength learned from a labeled set and the best test strength. Refer to Appendix \u00a7E for more examples.\nTable 2: Accuracy (%) on natural language classification tasks with 1-shot PaLM 2-S and PaLM 2-L Models (Anil et al., 2023). We report the mean and standard deviation for all results for 5 different in-context examples. We reproduce all baselines, and the implementation details are described in Appendix \u00a7D. The best and second-best results are marked in bold fonts and ranked by color.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ca5/5ca5ce02-527a-4684-8782-2c97778bdff8.png\" style=\"width: 50%;\"></div>\nModel\nPaLM 2-S\nPaLM 2-L\nMethod\nICL\nCC\nDC\nPC\nBC\nICL\nCC\nDC\nPC\nBC\nSST-2\n93.620.62\n95.500.25\n94.290.32\n95.710.10\n95.440.15\n93.165.18\n95.820.62\n94.912.01\n95.640.47\n95.780.55\nMNLI\n68.527.98\n60.0711.26\n63.451.99\n59.2913.79\n75.122.76\n72.773.65\n79.453.46\n71.534.86\n78.687.10\n81.342.29\nQNLI\n81.201.90\n56.863.29\n65.623.53\n69.8217.73\n82.451.82\n64.683.53\n69.714.89\n68.973.27\n61.0115.26\n87.901.24\nMRPC\n66.4210.15\n70.440.94\n68.580.21\n71.861.29\n70.052.40\n73.191.21\n72.403.53\n68.680.40\n75.392.60\n70.392.56\nQQP\n63.910.66\n65.555.34\n53.929.35\n65.283.42\n71.481.46\n82.570.75\n81.172.03\n78.321.82\n81.420.24\n79.561.40\nBoolQ\n83.993.90\n87.141.60\n87.641.10\n88.700.15\n87.830.10\n90.020.60\n90.150.54\n87.771.17\n64.4022.37\n90.100.22\nCB\n45.7110.61\n29.647.85\n65.713.20\n81.079.42\n78.213.19\n92.862.19\n85.727.78\n92.862.82\n89.297.25\n93.211.49\nCOPA\n96.402.30\n95.802.05\n96.402.88\n96.202.05\n96.402.07\n98.601.14\n97.201.10\n97.400.89\n99.000.71\n97.001.00\nRTE\n80.941.29\n79.780.92\n76.821.72\n80.431.07\n83.471.10\n75.092.11\n80.002.48\n79.211.95\n86.642.62\n85.422.48\nWiC\n50.690.59\n50.560.50\n49.970.13\n51.383.56\n61.102.07\n51.351.90\n55.586.38\n54.676.02\n57.8711.08\n64.838.59\nANLI-R1\n46.244.21\n42.543.20\n40.263.66\n40.286.46\n59.820.51\n63.062.63\n71.923.71\n73.563.88\n72.308.05\n75.003.03\nANLI-R2\n40.440.90\n38.360.82\n38.443.46\n41.884.50\n50.160.82\n58.401.19\n65.363.75\n65.481.91\n64.982.94\n67.302.34\nANLI-R3\n42.530.99\n38.781.04\n43.675.25\n37.500.81\n55.751.66\n61.353.14\n67.320.98\n66.230.72\n63.036.03\n66.380.74\nAvg.\n66.20\n62.39\n64.98\n67.65\n74.41\n75.16\n77.83\n76.89\n76.13\n81.09\n0\n1\n2\n4\n60\n70\n80\nAccuracy (%)\nRTE\n0\n1\n2\n4\n40\n60\n80\nMNLI\n0\n1\n2\n4\n50\n60\n70\nQQP\n0\n1\n2\n4\n50\n55\n60\nWiC\n<div style=\"text-align: center;\">Figure 6: The ICL performance on various calibration techniques over the number of ICL shots on PaLM 2-S. Each shot indicates 1 example per class in the demonstration. Lines and shades denote the mean and standard deviation over 5 random seeds, respectively.</div>\nWe give concrete examples in Fig. 5, which illustrates the effect of BCL where we plot the accuracy in QQP (Wang et al., 2018) and CB (De Marneffe et al., 2019) tasks over a range of \u03b3. We observe that \u03b3 = 1, which corresponds to BC without adjustment (purple line), leads to a strong but not optimal performance. By using the \u03b3 learned from the labeled data (a 128-shot randomly sampled set in this case), BCL estimates the contextual bias more precisely by leveraging the labeled data and achieves a performance that is very close to the optimal. We refer readers to Table 3 for more results. Calibrating Vision-Language Models. Recently, vision-language models (VLM) (Radford et al., 2021), which simultaneously encode visual and textual information, have demonstrated strong zeroshot generalization capability by rewriting class labels. However, the sources of bias as LLMs have also been observed in prompting VLMs (Alayrac et al., 2022) but have not been adequately addressed. In this work, we propose to apply BC to Zero-Shot (ZS) CLIP (Radford et al., 2021) and mitigate the biases in its zero-shot classifications. We follow the same notation from Sec. 3.1, where the test image is now x, and the prompt template becomes the context, C. Similarly, we append the BC layer at the output of the ZS CLIP and calibrate for each class following Eq. 1 & 2.\n# 5 EXPERIMENTS\n# 5.1 EXPERIMENTAL SETUP\nEvaluation Data. For natural language tasks, in contrast to previous works that only report on relatively simple single-sentence classification tasks (Zhao et al., 2021; Fei et al., 2023; Han et al., 2023), we conduct experiments on 13 more diverse and challenging classification tasks, including the\nMethod\nSST-2\nMNLI\nQNLI\nMRPC\nQQP\nBoolQ\nCB\nCOPA\nRTE\nWiC\nANLIR1\nANLIR2\nANLIR3\nAvg.\nBC\n95.4\n75.0\n83.5\n68.6\n70.3\n87.9\n75.0\n98.0\n84.1\n63.3\n59.8\n51.1\n53.3\n74.3\nBCL\n96.3\n75.0\n83.5\n74.3\n72.3\n88.8\n83.9\n99.0\n82.7\n63.2\n58.0\n49.7\n52.2\n75.3\nstandard GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) datasets. Specifically, we consider commonsense reasoning: BoolQ (Clark et al., 2019), COPA (Roemmele et al., 2011); word disambiguation: WiC (Pilehvar & Camacho-Collados, 2019); sentiment classification: SST-2 (Socher et al., 2013); paraphrasing: QQP, MRPC (Dolan & Brockett, 2005); natural language inference and entailment: ANLI-R{1,2,3} (Nie et al., 2020), CB (De Marneffe et al., 2019), RTE, QNLI (QA/NLI), MNLI (Williams et al., 2018). For image classification tasks, we include SVHN (Yuval, 2011), EuroSAT (Helber et al., 2019), and CLEVR (Johnson et al., 2017). Models. We conduct experiments mainly on the state-of-the-art PaLM 2 (Anil et al., 2023) for its variants with different sizes, PaLM 2-S, PaLM 2-M, and PaLM 2-L. PaLM 2 has achieved results that are competitive with GPT-4 (OpenAI, 2023), and readers are referred to Anil et al. (2023) for more model details. For VLMs, we report the results on CLIP ViT-B/16 (Radford et al., 2021).\nstandard GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) datasets. Specifically, we consider commonsense reasoning: BoolQ (Clark et al., 2019), COPA (Roemmele et al., 2011); word disambiguation: WiC (Pilehvar & Camacho-Collados, 2019); sentiment classification: SST-2 (Socher et al., 2013); paraphrasing: QQP, MRPC (Dolan & Brockett, 2005); natural language inference and entailment: ANLI-R{1,2,3} (Nie et al., 2020), CB (De Marneffe et al., 2019), RTE, QNLI (QA/NLI), MNLI (Williams et al., 2018). For image classification tasks, we include SVHN (Yuval, 2011), EuroSAT (Helber et al., 2019), and CLEVR (Johnson et al., 2017).\nModels. We conduct experiments mainly on the state-of-the-art PaLM 2 (Anil et al., 2023) for its variants with different sizes, PaLM 2-S, PaLM 2-M, and PaLM 2-L. PaLM 2 has achieved results that are competitive with GPT-4 (OpenAI, 2023), and readers are referred to Anil et al. (2023) for more model details. For VLMs, we report the results on CLIP ViT-B/16 (Radford et al., 2021).\n5.2 MAIN EXPERIMENTS\nExperiments on Natural Language Tasks. We present the results across a diverse set of NLP tasks in Table 2. Notably, BC consistently outperforms ICL, yielding significant performance enhancement of 8% and 6% on PaLM 2-S and PaLM 2-L, respectively. This shows that the BC implementation successfully mitigates the contextual bias from the in-context examples and unleashes the full potential of LLM in efficient learning and quick adaptation to new tasks. In addition, BC improves over the state-of-the-art PC baseline by 6% on PaLM 2-S, and surpasses the competitive CC baseline by another 3% on average on PaLM 2-L. Specifically, BC is a generalizable and cheaper technique across all evaluated tasks, delivering stable performance improvement, whereas previous baselines exhibit varying degrees of instability across tasks: DC baseline is the least competitive; CC displays more failure cases in multi-sentence classification tasks, particularly for paraphrasing and NLI tasks, as we hypothesized in Sec 3.3; PC, while occasionally competitive, exhibits large performance fluctuations, as evidenced by its large standard deviation, resulting in frequent substantial performance degradation.\nWe further analyze the performance of BC by varying the ICL shots from 0 to 4 shots as shown in Fig. 6, and BC again outperforms all baseline methods. We also observe an overall trend for improved performance when more shots are available, and the performance disparities between BC and ICL converge on some tasks, which suggests that BC allows LLMs to more effectively take advantage of more in-context demonstrations. We also observe that PC exhibits the worst sta-\nbility across the number of shots. In Table 3, we show that further slight gains, 1% on average, can be achieved by involving an adjustable strength parameter that refines the calibration and mitigates estimation errors. This alternative design not only makes BC applicable to both zero-shot and fewshot setups but also shows its capability to improve further from limited labeled data.\nCalibrating Vision-Language Models We further extend the applicable scenarios of BC to multi-modal learning to handle the bias inherent in the prompt template designs in CLIP. We select three tasks in which the previous visual-prompting method shows significant improvement (Oh et al., 2023). As shown in Fig. 7, BC significantly im-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3af0/3af0ac18-c7b2-4ede-a367-d1b9fb9a28c5.png\" style=\"width: 50%;\"></div>\nFigure 7: BC improves zero-shot (ZS) image classification: Accuracy (%) on image classification tasks with the zero-shot CLIP ViT-16/B. The BC implementation is zero-shot, and we apply BC together with the CLIP to demonstrate the effectiveness of BC in vision-language models. Refer to additional tasks in Appendix \u00a7E.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9419/94194ee3-2a5c-4de1-8863-ee8ad992a089.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: BC makes prompt engineering easier: Performance of BC with respect to ICL choices, ICL orders, prompt templates, and verbalizers.</div>\nproves the zero-shot baseline by 12% on average. This observation further highlights the presence o contextual bias even within vision-language models, and BC can successfully restore the performanc of VLM in image classification tasks, suggesting that BC may serve as a versatile and common technique for mitigating contextual biases across multiple modalities.\nRobustness. We analyze the robustness of BC with respect to common prompt engineering design choices that were previously shown to significantly affect LLM performance (Lu et al., 2022; Liu et al., 2022a): choices and orders of in-context examples, the prompt template for ICL, and the verbalizers, as shown in Fig. 8 evaluated on RTE. Setup details are listed in Appendix \u00a7F. First, we find that BC is more robust to ICL choices and can mostly achieve the same performance with different ICL examples. Additionally, given a single set of ICL shots, altering the order between each ICL example has minimal impact on the BC performance. However, it is worth noting that an optimal order selection can still lead to promising ICL performance. Furthermore, we analyze the robustness of BC under 10 designs of prompt templates, where BC shows consistent improvement over the ICL baseline. Therefore, though BC makes further improvements, a well-designed template can further enhance the performance of BC. Lastly, we examine the robustness of BC to variations in verbalizer designs. Remarkably, even when employing unconventional choices such as emoji pairs as the verbalizers (listed in Tables 7 & 10) leading to dramatic oscillations of ICL performance, BC largely recovers performance. This observation shows BC robustifies LLM predictions under common prompt design choices and makes prompt engineering easier.\nBatch Size. We study the impact of batch size on the performance of BC as shown in Fig. 9. In contrast to PC, which also leverages an unlabeled estimate set, BC is remarkably more sample efficient, achieving a strong performance with only around 10 unlabeled samples, whereas PC requires more than 500 unlabeled samples before its performance stabilizes.\n# 6 CONCLUSION\nWe first revisit previous calibration methods while addressing two critical research questions from an interpretation of decision boundaries, revealing their failure cases and deficiencies. We then propose Batch Calibration, a zero-shot and inference-only calibration technique. We also introduce an adjustable extension, BCL, which offers more refined calibrations when labeled data is accessible. While methodologically simple and easy to implement with negligible computation cost, we show that BC scales from a language-only setup to the vision-language context, achieving state-of-the-art performance in both modalities. BC significantly improves the robustness of LLMs with respect to prompt designs, and we expect easy prompt engineering with BC while exploring the potential of BC to generative tasks in the future.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a98a/a98a4f34-7ab4-4f6c-a66d-1edbb796131f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ICL PC BC</div>\nFigure 9: BC is data-efficient and insensitive to the batch size: Performance of BC across different sizes of an initial unlabeled set without using a running estimate of the contextual bias. We compare BC with the state-of-the-art PC baseline that also leverages unlabeled estimate set, and experiments are conducted on PaLM 2-S.\nWe thank Emily Salkey for her sincere project management support. We also thank Mohammad Havaei, Chirag Nagpal, Stephen Pfohl, Alexander D\u2019Amour, and Ahmad Beirami for fruitful suggestions and feedbacks and the PaLM 2 team at Google for helping with infrastructure questions.\n# REFERENCES\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\u00b4en Simonyan. Flamingo: a visual language model for few-shot learning. In NeurIPS, 2022. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pp. 3606\u20133613, 2014. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. Ganqu Cui, Wentao Li, Ning Ding, Longtao Huang, Zhiyuan Liu, and Maosong Sun. Decoder tuning: Efficient language understanding as decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15072\u201315087, Toronto, Canada, July 2023. Association for Computational Linguistics. Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pp. 107\u2013124, 2019. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369\u20133391, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, LIN Yong, Xiao Zhou, and Tong Zhang. Black-box prompt learning for pre-trained language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. Mitigating label biases for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14014\u201314031, Toronto, Canada, July 2023. Association for Computational Linguistics. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1321\u20131330, 2017. Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. Prototypical calibration for few-shot learning of language models. In The Eleventh International Conference on Learning Representations, 2023. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn\u2019t always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7038\u20137051, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 2790\u20132799, 2019. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprint arXiv:2110.11478, 2021. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 1988\u20131997, 2017. Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045\u20133059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582\u20134597, Online, August 2021. Association for Computational Linguistics. Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. TTN: A domain-shift aware batch normalization in test-time adaptation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, Dublin, Ireland and Online, May 2022a. Association for Computational Linguistics. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61\u201368, Dublin, Ireland, May 2022b. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, Dublin, Ireland, May 2022. Association for Computational Linguistics. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5316\u20135330, Dublin, Ireland, May 2022a. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11048\u201311064, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. Muhammad Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsupervised domain adaptation by normalization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 14745\u201314755, 2022. Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6): 47\u201360, 1996. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D\u2019Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4885\u20134901, Online, July 2020. Association for Computational Linguistics. Changdae Oh, Hyeji Hwang, Hee Young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song. Blackvip: Black-box visual prompting for robust transfer learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 24224\u201324235, 2023. OpenAI. Gpt-4 technical report, 2023. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8298\u20138319, Toronto, Canada, July 2023. Association for Computational Linguistics.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8298\u20138319, Toronto, Canada, July 2023. Association for Computational Linguistics.\nPouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023. Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267\u20131273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999. Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based instruction search for prompting large language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 3845\u20133864, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp. 8748\u20138763, 2021. Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 840\u2013854, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011, 2011. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655\u20132671, Seattle, United States, July 2022. Association for Computational Linguistics. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, and Nako Sung. On the effect of pretraining corpora on in-context learning by a large-scale language model. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5168\u20135186, Seattle, United States, July 2022. Association for Computational Linguistics. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222\u20134235, Online, November 2020. Association for Computational Linguistics. Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. Measuring inductive biases of in-context learning with underspecified demonstrations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11289\u201311310, Toronto, Canada, July 2023. Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 819\u2013862, Dublin, Ireland, May 2022. Association for Computational Linguistics. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, and Tomas Pfister. Better zero-shot reasoning with self-adaptive prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3493\u20133514, Toronto, Canada, July 2023a. Association for Computational Linguistics. Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Eisenschlos, Sercan Arik, and Tomas Pfister. Universal self-adaptive prompting. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7437\u20137462, Singapore, December 2023b. Association for Computational Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 3261\u20133275, 2019. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112\u20131122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. $k$nn prompting: Beyond-context learning with calibration-free nearest neighbor inference. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, and Minjoon Seo. Guess the instruction! flipped learning makes language models stronger zero-shot learners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 3063\u20133079, Toronto, Canada, July 2023. Association for Computational Linguistics. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Netzer Yuval. Reading digits in natural images with unsupervised feature learning. In Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E. Gonzalez. TEMPERA: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp. 12697\u201312706, 2021. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models\u2019 selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023. Han Zhou, Xingchen Wan, Ivan Vuli\u00b4c, and Anna Korhonen. Autopeft: Automatic configuration search for parameter-efficient fine-tuning. arXiv preprint arXiv:2301.12132, 2023a. Han Zhou, Xingchen Wan, Ivan Vuli\u00b4c, and Anna Korhonen. Survival of the most influential prompts: Efficient black-box prompt search via clustering and pruning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 13064\u201313077, Singapore, December 2023b. Association for Computational Linguistics. Yuliang Zou, Zizhao Zhang, Chun-Liang Li, Han Zhang, Tomas Pfister, and Jia-Bin Huang. Learning instance-specific adaptation for cross-domain segmentation. In Computer Vision - ECCV 2022 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIII, pp. 459\u2013476, 2022.\n# A ADDITIONAL RELATED WORK\nPrompt Learning. Prompt learning is an efficient learning pipeline for LLM as an alternative to traditional full-model fine-tuning (Liu et al., 2023). Soft prompting (Li & Liang, 2021; Lester et al., 2021; Liu et al., 2022b) enables fast adaptation of LLM by appending learnable continuous prompts in the embedding space while freezing the rest of weights. The recent development of parameter-efficient fine-tuning methods (Houlsby et al., 2019; Hu et al., 2022; Zhou et al., 2023a), which learn additional modules, may also be interpreted as a form of soft prompting (He et al., 2022). However, these soft prompt learning methods inevitably require gradients and internal model access. On the other hand, hard prompting (Shin et al., 2020) is an appealing learning category for learning discrete prompts. Recent efforts have been devoted to black-box prompt search without accessing model gradients, and more interpretable prompts can be found by reinforcement learning (Deng et al., 2022; Zhang et al., 2023), gradient estimation (Diao et al., 2023), and other derivative-free search algorithms (Prasad et al., 2023; Zhou et al., 2023b).\nTest-Time Adaptation. Test-time adaptation aims to mitigate the domain covariate shift using the test-time statistics. Wang et al. propose TENT that minimizes the entropy by updating the affine parameters in the BN layer. Nado et al. (2020) and Schneider et al. (2020) introduce using test-time batch statistics for the standardization in BN and mixing it with source statistics to conquer covariate shift, respectively. Similarly, mixing the statistics with predefined hyperparameters (You et al., 2021; Khurana et al., 2021), interpolating source and target-domain statistics (Lim et al., 2023), or using a running average estimate (Mirza et al., 2022; Hu et al., 2021) have also been proposed to adapt the BN layer. Zou et al. (2022) introduce strength parameters in adapting the standardization statistics in semantic segmentation tasks. We differentiate from test-time BN approaches by mitigating the bias in the novel context of LLM, and there is no source statistic similar to a BN layer in computer vision backbones.\n# B LIMITATIONS AND FUTURE WORK\nBC is a test-time method that relies on the target statistics from batched input. To mitigate any potential estimation errors from unlabelled data, we introduce the adjustable BCL extension to incorporate source statistics from labelled data. Though BC has shown remarkable sample efficiency in terms of batch sizes, it still requires a batch of inputs to estimate the contextual bias. We introduce a running estimation for BC from mini-batches, which subsequently stabilizes the prediction of LLMs when more mini-batches arrive. In future work, we will endeavor to explore calibration for generative tasks while extending BC to more models across modalities. We suspect that the contextual bias may also exist in short-form generation tasks. Motivated by Zhao et al. (2021), one possible solution for generative tasks is to calibrate over the logits of the first output token since the following tokens are likely deterministic based on the first token. Overall, BC is zero-shot, inference-only, and incurs negligible additional costs. We expect easy prompt engineering with BC for users building towards their own robust and responsible LLM applications.\n# C DATASET STATISTICS\nTable 4: Details of the dataset used for evaluation in the Table 2. |Test| denotes the number of te samples, where we consistently use the validation split as the test split because labels are not publicl available for some datasets.\nDataset\nObjective\n#sentences\n#classes\n|Test|\nSST-2\nSentence Classification\n1\n2\n872\nMNLI\nNLI\n2\n3\n9815\nQNLI\nQuestion-Answering NLI\n2\n2\n5463\nMRPC\nParaphrasing\n2\n2\n408\nQQP\nParaphrasing\n2\n2\n40430\nBoolQ\nCommonsense Reasoning\n2\n2\n3270\nCB\nNLI\n2\n3\n56\nCOPA\nCommonsense Reasoning\n3\n2\n100\nRTE\nNLI\n2\n2\n277\nWiC\nContext Comprehension\n3\n2\n638\nANLI-R1\nNLI\n2\n3\n1000\nANLI-R2\nNLI\n2\n3\n1000\nANLI-R3\nNLI\n2\n3\n1200\n# D IMPLEMENTATION DETAILS\nContextual Calibration (Zhao et al., 2021) (CC). We follow the original implementation of CC and take the mean of the log-probability over three content-free tokens as the test sample in the predefined template: \u2018N/A\u2019, \u2018\u2019, \u2018[MASK]\u2019. It incurs 3 additional API costs from LLMs. Domain-Context Calibration (Fei et al., 2023) (DC). We reproduce the DC baseline by using the same test set as the unlabeled text set to construct its bag-of-words. We then randomly sample tokens for an average length to form the content-free and in-domain input from the bag-of-words This process is then repeated randomly for 20 times, and we take the mean of the log-probability following the original implementation. It incurs 20 additional API costs from LLMs. Prototypical Calibration (Han et al., 2023) (PC). For a fair comparison, we use the same test set as the unlabeled estimate set for PC. We follow the same hyper-parameters reported by PC with 100 maximum iterations for EM and 100 times random initialization for the whole learning process to stabilize its estimation. It is noteworthy that this number of repetitions is costly and relatively slow, especially when the |Test| is large.\nBatch Calibration (BC). In all reported experiments, we compute the correction log-probability term \u02c6p(y|C) once after all test samples are seen. In the n-shot ICL experiments reported in Table 2 and Fig. 6, the k-shot ICL is concatenating k random training sample per class. In the BCL experiment that uses labeled samples, we use J \u00d7 128 randomly selected training samples as the labeled data. In the robustness study, we use 1 randomly sampled example as the context to study the performance of BC with respect to the ICL choices. We then conduct the ICL order experiment by re-ordering 4 randomly sampled ICL examples. The rest experiments are conducted on the standard 1-shot ICL setup.\n# E ADDITIONAL EXPERIMENTS\nTable 5: Accuracy (%) on natural language classification tasks with 0-shot PaLM 2-S and 1-shot PaLM 2-M models in a single seed.\nModel\nPaLM 2-S\n0-shot\nPaLM 2-M\n1-shot\nMethod\nICL\nCC\nDC\nPC\nBC\nICL\nCC\nDC\nPC\nBC\nSST-2\n94.61\n94.50\n94.61\n87.84\n95.18\n94.95\n95.87\n94.95\n96.22\n96.10\nMNLI\n45.87\n52.54\n42.50\n38.04\n53.67\n45.50\n54.43\n56.26\n43.81\n60.02\nQNLI\n49.28\n48.97\n49.44\n50.28\n49.55\n78.88\n75.56\n62.95\n77.39\n78.91\nMRPC\n69.12\n61.76\n69.85\n69.85\n64.95\n57.11\n73.53\n68.87\n69.85\n65.93\nQQP\n60.23\n51.16\n49.12\n48.98\n56.20\n66.18\n79.67\n74.32\n70.27\n75.13\nBoolQ\n86.51\n86.97\n76.88\n55.41\n84.04\n87.37\n88.53\n87.28\n88.78\n87.31\nCB\n85.71\n58.93\n55.36\n46.43\n67.86\n71.43\n69.64\n67.86\n50.00\n80.36\nCOPA\n88.00\n66.00\n90.00\n52.00\n88.00\n97.00\n96.00\n96.00\n97.00\n96.00\nRTE\n62.45\n67.15\n58.12\n68.23\n71.84\n77.62\n79.06\n68.23\n77.98\n80.51\nWiC\n58.31\n51.88\n52.82\n49.22\n58.30\n61.13\n64.11\n52.04\n65.52\n68.03\nANLI-R1\n39.80\n44.70\n43.00\n37.00\n50.00\n52.40\n52.40\n52.70\n35.70\n54.00\nANLI-R2\n36.80\n41.50\n40.70\n40.20\n45.10\n46.00\n50.70\n47.80\n35.80\n50.00\nANLI-R3\n42.67\n46.42\n43.08\n35.50\n48.50\n43.50\n45.67\n49.33\n32.42\n50.50\nAvg.\n63.03\n59.42\n58.88\n52.23\n64.09\n67.62\n71.17\n67.58\n64.67\n72.52\nTable 6: Accuracy (%) on image classification tasks with the zero-shot CLIP ViT-16/B. We additionally report on UCF101 (Soomro et al., 2012), FGVC Aircraft (Maji et al., 2013), and DTD (Cimpoi et al., 2014).\nTable 6: Accuracy (%) on image classification tasks with the zero-shot CLIP ViT-16/B. We additionally report on UCF101 (Soomro et al., 2012), FGVC Aircraft (Maji et al., 2013), and DTD (Cimpoi\nMethod\nSVHN\nEuroSAT\nUCF\nCLEVR\nAircraft\nDTD\nAvg.\nZS\n18.0\n47.8\n66.7\n14.7\n24.8\n44.4\n36.7\nZS+BC\n35.0\n54.7\n66.0\n29.2\n22.3\n41.7\n41.5\nTable 7: Performance of BC with respect to prompt templates and verbalizers per example in Fig. 8. We report accuracy (%) on the RTE task on 1-shot PaLM 2-S. We refer each example ID to the index of templates and verbalizers listed in Tables 9 & 10.\nID\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTemplate\nICL\n81.2\n80.9\n82.3\n76.9\n83.0\n83.4\n83.4\n82.7\n79.1\n84.5\nBC\n84.1\n80.9\n82.3\n85.6\n84.1\n81.2\n88.1\n82.3\n83.0\n86.6\nVerbalizer\nICL\n81.2\n76.9\n74.0\n74.0\n67.5\n69.0\n71.5\n68.6\n65.3\n66.4\nBC\n84.1\n83.4\n83.4\n80.1\n78.0\n70.0\n82.7\n82.7\n83.8\n83.8\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ec0/0ec03b8b-4f8e-4b56-af5c-89c25201e9f9.png\" style=\"width: 50%;\"></div>\nFigure 10: Visualization of the decision boundaries of uncalibrated ICL, and after applying existing calibration methods and the proposed BC. We list all binary classification tasks from the evaluation\nFigure 10: Visualization of the decision boundaries of uncalibrated ICL, and after applying existing calibration methods and the proposed BC. We list all binary classification tasks from the evaluation set.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/009c/009c5e2d-cf3e-4a88-ac15-7c8c1f9c241b.png\" style=\"width: 50%;\"></div>\nFigure 11: The performance of an adaptable batch calibration layer (BCL) compared to the zero-shot BC with a changing strength. The strength \u03b3 at 0 and 1 represent the uncalibrated ICL and BC, respectively. We highlight the optimal strength learned from a labeled set by a red vertical line and the best test strength by a green line.\nDataset\nTemplate\nLabel Set\nSST-2\nReview: {sentence}\nSentiment: {label}\nnegative /\npositive\nMNLI\nCB\nANLI\nPremise: {premise}\nHypothesis: {hypothesis}\nAnswer: {label}\nyes / maybe / no\nQNLI\nQuestion: {question}\nSentence: {sentence}\nLabel: {label}\nyes / no\nMRPC\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nEquivalence: {label}\nno / yes\nQQP\nQuestion 1: {question1}\nQuestion 2: {question2}\nDuplicate: {label}\nno / yes\nBoolQ\n{passage}\nQuestion: {question}\nAnswer: {label}\nno / yes\nCOPA\nPremise: {premise}\nChoice1: {choice1}\nChoice2: {choice2}\nAnswer: {label}\n1 / 2\nRTE\nPremise: {sentence1}\nHypothesis: {sentence2}\nAnswer: {label}\nyes / no\nWiC\nSentence1: {sentence1}\nSentence2: {sentence2}\nWord: {word}\nAnswer: {label}\nfalse / true\nID\nTemplate\nLabel Set\n1\nPremise: {sentence1}\nHypothesis: {sentence2}\nAnswer: {label}\nyes / no\n2\n{sentence1}\nHypothesis: {sentence2}\nAnswer: {label}\n3\n{sentence1}\nQuestion: {sentence2}\nAnswer: {label}\n4\n{sentence1}\nQuestion: {sentence2}\n{label}\n5\n{sentence1}\nQuestion: {sentence2}\nyes or no? Answer: {label}\n6\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nAnswer: {label}\n7\nPremise: {sentence1}\nHypothesis: {sentence2}\nLabel: {label}\n8\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nLabel: {label}\n9\nDetermine if the sentence 2 is true based on the Sentence 1 below\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nAnswer: {label}\n10\nDetermine if the sentence 2 is true or false based on the Sentence 1 below\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nAnswer: {label}\nTable 10: Verbalizer choices for the robustness experiment conducted on RTE in Fig. 8, where we include emoji pairs for ID 8, 9, 10.\nID\nLabel Set\nTemplate\n1\nyes / no\nPremise: {sentence1}\nHypothesis: {sentence2}\nAnswer: {label}\n2\ntrue / false\n3\ncorrect / incorrect\n4\npositive / negative\n5\ngood / bad\n6\ngreat / terrible\n7\nit was true / it was false\n8\n:thumbs up / :thumbs down\n9\n:man gesturing ok / :man gesturing no\n10\n:check mark / :cross mark\n<div style=\"text-align: center;\">Table 11: Prompt templates for the 0-shot experiments.</div>\nDataset\nTemplate\nLabel Set\nSST-2\nReview: {sentence}\nSentiment: {label}\nnegative /\npositive\nMNLI\nCB\nANLI\n{premise}\nQuestion: {hypothesis} yes, no, or maybe?\nAnswer: {label}\nyes / maybe / no\nQNLI\n{question}\nQuestion: {sentence} yes or no?\nAnswer: {label}\nyes / no\nMRPC\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nEquivalence: {label}\nno / yes\nQQP\nQuestion 1: {question1}\nQuestion 2: {question2}\nDuplicate: {label}\nno / yes\nBoolQ\n{passage}\nQuestion: {question}\nAnswer: {label}\nno / yes\nCOPA\nPremise: {premise}\nChoice1: {choice1}\nChoice2: {choice2}\nAnswer: {label}\n1 / 2\nRTE\n{sentence1}\nQuestion: {sentence2} yes or no?\nAnswer: {label}\nyes / no\nWiC\n{sentence1}\n{sentence2}\nQuestion: Is the word \u2019{word}\u2019 used in the same way in the two\nsentences above?\nAnswer: {label}\nno / yes\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of prompt brittleness and bias factors in large language models (LLMs) that lead to performance degradation. Existing calibration methods have been developed to mitigate these biases, but a unified analysis revealing their limitations is lacking, necessitating a new method.",
        "problem": {
            "definition": "The problem is the sensitivity of LLMs to prompt design choices, which can lead to biased predictions and hinder effective adaptation to new tasks.",
            "key obstacle": "The main challenge is the inability of existing calibration methods to adequately address various sources of bias in LLMs, especially in complex multi-sentence classification tasks."
        },
        "idea": {
            "intuition": "The inspiration for Batch Calibration (BC) comes from the need to accurately model and control contextual bias in LLMs by leveraging batched input.",
            "opinion": "BC is proposed as a zero-shot, inference-only calibration method that effectively mitigates contextual bias without incurring significant computational costs.",
            "innovation": "BC differs from existing methods by focusing on a linear decision boundary and estimating contextual bias from batched inputs rather than relying on content-free tokens."
        },
        "method": {
            "method name": "Batch Calibration",
            "method abbreviation": "BC",
            "method definition": "BC is a calibration technique designed to estimate contextual bias in LLM outputs by marginalizing scores across a batch of inputs.",
            "method description": "BC is a simple, zero-shot calibration method that adjusts LLM outputs based on contextual bias derived from batched inputs.",
            "method steps": "1. Input a batch of examples; 2. Compute the output scores; 3. Estimate the contextual bias; 4. Adjust the output scores based on the bias.",
            "principle": "BC is effective due to its ability to leverage multiple samples to accurately model contextual bias, leading to more robust predictions."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on over 10 natural language understanding tasks and image classification tasks using PaLM 2 and CLIP models.",
            "evaluation method": "Performance was assessed by comparing the accuracy of BC against various calibration baselines across multiple tasks."
        },
        "conclusion": "BC demonstrates significant improvements over existing calibration methods, achieving state-of-the-art performance across tasks while being simple to implement and requiring minimal computational resources.",
        "discussion": {
            "advantage": "The key advantages of BC include its robustness to prompt design choices, zero-shot capability, and negligible computational overhead.",
            "limitation": "BC still relies on batched input to estimate contextual bias, which may introduce errors if the batch is not representative.",
            "future work": "Future research will explore extending BC to generative tasks and further improving its adaptability across different models and modalities."
        },
        "other info": {
            "additional details": {
                "contribution": "BC provides a unified framework for understanding calibration methods and offers a practical solution for mitigating biases in LLMs.",
                "future directions": "Investigating the application of BC in generative tasks and its performance in multi-modal settings."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.4",
            "key information": "Prompt brittleness and bias factors in large language models (LLMs) lead to performance degradation, necessitating effective prompt engineering to mitigate these issues."
        },
        {
            "section number": "3.1",
            "key information": "The sensitivity of LLMs to prompt design choices can lead to biased predictions and hinder effective adaptation to new tasks."
        },
        {
            "section number": "3.3",
            "key information": "Batch Calibration (BC) is a proposed calibration technique that estimates contextual bias in LLM outputs by marginalizing scores across a batch of inputs."
        },
        {
            "section number": "6.1",
            "key information": "The main challenge is the inability of existing calibration methods to adequately address various sources of bias in LLMs, especially in complex multi-sentence classification tasks."
        },
        {
            "section number": "6.4",
            "key information": "BC provides a unified framework for understanding calibration methods and offers a practical solution for mitigating biases in LLMs, which addresses scalability and applicability challenges."
        },
        {
            "section number": "7",
            "key information": "BC demonstrates significant improvements over existing calibration methods, achieving state-of-the-art performance across tasks while being simple to implement and requiring minimal computational resources."
        }
    ],
    "similarity_score": 0.7102818155676286,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Batch Calibration_ Rethinking Calibration for In-Context Learning and Prompt Engineering.json"
}