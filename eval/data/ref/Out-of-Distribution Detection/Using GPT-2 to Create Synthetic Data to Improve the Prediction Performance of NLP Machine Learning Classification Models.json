{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2104.10658",
    "title": "Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models",
    "abstract": "Classification Models use input data to predict the likelihood that the subsequent input data will fall into predetermined categories. To perform effective classifications, these models require large datasets for training. It is becoming common practice to utilize synthetic data to boost the performance of Machine Learning Models. It is reported that Shell is using synthetic data to build models to detect problems that rarely occur; for example Shell created synthetic data to help models to identify deteriorating oil lines. It is common practice for Machine Learning Practitioners to generate synthetic data by rotating, flipping, and cropping images to increase the volume of image data to train Convolutional Neural Networks. The purpose of this paper is to explore creating and utilizing synthetic NLP data to improve the performance of Natural Language Processing Machine Learning Classification Models. In this paper I used a Yelp pizza restaurant reviews dataset and transfer learning to fine-tune a pre-trained GPT-2 Transformer Model to generate synthetic pizza reviews data. I then combined this synthetic data with the original genuine data to create a new joint dataset. The new combined model significantly outperformed the original model in accuracy and precision.",
    "bib_name": "whitfield2021usinggpt2createsynthetic",
    "md_text": "# Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models\n# Dewayne Whitfield\nDewayne Whitfield\n# Abstract:\nClassification Models use input data to predict the likelihood that the subsequent input data will fall into predetermined categories. To perform effective classifications, these models require large datasets for training. It is becoming common practice to utilize synthetic data to boost the performance of Machine Learning Models. It is reported that Shell is using synthetic data to build models to detect problems that rarely occur; for example Shell created synthetic data to help models to identify deteriorating oil lines.(Higginbotham, 2020) It is common practice for Machine Learning Practitioners to generate synthetic data by rotating, flipping, and cropping images to increase the volume of image data to train Convolutional Neural Networks(CNN). The purpose of this paper is to explore creating and utilizing synthetic NLP data to improve the performance of Natural Language Processing (NLP) Machine Learning Classification Models. In this paper I used a Yelp pizza restaurant reviews dataset and transfer learning to fine-tune a pre-trained GPT-2 Transformer Model to generate synthetic pizza reviews data. I then combined this synthetic data with the original genuine data to create a new joint dataset. For performance comparison purposes,  I built two baseline models on two separate datasets using the Multinomial Naive Bayes Classifier algorithm. The two datasets were: The Yelp Pizza Reviews Dataset (450 observations) and a combined Yelp Pizza Reviews and Synthetic Yelp Reviews Dataset(11,380 observations). I created a separate ground truth dataset from the original Yelp Pizza Reviews Dataset that would serve as a ground truth test dataset. I then executed an analysis of the baseline models on the single ground truth test dataset to establish the following prediction performance metrics for each baseline model: precision, accuracy, recall, F1, and a confusion matrix. The combined Yelp Pizza Review Dataset outperformed the genuine Yelp Pizza Reviews Dataset on each of the performance metrics. The Baseline Model\u2019s Accuracy was increased from .8281 to .8913, a 7.63% increase. The Baseline Model\u2019s Precision was increased from .7979 to .8737, a 9.49% increase.\n# 1. Business Value:\nIn 2018, U.S. companies spent nearly $19.2 Billion on data acquisition and solutions to manage process, and analyze this data.(Sweeney, 2019) The technique discussed in this paper could reduce the cost of data acquisition by reducing the amount of data needed to train high performance NLP classification models. This technique could also be used to improve current\nmodels by expanding training datasets. These benefits could lead to companies and organizations achieving their goals more effectively while minimizing costs.\n# 2. Methodology:\n# 2.1 Introduction\nFor the research conducted in this paper I used the GPT-2 transformer model and Yelp open dataset pizza reviews to create the synthetic data.\nFor the research conducted in this paper I used the GPT-2 transformer model and Yelp open dataset pizza reviews to create the synthetic data.\n# 2.1 GPT-2\nDeveloped by OpenAI, GPT-2 is a large-scale transformer-based language model that is pre-trained on a large corpus of text: 8 million high-quality webpages.(Cheng,2020) The objective of GPT-2 is to predict the next word given all of the previous words within some text(Radford, 2020). The GPT-2 model can be trained with an additional custom dataset using a method called transfer learning to produce more relevant text.\n# 2.2 Yelp Open Dataset Reviews\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98b9/98b9e4b9-d2d9-493c-8d10-02e7a36a9737.png\" style=\"width: 50%;\"></div>\n# Figure 1:  Sample of Yelp Pizza Reviews Data Subset\n<div style=\"text-align: center;\">Figure 1:  Sample of Yelp Pizza Reviews Data Subset</div>\nThe Yelp Open Dataset contains anonymized reviews on various businesses and services (Yelp). For this paper I created a subset of data of pizza restaurant reviews. Within this subset of data I divided the ratings into \u201cPositive\u201d and \u201cNegative\u201d. Ratings that were 4 or 5 stars were categorized as \u201cPositive\u201d. Ratings that were 1 or 2 stars were categorized as \u201cNegative\u201d. For this paper my Negative dataset contained 225 observations and the Positive dataset also contained 225 observations.\n# 2.3 Technical Approach\nThe intent of the research in this paper is to train two GPT-2 models on a small subset of Positive and Negative Yelp Pizza Reviews data. I will then use the two GPT-2 models to produce synthetic Positive and Negative review datasets. I will finally combine the new synthetic datasets with the genuine dataset and fit a classification model to this dataset that will have the ability to determine negative and positive sentiment of pizza restaurant reviews.\n# 2.3.1 Generating Synthetic Review Data\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2265/226556ca-c459-4e5f-9fcc-40e671c65364.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2:  Synthetic Review Generation and Dataflow</div>\nMy first task was to create two GPT-2 models and train one of them on genuine negative Yelp pizza review data and the other model on genuine positive Yelp pizza review data. I then had these models generate synthetic negative and positive review data that was combined into one a single dataset.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/310b/310be61c-958e-48a2-8bda-ca41a5643d2c.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\">Figure 3:  GPT-2 Initiation</div>\n# 2.3.2 Synthetic Review Generation\nI chose the 355 million parameter GPT-2 model to build my two models. I used Google Colab as my development notebook.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d11/3d11e58f-153e-48da-a94a-4c6280f23fb0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4:  GPT-2 text generation prompt</div>\nI used the GPT2.Generate method to generate synthetic reviews. On average, my responses were 70 words long.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/95fe/95fe4db9-ffe8-4b66-b7b9-b673f0edc2b2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5:  Samples of GPT-2 Generated Prompts</div>\n# 2.3.3 GPT-2 Text Generation\nWhen generating synthetic reviews I wanted to ensure that the responses expanded on the genuine data and produced responses that were a strong representation of the genuine data. So when I wrote the prefix prompts I used words that were heavily represented in the genuine datasets.  I wrote a Python Function that organized the genuine dataset corpus into trigrams (3 word consecutive combinations), bigrams (2 word combinations) and words. This function also provides a count and numerically sorts the occurrences of these words and combinations.\nTable 1:  Sample of GPT-2 Prompt Aid Tool\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e301/e301207a-ec64-4fb1-ac6a-1a636d3b1c6a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 1:  Sample of GPT-2 Prompt Aid Tool</div>\n2.3.4 Genuine and Synthetic Dataset Concatenation\n2.3.4 Genuine and Synthetic Dataset Concatenation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5096/50968d7e-ce18-4f34-b3a3-27d74c90d303.png\" style=\"width: 50%;\"></div>\nAfter I created the synthetic Positive and Negative datasets I concatenated them with th genuine Negative and Positive Datasets.\n2.3.5 Performance Testing\n# 2.3.5 Performance Testing\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6e4/b6e43cc2-ddf3-41d3-9490-29df5fea65e1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7:  Baseline Model Testing</div>\nTo ensure there was a fair and equal analysis of the performance metrics, I used the scikit-learn train_test_split method to establish a single ground truth test set consisting of 198 observations derived from a totally separate dataset from the Yelp Open Dataset. I then built two baseline models on two datasets using the Multinomial Naive Bayes Classifier algorithm. The two datasets were: The genuine Yelp Pizza Reviews Dataset (450 observations) and the combined Genuine and Synthetic Yelp Reviews Dataset(11,380 observations).\n# 2.3.6 Performance Testing Results\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e4c/6e4cbaf9-b7f9-4612-ad94-ffc8c5541195.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 2:  Baseline Model Performance Metrics</div>\nI chose Precision, Accuracy, Recall, and F1 as performance metrics for the three baseline models. Overall, the Combined (Synthetic and Genuine) Model outperformed the Genuine Model on all performance metrics.\n# \n# \n# ure 8:  Baseline Model Confusion Model Analysis Result\nI also performed a Confusion Matrix Analysis. The Combined (Synthetic and Genuine) Model showed that the Genuine Model had more True Positive but less True Negatives when compared to the Combined (Synthetic and Genuine) Model. The Combined (Synthetic and\n\n# \nGenuine) Model had more False Positives but less False Negatives when compared to the Genuine Model.\nGenuine) Model had more False Positives but less False Negatives when compared to the Genuine Model.\n# 3. Concluding Remarks:\nIn conclusion, the Synthetic and Genuine Dataset performed well in all performance metrics This technique has the possibility of allowing organizations and businesses to build high performing NLP classification models without the high cost associated with large scale data acquisition. There are opportunities in exploring this technique on datasets with a larger observation count. There are also opportunities in exploring GPT-2 prompt design to better guide the GPT-2 model in generating relevant text. This is an exciting Machine Learning Technique that I feel deserves further exploration.\n1. Higginbotham, S. (2020, June 29). Fake data is great data when it comes to machine learning. Retrieved December 20, 2020, from https://staceyoniot.com/fake-data-is-great-data-when-it-comes-to-machine-learning/ 2. Sweeney, E. (2019, March 06). IAB: 78% of marketers will spend more on data in 2019. Retrieved December 31, 2020, from https://www.marketingdive.com/news/iab-78-of-marketers-will-spend-more-on-data-in-20 19/549811/\n3. Cheng, R. (2020, July 22). Fine-tuning GPT2 for Text Generation Using Pytorch. Retrieved December 31, 2020, from https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61 a4f1ba7 4. Radford, A. (2020, September 03). Better Language Models and Their Implications. Retrieved December 31, 2020, from https://openai.com/blog/better-language-models/ 5. Yelp Open Dataset. (n.d.). Retrieved January 01, 2021, from https://www.yelp.com/dataset 6. Whitfield, D. (2020, December 30). Retrieved December 31, 2020, from https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/ main/GPT_Prompt_Aid\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving the performance of Natural Language Processing (NLP) classification models by utilizing synthetic data generated through the GPT-2 model, highlighting the limitations of existing methods that rely solely on genuine datasets.",
        "problem": {
            "definition": "The problem is the insufficient size of genuine datasets for training NLP classification models, which hampers their predictive performance.",
            "key obstacle": "The main challenge is the high cost and effort involved in acquiring large datasets necessary for training effective NLP models."
        },
        "idea": {
            "intuition": "The idea was inspired by the successful use of synthetic data in other domains, such as image processing, where data augmentation techniques have significantly improved model performance.",
            "opinion": "The proposed idea involves generating synthetic NLP data using a fine-tuned GPT-2 model to augment existing datasets for better classification performance.",
            "innovation": "The key innovation lies in the application of the GPT-2 model for creating synthetic text data specifically tailored for NLP classification tasks, which differs from traditional data augmentation techniques."
        },
        "method": {
            "method name": "Synthetic NLP Data Generation using GPT-2",
            "method abbreviation": "SNDG-GPT2",
            "method definition": "This method involves training two separate GPT-2 models on genuine positive and negative Yelp pizza reviews to generate synthetic reviews, which are then combined with the original dataset.",
            "method description": "The core of the method is to generate synthetic reviews that mimic genuine data, thereby expanding the training dataset for improved model performance.",
            "method steps": [
                "Train one GPT-2 model on genuine negative reviews.",
                "Train another GPT-2 model on genuine positive reviews.",
                "Generate synthetic reviews using the trained models.",
                "Combine synthetic reviews with the original dataset.",
                "Fit a classification model to the combined dataset."
            ],
            "principle": "The method is effective because it leverages the generative capabilities of the GPT-2 model to create realistic text data that enhances the training process for NLP classification models."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the Yelp Pizza Reviews dataset consisting of 450 genuine observations and a combined dataset of 11,380 observations that included synthetic data.",
            "evaluation method": "The performance of the models was assessed using precision, accuracy, recall, F1 score, and confusion matrix analysis on a separate ground truth test dataset."
        },
        "conclusion": "The results indicated that the combined dataset of synthetic and genuine reviews significantly outperformed the genuine dataset alone, demonstrating the potential of synthetic data generation techniques in enhancing NLP classification models.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to reduce the need for large-scale data acquisition, thereby lowering costs and improving model performance with limited genuine data.",
            "limitation": "A limitation of the method is that the quality of synthetic data depends on the effectiveness of the training process of the GPT-2 model, which may not always produce high-quality text.",
            "future work": "Future research could explore the application of this technique on larger datasets and investigate the optimization of GPT-2 prompt design to improve the relevance and quality of generated text."
        },
        "other info": {
            "business value": "This technique could potentially save U.S. companies significant amounts in data acquisition costs while enhancing their NLP model performance.",
            "dataset details": {
                "positive reviews": 225,
                "negative reviews": 225
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "This paper addresses the issue of improving the performance of Natural Language Processing (NLP) classification models by utilizing synthetic data generated through the GPT-2 model, highlighting the limitations of existing methods that rely solely on genuine datasets."
        },
        {
            "section number": "2.2",
            "key information": "The problem is the insufficient size of genuine datasets for training NLP classification models, which hampers their predictive performance."
        },
        {
            "section number": "3.5",
            "key information": "The proposed idea involves generating synthetic NLP data using a fine-tuned GPT-2 model to augment existing datasets for better classification performance."
        },
        {
            "section number": "4.2",
            "key information": "The primary advantage of the proposed approach is its ability to reduce the need for large-scale data acquisition, thereby lowering costs and improving model performance with limited genuine data."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the method is that the quality of synthetic data depends on the effectiveness of the training process of the GPT-2 model, which may not always produce high-quality text."
        }
    ],
    "similarity_score": 0.6740391029189854,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models.json"
}