{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2104.02189",
    "title": "Robust Classification Under $\\ell_0$ Attack for the Gaussian Mixture Model",
    "abstract": "It is well-known that machine learning models are vulnerable to small but cleverly-designed adversarial perturbations that can cause misclassification. While there has been major progress in designing attacks and defenses for various adversarial settings, many fundamental and theoretical problems are yet to be resolved. In this paper, we consider classification in the presence of $\\ell_0$-bounded adversarial perturbations, a.k.a. sparse attacks. This setting is significantly different from other $\\ell_p$-adversarial settings, with $p\\geq 1$, as the $\\ell_0$-ball is non-convex and highly non-smooth. Under the assumption that data is distributed according to the Gaussian mixture model, our goal is to characterize the optimal robust classifier and the corresponding robust classification error as well as a variety of trade-offs between robustness, accuracy, and the adversary's budget. To this end, we develop a novel classification algorithm called FilTrun that has two main modules: Filtration and Truncation. The key idea of our method is to first filter out the non-robust coordinates of the input and then apply a carefully-designed truncated inner product for classification. By analyzing the performance of FilTrun, we derive an upper bound on the optimal robust classification error. We also find a lower bound by designing a specific adversarial strategy that enables us to derive the corresponding robust classifier and its achieved error. For the case that the covariance matrix of the Gaussian mixtures is diagonal, we show that as the input's dimension gets large, the upper and lower bounds converge; i.e. we characterize the asymptotically-optimal robust classifier. Throughout, we discuss several examples that illustrate interesting behaviors such as the existence of a phase transition for adversary's budget determining whether the effect of adversarial perturbation can be fully neutralized.",
    "bib_name": "delgosha2021robustclassificationell0attack",
    "md_text": "# Robust Classification Under \u21130 Attack for the Gaussian Mixture Model\nPayam Delgosha\u2217 Hamed Hassani\u2020 Ramtin Pedarsani\u2021 April 7, 2021\n# Payam Delgosha\u2217 Hamed Hassani\u2020 Ramtin Pedarsani\u2021 April 7, 2021\nAbstract\nIt is well-known that machine learning models are vulnerable to small but cleverly-designed adversarial perturbations that can cause misclassification. While there has been major progress in designing attacks and defenses for various adversarial settings, many fundamental and theoretical problems are yet to be resolved. In this paper, we consider classification in the presence of \u21130bounded adversarial perturbations, a.k.a. sparse attacks. This setting is significantly different from other \u2113p-adversarial settings, with p \u22651, as the \u21130-ball is non-convex and highly non-smooth. Under the assumption that data is distributed according to the Gaussian mixture model, our goal is to characterize the optimal robust classifier and the corresponding robust classification error as well as a variety of trade-offs between robustness, accuracy, and the adversary\u2019s budget. To this end, we develop a novel classification algorithm called FilTrun that has two main modules: Filtration and Truncation. The key idea of our method is to first filter out the non-robust coordinates of the input and then apply a carefully-designed truncated inner product for classification. By analyzing the performance of FilTrun, we derive an upper bound on the optimal robust classification error. We further find a lower bound by designing a specific adversarial strategy that enables us to derive the corresponding robust classifier and its achieved error. For the case that the covariance matrix of the Gaussian mixtures is diagonal, we show that as the input\u2019s dimension gets large, the upper and lower bounds converge; i.e. we characterize the asymptotically-optimal robust classifier. Throughout, we discuss several examples that illustrate interesting behaviors such as the existence of a phase transition for adversary\u2019s budget determining whether the effect of adversarial perturbation can be fully neutralized or not.\narXiv:2104.02189v1\n# 1 Introduction\nMachine learning has been widely used in a variety of applications including image recognition, virtual assistants, autonomous driving, many of which are safety-critical. Adversarial attacks to machine learning models in the form of a small perturbation added to the input have been shown to be effective in causing classification errors [BCM+13, SZS+14, GSS14, CW17, MMS+17]. Formally, the adversary aims to perturb the data in a small \u2113p-neighborhood so that the perturbed data is \u201cclose\u201d to the original data (e.g. imperceptible perturbation in the case of an image) and misclassification occurs. There have been a variety of attacks and defenses proposed in the literature which mostly focus\n\u2217Department of Computer Science, University of Illinois at Urbana Champaign, IL, delgosha@illinois.edu \u2020Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, hassani@seas.upenn.edu \u2021Department of Electrical and Computer Engineering, University of California, Santa Barbara, Santa Barbara, CA, ramtin@ece.ucsb.edu\nagainst adversarial attacks is iterative training with adversarial examples [MMS+18]. While adversarial training can improve robustness, it is shown that there is a fundamental tradeoff between robustness and test accuracy, and such defenses typically lack good generalization performance [TSE+19, SZC+18, RXY+19, AMKP19, ZYJ+19, JSH20]. The focus of this paper is different from such prior work as we consider the problem of robust classification under \u21130-bounded attacks. In this setting, given a pre-specified budget k, the adversary can choose up to k coordinates and arbitrarily change the value of the input at those coordinates. In other words, the adversary can change the input within the so-called \u21130-ball of radius k. In contrast with \u2113p-balls (p \u22651), the \u21130-ball is non-convex and highly non-smooth. Moreover, the \u21130 ball contains inherent discrete (combinatorial) structures that can be exploited by both the learner and the adversary. As a result, the \u21130-adversarial setting bears several fundamental challenges that are absent in other adversarial settings commonly studied in the literature and most techniques from prior work do not readily apply in the \u21130 setting. Complicating matters further, it can be shown that any piece-wise linear classifier, e.g. a feed-forward deep neural network with ReLu activations, completely fails in the \u21130 setting [SSRD19]. These all point to the fact that new methodologies are required in the \u21130 setting. The \u21130-adversarial setting involves sparse attacks that perturb only a small portion of the input signal. This has a variety of applications including natural language processing [JJZS19], malware detection [GPM+16], and physical attacks in object detection [LSK19]. Prior work on \u21130 adversarial attacks can be divided into two categories of white-box attacks that are gradient-based, e.g. [CW17, PMJ+16, MMDF19], and black-box attacks based on zeroth-order optimization, e.g. [SRBB18, CAS+20]. Defense strategies against \u21130-bounded attacks have also been proposed, e.g. defenses based on randomized ablation [LF20] and defensive distillation [PMW+16]. Moreover, [SSRD19] develops a simple mathematical framework to show the existence of targeted adversarial examples with \u21130-bounded perturbation in arbitrarily deep neural networks. Despite this interesting recent progress and practical relevance, many fundamental theoretical questions in the \u21130-setting have so far been unanswered: What are the key properties of a robust classifier (recall that all piece-wise linear classifiers fail)? What is the optimal robust classifier in standard theoretical settings such the Gaussian mixture model for data? Is there a trade-off between robustness and accuracy? How does the (optimal) robust classification error behave as the adversary\u2019s budget k increases? Are there any phase transitions? We consider the problem of classification with \u21130-adversarially perturbed inputs under the assumption that data is distributed according to the Gaussian mixture model. We formally introduce this setting in Section 2, and address the questions above in the proceeding sections. In particular, instead of searching for the exact form of the optimal robust classifier (which is intractable), we follow a design-based approach: We introduce a novel algorithm for classification as well as strategies for the adversary. We then precisely characterize the error performance of these methodologies, and consequently, analyse the optimal robust classification error, tradeoffs between robustness and accuracy, phase transitions, etc. We envision that our proposed classification method introduces important modules and insights that are necessary to obtain robustness against \u21130-adversaries for general data distributions (and practical datasets), going beyond the theoretical setting of this paper. Summary of Contributions. The main contributions of this paper are as follows:\n\u2022 We propose a new robust classification algorithm called FilTrun that is based on two main modules: Filtration and Truncation (See Section 3.1.1 and Algorithm 1 therein). The filtration module removes the non-robust coordinates (features) from the input by zeroing out their values. The result is then passed through the truncation module which returns a label by computing a truncated inner product with a weight vector whose weights are optimized according to the distribution of un-filtered (surviving) coordinates. The truncation module is inspired by tools\nfrom robust statistics and guarantees that major outlier values in the input vector, which are possibly caused by the adversary, do not pass to affect the final decision. We highlight that the proposed classifier is highly nonlinear. This is consistent with the simple observation that any linear classifier fails to be robust in the presence of \u21130 attacks. \u2022 We analytically derive the robust classification error of the proposed classifier. This in particular serves as an upper bound on the optimal robust classification error (See Theorem 1 and Corollary 1). \u2022 We introduce adversarial strategies which, given sufficient budget, perturb the input in a way that the information about the true label is totally erased within the adversarially modified coordinates. The key idea is to pick a subset of the coordinates and to modify their distribution so that they become independent from the true label. This leads to a lower bound for the optimal robust error. (See Theorems 2 and 3). \u2022 In the case of having a diagonal covariance matrix for the Gaussian mixtures, we prove that our proposed algorithm FilTrun is indeed asymptotically-optimal, i.e. as the input dimension d approaches infinity, the upper and lower bounds converge to the same analytical expression (See Theorems 4 in Section 3.3.2). To the best of our knowledge, this is the first result that establishes optimality for the robust classification error of any mathematical model with \u21130 attack. \u2022 We discuss our results through several example scenarios. In certain scenarios, a phase transition is observed in the sense that for a threshold \u03b10, when the adversary\u2019s budget is asymptotically below d\u03b10, its effect can be completely neutralized, while if the adversary\u2019s budget is above d\u03b10, no classifier can do better than a naive classifier. In some other scenarios, no sharp phase transition is existent, leading to a trade-off between robustness and accuracy.\n# 2 Problem Formulation\nWe consider the binary Gaussian mixture model where the distribution for the data generation is specified by the label being y \u223cUnif{\u00b11} and x \u223cN(y\u00b5, \u03a3), i.e. the Gaussian distribution with mean y\u00b5 and covariance matrix \u03a3, where \u00b5 \u2208Rd and \u03a3 is positive definite. Hereafter, we denote this distribution by (x, y) \u223cD and refer to y as the label and to x as the input. Our results correspond to arbitrary choices of \u00b5 and \u03a3, however, we consider as running example an important special case in which \u03a3 is a diagonal matrix, i.e. the coordinates of x are independent conditioned on y. Focusing on classification, we consider functions of the form C : Rd \u2192{\u22121, 1} that predict the label from the input. As a metric for the discrepancy between the prediction of the classifier on the input x and the true label y, we consider the 0-1 loss \u2113(C; x, y) = 1 [C(x) \u0338= y] . We consider classification in the presence of an adversary that perturbs the input x within the \u21130-ball of radius k:\n# B0(x, k) := {x\u2032 \u2208Rd : \u2225x \u2212x\u2032\u22250 \u2264k},\nwhere for x = (x1, \u00b7 \u00b7 \u00b7 , xd) we define \u2225x\u22250 := \ufffdd i=1 1 [xi \u0338= 0]. In other words, the adversary ca arbitrarily modify at most k coordinates of x to obtain x\u2032, and feed the new vector x\u2032 to the classifie We call k the budget of the adversary. In this setting, the robust classification error of a classifier C  defined to be the following:\n(1)\n# We aim to design classfiers with minimum robust classification error. Hence, we define the optimal robust classification error by minimizing (1) over all possible classifiers:\nL\u2217 \u00b5,\u03a3(k) := inf C L\u00b5,\u03a3(C, k).\nOur goal in this paper is to precisely characterize L\u2217 \u00b5,\u03a3(k) parameterized by \u03a3, \u00b5 and in different egimes of the adversary\u2019s budget k.\nOur goal in this paper is to precisely characterize L\u2217 \u00b5,\u03a3(k) parameterized by \u03a3, \u00b5 and in different regimes of the adversary\u2019s budget k. It is well known that in the absence of the adversary, i.e. when k = 0, the Bayes optimal classifier is the linear classifier C(x) = sgn \ufffd \u27e8\u03a3\u22121\u00b5, x\u27e9 \ufffd which achieves the optimal standard error of \u00af\u03a6(\u2225\u03bd\u22252) where \u03bd := \u03a3\u22121/2\u00b5 and \u00af\u03a6(x) := 1 \u2212\u03a6(x) denotes the complementary CDF of a standard normal distribution. In order to fix the baseline, specifically to have a meaningful asymptotic discussion, we may assume without loss of generality that\nHence, the optimal standard error, which is a lower bound for (2), becomes \u00af\u03a6(1). To highlight some of the main challenges of the \u21130-adversarial setting, we note that linear classifiers in general have been very successful in the Gaussian mixture setting. Apart from the fact that the Bayes-optimal classier is linear (when there is no adversary), even when the adversarial corruptions are chosen in a \u2113p-ball for p \u22651 it can be shown that the optimal robust classifiers in many cases are also linear (see [BCM19, DHHR20]). In contrast, in the presence of \u21130-adversaries, it is not hard to show that any linear classifier completely fail. More precisely, when C is linear and k \u22651, we have L\u00b5,\u03a3(C, k) = 1 2. Such failure of linear classifiers showcases, on the one hand, how powerful the adversary is, and on the other hand, the necessity of new methodologies in designing robust classifiers. Further Related Work. For \u2113p adversaries, p \u22651, Gaussian mixture models have been the main setting used in prior work to investigate optimal rules, trade-offs, and various other phenomena for robust classification; See e.g. [SST+18, BCM19, DHHR20, Hay20, RW20, DWR20, PJ20, CMZK20, MCK20, PMP20]. Further, [SHS+18] considers data to be uniformly distributed on the sphere or cube and shows the inevitability of adversarial examples in \u2113p-settings, p \u22650. In contrast, to the best of our knowledge, our work provides the first comprehensive study on the \u21130-adversarial setting using the Gaussian mixture model. Notation. Given two vectors x, y \u2208Rd, x \u2299y \u2208Rd denotes the elementwise product of x and y, i.e. (x1y1, . . . , xdyd). Moreover, sort(x) denotes the vector containing the elements in x in descending order. For a \u2208R, sgn(a) returns the sign of a. We use [d] to denote the set {1, . . ., d} and [i : j] denotes the set {i, i + 1, . . ., j}. Given a vector x \u2208Rd and a subset A \u2286[d], xA = (xa : a \u2208A) \u2208R|A| denotes the subvector of x consisting of the coordinates in A. Given a matrix \u03a3, its diagonal part, denoted by \ufffd\u03a3, has the same diagonal entries as \u03a3 and its other entries are 0. Given a matrix A \u2208Rd\u00d7d, \u2225A\u2225\u221e denotes the operator norm of A induced by the vector \u2113\u221enorm, i.e. \u2225A\u2225\u221e:= supx\u0338=0 \u2225Ax\u2225\u221e/\u2225x\u2225\u221e= max1\u2264i\u2264d \ufffdd j=1 |Ai,j|.\n# 3 Main Results\nIn this section, we state our main results that include (i) the proposed algorithm and its performance analysis that serves as an upper bound on the optimal robust classification error (Section 3.1), (ii) lower bound on the optimal robust classification error (Section 3.2), and (iii) discussion on the optimality of the proposed algorithm (Section 3.3). Throughout, we illustrate our theoretical results and their ramifications via several examples.\n(2)\n(3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d49/6d4967b4-3fd1-40ea-9142-06433f94d41d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Schematic of FilTrun.</div>\n# 3.1 Upper Bound on the Optimal Robust Classification Error: Algorithm Description and Theoretical Guarantees\nIn Section 3.1.1, we introduce FilTrun, our proposed robust classification algorithm, and in Sec tion 3.1.2, we analyze its performance.\n# 3.1.1 Algorithm Description\nWe describe our proposed algorithm FilTrun, a robust classifier which is based on two main modules: Truncation and Filtration. We first introduce each of these modules and then proceed with describing\nWe describe our proposed algorithm FilTrun, a robust classifier which is based on two main modules: Truncation and Filtration. We first introduce each of these modules and then proceed with describing the classifier. Truncation. Given vectors w, x \u2208Rd and an integer 0 \u2264k < d/2, we define the k\u2013truncated inner product of w and x as the summation of the element-wise product of w and x after removing the top and bottom k elements, and denote it by \u27e8w, x\u27e9k. More precisely, let z := w \u2299x \u2208Rd be the element-wise product of w and x and let s = (s1, \u00b7 \u00b7 \u00b7 , sd) = sort(z) be obtained by sorting coordinates of z in descending order. We then define\nTruncation. Given vectors w, x \u2208Rd and an integer 0 \u2264k < d/2, we define the k\u2013truncated inner product of w and x as the summation of the element-wise product of w and x after removing the top and bottom k elements, and denote it by \u27e8w, x\u27e9k. More precisely, let z := w \u2299x \u2208Rd be the element-wise product of w and x and let s = (s1, \u00b7 \u00b7 \u00b7 , sd) = sort(z) be obtained by sorting coordinates of z in descending order. We then define\nNote that when k = 0, this reduces to the normal inner product \u27e8w, x\u27e9. Truncation is a natural method to remove \u201coutliers\u201d which might exist in the data due to an adversary modifying some coordinates. Therefore, we expect the truncated inner product to be robust against \u21130 perturbations. The following lemma formalizes this. The proof of Lemma 1 is given in Appendix A.\nLemma 1. Given x, x\u2032, w \u2208Rd, for integer k satisfying \u2225x \u2212x\u2032\u22250 \u2264k < d/2, we have\n|\u27e8w, x\u2032\u27e9k \u2212\u27e8w, x\u27e9| \u22648k\u2225w \u2299x\u2225\u221e.\nIn the context of our problem, this lemma suggests that if the budget of the adversary is at most k, we can bound the difference between the k\u2013truncated inner product between w and the adversarially modified sample x\u2032 and the (non-truncated) inner product between w and the original sample x. Recall that in the absence of the adversary, the optimal Bayes classifier is a linear classifier of the form sgn(\u27e8w, x\u27e9) with w = \u03a3\u22121\u00b5. Hence, motivated by Lemma 1, one can argue that sgn(\u27e8w, x\u2032\u27e9k) would be robust against \u21130 adversarial attacks with budget at most k assuming we can appropriately control the bound of Lemma 1. However, this is not enough\u2013it turns out that in certain cases, we need to filter out some of the input coordinates and perform the truncation on the remaining coordinates, which we call the surviving coordinates.\n(4)\nFiltration refers to discarding some of the coordinates of the input. Intuitively, these coordinates are the non-robust features which do more harm than good when the input is adversarially corrupted. More precisely, given a fixed and nonempty subset of coordinates F \u2286[d], we define the classifier C(k) F as follows:\nwhere\nand\n\ufffd  \u2212 \u2212 | \ufffd is the covariance matrix of xF conditioned on y, which is essentially the submatrix of \u03a3 corresponding to the elements in F. Note that w(F) is the optimal Bayes classifier of y given xF in the absence of the adversary. It is easy to see that when \u03a3 is diagonal, w(F) = wF , but this might not hold in general. Algorithm 1 and Figure 1 illustrate the classification procedure FilTrun given in (5). So far we have not explained how the set F is chosen and the algorithm works with any such set given as an input. Later we discuss how the set F is chosen (see Remarks 2 and 5).\nAlgorithm 1 FilTrun\nInput:\nk: adversary\u2019s \u21130 budget\n\u00b5, \u03a3: parameters of the Gaussian distribution\nF: the set of surviving coordinates\nx\u2032: the corrupted input\nOutput:\nC(k)\nF (x\u2032)\n1: function FilTrun(k, \u00b5, \u03a3, F, x\u2032)\n2:\nFiltering: Construct \u00b5F, \u03a3F and x\u2032\nF corresponding to the coordinates in F\n3:\nCompute w(F) \u2190\u03a3\u22121\nF \u00b5F\n4:\nTruncation: Compute \u27e8w(F), x\u2032\nF \u27e9k\n5:\nReturn sgn (\u27e8w(F), x\u2032\nF \u27e9k)\n6: end function\n3.1.2 Upper bound on the robust classification error of FilTrun Theorem 1 below states an upper bound for the robust error associated with the classification algorithm FilTrun introduced in Section 3.1.1. In particular, this yields an upper bound on the optimal robust classification error. The proof of Theorem 1 is given in Appendix B. Theorem 1. Assume that \u00b5, \u03a3 are given such that (3) holds. For a given nonempty F \u2286[d] and 0 \u2264k < d/2, we have\nwhere \u03a3F is defined in (6), \ufffd\u03a3F is the diagonal part of \u03a3F , and\nwhere \u03a3F is defined in (6), \ufffd\u03a3F is the diagonal part of \u03a3F , and \u03bd(F) := \u03a3\u22121/2 F \u00b5F .\n(5)\n(6)\n(7)\nRemark 1. Recall from Section 3.1.1 that F is the set of coordinates used for classification (i.e. the information in the coordinates F c is discarded). Therefore, we essentially work with xF as an input. If the adversary is not present, the optimal classification error is achieved via the Bayes linear classifier which has error \u00af\u03a6(\u2225\u03bd(F)\u22252). However, due to the existence of an adversary, we need to perform truncation which influences the error through the second term inside the argument of \u00af\u03a6 in (7). Remark 2. The bound in Theorem 1 can be used as a guide to choose the set of surviving coordinates F. More precisely, we can choose F which minimizes the right hand side in (8). Later, in Section 3.3, we discuss a simpler mechanism for choosing F when the covariance matrix \u03a3 is diagonal (see Remark 5 therein). Here, we outline the proof of Theorem 1. Due to the symmetry, we only need to analyze the classification error when y = 1. In this case, an error occurs only when there exists some x\u2032 \u2208B0(x, k) such that \u27e8w(F), x\u2032 F \u27e9k \u22640. But since \u2225x\u2032 F \u2212xF\u22250 \u2264\u2225x\u2032 \u2212x\u22250 \u2264k, Lemma 1 implies that for such x\u2032, we have |\u27e8w(F), x\u2032 F \u27e9k \u2212\u27e8w(F), xF \u27e9| \u22648k\u2225w(F) \u2299xF \u2225\u221e. Therefore, the robust classification error is upper bounded by P (\u27e8w(F), xF \u27e9\u22648k\u2225w(F) \u2299xF \u2225\u221e). But the random variable \u27e8w(F), xF \u27e9 is Gaussian with a known distribution, and the proof follows by bounding \u2225w(F) \u2299xF \u2225\u221e. See Appendix B for details. When the covariance matrix \u03a3 is diagonal, \u03a3F is also diagonal and \ufffd\u03a31/2 F \u03a3\u22121/2 F = I. Moreover, \u03bd(F) = \u03bdF where \u03bd := \u03a3\u22121/2\u00b5. This yields the following corollary of Theorem 1. Corollary 1. Assume that \u00b5, \u03a3 are given such that (3) holds and \u03a3 is diagonal. Then, for nonempty F \u2286[d] we have\nand in particular\nNow we discuss the above bounds via two examples, which we use as running examples to discuss our results in the subsequent sections as well. In the following, Id \u2208Rd\u00d7d and 1d \u2208Rd denote the d \u00d7 d identity matrix and the all-ones vector of size d, respectively. Example 1. Let \u03a3 = Id and \u00b5 = 1 \u221a d1d. In the absence of the adversary, the optimal Bayes classification error is \u00af\u03a6(1). Moreover, simplifying the bounds in Corollary 1, we get\nNow we discuss the above bounds via two examples, which we use as running examples to discuss our results in the subsequent sections as well. In the following, Id \u2208Rd\u00d7d and 1d \u2208Rd denote the d \u00d7 d identity matrix and the all-ones vector of size d, respectively. Example 1. Let \u03a3 = I and \u00b5 = 1 \u221a 1. In the absence of the adversary, the optimal Bayes classifi-\nThis is minimized when F = [d], resulting in\n(8)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f725/f7253a65-3916-4aba-884d-ef1dad48f3b3.png\" style=\"width: 50%;\"></div>\nFigure 2: The idea behind our proposed strategy for the adversary when d = 1. Assume \u00b51 > 0 and the adversary observes a realization (x1, y) such that y = 1, meaning that x1 is a realization of N(\u00b51, \u03c32 1) (i.e. the blue curve). If x1 \u22640, the adversary leaves it unchanged, i.e. x\u2032 1 = x1. On the other hand, if x1 > 0, we compute the ratio between the two densities (which is precisely p1(x1, y) shown in the figure), and with probability p1(x1, y) we pick x\u2032 1 from an arbitrary distribution (e.g. Uniform[\u22121, 1]). When y = \u22121, we follow a similar procedure, but reversed. It is easy to see that by doing so, the distribution of x\u2032 1 is the same when y = 1 and y = \u22121, hence x\u2032 1 bears no information about y. Note that if k = o( \ufffd d/ log d), the upper bound is approximately \u00af\u03a6(1) which is the optimal classification error in the absence of the adversary. This means that for k = o( \ufffd d/ log d), the effect of the adversary can be completely neutralized. We will show a lower bound for this example later in Section 3.2 (see Example 3 therein) which shows that when k \u2265 \u221a d log d, no classifier can do asymptotically better than a naive classifier. This establishes a phase transition at k = \u221a d up to logarithmic terms. Example 2. Let \u03a3 = Id and \u00b5 = (d\u22121 3 , cd\u22121 2 , cd\u22121 2 , . . . , cd\u22121 2 ) where c is chosen such that \u2225\u00b5\u22252 = 1, resulting in an optimal standard error of \u00af\u03a6(1) in the absence of the adversary. It turns out that the set F that optimizes the bound in Corollary 1 is the set [2 : d], i.e. we need to discard the first coordinate. In addition to this, we can see that if the classifier does not discard the first coordinate, it can neutralize adversarial attacks with budget of at most d 1 3 \u2212\u01eb, while discarding the first coordinate makes the classifier immune to adversarial budgets up to d 1 2 \u2212\u01eb. In fact, although the first coordinate is more informative compared to the other coordinates, due to this very same reason it is more susceptible to adversarial attacks, and it can do more harm than good when the input is adversarially corrupted. This example highlights the importance of the filtration phase.\n# 3.2 Lower Bound on Optimal Robust Classification Error: Strategie the Adversary\n# 3.2 Lower Bound on Optimal Robust Classification Error: Strategies for\nIn this section, we provide a lower bound on the optimal robust classification error. This is accomplished by introducing an attack strategy for the adversary, and showing that given such a fixed attack, no classifier can achieve better than the lower bound that we introduce. The strategy is best understood when the covariance matrix is diagonal. Therefore, we first assume that \u03a3 is diagonal and denote the diagonal elements of \u03a3 by \u03c32 1, . . . , \u03c32 d. We later use our strategy for diagonal covariance matrices to get a general lower bound for arbitrary \u03a3 (see Theorem 3 at the end of this section). Assume that the adversary observes realizations (x, y) \u223cD generated from the Gaussian mixture model with parameters \u00b5, \u03a3, where \u03a3 is diagonal. A randomized strategy for the adversary with budget k is identified by a probability distribution which upon observing such realizations (x, y), generates a random vector x\u2032 that satisfies P (\u2225x\u2032 \u2212x\u22250 \u2264k | x, y) = 1. The goal of the adversary is to design this randomized strategy in a way that the corrupted vector x\u2032 bears very little information (or even no information) about the label y. In this way, the loss in (2) will be maximized. Before rigorously defining our proposed strategy for the adversary, we illustrated its main idea when d = 1 in Figure 2.\nRecall that \u03bd = \u03a3\u22121/2\u00b5. Since \u03a3 is diagonal, \u03bdi = \u00b5i/\u03c3i. We will fix a set of coordinates A \u2286[d] and a specific value for the budget k(A) = \u2225\u03bdA\u22251 log d. We introduce a randomized strategy for the adversary with the following properties: (i) it can change up to k(A) coordinates of the input; and (ii) all the changed coordinates belong to A, i.e. the coordinates in Ac are left untouched. We denote this adversarial strategy by Adv(A). Given A \u2282[d], having observed (x, y), Adv(A) follows the procedure explained below. Let Z = (Z1, \u00b7 \u00b7 \u00b7 , Zd) \u2208Rd be a random vector that Adv(A) constructs using the true input x. First of all, recall that Adv(A) does not touch the coordinates that are not in A, i.e. for i \u2208Ac we let Zi = xi. For each i \u2208A, the adversary\u2019s act is simple: it either leaves the value unchanged, i.e. Zi = xi, or it erases the value, i.e. Zi \u223cUnif[\u22121, 1]\u2013a completely random value between \u22121 and +1. This binary decision is encoded through a Bernoulli random variable Ii taking value 0 with probability pi(xi, y) and value 1 otherwise. Here pi(xi, y) is defined as \ufffd\nNote that the condition sgn(xi) = sgn(y\u00b5i) ensures that pi(xi, y) \u22641. In sum Adv(A) lets\n  where Ii = Bernoulli (1 \u2212pi(xi, yi)), and the random variables Ii are generated completely independently w.r.t. all the other variables. It is easy to see that the following holds for the conditional density\n  where Ii = Bernoulli (1 \u2212pi(xi, yi)), and the random variables Ii are generated completely indepen dently w.r.t. all the other variables. It is easy to see that the following holds for the conditional densit of ZA given y\nwhere for i \u2208A\nIn other words, \u03b1i is the probability of changing coordinate i. Finally, Adv(A) checks if the vectors Z and x differ within the budget constraint k(A) := \u2225\u03bdA\u22251 log d. Define x\u2032 as follows:\nIt can be shown that with high probability, Z is indeed within the specified budget and x\u2032 = Z. From this definition, it is evident that with probability one we have\nIt can be shown that with high probability, Z is indeed within the specified budget and x\u2032 = Z. From this definition, it is evident that with probability one we have \u2225x\u2032 \u2212x\u2225 \u2264\u2225\u03bd\u2225 log d, (12\n\u2225x\u2032 \u2212x\u22250 \u2264\u2225\u03bdA\u22251 log d,\nand hence Adv(A) is a randomized adversarial strategy that only changes the coordinates in A and has budget k(A) = \u2225\u03bdA\u22251 log d. Now we use this adversarial strategy to show the following result. The proof of Theorem 2 is given in Appendix C. Theorem 2. Assume that the covariance matrix \u03a3 is diagonal and let \u03bd = \u03a3\u22121/2\u00b5. Then for any subset A \u2286[d], we have L\u2217 \u00b5,\u03a3 \ufffd \u2225\u03bdA\u22251 log d \ufffd \u2265\u00af\u03a6(\u2225\u03bdAc\u22252) \u2212 1 .\n(9)\n(10)\n(11)\n(12)\nThe main idea behind this result and the above adversarial strategy is that due to (10), ZA is independent from y and since the coordinates of the input are independent from each other, and since with high probability x\u2032 = Z, the coordinates in A have no useful information for the classifier. Hence, the classifier can do no better than the optimal Bayes classifier for the remaining coordinates in Ac, which results in a classification error of \u00af\u03a6(\u2225\u03bdAc\u22252). We now apply the bound of Thm 2 to Examples 1, 2 that we discussed in Section 3.1.2.\nwhich results in a classification error of \u00af\u03a6(\u2225\u03bdAc\u22252). We now apply the bound of Thm 2 to Examples 1, 2 that we discussed in Section 3.1.2. Example 3. Assume that \u00b5 and \u03a3 are as in Example 1. Applying the bound in Theorem 2, we get\nExample 3. Assume that \u00b5 and \u03a3 are as in Example 1. Applying the bound in Theorem 2, we get\nExample 3. Assume that \u00b5 and \u03a3 are as in Example 1. Applying the bound in Theorem 2, we get\nTherefore, setting A = [d], we obtain a lower bound of almost \u00af\u03a6(0) = 1/2 for adversarial budget \u221a d log d. In other words, if the adversarial budget is more than \u221a d log d, asymptotically no classifier can do better than a random guess. This together with the discussion in Example 1 establishes a phase transition around \u221a d (modulo logarithmic terms). Example 4. Assume that \u00b5 and \u03a3 are as in Example 2. Applying the bound of Theorem 2 with A = [d], we obtain L\u2217 \u00b5,\u03a3(k) \u2265\u00af\u03a6(0) \u22121/ log d \u22481/2 where k = (d\u22121 3 + c(d \u22121)/ \u221a d) log d \u2248 \u221a d log d. Hence, comparing this to Example 2, we find similar to Example 3 above that a phase transition occurs around adversarial budget \u221a d up to logarithmic terms. Now we state our general lower bound which holds for an arbitrary covariance matrix. This is Theorem 3 below, whose proof is provided in Appendix D. Given \u00b5 and \u03a3, we define the d \u00d7 d matrix R where the i, j entry in R is Ri,j = \u03a3i,j/\ufffd\u03a3i,i\u03a3j,j. In other words, Ri,j is the correlation coefficient between the ith and the jth coordinates in our Gaussian noise. Equivalently, with \ufffd\u03a3 being the diagonal part of \u03a3, we may write\nTherefore, setting A = [d], we obtain a lower bound of almost \u00af\u03a6(0) = 1/2 for adversarial budget \u221a d log d. In other words, if the adversarial budget is more than \u221a d log d, asymptotically no classifier can do better than a random guess. This together with the discussion in Example 1 establishes a phase transition around \u221a d (modulo logarithmic terms).\nExample 4. Assume that \u00b5 and \u03a3 are as in Example 2. Applying the bound of Theorem 2 with A = [d], we obtain L\u2217 \u00b5,\u03a3(k) \u2265\u00af\u03a6(0) \u22121/ log d \u22481/2 where k = (d\u22121 3 + c(d \u22121)/ \u221a d) log d \u2248 \u221a d log d. Hence, comparing this to Example 2, we find similar to Example 3 above that a phase transition occurs around adversarial budget \u221a d up to logarithmic terms.\nNow we state our general lower bound which holds for an arbitrary covariance matrix. This i Theorem 3 below, whose proof is provided in Appendix D. Given \u00b5 and \u03a3, we define the d \u00d7 d matri R where the i, j entry in R is Ri,j = \u03a3i,j/\ufffd\u03a3i,i\u03a3j,j. In other words, Ri,j is the correlation coefficien between the ith and the jth coordinates in our Gaussian noise. Equivalently, with \ufffd\u03a3 being the diagona part of \u03a3, we may write\n \ufffd \ufffd It is evident that since \u03a3 is assumed to be positive definite, R is also positive definite. Furtherm we define u = (u1, . . . , ud) where \u00b5\n\ufffd Theorem 3. With u and R defined as in (13) and (14) respectively, for all A \u2286[d], we have\nwhere \u03b6min > 0 denotes the minimum eigenvalue of R.\nRemark 3. Note that when \u03a3 is diagonal, we have R = Id, \u03b6min = 1, and u = \u03bd = \u03a3\u22121/2\u00b5. Therefore, the bound in Theorem 3 reduces to that of Theorem 2.\n# 3.3 Optimality of FilTrun in the diagonal regim\nWe have already seen for our two running examples that up to logarithmic terms, our lower and upper bounds match (Examples 1 and 2 for upper bound, and their matching lower bounds in Examples 3 and 4, respectively). First, in Section 3.3.1, we show that our lower and upper bounds indeed match up to logarithmic terms in the diagonal regime, i.e. when the covariance matrix is diagonal. Then, in Section This in particular implies that our robust classification algorithm FilTrun is optimal in this regime.\n(13)\n(14)\n# 3.3.1 Comparing the Bounds\nIn Theorem 4 below, in the diagonal regime we compare our upper bound of Corollary 1 and our lower bound of Theorem 2. Proof of Theorem 4 is given in Appendix E. Recall that \u03bd := \u03a3\u22121/2\u00b5 and we assume (3) holds. When \u03a3 is diagonal and its diagonal entries are \u03c32 1, . . . , \u03c32 d, we have \u03bdi = \u00b5i/\u03c3i. Without loss of generality, we may assume that the coordinates of \u03bd are decreasingly ordered such that\nGiven c \u2208[0, 1], we define\nTheorem 4. If \u03a3 is diagonal and the coordinates in \u03bd are sorted as in (15), then: 1. For 0 \u2264c < 1, we have\nheorem 4. If \u03a3 is diagonal and the coordinates in \u03bd are sorted as in (15), then:\n2. For 0 < c \u22641, we have\nL\u2217 \u00b5,\u03a3(\u2225\u03bd[1:\u03bbc]\u22251 log d) \u2265\u00af\u03a6( \ufffd 1 \u2212c2) \u2212 1 log d.\nL\u2217 \u00b5,\u03a3(\u2225\u03bd[1:\u03bbc]\u22251 log d) \u2265\u00af\u03a6( \ufffd 1 \u2212c2) \u2212 1 log d.\nL\u2217 \u00b5,\u03a3(\u2225\u03bd[1:\u03bbc]\u22251) \u2248\u00af\u03a6( \ufffd 1 \u2212c2).\n\ufffd Recall from our previous discussion that we are interested in studying adversarial budgets scaling as d\u03b1, which justifies neglecting the multiplicative logarithmic terms. Furthermore, following the proof of Theorem 4, the upper bound in the first part is obtained by our robust classifier by setting F = {\u03bbc, . . . , d}. Roughly speaking, the classifier discards the coordinates in \u03bd which constitute fraction c of the \u21132 norm of \u03bd, and performs a truncated inner product classification on the remaining coordinates. But the \u21132 norm of the remaining coordinates is roughly \u221a 1 \u2212c2, and the effect of truncation is vanishing as long as the adversarial power is below \u2225\u03bd[1:\u03bbc]\u22251 by a logarithmic factor. Note that although the top coordinates in \u03bd are relatively more important in terms of the classification power, due to the same reason, they are more susceptible to adversarial attack.\nRemark 5. In view of Theorem 4 and Remark 4, we can introduce the following mechanism for choosing the surviving set F for the adversary given adversarial power k. Let r(k) = min{r : \u2225\u03bd[1:r]\u22251 \u2265 k log d} and set F = [r(k) : d]. Then the classifier C(k) F achieves the optimal robust classification error of almost \u00af\u03a6( \u221a 1 \u2212c2) where c = \u2225\u03bd[1:r(k)]\u22252.\n# Asymptotic Analysis, Phase Transitions, and Trade-offs\nIn this section, we perform a thorough analysis when the adversarial budget scales as d\u03b1 using our results in the diagonal regime. Here, we describe the main messages. (i) We show that our bounds asymptotically match in the diagonal regime and FilTrun is indeed optimal. (ii) Through the asymptotic analysis, we observe that in some scenarios, a sharp phase transition on the optimal robust error occurs as we increase \u03b1 := logd k (See Figure 3-(a)). We have already given examples of such scenarios (e.g. Example 1). In such cases, below the transition, i.e. when \u03b1 < \u03b10, the optimal robust error is\n(15)\n(16)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c224/c2248f5e-286e-4ac6-ac95-4979af6aac74.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Asymptotic behavior in the diagonal regime: Illustration of scenarios with (a) a ph transition, and (b) no phase transition</div>\nthe same as the optimal standard error. And when we are above the transition, i.e. when \u03b1 > \u03b10, any classifier becomes useless as the robust error becomes 1 2. As a result, asymptotically speaking, there exists no tradeoff between robustness and standard accuracy in scenarios where there is a sharp transition. However, there are other scenarios where instead of a sharp phase transition, in the asymptotic regime, the optimal robust error continuously increases as a function of adversary\u2019s budget (see Figure 3-(b)). In such scenarios, there exists a non-trivial tradeoff between robustness and standard accuracy. I.e. to achieve optimal robust error it is necessary to filter many informative coordinates which hurts the standard accuracy. See Example 6 below. In order to perform an asymptotic analysis, we assume that the dimension of the space, d, goes to infinity. More precisely, we assume that we have a sequence (\u00b5(d), \u03a3(d)) where for each d, \u00b5(d) \u2208Rd and \u03a3(d) is a diagonal covariance matrix with nonzero diagonal entries. We define\nAs usual, as in (3), in order to keep the optimal classification error in the absence of the adversary\nAs usual, as in (3), in order to keep the optimal classification error in the absence of the adversary fixed, we assume that\nFurthermore, without loss of generality, we assume that the coordinates in \u03bd are sorted in a descending order with respect to their magnitude, i.e.\nTo simplify the notation, we use L\u2217 d(.) as a shorthand for L\u2217 \u00b5(d),\u03a3(d)(.). We are mainly interested in studying the asymptotic behavior of L\u2217 d(kd) when kd is a sequence of adversarial budgets so that kd behaves like d\u03b1. Motivated by Theorem 4, it is natural to define\n\u03bb(d) c := min{\u03bb : \u2225\u03bd(d) [1:\u03bb]\u22252 \u2265c}\n\u03bb(d) c := min{\u03bb : \u2225\u03bd(d) [1:\u03bb]\u22252 \u2265c} for 0 < \nFurthermore, for 0 < c \u22641, we define\nNote that since c > 0, \u03bb(d) c \u22651 and \u2225\u03bd(d) [1:\u03bb(d) c ]\u22251 > 0. Therefore, \u03a8d(c) is well-defined. Furthermore, it is easy to verify the following properties for the function \u03a8d(.): Lemma 2. \u03a8d(.) is nonincreasing and \u03a8d(c) \u2208[\u22121/2, 1/2] for all c \u2208(0, 1].\n(17)\n(18)\n(19)\n(20)\nOn the other hand, note that for c > 0, we have \u03bb(d) c \u22651 and \u03a8d(c) \u2265logd |\u03bd(d) 1 | = logd \u2225\u03bd\u2225\u221e. Furthermore, we have 1 = \u2225\u03bd(d)\u22252 2 \u2264d\u2225\u03bd(d)\u2225\u221ewhich implies that \u2225\u03bd(d)\u2225\u221e\u22651/ \u221a d. Consequently, \u03a8d(c) \u2265logd 1/ \u221a d = \u22121/2. This completes the proof.\nOn the other hand, note that for c > 0, we have \u03bb(d) c \u22651 and \u03a8d(c) \u2265logd |\u03bd(d) 1 | = logd \u2225\u03bd\u2225\u221e. Furthermore, we have 1 = \u2225\u03bd(d)\u22252 2 \u2264d\u2225\u03bd(d)\u2225\u221ewhich implies that \u2225\u03bd(d)\u2225\u221e\u22651/ \u221a d. Consequently, \u03a8d(c) \u2265logd 1/ \u221a d = \u22121/2. This completes the proof. Roughly speaking, Theorem 4 implies that if kd behaves like d\u03a8d(c), then L\u2217(kd) \u2248\u00af\u03a6( \u221a 1 \u2212c2). In order to transform this into a formal asymptotic argument, we assume that for all c \u2208(0, 1], the sequence \u03a8d(c) is convergent, and we define \u03a8\u221e(c) := limd\u2192\u221e\u03a8d(c) as the limit. Since \u03a8d(.) is nondecreasing, if the pointwise limit \u03a8\u221e(.) exists, it is also nondecreasing and we may define\nRoughly speaking, Theorem 4 implies that if kd behaves like d\u03a8d(c), then L\u2217(kd) \u2248\u00af\u03a6( \u221a 1 \u2212c2). In order to transform this into a formal asymptotic argument, we assume that for all c \u2208(0, 1], the sequence \u03a8d(c) is convergent, and we define \u03a8\u221e(c) := limd\u2192\u221e\u03a8d(c) as the limit. Since \u03a8d(.) is nondecreasing, if the pointwise limit \u03a8\u221e(.) exists, it is also nondecreasing and we may define\nAdditionally, we can show the following lemma. Lemma 3. If \u03a8\u221e(.) exists as above, then \u03a8\u221e(c) \u2208[0, 1/2] for all c \u2208[0, 1]. Proof. For all c > 0 and all d, we have\nTherefore\n\u03a8\u221e(c) = lim d\u2192\u221e\u03a8d(c) = lim d\u2192\u221elogd \u2225\u03bd(d) [1:\u03bb(d) c ]\u22251 \u2265lim inf d\u2192\u221e2 logd c = 0.\nIt is sometimes more convenient to state the above theorem in terms of the pseudo inverse of the function \u03a8\u221e(.) defined as follows. For \u03b1 \u2208[0, 1], we define\n\ufffd Note that since \u03a8\u221e(c) \u22641/2 for all c \u2208[0, 1], we have \u03a8\u22121 \u221e(\u03b1) = 1 2 \u2200c > 1 2. With this, we can restate Theorem 5 as follows. Corollary 2. In the setup of Theorem 5, for \u03b1 \u2208[0, 1] we have 1. If lim sup logd kd < \u03b1 then lim sup L\u2217 d(kd) \u2264\u03a8\u22121 \u221e(\u03b1).\nWith this, we can restate Theorem 5 as follows. Corollary 2. In the setup of Theorem 5, for \u03b1 \u2208[0, 1] we have 1. If lim sup logd kd < \u03b1 then lim sup L\u2217 d(kd) \u2264\u03a8\u22121 \u221e(\u03b1).\n(21)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c4e6/c4e6e63b-42d7-4ba4-9050-cc7f81cda24c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: \u03a8\u221e(.) and \u03a8\u22121 \u221e(.) for Example 5. This observe a phase transition at \u221a d where below this threshold, adversary\u2019s effect can completely be neutralized, while above this threshold, the classifier can only achieve the trivial bound.</div>\nUsing (19), we have \u03bb(d) c = \u230adc2\u230band\nTherefore, sending d \u2192\u221e, we realize that\nMoreover, using (21), we get\nFigure 4 illustrates \u03a8\u221e(.) and \u03a8\u22121 \u221e(.) for this example. Therefore, employing Corollary 2, we realiz that\n1. If lim sup logd kd < 1/2 then lim sup L\u2217 d(kd) \u2264\u00af\u03a6(1) 2. If lim inf logd kd > 1/2 then L\u2217(kd) \u22651/2.\n2. If lim inf logd kd > 1/2 then L\u2217(kd) \u22651/2.\nIn other words, we observe a phase transition around \u221a d in the sense that if the adversary\u2019s budget is asymptoticallly below \u221a d, the classifier can achieve the robust classification error \u00af\u03a6(1), i.e. as if there is no adversary, while if the adversary\u2019s budget is asymptotically above \u221a d, no classifier can achieve a robust classification error better than that of a trivial classifier. This is consistent with the previous observations in this case, i.e. Examples 1 and 3.\nIt is interesting to observe that not always we have a phase transition as in the above example. Belo we discuss an example in which we have no phase transition, and the asymptotic robust classificatio error gradually increases as a function of the adversary\u2019s budget.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/09d7/09d7454b-c5b6-4747-829a-447c7422d455.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: \u03a8\u221e(.) and \u03a8\u22121 \u221e(.) for Examples 6. Unlike Example 5, we do not have a phase transitio here and the asymptotic optimal robust classification error continuously increases as a function of th adversarial \u21130 budget.</div>\n<div style=\"text-align: center;\">Figure 5: \u03a8\u221e(.) and \u03a8\u22121 \u221e(.) for Examples 6. Unlike Example 5, we do not have a phase transition here and the asymptotic optimal robust classification error continuously increases as a function of the</div>\nExample 6. Let \u03a3 = Id. Assume that d = 2n \u22121 for some integer n and define\n\ufffd \ufffd More precisely, we split the unit \u21132 norm of \u00b5(d) into n blocks, where the first block is the first coordinate, the second block is the second two coordinate, the ith block constitutes of 2i coordinates, and the final block is the last d/2 coordinates. Moreover, the power is uniformly distributed within each block. It is easy to see that for c = \ufffd m/n for 1 \u2264m \u2264n, we have \u03bb(d) c = 2m \u22121 and\nTherefore, \u03a8d(.) converges pointwise to \u03a8\u221e(.) such that \u03a8\u221e(c) = c2/2 for 0 \u2264c \u22641. Thereby, we\nTherefore, \u03a8d(.) converges pointwise to \u03a8\u221e(.) such that \u03a8\u221e(c) = c2/2 for 0 \u2264c \u22641. Thereby, w have \ufffd\n\u03a8\u22121 \u221e(\u03b1) = \ufffd\u00af\u03a6(1 \u22122\u03b1) 0 \u2264\u03b1 \u22641/2 1 2 1/2 < \u03b1 \u22641.\nFigure 5 illustrates \u03a8\u221e(.) and \u03a8\u22121 \u221e(.) in this examples. As we can see, unlike Example 5, we do not have a phase transition here. In fact, the asymptotic optimal robust classification error continuously increases as a function of adversarial \u21130 budget.\n# 4 Conclusion\nIn this paper, we studied the binary Gaussian mixture model under \u21130 attack. We developed a novel nonlinear classifier called FilTrun that first cleverly selects the robust coordinates of the input and then classifies based on a truncated inner product operation. Analyzing the performance of our proposed method, we derived an upper bound on optimal robust classification error. We further derived a lower bound on this, and showed the efficacy of FilTrun: when the covariance matrix of Gaussian mixtures is diagonal, FilTrun is asymptotically optimal. There are many directions to be pursued. Deriving a tighter lower bound and resolving the optimality gap for the case of non-diagonal covariance matrices remains open. Applying the key ideas of FilTrun, filtration and truncation, to a more complicated setting (e.g. neural networks) can be of great importance from a practical viewpoint. A crucial message of this paper is to emphasize the importance of nonlinear operations such as truncation for designing defense against \u21130 attacks. Finally,\nanalyzing robust classification error with \u21130 attacks for more complex stylized models such as multiclass Gaussian mixtures, two-layer neural networks, neural tangent kernel models, etc. is a promising future direction.\n# References\n[ACW18] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, ICML, Stockholm, Sweden, July 1015, pages 274\u2013283, 2018. [AMKP19] Abed AlRahman Al Makdah, Vaibhav Katewa, and Fabio Pasqualetti. A fundamental performance limitation for adversarial classification. IEEE Control Systems Letters, 4(1):169\u2013174, 2019. [BCM+13] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u02c7Srndi\u00b4c, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pages 387\u2013402. Springer, 2013. [BCM19] Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness from optimal transport. In Advances in Neural Information Processing Systems, 8-14 December 2019, Vancouver, BC, Canada, pages 7496\u20137508, 2019. [CAS+20] Francesco Croce, Maksym Andriushchenko, Naman D Singh, Nicolas Flammarion, and Matthias Hein. Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks. arXiv preprint arXiv:2006.12834, 2020. [CMZK20] Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization gap between adversarially robust and standard models. In International Conference on Machine Learning, pages 1670\u20131680. PMLR, 2020. [CW17] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, San Jose, CA, USA, May 22-26,, pages 39\u201357, 2017. [DHHR20] Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. Provable tradeoffs in adversarially robust classification. arXiv preprint arXiv:2006.05161, 2020. [DWR20] Chen Dan, Yuting Wei, and Pradeep Ravikumar. Sharp statistical guaratees for adversarially robust gaussian classification. In International Conference on Machine Learning, pages 2345\u20132355. PMLR, 2020. [GPM+16] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. Adversarial perturbations against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435, 2016. [GSS14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. [Hay20] Jamie Hayes. Provable trade-offs between private & robust machine learning. arXiv preprint arXiv:2006.04622, 2020.\n[JJZS19] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language attack on text classification and entailment. arXiv preprint arXiv:1907.11932, 2, 2019. [JSH20] Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. Precise tradeoffs in adversarial training for linear regression. In Conference on Learning Theory, pages 2034\u20132078. PMLR, 2020. [LF20] Alexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks by randomized ablation. In AAAI, pages 4585\u20134593, 2020. [LSK19] Juncheng Li, Frank Schmidt, and Zico Kolter. Adversarial camera stickers: A physical camera-based attack on deep learning systems. In International Conference on Machine Learning, pages 3896\u20133904. PMLR, 2019. [MCK20] Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. arXiv preprint arXiv:2002.11080, 2020. [MGMP] Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, and Ramtin Pedarsani. Sparsity-based defense against adversarial attacks on linear classifiers. In 2018 IEEE International Symposium on Information Theory, ISIT, Vail, CO, USA, June 17-22, 2018, pages 31\u201335. [MMDF19] Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Sparsefool: a few pixels make a big difference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9087\u20139096, 2019. [MMS+17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. [MMS+18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [PJ20] Muni Sreenivas Pydi and Varun Jog. Adversarial risk via optimal transport and optimal couplings. In International Conference on Machine Learning, pages 7814\u20137823. PMLR, 2020. [PMJ+16] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on security and privacy (EuroS&P), pages 372\u2013387. IEEE, 2016. [PMP20] Bhagyashree Puranik, Upamanyu Madhow, and Ramtin Pedarsani. Adversarially robust classification based on glrt. arXiv preprint arXiv:2011.07835, 2020. [PMW+16] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pages 582\u2013597. IEEE, 2016. [RW20] Eitan Richardson and Yair Weiss. A bayes-optimal view on adversarial examples. arXiv preprint arXiv:2002.08859, 2020.\n[RXY+19] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Adversarial training can hurt generalization. arXiv preprint arXiv:1906.06032, 2019. [SHS+18] Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial examples inevitable? arXiv preprint arXiv:1809.02104, 2018. [SRBB18] Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust neural network model on mnist. arXiv preprint arXiv:1805.09190, 2018. [SSRD19] Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. A simple explanation for the existence of adversarial examples with small hamming distance. arXiv preprint arXiv:1901.10861, 2019. [SST+18] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. In Advances in Neural Information Processing Systems, pages 5014\u20135026, 2018. [SZC+18] Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the cost of accuracy?\u2013a comprehensive study on the robustness of 18 deep image classification models. In Proceedings of the European Conference on Computer Vision (ECCV), pages 631\u2013648, 2018. [SZS+14] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014, Banff, AB, Canada, April 14-16, 2014. [TSE+19] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, number 2019, 2019. [WK] Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In Proceedings of the 35th International Conference on Machine Learning, ICML, Stockholm, Sweden, July 10-15, 2018. [ZYJ+19] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, pages 7472\u20137482. PMLR, 2019.\n# A Proof of Lemma 1\nIn this section, we prove Lemma 1. First we need to define some notations and discuss some lemmas. Given x = (x1, . . . , xd) \u2208Rd, we define the sample average of x as Mean(x) := \ufffdd i=1 xi/d. Moreover, we define truncated sum TSumk(x) for k < n/2 as follows. Let x(1) \u2264x(2) \u2264\u00b7 \u00b7 \u00b7 \u2264x(n) be the set of sorted values in x. We define\nwhich is the truncated sum of the elements in x after removing the top and bottom k values. Fo instance, TSum1(1, 1, 2, 3, 4, 5) = 1 + 2 + 3 + 4 = 10. Moreover, we define the truncated mean of x a follows: TSum\nwhich is the truncated sum of the elements in x after removing the top and bottom k values. For instance, TSum1(1, 1, 2, 3, 4, 5) = 1 + 2 + 3 + 4 = 10. Moreover, we define the truncated mean of x as\n \u2212 Note that when k = 0, the above quantities reduce to the sum and the sample average, respectivel It is straightforward to see that\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd Lemma 4. Assume that x = (x1, . . . , xd) \u2208Rd and x\u2032 = (x\u2032 1, . . . , x\u2032 d) \u2208Rd are given such that x\u2032 is identical to x in all but at most k < d/2 coordinates, i.e. \u2225x \u2212x\u2032\u22250 \u2264k. Moreover, assume that for some M < \u221e, we have |xi| \u2264M for all 1 \u2264i \u2264d. Then, if x\u2032 (1) \u2264x\u2032 (2) \u2264\u00b7 \u00b7 \u00b7 \u2264x\u2032 (d) are the sorted coordinates in x\u2032, we have\nEssentially, what Lemma 4 states is that if we modify at most k coordinates in a vector whose elements are bounded by M, in the resulting vector, after truncating the top and bottom k coordinates, all the surviving values are also bounded by M.\nEssentially, what Lemma 4 states is that if we modify at most k coordinates in a vector whose elements are bounded by M, in the resulting vector, after truncating the top and bottom k coordinates, all the surviving values are also bounded by M. Proof of Lemma 4. Let i1, . . . , il for l \u2264k be the coordinates where x\u2032 differs from x, i.e. xij \u0338= x\u2032 ij for 1 \u2264j \u2264l. Note that if |x\u2032 ij| > M for any of 1 \u2264j \u2264l, then x\u2032 ij will definitely fall into the top or bottom k coordinates in the sorted list x\u2032 (1) \u2264\u00b7 \u00b7 \u00b7 \u2264x\u2032 (d), since all the d \u2212l \u2265d \u2212k remaining coordinates in x\u2032 are bounded by M. This means that all the surviving coordinates x\u2032 (k+1), . . . , x\u2032 (d\u2212k) after truncating top and bottom k coordinates in x\u2032 are indeed bounded by M which completes the proof. Lemma 5. Assume that x = (x1, . . . , xd) \u2208Rd is given such that |xi| \u2264M for all 1 \u2264i \u2264d. Also, assume that x\u2032 = (x\u2032 1, . . . , x\u2032 d) \u2208Rd is identical to x in all but at most k coordinates, i.e. \u2225x\u2212x\u2032\u22250 \u2264k. Then, we have |TSum(x) \u2212TSum(x\u2032)| \u22646kM.\nLemma 5. Assume that x = (x1, . . . , xd) \u2208Rd is given such that |xi| \u2264M for all 1 \u2264i \u2264d. A assume that x\u2032 = (x\u2032 1, . . . , x\u2032 d) \u2208Rd is identical to x in all but at most k coordinates, i.e. \u2225x\u2212x\u2032\u22250 \u2264 Then, we have\nProof. Let x\u03c3(1) \u2264\u00b7 \u00b7 \u00b7 \u2264x\u03c3(d) and x\u2032 \u03c3\u2032(1) \u2264\u00b7 \u00b7 \u00b7 \u2264x\u2032 \u03c3\u2032(d) be the sorted elements in x and x\u2032 wit permutations \u03c3 and \u03c3\u2032, respectively. Following the definition, we have\nTSumk(x) = d\u2212k \ufffd i=k+1 x\u03c3(i) = \ufffd i:\u03c3\u22121(i)\u2208{k+1,...,d\u2212k} xi = d \ufffd i=1 1 \ufffd \u03c3\u22121(i) \u2208{k + 1, . . . , d \u2212k} \ufffd xi.\ngiven |xi| \u2264M \u22001 \u2264i \u2264n.\n(22)\n\ufffd To simplify the notation, for 1 \u2264i \u2264d, we define yi := 1 \ufffd \u03c3\u22121(i) \u2208{k + 1, . . . , d \u2212k} \ufffd xi and y\u2032 i := 1 \ufffd \u03c3\u2032\u22121(i) \u2208{k + 1, . . . , d \u2212k} \ufffd x\u2032 i. Moreover, let\n\ufffd A1 := {1 \u2264i \u2264d : \u03c3\u22121(i) \u2208{k + 1, . . . , d \u2212k} and \u03c3\u2032\u22121(i) /\u2208{k + 1, . . . , d \u2212k}} A2 := {1 \u2264i \u2264d : \u03c3\u22121(i) /\u2208{k + 1, . . . , d \u2212k} and \u03c3\u2032\u22121(i) \u2208{k + 1, . . . , d \u2212k}} A3 := {1 \u2264i \u2264d : \u03c3\u22121(i) \u2208{k + 1, . . . , d \u2212k} and \u03c3\u2032\u22121(i) \u2208{k + 1, . . . , d \u2212k} and xi \u0338= x\u2032 i} A := A1 \u222aA2 \u222aA3.\nNote that if i /\u2208A, either \u03c3\u22121(i) /\u2208{k + 1, . . . , d \u2212k} and \u03c3\u2032\u22121(i) /\u2208{k + 1, . . . , d \u2212k}, in which case yi = y\u2032 i = 0; or \u03c3\u22121(i) \u2208{k + 1, . . . , d \u2212k}, \u03c3\u2032\u22121(i) \u2208{k + 1, . . . , d \u2212k}, and xi = x\u2032 i, in which case yi = y\u2032 i = xi = x\u2032 i. This means that yi = y\u2032 i for i /\u2208A and\nNote that for i \u2208A1, we have y\u2032 i = 0 and yi = xi, implying |yi \u2212y\u2032 i| = |xi| \u2264M. On the other hand, for i \u2208A2, yi = 0 and y\u2032 i = x\u2032 i. But since \u03c3\u2032\u22121(i) \u2208{k + 1, . . . , d \u2212k}, using Lemma 4, we have |yi \u2212y\u2032 i| = |x\u2032 i| \u2264M. Moreover, for i \u2208A3, we have yi = xi and y\u2032 i = x\u2032 i. Also, from Lemma 4, we have |x\u2032 i| \u2264M. Thereby, |yi \u2212y\u2032 i| \u2264|xi| + |x\u2032 i| \u22642M. Putting all these together, we get\n\ufffd i\u2208A1 |yi \u2212y\u2032 i| + \ufffd i\u2208A2 |yi \u2212y\u2032 i| + \ufffd i\u2208A3 |yi \u2212y\u2032 i| \u2264M|A1| + M|A2| + 2M|A3|.\nSimilarly,\n|A2| \u22642k.\nOn the other hand,\n|A3| \u2264|{1 \u2264i \u2264d : xi \u0338= x\u2032 i}| \u2264k. Using (25), (26), and (27) back into (24) and comparing with (23), we realize that |TSumk(x) \u2212TSumk(x\u2032)| \u22646kM,\nThe following is a direct consequence of Lemma 5. Corollary 3. Given x, x\u2032 \u2208Rd and integer k satisfying \u2225x \u2212x\u2032\u22250 \u2264k < d/2, we have |TSumk(x) \u2212TSumk(x\u2032)| \u22646k min{\u2225x\u2225\u221e, \u2225x\u2032\u2225\u221e}. We are now ready to give the proof of Lemma 1:\n(23)\n(24)\n(25)\n(26)\n(27)\n\n|\u27e8w, x\u2032\u27e9k \u2212\u27e8w, x\u27e9| \u2264|\u27e8w, x\u2032\u27e9k \u2212\u27e8w, x\u27e9k| + |\u27e8w, x\u27e9k \u2212\u27e8w, x\u27e9| \u2264|\u27e8w, x\u2032\u27e9k \u2212\u27e8w, x\u27e9k| + 2k\u2225w \u2299x\u2225\u221e = |TSumk(w \u2299x\u2032) \u2212TSumk(w \u2299x)| + 2k\u2225w \u2299x\u2225\u221e (a) \u22646k\u2225w \u2299x\u2225\u221e+ 2k\u2225w \u2299x\u2225\u221e = 8k\u2225w \u2299x\u2225\u221e,\nwhere in step (a) we have used \u2225w \u2299x\u2032 \u2212w \u2299x\u22250 \u2264\u2225x\u2032 \u2212x\u22250 \u2264k together with Corollary 3. This completes the proof.\n# B Proof of the Upper Bound (Theorem 1)\nGiven x \u2208Rd and y \u2208{\u00b11}, define\n\u2113(k)(C(k) F ; x, y) := max x\u2032\u2208B0(x,k) \u2113(C(k) F ; x\u2032, y).\nWe have\n\u2113(k)(C(k) F ; x, 1) = 1 \ufffd \u2203x\u2032 \u2208B0(x, k) : C(k) F (x\u2032) \u0338= 1 \ufffd = 1 [\u2203x\u2032 \u2208B0(x, k) : \u27e8w(F), x\u2032 F \u27e9k \u22640]\nUsing Lemma 1, for x\u2032 such that \u2225x\u2032 \u2212x\u22250 \u22640, since \u2225x\u2032 F \u2212xF \u22250 \u2264\u2225x\u2032 \u2212x\u22250 \u2264k, we have |\u27e8w(F), x\u2032 F \u27e9k \u2212\u27e8w(F), xF \u27e9| \u22648k\u2225w(F) \u2299xF \u2225\u221e.\nThis means that\n# 1 [\u2203x\u2032 \u2208B0(x, k) : \u27e8w(F), x\u2032 F \u27e9k \u22640] \u2264 1 [\u27e8w(F), xF \u27e9\u22648k\u2225w(F) \u2299xF \u2225\u221e] ,\nand\nE(x,y)\u223cD \ufffd \u2113(k)(C(k) F ; x, 1)|y = 1 \ufffd \u2264P (\u27e8w(F), xF \u27e9\u22648k\u2225w(F) \u2299xF \u2225\u221e| y = 1) .\n\ufffd \ufffd Let \u03a3F be as defined in (6) and let \ufffd\u03a3F be the diagonal part of \u03a3F . Note that since \u03a3 is positi definite, \ufffd\u03a3F is diagonal with positive diagonal entries. Hence, we may write\n\ufffd\ufffd\ufffd \ufffd Let \u03c32 i denote the ith diagonal coordinate of \u03a3. Fix i \u2208F and note that conditioned on y = 1, we have xi \u223cN(\u00b5i, \u03c32 i ). On the other hand, with a := \ufffd\u03a3\u22121/2 F xF , we have ai \u223cN(\u03c3\u22121 i \u00b5i, 1). Note that \u00af\u03a6(\u03c3\u22121 i \u00b5i) is the optimal Bayes classification error of y given xi only, which is indeed not smaller than the optimal Bayes classification error of y given the whole vector x, which is in turn equal to \u00af\u03a6(\u2225\u03bd\u22252) = \u00af\u03a6(1). Since \u00af\u03a6 is decreasing, this implies \u03c3\u22121 i \u00b5i \u22641. Consequently, by union bound, we\n\ufffd\ufffd\ufffd \ufffd Let \u03c32 i denote the ith diagonal coordinate of \u03a3. Fix i \u2208F and note that conditioned on y = 1, we have xi \u223cN(\u00b5i, \u03c32 i ). On the other hand, with a := \ufffd\u03a3\u22121/2 F xF , we have ai \u223cN(\u03c3\u22121 i \u00b5i, 1). Note that \u00af\u03a6(\u03c3\u22121 i \u00b5i) is the optimal Bayes classification error of y given xi only, which is indeed not smaller than the optimal Bayes classification error of y given the whole vector x, which is in turn equal to \u00af\u03a6(\u2225\u03bd\u22252) = \u00af\u03a6(1). Since \u00af\u03a6 is decreasing, this implies \u03c3\u22121 i \u00b5i \u22641. Consequently, by union bound, we\n(28)\n(29)\nThereby, we get\nOn the other hand, we have\n\ufffd \ufffd \ufffd where \u2225\ufffd\u03a31/2 F \u03a3\u22121/2 F \u2225\u221edenotes the operator norm of \ufffd\u03a31/2 F \u03a3\u22121/2 F induced by the vector \u2113\u221enorm. Us ing (29), (30), and (31) back into (28) and simplifying, we get\n\ufffd \ufffd It is easy to see that conditioned on y = 1, \u27e8w(F), xF \u27e9\u223cN(\u2225\u03bd(F)\u22252 2, \u2225\u03bd(F)\u22252 2). Using this in th above bound, we get\nDue to the symmetry, we have the same bound conditioned on y = \u22121 which yields the desired result\n# C Proof of the Lower Bound in the Diagonal Regime (Theo rem 2)\nBefore giving the proof of Theorem 2, we need the following lemma. Lemma 6. For any random adversarial strategy with budget k which has a density function fx\u2032|x,y, we have\nBefore giving the proof of Theorem 2, we need the following lemma. Lemma 6. For any random adversarial strategy with budget k which has a density function fx\u2032|x,y, we have\n\ufffd \ufffd \ufffd\ufffd\ufffd Proof. Note that the right hand side is indeed the Bayes optimal error associated with the MAP estimator assuming that the classifier knows adversary\u2019s strategy. Since the classifier does not know the adversary\u2019s strategy in general, the right hand side is indeed a lower bound on the optimal robust classification error.\n(30)\n(31)\nProof of Theorem 2. Note that when A is empty, there is no adversarial modification and the standard Bayes analysis implies that L\u2217 \u00b5,\u03a3(0) = \u00af\u03a6(\u2225\u03bd\u22252) = \u00af\u03a6(\u2225\u03bdAc\u22252) and the desired bound holds. Hence, we may assume that A is nonempty for the rest of the proof. Note that due to (12), the randomized strategy Adv(A) is valid for the adversary given the budget \u2225\u03bd\u22251 log d. Thereby we may use Lemma 6 with Adv(A) to bound L\u2217 \u00b5,\u03a3(\u2225\u03bdA\u22251 log d) from below. Before that, we show that with high probability under the above randomized strategy for the adversary, recalling the definition of random variables Ii for i \u2208A from (9), we have \ufffd i\u2208A Ii \u2264\u2225\u03bdA\u22251 log d and hence x\u2032 = Z. It is easy to see that for each i, P (Ii = 1|y = 1) = P (Ii = 1|y = \u22121); therefore,\nHence, we have\nP (Ii = 1) = P (Ii = 1|y = 1) = P (Ii = 1|y = \u22121) \u2264 \ufffd\ufffd 2 \u03c0 |\u03bdi| \ufffd \u22271.\nTherefore, using Markov\u2019s inequality, if I is the indicator of the event \ufffd i\u2208A Ii > \u2225\u03bdA\u22251 log d, we have \ufffd\nTherefore, using Markov\u2019s inequality, if I is the indicator of the event \ufffd i\u2208A Ii > \u2225\u03bdA\u22251 log d, we h P (I = 1) = P (I = 1|y = 1) = P (I = 1|y = \u22121) \u2264 \ufffd 2/\u03c0 \ufffd i\u2208A |\u03bdi| \u2225\u03bdA\u22251 log d \u2264 1 log d. (\n \ufffd \u2208 \u2225\u2225 P (I = 1) = P (I = 1|y = 1) = P (I = 1|y = \u22121) \u2264 \ufffd 2/\u03c0 \ufffd i\u2208A |\u03bdi| \u2225\u03bdA\u22251 log d \u2264 1 log d. (32\nwhere (a) uses the symmetry, (b) uses the fact that when I = 0, by definition we have x\u2032 = Z, and (c) uses (10) and (32).\n(32)\n\ufffd \ufffd where (a) uses the fact that by definition, when I = 0, we have x\u2032 = Z, and (b) uses (32). Note that since Zi are conditionally independent given y, we have fZ|y(Z|y) = fZA|y(ZA|y)fZAc|y(ZAc|y).\n\ufffd \ufffd where (a) uses the fact that by definition, when I = 0, we have x\u2032 = Z, and (b) uses (32). Note that since Zi are conditionally independent given y, we have\nfZ|y(Z|y) = fZA|y(ZA|y)fZAc|y(ZAc|y).\nBut from (10), we have fZA|y(ZA|1) = fZA|y(ZA| \u22121) with probability one. Using this in (33), we get\nBut from (10), we have fZA|y(ZA|1) = fZA|y(ZA| \u22121) with probability one. Using this in (33), we get \ufffd \ufffd 1 1\n\ufffd \ufffd We may combine the two cases following the convention that when A = [d], Ac = \u2205and \u2225\u03bdAc\u22252 = 0 This completes the proof.\n# Proof of the General Lower Bound (Theorem 3)\nIn this section, we prove Theorem 3 by providing a general lower bound for the optimal robust classification error which relaxes the diagonal assumption for the covariance matrix. Our strategy is to approximate the covariance matrix by a diagonal matrix and use our lower bound of Theorem 2. It turns out that the optimal robust classification error is monotone with respect to the positive definite ordering of the covariance matrix. Lemma 7 below formalizes this. Intuitively speaking, the reason is that more noise makes the classification more difficult, resulting in an increase in the optimal robust classification error.\nLemma 7. Assume that \u00b5 \u2208Rd and \u03a31 and \u03a32 are two positive definite covariance matrices such that \u03a31 \u2aaf\u03a32. Then for 0 \u2264k \u2264d we have\nL\u2217 \u00b5,\u03a31(k) \u2264L\u2217 \u00b5,\u03a32(k).\nProof. Let y \u223cUnif(\u00b11), x1 \u223cN(y\u00b5, \u03a31) and x2 \u223cN(y\u00b5, \u03a32). Since \u03a31 \u2aaf\u03a32, we may write \u03a32 = \u03a31 + A such that A \u2ab00. In addition to this, we may couple x1, x2 on the same probability space as x2 = x1 + Z where Z \u223cN(0, A) is independent from all other variables. Now, fix a classifier C2 : Rd \u2192{\u00b11} and note that\nL\u00b5,\u03a32(C2, k) = P (\u2203x\u2032 \u2208B0(x2, k) : C2(x\u2032) \u0338= y) = P (\u2203x\u2032 \u2208B0(x1 + Z, k) : C2(x\u2032) \u0338= y) = P (\u2203x\u2032\u2032 \u2208B0(x1, k) : C2(x\u2032\u2032 + Z) \u0338= y) \u2265 inf \ufffd C2:Rd\u00d7Rd\u2192{\u00b11} P \ufffd \u2203x\u2032\u2032 \u2208B0(x1, k) : \ufffdC2(x\u2032\u2032, Z) \u0338= y \ufffd\n(33)\n(34)\nBut for z \u2208Rd, if we let \ufffdC2,z(x) := \ufffdC2(x, z), we get\n \ufffd But for z \u2208Rd, if we let \ufffdC2,z(x) := \ufffdC2(x, z), we get\n \ufffd \ufffd P \ufffd \u2203x\u2032\u2032 \u2208B0(x1, k) : \ufffdC2(x1, z) \u0338= y \ufffd = P \ufffd \u2203x\u2032\u2032 \u2208B0(x1, k) : \ufffdC2,z(x1) \u0338= y \ufffd \ufffd\nComparing this with (34) and (35), we realize that L\u00b5,\u03a32(C2, k) \u2265L\u2217 \u00b5,\u03a31(k). Since this holds for arbitrary C2, optimizing for C2 yields the desired result.\nNote that since \u03a3 is positive definite, we have \u03a3 \u2ab0\u03b1Id where \u03b1 > 0 is the minimum eigenvalue of \u03a3. Therefore, we may use Lemma 7 together with the lower bound of Theorem 2 for L\u2217 \u00b5,\u03b1Id(.) to obtain a lower bound for L\u2217 \u00b5,\u03a3(.). However, it turns out that it is more efficient in some scenarios to first normalize the diagonal entries of the covariance matrix. More precisely, define the d \u00d7 d matrix R where the i, j entry in R is Ri,j = \u03a3i,j/\ufffd\u03a3ii\u03a3jj. In other words, Ri,j is the correlation coefficient between the ith and the jth coordinates in our Gaussian noise. Equivalently, with \ufffd\u03a3 being the diagonal part of \u03a3, we may write\n \ufffd \ufffd It is evident that since \u03a3 is assumed to be positive definite, R is also positive definite. In fact, R is the covariance matrix of the normalized random vector x\u2032 such that x\u2032 i = xi/ \u221a \u03a3i,i where x \u223cN(y\u00b5, \u03a3) Also , all the diagonal entries in R are equal to 1, and when \u03a3 is diagonal, R = Id is the identity matrix. Furthermore, we define u = (u1, . . . , ud) where\n\ufffd In fact, with x\u2032 being the normalized of x as above, we have u = E [x\u2032|y = 1]. In Lemma 8, we show that such coordinate-wise normalization does not affect the optimal robust classiciation error. The main reason for this is that any coordinate-wise product of a vector by positive values does not change the \u21130 norm. This property is unique to the combinatorial \u21130 norm, and indeed does not hold for \u2113p norms for p \u22651. Lemma 8. Given a vector a \u2208Rd with strictly positive entries, if we define \u00b5\u2032 \u2208Rd and \u03a3\u2032 \u2208Rd\u00d7d as \u00b5\u2032 i = ai\u00b5i and \u03a3\u2032i,j = aiaj\u03a3i,j, then we have L\u2217 \u00b5,\u03a3(k) = L\u2217 \u00b5\u2032,\u03a3\u2032(k) \u22000 \u2264k \u2264d. In particular, with u and R defined above, we have L\u2217 \u00b5,\u03a3(k) = L\u2217 u,R(k) \u22000 \u2264k \u2264d.\n(35)\n(36)\n(37)\nLet x \u223cN(y\u00b5, \u03a3), i.e. (x, y) \u223cD, and define x\u2032 := a\u2299x. Note that x\u2032 \u223cN(y\u00b5\u2032, \u03a3\u2032). Let D\u2032 denote the joint distribution of (x\u2032, Y ). Recall that by definition L\u00b5,\u03a3(C, k) = E(x,y)\u223cD \ufffd maxx\u2032\u2208B0(x,k) \u2113(C; x\u2032, y) \ufffd . Note that x\u2032 \u2208B0(x, k) iff \u2225x\u2032 \u2212x\u22250 \u2264k. Since all the entries in a are nonzero, this is equivalent to \u2225a \u2299x\u2032 \u2212a \u2299x\u22250 \u2264k which is in turn equivalent to a \u2299x\u2032 \u2208B0(a \u2299x, k). Therefore, if a\u22121 denotes the elementwise inverse of a, we may write\nLet C\u2032 be the classifier defined that C\u2032(x) := C(a \u2299x). With this, we can rewrite the above as\nned that C\u2032(x) := C(a \u2299x). With this, we can rewrite the above as\nComparing this with (38) and sending to zero, we realize that L\u2217 \u00b5,\u03a3(k) \u2265L\u2217 \u00b5\u2032,\u03a3\u2032(k). Changing the order of (\u00b5, \u03a3) and (\u00b5\u2032, \u03a3\u2032) and replacing a with a\u22121 yields the other direction and completes the proof. Using the above tools, we are now ready to prove Theorem 3. Proof of Theorem 3. Note that since \u03a3 is positive definite, R is also positive definite and \u03b6min > 0. Moreover, we have R \u2ab0\u03b6minId. Therefore, using Lemmas 7 and 8 above, we realize that for all k, we have\nComparing this with (38) and sending to zero, we realize that L\u2217 \u00b5,\u03a3(k) \u2265L\u2217 \u00b5\u2032,\u03a3\u2032(k). Changing the order of (\u00b5, \u03a3",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of robust classification under \u21130-bounded adversarial perturbations, highlighting the limitations of existing methods and the need for a new approach due to the unique challenges posed by \u21130 attacks.",
        "problem": {
            "definition": "The problem is to classify data that has been perturbed by an adversary within an \u21130-ball of a specified budget k, focusing on the Gaussian mixture model for data distribution.",
            "key obstacle": "Existing methods fail to effectively handle \u21130 attacks due to their non-convex and non-smooth nature, leading to a complete failure of linear classifiers in this setting."
        },
        "idea": {
            "intuition": "The method is inspired by the need to filter out non-robust features from the input to improve classification accuracy in the presence of adversarial attacks.",
            "opinion": "The proposed idea introduces a novel classification algorithm called FilTrun, which utilizes filtration and truncation to enhance robustness against \u21130 attacks.",
            "innovation": "The key innovation lies in the combination of filtration and a truncated inner product, which differentiates this method from existing linear classifiers that are ineffective under \u21130 adversarial settings."
        },
        "method": {
            "method name": "FilTrun",
            "method abbreviation": "FT",
            "method definition": "FilTrun is a robust classification algorithm that filters out non-robust coordinates and applies a truncated inner product for classification.",
            "method description": "The method operates by first zeroing out non-robust features and then computing a truncated inner product with the remaining features.",
            "method steps": [
                "Input the adversary's budget k, parameters \u00b5 and \u03a3, and the set of surviving coordinates F.",
                "Filter the input to retain only the surviving coordinates.",
                "Compute the weight vector based on the surviving coordinates.",
                "Perform the truncated inner product with the filtered input.",
                "Output the classification result based on the sign of the truncated inner product."
            ],
            "principle": "The effectiveness of FilTrun stems from its ability to ignore outlier values that may be introduced by adversarial perturbations, thus maintaining robust classification."
        },
        "experiments": {
            "evaluation setting": "The experiments are conducted under the binary Gaussian mixture model, with various configurations of the covariance matrix and adversarial budgets to assess the performance of the FilTrun algorithm.",
            "evaluation method": "The performance is evaluated by comparing the robust classification error against upper and lower bounds derived analytically, using a combination of theoretical analysis and empirical testing."
        },
        "conclusion": "The outcomes demonstrate that FilTrun effectively achieves asymptotically optimal robust classification error when the covariance matrix is diagonal, and highlights the potential for further applications in more complex scenarios.",
        "discussion": {
            "advantage": "The primary advantage of FilTrun is its nonlinear approach to classification, which significantly improves robustness against \u21130 adversarial attacks compared to linear classifiers.",
            "limitation": "A limitation of the method is its reliance on the assumption of a diagonal covariance matrix, which may not hold in all practical scenarios.",
            "future work": "Future research directions include exploring tighter lower bounds for non-diagonal covariance matrices and applying the filtration and truncation techniques to more complex models such as neural networks."
        },
        "other info": {
            "additional details": {
                "contribution1": "Introduction of a novel robust classification algorithm called FilTrun.",
                "contribution2": "Derivation of both upper and lower bounds on the optimal robust classification error.",
                "contribution3": "Identification of phase transitions in the effectiveness of adversarial attacks based on the adversary's budget."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.4",
            "key information": "The method introduces a novel classification algorithm called FilTrun, which utilizes filtration and truncation to enhance robustness against \u21130 attacks."
        },
        {
            "section number": "3.4",
            "key information": "FilTrun is a robust classification algorithm that filters out non-robust coordinates and applies a truncated inner product for classification."
        },
        {
            "section number": "5.2",
            "key information": "The primary advantage of FilTrun is its nonlinear approach to classification, which significantly improves robustness against \u21130 adversarial attacks compared to linear classifiers."
        },
        {
            "section number": "7.1",
            "key information": "Existing methods fail to effectively handle \u21130 attacks due to their non-convex and non-smooth nature, leading to a complete failure of linear classifiers in this setting."
        },
        {
            "section number": "7.2",
            "key information": "Future research directions include exploring tighter lower bounds for non-diagonal covariance matrices and applying the filtration and truncation techniques to more complex models such as neural networks."
        }
    ],
    "similarity_score": 0.6756677604284526,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Robust Classification Under $_ell_0$ Attack for the Gaussian Mixture Model.json"
}