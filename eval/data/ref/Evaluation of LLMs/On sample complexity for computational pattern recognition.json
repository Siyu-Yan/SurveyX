{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:cs/0502074",
    "title": "On sample complexity for computational pattern recognition",
    "abstract": "In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if we restrict our attention to computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples.\n  The task of pattern recognition is considered in conjunction with another learning problem -- data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.",
    "bib_name": "ryabko2005samplecomplexitycomputationalpattern",
    "md_text": "# On computability of pattern recognition problems\nDaniil Ryabko\nIDSIA, Galleria 2, CH-6928 Manno-Lugano, Switzerland\u22c6\u22c6 daniil@ryabko.net\nAbstract. In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if we restrict our attention to computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples. The task of pattern recognition is considered in conjunction with another learning problem \u2014 data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.\n# 1 Introduction\nThe task of pattern recognition consists in predicting an unknown label of some observation (or object). For instance, the object can be an image of a handwritten letter, in which case the label is the actual letter represented by this image. Other examples include DNA sequence identification, recognition of an illness based on a set of symptoms, speech recognition, and many others. More formally, the objects are drawn independently from the object space X (usually X = [0, 1]d or Rd) according to some unknown but fixed probability distribution P on X, and labels are defined according to some function \u03b7 : X \u2192 Y , where Y is a finite set (often Y = {0, 1}). The task is to construct a function \u03d5 : {0, 1}\u2217\u2192Y which approximates \u03b7, i.e. for which P{x : \u03b7(x) \u0338= \u03d5(x)} is small, where P and \u03b7 are unknown but examples x1, y1, . . . , xn, yn are given; yi := \u03b7(xi). In the framework of statistical learning theory [7],[8] it is assumed that the function \u03b7 belongs to some known class of functions C. Good error estimated can be obtained if the class C is small enough. More formally, the number of examples required to obtain a certain level of accuracy (or the sample complexity of C) is linear in the VC-dimension of C. \u22c6\u22c6This work was supported by SNF grant 200020-107616\n\u22c6\u22c6This work was supported by SNF grant 200020-107616\nIn this work we investigate the question whether such bounds can be obtained if we consider only computable (on some Turing machine) pattern recognition methods. To make the problem more realistic, we also assume that the target function \u03b7 is also computable. Both the predictors and the target functions are of the form {0, 1}\u221e\u2192{0, 1}. We show that there are classes Ck of functions for which the number of examples needed to approximate the pattern recognition problem to a certain accuracy grows faster in the VC dimension of the class than any computable function (rather than being linear as in the statistical setting). In particular this holds if Ck is the class of all computable functions of length not greater than k. Importantly, the same negative result holds even if we allow the data to be generated \u201cactively\u201d, e.g. by some algorithm, rather than just by some fixed probability distribution. To obtain this negative result we consider the task of data compression: an impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition. We also analyze how tight is the negative result, and show that for some simple computable rule (based on the nearest neighbour estimate) the sample complexity is finite in k, under different definitions of computational patterning recognition task. In comparison to the vast literature on pattern recognition and related learning problems relatively little attention had been paid to the \u201ccomputable\u201d version of the task; at least this concerns the task of approximating any computable function. There is a track of research in which different concepts of computable learnability of functions on countable domains are studied, see [2]. A link between this framework and statistical learning theory is proposed in [5], where it is argued that for a uniform learnability finite VC dimension is required. Another approach is to consider pattern recognition methods as functions computable in polynomial time, or under other resource constraints. This approach leads to many interesting results, but it usually considers more specified settings of a learning problem, such as learning DNFs, finite automata, etc. See [3] for an introduction to this theory and for references.\n# 2 Preliminaries\nA (binary) string is a member of the set {0, 1}\u2217= \u222a\u221e i=0{0, 1}n. The length of a string x will be denoted by |x|, while xi is the ith element of x, 1 \u2264i \u2264|x|. For a set A the symbol |A| is used for the number of elements in A. We will assume the lexicographical order on the set of strings, and when necessary will identify {0, 1}\u2217and N via this ordering. Let N be the sets of natural numbers. The symbol log is used for log2. For a real number \u03b1 the symbol \u231c\u03b1\u231dis the least natural number not smaller than \u03b1. In pattern recognition a labelling function is usually a function from the interval [0, 1] or [0, 1]d (sometimes more general spaces are considered) to a finite space Y := {0, 1}. As we are interested in computable functions, we consider instead the functions of the form {0, 1}\u221e\u2192{0, 1}. Moreover, we call a partial\nrecursive function (or program) \u03b7 a labelling function if there exists such t =: t(\u03b7) \u2208N that \u03b7 accepts all strings from Xt := {0, 1}t and only such strings 1. For an introduction to the computability theory see for example [6]. It can be argued that this definition of a labelling function is too restrictive to approximate well the notion of a real function. However, as we are after negative results (for the class of all labelling functions), it is not a disadvantage. Other possible definitions are discussed in Section 4, where we are concerned with tightness of our negative results. All computable function can be encoded (in a canonical way) and thus the set of computable functions can be effectively enumerated. Define the length of \u03b7 as l(\u03b7) := |n| where n is the minimal number of \u03b7 in such enumeration. Define the task of computational pattern recognition as follows. An (unknown) labelling function \u03b7 is fixed. The objects x1, . . . , xn \u2208X are drawn according to some distribution P on Xt(\u03b7). The labels yi are defined according to \u03b7, that is yi := \u03b7(xi). A predictor is a family of functions (indexed by n)\ntaking values in Y , such that for any n and any t \u2208N, if xi \u2208Xt for each i, 1 \u2264i \u2264n, then the marginal \u03d5(x) is a total recursive function on Xt (that is, \u03d5n(x) accepts any x \u2208Xt). We will often identify \u03d5n with its marginal \u03d5n(x) when the values of other variables are clear. Thus, given a sample x1, y1, . . . , xn, yn of labelled objects of the same size t, a predictor produces a computable function; this function is supposed to approximate the labelling function \u03b7 on Xt. A computable predictor is a predictor which for any t \u2208N and any n \u2208N is a total recursive function on Xt \u00d7 Y \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xt \u00d7 Y \u00d7 Xt\n# 3 Main results\nWe are interested in what size sample is required to approximate a labelling function \u03b7. Moreover, for a (computable) predictor \u03d5, a labelling function \u03b7 and 0 < \u03b5 \u2208R define\n\u03b4n(\u03d5, \u03b7, \u03b5) := sup Pt Pt \ufffd x1, . . . , xn \u2208Xt :\nPt \ufffd x \u2208Xt : \u03d5n(x1, y1, . . . , xn, yn, x) \u0338= \u03b7(x) \ufffd > \u03b5 \ufffd\n\ufffd \ufffd where t = t(\u03b7) and Pt ranges over all distributions on Xt. For \u03b4 \u2208R, \u03b4 > 0 define the sample complexity of \u03b7 with respect to \u03d5 as\nN(\u03d5, \u03b7, \u03b4, \u03b5) := min{n \u2208N : \u03b4n(\u03d5, \u03b7, \u03b5) \u2264\u03b4}.\n{ \u2208 \u2264}  It is not essential for this definition that \u03b7 is not a total function. An equivalent (for our purposes) definition would be as follows. A labelling function is any total function which outputs the string 00 on all inputs except on the strings of some length t =: t(\u03b7), on each of which it outputs either 0 or 1.\nThe number N(\u03d5, \u03b7, \u03b4, \u03b5) is the minimal sample size required for a predictor \u03d5 to achieve \u03b5-accuracy with probability 1\u2212\u03b4 when the (unknown) labelling function is \u03b7. We can use statistical learning theory [7] to derive the following statement Proposition 1. There exists a predictor \u03d5 such that\n# N(\u03d5, \u03b7, \u03b4, \u03b5) \u2264max \ufffd l(\u03b7)8 \u03b5 log 13 \u03b5 , 4 \u03b5 log 2 \u03b4 \ufffd\nN(\u03d5, \u03b7, \u03b4, \u03b5) \u2264max \ufffd l(\u03b7)8 \u03b5 log 13 \u03b5 , 4 \u03b5 log 2 \u03b4 \ufffd\nfor any labelling function \u03b7 and any \u03b5, \u03b4 > 0.\nObserve that the bound is linear in the length of \u03b7. In what follows the proof of this simple statement, we investigate the question of whether any such bounds exist if we restrict our attention to computable predictors.\nProof. The predictor \u03d5 is defined as follows. For each sample x1, y1, . . . , xn, yn it finds a shortest program \u00af\u03b7 such that \u00af\u03b7(xi) = yi for all i \u2264n. Clearly, l(\u00af\u03b7) \u2264l(\u03b7). Observe that the VC-dimension of the class of all functions of length not greater than l(\u03b7) is bounded from above by l(\u03b7), as there are not more than 2l(\u03b7) such functions. Moreover, \u03d5 minimises empirical risk over this class of functions. It remains to use the following bound (see e.g. [1], Corollary 12.4)\n# sup \u03b7\u2208C N(\u03d5, \u03b7, \u03b4, \u03b5) \u2264max \ufffd V (C)8 \u03b5 log 13 \u03b5 , 4 \u03b5 log 2 \u03b4 \ufffd\n# where V (C) is the VC-dimension of the class C.\nwhere V (C) is the VC-dimension of the class C.\nThe main result of this work is that for any computable predictor \u03d5 there is no computable upper bound in terms of l(\u03b7) on the sample complexity of the function \u03b7 with respect to \u03d5: Theorem 1. For any computable predictor \u03d5 and any total recursive function \u03b2 : N \u2192N there exist a labelling function \u03b7, and some n > \u03b2(l(\u03b7)) such that P{x \u2208Xt(\u03b7) : \u03d5(x1, y1, . . . , xn, yn, x) \u0338= \u03b7(x)} > 0.05, for any x1, . . . , xn \u2208Xt(\u03b7), where yi = \u03b7(xi) and P is the uniform distribution on Xt(\u03b7). For example, we can take \u03b2(n) = 2n, or 22n. Corollary 1. For any computable predictor \u03d5, any total recursive function \u03b2 : N \u2192N and any \u03b4 < 1 sup \u03b7:l(\u03b7)\u2264k N(\u03d5, \u03b7, \u03b4, 0.05) > \u03b2(k)\nThe main result of this work is that for any computable predictor \u03d5 there is no computable upper bound in terms of l(\u03b7) on the sample complexity of the function \u03b7 with respect to \u03d5:\nsup \u03b7:l(\u03b7)\u2264k N(\u03d5, \u03b7, \u03b4, 0.05) > \u03b2(k)\nfrom some k on.\nObserve that there is no \u03b4 in the formulation of Theorem 1. Moreover, it is not important how the objects (x1, . . . , xn) are generated \u2014 it can be any individual sample. In fact, we can assume that the sample is chosen in any manner, for example by some algorithm. This means that no computable upper bound on sample complexity exists even for active learning algorithms. It appears that the task of pattern recognition is closely related to another learning task \u2014 data compression. Moreover, to prove Theorem 1 we need a similar negative result for this task. Thus before proceeding with the proof of the theorem, we introduce the task of data compression and derive some negative results for it. We call a total recursive function \u03c8 : {0, 1}\u2217\u2192{0, 1}\u2217an data compressor if it is an injection (i.e. x1 \u0338= x2 implies \u03c8(x1) \u0338= \u03c8(x2)). We say that an data compressor compresses the string x if |\u03c8(x)| < |x|. Clearly, for any natural n any data compressor compresses not more than a half of strings of size not greater than n. We will now present a definition of Kolmogorov complexity; for fine details see [4], [9]. The complexity of a string x \u2208{0, 1}\u2217with respect to a machine \u03b6 is defined as\n  where p ranges over all partial functions (minimum over empty set is defined as \u221e). There exists such a machine \u03b6 that C\u03b6(x) \u2264C\u03b6\u2032(x) + c\u03b6\u2032 for any x and any machine \u03b6\u2032 (the constant c\u03b6\u2032 depends on \u03b6\u2032 but not on x). Fix any such \u03b6 and define the Kolmogorov complexity of a string x \u2208{0, 1}\u2217as\nClearly, C(x) \u2264|x| + b for any x and for some b depending only on \u03b6. A string is called c-incompressible if C(x) \u2265|x| \u2212c. Obviously, any data compressor can not compresses many c-incompressible strings, for any c. However, highly compressible strings (that is, strings with Kolmogorov complexity low relatively to their length) might be expected to be compressed well by some sensible data compressor. The following lemma shows that it can not be always the case, no matter what we mean by \u201crelatively low\u201d. The proof of this lemma is followed by the proof of Theorem 1.\nLemma 1. For any data compressor \u03c8 and any total recursive function \u03b3 : N \u2192 N such that \u03b3 goes monotonically to infinity there exists a binary string x such that C(x) \u2264\u03b3(|x|) and |\u03c8(x)| \u2265|x|.\nProof. Suppose the contrary, i.e. that there exist an data compressor \u03c8 and some function \u03b3 : N \u2192N monotonically increasing to infinity such that for any string x if C(x) \u2264\u03b3(|x|) then \u03c8(x) < |x|. Let T be the set of all strings which are not compressed by \u03c8\nDefine the function \u03c4 on the set T as follows: \u03c4(x) is the number of the element x in T \u03c4(x) := #{x\u2032 \u2208T : x\u2032 \u2264x}\nDefine the function \u03c4 on the set T as follows: \u03c4(x) is the number of th element x in T \u2032 \u2032 \nfor each x \u2208T . Obviously, the set T is infinite. Moreover, \u03c4(x) \u2264x for any x \u2208T (recall that we identify {0, 1}\u2217and N via lexicographical ordering). Observe that \u03c4 is a total recursive function on T and onto N. Thus \u03c4 \u22121 : N \u2192{0, 1}\u2217is a total recursive function on N. Thus, for any x \u2208T ,\nC(\u03c4(x)) \u2265C(\u03c4 \u22121(\u03c4(x)) \u2212c = C(x) \u2212c > \u03b3(|x|) \u2212c,\nfor constant c depending only on \u03c4, where the first inequality follows from computability of \u03c4 \u22121 and the last from the definition of T . It is a well-known result (see e.g. [4], Theorem 2.3.1) that for any partial function \u03b4 that goes monotonically to infinity there is x \u2208{0, 1}\u2217such that C(x) \u2264\u03b4(|x|). In particular, allowing \u03b4(|x|) = \u03b3(|x|) \u22122c, we conclude that there exist such x \u2208T that\nwhich contradicts (1).\n# which contradicts (1).\nProof of Theorem 1. Suppose the contrary, that is that there exists such a computable predictor \u03d5 and a total function \u03b2 : N \u2192N such that for any labelling function \u03b7, and any n > \u03b2(l(\u03b7)) we have\nP{x : \u03d5(x1, y1, . . . , xn, yn, x) \u0338= \u03b7(x)} \u22640.05,\nfor some xi \u2208Xt(\u03b7), yi = \u03b7(xi), i \u2208N, where P is the uniform distribution on Xt(\u03b7). Not restricting generality we can assume that \u03b2 is strictly increasing. Define the (total) function \u03b2\u22121(n) := max{m \u2208N : \u03b2(m) \u2264n}. Define \u03b5 := 0.05. Construct the data compressor \u03c8 as follows. For each y \u2208{0, 1}\u2217define m := |y|, t := \u231clog m\u231d. Generate (lexicographically) first m strings of length t and denote them by xi, 1 \u2264i \u2264m. Define the labelling function \u03b7y as follows: t(\u03b7y) = t and \u03b7y(xi) = yi, 1 \u2264i \u2264m. Clearly, C(\u03b7y) \u2264C(y) + c, where c is some universal constant capturing the above description. Let n := \u221am. Next we run the predictor \u03d5 on all possible tuples x = (x1, . . . , xn) \u2208X n t and each time count errors that \u03d5 makes on all elements of Xt:\nE(x) := {x \u2208Xt : \u03d5(x1, y1, . . . , xn, yn, x) \u0338= \u03b7y(x)}.\nIf |E(x)| > \u03b5m for each x \u2208Xt then \u03c8(y) := 0y. Otherwise proceed as follows. Fix some tuple x = (x\u2032 1, . . . , x\u2032 n) such that |E(x)| \u2264\u03b5m, and let H := {x\u2032 1, . . . , x\u2032 n} be the unordered tuple x. Define\n\u03bai := \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 e0 xi \u2208E(x)\\H, yi = 0 e1 xi \u2208E(x)\\H, yi = 1 c0 xi \u2208H, yi = 0 c1 xi \u2208H, yi = 1 \u2217 otherwise\n(1)\nfor 1 \u2264i \u2264m. Thus, each \u03bai is a member of a five-letter alphabet (a five-element set) {e0, e1, c0, c1, \u2217}. Denote the string \u03ba1 . . . \u03bam by K. Observe that the string K, the predictor \u03d5 and the order of (x\u2032 1, . . . , x\u2032 n) (which is not contained in K) are sufficient to restore the string y. Furthermore, the n-tuple (x\u2032 1, . . . , x\u2032 n) can be obtained from H (the un-ordered tuple) by the appropriate permutation; let r be the number of this permutation in some fixed ordering of all n! such permutations. Using Stirling\u2019s formula, we have |r| \u2264 2n log n; moreover, to encode r with some self-delimiting code we need not more than 4n log n symbols (for n > 3). Denote such encoding of r by \u03c1. Next, as there are (1 \u2212\u03b5 \u2212 1 \u221am)m symbols \u2217in the m-element string K, it can be encoded by some simple binary code \u03c3 in such a way that\nIndeed, construct \u03c3 as follows. First replace all occurrences of the string \u2217\u2217with 0. Encode the rest of the symbols with any fixed 4-bit encoding such that the code of each letter starts with 1. Clearly, \u03c3(K) is uniquely decodable. Moreover, it is easy to check that (2) is satisfied, as there are not less than 1 2(m\u22122(\u03b5m+n)) occurrences of the string \u2217\u2217. We also need to write m in a self-delimiting way (denote it by s); clearly, |s| \u22642 log m. Finally, \u03c8(\u00afy) = 1\u03c1s\u03c3(K) and |\u03c8(y)| \u2264|\u00afy|, for m > 210. Thus, \u03c8 compresses any \u00afy such that n > \u03b2(C(\u03b7y)); i.e. such that \u221am > \u03b2(C(\u03b7y)) \u2265\u03b2(C(y) + c). This contradicts Lemma 1 with \u03b3(k) := \u03b2\u22121( \u221a k \u2212c). \u2293\u2294\n# 4 On tightness of the negative results\nIn this section we discuss how tight are the conditions of the statements and to what extend they depend on the definitions. Let us consider a question whether there exist any (not necessarily computable) sample-complexity function\nat least for some predictor \u03d5, or it is always infinity from some k on. Proposition 2. There exist a predictor \u03d5 such that N\u03d5(k, \u03b4, \u03b5) < \u221efor any \u03b5, \u03b4 > 0 and any k \u2208N. Proof. Clearly, C(\u03b7) \u2265C(t\u03b7). Moreover, lim inft\u2192\u221eC(t) = \u221eso that max{t\u03b7 : l(\u03b7) \u2264k} < \u221e for any k. It follows that the \u201cpointwise\u201d predictor \u03d5(x1, y1, . . . , xn, yn, x) = \ufffdyi if x = xi, 1 \u2264i \u2264n 0 x /\u2208{x1, . . . , xn} (3) satisfies the conditions of the proposition.\n(2)\n(3)\nIt can be argued that probably this statement is due to our definition of a labelling function. Next we will discuss some other variants of this definition. First, observe that if we define a labelling function as any total function on {0, 1}\u2217then some labelling functions will not approximate any real function; for example such is the function \u03b7+ which counts bitwise sum of its input: \u03b7+(x) := \ufffd|x| i=1 xi mod 2. That is why we require a labelling function to be defined only on Xt for some t. Another way to define a labelling function (which perhaps makes labelling functions most close to real functions) is as a function which accepts any infinite binary string. Let us call an i-labelling function any total recursive function \u03b7 : {0, 1}\u221e\u2192{0, 1}. That is, \u03b7 is computable on a Turing machine with an input tape on which one way infinite input is written, an output tape and possibly some working tapes. The program \u03b7 is required to halt on any input. The next proposition shows that even if we consider such definition the situation does not change. The definition of a labelling function \u03b7 in which it accepts only finite strings is chosen in order to stay within conventional computability theory. Lemma 2. For any i-labelling function \u03b7 there exist n\u03b7 \u2208N such that \u03b7 does not scan its input tape further position n\u03b7. In particular, \u03b7(x) = \u03b7(x\u2032) as soon as xi = x\u2032 i for any i \u2264n\u03b7. Proof. For any x \u2208{0, 1}\u2217the program \u03b7 does not scan its tape further some position n(x) (otherwise \u03b7 does not halt on x). For any \u03c7 \u2208{0, 1}\u221edenote by n\u03b7(\u03c7) the maximal n \u2208N such that \u03b7 scans the input tape up to the position n on the input \u03c7. Suppose that sup\u03c7\u2208{0,1}\u221en\u03b7(\u03c7) = \u221e, i.e. that the proposition is false. Define x0 to be the empty string. Furthermore, let\nBy our assumption, xi is defined for each i \u2208N. Moreover, it easy to check that \u03b7 never stops on the input string x1x2 . . . . Besides, it is easy to check that the number n\u03b7 is computable. Finally, it can be easily verified that Proposition 2 holds true if we consider i-labelling functions instead of labelling functions, constructing the required predictor based on the nearest neighbour predictor. Proposition 3. There exist a predictor \u03d5 such that iN\u03d5(k, \u03b4, \u03b5) < \u221efor any \u03b5, \u03b4 > 0 and any k \u2208N, where iN is defined as N with labelling functions replaced by i-labelling functions. Proof. Indeed, it suffices to replace the \u201cpointwise\u201d predictor in the proof of Proposition 2 by the following predictor \u03d5, which assigns to the object x the label of that object among x1, . . . , xn with whom x has longest mutual prefix: \u03d5(x1, y1, . . . , xn, yn, x) := yk, where k := argmax1\u2264m\u2264n{max{i \u2208N : x1 . . . xi = x1 m . . . xi m}};\nto avoid infinite run in case of ties, \u03d5 considers only first (say) n digits of xi an break ties in favour of the lowest index.\n# References\n1. L. Devroye, L. Gy\u00a8orfi, G. Lugosi, A probabilistic theory of pattern recognition. New York: Springer, 1996. 2. S. Jain, D. Osherson, J. Royer, and A. Sharma. Systems That Learn: An Introduction to Learning Theory, 2nd edition. The MIT Press, Cambridge, MA, 1999. 3. M. Kearns and U. Vazirani An Introduction to Computational Learning Theory The MIT Press, Cambridge, Massachusetts, 1994. 4. M. Li, P. Vit\u00b4anyi. An introduction to Kolmogorov complexity and its applications. Second edition, Springer, 1997. 5. W. Menzel, F. Stephan. Inductive versus approximative learning. In: Perspectives of Adaptivity and learning, edited by R. Kuehn et al., pp. 187\u2013209, Springer, 2003. 6. H. Rogers. Theory of recursive functions and effective computability, McGraw-Hill Book Company, 1967. 7. V. Vapnik, and A. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974 8. V. Vapnik, Statistical Learning Theory: New York etc.: John Wiley & Sons, Inc. 1998 9. A.K. Zvonkin and L.A. Levin. The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms. Russian Math. Surveys, 25(6), pp 83\u2013124, 1970.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of computability in pattern recognition, specifically investigating the relationship between the number of examples required to approximate an unknown labeling function and the constraints of computable methods.",
        "problem": {
            "definition": "The problem is to determine the sample complexity required for computable pattern recognition methods to approximate an unknown labeling function that is also computable.",
            "key obstacle": "The main challenge is that the number of examples needed grows faster than any computable function, contrary to the linear bounds established in statistical learning theory."
        },
        "idea": {
            "intuition": "The idea was inspired by the limitations observed in statistical settings of pattern recognition when restricted to computable functions.",
            "opinion": "The paper posits that computable methods for pattern recognition face inherent limitations that are not present in non-computable settings.",
            "innovation": "The primary improvement is the identification of the non-linear growth of sample complexity in computable methods, contrasting with established linear relationships in statistical learning."
        },
        "Theory": {
            "perspective": "The theoretical perspective considers the implications of computability on pattern recognition and data compression, linking these two learning tasks.",
            "opinion": "It is assumed that while some functions can be approximated efficiently in non-computable settings, computable functions face significant barriers.",
            "proof": "The proof is derived from an impossibility result in data compression, which is used to estimate sample complexity for pattern recognition."
        },
        "experiments": {
            "evaluation setting": "The evaluation involves classes of computable functions and their sample complexity under various distributions, particularly focusing on classes of functions of bounded length.",
            "evaluation method": "The evaluation method includes analyzing sample sizes required for predictors to achieve a certain level of accuracy in approximating labeling functions."
        },
        "conclusion": "The conclusion drawn is that there is no computable upper bound on the sample complexity of computable predictors, indicating fundamental limitations in computable pattern recognition.",
        "discussion": {
            "advantage": "The advantage of this paper lies in its rigorous exploration of computability in pattern recognition, providing new insights into the limitations of computable methods.",
            "limitation": "A limitation is that the results are primarily negative, indicating what cannot be achieved, rather than providing constructive methods for improvement.",
            "future work": "Future work could explore alternative definitions of computability and their implications for sample complexity in pattern recognition, as well as potential methods to overcome identified limitations."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "5",
            "key information": "The main challenge is that the number of examples needed grows faster than any computable function, contrary to the linear bounds established in statistical learning theory."
        },
        {
            "section number": "5.1",
            "key information": "The paper posits that computable methods for pattern recognition face inherent limitations that are not present in non-computable settings."
        },
        {
            "section number": "5.3",
            "key information": "The conclusion drawn is that there is no computable upper bound on the sample complexity of computable predictors, indicating fundamental limitations in computable pattern recognition."
        },
        {
            "section number": "6",
            "key information": "Future work could explore alternative definitions of computability and their implications for sample complexity in pattern recognition, as well as potential methods to overcome identified limitations."
        }
    ],
    "similarity_score": 0.5603336709618346,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0214_large/papers/On sample complexity for computational pattern recognition.json"
}