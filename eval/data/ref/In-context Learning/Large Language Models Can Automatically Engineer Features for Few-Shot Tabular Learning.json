{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.09491",
    "title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning",
    "abstract": "Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.",
    "bib_name": "han2024largelanguagemodelsautomatically",
    "md_text": "# Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning\nSungwon Han 1 Jinsung Yoon 2 Sercan \u00a8O. Arik 2 Tomas Pfister 2\n# Abstract\nLarge Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.\narXiv:2404.09491v2\n# 1. Introduction\nIn recent years, Large Language Models (LLMs) have shown impressive abilities for generating suitable responses for previously unseen tasks without requiring task-specific fine-tuning (Creswell et al., 2022; Imani et al., 2023; Taylor et al., 2022). Trained on extensive text corpora, the rich prior knowledge of LLMs can substitute the need for expert domain knowledge in many areas (Gen, 2021; Singhal et al.,\n1Work done at Google as a research intern. School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea 2Google Cloud AI, Sunnyvale, California, USA. Correspondence to: Jinsung Yoon <jinsungyoon@google.com>.\n1Work done at Google as a research intern. School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea 2Google Cloud AI, Sunnyvale, California, USA. Correspondence to: Jinsung Yoon <jinsungyoon@google.com>.\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\n2023; Wilcox & Hripcsak, 2003), playing a crucial role in the automation of diverse applications, including dialogue analysis (Finch et al., 2023) and program repair (Fan et al., 2023). Particularly, by incorporating a task description and few examples into prompts, LLMs can understand the context of a task and \u201cattend\u201d to important features and examples (Brown et al., 2020; Zhao et al., 2023). This demonstrates the capability of LLMs to analyze data, highlighting their potential to perform as an expert feature engineer.\n2023; Wilcox & Hripcsak, 2003), playing a crucial role in the automation of diverse applications, including dialogue analysis (Finch et al., 2023) and program repair (Fan et al., 2023). Particularly, by incorporating a task description and few examples into prompts, LLMs can understand the context of a task and \u201cattend\u201d to important features and examples (Brown et al., 2020; Zhao et al., 2023). This demonstrates the capability of LLMs to analyze data, highlighting their potential to perform as an expert feature engineer. Utilization of LLMs\u2019 prior knowledge and reasoning capabilities have been extended to the domain of tabular learning. For instance, knowledge such as related to the increased risk of certain diseases in older individuals can be beneficial at disease prediction tasks. In this context, recent works define task and feature descriptions in natural language, serialize the data, and then feed it into an LLM (Dinh et al., 2022; Wang et al., 2023). This is often followed by either using incontext learning (Nam et al., 2023a) or parameter-efficient fine-tuning (Dinh et al., 2022; Hegselmann et al., 2023). Prior knowledge provided by LLMs has proven effective, outperforming conventional tabular learning baselines in low-shot regimes (Hegselmann et al., 2023). Current LLM-based tabular learning methods have some limitations. For end-to-end predictions, at least one LLM inference per sample is required, making it computationally expensive. Most methods require fine-tuning the LLM for high accuracy, making them infeasible to the application to the LLMs that do not come with accessible training codebases or tuning APIs, a prominent concern given that recently-proposed top-performance LLMs only permit limited access via inference APIs (Anil et al., 2023; OpenAI, 2023). In addition, most approaches are not suitable with lengthy prompts (Liu et al., 2023) \u2013 when the number of features in tabular data grows to the point where the serialized text length becomes long, the approach often turns infeasible or experiences a decline in performance. These constraints pose fundamental challenges to the real-world application of LLM-based tabular learning models. Our proposed approach goes beyond treating LLMs to yield end-to-end predictions. Instead, we assign them the role of feature engineering, with them proposing a more efficient way to utilize their prior knowledge. Specifically, we\nintroduce a novel in-context learning framework, FeatLLM, which aims to understand the \u201ccriteria\u201d underlying LLM predictions. For instance, for the task of predicting a particular disease, the LLM can directly infer and generate rules that determine which feature conditions result in identifying the disease. The criteria (or rules) are used to create new features that can replace existing ones in solving tasks. This approach shifts the capability of the LLM to feature generation, yielding a simpler downstream model \u2013 once the features are discovered, a complex machine learning model is no longer needed for inference, improving the inference latency. By leveraging the in-context learning ability of LLMs, it\u2019s possible to extract features without the need for model training, by adding example demonstrations to the prompts. Furthermore, by employing techniques from ensemble methods (Breiman, 2001), like feature bagging, we can mitigate the challenge of excessive prompt size. FeatLLM structures the input prompt for engineering new features into the following two sub-tasks: (1) understanding the problem and inferring the relationship between features and targets; and (2) based on the knowledge from the previous step and few-shot examples, deriving decisive rules to distinguish classes. This structured reasoning approach leads the LLMs to filter out uninformative features for rule extraction. The deduced rules are then applied to the data (parsed them as program code), and the data are transformed into a binary feature indicating whether or not the samples adheres to each rule. Finally, a model with low complexity is fitted to estimate class probabilities from these new features. The robustness is further improved by employing an ensemble approach, repeatedly generating features in conjunction with feature bagging. By appropriately adjusting the number of features and samples chosen in bagging, the challenge of excessive prompt size is mitigated. FeatLLM is free from training constraints and yields low inference cost, rendering the application of the LLM feasible for various real-world problems. We evaluate FeatLLM on 13 different tabular datasets in low-shot regimes, showing its strong and robust performance. We show that LLMs proficiently utilize both prior knowledge and the knowledge from the data, extracting high-quality features. Our framework outperforms contemporary few-shot learning baselines across various settings. The code is released via anonymized GitHub link at https: //github.com/Sungwon-Han/FeatLLM.\n# 2. Related Work\n# 2.1. Few-Shot Learning with Tabular Data\nThe advancement of few-shot learning has enabled the extraction of generalizable representations from data, even with minimal labeling costs (Chen et al., 2018). Initially\nits strong generalizability across various data modalities including text, audio, and more recently, tabular data (Majumder et al., 2022; Nam et al., 2023b; Schick & Sch\u00a8utze, 2021). Learning from a small number of labeled samples presents the risk of learning spurious correlations (Bartlett et al., 2021). Consequently, recent efforts have employed additional sources of information or introduced domainspecific prior knowledge to impart appropriate inductive biases for model training. For example, utilizing large quantities of more readily-available unlabeled datasets with judicious augmentation for semi-supervised learning (Ucar et al., 2021; Yoon et al., 2020), or seeking good model initialization through task-agnostic unsupervised pre-training (Somepalli et al., 2021). Such unsupervised objectives can involve masking or corrupting part of the data and then using a reconstruction objective (Arik & Pfister, 2021; Majmundar et al., 2022) or employing data augmentation to apply contrastive learning objectives (Bahri et al., 2022; Chen et al., 2020). However, the heterogeneous nature of tabular data makes it challenging to find universally applicable augmentations, and these strategies have not been notably effective in few-shot settings (Nam et al., 2023b). Recent work have proposed training models on a broad range of tabular datasets beyond those related to the target task and then applying transfer learning to the target task (Zhang et al., 2023; Levin et al., 2022; Zhu et al., 2023). For instance, TransTab (Wang & Sun, 2022) trains transformer-based models learning of semantic relationships between columns through column names, not just table values. The knowledge extracted from various tables and columns has facilitated zero-shot or few-shot inference for new tabular tasks. Meanwhile, TabPFN (Hollmann et al., 2023) mixes different types of distributions to create synthetic data for pre-training a Bayesian neural network. After pre-training, inference is performed by inputting both training and test samples of the target task without further training. The prior knowledge gained from various datasets helped surpass the performance of traditional tree-based approaches and is proved to be beneficial in few-shot settings.\n# 2.2. Language-Interfaced Tabular Learning\nAnother promising approach to leverage prior knowledge is the incorporation of large language models (LLMs) (Anil et al., 2023; OpenAI, 2023). LLMs, trained on extensive and diverse text corpora, have demonstrated the ability to generalize to unseen tasks, proving their utility across various domains (Brown et al., 2020). Recent studies have attempted to harness the rich prior knowledge encapsulated by LLMs for tabular learning. They typically propose serializing tabular data into text and feeding them into the input prompts of LLMs (Dinh et al., 2022; Hegselmann et al., 2023; Wang et al., 2023). By including tabular data, fea-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fae0/fae002d7-108e-4224-8d19-83d152d44629.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Illustration of FeatLLM. FeatLLM extracts rules for each class, utilizing prior knowledge and few-shot examples These rules are then parsed and applied to create binary features for data samples. A linear layer is trained on these binary features to estimate class likelihoods. This procedure is repeated multiple times for ensembling.</div>\nture descriptions, and task descriptions in the input prompts, these can infer the relationships between tasks and features to perform inference. Research has evolved to either specifically train models using parameter-efficient tuning techniques like LoRA (Hu et al., 2021) or IA3 (Liu et al., 2022), or employ in-context learning by adding few-shot example demonstrations to the prompts without training (Dinh et al., 2022; Hegselmann et al., 2023; Nam et al., 2023a; Slack & Singh, 2023).\nture descriptions, and task descriptions in the input prompts, these can infer the relationships between tasks and features to perform inference. Research has evolved to either specifically train models using parameter-efficient tuning techniques like LoRA (Hu et al., 2021) or IA3 (Liu et al., 2022), or employ in-context learning by adding few-shot example demonstrations to the prompts without training (Dinh et al., 2022; Hegselmann et al., 2023; Nam et al., 2023a; Slack & Singh, 2023). Despite the effectiveness of the aforementioned methods in improving few-shot tabular learning, the requirement to always pass inference through the LLM poses challenges for application in industrial domains. This study aims to depart from the conventional black-box approach of processing each sample\u2019s prediction in an end-to-end fashion via an LLM. Instead, it targets the LLM to directly infer the conditions or rules associated with each class. FeatLLM parses the generated rules into the program code to create new features, enabling predictions without the need to pass through the LLM and leveraging in-context learning to extract rules utilizing prior knowledge and information from few-shot samples.\nDespite the effectiveness of the aforementioned methods in improving few-shot tabular learning, the requirement to always pass inference through the LLM poses challenges for application in industrial domains. This study aims to depart from the conventional black-box approach of processing each sample\u2019s prediction in an end-to-end fashion via an LLM. Instead, it targets the LLM to directly infer the conditions or rules associated with each class. FeatLLM parses the generated rules into the program code to create new features, enabling predictions without the need to pass through the LLM and leveraging in-context learning to extract rules utilizing prior knowledge and information from few-shot samples.\n# 3. Methods\nFeatLLM is designed to extract \u201crules\u201d, the conditions associated with each answer class, from the few samples it inputs, as depicted in Figure 1. The extracted rules are parsed through an LLM, and then used to create new binary features of the given sample indicating whether or not each rule is satisfied. The set of new binary features are input to a dot product operation with non-negative trainable weights, to estimate the likelihood of each class. By training this model, we learn the importance of each feature and how it affects the class likelihood (Section 3.2). This process is repeated multiple times with bagging and then combined via ensembling. Problem formulation and details of each stage are explained below.\nProblem formulation. Let\u2019s consider a tabular dataset with N labeled samples D = {(xi, yi)}N i=1. The dataset D consists of d input features (i.e., each xi is a d-dimensional vector), and comes with natural language feature names, denoted by F = {fj}d j=1, along with their definitions provided separately. Assuming the classification setup, the label yi is defined as one of the predefined set of classes. For the k-shot learning experiments, only k (< N) labeled samples are randomly sampled to train the model.\n# 3.1. Prompt Design for Extracting Rules\nTo enable the LLM to extract rules based on a more accurate reasoning path, we guide the problem-solving process to mimic how an expert human might approach a tabular learning task. Our prompt comprises three main components as follows (shown in Fig. 2 and Appendix A.2):\nBasic information description. This part provides essential information for solving the problem (see orange text in Fig. 2). It includes a description of the task to be solved with label information, descriptions of features, and example demonstrations. The task description is formulated as a question (e.g., \u201cDoes this patient\u2019s myocardial infarction complications data indicate chronic heart failure? Yes or no?\u201d. More examples are provided in Appendix A.1). The feature description indicates its value type and optionally includes its definition. For categorical variables, examples of the possible categories can be provided. As example demonstrations, few training samples are serialized into text, along with their ground-truth labels. Serialization follows the format used in previous studies (Dinh et al., 2022; Hegselmann et al., 2023):\n(1)\nReasoning instruction. We aim to enhance the LLM\u2019s reasoning by providing guidance through reasoning instruc-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f83/7f8354f4-834c-4e09-8a60-f620095e82db.png\" style=\"width: 50%;\"></div>\nFigure 2: Prompt for rule extraction. Text in orange provides basic information description; blue text outlines reasoning instruction; and yellow text details response instruction.\ntions (see blue text in Fig. 2). The reasoning instruction begins with an introductory sentence similar to the chainof-thought approach (Wei et al., 2022b). Then, we divide the LLM\u2019s inference process into two steps. In the first step, the LLM is encouraged to infer the causal relationship or tendency between features and the task description. This is done without referring to example demonstrations, relying instead on general knowledge or common sense, allowing the LLM to fully extract prior knowledge about the problem. In the second step, the LLM uses example demonstrations and the information from the first step to deduce rules for each class. This two-step reasoning process prevents the model from identifying spurious correlations in irrelevant columns and can assist in focusing on more significant features. To prevent extracting too few or too many rules, potentially leading to either underfitting or creating an\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b7b/2b7bb08c-8752-4f4e-8976-af940483a227.png\" style=\"width: 50%;\"></div>\nFigure 3: Prompt for parsing rules. This prompt incorporates the rules generated in the previous stage, placed within the <Conditions> section.\n# 3.2. Inferring Class Likelihood via Rules\nParsing rules for feature generation. We utilize the rules generated in the previous stage to create new binary features. These features are created for each class, indicating whether the sample satisfies the rules associated with that class (i.e., either \u20190\u2019 or \u20191\u2019). They can be used in predicting how likely a sample belongs to each class. However, since the rules generated by the LLM are based on natural language, designing a parsing strategy is required for automatic feature generation. Even though we constrain formatting of responses and rules during rule generation for easier parsing, the LLM\u2019s output can include noisy syntactic changes (Kova\u02c7c et al., 2023). For instance, the LLM might alter expressions, like\nsubstituting \u201c>\u201d or \u201c<\u201d with phrases like \u201cgreater than\u201d or \u201cless than.\u201d This variability makes parsing more challenging. To address the challenges of parsing noisy text, instead of building complex program code, we leverage the LLM itself. The LLM is good at understanding the semantic meaning despite syntactic changes in text, and it has the ability to generate a simple code with instructions (given it\u2019s trained on code data corpus). To leverage these, we include the function name, input and output descriptions, and inferred rules in the prompt, then input them into the LLM. Figures 3 and A.4-A.5 in Appendix show the prompt examples and the corresponding outcomes. The generated code is executed using Python\u2019s exec() function to perform data conversion.\nsubstituting \u201c>\u201d or \u201c<\u201d with phrases like \u201cgreater than\u201d or \u201cless than.\u201d This variability makes parsing more challenging. To address the challenges of parsing noisy text, instead of building complex program code, we leverage the LLM itself. The LLM is good at understanding the semantic meaning despite syntactic changes in text, and it has the ability to generate a simple code with instructions (given it\u2019s trained on code data corpus). To leverage these, we include the function name, input and output descriptions, and inferred rules in the prompt, then input them into the LLM. Figures 3 and A.4-A.5 in Appendix show the prompt examples and the corresponding outcomes. The generated code is executed using Python\u2019s exec() function to perform data conversion. Inferring class likelihood. Given new binary features from rules for each class, a simple method to measure the class likelihood of the sample is to count how many rules of each class it satisfies (i.e., the sum of the binary feature per class). However, not all rules and features carry the same importance, necessitating learning their significance from training samples. We aim to learn this importance using a conventional machine learning (ML) model, applied to each class\u2019s binary feature. For example, we can consider a linear model without bias. Let us denote the generated binary feature from sample xi for class k as zi k \u2208{0, 1}R, where R is the number of rules per each class and c is the number of classes. The probability for each class pi is computed as follows:\nInferring class likelihood. Given new binary features from rules for each class, a simple method to measure the class likelihood of the sample is to count how many rules of each class it satisfies (i.e., the sum of the binary feature per class). However, not all rules and features carry the same importance, necessitating learning their significance from training samples. We aim to learn this importance using a conventional machine learning (ML) model, applied to each class\u2019s binary feature. For example, we can consider a linear model without bias. Let us denote the generated binary feature from sample xi for class k as zi k \u2208{0, 1}R, where R is the number of rules per each class and c is the number of classes. The probability for each class pi is computed as follows:\n(2)\nHere, wk represent the trainable weight in the linear model and they signify the importance of each feature. By projecting these weights into a positive subspace, we can ensure that the features generated by the LLM do not reduce the likelihood of a class during learning. The logit vector is then converted into class probabilities through the softmax function. We optimize the cross-entropy loss to learn the weight wk using a few-shot training samples. Ensembling with bagging. Finally, we repeatedly execute the entire process to create multiple inference models to make the final prediction via ensemble. Given the collection of trained weights from T trials, denoted as {w[t]}T t=1, the prediction is made by averaging the probability for each class pi computed with each weight w[t] as in Eq. 2. To introduce randomness in rule generation for ensemble, a sufficiently high temperature is set for LLM inference. We also randomize the order of few-shot demonstrations based on the observation that the order can impact the query outcome of LLM (Lu et al., 2022)2. To utilize the diversity in prediction for more robust predictions, we adopt bagging to select a subset of features or instances for each trial,\nwhich gets particularly useful for larger number of training samples or features. The proposed ensemble approach offers two main advantages. First, even if the LLM generates incorrect rules in some trials, the correct rules from other trials can compensate, enhancing the overall robustness of the framework. This ties into the LLM\u2019s self-consistency (Wang et al., 2022), as rules commonly inferred across multiple trials are more likely to be accurate. Second, the ensemble method addresses the limitation of LLM\u2019s prompt size. When the tabular data exceed the allowable input prompt size, bagging helps reduce it, ensuring the model\u2019s robust performance. While a single rule set might not capture the relationships of all features, the ensemble of various weak learners created across multiple trials can nullify the impact of incomplete feature/instance coverage in individual trials.\n# 4. Experiments\nWe evaluate FeatLLM over multiple tabular datasets, and conduct various analyses to investigate how our framework works. Due to space constraints, additional experiments, such as varying LLMs, qualitative analysis, and others, are described in Appendix.\n# 4.1. Performance Evaluation\nDatasets. Our experiment utilizes 13 datasets for binary or multi-class classification tasks: (1) Adult (Asuncion & Newman, 2007) for predicting whether an individual earns over $50,000 annually; (2) Bank (Moro et al., 2014) for predicting whether a customer will subscribe to a term deposit; (3) Blood (Yeh et al., 2009) for predicting whether donors will return for subsequent donations; (4) Car (Kadra et al., 2021) for predicting the quality of a car; (5) Communities (Redmond, 2009) for predicting the crime rate in a specified region; (6) Credit-g (Kadra et al., 2021) for predicting whether an individual poses a good or bad credit risk; (7) Diabetes3 for predicting the presence of diabetes in an individual; (8) Heart4 for predicting the occurrence of coronary artery disease in an individual; and (9) Myocardial (Golovenkin & Voino-Yasenetsky, 2020) for predicting whether an individual suffers from chronic heart failure. We also include more recent datasets and synthetic datasets which have been less frequently discussed and were not included in the pre-training process of LLMs: (10) Cultivars for predicting how high the grain yield of this soybean cultivar will be5; (11) NHANES for predicting the given person\u2019s age group from the record6; (12) Sequence-type for\n3kaggle.com/datasets/uciml/ pima-indians-diabetes-database 4kaggle.com/datasets/fedesoriano/ heart-failure-prediction 5archive.ics.uci.edu/dataset/913 6archive.ics.uci.edu/dataset/887\n<div style=\"text-align: center;\">Table 1: Evaluation results, showing AUC across 11 datasets with attributes fewer than 100, are presented. Best performance are bolded, and our framework\u2019s performances, when second-best, are underlined. Results with more training shots (e.g., 32 64) and an additional baseline (e.g., RandomForest) are provided in Appendix K.</div>\nData\nShot\nLogReg\nXGBoost\nSCARF\nTabPFN\nSTUNT\nIn-context\nTABLET\nTabLLM\nOurs\nAdult\n4\n72.10\u00b112.30\n50.00\u00b10.00\n58.34\u00b115.42 60.89\u00b123.28 67.43\u00b129.61\n77.51\u00b15.24\n75.29\u00b112.24\n83.57\u00b12.69\n86.68\u00b10.86\n8\n76.02\u00b13.37\n59.19\u00b16.92\n72.42\u00b18.95\n70.42\u00b19.96\n82.16\u00b16.93\n79.30\u00b12.89\n77.56\u00b17.56\n83.52\u00b14.30\n87.89\u00b10.06\n16\n75.20\u00b15.10\n60.68\u00b113.92\n75.63\u00b19.56\n70.34\u00b19.96\n80.57\u00b110.93\n79.50\u00b14.57\n79.74\u00b15.64\n83.23\u00b12.45\n87.54\u00b10.50\nBank\n4\n63.70\u00b13.87\n50.00\u00b10.00\n58.53\u00b15.49\n63.19\u00b111.60 56.34\u00b112.82\n61.38\u00b11.30\n58.11\u00b16.29\n62.51\u00b18.95\n70.45\u00b13.69\n8\n72.52\u00b13.21\n58.78\u00b110.54 55.28\u00b111.88\n62.81\u00b17.84\n63.01\u00b18.78\n69.57\u00b113.35\n69.08\u00b16.00\n63.19\u00b15.79\n75.85\u00b16.66\n16\n77.51\u00b13.09\n70.34\u00b15.86\n65.81\u00b11.79\n73.79\u00b12.21\n69.85\u00b10.95\n69.76\u00b18.55\n69.40\u00b111.28\n63.73\u00b16.43\n78.41\u00b11.08\nBlood\n4\n56.79\u00b126.02\n50.00\u00b10.00\n56.22\u00b121.00 58.72\u00b119.16\n48.57\u00b16.04\n56.30\u00b112.43 56.45\u00b115.45 55.87\u00b113.49\n68.34\u00b17.48\n8\n68.51\u00b15.16\n59.97\u00b11.36\n65.77\u00b15.00\n66.30\u00b110.01\n60.00\u00b14.84\n58.99\u00b110.12 56.37\u00b111.56\n66.01\u00b19.25\n70.37\u00b13.23\n16\n68.30\u00b16.16\n63.28\u00b17.62\n66.27\u00b15.04\n64.14\u00b16.80\n54.76\u00b14.53\n56.59\u00b15.21\n60.62\u00b14.13\n65.14\u00b17.55\n70.07\u00b15.19\nCar\n4\n62.38\u00b14.13\n50.00\u00b10.00\n62.52\u00b13.80\n58.14\u00b14.15\n61.32\u00b13.83\n62.47\u00b12.47\n60.21\u00b14.81\n85.82\u00b13.65\n72.69\u00b11.52\n8\n72.05\u00b11.20\n64.00\u00b13.57\n72.23\u00b12.59\n63.95\u00b14.35\n67.86\u00b10.49\n67.57\u00b13.44\n65.53\u00b18.00\n87.43\u00b12.56\n73.26\u00b11.46\n16\n82.42\u00b14.13\n72.26\u00b14.43\n75.77\u00b12.71\n71.35\u00b15.33\n75.56\u00b12.88\n76.94\u00b13.04\n74.02\u00b11.01\n88.65\u00b12.63\n79.43\u00b11.24\nCredit-g\n4\n52.68\u00b14.46\n50.00\u00b10.00\n48.92\u00b14.60\n54.00\u00b17.34\n48.80\u00b16.76\n52.99\u00b14.08\n54.33\u00b16.54\n51.90\u00b19.40\n55.94\u00b11.10\n8\n55.52\u00b18.88\n52.22\u00b14.90\n55.26\u00b13.92\n52.58\u00b111.27\n54.50\u00b18.25\n52.43\u00b14.36\n52.90\u00b15.79\n56.42\u00b112.89\n57.42\u00b13.10\n16\n58.26\u00b15.17\n56.23\u00b14.37\n59.22\u00b111.38\n58.91\u00b18.04\n57.63\u00b17.58\n55.29\u00b14.80\n51.65\u00b14.02\n60.38\u00b114.03\n56.60\u00b12.22\nDiabetes\n4\n57.09\u00b118.84\n50.00\u00b10.00\n62.35\u00b17.48\n56.28\u00b113.01\n64.22\u00b16.78\n71.71\u00b15.31\n63.96\u00b13.32\n70.42\u00b13.69\n80.28\u00b10.75\n8\n65.52\u00b113.18 50.86\u00b122.03 64.69\u00b113.33\n69.08\u00b19.68\n67.39\u00b112.92\n72.21\u00b12.07\n65.47\u00b13.95\n64.30\u00b15.88\n79.38\u00b11.66\n16\n73.44\u00b10.52\n65.69\u00b16.54\n71.86\u00b13.16\n73.69\u00b13.21\n73.79\u00b16.48\n71.64\u00b15.05\n66.71\u00b10.76\n67.34\u00b12.79\n80.15\u00b11.35\nHeart\n4\n70.54\u00b13.83\n50.00\u00b10.00\n59.38\u00b13.42\n67.33\u00b115.29\n88.27\u00b13.32\n60.76\u00b14.00\n68.19\u00b111.17\n59.74\u00b14.49\n75.66\u00b14.59\n8\n78.12\u00b110.59\n55.88\u00b13.98\n74.35\u00b16.93\n77.89\u00b12.34\n88.78\u00b12.38\n65.46\u00b13.77\n69.85\u00b110.82\n70.14\u00b17.91\n79.46\u00b12.16\n16\n83.02\u00b13.70\n78.62\u00b17.14\n83.66\u00b15.91\n81.45\u00b15.05\n89.13\u00b12.10\n67.00\u00b17.83\n68.39\u00b111.73\n81.72\u00b13.92\n83.71\u00b11.88\nCultivars\n4\n53.45\u00b110.79\n50.00\u00b10.00\n46.99\u00b16.33\n49.80\u00b115.90\n57.10\u00b18.66\n51.38\u00b12.48\n54.28\u00b13.73\n54.39\u00b15.61\n55.63\u00b15.24\n8\n56.22\u00b111.87\n52.60\u00b16.31\n51.76\u00b19.99\n54.72\u00b19.35\n57.26\u00b19.52\n51.68\u00b14.43\n51.48\u00b13.85\n52.86\u00b16.13\n56.97\u00b15.08\n16\n60.35\u00b14.23\n56.87\u00b12.50\n57.06\u00b19.27\n54.92\u00b18.32\n60.09\u00b17.64\n54.31\u00b16.12\n57.44\u00b13.53\n56.97\u00b12.22\n57.19\u00b15.30\nNHANES\n4\n91.96\u00b17.02\n50.00\u00b10.00\n51.58\u00b14.66\n80.74\u00b13.89\n69.32\u00b119.59\n91.84\u00b13.79\n93.54\u00b14.20\n99.49\u00b10.23\n92.20\u00b11.71\n8\n92.38\u00b18.21\n92.92\u00b14.56\n55.07\u00b14.79\n85.10\u00b15.88\n68.56\u00b118.35\n86.67\u00b15.49\n94.25\u00b13.35\n100.00\u00b10.00\n93.29\u00b17.01\n16\n94.12\u00b13.88\n94.52\u00b16.28\n89.78\u00b16.77\n95.54\u00b10.82\n68.62\u00b119.81\n93.33\u00b14.47\n95.02\u00b11.57\n100.00\u00b10.00\n95.64\u00b14.67\nSequence\n4\n66.80\u00b14.98\n50.00\u00b10.00\n67.08\u00b12.75\n71.93\u00b14.17\n60.12\u00b16.51\n93.01\u00b11.84\n91.00\u00b14.25\n75.38\u00b15.21\n98.60\u00b10.66\n-type\n8\n72.55\u00b14.37\n61.01\u00b110.96\n60.53\u00b16.91\n79.55\u00b16.71\n60.58\u00b19.58\n94.29\u00b11.64\n90.18\u00b13.73\n79.91\u00b12.13\n98.70\u00b11.44\n16\n83.10\u00b16.86\n80.31\u00b15.79\n73.14\u00b15.29\n89.59\u00b12.33\n65.12\u00b15.28\n93.46\u00b10.72\n92.09\u00b12.90\n86.97\u00b11.97\n99.29\u00b10.78\nSolution\n4\n72.71\u00b11.96\n50.00\u00b10.00\n68.48\u00b114.97\n71.19\u00b13.90\n64.52\u00b18.41\n73.53\u00b11.94\n80.26\u00b15.11\n73.75\u00b13.90\n100.00\u00b10.00\n-mix\n8\n82.91\u00b17.74\n58.31\u00b119.17\n56.67\u00b17.64\n82.42\u00b13.72\n72.04\u00b110.34\n76.34\u00b14.26\n83.19\u00b13.70\n76.57\u00b14.80\n99.81\u00b10.08\n16\n83.87\u00b11.90\n68.09\u00b11.76\n70.37\u00b113.96\n84.39\u00b10.83\n71.82\u00b18.79\n82.15\u00b15.01\n88.10\u00b14.72\n84.28\u00b15.15\n99.67\u00b10.13\nAverage\n4\n65.47\n50.00\n58.22\n62.93\n62.36\n68.44\n68.69\n70.26\n77.86\n8\n72.03\n60.52\n62.18\n69.53\n67.47\n70.41\n70.53\n72.76\n79.31\n16\n76.33\n69.72\n71.69\n74.37\n69.72\n72.72\n73.02\n76.22\n80.70\nclassifying a given sequence as one of four types: arithmetic, geometric, Fibonacci, or Collatz; (13) Solution-mix for predicting whether the percentage concentration of a mixture from four solutions exceeds 0.5. Former two datasets were recently donated to the UCI Machine Learning Repository after September 2023, ensuring they were not part of the pre-training for the LLM API (gpt-3.5-turbo-0613) used by our model. The latter two are synthetic datasets which ensures they could not be memorized by the LLM API and proceeded with the evaluation. These datasets vary in size and complexity, as detailed in Table 2. Each dataset provides a clear name and description for each attribute. Datasets like \u2018Communities\u2019 and\n\u2018Myocardial\u2019 include over 100 attributes, posing challenges due to limited context window size in LLM.\nBaselines. We conduct comparisons with nine baselines. The first three baselines are conventional tabular learning approaches, including (1) Logistic regression (LogReg), (2) XGBoost (Chen & Guestrin, 2016), and (3) RandomForest (Ho, 1995). The next two baselines assume the use of unlabeled datasets, including (4) SCARF (Bahri et al., 2022) and (5) STUNT (Nam et al., 2023b). Note that obtaining a large quantity of unlabeled dataset may be infeasible in real-world applications. Another recent baseline is (6) TabPFN (Hollmann et al., 2023), which generates a synthetic dataset with diverse distribution to pretrain the model. The final three baselines utilize LLM, including\nTable 2: Basic information of each dataset used in our experiments. The numbers in parentheses indicate the count of categorical and numerical attributes, respectively.\nData\n# of samples\n# of features\nLabel ratio (%)\nAdult\n48842\n14 (7/7)\n76:24\nBank\n45211\n16 (8/8)\n88:12\nBlood\n748\n4 (0/4)\n76:24\nCar\n1728\n6 (5/1)\n70:22:4:4\nCommunities\n1994\n103 (1/102)\n34:33:33\nCredit-g\n1000\n20 (12/8)\n70:30\nDiabetes\n768\n8 (0/8)\n65:35\nHeart\n918\n11 (4/7)\n45:55\nMyocardial\n1700\n111 (94/17)\n22:78\nCultivars\n320\n10 (3/7)\n50:50\nNHANES\n6287\n8 (1/7)\n84:16\nSequence-type\n250\n5 (0/5)\n40:40:10:10\nSolution-mix\n300\n8 (0/8)\n52:48\n(7) In-context learning (In-context) (Wei et al., 2022a), (8) TABLET (Slack & Singh, 2023), and (9) TabLLM (Hegselmann et al., 2023). In-context learning directly embeds few-shot training samples into the input prompt without tuning. TABLET further incorporates additional hints into the prompt through an external classifier via rule-sets and prototypes to enhance the quality of inference. TabLLM employs parameter-efficient tuning technique like IA3 (Liu et al., 2022) to train the LLM with few-shot samples. Implementation details. FeatLLM uses GPT-3.5 as its LLM backbone, yet is designed to be agnostic to the choice of LLMs (see Appendix H for results with different backbones). The temperature for the LLM inference is set to 0.5 and the top-p value is set to the default value of 1 in API. We set the number of ensembles and the number of rules for extracting to 20 and 10 respectively. Details on hyper-parameter impacts are in Figure 6 and Appendix B. We use the Adam optimizer of a learning rate 0.01 for the linear model, training for 200 epochs. We employ k-fold cross-validation for optimal epoch selection. In replicating baselines, GPT-3.5 is utilized for in-context learning, whereas T0 (Sanh et al., 2022) is employed in the TabLLM, following the original setting. Note that TabLLM restricts the use of LLM to only those with publicly available checkpoints; therefore, GPT-3.5 is not accessible in this context. Regarding traditional machine learning methods, such as LogReg, XGBoost, and RandomForest, we determined their parameters using Grid-search combined with k-fold cross validation. The value of k is set either as 2 or 4, guaranteeing that the training set includes at least one example of each class. For more detailed information on implementations, please refer to Appendix C. Results. In Tables 1 and 3, performance comparisons across a range of datasets are presented. We repeat experiments three times with different randomness on the choice\n(7) In-context learning (In-context) (Wei et al., 2022a), (8) TABLET (Slack & Singh, 2023), and (9) TabLLM (Hegselmann et al., 2023). In-context learning directly embeds few-shot training samples into the input prompt without tuning. TABLET further incorporates additional hints into the prompt through an external classifier via rule-sets and prototypes to enhance the quality of inference. TabLLM employs parameter-efficient tuning technique like IA3 (Liu et al., 2022) to train the LLM with few-shot samples.\nits LLM backbone, yet is designed to be agnostic to the choice of LLMs (see Appendix H for results with different backbones). The temperature for the LLM inference is set to 0.5 and the top-p value is set to the default value of 1 in API. We set the number of ensembles and the number of rules for extracting to 20 and 10 respectively. Details on hyper-parameter impacts are in Figure 6 and Appendix B. We use the Adam optimizer of a learning rate 0.01 for the linear model, training for 200 epochs. We employ k-fold cross-validation for optimal epoch selection. In replicating baselines, GPT-3.5 is utilized for in-context learning, whereas T0 (Sanh et al., 2022) is employed in the TabLLM, following the original setting. Note that TabLLM restricts the use of LLM to only those with publicly available checkpoints; therefore, GPT-3.5 is not accessible in this context. Regarding traditional machine learning methods, such as LogReg, XGBoost, and RandomForest, we determined their parameters using Grid-search combined with k-fold cross validation. The value of k is set either as 2 or 4, guaranteeing that the training set includes at least one example of each class. For more detailed information on implementations, please refer to Appendix C. Results. In Tables 1 and 3, performance comparisons\nResults. In Tables 1 and 3, performance comparisons across a range of datasets are presented. We repeat experiments three times with different randomness on the choice\nTable 3: Evaluation results with AUC across two datasets with over 100 attributes. TabPFN and two LLM-based baselines are excluded in the table as they failed to make inference over the dataset with a large number of features.\nCommunities\nShots\n4\n8\n16\nLogReg\n67.45\u00b113.26\n73.73\u00b15.45\n72.55\u00b14.83\nXGBoost\n53.94\u00b14.19\n66.65\u00b14.50\n68.01\u00b11.97\nRandomForest\n66.09\u00b110.52\n71.16\u00b14.61\n71.66\u00b14.81\nSCARF\n66.18\u00b19.13\n72.69\u00b13.79\n73.09\u00b12.84\nSTUNT\n66.87\u00b114.10\n76.36\u00b14.55\n77.29\u00b12.56\nFeatLLM\n75.39\u00b15.05\n76.59\u00b11.25\n76.25\u00b10.64\nMyocardial\nShots\n4\n8\n16\nLogReg\n51.25\u00b13.85\n55.34\u00b11.11\n60.00\u00b15.16\nXGBoost\n50.00\u00b10.00\n55.63\u00b12.92\n56.55\u00b112.22\nRandomForest\n51.91\u00b14.49\n52.77\u00b15.83\n54.16\u00b14.53\nSCARF\n47.70\u00b14.10\n49.37\u00b13.41\n54.31\u00b11.42\nSTUNT\n52.77\u00b12.01\n55.40\u00b14.41\n61.22\u00b13.45\nFeatLLM\n52.87\u00b13.44\n56.22\u00b11.64\n55.32\u00b19.15\nof training/test sets, and present the average and standard deviation of the AUC values. FeatLLM consistently ranks as the top performer or secures the second place when compared with other baseline models. Our framework, when utilizing just 4 shots, achieves comparable performance to conventional tabular baselines that require 32 shots (see Fig. 4). Notably, while STUNT and TabPFN demonstrated significant performance improvements on certain datasets, their overall enhancement over conventional machine learning models was not substantial. Moreover, models utilizing LLM, such as in-context learning, TABLET, or TabLLM, failed to infer on tabular datasets with a large number of features. Our framework, employing both bagging and ensemble techniques, proved to be effective even on datasets with numerous features, exhibiting satisfactory performance. It is important to note that our bagging and ensemble concept could be adapted to existing LLM-based approaches. However, such an adaptation would double the number of per-sample inference, leading to a substantial increase in computational complexity and thus becoming impractical.\n# 4.2. Analysis & Discussion\nAblation study. We analyze the contributions of major components to the performance by performing ablation studies, focusing on: (1) -Tuning: omitting the weight tuning process in the linear model, summing up the binary feature to compute the logit instead; (2) -Ensemble: omitting the ensemble process; (3) -Description: omitting the feature description, and (4) -Reasoning: omitting the Step-1 process in reasoning instruction part when generating rules.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/11b8/11b84167-32b5-4227-b4cc-c174b2f7169e.png\" style=\"width: 50%;\"></div>\nModel\nTraining (in seconds) Inference (in milliseconds)\nLogReg\n0.721\n0.001\nXGBoost\n28.512\n0.006\nRandomForest\n1.343\n0.001\nSCARF\n426.859\n0.002\nTabPFN\n0.440\n1.149\nSTUNT\n642.796\n0.006\nIn-context\u2020\nN/A\n463.000\nTABLET\u2020\n0.813\n523.254\nTabLLM\n251.242\n335.127\nFeatLLM\u2020\n860.094\n0.006\n<div style=\"text-align: center;\">Figure 4: Performance comparison summaries among conventional tabular baselines and FeatLLM. Averaged AUC over all datasets across the number of shots are reported.</div>\nFigure 4: Performance comparison summaries among conventional tabular baselines and FeatLLM. Averaged AUC over all datasets across the number of shots are reported.\nTable 4 summarizes the change in the AUC from ablations averaged across all datasets. In all cases, modifying the ablated component results in performance degradation. Interestingly, the benefit of weight tuning becomes higher when the number of shots increases. This indicates that when there is a large amount of data, accurate estimation of the importance of rules becomes feasible, making the effect of tuning significant. On the other hand, the effect of feature descriptions and reasoning instructions are high when the number of shot is small. This suggests that the efficient utilization of the prior knowledge of LLM becomes crucial for performance improvements. Lastly, the proposed ensemble approach consistently improves the performance in all settings. Table 4: Ablation study summaries. Averaged changes of AUC with standard error after altering or omitting modules over 13 datasets with three different trials are reported.\nShot\nFeatLLM\n-Tuning\n-Ensemble\n-Description\n-Reasoning\n4\n75.7\n-1.41\u00b11.00\n-5.39\u00b10.81\n-1.76\u00b11.06\n-5.03\u00b11.96\n8\n77.3\n-2.72\u00b10.93\n-6.96\u00b11.40\n-1.20\u00b10.33\n-3.55\u00b10.81\n16\n78.4\n-2.57\u00b10.73\n-6.65\u00b11.18\n-0.26\u00b10.31\n-1.50\u00b10.87\n32\n80.3\n-5.75\u00b11.19\n-7.38\u00b11.34\n-0.29\u00b10.58\n-2.42\u00b11.15\n64\n81.4\n-4.88\u00b11.40\n-6.09\u00b10.96\n-0.70\u00b10.54\n-1.71\u00b10.47\nAvg\n78.6\n-3.47\u00b10.51\n-6.49\u00b10.51\n-0.84\u00b10.28\n-2.84\u00b10.53\nTraining & inference time comparison. We compare the training and inference time between different methods. The experiments are performed on the Adult dataset, including a training set of 16 shots. One A100 GPU is used as the default, except for TabLLM which uses four A100 GPUs for model parallelism. Table 5 provides the runtime comparisons. Notably, FeatLLM shows a relatively low inference time, comparable to that of conventional methods. In contrast, other LLM-based approaches, such as In-context learning, TABLET, and TabLLM, exhibit much higher inference time. In terms of training time, FeatLLM is comparable with other LLM-based methods \u2013 it requires only 30 API queries, which does not impose a significant burden on the budget and even can be parallelized.\n<div style=\"text-align: center;\">Table 5: Comparison of training (with 16 samples) and inference (over one sample) time on Adult.</div>\n\u2020 These models employ API queries, where the runtime is subject to the API\u2019s status at the time of use.\nQuality of features generated by FeatLLM. We conduct a simple experiment to investigate the informativeness of rule-based features generated by FeatLLM in solving practical tasks. Specifically, we measure the average AUROC between the newly generated features and the target labels, and then compute the ratio of useful features with an AUROC higher than 0.5 for each dataset. Table 6 below summarizes the results. It confirms that the majority of the created rules are relevant to the target attribute and provide informative insights for addressing the target task (refer to the column \u201cBefore tuning\u201d). Moreover, during the process of tuning the linear model with data, rules that have little relevance to the data (i.e., rules with learned weights below the threshold) are automatically filtered out (refer to the column \u201cAfter tuning\u201d). Note that the threshold was set at 0.1, taking into consideration the overall histogram of weights. Table 6: Evaluation of feature quality before and after linear model tuning. Ratio of newly-generated features with an AUROC higher than 0.5 is reported across datasets.\nTable 6: Evaluation of feature quality before and after linear model tuning. Ratio of newly-generated features with an AUROC higher than 0.5 is reported across datasets.\nMethod\nRatio of useful rules\nBefore tuning\nAfter tuning\nAdult\n0.971\n0.980\nBank\n0.627\n0.958\nBlood\n0.889\n0.975\nCar\n0.712\n0.849\nCommunities\n0.767\n0.889\nCredit-g\n0.643\n0.802\nDiabetes\n0.974\n0.972\nHeart\n0.800\n0.924\nMyocardial\n0.690\n0.744\nCultivars\n0.775\n0.927\nNHANES\n0.600\n0.832\nSequence-type\n0.896\n0.985\nSolution-mix\n0.917\n1.000\nAverage\n0.789\n0.911\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2696/26969aaf-bd58-402d-acf7-0f0b71ce5ea1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/54bf/54bfc86c-ad61-4a8e-8926-dad30fa39795.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 5: Visualization of performance impact from spurious correlations. The results exhibit the models\u2019 performance (AUC) each time a noisy column from the Adult dataset is added to the original Heart dataset. XGBoost is excluded here due to its lower performance.\nEffect of adding spurious correlations. To ascertain how effectively various methods filter out noisy spurious correlations in a low-shot regime, we conducted an analysis. The experiment utilized the Heart dataset, to which we randomly attach columns from the Adult dataset that are irrelevant to the task of the Heart dataset. We then measure the performance changes of the models while progressively increasing the number of unrelated columns. For a fair comparison, we assume the same 8 shots of labeled samples without considering an unlabeled dataset, thereby excluding methods like SCARF or STUNT from the comparison.\nFigure 5 visualizes the performance changes due to spurious correlations. Compared to baselines, FeatLLM showed the least performance degradation from spurious correlations. This is likely because our framework forces the consideration of the relationship between the task and features based on prior knowledge when extracting rules, preventing the generation of rules based on irrelevant columns and thereby yielding robust results. Indeed, we observe that rules based on noisy columns accounted for only 1-2% of the total. Meanwhile, we also observe that FeatLLM can also learn spurious correlations and misunderstand causal relationships between task and features if columns from closely related data are randomly attached (see Appendix I).\nHyper-parameter analysis FeatLLM has two main hyper-parameters. One is the number of ensembles, and the other is how many rules per class to extract during each LLM query. We conducted experiments to investigate the impact of these hyper-parameters on performance. The results showed that performance improves as the number of ensembles increases, but beyond a certain point (i.e., around 20), the performance gains begin to converge (see Fig. 11 in Appendix). As for the number of rules, while there wasn\u2019t a statistically significant difference, we determined that 10 is the most appropriate number considering the trade-off\n<div style=\"text-align: center;\">Figure 6: Effect of the number of rules per each rule extraction. Averaged AUC over all datasets is reported.</div>\nbetween prompt length and performance (see Fig. 6). We speculate this result is due to more rules enlarging input dimensionality, leading to an increase in information, yet also resulting in overfitting within the low-shot regime.\n# 5. Conclusion\nWe present FeatLLM which advances the state-of-the-art in LLM-based few-shot tabular learning. FeatLLM goes beyond using the LLMs for predictions for each sample, and focuses on generating criteria for making predictions as feature engineers. Through rule generation and an ensemble process with bagging, FeatLLM requires minimal inference cost, needs only API access without training, and overcomes the constraints to data feature size. FeatLLM achieves high prediction performance and is a superior alternative for applying LLM on real-world tabular data in a data-efficient way.\nFeatLLM is designed to focus on low-shot learing regime only. Our future goal is to expand its capabilities of crafting new features for datasets with a larger number of samples, exploring various feature types beyond rules, and bringing interpretability, which enables its application across a wider array of tasks.\n# 6. Broader Impact\nThe proposed framework, FeatLLM, aims to utilize the prior knowledge of LLM in performing tabular learning tasks to go beyond the achievable performance with standard supervised tabular learning. To push the data efficiency in learning, our research makes important contributions to mitigate the issue of overfitting due to limited training samples, by supplementing with prior knowledge. FeatLLM is particularly beneficial for practitioners in real-world scenarios (such as from finance and healthcare industries) where labeling is challenging due to the high human cost or label collection difficulties, and prior knowledge in LLMs can be potentially very valuable (as the LLMs are pretrained\nwith relevant finance or healthcare related text). As one potential important future work direction towards widespread adoption, analyses of the impact of the societal biases and misinformation embedded in the prior knowledge of LLMs would be important, as the prior knowledge would be directly affecting the predictions even dominating over the observed or collected labeled data samples.\n# Acknowledgement\nWe sincerely thank Scott Yak and Yihe Dong for their valuable feedback and discussion on this work.\n# References\n100,000 genomes pilot on rare-disease diagnosis in health care \u2014 preliminary report. New England Journal of Medicine, 385(20):1868\u20131880, 2021. Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Arik, S. \u00a8O. and Pfister, T. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 6679\u20136687, 2021. Asuncion, A. and Newman, D. Uci machine learning repository, 2007. Bahri, D., Jiang, H., Tay, Y., and Metzler, D. Scarf: Selfsupervised contrastive learning using random feature corruption. In International Conference on Learning Representations, 2022. Bartlett, P. L., Montanari, A., and Rakhlin, A. Deep learning: a statistical viewpoint. Acta numerica, 30:87\u2013201, 2021. Breiman, L. Random forests. Machine learning, 45:5\u201332, 2001. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020. Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785\u2013794, 2016. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020.\nChen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang, J.-B. A closer look at few-shot classification. In International Conference on Learning Representations, 2018. Creswell, A., Shanahan, M., and Higgins, I. Selectioninference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, 2022. Dinh, T., Zeng, Y., Zhang, R., Lin, Z., Gira, M., Rajput, S., Sohn, J.-y., Papailiopoulos, D., and Lee, K. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:11763\u201311784, 2022. Fan, Z., Gao, X., Mirchev, M., Roychoudhury, A., and Tan, S. H. Automated repair of programs from large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1469\u2013 1481. IEEE, 2023. Finch, S. E., Paek, E. S., and Choi, J. D. Leveraging large language models for automated dialogue analysis. In Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pp. 202\u2013215, 2023. Golovenkin, S.E., S. V. R. D. S. P. N. S. O. Y. and Voino-Yasenetsky, V. Myocardial infarction complications. UCI Machine Learning Repository, 2020. DOI: https://doi.org/10.24432/C53P5M. Hegselmann, S., Buendia, A., Lang, H., Agrawal, M., Jiang, X., and Sontag, D. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pp. 5549\u20135581. PMLR, 2023. Ho, T. K. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition, volume 1, pp. 278\u2013282. IEEE, 1995. Hollmann, N., M\u00a8uller, S., Eggensperger, K., and Hutter, F. Tabpfn: A transformer that solves small tabular classification problems in a second. In The Eleventh International Conference on Learning Representations, 2023. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Imani, S., Du, L., and Shrivastava, H. Mathprompter: Mathematical reasoning using large language models. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023. Jakobsen, J. C., Gluud, C., Wetterslev, J., and Winkel, P. When and how should multiple imputation be used for\nhandling missing data in randomised clinical trials\u2013a practical guide with flowcharts. BMC medical research methodology, 17(1):1\u201310, 2017. Kadra, A., Lindauer, M., Hutter, F., and Grabocka, J. Welltuned simple nets excel on tabular datasets. Advances in neural information processing systems, 34:23928\u201323941, 2021. Kang, H. The prevention and handling of the missing data. Korean journal of anesthesiology, 64(5):402\u2013406, 2013. Kova\u02c7c, G., Sawayama, M., Portelas, R., Colas, C., Dominey, P. F., and Oudeyer, P.-Y. Large language models as superpositions of cultural perspectives. arXiv preprint arXiv:2307.07870, 2023. Kuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u2013 97, 1955. Levin, R., Cherepanova, V., Schwarzschild, A., Bansal, A., Bruss, C. B., Goldstein, T., Wilson, A. G., and Goldblum, M. Transfer learning with deep tabular models. In The Eleventh International Conference on Learning Representations, 2022. Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. A. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35: 1950\u20131965, 2022. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, 2022. Majmundar, K. A., Goyal, S., Netrapalli, P., and Jain, P. Met: Masked encoding for tabular data. In NeurIPS 2022 First Table Representation Workshop, 2022. Majumder, S., Chen, C., Al-Halah, Z., and Grauman, K. Few-shot audio-visual learning of environment acoustics. Advances in Neural Information Processing Systems, 35: 2522\u20132536, 2022. Moro, S., Cortez, P., and Rita, P. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22\u201331, 2014.\nMajumder, S., Chen, C., Al-Halah, Z., and Grauman, K. Few-shot audio-visual learning of environment acoustics. Advances in Neural Information Processing Systems, 35: 2522\u20132536, 2022.\nMoro, S., Cortez, P., and Rita, P. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22\u201331, 2014.\nNam, J., Song, W., Park, S. H., Tack, J., Yun, S., Kim, J., and Shin, J. Semi-supervised tabular classification via incontext learning of large language models. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023a.\n# OpenAI. Gpt-4 technical report, 2023.\nRedmond, M. Communities and Crime. Machine Learning Repository, 2009. https://doi.org/10.24432/C53W3X.\nUCI DOI:\nUCI DOI:\nSanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022.\nSchick, T. and Sch\u00a8utze, H. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255\u2013269, 2021.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al. Large language models encode clinical knowledge. Nature, 620(7972):172\u2013180, 2023.\nSlack, D. and Singh, S. Tablet: Learning from instructions for tabular data. arXiv preprint arXiv:2304.13188, 2023.\nSomepalli, G., Goldblum, M., Schwarzschild, A., Bruss, C. B., and Goldstein, T. Saint: Improved neural networks for tabular data via row attention and contrastive pretraining. arXiv preprint arXiv:2106.01342, 2021.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.\nUcar, T., Hajiramezanali, E., and Edwards, L. Subtab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34:18853\u201318865, 2021.\nVan Buuren, S. and Groothuis-Oudshoorn, K. mice: Multivariate imputation by chained equations in r. Journal of statistical software, 45:1\u201367, 2011.\nWang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022. Wang, Z. and Sun, J. Transtab: Learning transferable tabular transformers across tables. Advances in Neural Information Processing Systems, 35:2902\u20132915, 2022. Wang, Z., Gao, C., Xiao, C., and Sun, J. Anypredict: Foundation model for tabular prediction. arXiv preprint arXiv:2305.12081, 2023. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837, 2022b. Wilcox, A. B. and Hripcsak, G. The role of domain knowledge in automating medical text report classification. Journal of the American Medical Informatics Association, 10(4):330\u2013338, 2003. Yeh, I.-C., Yang, K.-J., and Ting, T.-M. Knowledge discovery on rfm model using bernoulli sequence. Expert Systems with applications, 36(3):5866\u20135871, 2009. Yoon, J., Zhang, Y., Jordon, J., and van der Schaar, M. Vime: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033\u201311043, 2020. Zhang, T., Wang, S., Yan, S., Li, J., and Liu, Q. Generative table pre-training empowers models for tabular prediction. arXiv preprint arXiv:2305.09696, 2023. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zhu, B., Shi, X., Erickson, N., Li, M., Karypis, G., and Shoaran, M. Xtab: Cross-table pretraining for tabular transformers. arXiv preprint arXiv:2305.06090, 2023.\n# A. Examples of Prompts and Their Corresponding Outputs\nA.1. Task Description for Each Dataset\nThis section demonstrates creation of prompts for rule extraction for each task, along with the task descriptions we use (see Section 3.1 - Basic Information Description). The descriptions of the tasks are developed by referring to the descriptions in the original data links and previous works (Hegselmann et al., 2023), as outlined in the table below. Table 7: Description of the tasks associated with each dataset utilized in the prompt. The text in blue describes the label information, providing the list of possible answer classes.\nThis section demonstrates creation of prompts for rule extraction for each task, along with the task descriptions we use (se Section 3.1 - Basic Information Description). The descriptions of the tasks are developed by referring to the descriptions i the original data links and previous works (Hegselmann et al., 2023), as outlined in the table below.\nData\nTask description\nAdult\nDoes this person earn more than 50000 dollars per year? Yes or no?\nBank\nDoes this client subscribe to a term deposit? Yes or no?\nBlood\nDid the person donate blood? Yes or no?\nCar\nHow would you rate the decision to buy this car? Unacceptable, acceptable, good or very good?\nCommunities\nHow high will the rate of violent crimes per 100K population be in this area. Low, medium, or high?\nCredit-g\nDoes this person receive a credit? Yes or no?\nDiabetes\nDoes this patient have diabetes? Yes or no?\nHeart\nDoes the coronary angiography of this patient show a heart disease? Yes or no?\nMyocardial\nDoes the myocardial infarction complications data of this patient show chronic heart failure? Yes or no?\nCultivars\nHow high will the grain yield of this soybean cultivar. Low or high?\nNHANES\nPredict this person\u2019s age group from the given record. Senior or non-senior?\nSequence-type\nWhat is the type of following sequence? Arithmetic, geometric, fibonacci, or collatz?\nSolution-mix\nGiven the volumes and concentrations of four solutions, does the percent concentration of the mixed\nsolution over 0.5? Yes or no?\n# A.2. Example Prompt for Extracting Rules\nYou are an expert. Given the task description and the list of features and data examples, you are extracting conditions for each answer class to solve the task. Task: Does the coronary angiography of this patient show a heart disease? Yes or no? Features: - Age: age of the patient (numerical variable) - Sex: sex of the patient (categorical variable with categories [M, F]) - ChestPainType: chest pain type (categorical variable with categories [ATA, NAP, ASY, TA]) - RestingBP: resting blood pressure [mm Hg] (numerical variable) - Cholesterol: serum cholesterol [mm/dl] (numerical variable) - FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise] (numerical variable) - RestingECG: resting electrocardiogram results (categorical variable with categories [Normal, ST, LVH]) - MaxHR: maximum heart rate achieved (numerical variable) - ExerciseAngina: exercise-induced angina (categorical variable with categories [N, Y]) - Oldpeak: oldpeak = ST [Numeric value measured in depression] (numerical variable) - ST Slope: the slope of the peak exercise ST segment (categorical variable with categories [Up, Flat, Down]) Examples: Age is 63. Sex is M. ChestPainType is NAP. RestingBP is 130. Cholesterol is 0. FastingBS is 1. RestingECG is ST. MaxHR is 160. ExerciseAngina is N. Oldpeak is 3.0. ST Slope is Flat. Answer: no Age is 39. Sex is M. ChestPainType is ATA. RestingBP is 120. Cholesterol is 204. FastingBS is 0. RestingECG is Normal. MaxHR is 145. ExerciseAngina is N. Oldpeak is 0.0. ST Slope is Up. Answer: no Age is 58. Sex is M. ChestPainType is NAP. RestingBP is 160. Cholesterol is 211. FastingBS is 1. RestingECG is ST. MaxHR is 92. ExerciseAngina is N. Oldpeak is 0.0. ST Slope is Flat. Answer: yes Age is 55. Sex is M. ChestPainType is ASY. RestingBP is 160. Cholesterol is 289. FastingBS is 0. RestingECG is LVH. MaxHR is 145. ExerciseAngina is Y. Oldpeak is 0.8. ST Slope is Flat. Answer: yes\n- Oldpeak: oldpeak = ST [Numeric value measured in depression] (numerical variable) - ST Slope: the slope of the peak exercise ST segment (categorical variable with categories [Up, Flat, Down]) Examples: Age is 63. Sex is M. ChestPainType is NAP. RestingBP is 130. Cholesterol is 0. FastingBS is 1. RestingECG is ST. MaxHR is 160. ExerciseAngina is N. Oldpeak is 3.0. ST Slope is Flat. Answer: no Age is 39. Sex is M. ChestPainType is ATA. RestingBP is 120. Cholesterol is 204. FastingBS is 0. RestingECG is Normal. MaxHR is 145. ExerciseAngina is N. Oldpeak is 0.0. ST Slope is Up. Answer: no Age is 58. Sex is M. ChestPainType is NAP. RestingBP is 160. Cholesterol is 211. FastingBS is 1. RestingECG is ST. MaxHR is 92. ExerciseAngina is N. Oldpeak is 0.0. ST Slope is Flat. Answer: yes Age is 55. Sex is M. ChestPainType is ASY. RestingBP is 160. Cholesterol is 289. FastingBS is 0. RestingECG is LVH. MaxHR is 145. ExerciseAngina is Y. Oldpeak is 0.8. ST Slope is Flat. Answer: yes\nFormat for [Condition]: For the categorical variable only, - [Feature] is in [List of categories] For the numerical variable only, - [Feature] (> or >= or < or <=) [Value] - [Feature] is within range of [Value start, Value end]\nFigure 7: Example prompt for rule extraction in the \u2018Heart\u2019 dataset\n# A.3. Example Outcome from Rule Extraction\nWe demonstrate how rules are derived through prompts by showing an example of an outcome. If we input the prompt from the previous subsection (see Fig. 7), the following result is obtained.\nStep 1. The relationship between each feature and the task description: - Age: There is a tendency for older age to be associated with a higher risk of heart disease. - Sex: There may be a difference in the prevalence of heart disease between males and females, with males potentially having a higher risk. - ChestPainType: Certain types of chest pain, such as NAP and ASY, may be indicative of heart disease. - RestingBP: Higher resting blood pressure levels may be associated with a higher risk of heart disease. - Cholesterol: Higher levels of serum cholesterol may be associated with a higher risk of heart disease. - FastingBS: Fasting blood sugar levels above 120 mg/dl may be indicative of heart disease. - RestingECG: Abnormal resting electrocardiogram results, such as ST and LVH, may be indicative of heart disease. - MaxHR: Lower maximum heart rate achieved may be associated with a higher risk of heart disease. - ExerciseAngina: The presence of exercise-induced angina may be indicative of heart disease. - Oldpeak: Higher values of oldpeak (ST depression) may be associated with a higher risk of heart disease. - ST Slope: The slope of the peak exercise ST segment may provide information about the presence of heart disease.\n# A.4. Example Prompt for Parsing Rules\nUtilizing the given rules, we convert them into code capable of transforming each data point into a binary feature. This binary feature indicates whether the sample adheres to the rules associated with its class. Below is an example prompt for parsing rules, employing the same example as in the previous subsection.\nWrap only the function part with <start> and <end>, and do not add any comments, descriptions, and package importing lines in the code.\nFigure 9: Example prompt for parsing rules in the \u2018Heart\u2019 dataset\n# A.5. Example Outcome After Parsing\nThe result of the parsing request is exemplified as follows.\n<start> def extracting features no(df input): df output = pd.DataFrame() df output[\u2018Age\u2019] = df input[\u2018Age\u2019].apply(lambda x: 1 if x >= 40 and x <= 60 else 0) df output[\u2018Sex\u2019] = df input[\u2018Sex\u2019].apply(lambda x: 1 if x == \u2018F\u2019 else 0) df output[\u2018ChestPainType\u2019] = df input[\u2018ChestPainType\u2019].apply(lambda x: 1 if x in [\u2018ATA\u2019, \u2018NAP\u2019] else 0) df output[\u2018RestingBP\u2019] = df input[\u2018RestingBP\u2019].apply(lambda x: 1 if x < 140 else 0) df output[\u2018Cholesterol\u2019] = df input[\u2018Cholesterol\u2019].apply(lambda x: 1 if x < 200 else 0) df output[\u2018FastingBS\u2019] = df input[\u2018FastingBS\u2019].apply(lambda x: 1 if x == 0 else 0) df output[\u2018RestingECG\u2019] = df input[\u2018RestingECG\u2019].apply(lambda x: 1 if x == \u2018Normal\u2019 else 0) df output[\u2018MaxHR\u2019] = df input[\u2018MaxHR\u2019].apply(lambda x: 1 if x > 140 else 0) df output[\u2018ExerciseAngina\u2019] = df input[\u2018ExerciseAngina\u2019].apply(lambda x: 1 if x = \u2018N\u2019 else 0) df output[\u2018Oldpeak\u2019] = df input[\u2018Oldpeak\u2019].apply(lambda x: 1 if x < 1.0 else 0)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afc5/afc583c4-3f97-46b5-abe7-45b74fec21be.png\" style=\"width: 50%;\"></div>\n# B. Hyper-parameter analysis\nFeatLLM has two main hyper-parameters. One is the number of ensembles, and the other is how many rules per class to extract during each LLM query. We conducted experiments to investigate the impact of these hyper-parameters on performance. The results showed that performance improves as the number of ensembles increases, but beyond a certain point (i.e., around 20), the performance gains begin to converge (see Fig. 11). For an analysis of the impact of the number of rules, please refer to Section 4.2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bfcc/bfcc76d1-8fd8-478d-a183-07cd9a442b42.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 11: Effect of the number of ensembles. Averaged AUC over all datasets is reported.\n# C. Baseline Implementation Details\nIn this section, we provide a detailed information on the implementation of baselines. In our study, we adhere to the settings and parameters outlined in the original papers for various baselines. During model training, the dataset was divided allocating 20% as the test set. This division was carried out using stratified sampling to ensure a representative distribution of classes. From the remaining data, k-shot samples were selected, constructing a balanced training set. We repeated experiments multiple times with different randomness on the choice of training/test sets. For conventional machine learning methods, including logistic regression (LogReg), RandomForest, and XGBoost, we determined their parameters using Grid-search combined with k-shot cross validation. Following the previous work (Hegselmann et al., 2023), the search space of each parameter is described in Tables below.\nParameter\nSearch space\npenalty\nl1, l2\nC\n100, 10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5\nTable 8: Hyperparameter search space for LogReg\nTable 9: Hyperparameter search space for RandomForest\nParameter\nSearch space\nmax depth\n2, 4, 6, 8, 10, 12\nalpha\n1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1\nlambda\n1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1\neta\n0.01, 0.03, 0.1, 0.3\n<div style=\"text-align: center;\">Table 10: Hyperparameter search space for XGBoost</div>\nFor baseline models that incorporate unlabeled data in training, such as SCARF and STUNT, any data not used in the test set or as k-shot training samples is treated as the unlabeled dataset. To ensure that there are no missing values in the training and test data, columns representing more than 20% of the entire dataset were removed, similarly following the literature (Kang, 2013; Jakobsen et al., 2017). Subsequently, any rows containing missing values were also eliminated. For TabPFN, we utilized the official GitHub repository7 to reproduce results, maintaining the default parameter settings as specified. Regarding LLM-based baselines, we utilized GPT-3.5 for in-context learning, while T0 was chosen for the TabLLM, in line with their original usage settings. To calculate the AUC for the in-context learning and TABLET with the GPT-3.5 API, we performed the inference process three times, introducing variability with a nonzero temperature setting (i.e., 1). The results were then averaged to estimate the probabilities for each class. In contrast, for the TabLLM, we employed T0, utilizing its publicly available checkpoint. We directly calculated the probability of the class name token to estimate the class probabilities.\nParameter\nSearch space\nbootstrap\nTrue, False\nmax depth\n2, 4, 6, 8, 10, 12\nn estimators\n2, 4, 8, 16, 32, 64, 128, 256\n# D. Diversity of Features Generated by FeatLLM\nIn our model, to guarantee diversity of features (i.e., rules) generated from the LLM in each iteration, we set the temperature of LLMs to a value greater than 0 (i.e., 0.5) to introduce randomness into the generation, applied bagging strategies (i.e., feature and sample bagging), and shuffled the order of training samples in the prompt for each iteration. To empirically verify the diversity of features, we conducted the following analysis: 1. Given two sets of rules from different iterations, we applied each set\u2019s rules across the dataset to generate binary features. 2. We used the Hungarian Maximum Matching Algorithm (Kuhn, 1955) to create a bipartite graph connecting pairs of binary features between the two sets with the highest correlation. 3. The average correlation of the selected pairs was calculated. 4. This process (steps 1-3) was repeated for all pairs of iterations to determine the average and variance of the correlations.\nIn our model, to guarantee diversity of features (i.e., rules) generated from the LLM in each iteration, we set the temperature of LLMs to a value greater than 0 (i.e., 0.5) to introduce randomness into the generation, applied bagging strategies (i.e., feature and sample bagging), and shuffled the order of training samples in the prompt for each iteration. To empirically verify the diversity of features, we conducted the following analysis:\n1. Given two sets of rules from different iterations, we applied each set\u2019s rules across the dataset to generate binary features. 2. We used the Hungarian Maximum Matching Algorithm (Kuhn, 1955) to create a bipartite graph connecting pairs of binary features between the two sets with the highest correlation. 3. The average correlation of the selected pairs was calculated. 4. This process (steps 1-3) was repeated for all pairs of iterations to determine the average and variance of the correlations.\nWe conducted an ablation study to assess the effects of three components (i.e., temperature, bagging, instance shuffling) on enhancing rule diversity. Table 11 below presents the results of average correlation across iterations, where a lower correlation signifies a more diverse set of extracted rules. We observed that the average correlation",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of tabular learning, which is crucial for many real-world applications. Previous methods using Large Language Models (LLMs) for tabular data have limitations, such as requiring one inference per sample, which is computationally expensive, and often necessitating fine-tuning of the LLM for high accuracy. These constraints hinder the practical application of LLMs in tabular learning tasks.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of efficiently utilizing LLMs for few-shot tabular learning without the need for extensive fine-tuning or multiple inferences per sample.",
            "key obstacle": "The main difficulty lies in the computational expense and inefficiency of existing methods that require LLM inference for each sample, as well as the limitations imposed by prompt size and the need for model training."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is to leverage the LLM's ability to generate features rather than using it for direct predictions, thus reducing computational costs and improving efficiency.",
            "opinion": "The proposed idea, FeatLLM, involves using LLMs to engineer features that are tailored for tabular predictions, enabling a simple downstream model to infer class likelihoods.",
            "innovation": "The innovation of FeatLLM lies in its ability to generate binary features from rules extracted by the LLM, allowing for predictions without requiring the LLM for each sample, thus overcoming prompt size limitations and reducing inference costs."
        },
        "method": {
            "method name": "FeatLLM",
            "method abbreviation": "FL",
            "method definition": "FeatLLM is a framework that utilizes LLMs to generate features for tabular data, which are then used in a simple predictive model for class likelihood estimation.",
            "method description": "FeatLLM extracts rules from LLMs to create binary features that indicate whether rules are satisfied by data samples.",
            "method steps": [
                "Understand the problem and infer relationships between features and targets.",
                "Generate rules based on the inferred relationships and few-shot examples.",
                "Transform the extracted rules into binary features for data samples.",
                "Fit a simple model to estimate class probabilities from the binary features.",
                "Employ an ensemble approach to enhance robustness and mitigate prompt size challenges."
            ],
            "principle": "The effectiveness of FeatLLM in solving the problem stems from its ability to leverage LLMs' prior knowledge and reasoning capabilities to generate informative features, which simplifies the downstream modeling process."
        },
        "experiments": {
            "evaluation setting": "The evaluation was conducted across 13 different tabular datasets, including both binary and multi-class classification tasks, with a focus on low-shot learning scenarios.",
            "evaluation method": "Performance was assessed by comparing the results of FeatLLM against several baseline methods, measuring metrics such as AUC across different datasets and shot configurations."
        },
        "conclusion": "FeatLLM demonstrates significant advancements in few-shot tabular learning by efficiently utilizing LLMs for feature generation, achieving high performance with minimal inference costs and without extensive model training.",
        "discussion": {
            "advantage": "The key advantages of FeatLLM include reduced computational costs, the ability to handle long prompts, and high-quality feature generation that enhances prediction accuracy.",
            "limitation": "A limitation of the method is that it may not perform optimally in scenarios where the prior knowledge encoded in the LLM is not relevant to the specific task at hand.",
            "future work": "Future research directions include expanding FeatLLM's capabilities to handle larger datasets, exploring diverse feature types beyond rules, and improving interpretability for broader applications."
        },
        "other info": {
            "info1": "The code for FeatLLM is available via an anonymized GitHub link.",
            "info2": {
                "info2.1": "The framework utilizes GPT-3.5 as its backbone but is agnostic to other LLMs.",
                "info2.2": "The method is particularly beneficial in domains like finance and healthcare where labeled data is scarce."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of tabular learning, which is crucial for many real-world applications."
        },
        {
            "section number": "1.2",
            "key information": "The significance of in-context learning is highlighted by the limitations of previous methods using LLMs for tabular data, which hinder practical applications."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea, FeatLLM, utilizes LLMs to engineer features tailored for tabular predictions, demonstrating the role of LLMs in enhancing in-context learning."
        },
        {
            "section number": "1.4",
            "key information": "FeatLLM reduces computational costs and improves efficiency by generating features rather than using LLMs for direct predictions."
        },
        {
            "section number": "2.1",
            "key information": "The historical context includes challenges in efficiently utilizing LLMs for few-shot tabular learning without extensive fine-tuning."
        },
        {
            "section number": "3.1",
            "key information": "FeatLLM's framework allows LLMs to adapt to various contexts by generating binary features from rules extracted by the LLM."
        },
        {
            "section number": "4.1",
            "key information": "Effective prompt design is crucial as it reduces inference costs and enhances prediction accuracy in the context of the FeatLLM method."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of FeatLLM is that it may not perform optimally when the prior knowledge encoded in the LLM is not relevant to the specific task."
        },
        {
            "section number": "6.2",
            "key information": "The method addresses computational costs by allowing for predictions without requiring the LLM for each sample."
        }
    ],
    "similarity_score": 0.7343134188616547,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning.json"
}