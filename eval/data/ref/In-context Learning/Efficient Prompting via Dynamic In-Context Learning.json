{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.11170",
    "title": "Efficient Prompting via Dynamic In-Context Learning",
    "abstract": "The primary way of building AI applications is shifting from training specialist models to prompting generalist models. A common practice for prompting generalist models, often referred to as in-context learning, is to append a few examples (demonstrations) to the prompt to help the model better understand the task. While effective, in-context learning can be inefficient because it makes the input prompt much longer, consuming valuable space in the context window and leading to larger computational costs. In this paper, we propose DYNAICL, a recipe for efficient prompting with black-box generalist models that dynamically allocate in-context examples according to the input complexity and the computational budget. To achieve this, we train a meta controller that predicts the number of in-context examples suitable for the generalist model to make a good prediction based on the performance-efficiency trade-off for a specific input. We then dynamically allocate the number of demonstrations for an input according to predictions from the meta controller and the given computation budget. Experimental results show that dynamic example allocation helps achieve a better performance-efficiency trade-off in two practical settings where computational resources or the required performance is constrained. Specifically, DYNAICL saves up to 46% token budget compared to the common practice that allocates the same number of in-context examples to each input. We also find that a meta controller trained on a certain backbone model and tasks can successfully generalize to unseen models and tasks.",
    "bib_name": "zhou2023efficientpromptingdynamicincontext",
    "md_text": "# Efficient Prompting via Dynamic In-Context Learning\n# Wangchunshu Zhou Yuchen Eleanor Jiang Ryan Cotterell Mrinmaya Sachan\nwangchunshu.zhou, yuchen.jiang, ryan.cotterell, mrinmaya.sachan}@inf.ethz.ch\n# Abstract\nThe primary way of building AI applications is shifting from training specialist models to prompting generalist models. A common practice for prompting generalist models, often referred to as in-context learning, is to append a few examples (demonstrations) to the prompt to help the model better understand the task. While effective, in-context learning can be inefficient because it makes the input prompt much longer, consuming valuable space in the context window and leading to larger computational costs. In this paper, we propose DYNAICL, a recipe for efficient prompting with black-box generalist models that dynamically allocate in-context examples according to the input complexity and the computational budget. To achieve this, we train a meta controller that predicts the number of in-context examples suitable for the generalist model to make a good prediction based on the performance-efficiency trade-off for a specific input. We then dynamically allocate the number of demonstrations for an input according to predictions from the meta controller and the given computation budget. Experimental results show that dynamic example allocation helps achieve a better performance-efficiency trade-off in two practical settings where computational resources or the required performance is constrained. Specifically, DYNAICL saves up to 46% token budget compared to the common practice that allocates the same number of in-context examples to each input. We also find that a meta controller trained on a certain backbone model and tasks can successfully generalize to unseen models and tasks.\nThe field of Artificial Intelligence is witnessing a major paradigm shift from training and deploying multiple specialist models for specific tasks to pre-training a generalist model (e.g., a large language model (LLM)) and prompting for different tasks [1\u20138]. While prompting is an elegant and effective way to utilize generalist models, the issue of computational inefficiency remains a major limitation. We identify two key sources of the computational inefficiency of prompting generalist models: model size and sample size. The former is arguably a prerequisite for generalist models to solve all kinds of tasks via prompting and there already exist a number of model compression techniques [9\u201312] that aim to reduce the size of generalist models. One obvious limitation of these approaches is that they all require users to have access to the model parameters, which may not be the case in the era of generalist models. For instance, some state-of-the-art generalist models like ChatGPT, Bard, PaLM, and Claude, are pre-trained by corporations and therefore closed-sourced for commercial reasons. In this paper, we instead focus on reducing sample size, a relatively new perspective for improving the efficiency of black-box generalist models of which the parameters are unavailable to users. This particular direction has received relatively limited or negligible exploration within the era of specialist models, as the inputs and outputs associated with it are clearly defined and largely devoid of redundancy. This is no longer true in the context of prompting generalist models such as LLMs\nPreprint. Work In Progress.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2adf/2adf6ad0-d280-45de-b945-8427b7ee434c.png\" style=\"width: 50%;\"></div>\nbecause we have a lot of different ways to prompt a model that results in prompts of different lengths. We identify the main factor influencing the prompt length to be the use of in-context learning and the number of in-context examples (demonstrations) in the prompt. Specifically, in-context learning [3] refers to the practice of adding a few exemplar input-output pairs that are related to the input, which helps the generalist model better understand and solve the problem. Although it is still unclear how in-context examples help a generalist model [13\u201315], it is evident that samples of greater complexity necessitate a greater number of in-context examples for a generalist model to acquire contextual understanding. Conversely, simpler samples may be solvable even without relying on in-context learning. This is confirmed by our preliminary study, which also finds that assigning more in-context examples to simple samples occasionally confuses the generalist model and turns its prediction from correct to erroneous. This suggests that the current practice of allocating a fixed number of in-context examples for all inputs is sub-optimal. To this end, we propose Dynamic In-Context Learning (DYNAICL), a dynamic computation framework for prompting generalist models. DYNAICL is conceptually similar to previous work on input adaptive computation for specialist models [16\u201321]. The main difference is that DYNAICL aims to dynamically adjust the size of the input while previous work focuses on adjusting the complexity of the model. This results in a major advantage of DYNAICL: it only operates on inputs, thus is disentangled with model architectures or parameters, and suits an increasingly common scenario in the era of generalist models where the users do not have access to the model\u2019s parameters. To achieve this, we train a meta controller that predicts the number of in-context examples suitable for the generalist model to make a good performance-efficiency trade-off given a specific input. The meta controller can be instantiated with a smaller pre-trained model (e.g., FLAN-T5 [22]) and multi-task fine-tuned with the combination of supervised learning with a novel data synthesis algorithm and reinforcement learning with rewards based on performance-efficiency trade-off. Then at test time, we can dynamically allocate the number of demonstrations for an input according to both the predictions from the meta controller and the computation budget. We illustrate the procedure of efficient prompting with DYNAICL in Figure 1. We test the effectiveness of DYNAICL in the context of prompting LLMs due to its prominence as the predominant use case for generalist models at present. We experiment with ChatGPT as the generalist model and train a meta controller on a subset of the FLAN datasets collection [23]. We evaluate DYNAICL in two practical settings where either the computational resources or the performance is under constraints. We find that compared with the common practice of uniformly allocating in-context examples, DYNAICL can achieve an averaged absolute performance improvement of 2.6% within a certain computational budget or reach a certain performance requirement with up to 46% less compute (in terms of total token consumption) across 8 tasks. We also find that a meta controller trained on certain tasks with a certain generalist model (i.e., ChatGPT) can generalize well to unseen tasks (even with different output formats) and other generalist models (e.g., LLAMA [8]). To the best\nof our knowledge, our work is among the first approaches that can accelerate a black-box generalist model without access to its parameters.\n# 2 Methodology\n# 2.1 Background: Prompting and In-Context Learning\nWe first recall some basics of prompting and in-context learning. Prompting refers to the process of providing a prompt, which typically contains a description of the task and the task input, to a generalist model that guides its response generation. Formally, let G be a generalist model and P be a prompt. Then, the output O is given by: O = G(P). Prompting relies on the generalist model\u2019s ability to understand and follow abstract instructions, which sometimes leads to unsatisfactory empirical performance, especially for hard tasks that require complex reasoning. On the other hand, in-context learning leverages the ability of a generalist model to adapt to new information provided within the input context. Formally, given N labeled examples {(xi, yi)}N i=1 and a hand-crafted template T , in-context learning first verbalizes each input-output pair with a template, resulting in demonstrations di = T (xi, yi). Then the generalist model takes the concatenation of the original prompt and the demonstrations to generate the output:\nO = G(P \u2295d1 \u2295d2 \u2295\u00b7 \u00b7 \u00b7 \u2295dN) where \u2295denotes the concatenation of token sequences.\n# 2.2 Meta Controller\nArchitecture and Input/Output Formats: The meta controller C can be modeled by any sequence generation model including both encoder-decoder models and decoder-only models. We use an instruction-tuned model such as FLAN-T5 as the backbone for the meta controller to facilitate training. As illustrated in Figure 1, it receives a task instruction and an input, which is identical to most instruction tuning literature [24, 22, 25]. But instead of generating the corresponding outputs like instruction-tuned models, our meta controller is trained to generate the number of in-context examples suitable for the input to achieve the best performance-efficiency trade-off, which we denote as k. This process can be expressed by k = C(P). The output expresses the confidence modeling of the meta controller for the generalist model to some extent. This method pertains to, albeit distinguishes itself from, prior existing work on model calibration [26, 27], which addresses the inherent confidence levels of the model itself. Training We then present our two-stage training framework for the meta controller. In the first stage, we train the meta controller to predict the minimum number of in-context examples for the generalist model to produce a good output. \u201cA good output\u201d can have different definitions for different tasks. For example, it can be defined as predicting the correct label with a high probability for classification tasks and generating outputs similar to the ground truth for generation tasks. In this paper, we consider only classification tasks following [28, 29]. To synthesize training data for supervised training, we propose a simple and intuitive data generation method. Specifically, for a prompt P, we consider the minimum number of in-context examples k\u2217for it to be the number that makes the generalist model\u2019s expected accuracy exceed a certain (hand-crafted) threshold t:\nk\u2217= min k\u2208N \ufffd k \ufffd\ufffd\ufffdE(xi1,yi1)...(xik ,yik ))\u223cDk [Acc(G(P, T (x1, y1) \u2295\u00b7 \u00b7 \u00b7 \u2295T (xk, yk)))] > t \ufffd\n\ufffd \ufffd\ufffd\ufffd \ufffd where Dk denotes all subsets of the training data of size k. We synthesize (P, k\u2217) pairs on a mixture of instruction-tuning datasets from the FLAN collection and train the meta controller with maximum likelihood estimation.\n(1)\n(2)\nwe define the reward R to be a linear interpolation of the expected performance (defined as accuracy in case of classification task), and the efficiency, defined as the number of in-context examples k:\nwhere \u03b1 is the weight controlling whether the controller should lean towards better performance o efficiency. The meta controller C is then fine-tuned with policy gradient:\nwhere P is the set of prompts from a mixture of instruction tuning datasets, b is the baseline calculated as the moving average of previous rewards, and C(k|P, \u03b8) denotes the predicted probability mass of k from the meta controller C for a prompt P. The training framework can be easily adapted for generation tasks by changing the accuracy metric to some generation metrics such as BLEU [30] or BERTScore [31], and doing some normalization to make it compatible with classification tasks. We leave this for future work.\nwhere P is the set of prompts from a mixture of instruction tuning datasets, b is the baseline calculated as the moving average of previous rewards, and C(k|P, \u03b8) denotes the predicted probability mass of k from the meta controller C for a prompt P.\nThe training framework can be easily adapted for generation tasks by changing the accuracy metric to some generation metrics such as BLEU [30] or BERTScore [31], and doing some normalization to make it compatible with classification tasks. We leave this for future work.\n# 2.3 Dynamic In-Context Example Allocation\nAfter training, the meta controller predicts the number of in-context examples for a specific input. This is a naive version of DYNAICL. However, in practice one may have a different computation budget. Therefore it is often desirable to normalize the predictions from the meta controller and dynamically adjust the actual number of in-context examples according to the computation budget. In this work, we propose a simple recipe for dynamic in-context example allocation. Assuming we have a budget of N tokens1 for K samples. The uniform baseline is to allocate N/(K \u00b7 L) in-context examples for each sample assuming L is the average length of an example. DYNAICL instead allocates E in-context examples for an input P following:\nwhere C(P) is the prediction from the meta controller, [] denotes the rounding operator, \u02dcC is the averaged prediction for all examples, and \u03b2 is the token saving ratio ranging from 0 to 1.\n# 3 Experiments\nIn this section, we test the empirical effectiveness of DYNAICL by experimenting on some NLP tasks with ChatGPT, a popular large language model, as the generalist model. We first describe the experimental settings. Then we begin with a preliminary study about the impact of the number of in-context examples to motivate our approach. After that, we evaluate DYNAICL by answering two research questions for two realistic settings:\n\u2022 RQ1: To what extent can DYNAICL improves the performance of a generalist model with fixed computational budgets? \u2022 RQ2: To what extent can DYNAICL reduce computational cost or token consumption for a generalist model to achieve a fixed target performance?\n# 3.1 Experimental Settings\nModels We consider ChatGPT as the generalist model for training the meta controller and the main experiments. We use LLAMA-65B as an unseen generalist model for evaluating the generalization ability of the meta controller. We use FLAN-T5-large, which has less than 1B parameters, to initialize the meta controller. We also test with FLAN-T5-base in the analysis.\nTasks We use a subset in the FLAN collection containing 30+ classification tasks to train the meta controller. For evaluation, we test DYNAICL on both seen and unseen tasks, which are explicitly excluded from the training data for the meta controller. To be specific, we use SST-2 [32] AGNews [33], RTE [34\u201337], CB [38], ARC-E [39], ARC-C [39], MRPC [40], and COPA [41] as\n(4)\n(5)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/335b/335bd525-2eb6-40b6-a3c0-3168b27fdff7.png\" style=\"width: 50%;\"></div>\nFigure 2: Distribution of the number of incontext examples that suffice for making the correct prediction for samples that cannot be answered correctly by zero-shot inference with generalist models but can be solved with in-context learning for up to 10 shots. The generalist model we consider are ChatGPT and LLAMA-65B, and the dataset is CSQA.\nthe seen tasks, and PIQA [42], OpenBookQA [43], CommonsenseQA [44], TriviaQA [45], Natural Questions [46], and Web Questions [47] as unseen tasks. It is noteworthy that TriviaQA, Natural Questions, and Web Questions are not classification tasks but a trained meta controller can still be used despite being trained only on classification tasks. This is because its input format (i.e., instruction + input) is agnostic to the type of the task. Training Details We follow Wei et al. [22] and fine-tune the meta controller for 30k/5k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer [48] with a learning rate of 3e-5/1e-5, for the first/second training stage, respectively. Baselines We mainly compare DYNAICL with the uniform baseline that allocates the same number of in-context examples for each sample, and the random baseline that randomly samples a number of in-context examples from a Gaussian distribution. We only compare these two naive baselines\nBaselines We mainly compare DYNAICL with the uniform baseline that allocates the same number of in-context examples for each sample, and the random baseline that randomly samples a number of in-context examples from a Gaussian distribution. We only compare these two naive baselines because there is no prior work in this direction and popular methods for efficient NLP can not be applied in this setting.\n# 3.2 Preliminary Study: How Much Do More In-Context Examples Help?\nWe first conduct a preliminary study investigating the role of adding more in-context examples to the prompt for different samples. We first test if most samples for a task require a similar amount of in-context examples for a generalist model to generate a good output. We plot the distribution of the number of in-context examples that suffice for making the correct prediction for samples from the CommonsenseQA dataset that cannot be answered correctly by zero-shot inference with ChatGPT or LLAMA-65B but can be solved with in-context learning for up to 10 shots. As shown in Figure 2, different samples requires a very different amount of in-context examples. Some hard examples require 10 in-context examples for a generalist model to make the correct prediction while most examples require only one in-context example or can be solved with zero-shot inference. This observation confirms the necessity of dynamically allocating in-context examples according to sample difficulties. Moreover, we can see that ChatGPT and LLAMA-65B share similar trends in the Figure. This suggests that a meta controller trained with one generalist model may be able to generalize to other generalist models, which is later proved in our analysis. Then we further analyze the effect of scaling more in-context examples. As shown in Figure 3, the effectiveness of adding more in-context examples to the prompt is amortized when there are already a few (e.g., 5) in-context examples. This also supports our motivation that only a few samples require many in-context examples and uniformly allocating an equal number of in-context examples for all samples is a waste of tokens and computation. More interestingly, we find that sometimes it can be\n\u2206Accuracy\n\u0017\u2192\u0013\n\u0013\u2192\u0017\nzero-shot \u21921-shot\n+ 2.5%\n3.9%\n1.4%\n1-shot \u21925-shots\n+ 1.4%\n1.9%\n0.5%\n5-shots \u219264-shots\n+ 0.3%\n0.7%\n0.4%\nFigure 3: The impact of adding more incontext examples. \u2206Accuracy denotes the change of accuracy after adding more incontext examples. \u0017\u2192\u0013 and \u0013\u2192\u0017 denotes the percentage of examples of which the predictions are changed from incorrect to correct, and vice versa, after adding more in-context examples. We use ChatGPT as the generalist model and TriviaQA as the dataset.\nModels\nSST-2\nAGNews\nRTE\nCB\nARC-E\nARC-C\nMRPC\nCOPA\nAvg. Acc\nzero-shot\nChatGPT\n88.5\n84.5\n84.5\n89.5\n85.1\n61.0\n88.4\n67.2\n81.1\nBudget: 5-shots on average\nUniform\n93.2\n87.9\n86.1\n91.1\n88.3\n64.8\n90.4\n88.2\n86.2\nRandom\n93.0\n87.7\n86.1\n91.0\n88.1\n65.0\n90.4\n89.4\n86.3\nDYNAICL\n95.3\n90.2\n88.1\n92.9\n90.5\n68.4\n91.8\n93.0\n88.8\nBudget: 10-shots on average\nUniform\n95.8\n90.9\n88.5\n93.1\n90.8\n68.3\n92.0\n93.4\n89.1\nRandom\n95.9\n90.7\n88.4\n93.3\n90.8\n68.2\n92.1\n92.8\n88.9\nDYNAICL\n96.7\n92.5\n90.0\n94.1\n91.9\n70.0\n93.1\n95.8\n90.5\nTable 1: Main results on seen tasks during meta controller training. The total computation/token budget is the same inside each group. DYNAICL consistently outperforms all baselines across all tasks under different computation/token budgets.\nharmful to include more in-context examples for a sample that can already be correctly solved by the generalist model, which is shown by a non-negligible amount of samples\u2019 predictions are changed from correct to incorrect after adding more in-context examples. This further confirms the potential of DYNAICL to achieve better performance while consuming fewer tokens.\n# 3.3 Main Results\nWe first compare the performance of DYNAICL with the baselines in Table 1. We can see that DYNAICL leads to an averaged performance improvement of 2.6% and 1.4% over the uniform baseline with budgets of 5 and 10 in-context examples for each sample, respectively. This confirms that DYNAICL leads to improved performance with fixed budgets. We also plot the trend of averaged performance on seen tasks with different token-saving ratios in Figure 4 (a). We can see that DYNAICL leads to consistent improvements across all budgets and the improvements are larger when the computation/token budget is more limited. We then show the extent to which DYNAICL can save tokens for achieving a fixed target performance in Figure 4 (b). We can see that DYNAICL consistently require fewer tokens to match the performance achieved by the uniform baseline with certain budgets. Specifically, DYNAICL only consumes 108 tokens on average to match the performance of the common practice with 200 tokens on average. This confirms that DYNAICL can effectively reduce token/computation consumption for achieving a fixed target performance.\n# 3.4 Analysis\nWe then conduct an analysis investigating the impact of different components in DYNAICL and the generalization ability of DYNAICL on unseen tasks or generalist models when training the meta controller.\nWe then conduct an analysis investigating the impact of different components in DYNAICL and the generalization ability of DYNAICL on unseen tasks or generalist models when training the meta\nAblation Study We first analyze the impact of the two training stages, the size of the meta controller, and the number of tasks the meta controller is trained with. The results are shown in Table 2. We find that both training stages contributes to the performance of DYNAICL and the first stage is more important. We think this is because the first training stage provides an important starting point for the second stage using reinforcement learning. We also find that DYNAICL with a smaller meta controller or a meta controller train on fewer tasks also achieves competitive performances. Generalization on Unseen Tasks We then test how well DYNAICL can generalize on unseen tasks. The results are shown in Table 3. We find that DYNAICL consistently leads to performance improvements across all 6 unseen tasks. Notably, DYNAICL also leads to substantial improvements on Natural Questions and Web Questions, which are generative question answering datasets that are very different from text classification tasks during training. This confirms that DYNAICL can generalize well on tasks that are not used to train the meta controller.\nAblation Study We first analyze the impact of the two training stages, the size of the meta controller, and the number of tasks the meta controller is trained with. The results are shown in Table 2. We find that both training stages contributes to the performance of DYNAICL and the first stage is more important. We think this is because the first training stage provides an important starting point for the second stage using reinforcement learning. We also find that DYNAICL with a smaller meta controller or a meta controller train on fewer tasks also achieves competitive performances.\nGeneralization on Unseen Tasks We then test how well DYNAICL can generalize on unseen tasks. The results are shown in Table 3. We find that DYNAICL consistently leads to performance improvements across all 6 unseen tasks. Notably, DYNAICL also leads to substantial improvements on Natural Questions and Web Questions, which are generative question answering datasets that are very different from text classification tasks during training. This confirms that DYNAICL can generalize well on tasks that are not used to train the meta controller.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/35c8/35c87a68-6778-46d1-b241-d527f9b99bc8.png\" style=\"width: 50%;\"></div>\nFigure 4: Performance of DYNAICL on settings where either the computation (token) budget is fixed (a) or the target performance is fixed (b).\nModels\nSST-2\nAGNews\nRTE\nCB\nARC-E\nARC-C\nMRPC\nCOPA\nAvg. Acc\nBudget: 5-shots on average\nUniform\n93.2\n87.9\n86.1\n91.1\n88.3\n64.8\n90.4\n88.0\n86.2\nDYNAICL\n95.3\n90.2\n88.1\n92.9\n90.5\n68.4\n91.8\n93.0\n88.8\n- first stage\n93.8\n88.4\n86.6\n91.8\n89.1\n65.5\n90.8\n89.6\n86.9\n- second stage\n94.4\n89.5\n87.5\n92.1\n89.5\n67.1\n91.2\n91.4\n87.8\nw/ smaller model\n94.8\n89.2\n87.5\n92.3\n90.2\n67.7\n91.3\n92.2\n88.2\nw/ fewer tasks\n95.0\n89.3\n87.3\n92.5\n90.0\n68.0\n91.5\n92.4\n88.3\nTable 2: Ablation study results. \u201c- first stage\u201d and \u201c- second stage\u201d denotes the ablated variants where the meta controller is not trained with the first or second stage training, respectively. \u201cw/ smaller model\u201d and \u201cw/ fewer tasks\u201d denotes the ablated variants where the meta controller is parameterized with FLAN-T5-Base and the meta controller is trained with 50% less training tasks.\nGeneralization on Unseen Generalist Models We also test if DYNAICL can generalize to other generalist models that are not used for training the meta controller by applying the meta controller trained with ChatGPT with LLAMA-65B as the generalist model. Results in Figure 5 (a) show that DYNAICL still saves a great number of tokens for achieving the same performance with the uniform baseline even tested with a different generalist model. This confirms that DYNAICL can generalize well on generalist models that are not used to train the meta controller. Distribution of In-context Examples Count We then plot the distribution of samples according to the number of in-context examples allocated for them to better understand the meta controller. As shown in Figure 5 (b), with a target budget of 5 in-context examples, a large portion of samples are allocated with 5 in-context examples in DYNAICL. This indicates that most samples are predicted to need a similar number of in-context examples as the averaged prediction. We also find that more samples are assigned with fewer than 5 in-context examples while a few hard samples are assigned with more in-context examples. We present a qualitative study of different samples and the corresponding number of in-context examples allocated to them in the Appendix. Computation Cost of the Meta Controller Finally, it is noteworthy that the meta controller does add some computational cost and latency overhead to the overall prompting procedure. However, since the meta controller can use a very small backbone such as T5-large or T5-base, its computation cost is negligible compared to that of a generalist model. To be specific, the computational cost (in\nGeneralization on Unseen Generalist Models We also test if DYNAICL can generalize to other generalist models that are not used for training the meta controller by applying the meta controller trained with ChatGPT with LLAMA-65B as the generalist model. Results in Figure 5 (a) show that DYNAICL still saves a great number of tokens for achieving the same performance with the uniform baseline even tested with a different generalist model. This confirms that DYNAICL can generalize well on generalist models that are not used to train the meta controller.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e265/e26596a3-6414-4242-96bc-a62dce68bce2.png\" style=\"width: 50%;\"></div>\n(b) Token saving ratio of DYNAICL compared to the uniform baseline under performance constraints, which are defined by the performance of the uniform baseline with different token budgets. Each point (x,y) in the line indicates that on average, DYNAICL needs to use y tokens to match the performance of the uniform baseline with x tokens.\nModels\nPIQA\nOBQA\nCSQA\nTriviaQA (EM)\nNaturalQ (EM)\nWebQS (EM)\nAvg.\nzero-shot\nChatGPT\n83.3\n60.9\n74.5\n80.2\n27.5\n22.9\n58.2\nBudget: 5-shots on average\nUniform\n84.3\n61.5\n76.6\n84.1\n37.1\n26.3\n61.6\nDYNAICL\n85.4\n62.8\n77.2\n84.4\n40.2\n28.8\n63.1\nBudget: 10-shots on average\nUniform\n85.9\n63.1\n77.4\n84.3\n40.8\n29.2\n63.4\nDYNAICL\n86.3\n63.7\n77.9\n84.5\n42.4\n29.9\n64.1\nTable 3: Analysis of the generalization ability of DYNAICL on datasets that are unseen when training the meta controller. Tasks with (EM) suffix denotes the task is generative question answering and we use exact match as the metric. DYNAICL still consistently outperforms the baseline across all tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f2a/1f2a975c-9860-4579-995b-003b86bd82df.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Token saving ratio of DYNAICL compared to the uniform baseline under different performance constraints on seen tasks. DYNAICL is trained with ChatGPT but tested with LLAMA-65B.</div>\n<div style=\"text-align: center;\">Figure 5: Analysis on the generalization ability of DYNAICL on unseen generalist models and the distribution of samples according to the number of in-context examples allocated for them.</div>\nFigure 5: Analysis on the generalization ability of DYNAICL on unseen generalist models and t distribution of samples according to the number of in-context examples allocated for them.\nterms of FLOPs) of a T5-large based meta controller for a sample of 50 tokens is less than 0.1% of the change of the computation cost when changing the input from 200 tokens to 199 tokens, or less than 0.0005% of the computational cost saved by reducing one in-context example from the prompt. Similarly, since the meta controller only needs to predict 1 or 2 tokens, the latency overhead accounts for only 0.1% to 0.2% of the latency of calling the GPT-3.5-turbo API, and reducing one in-context example will lead to a speedup of around 10%. In sum, we believe the computational and latency overhead from the meta controller is almost negligible.\n# 4 Related Works\n# 4.1 Generalist Models, Prompting, and In-context Learning\nTraining a generalist model that can solve a wide range of tasks without task-specific training has been a long-standing goal in the field of artificial intelligence. One pioneering work dates back to Collobert and Weston [49] that attempted to solve all NLP tasks with a shared architecture using multi-task learning. This idea is further improved by decaNLP [50] that proposes to convert all NLP tasks to question answering format. T5 [51] then improves this paradigm by using text-to-text format for unifying all NLP tasks, which is more general and friendly to scaling. Finally, GPT-3 [3] show that by scaling model size, training data, and training FLOPs, a large language model can serve as a generalist model that solves many tasks by simply writing a prompt that describes the task and the input. They also showed that the zero-shot ability of a large language model can be further improved\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4911/49118d48-8a76-4ef0-85c8-8f1d84a5eb60.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Distribution of samples (on seen tasks) according to the number of in-context examples allocated for them. The computational budget is fixed to 5 in-context examples per sample.</div>\nby adding a few input-output demonstrations in the prompt to help the model better understand the task. Since then, a large number of work has been done for improving and understanding prompting and in-context learning with large language models. For instance, Schick and Sch\u00fctze [52] show that small encoder models can also be prompted. Min et al. [13] show that in-context examples mainly help a generalist model learn output label space and distribution of input text. Kadavath et al. [27] prove that generalist models are well calibrated and can be trained to model their confidence level. Hao et al. [28] and Li et al. [29] show that in-context learning with many examples improves the overall performance of a generalist model. Recently, the paradigm of prompting generalist models is successfully transferred to other modalities other than language. For example, Zhou et al. [53] and Li et al. [54] explored prompting vision models. It is foreseeable that prompting generalist models will become the de-facto paradigm for most domains in artificial intelligence.\n# 4.2 Efficient Deep Learning\nImproving the performance-efficiency trade-off of deep learning models is a very interesting research problem and is attracting more and more attention since the rise of large generalist models [55, 56]. A large number of work has been done on improving the speed and efficiency of large language models, including both static methods such as knowledge distillation [57, 58, 9], pruning [59, 10], quantization [60, 61, 11] and module replacing [12]; and dynamic methods such as adaptive computation [17], early-exiting [18\u201320], and model cascade [62, 63]. However, most of the aforementioned methods require access to the model parameters, which may not be possible in the era of generalist models since most state-of-the-art generalist models such as ChatGPT and PaLM are closed-sourced. One potential exemption is model cascade, which first sends the input to a cheaper model and optionally sends it to a more powerful model if the previous model is not confident enough. This method, however, also faces server latency issues because harder samples will be computed by multiple generalist models sequentially. Modarressi et al. [64] proposed dynamic input reduction methods to reduce the length of the input. However, their approach requires access to the model parameters and needs to modify the model architecture and additional training. Concurrently to our work, Mu et al. [65] proposes to train gist tokens to replace long-form prompts and show promising results on prompt compression. However, this approach is still limited to white-box settings where the model parameter is available and also compromises interpretability. Our approach instead focuses on the black-box setting and can be applied to any closed-sourced generalist models.\n# 5 Limitations\nAs for technical limitations, the main limitation of this work is that we only test DYNAICL on NLP tasks with LLMs as the backbone, while it may also be interesting to test on other modalities such as vision tasks with multi-modal generalist models. This is because the main experiments are conducted before multi-modal instruction following models such as LLAVA came out. We leave this for future work. Another limitation is that we only train the meta controller with text classification datasets. We explain how the meta controller can be trained on generation tasks at the end of Section 2.2. We also experiment with some generative question answering datasets and show DYNAICL trained only on classification tasks can successfully transfer to these tasks. Finally, the dynamic in-context example allocation algorithm is quite naive. Potential improvements may be made using some more sophisticated planning or optimization algorithms. We also leave this for future work. As for social impact, this work aims to reduce the token/computation consumption of prompting generalist models. It probably leads to a positive environmental impact and will unlikely lead to any negative social impact.\n# 6 Conclusions\nThis paper introduces DYNAICL, a framework for efficiently prompting generalist models. We propose to train a meta controller that predicts the suitable number of in-context examples for a specific sample with a two-stage training framework. During inference, DYNAICL dynamically allocate different number of in-context examples to samples according to the predictions from the meta controller and the given computational budget. Our experiments show that DYNAICL consistently\nleads to better performance-efficiency trade-offs across tasks and generalist models that are either seen or unseen when training the meta controller. This work is among the first that investigates how to prompt generalist models more efficiently even they are only available through API calls and will hopefully shed light on this direction.\n# References\n1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=TG8KACxEON.\n[5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=TG8KACxEON.\n# 6] OpenAI. Gpt-4 technical report, 2023.\n[7] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [9] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020. [10] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf. [11] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD.\n[7] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[40] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002. [41] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90\u201395, 2011. [42] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In AAAI, pages 7432\u20137439. AAAI Press, 2020. [43] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381\u20132391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260. [44] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u2013 4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. [45] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. [46] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. [47] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1160. [48] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost, 2018. [49] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, page 160\u2013167, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605582054. doi: 10.1145/1390156.1390177. URL https://doi.org/10.1145/1390156.1390177. [50] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering, 2018. [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020. [52] Timo Schick and Hinrich Sch\u00fctze. It\u2019s not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u2013 2352, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.185. URL https://aclanthology.org/2021.naacl-main.185.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of computational inefficiency in prompting generalist models for AI applications, particularly focusing on the limitations of in-context learning and the need for a method that dynamically allocates in-context examples based on input complexity and computational budget.",
        "problem": {
            "definition": "The problem involves the inefficient use of computational resources when prompting generalist models, as traditional methods allocate a fixed number of in-context examples regardless of input complexity.",
            "key obstacle": "The main challenge is that simpler samples may not require as many in-context examples, and over-allocating can confuse the model, leading to suboptimal performance."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that different inputs require varying amounts of contextual information for effective model performance.",
            "opinion": "The proposed idea, DYNAICL, entails a dynamic allocation of in-context examples based on a meta controller's predictions, which balances performance and efficiency.",
            "innovation": "DYNAICL differentiates itself by dynamically adjusting the number of in-context examples based on the specific input and computational constraints, as opposed to static allocation methods."
        },
        "method": {
            "method name": "Dynamic In-Context Learning",
            "method abbreviation": "DYNAICL",
            "method definition": "DYNAICL is a framework that dynamically allocates in-context examples for generalist models based on input complexity and a predefined computational budget.",
            "method description": "The method allows for flexible prompting by predicting the optimal number of in-context examples needed for each input.",
            "method steps": [
                "Train a meta controller to predict the number of in-context examples based on input characteristics.",
                "At inference, use the meta controller's predictions to allocate in-context examples dynamically according to the computational budget."
            ],
            "principle": "The effectiveness of DYNAICL lies in its ability to tailor the amount of contextual information to the complexity of the input, thereby optimizing the performance-efficiency trade-off."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using ChatGPT as the generalist model on a variety of classification tasks, comparing DYNAICL against uniform and random baselines under different computational budgets.",
            "evaluation method": "Performance was assessed by measuring accuracy improvements and token savings achieved by DYNAICL compared to baseline methods."
        },
        "conclusion": "The experiments demonstrate that DYNAICL significantly enhances the performance-efficiency trade-off in prompting generalist models, achieving better results with fewer computational resources.",
        "discussion": {
            "advantage": "DYNAICL's key advantage is its ability to adaptively allocate in-context examples, improving model performance and reducing token consumption compared to traditional methods.",
            "limitation": "A limitation of this work is that DYNAICL has only been tested on NLP tasks, and its applicability to other modalities remains unexplored.",
            "future work": "Future research may focus on extending DYNAICL to other task types, refining the dynamic allocation algorithm, and exploring its use in multi-modal settings."
        },
        "other info": {
            "additional details": {
                "meta controller": "The meta controller is trained using a two-stage framework that includes supervised learning and reinforcement learning to optimize performance-efficiency.",
                "generalization": "DYNAICL has shown the ability to generalize well across unseen tasks and different generalist models."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses computational inefficiency in prompting generalist models, highlighting the limitations of traditional in-context learning methods."
        },
        {
            "section number": "1.4",
            "key information": "The proposed method, DYNAICL, emphasizes the critical role of dynamically allocating in-context examples based on input complexity and computational budget."
        },
        {
            "section number": "3.1",
            "key information": "DYNAICL allows for flexible prompting by predicting the optimal number of in-context examples needed for each input, enhancing adaptation to various contexts."
        },
        {
            "section number": "3.4",
            "key information": "The effectiveness of DYNAICL lies in its ability to tailor the amount of contextual information to the complexity of the input, optimizing the performance-efficiency trade-off."
        },
        {
            "section number": "6.1",
            "key information": "The main challenge identified is that simpler samples may not require as many in-context examples, and over-allocating can confuse the model, leading to suboptimal performance."
        },
        {
            "section number": "6.4",
            "key information": "A limitation of DYNAICL is that it has only been tested on NLP tasks, and its applicability to other modalities remains unexplored."
        }
    ],
    "similarity_score": 0.7159796709331223,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Efficient Prompting via Dynamic In-Context Learning.json"
}