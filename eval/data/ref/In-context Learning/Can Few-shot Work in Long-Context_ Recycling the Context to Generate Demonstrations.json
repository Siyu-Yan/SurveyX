{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.13632",
    "title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations",
    "abstract": "Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains suboptimal. In-Context Learning (ICL) with fewshot examples may be an appealing solution to enhance LLM performance in this scenario; However, na\u00efvely adding ICL examples with long context introduces challenges, including substantial token overhead added for each fewshot example and context mismatch between the demonstrations and the target query. In this work, we propose to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements (+16 absolute points on average across models) on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.",
    "bib_name": "cattan2024fewshotworklongcontextrecycling",
    "md_text": "# Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations\nArie Cattan1,2* Alon Jacovi2 Alex Fabrikant3 Jonathan Herzig2 Roee Aharoni2 Hannah Rashkin3 Dror Marcus2 Avinatan Hassidim2 Yossi Matias2 Idan Szpektor2 Avi Caciularu2 1Bar-Ilan University 2Google Research 3Google DeepMind cattana@google.com\nAbstract\n# Abstract\nDespite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains suboptimal. In-Context Learning (ICL) with fewshot examples may be an appealing solution to enhance LLM performance in this scenario; However, na\u00efvely adding ICL examples with long context introduces challenges, including substantial token overhead added for each fewshot example and context mismatch between the demonstrations and the target query. In this work, we propose to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements (+16 absolute points on average across models) on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.\n18 Oct 2024\n[cs.CL\n# 1 Introduction\nLong contexts are prevalent in various domains, ranging from legal documents and scientific articles to lengthy reports and novels. These may consist of a single extensive document or multiple passages,\n*Work done during an internship at Google.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d662/d662b1ed-979e-43ab-974f-dd259019b3ec.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 1: Performance of Gemini Flash (v1.5) on a sample of the Lost-in-the-middle dataset (Liu et al., 2023). The X-axis is the position of the relevant passage in the context. The baseline (blue line) displays a Ushaped curve, performing well only when the relevant passage is at the beginning or end of the input. The oracle (green line) shows significant performance gain when the relevant passage ID is provided in the prompt, showing that the identification of supporting evidence(s) is a major challenge. DOUBLEDIPPER (our method, orange line) flattens this U-shaped trend.\ntypically retrieved through specific retrieval mechanisms (e.g., RAG; Lewis et al., 2020). Yet, while Large Language Models (LLMs) have demonstrated impressive capabilities in a variety of tasks including answering questions requiring one or multiple reasoning steps, they often struggle to answer simple questions when faced with long contexts. Despite substantial engineering efforts (Chen et al., 2023) to extend the context window of LLMs to extremely long inputs (32k and even 1M tokens), these models continue to struggle with much shorter inputs, comprising only a few thousand tokens. In order to answer questions from long inputs, models should implicitly identify relevant information segments and then reason over these segments to formulate an answer. It has been shown that LLMs struggle when the relevant information is\nburied in the middle of the context (Liu et al., 2023) or obscured by numerous irrelevant details (Levy et al., 2024). Our analysis (illustrated in Figure 1) identifies the identification of relevant information as a major performance bottleneck of current models in long contexts. In this work, we introduce a novel method to enhance the QA performance of LLMs in long input setups (to allow direct comparisons across a wide swath of models, we limit \"long context\" here to 1-3k tokens). Our approach, termed DOUBLEDIPPER, leverages LLMs\u2019 In-Context Learning (ICL) capability and is based on two principles. First, instead of typical ICL, where each few-shot example is standalone with a separate length context and a question-answer (QA) pair, we propose to recycle the given input context and automatically generate few-shot examples from this context. Specifically, we randomly select a few paragraphs from the given input context and generate QA pairs for each passage. These generated QAs serve as demonstration examples and are placed between the input context and the target input question. Figure 2 illustrates the differences between the traditional ICL with few-shot examples and DOUBLEDIPPER. Second, we further enrich each ICL example to instruct the model to explicitly identify the paragraph(s) containing the relevant information before generating the answer. This can be regarded as a structured Chain of Thought that incentivizes the model to pinpoint relevant information before reasoning, an essential capability for long-context processing. By generating few-shot demonstrations from various sections of the input context while instructing the model to identify relevant passages, DOUBLEDIPPER encourages the model to develop deeper reading comprehension skills specific to the given input evidence. This, in turn, allows the model to answer subsequent queries with higher accuracy. DOUBLEDIPPER presents several advantages. In terms of efficiency, since each example does not include its own input context, our method adds to the original prompt a minimal number of tokens, resulting in a substantially cheaper inference than traditional ICL. Additionally, recycling the same context for ICL demonstrations ensures that the few-shot examples refer to the same domain as the input question, thus obviating the need for external retrieval processes. Finally, DOUBLEDIPPER generates answers with attribution to relevant paragraphs, improving the model\u2019s lookup ability\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5d5a/5d5ae0db-130f-48ea-ab72-a96561a56732.png\" style=\"width: 50%;\"></div>\nFigure 2: Comparison of traditional In-ContextLearning (ICL) and our new method. In traditional ICL (left), each example comprises a possibly lengthy context, accompanied by a query and an answer, typically derived from the training dataset. Conversely, our approach (right) simplifies each example to just a question and an answer, both of which are generated directly from the provided input context.\nand offering transparency, which substantially simplifies human evaluation (Slobodkin et al., 2024). We applied DOUBLEDIPPER to a variety of LLMs, both commercial (Gemini Pro, Nano and Flash; Reid et al., 2024) and open-source (Llama 3.1 (Dubey et al., 2024), Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024) and Gemma (Riviere et al., 2024)), and evaluate it on various QA datasets with long inputs, including common multihop QA datasets. Our experiments demonstrate that with only 3 self-generated few-shot examples, DOUBLEDIPPER consistently outperforms the baseline on our evaluation set by 16 absolute points on average across models. In addition, for some models, DOUBLEDIPPER enhances the robustness to the position of the relevant information within the text. Interestingly, while our few-shot examples focus on single-paragraph answers, DOUBLEDIPPER generalizes well to multi-hop QAs and where the answer requires information from multiple passages.\n# 2 Background\nChallenges in Long Context for Language Modeling. LLMs have been well-documented to struggle when input length grows (An et al., 2023), and especially so when it exceeds input lengths seen during training (Anil et al., 2022). Various methods have been proposed to advance long-context\ncapabilities: Architectural, e.g., to augment the embedding layer to cleverly extrapolate to unseen lengths (Vaswani et al., 2017; Press et al., 2021; Caciularu et al., 2022; Hsieh et al., 2024); and via data, e.g., to incorporate longer inputs and more challenging long-context scenarios into training (He et al., 2023; Chen et al., 2023). However, this challenging problem stubbornly remains in competitive models today (Liu et al., 2023; Bishop et al., 2023; Levy et al., 2024). In contrast to the above methods, DOUBLEDIPPER does not involve training or architectural changes. In documenting and exploring LLM performance in long-context settings, many different benchmarks targeting it have been proposed, such as Scrolls and Zero-Scrolls (Shaham et al., 2022, 2023), Loogle (Li et al., 2023), LongBench (Bai et al., 2023), inter alia. The problem of designing informative and reliable benchmarks in longcontext is an an active, ever-changing area of research (Goldman et al., 2024; Yen et al., 2024). We describe the most relevant evaluation benchmarks used in this work in Section 4.\nIn-Context Learning The area of in-context learning (ICL) is a class of prompting techniques where demonstrations are added to the prompt in order to steer or improve model behavior (Min et al., 2022a). Typically, in-context learning involves hand-crafted demonstrations (Song et al., 2022), automatic retrieval of demonstrations from a larger set (Paranjape et al., 2023), or instructing the model to perform various tasks one after another in a pipeline (Gao et al., 2022). Recent improvements in long-context capabilities of LLMs have also had effect on improving the yield from incontext learning by simply using more short-length demonstrations (Agarwal et al., 2024). While such methods are widely used for their effectiveness (Brown et al., 2020b; an Luo et al., 2024), they remain under-explored in settings of long-context. The reason is simple: If the context is already extremely long, adding additional demonstrations comparable in length to the input context will likely amplify the existing limitations of long context handling (Li et al., 2024c). More related to our work, a few recent studies propose to prompt LLMs for automatically generating in-context demonstrations for various short context tasks (Kim et al., 2022; Yasunaga et al., 2024; Li et al., 2024a,b). However, applying those methods to long contexts is not straightforward because\nLLMs would need to generate long demonstrations. In this work, we tackle these challenges and present a novel ICL method that recycles the given long context and further instructs models to identify the relevant information before generating the answer.\n# 3 DOUBLEDIPPER\nRecall that our work focuses on the task of question answering (QA) with long input context comprising multiple paragraphs. In addition to the answer, we aim to identify the supporting paragraphs in order to provide attribution. Formally, given a long input text D composed of n paragraphs D = {p1, p2, ..., pn} and a question q, the goal is to generate the answer a and identify the set(s) of paragraphs that support the answer S = {s1, ..., sk}. The number of the supporting paragraphs is not known in advance and can be one or more. We describe DOUBLEDIPPER, an efficient method for improving the performance of large language models (LLMs) when dealing with long contexts. The core principles of DOUBLEDIPPER involve: (1) recycling the input context to automatically generate few-shot examples, and (2) \u201cteaching\u201d the model via in-context learning (ICL) to explicitly pinpoint the supporting paragraphs before generating the answer. Figure 3 illustrates DOUBLEDIPPER. Starting with the input paragraphs D, we initially select k paragraphs at random (e.g., paragraphs 15, 5, and 17, for k := 3). For each chosen paragraph, we prompt the model to formulate a question that pertains to the specific paragraph, accompanied by an appropriate answer (for further details on prompt specifications, refer to Appendix A). Each generated QA pair is directly associated with its origin paragraph, enabling us to assemble the following structured in-context demonstration, shown as the DOUBLEDIPPER block in Figure 3:\nQuestion : qi Evidence : pi Answer : ai\nHere, pi indicates the index of the paragraph associated with the QA pair (qi, ai). Given a test question q, we then form a QA prompt by concatenating the original input context D, the compiled demonstrations and q. The model first generates the one or more indices of the supporting paragraph(s), followed by the answer.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b2a1/b2a1514f-c3fb-4836-a924-1a6dc5857268.png\" style=\"width: 50%;\"></div>\nWho was in charge of the state where Graft-De Rijp is located? Evidence: [6, 18] Answer: Johan Remkes\nFigure 3: Example of DOUBLEDIPPER applied to the MuSique dataset. Given 20 passages as input, DOUBLEDIPPER randomly selects 3 passages (specifically passages 15, 5, 17) and automatically generates a question-pair for each one. As each QA is associated with its respective paragraph, we form the demonstrations to instruct the model to identify the relevant passage(s) and the correct answer.\nUnlike traditional few shot examples that instruct the model about a specific task, DOUBLEDIPPER aims to coach the model on how to \u201chandle\u201d the specific input context. This is achieved by guiding the model to explicitly localize relevant information before generating the answer. Also, by randomly sampling multiple paragraphs from the input, DOUBLEDIPPER guarantees that the ICL demonstrations involve reading different parts of the context, allowing the model to better comprehend the input text. Beyond improving the performance of the QA task, instructing the model to provide the supporting paragraphs is valuable on its own as it offers transparency and substantially eases human evaluation (Slobodkin et al., 2024). DOUBLEDIPPER offers several advantages. First, as each example in the demonstration consists only of a question, an answer and the ID of relevant passage, the number of added tokens due to the extra demonstrations is minimal, leading to a low additional cost and computation compared to the traditional In-Context-Learning. Furthermore, by reusing the same context to generate demonstrations, our approach guarantees that all few shot examples are derived from the exact same domain\nUnlike traditional few shot examples that instruct the model about a specific task, DOUBLEDIPPER aims to coach the model on how to \u201chandle\u201d the specific input context. This is achieved by guiding the model to explicitly localize relevant information before generating the answer. Also, by randomly sampling multiple paragraphs from the input, DOUBLEDIPPER guarantees that the ICL demonstrations involve reading different parts of the context, allowing the model to better comprehend the input text. Beyond improving the performance of the QA task, instructing the model to provide the supporting paragraphs is valuable on its own as it offers transparency and substantially eases human evaluation (Slobodkin et al., 2024).\nDOUBLEDIPPER offers several advantages. First, as each example in the demonstration consists only of a question, an answer and the ID of relevant passage, the number of added tokens due to the extra demonstrations is minimal, leading to a low additional cost and computation compared to the traditional In-Context-Learning. Furthermore, by reusing the same context to generate demonstrations, our approach guarantees that all few shot examples are derived from the exact same domain\nSize\nContext Window\nGemini v1.5\nNano\n32k\nFlash\n32k\nPro\n32k\nLlama 3.1\n7B\n128k\nMistral v0.3\n7B\n32k\nMixtral v0.1\n7x8B\n32k\nGemma 2\n2B\n8k\n9B\n8k\n<div style=\"text-align: center;\">Table 1: Models used with DOUBLEDIPPER.</div>\nTable 1: Models used with DOUBLEDIPPER.\nas the input query (Rubin et al., 2022). Finally, we observe that, although the QA pairs in the demonstration are confined to individual paragraphs, the actual query q may require reasoning over multiple paragraphs (i.e., multi-hop QA). Surprisingly, LLMs can generalize from DOUBLEDIPPER local examples to these complex, global questions and successfully generate indices to multiple paragraphs (see Section 5).\n# 4 Experiments\nDatasets We apply our method to various datasets, each presenting its own domain-specific\nDataset\n# Instances\nAvg. # tokens\nLost-in-the-middle\n2,500\n2,815\nFLenQA\n1,500\n3,225\nHotpotQA\n500\n1,646\n2Wiki\n500\n1,222\nMuSiQue\n500\n2,549\n<div style=\"text-align: center;\"># Instances Avg. # tokens</div>\nTable 2: Evaluation datasets in our experiments. The average number of tokens is computed according to Gemma\u2019s tokenization of the simple prompt.\nchallenges. We selected these datasets because the supporting paragraphs are also annotated. Overall our evaluation set includes 5.5K instances, with statistics of each dataset given in Table 2. The Lost-in-the-middle dataset (Liu et al., 2023) includes examples from NaturalQuestionsOpen (Kwiatkowski et al., 2019; Lee et al., 2019). Each instance consists of twenty Wikipedia passages, with only one passage containing the answer to the query. The remaining passages are distractors that are lexically similar but do not contain the answer. To assess the robustness of large language models (LLMs) to the position of relevant information, Liu et al. (2023) evaluated cases where the relevant passage appeared in positions 1, 5, 10, 15, and 20. Following their methodology, we sampled 500 instances for each position, resulting in a total of 2,500 instances. FLenQA (Levy et al., 2024) is a benchmark that includes simple questions with answers of either \u201cTrue\u201d or \u201cFalse\u201d based on two key sentences. FLenQA includes three subtasks. The first subtask is Monotone Relations (MonoRel), where each instance asks whether a transitive relation between two entities holds based on the context (e.g., \"Is X younger than Y?\" based on the sentences \"X is younger than Z\" and \"Z is younger than Y\"). The second subtask, People In Rooms (PIR), involves one key sentence indicating that a person is in a specific room and another key sentence describing a property of this room. The question asks whether the person is in a room with the described property. The final subtask is Simplified Rule Taker (SRT), based on RuleTaker (Clark et al., 2020). Each instance consists of a logical rule, two sentences each introducing a fact, and a question over the rule and facts. For each subtask, FLenQA includes contexts with varying lengths, from 50 to 3,000 tokens, by simply adding irrelevant text, demonstrating consistent performance degradation with increased\ninput length. In our experiments, we sampled 250 instances for each subtask with input lengths of 2,000 and 3,000 tokens, leading to a total of 1,500 instances for FLenQA. In addition, we evaluate our method on common multi-hop QA benchmarks. We sampled 500 instances from each of the following datasets: HotPotQA (Yang et al., 2018), 2Wiki (Ho et al., 2020), and MuSiQue (Trivedi et al., 2021). In all these datasets, the input text includes multiple passages, and models need to perform at least two steps of reasoning over different passages in order to answer the question. Models We apply DOUBLEDIPPER to a variety of models, both commercial and open-source. The commercial models include Gemini 1.5 Pro, Gemini 1.5 Flash and Gemini 1.0 Nano (Reid et al., 2024). The open-source models we tested are Llama 3.1 8B (Dubey et al., 2024), Gemma 2B (v2) and Gemma 9B (v2) (Riviere et al., 2024), Mistral-7B-Instruct (v0.2) (Jiang et al., 2023) and Mixtral-8x7B-Instruct (v0.1) (Jiang et al., 2024). Details about models\u2019 size and context window are shown in Table 1. Few-shot generation in DOUBLEDIPPER is an auxiliary task and should ideally run in an efficient time without requiring heavy resources. Therefore, in our main experiments, we employ Gemma 2B to generate the demonstrations at it is the smallest and most efficient model used in our experiments. See Section 6 for an ablation analysis of the effect of the chosen model for generating the demonstrations. Baselines We compare DOUBLEDIPPER to the vanilla baseline, which takes as input the entire context D and the query q and generates only the answer a. We also compare to Zero-shot + Evidence Retrieval, which prompts the model in a zero-shot setting to identify the relevant passage(s) before generating the answer. Evaluation We evaluate each dataset with the original evaluation metrics. Namely, we report Accuracy for Lost-in-the-middle (Liu et al., 2023) and FLenQA (Levy et al., 2024), and Token F1 for HotPotQA (Yang et al., 2018), 2Wiki (Ho et al., 2020) and MuSique (Trivedi et al., 2021). In addition to the task\u2019s accuracy, we also evaluate the performance of the identification of the supporting paragraph(s), by computing the F1 score on the predicted set of supporting passages compared to the ground truth (Yang et al., 2018; Ho et al.,\nEvaluation We evaluate each dataset with the original evaluation metrics. Namely, we report Accuracy for Lost-in-the-middle (Liu et al., 2023) and FLenQA (Levy et al., 2024), and Token F1 for HotPotQA (Yang et al., 2018), 2Wiki (Ho et al., 2020) and MuSique (Trivedi et al., 2021). In addition to the task\u2019s accuracy, we also evaluate the performance of the identification of the supporting paragraph(s), by computing the F1 score on the predicted set of supporting passages compared to the ground truth (Yang et al., 2018; Ho et al.,\nAvg.\n2Wiki\nMonoRel\nPIR\nSRT\nHotPotQA\nLost\nMuSique\nGemini Pro (vanilla)\n60.5\n24.9\n95.0\n97.6\n64.4\n46.7\n71.6\n23.1\nZero-shot + Evidence Retrieval\n62.3\n32.5\n94.6\n95.8\n62.4\n49.6\n74.8\n26.5\nDOUBLEDIPPER\n70.4\n46.8\n97.4\n99.0\n79.6\n60.9\n72.4\n36.4\nGemini Flash (vanilla)\n42.9\n10.2\n70.0\n86.0\n57.6\n10.0\n59.5\n7.3\nZero-shot + Evidence Retrieval\n58.2\n30.2\n78.8\n90.6\n65.0\n44.9\n67.4\n30.2\nDOUBLEDIPPER\n66.1\n48.0\n85.8\n95.0\n68.6\n60.6\n65.0\n39.7\nGemini Nano (vanilla)\n41.6\n10.8\n72.2\n66.8\n55.4\n21.3\n59.6\n5.2\nZero-shot + Evidence Retrieval\n56.5\n32.0\n82.4\n82.4\n56.4\n56.7\n60.5\n25.2\nDOUBLEDIPPER\n62.1\n40.6\n86.6\n95.4\n56.2\n65.1\n60.5\n30.4\nGemma 2 2B (v2) (vanilla)\n38.6\n8.9\n71.8\n68.6\n51.2\n13.3\n49.6\n6.5\nZero-shot + Evidence Retrieval\n42.0\n22.3\n66.8\n70.6\n40.2\n30.6\n47.6\n16.2\nDOUBLEDIPPER\n49.5\n23.7\n85.8\n81.6\n50.0\n39.9\n46.7\n18.8\nGemma 2 9B (v2) (vanilla)\n44.0\n11.4\n74.8\n81.8\n55.6\n13.8\n61.0\n9.3\nZero-shot + Evidence Retrieval\n58.7\n38.6\n82.0\n83.4\n59.4\n56.1\n64.4\n26.8\nDOUBLEDIPPER\n61.2\n41.7\n84.0\n95.0\n51.4\n61.2\n61.8\n33.3\nLlama 3.1 8B (vanilla)\n37.2\n11.8\n56.2\n52.2\n48.6\n20.7\n63.5\n7.4\nZero-shot + Evidence Retrieval\n53.1\n42.2\n71.8\n65.4\n50.8\n55.3\n62.0\n24.5\nDOUBLEDIPPER\n59.9\n38.7\n91.2\n90.6\n51.0\n59.3\n58.1\n30.1\nMistral 7B (v0.3) (vanilla)\n37.4\n14.1\n59.2\n57.4\n50.2\n15.8\n60.8\n4.6\nZero-shot + Evidence Retrieval\n44.0\n23.8\n66.2\n62.4\n49.6\n34.1\n58.6\n13.6\nDOUBLEDIPPER\n51.0\n28.6\n68.4\n88.8\n50.6\n43.4\n60.7\n16.7\nMixtral 7x8B (v0.1) (vanilla)\n42.6\n13.7\n73.0\n66.2\n51.0\n18.2\n67.7\n8.4\nZero-shot + Evidence Retrieval\n47.4\n18.8\n81.8\n73.6\n50.6\n26.3\n67.9\n13.1\nDOUBLEDIPPER\n52.2\n22.3\n91.8\n86.0\n47.8\n35.1\n66.6\n16.0\nTable 3: Accuracy of the QA task for the vanilla baseline (prompting the model to only answer the question), Zero-shot + Evidence Retrieval (prompting the model to explicitly identify the relevant passage(s) before generating the answer) and DOUBLEDIPPER with 3 demonstrations generated by Gemma 2 2B.\n# 2020; Trivedi et al., 2021).\nImplementation Details We randomly select three passages from the input (see Section 6 for an analysis of the number of self-generated demonstrations on the performance), each containing at least two sentences, and ask the model to generate a single QA pair for each passage (see Appendix A for the exact prompt). For all experiments, including few-shot generation and question-answering, we use a temperature setting of 0.\n# 5 Results\nResult 1: DOUBLEDIPPER offers a substantial performance boost. Table 3 presents the QA performance of the baseline, Zero-shot + Evidence Retrieval and DOUBLEDIPPER on our evaluation set. The results first show that prompting models to explicitly identify the relevant paragraphs before generating the answer (Zero-shot + Evidence Retrieval) leads to a performance improvement of 9.7 points on average across models over the vanilla baseline. DOUBLEDIPPER offers an additional substantial boost of 6.3 points for all models on average, culminating in a overall improvement of 16 absolute points over the vanilla baseline. Notably, while DOUBLEDIPPER produces simple QAs answerable from a single paragraph, it always surpasses the baseline in multi-hop QA datasets\n(HotPotQA, 2Wiki and MuSique). Likewise, DOUBLEDIPPER outperforms the baseline also on the FLenQA datasets (PIR, MonoRel and SRT), which involve synthetic True/False questions although the demonstrations in DOUBLEDIPPER are typically simple factoid questions.\n# Result 2: Learning to retrieve the evidence(s) with DOUBLEDIPPER is more effective in commercial models than in open source. Table 4\npresents the performance of the supporting paragraphs prediction for the Zero-shot + Evidence Retrieval and DOUBLEDIPPER on our evaluation set. For all commercial models and Gemma 2B, DOUBLEDIPPER predicts better the supporting paragraphs than in the zero-shot setting (+2.6 F1 for Gemini Pro, +3.4 F1 for Gemini Flash, +1.8 F1 for Gemini Nano and +6.5 F1 for Gemma 2B). On the other hand, DOUBLEDIPPER slightly hurts the performance of common open source models (e.g., -2.8 F1 for Mistral and -0.3 F1 for Llama 3.1). This difference is because DOUBLEDIPPER\u2019s demonstrations are based on a single paragraph, where commercial models can generalize better and predict multiple evidences. Indeed, Gemini Pro predicts on average 2 evidences for the datasets 2Wiki, MuSique, PIR, MonoRel, SRT and HotPotQA whereas Gemma 9B predicts 1.2 evidences for each instance. In fact, the only dataset with\nAvg.\n2Wiki\nMonoRel\nPIR\nSRT\nHotPotQA\nLost\nMuSique\nGemini Pro\nZero-shot + Evidence Retrieval\n83.7\n96.7\n97.7\n97.5\n62.8\n92.1\n63.6\n75.3\nDOUBLEDIPPER\n86.3\n94.4\n99.8\n97.1\n80.9\n90.0\n66.4\n75.4\nGemini Flash\nZero-shot + Evidence Retrieval\n75.7\n82.3\n90.5\n72.6\n70.4\n80.9\n67.3\n65.9\nDOUBLEDIPPER\n79.1\n83.7\n98.3\n80.2\n71.2\n84.6\n66.1\n69.5\nGemini Nano\nZero-shot + Evidence Retrieval\n68.1\n76.3\n77.6\n71.6\n66.0\n74.1\n55.8\n55.2\nDOUBLEDIPPER\n69.9\n76.6\n86.6\n74.5\n59.8\n76.0\n59.2\n56.7\nGemma 2B\nZero-shot + Evidence Retrieval\n39.1\n57.6\n49.4\n44.3\n18.9\n53.1\n14.0\n36.4\nDOUBLEDIPPER\n45.6\n56.5\n60.7\n57.5\n13.1\n58.1\n33.1\n39.9\nGemma 9B\nZero-shot + Evidence Retrieval\n61.9\n76.7\n69.3\n60.3\n43.2\n76.5\n51.5\n55.9\nDOUBLEDIPPER\n57.0\n74.2\n52.5\n59.0\n23.1\n78.4\n55.3\n56.8\nLlama 3.1 8B\nZero-shot + Evidence Retrieval\n61.7\n53.2\n86.5\n66.9\n67.8\n63.2\n41.1\n53.5\nDOUBLEDIPPER\n61.4\n68.9\n71.4\n54.9\n52.8\n73.7\n53.0\n54.8\nMistral 7B (v0.3)\nZero-shot + Evidence Retrieval\n46.6\n62.4\n49.8\n46.2\n17.5\n64.0\n43.4\n43.0\nDOUBLEDIPPER\n43.8\n63.4\n33.8\n42.5\n4.2\n66.7\n55.6\n40.2\nMixtral 7x8B v(0.1)\nZero-shot + Evidence Retrieval\n60.0\n72.4\n69.4\n64.6\n43.4\n76.9\n40.6\n52.4\nDOUBLEDIPPER\n58.9\n70.4\n81.6\n63.6\n18.3\n75.0\n50.4\n53.2\nTable 4: Performance (F1) of supporting paragraph(s) predictio\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03de/03de2c6c-0127-4b7c-927a-104136593e40.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance (accuracy) of Gemma 2 9B, Mixtral 8x7B and Gemini Flash with and without DOUBLEDIPPER on our sample of the Lost-in-the-middle dataset (Liu et al., 2023) according to the position of the document that contains the answer.</div>\na single evidence is \u201cLost\u201d and almost all models highly benefit from DOUBLEDIPPER (e.g, +11.9 F1 for Llama 3.1, +12.2 F1 for Mistral 7B and +9.8 for Mixtral 7x8B).\nResult 3: DOUBLEDIPPER makes models more robust to the position of relevant information. Following Liu et al. (2023), Figure 4 shows the performance of Gemma 2 9B, Mixtral 8x7B and Gemini Flash for both the baseline and DOUBLEDIPPER on our sample of the Lost-of-the-middle dataset, according to the position of the document that contains the answer. See Appendix B for the performance curve of the other tested models, which show similar trends. Overall, the performance curve for DOUBLEDIPPER consistently surpasses the baseline when the relevant information appears \u201cin the middle\u201d and sometimes also at the beginning and/or the end (e.g., Gemini Flash). This variation can likely be attributed to the inherent biases of LLMs towards the beginning and end of inputs, while adding in context demonstrations mitigates this bias. This reveals that beyond improving performance, DOU-\nGemma 2B\nSelf\nGemini Pro\nGemini Pro\n70.4\n71.6\n71.6\nGemini Nano\n62.1\n61.7\n62.8\nGemini Flash\n66.1\n67.5\n68.1\nGemma 2B\n49.5\n49.5\n51.0\nGemma 9B\n61.2\n62.0\n63.5\nLlama 3.1\n59.9\n60.1\n61.5\nMistral v0.3\n51.0\n49.8\n52.6\nMixtral\n52.2\n49.8\n54.4\nTable 5: Average performance of DOUBLEDIPPER with different models for generating the demonstrations. See Appendix C.2 for the results on each evaluation dataset.\nBLEDIPPER can make the model more robust to the position of the relevant document.\n# 6 Ablation Studies\nHow many examples are needed? In Table 6, we explore the impact of varying k, the number of self-generated few-shot examples in DOUBLEDIPPER to 1, 3, 5, and 10. On average, a single demonstration already provides an improvement over the baseline. Adding 3 demonstrations adds another\nk = 1\nk = 3\nk = 5\nk = 10\nGemini Nano\n60.0\n62.1\n62.2\n62.3\nGemini Flash\n65.3\n66.1\n65.9\n66.1\nGemma 2B (v2)\n47.0\n49.5\n49.6\n49.9\nGemma 9B (v2)\n58.7\n61.2\n61.4\n61.3\nLlama 3.1\n57.7\n59.9\n60.6\n61.4\nMistral 7B (v0.3)\n48.9\n51.0\n51.1\n51.4\nMixtral 7x8B (v0.1)\n49.3\n52.2\n51.7\n52.2\nTable 6: Average performance on our evaluation set with various numbers of self-generated few shot demonstrations (k) in DOUBLEDIPPER. See Appendix C.1 for the results on each evaluation dataset.\nboost of 2 points, while increasing the number of demonstrations to 5 and 10 leads to a marginal improvement. This finding is in line with previous work (Brown et al., 2020a; Min et al., 2022b). We conclude that a small number of examples carries most of the benefit with our method, but given additional computation budget, adding more examples does carry additional minor benefit. Investigating the effect of the few-shot generator To understand the impact of the default chosen model (Gemma 2 2B) for generating the demonstrations, we conducted two additional experiments. The first experiment is SELF in which we use the same model for generating the demonstrations and for answering the original question. In the second experiment, we generate the demonstrations with the best LLM used in our experiments, namely Gemini Pro. The average results are reported in Table 5 and the performance for each evaluation dataset is presented in Appendix C.2. The results show that generating the demonstrations with Gemma 2 or SELF achieves similar performance, while Gemini Pro leads to a consistent increase in performance across models, indicating that future better models can improve further the performance. As mentioned in Section 3, DOUBLEDIPPER promotes also efficiency by adding to the original prompt only a few extra tokens, leading to a significantly cheaper inference than the traditional ICL. DOUBLEDIPPER without identification of supporting paragraphs To ablate the second principle in DOUBLEDIPPER, which is the explicit identification of the supporting paragraphs before generating the answer, we prompt the open source models with self-generated few shot examples that comprise only question-answer pairs (without instructing the model to retrieve the relevant pas-\nboost of 2 points, while increasing the number of demonstrations to 5 and 10 leads to a marginal improvement. This finding is in line with previous work (Brown et al., 2020a; Min et al., 2022b). We conclude that a small number of examples carries most of the benefit with our method, but given additional computation budget, adding more examples does carry additional minor benefit.\nInvestigating the effect of the few-shot generator To understand the impact of the default chosen model (Gemma 2 2B) for generating the demonstrations, we conducted two additional experiments. The first experiment is SELF in which we use the same model for generating the demonstrations and for answering the original question. In the second experiment, we generate the demonstrations with the best LLM used in our experiments, namely Gemini Pro. The average results are reported in Table 5 and the performance for each evaluation dataset is presented in Appendix C.2. The results show that generating the demonstrations with Gemma 2 or SELF achieves similar performance, while Gemini Pro leads to a consistent increase in performance across models, indicating that future better models can improve further the performance. As mentioned in Section 3, DOUBLEDIPPER promotes also efficiency by adding to the original prompt only a few extra tokens, leading to a significantly cheaper inference than the traditional ICL.\nporting paragraphs To ablate the second principle in DOUBLEDIPPER, which is the explicit identification of the supporting paragraphs before generating the answer, we prompt the open source models with self-generated few shot examples that comprise only question-answer pairs (without instructing the model to retrieve the relevant pas-\nsage(s)). For all models, the average QA performance of DOUBLEDIPPER without the evidence is lower than DOUBLEDIPPER with evidence identification. When averaging results across models and datasets, we report a substantial F1 drop from 54.8 to 46.6. Full results are available in Appendix C.3 in Table 9.\nQualitative analysis: Correctness of the generated QA pairs We manually analyze 150 QAs generated by Gemma 2B as demonstrations. Our review confirms that 93.5% of these self-generated QAs are correct, meaning that the question is meaningful and the answer could be found in the corresponding paragraph.\n# 7 Conclusion\nWe develop DOUBLEDIPPER, a straightforward method for enhancing the performance of Question Answering with long context and providing attribution to the relevant paragraph(s) in the input. By recycling the input context to generate the few shot examples, each demonstration includes solely a question, an answer and a pointer to the relevant paragraph, without a separate context, thus effectively addressing the challenging of In-ContextLearning with long context. Experimental results show that our approach substantially ourperforms the baselines in various QA settings, including distractor passages in the input, True/False questions and multi-hop QA.\n# 8 Limitations\nOne notable limitation of our approach is the extended inference time required for generating question-answer pairs. Future research could mitigate this issue by developing smaller, specialized models specifically tailored for QA generation. Additionally, our evaluation set is constrained to instances that are solely in English and range between 1,000 to 4,000 tokens. Expanding the diversity of languages and token ranges could enhance the robustness and applicability of our findings. Lastly, although we employ a strategy of randomly sampling k paragraphs from the input to ensure the model engages with varied segments of the text, we did not optimize the selection of these paragraphs. Future work could explore more strategic methods for paragraph selection to potentially enhance the efficacy and relevance of the generated examples.\nRishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. 2024. Many-shot incontext learning. Preprint, arXiv:2404.11018. Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. Preprint, arXiv:2307.11088. an Luo, Xin Xu, Yue Liu, Panupong Pasupat, and Mehran Kazemi. 2024. In-context learning with retrieved demonstrations for language models: A survey. ArXiv, abs/2401.11624. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022. Exploring length generalization in large language models. ArXiv, abs/2207.04901. Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508. Jennifer A Bishop, Qianqian Xie, and Sophia Ananiadou. 2023. Longdocfactscore: Evaluating the factuality of long document abstractive summarisation. ArXiv, abs/2309.12455.\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020a. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners. ArXiv, abs/2005.14165.\nAvi Caciularu, Ido Dagan, Jacob Goldberger, and Arman Cohan. 2022. Long context question answering via supervised contrastive learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2872\u20132879, Seattle, United States. Association for Computational Linguistics. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023. Longlora: Efficient fine-tuning of long-context large language models. ArXiv, abs/2309.12307. Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In International Joint Conference on Artificial Intelligence.\nbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cant\u00f3n Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline C. Muzzi, Mahesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasi\u00b4c, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen\nKrishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm\u2019an, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizen-\nstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, U KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, A Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Doll\u00e1r, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. ArXiv, abs/2407.21783.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2022. Rarr: Researching and revising what language models say, using language models. In\nAnnual Meeting of the Association for Computational Linguistics.\nCheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. 2024. Found in the middle: Calibrating positional attention bias improves long context utilization. In Findings of the Association for Computational Linguistics ACL 2024, pages 14982\u2013 14995, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u2019elio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2024. Mixtral of experts. ArXiv, abs/2401.04088.\nAlbert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u2019elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825.\nHyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. ArXiv, abs/2206.08082.\nHyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. ArXiv, abs/2206.08082.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy. Association for Computational Linguistics.\nMosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task, more tokens: the impact of input length on the reasoning performance of large language models. ArXiv, abs/2402.14848.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA. Curran Associates Inc.\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? ArXiv, abs/2311.04939.\nJunlong Li, Jinyuan Wang, Zhuosheng Zhang, and Hai Zhao. 2024a. Self-prompting large language models for zero-shot open-domain QA. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 296\u2013310, Mexico City, Mexico. Association for Computational Linguistics.\n# Rui Li, Guoyin Wang, and Jiwei Li. 2024b. Are humangenerated demonstrations necessary for in-context learning? In The Twelfth International Conference on Learning Representations.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Rethinking the role of demonstrations: What makes in-context learning work? ArXiv, abs/2202.12837.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Rethinking the role of demonstrations: What makes in-context learning work? ArXiv, abs/2202.12837.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nBhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multistep reasoning and tool-use for large language models. ArXiv, abs/2303.09014.\n# Ofir Press, Noah A. Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. ArXiv, abs/2108.12409.\nachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem W. Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tom\u00e1s Kocisk\u00fd, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, J Christopher Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Ying-Qi Miao, Luk\u00e1s Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontan\u2019on, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers,\nRuizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, A.E. Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Venkatesh Ramasesh, Anton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matt Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara N. Sainath, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela de Castro Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adri\u00e0 Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, S\u2019ebastien M. R. Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Joshua Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost R. van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya B Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, S\u2019ebastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Michael B. Chang, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravichandra Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Luvci\u2019c, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjosund, Elena Buchatskaya, Elspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Jane Labanowski, Nicola De Cao, David Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos L. Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Zhufeng Pan, Zachary Nado,\nStephanie Winkler, Dian Yu, Mohammad Saleh, Lorenzo Maggiore, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, ChungCheng Chiu, Zoe C. Ashwood, Khuslen Baatarsukh, Sina Samangooei, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabriel Barth-Maron, Craig Swanson, Dominika Rogozi\u2019nska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Ren shen Wang, Dave Lacey, Anastasija Ili\u2019c, Yao Zhao, Woohyun Han, Lora Aroyo, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Raphael Lopez Kaufman, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, T. Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Anais White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana Kulizhskaya, Sonam Goenka, Brennan Saeta, Kiran Vodrahalli, Christian Frank, Dario de Cesare, Brona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, cCauglar Unlu, David Reid, Zora Tung, Daniel F. Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Rui Zhu, Ricardo Aguilar, Mai Gim\u2019enez, Jiawei Xia, Olivier Dousse, Willi Gierke, Soheil Hassas Yeganeh, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder, Daniel Niels Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Daniel Toyama, Mrinal Shukla, Sujoy Basu, Abhi Mohan, Nicholas Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, Donghyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, F\u00e9lix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alexey Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Anna Bulanova, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui\nWu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Cl\u2019ement Farabet, Pedro Valenzuela, Quan Yuan, Christoper A. Welty, Ananth Agarwal, Mianna Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Elnaz Davoodi, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca SantamariaFernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, A. Ya. Khorlin, Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Alejandro Lince, Norman Casagrande, Jay Hoover, Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Ji\u02c7ri Sima, Anna Koop, Praveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Kalpesh Krishna, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas Fitzgerald, Aviral Kumar, Sanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel El Kaed, Jing Li, Jakub Sygnowski, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Poder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Junwen Bai, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, Oriol Vinyals, and Alexandra Chronopoulou. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv, abs/2403.05530.\nemma Team Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u2019eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u2019e, Johan Ferret, Peter Liu, Pouya Dehghani Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Sta\u00b4nczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Boxi Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Christoper A. Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozi\u2019nska, D. Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin,\nGabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci\u2019nska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost R. van Amersfoort, Josh Gordon, Josh Lipschultz, Joshua Newlan, Junsong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, L. Sifre, L. Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Gorner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, S. Mc Carthy, Sarah Perrin, S\u2019ebastien Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tom\u00e1s Kocisk\u00fd, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Brian Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeffrey Dean, Demis Hassabis, Koray Kavukcuoglu, Cl\u2019ement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. 2024. Gemma 2: Improving open language models at a practical size. ArXiv, abs/2408.00118.\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977\u20137989, Singapore. Association for Computational Linguistics.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison over long lan-\nguage sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. 2024. Attribute first, then generate: Locally-attributable grounded text generation. ArXiv, abs/2403.17104. Yisheng Song, Ting-Yuan Wang, Puyu Cai, Subrota Kumar Mondal, and Jyoti Prakash Sahoo. 2022. A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities. ACM Computing Surveys, 55:1 \u2013 40. H. Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2021. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539\u2013554. Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Neural Information Processing Systems. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. 2024. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. Helmet: How to evaluate longcontext language models effectively and thoroughly.\n# A Prompts\nFigure 5 shows the zero-shot prompt we use for generating the question-answer pairs in DOUBLEDIPPER. For the QA prompts, we use the same instructions and prompt template as the original papers (Lost-in-the-middle and FLenQA) and add a simple line for the instructions in other multi-hop QA datasets: \u201cPlease answer the question based on the given passages below.\u201d. For MuSique, since the dataset includes questions that are not answerable, we add the following sentence to the prompt: \u201cIf the question can\u2019t be answered given the given passages, please write \"unanswerable\"\u201d.\n# B Lost-in-the-middle\nFigure 6 shows the QA accuracy of the models Gemma 2 2B, Mistral 7B, Gemini Nano and Gemini Pro on our subset of the \u201cLost-in-the-middle\u201d dataset.\n# C Analysis\n# C.1 Impact of the Number of Demonstrations in DOUBLEDIPPER\nTable 7 presents the results of DOUBLEDIPPER with 1, 3 (main experiment in the paper), 5 and 10 generated demonstrations. For all these experiments, the demonstrations were generated by Gemma 2 2B. Figure 7 shows the QA accuracy of DOUBLEDIPPER on \u201cLost\u201d according to the position of the relevant passage for each k \u2208{1, 3, 5, 10}.\n# C.2 Impact of the few-shot generator\nTable 8 presents the detailed QA performance of all models with different models for generating DOUBLEDIPPER\u2019s demonstrations. As mentioned in the paper (Section 6), generating the demonstrations with the best model (ie. Gemini Pro) achieves the best performance overall.\n# C.3 Impact of the identification of supporting paragraphs in the QA generation\nTable 9 compares the performance of DOUBLEDIPPER to DOUBLEDIPPER without evidence identification.\nAvg.\n2Wiki\nMonoRel\nPIR\nSRT\nHotPotQA\nLost\nMuSique\nGemini Nano\nk = 1\n60.03\n37.68\n85.20\n88.20\n56.40\n62.50\n60.68\n29.56\nk = 3\n62.12\n40.55\n86.60\n95.40\n56.20\n65.12\n60.52\n30.44\nk = 5\n62.25\n41.79\n87.00\n95.60\n55.20\n65.05\n60.56\n30.52\nk = 10\n62.33\n43.16\n86.00\n96.20\n55.20\n65.37\n60.44\n29.96\nGemini Flash\nk = 1\n65.34\n48.83\n84.80\n90.80\n66.40\n60.17\n65.16\n41.19\nk = 3\n66.11\n48.03\n85.80\n95.00\n68.60\n60.58\n65.04\n39.71\nk = 5\n65.87\n47.49\n87.20\n95.00\n66.60\n61.13\n64.20\n39.48\nk = 10\n66.08\n46.13\n86.20\n95.20\n69.00\n62.03\n63.80\n40.18\nGemma 2B (v2)\nk = 1\n47.05\n24.05\n73.60\n77.00\n50.20\n41.32\n47.04\n16.13\nk = 3\n49.48\n23.66\n85.80\n81.60\n50.00\n39.85\n46.68\n18.77\nk = 5\n49.59\n26.70\n85.40\n80.40\n48.40\n40.34\n46.56\n19.30\nk = 10\n49.",
    "paper_type": "method",
    "attri": {
        "background": "Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains suboptimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario; However, na\u00efvely adding ICL examples with long context introduces challenges, including substantial token overhead added for each few-shot example and context mismatch between the demonstrations and the target query.",
        "problem": {
            "definition": "The problem addressed in this paper is the difficulty of LLMs in answering questions based on long input contexts, particularly when relevant information is located in the middle of these contexts.",
            "key obstacle": "Existing methods struggle with identifying relevant information within long contexts, leading to poor performance on simple questions even with shorter inputs."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that LLMs struggle to identify relevant segments in long contexts, which can be mitigated by efficiently generating few-shot examples from the same context.",
            "opinion": "The proposed idea involves recycling the input context to automatically generate few-shot examples for long-context QA tasks, ensuring that the demonstrations leverage the same context as the target query.",
            "innovation": "The key innovation of this method, DOUBLEDIPPER, is that it generates few-shot examples from the same long input context, reducing token overhead and improving relevance compared to traditional ICL methods."
        },
        "method": {
            "method name": "DOUBLEDIPPER",
            "method abbreviation": "DIPPER",
            "method definition": "DOUBLEDIPPER is a method that enhances the performance of LLMs in long-context question answering by generating few-shot examples derived from the same input context.",
            "method description": "The method generates few-shot examples from the original input context and instructs the model to identify relevant paragraphs before answering.",
            "method steps": "1. Select paragraphs from the input context. 2. Generate question-answer pairs for each selected paragraph. 3. Concatenate the original context, generated examples, and the target question to form the final prompt. 4. Instruct the model to identify relevant paragraphs and generate the answer.",
            "principle": "This method is effective because it reduces token overhead and ensures that the few-shot examples are contextually relevant, which helps the model to better understand and answer questions."
        },
        "experiments": {
            "evaluation setting": "The method was evaluated on various datasets, including Lost-in-the-middle, FLenQA, HotpotQA, 2Wiki, and MuSiQue, with a total of 5.5K instances.",
            "evaluation method": "Performance was assessed using accuracy and F1 scores, comparing DOUBLEDIPPER against baseline methods and zero-shot evidence retrieval approaches."
        },
        "conclusion": "DOUBLEDIPPER significantly improves the performance of LLMs in long-context question answering by effectively generating relevant few-shot examples and providing attribution to the source paragraphs.",
        "discussion": {
            "advantage": "The main advantage of DOUBLEDIPPER is its ability to enhance model performance while minimizing token overhead and ensuring contextual relevance, leading to higher accuracy in QA tasks.",
            "limitation": "A limitation of the approach is the extended inference time required for generating question-answer pairs, which may hinder efficiency.",
            "future work": "Future research could focus on optimizing the paragraph selection process and developing smaller models tailored for QA generation to improve efficiency and applicability."
        },
        "other info": {
            "additional information": {
                "info1": "The method demonstrates substantial improvements across multiple models and datasets, achieving an average performance boost of 16 absolute points over baseline methods.",
                "info2": {
                    "info2.1": "DOUBLEDIPPER was tested on both commercial and open-source models, showing varying degrees of effectiveness.",
                    "info2.2": "The evaluation metrics included accuracy for datasets like Lost-in-the-middle and FLenQA, and Token F1 for multi-hop QA datasets."
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-Context Learning (ICL) with few-shot examples may enhance LLM performance in tasks involving long contexts."
        },
        {
            "section number": "1.3",
            "key information": "Despite advancements, LLMs struggle with answering questions based on long input contexts, particularly when relevant information is in the middle."
        },
        {
            "section number": "3.1",
            "key information": "The method DOUBLEDIPPER enhances LLM adaptation by generating few-shot examples from the same input context, improving relevance."
        },
        {
            "section number": "3.3",
            "key information": "DOUBLEDIPPER generates question-answer pairs from selected paragraphs to enhance in-context learning."
        },
        {
            "section number": "6.2",
            "key information": "A limitation of the DOUBLEDIPPER approach is the extended inference time required for generating question-answer pairs, which may hinder efficiency."
        },
        {
            "section number": "7",
            "key information": "DOUBLEDIPPER significantly improves LLM performance in long-context question answering by generating relevant few-shot examples."
        }
    ],
    "similarity_score": 0.752703358167931,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Can Few-shot Work in Long-Context_ Recycling the Context to Generate Demonstrations.json"
}