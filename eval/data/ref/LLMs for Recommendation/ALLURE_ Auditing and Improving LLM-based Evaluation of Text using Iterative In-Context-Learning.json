{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.13701",
    "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
    "abstract": "From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.",
    "bib_name": "hasanbeig2023allureauditingimprovingllmbased",
    "md_text": "# ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\nHOSEIN HASANBEIG, Microsoft Research, USA HITESHI SHARMA, Microsoft, USA LEO BETTHAUSER, Microsoft, USA FELIPE VIEIRA FRUJERI, Microsoft, USA IDA MOMENNEJAD, Microsoft Research, USA\n 27 Sep 2023\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a099/a0993094-18f4-4176-a4e2-ca4296aedd1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. ALLURE architecture: a closed-loop in-context-learning protocol</div>\nFrom grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity. CCS Concepts: \u2022 Human-centered computing; Additional Key Words and Phrases: Large Language Models, Audit\nCCS Concepts: \u2022 Human-centered computing; Additional Key Words and Phrases: Large Language Models, Audit\n# 1 INTRODUCTION\nLarge Language Models (LLMs) are rapidly becoming ubiquitous in automation, such as the evaluation of text generated by humans and AI alike [63]. Examples range from summarizing medical notes [28, 48] and reviewing LLM outpu for errors, to grading term papers and educational use [59]. In fact some studies report that humans may prefe\nLLMs\u2019 summarization in some domains [34]. However, while the accuracy of evaluation and summarization can have critical ethical, medical, and educational consequences [59], LLMs display identifiable categories of failure modes when evaluating other text. For instance, previous work has discovered a bias in LLM-based evaluators [57], and while others improved LLM evaluations by prompting two LLMs to reach a mutual agreement on two answers [32], identifying and correcting these failure modes remain crucial to tackling both technical and ethical challenges to LLM use in evaluation and summarization of text [34, 59]. To address such challenges for LLM-based evaluation of text, here we propose ALLURE, a protocol for systematic auditing and improving LLM-based evaluation accuracy with iterative in-context-learning (ICL). Using ALLURE we audit GPT-4\u2019s evaluation of responses generated by eight LLMs, identify their failure modes, and improve GPT-4\u2019s evaluation accuracy. The eight LLMs, whose resposnes were evaluated by GPT-4, include GPT-4 [1], GPT-3.5-turbo-175B [41], text-Davinci-3-175B [5], Bard [52], Anthropic Claude-1-52B [2], LLaMA-13B [54], Cohere-52.4B [9], Alpaca-7B [51]. In order to do so, we store examples of failures in memory and use these examples to generate in-context-learning (ICL) prompts and iteratively improve them to optimize GPT-4\u2019s evaluation of text. We employ a signal detection theoretic approach in our analysis, and apply the protocol to GPT-4\u2019s evaluation of planning behavior of 8 LLMs in an RL task (Experiment 1) and improving GPT-4\u2019s summarization of news (Experiment 2). In our experiments, we categorized failure modes of GPT-4 as an evaluator in two different domains: LLMs performing RL planning tasks, and LLMs performing summarization for news articles. We then used iterative ICL to improve GPT-4\u2019s evaluation performance, testing the robustness of our method by ablating instructions and prompts, number and types of ICL examples and more guidance. In Experiment 1, we focus on 8 LLMs\u2019 planning output on RL tasks, categorize their failure modes, and use iterative ICL to improve GPT-4\u2019s performance as an evaluator, while testing the robustness of the impact of ICL prompts to the use of examples from different failure mode clusters using an ablation approach. In Experiment 2, we use a similar approach to increase GPT-4\u2019s evaluation of news summarization by four LLMs (T5 [44], GPT-2 [43], Pegasus [61], and Improve-abs [30]). We show that GPT-4\u2019s evaluation is sensitive to the instructions and prompts: providing more context, examples, and guidance can improve evaluation. We used an ICL approach, and found that it is not merely the number of examples that is used in the ICL but the type of examples used. Namely, the examples need to be related to the specific type of error or \"failure mode\", which we have identified and named in Section 4.1. We found that it is not merely the number of examples, but the type of examples used in the ICL that can influence the performance of GPT-4 as an evaluator. Namely, the examples need to be related to the specific type of error or \"failure mode\", which we have identified and named in Section 4.1. While the addition of ICL examples can improve GPT-4\u2019s evaluation performance, increasing the number of examples does not improve performance in an additive manner (both at a problem class and overall). Notably, we observed that, increasing the number of examples in ICL initially, leads to a dip in ICL-based improvement, but improves after a certain amount is added. Furthermore, we found that different set of \"similar\" ICL examples yields asymmetric improvements in GPT-4\u2019s evaluation accuracy. This points to the importance of analyzing selected examples for ICL prompts holistically, considering the relationship of different clusters of failure modes and categories of examples.\n# 2 RELATED WORK\nThe swift progress of Large Language Models (LLMs) has emphasized the significance of assessing their generate responses. Conventional n-gram metrics such as ROUGE [46], and sophisticated model-based evaluations such a BERTScore[62] and BARTScore [60], are not sufficient for such a thorough assessment [25]. There has been a serie\nof work which use ChatGPT and GPT-4 as evaluators. For example, [56] uses ChatGPT to evaluate five NLG metaevaluation datasets including summarization, story generation and data-to-text tasks. They find that the ChatGPT evaluator has a high correlation with humans in most cases, especially for creative NLG tasks (e.g., story generation). GPTScore [14] evaluates texts using GPT-3 by formulating the evaluation task as a conditional generation problem. On the other hand, [35], the authors use GPT-3.5 and GPT-4 to formulate the evaluation task as a form-filling problem. They demonstrate human-level evaluation capability in various NLP tasks. In [29], the algorithms based on GPT-3.5 and GPT-4 outperform existing algorithms on the causal discovery, counterfactual reasoning, and actual causality inferring. The authors in [65] present LLM-as-a-judge evaluate models on more open-ended questions and conclude that it is a scalable and explainable way to approximate human preferences. Despite all the impressive successes, these LLMs are not reliable. The authors in [66] found that the fact-based questionanswering capability of ChatGPT does not improve compared to its predecessors. Hence, it is risky to use these models in domains where factual correctness is critical, for instance in legal domain [11] where they conclude that LLMs are not ready for fully automatic deployment for legal case judgement summarization and human-in-the-loop approach (manually annotating for inconsistencies) is rather more suitable. Furthermore, these LLMs are sensitive to the prompts, instructions and inputs [55] and one can skew the evaluation results by easy manipulations [57] .\n# 3 IN-CONTEXT-LEARNING AND ALLURE\nIn-context-learning (ICL) refers to the process of using the given context to guide a model\u2019s understanding and response generation. Unlike traditional methods, where models are fine-tuned on specific tasks with pre-defined objectives, ICL leverages the immediate context (e.g., through several examples) to make better predictions and generate more accurate responses [43]. ICL is especially relevant in LLM-based evaluation of text, where context plays a crucial role in comprehending and generating meaningful, coherent evaluation. By considering the immediate context, models can better adapt to variations in language, style, and content, subsequently enhancing their overall performance [5]. The primary concept of ICL is acquiring knowledge through analogies [58]. Initially, ICL needs several examples to establish a demonstration context, typically presented in natural language templates [33, 49]. Following this, ICL combines a query with the demonstration context to create a prompt, which is then given as an input to the language model for predictions. ICL avoids parameter updates and directly executes predictions using pre-trained language models, in contrast with supervised learning which demands a training phase involving gradient descent to modify model parameters. The model is anticipated to discern the concealed pattern within the demonstration and subsequently generate accurate predictions [12]. ICL can be thought of in relation to the notion of episodic memory in reinforcement learning (RL) and metalearning [15]. Episodic memory stores records of critical past events, bundling memories that are similar within a given context. The context or items are in turn retrieved as reference points when making new decisions that are similar to these past experiences along one feature dimension or other. In RL, episodic memory can enable an agent to approximate value functions over complex state spaces, generalization of structures and context, learn with low number of samples, and given long-term dependencies between actions and rewards [15, 36, 39]. In short, episodic memory offers a solution to avoid forgetting and improve generalization while training with a lower sample complexity [4, 31]. The ICL approach presents numerous appealing benefits as a novel paradigm. Firstly, it mirrors the human decisionmaking process by drawing on analogies. In addition, the use of natural language for demonstrations enables an intelligible interface for interaction with LLMs. Thus, the integration of human expertise by modifying demonstrations and templates is simplified. Lastly, in contrast to supervised training, ICL operates as a training-free learning framework,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/82bc/82bc1fed-9612-4c3c-b1c2-f35890da53af.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. A close-up view of the trapezoid within the ALLURE architecture, as shown in Figure 1. The generated response by a taskreceiving LLM are compared against the correct answer automatically. Consider a binary classification task. If the generated response matches the correct answer and the assigned label is \u201clabel: 0\u201d, or the generated response does not match the correct answer and the assigned label is \u201clabel: 1\u201d, then the assigned label by the evaluator LLM is potentially incorrect. From the generated response and the assigned label, an in-context-learning (ICL) example is generated using a predefined template. In the constructed ICL example, a (hypothetical) user repeats the response, and an (hypothetical) assistant assigns the correct label, setting an example for the evaluator. This is then audited by a human to ensure the quality of the generated ICL examples as in Figure 1. The ICL examples are finally stored in the memory, where each example includes the generated response , the task correct answer , and</div>\nwhich not only substantially lowers computational expenses for adapting to new real-world tasks [12]. Although ICL shows promise, further investigation reveals that it can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art [64]. The functioning mechanism of the ICL remains ambiguous, and only a limited number of studies have offered initial clarifications [10]. ALLURE: In this paper, we put forth a semi-automated closed-loop approach for ICL, which is based on the protocol depicted in Figure 1. In the first loop, the Evaluator LLM assesses the Responses generated by the Generator LLMs and assigns a label to each response. The correctness of the generated labels are verified using Regular Expression Matching between the generated responses and the correct response to the Input Task. Regular expressions are patterns that are widely used to match character combinations in text. The regular expression matching filters out potential candidates for ICL when the evaluator labels are wrong. When discovering such erroneous labels, an ICL example is constructed following the framework in Figure 2. The structure of the constructed ICL example is as follows. The user reiterates the response along with the expected correct answer, and the assistant, i.e., the evaluator LLM, assigns the correct label. The constructed ICL example is then audited to ensure the correctness of the assigned label. If the ICL example passes the audit, then it is added to the ICL Examples Memory database. The examples in this database are presented to the evaluator LLM as a meta-prompt which essentially robustifies the evaluator against its past mistakes. Similar to the episodic memory in RL, this memory database grows as the evaluator interacts with more tasks, allowing the evaluator to generate more accurate labels for unseen tasks.\n# 4 EXPERIMENT 1: AUDITING GPT-4\u2019S EVALUATION OF PLANNING BEHAVIOR IN EIGHT LLMS\nMost LLMs struggle with planning behavior [38]. The experiments discussed in this section are based on the findings in [38]. The authors in [38] investigated whether state-of-the-art LLMs understand the latent structure of planning problems in several cognitive maps. A cognitive map is a representation of latent relational structures that underlies a task in an environment, and supports decision-making, reasoning, and deduction in both biological and artificial agents [3, 6, 36, 53]. Such representations also emerge in model-based [50] or task-oriented model-free reinforcement learning (RL) [16\u201324], which capture a latent construct of the task. To measure LLMs behavioral signatures, the prompts designed in [38] make use of several cognitive maps in a set of tasks adapted from existing human behavioral experiments [37, 39, 40, 42, 47]. These engineered prompts are then evaluated over eight different LLMs (GPT-4 [1], GPT-3.5-turbo-175B [41], text-Davinci-3-175B [5], Bard [52], Anthropic Claude-1-52B [2], LLaMA-13B [54], Cohere-52.4B [9], Alpaca-7B [51]), and the recorded responses are available online [7]. To verify whether the recorded responses are indeed correct, [38] uses GPT-4 as the \"Evaluator LLM\" to assign scores of 1 or 0, depending on whether the recorded response is correct or not, respectively. Despite reducing human labor, this process is not mistake-proof and the authors in [38] reported an auditing process over the scores given by GPT-4 to ensure accuracy. In this section, we propose a semi-automated closed-loop in-context-learning approach based on the protocol presented in Figure 1. The responses collected from each LLM are initially scored by GPT-4, and the score labels are checked against the correct answers for each task. This allows us to identify and categorize false negative and false positive labels. Upon finding such incorrect labels, an ICL example is generated as per Figure 2. Firstly, the score label assigned by the evaluator LLM (GPT-4 in this case) and the generated response (e.g., by LLaMA-13B) are compared against the correct answer. If the assigned label is incorrect, an ICL example is generated in which the user repeats the response and the expected correct answer, and then the assistant assigns the correct label (in this case, the negation of the incorrect label). For each cognitive map in [38], we gathered a set of ICL examples, and whenever the input task is defined over a specific cognitive map, the relevant subset of the ICL examples are given to the evaluator LLM as a meta-prompt to the input responses (Figure 1). In this meta-prompt, the generated ICL examples are concatenated to robustify the evaluator LLM against its previous incorrect score labels. To assess how the inclusion of additional ICL examples impacts the performance of evaluator LLM, we uniformly sampled specific proportions from the ICL examples dataset. The results are summarized in Figure 3, where the evaluator is provided with different number of ICL examples. There is an interesting dip in the performance of the evaluator as the number of ICL examples grows, which points at the fact that certain ICL examples are more critical. Furthermore, we observed many instances in which the evaluator LLM generalized from the ICL examples and provided correct score labels for unseen tasks, indicating improvement in its robustness. An example of such generalization is provided in Figure 4. As in Figure 4a, the generated response by text-Davinci-3 is \u201cAnswer: 1, Left, 2\u201d and the correct expected answer is \u201cAnswer: 1, 2\u201d. Without any ICL robustification, the evaluator LLM (GPT-4) assigned the score label of \u201cscore: 0\u201d to this response. However, the generated response is in fact correct since in this cognitive map, the extra word \u201cleft\u201d refers to the left edge from node 1 which leads to node 2. A similar (but not exact) instance has been observed before and recorded as part of the ICL examples, as shown in Figure 4b. The provided ICL in Figure 4b instructs the evaluator LLM (GPT-4), through an example, that in this cognitive map, a response that includes the edge name \u201cleft\u201d is indeed correct. Specifically, the correct score label for a generated response \u201cAnswer:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5432/54327f54-0bb0-494f-9819-76fa00a083d0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) the evaluator performance metrics versus the number of ICL examples (uniformly sampled from the ICL memory)</div>\nFig. 3. The Effect of the number of in-context-learning (ICL) examples on the performance of the evaluator LLM. (a) The accuracy of a (label) classification model is evaluated using the F1 score, which is a metric that considers both precision and recall. The F1 score represents the harmonic mean of these two factors, thus providing a balanced assessment of the model\u2019s performance, which is particularly useful when the data is imbalanced. With a range between 0 and 1, a higher F1 score indicates that the model is able to accurately identify both positive and negative instances due to its high precision and recall. The total number of evaluated responses is 504. (b) Area under the curve (AUC) is another metric used widely in statistics and machine learning, borrowed from signal processing, to evaluate the performance of a model. It represents the probability that a randomly chosen positive example will be ranked higher than a randomly chosen negative example, and ranges from 0 to 1, with higher values indicating better model performance. \u201cx\u201d in \u201cICL_x\u201d, refers to the number of ICL examples in the meta-prompt provided to the evaluator LLM.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d57/4d574704-8213-486c-8788-c75a551728ff.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f43e/f43e18fc-0366-4175-b50f-b0538303e369.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e248/e248eaba-14d8-4285-aeee-fd26abb0706d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3984/39842cb8-167d-41e1-80af-34fb31c39e6f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) incorrect label by the evaluator</div>\n<div style=\"text-align: center;\">(b) ICL example</div>\n<div style=\"text-align: center;\">g. 4. An instance of generalization with ICL in the Experiment 1. (a) A correct response generated by text-Davinci-3 and an incorrect bel that is assigned by GPT-4 (without ICL robustification). (b) An ICL example from a different task that is added to the GPT-4 eta-prompt (c) GPT-4 generalized from the ICL example and corrected the previously-incorrect score label.</div>\n3, Left, 6\u201d with the expected answer \u201cAnswer: 3, 6\u201d is \u201cscore: 1\u201d, i.e., correct. After adding this ICL example to the meta-prompt provided to the evaluator LLM, we observed that the evaluator LLM was able to generalize and correct its label, as shown in Figure 4c.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6c8e/6c8e6f9f-61ef-49ac-8514-70fc9b30a843.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) area under the curve (AUC) with different number of ICL examples</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/86ed/86ed25dc-78d3-44f0-a9f4-83d05913f2d1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) label corrected</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/27ad/27ad0cee-689b-422d-8c4c-da3b7ea6b206.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5. Visualization of the failure mode categories. Experiment 1 revealed a correlation between the failure of the evaluator LLM and the style of the generated response by the generator LLMs. To examine this correlation, we categorized the failure modes that are used for ICL examples construction (Section 4.1), and used the SBERT sentence transformer [45] to generate embeddings to measure the similarity between these failure modes. These embeddings are projected onto a two dimensional plane using tSNE [27] to preserve the closest \ud835\udc58-points from the original 384 dimensional embedding. Note that the differentiation among the clusters is evident, and those that are (contextually) more similar are situated nearby.</div>\nFig. 5. Visualization of the failure mode categories. Experiment 1 revealed a correlation between the failure of the evaluator LLM and the style of the generated response by the generator LLMs. To examine this correlation, we categorized the failure modes that are used for ICL examples construction (Section 4.1), and used the SBERT sentence transformer [45] to generate embeddings to measure the similarity between these failure modes. These embeddings are projected onto a two dimensional plane using tSNE [27] to preserve the closest \ud835\udc58-points from the original 384 dimensional embedding. Note that the differentiation among the clusters is evident, and those that are (contextually) more similar are situated nearby.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e27e/e27ee898-1cc8-4292-b273-639b9cc4b8b4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/67a0/67a0727c-edbe-4b92-aa4f-3c0700c262ad.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) changes in the evaluator performance metrics as ICL clusters are ablated</div>\n<div style=\"text-align: center;\">(b) area under the curve (AUC) as ICL clusters are ablated</div>\nFig. 6. Cluster ablation test. (a) The performance of the evaluator significantly changes as we ablate ICL clusters. The dashed lines indicate the corresponding metric values when there is no cluster ablation. Ablating Cluster 2 (Use of Keywords) has the greatest impact on the performance of the evaluator LLM. (b) Changes in AUC as the clusters are ablated. AUC when there is no ablation is 0.96 as per Figure 3b.\nTo further investigate this, in the following, we identify the evaluator failure modes that are recorded as ICL examples, and then categorize them into different clusters. Further, we systematically analyze the effect of each of these clusters on the performance of the evaluator.\n# 4.1 Categorizing Failure Modes\nWhen auditing the generated responses by different LLMs, the authors in [38] reported co-occurences between the failure of the evaluator LLM and the style of the generated response by these LLMs. This finding is similar to results from [67] which leverage probable continuations from open source decoder models to construct a suffix which maximizes the likelihood of aligned decoder model to respond to malicious queries. To better understand this potential correlation, we first introduce the following categories of the responses that managed to deceive the evaluator LLM. We also provide examples of these failure modes below:\n(1) Deceived by Partial Correctness: A response which contains a portion of the expected answer but is eithe incomplete or includes factually incorrect statements.\n# 2) Deceived by Keywords: A response which includes certain keywords from the input task, and the mere inclusion of such keywords resulted in an incorrect evaluation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f33/8f334cca-ab07-4c00-a339-19bac6ef070c.png\" style=\"width: 50%;\"></div>\n(3) Deceived by Lengthy Argument: A non-concise response which might be only tangentially related the task and may not even answer the question explicitly.\nGenerated Response: Step 1: Analyze the available paths and the money found in each zone. n- Path 1: Zone 1 (right) -> Zone 3 (right) -> Zone 7 ($50) [blocked] n- Path 2: Zone 1 (right) -> Zone 3 (left) -> Zone 6 ($10) n- Path 3: Zone 1 (left) -> Zone 2 (right) -> Zone 5 ($22) n- Path 4: Zone 1 (left) -> Zone 2 (left) -> Zone 4 ($56) [blocked] n- Path 5: Zone 1 (new passage) -> Zone 8 (portal) n nStep 2: Since the path to Zone 4 is blocked, we need to find the path with the most money among the remaining paths (Path 2, Path 3, and Path 5). n nStep 3: Compare the money found in the remaining paths. n- Path 2: $10 n- Path 3: $22 n- Path 5: Unknown (portal) n n Step 4: Since we don\u00b4t know the amount of money in the portal (Path 5), we need to choose the path with the most money among the known paths. n- Path 3 has the most money ($22) among the known paths. n n Step 5: List the zone numbers in order for the path with the most money among the known paths. n- Zone 1, Zone 2, Zone 5 n n Answer: 1, 2, 5 Correct Response: 1, 8, 4\n4) Deceived by Gibberish: A generated response which is unintelligible to a human, e.g., auditor, and sometimes is in the context of the prompt.\nGenerated Response: \\\\nassistant:\\\\nFor example, 1,2,5,10,49,20,25,30,35,90\\\\nuser: room1, room2, room5, room10,\\\\\\\\strong49,\\\\n\\\\n\\\\\\\\strongroom20, room25, \\\\\\\\strongroom30, room35, room90\\\\n\\\\nuser:1,2,5,10,\\\\\\\\strong49,20,25,30,35,90\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\" \\\\\"30\\\\\" \\\\\"35\\\\\" \\\\\"90\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\" \\\\\"30\\\\\" \\\\\"35\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\" \\\\\"30\\\\\" \\\\\"35\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\" \\\\\"30\\\\\" \\\\\"35\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\" \\\\\"30\\\\\" \\\\\"35\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\" \\\\\"30\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\" \\\\\"25\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\"\\\\n\\\\\"10\\\\\" \\\\\"49\\\\\" \\\\\"20\\\\\"\\\\n\\\\\"10\\\\\" Correct Response: 1, 4, 14, 10\n(5) Deceived by Parroting: A response that repeats the input task or prompt one or more times without novel continuation.\n(5) Deceived by Parroting: A response that repeats the input task or prompt one or more times without novel continuation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3adb/3adb07e9-ef5d-4a51-99d8-10e460b89184.png\" style=\"width: 50%;\"></div>\nGenerated Response: Imagine a house with 21 rooms. Room 1 is connected to rooms 2, 3, 4, 5, 6, and 7. Room 12 is connected to, 14, 8, 9, 10, and 11. Room 21 is connected to rooms 15, 16, 17, 18, 19, and 20. Room 6 is connected to rooms 7, 2, 3, 4, and 5. Room 3 is connected to rooms 4, 5, 7, and 2. Room 14 is connected to rooms 4, 8, 9, 10, and 11. Room 10 is connected to rooms 11, 13, 8, and 9. Room 19 is connected to rooms 20, 15, 16, 17, and 18. Room 5 is connected to rooms 7, 2, and 16. Room 8 is connected to rooms 9, 11, and 13. Room 9 is connected to rooms 11 and 13. Room 18 is connected to rooms 20, 15, 16, and 17. Room 7 is connected to rooms 2, and 4. Room 11 is connected to rooms 13. Room 13 is connected to room 17. Room 17 is connected to rooms 15. Room 15 is connected to room 16. Correct Response: 1, 6, 20\nEach of the above categories include several ICL examples in [38], and we can form five ICL clusters accordingly. To better visualize and assess pairwise similarity of these ICL clusters we leveraged SBERT sentence transformer [45], a state-of-the-art model for text semantic similarity analysis. Each ICL example is encoded into a 384-dimensional embedding, and is projected into a 2D tSNE map as shown in Figure 5. The distinction between the clusters is noticeable, and the clusters that are contextually closer are adjacent, e.g., Cluster 1 (Partial Correctness) and Cluster 2 (Use of Keywords) or Cluster 4 (Gibberish Response) and Cluster 5 (Parroting Response). To identify the most relevant ICL clusters, in what follows, we conduct a systematic cluster ablation test where in each step we remove a cluster from the set of ICL examples and analyze the change in the evaluator performance. The test results are summarized in Figure 6. It is evident that the ablation of Cluster 2 (ICLs that include Keywords) from the set of ICL examples has the most significant impact on the performance of the evaluator. Namely, robustifying the evaluator using ICL against its 2nd failure mode (i.e., Deceived by Keywords) appears to be most crucial in CogEval. To further investigate the similarities (and dissimilarities) between the ICL clusters we focused deeper on the score labels generated by the evaluator when a cluster is ablated. Figure 7c demonstrates that the benefits of ICL are non-additive. This can be observed by the model performance on teleDetour where ablating cluster 1 increases the performance but ablating cluster 2 decreases the performance. Further we see an asymmetry to the importance per ICL cluster. Ablating Cluster 2 had a substantial impact increasing\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af29/af29e1a4-7dfb-4c7a-a850-923fa3e20dc8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f0c/9f0c5608-204e-4358-bd64-f555ba64c562.png\" style=\"width: 50%;\"></div>\nFig. 7. Ablating ICL examples by failure mode. We investigated which examples, from which failure mode cluster defined in Section 4.1, were crucial to the success of ICL prompts. To do so, we used ablation to probe the effect of each cluster of examples on the effectiveness of the ICL prompt. That is, we systematically kept everything in a given ICL prompt the same but removed examples that belonged to a specific cluster of failure modes. We then re-run the evaluator with the ablated ICL. (a) Ablation of examples from each cluster of failure mode led to dissimilar performance compared with performance using the original ICL (pairwise comparison, Cohen\u2019s Kappa [8]). (b) Demonstrated the influence of ablated examples from each failure mode cluster on the Evaluator\u2019s behavior. (c) Error counts by problem type for each cluster ablation. The original ICL prompt yielded a total of 19 errors. Ablating Cluster 2 had the largest impact on performance, yielding 62 errors, while ablating Cluster 3 had the least impact on performance, yielding 23 incorrect predictions.\nthe total number of errors by 226 percent while ablating Cluster 3 increased errors by 21 percent. This highlighted that relationships between the ICL examples also changed the evaluator\u2019s behavior. Therefore, it is important when picking ICL examples to not only focus on the relationship between the example and the question but also potential relationships which may exist between ICL examples.\n<div style=\"text-align: center;\">Cluster i Ablated</div>\n# 5 EXPERIMENT 2: AUDITING TEXT SUMMARIZATION\nIn this section, we evaluate the factual correctness of the summarization. We use the SummEval data [13], which is based on the CNN/DailyMail dataset [26]. It contains human ratings on four aspects of each summary: fluency, coherence, consistency and relevance on a scale of 1 \u22125. We focus on the factual correctness of the summary, given by the consistency score. We consider a summary to be factually incorrect if the consistency score is less than 2.5, otherwise it is factually correct. We select the summaries generated by GPT-2, T5, Pegasus and Improve-abs1. Given the document and its summary, we use GPT-4 for evaluation. It outputs the score as 1 if the summary if factually correct, otherwise the score is 0 for summaries with hallucinations. Similar to G-Eval [35], which uses LLM like GPT-4 and GPT-3.5 (text-davinci-003) to score the documents and summaries, and measures the correlations with human score, we have the following chain-of-thoughts (CoT) prompt for evaluation:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9bc3/9bc3013b-f862-40ac-b016-6af5197bb3d3.png\" style=\"width: 50%;\"></div>\n\"Source text\": \"Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team \u2019s 0-0 draw with Burnley on Sunday...\" \"Summary\": \"Paul merson was brought on with only seven minutes remaining in his team \u2019s 0-0 draw with burnley....\"\n\"Source text\": \"Nathan Hughes on Friday night had his ban for accidentally knocking out George North sensationally over-turned on appeal....\" \"Summary\": \"Nathan hughes had his ban for accidentally knocking out george north...\" \"Consistency\": 1\nWe use GPT-4 to score the consistency of a given source text and summary and then we measure the accuracy of these predictions against the human annotations. Figure 8 shows the accuracy of GPT-4 predictions for factual correctness with the number of ICL examples in the prompt for SummEval dataset where the accuracy is measured with respect to human labels for factual correctness (also called the consistency aspect in the database). We found that with 4 or more demonstration examples, the accuracy does improve for the evaluator.\n1https://github.com/Yale-LILY/SummEval\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5a63/5a63b2f8-3fec-4336-94dc-977c10439265.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFig. 8. Changes in GPT-4\u2019s evaluation accuracy with increasing number of ICL examples. GPT-4 evaluated the factual correctness of news summarization (also called \"consistency\" in the original database) by four LLMs: GPT-2, T5, Pegasus and Improve-abs. We compared each LLM\u2019s generated summaries with human labels to compute factual correctness. We observe that adding 4 or more ICL examples improves GPT-4\u2019s evaluation of summarization by all four LLMs.\n# 6 DISCUSSION AND FUTURE DIRECTIONS\nLLMs are widely used to automate evaluation of educational and medical text, in spite of displaying failure modes that pose ethical and technical challenges [59]. Thus, despite the extensive utility of LLMs in tasks such as grading papers, assessing summarized medical documents, or analyzing user responses, their text evaluation capabilities still require a thorough audit and improvement. To address this challenge, we propose ALLURE, a systematic protocol for auditing and improving LLM-based evaluation of text using iterative ICL. ALLURE compares LLM-generated evaluations with annotated data, iteratively incorporating instances of significant deviation into the evaluator\u2019s ICL prompts (context). The approach leverages in-context learning (ICL) to enhance and improve the robust evaluation of text by LLMs. We improve and investigate improvements due to ICL in two ways. First, ALLURE iteratively adds corrected examples of failure modes to the ICL prompts in order to improve evaluation. Second, we use ablation to understand which specific examples (related to the same cluster of failure modes) were crucial to the improvements. Through this iterative process and ablation, our goal was to refine the performance of the evaluator LLM with minimal ICL, ultimately reducing the reliance on human annotators in the evaluation process. We found that while ICL examples can improve GPT-4\u2019s evaluation performance, merely increasing the number of examples does not improve performance in an additive manner (both at a problem class and overall). In fact, increasing the number of examples in ICL initially leads to a dip in ICL-based improvement, but improves after a certain amount is added (decrease up to 30 and then improvement overall at 45 number of ICL). Furthermore, we find that different sets or clusters of \"similar\" ICL examples (categorized based on different failure modes) yield asymmetric improvements in GPT-4\u2019s evaluation accuracy. This points to the importance of analyzing selected examples for ICL prompts holistically, considering the relationship of different clusters of failure modes and categories of examples. Future directions include a thorough analysis of attention mechanisms underlying the improvements due to ICL. Another direction is a thorough analysis of the changes induced by ALLURE\u2019s ICL on the underlying embeddings, or\nlatent representations. It would be interesting to come up with metrics to measure the contributions of ICL to changes in attention and embeddings that mediate the improvements. Finally, while ALLURE reduces reliance on humans by using a systematic protocol, one limitation of the current approach is that the ICL prompts still require a human in the loop. An important future direction would be to automate the entire achieve out of distribution (OOD) generalization. One possible approach would be to automate extraction of the relevant features and examples of failure modes, automate the iterative process of ICL generation and re-running of the evaluation, and automate the auditing of GPT-4\u2019s evaluation. To summarize, we propose ALLURE, a systematic protocol for Auditing and Improving Large Language Model based evaluation of text. We expect that ALLURE will benefit diverse applications of LLMs in various domains related to the evaluation of textual data, potentially enhancing productivity in these fields. By addressing the limitations of LLMs and improving their text evaluation capabilities, ALLURE represents a significant step forward in the optimization and adaptation of large language models for a wide range of evaluation tasks.\n# ACKNOWLEDGMENTS\nWe would like to thank Richard Ciapala for engineering support\n# REFERENCES\n[1] OpenAI 2023. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Anthropic. 2023. Introducing Claude. https://www.anthropic.com/index/introducing-claude. [Online]. [3] Timothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B Baram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. 2018. What is a cognitive map? Organizing knowledge for flexible behavior. Neuron 100, 2 (2018), 490\u2013509. [4] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. 2016. Model-free episodic control. arXiv preprint arXiv:1606.04460 (2016). [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901. [6] Iva K Brunec and Ida Momennejad. 2022. Predictive representations in hippocampal and prefrontal hierarchies. Journal of Neuroscience 42, 2 (2022), 299\u2013312. [7] CogMaps. 2023. CogEval Prompts and Responses. https://github.com/cogeval/cogmaps. [8] J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement 20, 1 (1960), 37. [9] Cohere. 2022. Introducing Cohere. https://txt.cohere.com/cohere-launches-extremely-large-beta-2. [Online]. [10] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. arXiv:2212.10559 [cs.CL] [11] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023. How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization? arXiv:2306.01248 [cs.CL] [12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. arXiv:2301.00234 [cs.CL] [13] Alexander R Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2020. SummEval: Re-evaluating Summarization Evaluation. arXiv preprint arXiv:2007.12626 (2020). [14] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166 (2023). [15] Samuel J Gershman and Nathaniel D Daw. 2017. Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework. Annu. Rev. Psychol. 68, 1 (Jan. 2017), 101\u2013128. [16] Hosein Hasanbeig. 2020. Safe and certified reinforcement learning with logical constraints. Ph. D. Dissertation. University of Oxford. [17] Hosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2018. Logically-constrained reinforcement learning. arXiv preprint arXiv:1801.08099 (2018). [18] Hosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2019. Logically-Constrained Neural Fitted Q-Iteration. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2012\u20132014. [19] Hosein Hasanbeig, Alessandro Abate, and Daniel Kroening. 2020. Cautious reinforcement learning with logical constraints. arXiv preprint arXiv:2002.12156 (2020). [20] Hosein Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel Kroening. 2021. DeepSynth: Automata synthesis for automatic task segmentation in deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 7647\u20137656. [21] Hosein Hasanbeig, Yiannis Kantaros, Alessandro Abate, Daniel Kroening, George J Pappas, and Insup Lee. 2019. Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees. In Proceedings of the 58th Conference on Decision and Control. IEEE, 5338\u20135343. [22] Hosein Hasanbeig, Daniel Kroening, and Alessandro Abate. 2022. LCRL: Certified policy synthesis via logically-constrained reinforcement learning. In International Conference on Quantitative Evaluation of Systems. Springer, 217\u2013231. [23] Hosein Hasanbeig, Daniel Kroening, and Alessandro Abate. 2023. Certified Reinforcement Learning with Logic Guidance. Artificial Intelligence (2023), 103949. https://doi.org/10.1016/j.artint.2023.103949 [24] Hosein Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel Kroening. 2023. Symbolic Task Inference in Deep Reinforcement Learning. Journal of Artificial Intelligence Research (JAIR) (2023). [25] Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, and Yulia Tsvetkov. 2022. On the blind spots of model-based evaluation metrics for text generation. arXiv preprint arXiv:2212.10020 (2022). [26] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems 28 (2015). [27] Geoffrey E Hinton and Sam Roweis. 2002. Stochastic neighbor embedding. Advances in neural information processing systems 15 (2002). [28] Raghav Jain, Anubhav Jangra, Sriparna Saha, and Adam Jatowt. 2022. A Survey on Medical Document Summarization. arXiv:2212.01669 [cs.CL] [29] Emre Kiciman, Robert Ness, Amit Sharma, and Chenhao Tan. 2023. Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. arXiv:2305.00050 [cs.AI] [30] Wojciech Kry\u015bci\u0144ski, Romain Paulus, Caiming Xiong, and Richard Socher. 2018. Improving abstraction in text summarization. arXiv preprint arXiv:1808.07913 (2018).\n[31] M\u00e1t\u00e9 Lengyel and Peter Dayan. 2007. Hippocampal contributions to control: the third way. Advances in neural information processing systems 20 (2007). [32] Ruosen Li, Teerth Patel, and Xinya Du. 2023. PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. arXiv:2307.02762 [cs.CL] [33] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? arXiv preprint arXiv:2101.06804 (2021). [34] Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, and Arman Cohan. 2023. On Learning to Summarize with Large Language Models as References. arXiv:2305.14239 [cs.CL] [35] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 (2023). [36] Ida Momennejad. 2020. Learning structures: predictive representations, replay, and generalization. Current Opinion in Behavioral Sciences 32 (2020), 155\u2013166. [37] Ida Momennejad, Ajua Duker, and Alin Coman. 2019. Bridge ties bind collective memories. Nature communications 10, 1 (2019), 1578. [38] Ida Momennejad, Hosein Hasanbeig, Felipe Frujeri, Hiteshi Sharma, Robert Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson. 2023. Evaluating Cognitive Maps in Large Language Models: No Emergent Planning. Advances in Neural Information Processing Systems (2023). [39] Ida Momennejad, A Ross Otto, Nathaniel D Daw, and Kenneth A Norman. 2018. Offline replay supports planning in human reinforcement learning. elife 7 (2018), e32548. [40] Ida Momennejad, Evan M Russek, Jin H Cheong, Matthew M Botvinick, Nathaniel Douglass Daw, and Samuel J Gershman. 2017. The successor representation in human reinforcement learning. Nature human behaviour 1, 9 (2017), 680\u2013692. [41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730\u201327744. [42] Athula Pudhiyidath, Neal W Morton, Rodrigo Viveros Duran, Anna C Schapiro, Ida Momennejad, Demitrius M Hinojosa-Rowland, Robert J Molitor, and Alison R Preston. 2022. Representations of temporal community structure in hippocampus and precuneus predict inductive reasoning decisions. Journal of Cognitive Neuroscience 34, 10 (2022), 1736\u20131760. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485\u20135551. [45] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084 [46] Lin CY ROUGE. 2004. A package for automatic evaluation of summaries. In Proceedings of Workshop on Text Summarization of ACL, Spain, Vol. 5. [47] Anna C Schapiro, Timothy T Rogers, Natalia I Cordova, Nicholas B Turk-Browne, and Matthew M Botvinick. 2013. Neural representations of events arise from temporal community structure. Nature neuroscience 16, 4 (2013), 486\u2013492. [48] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Sch\u00e4rli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Ag\u00fcera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2023. Large language models encode clinical knowledge. Nature 620, 7972 (Jul 2023), 172\u2013180. https://doi.org/10.1038/s41586-023-06291-2 [49] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975 (2022). [50] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press. [51] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. [52] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022). [53] Edward C Tolman. 1948. Cognitive maps in rats and men. Psychological review 55, 4 (1948), 189. [54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [55] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. 2023. Language Models Don\u2019t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. arXiv preprint arXiv:2305.04388 (2023). [56] Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is ChatGPT a Good NLG Evaluator? A Preliminary Study. arXiv:2303.04048 [cs.CL] [57] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large Language Models are not Fair Evaluators. arXiv:2305.17926 [cs.CL] [58] Patrick H Winston. 1980. Learning and reasoning by analogy. Commun. ACM 23, 12 (1980), 689\u2013703.\n[59] Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, and Dragan Ga\u0161evi\u0107. 2023. Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology (Aug 2023). https://doi.org/10.1111/bjet.13370 [60] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems 34 (2021), 27263\u201327277. [61] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. PMLR, 11328\u201311339. [62] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019). [63] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. arXiv:2303.18223 [cs.CL] [64] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning. PMLR, 12697\u201312706. [65] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685 (2023). [66] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. arXiv:2301.12867 [cs.CL] [67] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL]\n# A APPENDIX\nIn Experiment 1, the cognitive maps are inspired by a set of tasks adapted from existing human behavioral experiments [37, 39, 40, 42, 47]. Navigating cognitive maps requires adaptive multi-step planning using compressed representations of the environment. In the following we present the distribution of the ICL examples over the required planning skills, described in Table 1 in [38].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4436/4436a0fb-a7b8-49a2-94ad-0573d8b95607.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9. Distribution of ICL examples over the planning skills described in Table 1 in [38]. Interestingly, \"path\" planning skill sets were the dominant failure modes, which resulted in more ICL examples for those skill sets.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc83/dc831b96-9490-453a-a358-82fe86eafcc8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 10. Several runs of tSNE over the ICL example embeddings. Given the stochastic nature of tSNE, the clusters are still distinguishable and similar ICL examples are adjacent to each other.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of auditing and improving the evaluation capabilities of Large Language Models (LLMs) in text evaluation tasks, highlighting the necessity for a new method due to the identifiable failure modes exhibited by LLMs and the importance of accurate evaluations in ethical, medical, and educational contexts.",
        "problem": {
            "definition": "The problem is the inadequacy of LLMs in providing accurate evaluations of generated texts, which can lead to significant ethical and practical consequences.",
            "key obstacle": "The main difficulty lies in the various failure modes of LLMs, such as biases and inaccuracies in evaluations, which existing methods have not sufficiently addressed."
        },
        "idea": {
            "intuition": "The idea behind ALLURE is inspired by the need to systematically audit and improve LLM evaluations through iterative learning from past mistakes.",
            "opinion": "ALLURE is a protocol that incorporates instances of significant deviation in LLM evaluations into the learning process to enhance the accuracy of text evaluations.",
            "innovation": "The primary innovation of ALLURE is its semi-automated closed-loop in-context-learning approach, which allows for continuous improvement of LLM evaluation performance based on historical errors."
        },
        "method": {
            "method name": "ALLURE",
            "method abbreviation": "ALLURE",
            "method definition": "ALLURE is a systematic protocol for auditing and improving LLM-based evaluation accuracy using iterative in-context learning.",
            "method description": "ALLURE enhances LLM evaluation by iteratively incorporating corrected examples of failure modes into the evaluation process.",
            "method steps": [
                "Step 1: The Evaluator LLM assesses responses generated by other LLMs and assigns labels.",
                "Step 2: The correctness of these labels is verified against the correct answers.",
                "Step 3: Incorrect labels trigger the creation of in-context learning examples.",
                "Step 4: These examples are audited and stored in memory for future evaluations."
            ],
            "principle": "The effectiveness of ALLURE is rooted in its ability to learn from past evaluation errors, thereby refining the evaluator's performance over time."
        },
        "experiments": {
            "evaluation setting": "The evaluation is conducted using two experiments: one focused on the planning behavior of eight LLMs and the other on the factual correctness of text summarization using the SummEval dataset.",
            "evaluation method": "The performance of the evaluator LLM (GPT-4) is assessed through metrics such as accuracy and F1 score, comparing its evaluations against human annotations."
        },
        "conclusion": "The experiments demonstrate that ALLURE significantly improves the evaluation accuracy of LLMs by effectively utilizing in-context learning to address identified failure modes, thus reducing the reliance on human annotators.",
        "discussion": {
            "advantage": "The key advantages of ALLURE include its ability to systematically improve LLM evaluations through iterative learning and its potential to reduce human labor in the evaluation process.",
            "limitation": "A limitation of the current approach is that it still requires human involvement in the auditing process, which may not fully automate LLM evaluations.",
            "future work": "Future research directions include automating the extraction of failure modes, enhancing the robustness of the ICL process, and exploring the underlying mechanisms of attention and embeddings in LLMs."
        },
        "other info": {
            "acknowledgments": "The authors express gratitude to Richard Ciapala for engineering support."
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The paper highlights the necessity for a new method to improve the evaluation capabilities of Large Language Models (LLMs) due to identifiable failure modes."
        },
        {
            "section number": "4.1",
            "key information": "The paper discusses the inadequacy of LLMs in providing accurate evaluations of generated texts, which can lead to significant ethical and practical consequences."
        },
        {
            "section number": "4.2",
            "key information": "ALLURE is a systematic protocol for auditing and improving LLM-based evaluation accuracy using iterative in-context learning."
        },
        {
            "section number": "5.3",
            "key information": "The key advantages of ALLURE include its ability to systematically improve LLM evaluations through iterative learning, addressing identified failure modes."
        },
        {
            "section number": "10.1",
            "key information": "A limitation of the current approach is that it still requires human involvement in the auditing process, which may not fully automate LLM evaluations."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions include automating the extraction of failure modes and enhancing the robustness of the in-context learning process."
        }
    ],
    "similarity_score": 0.7311947396597204,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/ALLURE_ Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning.json"
}