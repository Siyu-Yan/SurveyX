{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.12785",
    "title": "In-Context Learning of Energy Functions",
    "abstract": "In-context learning is a powerful capability of certain machine learning models that arguably underpins the success of today\u2019s frontier AI models. However, in-context learning is critically limited to settings where the in-context distribution of interest pICL \u03b8 (x|D) can be straightforwardly expressed and/or parameterized by the model; for instance, language modeling relies on expressing the next-token distribution as a categorical distribution parameterized by the network\u2019s output logits. In this work, we present a more general form of in-context learning without such a limitation that we call in-context learning of energy functions. The idea is to instead learn the unconstrained and arbitrary in-context energy function EICL \u03b8 (x|D) corresponding to the in-context distribution pICL \u03b8 (x|D). To do this, we use classic ideas from energy-based modeling. We provide preliminary evidence that our method empirically works on synthetic data. Interestingly, our work contributes (to the best of our knowledge) the first example of in-context learning where the input space and output space differ from one another, suggesting that in-context learning is a more-general capability than previously realized.",
    "bib_name": "schaeffer2024incontextlearningenergyfunctions",
    "md_text": "# In-Context Learning of Energy Functions\nRylan Schaeffer 1 Mikail Khona 2 Sanmi Koyejo 1\n# Abstract\nIn-context learning is a powerful capability of certain machine learning models that arguably underpins the success of today\u2019s frontier AI models. However, in-context learning is critically limited to settings where the in-context distribution of interest pICL \u03b8 (x|D) can be straightforwardly expressed and/or parameterized by the model; for instance, language modeling relies on expressing the next-token distribution as a categorical distribution parameterized by the network\u2019s output logits. In this work, we present a more general form of in-context learning without such a limitation that we call in-context learning of energy functions. The idea is to instead learn the unconstrained and arbitrary in-context energy function EICL \u03b8 (x|D) corresponding to the in-context distribution pICL \u03b8 (x|D). To do this, we use classic ideas from energy-based modeling. We provide preliminary evidence that our method empirically works on synthetic data. Interestingly, our work contributes (to the best of our knowledge) the first example of in-context learning where the input space and output space differ from one another, suggesting that in-context learning is a more-general capability than previously realized.\narXiv:2406.12785v1\n# 1. Introduction\nProbabilistic modeling often aims to learn and/or sample from a probability distribution. In the specific context of in-context learning, the distribution of interest is oftentimes a conditional distribution where some data D is provided \u201cin-context\u201d:\n(1)\n|D For concreteness, the in-context data might be text (Brown et al., 2020), synthetic linear regression covariates and tar-\n*Equal contribution 1Computer Science, Stanford University 2Physics, MIT. Correspondence to: Rylan Schaeffer <rschaef@cs.stanford.edu>, Sanmi Koyejo <sanmi@cs.stanford.edu>.\nProceedings of the 1 st Workshop on In-Context Learning at the 41 st International Conference on Machine Learning, Vienna, Austria. 2024. Copyright 2024 by the author(s).\ngets (Garg et al., 2022), or images and assigned classes (Chan et al., 2022). Directly learning this conditional distribution can be straightforward if the probability distribution can be easily parameterized; for instance, next-token prediction can be readily specified as a classification problem, where the conditional distribution is a categorical distribution parameterized by the model\u2019s output logits. However, this limits the expressivity of in-context learning to situations where the conditional distribution can be straightforwardly parameterized.\nIn this work, we explore a more general form of in-context learning with no such constraint on how readily the conditional distribution can be specified. We call this more general form in-context learning of energy functions. The key insight is that rather than dealing with the constrained conditional distribution, we instead re-express it in its Boltzmann distribution form (Bishop & Nasrabadi, 2006):\n(2)\nwhere Z(\u03b8) def = \ufffd x\u2208X exp(\u2212E(x)) dx . This alternative form is preferable because the energy function is an arbitrary unconstrained function E : X \u00d7 D \u2192R that can be used to express any probability distribution without requiring a particular form. We then propose learning the in-context energy function EICL \u03b8 (x|D) rather than the constrained in-context conditional distribution pICL \u03b8 (x|D), which we accomplish by drawing upon well-established ideas in probabilistic modeling called energy-based models (Hinton, 2002; Mordatch, 2018; Du & Mordatch, 2019; Du et al., 2020).\n# 2. In-Context Learning of Energy Functions 2.1. Learning In-Context Energy Functions\nOur goal is to learn the in-context energy function:\nEICL \u03b8 (x|D)\n(3)\nWhat concretely does this mean? We seek a model with parameters \u03b8 that accepts as input a dataset D with arbitrary cardinality and a single datum x, and adaptively changes its output energy function EICL \u03b8 (x|D) based on the input dataset D without changing its parameters \u03b8.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/16d6/16d67ac5-0eac-481f-854d-e12e40c54b38.png\" style=\"width: 50%;\"></div>\nFigure 1. In-Context Learning of Energy Functions. Transformers learn to compute energy functions EICL \u03b8 (x|D) corresponding to probability distributions pICL(x|D), where D are in-context datasets that vary during pretraining. At inference time, when conditioned on a new in-context dataset, the transformer computes a new energy function using fixed network parameters \u03b8. The transformers\u2019 energy landscapes progressively sharpen as additional in-context training data are conditioned upon (left to right). Bottom. The energy function EICL \u03b8 (x|D) can be used to compute a gradient with respect to x that enables sampling higher probability points, without requiring a restricted parametric form for the corresponding conditional probability distribution pICL \u03b8 (x|D).\nFor concreteness, in the context of conditional probabilistic modeling, a causal transformer is typically trained to output a conditional probability distribution at every index, i.e.,\nInstead of learning each conditional distribution p\u03b8(xn|x<n), we instead learn the corresponding energy function E\u03b8(xn|x<n). This means that the transformer instead outputs a scalar at every index, regardless of the shape of the inputs:\nThis scalar at each index is the model\u2019s estimate of the energy at the last (nth) input datum, based on an energy function constructed from the previous n \u22121 datapoints. To achieve this practically, we use causal GPT-style transformers (Vaswani et al., 2017; Radford et al., 2018; 2019). Just like with standard in-context learning of language models, we train our transformers by minimizing the negative log\n# conditional probability, averaging over possible in-context datasets:\n (4)\nDue to the intractable partition function in Eqn. 4, we minimize the loss using contrastive divergence (Hinton, 2002). Letting x+ denote real training data and x\u2212denote confabulated (i.e. synthetic) data sampled from the learned energy function, the gradient of the loss function can be reexpressed in a more manageable form:\nn c t i o n t r a i n i n g s t e p ( batch ) : # Compute energy on r e a l data . r e a l d a t a = batch [ \u201d r e a l d a t a \u201d ] e n e r g i e s o n r e a l d a t a = transformer ebm . forward ( r e a l d a t a ) # Sample new c o n f a b u l a t e d data using Langevin MCMC. i n i t i a l s a m p l e d d a t a = batch [ \u201d i n i t i a l s a m p l e d d a t a \u201d ] c o n f a b d a t a = sample data with langevin mcmc ( r e a l d a t a , i n i t i a l s a m p l e d d a t a ) # Compute energy on sampled c o n f a b u l a t o r y data . e n e r g i e s o n c o n f a b d a t a = zeros ( . . . ) for s e q i d x in range ( max seq len ) : for c o n f i d x in range ( n c o n f a b u l a t e d s a m p l e s ) : r e a l d a t a u p t o s e q i d x = clone ( r e a l d a t a [ : , : s e q i d x +1 , : ] ) r e a l d a t a u p t o s e q i d x [ : , \u22121 , : ] = sampled data [ : , conf idx , seq idx , : ] e n e r g y o n c o n f a b d a t a = transformer ebm . forward ( r e a l d a t a u p t o s e q i d x ) e n e r g i e s o n c o n f a b d a t a [ : , conf idx , seq idx , : ] += e n e r g y o n c o n f a b d a t a [ : , \u22121 , # Compute d i f f e r e n c e in energy between r e a l and c o n f a b u l a t o r y data . d i f f o f e n e r g y = e n e r g i e s o n r e a l d a t a \u2212e n e r g i e s o n c o n f a b d a t a # Compute t o t a l l o s s . t o t a l l o s s = mean ( d i f f o f e n e r g y )\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ef5/3ef59f94-5e8b-403f-a0c2-bdaf9566e267.png\" style=\"width: 50%;\"></div>\nreturn t o t a l l o s s\nFigure 2. Pseudocode for Training In-Context Learning of Energy Funct\nThis equation tells us that we can minimize the negative log likelihood by equivalently minimizing the energy of real data (conditioning upon the in-context data) context while simultaneously maximizing the energy of confabulated data (again conditioning upon the in-context data). Training Python pseudocode is given in Figure 2.\n# 2.2. Sampling From In-Context Energy Functions\nTo sample from the conditional distribution pICL \u03b8 (x|D), we follow standard practice in energy-based modeling (Hinton, 2002; Du & Mordatch, 2019; Du et al., 2020): We first choose N data (deterministically or stochastically) to condition on, and sample x\u2212 0 \u223cU for some distribution U to compute the initial energy E\u03b8(x\u2212 0 |D). We then use Langevin dynamics to iteratively increase the probability of x\u2212 0 by sampling with \u03c9t \u223cN(0, \u03c32) and minimizing the energy with respect to x\u2212 t for t = [T] steps:\nThis in-context learning of energy functions is akin to Mordatch (2018), but rather than conditioning on a \u201cmask\u201d and \u201cconcepts\u201d, we instead condition on sequences of data from the same distribution and we additionally replace the all-toall relational network with a causal transformer.\n# 2.3. Preliminary Experimental Results of In-Context Learning of Energy Functions\nAs proof of concept, we train causal transformer-based ICLEBMs on synthetic mixture-of-Gaussian datasets. The transformers have 6 layers, 8 heads, 128 embedding dimensions, and GeLU nonlinearities (Hendrycks & Gimpel, 2016). The transformers are pretrained on a set of randomly sampled synthetic 2-dimensional mixture of three Gaussians with uniform mixing proportions with Langevin noise scale 0.01 and 15 MCMC steps of size \u03b1 = 3.16. After pretraining, we then freeze the ICL-EBMs\u2019 parameters and measure whether the model can adapt its energy function to new in-context datasets drawn from the same distribution as the pretraining datasets. The energy landscapes of frozen ICL EBMs display clear signs of in-context learning (Fig. 1).\n# 3. Discussion\nTo the best of our knowledge, this is the first instance of in-context learning where the input and output spaces differ. This stands in stark comparison with more common examples of in-context learning such as language modeling (Brown et al., 2020), linear regression (Garg et al., 2022) and image classification (Chan et al., 2022). Our method is\nsimilar to that of Mordatch (2018), as well as M\u00a8uller et al. (2022). Our results demonstrate that transformers are more capable of different types of in-context learning than previously known, and our results demonstrate that transformers can successfully learn energy functions rather than probability distributions. Although our results are quite preliminary, we believe this is an exciting direction that can be pushed significantly further.\nLimitations Energy-based models require differentiating with respect to network inputs during training, often with tens to hundreds of backwards steps per batch, making training these models significantly more expensive than standard pretraining. Future work should aim to solve this problem.\n# References\nBishop, C. M. and Nasrabadi, N. M. Pattern recognition and machine learning, volume 4. Springer, 2006.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020.\nChan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022.\nInternational Conference on Learning Representations, 2022. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of in-context learning in machine learning models, specifically the constraint of having to express conditional distributions straightforwardly, which hampers the expressivity of these models.",
        "problem": {
            "definition": "The problem is the restricted expressivity of in-context learning methods, which are limited to scenarios where the conditional distribution can be easily parameterized.",
            "key obstacle": "The main challenge is that existing methods require a specific form for the conditional distribution, which limits their applicability and effectiveness."
        },
        "idea": {
            "intuition": "The idea stems from the observation that in-context learning can be generalized beyond the constraints of parameterized distributions, allowing for a broader range of applications.",
            "opinion": "The proposed idea is to learn an unconstrained energy function that corresponds to the in-context distribution, rather than relying on a constrained conditional distribution.",
            "innovation": "This method innovates by allowing the input space and output space to differ, which is a significant departure from traditional in-context learning approaches."
        },
        "method": {
            "method name": "In-Context Learning of Energy Functions",
            "method abbreviation": "ICLEF",
            "method definition": "The method involves learning an arbitrary energy function that can express any probability distribution, without requiring a specific parametric form.",
            "method description": "The core of the method lies in learning an energy function that adapts based on the input dataset without changing the model's parameters.",
            "method steps": "1. Input dataset D and datum x are provided. 2. The model computes the energy function based on D. 3. The model minimizes the negative log likelihood using contrastive divergence. 4. Energy is computed for real and confabulated data. 5. The model updates its energy function iteratively.",
            "principle": "The effectiveness of this method arises from its ability to represent any distribution through an unconstrained energy function, allowing for greater flexibility in modeling complex relationships."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using causal transformer-based energy-based models (ICLEBMs) trained on synthetic mixture-of-Gaussian datasets.",
            "evaluation method": "The performance was assessed by freezing the model parameters after pretraining and measuring the model's ability to adapt its energy function to new datasets drawn from the same distribution."
        },
        "conclusion": "The experiments indicate that the proposed method can effectively learn energy functions, demonstrating the potential for in-context learning to be more versatile than previously understood.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to learn energy functions rather than constrained probability distributions, allowing for more complex in-context learning scenarios.",
            "limitation": "A significant limitation is the computational expense of training energy-based models, which require extensive differentiation with respect to inputs.",
            "future work": "Future research should focus on improving the efficiency of training these models to make them more practical for broader applications."
        },
        "other info": {
            "info1": "This work presents the first instance of in-context learning with differing input and output spaces.",
            "info2": {
                "info2.1": "The method uses Langevin MCMC for sampling.",
                "info2.2": "Preliminary results show clear signs of in-context learning in the energy landscapes."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning is constrained by the need to express conditional distributions straightforwardly, which limits the expressivity of machine learning models."
        },
        {
            "section number": "3.2",
            "key information": "The proposed method learns an unconstrained energy function that corresponds to the in-context distribution, allowing for a broader range of applications beyond traditional parameterized distributions."
        },
        {
            "section number": "3.3",
            "key information": "The method, In-Context Learning of Energy Functions (ICLEF), innovates by allowing the input space and output space to differ, which is a significant departure from conventional in-context learning approaches."
        },
        {
            "section number": "4.1",
            "key information": "The proposed approach allows for learning energy functions rather than constrained probability distributions, which can significantly influence the outcomes of in-context learning."
        },
        {
            "section number": "6.2",
            "key information": "A significant limitation of the proposed method is the computational expense of training energy-based models, which require extensive differentiation with respect to inputs."
        },
        {
            "section number": "7",
            "key information": "The experiments indicate that the proposed method can effectively learn energy functions, demonstrating the potential for in-context learning to be more versatile than previously understood."
        }
    ],
    "similarity_score": 0.6939128798293761,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning of Energy Functions.json"
}