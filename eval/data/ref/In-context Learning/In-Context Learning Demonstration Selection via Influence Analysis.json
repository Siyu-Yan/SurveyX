{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.11750",
    "title": "In-Context Learning Demonstration Selection via Influence Analysis",
    "abstract": "Large Language Models (LLMs) have showcased their In-Context Learning (ICL) capabilities, enabling few-shot learning without the need for gradient updates. Despite its advantages, the effectiveness of ICL heavily depends on the choice of demonstrations. Selecting the most effective demonstrations for ICL remains a significant research challenge. To tackle this issue, we propose a demonstration selection method named InfICL, which utilizes influence functions to analyze impacts of training samples. By identifying the most influential training samples as demonstrations, InfICL aims to enhance the ICL generalization performance. To keep InfICL cost-effective, we only use the LLM to generate sample input embeddings, avoiding expensive fine-tuning. Through empirical studies on various real-world datasets, we demonstrate advantages of InfICL compared to state-ofthe-art baselines.",
    "bib_name": "s2024incontextlearningdemonstrationselection",
    "md_text": "# IN-CONTEXT LEARNING DEMONSTRATION SELECTION VIA INFLUENCE ANALYSIS\n<div style=\"text-align: center;\">IN-CONTEXT LEARNING DEMONSTRATION SELECTION VIA INFLUENCE ANALYSIS</div>\nA PREPRINT\nVinay M.S.\u2217\nUniversity of Arkansas\nFayetteville, AR 72701, USA\nvmadanbh@uark.edu\nMinh-Hao Van\u2217\nUniversity of Arkansas\nFayetteville, AR 72701, USA\nhaovan@uark.edu\nXintao Wu\nUniversity of Arkansas\nFayetteville, AR 72701, USA\nxintaowu@uark.edu\n# ABSTRACT\nLarge Language Models (LLMs) have showcased their In-Context Learning (ICL) capabilities, enabling few-shot learning without the need for gradient updates. Despite its advantages, the effectiveness of ICL heavily depends on the choice of demonstrations. Selecting the most effective demonstrations for ICL remains a significant research challenge. To tackle this issue, we propose a demonstration selection method named InfICL, which utilizes influence functions to analyze impacts of training samples. By identifying the most influential training samples as demonstrations, InfICL aims to enhance the ICL generalization performance. To keep InfICL cost-effective, we only use the LLM to generate sample input embeddings, avoiding expensive fine-tuning. Through empirical studies on various real-world datasets, we demonstrate advantages of InfICL compared to state-ofthe-art baselines.\narXiv:2402.11750v2 \nKeywords large language models \u00b7 in-context learning \u00b7 demonstration selection \u00b7 influence functions\n# 1 Introduction\narXiv:2402.11\nLarge Language Models (LLMs) have demonstrated their ability to perform few-shot inference through In-Conte Learning (ICL) [Brown et al., 2020]. Specifically, by providing a few demonstrations for the given task, the LLM  able to perform test case inference without performing any model gradient update.\nLarge Language Models (LLMs) have demonstrated their ability to perform few-shot inference through In-Context Learning (ICL) [Brown et al., 2020]. Specifically, by providing a few demonstrations for the given task, the LLM is able to perform test case inference without performing any model gradient update. ICL has several benefits such as few-shot learning, avoiding model fine-tuning, and versatility to different learning tasks. Despite these benefits, the ICL performance is sensitive to the selected demonstrations. To address this limitation, many different approaches have been proposed for demonstration selection, e.g., selecting demonstrations which are similar to the test case in the embedding space [Gao et al., 2021, Liu et al., 2022, Wu et al., 2023, Qin et al., 2023, Yang et al., 2022], learning a deep learning-based demonstration retriever [Rubin et al., 2022, Luo et al., 2023, Chen et al., 2020, Karpukhin et al., 2020, Scarlatos and Lan, 2023, Zhang et al., 2022, Li et al., 2023], selecting demonstrations based on LLM feedback [Li and Qiu, 2023, Chen et al., 2023b, Wang et al., 2023], etc. However, there is a lack of consensus regarding the most effective demonstration selection approach [Nguyen and Wong, 2023]. The current research challenge is to identify those demonstrations which are the most effective or influential for improving the ICL generalization performance. We address this challenge by employing influence functions [Koh and Liang, 2017]. Specifically, influence functions provide mechanisms to analyze effects or influences of training samples on the model without retraining the model. For example, influence functions can be used to analyze the model effects after up-weighting or removing a training sample. The training samples which have higher influences naturally provide more contributions to the model learning process. Intuitively, identifying these influential training samples can aid in improving the ICL generalization performance. In this work, we focus on the text classification problem, and propose an influence function analysis-based demonstration selection method called InfICL. Since we need to perform influence function analysis on the training samples, an obvious approach is to calculate these influence scores by using the LLM itself [Grosse et al., 2023]. However, for\narXiv:2402\n*These authors contributed equally to this work.\nlarge and complex deep learning models, the influence function analysis becomes erroneous [Basu et al., 2021]. Another approach is to fine tune the final layers of the LLM and perform influence function analysis by using these final layers. However, fine tuning LLM is a highly resource intensive task. To address these practical challenges, we only employ the LLM to generate sample embeddings. By employing these LLM generated training sample embeddings, we train a simple classifier. We analyze the influence of each training sample by using the classifier and a validation set. Finally, we select the most influential training samples from each class as the demonstration set. We summarize our main contributions below.\n\u2022 We propose a ICL demonstration selection method called InfICL which is based on influence function analysis. \u2022 We present a running cost analysis study and compare our InfICL to other advanced influence analysisbased demonstration selection methods [Nguyen and Wong, 2023, Chang and Jia, 2023]. In particular, we demonstrate that these contemporary methods require an exceedingly high number of LLM access calls in comparison to our InfICL. \u2022 We present an empirical study conducted on multiple real-world datasets and four LLMs of varying sizes. In this empirical study, we show that our InfICL can outperform the contemporary demonstration selection methods.\n# 2 Related Work\nOur work mainly focuses on designing an demonstration selection method for ICL through influence analysis. Demonstration Selection. Recently, the problem of demonstration selection for ICL has received a significant attention in the literature. We direct the interested readers to [Liu et al., 2021, Dong et al., 2023] for detailed surveys regrading different demonstration selection methods. One of the popular approaches for demonstration selection is to select those training samples as demonstrations which are similar to the test sample in the embedding space [Gao et al., 2021, Liu et al., 2022, Wu et al., 2023, Qin et al., 2023, Yang et al., 2022]. Another popular approach is to employ a demonstration retriever to perform demonstration selection. Specifically, the demonstration retriever is a deep learning based model. Rubin et al. [2022] and Luo et al. [2023] train their demonstration retriever by employing contrastive loss [Chen et al., 2020]. Li et al. [2023] employ in-batch negative loss [Karpukhin et al., 2020]. Scarlatos and Lan [2023] and Zhang et al. [2022] employ reinforcement learning to train their demonstration retriever. In our work, we do not utilize any complex demonstration retriever, and design a simple method which operates on LLM embeddings. Recently, LLM feedback based demonstration selection methods have been proposed. Specifically, the LLM is queried for its prediction confidence on each training sample. Li and Qiu [2023] identify training samples which are more informative. Chen et al. [2023b] select training points which are less sensitive to predictions. Wang et al. [2023] fine tune the LLM by using only the final emdedding layer and model the demonstration selection as a topic model. These methods can also be considered as influence based methods because they analyze the influence of training samples by using direct LLM feedback. Influence Functions. For machine learning applications, influence functions have been used for different tasks, e.g., filtering or relabeling mislabeled training data [Kong et al., 2022], designing data poisoning attacks [Fang et al., 2020, Jagielski et al., 2021], designing data augmentation strategies [Lee et al., 2020, Oh et al., 2021], and analyzing label memorization effects [Feldman and Zhang, 2020]. For LLMs, influence functions have been used to identify data artifacts [Han et al., 2020], identify biases in word embeddings [Brunet et al., 2019], and explaining the LLM performance [Grosse et al., 2023, Han and Tsvetkov, 2021].\nOur work mainly focuses on designing an demonstration selection method for ICL through influence analysis. Demonstration Selection. Recently, the problem of demonstration selection for ICL has received a significant attention in the literature. We direct the interested readers to [Liu et al., 2021, Dong et al., 2023] for detailed surveys regrading different demonstration selection methods. One of the popular approaches for demonstration selection is to select those training samples as demonstrations which are similar to the test sample in the embedding space [Gao et al., 2021, Liu et al., 2022, Wu et al., 2023, Qin et al., 2023, Yang et al., 2022]. Another popular approach is to employ a demonstration retriever to perform demonstration selection. Specifically, the demonstration retriever is a deep learning based model. Rubin et al. [2022] and Luo et al. [2023] train their demonstration retriever by employing contrastive loss [Chen et al., 2020]. Li et al. [2023] employ in-batch negative loss [Karpukhin et al., 2020]. Scarlatos and Lan [2023] and Zhang et al. [2022] employ reinforcement learning to train their demonstration retriever. In our work, we do not utilize any complex demonstration retriever, and design a simple method which operates on LLM embeddings. Recently, LLM feedback based demonstration selection methods have been proposed. Specifically, the LLM is queried for its prediction confidence on each training sample. Li and Qiu [2023] identify training samples which are more informative. Chen et al. [2023b] select training points which are less sensitive to predictions. Wang et al. [2023] fine tune the LLM by using only the final emdedding layer and model the demonstration selection as a topic model. These methods can also be considered as influence based methods because they analyze the influence of training samples by using direct LLM feedback. Influence Functions. For machine learning applications, influence functions have been used for different tasks, e.g., filtering or relabeling mislabeled training data [Kong et al., 2022], designing data poisoning attacks [Fang et al., 2020, Jagielski et al., 2021], designing data augmentation strategies [Lee et al., 2020, Oh et al., 2021], and analyzing label memorization effects [Feldman and Zhang, 2020]. For LLMs, influence functions have been used to identify data artifacts [Han et al., 2020], identify biases in word embeddings [Brunet et al., 2019], and explaining the LLM performance [Grosse et al., 2023, Han and Tsvetkov, 2021]. Influence analysis can be broadly divided into two categories: retraining based [Ilyas et al., 2022] and gradient based methods also called as influence functions [Koh and Liang, 2017]. The retraining based methods collect random subsets of the training set. Then, the influence of each training sample in the collected subset is calculated by either model retraining or by learning a linear surrogate. However, the retraining based methods have high running costs, and are not scalable to large datasets because to effectively cover all the training samples, a large number of subsets have to be constructed and evaluated [Grosse et al., 2023]. Nguyen and Wong [2023] and Chang and Jia [2023] employ retraining based influence analysis to construct the demonstration sets and as a result, their proposed demonstration selection methods incur high running costs. We provide a detailed design description about these demonstration selection methods and compare their running costs against our InfICL in Section 3.2. Specifically, we show that by using the gradient based influence analysis for constructing demonstration sets, we can overcome the high running cost challenge associated with the retraining based influence analysis methods.\n# 3 Proposed Method\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a85/0a851523-7014-4655-95c1-4d3d9eb308aa.png\" style=\"width: 50%;\"></div>\nFigure 1: Illustration of ICL for the text classification task through our InfICL. Initially, by employing the local LLM, embeddings for all the training and validation set inputs are generated. A local classifier is then trained by employing training input embeddings and labels. InfICL determines K demonstration examples based on influence scores. Finally, the demonstration set and each test case are sent to an external LLM for inference. We consider the text classification task having a training set T with n training points denoted as zi = {(xi, yi)}n i=1. Here, xi and yi denote the embedding vector for the ith training sample input si and its corresponding label, respectively. Let C denote the class set for the target variable yi and yi \u2208C. We employ a validation set denoted as V.\nFigure 1: Illustration of ICL for the text classification task through our InfICL. Initially, by employing the local LLM, embeddings for all the training and validation set inputs are generated. A local classifier is then trained by employing training input embeddings and labels. InfICL determines K demonstration examples based on influence scores. Finally, the demonstration set and each test case are sent to an external LLM for inference.\n<div style=\"text-align: center;\">Figure 1: Illustration of ICL for the text classification task through our InfICL. Initially, by employing the local LLM, embeddings for all the training and validation set inputs are generated. A local classifier is then trained by employing training input embeddings and labels. InfICL determines K demonstration examples based on influence scores. Finally, the demonstration set and each test case are sent to an external LLM for inference.</div>\n# 3.1 Algorithm\nFigure 1 shows our influence analysis based demonstration selection method. We employ separate LLMs for demonstration selection and test case inference called local LLM P and external LLM Q, respectively. For local LLM P, to reduce training costs, we employ a light-weight LLM and use it to generate embeddings for the input texts. Let EP denote the embedding layer of P which generates the sample embeddings. Here, xi = EP(si, \u03d5), where parameter \u03d5 \u2208\u03a6, and \u03a6 denotes the local LLM (P) parameter space. We denote Lnt(si, \u03d5) as the next token prediction loss for P. For external LLM Q, we opt a powerful and heavier LLM. We include a local classifier denoted as F(xi, \u03b8) with the input of embeddings and parameterized with \u03b8 \u2208\u0398, and \u0398 denotes the classifier (F) parameter space. We denote Lf(zi, \u03b8) as the classifier training loss. Our goal is to select K suitable demonstrations for the given text classification task. Note that K is analogous to the number of shots in few-shot learning and is constrained by the employed external LLM. We employ a balanced selection approach wherein we select equal number of demonstrations from each class c \u2208C. Specifically, we select R (R = \u230aK/|C|\u230b) suitable training set points from each class as demonstrations. Algorithm 1 shows the pseudo code of our InfICL. The inputs include training set T , validation set V, classifier F, loss Lf, the number of demonstration examples per class R, and local LLM P. Initially, by employing the local LLM P, we generate embeddings for all training and validation inputs. In lines 2-4, we train the local classifer F using the embeddings and labels. Next, we calculate influence score of each training point (lines 5-7). For each class c \u2208C, we select the top-R training points {zc i }R i=1 as demonstrations from T based on influence scores (lines 8-10). Finally, we return the constructed demonstration set \u222ac\u2208C{zc i }R i=1.\nAlgorithm 1 InfICL demonstration selection.\nInputs: T , V, F, Lf, R, and P.\nOutput: demonstration set \u222ac\u2208C{zc\ni }R\ni=1.\n1: generate embeddings for all training and validation inputs through P;\n2: for each training epoch do\n3:\ntrain classifier F on T by using Lf;\n4: for each zi = (xi, yi) \u2208T do\n5:\ncalculate its influence score by using Eq 1;\n6: for each class c \u2208C do\n7:\nselect top-R training points {zc\ni }R\ni=1 from T based on influence scores;\n8: return \u222ac\u2208C{zc\ni }R\ni=1\nInfluence Functions. The main goal of the influence functions is to study the effect of training points on model prediction [Koh and Liang, 2017]. Influence functions provide a practical solution wherein, the model parameter change can be studied without retraining the model. Let 1 n \ufffdn i=1 Lf(zi, \u03b8) be the empirical risk and its minimizer is given by \ufffd\u03b8 = arg min\u03b8\u2208\u0398 1 n \ufffdn i=1 Lf(zi, \u03b8). It is assumed that the empirical risk is twice differentiable and strictly convex. However, this assumption can be practically relaxed. The influence of up-weighting training point z on the classifier parameter \u03b8 can be calculated by using the influence function as\n\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd \ufffd where H\ufffd\u03b8 = 1 n \ufffdn i=1 \u22072 \u03b8Lf(zi, \ufffd\u03b8) is the Hessian and it is positive definite by assumption. Next the influence of up-weighting z on the loss Lf at a validation point zval \u2208 V is given by\n \ufffd \ufffd  For the entire validation set V, the influence of up-weighting\n \ufffd \ufffd  \ufffd  the entire validation set V, the influence of up-weighting z on the loss Lf at V is given by\n \ufffd \ufffd  \ufffd e influence of up-weighting z on the loss Lf at V is given\n\uf8f0  \ufffd \uf8fb  \ufffd Specifically, the highly influential training points are those with most positive (\u2212Iup,loss(z, V)) scores [Koh and Liang, 2017]. We employ Inf(z, V) = \u2212Iup,loss(z, V) as the influence score to analyze the influence of up-weighting each training point z on the loss Lf at V. This is because, the training points which have high influences on the validation loss provide richer information for model learning, and can become better demonstrations for the ICL task. Personalized Demonstration Selection. We can easily extend InfICL to construct a personalized demonstration set for each test case xtest. Specifically, we can extend InfICL to this setting by scoring each training point zi \u2208T as\nscore(zi) = \u03bbInf(zi, V) + (1 \u2212\u03bb)sim(xi, xtest) (2) where sim(\u00b7, \u00b7) denotes the cosine similarity between the input embeddings, and \u03bb is the weight which can be set by analyzing the accuracy performance on the validation set. The top-R training points from each class based on score(zi) are included in the demonstration set.\n# 3.2 Running Cost Analysis\nIn this section, we study the running costs of our InfICL along with other influence analysis based demonstration selection methods, Influence [Nguyen and Wong, 2023] and Curation [Chang and Jia, 2023]. Note that both methods employ retraining based influence analysis approach and our InfICL employs gradient based influence analysis approach. We show running cost benefits of our InfICL over Influence and Curation. We quantify the running costs of demonstration selection methods by analyzing the total number of LLM access (API) calls for both local and external LLMs. Specifically, the unit cost of local LLM (P) access call for generating an embedding for a single training point is denoted as CP. Similarly, the unit cost of external LLM (Q) access call for performing inference on a single test or validation case is denoted as CQ. Note that CQ is usually much higher than CP. This is because CQ involves ICL cost w.r.t external LLM and CP only generates the final layer embeddings which\nwhere\n(2)\nonly incurs model forward pass cost. We show the running costs of different influence analysis based demonstration selection methods in Table 1 and provide a detailed description below.\n<div style=\"text-align: center;\">Table 1: Running cost analysis of influence analysis based demonstration selection methods.</div>\nMethods\nInfluence [Nguyen and Wong, 2023]\nCuration [Chang and Jia, 2023]\nInfICL\nCondAcc\nData Models\nRunning Cost\nO (CQ|V|M)\nO (CQ|V|MK!)\nO (CQ|V|M)\nO (CP|T | + CQ)\nInfICL. We generate embeddings for all training points and generating embedding for each training point requires a single local LLM access call. Thus, the total cost of local LLM access calls for embedding generation is CP|T |. For the test case inference, we require a single external LLM access call and the cost is CQ. Thus, the the running cost of our demonstration selection method is given by O (CP|T | + CQ). For influence estimation, we use a fully connected neural network as the backbone architecture for the classifier F. Let d be the number of parameters in F. Calculating the loss of |T | training samples takes O(d|T |). In the implementation, we use LiSSA Agarwal et al. [2017] method to approximate the inverse Hessian-Vector product (iHVP) of \ufffd1 |V| \ufffd zj\u2208V \u2207\u03b8Lf \ufffd zj, \ufffd\u03b8 \ufffd\ufffd\u22a4 H\u22121 \ufffd\u03b8 , which costs O(|V|d + rjd) where r is the recursion depth and j is the number of repeats. As both validation set and \u03b8 are fixed, there is only one computation of iHVP. The sorting time needed for ranking potential demonstrations by influence analysis is O(|T | log(|T |)) on average. Consequently, the influence estimation process takes O(d|T | + |V|d + rjd + |T | log(|T |)). In a practical setting, |V| is sufficiently small compared to |T | (|V| \u226a|T |) and rj \u2248|T |. Therefore, the running time for calculating influence scores is O(d|T | + |T | log(|T |)). Influence [Nguyen and Wong, 2023]. Initially, M random demonstrations are constructed from T . For each constructed demonstration set Si where |Si| = K, its ICL generalization performance on the entire validation set V is calculated by using the external LLM. Then, the influence of each training point zj \u2208Si is calculated as the difference between the average performance of demonstration sets including zj and the average performance of demonstration sets omitting zj. Through this design analysis, we can infer the running cost of Influence as O (CQ|V|M). Curation [Chang and Jia, 2023]. There are two variants: CondAcc and Data Models. Specifically, the CondAcc variant is almost similar to Influence. However, for each constructed random demonstration set, the ICL generalization performance of its each permutation on V is separately evaluated. Thereby, the running cost of CondAcc is given by O (CQ|V|MK!). In the Data Models variant, a surrogate linear model is trained to mimic the prediction performance of the external LLM. Similar to Influence, M random demonstration sets are constructed. Each random demonstration set is used to train a separate linear model. For a given random demonstration set, the employed linear model training loss calculates the difference between generalization performances of linear model and external LLM (based on ICL) on the validation set. After this training, the influence of each training point belonging to a random demonstration set is calculated by analyzing the linear model parameters. Through this design analysis, we can infer the running cost of Data Models as O (CQ|V|M). The running costs of both Influence and Curation are dominated by the term CQ|V|M. Here, M which denotes the number of constructed random demonstration sets, needs to be large in-order to effectively cover the entire training set, and to obtain good estimates of influence scores [Nguyen and Wong, 2023]. As a consequence, both Influence and Curation incur an extremely large amount of external LLM access calls. For InfICL, we approximately require |T | local LLM access calls, which makes InfICL much more cost-effective than both Influence and Curation.\n# 3.3 Design Intuitions\nIn this section, we describe our intuitions behind the design of our InfICL. Specifically, we describe about the plausibility that the influential training points identified for the classifier F can also become influential for both local LLM P and external LLM Q. For our analysis, to differentiate influence functions for classifier F and local LLM P, we denote Iup,params(zi, \u03b8) and Iup,params(si, \u03d5) as the up-weighted influence functions for F and P, respectively. Here, the up-weighted influence for local LLM P w.r.t next token prediction loss Lnt is given by:\n\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd Consider the scenario when the embedding space is clustered and training points in the same cluster share the same label. This scenario is not unrealistic because P tends to generate closer embeddings for those training inputs which are similar to each other and share the same label. Consider two training points zi = (xi, yi) and zj = (xj, yj) belonging to dense and sparse clusters, respectively. Influence functions typically assign higher influence scores to\n<div style=\"text-align: center;\">Table 2: Dataset Details.</div>\nDataset\nSize\nPositive Class\n|T |\n|V|\nTest Set Size\nCoLA\n9594\n70%\n8466\n85\n1043\nRTE\n2717\n50%\n2466\n24\n277\nSST2\n20872\n50%\n19800\n200\n872\ntraining points from sparse clusters compared to those from dense clusters. This is because, in dense clusters, the removal of a single training point is compensated for by the many similar points within the cluster that can effectively fill its absence. Hence, for the classifier F, we can hypothesize that Iup,params(zi, \u03b8) \u2264Iup,params(zj, \u03b8). Since the embedding space is generated by the local LLM P, we can apply the same argument used for F, and can further hypothesize that for P we have that Iup,params(si, \u03d5) \u2264Iup,params(sj, \u03d5). Therefore, the influential training points for F can also become influential for P. Most LLMs are pre-trained using the next token prediction strategy and memorize their underlying training data. Consequently, the external LLM Q tends to generate a dense cluster containing si and numerous other similar training inputs in its own embedding space. As a result, si tends to have lower influence than sj for Q. Thus, it is plausible that the influential training points for the local LLM P can also be influential for the external LLM Q. This hypothesis was also empirically validated in [Grosse et al., 2023].\n# 4 Experiments\n# 4.1 Experimental Setup\nDatasets. We use three real-world datasets for our empirical evaluation study, Corpus of Linguistic Acceptability (CoLA) [Warstadt et al., 2018], Recognizing Textual Entailment (RTE) [Dagan et al., 2005], and Stanford Sentiment Tree-bank version2 (SST2) [Socher et al., 2013]. The CoLA dataset contains sentences from different linguistics publications, which are expertly annotated for grammatical acceptability by their original authors. Each sentence is either labeled as acceptable or unacceptable. The RTE dataset sample contains two text fragments denoted as premise and hypothesis, and the corresponding label indicates whether the meaning of the hypothesis can be inferred from the text (yes or no). The SST2 is a sentiment analysis dataset wherein, each sentence is labeled as either positive or negative. Table 2 shows dataset details including training, validation, and test splits. Baselines. We employ two different groups of baselines called the non-influence analysis based baselines which select demonstrations without analyzing influences of training points and influence analysis based baselines which employ influence score calculation to select demonstrations. We select three non-influence analysis based baselines: Zero-shot which directly performs test case inference without any demonstrations, Random where demonstrations are selected based on random sampling, and RICES [Yang et al., 2022] where the training points are scored based on their cosine similarity to the test sample in the embedding space and then the top-R training points from each class are selected as demonstrations. We select three influence analysis based baselines: Influence [Nguyen and Wong, 2023], CondACC and Data Models [Chang and Jia, 2023]. We have described these three baselines in Section 3.2. We also compare InfICL against another simple baseline called Classifier where we directly employ a three layer neural network for test case inference. Training Details. We employ Llama-2-7B [Touvron et al., 2023] as the local LLM. For the external LLM, we separately evaluate on OPT-6.7B, Llama-2-7B, Llama-2-13B, and Llama-2-70B. All Llama-family models are chat versions. The embedding size is 4096. For the classifier, we employ a fully connected neural network with three layers. All experiments are executed on V100-32GB GPU with Intel Xeon 6258R for small models and A100-40GB with AMD EPYC 7543 for large models. We train the classifier using Adam optimizer in 20 epochs with learning rates of 0.001 for CoLA and 0.01 for RTE and SST2.\n# 4.2 Experimental Results\nComparison to non-influence analysis based baselines. We show performances of our InfICL and non-influence analysis based baselines on external LLMs in Table 3. Clearly, our InfICL shows an overall better performance than Zero-shot, Random, and RICES across all three datasets and four external LLMs. Zero-shot does not involve any demonstrations. Therefore, the external LLM does not get any opportunity to better understand the given task and as a result, Zero-shot performance is not noticeable. Random under-performs compared to InfICL, indicating that randomly selecting demonstrations does not offer a high-quality learning opportunity to the LLM. Although RICES\n<div style=\"text-align: center;\">Table 3: Performances of our InfICL and non-influence analysis based baselines (mean\u00b1std) for external LLMs. Scores are reported after 5 runs. For each external LLM, the best values for each shot are bold highlighted. \u2018N/A\u2019 denotes non-applicable and \u2018\u2013\u2019 denotes non-feasible results due to the limitation of LLM\u2019s context length.</div>\nExternal LLM (Q)\nShots (K)\nMethod\nCoLA\nRTE\nSST2\nAccuracy (%)\u2191\nF1 (%)\u2191\nAccuracy (%)\u2191\nF1 (%)\u2191\nAccuracy (%)\u2191\nF1 (%)\u2191\nN/A\nN/A\nClassifier\n82.83 \u00b10.00\n88.18 \u00b10.00\n57.76 \u00b10.00\n58.95 \u00b10.00\n94.50 \u00b10.00\n94.48 \u00b10.00\nLlama-2-7B\n0\nZero-shot\n63.39 \u00b10.00\n68.81 \u00b10.00\n69.19 \u00b10.00\n68.83 \u00b10.00\n88.76 \u00b10.00\n88.11 \u00b10.00\n8\nRandom\n70.35 \u00b13.68\n75.70 \u00b14.53\n74.97 \u00b10.21\n77.31 \u00b11.19\n93.58 \u00b11.82\n93.88 \u00b11.54\nRICES\n70.74 \u00b10.41\n78.50 \u00b10.28\n77.38 \u00b11.16\n80.34 \u00b10.61\n93.88 \u00b10.07\n94.12 \u00b10.06\nInfICL\n74.19 \u00b12.39\n81.10 \u00b12.49\n77.26 \u00b11.25\n80.16 \u00b11.36\n94.92 \u00b10.70\n95.04 \u00b10.66\n16\nRandom\n70.20 \u00b12.30\n75.54 \u00b12.6\n77.02 \u00b10.83\n79.24 \u00b11.20\n93.16 \u00b12.84\n93.58 \u00b12.39\nRICES\n73.71 \u00b10.52\n80.97 \u00b10.42\n76.77 \u00b11.37\n80.44 \u00b11.29\n93.88 \u00b10.96\n94.10 \u00b10.92\nInfICL\n74.75 \u00b11.32\n81.39 \u00b10.92\n78.58 \u00b10.55\n80.98 \u00b10.41\n95.26 \u00b10.07\n95.39 \u00b10.13\n32\nRandom\n73.00 \u00b11.68\n78.74 \u00b12.00\n77.38 \u00b11.10\n79.87 \u00b11.02\n91.78 \u00b14.22\n92.43 \u00b13.43\nRICES\n74.02 \u00b10.51\n80.96 \u00b10.86\n73.89 \u00b10.55\n75.86 \u00b10.48\n91.82 \u00b10.18\n92.02 \u00b10.12\nInfICL\n73.48 \u00b10.74\n79.50 \u00b11.19\n77.74 \u00b10.55\n79.92 \u00b11.14\n95.15 \u00b10.13\n95.30 \u00b10.09\nLlama-2-13B\n0\nZero-shot\n50.07 \u00b10.00\n45.29 \u00b10.00\n77.25 \u00b10.00\n78.82 \u00b10.00\n84.40 \u00b10.00\n86.07 \u00b10.00\n8\nRandom\n73.17 \u00b13.76\n78.53 \u00b15.15\n80.39 \u00b10.21\n82.51 \u00b10.80\n95.49 \u00b10.13\n95.61 \u00b10.11\nRICES\n73.42 \u00b10.92\n81.37 \u00b10.75\n77.86 \u00b11.16\n81.89 \u00b10.84\n94.30 \u00b10.57\n94.59 \u00b10.49\nInfICL\n76.66 \u00b11.71\n82.31 \u00b11.47\n82.43 \u00b12.21\n84.25 \u00b11.74\n95.64 \u00b10.80\n95.67 \u00b10.85\n16\nRandom\n75.40 \u00b11.48\n81.48 \u00b11.98\n82.31 \u00b11.57\n84.08 \u00b11.29\n95.60 \u00b10.40\n95.70 \u00b10.36\nRICES\n73.94 \u00b10.88\n82.11 \u00b10.48\n79.66 \u00b11.37\n82.80 \u00b11.27\n93.04 \u00b11.47\n93.50 \u00b11.26\nInfICL\n77.47 \u00b10.32\n84.58 \u00b10.47\n83.63 \u00b10.21\n85.08 \u00b10.39\n95.87 \u00b10.11\n95.94 \u00b10.16\n32\nRandom\n75.95 \u00b11.74\n83.06 \u00b11.27\n81.76 \u00b11.26\n82.49 \u00b11.54\n94.72 \u00b10.60\n94.96 \u00b10.51\nRICES\n73.23 \u00b10.70\n82.12 \u00b10.69\n77.08 \u00b10.91\n77.70 \u00b10.99\n92.51 \u00b11.92\n93.05 \u00b11.65\nInfICL\n76.05 \u00b10.81\n84.20 \u00b10.41\n82.67 \u00b11.08\n83.67 \u00b11.14\n95.95 \u00b10.13\n96.04 \u00b10.15\nOPT-6.7B\n0\nZero-shot\n66.92 \u00b10.00\n80.07 \u00b10.00\n54.15 \u00b10.00\n60.44 \u00b10.00\n54.82 \u00b10.00\n54.29 \u00b10.00\n8\nRandom\n63.37 \u00b10.17\n75.43 \u00b12.64\n56.92 \u00b12.73\n67.98 \u00b12.81\n60.78 \u00b10.30\n71.74 \u00b10.24\nRICES\n64.30 \u00b10.11\n76.85 \u00b10.20\n55.60 \u00b11.30\n69.02 \u00b10.06\n69.72 \u00b10.70\n58.66 \u00b11.30\nInfICL\n63.50 \u00b10.78\n76.76 \u00b10.33\n57.76 \u00b10.63\n70.43 \u00b10.80\n91.40 \u00b11.39\n91.95 \u00b11.12\n16\nRandom\n62.03 \u00b10.50\n77.07 \u00b12.87\n54.51 \u00b10.63\n63.84 \u00b10.86\n59.44 \u00b12.02\n71.31 \u00b10.91\nRICES\n63.69 \u00b10.06\n76.03 \u00b10.01\n52.11 \u00b11.10\n66.31 \u00b10.68\n75.84 \u00b10.13\n70.08 \u00b10.18\nInfICL\n63.79 \u00b10.55\n76.48 \u00b10.70\n57.28 \u00b10.91\n70.14 \u00b10.50\n90.71 \u00b11.58\n91.34 \u00b11.21\n32\nRandom\n59.66 \u00b10.70\n72.24 \u00b11.62\n\u2013\n\u2013\n61.28 \u00b10.52\n72.09 \u00b10.36\nRICES\n61.39 \u00b10.22\n74.38 \u00b10.32\n\u2013\n\u2013\n79.05 \u00b10.33\n75.38 \u00b10.45\nInfICL\n61.77 \u00b10.77\n73.48 \u00b11.60\n\u2013\n\u2013\n93.58 \u00b10.40\n93.69 \u00b10.37\nLlama-2-70B\n0\nZero-shot\n74.02 \u00b10.00\n78.61 \u00b10.00\n80.14 \u00b10.00\n79.25 \u00b10.00\n93.12 \u00b10.00\n93.45 \u00b10.00\n8\nRandom\n74.78 \u00b14.51\n78.79 \u00b15.00\n86.28 \u00b10.36\n87.53 \u00b10.35\n89.18 \u00b14.60\n90.35 \u00b13.76\nRICES\n78.91 \u00b10.47\n85.29 \u00b10.40\n84.72 \u00b10.21\n86.45 \u00b10.24\n91.40 \u00b10.11\n91.14 \u00b10.13\nInfICL\n79.71 \u00b13.02\n84.84 \u00b13.31\n87.61 \u00b10.91\n88.46 \u00b10.87\n94.80 \u00b10.75\n95.02 \u00b10.65\n16\nRandom\n77.28 \u00b11.42\n81.73 \u00b11.49\n86.04 \u00b11.10\n87.64 \u00b10.64\n90.79 \u00b13.85\n91.60 \u00b13.15\nRICES\n77.82 \u00b10.31\n84.62 \u00b10.33\n83.39 \u00b10.63\n85.72 \u00b10.46\n91.36 \u00b10.07\n91.11 \u00b10.10\nInfICL\n80.92 \u00b11.60\n86.32 \u00b11.10\n87.97 \u00b10.21\n89.03 \u00b10.22\n94.61 \u00b11.09\n94.76 \u00b10.96\n32\nRandom\n78.65 \u00b10.87\n83.56 \u00b11.56\n87.00 \u00b10.36\n88.56 \u00b10.41\n92.32 \u00b12.60\n92.85 \u00b12.22\nRICES\n76.93 \u00b10.24\n84.24 \u00b10.17\n80.14 \u00b10.21\n82.54 \u00b10.13\n91.44 \u00b10.07\n91.53 \u00b10.05\nInfICL\n78.94 \u00b11.30\n85.36 \u00b10.93\n88.09 \u00b10.36\n89.11 \u00b10.27\n95.53 \u00b10.34\n95.67 \u00b10.32\noffers personalized demonstrations, it fails to select highly influential demonstrations. This selection is crucial for enhancing the ICL performance. Hence, RICES also under-performs relative to InfICL. For Llama-2-7B and SST2 dataset, our InfICL shows superior performance against baselines. However, RICES outperforms InfICL with 8 and 32 shots for RTE and CoLA datasets, respectively. This is because, in a few cases, choosing personalized demonstrations that are similar to the test sample can enhance performance compared to influence analysis. For Llama-2-13B and across all three datasets, our InfICL clearly outperforms baselines. Counter-intuitively, InfICL performs better with 16 shots compared to 32 shots. This outlier phenomenon can sometimes occur due to the information interference effect between demonstrations [Chen et al., 2023a]. For OPT-6.7B and both RTE and SST2 datasets, InfICL maintains its superior performance over baselines. However, for CoLA dataset, Zero-shot outperforms other methods. OPT-6.7B is a small sized LLM compared to other external LLMs. Consequently, in some datasets like CoLA, it does not effectively utilize demonstrations. For the Llama-2-70B and across all datasets, our InfICL outperforms baselines. We further perform student\u2019s t-test between InfICL and non-influence analysis based baselines on all three datasets and four external LLMs. We perform this analysis on accuracy scores and the results are shown in Table 4. Out of 24 t-test cases, the p-values show statistical significance in 20 cases (based on the threshold of 0.05), which demonstrating the superiority of our InflCL. Correlation between influence scores and InfICL performance. We conduct an empirical study to analyze the correlation between influence scores and InfICL performance. As previously mentioned in Section 3.1, training points\nTable 4: Student\u2019s t-test analysis results between our InfICL and non-influence analysis based baselines. The p-value is calculated by using the accuracy scores for all shots and runs. Statistically significant p-values are bold highlighted (p-value).\nTable 4: Student\u2019s t-test analysis results between our InfICL and non-influence analysis based baselines. The p-value is calculated by using the accuracy scores for all shots and runs. Statistically significant p-values are bold highlighted (p-value < 0.05).\nExternal LLM\nDataset\nMethod 1\nMethod 2\np-value\nLlama-2-7B\nCoLA\nInfICL\nRandom\n0.0449\nRICES\n0.7363\nRTE\nInfICL\nRandom\n0.0207\nRICES\n0.0286\nSST2\nInfICL\nRandom\n0.0296\nRICES\n0.0002\nLlama-2-13B\nCoLA\nInfICL\nRandom\n0.0229\nRICES\n0.0007\nRTE\nInfICL\nRandom\n0.0384\nRICES\n0.0005\nSST2\nInfICL\nRandom\n0.0324\nRICES\n0.0001\nOPT-6.7B\nCoLA\nInfICL\nRandom\n0.0686\nRICES\n0.8550\nRTE\nInfICL\nRandom\n0.1190\nRICES\n0.0075\nSST2\nInfICL\nRandom\n0.0001\nRICES\n0.0001\nLlama-2-70B\nCoLA\nInfICL\nRandom\n0.0247\nRICES\n0.0125\nRTE\nInfICL\nRandom\n0.0002\nRICES\n0.0001\nSST2\nInfICL\nRandom\n0.0030\nRICES\n0.0001\nTable 5: Effect of choosing training points from different range of influence scores on the InfICL performance. S are reported after 5 runs. External model: Llama-2-7B. Dataset: CoLA.\nShots\nInfl. Scores\nAccuracy (%)\u2191\nF1 (%)\u2191\n8\nTop Positive\n74.19 \u00b12.39\n81.10 \u00b12.49\nMiddle\n72.94 \u00b11.88\n80.22 \u00b12.56\nTop Negative\n71.54 \u00b11.43\n81.67 \u00b10.67\n16\nTop positive\n74.75 \u00b11.32\n81.39 \u00b10.92\nMiddle\n73.46 \u00b12.12\n81.01 \u00b12.46\nTop negative\n69.78 \u00b13.54\n78.58 \u00b13.69\n32\nTop positive\n73.02 \u00b11.73\n82.05 \u00b11.16\nMiddle\n71.81 \u00b14.13\n79.09 \u00b14.63\nTop negative\n64.29 \u00b13.40\n71.24 \u00b14.45\nexhibiting higher positive influence scores have the potential to enhance the InfICL predictive performance. In this empirical study, we assess how selecting demonstrations from varying influence ranges impacts the InfICL performance. We initially rank training points based on their influence scores in descending order, then form different demonstration sets using three strategies: selecting training points with the highest positive influence, those within the mid-range of influence, and those with the highest negative influence. We report the InfICL performance for different ranges of influence scores in Table 5. Notably, opting for training points with the highest positive influence scores as demonstrations yields the most favorable performance. Comparison to influence analysis based baselines. Since the employed influence analysis based baselines Influence, CondACC, and Data Models have an extremely high running costs, they can only run on a small size training and validation sets. To conduct a fair comparison, we run our InfICL and baselines in the same dataset setting as mentioned in Nguyen and Wong [2023], which has train/validation/test size as 400/200/500, respectively. In-order to reduce the high cost of experimentation, we conduct our empirical study using two external LLMs OPT-6.7B and Llama-2-7B and on two datasets CoLA and RTE. We show the empirical results comparing our InfICL with other influence analysis based baselines in Table 6. For the CoLA dataset and for both Llama-2-7B and OPT-6.7B, InfICL shows an overall better performance than other baselines. For the RTE dataset and Llama-2-7B, InfICL again outperforms Influence. However, for OPT-6.7B, InfICL has a lower accuracy than Influence for 12 shots. This indicates that the chosen 12 demonstrations based on InfICL do not convey sufficient information that can be exploited by OPT-6.7B. For the setting of Llama-2-7B and 4 shots on CoLA dataset, our InfICL incurs 10 minutes of execution latency, Influence takes 3.5 hours, and both CondAcc and Data Models take more than 80 hours.\nTable 6: Test accuracy of our InfICL and influence analysis based baselines on different external LLMs. Asterik denotes the results extracted from Nguyen and Wong [2023]. Cells marked \u2019\u2013\u2019 denotes non-feasible results due to\nTable 6: Test accuracy of our InfICL and influence analysis based baselines on different external LLMs. Asteri denotes the results extracted from Nguyen and Wong [2023]. Cells marked \u2019\u2013\u2019 denotes non-feasible results due t extremely high training latency.\nDataset\nExt. LLM\nShots\nInfICL\nInfluence\nCondAcc\nData Models\nCoLA\nOPT-6.7B\n4\n68.20\n31.80\n48.40\n45.50\n8\n64.80\n46.00\n35.60\n36.50\n16\n69.00\n46.80\n\u2013\n\u2013\n32\n69.20\n58.60\n\u2013\n\u2013\nLlama-2-7B\n4\n77.80\n74.40\n72.90\n71.2\n8\n77.80\n78.20\n\u2013\n\u2013\n16\n77.20\n73.20\n\u2013\n\u2013\n32\n76.80\n74.40\n\u2013\n\u2013\nRTE\nOPT-6.7B\n12\n51.20\n62.70\u2217\n\u2013\n\u2013\nLlama-2-7B\n12\n77.60\n75.60\n\u2013\n\u2013\n# 5 Conclusion\nIn this work, we introduced a demonstration selection method for ICL by analyzing influences of training samples using influence functions. Our approach utilizes a local LLM to generate sample embeddings thereby, avoiding the expensive fine-tuning of the LLM. Empirical studies on various real-world datasets demonstrated advantages of our method over state-of-the-art baselines. For future work, we aim to expand our demonstration selection method to Large Vision-language Models (LVMs), and extend our method to address more complex problems such as massive multitask language understanding. We release our source code at https://tinyurl.com/edry6nn4.\n# 6 Limitations\nAlthough we have demonstrated that influence function analysis can be effective for selecting ICL demonstrations, we have not conducted an in-depth interpretability study on why influence functions improve ICL performance. We based our use of influence functions on the intuition that highly influential training samples benefit model learning. However, since ICL does not involve any model gradient updates and differs significantly from gradient update-based learning, a theoretical study is needed to connect mechanisms of ICL with gradient update-based models [Xie et al., 2022], and show that highly influential training samples can also enhance ICL performance.\n# Acknowledgement\nThis work was supported in part by NSF grants 1920920 and 1946391.\n# References\nNaman Agarwal, Brian Bullins, and Elad Hazan. 2017. Second-order stochastic optimization for machine learning in linear time. The Journal of Machine Learning Research, 18(1):4148\u20134187. Samyadeep Basu, Phil Pope, and Soheil Feizi. 2021. Influence functions in deep learning are fragile. In International Conference on Learning Representations. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems. Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the origins of bias in word embeddings. In Proceedings of the 36th International Conference on Machine Learning. Ting-Yun Chang and Robin Jia. 2023. Data curation alone can stabilize in-context learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023a. How many demonstrations do you need for in-context learning? In Findings of the Association for Computational Linguistics: EMNLP. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML.\nYanda Chen, Chen Zhao, Zhou Yu, Kathleen R. McKeown, and He He. 2023b. On the relation between sensitivity and accuracy in in-context learning. In Findings of the Association for Computational Linguistics: EMNLP. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234. Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. 2020. Influence function based data poisoning attacks to top-n recommender systems. In Proceedings of The Web Conference. Vitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long tail via influence estimation. In Annual Conference on Neural Information Processing Systems. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Roger B. Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukosiute, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. 2023. Studying large language model generalization with influence functions. CoRR, abs/2308.03296. Xiaochuang Han and Yulia Tsvetkov. 2021. Influence tuning: Demoting spurious correlations via instance attribution and instance-driven updates. In Findings of the Association for Computational Linguistics: EMNLP. Xiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov. 2020. Explaining black box predictions and unveiling data artifacts through influence functions. ArXiv. Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 2022. Datamodels: Understanding predictions with data and data with predictions. In International Conference on Machine Learning, ICML. Matthew Jagielski, Giorgio Severi, Niklas Pousette Harger, and Alina Oprea. 2021. Subpopulation data poisoning attacks. In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning. Shuming Kong, Yanyan Shen, and Linpeng Huang. 2022. Resolving training biases via influence-based data relabeling. In International Conference on Learning Representations. Donghoon Lee, Hyunsin Park, Trung Pham, and Chang D. Yoo. 2020. Learning augmentation network via influence functions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for in-context learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. Xiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586. Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Seyed Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y. Zhao. 2023. Dr.icl: Demonstration-retrieved in-context learning. CoRR, abs/2305.14128. Tai Nguyen and Eric Wong. 2023. In-context example selection with influences. CoRR, abs/2302.11042. Sejoon Oh, Sungchul Kim, Ryan A. Rossi, and Srijan Kumar. 2021. Influence-guided data augmentation for neural tensor completion. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management.\nChengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. 2023. In-context learning with iterative demonstration selection. CoRR, abs/2310.09881. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL. Alexander Scarlatos and Andrew S. Lan. 2023. Reticl: Sequential retrieval of in-context examples with reinforcement learning. CoRR, abs/2305.14502. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00b4elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971. Xinyi Wang, Wanrong Zhu, and William Yang Wang. 2023. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv:2301.11916. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An empirical study of GPT-3 for few-shot knowledge-based VQA. In Thirty-Sixth Conference on Artificial Intelligence, AAAI. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of selecting effective demonstrations for In-Context Learning (ICL) in Large Language Models (LLMs), highlighting the sensitivity of ICL performance to the choice of demonstrations and the limitations of existing methods.",
        "problem": {
            "definition": "The problem this paper aims to solve is the challenge of selecting the most effective demonstrations for ICL, which is crucial for enhancing the generalization performance of LLMs.",
            "key obstacle": "A key obstacle is the lack of consensus on the most effective demonstration selection method and the high costs associated with current approaches."
        },
        "idea": {
            "intuition": "The intuition behind this work is that identifying influential training samples can significantly improve ICL performance without requiring expensive model fine-tuning.",
            "opinion": "The proposed idea, InfICL, utilizes influence functions to analyze the impact of training samples, enabling the selection of the most influential samples as demonstrations.",
            "innovation": "The primary innovation of InfICL lies in its use of influence function analysis for demonstration selection, which is more cost-effective than existing methods that require extensive model retraining."
        },
        "method": {
            "method name": "InfICL",
            "method abbreviation": "InfICL",
            "method definition": "InfICL is a demonstration selection method for ICL that employs influence function analysis to identify the most impactful training samples.",
            "method description": "InfICL generates embeddings for training samples using a local LLM and selects influential samples as demonstrations for ICL.",
            "method steps": [
                "Generate embeddings for all training and validation inputs using a local LLM.",
                "Train a local classifier using these embeddings.",
                "Calculate influence scores for each training sample.",
                "Select the top-R training samples from each class based on their influence scores."
            ],
            "principle": "This method is effective because it leverages influence scores to identify training samples that provide the most significant contributions to the model's learning process."
        },
        "experiments": {
            "evaluation setting": "Empirical studies were conducted on three real-world datasets: CoLA, RTE, and SST2, using various LLMs of different sizes for comparison.",
            "evaluation method": "The performance of InfICL was assessed by measuring accuracy and F1 scores against several baseline methods through multiple runs."
        },
        "conclusion": "The experiments demonstrated that InfICL outperforms state-of-the-art methods for demonstration selection in ICL, showing its potential for improving generalization performance in LLMs.",
        "discussion": {
            "advantage": "The key advantages of InfICL include its cost-effectiveness and ability to select high-quality demonstrations that enhance ICL performance without the need for extensive retraining.",
            "limitation": "A limitation of the method is the lack of an in-depth interpretability study regarding why influence functions improve ICL performance, necessitating further theoretical exploration.",
            "future work": "Future research will focus on extending InfICL to larger vision-language models and tackling more complex tasks in language understanding."
        },
        "other info": {
            "source code": "https://tinyurl.com/edry6nn4",
            "acknowledgement": "This work was supported in part by NSF grants 1920920 and 1946391."
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The proposed idea, InfICL, utilizes influence functions to analyze the impact of training samples, enabling the selection of the most influential samples as demonstrations."
        },
        {
            "section number": "3.4",
            "key information": "InfICL generates embeddings for training samples using a local LLM and selects influential samples as demonstrations for ICL."
        },
        {
            "section number": "4.2",
            "key information": "A key obstacle is the lack of consensus on the most effective demonstration selection method and the high costs associated with current approaches."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the method is the lack of an in-depth interpretability study regarding why influence functions improve ICL performance, necessitating further theoretical exploration."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrated that InfICL outperforms state-of-the-art methods for demonstration selection in ICL, showing its potential for improving generalization performance in LLMs."
        }
    ],
    "similarity_score": 0.7341140433148795,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning Demonstration Selection via Influence Analysis.json"
}