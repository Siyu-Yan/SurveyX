{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.14907",
    "title": "Coverage-based Example Selection for In-Context Learning",
    "abstract": "In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using SetBSR outperforms independent ranking by up to 17 points on average and, despite being trainingfree, surpasses methods that leverage task or LLM-specific training.1",
    "bib_name": "gupta2023coveragebasedexampleselectionincontext",
    "md_text": "Shivanshu Gupta1 Matt Gardner2 Sameer Singh1 1University of California Irvine 2Scaled Cognition {shivag5,sameer}@uci.edu, mgardner@scaledcognition.com\n# Abstract\nIn-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using SetBSR outperforms independent ranking by up to 17 points on average and, despite being trainingfree, surpasses methods that leverage task or LLM-specific training.1\narXiv:2305.14907v3\narXiv:2305.14\n# 1 Introduction\nLarge language models (LLMs) (Devlin et al., 2019; Brown et al., 2020) are capable of generalizing to novel tasks (Brown et al., 2020) by conditioning on textual prompts consisting of a few task examples. This training-free paradigm of fewshot inference, known as in-context learning (ICL), reduces the cost of modeling new tasks while also providing an interpretable and customizable interface (Liu et al., 2022; Wei et al., 2023) and improving generalization (Anil et al., 2022; Qiu et al., 2022b; Drozdov et al., 2023) and reasoning skills (Wei et al., 2023). However, ICL performance is critically sensitive to the choice of demonstrations (Zhao et al., 2021; Liu et al., 2022; Lu et al., 2022; Rubin et al., 2022; Schick and Sch\u00fctze,\n1https://github.com/Shivanshu-Gupta/ icl-coverage\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9946/9946d32e-1220-4302-a44e-4d74d7beeb97.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Coverage-based Set Selection</div>\nFigure 1: (a) Test input with salient aspects highlighted. (a) Independently selecting similar examples leads to redundancy and failure to demonstrate all salient aspects, in this case, the need to identify the manager. (b) Coverage-based selection using SET-BSR mitigates this by selecting a less similar example that contains the missing information. Blue indicates LLM generation.\n# 2021), as the LLM relies on them for understanding and solving the test instance.\nThe standard approach to selecting ICL examples or demonstrations from a pool of candidates is to independently score them using a relevance metric and choose the top-ranked ones. However, cosine similarity and BM25, the two commonly used metrics, are sub-optimal for selecting demonstrations due to their reliance on a single dense embedding and unigram overlap, respectively. Moreover, since it selects examples independently, this approach ignores their utility as a set. It is particularly inadequate for complex compositional tasks like semantic parsing (Levy et al., 2022) where no single candidate might contain all reasoning patterns, and an independent selection would select multiple redundant examples with the same reasoning patterns but fail to demonstrate the others. Figure 1 shows a\nfailure case where similarity-based selection picks paraphrased examples that fail to demonstrate how to find a manager. Prior work on selecting demonstrations as a set (Ye et al., 2023; Levy et al., 2022) required task and/or LLM-specific training, limiting their utility. For this reason, simple yet widely applicable training-free methods like BM25 and cosine similarity remain the most popular approaches for ICL example selection. In this work, we propose a novel framework for selecting sets of maximally informative demonstrations for the salient aspects of the test input, e.g., reasoning patterns, entities, etc. Examples selected using this framework are informative about the test input and help the LLM understand and perform the task. We use this framework to explore different ways to characterize salient aspects, including syntactic structures like dependency parse subtrees and contextual token embeddings, while using BM25 and BERTScore (Zhang et al., 2020) to measure their coverage, respectively. To select the demonstrations as a set, we extend the coverage metrics to measure the overall informativeness of a set of demonstrations. We show that these set-level metrics are submodular and can be efficiently optimized to find demonstration sets that maximally cover the salient aspects. We evaluate our ICL example selection methods on 15 diverse datasets, including 6 semantic parsing, 2 numerical reasoning, and 7 classification datasets, and with 7 LLMs of varying sizes and pretraining. Among instance-level metrics, BSR, the recall version of BERTScore, consistently outperforms standard retrieval metrics on all datasets and LLMs, beating cosine similarity by up to 8 points on average in semantic parsing datasets and 15 points in the rest. Selecting demonstrations as a set using SET-BSR, the set-extension of BSR, leads to further gains in semantic parsing and is particularly effective in compositional settings where the gains grow with LLM size. With Codex, a 175B parameter LLM, SET-BSR outperforms cosine similarity by 17% on average with up to 49% improvement in some splits, and, despite being training-free, outperforms even trained methods like those from Rubin et al. (2022), Levy et al. (2022), and Ye et al. (2023) that require task and/or LLM-specific training.\n# 2 Related Work\nIn-context learning for few-shot inference facilitates the use of LLMs for novel tasks without the\nneed for expensive supervised fine-tuning. In addition to reduced cost, it has several other advantages over supervised fine-tuning: it provides a more interpretable and customizable interface to using LLMs (Liu et al., 2022; Wei et al., 2023); and retention of linguistic understanding and knowledge from pretraining leading to improved generalization (Anil et al., 2022; Qiu et al., 2022b; Drozdov et al., 2023) and reasoning skills (Wei et al., 2023). However, the performance of ICL is critically sensitive to the choice of demonstrations (Zhao et al., 2021; Liu et al., 2022). This has led to a growing interest in techniques for selecting good demonstrations. Prior work can be roughly classified into (1) independently scoring and retrieving examples (Liu et al., 2022; Rubin et al., 2022), (2) selecting diverse examples to reduce redundancy among them (Su et al., 2022; Levy et al., 2022; Agrawal et al., 2022; Ye et al., 2022), and (3) selecting examples that minimize the entropy of the LLM\u2019s output distribution for the test input (Lu et al., 2022; Wu et al., 2023). Recent work has also trained RL agents (Lu et al., 2023) and used Bayesian inference (Wang et al., 2023). The most similar studies to ours are Levy et al. (2022) and Ye et al. (2023). Levy et al. (2022) select diverse demonstrations that cover substructures of the target output predicted by task-specific classifiers but are limited in applicability to a few semantic parsing tasks. Ye et al. (2023) use Determinantal Point Processes (Kulesza, 2012) to select a diverse set of demonstrations similar to the test instance but do not optimize for coverage directly and require training with the LLM. Moreover, both methods require task or LLM-specific training that limits their use and effectiveness for larger LMs.\n# 3 Preliminaries\nIn-context learning is the ability of LLMs to solve novel tasks by merely conditioning on a few task demonstrations. Formally, given demonstrations {(xi, yi)}k i=1 and the test input xtest, it involves using textual templates to linearize instance inputs and outputs into sequences of tokens from the LLM vocabulary, x = I(x) = \u27e8x1 . . . x|x|\u27e9and y = O(y) = \u27e8y1 . . . y|y|\u27e9. The linearizations are then concatenated to form a prompt and fed to the LLM for conditional generation of the test output:\nytest \u223cPLM (\u00b7 | x1, y1, . . . , xK, yK, xtest ) \nThe interpretable and training-free nature of ICL makes it an attractive alternative to supervised finetuning. However, its performance is highly sensitive to the choice and order of demonstrations.\nDemonstration Selection identifies which examples to include in the prompt for any test instance. Formally, given a test input xtest and a pool of candidates T = {zi}N i=1 = {(xi, yi)}N i=1, the goal is to select a subset of k \u226aN demonstrations that when included in the context make ytest the most likely generation. A naive approach is to randomly sample k instances from T , but this is sub-optimal since the demonstrations are often completely unrelated to the test input. Instead, the standard approach to selecting demonstrations that are informative about the test input is to independently assign each candidate z a score score(xtest, z) using a relevance metric and then select the top k candidates. Relevance Metrics The two most commonly used relevance metrics for scoring demonstration are cosine similarity and BM25. Cosine similarity uses a representation function R to independently map the textual linearizations of inputs to unit-norm embeddings rx = R(x) in a common vector space and then scores the candidate z using the dot product, cosine(xtest, z) = rT xtestrz. BM25, on the other hand, is a sparse information retrieval algorithm belonging to a class of TF-IDF measures that view the test input and the candidates as bags of terms and measures relevance as a weighted recall or coverage of these terms:\ntfidf(xtest, z) = \ufffd s\u2208Txtest idf(s)tf(s, Tz) (2)\n(2)\nHere Tx and Tz are the set of terms in x and z respectively, and tf(s, Tz) and idf(s) are the term frequency and inverse document frequency statistics that measure the coverage of a particular term and the relative importance of terms respectively. We use tf and idf as per the Okapi variant of BM25 (Robertson et al., 1993; Jones et al., 2000).\n# 4 Informative Demonstrations\nThe limitation of the standard demonstration selection approach is that by independently scoring the demonstrations, it ignores their utility as a set. For ICL to work, the demonstrations included in the context need to be informative about how to understand and solve the test input. In this section\nand the next, we describe our approach to selecting informative sets of demonstrations for ICL. We begin by defining our notion of informativeness of demonstrations in ICL and describing how to measure it. Thereafter, in \u00a75, we will discuss how to extend this notion to an algorithm for selecting optimally informative sets of demonstrations. Informativeness Demonstrations should demonstrate the salient aspects, e.g., reasoning patterns, entities, etc., of the test input. Formally, denoting Sxtest as the set of salient aspects of the test input, we measure the informativeness of a demonstration z in terms of the coverage of such salient aspects,\nwhere c(s, z) measures the coverage (or recall) of a single salient aspect s by z. Salient Aspects Both cosine similarity and BM25 are special cases of Eq. 3 for different notions of salient aspects. For BM25, Sxtest = Txtest, the set of unigrams in x, and c(s, z) = idf(s)tf(s, Tz). And cosine similarity, although not explicitly a recall metric, can also be interpreted as evaluating coverage of the dimensions of the test input embedding by defining Sxtest = [1, d], the dimensions of the dense embedding as the salient aspects, i.e.,\n(4)\nThe above interpretations reveal why neither cosine similarity nor BM25 are good measures of informativeness. While cosine similarity captures some aspects of semantic similarity (depending on the embedding), it is limited to a single embedding. And, unigrams, the commonly used terms with BM25, are too small to capture most salient aspects. A good measure of informativeness necessitates an accurate characterization of salient aspects. One way might be to use larger syntactic substructures of the input as terms with BM25. We experiment with using larger n-grams and subtrees of the dependency parse tree. However, such syntactic structures are constrained to the surface form of the instance and hence may not capture meaning and aspects like reasoning patterns. A better way to capture salient aspects is to use contextualized token embeddings, the idea behind the BERTScore (Zhang et al., 2020) metric.\nBERTScore was originally proposed as a metric for evaluating the quality of machine-generated text (e.g., machine translation) by comparing it to a reference text. It leverages pre-trained contextual embeddings to match words in the candidate and reference sentences by cosine similarity and compute precision, recall, and F1 measures. Formally, given the sequences of contextual embeddings \u27e8x1, x2, . . . , x|x|\u27e9and \u27e8z1, z2, . . . , z|z|\u27e9 of tokens in x = \u27e8x1, x2, . . . , x|x|\u27e9and z = \u27e8z1, z2, . . . , z|z|\u27e9respectively, the recall measure, BERTScore-Recall (BSR), is defined as:\n(5)\nHere, w(xi) is a weight assigned to token xi and can be defined as 1 |x| if treating each token as equally important or idf(xi) \ufffd xi\u2208x idf(xi) if downweighting rare words. The precision measure is defined analogously, while the F1 measure is the harmonic mean of the two. BSR is also a special case of Eq. 3 with contextualized tokens as salient aspects, i.e., Sx = \u27e8x1, x2, . . . , x|x|\u27e9and can be used to select examples by treating them as candidates and the test input as the reference. The following table summarizes the informativeness measures and salient aspects in this work.\nMetric\nSalient Aspects\nCosine\nembedding dimensions\nBM25\nunigrams, n-grams, dependency parse subtrees\nBERTScore contextual token embeddings\n# 5 Set-level Information Coverage\nSo far, we have focused on measuring the informativeness of a single demonstration to rank and independently select the most informative ones. However, as depicted in Fig. 1, when no single single candidate demonstrates all salient aspects, this approach can fail to cover all of them while also selecting redundant demonstrations that provide no new information. A scenario where this can happen is when the candidate pool contains close paraphrases (or duplicates). This suggests that demonstrations should be selected as a set. Set Metric To evaluate the informativeness of a set of examples Z, we propose to extend the coverage measure in Eq. 3 to a measure for sets as follows:\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/74b3/74b36a16-1d2d-4583-86e5-f3ff6e5e9098.png\" style=\"width: 50%;\"></div>\nAlgorithm 1 Greedy Optimization of Set Coverage\nRequire: Instance pool T ; test input xtest; desired number of\ndemonstrations k; coverage scoring function setcov\n1: Z \u2190\u2205\n\u25b7Selected Demonstrations\n2: Zcurr \u2190\u2205\n\u25b7Current Set Cover\n3: curr_cov \u2190\u2212inf\n4: while |Z|< k do\n5:\nz\u2217, next_cov = argmax\nz\u2208T \u2212Z\nsetcov (xtest , Zcurr \u222az)\n6:\nif next_cov > curr_cov then\n\u25b7Pick z\u2217\n7:\ncurr_cov \u2190next_cov\n8:\nZ \u2190Z \u222az\u2217\n9:\nZcurr \u2190Zcurr \u222az\u2217\n10:\nelse\n\u25b7Or start new cover\n11:\nZcurr \u2190\u2205, curr_cov \u2190\u2212inf\n12:\nend if\n13: end while\n14: return Z\nIntuitively, this measures the coverage of each salient aspect as the best coverage it receives from any example in the set. In other words, maximizing it requires that every salient aspect appears at least once in some demonstration without considering which or how many. Since cosine similarity, BM25, and BSR are all special cases of Eq. 3, they can be extended to set measures using Eq. 6. Submodularity Given the combinatorial space of sets of demonstrations, for a measure on sets to be practical, it needs to be efficiently optimizable. Fortunately, the set-level metric, as defined above, is also submodular for any definition of c(s, z). We prove this in Appendix A. Intuitively, this follows from the facts that (1) for any given test instance, c(s, z) assigns a scalar weight to each demonstration z \u2208Z, (2) the maximum of weights across set elements is submodular, and (3) the sum of submodular functions is also submodular. This means that the set-level metric can be optimized using a greedy algorithm with a constant factor approximation guarantee (Nemhauser et al., 1978). Algorithm The greedy algorithm we use to select the optimal set is shown in Algorithm 1. In every iteration, it selects the example that maximally increases the coverage of the current set of demonstrations (lines 5-9). If no such example exists, it resets (lines 11). Using the following identity when computing the score for candidate sets (line 5),\n(7)\nand assuming constant time for computing each c(s, z), the time complexity of algorithm is\nO(kNL), where L = |Sxtest|. For BSR, the complexity of computing c(x, z) for all z \u2208Z is O(Td), where T is the total number of tokens in Z and d is the token embedding size. Thus, the time complexity of both instance and set-level BSR is dominated by the computation of c(x, z), and is O(LTd). While slower than cosine and BM25, we found it to be a small overhead to in-context learning for most datasets considered in this work. We discuss this further in App. C.\n# 6 Experimental Setup\n# 6.1 Datasets\nWe experiment with a total of 15 datasets including six diverse semantic parsing datasets viz. GeoQuery (Zelle and Mooney, 1996), ATIS (Hemphill et al., 1990; Dahl et al., 1994), Overnight (Wang et al., 2015), SMCalFlow (Andreas et al., 2020), BREAK (Wolfson et al., 2020), and MTOP (Li et al., 2021); a math-word problems (GSM8K (Cobbe et al., 2021)) and a machine reading comprehension (DROP (Dua et al., 2019)) dataset requiring multi-step numeric reasoning; and seven classification datasets spanning natural language inference, paraphrase detection and sentiment classification viz. QNLI (Wang et al., 2018), MNLI (Williams et al., 2018), RTE (Bentivogli et al., 2009), MRPC (Dolan and Brockett, 2005), PAWS (Zhang et al., 2019), QQP (Wang et al., 2018), and SST2 (Socher et al., 2013). We refer the reader to App. B for detailed descriptions of each dataset along with sample instances and prompt templates. In addition to the standard IID splits, we also evaluate compositional generalization using compositional splits wherever available. For GeoQuery we use three types of compositional splits: Template (Finegan-Dollak et al., 2018), TMCD (Keysers et al., 2020), and Length. Following Levy et al. (2022), we use the compositional splits\u2014 three Template, three TMCD, and one Length\u2014 generated by Qiu et al. (2022a) and average results across the TMCD and Template splits. For ATIS and Overnight, we experiment with Template splits (Finegan-Dollak et al., 2018) generated by Gupta et al. (2022). For SMCalFlow, we experiment with splits in SMCalFlow-CS (Yin et al., 2021): an IID split (8-S) and a compositional split (32-C). For all the splits, following prior work (Ye et al., 2023; Rubin et al., 2022) we randomly subsample 44,000 instances from the train set to use as pool to select demonstrations from. For evaluation, we\nuse a random subsample of 1000 instance of the validation set if available, and the test set otherwise. We use Exact Match (EM) accuracy for all datasets except BREAK where we use LF-EM (Hasson and Berant, 2021), which is preferred over EM for semantic equivalence.\nWe experiment with the following LLMs: GPTNeo-2.7B (Black et al., 2021): A 2.7B-parameter LM trained on The Pile (Gao et al., 2020), an 825 GB text corpus. LLaMA (Touvron et al., 2023): A collection of LMs ranging from 7B to 65B parameters pretrained on CommonCrawl, GitHub, Arxiv, etc. We experiment with LLaMA7B and LLaMA-13B. StarCoder (Li et al., 2023): A 15.5B parameter model trained on 80+ programming languages (Kocetkov et al., 2022). GPT3.5-Turbo2: 175B LM trained with RL to follow instructions and optimized for chat. Cushman, Codex3 (Chen et al., 2021): 12B and 175B parameter code-pretrained LMs. GPT-Neo-2.7B, LLaMA-7B, LLaMA-13B, and Cushman have context window lengths of 2048, GPT-3.5-Turbo of 4096, Codex of 8001, and StarCoder of 8192.\n# 6.3 Methods\nWe compare the following training-free metrics: Cosine similarity (COSINE) We use the SentenceBert library (Reimers and Gurevych, 2019) with the all-mpnet-base-v2 model. For independent selection, we use FAISS 4 (Johnson et al., 2019) retrieve the most similar examples. BM25 (BM25) We use the Okapi variant (Robertson et al., 1993; Jones et al., 2000) of BM25 from the rank_bm255 library with three syntactic structures as terms: unigrams, size-4 or smaller n-grams, and size-4 or smaller subtrees of the input dependency parse (obtained using the spaCy6). BERTScore We use the bert_score7 library (Zhang et al., 2020) with deberta-large-mnli and deberta-base-mnli models which are DeBERTa models (He et al., 2021) finetuned on the MNLI dataset (Williams et al., 2018). We will refer\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99a8/99a8c360-a280-4dee-8ad9-81a92a344846.png\" style=\"width: 50%;\"></div>\nSelector\nGPT-Neo\nLLaMA-7B LLaMA-13B\nCushman\nStarCoder\nGPT-3.5-Turbo\nCodex\nTraining\nFree\nRANDOM\n5.5 (-23.3)\n5.7 (-28.7)\n9.8 (-28.9)\n12.0 (-32.9)\n13.6 (-33.5)\n13.0 (-31.9)\n20.7 (-32.6)\nCOSINE\n28.8\n34.4\n38.7\n44.9\n47.1\n44.9\n53.4\nBM25\n31.2 (+2.4)\n36.7 (+2.3)\n42.8 (+4.0)\n49.7 (+4.8)\n52.9 (+5.7)\n50.3 (+5.4)\n60.9 (+7.5)\nTrained\nEPR\n38.3 (+9.5)\n43.7 (+9.3)\n48.1 (+9.4)\n51.8 (+7.0)\n53.5 (+6.4)\n47.4 (+2.5)\n58.5 (+5.1)\nCEIL\n38.1 (+9.3)\n44.5 (+10.1)\n49.9 (+11.2)\n54.8 (+9.9)\n57.3 (+10.2)\n51.2 (+6.3)\n64.0 (+10.7)\nOurs\nBSR\n34.1 (+5.3)\n40.1 (+5.8)\n46.5 (+7.8)\n52.6 (+7.7)\n54.8 (+7.7)\n52.7 (+7.8)\n61.2 (+7.9)\nSET-BSR\n35.8 (+7.0)\n43.8 (+9.4)\n51.4 (+12.7)\n59.5 (+14.6) 61.6 (+14.5)\n60.1 (+15.2)\n70.3 (+16.9)\nTable 1: Average 8-shot ICL performance across all splits of semantic parsing datasets using different LLMs and demonstration-selection methods with absolute improvement over COSINE in brackets. Both BSR and SET-BSR outperform prior training-free methods, with the latter outperforming even trained methods with larger LLMs.\nto the recall, precision, and F1 variants as BSR, BSP, and BSF1, respectively. Unless specified otherwise, we do not apply importance weighting (IDF) and use deberta-large-mnli. Additionally, we experiment with (1) a random baseline (RANDOM) that randomly selects demonstrations from the pool, and (2) with the set-extensions of COSINE, BM25 and BSR as described in \u00a75 which will be referred to as SETCOSINE, SET-BM25, and SET-BSR respectively.\n# 6.3.2 Trained Methods\nWe also compare with methods that require task or LLM-specific training. EPR (Rubin et al., 2022) uses LLM perplexity to train a dense retriever for each dataset. CEIL (Ye et al., 2023) uses EPR and an LLM to train a Determinantal Point Process (Kulesza, 2012) for each dataset and then uses it to select examples. We use Ye et al. (2023)\u2019s implementation of EPR and CEIL and use GPTNeo-2.7B LLM. We also compare with LFCOV (Levy et al., 2022), a method for semantic parsing, specifically SMCalFlow-CS and GeoQuery. It trains a classifier to predict logical form substructures and then selects diverse examples containing them. We use the shots provided by the authors.\n# 6.4 Prompt Construction\nFor k-shot (we use k = 8 unless specified otherwise) ICL with any given dataset (\u00a7 6.1), demonstration selection method (\u00a7 6.3) and LLM (\u00a7 6.2), we construct the prompt as follows: (1) select up to k demonstrations depending on the context window of the LLM; (2) order the demonstrations in increasing order of relevance so that the most relevant demonstrations appear closest to the test input; and (3) linearize the ordered demonstrations and the test input using the dataset\u2019s prompt template in Table 5 and concatenate to form the prompt. For set-selection methods, the demonstrations are or-\nSelector\n8_S\n32_C\nTraining\nFree\nRANDOM\n31.9 (-22.8)\n7.4 (-4.5)\nCOSINE\n54.7\n11.9\nBM25\n65.4 (+10.7) 29.4 (+17.5)\nTrained\nEPR\n76.3 (+21.6)\n21.7 (+9.8)\nCEIL\n77.5 (+22.8) 40.1 (+28.2)\nLFCOV\n66.3 (+11.6) 45.9 (+33.9)\nOurs\nBSR\n72.5 (+17.8) 31.5 (+19.6)\nSET-BSR\n75.7 (+21.0) 61.2 (+49.3)\nTable 2: 8-shot ICL accuracy on SMCalFlow-CS using Codex with absolute improvement over COSINE in brackets. SET-BSR is competitive with trained methods on the IID split while dramatically outperforming them on the compositional split.\ndered by their corresponding instance-level score. For the trained baselines, we use orderings recommended by the corresponding authors.\n# 7 Results\nWe begin by comparing the performance of our proposed methods, BSR and SET-BSR, with prior training-free and state-of-the-art trained methods in \u00a7 7.1. We then analyze the different metrics for measuring informativeness of individual demonstrations (\u00a7 7.2) and the impact of coverage-based set selection using our set extension (\u00a7 7.3).\n# 7.1 Main Results\nTable 1 compares average performance across all semantic parsing splits for seven LLMs of varying sizes. See Table 2 for comparison with LFCOV, which only works with GeoQuery and SMCalFlowCS and Table 11 for results on individual splits. While BSR consistently outperforms COSINE and BM25 for all LLMs, set-selection using SET-BSR leads to further dramatic gains with upto 17% improvement over COSINE with Codex, beating even state-of-the-art trained methods like EPR and CEIL by 12 and 6 points, respectively. Further, from\n<div style=\"text-align: center;\">Selector GSM8K DROP MNLI PAWS SST2</div>\nTraining\nFree\nRandom\n60.6\n62.7\n41.9\n48\n86.9\nCosine\n64\n65.4\n44.0\n52.5\n81.9\nBM25\n64.8\n66.9\n42.2\n55.2\n82.6\nTrained\nEPR\n61.7\n-\n66.1\u2020\n-\n-\nCEIL\n63.1\n-\n71.7\u2020\n-\n-\nOurs\nBSR\n68.1\n68.1\n76.7\n75\n90.9\nSet-BSR\n67.4\n66.4\n78.6\n74.9\n61.5\nTable 3: 8-shot ICL performance for tasks other than semantic parsing (using GPT-Neo-2.7B for the classification tasks and Codex for the harder GSM8K and DROP). BSR is competitive with prior methods, however, as these are IID splits, SET-BSR doesn\u2019t lead to further gains. \u2020 50-shot results from Ye et al. (2023).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c1cc/c1cce5bf-713e-441d-9475-0a358ade1d80.png\" style=\"width: 50%;\"></div>\nFigure 2: Gain in average ICL accuracy compared to COSINE on IID and COMPositional splits in semantic parsing. Trained methods (EPR and CEIL) become less effective with larger LLMs on IID splits. This is unlike SET-BSR, which, on compositional splits, even becomes more effective with larger LLMs. Table 3, we see that, unlike SET-BSR, BSR is effective even for non-semantic parsing datasets outperforming COSINE by 15 points on average with GPT-Neo-2.7B (see Table 12), and often even EPR and CEIL (see Table 13). All the above improvements were statistically significant (p < 0.05) under paired permutation-tests. SET-BSR is more effective with larger LLMs The effectiveness of SET-BSR monotonically improves as LLMs become more powerful. The trend is particularly pronounced in compositional splits, where it gets 25% absolute improvement v/s COSINE on average (see Fig. 2) and 49% improvement on the 32-C split of SMCalFlow-CS (see Table 2). Trained methods do not leverage larger LLMs As EPR and CEIL are trained using GPT-Neo-2.7B, they have difficulty generalizing to and taking ad-\nSelector\nALL\nIID\nCOMP\nBSF1\n60.6 71.0\n50.1\nBSP\n54.3 65.5\n43.2\nBSR\n61.2 71.5\n50.9\nBM25\n60.9 68.9\n52.8\n+ Coverage\n56.4 63.4\n49.5\nBM25[4-gram]\n59.1 67.1\n51.0\n+ Coverage\n64.5 68.9\n60.2\nBM25[4-depst]\n57.8 65.5\n50.0\n+ Coverage\n64.9 68.6\n61.2\nTable 4: Average 8-shot ICL performance with Codex on IID, COMPositional, and ALL semantic parsing splits. Top compares different variants of BERTScore, white Bottom compares the different variants of BM25. vantage of larger, more powerful LLMs, becoming less effective on IID splits (Fig. 2), and failing on GSM8K (Table 3). The latter is likely because GPTNeo-2.7B itself fails on GSM8K (Table 13), which requires Chain-of-Thought reasoning, an emergent ability of larger LLMs (Wei et al., 2022). As training with increasingly large LLMs is prohibitively expensive and impractical, these results demonstrate serious limitations of trained methods.\n# 7.2 Measure of Informativeness\nContextual embeddings capture salient aspects From Tables 1 and 3, it is clear that BSR consistently outperforms COSINE and BM25. This is true even when using the same encoder (see App. D), is seen in both IID and compositional splits (see Fig. 2), and with varying number of demonstrations (see Fig. 4). Larger syntactic substructures did not improve BM25 as seen in Table 4 (Bottom). These results show that contextual embeddings are indeed better at capturing salient aspects.\n# Recall outperforms other measures\nRecall outperforms other measures Comparing the variants of BERTScore, for Codex in Table 4 (Top), and other LLMs in Fig. 7 in App. D, it is evident that recall is on par with, or better than, the F1 metric. This supports our hypothesis that recall or coverage (of salient aspects) is a useful metric for informativeness. We include additional ablations in App. D, analyzing the effect of using importance weighting (IDF) and using a larger LM to compute token embeddings for BSR.\n# 7.3 Coverage-based Set Selection\nImpact on performance From Fig. 3, we see that coverage-based set selection is most effective in compositional splits where it improves the average performance of all metrics, including COSINE.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/56b1/56b14602-5d42-4ce4-ac4a-fa439534199a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Change in average performance on different types of splits of semantic parsing datasets from set-selection using our set metrics v/s the corresponding instance-level metric. Coverage-based set selection is most useful in compositional splits and when covering larger syntactic structures (BM25) or contextual embeddings (BSR).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/02be/02bef1cb-6aef-4bd6-9dac-9f14a13c6751.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4019/40198dc6-818a-4826-86fd-3e23c151840d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Average performance on IID and COMP semantic parsing splits with Codex. SET-BSR consistently outperforms independent selection.</div>\nThis shows the importance of selecting demonstrations as a set in compositional settings where examples demonstrating all the salient aspects of the test input are even less likely to exist. The set extension is less effective in IID splits and even hurts performance for COSINE and vanilla unigram BM25. Overall, BSR and BM25 with larger substructures benefit the most from the set extension. We provided further analyses of improvements from set selection and the impact of reordering in App. D. Illustrative Example We present a GeoQuery test input in Fig 5 along with demonstrations (only the inputs) selected by COSINE and SET-BSR (more examples in Appendix E). COSINE selections tend to be redundant, with repeated operations, and are somewhat restrictive, mostly limited to the min operation. Contrastingly, SET-BSR exhibits a more balanced selection, opting for demonstrations of comparable complexity to the test instance and collectively encapsulating all necessary operations. Failure Cases There are a few limitations of coverage-based set-selection using SET-BSR. First, by only considering uncovered aspects, it sacrifices\nFigure 5: Demonstrations selected for a GeoQuery input (outputs omitted for clarity). COSINE demonstrations are redundant (repeated operations) and limited (only cover \u201cpopulation\u201d aspect). SET-BSR, instead, selects demonstrations that are similarly complex as the test instance and, together, cover all required operations. the relevance of individual demonstrations to prioritize coverage of all aspects with the set (see Table 9 for an example from GSM8K). Additionally, even contextual token embeddings can only capture salient aspects that are explicitly expressed in the input text and thus may not be suitable for tasks where the salient aspects are more abstract and require reasoning themselves (see Table 10 for an example from QNLI). We leave it to future work to explore better measures of informativeness, including better characterizations of salient aspects.\n# 8 Conclusion\nThis paper presents a novel framework for selecting informative sets of demonstrations that cover salient aspects of the test input to aid the language model (LLM) in solving it. We explore different ways to characterize these aspects and quantify their coverage. Evaluation on a wide\nrange of tasks and LLMs validates the effectiveness of BERTScore-Recall as a measure of informativeness of individual demonstrations. Further, our results demonstrate the superiority of SET-BSR in selecting informative sets of demonstrations compositional tasks like semantic parsing and highlight the ability of coverage-based demonstration selection, unlike trained methods, to leverage increasingly powerful larger LLMs. Our code base is available at https://github.com/ Shivanshu-Gupta/icl-coverage.\n# Acknowledgements\nWe would like to thank the anonymous reviewers for their feedback. This work was sponsored in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research and in part by the NSF award #IIS2046873. The views expressed are those of the authors and do not reflect the policy of the funding agencies.\nContextual token embeddings require the salient aspects to be expressed in text and hence may not be able to capture them for all tasks. Moreover, since it requires computing a dot product for every pair of test and candidate instance tokens, this causes it to scale quadratically with the average number of tokens making it computationally infeasible for tasks with very long textual linearizations. Future work can thus explore more general characterizations of salient aspects and more efficient methods for selecting demonstrations covering them.\n# References\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022. Incontext examples selection for machine translation.\ncob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020.\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.\nDeborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the ATIS task: The ATIS-3 corpus. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). Andrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2023. Compositional semantic parsing with large language models. In The Eleventh International Conference on Learning Representations. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368\u20132378, Minneapolis, Minnesota. Association for Computational Linguistics. Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving textto-SQL evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 351\u2013360, Melbourne, Australia. Association for Computational Linguistics. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling. Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Structurally diverse sampling for sampleefficient training and comprehensive evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4966\u20134979, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Matan Hasson and Jonathan Berant. 2021. Question decomposition with dependency graphs.\nMatan Hasson and Jonathan Berant. 2021. Question decomposition with dependency graphs.\nKaren Sp\u00e4rck Jones, Steve Walker, and Stephen E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments - part 2. Inf. Process. Manag., 36:809\u2013840.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2950\u20132962, Online. Association for Computational Linguistics.\naymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo\nWang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you!\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. 2022b. Evaluating the impact of model scale for compositional generalization in semantic parsing. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pages 9157\u20139179, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.\nStephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1993. Okapi at trec. 500207, pages 109\u2013123. National Institute of Standards and Technology.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics.\nXinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. Yushi Wang, Jonathan Berant, and Percy Liang. 2015. Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1332\u20131342, Beijing, China. Association for Computational Linguistics. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 8:183\u2013198. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. 2022. Complementary explanations for effective in-context learning. Pengcheng Yin, Hao Fang, Graham Neubig, Adam Pauls, Emmanouil Antonios Platanios, Yu Su, Sam Thomson, and Jacob Andreas. 2021. Compositional generalization for neural semantic parsing via spanlevel supervised attention. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2810\u20132823, Online. Association for Computational Linguistics.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI\u201996, pages 1050\u20131055. AAAI Press. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298\u20131308, Minneapolis, Minnesota. Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\n# A Submodularity\nDefinition A.1 (Submodular Function). If \u2126is a finite set, a submodular function is a set function f : 2\u2126\u2192R, where 2\u2126denotes the power set of \u2126, which satisfies one of the following equivalent conditions.\nTheorem A.1. The function fmaxw (X) = max x\u2208X wx is submodular for any assignment of weights wx to the elements x \u2208\u2126.\nAdding these two inequalities together, we get the third definition of submodularity and thus fmaxw is submodular.\nTheorem A.2. If {fi}n i=1 are all submodular functions, then n\ufffd i=1 fi is also submodular.\n\ufffd Proof. We show this for n = 2:\nTherefore, f1 + f2 is submodular using the second definition of submodularity. By induction, this is true for any number n of functions.\nTheorem A.3. The set-level coverage metric setcov (xtest , Z) as defined in Eq. 6 is submodular for any definition of c(s, z).\nProof. From Theorem A.1, the function fs(Z) defined as fs(Z) = max z\u2208Z c(s, z) is submodular for any definition of c(s, z). Further, since from Theorem A.2, the sum of submodular functions is also submodular, setcov (xtest , Z) = \ufffd s\u2208S fs(Z) is\n# B Datasets\nWe use 15 diverse datasets, including 6 semantic parsing, 2 numerical reasoning, and 7 classification datasets.\n# B.1 Semantic Parsing\nWe use 6 semantic parsing datasets with IID and compositional splits for our experiments. Table 5 shows sample instances from each dataset we experiment with along with the textual template we use to linearize the instances. The ICL prompt is constructed by concatenating the templatized demonstrations and the test instance using \\n\\n as the separator. GeoQuery (Zelle and Mooney, 1996): A dataset containing 880 natural language questions about US geography paired with Prolog programs. In addition to the standard (IID) split, we experiment with three types of compositional splits: (1) Template split where the training and test sets have disjoint program templates (Finegan-Dollak et al., 2018); (2) TMCD split which creates train and test sets with maximal compound divergence and minimal atom divergence (Keysers et al., 2020); and (3) Length split which evaluates for length generalization by testing on sequences longer than ones in training. Following Levy et al. (2022), we use the compositional splits \u2014 three Template, three TMCD, and one Length \u2014 generated by Qiu et al. (2022a) and average results across the TMCD and Template splits. ATIS (Hemphill et al., 1990; Dahl et al., 1994): A dataset of natural language queries about aviation paired with \u03bb-calculus programs. We experiment with an IID split and a Template split (Finegan-Dollak et al., 2018) for evaluating compositional generalization, both taken from (Gupta et al., 2022). Overnight (Wang et al., 2015): A dataset containing both synthetic and natural language utterances from 11 domains (e.g. socialnetwork, restaurants, etc.) paired with Lambda-DCS logical forms. We experiment with an IID and a Template split of\nthe socialnetwork domain taken from (Gupta et al 2022).\n2022). SMCalFlow (Andreas et al., 2020): A dataset of task-oriented natural language dialogs about calendars, weather, places, and people paired with executable dataflow programs. SMCalFlow-CS (Yin et al., 2021) is a subset of SMCalFlow containing single-turn dialogs involving two domains (organization structure and calendar event creation), each having its own set of program symbols with two types of test sets: a cross-domain (C) test set containing only instances where both domains appear and meant to test for compositional generalization, and a single-domain (S) test set contains instances with only single-domain for in-distribution evaluation. For compositional evaluation, we use the 32-C split which is a few-shot cross-domain split where the training set includes 32 cross-domain examples. For our IID evaluation, following Levy et al. (2022), we use the 8-S split. Additionally, we use the programs with the simplified syntax provided by (Meron, 2022). BREAK (Wolfson et al., 2020) is a dataset that maps complex natural language questions into a language-based meaning representation (QDMR) comprising an ordered list of atomic steps necessary to answer the question. Following (Rubin et al., 2022), we use the low-level Break subset where the targets are logical forms comprising lists of operators with their arguments based on the corresponding QDMR. MTOP (Li et al., 2021): A multilingual taskoriented semantic parsing dataset spanning six languages and 11 domains. The target commands are complex queries featuring nested intent-slot prediction. We use the English subset of MTOP from (Rubin et al., 2022).\n# B.2 Non-Semantic Parsing\nWe additionally experiment with the standard IID splits of 9 non-semantic parsing datasets from the following categories: Numerical Reasoning: For this category, we experiment with GSM8K (Cobbe et al., 2021), a chain-of-thought reasoning (Wei et al., 2023) dataset of grade school-level arithmetic reasoning problems expressed in natural language and DROP (Dua et al., 2019), a dataset of question-answer pairs where the questions are about paragraphs containing numerical information and the answers are spans in the paragraph.\nClassification: For this category, we experiment with three Natural Language Inference (NLI) datasests (QNLI (Wang et al., 2018), MNLI (Williams et al., 2018), and RTE (Bentivogli et al., 2009)), three Paraphrase Detection datasets (MRPC (Dolan and Brockett, 2005), PAWS (Zhang et al., 2019), and QQP (Wang et al., 2018)) and one Sentiment Classification dataset (SST2 (Socher et al., 2013)).\n# C Selection Time\nDespite their O(LTd) time complexity, we found example selection using both BSR and SET-BSR to be fast enough to not be a bottleneck to incontext learning for most datasets considered in this work. By using a GPU to compute c(x, z)s, we could get both to work in the order tens of milliseconds per test input on average which was significantly faster than the LLM inference time itself. The exceptions were DROP, PAWS, QQP, MNLI and QNLI for which the selection took >1 second due to much longer instances and/or larger instance pool. We leave it to future work to explore more efficient ways to measure informativeness.\n# D Additional Analyses\nBM25 From Fig. 6 we can see that coverage-based selection using BM25 with larger substructures outperforms vanilla unigram BM25 in compositional splits. BERTScore-Recall Examining the impact of importance weighting in Fig. 8 which compares the performance change with using importance weighting (IDF) in BSR, we can see that its effect is not consistent across different LLMs. We also did not see any consistent improvement from using larger deberta-large-mnli for computing token embeddings for instance-level BSR (see Fig. 9). However, it did help with set-level selection using SET-BSR. Reordering We found the reordering of demonstrations according to the corresponding instance-level metric to only be necessary for smaller LLMs (see Fig. 10), with it even hurting the performance of larger LLMs. We believe this is because larger and code-pretrained LLMs are more capable at composing the salient aspects in the different demonstrations and taking advantage of the full context. BSR outperforms Cosine even with the same encoder In \u00a7 7.2, we showed that BSR with\nDataset\nExample Template\nSample Instance\nOvernight\n{source}\\t{target} source: employees who finish after alices birthday\ntarget:\n(call listValue (call getProperty ((lambda s (call filter (var\ns) (call ensureNumericProperty (string employment_end_date)) (string\n>) (call ensureNumericEntity (call getProperty en.person.alice (string\nbirthdate))))) (call domain (string employee))) (string employee)))\nATIS\n{source}\\t{target} source: give me the flights from pittsburgh to los angeles thursday evening\ntarget:\n( lambda $0 e ( and ( flight $0 ) ( during_day $0 evening :\npd ) ( from $0 pittsburgh : ci ) ( to $0 los_angeles : ci ) ( day $0\nthursday : da ) ) )\nGeoQuery\n{source}\\t{target} source: which river traverses most states\ntarget:\nanswer ( most ( river, traverse_2, state ) )\nSMCalFlow {source}\\t{target} source: Please put a 2 o\u2019clock on my schedule where I\u2019m meeting with boss Daniel.\ntarget:\nCreateEvent(AND(with_attendee(\"\nDaniel\n\"),starts_at(NextTime(time=NumberPM(2)))))\nBREAK\n{source}\\t{target} source: Is there another cube that is the same size as the cyan cube; what color is it?\ntarget:\nreturn the cyan cube ;return size of #1 ;return cubes besides\n#1 ;return sizes of #3 ;return #3 where #4 is the same as #2 ;return\ncolor of #5\nMTOP\n{source}\\t{target} source: latest news from washington times please\ntarget:\n[IN:GET_STORIES_NEWS [SL:DATE_TIME latest ] [SL:NEWS_TYPE news\n] [SL:NEWS_SOURCE washington times ] ]\nTable 5: Semantic Parsing Datasets with corresponding sample instances and example templates used in for ICL.\nTable 5: Semantic Parsing Datasets with corresponding deberta-large-mnli outperforms Cosine with all-mpnet-base-v2. Tables 15, 16, 17, and 18 show that the same trend holds even when using the same encoder, bert-base-uncased, for both metrics confirming that contextual embeddings are indeed better at capturing salient aspects. Recall of Syntactic Structures The improvements from set-based selection may be explained by Fig. 11 where we see that set-extensions COSINE and unigram BM25 reduce the recall of substructures of the test input whereas the recalls increase with set-extensions of both BM25[4-GRAM] and BM25[4-DEPST], and even BSR, which does not explicity consider these substructures.\n# E Qualitative Analysis of Prompts\nTables 7, 8 show demonstrations selected using COSINE and SET-BSR for instances from MTOP and SMCalFlow-CS respectively. In each case, COSINE find demonstrations that are all very similar to the test input but fails to demonstrate some salient aspect, whereas BSR selects less similar instances but ensures complete coverage of all salient aspects. Tables 9 and 10 additionally illustrate limitations of set-selection and of token-embeddings in capturing salient aspects.\n# F All Results\nTables 11 contains 8-shot ICL results for our proposed methods and prior learning-free and learningbased demonstration selection on all the LLMs\nfor all the semantic parsing datasets. For numerical reasoning and classification datasets, Tables 12 and 13 compare 8-shot ICL performance with prior training-free and trained methods, respectively. Table 14 provides average performances across all datasets. Additionally, Tables 15, 16, 17, 18, 20, and 21 contain results on semantic parsing datasets of all ablations of learning-free selection methods we ran, with GPT-Neo-2.7B, LLaMA-7B, LLaMA13B, StarCoder, Cushman, and Codex, respectively. We did not run ablations on GPT-3.5-Turbo due to its cost.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/88ea/88eaccd4-4818-43ad-b8f9-17b4911e256f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Absolute improvement in average 8-shot ICL performance on different types of semantic parsing splits rom using the set extensions SET-BM25 with larger substructures over vanilla BM25.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98a2/98a25f07-5e7f-4bca-9800-fece41978e90.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Comparison of 8-shot ICL performance of different variants of BERTScore with token embeddings computed using deberta-base-mnli. For easier visualization, since we found BERTScore-Precision to consistently perform worst, we show absolute improvement in average performance on different types of splits from the recall and F1 metrics over the precision metric.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f56/7f568a6c-d219-4a93-8fb0-51fab193b009.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Impact on average 8-shot ICL performance on semantic parsing splits from using importance weighting (IDF) in BSR.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f409/f4091f64-41a1-4a05-b177-a39cf1d6e21c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Impact on average 8-shot ICL performance on semantic parsing splits from using a larger deberta-large-mnli LLM for computing contextual token embeddings v/s using deberta-base-mnli in BSR and SET-BSR.</div>\n<div style=\"text-align: center;\">Figure 9: Impact on average 8-shot ICL performance on semantic parsing splits from using a larger deberta-large-mnli LLM for computing contextual token embeddings v/s using deberta-base-mnli in BSR</div>\nDataset\nExample Template\nSample Instance\nGSM8K Question: {question}\nSolution: {solution}\nquestion: Natalia sold clips to 48 of her friends in April, and then she sold half as\nmany clips in May. How many clips did Natalia sell altogether in April and May?\nsolution: Natalia sold 48/2 = \u00ab48/2=24\u00bb24 clips in May. Natalia sold 48+24 =\n\u00ab48+24=72\u00bb72 clips altogether in April and May. #### 72\nDROP\nPassage: {passage}\nQuestion: {question}\nAnswer: {answer}\npassage: To start the season, the Lions traveled south to Tampa, Florida to take\non the Tampa Bay Buccaneers. The Lions scored first in the first quarter with a\n23-yard field goal by Jason Hanson. The Buccaneers tied it up with a 38-yard field\ngoal by Connor Barth, then took the lead when Aqib Talib intercepted a pass from\nMatthew Stafford and ran it in 28 yards. The Lions responded with a 28-yard field\ngoal. In the second quarter, Detroit took the lead with a 36-yard touchdown catch by\nCalvin Johnson, and later added more points when Tony Scheffler caught an 11-yard\nTD pass. Tampa Bay responded with a 31-yard field goal just before halftime. The\nsecond half was relatively quiet, with each team only scoring one touchdown. First,\nDetroit\u2019s Calvin Johnson caught a 1-yard pass in the third quarter. The game\u2019s final\npoints came when Mike Williams of Tampa Bay caught a 5-yard pass. The Lions\nwon their regular season opener for the first time since 2007\nquestion: How many points did the buccaneers need to tie in the first?\nanswer: 3\nQNLI\nQuestion: {question}\nSentence: {sentence}\nAnswer: {label}\nsentence: Unlike the two seasons before it and most of the seasons that followed,\nDigimon Tamers takes a darker and more realistic approach to its story featuring\nDigimon who do not reincarnate after their deaths and more complex character\ndevelopment in the original Japanese.\nquestion: When did the third Digimon series begin?\nlabel: No\nMNLI\nPremise: {premise}\nHypothesis: {hypothesis}\nAnswer: {label}\npremise: The new rights are nice enough\nhypothesis: Everyone really likes the newest benefits\nlabel: Maybe\nRTE\nPremise: {premise}\nHypothesis: {hypothesis}\nAnswer: {label}\npremise: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung\ncancer at age 44, according to the Christopher Reeve Foundation.\nhypothesis: Christopher Reeve had an accident.\nlabel: Yes\nMRPC\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nAnswer: {label}\nsentence1: He said the foodservice pie business doesn \u2019t fit the company \u2019s long-term\ngrowth strategy.\nsentence2: \" The foodservice pie business does not fit our long-term growth strategy\n.\nlabel: Yes\nPAWS\nSentence 1: {sentence1}\nSentence 2: {sentence2}\nAnswer: {label}\nsentence1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6\nother players representing Britain , also on a tour of Australia .\nsentence2: \"Bradd Crellin also represented BARLA Great Britain on a tour through\nAustralia on a tour through Australia with 6 other players representing Cumbria .\nlabel: No\nQQP\nQuestion 1: {question1}\nQuestion 2: {question2}\nAnswer: {label}\nquestion1: Why are African-Americans so beautiful?\nquestion2: \"Why are hispanics so beautiful?\nlabel: No\nSST2\nReview: {sentence}\nAnswer: {label}\nsentence: it \u2019s a charming and often affecting journey .\nlabel: Positive\nTable 6: Non-Semantic Parsing Datasets with corresponding sample instances and example templates used for ICL.\nGSM8K Question: {question} Solution: {solution} question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? solution: Natalia sold 48/2 = \u00ab48/2=24\u00bb24 clips in May. Natalia sold 48+24 = \u00ab48+24=72\u00bb72 clips altogether in April and May. #### 72 DROP Passage: {passage} Question: {question} Answer: {answer} passage: To start the season, the Lions traveled south to Tampa, Florida to take on the Tampa Bay Buccaneers. The Lions scored first in the first quarter with a 23-yard field goal by Jason Hanson. The Buccaneers tied it up with a 38-yard field goal by Connor Barth, then took the lead when Aqib Talib intercepted a pass from Matthew Stafford and ran it in 28 yards. The Lions responded with a 28-yard field goal. In the second quarter, Detroit took the lead with a 36-yard touchdown catch by Calvin Johnson, and later added more points when Tony Scheffler caught an 11-yard TD pass. Tampa Bay responded with a 31-yard field goal just before halftime. The second half was relatively quiet, with each team only scoring one touchdown. First, Detroit\u2019s Calvin Johnson caught a 1-yard pass in the third quarter. The game\u2019s final points came when Mike Williams of Tampa Bay caught a 5-yard pass. The Lions won their regular season opener for the first time since 2007 question: How many points did the buccaneers need to tie in the first? answer: 3 QNLI Question: {question} Sentence: {sentence} Answer: {label} sentence: Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. question: When did the third Digimon series begin? label: No MNLI Premise: {premise} Hypothesis: {hypothesis} Answer: {label} premise: The new rights are nice enough hypothesis: Everyone really likes the newest benefits label: Maybe RTE Premise: {premise} Hypothesis: {hypothesis} Answer: {label} premise: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation. hypothesis: Christopher Reeve had an accident. label: Yes MRPC Sentence 1: {sentence1} Sentence 2: {sentence2} Answer: {label} sentence1: He said the foodservice pie business doesn \u2019t fit the company \u2019s long-term growth strategy. sentence2: \" The foodservice pie business does not fit our long-term growth strategy . label: Yes PAWS Sentence 1: {sentence1} Sentence 2: {sentence2} Answer: {label} sentence1: Bradd Crellin represented BARLA Cumbria on a tour of Australia with 6 other players representing Britain , also on a tour of Australia . sentence2: \"Bradd Crellin also represented BARLA Great Britain on a tour through Australia on a tour through Australia with 6 other players representing Cumbria . label: No QQP Question 1: {question1} Question 2: {question2} Answer: {label} question1: Why are African-Americans so beautiful? question2: \"Why are hispanics so beautiful? label: No SST2 Review: {sentence} Answer: {label} sentence: it \u2019s a charming and often affecting journey . label: Positive\nTable 6: Non-Semantic Parsing Datasets with corresponding sample instances and example templates used for ICL.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/114e/114e1f33-2c8f-4578-8f0c-1f2c05f7b807.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Impact on average 8-shot ICL performance on semantic parsing splits from reordering the demonstration selected by the different set-level metric using the corresponding instance-level metric as absolute gain v/s th unreordered version.</div>\nSelector\nPrompt\nCOSINE\nSentence: Easy vegan recipes\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_ATTRIBUTE Easy ] [SL:RECIPES_TYPE vegan ] ]\nSentence: Vegetarian recipes\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_TYPE Vegetarian ] ]\nSentence: Please find me vegan recipes\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_TYPE vegan ] ]\nSentence: Give me vegan recipes\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_TYPE vegan ] ]\nSET-BSR\nSentence: I have a nut allergy. Find me a dessert recipe\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_EXCLUDED_INGREDIENT nut ] [SL:RECIPES_MEAL dessert ]\n]\nSentence: Create a video message for Victoria with plan options for dinner with family next\nweek\nLogical Form: [IN:SEND_MESSAGE [SL:TYPE_CONTENT video ] [SL:RECIPIENT Victoria ] ]\nSentence: What are some no-bake dessert ideas\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_COOKING_METHOD no - bake ] [SL:RECIPES_MEAL dessert\n] ]\nSentence: Vegan birthday cakes\nLogical Form: [IN:GET_RECIPES [SL:RECIPES_TYPE Vegan ] [SL:RECIPES_DISH birthday cakes ] ]\nTable 7: Demonstrations selected for the MTOP input: Vegan desert options with target output [IN:GET_RECIPES [SL:RECIPES_TYPE Vegan ] [SL:RECIPES_DISH birthday cakes ] ]. COSINE\u2019s reliance on a single dense embedding means it is unable to account for the fact that \"options\" could mean dishes and not just recipes.\nSelector\nPrompt\nCOSINE\nSentence: I need a meeting with Elli tomorrow at 11 pm\nLogical Form: CreateEvent(AND(with_attendee (\" Elli \"),starts_at(Tomorrow ()),starts_at(NumberPM\n(11))))\nSentence: Set a meeting with Elli for tomorrow at 2 pm through the end of the day and call it\nRecap\nLogical Form: CreateEvent(AND(ends_at(AND(GE(DateTime ?(date=Tomorrow (),time=NumberPM (2))),\nEndOfWorkDay ())),with_attendee (\" Elli \"),has_subject (\" Recap \"),starts_at(Tomorrow ()),\nstarts_at(NumberPM (2))))\nSentence: Schedule a meeting with Elli for tomorrow at 4 pm through the end of the workday\nLogical Form: CreateEvent(AND(ends_at(AND(GE(DateTime ?(date=Tomorrow (),time=NumberPM (4))),\nEndOfWorkDay ())),with_attendee (\" Elli \"),starts_at(Tomorrow ()),starts_at(NumberPM (4))))\nSentence: Schedule a meeting with Elli from 4 PM until the end of the day tomorrow .\nLogical Form: CreateEvent(AND(ends_at(AND(GE(DateTime ?(date=Tomorrow (),time=NumberPM (4))),\nEndOfWorkDay ())),with_attendee (\" Elli \"),starts_at(Tomorrow ()),starts_at(NumberPM (4))))\nSET-BSR\nSentence: I need a doctor 's appointment on Wednesday morning .\nLogical Form: CreateEvent(AND(has_subject (\" doctor 's appointment \"),starts_at(Morning ()),\nstarts_at(NextDOW (\" WEDNESDAY \"))))\nSentence: I need to see Alice and her boss next Monday at 3 pm .\nLogical Form: CreateEvent(AND(with_attendee (\" Alice \"),with_attendee(FindManager (\" Alice \")),\nstarts_at(NextDOW (\" MONDAY \")),starts_at(NumberPM (3))))\nSentence: Schedule a meeting with Jake , Elli , and Jesse for Friday at 2 pm .\nLogical Form: CreateEvent(AND(with_attendee (\" Jesse \"),with_attendee (\" Jake \"),with_attendee (\"\nElli \"),starts_at(NextDOW (\" FRIDAY \")),starts_at(NumberPM (2))))\nSentence: I need to schedule a meeting with Jeff 's supervisor Lynne for tomorrow at 10 AM .\nLogical Form: CreateEvent(AND(with_attendee (\" Lynne \"),starts_at(Tomorrow ()),starts_at(\nNumberAM (10))))\nTable 8: Demonstrations selected for the SMCalFlow-CS input: Schedule a meeting with Elli and he manager \u2019s boss tomorrow morning. SET-BSR is able to find demonstrations covering all fragments of the te input while COSINE fails to include anything which involves finding someones manager.\nSelector\nPrompt\nCOSINE\nQuestion: Justin has a box that is 12 inches in height. The length of the box is 3 times its\nheight and 4 times its width. What is the volume of the box?\nQuestion: John builds a box.\nThe box is 26 inches by 26 inches by 14 inches.\nThe walls are 1\ninch thick on each side.\nHow much is the internal volume in cubic feet?\nBSR\nQuestion: A window is made up of 8 glass panes. Each pane has a length of 12 inches and a\nwidth of 8 inches. What is the area of the window?\nQuestion: John builds a box.\nThe box is 26 inches by 26 inches by 14 inches.\nThe walls are 1\ninch thick on each side.\nHow much is the internal volume in cubic feet?\nSET-BSR\nQuestion: Jazel has 3 sticks. One stick is 3 centimeters long. The second stick is twice as\nlong while the third stick is 1 centimeter shorter than the second stick. What is the\ntotal length of Jazel 's sticks when they are put together?\nQuestion: John builds a box.\nThe box is 26 inches by 26 inches by 14 inches.\nThe walls are 1\ninch thick on each side.\nHow much is the internal volume in cubic feet?\nTable 9: Demonstrations selected by different methods for the GSM8K input: John has 3 boxes. Each box is 5 inches by 6 inches by 4 inches. The walls are 1 inch thick. What is the total inner volume of all 3 boxes? We only show the inputs for clarity. Only BSR solves this input (2-shot ICL with Codex). All three methods select one example that demonstrates most of the aspects of the test input, i.e., computing the volume of a box after subtracting wall thickness. The remaining aspect is computing the total of a quantity computed for 3 identical items. COSINE fails to do so, selecting yet another example that requires computing a single box\u2019s volume. Since SET-BSR prioritizes coverage of the remaining aspect, it selects an example that has exactly three items whose total length has to be computed but overall is not very similar in reasoning. BSR on the other hand tries to find an example that demonstrates all aspects by itself and happens to find one that partially demonstrates the remaining aspect as well.\nSelector\nPrompt\nBSR\nBegun in 1960 and opened to traffic in 1968, the bridge is a two -tiered road and rail design\nspanning 4,600 metres on the upper deck , with approximately 1,580 metres spanning the\nriver itself. Can we\nknow \"What type of design is the bridge ?\"? Yes\nThe BBC also introduced Ceefax , the first teletext service , starting in 1974. Can we know \"What\nkind of service was Ceefax ?\"? Yes\nThe Water , Sanitation and Hygiene (WSH) program of the Gates Foundation was launched in mid\n-2005 as a \"Learning Initiative ,\" and became a full -fledged program under the Global\nDevelopment Division in early 2010. Can we know \"What was the WSH program launched in\n2005\"? Yes\nTelevision broadcasting in Hyderabad began in 1974 with the launch of Doordarshan , the\nGovernment of India 's public service broadcaster , which transmits two free -to -air\nterrestrial television channelsand one satellite channel. Can we know \"What is Doordarshan\n?\"? Yes\nTable 10: Top four demonstrations selected by different methods for the QNLI input: Telenet was incorporated in 1973 and started operations in 1975. Can we know \"What was telenet\"? Since BSR doesn\u2019t hav access to the labels and also cannot reason about the inputs themselves, it cannot account for the fact that the contex in the test input does not contain the answer for the question and selects demonstrations that are all answered \"Yes even though the answer to the test input is \"No\".\nDataset\nATIS\nOvernight\nBreak MTOP\nGeoQuery\nSMCalFlow-CS\nAVERAGE\nSplit\nIID/Templ. IID/Templ.\nIID\nIID\nIID/Templ./TMCD/Len.\n8_S/32_C\nAll/IID/Comp.\nLM Selector\nGPT-Neo-2.7B\nEPR\n66.1 / 12.2\n52.3 / 0.9\n29.9\n62.2\n71.4 / 33.6 / 43.6 / 28.8\n54.5 / 3.6\n38.3 / 56.1 / 20.5\nCEIL\n67.8 / 18.7\n50.7 / 2.1\n29.9\n60.5\n65.4 / 30.2 / 43.6 / 25.2\n59.1 / 3.8\n38.1 / 55.6 / 20.6\nRandom\n12.4 / 0.0\n3.6 / 0.0\n1.9\n1.3\n17.5 / 11.0 / 14.0 / 0.9\n3.0 / 0.0\n5.5 / 6.6 / 4.3\nCosine\n46.1 / 6.5\n38.3 / 0.4\n22.3\n43.9\n67.9 / 24.1 / 41.4 / 28.5\n25.2 / 1.2\n28.8 / 40.6 / 17.0\nBM25\n49.5 / 7.4\n33.7 / 3.0\n26.5\n47.7\n63.6 / 40.6 / 42.1 / 25.5\n32.0 / 3.2\n31.2 / 42.2 / 20.3\nBSR\n48.3 / 7.8\n40.1 / 2.6\n29.1\n54.5\n67.1 / 40.7 / 47.7 / 28.2\n39.7 / 3.5\n34.1 / 46.5 / 21.7\nSet-BSR\n54.6 / 13.2\n43.2 / 4.9\n28.6\n55.1\n67.1 / 45.3 / 45.4 / 26.4\n41.5 / 4.8\n35.8 / 48.4 / 23.3\nLLaMA-7B\nEPR\n73.0 / 21.0\n57.7 / 1.8\n33.2\n65.2\n75.4 / 49.3 / 45.8 / 30.3\n64.0 / 8.0\n43.7 / 61.4 / 26.0\nCEIL\n74.0 / 30.5\n55.8 / 4.4\n36.1\n66.8\n66.8 / 50.5 / 45.3 / 24.2\n67.4 / 11.9\n44.5 / 61.1 / 27.8\nRandom\n9.5 / 0.0\n4.2 / 0.5\n8.8\n2.8\n9.3 / 13.3 / 9.2 / 4.5\n6.2 / 0.0\n5.7 / 6.8 / 4.6\nCosine\n56.7 / 11.5\n48.7 / 0.0\n26.1\n49.8\n73.9 / 33.5 / 42.6 / 29.4\n37.3 / 3.2\n34.4 / 48.8 / 20.0\nBM25\n61.0 / 12.5\n45.1 / 2.5\n30.1\n53.6\n67.9 / 39.5 / 44.9 / 30.6\n43.4 / 9.5\n36.7 / 50.2 / 23.3\nBSR\n60.9 / 14.3\n51.2 / 3.0\n32.5\n59.1\n72.5 / 47.2 / 46.9 / 30.3\n54.1 / 9.8\n40.1 / 55.0 / 25.2\nSet-BSR\n64.3 / 21.2\n51.5 / 6.3\n33.7\n61.9\n76.1 / 52.3 / 48.5 / 35.8\n54.5 / 19.3\n43.8 / 57.0 / ",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The ability of large language models (LLMs) to generalize to novel tasks through in-context learning (ICL) is influenced by the choice of demonstrations. Existing methods for selecting demonstrations often lead to redundancy and fail to cover all salient aspects necessary for solving complex tasks.",
            "purpose of benchmark": "The benchmark is intended to evaluate the effectiveness of different methods for selecting informative demonstrations that enhance the performance of LLMs in ICL settings."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of selecting the most informative examples from a pool of candidates to aid LLMs in understanding and solving test instances effectively.",
            "key obstacle": "Existing benchmarks suffer from limitations in selecting diverse and informative demonstrations, often resulting in redundancy and insufficient coverage of necessary reasoning patterns."
        },
        "idea": {
            "intuition": "The creation of this benchmark was inspired by the observation that current demonstration selection methods do not adequately capture the diverse reasoning patterns required for complex tasks.",
            "opinion": "The authors believe that improving the selection of demonstrations is crucial for advancing ICL and enhancing the capabilities of LLMs.",
            "innovation": "This benchmark introduces BERTScore-Recall (BSR) and Set-BSR metrics, which optimize the selection of demonstrations based on their informativeness and coverage of salient aspects, significantly improving performance compared to existing methods.",
            "benchmark abbreviation": "BSR"
        },
        "dataset": {
            "source": "The dataset was created using a combination of real-world data and synthetic examples across 15 diverse datasets including semantic parsing, numerical reasoning, and classification tasks.",
            "desc": "The benchmark includes 15 datasets with varying tasks, allowing for comprehensive evaluation of demonstration selection methods.",
            "content": "The dataset comprises examples from semantic parsing, numerical reasoning, and classification tasks, including both natural language inputs and corresponding logical forms.",
            "size": "15",
            "domain": "Semantic Parsing",
            "task format": "Demonstration Selection"
        },
        "metrics": {
            "metric name": "BSR, SET-BSR",
            "aspect": "Informativeness and coverage of salient aspects",
            "principle": "The metrics were selected based on their ability to measure the relevance and diversity of selected demonstrations in relation to the test input.",
            "procedure": "Models are evaluated by measuring the performance improvement in ICL tasks when using the selected demonstrations compared to standard metrics."
        },
        "experiments": {
            "model": "The benchmark tests various LLMs, including GPT-Neo, LLaMA, and Codex, evaluating their performance with different demonstration selection methods.",
            "procedure": "Models are trained and evaluated using a k-shot ICL approach, where demonstrations are selected based on BSR and SET-BSR metrics.",
            "result": "Results show that using BSR and SET-BSR improves the performance of LLMs on various tasks significantly compared to traditional metrics.",
            "variability": "Variability is accounted for by conducting multiple trials across different datasets and measuring statistical significance."
        },
        "conclusion": "The experiments demonstrate that BSR and SET-BSR are effective in selecting informative demonstrations that significantly improve the performance of LLMs in ICL tasks, indicating their potential impact on future research in this area.",
        "discussion": {
            "advantage": "The benchmark provides a robust framework for evaluating demonstration selection methods, contributing valuable insights into improving ICL performance.",
            "limitation": "Despite its advantages, the benchmark may struggle with tasks where salient aspects are abstract and not explicitly expressed in the input text.",
            "future work": "Future research could focus on developing more efficient methods for selecting demonstrations and exploring better characterizations of salient aspects."
        },
        "other info": {
            "acknowledgements": "This work was sponsored in part by the DARPA MCS program and the NSF award #IIS2046873.",
            "repository": "The code base for the benchmark is available at https://github.com/Shivanshu-Gupta/icl-coverage."
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The benchmark introduces BERTScore-Recall (BSR) and Set-BSR metrics, which optimize the selection of demonstrations based on their informativeness and coverage of salient aspects, significantly improving performance compared to existing methods."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark addresses the problem of selecting the most informative examples from a pool of candidates to aid LLMs in understanding and solving test instances effectively."
        },
        {
            "section number": "4.1",
            "key information": "The authors believe that improving the selection of demonstrations is crucial for advancing in-context learning (ICL) and enhancing the capabilities of LLMs."
        },
        {
            "section number": "2.1",
            "key information": "The ability of large language models (LLMs) to generalize to novel tasks through in-context learning (ICL) is influenced by the choice of demonstrations."
        },
        {
            "section number": "6.1",
            "key information": "Existing benchmarks suffer from limitations in selecting diverse and informative demonstrations, often resulting in redundancy and insufficient coverage of necessary reasoning patterns."
        }
    ],
    "similarity_score": 0.6975748483754628,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Coverage-based Example Selection for In-Context Learning.json"
}