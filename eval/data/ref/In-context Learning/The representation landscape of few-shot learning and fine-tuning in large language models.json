{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.03662",
    "title": "The representation landscape of few-shot learning and fine-tuning in large language models",
    "abstract": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.",
    "bib_name": "doimo2024representationlandscapefewshotlearning",
    "md_text": "# The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models\nDiego Doimo Alessandro Serra Alessio Ansuini Alberto Cazzaniga\nArea Science Park, Trieste, Italy {diego.doimo,alessandro.serra,alessio.ansuini,alberto.cazzaniga} @areasciencepark.it\n# Abstract\nIn-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.\narXiv:2409.03662v2\n# 1 Introduction\nWith the rise of pre-trained large language models (LLMs), supervised fine-tuning (SFT) and incontext learning (ICL) have become the central paradigms for solving domain-specific language tasks [1]. Fine-tuning requires a set of labeled examples to adapt the pre-trained LLM to the target task by modifying all or part of the model parameters. ICL, on the other hand, is preferred when little or no supervised data is available. In ICL, the model receives an input request with a task description followed by a few examples. It then generates a response based on this context without updating its parameters. While operationally, the differences between SFT and ICL are clear in how they handle model parameters, it is less clear how they affect the model\u2019s representation space. Although both methods can achieve similar performance, it is unknown whether they also structure their internal representations in the same way. Differently from recent contributions which compared ICL and SFT in terms of generalization performance [2\u20134] and efficiency [5], in this work we analyze how these two learning paradigms affect the geometry of the representations. Previous studies described the geometry of the hidden layers using distances and angles [6, 7], often relying on low-dimensional projections of the data [8\u201310]. In contrast, we take a densitybased approach [11] that leverages the low-dimensionality of the hidden layers [12\u201314] and finds\nPreprint. Under review.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c5ea/c5eace9b-6377-497d-a15a-1d17a81c6959.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Consistency between the peak composition and the MMLU subjects. A schematic view of the density peaks in the early. The coloring reflects the presence of the subjects.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/17a9/17a935d1-9759-4bae-a1ed-1f6054f7e8a4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Consistency between the peak composition and the MMLU answers. A schematic view of the density peaks in the late layers. The coloring reflects the presence of answers with the same letter: A (blue), B (okra), C (pink), and D (light blue).</div>\n<div style=\"text-align: center;\">(b) Consistency between the peak composition and the MMLU answers. A schematic view of the densit peaks in the late layers. The coloring reflects the presence of answers with the same letter: A (blue), B (okra),  (pink), and D (light blue).</div>\nFigure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a questionanswering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset\u2019s subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.\nthe probability modes directly on the data manifold without performing an explicit dimension reduction.\nFor ICL and fine-tuning, we track how the multimodal structure of the data evolves across different layers as LLMs solve a semantically rich multiple-choice question task and measure how the geometrical properties intertwine with the emergence of high-level, abstract concepts. We study ICL in a few-shot prompting setup and find that despite ICL and SFT can reach the same performance, they affect the geometry of the representations differently. Few-shot learning creates more interpretable representations in early layers of the network, organized according to the underlying semantics of data (Fig. 1, top-center). On the other hand, fine-tuning induces a multimodal structure coherent with the answer identity in the later layers of the network (Fig. 1, bottom-right). The key findings of our study are:\n1. Both few-shot learning and fine-tuning show a clear division between model layers, a marked peak in the intrinsic dimension of the dataset, and a sudden change in the geometrical structure of the representations (Secion 4.1); 2. Few-shot learning leads to semantically meaningful clustering of the representations in early layers, organized hierarchically by subject (Section 4.2); 3. Fine-tuning enhances the sharp emergence of clustered representations according to answers in the second half of the network (Section 4.3).\n3. Fine-tuning enhances the sharp emergence of clustered representations according to answers in the second half of the network (Section 4.3).\nIn summary, our geometric analysis reveals a transition between layers that encode high-level semantic information and those involved in generating answers. By studying the probability landscape on either side of this transition, we uncover how fine-tuning and few-shot learning take different approaches to extract information from LLMs, ultimately solving the same problem in distinct ways.\n# 2 Related work\nProbing the geometry of the embeddings. A classical approach to understanding the linguistic content encoded in token representations is probing [15]. Inspecting the embeddings with linear [16, 17] or nonlinear [18, 19] classifier probes allowed to extract morphological [20] syntactic [6] and semantic [17, 21\u201324] information. Probing has also been used to explore the geometrical properties of hidden layers. LLMs can represent linearly in hidden layers concepts such as space and time [25], the structure of the Othello game board [26], the truth or falsehood of factual statements [27]. The linear representation hypotheses [28] can be used to show that LLMs encode hyponym relations as simplices in their last hidden layer [29]. However, another line of work suggests that LLMs represent temporal concepts like days, months, and years, as well as arithmetic operations in a nonlinear way using circular and cylindric features [30, 31]. Describing the geometry of the embeddings directly. Directly analyzing the geometrical distribution of embedding vectors and their clustering can also provide insights into how LLMs organize their internal knowledge. Early studies on BERT, GPT2, and ELMo found that token embeddings are distributed anisotropically, forming narrow cones [32] and isolated clusters [7, 13]. This phenomenon has also been observed in CLIP embeddings [33], where image and text tokens occupy different cones, separated by a gap. A similar gap also exists between the subspace of different languages in multilingual LLMs [10], which enables approximating language translation with geometric translation between these subspaces. Recent work has shown that changes in the intrinsic dimension of the representations is related to different stages of information processing in LLMs, linking the rise of abstract semantic content to layers characterized by geometric compression [34] and the transition from surface-level to syntactic and semantic processing to layers of high dimension [35]. Comparing in-context learning and fine-tuning in LLMs. Recent studies have compared ICL to fine-tuning in LLMs by analyzing their ability to generalize, their efficiency, and how well they handle changes in the training data. Several factors can influence the outcomes when comparing ICL and fine-tuning. These include the format of the prompts in ICL [36, 37], the amount of training data used for fine-tuning [38, 39], and the size of models being compared [40]. In large models, ICL can be more robust to domain shifts and text perturbations than it is fine-tuning smaller-scale ones [2, 3]. However, when ICL and fine-tuning are compared in models of the same size fine-tuned on sufficient data, SFT can be more robust out-of-distribution, especially for medium-sized models [4]. Additionally, SFT achieves higher accuracy with lower inference costs [41].\nDescribing the geometry of the embeddings directly. Directly analyzing the geometrical distribution of embedding vectors and their clustering can also provide insights into how LLMs organize their internal knowledge. Early studies on BERT, GPT2, and ELMo found that token embeddings are distributed anisotropically, forming narrow cones [32] and isolated clusters [7, 13]. This phenomenon has also been observed in CLIP embeddings [33], where image and text tokens occupy different cones, separated by a gap. A similar gap also exists between the subspace of different languages in multilingual LLMs [10], which enables approximating language translation with geometric translation between these subspaces. Recent work has shown that changes in the intrinsic dimension of the representations is related to different stages of information processing in LLMs, linking the rise of abstract semantic content to layers characterized by geometric compression [34] and the transition from surface-level to syntactic and semantic processing to layers of high dimension [35]. Comparing in-context learning and fine-tuning in LLMs. Recent studies have compared ICL to fine-tuning in LLMs by analyzing their ability to generalize, their efficiency, and how well they handle changes in the training data. Several factors can influence the outcomes when comparing ICL and fine-tuning. These include the format of the prompts in ICL [36, 37], the amount of training data used for fine-tuning [38, 39], and the size of models being compared [40]. In large models, ICL can be more robust to domain shifts and text perturbations than it is fine-tuning smaller-scale ones [2, 3]. However, when ICL and fine-tuning are compared in models of the same size fine-tuned on sufficient data, SFT can be more robust out-of-distribution, especially for medium-sized models [4]. Additionally, SFT achieves higher accuracy with lower inference costs [41].\nComparing in-context learning and fine-tuning in LLMs. Recent studies have compared ICL to fine-tuning in LLMs by analyzing their ability to generalize, their efficiency, and how well they handle changes in the training data. Several factors can influence the outcomes when comparing ICL and fine-tuning. These include the format of the prompts in ICL [36, 37], the amount of training data used for fine-tuning [38, 39], and the size of models being compared [40]. In large models, ICL can be more robust to domain shifts and text perturbations than it is fine-tuning smaller-scale ones [2, 3]. However, when ICL and fine-tuning are compared in models of the same size fine-tuned on sufficient data, SFT can be more robust out-of-distribution, especially for medium-sized models [4]. Additionally, SFT achieves higher accuracy with lower inference costs [41].\n# 3 Methods\n# 3.1 Models and Dataset.\nMMLU Dataset. We analyze the Massive Multitask Language Understanding question answering dataset, MMLU [42], taking the implementation of cais_mmlu from Huggingface. The MMLU test set is one of the most widely used benchmarks for testing factual knowledge in state-of-the-art LLMs [43\u201346]. The dataset consists of multiple-choice question-answer pairs divided into 57 subjects. All the questions have four possible options labeled with the letters \"A,\" \"B,\" \"C,\" and \"D.\" When prompted with a question and a set of options, the LLMs must output the letter of the right answer. In this work, we will analyze the MMLU test set, where each subject contains at least 100 samples, with a median population of 152 samples and the most populated class containing about 1534 examples. To reduce the class imbalance without excessively reducing the dataset size, we randomly choose up to 200 examples from each subject. The final size of our dataset is 9181.\nMMLU Dataset. We analyze the Massive Multitask Language Understanding question answering dataset, MMLU [42], taking the implementation of cais_mmlu from Huggingface. The MMLU test set is one of the most widely used benchmarks for testing factual knowledge in state-of-the-art LLMs [43\u201346]. The dataset consists of multiple-choice question-answer pairs divided into 57 subjects. All the questions have four possible options labeled with the letters \"A,\" \"B,\" \"C,\" and \"D.\" When prompted with a question and a set of options, the LLMs must output the letter of the right answer. In this work, we will analyze the MMLU test set, where each subject contains at least 100 samples, with a median population of 152 samples and the most populated class containing about 1534 examples. To reduce the class imbalance without excessively reducing the dataset size, we randomly choose up to 200 examples from each subject. The final size of our dataset is 9181. Language models and token representations analyzed. We study the models of Llama3 [45], Llama2 [47] families, and Mistral [43]. We choose these LLMs because, as of May 2024, they are among the most competitive open-source models on the MMLU benchmark, with an accuracy significantly higher than the baseline of random guessing (25%, see Table A1). All the models we analyze are decoder-only, with a layer normalization at the beginning of each attention and MLP block. Llama2-7b, Llama3-8b, and Mistral-7b have 32 hidden representations, Llama2-13b 40, Llama2-70 and Llama3-70 80. In all cases, we analyze the representation of the last token of the prompt after the normalization layer at the beginning of each transformer block. For transformers\nLanguage models and token representations analyzed. We study the models of Llama3 [45], Llama2 [47] families, and Mistral [43]. We choose these LLMs because, as of May 2024, they are among the most competitive open-source models on the MMLU benchmark, with an accuracy significantly higher than the baseline of random guessing (25%, see Table A1). All the models we analyze are decoder-only, with a layer normalization at the beginning of each attention and MLP block. Llama2-7b, Llama3-8b, and Mistral-7b have 32 hidden representations, Llama2-13b 40, Llama2-70 and Llama3-70 80. In all cases, we analyze the representation of the last token of the prompt after the normalization layer at the beginning of each transformer block. For transformers\ntrained to predict the next token, the last token is the only one that can attend to all the sequence, and in the output layer, it encodes the answer to the question.\nFew-shot and fine-tuning details. We sample the shots from the MMLU dev set, which has five shots per subject. This choice differs from the standard practice, where the shots are always given in the same order for every input question. Table A1 shows that the final accuracies are consistent with the values reported in the models\u2019 technical reports [43, 45, 47]. We fine-tune the models with LoRA [48] on a training set formed by the union of the dev and some question-answer pairs of validation sets to reach an accuracy comparable to the 5-shot one. The specific training details are in Sec. A.\n3.2 Density peaks clustering We study the structure of the probability density of data representations with the Advanced Density Peaks algorithm (ADP) presented in D\u2019Errico et al. [11] and implemented in the DADApy package [49]. ADP is a mode-seeking, density-based clustering algorithm that finds the modes of the probability density by harnessing the low-dimensional structure of the data without performing any explicit dimensional reduction. ADP also estimates the density of the saddle points between pairs of clusters, which measures their similarity and provides information on their hierarchical arrangement. At a high level, the ADP algorithm can be divided into three steps: the estimation of the data\u2019s intrinsic dimension (ID), the estimation of the density around each point, and a final density-based clustering of the data. Intrinsic dimension estimation. We measure the ID of the token embeddings with Gride [50]. Gride estimates the ID of data points embedded in RD, using the distances between a token and its nearest neighbors. This is done by maximizing the likelihood function L(\u00b5k) = d(\u00b5d k\u22121)k\u22121 B(k,k)\u00b5d(2k\u22121)+1 k where \u00b5k is the ratio of the Euclidean distances between a point and its nearest neighbors of rank k2 = 2k and k1 = k, d is the ID, and B(k, k) a normalizing Beta function. By increasing the value of k, the ID is measured on nearest neighbors of increasing distance. The ID estimate is chosen as the value where the ID is less dependent on the hyperparameter k and the graph d(k) exhibits a plateau [50, 51]. On this basis, we choose k = 16. Density estimation. We measure the local density \u03c1i,k with a kNN estimate: \u03c1i,k = k/NVi,k. Here, N is the number of data points and Vi,k the volume of the ball, which has a radius equal to the distance between the point i and its kth nearest neighbor. Crucially, in this step, we compute the volume using the intrinsic dimension, setting k = 16, the value used to estimate the ID. Density-based clustering. With the knowledge of the \u03c1i, we find a collection of density maxima C = {c1, ...cn}, assign the data points around them, and find the saddle point density \u03c1\u03b1,\u03b2 between a pair of clusters c\u03b1 c\u03b2 with the procedure described in Sec. B. We can not regard all the local density maxima as genuine probability modes due to random density fluctuations arising from finite sampling. ADP assesses the statistical reliability of the density maxima with a t-test on log \u03c1\u03b1 \u2212log \u03c1\u03b1,\u03b2, where \u03c1\u03b1 is the maximum density in c\u03b1. Once the confidence level Z is fixed, all the clusters that do not pass the t-test are merged since the value of their density peaks is compatible with the density of the saddle point. The process is repeated until all the peaks satisfy the t-test and are statistically robust with a confidence Z. We set Z = 1.6, the default value of the DADAPy package. In the following sections, we will also use the notion of core cluster points defined as the set of points with a density higher than the lowest saddle point density. These are the points whose assignation to a cluster is considered reliable [11, 52]. With a slight abuse of terminology, we will use the terms \"clusters\", \"density peaks\", and \"probability modes\" interchangeably. Measuring the cluster similarity. The ADP algorithm considers two clusters similar if connected through a high-density saddle point. This is done defining the dissimilarity S\u03b1,\u03b2 between a pair of clusters c\u03b1 and c\u03b2 as S\u03b1,\u03b2 = log \u03c1max \u2212log \u03c1\u03b1,\u03b2, \u03c1max being the density of the highest peak. With S\u03b1,\u03b2 we perform hierarchical clustering of the peaks. We link the peaks starting from the pair with the lowest dissimilarity according to S\u03b1,\u03b2 and update the saddle point density between their union c\u03b3 and the rest of the peaks c\u03b4i with the WPGMA linkage strategy [53]: log \u03c1\u03b3,\u03b4i = log \u03c1\u03b1,\u03b4i + log \u03c1\u03b2,\u03b4i 2 .\n# 3.2 Density peaks clustering\nWe study the structure of the probability density of data representations with the Advanced Density Peaks algorithm (ADP) presented in D\u2019Errico et al. [11] and implemented in the DADApy package [49]. ADP is a mode-seeking, density-based clustering algorithm that finds the modes of the probability density by harnessing the low-dimensional structure of the data without performing any explicit dimensional reduction. ADP also estimates the density of the saddle points between pairs of clusters, which measures their similarity and provides information on their hierarchical arrangement. At a high level, the ADP algorithm can be divided into three steps: the estimation of the data\u2019s intrinsic dimension (ID), the estimation of the density around each point, and a final density-based clustering of the data.\nDensity estimation. We measure the local density \u03c1i,k with a kNN estimate: \u03c1i,k = k/NVi,k. Here, N is the number of data points and Vi,k the volume of the ball, which has a radius equal to the distance between the point i and its kth nearest neighbor. Crucially, in this step, we compute the volume using the intrinsic dimension, setting k = 16, the value used to estimate the ID.\nDensity-based clustering. With the knowledge of the \u03c1i, we find a collection of density maxima C = {c1, ...cn}, assign the data points around them, and find the saddle point density \u03c1\u03b1,\u03b2 between a pair of clusters c\u03b1 c\u03b2 with the procedure described in Sec. B. We can not regard all the local density maxima as genuine probability modes due to random density fluctuations arising from finite sampling. ADP assesses the statistical reliability of the density maxima with a t-test on log \u03c1\u03b1 \u2212log \u03c1\u03b1,\u03b2, where \u03c1\u03b1 is the maximum density in c\u03b1. Once the confidence level Z is fixed, all the clusters that do not pass the t-test are merged since the value of their density peaks is compatible with the density of the saddle point. The process is repeated until all the peaks satisfy the t-test and are statistically robust with a confidence Z. We set Z = 1.6, the default value of the DADAPy package. In the following sections, we will also use the notion of core cluster points defined as the set of points with a density higher than the lowest saddle point density. These are the points whose assignation to a cluster is considered reliable [11, 52]. With a slight abuse of terminology, we will use the terms \"clusters\", \"density peaks\", and \"probability modes\" interchangeably. Measuring the cluster similarity. The ADP algorithm considers two clusters similar if connected through a high-density saddle point. This is done defining the dissimilarity S\u03b1,\u03b2 between a pair of clusters c\u03b1 and c\u03b2 as S\u03b1,\u03b2 = log \u03c1max \u2212log \u03c1\u03b1,\u03b2, \u03c1max being the density of the highest peak. With S\u03b1,\u03b2 we perform hierarchical clustering of the peaks. We link the peaks starting from the pair with the lowest dissimilarity according to S\u03b1,\u03b2 and update the saddle point density between their union c\u03b3 and the rest of the peaks c\u03b4i with the WPGMA linkage strategy [53]: log \u03c1\u03b3,\u03b4i = log \u03c1\u03b1,\u03b4i + log \u03c1\u03b2,\u03b4i 2 .\nMeasuring the cluster similarity. The ADP algorithm considers two clusters similar if connected through a high-density saddle point. This is done defining the dissimilarity S\u03b1,\u03b2 between a pair of clusters c\u03b1 and c\u03b2 as S\u03b1,\u03b2 = log \u03c1max \u2212log \u03c1\u03b1,\u03b2, \u03c1max being the density of the highest peak. With S\u03b1,\u03b2 we perform hierarchical clustering of the peaks. We link the peaks starting from the pair with the lowest dissimilarity according to S\u03b1,\u03b2 and update the saddle point density between their union c\u03b3 and the rest of the peaks c\u03b4i with the WPGMA linkage strategy [53]: log \u03c1\u03b3,\u03b4i = log \u03c1\u03b1,\u03b4i + log \u03c1\u03b2,\u03b4i 2 .\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be33/be33f730-b305-4200-85db-794b83e0fd0d.png\" style=\"width: 50%;\"></div>\nFigure 2: Intrinsic dimension, number of density peaks, and fraction of core points. Figure shows the ID (left), the number of density peaks (center), and the fraction of core points (right) for the last-token representation of Llama3-8b for an increasing number of few-shots and fine-tuned models. The three quantities change in the proximity of layer 17 in a two-phased fashion.\n<div style=\"text-align: center;\">Figure 2: Intrinsic dimension, number of density peaks, and fraction of core points. Figure shows the ID (left), the number of density peaks (center), and the fraction of core points (right) for the last-token representation of Llama3-8b for an increasing number of few-shots and fine-tuned models. The three quantities change in the proximity of layer 17 in a two-phased fashion.</div>\nAfter the update of S, we repeat the procedure until we merge all the clusters. In this way, we display the topography [11] of the representation landscape of a layer, namely the relations between the density peaks, as a dendrogram.\nReproducibility. We run the experiments on a single Nvidia A100 GPU with 40GB memory. Extracting the hidden representation of 70 billion parameter models requires 5 A100, and their fine-tuning requires 8 A100. We provide code to reproduce our experiments at https://github. com/diegodoimo/geometry_icl_finetuning.\n# 4 Results\n# 1 The geometry of LLMs\u2019 representations shows a two-phased beha\nWe start by exploring the geometric properties of the representation landscape of LLMs. Our analysis proceeds from a broad description of the manifold geometry to its finer details. First, we measure the intrinsic dimension (ID) to understand the global structure of the data manifold. Next, we will describe the intermediate-scale behavior, counting the number of probability modes on it. Finally, we analyze the density distribution at the level of individual data points within the clusters. These three quantities consistently show a two-phased behavior across the hidden layers of the LLMs we analyzed. All profiles of this and the following sections are smoothed using a moving average over two consecutive layers. We report the original profiles in the Appendix from Sec. D.2 to D.4 and in Sec. D.6.\n# Abrupt changes in intrinsic dimension and probability landscape in middle layers.\nthe ID of the hidden representations with the Gride algorithm (see Sec. 3.2). Figure 2 shows the results for Llama3-8b; the analysis on the other models can be found in Sec. D.2 to D.4 of the Appendix. The left panel shows that the ID changes through the layers with two phases, increasing during the first half of Llama3-8b and decreasing towards the output layers. Specifically, the ID rises from 2.5 after the first attention block and peaks around layer 16. The value at the peak increases with the number of shots, from 14 in the base model to 16.5 when a 5-shot context is added. The fine-tuned model (0-shot) reaches a maximum ID of 21 at this layer. In the second half of the network, the IDs sharply decrease over the next three layers. For the few-shot representations, the ID profiles gradually decay in the final part of the network, while for the 0-shot models, the ID increases again. The same two-phased behavior appears in the evolution of the number of clusters on the hidden manifold (center panel). In the first half of the network, the probability landscape has a higher number of modes, ranging between 60 and 70, when the model is given shots, roughly matching the number of subjects. In the 0-shot and fine-tuned cases, it remains below 40. After layer 16, the number of peaks decreases significantly, remaining between 10 and 20. The right panel describes the representation landscape within individual clusters, measuring the fraction of core cluster points \u2013 those with a density higher than the lowest saddle point, indicating a reliable cluster assignment [11]. Before layer 17, the core point fraction is higher, indicating a better separation between probability\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a474/a4749839-5c4e-4a9e-b18a-35b9c9b13ca5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Adjusted Rand Index (ARI) between clusters and subjects. ARI between clusters and the subjects for Llama-3-8b (left), Llama-3-70b (center), and Mistral-7b (right) for an increasing number of few-shots and fine-tuned representations. In all cases, the match between cluster and subjects partition is highest at the beginning of the network and for an increasing number of shots.</div>\nmodes. Notably, the few-shot setting shows a core-point fraction of about 0.6, much higher than the 0-shot and fine-tuned case, which remains around 0.2. The evolution of ID, number of clusters, and core cluster points is qualitatively consistent among the models we analyzed. In the Llama2 family, the ID peak is less evident (Fig. A3-bottom). In particular, it is absent for the less accurate model, Llama2-7b. Nonetheless, the number of clusters, core points, and the remaining quantities, presented in the following sections, change according to the same two-phased trend as the other models.\nThe evolution of ID, number of clusters, and core cluster points is qualitatively consistent among the models we analyzed. In the Llama2 family, the ID peak is less evident (Fig. A3-bottom). In particular, it is absent for the less accurate model, Llama2-7b. Nonetheless, the number of clusters, core points, and the remaining quantities, presented in the following sections, change according to the same two-phased trend as the other models.\n# 4.2 The probability landscape before the transition.\nWe now describe the data distribution by focusing on the semantic content of the last token, specifically analyzing whether the cluster composition is consistent with the prompt\u2019s topic. Using the 57 MMLU subjects as a reference, we compare the differences in the early layers of the LLMs between ICL and fine-tuning.\nFew-shot learning forms clusters grouped by subject. To evaluate how well the clusters align with the subjects, we use the Adjusted Rand Index (ARI) [54]. An ARI of zero indicates that the density peaks do not correspond to subjects, while an ARI of one means a perfect match (see Appendix C for a detailed presentation). Figure 3 shows that as the number of few-shots increases, the ARI rises from below 0.27 in the 0-shot context to 0.82 in Llama3-8b (left), 0.72 in Llama3-70b (center) and 0.63 in Mistral (right) in the 5-shot settings. These ARI values correspond to a remarkable degree of purity of the clusters with respect to the subject composition. When the ARI is at its highest, 75 out of 77 clusters in Llama3-8b (layer 4), 53 out of 69 in Llama3-70b (layer 7), and 43 out of 53 in Mistral (layer 5) contain more than 80% of tokens from the same subject. In the next section, leveraging this high homogeneity of the clusters, we will connect the cluster similarity to the similarity between the subjects of the points they contain. Additionally, when the number of shots grows, the ARI peak shifts to earlier layers, and the peak becomes narrower. For example, in Llama3-8b, the 0-shot profile (blue) has a broad plateau extending until layer 13. With one and two-shot settings (orange and green), the profiles show a couple of peaks between layers 3 and 9, and with 5-shots (red), a single large maximum at layer 3. The trend is consistent across other models, especially those with 70 billion parameters (see Fig. 3-center for Llama3-70b, and Fig. A9 in the Appendix for Llama2-70b). In all cases, providing a longer, contextually relevant prompt enables models to identify high-level semantic features (i.e., the subjects) more accurately and earlier in their hidden representations. Few-shot prompting is not the only factor that increases the ARI with the subject. In the 0-shot setup, as the performance improves, LLMs organize their hidden representations more coherently with respect to the subject. In Llama3-8b and 70b models, where the 0-shot accuracy is above 62% (see Table A1), the 0-shot ARI is around 0.25. For the rest of the models with lower accuracy (below 56%), the ARI is below 0.18 (see blue profiles in Figs. 3 and A7).\nThe distribution of the density peaks mirrors the subject similarity. When the models learn \"in context\", not only do the number and composition of density peaks become more consistent with the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/13bc/13bcd2aa-67b9-42d5-a909-4c43d360045f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 4: Density peaks in the layers that best encode the subjects in Llama3-8b. The dendrograms show the organization of the density peaks in Llama3-8b in the layers where the ARI with the subjects is highest for the 5-shot setup (top) and 0-shot set-up (bottom left) and fine-tuned model (bottomright). In the 5-shot setup, the clusters are populated by examples from one or two related subjects, and their similarity reflects the semantic relationships between the subjects. In 0-shot and fine-tuned representations (bottom panels), some large clusters contain many subjects.\nsubjects, but they also organize hierarchically to reflect their semantic relationships. In this section, we describe only the cluster distribution in Llama3-8b in the layers where the ARI is highest and report the other models in the Appendix, Sec. D.7. Figure 4 shows the probability landscapes of the Llama3-8b in 0-shot (bottom left), 5-shot (top), and fine-tuned model (bottom-right) as dendrograms. Dendrograms are helpful visual descriptions of hierarchical clustering algorithms [55]. We perform hierarchical clustering of the density peaks using the agglomerative procedure described in Sec. 3.2. In the layers where the ARI is highest, the density peaks are homogeneous, and we can assign a single subject to each leaf of the dendrogram. This one-to-one mapping between clusters and subjects allows us to estimate subject similarity based on the dendrogram obtained from the (density-based) hierarchical clustering of the peaks. In all cases (0-shot, 5-shot, and fine-tuned model), clusters of subjects from the same broader field (STEM, medicine/biology, humanities, etc.) tend to be close together. However, in 0-shot and fine-tuned settings, the probability landscape has fewer and less pure density peaks at the subject level. In contrast, in the 5-shot setting, the number of clusters and their purity increase, and the 77 peaks are organized according to their high-level semantic relationships. For example, in the top panel of Fig. 4, four major groups of similar subjects can be identified: medicine and biology (orange), philosophy, jurisprudence, and moral disputes (blue), math, physics, and chemistry (red) and machine learning and computer science (green). In addition, these groups and hierarchical structures are consistent across different models (see A11 to A12). For instance, clusters related to statistics, machine learning, and computer science are often grouped together, as are those of chemistry, physics, and electrical engineering, or economy, geography, and global facts. The structure we described is also robust to changes in the confidence level Z. In the Appendix, from Sec. E.2.1 to Sec. E.2.8, we report the dendrograms obtained with Z = 0 and Z = 4. Importantly, even with Z = 0, where all local density maxima are considered as probability modes, the probability landscape of the 0-shot and fine-tuned models remains largely mixed. In contrast, the probability landscape of 5-shot representation is more stable to variations of Z.\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ac28/ac288d52-aaf0-492f-9c27-13aa66a4a211.png\" style=\"width: 50%;\"></div>\nFigure 5: Adjusted Rand Index between clusters and final answers. Adjusted Rand Index (ARI) between clusters and the MMLU answers (test set) for Llama3-8b (left), Llama3-70b (center), and Mistral-7b (right). In the second part of the network, the purity of the clusters w.r.t the answer partition is highest for fine-tuned models.\n<div style=\"text-align: center;\">Figure 5: Adjusted Rand Index between clusters and final answers. Adjusted Rand Index (ARI) between clusters and the MMLU answers (test set) for Llama3-8b (left), Llama3-70b (center), and Mistral-7b (right). In the second part of the network, the purity of the clusters w.r.t the answer partition is highest for fine-tuned models.</div>\n# 4.3 The probability landscape after the transition.\nIn Sec. 4.1, we observed that the number of density peaks decreases in the middle layers of the network. This reduction happens because the model needs to identify the answer from four options at the output, causing points related to the same answer to cluster around the same unembedding vector. In addition, when the model is uncertain about its predictions, some output embeddings tend to lie close to the decision boundaries of the last hidden representation, resulting in a flatter density distribution with fewer peaks. Even when the model is highly accurate, the linear separability of the answers does not guarantee distinct density peaks because the embeddings may still be near the decision boundary as long as they are on the correct side. However, more pronounced density peaks emerge as the model confidence grows and the data moves away from the decision boundaries. This section shows that SFT sharpens these density peaks in the later layers more than ICL. However, as model size and accuracy increase, the representation landscapes of ICL and SFT become more similar.\nFine-tuned density peaks better encode the answers better than few-shot ones. We evaluate how well the clusters match the answer partition (i.e., \"A,\" \"B,\" \"C,\" \"D\") using the ARI (see Sec. 4.2). When the models are fine-tuned, four to five large clusters emerge in the second part of the network, grouping answers with the same label. These clusters collect more than 70% of the data between layers 20 and 30 in Llama3-8b, more than 90% between layers 45 and 75 of Llama3-70b, and more than 65% between layers 21 and 30 in Mistral. In these clusters, the most common letter represents over 90% of the point in Llama3-8b, over 70% in Llama3-70b, and over 90% in Mistral. As a result, the ARI (see purple profiles in Fig. 5) rises sharply in the middle of the network, reaching approximately 0.25 in Llama3-8b, 0.45 in Llama3-70b, and 0.2 in Mistral. These ARI are related to the MMLU test accuracies of 65%, 78.5%, and 62%, respectively (see Table A2). In contrast, in ICL, the clusters are more mixed, and their number is smaller. In Llama3-8b in the 5-shot setup, one cluster contains 70% of the points in the last layers. In the 0-shot case, four clusters with a roughly equal distribution of letters contain the same amount of data (blue profile). Similar trends appear in Mistral\u2019s late layers. In both models, the ARI values for few-shot context stay below 0.05 (Fig. 5). Interestingly, in Llama3-70b (and to a lesser extent in Llama2-70b, see Fig. A8-right), the representation landscape of ICL starts to resemble that of the fine-tuned models. Between layers 40 and 77, about 80% of the dataset forms five large peaks, and in four of them, the fraction of the most common letter is above 0.9, similar to fine-tuned models. Consequently, in these layers, the ARI for few-shot contexts (see orange, green, and red profiles in Fig. 5-center) oscillates between 0.35 and 0.40, except for the 0-shot profile, which decays from 0.3 to 0.05 (blue profile). The different ways in which fine-tuning and ICL shape the representations of the network in the second half depend on the learning protocol, model size, and performance. In smaller models with moderate accuracy (below 65% / 70%), SFT and ICL can perform similarly as in the 5-shot setup (see Table A1) but they alter the geometry of the layers in different ways. However, with higher accuracy models like Llama3-70b (accuracy above 75%), both the performance of the model and the topography of the hidden representation tend to converge.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/197d/197db063-f184-46b6-b76c-3e1895d4a707.png\" style=\"width: 50%;\"></div>\nFigure 6: Evolution of similarity between 0-shot and fine-tuned models during training.. Panels show the dynamics of the representations\u2019 similarity with the base models for Llama3-8b (left) and Llama3-70b (center), and Mistral (right). Late representations change the most during fine-tuning.\n<div style=\"text-align: center;\">Figure 6: Evolution of similarity between 0-shot and fine-tuned models during training.. Panels show the dynamics of the representations\u2019 similarity with the base models for Llama3-8b (left) and Llama3-70b (center), and Mistral (right). Late representations change the most during fine-tuning.</div>\nFine-tuning primarily alters the representations after the transition. In fine-tuned models, training leads to the emergence of structured representations that align with the labels. Figure 6 shows where, during the training, layers change the most in Llama3-8b (left), Llama3-70b (center), and Mistral (right). We compare the fine-tuned checkpoints with the 0-shot representations before training begins. To measure the similarity between representations, we use the neighborhood overlap metric [34, 56] that calculates the fraction of the first k nearest neighbors of each point shared between pairs of representations, averaged over the dataset. Figure 6 shows that the similarity between representations is lower in the second part of the networks, decaying more sharply to its final value in the middle of the network from 0.5 to roughly 0.3 between layers 13 and 17 in Llama3-8b and Mistral, from 0.6 to 0.4 after layer 33 in Llama3-70b (see dark blue profiles). This picture is consistent with what is shown in the previous sections. In the first half of the network, the representation landscapes of 0-shot and fine-tuned models are similar both geometrically (Fig. 2) and semantically (Figs. 3 and 4). In the second half of the network, where the representations are modified more during the training, fine-tuned models develop fewer peaks, more consistent with the label distribution than those of the other models (Fig. 5).\n# 5 Discussion and conclusion\nThis study described how the probability landscape within the hidden layers of language models changes as they solve a question-answering task, comparing the differences between in-context learning and fine-tuning. We identified two phases in the model\u2019s internal processing, which are separated by significant changes in the geometry of the middle layers. The transition is marked by a peak in the ID and a sharp decrease in the number and separation of the probability modes. Notably, few-shot learning and fine-tuning display complementary behavior with respect to this transition. When examples are included in the prompt, the early layers of LLMs exhibit a welldefined hierarchical organization of the density peaks that recovers semantic relationships among questions\u2019 subjects. Conversely, fine-tuning primarily modifies the representations to encode the answers after the transition in the middle of the network. Advantages of the density-based clustering approach. Our research highlights how variations in density within the hidden layers relate to the emergence of different levels of semantic abstraction, a concept previously explored by Doimo et al. [56] in convolutional neural networks (CNNs) trained for classification. In CNNs, the probability landscape remains unimodal until the last handful of layers, where multiple probability modes emerge according to a hierarchical structure that mirrors the similarity of the categories. In decoder-only LLMs solving a semantically rich question-answering task, these hierarchically organized density peaks appear in the early layers of the models, especially when they learn in context. Our methodology also extends the work of Park et al. [29], enabling the discovery of meaningful hierarchies of concepts beyond the final hidden layer of LLMs, where the data representation can be non-linear [30]. Moreover, unlike previous studies that utilized k-means to identify clusters within hidden representations [9, 13, 57], the density peak method does not assume a convex cluster shape or impose a priori the number of clusters. Instead, clusters emerge naturally once a specific Z value is set (see Sec. 3.2). This allows for the automatic discovery of potentially meaningful data categorizations based on the\nstructure of the representation landscape without specifying external linguistic labels and without introducing additional probing parameters. In this respect, an approach is similar to that of Michael et al. [58], who used a weakly supervised method relying on pairs of positive/negative samples to uncover latent ontologies within representations.\nIntrinsic dimension and information processing. As discussed in section 4.1, a peak in intrinsic dimension separates two groups of layers serving different functions and being distinctly influenced by in-context learning and supervised fine-tuning. Other studies have also highlighted the crucial role of ID peaks in marking blocks of layers dedicated to different stages of information processing within deep neural networks. For example, Ansuini et al. [12] observed a peak of the ID in the intermediate layers of CNNs, separating layers that remove low-level image features like brightness from those that focus on extracting abstract concepts necessary for classification. In transformers trained to generate images, Valeriani et al. [34] identified two intermediate peaks delimiting layers rich in semantic features of the data characterized by geometric compression. In LLMs, Cheng et al. [35] showed that an ID peak marks a transition from representation that encodes surface-level linguistic properties to one rich in syntactic and semantic information. These studies suggest that ID peaks consistently indicate transitions between different stages of information processing within the hidden layers.\n# Application to adaptive low-rank fine-tuning. Our findings could improve stra\nApplication to adaptive low-rank fine-tuning. Our findings could improve strategies for adaptive low-rank fine-tuning. Several studies [59\u201361] tried to adjust the ranks of the LoRA matrices based on various criteria of \u2018importance\u2019 or relevance to downstream tasks. Our analysis of the similarity between fine-tuned and pre-trained layers (see Fig. 6) reveals that later layers are most impacted by fine-tuning, indicating that these layers should be assigned ranks. This approach would naturally prevent unnecessary modifications to the early layers during fine-tuning.\nLimitations. Estimating the density reliably requires a good sampling of the probability landscape. This can be a delicate condition if the intrinsic dimension is high, as often happens in neural network hidden layers. The ID values we report in this work lie between 4 and 22, with most of the layers having an ID below 16. Rodriguez et al. [62] showed that the density can be estimated reliably up to 15-20 dimensional spaces. Still, the upper bounds are problem-specific and depend on the density estimator used, the nature of the data, and the dataset size. In this work, we analyzed MMLU, which has a semantically rich set of topics characterized by good enough sampling. In the Appendix, in Sec. D, we show that our results extend to another dataset mixture with a subject partition similar to MMLU. However, the analysis of other QA datasets and generic textual sources would make our observations more general. The prompt structure can also be made more general. In the current study, we studied ICL framed as few-shot learning but further investigations on more differentiated contexts would strengthen our findings. Finally, the description of the transition observed in the proximity of half of the network can be analyzed more in detail, for instance, by providing an interpretation of the mechanism [63] underlying the information flow from the context to the last token position [64].\n# Acknowledgements and disclosure of funding\nWe thank Alessandro Laio for many helpful discussions and valuable suggestions. We also thank the technical support of the Laboratory of Data Engineering staff and acknowledge the AREA Science Park supercomputing platform ORFEO.\nA.A., A.C., and D. D. were supported by the project \u201cSupporto alla diagnosi di malattie rare tramite l\u2019intelligenza artificiale\"- CUP: F53C22001770002. A.A., A. C. were supported by the European Union \u2013 NextGenerationEU within the project PNRR \"PRP@CERIC\" IR0000028 - Mission 4 Component 2 Investment 3.1 Action 3.1.1. A.S. was supported by the project PON \u201cBIO Open Lab (BOL) - Raforzamento del capitale umano\u201d- CUP: J72F20000940007.\n# References\n[1] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021. [2] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee BoydGraber, and Lijuan Wang. Prompting GPT-3 to be reliable. In The Eleventh International Conference on Learning Representations, 2023. [3] Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, and Ludwig Schmidt. Exploring the landscape of distributional robustness for question answering models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5971\u20135987, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [4] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 12284\u201312314, Toronto, Canada, July 2023. Association for Computational Linguistics. [5] Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [6] John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129\u20134138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [7] William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4527\u20134546, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [8] Daniel Loureiro, Kiamehr Rezaee, Mohammad Taher Pilehvar, and Jos\u00e9 Camacho-Collados. Analysis and evaluation of language models for word sense disambiguation. Computational Linguistics, 47:387\u2013443, 2021. [9] Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. Visualizing and measuring the geometry of bert. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. 10] Tyler Chang, Zhuowen Tu, and Benjamin Bergen. The geometry of multilingual language model representations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 119\u2013136, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. 11] Maria d\u2019Errico, Elena Facco, Alessandro Laio, and Alex Rodriguez. Automatic topography of high-dimensional data sets by non-parametric density peak clustering. Information Sciences, 560:476\u2013492, 2021. 12] Alessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan. Intrinsic dimension of data representations in deep neural networks. Advances in Neural Information Processing Systems, 32, 2019. 13] Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. Isotropy in the contextual embedding space: Clusters and manifolds. In International Conference on Learning Representations, 2021.\n[14] Emily Cheng, Corentin Kervadec, and Marco Baroni. Bridging information-theoretic and geometric compression in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12397\u201312420, Singapore, December 2023. Association for Computational Linguistics. [15] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207\u2013219, March 2022. [16] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes, 2017. [17] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073\u2013 1094, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [18] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations, 2019. [19] Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00efc Barrault, and Marco Baroni. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126\u20132136, Melbourne, Australia, July 2018. Association for Computational Linguistics. [20] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine translation models learn about morphology? In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861\u2013872, Vancouver, Canada, July 2017. Association for Computational Linguistics. [21] Yonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1\u201310, Taipei, Taiwan, November 2017. Asian Federation of Natural Language Processing. [22] T. Blevins, O. Levy, and L. Zettlemoyer. Deep rnns encode soft hierarchical syntax. Proceedings of the 56th Association for Computational Linguistics (ACL), 2018. [23] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u20134601, Florence, Italy, July 2019. Association for Computational Linguistics. [24] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2020. [25] Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023. [26] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 16\u201330, Singapore, December 2023. Association for Computational Linguistics. [27] Wes Gurnee and Max Tegmark. Language models represent space and time. In The Twelfth International Conference on Learning Representations, 2024.\n[28] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 39643\u201339666. PMLR, 21\u201327 Jul 2024. [29] Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. The geometry of categorical and hierarchical concepts in large language models, 2024. [30] Joshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, and Max Tegmark. Not all language model features are linear, 2024. [31] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022. [32] Kawin Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 55\u201365, Hong Kong, China, November 2019. Association for Computational Linguistics. [33] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 17612\u201317625. Curran Associates, Inc., 2022. [34] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. Advances in Neural Information Processing Systems, 36, 2024. [35] Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, and Marco Baroni. Emergence of a high-dimensional abstraction phase in language transformers. arXiv preprint arXiv:2405.15471, 2024. [36] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR, 18\u201324 Jul 2021. [37] Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300\u20132344, Seattle, United States, July 2022. Association for Computational Linguistics. [38] Teven Le Scao and Alexander Rush. How many data points is a prompt worth? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627\u20132636, Online, June 2021. Association for Computational Linguistics. [39] Branislav Pecher, Ivan Srba, and Maria Bielikova. Comparing specialised small and general large language models on text classification: 100 labelled samples to achieve break-even performance, 2024. [40] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n[41] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 1950\u20131965. Curran Associates, Inc., 2022. [42] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [43] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [44] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [45] Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. [46] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [48] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [49] Aldo Glielmo, Iuri Macocco, Diego Doimo, Matteo Carli, Claudio Zeni, Romina Wild, Maria d\u2019Errico, Alex Rodriguez, and Alessandro Laio. Dadapy: Distance-based analysis of datamanifolds in python. Patterns, page 100589, 2022. [50] Francesco Denti, Diego Doimo, Alessandro Laio, and Antonietta Mira. The generalized ratios intrinsic dimension estimator. Scientific Reports, 12(1):20005, 2022. [51] Elena Facco, Andrea Pagnani, Elena Tea Russo, and Alessandro Laio. The intrinsic dimension of protein sequence evolution. PLoS computational biology, 15(4):e1006767, 2019. [52] Alex Rodriguez and Alessandro Laio. Clustering by fast search and find of density peaks. Science, 344(6191):1492\u20131496, 2014. [53] Robert R Sokal. A statistical method for evaluating systematic relationships. Univ. Kansas, Sci. Bull., 38:1409\u20131438, 1958. [54] Lawrence Hubert and Phipps Arabie. Comparing partitions. J. of Classification, 2(1):193\u2013218, 1985. [55] R. Tibshirani, T. Hastie, and J.H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction : with 200 Full-color Illustrations. Springer series in statistics. Springer, 2001. [56] Diego Doimo, Aldo Glielmo, Alessio Ansuini, and Alessandro Laio. Hierarchical nucleation in deep neural networks. Advances in Neural Information Processing Systems, 33:7526\u20137536, 2020. [57] Sara Rajaee and Mohammad Taher Pilehvar. A cluster-based approach for improving isotropy in contextual embedding space. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short\n[58] Julian Michael, Jan A. Botha, and Ian Tenney. Asking without telling: Exploring latent ontologies in contextual representations. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6792\u20136812, Online, November 2020. Association for Computational Linguistics. [59] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. [60] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. DyLoRA: Parameterefficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3274\u20133287, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. [61] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4133\u20134145, Singapore, December 2023. Association for Computational Linguistics. [62] Maria d\u2019Errico, Elena Facco, Alessandro Laio, and Alex Rodriguez. Automatic topography of high-dimensional data sets by non-parametric density peak clustering. Information Sciences, 560:476\u2013492, 2021. [63] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. [64] Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla, 2023. [65] Glenn W. Milligan and Martha C. Cooper. A study of the comparability of external criteria for hierarchical cluster analysis. Multivariate Behavioral Research, 21(4):441\u2013458, 1986. [66] William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846\u2013850, 1971. [67] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: A theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889\u20137901, Singapore, December 2023. Association for Computational Linguistics. [68] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2024. [69] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785\u2013794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.\n# Appendix\n# A Fine-tuning setup.\nThe dataset on which we fine-tune the models is the union of the MMLU dev set (all five examples per subject are selected) and a subset of the MMLU validation set. We select up to 20 examples per subject for Llama2-7b and Llama2-13b and up to 40 examples per subject for the rest of the models. In the first case, the dataset size is 1065, and the second is 1439. We train Llama3-8b and Mistral-7b for 6 epochs and the remaining models for 4. We fine-tune the models with LoRA. The LoRA rank is 64, \u03b1 is 16, and dropout = 0.1. For the 70 billion models, we choose a rank of 128, and \u03b1 is 32. This is an empirically reasonable choice since the embedding dimension of these models is two times larger than the 7 billion ones. We use a learning rate = 2 \u00b7 10\u22124; for the 70 billion models we decrease it to 1 \u00b7 10\u22124. For all the models, we apply a cosine annealing scheduler and a linear warm-up for 5% of the total iterations. We fine-tune all the models with batch size = 16 using the Adam optimizer without weight decay. Below, we report the performance of the MMLU test set on all the 14042 samples, for 0-shot, 5-shot, and fine-tuned models. To compute the macro average, we first measure the accuracy for each subject. Then, we compute the arithmetic mean of the subject accuracies. The micro average is the fraction of correct answers taken over the dataset. The two quantities differ since the dataset is unbalanced.\n<div style=\"text-align: center;\">-shot accuracies. We report micro and macro averages on the MMLU tes</div>\nmodel\nnum shots\naccuracy % (macro/micro)\nLlama3-8b\n0 shot\n63.7 / 62.1\nLlama3-8b\n5 shot\n66.5 / 65.1\nLlama3-70b\n0 shot\n76.7 / 75.0\nLlama3-70b\n5 shot\n79.2 / 78.5\nMistral-7b\n0 shot\n57.8 / 55.9\nMistral-7b\n5 shot\n63.7 / 62.1\nLlama2-7b\n0 shot\n38.9 / 37.5\nLlama2-7b\n5 shot\n46.6 / 45.9\nLlama2-13b\n0 shot\n52.9 / 52.0\nLlama2-13b\n5 shot\n55.4 / 54.8\nLlama2-70b\n0 shot\n66.3 / 65.5\nLlama2-70b\n5 shot\n69.5 / 68.8\n<div style=\"text-align: center;\">Table A2: Fine-tuning accuracies. We report the micro and macro averages on the MMLU test set We only report the micro average on the train sets.</div>\nmodel\nepochs\naccuracy % test (macro/micro)\naccuracy % train (micro)\nLlama3-8b\n6\n65.6 / 64.8\n94.8\nLlama3-70b\n4\n78.5 / 79.0\n93.8\nMistral-7b\n6\n61.4 / 59.9\n96.4\nLlama2-7b\n4\n51.9 / 50.7\n73.8\nLlama2-13b\n4\n56.1 / 55.5\n79.7\nLlama2-70b\n4\n69.9 / 71.1\n92.4\n# B Iterative search of density peaks and saddle points.\nLet Nk(i) be the set of k points nearest to xi in Euclidean distance at a given layer l.\n N The first step of the density-peaks clustering is finding the point of maximum density \u03c1i (namely the probability peaks). Data point i is a maximum if the following two properties hold: (I) \u03c1i > \u03c1j for all the points j belonging to Nk(i); (II) i does not belong to the neighborhood Nk(j) of any other point of higher density [11]. (I) and (II) must be jointly verified as the neighborhood ranks are not symmetric between pairs of points. A different integer label C = {c1, ...cn} is then assigned to each of the n maxima. The data points that are not maxima are iteratively linked to one of these labels by assigning the same label as its nearest neighbor of higher density to each point. The set of points with the same label corresponds to a cluster. The saddle points between two clusters are identified as the points of maximum density between those lying on the borders between the clusters. A point xi \u2208c\u03b1 is assumed to belong to the border \u2202c\u03b1,c\u03b2 with a different peak c\u03b2 if exists a point xj \u2208Nk(i) \u2229c\u03b2 whose distance from i is smaller than the distance from any other point belonging to c\u03b1. The saddle point between c\u03b1 and c\u03b2 is the point of maximum density in \u2202c\u03b1,c\u03b2.\nThe saddle points between two clusters are identified as the points of maximum density between those lying on the borders between the clusters. A point xi \u2208c\u03b1 is assumed to belong to the border \u2202c\u03b1,c\u03b2 with a different peak c\u03b2 if exists a point xj \u2208Nk(i) \u2229c\u03b2 whose distance from i is smaller than the distance from any other point belonging to c\u03b1. The saddle point between c\u03b1 and c\u03b2 is the point of maximum density in \u2202c\u03b1,c\u03b2.\n# C The Adjusted Rand Index.\nTo determine to which degree the density peaks are consistent with the abstractions of the data, we will compare the clustering induced by the density peaks algorithm with the partition given by the 300 classes and that given by the high-level subdivision in animals and objects. Among the many possible scores, we choose the Adjusted Rand Index (ARI) [54], one of the best clusters of external evaluation measures according to [65]. The Rand Index (RI) [66] measures the consistency between a cluster partition C to a reference partition R counting how many times a pair of points: a are placed in the same group in C and R; b are placed in different groups in C and R; c are placed in different groups in C but in the same group in R; d are placed in the same group in C but in different groups in R; and measures the consistency with RI = (a+b)/(a+b+c+d). The Rand Index is not corrected for chance, meaning it does not give a constant value (e.g., zero) when the two assignments are random. [54] proposed to adjust RI, taking into account the expected value of the Rand Index, nc, under a suitable null model for chance:\nARI is equal to 1 when the two partitions are consistent and 0 when the assignments are random and can take negative values. A large value of ARI not only implies that instances of the same class are put in the same cluster (homogeneity) but also that the data points of a class are assigned to a single cluster (completeness).\n(1)\n# D Additional experiments\n# D.1 Analysis on an additional dataset mixture.\nIn this section, we validate the findings shown in Section 4.1 on a dataset constructed from TheoremQA [67], ScibenchQA [68], Stemez [3], and RACE [69]. This dataset contains roughly 6700 examples not included in MMLU, ten subjects from STEM topics, and a middle school reading comprehension task (RACE), with at least 300 examples per subject. We keep four choices for each answer. The 0-shot, 5-shot, and fine-tuned accuracies in Llama3-8b are 55%, 57%, and 58%, respectively. In Fig. A1-left, we see that the intrinsic dimension profiles have a peak around layers 15/16, the same layers as in MMLU (see Fig. 2-left). This peak in ID signals the transition between the two phases described in the paper. Before layer 17, few-shot models encode better information about the subjects (ARI with subjects above 0.8). Between layers 3 and 7, the peaks in 5-shot layers reflect the semantic similarity of the subjects (see the dendrograms for layer five reported in Fig. A2). Fine-tuning instead changes the representations after layer 17, where ARI with the answers for the is \u223c0.15, higher than that of the 5-shot and 0-shot models. The absolute value is lower than that reported in the main paper (Fig. 5-left) because the fine-tuned accuracy reached on the STEM subjects in this dataset is lower. Overall, the results are consistent with those shown in the paper for MMLU.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6bb6/6bb6e8b2-a99f-4265-9e07-32e100df48b5.png\" style=\"width: 50%;\"></div>\nFigure A1: Intrinsic dimension (left), ARI with the subjects (center) and with the answers (right) in Llama 3 8b. The dataset is constructed from Scibench, which has 541 QA pairs about chemistry, math, physics), TheoremQA with 598 QA pairs about business, computer science, math, and physics, Stemez with 4083 QA pairs about biology, business, chemistry, computer science, economics, engineering, physics, psychology, and the test set of RACE with 1436 QA pairs about middle school tests. We finetuned Llama 3 8b, keeping 50 examples per topic.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/13a5/13a5f102-5231-4fea-8e3a-c01b5c48b60f.png\" style=\"width: 50%;\"></div>\nre A2: Organization of the density peaks of layer 5 of Llama 3 8b in t\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2aa4/2aa477e1-cb30-493a-b449-74c6997a0344.png\" style=\"width: 50%;\"></div>\nFigure A3: Intrinsic dimension (ID) of the data representations in hidden layers of large transformers for increasing number of shots in the prompt and fine-tuned representations. Llama 3 8b (upper left), Llama 3 70b (upper center), Mistral 7b (upper right), Llama 2 7b (lower left), Llama 2 13b (lower center), and Llama 2 70b (lower right). For each layer, we calculated the intrinsic dimension of the hidden representations produced at the last token of the prompt, which provides the answer. The algorithm used is Gride with k = 16 nearest neighbors considered. We observe that all models exhibit similar behavior in the early layers regardless of the number of shots in the prompt. However, from the middle of the network onwards, the profiles diverge. Configurations that achieve a higher agreement between clusters and the letter given as the answer (as shown in A10) in the last layers will have a lower intrinsic dimension. These results support the intuitive observation that fewer features are needed to represent the information as the model starts to encode the letter it will provide as the answer.\nD.2.1 Scale analysis of the intrinsic dimension.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8273/82733b5f-7dc4-4ade-98b9-4a47531cf2d8.png\" style=\"width: 50%;\"></div>\nFigure A4: Scale analysis of the intrinsic dimension for Llama 3 8b. The plot presented herein depicts the intrinsic dimension (ID) at layers 5, 10, 15, 20, 25, and 30, varying the number of nearest neighbors k utilized by the Gride algorithm. Following the methodology delineated in [50, 51]; we observe the evolution of the ID as a function of k to identify a plateau in the estimates. After initial growth, from k = 8 to k = 16, the ID stabilizes and then decreases. Consequently, we select k = 16 as it represents the scale appropriate for examining the topology of the data being.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aeea/aeea5fba-5652-410b-aa97-8ccb8c48e0bb.png\" style=\"width: 50%;\"></div>\nFigure A5: Number of clusters across layers in large transformers. The plots above show the number of clusters identified by the Adaptive Density Peak (ADP) clustering algorithm at each model layer. In configurations where multiple examples are provided in the context, we can observe that the number of clusters closely matches the 57 subject categories present in the MMLU dataset. This phenomenon aligns with the observation that the agreement between the cluster partition and the subject-induced partition from MMLU is primarily influenced by the number of examples provided, as illustrated in Figure 3.\n# D.4 Core points\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fff1/fff1d7ae-4ac2-48b5-be46-013e12a02693.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure A6: Number of core points across layers in large transformers. The figure shows the fraction of points classified by ADP as \"core,\" indicating they were assigned to a cluster with higher confidence. Conversely, the algorithm designates points with lower densities than the highest border density between clusters as \"halo\" and thus, these points are discarded.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d47c/d47caa8c-9723-4bbc-83f8-d92f9db5041a.png\" style=\"width: 50%;\"></div>\nFigure A7: Adjusted Rand Index (ARI) between clusters and MMLU subjects (test set). The figure shows the ARI with the subjects for Llama2-7b (left), Llama2-13b (center), and Llama2-70b (right) for increasing the number of shots in the prompt and fine-tuned representations. The plot illustrates the agreement between the partition generated by the clusters and the partition induced by the subject labels from the MMLU dataset. The clustering was performed using Density Peak Clustering with Z = 1.6, the default value of the DADAPy package. As depicted in 3, the agreement is higher at the initial layers of the network and increases with a growing number of examples provided in the context. In other words, the clustering aligns more closely with the subject label in the early layers of the network in the few-shot setting.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d28/9d2807b5-05ac-456b-9492-83471fa97d72.png\" style=\"width: 50%;\"></div>\nFigure A8: Adjusted Rand Index between clusters and final answers. Llama2-7b (left), Llama213b (center), and llama2-70b (right) for increasing number of shots in the prompt and fine-tuned representations. The plot depicts the correspondence between the partitions generated by the clusters and the partitions induced by the model\u2019s predicted answer letters. The clustering was performed using Density Peak Clustering with Z = 1.6, the default value of the DADAPy package. As shown in 3, we can observe a trend that is opposite to the one seen with the subject label: the purity of the clusters with respect to the answer partition is higher in the final layers of the model, and it does not depend on the number of examples provided in the prompt. In this case, the fine-tuned model with zero examples in the context (0-shot) achieves the highest Adjusted Rand Index (ARI), indicating the highest agreement between the clustering and the model\u2019s predicted answer partitions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7cf/b7cf9acc-50fa-45f3-8834-a1a2aebfcf76.png\" style=\"width: 50%;\"></div>\nFigure A9: Adjusted Rand Index (ARI) between clusters and MMLU subjects (test set) without moving average. Llama 3 8b (upper left), Llama 3 70b (upper center), Mistral 7b (upper right), Llama 2 7b (lower left), Llama 2 13b (lower center),",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of how in-context learning (ICL) and supervised fine-tuning (SFT) influence the internal representations of large language models (LLMs) while solving domain-specific tasks, particularly in the context of question-answering tasks.",
        "problem": {
            "definition": "The problem is to understand the differences in representation structures induced by ICL and SFT in LLMs, despite both methods achieving similar performance levels.",
            "key obstacle": "The main challenge is the lack of understanding of how these two learning paradigms affect the model\u2019s representation space and the internal structures created within the models."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that different learning paradigms might lead to distinct internal representations within LLMs.",
            "opinion": "The authors propose that ICL and SFT, while both effective, yield fundamentally different internal representation landscapes.",
            "innovation": "The main improvement compared to previous methods is the density-based approach taken to analyze the hidden representations, which reveals differences in how ICL and SFT structure the model's internal knowledge."
        },
        "Theory": {
            "perspective": "The theoretical perspective is grounded in the geometry of representations within neural networks, particularly focusing on intrinsic dimensionality and density peaks.",
            "opinion": "The authors assume that the geometry of the hidden layers is crucial for understanding how LLMs process information and generate outputs.",
            "proof": "The proof is derived from analyzing the intrinsic dimensions and density peaks of the representations across the layers of LLMs, showing distinct patterns for ICL and SFT."
        },
        "experiments": {
            "evaluation setting": "The evaluation is conducted using the Massive Multitask Language Understanding (MMLU) dataset, with a focus on question-answer pairs across various subjects. The models analyzed include Llama3, Llama2, and Mistral families.",
            "evaluation method": "The evaluation involves measuring the intrinsic dimension, the number of density peaks, and the Adjusted Rand Index (ARI) to assess the clustering of representations in relation to subjects and answers."
        },
        "conclusion": "The study concludes that ICL and SFT induce different geometrical structures in the representations of LLMs, with ICL leading to more interpretable and semantically organized representations in early layers, while SFT enhances the clustering of representations related to answers in later layers.",
        "discussion": {
            "advantage": "The paper provides insights into the distinct computational strategies employed by LLMs under different learning paradigms, which can inform better model design and fine-tuning strategies.",
            "limitation": "A limitation of the study is the reliance on specific datasets, which may not generalize to other contexts or tasks.",
            "future work": "Future research could explore the application of these findings to other datasets and investigate different prompting techniques beyond few-shot learning."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "This paper addresses how in-context learning (ICL) and supervised fine-tuning (SFT) influence the internal representations of large language models (LLMs) while solving domain-specific tasks, particularly in the context of question-answering tasks."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective is grounded in the geometry of representations within neural networks, particularly focusing on intrinsic dimensionality and density peaks."
        },
        {
            "section number": "3.3",
            "key information": "The paper employs a density-based approach to analyze the hidden representations, revealing differences in how ICL and SFT structure the model's internal knowledge."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the study is the reliance on specific datasets, which may not generalize to other contexts or tasks."
        },
        {
            "section number": "7",
            "key information": "The study concludes that ICL and SFT induce different geometrical structures in the representations of LLMs, with ICL leading to more interpretable and semantically organized representations in early layers."
        }
    ],
    "similarity_score": 0.6926774944413969,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/The representation landscape of few-shot learning and fine-tuning in large language models.json"
}