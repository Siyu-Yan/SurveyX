{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.12460",
    "title": "Democratize with Care: The need for fairness specific features in user-interface based open source AutoML tools",
    "abstract": "AI is increasingly playing a pivotal role in businesses and organizations, impacting the outcomes and interests of human users. Automated Machine Learning (AutoML) streamlines the machine learning model development process by automating repetitive tasks and making data-driven decisions, enabling even non-experts to construct high-quality models efficiently. This democratization allows more users (including non-experts) to access and utilize state-of-the-art machine-learning expertise. However, AutoML tools may also propagate bias in the way these tools handle the data, model choices, and optimization approaches adopted. We conducted an experimental study of User-interface-based open source AutoML tools (DataRobot, H2O Studio, Dataiku, and Rapidminer Studio) to examine if they had features to assist users in developing fairness-aware machine learning models. The experiments covered the following considerations for the evaluation of features: understanding use case context, data representation, feature relevance and sensitivity, data bias and preprocessing techniques, data handling capabilities, training-testing split, hyperparameter handling, and constraints, fairness-oriented model development, explainability and ability to download and edit models by the user. The results revealed inadequacies in features that could support in fairness-aware model development. Further, the results also highlight the need to establish certain essential features for promoting fairness in AutoML tools.",
    "bib_name": "narayanan2023democratizecareneedfairness",
    "md_text": "# Democratise with Care: The need for fairness specific features in user-interface based open source AutoML tools\n16 Dec 2023\nSundaraparipurnan Narayanan\nDoctoral Candidate, University of Bordeaux sundar.narayanan@aitechethics.com\nAI is increasingly playing a pivotal role in businesses and organizations, impacting the outcomes and interests of human users. Automated Machine Learning (AutoML) streamlines the machine learning model development process by automating repetitive tasks and making data-driven decisions, enabling even nonexperts to construct high-quality models efficiently. This democratization allows more users (including non-experts) to access and utilize state-of-the-art machinelearning expertise. However, AutoML tools may also propagate bias in the way these tools handle the data, model choices, and optimization approaches adopted. We conducted an experimental study of User-interface-based open source AutoML tools (DataRobot, H2O Studio, Dataiku, and Rapidminer Studio) to examine if they had features to assist users in developing fairness-aware machine learning models. The experiments covered the following considerations for the evaluation of features: understanding use case context, data representation, feature relevance and sensitivity, data bias and preprocessing techniques, data handling capabilities, training-testing split, hyperparameter handling, and constraints, fairness-oriented model development, explainability and ability to download and edit models by the user. The results revealed inadequacies in features that could support in fairnessaware model development. Further, the results also highlight the need to establish certain essential features for promoting fairness in AutoML tools.\narXiv:2312.12460v1\n# 1 Introduction\nArtificial intelligence (AI), a disruptive technology since its inception, impacts various human rights issues, from discrimination to supply chains [12] . AI isn\u2019t just for engineers and technical staff; it\u2019s a business necessity. As technology evolves over the coming years, more significant risks will be associated with adopting advanced technology[1] . AI is increasingly playing a pivotal role in businesses and organizations, impacting the outcomes and interests of human users [7] . Many machine learning methods\u2019 performance is susceptible to many design decisions. To achieve consistent success, machine learning experts must select the appropriate features, workflows, and algorithms; these complex tasks may be too difficult for non-experts to tackle alone, spurring demand for methods that can be employed without special knowledge or skillsets. Such methods are collectively called Automated Machine Learning [10]. Automated Machine Learning, also called automated ML or AutoML automates the tedious, repetitive tasks involved in machine learning model development. This enables data scientists, analysts, developers, and programmers to construct large-scale models that are efficient and productive while still maintaining model quality. Automated machine learning (AutoML) automates these decisions through data-driven, objective methods: the user provides input data and the AutoML system auto-\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nmatically determines the most suitable approach for their particular use case. AutoML is perceived as a democratization of machine learning; with AutoML, customized state-of-the-art machine learning expertise is within everyone\u2019s reach. Historic adverse incidents have established that AI technology also carries risks that could negatively affect individuals, groups and organizations, communities, society, and the environment. With the increasing importance of algorithmic decision-making in distributing limited resources and the growing dependence of humans on AI\u2019s outcomes, concerns regarding fairness are on the rise. Evidence shows that algorithmic systems can perpetuate racism, classism, sexism, and other forms of discrimination that cause tangible harm. They were implicated in cases where people of color were denied benefits such as kidney transplants and mortgages due to biases in facial recognition technology [6]. These are just some documented harm caused by algorithmic systems [14]. Essentially, Fairness in ML ensures that ML models do not discriminate against certain groups of people. For example, a fairness-aware ML model should not predict that a person is more likely to commit a crime based on race or gender. AutoML systems are not an exception in this context. AutoML systems are becoming more widely utilized. Gartner predicts that sixty-five percent of all application development activities will use low-code or no-code tools by 2024. AutoML falls in the low-code or no-code applications category, enabling non-expert users to build machine learning models. Non-expert users of AutoML tools may be particularly vulnerable to fairness concerns. This is because they may not have the expertise to identify or address potential fairness issues in the data or the model. Hence, assessing the risks of unfair AutoML systems and whether there are sufficient fairness mitigation techniques to increase overall fairness is essential [24]. This paper deals with the experimental study of four AutoML tools to examine if they sufficiently support fairness expectations for non-expert users. This research is focused on gathering information on features within the AutoML tools that help fairness.\n# 2 Experimental Study\n# 2.1 Identification of AutoML tools for the experiment\nTo identify AutoML tools for fairness features evaluation, we reviewed 16 tools, including 10 user interface (UI)-based tools (RapidMiner Studio, H2O Studio, Google Vertex AI, Amazon Web Services AutoML, IBM WatsonX, Azure AutoML, Data Robot, Dataiku, KNIME, and Alteryx) and 6 code libraries (PyCaret, Auto-SKlearn, AutoGluon, MLBOX, TPOT, and Auto-Keras). We focused on UI-based tools because they are more accessible to users of all skill levels. Of the UI-based tools, we excluded cloud-based platform tools that require platform understanding for use (Google Vertex AI, Amazon Web Services AutoML, IBM WatsonX, and Azure AutoML). For the above identification, we considered the following criteria: (1) Ease of use: AutoML tools should be easy to learn and use, even for users with limited machine learning experience; (2) Data loading and preparation: AutoML tools should provide a simple and efficient way to load and prepare data for machine learning; (3) Platform accessibility: AutoML tools should be accessible to users on a variety of platforms, including cloud-based and desktop environments; and (4) Non-expert friendly user interface: AutoML tools should have a user interface that is easy to understand and use, even for users with no prior knowledge of machine learning. These criteria were chosen to ensure that we selected AutoML tools that are accessible and useful to a wide range of users. This left us with six UI-based tools for evaluation: RapidMiner Studio, H2O Studio, Data Robot, Dataiku, KNIME, and Alteryx. For the research, we examine RapidMiner Studio, H2O Studio, Data Robot, and Dataiku, in the first phase.\n# 2.2 Fairness features consideration for experimental study\nThis experimental study focused on an audit approach to examine the fairness features available in the UI based tools for evaluation. The fairness features considered for the experimejtal study could includes Dataset diversity and representation [22], checks for biases and unbalanced data [25], applies consistent preprocessing [15], ensures fair training-testing split [15], removes sensitive features [9]; [19], avoids hyperparameter bias, prevents overfitting [16], tests on diverse data, avoids underfitting [8], evaluates using fairness-sensitive metrics [3] [2] [18] [21] and compares to a benchmark [20] , alerts about high-risk sensitive features [13] [23], ensures model robustness to data variations , and handles missing or incomplete, noisy or corrupted, rare or unusual data points [11] [4] , or imbalanced datasets in a fair and unbiased manner [5]. In addition, we considered seeking use\ncase context, understanding data sensitivity, handling monotonic constraints, developing fairnessaware models, explaining the prediction results, handling model documentation, allowing users to download the model/ code, and highlighting the model\u2019s limitations as additional considerations for evaluation.\n# 2.3 Approach towards experiments\nOur research aimed to assess the fairness support capabilities of four prominent AutoML tools: RapidMiner Studio, H2O Studio, DataRobot, and Dataiku. The study was initiated by downloading and installing the necessary software. Subsequently, a sample dataset was introduced. The dataset pertains to the direct marketing initiatives (phone calls) conducted by a banking institution in Portugal. The dataset comprised information on credit default occurrences among customers, including demographic variables such as age and marital status [17]. The main aim of our study was to utilize the provided dataset to train machine learning models to predict the likelihood of a credit default. During our review, we directed our attention toward criteria to evaluate the fairness capabilities of each tool. In this study, we investigated the presence of three fundamental categories of features: (a) features that could promote fairness, (b) features that could facilitate the identification of fairness issues by the user, and (c) features that had the options for users to make choices regarding feature selection, model building, and other related processes.\n# 3 Results from the Study\nxploring fairness features in prominent AutoML tools, we evaluated Rapidminer Studio, H2O dio, Data Robot, and Dataiku across various criteria for fair machine learning development. 1. Use Case Context and Understanding: None of the tools provided features specifically seeking the context of use cases. Similarly, no tool provided users with options for specifying use case context or understanding data sensitivity. 2. Data Representation: All tools lacked explicit features for ensuring dataset diversity and representation. Additionally, they did not support user-based discovery for dataset diversity or provide options. The tools had the option to assess the statistical distribution of the features. 3. Feature Relevance and Sensitivity: Each tool successfully highlights the relevance of features, supporting user-based discovery of such elements. While Rapidminer Studio could highlight and handle sensitive features, the others did not have such a feature. 4. Data Bias and Preprocessing: Bias detection was notably weak across the tools. However, Rapidminer Studio and Dataiku allowed users to discover unbalanced data. All tools had a mechanism for consistently applying preprocessing approaches, Rapidminer Studio, H2O Studio, and Dataiku allowed users to find these preprocessing methods, while Rapidminer provided users with an option to make changes to the pre-processing parameters. 5. Data Handling Capabilities: Each tool could handle missing or atypical data points. H2O Studio, Data Robot, and Dataiku also supported user discovery for these data handling methods. However, none of the tools allowed the user to suggest/ change the approach to handling missing or atypical data on the application. 6. Training and Testing Split: All tools ensured a consistent approach to the training-testing split. However, the tools did not allow the user to determine the extent of the training and testing split, except Data Robot, which offered limited customizability based on percentages. 7. Hyperparameters and Constraints: All the tools handled hyperparameters, but only Dataiku enabled user discovery for this process. Further, only H2O provided an option for the user to make changes to the hyperparameter considerations. Also, only Dataiku provided a feature for handling constraints, allowing users to discover and modify them. 8. Fairness-Oriented Model Development: None of the tools had features to develop fairness-aware models. While Dataiku had a part through which the user can evaluate the model\u2019s fairness developed with specific metrics (demographic parity, equalized odds, equality of opportunity, and predictive rate parity), the other tools did not have any such feature.\n9. Explainability and Model Limitations: Rapidminer Studio, Data Robot, and Dataiku could explain decision-making, with Data Robot offering explanations through an app interface supporting users to discover the model performancethrough explanations. RapidMiner had a feature that allowed users to choose whether to train the model to provide explanations for predictions or otherwise at the time of model development. However, none of the applications provided any limitations associated with the models for the user to be aware when deploying these models in a certain use case context. 10. Documentation and Download of Model: All tools provided documentation of the model training; however, this documentation does not include any references to fairness-aware strategies applied in the model. In addition, H2O, Dataiku, and Rapidminer allow the user to download the model for changes or deployment elsewhere.\n# 4 Discussion and Conclusion\nThe evaluation of fairness attributes in prominent AutoML tools, such as Rapidminer Studio, H2O Studio, Data Robot, and Dataiku, has unveiled a disconcerting deficiency in providing full assistance for fairness-aware machine learning. Although all the tools described in this study possess qualities such as highlighting the importance of features and capacities for processing data, none of them specifically address the development of fairness. The observed deficiencies include the lack of features that address the contextual use case, promote dataset diversity, or facilitate the creation of models with a focus on fairness. The solutions focused on data processing functionalities such as managing missing data or hyperparameters but did not provide users with substantial capabilities to customize fairness strategies. As noted, the absence of fairness features or context-aware considerations has the potential to result in the development of biased or unfair models unintentionally. Considering that these tools are frequently utilized by individuals lacking expertise in the field, an increased risk is involved. These users may lack awareness regarding the potential biases included in their models, hence perpetuating biased decision-making. The absence of fairness considerations can result in societal ramifications in a socio-technological environment where machine learning models substantially influence decision-making processes. The results underscore the need for AutoML platforms to integrate comprehensive fairness-aware features, ensuring more ethical and equitable machine learning deployments. In conclusion, while the examined AutoML tools demonstrate robust features in some fairnessrelated regions, there are significant gaps, especially concerning developing fairness-aware models and evaluations. While the current research focused on fairness-aware considerations, AutoML tools, especially those designed for non-experts, shall prioritize features that promote responsible and trustworthy models to ensure their widespread adoption doesn\u2019t perpetuate harm. Some examples of these features include bias detection and mitigation techniques, built-in privacy protections, and explainability mechanisms to help users understand the model\u2019s decision-making process. Additionally, AutoML tools should incorporate ethical considerations in their optimization process, ensuring that the models produced are fair, transparent, and aligned with societal values. By combining these functionalities, AutoML can democratize machine learning but also help mitigate potential risks and foster trust in the technology.\n# References\n[1] Christoph Bartneck et al. \u201cRisks in the Business of AI\u201d. In: SpringerBriefs in Ethics (2021). DOI: 10.1007/978-3-030-51110-4_6. URL: https://link.springer.com/chapter/10.1007/978-3-030-51110-4_6. [2] Alessandro Castelnovo et al. \u201cA clarification of the nuances in the fairness metrics landscape\u201d. In: Scientific Reports 2022 12:1 (2022). DOI: 10.1038/s41598-022-07939-1. URL: https://www.nature.com/articles/s41598-022-07939-1. [3] Alessandro Castelnovo et al. \u201cThe Zoo of Fairness Metrics in Machine Learning\u201d. In: (2021). DOI: 10.21203/RS.3.RS-1162350/V1. URL: https://www.researchsquare.com%20https://www.researchsquare.com/article/rs-1162350/\n[4] Jiahao Chen et al. \u201cFairness under unawareness: Assessing disparity when protected class is unobserved\u201d. In: FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency (2019). DOI: 10.1145/3287560.3287594. URL: https://dl.acm.org/doi/10.1145/3287560.3287594. [5] Zhenpeng Chen et al. \u201cA Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers\u201d. In: ACM Transactions on Software Engineering and Methodology (2022). DOI: 10.1145/3583561. URL: https://arxiv.org/abs/2207.03277v3. [6] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. \u201cWho Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem\u201d. In: ACM International Conference Proceeding Series (2022). DOI: 10.1145/3531146.3533213. URL: https://dl.acm.org/doi/10.1145/3531146.3533213. [7] David De Cremer and Garry Kasparov. \u201cThe ethical AI\u2014paradox: why better technology needs more and not less human responsibility\u201d. In: AI and Ethics 2021 2:1 (2021). DOI: 10.1007/S43681-021-00075-Y. URL: https://link.springer.com/article/10.1007/s43681-021-00075-y. [8] Padraig Cunningham and Sarah Jane Delany. \u201cUnderestimation Bias and Underfitting in Machine Learning\u201d. In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) (2020). DOI: 10.1007/978-3-030-73959-1_2. URL: http://arxiv.org/abs/2005.09052%20http://dx.doi.org/10.1007/978-3-030-73959-1_2. [9] Runshan Fu, Yan Huang, and Param Vir Singh. \u201cArtificial Intelligence and Algorithmic Bias: Source, Detection, Mitigation, and Implications\u201d. In: INFORMS Tutorials in Operations Research (2020). DOI: 10.1287/EDUC.2020.0215. URL: https://pubsonline.informs.org/doi/abs/10.1287/educ.2020.0215. [10] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. \u201cAutomated Machine Learning\u201d. In: (2019). DOI: 10.1007/978-3-030-05318-5. URL: http://link.springer.com/10.1007/978-3-030-05318-5. [11] Nathan Kallus, Xiaojie Mao, and Angela Zhou. \u201cAssessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination\u201d. In: (2021). DOI: 10.1287/MNSC.2020.3850. URL: https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2020.3850. [12] Alexander Kriebitz and Christoph L\u00fctge. \u201cArtificial Intelligence and Human Rights: A Business Ethical Assessment\u201d. In: Business and Human Rights Journal (2020). DOI: 10.1017/BHJ.2019.28. URL: https://www.cambridge.org/core/journals/business-and-human-rights-journal/articl [13] Mingchen Li et al. \u201cAutoBalance: Optimized Loss Functions for Imbalanced Data\u201d. In: Advances in Neural Information Processing Systems (2022). URL: https://arxiv.org/abs/2201.01212v1. [14] Emmanuel Martinez and Lauren Kirchner. \u201cThe Secret Bias Hidden in Mortgage-Approval Algorithms \u2013 The Markup\u201d. In: ACM International Conference Proceeding Series (2021). URL: https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-appro [15] Ninareh Mehrabi et al. \u201cA Survey on Bias and Fairness in Machine Learning\u201d. In: ACM Computing Surveys (CSUR) (2021). DOI: 10.1145/3457607. URL: https://dl.acm.org/doi/10.1145/3457607. [16] \u201cMitigating Bias in Radiology Machine Learning: 2. Model Development\u201d. In: Radiology: Artificial Intelligence 4 (2022). DOI: 10.1148/RYAI.220010/ASSET/IMAGES/LARGE/RYAI.220010.FIG4.JPEG. URL: https://pubs.rsna.org/doi/10.1148/ryai.220010. [17] S\u00e9rgio Moro, Paulo Cortez, and Paulo Rita. \u201cA data-driven approach to predict the success of bank telemarketing\u201d. In: Decision Support Systems (2014). DOI: 10.1016/J.DSS.2014.03.001. [18] Debarghya Mukherjee et al. \u201cTwo Simple Ways to Learn Individual Fairness Metrics from Data\u201d. In: (2020). URL: https://proceedings.mlr.press/v119/mukherjee20a.html.\n[19] Eirini Ntoutsi et al. \u201cBias in data-driven artificial intelligence systems\u2014An introductory survey\u201d. In: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery (2020). DOI: 10.1002/WIDM.1356. URL: https://onlinelibrary.wiley.com/doi/full/10.1002/widm.1356. [20] Tai Le Quy et al. \u201cA survey on datasets for fairness-aware machine learning\u201d. In: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery (2022). DOI: 10.1002/WIDM.1452. URL: https://onlinelibrary.wiley.com/doi/full/10.1002/widm.1452. [21] Pedro Saleiro et al. \u201cAequitas: A Bias and Fairness Audit Toolkit\u201d. In: (2018). URL: https://arxiv.org/abs/1811.05577v2. [22] TrewinShari et al. \u201cConsiderations for AI fairness for people with disabilities\u201d. In: AI Matters (2019). DOI: 10.1145/3362077.3362086. URL: https://dl.acm.org/doi/10.1145/3362077.3362086. [23] Michael Veale, Max Van Kleek, and Reuben Binns. \u201cFairness and accountability design needs for algorithmic support in high-stakes public sector decision-making\u201d. In: Conference on Human Factors in Computing Systems - Proceedings (2018). DOI: 10.1145/3173574.3174014. URL: https://dl.acm.org/doi/10.1145/3173574.3174014. [24] Qingyun Wu and Chi Wang. \u201cFAIRAUTOML: EMBRACING UNFAIRNESS MITIGATION IN AUTOML\u201d. In: (). [25] Yukun Zhang and Longsheng Zhou. \u201cFairness Assessment for Artificial Intelligence in Financial Industry\u201d. In: (2019). URL: https://arxiv.org/abs/1912.07211v1.\nBibliography management: biblatex package ShareLATEX\n# 1 First section\n1 First section [biggio2018wild]\n# 1 First section [biggio2018wild]\nThis figure \"vanet_emergency.jpg\" is available in \"jpg\"\ufffd format from: http://arxiv.org/ps/2312.12460v1\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The increasing reliance on AI in various sectors has raised concerns regarding fairness and bias in machine learning, particularly in Automated Machine Learning (AutoML) tools that are accessible to non-experts. Historical instances of AI perpetuating discrimination highlight the necessity for fairness-aware features in these tools.",
            "purpose of benchmark": "The benchmark aims to evaluate the fairness-support capabilities of user-interface-based open source AutoML tools, focusing on their ability to assist non-expert users in developing fairness-aware machine learning models."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of ensuring fairness in machine learning models developed using AutoML tools, particularly for users lacking expertise in identifying and mitigating biases.",
            "key obstacle": "Existing benchmarks often do not adequately evaluate the fairness features of AutoML tools, leaving a gap in understanding how these tools can support fairness in model development."
        },
        "idea": {
            "intuition": "The development of this benchmark was inspired by the observation that non-expert users of AutoML tools are at risk of creating biased models due to a lack of fairness-aware features in these tools.",
            "opinion": "The authors emphasize the critical need for fairness features in AutoML tools to prevent the unintentional perpetuation of bias and discrimination in AI applications.",
            "innovation": "This benchmark introduces a novel evaluation framework specifically designed to assess the fairness capabilities of AutoML tools, differing from previous benchmarks that may not focus on user-interface-based tools.",
            "benchmark abbreviation": "FAIR-AUTOML"
        },
        "dataset": {
            "source": "The dataset was sourced from real-world data related to credit default occurrences among customers of a banking institution in Portugal.",
            "desc": "The dataset includes demographic variables and credit default information, providing a context for evaluating fairness in model predictions.",
            "content": "The dataset contains structured data, including demographic attributes and binary outcomes related to credit default, which are essential for the benchmark.",
            "size": "1,000",
            "domain": "Credit Scoring",
            "task format": "Classification"
        },
        "metrics": {
            "metric name": "Demographic Parity, Equalized Odds",
            "aspect": "The metrics focus on evaluating the fairness of model predictions across different demographic groups.",
            "principle": "The choice of metrics is guided by the need to ensure that the model's predictions do not disproportionately favor or disadvantage any particular group.",
            "procedure": "Model performance is evaluated using the selected fairness metrics, comparing the outcomes for different demographic groups to assess equity."
        },
        "experiments": {
            "model": "The models tested include RapidMiner Studio, H2O Studio, DataRobot, and Dataiku, which are prominent user-interface-based AutoML tools.",
            "procedure": "The experimental setup involved installing the tools, introducing the sample dataset, and evaluating the fairness features of each tool based on predefined criteria.",
            "result": "The study revealed significant deficiencies in the fairness-support features of all evaluated tools, indicating a lack of adequate support for developing fairness-aware models.",
            "variability": "Variability in results was accounted for by conducting multiple trials and assessing the tools against diverse data handling capabilities."
        },
        "conclusion": "The findings underscore the urgent need for AutoML tools to integrate comprehensive fairness-aware features, as the current tools lack sufficient capabilities to support non-expert users in developing equitable machine learning models.",
        "discussion": {
            "advantage": "The benchmark highlights the strengths of the evaluated AutoML tools in data processing and feature relevance, but it also reveals critical gaps in fairness support.",
            "limitation": "The primary limitation is that none of the tools provide robust features for developing fairness-aware models, which could lead to biased outcomes.",
            "future work": "Future research should focus on enhancing AutoML tools with fairness features, such as bias detection mechanisms and user-guided fairness strategies, to promote responsible AI development."
        },
        "other info": {
            "info1": "The study emphasizes the importance of integrating ethical considerations into the design and functionality of AutoML tools.",
            "info2": {
                "info2.1": "The benchmark serves as a guideline for developers to improve fairness in AutoML tools.",
                "info2.2": "The research calls for collaboration among stakeholders to establish standards for fairness in automated machine learning."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The increasing reliance on AI in various sectors has raised concerns regarding fairness and bias in machine learning, particularly in Automated Machine Learning (AutoML) tools that are accessible to non-experts."
        },
        {
            "section number": "1.2",
            "key information": "The authors emphasize the critical need for fairness features in AutoML tools to prevent the unintentional perpetuation of bias and discrimination in AI applications."
        },
        {
            "section number": "1.3",
            "key information": "The benchmark aims to evaluate the fairness-support capabilities of user-interface-based open source AutoML tools, focusing on their ability to assist non-expert users in developing fairness-aware machine learning models."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the challenge of ensuring fairness in machine learning models developed using AutoML tools, particularly for users lacking expertise in identifying and mitigating biases."
        },
        {
            "section number": "4.1",
            "key information": "The findings underscore the urgent need for AutoML tools to integrate comprehensive fairness-aware features, as the current tools lack sufficient capabilities to support non-expert users in developing equitable machine learning models."
        },
        {
            "section number": "7.1",
            "key information": "The primary limitation is that none of the tools provide robust features for developing fairness-aware models, which could lead to biased outcomes."
        },
        {
            "section number": "7.4",
            "key information": "Future research should focus on enhancing AutoML tools with fairness features, such as bias detection mechanisms and user-guided fairness strategies, to promote responsible AI development."
        }
    ],
    "similarity_score": 0.5926646369657215,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Democratize with Care_ The need for fairness specific features in user-interface based open source AutoML tools.json"
}