# A Survey of Large Language Models in Medicine: Progress, Application, and Challenge  

Hongjian Zhou1,\*, Fenglin Liu1,\*,† Boyang $\mathbf{Gu}^{2,\star}$ , Xinyu ${\bf Z o u^{3,\star}}$ , Jinfa Huang4,\*, Jinge $\mathbb{W}\mathbf{u}^{5,\star}$ , Yiru $L^{j}$ , Sam Chen7, Peilin Zhou8, Junling Liu9, Yining Hua10, Chengfeng Mao11, Chenyu You12,Xian $\mathbb{W}\mathbf{u}^{13}$ ,Yefeng Zheng13,Lei Clifton1, Zheng $\mathsf{L i^{14,\dagger}}$ , Jiebo Luo4,†, David Clifton1,15,†  

\* Core Contributors, ordered by a coin toss. $^{\dagger}$ Corresponding Authors.   
1University of Oxford, 2Imperial College London, 3University of Waterloo,   
4University of Rochester, 5University College London, 6Western University,   
7University of Georgia, 8Hong Kong University of Science and Technology (Guangzhou),   
9Alibaba, 10Harvard T.H. Chan School of Public Health, 11Massachusetts Institute of Technology,   
12Yale University, 13Tencent, 14Amazon, $^{15}\mathrm{O}!$ xford-Suzhou Centre for Advanced Research   
{fenglin.liu,david.clifton}@eng.ox.ac.uk, amzzhe@amazon.com, jluo@cs.rochester.edu  

# ABSTRACT  

Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a comprehensive review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specifc needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide a clear understanding of the distinct advantages and limitations of LLMs in medicine. Overall, in this review, we address the following study questions: 1) What are the practices for developing medical LLMs? 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities and challenges of LLMs in medicine and serve as a practical resource for constructing effective medical LLMs. We also maintain a regularly updated list of practical guides on medical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide.  

# BOX: Key points  

• Existing medical LLMs, ranging from 110 million to 520 billion parameters, are mainly developed through pre-training, fne-tuning, and prompting methods, utilizing large-scale medical corpora from diverse sources.   
• Most existing works evaluate their performance on exam-style QA tasks. A reasonable combination of different fne-tuning and prompting methods enables LLMs to achieve comparable or even better results than experts.   
• LLMs’ poor performance in non-QA tasks without pre-set options, which are common in clinical practice, indicates a considerable need for advancement before LLMs can be integrated into the actual clinical decision-making process.   
• Adapting medical LLMs to various clinical applications has received increasing research interest. However, large-scale clinical trials specifcally targeting existing medical LLMs are currently missing.   
• Addressing the challenges and future directions—regarding mitigating hallucinations; establishing robust data, benchmarks, metrics; and addressing ethical, safety, and regulatory concerns, through interdisciplinary collaborations—is important to accelerate the integration of LLMs into the clinic.  

![](images/8e13a0b8327ca2744520c97e585f57cbbd1fe6911cf9e78713616377a1ba51f8.jpg)  
Figure 1. An overview of the practical guides for medical large language models.  

# 1 Introduction  

The recently emerged general large language models (LLMs)1,2, such as $\mathrm{PaLM}^{3}$ , LLaMA4,5, GPT-series6,7, and ChatGLM8,9, have advanced the state-of-the-art in various natural language processing (NLP) tasks, including text generation, text summarization, and question answering. Inspired by these successes, several endeavors have been made to adapt general LLMs to the medicine domain, leading to the emergence of medical LLMs10,11. For example, based on $\mathrm{PaLM}^{3}$ and GPT- $.4^{7}$ , MedPaLM- $\cdot2^{11}$ and MedPrompt12 have respectively achieved a competitive accuracy of 86.5 and 90.2 compared to human experts $(87.0^{13})$ in the United States Medical Licensing Examination (USMLE)14. In particular, based on publicly available general LLMs (e.g. LLaMA4,5), a wide range of medical LLMs, including ChatDoctor15, MedAlpaca16, PMC-LLaMA13, BenTsao17, and Clinical Camel18, have been introduced. As a result, medical LLMs have gained growing research interests in assisting medical professionals to improve patient care 19,20.  

Although existing medical LLMs have achieved promising results, there are some key issues in their development and application that need to be addressed. First, many of these models primarily focus on medical dialogue and medical questionanswering tasks, but their practical utility in clinical practice is often overlooked 19. Recent research and reviews 19,21,22 have begun to explore the potential of medical LLMs in different clinical scenarios, including Electronic Health Records (EHRs)23, discharge summary generation 20, health education 24, and care planning 11. However, they primarily focus on presenting clinical applications of LLMs, especially online commercial LLMs like ChatGPT (including GPT-3.5 and GPT- $.4^{7}$ ), without providing practical guidelines for the development of medical LLMs. Besides, they mainly perform case studies to conduct the human evaluation on a small number of samples, thus lacking evaluation datasets for assessing model performance in clinical scenarios. Second, most existing medical LLMs report their performances mainly on answering medical questions, neglecting other biomedical domains, such as medical language understanding and generation. These research gaps motivate this review which offers a comprehensive review of the development of LLMs and their applications in medicine. We aim to cover topics on existing medical LLMs, various medical tasks, clinical applications, and arising challenges.  

As shown in Figure 1, this review seeks to answer the following questions. Section 2: What are LLMs? How can medical LLMs be effectively built? Section 3: How are the current medical LLMs evaluated? What capabilities do medical LLMs offer beyond traditional models? Section 4: How should medical LLMs be applied in clinical settings? Section 5: What challenges should be addressed when implementing medical LLMs in clinical practice? Section 6: How can we optimize the construction of medical LLMs to enhance their applicability in clinical settings, ultimately contributing to medicine and creating a positive societal impact?  

For the frst question, we analyze the foundational principles underpinning current medical LLMs, providing detailed descriptions of their architecture, parameter scales, and the datasets used during their development. This exposition aims to serve as a valuable resource for researchers and clinicians designing medical LLMs tailored to specifc requirements, such as computational constraints, data privacy concerns, and the integration of local knowledge bases. For the second question, we evaluate the performance of medical LLMs across ten biomedical NLP tasks, encompassing both discriminative and generative tasks. This comparative analysis elucidates how these models outperform traditional AI approaches, offering insights into the specifc capabilities that render LLMs effective in clinical environments. The third question, the practical deployment of medical LLMs in clinical settings, is explored through the development of guidelines tailored for seven distinct clinical application scenarios. This section outlines practical implementations, emphasizing specifc functionalities of medical LLMs that are leveraged in each scenario. The fourth question emphasizes addressing the challenges associated with the clinical deployment of medical LLMs, such as the risk of generating factually inaccurate yet plausible outputs (hallucination), and the ethical, legal, and safety implications. Citing recent studies, we argue for a comprehensive evaluation framework that assesses the trustworthiness of medical LLMs to ensure their responsible and effective utilization in healthcare. For the last question, we propose future research directions to advance the medical LLMs feld. This includes fostering interdisciplinary collaboration between AI specialists and medical professionals, advocating for a ’doctor-in-the-loop’ approach, and emphasizing human-centered design principles.  

By establishing robust training data, benchmarks, metrics, and deployment strategies through co-development efforts, we aim to accelerate the responsible and effcacious integration of medical LLMs into clinical practice. This study therefore seeks to stimulate continued research and development in this interdisciplinary feld, with the objective of realizing the profound potential of medical LLMs in enhancing clinical practice and advancing medical science for the betterment of society.  

# BOX 1: Background of Large Language Models (LLMs)  

The impressive performance of LLMs can be attributed to Transformer-based language models, large-scale pre-training, and scaling laws.  

Language Models A language model 25,26,27 is a probabilistic model that models the joint probability distribution of tokens (meaningful units of text, such as words or subwords or morphemes) in a sequence, i.e., the probabilities of how words and phrases are used in sequences. Therefore, it can predict the likelihood of a sequence of tokens given the previous tokens, which can be used to predict the next token in a sequence or to generate new sequences.  

The Transformer architecture The recurrent neural network (RNN) 28,26 has been widely used for language modeling by processing tokens sequentially and maintaining a vector named hidden state that encodes the context of previous tokens. Nonetheless, sequential processing makes it unsuitable for parallel training and limits its ability to capture long-range dependencies, making it computationally expensive and hindering its learning ability for long sequences. The strength of the Transformer 29 lies in its fully attentive mechanism, which relies exclusively on the attention mechanism and eliminates the need for recurrence. When processing each token, the attention mechanism computes a weighted sum of the other input tokens, where the weights are determined by the relevance between each input token and the current token. It allows the model to adaptively focus on different parts of the sequence to effectively learn the joint probability distribution of tokens. Therefore, Transformer not only enables effcient modeling of long-text but also allows highly paralleled training 30, thus reducing training costs. They make the Transformer highly scalable, and therefore it is effcient to obtain LLMs through the large-scale pre-training strategy.  

Large-scale Pre-training The LLMs are trained on massive corpora of unlabeled texts (e.g., CommonCrawl, Wiki, and Books) to learn rich linguistic knowledge and language patterns. The common training objectives are masked language modeling (MLM) and next token prediction (NTP). In MLM, a portion of the input text is masked, and the model is tasked with predicting the masked text based on the remaining unmasked context, encouraging the model to capture the semantic and syntactic relationships between tokens 30; NTP is another common training objective, where the model is required to predict the next token in a sequence given the previous tokens. It helps the model to predict the next token 6.  

Scaling Laws LLMs are the scaled-up versions of Transformer architecture 29 with increased numbers of Transformer layers, model parameters, and volume of pre-training data. The “scaling laws” 31,32 predict how much improvement can be expected in a  

model’s performance as its size increases (in terms of parameters, layers, data, or the amount of training computed). The scaling laws proposed by OpenAI 31 show that to achieve optimal model performance, the budget allocation for model size should be larger than the data.  

The scaling laws proposed by Google DeepMind 32 show that both model and data sizes should be increased in equal scales. The scaling laws guide researchers to allocate resources and anticipate the benefts of scaling models.  

General Large Language Models Existing general LLMs can be divided into three categories based on their architecture (Table 1).  

Encoder-only LLMs consisting of a stack of Transformer encoder layers, employ a bidirectional training strategy that allows them to integrate context from both the left and the right of a given token in the input sequence. This bi-directionality enables the models to achieve a deep understanding of the input sentences 30. Therefore, encoder-only LLMs are particularly suitable for language understanding tasks (e.g., sentiment analysis document classifcation) where the full context of the input is essential for accurate predictions. BERT 30 and DeBERTa 33 are the representative encoder-only LLMs.  

Decoder-only LLMs utilize a stack of Transformer decoder layers and are characterized by their uni-directional (left-to-right) processing of text, enabling them to generate language sequentially. This architecture is trained unidirectionally using the next token prediction training objective to predict the next token in a sequence, given all the previous tokens. After training, the decoder-only LLMs generate sequences autoregressively (i.e. token-by-token). The examples are the GPT-series developed by OpenAI 6,7, the LLaMA-series developed by Meta 4,5, and the $\mathrm{PaLM}^{3}$ and Bard (Gemini) 34 developed by Google. Based on the LLaMA model, Alpaca 35 is fne-tuned with 52k self-instructed data supervision. In addition, Baichuan 36 is trained on approximately 1.2 trillion tokens that support bilingual communication in Chinese and English. These LLMs have been used successfully in language generation.  

Encoder-decoder LLMs are designed to simultaneously process input sequences and generate output sequences. They consist of a stack of bidirectional Transformer encoder layers followed by a stack of unidirectional Transformer decoder layers. The encoder processes and understands the input sequences, while the decoder generates the output sequences 8,9,37. Representative examples of encoder-decoder LLMs include Flan-T5 38, and ChatGLM 8,9. Specifcally, ChatGLM 8,9 has 6.2B parameters and is a conversational open-source LLM specially optimized for Chinese to support Chinese-English bilingual question-answering.  

Table 1. Summary of existing general (large) language models, their underlying structures, numbers of parameters, and datasets used for model training. Column “# params” shows the number of parameters, M: million, B: billion.   


<html><body><table><tr><td>Domains</td><td>Model Structures</td><td>Models</td><td>#Params</td><td>Pre-trainData Scale</td></tr><tr><td rowspan="2">General-domain</td><td>Encoder-only</td><td>BERT30 RoBERTa 39 DeBERTa 33 GPT-240 Vicuna 41 Alpaca 35</td><td>110M/340M 355M 1.5B 1.5B 7B/13B</td><td>3.3B tokens 161GB 160GB 40GB LLaMA+70Kdialogues</td></tr><tr><td>Decoder-only (Large) Language Models</td><td>Mistral 42 LLaMA 4 LLaMA-25 LLaMA-343 GPT-36 Qwen 44 PaLM3 FLAN-PaLM37 Gemini (Bard) 34 GPT-3.545 GPT-47 Claude-346</td><td>7B/13B 7B 7B/13B/33B/65B 7B/13B/34B/70B 8B/70B 6.7B/13B/175B 1.8B/7B/14B/72B 8B/62B/540B 540B</td><td>LLaMA+52KIFT 1.4T tokens 2T tokens 15T tokens 300B tokens 3T tokens 780Btokens</td></tr><tr><td></td><td>Encoder-Decoder</td><td>BART 47 ChatGLM 8.9 T538 FLAN-T537 UL248 GLM9</td><td>140M/400M 6.2B 11B 3B/11B 19.5B 130B</td><td>160GB 1T tokens 1T tokens 780B tokens 1T tokens 400B tokens</td></tr></table></body></html>  

# 2 The Principles of Medical Large Language Models  

Box 1 and Table 1 briefy introduce the background of general LLMs1, e.g., GPT- $.4^{7}$ . Table 2 summarizes the currently available medical LLMs according to their model development. Existing medical LLMs are mainly pre-trained from scratch, fne-tuned from existing general LLMs, or directly obtained through prompting to align the general LLMs to the medical domain. Therefore, we introduce the principles of medical LLMs in terms of these three methods: pre-training, fne-tuning, and prompting. Meanwhile, we further summarize the medical LLMs according to their model architectures in Figure 2.  

Table 2. Summary of existing medical-domain LLMs, in terms of their model development, the number of parameters (# params), the scale of pre-training/fne-tuning data, and the data source. M: million, B: billion.   


<html><body><table><tr><td rowspan="16">Domains</td><td>Model Development Models</td><td>BioBERT49</td><td># Params 110M</td><td>Data Scale</td><td>Data Source</td></tr><tr><td rowspan="16">Pre-training (Sec. 2.1)</td><td>PubMedBERT52</td><td>110M/340M</td><td>18B tokens 3.2B tokens</td><td>PubMed 50+PMC51 PubMed 50+PMC51</td></tr><tr><td>SciBERT53</td><td>110M</td><td>3.17B tokens</td><td>Literature 54</td></tr><tr><td></td><td>110M</td><td></td><td></td></tr><tr><td>NYUTron ss ClinicalBERT56</td><td>110M</td><td>7.25M notes, 4.1B tokens</td><td>NYU Notes55 MIMIC-I157</td></tr><tr><td>BioM-ELECTRA5s</td><td>110M/335M</td><td>112k clinical notes</td><td></td></tr><tr><td></td><td></td><td></td><td>PubMed 50</td></tr><tr><td>BioMed-RoBERTa59</td><td>125M</td><td>7.55B tokens</td><td>S2ORC60</td></tr><tr><td>BioLinkBERT61</td><td>110M/340M</td><td>21GB >4.5B tokens</td><td>PubMed 50</td></tr><tr><td>BlueBERT62.63.64</td><td>110M/340M</td><td></td><td>PubMed 50+MIMIC-III57</td></tr><tr><td>SciFive 65</td><td>220M/770M</td><td></td><td>PubMed 50+PMC 51</td></tr><tr><td rowspan="20">Medical-domain (Sec. 2) LLMs</td><td>ClinicalT5 66</td><td>220M/770M 330M</td><td>2M clinical notes</td><td>MIMIC-I 57</td></tr><tr><td>MedCPT67</td><td></td><td>255M articles</td><td>PubMed 50</td></tr><tr><td>DRAGON68</td><td>360M</td><td>6GB</td><td>BookCorpus 69</td></tr><tr><td>BioGPT70</td><td>1.5B</td><td>15M articles</td><td>PubMed 50</td></tr><tr><td>BioMedLM71</td><td>2.7B</td><td>110GB</td><td>Pile 72</td></tr><tr><td>OphGLM73</td><td>6.2B</td><td>20k dialogues</td><td>MedDialog 74</td></tr><tr><td>GatorTron 23</td><td>8.9B</td><td>>82Btokens+6Btokens 2.5B tokens+0.5B tokens</td><td>EHRs 23+PubMed 50 Wiki+MIMIC-II7</td></tr><tr><td>GatorTronGPT75</td><td>5B/20B</td><td>277B tokens</td><td>EHRs 75</td></tr><tr><td>DoctorGLM76</td><td>6.2B</td><td>323MB dialogues</td><td>CMD.77</td></tr><tr><td>BianQue 78</td><td>6.2B</td><td>2.4M dialogues</td><td>BianQueCorpus 78</td></tr><tr><td>ClinicalGPT 79</td><td>7B</td><td>96k EHRs + 100k dialogues 192medical QA</td><td>MD-EHR 79 +MedDialog 74 VariousMedQA14</td></tr><tr><td>Qilin-Med 8</td><td>7B</td><td>3GB</td><td>ChiMed 80</td></tr><tr><td>ChatDoctor 's</td><td>7B</td><td>110k dialogues</td><td>HealthCareMagic *1 +iCliniq α2</td></tr><tr><td>BenTsao 17 HuatuoGPT 84</td><td>7B</td><td>8k instructions</td><td>CMeKG-8K83</td></tr><tr><td>Baize-healthcare &5</td><td>7B</td><td>226k instructions&dialogues</td><td>Hybrid SFT &4</td></tr><tr><td></td><td>7B</td><td>101K dialogues</td><td>Quora+MedQuAD86</td><td></td></tr><tr><td></td><td>10B 7B/13B</td><td>>26B tokens</td><td></td><td>S2ORC60</td></tr><tr><td></td><td>BioMedGPT87</td><td></td><td>160k medical QA</td><td>Medical Meadow 16</td></tr><tr><td></td><td>MedAlpaca 16 AlpaCare 8</td><td></td><td>52k instructions</td><td>MedInstruct-52k88</td></tr><tr><td></td><td>Zhongjing 9</td><td>7B/13B 13B</td><td>70k dialogues</td><td>CMtMedQA89</td></tr><tr><td></td><td>PMC-LLaMA 13</td><td>13B</td><td>79.2B tokens</td><td>Books+Literature 60+MedC-I 13</td></tr><tr><td>Fine-tuning (Sec.2.2)</td><td></td><td></td><td>109k EHRs</td><td>eICU-CRD91+MIMIC-IV92</td></tr><tr><td></td><td>CPLLM %</td><td>13B</td><td></td><td></td></tr><tr><td></td><td>Med42 93</td><td>7B/70B</td><td>250M tokens</td><td>PubMed 50 + MedQA 14 + OpenOrca</td></tr><tr><td></td><td>MEDITRON 94.95</td><td>7B/70B</td><td>48.1B tokens</td><td>PubMed 50+Guidelines 94</td></tr><tr><td></td><td>OpenBioLLM% MedLlama3-v20 97</td><td>8B/70B</td><td></td><td></td></tr><tr><td></td><td>Clinical CameI I&</td><td>8B/70B</td><td>70k dialogues+100k articles</td><td>ShareGPT 98+PubMed 50</td></tr><tr><td></td><td>MedPaLM-2 11</td><td>13B/70B</td><td>4k medical QA</td><td>MedQA4</td></tr><tr><td></td><td>Med-Flamingo 9</td><td>340B</td><td>193k medical QA</td><td>MultiMedQA 11 Multimodal Textbook+PMC-OA 99</td></tr><tr><td></td><td></td><td></td><td>600k pairs</td><td>VQA-RAD 100 +PathVQA 101 PMC-15M102+VQA-RAD 100</td></tr><tr><td></td><td>LLaVA-Med 102</td><td></td><td>660k pairs</td><td>SLAKE 103+PathVQA 101</td></tr><tr><td></td><td>MAIRA-1 104 RadFM 106</td><td></td><td>337k pairs 32M pairs</td><td>MIMIC-CXR105 MedMD 106</td></tr><tr><td></td><td>Med-Gemini 107.108</td><td></td><td></td><td>MedQA-R&RS10++MultiMedQA1 +MIMIC-II157+MultiMedBench 109</td></tr><tr><td></td><td>CodeX110</td><td>GPT-3.5 / LLaMA-2 Chain-of-Thought (CoT) 11</td><td></td><td></td></tr><tr><td></td><td>DeID-GPT112</td><td>ChatGPT / GPT-4</td><td>Chain-of-Thought (CoT) I1</td><td></td></tr><tr><td rowspan="8">Prompting (Sec.2.3)</td><td>ChatCADI13</td><td>ChatGPT</td><td>In-Context Learning (ICL)</td><td></td></tr><tr><td>Dr. Knows114</td><td>ChatGPT</td><td>ICL</td><td>UMLS 115</td></tr><tr><td>MedPaLM 10</td><td>PaLM (540B)</td><td>CoT & ICL</td><td>MultiMedQA1</td></tr><tr><td>MedPrompt 12</td><td>GPT-4</td><td>CoT& ICL I11</td><td></td></tr><tr><td>Chat-Orthopedist I16</td><td>ChatGPT</td><td></td><td></td></tr><tr><td>QA-RAG 120</td><td></td><td></td><td></td></tr><tr><td></td><td>ChatGPT</td><td>RAG</td><td>FDA QA 120</td></tr><tr><td>Almanac 121</td><td>ChatGPT</td><td>RAG & CoT</td><td>Clinical QA 121</td></tr><tr><td>Oncology-GPT-493</td><td>GPT-4</td><td>RAG & ICL</td><td>Oncology Guidelines from ASCO and ESMO</td></tr></table></body></html>  

# 2.1 Pre-training  

Pre-training typically involves training an LLM on a large corpus of medical texts, including both structured and unstructured text, to learn the rich medical knowledge. The corpus may include $\mathrm{EHRs}^{75}$ , clinical notes 23, and medical literature 56. In particular, PubMed50, MIMIC-III clinical notes57, and PubMed Central (PMC) literature51, are three widely used medical corpora for medical LLM pre-training. A single corpus or a combination of corpora may be used for pre-training. For example, PubMedBERT52 and ClinicalBERT are pre-trained on PubMed and MIMIC-III, respectively. In contrast, BlueBERT62 combines both corpora for pre-training; BioBERT49 is pre-trained on both PubMed and PMC. The University of Florida (UF) Health EHRs are further introduced in pre-training GatorTron23 and GatorTronGPT75. MEDITRON94 is pre-trained on Clinical Practice Guidelines (CPGs). The CPGs are used to guide healthcare practitioners and patients in making evidence-based decisions about diagnosis, treatment, and management.  

To meet the needs of the medical domain, pre-training medical LLMs typically involve refning the following commonly used training objectives in general LLMs: masked language modeling, next sentence prediction, and next token prediction (Please see Box 1 for an introduction of these three pre-training objectives). For example. BERT-series models (e.g., BioBERT 49, PubMedBERT52, ClinicalBERT56, and GatorTron23) mainly adopt the masked language modeling and the next sentence prediction for pre-training; GPT-series models (e.g., BioGPT70, and GatorTronGPT75) mainly adopt the next token prediction for pre-training. It is worth mentioning that BERT-like Medical LLMs (e.g., BioBERT49, PubMedBERT52, Clinical BERT56) are originally derived from the general domain BERT or RoBERTa models. To clarify the differences between different models, in our Table 2, we only show the data source used to further construct medical LLMs. In particular, a more recent work122 provides a systematic review of existing clinical text datasets for LLMs. After pre-training, medical LLMs can learn rich medical knowledge that can be leveraged to achieve strong performance on different medical tasks.  

![](images/90e9acf546b32fa376a4d2ddf555eda7136bb76ea45e29e9aff01d4850607b48.jpg)  
Figure 2. We adopt the data from Table 2 to demonstrate the development of model sizes for medical large language models in different model architectures, i.e., BERT-like, ChatGLM/LLaMA-like, and GPT/PaLM-like.  

# 2.2 Fine-tuning  

It is high-cost and time-consuming to train a medical LLM from scratch, due to its requirement of substantial (e.g., several days or even weeks) computational power and manual labor. One solution is to fne-tune the general LLMs with medical data, and researchers have proposed different fne-tuning methods11,16,18 for learning domain-specifc medical knowledge and obtaining medical LLMs. Current fne-tuning methods include Supervised Fine-Tuning (SFT), Instruction Fine-Tuning (IFT), and Parameter-Effcient Fine-Tuning (PEFT). The resulting fne-tuned medical LLMs are summarized in Table 2. SFT can serve as continued pre-training to fne-tune general LLMs on existing (usually unlabeled) medical corpora. IFT focuses on fne-tuning general LLMs on instruction-based medical data containing additional (usually human-constructed) instructions.  

Supervised Fine-Tuning (SFT) aims to leverage high-quality medical corpus, which can be physician-patient conversations15, medical question-answering 16, and knowledge graphs 80,17. The constructed SFT data serves as a continuation of the pre-training data to further pre-train the general LLMs with the same training objectives, e.g., next token prediction. SFT provides an additional pre-training phase that allows the general LLMs to learn rich medical knowledge and align with the medical domain, thus transforming them into specialized medical LLMs.  

The diversity of SFT enables the development of diverse medical LLMs by training on different types of medical corpus. For example, DoctorGLM76 and ChatDoctor15 are obtained by fne-tuning the general LLMs ChatGLM8,9 and LLaMA4 on the physician-patient dialogue data, respectively. MedAlpaca16 based on the general LLM Alpaca35 is fne-tuned using over 160,000 medical QA pairs sourced from diverse medical corpora. Clinicalcamel18 combines physician-patient conversations, clinical literature, and medical QA pairs to refne the LLaMA-2 model5. In particular, Qilin-Med80 and Zhongjing89 are obtained by incorporating the knowledge graph to perform fne-tuning on the Baichuan36 and LLaMA4, respectively.  

In summary, existing studies have demonstrated the effcacy of SFT in adapting general LLMs to the medical domain. They show that SFT improves not only the model’s capability for understanding and generating medical text, but also its ability to provide accurate clinical decision support 123.  

Instruction Fine-Tuning (IFT) constructs instruction-based training datasets 124,123,1, which typically comprise instructioninput-output triples, e.g., instruction-question-answer. The primary goal of IFT is to enhance the model’s ability to follow various human/task instructions, align their outputs with the medical domain, and thereby produce a specialized medical LLM.  

Thus, the main difference between SFT and IFT is that the former focuses primarily on injecting medical knowledge into a general LLM through continued pre-training, thus improving its ability to understand the medical text and accurately predict the next token. In contrast, IFT aims to improve the model’s instruction following ability and adjust its outputs to match the given instructions, rather than accurately predicting the next token as in $S\mathrm{FT}^{124}$ . As a result, SFT emphasizes the quantity of training data, while IFT emphasizes their quality and diversity. Since IFT and SFT are both capable of improving model performance, there have been some recent works89,80,88 attempting to combine them for obtaining robust medical LLMs.  

In other words, to enhance the performance of LLMs through IFT, it is essential to ensure that the training data for IFT are of high quality and encompass a wide range of medical instructions and medical scenarios. To this end, MedPaLM- $\cdot2^{11}$ invited qualifed medical professionals to develop the instruction data for fne-tuning the general PaLM. BenTsao17 and ChatGLMMed125 constructed the knowledge-based instruction data from the knowledge graph. Zhongjing89 further incorporated the multi-turn dialogue as the instruction data to perform IFT. MedAlpaca16 simultaneously incorporated the medical dialogues and medical QA pairs for instruction fne-tuning.  

Recent advancements in multimodal LLMs have expanded the capabilities of LLMs to process complex and multimodal medical data. Notable examples include Med-Flamingo99, LLaVA-Med102, and Med-Gemini108. Med-Flamingo99 undergoes IFT on medical image-text data, learning to identify abnormalities and generate diagnostic reports. LLaVA-Med’s102 two-stage IFT process involves aligning medical concepts across visual and textual modalities, followed by fne-tuning the model on diverse medical instructions. Med-Gemini’s108 IFT utilizes a curated dataset of medical instructions and multimodal data, enabling it to comprehend complex medical concepts, procedures, and diagnostic reasoning. Meanwhile, MAIRA-1104 and RadFM106 are two multimodal LLMs specifcally designed for radiology applications. MAIRA-1104 employs IFT on a dataset of radiology instructions and corresponding medical images, enabling it to analyze radiological images and generate accurate diagnostic reports. RadFM106, on the other hand, leverages a pre-training approach on a large corpus of radiology-specifc image-text data, followed by instruction fne-tuning on a diverse set of radiology instructions. These models’ multimodal IFT approaches enable them to bridge the gap between visual and textual medical information, perform a wide range of medical tasks accurately, and generate context-aware responses to complex medical queries.  

Parameter-Effcient Fine-Tuning (PEFT) aims to substantially reduce computational and memory requirements for fne-tuning general LLMs. The main idea is to keep most of the parameters in pre-trained LLMs unchanged, by fne-tuning only the smallest subset of parameters (or additional parameters) in these LLMs. Commonly used PEFT techniques include Low-Rank Adaptation (LoRA)126, Prefx Tuning127, and Adapter Tuning128,129.  

In contrast to fne-tuning full-rank weight matrices, 1) LoRA preserves the parameters of the original LLMs and only adds trainable low-rank matrices into the self-attention module of each Transformer layer126. Therefore, LoRA can substantially reduce the number of trainable parameters and improve the effciency of fne-tuning, while still enabling the fne-tuned LLM to capture effectively the characteristics of the tasks. 2) Prefx Tuning takes a different approach from LoRA by adding a small set of continuous task-specifc vectors (i.e. “prefxes”) to the input of each Transformer layer127,1. These prefxes serve as the additional context to guide the generation of the model without changing the original pre-trained parameter weights. 3) Adapter Tuning involves introducing small neural network modules, known as adapters, into each Transformer layer of the pre-trained LLMs130. These adapters are fne-tuned while keeping the original model parameters frozen130, thus allowing for fexible and effcient fne-tuning. The number of trainable parameters introduced by adapters is relatively small, yet they enable the LLMs to adapt to clinical scenarios or tasks effectively.  

In general, PEFT is valuable for developing LLMs that meet unique needs in specifc (e.g., medical) domains, due to its ability to reduce computational demands while maintaining the model performance. For example, medical LLMs DoctorGLM76, MedAlpaca16, Baize-Healthcare85, Zhongjing89, CPLLM90, and Clinical Camel18 adopted the LoRA126 to perform parameter-effcient fne-tuning to effciently align the general LLMs to the medical domain.  

# 2.3 Prompting  

Fine-tuning considerably reduces computational costs compared to pre-training, but it requires further model training and collections of high-quality datasets for fne-tuning, thus still consuming some computational resources and manual labor. In contrast, the “prompting” methods effciently align general LLMs (e.g., $\mathrm{PaLM}^{3}$ ) to the medical domain (e.g., MedPaLM10), without training any model parameters. Popular prompting methods include In-Context Learning (ICL), Chain-of-Thought (CoT) prompting, Prompt Tuning, and Retrieval-Augmented Generation (RAG).  

In-Context Learning (ICL) aims to directly give instructions to prompt the LLM to perform a task effciently. In general, the ICL consists of four process: task understanding, context learning, knowledge reasoning, and answer generation. First, the model must understand the specifc requirements and goals of the task. Second, the model learns to understand the contextual information related to the task with argument context. Then, use the model’s internal knowledge and reasoning capabilities to understand the patterns and logic in the example. Finally, the model generates the task-related answers. The advantage of ICL is that it does not require a large amount of labeled data for fne-tuning. Based on the type and number of input examples, ICL can be divided into three categories131. 1) One-shot Prompting: Only one example and task description are allowed to be entered. 2) Few-shot Prompting: Allows the input of multiple instances and task descriptions. 3) Zero-shot Prompting: Only task descriptions are allowed to be entered. ICL presents the LLMs making task predictions based on contexts augmented with a few examples and task demonstrations. It allows the LLMs to learn from these examples or demonstrations to accurately perform the task and follow the given examples to give corresponding answers6. Therefore, ICL allows LLMs to accurately understand and respond to medical queries. For example, MedPaLM10 substantially improves the task performance by providing the general LLM, PaLM3, with a small number of task examples such as medical QA pairs.  

Chain-of-Thought (CoT) Prompting further improves the accuracy and logic of model output, compared with In-Context Learning. Specifcally, through prompting words, CoT aims to prompt the model to generate intermediate steps or paths of reasoning when dealing with downstream (complex) problems111. Moreover, CoT can be combined with few-shot prompting by giving reasoning examples, thus enabling medical LLMs to give reasoning processes when generating responses. For tasks involving complex reasoning, such as medical QA, CoT has been shown to effectively improve model performance10,11. Medical LLMs, such as DeID-GPT112, MedPaLM10, and MedPrompt12, use CoT prompting to assist them in simulating a diagnostic thought process, thus providing more transparent and interpretable predictions or diagnoses. In particular, MedPrompt12 directly prompts a general LLM, GPT- $.4^{7}$ , to outperform the fne-tuned medical LLMs on medical QA without training any model parameters.  

Prompt Tuning aims to improve the model performance by employing both prompting and fne-tuning techniques132,129. The prompt tuning method introduces learnable prompts, i.e. trainable continuous vectors, which can be optimized or adjusted during the fne-tuning process to better adapt to different medical scenarios and tasks. Therefore, they provide a more fexible way of prompting LLMs than the “prompting alone” methods that use discrete and fxed prompts, as described above. In contrast to traditional fne-tuning methods that train all model parameters, prompt tuning only tunes a very small set of parameters associated with the prompts themselves, instead of extensively training the model parameters. Thus, prompt tuning effectively and accurately responds to medical problems12, with minimal incurring computational cost.  

Existing medical LLMs that employ the prompting techniques are listed in Table 2. Recently, MedPaLM10 and MedPaLM$2^{11}$ propose to combine all the above prompting methods, resulting in Instruction Prompt Tuning, to achieve strong performances on various medical question-answering datasets. In particular, using the MedQA dataset for the US Medical Licensing Examination (USMLE), MedPaLM- $\cdot2^{11}$ achieves a competitive overall accuracy of $86.5\%$ compared to human experts $(87.0\%)$ surpassing previous state-of-the-art method MedPaLM10 by a large margin $(19\%)$ .  

Retrieval-Augmented Generation (RAG) enhances the performance of LLMs by integrating external knowledge into the generation process. In detail, RAG can be used to minimize LLM’s hallucinations, obscure reasoning processes, and reliance on outdated information by incorporating external database knowledge133. RAG consists of three main components: retrieval, augmentation, and generation. The retrieval component employs various indexing strategies and input query processing techniques to search and top-ranked relevant information from an external knowledge base. The retrieved external data is then augmented into the LLM’s prompt, providing additional context and grounding for the generated response. By directly updating the external knowledge base, RAG mitigates the risk of catastrophic forgetting associated with model weight modifcations, making it particularly suitable for domains with low error tolerance and rapidly evolving information, such as the medical feld. In contrast to traditional fne-tuning methods, RAG enables the timely incorporation of new medical information without compromising the model’s previously acquired knowledge, ensuring the generated outputs remain accurate and up-to-date in the face of evolving medical challenges. Most recently, researchers proposed the frst benchmark MIRAGE134 based on medical information RAG, including 7,663 questions from fve medical QA datasets, which has been established to both steer research and facilitate the practical deployment of medical RAG systems  

In RAG, retrieval can be achieved by calculating the similarity between the embeddings of the question and document chunks, where the semantic representation capability of embedding models plays a key role. Recent research has introduced prominent embedding models such as AngIE135, Voyage136, and BGE137. In addition to embedding, the retrieval process can be optimized via various strategies such as adaptive retrieval, recursive retrieval, and iterative retrieval 138,139,140. Several recent works have demonstrated the effectiveness of RAG in medical and pharmaceutical domains. Almanac121 is a large language framework augmented with retrieval capabilities for medical guidelines and treatment recommendations, surpassing the performance of ChatGPT on clinical scenario evaluations, particularly in terms of completeness and safety. Another work QA-RAG120 employs RAG with LLM for pharmaceutical regulatory tasks, where the model searches for relevant guideline documents and provides answers based on the retrieved guidelines. Chat-Orthopedist116, a retrieval-augmented LLM, assists adolescent idiopathic scoliosis (AIS) patients and their families in preparing for meaningful discussions with clinicians by providing accurate and comprehensible responses to patient inquiries, leveraging AIS domain knowledge.  

# 2.4 Discussion  

This section discusses the principles of medical LLMs, including three types of methods for building models: pre-training, fne-tuning, and prompting. To meet the needs of practical medical applications, users can choose proper medical LLMs according to the magnitude of their own computing resources. Companies or institutes with massive computing resources can either pre-train an application-level medical LLM from scratch or fne-tune existing open-source general LLM models (e.g., LLaMA43) using large-scale medical data. The results in existing literature (e.g., MedPaLM-211, MedAlpaca16 and Clinical Camel18) have shown that fne-tuning general LLMs on medical data122 can boost their performance of medical tasks. For example, Clinical Camel18, which is fne-tuned on the LLaMA-2-70B5 model, even outperforms GPT-418. However, for small enterprises or individuals with certain computing resources, combining with the understanding of medical tasks and a reasonable combination of ICL, prompting engineering, and RAG, to prompt black-box LLMs may also achieve strong results. For example, MedPrompt12 stimulates the commercial LLM GPT- $.4^{7}$ through an appropriate combination of prompt strategies to achieve comparable or even better results than fne-tuned medical LLMs (e.g., MedPaLM- $\cdot2^{11}$ ) and human experts, suggesting that a mix of prompting strategies is an effcient and green solution in the medical domain rather than fne-tuning.  

![](images/cc76202c3542e7b590f7ba7ae2e681e2285d0df0f212439b0050388b08422636.jpg)  
Figure 3. Performance (Dataset-Metric (Task)) comparison between the GPT-3.5 turbo, GPT-4, state-of-the-art task-specifc lightweight models (Fine-tuned), and human experts, on seven medical tasks across eleven datasets. All data presented in our Figures originates from published and peer-reviewed literature. Please refer to the supplementary material for the detailed data.  

# 3 Medical Tasks  

In this section, we will introduce two popular types of medical machine learning tasks: generative and discriminative tasks, including ten representative tasks that further build up clinical applications. Figure 3 illustrates the performance comparisons between different LLMs. For clarity, we will only cover a general discussion of those tasks. The detailed defnition of the task and the performance comparisons can be found in our supplementary material.  

# 3.1 Discriminative Tasks  

Discriminative tasks are for categorizing or differentiating data into specifc classes or categories based on given input data. They involve making distinctions between different types of data, often to categorize, classify, or extract relevant information from structured text or unstructured text. The representative tasks include Question Answering, Entity Extraction, Relation Extraction, Text Classifcation, Natural Language Inference, Semantic Textual Similarity, and Information Retrieval.  

The typical input for discriminative tasks can be medical questions, clinical notes, medical documents, research papers, and patient EHRs. The output can be labels, categories, extracted entities, relationships, or answers to specifc questions, which are often structured and categorized information derived from the input text. In existing LLMs, the discriminative tasks are widely studied and used to make predictions and extract information from input text.  

For example, based on medical knowledge, medical literature, or patient EHRs, the medical question answering (QA) task can provide precise answers to clinical questions, such as symptoms, treatment options, and drug interactions. This can help clinicians make more effcient and accurate diagnoses 10,11,19. Entity extraction can automatically identify and categorize critical information (i.e. entities) such as symptoms, medications, diseases, diagnoses, and lab results from patient EHRs, thus assisting in organizing and managing patient data. The following entity linking task aims to link the identifed entities in a structured knowledge base or a standardized terminology system, e.g., SNOMED CT141, UMLS115, or ICD codes142. This task is critical in clinical decision support or management systems, for better diagnosis, treatment planning, and patient care.  

![](images/b5b447b54343cc4c92af398bd379aaf03b6c7d7dec0f378ca2691ab5eac0d768.jpg)  
Figure 4. We demonstrate the development of medical large language models over time in different model development types through the scores of the United States Medical Licensing Examination (USMLE) from the MedQA dataset. Solid and dashed lines represent open-source and closed-source models, respectively.  

# 3.2 Generative Tasks  

Different from discriminative tasks that focus on understanding and categorizing the input text, generative tasks require a model to accurately generate fuent and appropriate new text based on given inputs. These tasks include medical text summarization143,144, medical text generation 70, and medical text simplifcation145.  

For medical text summarization, the input and output are typically long and detailed medical text (e.g., “Findings” in radiology reports), and a concise summarized text (e.g., the “Impression” in radiology reports). Such text contains important medical information that enables clinicians and patients to effciently capture the key points without going through the entire text. It can also help medical professionals to draft clinical notes by summarizing patient information or medical histories.  

In medical text generation, e.g., discharge instruction generation 146, the input can be medical conditions, symptoms, patient demographics, or even a set of medical notes or test results. The output can be a diagnosis recommendation of a medical condition, personalized instructional information, or health advice for the patient to manage their condition outside the hospital.  

Medical text simplifcation 145 aims to generate a simplifed version of the complex medical text by, for example, clarifying and explaining medical terms. Different from text summarization, which concentrates on giving shortened text while maintaining most of the original text meanings, text simplifcation focuses more on the readability part. In particular, complicated or opaque words will be replaced; complex syntactic structures will be improved; and rare concepts will be explained38. Thus, one example application is to generate easy-to-understand educational materials for patients from complex medical texts. It is useful for making medical information accessible to a general audience, without altering the essential meaning of the texts.  

# 3.3 Performance Comparisons  

Figure 3 shows that some existing general LLMs (e.g., GPT-3.5-turbo and GPT- $4^{7}$ ) have achieved strong performance on existing medical machine learning tasks. This is most noticeable for the QA task where GPT-4 (shown in the blue line in Figure 3) consistently outperforms existing task-specifc fne-tuned models and is even comparable to human experts (shown in the purple line). The QA datasets of evaluation include MedQA (USMLE)14, PubMedQA147, and MedMCQA148. To better understand the QA performance of existing medical LLMs, in Figure 4, we further demonstrate the QA performance of medical LLMs on the MedQA dataset over time in different model development types. It also clearly shows that current works, e.g., MedPrompt12 and Med-Gemini107,108, have successfully proposed several prompting and fne-tuning methods to enable LLMs to outperform human experts.  

However, on the non-QA discriminative tasks and generative tasks, as shown in Figure 3, the existing general LLMs perform worse than the task-specifc fne-tuned models. For example, for the non-QA discriminative tasks, the state-of-the-art task-specifc fne-tuned model BioBERT49 achieves an F1 score of 89.36, substantially exceeding the F1 score of 56.73 by  

![](images/cad762c3f2ec248a93add4cd97e095137b0a1a2673e5482ab8a2848a1330861e.jpg)  
Figure 5. Integrated overview of potential applications 114,150,151,152,153 of large language models in medicine.  

GPT-4, on the entity extraction task using the NCBI disease dataset149. For the generative tasks, we can see that the strong LLM GPT-4 clearly underperforms task-specifc lightweight models on all datasets. We hypothesize that the reason for the strong QA capability of the current general LLMs is that the QA task is close-ended; i.e. the correct answer is already provided by multiple candidates. In contrast, most non-QA tasks are open-ended where the model has to predict the correct answer from a large pool of possible candidates, or even without any candidates provided.  

Overall, the comparison proves that the current general LLMs have a strong question-answering capability, however, the capability on other tasks still needs to be improved. In detail, current LLMs are comparable to state-of-the-art models and human experts on the exam-style close-ended QA task with provided answer options. However, real-world open clinical practice usually involves answering open-ended questions without pre-set options and diverges far from the structured nature of exam-taking. The poor performance of LLMs in other non-QA tasks without options indicates a considerable need for advancement before LLMs can be integrated into the actual clinical decision-making process without answer options. Therefore, we advocate that the evaluation of medical LLMs should be extended to a broad range of tasks including non-QA tasks, instead of being limited mainly to medical QA tasks. Hereafter, we will discuss specifc clinical applications of LLMs, followed by their challenges and future directions.  

# 4 Clinical Applications  

Currently, most of existing medical LLMs are still in the research and development stage, with limited application and validation in real-world clinical scenarios. However, some initial attempts and explorations have begun to emerge. Researchers are also exploring the integration of large language models into clinical decision support systems to provide evidence-based recommendations and references155,55,154. Additionally, some research teams are developing tools based on large language models to assist in clinical trial recruitment by analyzing electronic health records to identify eligible participants 156. Some healthcare institutions are experimenting with using LLMs for clinical coding and formatting to improve effciency and accuracy in medical billing and reimbursement 161,162,163. Researchers are investigating the use of LLMs for clinical report generation, aiming to automate the process of creating coherent and accurate medical reports based on patient data113,104,106. LLMs are being integrated into medical robotics to enhance decision-making, collaboration, and diagnostic capabilities, improving surgical precision and effciency167,169,170. In the realm of medical language translation, efforts are being made to utilize LLMs to translate medical information into multiple languages for foreign patients 171,174,175 and to simplify complex medical terminology for lay people176,177, enhancing patient understanding and communication. In the feld of medical education, LLMs are being considered as tools to enhance learning experiences by providing personalized content, answering questions, and offering interactive educational materials 178,108. Certain organizations are testing the use of chatbots or virtual assistants to provide mental health support, aiming to increase the accessibility of mental health services 179,180,181. Furthermore, researchers are developing LLM-based systems for medical inquiry and response, aiming to provide accurate and timely answers to patients’ questions, triage inquiries, and assist healthcare professionals in addressing common concerns 189,190,191.  

Table 3. Summary of existing medical LLMs tailored to various clinical applications, in terms of their architecture, model development, the number of parameters, the scale of PT/FT data, and the data source. M: million, B: billion. PT: pre-training. FT: fne-tuning. ICL: in-context learning. CoT: chain-of-thought prompting. RAG: retrieval-augmented generation. This information provides guidelines on how to select and build models. We further provide the evaluation information (i.e., task and performance) to show how current works evaluate their models.   


<html><body><table><tr><td>Application</td><td>Model</td><td>Architecture</td><td colspan="2">Model Development #Params</td><td>Data Scale</td><td>Data Source</td><td>Evaluation (Task: Score)</td></tr><tr><td rowspan="5">Medical Decision-Making (Sec. 4)</td><td>Dr. Knows I14</td><td>GPT-3.5</td><td>ICL</td><td>154B</td><td>5820 notes</td><td>MIMIC-II$7+IN-HOUSE 114</td><td>Diagnosis Summarization: 30.72 ROUGE-L</td></tr><tr><td>DDx PaLM-2 154</td><td>PaLM-2</td><td>FT & ICL</td><td>340B</td><td></td><td>MultiMedQA+MIMIC-II 57</td><td>Differential Diagnosis: 0.591 top-10 Accuracy Readmission Prediction: 0.799 AUC</td></tr><tr><td>NYUTron s</td><td>BERT</td><td>PT & FT</td><td>110M</td><td>7.25M notes, 4.1B tokens NYU Notes 55</td><td></td><td>In-hospital Mortality Prediction: 0.949 AUC Comorbidity Index Prediction: 0.894 AUC Length of Stay Prediction: 0.787 AUC Insurance Denial Prediction: 0.872 AUC</td></tr><tr><td>Foresight I55</td><td>GPT-2</td><td>PT & FT</td><td>1.5B</td><td>35M notes</td><td>King's College Hospital, MIMIC-III South London and Maudsley Hospital</td><td>Next Biomedical Concept Forecast: 0.913 F1</td></tr><tr><td></td><td>GPT-4</td><td></td><td></td><td>184 patients</td><td>2016 SIGIR 157, 2021 & 2022 TREC 158</td><td>Ranking Clinical Trials: 0.733 P@ 10, 0.817 NDCG@ 10 Excluding clinical trials: 0.775 AUROC</td></tr><tr><td rowspan="4">Clinical Coding (Sec. 4)</td><td>PLM-ICD 159</td><td>RoBERTa</td><td>FT</td><td>355M</td><td>70,539 notes</td><td>MIMIC-1II160+MIMIC-III57</td><td>ICD Code Prediction: 0.926 AUC, 0.104 F1</td></tr><tr><td>DRG-LLaMA 161</td><td>LLaMA-7B</td><td>FT</td><td>7B</td><td>25k pairs</td><td>MIMIC-IV 105</td><td>Diagnosis-related Group Prediction: 0.327 F1</td></tr><tr><td>ChatICD162</td><td>ChatGPT</td><td>ICL</td><td></td><td>10k pairs</td><td>MIMIC-III57</td><td>ICD Code Prediction:0.920 AUC,0.681 F1</td></tr><tr><td>LLM-codex 163</td><td>ChatGPT+LSTM ICL</td><td></td><td></td><td></td><td>MIMIC-III57</td><td>ICD Code Prediction: 0.834 AUC, 0.468 F1</td></tr><tr><td rowspan="5">Clinical Report Generation (Sec. 4.3)</td><td>ImpressionGPT 164</td><td>ChatGPT</td><td>ICL & RAG</td><td>110M</td><td>184k reports</td><td>MIMIC-CXR I05+IU X-ray 165</td><td>Report Summarization: 47.93 ROUGE-L</td></tr><tr><td>RadAdapt 166</td><td>T5</td><td>FT</td><td>223M, 738M 80k reports</td><td></td><td>MIMIC-IIIS7</td><td>Report Summarization: 36.8 ROUGE-L</td></tr><tr><td>ChatCADI13</td><td>GPT-3</td><td>ICL</td><td>175B</td><td>300 reports</td><td>MIMIC-CXR105</td><td>Report Generation: 0.605 F1</td></tr><tr><td>MAIRA-1 104</td><td>ViT+Vicuna-7B</td><td>FT</td><td>8B</td><td>337k pairs</td><td>MIMIC-CXR105</td><td>Report Generation: 28.9 ROUGE-L</td></tr><tr><td>RadFM 106</td><td>ViT+LLaMA-13B PT& FT</td><td></td><td>14B</td><td>32M pairs</td><td>MedMD 106</td><td>Report Generation: 18.22 ROUGE-L</td></tr><tr><td rowspan="3">Medical Robotics (Sec. 4.4)</td><td>SuFIA 167</td><td>GPT-4</td><td>ICL</td><td></td><td>4 tasks</td><td>ORBIT-Surgical 68</td><td>Surgical Tasks: 100 Success Rate</td></tr><tr><td>UltrasoundGPT 169</td><td>GPT-4</td><td>ICL</td><td></td><td>522 tasks</td><td>-</td><td>Task Completion: 80 Success Rate</td></tr><tr><td>Robotic X-ray 170</td><td>GPT-4</td><td>ICL</td><td></td><td></td><td>=</td><td>X-ray Surgery: 7.6/10 Human Rating</td></tr><tr><td rowspan="5">Medical Language Translation (Sec. 4.5)</td><td>Medical mT5 171</td><td>T5</td><td>PT</td><td>738M, 3B</td><td>4.5B pairs</td><td>PubMed 50 +EMEA 172 ClinicalTrials 173, etc.</td><td>(Multi-Task) Sequence Labeling: 0.767 F1 Augment Mining 0.733 F1</td></tr><tr><td>Apollo 174</td><td>Qwen</td><td>PT&FT</td><td>1.8B-7B</td><td>2.5B pairs</td><td>ApolloCorpora 174</td><td>QA: 0.588 Accuracy</td></tr><tr><td>BiMedix 175</td><td>Mistral</td><td>FT</td><td>13B</td><td>1.3M pairs</td><td>BiMed1.3M 175</td><td>Question Answering: 0.654 Accuracy</td></tr><tr><td>Biomed-sum 176</td><td>BART</td><td>FT</td><td>406M</td><td>27k papers</td><td>BioCiteDB 176</td><td>Abstractive Summarization: 32.33 ROUGE-L</td></tr><tr><td>RALL 177</td><td>BART</td><td>FT & RAG</td><td>406M</td><td>63k pairs</td><td>CELLS 176</td><td>Lay Language Generation: N/A</td></tr><tr><td rowspan="2">Medical Education (Sec. 4.6)</td><td>ChatGPT178</td><td>GPT-3.5/GPT-4</td><td>ICL</td><td></td><td></td><td>- MedQA-R/RS I08+MultiMedQA 11</td><td>Curriculum Generation, Learning Planning</td></tr><tr><td>Med-Gemin 108</td><td>Gemini</td><td>FT &CoT</td><td></td><td></td><td>MIMIC-I157+MultiMedBench 109</td><td>Text-based QA: 0.911 Accuracy Multimodal QA: 0.935 Accuracy</td></tr><tr><td rowspan="3">Mental Health Support (Sec. 4.7)</td><td>PsyChat 179</td><td>ChatGLM</td><td>FT</td><td>6B</td><td>350k pairs</td><td>Xingling 179+Smilechat 179</td><td>Text Generation: 27.6 ROUGE-L</td></tr><tr><td>ChatCounselor I80</td><td>Vicuna</td><td>FT</td><td>7B</td><td>8k instructions</td><td>Psych8K 180 Dreaddit 182+DepSeverity 183+SDCNL184</td><td>Question Answering: Evaluated by ChatGPT</td></tr><tr><td>Mental-LLM181</td><td>Alpaca, FLAN-T5 FT & ICL</td><td></td><td>7B, 11B</td><td>31k pairs</td><td>CSSRS-Suicide 185+Red-Sam186 Twt-60Users 187+SAD188</td><td>Mental Health Prediction: 0.741 Accuracy</td></tr><tr><td rowspan="3">Medical Inquiry and Response (Sec. 4.8)</td><td>AMIE 189</td><td>PaLM2</td><td>FT</td><td>340B</td><td>>2M pairs</td><td>MedQA14+MultiMedBench109 MIMIC-I157+real-world diaglogue 189</td><td>Diagnostic Accuracy: 0.920 Top-10 Accuracy</td></tr><tr><td>Healthcare Copilot 190</td><td>ChatGPT</td><td>ICL</td><td></td><td></td><td>MedDialog 190</td><td>Inquiry Capability: 4.62/5 (ChatGPT) Conversational Fluency: 4.06/5 (ChatGPT) Response Accuracy: 4.56/5 (ChatGPT) Response Safety: 3.88/5 (ChatGPT)</td></tr><tr><td>Conversational Diagnosis 191 GPT-4/LLaMA</td><td></td><td>ICL</td><td></td><td>40k pairs</td><td>MIMIC-IV 92</td><td>Disease Screening: 0.770 Top-10 Hit Rate Differential Diagnosis: 0.910 Accuracy</td></tr></table></body></html>  

To this end, as shown in Figure 5, we will introduce the clinical applications of LLMs in this section. Each subsection contains a specifc application and discusses how LLMs perform this task. Table 3 summarizes the guidelines on how to select, build, and evaluate medical LLMs for various clinical applications. Although there are currently no large-scale clinical trials specifcally targeting these models, researchers are actively evaluating their effectiveness and safety in various healthcare settings. As research progresses and evidence accumulates, it is expected that the application of these large language models in ill gradually expand, subject to rigorous validation processes and ethical  

# 4.1 Medical Decision-Making  

Medical decision-making, including diagnosis, prognosis, treatment suggestion, risk prediction, clinical trial matching, etc., heavily relies on the synthesis and interpretation of vast amounts of information from various sources, such as patient medical histories, clinical data, and the latest medical literature. The advent of LLMs has opened up new opportunities for enhancing these critical processes in healthcare. These advanced models can rapidly process and comprehend massive volumes of medical data, literature, and legal guidelines, potentially aiding healthcare professionals in making more informed and legally sound decisions across a wide range of clinical scenarios 192,19. For instance, in medical diagnosis, LLMs can assist practitioners in analyzing objective medical data from tests and self-described subjective symptoms to conclude the most likely health problem occurring in the patient192. Similarly, LLMs can support treatment planning by providing personalized recommendations based on the latest clinical evidence and patient-specifc factors 19. Furthermore, LLMs can contribute to prognosis and risk prediction by identifying patterns and risk factors from large-scale patient data, enabling more accurate and timely interventions 55. By leveraging the power of LLMs, healthcare professionals can enhance their decision-making capabilities across the spectrum of clinical tasks, leading to improved patient outcomes and more effcient healthcare delivery.  

Guideline To create an effective LLM-based medical decision-making system, practitioners should begin with a robust LLM and enhance it with specialized medical knowledge. This section outlines key strategies and examples of successful implementations in this feld. Dr. Knows114 demonstrates the effcacy of integrating knowledge graphs from the Unifed Medical Language System (UMLS)115 to improve diagnosis prediction and provide treatment suggestions. This approach involves fne-tuning T5 models193 with extracted diagnoses as prompts and employing zero-shot prompting for LLMs like ChatGPT. Alternatively, models like DDx PaLM-2154 showcase the potential of fne-tuning LLMs (such as Google’s PaLM-2) with extensive medical datasets. This approach enables interactive diagnosis assistance, supporting clinicians in identifying potential diagnoses for better medical decision-making. NYUTron55 is pretrained and fne-tuned on various NYU hospitals and is capable of three clinical tasks (in-patient mortality prediction, comorbidity index prediction, and readmission prediction) and two operational tasks (insurance claim denial prediction and inpatient LOS prediction). Foresight155, is another model which trained on UK hospital patient data and can be used for forecasting the risk of disorders, differential diagnoses, suggest substances (e.g., to do with medicines, allergies, or poisonings) to be used, etc. For clinical trial matching, TrialGPT 156 presents a novel GPT-4-based framework that accurately predicts criterion-level eligibility with faithful explanations, reducing screening time for human experts. Evaluating LLM-based medical diagnosis systems requires task-specifc approaches. For general diagnostic accuracy, metrics like AUC, precision, recall, and F1 score are used with annotated datasets 154,155,156. Some works evaluate the diagnostics with free-text using ROUGE score and CUI F-score114. Crucially, all evaluations must include expert clinician review to ensure clinical relevance and potential real-world impact.  

Discussion One distinct limitation of using LLMs as the sole tool for medical diagnosis is the heavy reliance on subjective text inputs from the patient. Since LLMs are text-based, they lack the inherent capability to analyze medical diagnostic imagery. Given that objective medical diagnoses frequently depend on visual images, LLMs are often unable to directly conduct diagnostic assessments as they lack concrete visual evidence to support disease diagnosis194. However, they can help with diagnosis as a logical reasoning tool for improving accuracy in other vision-based models. One such example is ChatCAD113, where images are frst fed into an existing computer-aided diagnosis (CAD) model to obtain tensor outputs. These outputs are translated into natural language, which is subsequently fed into ChatCAD to summarize results and formulate diagnoses. ChatCAD achieves a recall score of 0.781, substantially higher than that (0.382) of the state-of-the-art task-specifc model. Nevertheless, all the aforementioned methods of implementing LLMs cannot directly process images; instead, they either rely on transforming images into text beforehand or rely on an external separate vision encoder to embed images.  

# 4.2 Clinical Coding  

Clinical coding, such as the International Classifcation of Diseases $(\mathrm{ICD})^{142}$ , medication coding, and procedure coding, plays a crucial role in healthcare by standardizing diagnostic, procedural, and treatment information. These codes are essential for tracking health metrics, treatment outcomes, billing, and reimbursement processes. However, the manual entry of these codes is time-consuming and prone to errors. Large language models (LLMs) have shown promise in automating the clinical coding process by extracting relevant medical terms from clinical notes and assigning corresponding codes, including ICD codes 159,161,162,163, medication codes (e.g., National Drug Code195 or $\mathrm{RxNorm}^{196}$ ), and procedure codes (e.g., Current Procedural Terminology197). By leveraging the vast medical knowledge and natural language understanding capabilities of LLMs, healthcare professionals can beneft from reduced workload and improved accuracy in clinical coding.  

builds upon the RoBERTa model39, fne-tuning it specifcally for ICD coding. It utilizes a domain-specifc base model with medicine-specifc knowledge to enhance its ability to understand medical terms and achieves strong performance on 70,539 notes from the MIMIC-II and MIMIC-III datasets57. Other LLM-based approaches for ICD coding include DRG-LLaMA161, which leverages the LLaMA model and applies parameter-effcient fne-tuning techniques, such as LoRA, to adapt the model to this task. ChatICD162 and LLM-codex163 both utilize the ChatGPT model with prompts for ICD coding, with LLM-codex163 taking a step further by training an LSTM model on top of the ChatGPT responses, demonstrating its strong performance.  

ICD coding is typically formulated as a multi-label classifcation task, with most work in this area utilizing the MIMIC-III dataset for training and evaluation. Models are assessed based on their F1 score, AUC, and Precision $@_{\mathrm{k}}$ , considering either the top k most frequent labels or the full label set. The development of LLMs for ICD coding has the potential to reduce the manual effort required from healthcare professionals, improve the accuracy and consistency of coded data, and facilitate more effcient billing and reimbursement processes.  

Discussion One challenge while deploying LLMs for clinical coding is the potential biases and hallucinations. In particular, traditional multi-label classifcation models can easily constrain their outputs to a predefned list of (usually ${>}1000$ ) candidate codes through a classifcation neural network. In contrast, generative LLMs can suffer from major hallucinations while the input text is lengthy. As a result, the LLM may assign an code that is not in the candidate list or a non-existent clinical code to the input text. It leads to confusion when interpreting medical records 23 and is, therefore, crucial to establish a proactive mechanism to detect and rectify errors before they enter patient EHRs.  

Currently, the majority of research on LLMs for clinical coding focuses on ICD coding due to its widespread use and the availability of large-scale datasets, such as MIMIC-III, which provide ample training data for model development and evaluation. However, there is a growing need for LLMs that can be applied to other types of clinical coding, such as medication and procedure coding. These coding systems are equally important for accurately capturing patient information, facilitating billing and reimbursement processes, and supporting clinical decision-making. Expanding the capabilities of LLMs to encompass medication and procedure coding would greatly enhance the effciency and accuracy of the clinical coding process. By leveraging the vast knowledge and natural language understanding capabilities of LLMs, healthcare professionals could beneft from automated coding systems that accurately extract medication and procedure information from clinical notes, reducing the time and effort required for manual coding.  

# 4.3 Clinical Report Generation  

Clinical reports, such as radiology reports, discharge summaries, and patient clinic letters, refer to standardized documentation that healthcare workers complete after each patient visit198. Therefore, clinical report generation usually involves text generation/summarization, and information retrieval. A large portion of the report is often medical diagnostic results. It is typically tedious for overworked clinicians to write clinical reports, and thus they are often incomplete or error-prone. Meanwhile, LLMs can be used intuitively as a summarization tool to help with clinical report generation. In this instance, LLMs act as an assistant tool for clinicians which helps improve effciency and reduce potential errors in lengthy reports 164,166.  

Another popular approach to generating clinical reports using LLMs involves incorporating a vision-based model to provide complementary information113,106,104. The vision model analyzes the input medical image and generates an annotation, which serves as a direct and supplementary input to the LLM alongside additional text prompts. By leveraging the combination of visual and textual information, the LLM produces accurate and fuent reports that adhere to the specifed parameters and structure.  

Guideline When developing LLM-based applications for radiology report generation, several models can serve as guidance and inspiration, which have different focus. General medical vision-language models like Med-Gemini108, LlaVA-Med102, and Med-Flamingo99 can be serviced as foundation models for the broad medical domain including, radiology, pathology, etc., where there are also models trained specifcally on radiographs, such as ChatCAD113, MAIRA-1104, and RadFM106, have shown superior performance in specifc subdomains. These models leverage the power of large language models and fne-tune them on domain-specifc data to generate radiology reports that accurately capture the relevant information and fndings.  

An alternative approach to radiology report generation focuses on language models that leverage textual data for report summarization. This can be achieved using either unimodal LLMs, which input a long report and generate a summary, or multimodal LLMs, which input both the long report and the related image to generate a summary. The vision-language models mentioned above can also be developed for report summarization. In terms of unimodal LLMs, ImpressionGPT164 serves as an example, employing dynamic prompt generation and iterative optimization to generate concise and informative report summaries. RadAdapt166 systematically evaluates various language models and lightweight adaptation methods, achieving optimal performance through pre-training on clinical text and parameter-effcient fne-tuning with LoRA, while also investigating the impact of few-shot prompting.  

rmance of LLM-based radiology report generation models, most work reli  

MIMIC-IV datasets for training and evaluation, as they are the largest publicly available free-text electronic health records (EHRs). Common automatic evaluation metrics include lexical methods such as BLEU199, ROUGE200, and METEOR201, as well as semantic-based methods like BERTScore202. Additionally, radiology-specifc metrics such as CheXbert similarity203, RadGraph204, and RadCliQ205 have been developed to better assess the quality and accuracy of the generated reports in the context of radiology.  

By leveraging these existing models and evaluation metrics, researchers and developers can create LLM-based radiology report generation applications that accurately and effciently produce high-quality reports, ultimately improving the effciency and effectiveness of radiology workfows.  

Discussion While LLMs have demonstrated the ability to generate clinical reports that are more comprehensive and precise than those written by human counterparts 144, they still face challenges in terms of hallucinations and literal interpretation of inputs, lacking the assumption-based perspective often employed by human doctors. Moreover, LLM-generated reports tend to be less concise compared to human-written ones. The evaluation of LLMs in this domain is particularly challenging due to the specialized nature of the content and the generative nature of the task. Current automatic evaluation methods for clinical report generation primarily focus on lexical metrics, which can lead to biased and inaccurate assessments of the contextual information present in the reports206. For instance, consider two sentences with similar meanings but different wordings: “The patient’s blood glucose level is within normal limits” and “The patient does not exhibit signs of hyperglycemia”. While both convey the absence of hyperglycemia, lexical evaluation metrics may struggle to accurately capture their semantic equivalence, as they rely on direct word-level comparisons. This discrepancy highlights the need for more sophisticated evaluation techniques that can account for the nuances and variations in expressing clinical information. Developing evaluation methods that go beyond surface-level similarities and consider the underlying medical context is crucial for ensuring the reliability and usefulness of LLMs in generating clinical reports.  

# 4.4 Medical Robotics  

Medical robotics is revolutionizing healthcare, offering precision in various aspects, such as surgical procedures and medical imaging207. Recent advancements in incorporating LLMs into medical robotics have shown promising results in enhancing the capabilities of these systems208. LLMs serve as a complementary technology to robotics, augmenting their decision-making, communication, interaction, and control abilities. For example, surgical robots assisted with LLMs enable minimally invasive procedures with increased accuracy and reduced patient recovery times169,208,209. Multi-agent planning systems designed with LLMs involve the coordination of multiple robotic units to perform collaborative tasks, enhancing surgical accuracy and effciency209. Additionally, in the feld of ultrasound and radiology diagnostics, LLMs have been combined with domain knowledge to enable precise diagnostics and dynamic scanning strategies, improving the effciency and quality of scans 169,170.  

Guideline Integrating LLMs into medical robotics poses challenges due to healthcare complexities and real-world evaluation diffculties. Nevertheless, three innovative systems from current research exemplify the potential of LLMs in enhancing medical robotics, serving as representative examples in this emerging feld. SuFIA167 showcases the integration of LLMs in robotic surgery. This system combines the advanced reasoning capabilities of LLMs, specifcally GPT-4 Turbo, with perception modules to implement high-level planning and low-level control of surgical robots for sub-task execution. In the feld of medical imaging, UltrasoundGPT169 presents an innovative approach to ultrasound-guided procedures. This system equips ultrasound robots with LLMs and domain-specifc knowledge, utilizing an ultrasound operation knowledge database to enable precise motion planning. UltrasoundGPT employs a dynamic scanning strategy based on prompt engineering, allowing LLMs to adjust motion planning during procedures. This system demonstrates improved ultrasound scan effciency and quality through verbal command interpretation, contributing to advancements in non-invasive diagnostics and streamlined workfows. Another noteworthy application involves the interpretation of domain-specifc language in X-ray-guided surgery170. This work introduces a minimal protocol enabling an LLM, specifcally GPT-4, to control a robotic $\boldsymbol{\mathrm{X}}$ -ray system, namely the Brainlab Loop-X device. This development showcases the potential of LLMs to enhance the precision and effciency of X-ray-guided surgical procedures through improved communication between surgeons and imaging systems.  

Evaluating such systems clinically can be complicated. The complexity of medical procedures, ethical considerations, and patient safety concerns make it diffcult to conduct comprehensive evaluations in actual healthcare environments. Consequently, most current evaluations rely heavily on simulated data and controlled laboratory settings. For instance, SuFIA and Robotic X-ray’s performance are assessed using a combination of simulated surgical scenarios and expert human evaluation167,170. Similarly, UltrasoundGPT is tested through the assessment of task completion169.  

Discussion Integrating LLMs into medical robotics algorithms for route planning and motion control poses a critical challenge due to the risk of errors and biases inherent in LLMs. The complex and dynamic nature of shared human-robot workspaces may lead to LLM-powered medical robots misjudging human intentions or making inappropriate decisions, posing safety risks. Future research opportunities could explore safety features for medical robots, such as sophisticated sensing technologies and physical design constraints, which aim to minimize the occurrence and consequences of judgment errors related to LLMs in shared human-robot environments210,211,212.  

# 4.5 Medical Language Translation  

There are two main areas of medical language translation; the translation of medical terminology from one language to another 171,174,175 and the translation of medical dialogue for ease of interpretation by non-professional personnel 176,177. Both areas are important for seamless communication between different groups. It promotes accurate diagnosis, treatment planning, and medication administration, minimizing medical errors and improving patient safety. By bridging the communication gap between healthcare providers and patients, it fosters informed decision-making, shared understanding, and enhanced patient satisfaction. Moreover, it empowers non-medical personnel to actively participate in patient care, promoting patient-centered care and cultural sensitivity. Effective medical language translation is essential for providing high-quality healthcare to diverse patient populations.  

Guideline In the development of multilingual LLMs for medical language translation, fne-tuning pre-trained models on parallel corpora of medical texts has proven to be an effective approach. By leveraging diverse datasets such as scientifc articles, clinical notes, and medical glossaries, these models can capture the nuances and domain-specifc meanings of medical terms across languages. Multilingual LLMs like Medical mT5171, Apollo174 and BiMediX175, which are trained on extensive medical datasets in multiple languages, can be further fne-tuned to accurately translate medical terminology between languages such as English, French, Spanish, Chinese, and Arabic. This enables seamless communication and knowledge sharing among healthcare professionals across linguistic boundaries.  

When translating medical dialogue for non-professional understanding, it is crucial to fne-tune LLMs on datasets that encompass both technical medical conversations and their corresponding lay-language explanations. This training approach allows the models to learn the mapping between complex medical jargon and more accessible language, facilitating better comprehension by patients and the general public. Techniques such as retrieval augmentation, which involves retrieving relevant lay-language explanations from external knowledge sources, can further enhance the quality and clarity of the translated dialogue176,177. By integrating domain-specifc knowledge from various sources, LLMs can generate more accurate and informative translations that cater to the needs of non-professional audiences.  

Evaluating the performance of multilingual LLMs in medical language translation requires a multi-faceted approach. Some of the models use multiple choice question and answering test data with the calculation of accuracy score174,175. For generative benchmark, such as summarization176,177, quantitative metrics such as BLEU199, ROUGE200, METEOR201, and BERTScore202 are commonly used to assess translation quality, but they should be supplemented with domain-specifc evaluation criteria. For medical translations, accuracy of terminology, preservation of clinical meaning, and consistency across languages are crucial factors. Human evaluation by bilingual medical experts is essential to validate the nuanced understanding of medical concepts across languages. For patient-oriented translations, comprehension tests with lay individuals can assess the effectiveness of jargon simplifcation.  

Discussion In both translation and simplifcation tasks, misinterpretation is a common occurrence that can have damaging consequences. In developing and deploying medical translation and simplifcation platforms, developers should prioritize professional datasets, such as textbooks and peer-reviewed journals for medical knowledge recall. This way, it will be less likely for misinformation from unreliable web sources to skew the output213. Another ethical consideration of using LLMs to perform medical translation is the potential for discriminatory verbiage to be inserted inadvertently into the output. Such verbiage is diffcult to prevent due to the nature of the pipeline. This may cause miscommunications and even have legal consequences.214.  

# 4.6 Medical Education  

LLMs can be incorporated into the medical education system in different ways, including facilitating study through explanations, aiding in language translation, answering questions, assisting with medical exam preparation, and providing Socratic-style tutoring215,152. Therefore, medical education could involve text generation, text simplifcation, semantic textual similarity, information retrieval, and etc. It has been suggested that medical education can be augmented by generating scenarios, problems, and corresponding answers by an LLM. Students will gain a richer educational experience through personalized study modules and case-based assessments, encountering a wider array of challenges and scenarios beyond those found in standard textbooks214. LLMs can also generate feedback on student responses to practical problems, allowing students to know their areas of weakness in real time. Inherently, these will better prepare these medical students for the real world since they would have been exposed to more scenarios216.  

Another use of LLMs in the medical feld is educating the public. Medical dialogues are often complex and diffcult to understand for the average patient. LLMs can tune the textual output of prompts to use varying degrees of medical terminology for different audiences. This will make medical information easy to understand for the average person while ensuring medical professionals have access to the most precise information 214.  

Guideline Integrating LLMs into medical education can start with existing pre-trained models such as ChatGPT217, and Med-Gemini108. Instead of developing models from scratch, it is often more effective to leverage the knowledge synthesis, question answering, and content generation capabilities of these powerful models. For instance, ChatGPT178 can provide explanations and clarifcations on complex medical concepts, facilitating self-study and reinforcing understanding. MedGemini 108, a multimodal model, can analyze medical images and generate detailed reports, aiding in the training of diagnostic skills. Institutions are exploring the integration of these language models into curricula, leveraging their strengths while ensuring proper oversight and ethical considerations. As this technology continues to advance, it holds promise for enhancing the effciency and accessibility of medical education while complementing human expertise.  

To evaluate the effectiveness of integrating LLMs into medical education, a combination of quantitative and qualitative methods should be employed. Current research focuses on the QA based evaluation108. Quantitative metrics can include student performance on assessments, such as exam scores and clinical skills evaluations, comparing outcomes before and after the introduction of LLM-based tools. Qualitative methods, such as surveys and focus groups, can gather feedback from students and educators on the perceived benefts, challenges, and areas for improvement in using LLMs for learning and teaching. Additionally, longitudinal studies can track the long-term impact of LLM integration on student learning outcomes, clinical competence, and career preparedness. By employing a comprehensive evaluation framework, institutions can iteratively refne their approach to leveraging LLMs in medical education, ensuring that these powerful tools are effectively harnessed to enhance learning while maintaining educational quality and ethical standards.  

Discussion Potential downsides of using LLMs in medical education include the current lack of ethical training and biases in training datasets 24. These biases, if not addressed, can propagate through the generated outputs, reinforcing stereotypes and potentially leading to discrimination in medical education. The lack of explicit ethical training during LLM development may also result in the generation of content that does not align with the ethical principles and guidelines of the medical profession, such as promoting unethical practices or violating patient privacy.  

Furthermore, the risk of misinformation, particularly in the form of hallucinations, presents a challenge in utilizing LLMs for medical education. LLMs can generate plausible-sounding but factually incorrect information, which can mislead students and healthcare professionals if relied upon without proper verifcation. This can lead to the propagation of misconceptions, inappropriate treatment strategies, or misdiagnosis 218. To mitigate these risks, it is essential to establish rigorous fact-checking and validation processes and emphasize the importance of critical thinking, evidence-based practice, and the verifcation of information from multiple reliable sources in medical education.  

# 4.7 Mental Health Support  

Mental health support involves both diagnosis and treatment. For example, depression is treated through a variety of psychotherapies, including cognitive behavior therapy, interpersonal psychotherapy, psychodynamic therapy, etc.153. Many of these techniques are primarily dominated by patient-doctor conversations, with lengthy treatment plans that are cost-prohibitive for many. The ability of LLMs to serve as conversation partners and companions may lower the barrier to entry for patients with fnancial or physical constraints 219, increasing the accessibility to mental health treatments 180. There have been various research works and discussions on the effects of incorporating LLMs into the treatment plan 180,220,221.  

The level of self-disclosure has a heavy impact on the effectiveness of mental health diagnosis and treatment. The degree of willingness to share has a direct impact on the diagnosis results and treatment plan. Studies have shown that patient willingness to discuss mental health-related topics with a robot is high 222,220. Alongside the convenience and lower fnancial stakes, mental health support by LLMs has the potential to be more effective than human counterparts in many scenarios.  

Guideline Development and deployment of LLMs targeted at mental health support can start with an existing LLM. Instead of pre-training or fne-tuning on general medical data, it is often better to use medical question and answer data as most of the LLM’s work will be talking to the patient, which involves back-and-forth conversation in the format of question and answering223. PsyChat179 is a client-centric LLM dialogue system that provides psychological support comprising fve modules: client behavior recognition, counselor strategy selection, input packer, response generator, and response selection. Specifcally, the response generator is fne-tuned with ChatGLM-6B with a vast dialogue dataset. Through both automatic and human evaluations, the system has demonstrated its effectiveness and practicality in real-life mental health support scenarios. ChatCounselor is designed to provide mental health support. It initializes from Vicuna and fne-tunes from an 8k size instruct-tuning dataset collected from real-world counseling dialogue examples180. Psy-LLM is an LLM aimed to be an assistive mental health tool to support the workfow of professional counselors, particularly to support those who might be suffering from depression or anxiety223. Another work presents a comprehensive evaluation of prompt engineering, few-shot, and fne-tuning techniques on multiple LLMs in the mental health domain 181. The results reveal that fne-tuning on a variety of datasets can improve LLM’s capability on multiple mental-health-specifc tasks across different datasets simultaneously181. The work also releases their model Mental-Alpaca and Mental-FLAN-T5 as open-source LLMs targeted at multiple mental health prediction tasks181. Evaluating mental health-focused language models involves a multi-faceted approach that combines automated metrics and expert human assessment. Automated evaluations measure the relevance, coherence, and empathy of the generated responses using specialized metrics tailored to the mental health domain. Mental health professionals conduct human evaluations through simulated counseling sessions, assessing the clinical appropriateness and therapeutic potential of the models’ responses. Recent research has introduced various evaluation frameworks that integrate tasks such as text generation (conversational response) 223, $\mathrm{QA}^{180}$ and mental health prediction181. Liu et al.180 prompt GPT-4 to compare ChatCounselor’s responses with other models based on specifc criteria and explanations. This multi-faceted approach provides researchers with a thorough understanding of the strengths and limitations of mental health-focused language models, enabling them to refne the models and develop more effective and reliable tools for mental health support.  

Discussion Two of the most critical diffculties in employing LLMs for mental health support are the lack of emotional understanding and the risk of inappropriate or harmful responses224. LLMs, being language models, may struggle to fully grasp and respond to the complex emotional states and needs of individuals seeking mental health support. They may not be able to provide the same level of empathy and human connection that is crucial in therapeutic interactions.  

Moreover, if not properly trained or controlled, LLMs may generate responses that are inappropriate, insensitive, or even harmful to individuals in vulnerable emotional states225. They may provide advice that is not grounded in evidence-based psychological practices or that goes against established mental health guidelines. Addressing these challenges requires rigorous training of LLMs in evidence-based practices, ethical considerations, and risk assessment protocols, as well as collaboration between mental health professionals and AI researchers.  

# 4.8 Medical Inquiry and Response  

The rapid advancement of LLMs also opens up new possibilities for improving healthcare delivery and patient care. LLMs, trained on vast amounts of medical knowledge, have the potential to understand and generate human-like text, making them suitable for tasks such as answering patient inquiries and assisting physicians in documentation190,226. As the demand for accessible and effcient healthcare services grows, researchers are exploring the use of medical LLMs to alleviate the burden on healthcare professionals and provide patients with reliable information and support. Therefore, medical inquiry and response could involve entity extraction, information retrieval, question answering.  

Guideline Large language models can be effectively integrated into medical consultation systems to provide AI-powered assistance to healthcare professionals and enhance patient care. Instead of relying solely on rule-based algorithms or limited datasets, these systems leverage the vast knowledge and reasoning capabilities of LLMs to engage in diagnostic conversations and provide personalized recommendations. For example, Healthcare Copilot190 combines dialogue, memory, and processing components to enable safe patient-LLM interactions, enhance conversations with historical data, and summarize consultations. Similarly, Google’s Articulate Medical Intelligence Explore (AMIE)189 employs a novel self-play-based simulated environment with automated feedback mechanisms, allowing the system to learn and adapt across diverse medical contexts. Another LLM-based diagnostic system191 emulates the thought processes of experienced physicians and leverages reinforcement learning techniques to assist in disease screening, initial diagnoses, and the parsing of medical guidelines. These pioneering systems showcase the potential of medical LLMs in providing high-quality, AI-powered consultations and assisting physicians in their daily practice, while emphasizing the importance of rigorous testing, ethical oversight, and collaboration between medical experts and AI researchers to ensure their safe and responsible deployment. These systems showcase the potential of medical LLMs in providing high-quality, AI-powered medical consultations and assisting physicians in their daily practice.  

Current evaluation of these systems often involves the calculation of metrics such as accuracy, precision, recall, and F1-score189 . Additionally, some studies conduct multi-dimensional assessments of the models’ performance, examining aspects such as inquiry capability, conversational fuency, response accuracy and safety using benchmarks and comparisons with human experts or well-established models like ChatGPT190. However, these metrics alone are not suffcient for a comprehensive real-world assessment. It is adviced that the evaluation of this should focus on the diagnostic accuracy, patient satisfaction, and adherence to medical guidelines227.  

Discussion However, there is still far from deploying them in the real-world healthcare system. Several challenges must be addressed before widespread deployment in real-world healthcare settings. One major concern is the potential for biased or inaccurate outputs, which could lead to improper medical advice or misdiagnosis218. Rigorous testing and validation across diverse patient populations and medical contexts are essential to ensure the reliability and generalizability of these systems. Additionally, the integration of medical LLMs into existing healthcare workfows and infrastructure may require substantial technical and organizational efforts. Privacy and security concerns surrounding patient data must also be carefully considered and addressed.  

Furthermore, the development and deployment of medical LLMs raise important ethical and responsible AI considerations. Ensuring transparency, explainability, and accountability in the decision-making processes of these systems is crucial to maintaining trust and facilitating informed consent from patients 228,229. The potential impact on the doctor-patient relationship and the role of human physicians in an AI-assisted healthcare setting must also be carefully examined. Ongoing collaboration between AI researchers, healthcare professionals, ethicists, and policymakers will be necessary to establish guidelines and best practices for the responsible development and deployment of medical LLMs in real-world healthcare settings.  

# 5 Challenges  

allenges and discuss solutions to the adoption of LLMs in an array of me  

# 5.1 Hallucination  

Hallucination of LLMs refers to the phenomenon where the generated output contains inaccurate or nonfactual information. It can be categorized into intrinsic and extrinsic hallucinations 230,218. Intrinsic hallucination generates outputs logically contradicting factual information, such as wrong calculations of mathematical formulas218. Extrinsic hallucination happens when the generated output cannot be verifed, typical examples include LLMs ‘faking’ citations that do not exist or ‘dodging the question. When integrating LLMs into the medical domain, fuent but nonfactual LLM hallucinations can lead to the dissemination of incorrect medical information, causing misdiagnoses, inappropriate treatments, and harmful patient education. It is therefore vital to ensure the accuracy of LLM outputs in the medical domain.  

Potential Solutions Current solutions to mitigate LLM hallucination can be categorized into training-time correction, generation-time correction, and retrieval-augmented correction. The frst (i.e. training-time correction) adjusts model parameter weights, thus reducing the probability of generating hallucinated outputs. Its examples include factually consistent reinforcement learning231 and contrastive learning232. The second (i.e. generation-time correction) adds a ‘reasoning’ process to the LLM inference to ensure reliability, using drawing multiple samples 233 or a confdence score to identify hallucination before the fnal generation. The third approach (i.e. retrieval-augmented correction) utilizes external resources to mitigate hallucination, for example, using factual documents as prompts234 or chain-of-retrieval prompting technique235.  

# 5.2 Lack of Evaluation Benchmarks and Metrics  

Current benchmarks and metrics often fail to evaluate LLM’s overall capabilities, especially in the medical domain. For example, MedQA (USMLE)14 and MedMCQA148 offer extensive coverage on QA tasks but fail to evaluate important LLM-specifc metrics, including trustworthiness, helpfulness, explainability, and faithfulness 206. It is therefore imperative to develop domain and LLM-specifc benchmarks and metrics.  

Potential Solutions Singhal et al.10 proposed HealthSearchQA consisting of commonly searched health queries, offering a more human-aligned benchmark for evaluating LLM’s capabilities in the medical domain. Benchmarks such as TruthfulQA236 and HaluEval237 evaluate more LLM-specifc metrics, such as truthfulness, but do not cover the medical domain. Future research is necessary to meet the need for more medical and LLM-specifc benchmarks and metrics than what is currently available.  

# 5.3 Domain Data Limitations  

Current datasets in the medical domain (Table 2) remain relatively small compared to datasets for training general-purpose LLMs (Table 1). These limited small datasets only cover a small space10 of the vase domain of medical knowledge. This results in LLMs exhibiting extraordinary performance on open benchmarks with extensive data coverage, yet falling short on real-life tasks such as differential diagnosis and personalized treatment planning 11.  

Although the volume of medical and health data is large, most require extensive ethical, legal, and privacy procedures to be accessed. In addition, these data are often unlabeled, and solutions to leverage these data, such as human labeling and unsupervised learning238, face challenges due to the lack of human expert resources and small margins of error.  

Potential Solutions Current state-of-the-art approaches11,15 typically fne-tune the LLMs on smaller open-sourced datasets to improve their domain-specifc performance. Another solution is to generate high-quality synthetic datasets using LLMs to broaden the knowledge coverage; however, it has been discovered that training on generated datasets causes models to forget 239. Future research is needed to validate the effectiveness of using synthetic data for LLMs in the medical feld.  

# 5.4 New Knowledge Adaptation  

LLMs are trained on extensive data to learn knowledge. Once trained, it is expensive and ineffcient to inject new knowledge into an LLM through re-training. However, it is sometimes necessary to update the knowledge of the LLM, for example, on a new adverse effect of a medication or a novel disease. Two problems occur during such knowledge updates. The frst problem is how to make LLMs appropriately ‘forget’ the old knowledge, as it is almost impossible to remove all ‘old knowledge’ from the training data, and the discrepancy between new and old knowledge can cause unintended association and bias240. The second problem is the timeliness of the additional knowledge - how do we ensure the model is updated in real-time241? Both problems pose substantial barriers to using LLMs in medical felds, where accurate and timely updates of medical knowledge are crucial in real-world implementations.  

Potential Solutions Current solutions to knowledge adaptation can be categorized into model editing and retrieval-augmented generation. Model editing242 alters the knowledge of the model by modifying its parameters. However, this method does not generalize well, with their effectiveness varying across different model architectures. In contrast, retrieval-augmented generation provides external knowledge sources as prompts during model inference; for example, Lewis et al.243 enabled model knowledge updates by updating the model’s external knowledge memory.  

# 5.5 Behavior Alignment  

Behavior alignment refers to the process of ensuring that the LLM’s behaviors align with the objectives of its task. Development efforts have been spent on aligning LLMs with general human behavior, but the behavior discrepancy between general humans and medical professionals remains challenging for adopting LLMs in the medical domain. For example, ChatGPT is well aligned with general human behavior, but their answers to medical consultations are not as concise and professional as those by human experts45. In addition, misalignment in the medical domain introduces unnecessary harm and ethical concerns244 that lead to undesirable consequences.  

Potential Solutions Current solutions include instruction fne-tuning, reinforcement learning from human feedback (RLHF)45, and prompt tuning132,129. Instruction fne-tuning124 refers to improving the performance of LLMs on specifc tasks based on explicit instructions. For example, Ouyang et al.45 used it to help LLMs generate less toxic and more suitable outputs. RLHF uses human feedback to evaluate and align the outputs of LLMs. It is effective in multiple tasks, including becoming helpful chatbots245 and decision-making agents246. Prompt tuning can also align LLMs to the expected output format. For example, Liu et al. 247 uses a prompting strategy, chain of hindsight, to enable the model to detect and correct its errors, thus aligning the generated output with human expectations.  

# 5.6 Ethical and Safety Concerns  

Concerns have been raised regarding using LLMs (e.g., ChatGPT) in the medical domain248, with a focus on ethics, accountability, and safety. For example, the scientifc community has disapproved of using ChatGPT in writing biomedical research papers 228 due to ethical concerns. The accountability of using LLMs as assistants to practice medicine is challenging 123,249. Li et al. 250 and Shen et al. 229 found that prompt injection can cause the LLM to leak personally identifable information (PII), e.g., email addresses, from its training data, which is a substantial vulnerability when implementing LLM in the medical domain.  

Potential Solutions With no immediate solutions available, we have nevertheless observed research efforts to understand the cause of these ethical and legal concerns. For example, Wei et al. 251 propose that PII leakage is attributed to the mismatched generalization between safety and capability objectives (i.e., the pre-training of LLMs utilizes a larger and more varied dataset compared to the dataset used for safety training, resulting in many of the model’s capabilities are not covered by safety training).  

# 5.7 Regulatory Challenges  

The regulatory landscape of LLMs presents distinct challenges due to their large scale, broad applicability and varying reliability across applications. As LLMs progressively permeate the felds of medicine and healthcare, their versatility allows a single LLM family to facilitate a multitude of tasks across a broad spectrum of interest groups. This represents a substantial departure from the AI-based medical technologies of the past, which were typically tailored to meet specifc medical needs and cater to particular interest groups 252,192. In addition, the recent innovations of AI-enabled personalized approaches in areas such as oncology also present challenges to the traditional one-for-all auditing process 253. This divergence and innovation necessitate regulators to develop adaptable, foresightful frameworks to ensure the safety, ethical standards, and privacy of the new family of LLMs-powered medical technologies.  

Potential Solutions To address the complex regulatory challenges without hindering innovation, regulators should devise adaptive, fexible, and robust frameworks. Drawing on the insights from Mesko and Topol252, creating a dedicated regulatory category and implementing patient design to enhance decision-making for LLMs used for medical purposes can better address their unique attributes and minimize harm. Furthermore, the insights outlined by Derraz et al. 253 emphasize the importance of implementing agile regulatory frameworks that can keep pace with the fast-paced advancements in personalized applications. Researchers both inside 252,253 and outside of healthcare 254,255 have proposed innovative strategies to regulate the use of LLMs involving (i) assessing LLMs-enabled applications in real-world settings, (ii) obligations of transparency of data and algorithms, (iii) adaptive risk assessment and mitigation processes, (iv) continuous testing and refnement of audited technologies. Such proactive regulatory adaptations are crucial to maintaining high standards of safety, ethics, and trustworthiness of medical technology.  

![](images/ead96ae320d4f527dac3e52402f4c38b1d39efe0c60ce917ab8d389a15c500f5.jpg)  
Figure 6. Future directions of LLMs in clinical medicine in terms of both development and deployment.  

# 6 Future Directions  

Although LLMs have already made an impact on people’s lives through chatbots and search engines, their integration into medicine is still in the infant stage. As shown in Figure 6, numerous new avenues of medical LLMs await researchers and practitioners to explore how to better serve the general public and patients.  

# 6.1 Introduction of New Benchmarks  

Recent studies have underscored the shortcomings of existing benchmarks in evaluating LLMs for clinical applications256,257. Traditional benchmarks, which primarily gauge accuracy in medical question-answering, inadequately capture the full spectrum of clinical skills necessary for LLMs10. Criticisms have been leveled against the use of human-centric standardized medical exams for LLM evaluation, arguing that passing these tests does not necessarily refect an LLM’s profciency in the nuanced expertise required in real-world clinical settings10. In response, there is an emerging consensus on the need for more comprehensive benchmarks. These should include capabilities like sourcing from authoritative medical references, adapting to the evolving landscape of medical knowledge, and clearly communicating uncertainties19,10. To further enhance the relevance of these benchmarks, new benchmarks should incorporate scenarios that test an LLM’s ability through simulation of real-world applications and adjust to feedback from clinicians while maintaining robustness. Additionally, considering the sensitive nature of healthcare, these benchmarks should also assess factors such as fairness, ethics, and equity, which, though crucial, pose quantifcation challenges10. While efforts such as the AMIE study have advanced benchmarking by utilizing real physician evaluations and comprehensive criteria rooted in actual clinical skills and communication, as refected in the Objective Structured Clinical Examination (OSCE), there remains a pressing need for benchmarks that are adaptive, scalable and robust for other diverse and personalized applications of LLMs. The aim is to create benchmarks that more effectively mirror diverse real-world clinical scenarios, thus providing a more accurate measure of LLMs’ suitability for their applications in medicine. Future research may focus on (i) using synthetic data along with real-world data to create benchmarks that are both comprehensive and scalable, (ii) using clinical guidelines and criteria to refect real-world values that are not normally included in traditional benchmarks, (iii) physician-in-the-loop benchmarks to evaluate the performance of LLMs leveraging their human counterparts or users.  

# 6.2 Multimodal LLM Integrated with Time-Series, Visual, and Audio Data  

Multimodal LLMs (MLLMs), or Large Multimodal Models (LMMs), are LLM-based models designed to perform multimodal (e.g., involving both visual and textual) tasks258. While LLMs primarily address NLP tasks, MLLMs support a broader range of tasks, such as comprehending the underlying meaning of a meme and generating website codes from images. This versatility suggests promising applications of MLLMs in medicine. Several MLLM-based frameworks integrating vision and language, e.g., MedPaLM $\mathbf{M}^{259}$ , LLaVA-Med260, Visual Med-Alpaca261, Med-Flamingo262, and Qilin-Med-VL263, have been proposed to adopt the medical image-text pairs for fne-tuning, thus enabling the medical LLMs to effciently understand the input medical (e.g., radiology) images. A recent study264 proposes to integrate vision, audio, and language inputs for automated diagnosis in dentistry. However, there exist only very few medical LLMs that can process time series data, such as electrocardiograms $\mathrm{(ECGs)}^{26\bar{5}}$ and sphygmomanometers (PPGs)266, despite such data being important for medical diagnosis and monitoring. Although early in their proposed research stages, these studies suggest that MLLMs trained at scale have the potential to effectively generalize across various domains and modalities outside of NLP tasks. However, the training of MLLMs at scale is still costly and ineffective, resulting in the size of MLLMs being much smaller than LLMs. Moving forward, future research may focus on (i) more effective processing, representation, and learning of multi-modal data and knowledge, (ii) cost-effective training of MLLMs, especially modalities that are more resource-demanding such as videos and images, (iii) collecting or accessing safely, currently unavailable, multi-modal data in medicine and healthcare.  

# 6.3 Medical Agents  

LLM-based agents 267,268 utilize LLMs as controllers to leverage their reasoning capabilities. By integrating LLMs with external tools and multimodal perceptions, these agents can interact with environments, learn from feedback, and acquire new skills, enabling them to solve complex tasks (e.g., software design, molecular dynamics simulation) through human-like behaviors, such as role-playing and communication269,270.  

However, integrating these agents effectively within the medical domain remains a challenge. The medical feld involves numerous roles 270 and decision-making processes, especially in disease diagnosis that often requires a series of investigations involving CT scans, ultrasounds, electrocardiograms, and blood tests. The idea of utilizing LLMs to model each of these roles, thereby creating collaborative medical agents, presents a promising direction. These agents could mimic the roles of radiologists, cardiologists, pathologists, etc., each specializing in interpreting specifc types of medical data. For example, a radiologist agent could analyze CT scans, while a pathologist agent could focus on blood test results. The collaboration among these specialized agents could lead to a more holistic and accurate diagnosis. By leveraging the comprehensive knowledge base and contextual understanding capabilities of LLMs, these agents not only interpret individual medical reports but also integrate these interpretations to form a cohesive medical opinion. To enhance the integration of LLMs-based agents, future research may explore (i) a seamless data pipeline that collects data from various devices and transforms them into data format compatible with LLMs (ii) effective communication and collaboration between agents, especially in areas such as ensuring truthfulness during communication, dispute resolution between agents, and role-based data security measures, (iii) real-time decision-making such as making timely decisions using data collected from remote monitoring devices, (iv) adaptive learning such as preparing for a new pandemic or learning from unseen medical conditions.  

# 6.4 LLMs in Underrepresented Specialties  

Current LLM research in medicine has largely focused on general medicine, likely due to the greater availability of data in this area11,249. This has resulted in the under-representation of LLM applications in specialized felds like ‘rehabilitation therapy’ or ‘sports medicine’. The latter, in particular, holds potential, given the global health challenges posed by physical inactivity. The World Health Organization identifes physical inactivity as a major risk factor for non-communicable diseases (NCDs), impacting over a quarter of the global adult population 271. Despite initiatives to incorporate physical activity (PA) into healthcare systems, implementation remains challenging, particularly in developing countries with limited PA education among healthcare providers271. LLMs could play a pivotal role in these settings by disseminating accurate PA knowledge and aiding in the creation of personalized PA programs272. Such applications could enhance PA levels, improving global health outcomes, especially in resource-constrained environments. To spark innovation in these underrepresented specialties, future research can focus on areas such as (i) effective data collection in underrepresented specialties, (ii) applications of LLMs in assisting with tasks of underrepresented specialties, (iii) using LLMs to help progress the research of these underrepresented specialties.  

# 6.5 Interdisciplinary Collaborations  

Just as interdisciplinary collaborations are crucial in safety-critical areas like nuclear energy production, collaborations between the medical and technology communities for developing medical LLMs are essential to ensure AI safety and effcacy in medicine. The medical community has primarily adopted LLMs provided by technology companies without rigorously questioning their data training, ethical protocols, or privacy protection. Medical professionals are therefore encouraged to actively participate in creating and deploying medical LLMs by providing relevant training data, defning the desired benefts of LLMs, and conducting tests in real-world scenarios to evaluate these benefts 19,21,22. Such assessments would help to determine the legal and medical risks associated with LLM use in medicine and inform strategies to mitigate LLM hallucination273. Additionally, training ‘bilingual’ professionals—those versed in both medicine and LLM technology—is increasingly vital due to the rapid integration of LLMs in healthcare. Future research may explore (i) interdisciplinary frameworks, such as frameworks to facilitate the sharing of localized data from rural clinics, (ii) ‘bilingual education programs’ that offer training from both worlds - AI and medicine, (iii) effective in-house development methods to help hospitals and physicians ‘guard’ patient data from corporations while still being able to embrace innovation.  

# References  

1. Zhao, W. X. et al. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).   
2. Yang, J. et al. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023).   
3. Chowdhery, A. et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).   
4. Touvron, H. et al. Llama: Open and effcient foundation language models. arXiv preprint arXiv:2302.13971 (2023).   
5. Touvron, H. et al. Llama 2: Open foundation and fne-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).   
6. Brown, T. et al. Language models are few-shot learners. Adv. neural information processing systems 33, 1877–1901 (2020).   
7. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).   
8. Du, Z. et al. Glm: General language model pretraining with autoregressive blank inflling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 320–335 (2022).   
9. Zeng, A. et al. Glm-130b: An open bilingual pre-trained model. In International Conference on Learning Representations (2022).   
10. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023).   
11. Singhal, K. et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617 (2023).   
12. Nori, H. et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452 (2023).   
13. Wu, C., Zhang, X., Zhang, Y., Wang, Y. & Xie, W. Pmc-llama: Further fnetuning llama on medical papers. arXiv preprint arXiv:2304.14454 (2023).   
14. Jin, D. et al. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021).   
15. Li, Y. et al. Chatdoctor: A medical chat model fne-tuned on a large language model meta-ai (llama) using medical domain knowledge. arXiv preprint arXiv:2303.14070 (2023).   
16. Han, T. et al. Medalpaca–an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247 (2023).   
17. Wang, H. et al. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975 (2023).   
18. Toma, A. et al. Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding. arXiv preprint arXiv:2305.12031 (2023).   
19. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat. medicine 29, 1930–1940 (2023).   
20. Patel, S. B. & Lam, K. Chatgpt: the future of discharge summaries? The Lancet Digit. Heal. 5, e107–e108 (2023).   
21. Omiye, J. A., Gui, H., Rezaei, S. J., Zou, J. & Daneshjou, R. Large language models in medicine: The potentials and pitfalls. Annals Intern. Medicine (2024).   
22. Clusmann, J. et al. The future landscape of large language models in medicine. Commun. Medicine 3, 141 (2023).   
23. Yang, X. et al. A large language model for electronic health records. NPJ Digit. Medicine 5, 194 (2022).   
24. Abd-Alrazaq, A. et al. Large language models in medical education: Opportunities, challenges, and future directions. JMIR Med. Educ. 9, e48291 (2023).   
25. Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model. Adv. neural information processing systems 13 (2000).   
26. Mikolov, T., Karafát, M., Burget, L., Cernocky\`, J. & Khudanpur, S. Recurrent neural network based language model. In Interspeech, vol. 2, 1045–1048 (2010).   
27. Sundermeyer, M., Ney, H. & Schlüter, R. From feedforward to recurrent lstm neural networks for language modeling. IEEE/ACM Transactions on Audio, Speech, Lang. Process. 23, 517–529 (2015).   
28. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 (1997).   
29. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).   
30. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).   
31. Kaplan, J. et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).   
32. Hoffmann, J. et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 (2022).   
33. He, P., Liu, X., Gao, J. & Chen, W. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 (2021).   
34. Google. Bard: A generative artifcial intelligence chatbot. https://gemini.google.com (2023).   
35. Taori, R. et al. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca (2023).   
36. Yang, A. et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305 (2023).   
37. Chung, H. W. et al. Scaling instruction-fnetuned language models. arXiv preprint arXiv:2210.11416 (2022).   
38. Joseph, S. et al. Multilingual simplifcation of medical texts. arXiv preprint arXiv:2305.12532 (2023).   
39. Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).   
40. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI blog 1, 9 (2019).   
41. Chiang, W.-L. et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\%$ chatgpt quality (2023).   
42. Jiang, A. Q. et al. Mistral 7b. arXiv preprint arXiv:2310.06825 (2023).   
43. Meta llama 3. https://github.com/meta-llama/llama3 (2024).   
44. Bai, J. et al. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).   
45. Ouyang, L. et al. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst. 35, 27730–27744 (2022).   
46. Claude. https://www.anthropic.com/claude (2024).   
47. Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).   
48. Tay, Y. et al. Ul2: Unifying language learning paradigms. In International Conference on Learning Representations (2022).   
49. Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 1234–1240 (2020).   
50. National Institutes of Health. PubMed Corpora (https://pubmed.ncbi.nlm.nih.gov/download/). In National Library of Medicine (2022).   
51. https://www.ncbi.nlm.nih.gov/pmc/.   
52. Gu, Y. et al. Domain-specifc language model pretraining for biomedical natural language processing. ACM Transactions on Comput. for Healthc. (HEALTH) 3, 1–23 (2021).   
53. Beltagy, I., Lo, K. & Cohan, A. Scibert: A pretrained language model for scientifc text. arXiv preprint arXiv:1903.10676 (2019).   
54. Ammar, W. et al. Construction of the literature graph in semantic scholar. arXiv preprint arXiv:1805.02262 (2018).   
55. Jiang, L. Y. et al. Health system-scale language models are all-purpose prediction engines. Nature 619, 357–362 (2023).   
56. Alsentzer, E. et al. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323 (2019).   
57. Johnson, A. E. et al. Mimic-iii, a freely accessible critical care database. Sci. data 3, 1–9 (2016).   
58. Alrowili, S. & Shanker, V. Large biomedical question answering models with albert and electra. In CLEF (Working Notes), 213–220 (2021).   
59. Gururangan, S. et al. Don’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of Association for Computational Linguistics (ACL) (2020).   
60. Lo, K., Wang, L. L., Neumann, M., Kinney, R. & Weld, D. S. S2orc: The semantic scholar open research corpus. arXiv preprint arXiv:1911.02782 (2019).   
61. Yasunaga, M., Leskovec, J. & Liang, P. Linkbert: Pretraining language models with document links. In Proceedings of Association for Computational Linguistics (ACL) (2022).   
62. Peng, Y., Yan, S. & Lu, Z. Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and Shared Task, 58–65 (2019).   
63. Mutinda, F. W. et al. Detecting redundancy in electronic medical records using clinical bert. In Proceedings of the Annual Conference of the Association for Natural Language Processing, 16–19 (2020).   
64. Mahajan, D. et al. Identifcation of semantically similar sentences in clinical notes: Iterative intermediate training using multi-task learning. JMIR medical informatics 8, e22508 (2020).   
65. Phan, L. N. et al. Scifve: a text-to-text transformer model for biomedical literature. arXiv preprint arXiv:2106.03598 (2021).   
66. Lu, Q., Dou, D. & Nguyen, T. Clinicalt5: A generative language model for clinical text. In Findings of the Association for Computational Linguistics: EMNLP 2022, 5436–5443 (2022).   
67. Jin, Q. et al. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. arXiv preprint arXiv:2307.00589 (2023).   
68. Yasunaga, M. et al. Deep bidirectional language-knowledge graph pretraining. Adv. Neural Inf. Process. Syst. 35, 37309–37323 (2022).   
69. Zhu, Y. et al. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, 19–27 (2015).   
70. Luo, R. et al. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefngs Bioinforma. 23, bbac409 (2022).   
71. Venigalla, A., Frankle, J. & Carbin, M. Biomedlm: a domain-specifc large language model for biomedical text. MosaicML. Accessed: Dec 23, 2 (2022).   
72. Gao, L. et al. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).   
73. Gao, W. et al. Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue. arXiv preprint arXiv:2306.12174 (2023).   
74. Chen, S. et al. Meddialog: a large-scale medical dialogue dataset. arXiv preprint arXiv:2004.03329 3 (2020).   
75. Peng, C. et al. A study of generative large language model for medical research and healthcare. arXiv preprint arXiv:2305.13523 (2023).   
76. Xiong, H. et al. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint arXiv:2304.01097 (2023).   
77. Toyhom. Chinese medical dialogue data. https://github.com/Toyhom/Chinese-medical-dialogue-data (2023). GitHub repository.   
78. Chen, Y. et al. Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt. arXiv preprint arXiv:2310.15896 (2023).   
79. Wang, G., Yang, G., Du, Z., Fan, L. & Li, X. Clinicalgpt: Large language models fnetuned with diverse medical data and comprehensive evaluation. arXiv preprint arXiv:2306.09968 (2023).   
80. Ye, Q. et al. Qilin-med: Multi-stage knowledge injection advanced medical large language model. arXiv preprint arXiv:2310.09089 (2023).   
81. Healthcaremagic. https://www.healthcaremagic.com.   
82. https://www.icliniq.com/.   
83. Byambasuren, O. et al. Preliminary study on the construction of chinese medical knowledge graph. J. Chin. Inf. Process. 33, 1–9 (2019).   
84. Zhang, H. et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint arXiv:2305.15075 (2023).   
85. Xu, C., Guo, D., Duan, N. & McAuley, J. Baize: An open-source chat model with parameter-effcient tuning on self-chat data. arXiv preprint arXiv:2304.01196 (2023).   
86. Abacha, A. B. & Demner-Fushman, D. A question-entailment approach to question answering. BMC Bioinforma. 20 (2019).   
87. Luo, Y. et al. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442 (2023).   
88. Zhang, X. et al. Alpacare: Instruction-tuned large language models for medical application. arXiv preprint arXiv:2310.14558 (2023).   
89. Yang, S. et al. Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue. arXiv preprint arXiv:2308.03549 (2023).   
90. Shoham, O. B. & Rappoport, N. Cpllm: Clinical prediction with large language models. arXiv preprint arXiv:2309.11295 (2023).   
91. Pollard, T. J. et al. The eicu collaborative research database, a freely available multi-center database for critical care research. Sci. data 5, 1–13 (2018).   
92. Johnson, A. et al. Mimic-iv. https://physionet.org/content/mimiciv/1.0/ (2020).   
93. Ferber, D. et al. Gpt-4 for information retrieval and comparison of medical oncology guidelines. NEJM AI 1, AIcs2300235 (2024).   
94. Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079 (2023).   
95. Bosselut, A. et al. Meditron: Open medical foundation models adapted for clinical practice. Preprint (2024).   
96. Ankit Pal, M. S. Openbiollms: Advancing open-source large language models for healthcare and life sciences. https: //huggingface.co/aaditya/OpenBioLLM-Llama3-70B (2024).   
97. Medllama3-v20. https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20 (2024).   
98. Sharegpt: Share your wildest chatgpt conversations with one click. https://sharegpt.com (2023).   
99. Moor, M. et al. Med-famingo: a multimodal medical few-shot learner. In Machine Learning for Health (ML4H), 353–367 (2023).   
100. Lau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. A dataset of clinically generated visual questions and answers about radiology images. Sci. data 5, 1–10 (2018).   
101. He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. Pathvqa: $30000+$ questions for medical visual question answering. arXiv preprint arXiv:2003.10286 (2020).   
102. Li, C. et al. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. Adv. Neural Inf. Process. Syst. 36 (2024).   
103. Liu, B. et al. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), 1650–1654 (IEEE, 2021).   
104. Hyland, S. L. et al. Maira-1: A specialised large multimodal model for radiology report generation. arXiv preprint arXiv:2311.13668 (2023).   
105. Johnson, A. E. et al. Mimic-cxr, a de-identifed publicly available database of chest radiographs with free-text reports. Sci. data 6, 317 (2019).   
106. Wu, C., Zhang, X., Zhang, Y., Wang, Y. & Xie, W. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463 (2023).   
107. Yang, L. et al. Advancing multimodal medical capabilities of gemini. arXiv preprint arXiv:2405.03162 (2024).   
108. Saab, K. et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416 (2024).   
109. Tanno, R. et al. Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. arXiv preprint arXiv:2311.18260 (2023).   
110. Liévin, V., Hother, C. E. & Winther, O. Can large language models reason about medical questions? arXiv preprint arXiv:2207.08143 (2022).   
111. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. Adv. Neural Inf. Process. Syst. 35, 24824–24837 (2022).   
112. Liu, Z. et al. Deid-gpt: Zero-shot medical text de-identifcation by gpt-4. arXiv preprint arXiv:2303.11032 (2023).   
113. Wang, S., Zhao, Z., Ouyang, X., Wang, Q. & Shen, D. Chatcad: Interactive computer-aided diagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257 (2023).   
114. Gao, Y. et al. Leveraging a medical knowledge graph into large language models for diagnosis prediction. arXiv e-prints arXiv–2308 (2023).   
115. Bodenreider, O. The unifed medical language system (umls): integrating biomedical terminology. Nucleic acids research 32, D267–D270 (2004).   
116. Shi, W. et al. Retrieval-augmented large language models for adolescent idiopathic scoliosis patients in shared decisionmaking. In Proceedings of the 14th ACM International Conference on BCB, 1–10 (2023).   
117. SRS. https://www.srs.org. Accessed: 2024-05-14.   
118. UpToDate. http://uptodate.com. Accessed: 2024-05-14.   
119. Dynamed. https://www.dynamed.com. Accessed: 2024-05-14.   
120. Kim, J. & Min, M. From rag to qa-rag: Integrating generative ai for pharmaceutical regulatory compliance process. arXiv preprint arXiv:2402.01717 (2024).   
121. Zakka, C. et al. Almanac—retrieval-augmented language models for clinical medicine. NEJM AI 1, AIoa2300068 (2024).   
122. Wu, J. et al. Clinical text datasets for medical artifcial intelligence and large language models—a systematic review. NEJM AI 1, AIra2400012 (2024).   
123. He, K. et al. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694 (2023).   
124. Zhang, S. et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).   
125. Wang, H., Liu, C., Zhao, S., Qin, B. & Liu, T. Chatglm-med. https://github.com/SCIR-HI/Med-ChatGLM (2023).   
126. Hu, E. J. et al. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).   
127. Li, X. L. & Liang, P. Prefx-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).   
128. Liu, X. et al. P-tuning: Prompt tuning can be comparable to fne-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 61–68 (2022).   
129. Liu, X. et al. P-tuning v2: Prompt tuning can be comparable to fne-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021).   
130. Houlsby, N. et al. Parameter-effcient transfer learning for nlp. In International Conference on Machine Learning, 2790–2799 (2019).   
131. Dong, Q. et al. A survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).   
132. Lester, B., Al-Rfou, R. & Constant, N. The power of scale for parameter-effcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3045–3059 (2021).   
133. Gao, Y. et al. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).   
134. Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking retrieval-augmented generation for medicine. arXiv preprint arXiv:2402.13178 (2024).   
135. Li, X. & Li, J. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871 (2023).   
136. Wang, G. et al. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 (2023).   
137. Chen, J. et al. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2309.07597 (2023).   
138. Shao, Z. et al. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294 (2023).   
139. Trivedi, H., Balasubramanian, N., Khot, T. & Sabharwal, A. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509 (2022).   
140. Asai, A., Wu, Z., Wang, Y., Sil, A. & Hajishirzi, H. Self-rag: Learning to retrieve, generate, and critique through self-refection. arXiv preprint arXiv:2310.11511 (2023).   
141. Donnelly, K. et al. Snomed-ct: The advanced terminology and coding system for ehealth. Stud. health technology informatics 121, 279 (2006).   
142. Organization, W. H. et al. International classifcation of diseases:[9th] ninth revision, basic tabulation list with alphabetic index (World Health Organization, 1978).   
143. Tang, L. et al. Evaluating large language models on medical evidence summarization. npj Digit. Medicine 6, 158 (2023).   
144. Van Veen, D. et al. Clinical text summarization: Adapting large language models can outperform human experts. arXiv preprint arXiv:2309.07430 (2023).   
145. Ondov, B., Attal, K. & Demner-Fushman, D. A survey of automated methods for biomedical text simplifcation. J. Am. Med. Informatics Assoc. 29, 1976–1988 (2022).   
146. Liu, F. et al. Retrieve, reason, and refne: Generating accurate and faithful patient instructions. Adv. Neural Inf. Process. Syst. 35, 18864–18877 (2022).   
147. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146 (2019).   
148. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, 248–260 (2022).   
149. Dog˘an, R. I., Leaman, R. & Lu, Z. Ncbi disease corpus: a resource for disease name recognition and concept normalization. J. biomedical informatics 47, 1–10 (2014).   
150. Dong, H. et al. Automated clinical coding: what, why, and where we are? NPJ digital medicine 5, 159 (2022).   
151. D’Onofrio, G. et al. Emotion recognizing by a robotic solution initiative. Sensors 22, 2861 (2022).   
152. Biri, S. K. et al. Assessing the utilization of large language models in medical education: Insights from undergraduate medical students. Cureus 15 (2023).   
153. Vaidyam, A. N., Wisniewski, H., Halamka, J. D., Kashavan, M. S. & Torous, J. B. Chatbots and conversational agents in mental health: a review of the psychiatric landscape. The Can. J. Psychiatry 64, 456–464 (2019).   
154. McDuff, D. et al. Towards accurate differential diagnosis with large language models. arXiv preprint arXiv:2312.00164 (2023).   
155. Kraljevic, Z. et al. Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study. The Lancet Digit. Heal. 6, e281–e290 (2024).   
156. Jin, Q. et al. Matching patients to clinical trials with large language models. ArXiv (2023).   
157. Koopman, B. & Zuccon, G. A test collection for matching patients to clinical trials. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, 669–672 (2016).   
158. Roberts, K., Demner-Fushman, D., Voorhees, E. M., Bedrick, S. & Hersh, W. R. Overview of the trec 2022 clinical trials track. In TREC (2022).   
159. Huang, C.-W., Tsai, S.-C. & Chen, Y.-N. Plm-icd: Automatic icd coding with pretrained language models. arXiv e-prints arXiv–2207 (2022).   
160. Saeed, M., Lieu, C., Raber, G. & Mark, R. G. Mimic ii: a massive temporal icu patient database to support research in intelligent patient monitoring. In Computers in cardiology, 641–644 (IEEE, 2002).   
161. Wang, H., Gao, C., Dantona, C., Hull, B. & Sun, J. Drg-llama: tuning llama model to predict diagnosis-related group for hospitalized patients. npj Digit. Medicine 7, 16 (2024).   
162. Liu, J., Yang, S., Peng, T., Hu, X. & Zhu, Q. Chaticd: Prompt learning for few-shot icd coding through chatgpt. In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 4360–4367 (2023).   
163. Yang, Z., Batra, S. S., Stremmel, J. & Halperin, E. Surpassing gpt-4 medical coding with a two-stage approach. arXiv preprint arXiv:2311.13735 (2023).   
164. Ma, C. et al. An iterative optimizing framework for radiology report summarization with chatgpt. IEEE Transactions on Artif. Intell. (2024).   
165. Open-i. https://openi.nlm.nih.gov/. Accessed: 2024-05-14.   
166. Van Veen, D. et al. Radadapt: Radiology report summarization via lightweight domain adaptation of large language models. arXiv preprint arXiv:2305.01146 (2023).   
167. Moghani, M. et al. Sufa: Language-guided augmented dexterity for robotic surgical assistants. arXiv preprint arXiv:2405.05226 (2024).   
168. Yu, Q. et al. Orbit-surgical: An open-simulation framework for learning surgical augmented dexterity. arXiv preprint arXiv:2404.16027 (2024).   
169. Xu, H. et al. Enhancing surgical robots with embodied intelligence for autonomous ultrasound scanning. arXiv preprint arXiv:2405.00461 (2024).   
170. Killeen, B. D., Chaudhary, S., Osgood, G. & Unberath, M. Take a shot! natural language control of intelligent robotic x-ray systems in surgery. Int. J. Comput. Assist. Radiol. Surg. 1–9 (2024).   
171. García-Ferrero, I. et al. Medical mt5: an open-source multilingual text-to-text llm for the medical domain. arXiv preprint arXiv:2404.07613 (2024).   
172. Tiedemann, J. Parallel data, tools and interfaces in opus. In Lrec, vol. 2012, 2214–2218 (2012).   
173. National Library of Medicine. Clinical trials. https://clinicaltrials.gov/ (2022). Accessed: 2024-05-14.   
174. Wang, X. et al. Apollo: Lightweight multilingual medical llms towards democratizing medical ai to 6b people. arXiv preprint arXiv:2403.03640 (2024).   
175. Pieri, S. et al. Bimedix: Bilingual medical mixture of experts llm. arXiv preprint arXiv:2402.13253 (2024).   
176. Tang, C., Wang, S., Goldsack, T. & Lin, C. Improving biomedical abstractive summarisation with knowledge aggregation from citation papers. arXiv preprint arXiv:2310.15684 (2023).   
177. Guo, Y., Qiu, W., Leroy, G., Wang, S. & Cohen, T. Retrieval augmentation of large language models for lay language generation. J. Biomed. Informatics 149, 104580 (2024).   
178. Peacock, J., Austin, A., Shapiro, M., Battista, A. & Samuel, A. Accelerating medical education with chatgpt: an implementation guide. MedEdPublish 13 (2023).   
179. Qiu, H., Li, A., Ma, L. & Lan, Z. Psychat: A client-centric dialogue system for mental health support. arXiv preprint arXiv:2312.04262 (2023).   
180. Liu, J. M. et al. Chatcounselor: A large language models for mental health support. arXiv preprint arXiv:2309.15461 (2023).   
181. Xu, X. et al. Mental-llm: Leveraging large language models for mental health prediction via online text data. Proc. ACM on Interactive, Mobile, Wearable Ubiquitous Technol. 8, 1–32 (2024).   
182. Turcan, E. & McKeown, K. Dreaddit: A reddit dataset for stress analysis in social media. arXiv preprint arXiv:1911.00133 (2019).   
183. Naseem, U., Dunn, A. G., Kim, J. & Khushi, M. Early identifcation of depression severity levels on reddit using ordinal classifcation. In Proceedings of the ACM Web Conference 2022, 2563–2572 (2022).   
184. Haque, A., Reddi, V. & Giallanza, T. Deep learning for suicide and depression identifcation with unsupervised label correction. In International Conference on Artifcial Neural Networks, 436–447 (2021).   
185. Gaur, M. et al. Knowledge-aware assessment of severity of suicide risk for early intervention. In The world wide web conference, 514–525 (2019).   
186. Sampath, K. & Durairaj, T. Data set creation and empirical analysis for detecting signs of depression from social media postings. In International Conference on Computational Intelligence in Data Science, 136–151 (2022).   
187. Jamil, Z. Monitoring tweets for depression to detect at-risk users. Ph.D. thesis, Université d’Ottawa/University of Ottawa (2017).   
188. Mauriello, M. L. et al. Sad: A stress annotated dataset for recognizing everyday stressors in sms-like conversational systems. In Extended abstracts of the 2021 CHI conference on human factors in computing systems, 1–7 (2021).   
189. Tu, T. et al. Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654 (2024).   
190. Ren, Z., Zhan, Y., Yu, B., Ding, L. & Tao, D. Healthcare copilot: Eliciting the power of general llms for medical consultation. arXiv preprint arXiv:2402.13408 (2024).   
191. Sun, Z., Luo, C. & Huang, Z. Conversational disease diagnosis via external planner-controlled large language models. arXiv preprint arXiv:2404.04292 (2024).   
192. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. Ai in health and medicine. Nat. medicine 28, 31–38 (2022).   
193. Raffel, C. et al. Exploring the limits of transfer learning with a unifed text-to-text transformer. The J. Mach. Learn. Res. 21, 5485–5551 (2020).   
194. Liu, F., Wu, X., Ge, S., Fan, W. & Zou, Y. Exploring and distilling posterior and prior knowledge for radiology report generation. In IEEE Conference on Computer Vision and Pattern Recognition (2021).   
195. Food, Administration, D. et al. National drug code directory (Consumer Protection and Environmental Health Service, Public Health Service . . . , 1976).   
196. Liu, S., Ma, W., Moore, R., Ganesan, V. & Nelson, S. Rxnorm: prescription for electronic drug information exchange. IT professional 7, 17–23 (2005).   
197. Elkin, P. L. & Brown, S. H. Current procedural terminology. In Terminology, Ontology and their Implementations, 367–370 (Springer, 2023).   
198. Liu, X. et al. Reporting guidelines for clinical trials evaluating artifcial intelligence interventions are needed. Nat. Medicine 25, 1467–1469 (2019).   
199. Papineni, K., Roukos, S., Ward, T. & Zhu, W. BLEU: a Method for automatic evaluation of machine translation. In Proceedings of Association for Computational Linguistics (ACL) (2002).   
200. Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Association for Computational Linguistics (ACL) (2004).   
201. Banerjee, S. & Lavie, A. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and summarization, 65–72 (2005).   
202. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).   
203. Smit, A. et al. Chexbert: combining automatic labelers and expert annotations for accurate radiology report labeling using bert. arXiv preprint arXiv:2004.09167 (2020).   
204. Jain, S. et al. Radgraph: Extracting clinical entities and relations from radiology reports. arXiv preprint arXiv:2106.14463 (2021).   
205. Yu, F. et al. Evaluating progress in automatic chest x-ray radiology report generation. Patterns 4 (2023).   
206. Xie, Q. et al. Faithful ai in medicine: A systematic review with large language models and beyond. medRxiv (2023).   
207. Dupont, P. E. et al. A decade retrospective of medical robotics research from 2010 to 2020. Sci. robotics 6, eabi8017 (2021).   
208. Chen, K. et al. Llm-assisted multi-teacher continual learning for visual question answering in robotic surgery. arXiv preprint arXiv:2402.16664 (2024).   
209. Wang, J. et al. Large language models for robotics: Opportunities, challenges, and perspectives. arXiv preprint arXiv:2401.04334 (2024).   
210. Weerarathna, I. N., Raymond, D. & Luharia, A. Human-robot collaboration for healthcare: A narrative review. Cureus 15 (2023).   
211. Moglia, A., Georgiou, K., Georgiou, E., Satava, R. M. & Cuschieri, A. A systematic review on artifcial intelligence in robot-assisted surgery. Int. J. Surg. 95, 106151 (2021).   
212. Xia, Y., Wang, S. & Kan, Z. A nested u-structure for instrument segmentation in robotic surgery. In International Conference on Advanced Robotics and Mechatronics (ICARM), 994–999 (2023).   
213. Chen, Y., Arunasalam, A. & Celik, Z. B. Can large language models provide security & privacy advice? measuring the ability of llms to refute misconceptions. In Proceedings of the 39th Annual Computer Security Applications Conference, 366–378 (2023).   
214. Karabacak, M. et al. The advent of generative language models in medical education. JMIR Med. Educ. 9, e48163 (2023).   
215. Qiu, J. et al. Large ai models in health informatics: Applications, challenges, and the future. IEEE J. Biomed. Heal. Informatics (2023).   
216. Ahn, S. The impending impacts of large language models on medical education. Korean J. Med. Educ. 35, 103 (2023).   
217. OpenAI. Chatgpt [large language model]. https://chat.openai.com (2023).   
218. Rawte, V., Sheth, A. & Das, A. A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922 (2023).   
219. Stock, A., Schlögl, S. & Groth, A. Tell me, what are you most afraid of? exploring the effects of agent representation on information disclosure in human-chatbot interaction. arXiv e-prints arXiv–2307 (2023).   
220. De Choudhury, M., Pendse, S. R. & Kumar, N. Benefts and harms of large language models in digital mental health. arXiv preprint arXiv:2311.14693 (2023).   
221. Hua, Y. et al. Large language models in mental health care: a scoping review (2024). 2401.02984.   
222. Robinson, N., Connolly, J., Suddrey, G. & Kavanagh, D. J. A brief wellbeing training session delivered by a humanoid social robot: A pilot randomized controlled trial. arXiv e-prints arXiv–2308 (2023).   
223. Lai, T. et al. Psy-llm: Scaling up global mental health psychological services with ai-based large language models. arXiv preprint arXiv:2307.11991 (2023).   
224. Ma, Z., Mei, Y. & Su, Z. Understanding the benefts and challenges of using large language model-based conversational agents for mental well-being support. In AMIA Annual Symposium Proceedings, vol. 2023, 1105 (2023).   
225. Chung, N. C., Dyer, G. & Brocki, L. Challenges of large language models for mental health counseling. arXiv preprint arXiv:2311.13857 (2023).   
226. Wang, J., Yang, Z., Yao, Z. & Yu, H. Jmlr: Joint medical llm and retrieval training for enhancing reasoning and professional question answering capability. arXiv preprint arXiv:2402.17887 (2024).   
227. Hager, P. et al. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nat. Medicine 1–10 (2024).   
228. Stokel-Walker, C. Chatgpt listed as author on research papers: many scientists disapprove. Nature 613, 620–621 (2023).   
229. Shen, X., Chen, Z., Backes, M., Shen, Y. & Zhang, Y. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825 (2023).   
230. Umapathi, L. K., Pal, A. & Sankarasubbu, M. Med-halt: Medical domain hallucination test for large language models. arXiv preprint arXiv:2307.15343 (2023).   
231. Roit, P. et al. Factually consistent summarization via reinforcement learning with textual entailment feedback. arXiv preprint arXiv:2306.00186 (2023).   
232. Chern, I.-C. et al. Improving factuality of abstractive summarization via contrastive reward learning. arXiv preprint arXiv:2307.04507 (2023).   
233. Manakul, P., Liusie, A. & Gales, M. J. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896 (2023).   
234. Shuster, K., Poff, S., Chen, M., Kiela, D. & Weston, J. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567 (2021).   
235. Dhuliawala, S. et al. Chain-of-verifcation reduces hallucination in large language models. arXiv preprint arXiv:2309.11495 (2023).   
236. Lin, S., Hilton, J. & Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 (2021).   
237. Li, J., Cheng, X., Zhao, W. X., Nie, J.-Y. & Wen, J.-R. Halueval: A large-scale hallucination evaluation benchmark for large language models. arXiv e-prints arXiv–2305 (2023).   
238. Liu, F. et al. Auto-encoding knowledge graph for unsupervised medical report generation. In Advances in Neural Information Processing Systems (2021).   
239. Shumailov, I. et al. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493 (2023).   
240. Hoelscher-Obermaier, J., Persson, J., Kran, E., Konstas, I. & Barez, F. Detecting edit failures in large language models: An improved specifcity benchmark. arXiv preprint arXiv:2305.17553 (2023).   
241. Liu, F. et al. A medical multimodal large language model for future pandemics. npj Digit. Medicine 6, 226 (2023).   
242. Yao, Y. et al. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172 (2023).   
243. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst. 33, 9459–9474 (2020).   
244. Hendrycks, D. et al. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275 (2020).   
245. Glaese, A. et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375 (2022).   
246. Nakano, R. et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 (2021).   
247. Liu, H., Sferrazza, C. & Abbeel, P. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676 3 (2023).   
248. Sallam, M. Chatgpt utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. In Healthcare, 887 (MDPI, 2023).   
249. Tian, S. et al. Opportunities and challenges for chatgpt and large language models in biomedicine and health. Briefngs Bioinforma. 25, bbad493 (2024).   
250. Li, H., Guo, D., Fan, W., Xu, M. & Song, Y. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197 (2023).   
251. Wei, A., Haghtalab, N. & Steinhardt, J. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483 (2023).   
252. Meskó, B. & Topol, E. J. The imperative for regulatory oversight of large language models (or generative ai) in healthcare. NPJ digital medicine 6, 120 (2023).   
253. Derraz, B. et al. New regulatory thinking is needed for ai-based personalised drug and cell therapies in precision oncology. NPJ Precis. Oncol. 8, 23 (2024).   
254. Hacker, P., Engel, A. & Mauer, M. Regulating chatgpt and other large generative ai models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, 1112–1123 (2023).   
255. Mökander, J., Schuett, J., Kirk, H. R. & Floridi, L. Auditing large language models: a three-layered approach. AI Ethics 1–31 (2023).   
256. Chen, Q. et al. An extensive benchmark study on biomedical text generation and mining with chatgpt. Bioinformatics 39, btad557 (2023).   
257. Chen, Q. et al. Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. arXiv preprint arXiv:2305.16326 (2023).   
258. Yin, S. et al. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549 (2023).   
259. Tu, T. et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334 (2023).   
260. Li, C. et al. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890 (2023).   
261. Shu, C., Liu, F. & Shareghi, C. Visual med-alpaca: A parameter-effcient biomedical llm with visual capabilities. https://github.com/cambridgeltl/visual-med-alpaca (2023).   
262. Moor, M. et al. Med-famingo: a multimodal medical few-shot learner. arXiv preprint arXiv:2307.15189 (2023).   
263. Liu, J. et al. Qilin-med-vl: Towards chinese large vision-language model for general healthcare. arXiv preprint arXiv:2310.17956 (2023).   
264. Huang, H. et al. Chatgpt for shaping the future of dentistry: the potential of multi-modal large language model. Int. J. Oral Sci. 15, 29 (2023).   
265. Li, J., Liu, C., Cheng, S., Arcucci, R. & Hong, S. Frozen language model helps ecg zero-shot learning. arXiv preprint arXiv:2303.12311 (2023).   
266. Englhardt, Z. et al. Exploring and characterizing large language models for embedded system development and debugging. arXiv preprint arXiv:2307.03817 (2023).   
267. Xi, Z. et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 (2023).   
268. Wang, L. et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432 (2023).   
269. Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D. & Ghanem, B. Camel: Communicative agents for "mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760 (2023).   
270. Tang, X. et al. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537 (2023).   
271. Organization, W. H. Physical activity (2022). Accessed: Aug. 18, 2023.   
272. Connor, M. & O’Neill, M. Large language models in sport science & medicine: Opportunities, risks and considerations. arXiv preprint arXiv:2305.03851 (2023).   
273. Mello, M. M. & Guha, N. Chatgpt and physicians’ malpractice risk. In JAMA Health Forum, e231938–e231938 (2023).  

# Acknowledgements  

This work was supported in part by the Pandemic Sciences Institute at the University of Oxford; the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC); an NIHR Research Professorship; a Royal Academy of Engineering Research Chair; the Well-come Trust funded VITAL project; the UK Research and Innovation (UKRI); the Engineering and Physical Sciences Research Council (EPSRC); and the InnoHK Hong Kong Centre for Cerebro-cardiovascular Engineering (COCHE).  

# Author Contributions  

FL, ZL, JL, and DC supervised the project. FL conceived and designed the study. HZ, FL, BG, XZ, JH, and WJ conducted the literature review, performed data analysis, and drafted the manuscript. All authors contributed to the interpretation and fnal manuscript preparation. All authors read and approved the fnal manuscript.  

# Competing Interests  

The authors declare no competing interests.  