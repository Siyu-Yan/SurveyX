{
    "from": "google",
    "scholar_id": "RRd7_sWE9lMJ",
    "detail_id": null,
    "title": "Exploring In-Context Learning of",
    "abstract": " Abstract\n\nEver since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner. Index Terms: In-context learning, speech language model, prompt tuning, few-shot learning, speech classification\n\n# 1. Introduction\n\nLarge language models (LLMs) [1, 2] have gained significant attention in recent years. With the development of LLMs like GPT-3 [3], researchers have discovered the potential for performing in-context learning (ICL) [3, 4]. ICL is a technique that enables LMs to learn to perform new tasks from a small number of demonstrations which are presented at the input of the LM. Formally, we consider a set of data points, denoted as x i, along with their corresponding labels, denoted as y i 1. Additionally, we have a target data point, x t, for which we want the LM to make an inference. To achieve this, we prepend the demonstrations, consisting of the data points and their labels, to the input sequence as follows: [x 1, y 1, x 2, y 2, ..., x n, y n, x t]. By learning the analogy between the data points and their labels, the LM is capable of directly predicting the label of x t. It is important to ",
    "bib_name": "ExploringI0",
    "md_text": "# Exploring In-Context Learning of Textless Speech Language Model for S Classification Tasks\n\nMing-Hao Hsu \u2217, 1, Kai-Wei Chang \u2217, 2, Shang-Wen Li 3, Hung-yi Lee 1\n\n1 Department of Electrical Engineering, National Taiwan University, Taiwan 2 Graduate Institute of Communication Engineering, National Taiwan University, Taiwa 3 Meta AI, USA\n\n# Abstract\n\nEver since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner. Index Terms: In-context learning, speech language model, prompt tuning, few-shot learning, speech classification\n\n# 1. Introduction\n\nLarge language models (LLMs) [1, 2] have gained significant attention in recent years. With the development of LLMs like GPT-3 [3], researchers have discovered the potential for performing in-context learning (ICL) [3, 4]. ICL is a technique that enables LMs to learn to perform new tasks from a small number of demonstrations which are presented at the input of the LM. Formally, we consider a set of data points, denoted as x i, along with their corresponding labels, denoted as y i 1. Additionally, we have a target data point, x t, for which we want the LM to make an inference. To achieve this, we prepend the demonstrations, consisting of the data points and their labels, to the input sequence as follows: [x 1, y 1, x 2, y 2, ..., x n, y n, x t]. By learning the analogy between the data points and their labels, the LM is capable of directly predicting the label of x t. It is important to note that throughout this learning process, the LM remains fixed, and there is no gradient backward process involved. Instead, the LM relies solely on the input demonstrations to acquire knowledge. ICL, as an emergent ability [9, 10] of the LLM, remains insufficiently optimized due to its disparity between the LM\u2019s\n\n*The first two authors contributed equally. 1 In this paper, we consider ICL following the definition in [4]. That is, the model learns the relationship between the \u201cdata point x\u201d and its \u201clabel y\u201d by providing them as demonstrations. Recent works, such as VALL-E [5] and AudioBox [6], also claim to perform ICL with large speech models. However, they primarily focus on learning the acoustic condition or speaker identity of the given speech prompt. This topic has been studied in the speech processing literature as \u201cvoice cloning [7]\u201d and \u201cstyle transfer [8]\u201d and is outside the scope of this paper.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f9a1/f9a1c516-3290-4b45-865e-9eeace683d23.png\" style=\"width: 50%;\"></div>\nFigure 1: This figure illustrates the framework of the proposed approach, where warmup training utilizes a variety of training tasks to instill in-context learning abilities in the speech language model. This allows the model to utilize task demonstrations effectively to tackle novel unseen tasks.\n\npre-training task. Regarding this, researchers have attempted to perform warmup training [4] in either a supervised [11] or self-supervised [12, 13] manner to enhance the ICL capability. The motivation for building an LM with ICL capability is that, as a new paradigm, ICL offers several advantages. First, ICL simplifies the integration of human knowledge into the LM by providing demonstrations. This process resembles the analogy reasoning process of humans [1]. Second, ICL incorporates demonstrations at the input, eliminating the need for backpropagation and gradient flow to establish connections between data points and their labels. As a result, computational costs are reduced. Third, LLMs are often released as a service in the real-world application [3, 14, 15]. ICL is particularly suitable for LM deployment since it only modifies the input, allowing LLMs to adapt to new tasks defined by the users [14]. Overall, in the NLP field, ICL has emerged as a powerful paradigm for utilizing LLMs. However, despite the recent advancements in large speech LMs, there is a notable lack of research on ICL in the domain of speech processing. Fortunately, the recently developed textless speech language models (speech LMs) offer an opportunity for us to explore ICL for speech processing.\n\nTextless speech LMs quantize speech representations into discrete speech tokens [16] and engage in the next token prediction pre-training task, akin to language models in the NLP field. These textless speech LMs are trained on large speechonly datasets and demonstrate robust generation capabilities without any text supervision. Demonstrating robust capabilities, these speech LMs can generate meaningful discrete tokens conditioned on given speech segments. Notable examples include the Generative Spoken Language Model (GSLM)[17], pGSLM[18], audioLM [19] and others. Although recently there have been researches utilizing text data to help speech language modeling (e.g. TWIST [20], VALL-E [5]), we focus on textless speech LM in this work. The reason is that the textless property offers appealing advantages, including but not limited to: (1) Language agnostic: When performing downstream tasks, textless speech LM is not restricted to text prompts with a specific language [5]. (2) Direct speech modeling: Textless speech LM directly models the speech signal, which bypasses the need for speech-text paired data that is usually expensive to collect. Furthermore, many languages lack a written or formal textual form. Overall, the textless property offers us a more general and easy-to-scale framework. We first examined the ICL ability of the current largest opensourced textless speech LM, GSLM. We observed that GSLM failed to comprehend the provided speech-label pairs for speech classification tasks, indicating the lack of emergent ability to perform ICL. If the LM is capable of performing ICL, it makes predictions by learning the input-label mapping [21], even with random labels [22]. A possible explanation is the LM performs \u201cimplicit fine-tuning\u201d during ICL [23]. The preliminary result shows the current speech LM does not possess such emergent ability 2. As shown in Fig. 1, to build a speech LM with ICL ability, we conduct a simple warmup training with parameter efficient tuning (PET) method, prompt tuning [24, 25]. Warmup training is performed on a set of training tasks to enable the speech LM to understand the demonstrations and make predictions when encountering unseen tasks. The experimental results indicate that GSLM, when subjected to warmup training, demonstrates the capability to perform ICL not only on seen tasks but also, surprisingly, on unseen tasks. It surpasses the random guessing baseline for all tasks and can outperform the support vector classifier (SVC) in most scenarios. It\u2019s worth noting that, in this paper, we aim to show the feasibility of ICL for speech LM, not outperform the current state-of-the-art methods. We believe that as more advanced textless speech LM continues to be developed, the significance of ICL behavior will become increasingly evident, mirroring the observations made in the NLP field (a notable example is the evolution from GPT-2 to GPT-3 [22]). Our contributions are as follows:\n\u2022 We investigate the in-context learning capability of the existing speech LM and identify its limitations in this regard.\n\u2022 We introduce the first speech LM that incorporates warmup training, enabling it to perform in-context learning effectively. This is the first speech LM with such capabilities.\n\u2022 We empirically demonstrate that the speech LM can effectively learn and adapt to unseen tasks through ICL and achieve non-trivial results, surpassing the performance of a random sampling baseline.\n2 In fact, we also examined a more powerful speech LM, TWIST [20], that adopts text data for pre-training. However, we do not observe the\n\n2 In fact, we also examined a more powerful speech LM, TWIST [20], that adopts text data for pre-training. However, we do not observe the ICL capability either.\n\n2 In fact, we also examined a more powerful speech LM, TWIST [20] that adopts text data for pre-training. However, we do not observe the ICL capability either.\n\n# 2. Method\n\nWe first investigated the ICL ability of the pre-trained GSLM [17], which is the largest open-sourced textless generative speech LM. In this paper, we focus on speech classification tasks. We provide speech-label pairs as demonstrations at the input and let the model predict the label of a target utterance. Our findings indicate that the current GSLM does not possess ICL ability. As shown in Table 1, \u201cw/o Warmup\u201d is the performance of directly applying ICL on GSLM without warmup training. \u201cRandom\u201d is the performance of randomly guessing a label for the speech classification tasks. We find that applying ICL directly to GSLM yields results worse than random guessing. To address this limitation and build a speech LM with ICL ability, we propose conducting warmup training on the speech LM. For the warmup training, we utilize a set of training tasks denoted as T train to enhance the speech LM\u2019s ICL capability. Specifically, we employ a PET method, prompt tuning [24, 25, 33]. Applying prompt tuning is a design choice, and other tuning methods can also be adopted in the warmup training, as discussed in [4]. However, We conduct prompt tuning for two reasons: (1) Preliminary experiments revealed that fine-tuning the entire model for warmup training is unstable and might lead to inferior ICL performance. (2) Prompt tuning prepends prompt vectors at the input side while keeping the pre-trained speech LM fixed. This preserves its generative capability, which is beneficial for future applications. In the following sections, we outline our approach to conducting warmup training on the set of training tasks T train and evaluate the ICL capability of the model on the training tasks T train (seen tasks) and testing tasks T test (unseen tasks).\n\n# 2.1. Warmup Training\n\nGiven a speech LM M that performs next token prediction on discrete speech tokens x:\n\nx t +1 = M (x 1, x 2, ..., x t),\n\nwhere t is the timestep. We first collect a set of training tasks T train to perform warmup training. Each task T i in T train uses its own dataset. To form one training data point for ICL warmup training, we conduct the following procedure: (1) We randomly sample n  utterances and their corresponding labels from a training task as demonstrations. (2) Following GSLM [17], the utterances are first encoded into discrete token sequences x 1, x 2, ..., x n. In addition, we utilize random label mapping (RLM) [34, 35, 22] to map the task\u2019s labels (target domain) to the discrete tokens (source domain), resulting in labels for the demonstrations y 1, y 2, ..., y n. (3) Each speech token sequence is then truncated or padded to the same utterance length L, yielding \u02dc x 1, \u02dc x 2, ..., \u02dc x n. We found this step critical as it provides a standardized format to the speech LM and simplifies the training. (4) The input data is constructed as\n\n(2)\n\nwhere \u201c\u27e8 s \u27e9\u201d is a separation token with trainable embedding, and \u02dc x t is the target utterance, which is the data we want the model to predict its label. During warmup training, we randomly sample the target utterance from the demonstrations, that is, \u02dc x t \u2208{\u02dc x 1, \u02dc x 2, ..., \u02dc x n}. The model then learns to compare the target utterance and the demonstrations in order to predict the correct label. We find this step simple but effective since it simplifies the training objective. The model is required to compare the target utterance with each\n\nGroup\nTask Type\nTask\nDataset\nw/ Warmup\nRandom\nw/o Warmup\nSVC\nGroup 1\nTtest\nSCR\nArabic SC [26]\n40.9 \u00b1 1.5\n28.0 \u00b1 1.0\n3.9 \u00b1 0.7\n50.8\nER\nIEMOCAP [27]\n47.7 \u00b1 1.8\n41.0 \u00b1 1.4\n3.1 \u00b1 0.4\n33.7\nTtrain\nSCR\nGoogle SC v2 [28]\n79.6 \u00b1 1.1\n25.1 \u00b1 1.6\n6.4 \u00b1 0.8\n43.8\nSCR\nLithuanian SC [29]\n80.5 \u00b1 1.1\n25.1 \u00b1 1.6\n8.4 \u00b1 1.2\n37.8\nSCR\nDysarthric Mandarin SC [30]\n57.8 \u00b1 0.8\n24.7 \u00b1 0.9\n9.8 \u00b1 .0.9\n12.0\nLID\nVoxforge [31]\n32.0 \u00b1 1.6\n23.8 \u00b1 2.7\n1.8 \u00b1 0.5\n29.7\nSD\nMUStARD [32]\n64.7 \u00b1 1.6\n54.7 \u00b1 1.0\n1.2 \u00b1 0.2\n60.9\nGroup 2\nTtest\nSCR\nArabic SC [26]\n36.5 \u00b1 1.2\n28.0 \u00b1 1.0\n3.9 \u00b1 0.7\n50.8\nSCR\nGoogle SC v2 [28]\n48.0 \u00b1 0.7\n25.1 \u00b1 1.6\n6.4 \u00b1 0.8\n43.8\nSD\nMUStARD [32]\n64.1 \u00b1 1.1\n54.7 \u00b1 1.0\n1.2 \u00b1 0.2\n60.9\nTtrain\nSCR\nDysarthric Mandarin SC [30]\n56.0 \u00b1 1.9\n24.7 \u00b1 0.9\n9.8 \u00b1 0.9\n12.0\nSCR\nLithuanian SC [29]\n80.5 \u00b1 0.9\n25.1 \u00b1 1.6\n8.4 \u00b1 1.2\n37.8\nLID\nVoxforge [31]\n29.9 \u00b1 0.5\n23.8 \u00b1 2.7\n1.8 \u00b1 0.5\n29.7\nER\nIEMOCAP [27]\n47.6 \u00b1 1.7\n41.0 \u00b1 1.4\n3.1 \u00b1 0.4\n33.7\ndemonstration and output its corresponding label. The learned behavior benefits ICL in the next stage. We then employ prompt tuning to learn the prompts P ICL to equip the model with ICL capability. Following [36], a set of prompt vectors P ICL are prepended at the input of the speech LM. The speech LM then makes the prediction conditioned on the demonstrations X and the prompt vectors P ICL. We then apply cross entropy loss L on the model prediction and the ground truth label of the target utterance y t for optimizing the prompts:\n\nP ICL \u2190 arg min P ICL L (M (X; P ICL), y t)),\n\n(3)\n\nwhere M (X; P ICL)  represents the model prediction conditioned on the input data X and the prompts P, and y t is the ground truth label corresponding to the target utterance \u02dc x t.\n\n2.2. In-context Learning\n\n# 2.2. In-context Learning\n\nAfter completing the warmup training, the model becomes ready to perform ICL on the training tasks T train (seen tasks) and testing tasks T test (unseen tasks). The process of preparing demonstrations during this stage is similar to the warmup stage as described in the formula (2). The difference is that, in the ICL stage, the target utterance \u02dc x t  is no longer included in the demonstrations. Instead, its corresponding label y t \u2208{y 1, y 2, ..., y n} is included, enabling the model to learn to make predictions based on analogies.\n\n# 3. Experimental Setup\n\n# 3.1. Tasks and Datasets\n\nWe evaluate speech LM\u2019s ICL ability on a diverse set of speech classification tasks with 7 datasets. These tasks include speech command recognition (SCR), emotion recognition (ER), language identification (LID), and sarcasm detection (SD). Also, the datasets involve varying languages, accents, domains, and label distributions, allowing a comprehensive evaluation of the transferability of ICL for speech LM. As shown in Table 1, we select two groups (Group 1 and Group 2) of training tasks T train and testing tasks T test. These tasks are combined in ways that introduce variety, for instance, tasks with different numbers of classes and different types of tasks. This approach enables us to\n\nevaluate our model\u2019s performance in ICL and gauge the impact of the training tasks on performance across a range of testing tasks. Our attention is directed explicitly toward the model\u2019s capacity to utilize learned ICL knowledge during warmup training when faced with a new task. This capability is essential for many practical uses in a multitude of real-world situations. We\u2019ve chosen particular datasets for both training and testing Our aim for each training dataset group includes generating a balanced dataset to avoid bias. To meet this goal, we sample 10,000 data points, each including 4 demonstrations from the training tasks. The same amount of data points is ensured for every task, providing a balanced dataset as each task gets an equal proportion of demonstrations. If we neglect to do this and simply use the entire dataset, some of the larger datasets would dominate a significant portion of the combined dataset, resulting in a skewed dataset.\n\n# 3.2. Implementation Detail\n\nWe adopt GSLM [17] as our backbone speech LM. Specifically, the GSLM is trained on top of discrete units encoded by HuBERT [37] SSL speech model and K-means clustering algorithm with 100 clusters. In warmup training, we conduct prompt tuning and use a prompt length equal to 5. This approach introduces a small fraction of trainable parameters, specifically less than 0.1% of the total 150 million parameters of the speech LM, simplifying the learning process. As described in Section 2, our initial approach involves enforcing a fixed length for utterances. In the primary experiment in Sec. 4.1, we fix the utterance length L to be 50. This standardization ensures consistent utterance lengths across multiple datasets and simplifies the training. We also investigate the impact of varying utterance lengths in Sec. 4.3. We provide four demonstrations and one target utterance in both warmup training and the ICL stage 3. Given the limited research of ICL on speech LM, we compare our GSLM with warmup training (w/ Warmup) with three baselines: (1) random guessing (Random), (2) ICL on GSLM\n\n3 In our preliminary study, we find four demonstrations as a suitable setup, for it provides a reasonable demonstration number while preventing the \u201ccurse of long sequence\u201d problem as discussed in the previous work [36]. How to incorporate more demonstrations and alleviate the long-form problem remains future work.\n\n3 In our preliminary study, we find four demonstrations as a suitable setup, for it provides a reasonable demonstration number while preventing the \u201ccurse of long sequence\u201d problem as discussed in the previous work [36]. How to incorporate more demonstrations and alleviate the long-form problem remains future work.\n\nTable 2: Guessing Rate Analysis. The results demonstrate that warmup training significantly improves the model\u2019s ability to make predictions based on the provided demonstrations in ICL.\n\nGroup\nTask Type\nGuessing Rate (Avg)\nw/o Warmup\nw/ Warmup\nGroup 1\nTtest\n13.9\n95.1\nTtrain\n21.0\n98.4\nGroup 2\nTtest\n15.0\n92.2\nTtrain\n21.7\n97.4\nwithout warmup training (w/o Warmup), and (3) a support vector classifier (SVC). The random guessing method entails making predictions by selecting labels at random from the demonstrations; while SVC is trained using the provided demonstrations to make predictions. We repeat the experiments five times and compute the average accuracy alongside its standard deviation, offering a more fair evaluation of the model\u2019s performance.\n\n# 4. Results\n\n# 4.1. Main Result\n\nIn Table 1, the result shows that ICL with warmup training consistently outperforms both the Random and w/o Warmup baselines and surpasses SVC for most tasks. For Group 1, in unseen tasks T test, ICL with warmup training excels on the Arabic SC dataset with a score of 40.9, notably higher than both Random and w/o Warmup, but lower than SVC\u2019s 50.8. In the IEMOCAP dataset, w/ Warmup surpasses all baselines, including SVC. In seen tasks T train, the w/ Warmup method is superior, particularly scoring 79.6 on Google SC v2 and 80.5 on Lithuanian SC, outpacing all baselines. Group 2 follows a similar trend; the w/ Warmup method leads across the board. In test tasks on Google SC v2, it scores 48.0, markedly higher than both Random and w/o Warmup and slightly better than SVC. Overall, in both Group 1 and Group 2, ICL with Warmup outperforms other baselines. The only exception is the Arabic SC task presented as an unseen task. Although GSLM can perform ICL when such cross-lingual tasks are in T train (such as Lithuanian SC and Mandarin SC), we hypothesize that for GSLM, it\u2019s still a challenge when presenting cross-lingual tasks as unseen tasks T test. These results underline the efficacy of the warmup training for ICL, as it consistently outperforms both baselines across a diverse range of speech tasks. This success opens up new paths for future studies and holds potential for more improvements in this field.\n\n# 4.2. Model Behavior Analysis\n\nWarmup training aims to equip the model with the ability to identify, compare demonstrations, and comprehend the target task. We assess the model\u2019s ability to predict labels based on demonstrations by examining its guessing rate during ICL. The guessing rate indicates the probability of the model identifying demonstrated labels. Higher guessing rates demonstrate effective ICL, with the model making predictions derived from the demonstrations. Table 2 compares guessing rates between models with and without warmup training across two groups, including seen and unseen tasks. Models lacking warmup training exhibit low guessing rates in both Group 1 and Group 2. Conversely, warmup training significantly boosts guessing rates to over 90%.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/682a/682ad67b-f19f-4bca-90b7-5f9f29add025.png\" style=\"width: 50%;\"></div>\nFigure 2: Model Predicting Attention Weights in ICL. Initially, the model primarily attends to the demonstrations within the first two layers. It then gradually shifts its focus to the target in layers 3 through 7, and finally towards the labels in the demonstrations in the final layers. For the demonstration purpose, we show the scenario where the utterance length L is 10.\n\n<div style=\"text-align: center;\">Table 3: Utterance Length Analysis. Accuracy comparison across different discrete tokens in the utterance is reported.\n</div>\n# of Tokens\n10\n30\n50\nNot Fixed\nGroup 1\nTtest\nArabic SC\n40.2\n41.7\n40.9\n23.7\nIEMOCAP\n42.2\n45.7\n47.7\n44.8\nWe further examine the model\u2019s behavior while predicting the label in ICL. The attention map in the model\u2019s attention layers during the execution of ICL is depicted in Figure 2. The figure reveals that the initial two layers mainly concentrate on the demonstrations. The focus then shifts to the target utterance in the middle layers (3rd to 7th) and finally shifts to the demonstrations\u2019 labels in the last layers (8th to 12th). Also, the model\u2019s continued attention to the prompts P ICL highlights the utility of warmup training in ICL. From the studies in this section, we can see that the warmup training effectively steers the model to perform ICL for unseen tasks.\n\n# 4.3. Utterance Length Analysis\n\nAs shown in Table 3, the model\u2019s performance is influenced by the number of discrete tokens in each utterance. If the length of the utterance is too long, although it might hold sufficient information, the GSLM could struggle with modeling such long sequences as reported in [25]. On the other hand, if utterances are too short, the model may lack the necessary information, leading to random guessing. We expect with more advanced speech LM built, more demonstrations with longer length can be incorporated to boost the performance.\n\n# 5. Conclusion\n\nThis paper presents the first successful application of in-context learning (ICL) to speech LM. We initially investigated the limitations of the current speech LM in performing ICL. With the proposed warmup training, the textless GSLM demonstrates ICL capability on seen tasks and successfully achieves non-trivial results on unseen tasks across diverse datasets. This paper does not aim to achieve competitive performance with ICL but to show its feasibility. We\u2019re also aware that the current capacity of the backbone GSLM is restricted. GPT-3 contains 175B parameters, while GSLM is 1000 times smaller, containing 150M parameters, which might cause the limitation of the ICL emergent ability. Future works include investigating ICL on more diverse speech LM and developing more effective warmup strategies for ICL.\n\n# 6. Acknowledgement\n\nWe thank the National Center for High-performance Computing (NCHC) of National Applied Research Laboratories (NARLabs) in Taiwan for providing computational and storage resources.\n\n# 7. References\n\n[1] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., \u201cA survey of large language models,\u201d arXiv preprint arXiv:2303.18223, 2023.\n[2] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., \u201cOn the opportunities and risks of foundation models,\u201d arXiv preprint arXiv:2108.07258, 2021.\n[3]  T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d  Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020.\n[4] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, \u201cA survey on in-context learning,\u201d arXiv preprint arXiv:2301.00234, 2022.\n[5] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d 2023.\n[6] A. Vyas, B. Shi, M. Le, A. Tjandra, Y.-C. Wu, B. Guo, J. Zhang, X. Zhang, R. Adkins, W. Ngan et al., \u201cAudiobox: Unified audio generation with natural language prompts,\u201d arXiv preprint arXiv:2312.15821, 2023.\n[7] S. Arik, J. Chen, K. Peng, W. Ping, and Y. Zhou, \u201cNeural voice cloning with a few samples,\u201d Advances in neural information processing systems, vol. 31, 2018.\n[8] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, \u201cStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\u201d in International conference on machine learning. PMLR, 2018, pp. 5180\u20135189.\n[9] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., \u201cEmergent abilities of large language models,\u201d Transactions on Machine Learning Research, 2022.\n10]  T. Webb, K. J. Holyoak, and H. Lu, \u201cEmergent analogical reasoning in large language models,\u201d arXiv preprint arXiv:2212.09196, 2022.\n11] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi, \u201cMetaicl: Learning to learn in context,\u201d in  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 2791\u20132809.\n12] M. Chen, J. Du, R. Pasunuru, T. Mihaylov, S. Iyer, V. Stoyanov, and Z. Kozareva, \u201cImproving in-context few-shot learning via selfsupervised training,\u201d in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 3558\u20133573.\n13] Y. Gu, L. Dong, F. Wei, and M. Huang, \u201cPre-training to learn in context,\u201d in ACL (1). Association for Computational Linguistics, 2023, pp. 4849\u20134870.\n14] T. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu, \u201cBlack-box tuning for language-model-as-a-service,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 20 841\u201320 855.\n15] OpenAI, \u201cIntroducing ChatGPT,\u201d 2022, https://openai.com/blog/chatgpt.\n16] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech resynthesis from discrete disentangled self-supervised representations,\u201d in Interspeech. ISCA, 2021, pp. 3615\u20133619.\n\n2022,\n\nA. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W. Hsu, A. Mohamed, and E. Dupoux, \u201cSpeech resynthesis from discrete disentangled self-supervised representations,\u201d in Interspeech. ISCA, 2021, pp. 3615\u20133619.\n\n[17] K. Lakhotia et al., \u201cOn generative spoken language modeling from raw audio,\u201d Transactions of the Association for Computational Linguistics, vol. 9, pp. 1336\u20131354, 2021.\n[18] E. Kharitonov et al., \u201cText-free prosody-aware generative spoken language modeling,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 8666\u20138681.\n[19] Z. Borsos et al., \u201cAudiolm: a language modeling approach to audio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.\n[20] M. Hassid et al., \u201cTextually pretrained speech language models,\u201d arXiv preprint arXiv:2305.13009, 2023.\n[21] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou et al., \u201cLarger language models do in-context learning differently,\u201d arXiv preprint arXiv:2303.03846, 2023.\n[22] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, \u201cRethinking the role of demonstrations: What makes in-context learning work?\u201d 2022.\n[23] D. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei, \u201cWhy can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers,\u201d in ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.\n[24] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \u201cPre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing,\u201d ACM Computing Surveys, vol. 55, no. 9, pp. 1\u201335, 2023.\n[25]  K.-W. Chang, W.-C. Tseng, S.-W. Li, and H. yi Lee, \u201cAn Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,\u201d in Proc. Interspeech 2022, 2022, pp. 5005\u20135009.\n[26] L. T. Benamer and O. A. Alkishriwo, \u201cDatabase for arabic speech commands recognition,\u201d in CEST, 2020.\n[27] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIemocap: Interactive emotional dyadic motion capture database,\u201d Language resources and evaluation, vol. 42, no. 4, pp. 335\u2013359, 2008.\n[28] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary speech recognition,\u201d 2018.\n[29] A. Kolesau and D. \u0160e\u0161ok, \u201cUnsupervised pre-training for voice activation,\u201d Applied Sciences, vol. 10, no. 23, p. 8643, 2020.\n[30] Y.-Y. Lin, W.-Z. Zheng, W. C. Chu, J.-Y. Han, Y.-H. Hung, G.-M. Ho, C.-Y. Chang, and Y.-H. Lai, \u201cA speech command control-based recognition system for dysarthric patients based on deep learning technology,\u201d Applied Sciences, 2021.\n[31] K. MacLean, \u201cVoxforge,\u201d available: http://www.voxforge.org/home.\n[32]  S. Castro, D. Hazarika, V. P\u00e9rez-Rosas, R. Zimmermann, R. Mihalcea, and S. Poria, \u201cTowards multimodal sarcasm detection (an _obviously_ perfect paper),\u201d in Proceedings of the 57th Conference of ACL, 2019, pp. 4619\u20134629.\n[33] K.-W. Chang, M.-H. Chen, Y.-P. Lin, J. N. Hsu, P. K.-M. Huang, C.-y. Huang, S.-W. Li, and H.-y. Lee, \u201cPrompting and adapter tuning for self-supervised encoder-decoder speech model,\u201d in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023, pp. 1\u20138.\n[34] A. Chen, Y. Yao, P.-Y. Chen, Y. Zhang, and S. Liu, \u201cUnderstanding and improving visual prompting: A label-mapping perspective,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 19 133\u201319 143.\n[35] Y.-Y. Tsai, P.-Y. Chen, and T.-Y. Ho, \u201cTransfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources,\u201d in  International Conference on Machine Learning. PMLR, 2020, pp. 9614\u20139624.\n[36] K.-W. Chang, Y.-K. Wang, H. Shen, I. thing Kang, W.-C. Tseng, S.-W. Li, and H. yi Lee, \u201cSpeechPrompt v2: Prompt tuning for speech classification tasks,\u201d 2023.\n\navailable:\n\n[37] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d  IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in speech processing, highlighting the limitations of existing speech language models (LMs) in performing ICL and the necessity for a new approach to enhance their capabilities.",
        "problem": {
            "definition": "The primary problem is the inability of current speech LMs, specifically the GSLM, to perform in-context learning for speech classification tasks, which limits their effectiveness in adapting to new tasks.",
            "key obstacle": "The core challenge is that existing methods do not allow speech LMs to learn from demonstrations effectively, resulting in poor performance when faced with unseen tasks."
        },
        "idea": {
            "intuition": "The idea was inspired by the success of in-context learning in natural language processing (NLP) and the potential for similar techniques to be applied in speech processing.",
            "opinion": "The proposed idea involves implementing warmup training on the GSLM to equip it with the ability to learn from demonstrations, thereby enabling it to perform unseen classification tasks effectively.",
            "innovation": "The key innovation lies in the introduction of warmup training that enhances the ICL capability of speech LMs, which is a novel approach not previously applied in this domain."
        },
        "method": {
            "method name": "Warmup Training for In-Context Learning",
            "method abbreviation": "WT-ICL",
            "method definition": "Warmup training involves a parameter-efficient tuning method that prepares the speech LM to learn from demonstrations, thereby enabling it to adapt to new tasks without requiring extensive retraining.",
            "method description": "The method utilizes prompt tuning to enhance the speech LM's ability to perform in-context learning by learning from a set of training tasks.",
            "method steps": [
                "Collect a set of training tasks to perform warmup training.",
                "Randomly sample utterances and their corresponding labels from the training tasks as demonstrations.",
                "Encode the utterances into discrete token sequences.",
                "Map the task's labels to the discrete tokens.",
                "Construct the input data with a separation token and the target utterance.",
                "Apply prompt tuning to learn prompt vectors for the model."
            ],
            "principle": "This method is effective because it allows the speech LM to utilize input demonstrations to make predictions, thereby overcoming the limitations of traditional training approaches."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on a diverse set of speech classification tasks using 7 datasets, including speech command recognition, emotion recognition, language identification, and sarcasm detection.",
            "evaluation method": "The performance of the method was assessed by comparing its accuracy against baseline methods, including random guessing and a support vector classifier, across seen and unseen tasks."
        },
        "conclusion": "The study successfully demonstrates that warmup training enables the textless GSLM to perform in-context learning effectively, achieving significant improvements in classification tasks and showcasing the feasibility of ICL in speech processing.",
        "discussion": {
            "advantage": "The proposed method significantly enhances the ability of speech LMs to learn from demonstrations, resulting in improved performance on both seen and unseen tasks.",
            "limitation": "The current implementation of the GSLM is limited by its smaller model size compared to larger LMs like GPT-3, which may restrict its emergent ICL capabilities.",
            "future work": "Future research should explore the application of ICL in more advanced speech LMs and investigate more effective warmup training strategies."
        },
        "other info": [
            {
                "Acknowledgement": "The authors thank the National Center for High-performance Computing (NCHC) of National Applied Research Laboratories (NARLabs) in Taiwan for providing computational and storage resources."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning (ICL) in speech processing, highlighting the limitations of existing speech language models (LMs) in performing ICL and the necessity for a new approach to enhance their capabilities."
        },
        {
            "section number": "1.2",
            "key information": "The primary problem is the inability of current speech LMs, specifically the GSLM, to perform in-context learning for speech classification tasks, which limits their effectiveness in adapting to new tasks."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method significantly enhances the ability of speech LMs to learn from demonstrations, resulting in improved performance on both seen and unseen tasks."
        },
        {
            "section number": "3.2",
            "key information": "The key innovation lies in the introduction of warmup training that enhances the ICL capability of speech LMs, which is a novel approach not previously applied in this domain."
        },
        {
            "section number": "4.1",
            "key information": "Warmup training involves a parameter-efficient tuning method that prepares the speech LM to learn from demonstrations, thereby enabling it to adapt to new tasks without requiring extensive retraining."
        },
        {
            "section number": "5.1",
            "key information": "The experiments were conducted on a diverse set of speech classification tasks using 7 datasets, including speech command recognition, emotion recognition, language identification, and sarcasm detection."
        },
        {
            "section number": "6.1",
            "key information": "The current implementation of the GSLM is limited by its smaller model size compared to larger LMs like GPT-3, which may restrict its emergent ICL capabilities."
        },
        {
            "section number": "7",
            "key information": "Future research should explore the application of ICL in more advanced speech LMs and investigate more effective warmup training strategies."
        }
    ],
    "similarity_score": 0.790704896905021,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f9a1/f9a1c516-3290-4b45-865e-9eeace683d23.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/682a/682ad67b-f19f-4bca-90b7-5f9f29add025.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Exploring In-Context Learning of.json"
}