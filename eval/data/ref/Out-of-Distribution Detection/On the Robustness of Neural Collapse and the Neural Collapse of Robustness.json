{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.07444",
    "title": "On the Robustness of Neural Collapse and the Neural Collapse of Robustness",
    "abstract": "Neural Collapse refers to the curious phenomenon in the end of training of a neural network, where feature vectors and classification weights converge to a very simple geometrical arrangement (a simplex). While it has been observed empirically in various cases and has been theoretically motivated, its connection with crucial properties of neural networks, like their generalization and robustness, remains unclear. In this work, we study the stability properties of these simplices. We find that the simplex structure disappears under small adversarial attacks, and that perturbed examples \"leap\" between simplex vertices. We further analyze the geometry of networks that are optimized to be robust against adversarial perturbations of the input, and find that Neural Collapse is a pervasive phenomenon in these cases as well, with clean and perturbed representations forming aligned simplices, and giving rise to a robust simple nearest-neighbor classifier. By studying the propagation of the amount of collapse inside the network, we identify novel properties of both robust and nonrobust machine learning models, and show that earlier, unlike later layers maintain reliable simplices on perturbed data. Our code is available at https://github.com/JingtongSu/ robust_neural_collapse.",
    "bib_name": "su2024robustnessneuralcollapseneural",
    "md_text": "# On the Robustness of Neural Collapse and the Neural Collapse of Robustness\nJingtong Su Center for Data Science New York University\n13 Nov 2024\nNikolaos Tsilivis Center for Data Science New York University\nJulia Kempe Center for Data Science and Courant Institute of Mathematical Sciences New York University\nReviewed on OpenReview: https: // openreview .net/ forum? id= OyXS4ZIqd3\n# Abstract\nNeural Collapse refers to the curious phenomenon in the end of training of a neural network, where feature vectors and classification weights converge to a very simple geometrical arrangement (a simplex). While it has been observed empirically in various cases and has been theoretically motivated, its connection with crucial properties of neural networks, like their generalization and robustness, remains unclear. In this work, we study the stability properties of these simplices. We find that the simplex structure disappears under small adversarial attacks, and that perturbed examples \"leap\" between simplex vertices. We further analyze the geometry of networks that are optimized to be robust against adversarial perturbations of the input, and find that Neural Collapse is a pervasive phenomenon in these cases as well, with clean and perturbed representations forming aligned simplices, and giving rise to a robust simple nearest-neighbor classifier. By studying the propagation of the amount of collapse inside the network, we identify novel properties of both robust and nonrobust machine learning models, and show that earlier, unlike later layers maintain reliable simplices on perturbed data. Our code is available at https://github.com/JingtongSu/ robust_neural_collapse.\narXiv:2311.07444v2\n# 1 Introduction\nDeep Neural Networks are nowadays the de facto choice for a vast majority of Machine Learning applications. Their success is often attributed to their ability to jointly learn rich feature functions from the data and to predict accurately based on these features. While the exact reasons for their generalization abilities still remain elusive despite a profusion of active research, they can, at least partially, be explained by implicit properties of the training algorithm, i.e some variant of gradient descent, that specifically biases the solutions towards having certain geometric properties (such as the maximum possible separation of the training points)(Neyshabur et al., 2015; Soudry et al., 2018). Reinforcing arguments about the simplicity of the networks found by stochastic gradient descent in classification settings, Papyan et al. (2020) made the surprising empirical observation that both the feature representations\nReinforcing arguments about the simplicity of the networks found by stochastic gradient descent in classification settings, Papyan et al. (2020) made the surprising empirical observation that both the feature representations\nkempe@nyu.edu\nin the penultimate layer (grouped by their corresponding class) and the weights of the final layer form a simplex equiangular tight frame (ETF) with C vertices, where C is the number of classes. Curiously, such a geometric arrangement becomes more pronounced well-beyond the point of (effectively) zero loss on the training data, motivating the common tendency of practitioners to optimize a network for as long as the computational budget allows. The collection of these empirical phenomena was termed Neural Collapse. While the results of Papyan et al. (2020) fueled much research in the field, many questions remain regarding the connection of Neural Collapse with properties like generalization and robustness of Neural Networks. In particular with regards to adversarial robustness, the ability of a model to withstand adversarial modifications of the input without effective drops in performance, it has been originally claimed that the instantiation of Neural Collapse has positive effect on defending against adversarial attacks (Papyan et al., 2020; Han et al., 2022). However, this seems to at least superficially contradict the fact that neural networks are not a priori adversarially robust (Szegedy et al., 2014; Carlini and Wagner, 2017). Our contributions. Through an extensive empirical investigation with computer vision datasets, we study the robustness of the formed simplices for several converged neural networks. Specifically, we find that gradient-based adversarial attacks with standard hyperparameters alter the feature representations, resulting in neither variability collapse nor simplex formation. To decouple our analysis from label dependencies that accompany untargeted attacks, we further perform targeted attacks that maintain class balance, and show that perturbed points end up almost exactly in the target class-mean vertex of the original simplex, meaning that even this optimal simplex arrangement is quite fragile. Moving forward, we pose the question of whether Neural Collapse can appear in other settings in Deep Learning, in particular in robust training settings. Interestingly, we find that Neural Collapse also happens during adversarial training (Goodfellow et al., 2015; Madry et al., 2018), a worst-case version of empirical risk minimization (Madry et al., 2018), and, in fact, simplices form both for the \u201cclean\u201d original samples and the adversarially perturbed training data. Curiously, the amount of collapse and simplex formation is much less prevalent when alternative robust training methods (Zhang et al., 2019) are deployed (with the same ability to fit the training data). Finally, we study the geometry of the inner layers of the network through the lens of Neural Collapse, and analyze the propagation of representations of both natural and adversarial data. We observe two novel phenomena inside standardly trained (non-robust) networks: (a) feature representations of adversarial examples in the earlier layers show some form of collapse which disappears later in the network, and (b) the nearest-neighbors classifiers formed by the class-centers of feature representations that correspond to early layers have significant accuracy on adversarial examples of the network.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e87/7e87cf10-12a3-474d-aef6-53c5ed9f4b9a.png\" style=\"width: 50%;\"></div>\nFigure 1: Visualisation of our findings. Sticks represent clean class-means. Small dots correspond to the representation of an individual datum. The color represents the ground-truth label, and the dotted lines represent the predicted class-means. Left to Right: clean representations with standardly-trained (ST) networks; perturbed representations with standardly-trained networks; clean representations with adversarially-trained (AT) networks; perturbed representations with adversarially-trained networks. With ST nets, the adversarial perturbations push the representation to \u201cleap\u201d towards another cluster with slight angular deviation. AT makes the simplex resilient to such adversarial attacks, with higher and intra-class variance.\n# To summarize, our contributions and findings are the following, partly illustrated in Figure 1:\n\u2022 Is NC robust? We initiate the study of the neural collapse phenomenon in the context of adversarial robustness, both for standarly trained networks under adversarial attacks and for adversarially trained robust networks to investigate the stability and prevalence of the NC phenomenon. Our work exposes considerable additional fundamental, and we think, surprising, geometrical structure: \u2022 No! For standardly trained networks we find that small, imperceptible adversarial perturbations of the training data remove any simplicial structure at the representation layer: neither variance collapse nor simplex representations appear under standard metrics. Further analysis through class-targeted attacks that preserve class-balance shows a \u201ccluster-leaping\u201d phenomenon: representations of adversarially perturbed data jump to the (angular) vicinity of the original class means. \u2022 Yes for AT networks! Two identical simplices emerge. Adversarially trained, robust, networks exhibit a simplex structure both on original clean and adversarially perturbed data, albeit of higher variance. These two simplices turn out to be the same. We find that the simple nearest-neighbor classifiers extracted from such models also exhibit robustness. \u2022 Early layers are more robust. Analyzing NC metrics in the representations of the inner layers, we observe that initial layers exhibit a higher degree of collapse on adversarial data. The resulting simplices, when used for Nearest Neighbor clustering, give surprisingly robust classifiers. This phenomenon disappears in later layers.\nThe outline of this paper is as follows: In Section 2 we summarize previous work, in Section 3 we describe the measures and methods we use, Section 4 gives our experimental results and the insights we derive from them, and Section 5 concludes.\n# 2 Related work\nNeural Collapse & Geometric properties of Optimization in Deep Learning. The term Neural Collapse was coined by Papyan et al. (2020) to describe phenomena about the feature representations of the last layer and the classification weights of a deep neural network at convergence. It collectively refers to the onset of variability collapse of within-class representations (NC1), the formation of two simplices (NC2) one from the class-mean representations and another from the classification weights - that are actually dual (NC3), and, finally, the underlying simplicity of the prediction rule of the network, which becomes nothing but a simple nearest-neighbor classifier (NC4) (see Section 3 for formal definitions). Papyan et al. (2020), using ideas from Information Theory, showed that the formation of a simplex is optimal in the presence of vanishing within-class variability. Mixon et al. (2022) introduced the unconstrained features model (independently proposed by Fang et al. (2021) as the Layer-Peeled model), a model where the feature representations are considered as free optimization variables, and showed that a global optimizer of this problem (for the MSE loss) exhibits Neural Collapse. Many derivative works have proven Neural Collapse modifying this model, by either considering other loss functions or trying to incorporate more deep learning elements into it (Fang et al., 2021; Zhu et al., 2021; Ji et al., 2022; E and Wojtowytsch, 2022; Zhou et al., 2022; Tirer and Bruna, 2022; Han et al., 2022). The notion of maximum separability dates back to Support Vector Machines (Cortes and Vapnik, 1995), while the bias of gradient-based optimization algorithms towards such solutions has been used to explain the success of boosting methods (Schapire et al., 1997), and, more recently, to motivate the generalization properties of neural networks (Neyshabur et al., 2015; Soudry et al., 2018; Lyu and Li, 2020). The connection between Neural Collapse and generalization of neural networks (on in-distribution and transfer-learning tasks) has been explored in Galanti et al. (2021); Hui et al. (2022). Finally, the propagation of Neural Collapse inside the network has been studied by Ben-Shaul and Dekel (2022); He and Su (2022); Hui et al. (2022); Li et al. (2022); Tirer et al. (2022); Rangamani et al. (2023). Adversarial Examples & Robustness. Neural Networks are famously susceptible to adversarial perturbations of their inputs, even of very small magnitude (Szegedy et al., 2014). Most of the attacks that drive the performance of networks to zero are gradient-based (Goodfellow et al., 2015; Carlini and Wagner, 2017). These perturbations are surprisingly consistent between different architectures and hyperparameters, they are in many cases transferable between models (Papernot et al., 2017), and they can also be made universal\nAdversarial Examples & Robustness. Neural Networks are famously susceptible to adversarial perturba tions of their inputs, even of very small magnitude (Szegedy et al., 2014). Most of the attacks that drive the performance of networks to zero are gradient-based (Goodfellow et al., 2015; Carlini and Wagner, 2017) These perturbations are surprisingly consistent between different architectures and hyperparameters, they are in many cases transferable between models (Papernot et al., 2017), and they can also be made universa\n(one perturbation for all inputs) (Moosavi-Dezfooli et al., 2017). For training robust models, one can resort to algorithms from robust optimization (Xu et al., 2009; Goodfellow et al., 2015; Madry et al., 2018). In particular, the most effective algorithm used in deep learning is called Adversarial Training (Madry et al., 2018). During adversarial training one alternates steps of generating adversarial examples and training on this data instead of the original one. Several variations of this approach have been proposed in the literature (e.g. Zhang et al. (2019); Shafahi et al. (2019); Wong et al. (2020)), modifying either the attack used for data generation or the loss used to measure mistakes. However, models produced by this algorithm, despite being relatively robust, still fall behind in terms of absolute performance (Croce et al., 2021), while there are still many unresolved conceptual questions about adversarial training (Rice et al., 2020a). In terms of the geometrical properties of the solutions, Li et al.; Lv and Zhu (2022) showed that in some cases (either in the presence of separable data or/and homogeneous networks) adversarial training converges to a solution that maximally separates the adversarial points.\n# 3 Background & Methodology\nIn this section, we proceed with formal definitions of Neural Collapse (NC), adversarial attacks, and Adversaria Training (AT), together with the variants we study in this paper.\n# 3.1 Notation\nLet X be an input space, and Y be an output space, with |Y| = C. Denote by S a given class-balanced dataset that consists of C classes and n data points per class. Let f : X \u2192Y be a neural network, with its final linear layer denoted as W. For each class c, the corresponding classifier \u2014 i.e. the c-th row of W \u2014 is denoted as wc, and the bias is called bc. Denote the representation of the i-th sample within class c as hi,c \u2208Rp, and the union of such representations H(S). We define the global-mean vector \u00b5G \u2208Rp, and class-mean vector \u00b5c \u2208Rp associated with S as \u00b5G \u225c 1 nC \ufffd i,c hi,c and \u00b5c \u225c1 n \ufffd i hi,c, c = 1, . . . , C. For brevity, we refer in the text to the globally-centered class-means, {\u00b5c \u2212\u00b5G}C c=1, as just class-means, since these vectors are constituents of the simplex. We denote \u02dc\u00b5c = (\u00b5c \u2212\u00b5G)/\u2225\u00b5c \u2212\u00b5G\u22252 the normalized class-means. Unless otherwise specified, the term \u201crepresentation\u201d refers to the penultimate layer of the network. Additionally, we will refer to class-means of representations of adversarial examples (see Section 3.3) as \u2018perturbed class-means.\u2019 This is because adversarially robust networks can still achieve 100% training accuracy on \u2018adversarial\u2019 perturbations of the original dataset S.\n# 3.2 Neural Collapse Concepts\nPapyan et al. (2020) demonstrate the prevalence of NC on networks optimized by SGD in the Terminal Phase of Training (TPT) \u2013 the phase beyond the point of zero training error and towards zero training loss \u2013 by tracing the following quantities. Throughout our paper, we closely follow Papyan et al. (2020) and Han et al. (2022) on formalization of NC1-NC4. Please refer to the Appendix A for exact definitions. (NC1) Variability collapse: For all classes c, the within-class variation collapses to zero: hi,c \u2192\u00b5c \u2200i \u2208[n], c \u2208[C].\nNC2, Equiangular) Class-Means converge to Equal, Maximally-Separated Pair-wise Angles: For every pair of distinct labels c \u0338= c\u2032,\nNC2, Equiangular) Class-Means converge to Equal, Maximally-Separated Pair-wise Angles: For every pair of distinct labels c \u0338= c\u2032, 1\n(NC2, Equinorm) Class-Means Converge to Equal Length: |\u2225\u00b5c \u2212\u00b5G\u22252 \u2212\u2225\u00b5c\u2032 \u2212\u00b5G\u22252| \u21920 \u2200c, c\u2032.\n(NC2, Equinorm) Class-Means Converge to Equal Length: |\u2225\u00b5c \u2212\u00b5G\u22252 \u2212\u2225\u00b5c\u2032 \u2212\u00b5G\u22252| \u21920 \u2200c, c\u2032. (NC3) Convergence to self-duality: The linear classifier W and the class-means converge to each other1: wc ||w||2 \u2212 \u00b5c \u2212\u00b5G ||\u00b5 \u2212\u00b5||2 \u21920 \u2200c.\n|\u2225\u00b5c \u2212\u00b5G\u22252 \u2212\u2225\u00b5c\u2032 \u2212\u00b5G\u22252| \u21920 \u2200c, c\u2032. (NC3) Convergence to self-duality: The linear classifier W and the class-means converge to each other1: wc ||wc||2 \u2212 \u00b5c \u2212\u00b5G ||\u00b5c \u2212\u00b5G||2 \u21920 \u2200c.\n1Using the definition of NC3 from Han et al. (2022).\n\u27e8\u27e9 \u2192 c\u2032 \u2225 \u2212\u2225 \u2200 \u2208S In this work, we compare representations of original and perturbed data, which imposes ambiguity on which class-mean vectors \u00b5c, \u00b5G to use (from S or S\u2032). In the spirit of the original definitions, for NC1-4 we will use the class means induced by the dataset S\u2032, even if different from the training set. NC4 studies the predictive power of the NCC classifier on S\u2032 by comparing it to the network classification output, which at TPT is equivalent to the ground truth label. For study of reference data S\u2032 outside the TPT, we introduce two quantities, which we use in Section 4 to study Neural Collapse in intermediate layers: NCC-Network Matching Rate: It measures the rate at which the NCC classifier defined in NC4 trained on S coincides with the output of the network on dataset S\u2032. Note that we use \u00b5c calculated by S. arg max c\u2032 \u27e8wc\u2032, h\u27e9+ bc\u2032 ?= arg min c\u2032 \u2225h \u2212\u00b5c\u2032\u22252, h \u2208H(S\u2032). NCC Accuracy: It measures the accuracy on dataset S\u2032 of the NCC classifier defined in NC4 trained on S. Note that we use \u00b5c calculated by S. ch denotes the ground-truth label of the input. ch ?= arg min c\u2032 \u2225h \u2212\u00b5c\u2032\u22252, h \u2208H(S\u2032). Note that when S = S\u2032, both NCC-Network Matching Rate and NCC Accuracy stem from (NC4). We also introduce the following measures to quantify the proximity of two simplices over C-classes: Simplex Similarity: We define the similarity measure between two C-class simplices with normalized class means \u02dc \u00b5c, \u02dc\u00b5 \u2032 c as AVGc arccos \u27e8\u02dc\u00b5c, \u02dc\u00b5\u2032 c\u27e9. Non-centered Angular Distance: Similarly, given two simplices, without taking the global mean \u00b5G and \u00b5\u2032  into account, we can calculate the angular distance with non-centered class-means directly:\nNon-centered Angular Distance: Similarly, given two simplices, without taking the global mean \u00b5G and \u00b5\u2032 G into account, we can calculate the angular distance with non-centered class-means directly:\n|||| |||| Note that the similarity and angular distance between a simplex and itself is zero.\n# Gradient-Based Adversarial Attack, Adversarial Training (AT),\nGiven a deep neural network f with parameters \u03b8, a clean example (x, y) and cross-entropy loss L(\u00b7, \u00b7), the untargeted adversarial perturbation is crafted by running multiple steps of projected gradient descent (PGD) to maximize the CE loss (Kurakin et al., 2017; Madry et al., 2018) (in what follows, we focus on \u2113\u221eadversary with \u21132 deferred to the appendix):   \ufffd   \ufffd\nB \ufffd  \u00b7\u2207L \ufffd where x0 = x is the original example, \u03b1 is the step size, \u02dcx = xN is the final adversarial example, and \u03a0 is the projection on the valid \u03f5-constraint set, B\u03f5 x, of the data. B\u03f5 x is usually taken as either an \u2113\u221eor \u21132 ball centered in x0. Further, to control the predicted label of \u02dcx, a variant called targeted attack minimizes the CE loss w.r.t. a target label yt \u0338= y: xk+1 = \u03a0B\u03f5 x0 \ufffd xk \u2212\u03b1 \u00b7 sign(\u2207xkL(f(xk), yt) \ufffd . (2) With a standardly-trained network, both these methods can effectively reduce the accuracy to 0%. To combat this phenomenon, robust optimization algorithms have been proposed. The most representative methodology, adversarial training (Madry et al., 2018), generates \u02dcx on-the-fly with Equation (1) for each epoch from x, and takes the model-gradient update on \u02dcx only. An alternative robust training variant, TRADES (Zhang et al., 2019), is of particular interest as it aims to address both robustness and clean accuracy. Thus the gradient steps of TRADES directly involve both x and \u00afx, where \u00afx is also obtained by PGD, but under the KL-divergence loss: xk+1 = \u03a0B\u03f5 x0 \ufffd xk + \u03b1 \u00b7 sign(\u2207xkLKL(f(x), f(xk)) \ufffd . (3)\n\ufffd \ufffd where x0 = x is the original example, \u03b1 is the step size, \u02dcx = xN is the final adversarial example, and \u03a0 is the projection on the valid \u03f5-constraint set, B\u03f5 x, of the data. B\u03f5 x is usually taken as either an \u2113\u221eor \u21132 ball centered in x0. Further, to control the predicted label of \u02dcx, a variant called targeted attack minimizes the CE loss w.r.t. a target label yt \u0338= y:   \ufffd \ufffd\n\ufffd \ufffd With a standardly-trained network, both these methods can effectively reduce the accuracy to 0%. To combat this phenomenon, robust optimization algorithms have been proposed. The most representative methodology, adversarial training (Madry et al., 2018), generates \u02dcx on-the-fly with Equation (1) for each epoch from x, and takes the model-gradient update on \u02dcx only. An alternative robust training variant, TRADES (Zhang et al., 2019), is of particular interest as it aims to address both robustness and clean accuracy. Thus the gradient steps of TRADES directly involve both x and \u00afx, where \u00afx is also obtained by PGD, but under the KL-divergence loss: xk+1 = \u03a0B\u03f5 x0 \ufffd xk + \u03b1 \u00b7 sign(\u2207xkLKL(f(x), f(xk)) \ufffd . (3)\n(1)\n(2)\n(3)\nThe total TRADES loss is a summation of the CE loss on the clean data and a KL-divergence (KLD) loss between the predicted probability of x and \u00afx with a regularization constant \u03b2: LCE(f(x), y) + \u03b2 \u00b7 LKL(f(x), f(\u00afx)). (4\n# 4 Experiments\nIn this section, we present our main experimental results measuring neural collapse in standardly (ST) and adversarially (AT) trained models. When collecting feature representations for adversarially perturbed data we always compute the epoch-relevant perturbations: for ST models throughout training we compute NC metrics for data perturbed relative to the model at the current training epoch. For AT models at each epoch we use the current (adversarially perturbed) training data. Datasets We consider image classification tasks on CIFAR-10, CIFAR-100 in our main text. Both datasets are balanced (in terms of images per class), so we comply with the original experimental setup of Papyan et al. (2020). We preprocess the images by subtracting their global (train) mean and dividing by the standard deviation. For the completeness of our experiments, we also consider a 10-class subset of ImageNet: ImageNette2. We pick the 160px variant. The results are presented in Appendix F. Models We train two large convolutional networks, a standard VGG and a Pre-Activation ResNet18, from a random initialization. Both models have sufficient capacity to fit the entire training data. We launch 3 independent runs and report the mean and standard deviation throughout our paper. Algorithms We train the networks using stochastic gradient descent, either optimizing the cross entropy loss (standard training - ST) or the worst case loss, bounded by either an \u21132 or \u2113\u221eperturbation (adversarial training - AT). For CIFAR-10/100 datasets, we adopt community-wide standard values for the perturbations following Rice et al. (2020a): for the \u2113\u221eadversary, we use radius \u03f5 = 8/255 and step size \u03b1 = 2/255 in Equation 1. For the \u21132 adversary, we use radius \u03f5 = 128/255 and step size \u03b1 = 15/255. We perform 10 PGD iterations. All networks are being trained for 400 epochs in order to reach the terminal phase of training (post zero-error), with batch size 128 and initial learning rate 1e-1. We drop the learning rate by a factor of 0.1 at the 100th and again at the 150th epoch. We also consider the TRADES algorithm (Zhang et al., 2019) with Equation (4), setting \u03b2 = 6 following Zhang et al. (2019). For ImageNette, we pick \u21132 radius \u03f5 = 1536/255 and step size \u03b1 = 360/255, and the same \u2113\u221ehyperparameters as for the CIFAR family. We perform 5 PGD iterations due to the larger image size. For full experimental details, please refer to Appendix B. Gaussian perturbation benchmark To disentangle the effect of the size of the perturbations and their adversarial nature, we benchmark our NC analysis with \u201cGaussian\u201d perturbations randomly drawn from N(0, (8/255)2), i.e. of the same variance as the adversarial perturbation. We present results for standardly trained models and \u2113\u221e-adversarially trained ones on CIFAR-10 in the main text and defer the rest of the results to the Appendix C and D, with similar conclusions. In Appendix E, we also study NC phenomena under smaller adversarial \u2113\u221eperturbations and smaller radii in adversarial training (2/255, 4/255 and 6/255); the results interpolate as one would expect. Previous research has observed that the neural collapse phenomenon under standard training does not hold on the test set (e.g., Hui et al. (2022).) We investigated the evolution of accuracy, loss, and neural collapse metrics for both clean and adversarially perturbed test set data. We not only reproduce the non-collapsed dynamic with ST, but also observe an even more severe non-collapse phenomenon on the test set under robust optimization. Please refer to Appendix G for the results.\nPrevious research has observed that the neural collapse phenomenon under standard training does not hold on the test set (e.g., Hui et al. (2022).) We investigated the evolution of accuracy, loss, and neural collapse metrics for both clean and adversarially perturbed test set data. We not only reproduce the non-collapsed dynamic with ST, but also observe an even more severe non-collapse phenomenon on the test set under robust optimization. Please refer to Appendix G for the results.\n# 4.1 Standardly trained neural nets\nInstability of Simplices The first and third column of Figure 2 show the evolution of the NC quantities as described in Section 3 for standardly trained models. We use both adversarially perturbed and Gaussian reference data to study the stability of the original simplices. As expected, NC metrics converge on the clean training data. For Gaussian reference data, Neural Collapse is only slightly attenuated and is hardly\n2https://github.com/fastai/imagenette\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3822/38223f7c-03f3-4573-83cf-aaf928210f8b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">clean robust (perturbed)</div>\n<div style=\"text-align: center;\">Figure 2: Accuracy, Loss, and NC evolution for standardly (ST) and adversarially (AT) trained VGG and ResNet. For AT models, clean and Guassian curves coincide. Setting: CIFAR-10, \u2113\u221eadversary.</div>\ndistinguishable from the clean curve, thus we choose to omit it. Strikingly, NC disappears for adversarially perturbed data. These findings suggest that the simplex formed by clean training data is robust against random perturbations, but fragile to adversarial attacks. The results certainly corroborate the conclusion that the representation class-means of perturbed points with ground-truth label c do not form any geometricallymeaningful structure at all. Re-emergence of Simplices: Cluster Leaping To gain more insight into the geometric structure of the representations of adversarial examples, we propose modifications to our metrics to understand whether any simplex-like structure on adversarial data disappears or if we could retrieve simplicial remnants. We thus ask whether labeling adversarial data with predicted labels instead of ground truth class labels might lead to re-emergence of a geometrical structure. Note that classes formed by predicted labels are not any more balanced in general (see left column of Figure 3) and in fact might have vanishing classes, especially for larger datasets like CIFAR-100. Since the elegant simplex-structure of NC is predicated on balanced classes, NC metrics as defined will be hampered by this imbalance. To gain some coarse insight into the\ngeometry - leveraging the existence of a simplex ETF with ST when centering with the global-mean vector \u00b5G - in Figure 3 we study perturbed representations relative to the original training-data (clean) simplex, by measuring how predicted class-means deviate from the corresponding clean class-means when centering them with the same clean global-mean vector \u00b5G. We outline two key insights: First, the predicted class-means have varying norms. Second, the angular distance between each pair of clean and predicted class-means is in general small, as conceptually illustrated in Figure 1. There, all correctly predicted clean data are wrongly classified after the adversarial perturbation is applied. Further, the predicted class-means have varying norms whilst keeping a small angular distance to the clean class-means, which are represented by the dotted lines and sticks, respectively. We conclude that adversarial attacks manipulate the feature space in an intriguing way by pushing the representation from the ground-truth class-mean to the (wrongly) predicted class-mean with very small angular deviation (\u201ccluster leaping\u201d). This observation is non-trivial, since adversarial attacks are performed by maximizing the CE loss w.r.t. the true label. Intuitively, a successful attack should push the representation to be not aligned with the last layer\u2019s true label weight vector, but whether it would align or not with a wrong class\u2019s weight vector is not a prior clear. Our experiments answer this question in the affirmative.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ac0/1ac03631-5941-4bb0-b9bb-500692e9d03e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Illustration of untargeted adversarial attacks on standardly trained, converged, models that correspond to one random seed. (CIFAR-10, \u2113\u221e). Left: Number of examples with a certain predicted label. Inner Left: The norms of clean class-means. Inner Right: The norms of predicted class-means with perturbed data. Right: Angular distance between clean and predicted class-mean with perturbed data. Upper: ResNet18; Lower: VGG11. For 10 classes, the between-class angular distance is arccos (\u22121 9) = 1.68 rad = 96.38 degrees, while 0.2 rad is only 11.4 degrees.</div>\nTargeted perturbations: merging simplices: To further understand the geometry of representations of perturbed data and circumvent the class-imbalance issue, we perform targeted attacks (Equation (2)) in a circular way (samples of class i are perturbed to resemble class (i + 1) mod C). Note that targeted attacks still result in 100% attack success rate. NC metrics for targeted perturbations are shown in red in Figure 2. As already hinted at by results from the untargeted attack, the NC2 Equiangular term collapses, but NC2 Equinorm does not. This illustrates that the targeted class-means also form a maximally-separated geometrical structure but with varying norms on CIFAR-10. Also, the non-converging NC1 indicates that within-class predicted representations have oscillating positions. Further, from the vanishing NC3, we can infer that for each class c, these non-centered predicted class-mean vectors are very close in the angular space to the non-centered clean class-means \u00b5c, as they both approach Wc/||Wc||, implying that clean and predicted simplices are close. To provide additional verification beyond NC3, in Figure 4 (left subplots) we measure Simplex Similarity and non-centered Angular Distance of the simplices formed by targeted adversarial examples and by clean examples as described in Section 3. These results give us a full glimpse of how standardly trained networks are non-robust and fail under adversarial attacks: adversarial perturbations break the simplex ETF by \u201cleaping\u201d the representation from one class-mean to another, forming a norm-imbalanced less concentrated structure around the original simplex.\nTargeted perturbations: merging simplices: To further understand the geometry of representations of perturbed data and circumvent the class-imbalance issue, we perform targeted attacks (Equation (2)) in a circular way (samples of class i are perturbed to resemble class (i + 1) mod C). Note that targeted attacks still result in 100% attack success rate.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66b8/66b8e39c-b66b-4f24-b01b-8f1ad20a57ad.png\" style=\"width: 50%;\"></div>\nFigure 4: Angular distance. Left and Inner Left: Average between targeted attack class-means and clean class-means on ST network. Inner Right and Right: Average between perturbed class-means and clean class-means on AT network. Setting: CIFAR-10, \u2113\u221eadversary.\n<div style=\"text-align: center;\">Figure 4: Angular distance. Left and Inner Left: Average between targeted attack class-means and clean class-means on ST network. Inner Right and Right: Average between perturbed class-means and clean class-means on AT network. Setting: CIFAR-10, \u2113\u221eadversary.</div>\n# 4.2 Neural Collapse during Adversarial Training\nWe train neural nets adversarially to full convergence with perfect clean and robust training accuracy and measure NC metrics for clean (original) and perturbed (epoch-wise) training data in Figure 2 (columns 2 and 4). Interestingly, we find that Neural Collapse qualitatively occurs in this setting as well, both for clean and perturbed data, and two simplices emerge. In particular, we find that for both ResNet18 and VGG, as robust training loss is driven to zero the NC metrics decrease on both the original (clean) train images and the perturbed points that we use on each epoch to update the parameters. Notice, however, that the extent of variability collapse (NC1) on the perturbed points is smaller than on the \u201cclean\u201d data or the Gaussian noise benchmark, indicating that clean examples are more concentrated around the vertices. To understand the relative positioning of the two simplices, we investigate the Simplex Similarity and Angular Distance between non-centered class-means in Figure 4 (right). The vanishing distance confirms these two simplices are exactly the same. We notice that the angular distance with AT grows initially and then gradually drops. We explain this phenomenon as follows. At the initial phase, the network\u2019s accuracy is only slightly higher than its robustness (see e.g., Figure 2) with both terms relatively low. Thus, untargeted attacks are not capable of modifying the features away from clean class-means yet. Gradually, as clean accuracy rises and clean clusters start to emerge, adversarial attacks become more and more successful, and thus we observe an increase in the angular distance between the classes. These results suggest that Adversarial Training nudges the network to learn simple representational structures (namely, a simplex ETF) not only on clean examples but also on perturbed examples to achieve robustness against adversarial perturbations. Equivalently, the simplices induced by robust networks are not fragile anymore, but resilient. Note also that NC4 results imply that there is a simple nearest-neighbor classifier that is robust against adversarial perturbations generated from the network.\nThese results suggest that Adversarial Training nudges the network to learn simple representational structure (namely, a simplex ETF) not only on clean examples but also on perturbed examples to achieve robustnes against adversarial perturbations. Equivalently, the simplices induced by robust networks are not fragil anymore, but resilient. Note also that NC4 results imply that there is a simple nearest-neighbor classifie that is robust against adversarial perturbations generated from the network.\n# 4.3 No Neural Collapse under TRADES Objective\nOne could conjecture that the formation of two very close simplices in robust models are necessary for robustness. Curiously, this is not the case for all training algorithms that produce robust models. In particular, Figure 5 demonstrates that a state-of-the-art algorithm that aims to balance clean and robust accuracy, TRADES (Zhang et al., 2019), shows fundamentally different behavior. Even though both terms of the loss (see Equation 4) are driven to zero, we do not observe Neural Collapse either qualitatively or quantitatively; the amount of collapse on both clean and perturbed data is roughly one order of magnitude larger than for adversarial training, and the feature representations do not approach the ETF formation, even well past the onset of the terminal phase. We view this as evidence that the prevalence of Neural Collapse is not necessary for robust classification.\n# 4.4 Neural Collapse in Early Layers and Early-Stage Robustness\nWhile originally variability collapse and simplex formation were observed for the last layer representations, follow-up studies extended the analysis to the intermediate layers of the neural network. In particular, He and Su (2022) found that the amount of variability collapse measured at different layers (at convergence) decreases\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a4f/9a4fcdc1-9aa7-48c2-a4cf-17c50a397192.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Accuracy, Loss and NC evolution with TRADES trained networks. Upper: ResNet18; Lower: VGG11. No simplices are formed with TRADES training. Setting: CIFAR-10, \u2113\u221eadversary. Note that we plot the KLD-loss here to showcase optimization convergence, to avoid the effect of the regularization constant \u03b2.</div>\nsmoothly as a function of the index of the layer. Further, Hui et al. (2022) coined the term Cascading Neural Collapse to describe the phenomenon of cascading variability collapse; starting from the end of the network, the collapse of one layer seemed to be signaling the collapse of the previous layers (albeit to a lesser extent). Here, we replicate this study of the intermediate layer computations, while also studying the representations of the perturbed points (both in standard and adversarial training). In particular, we collect the input of either convolutional or linear layers of the network at convergence, order them by depth index, and compute the NC quantities of Section 3. The results are presented in Figure 6. Both for ST and AT models, we reproduce the power law behavior observed in He and Su (2022) for clean data; the feature variability collapses progressively, and, interestingly, undergoes a slower decrease in the case of adversarial training. The adversarial data representations for ST models, however, while failing to collapse at the final layer (as already established in Figure 2), exhibit the same extent of Neural Collapse as those of the original data for the earlier layers. This hints that from the viewpoint of the earlier layers, clean and adversarial data are indistinguishable. And, this, is indeed the case! Looking at the first and third column of Figure 7, we observe that the simple classifier formed by the centers of the early layers is quite robust (\u223c40%) to these adversarial examples (both train and test). Curiously, this robustness is higher than the one of the simple classifiers defined by layers of an adversarially trained model (although the two numbers are not directly comparable). This is, undeniably, a peculiar phenomenon of standardly trained models that is worth more exploration; could it be that the lesser variability exhibited in the earlier layers is actually beneficial for robustness or is it just the stability of the feature space that makes prediction more robust?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1bb9/1bb9868c-197f-4ad7-b0c8-ca15471f9e82.png\" style=\"width: 50%;\"></div>\nFigure 6: Layerwise evolution of NC1, NC2 and NC4 for ST and AT networks. NC metrics for perturbed data tend to undergo some amount of clustering in the earlier layers. For AT, collapse undergoes a slower decrease through layers than for ST. Setting: CIFAR-10, \u2113\u221eadversary.\n<div style=\"text-align: center;\">Figure 6: Layerwise evolution of NC1, NC2 and NC4 for ST and AT networks. NC metrics for perturbed data tend to undergo some amount of clustering in the earlier layers. For AT, collapse undergoes a slower decrease through layers than for ST. Setting: CIFAR-10, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/95b5/95b5f15f-6b91-4353-aaf6-733b573b9236.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">perturbed NCC-Net Matching Rate</div>\nFigure 7: Layerwise NCC classifier. We measure the performance of the NCC classifier obtained from (training) class means on both train and test data. NCC Robustness refers to NCC Accuracy on perturbed data. Note that, on training data, the NCC Robustness and the perturbed NCC-Net Matching Rate curves overlap. Early layers give a surprisingly robust NCC classifier (NCC Robustness) for both train and test data. Setting: CIFAR-10, \u2113\u221eadversary.\n# 5 Conclusion\nNeural Collapse is an interesting phenomenon displayed by classification Neural Networks. We present experiments that quantify the sensitivity of this geometric arrangement to input perturbations, and, further, display that Neural Collapse can appear (but not always does!) in Neural Networks trained to be robust Specifically, we find Adversarial Training (Madry et al., 2018) induces severe Neural Collapse not only on clean data but also for perturbed data, while TRADES (Zhang et al., 2019) leads to no Neural Collapse at all. Interestingly, simple nearest-neighbors classifiers defined by feature representations (either final or earlier ones) from either standardly or adversarially trained Neural Networks can exhibit remarkable accuracy and\nrobustness, suggesting robustness is maintained in early layers for both situations, while it diminishes quickly across layers for standardly trained networks. We conclude that Neural Collapse is an elegant phenomenon, prevalent in many deep learning settings, with yet unclear connection to the generalization and robustness of Neural Networks: it is not necessary to appear in adversarially-robust optimization. Our findings on robust networks call for a theoretical analysis of Neural Collapse, possibly moving beyond the unconstrained feature model, which by construction doesn\u2019t seem to be able to reason about perturbations in the data.\n# Acknowledgments\nThis work was supported by the National Science Foundation under NSF Award 1922658, the Dean\u2019s Undergraduate Research Fund from the NYU College of Arts and Science, and in part through the NYU IT High Performance Computing resources, services, and staff expertise.\n# References\nIdo Ben-Shaul and Shai Dekel. Nearest class-center simplification through intermediate layers. CoRR, abs/2201.08924, 2022.\nNicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 39\u201357. IEEE Computer Society, 2017.\nFrancesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Weinan E and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, volume 145 of Proceedings of Machine Learning Research, pages 270\u2013290. PMLR, 16\u201319 Aug 2022. Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences of the United States of America, 118, 2021. Tomer Galanti, Andr\u00e1s Gy\u00f6rgy, and Marcus Hutter. On the role of neural collapse in transfer learning. CoRR, abs/2112.15121, 2021. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=w1UbdvWH_R3. Hangfeng He and Weijie J. Su. A law of data separation in deep learning. 2022. Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding generalization in deep learning. CoRR, abs/2202.08384, 2022. Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie J Su. An unconstrained layer-peeled perspective on neural collapse. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=WZ3yjh8coDg.\nFrancesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Weinan E and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, volume 145 of Proceedings of Machine Learning Research, pages 270\u2013290. PMLR, 16\u201319 Aug 2022. Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences of the United States of America, 118, 2021. Tomer Galanti, Andr\u00e1s Gy\u00f6rgy, and Marcus Hutter. On the role of neural collapse in transfer learning. CoRR, abs/2112.15121, 2021. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=w1UbdvWH_R3. Hangfeng He and Weijie J. Su. A law of data separation in deep learning. 2022. Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding generalization in deep learning. CoRR, abs/2202.08384, 2022. Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie J Su. An unconstrained layer-peeled perspective on neural collapse. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=WZ3yjh8coDg.\nAlexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings, 2017. Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, and Qing Qu. Principled and efficient transfer learning of deep models via neural collapse. arXiv preprint arXiv:2212.12206, 2022. Yan Li, Ethan X. Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial training on separable data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Bochen Lv and Zhanxing Zhu. Implicit bias of adversarial training for deep neural networks. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. In International Conference on Learning Representations, 2018. Dustin G. Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. Sampling Theory, Signal Processing, and Data Analysis, 20:11, 2022. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1765\u20131773, 2017. Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6614. Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Ramesh Karri, Ozgur Sinanoglu, AhmadReza Sadeghi, and Xun Yi, editors, Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April 2-6, 2017, pages 506\u2013519. ACM, 2017. Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u201324663, 2020. Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso Poggio. Feature learning in deep classifiers through intermediate neural collapse. Technical report, Center for Brains, Minds and Machines (CBMM), 2023. Leslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robust deep learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 8093\u20138104. PMLR, 2020a. Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In International Conference on Machine Learning, pages 8093\u20138104. PMLR, 2020b. Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In Douglas H. Fisher, editor, Proceedings of the Fourteenth International Conference on Machine Learning (ICML 1997), Nashville, Tennessee, USA, July 8-12, 1997, pages 322\u2013330. Morgan Kaufmann, 1997.\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3353\u20133364, 2019. Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. J. Mach. Learn. Res., 19:70:1\u201370:57, 2018. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks, 2014. Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 21478\u201321505. PMLR, 17\u201323 Jul 2022. Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed. Perturbation analysis of neural collapse. arXiv preprint arXiv:2210.16658, 2022. Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. J. Mach. Learn. Res., 10:1485\u20131510, 2009. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 7472\u20137482. PMLR, 2019. Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under MSE loss: Global optimality with unconstrained features. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27179\u201327202. PMLR, 17\u201323 Jul 2022. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n# Appendix\n# A Exact definitions of NC metrics\nSimplex ETF: A standard simplex ETF composed of C points is a set of points in RC, each point b to a column of \ufffd 1\nSimplex ETF: A standard simplex ETF composed of C points is a set of points in RC, each point belonging to a column of\nwhere I \u2208RC\u00d7C is the identity matrix and 1C = \ufffd1 \u00b7 \u00b7 \u00b7 1\ufffd\u22a4\u2208RC is the all-ones vector. In our discussion, a simplex can be thought of as a standard simplex ETF up to partial rotations, reflections, and rescaling. Between-class and within-class covariance: Using terminology developed in Section 3, we define between-class covariance \u03a3B \u2208Rp\u00d7p as \u03a3B \u225cAVGc(\u00b5c \u2212\u00b5G)(\u00b5c \u2212\u00b5G)\u22a4 and \u03a3W \u2208Rp\u00d7p as\nand \u03a3W \u2208Rp\u00d7p as\n(NC1) Variability Collapse:\nwhere \u2020 denotes the Moore-Penrose inverse. The NC1 curve corresponds to Tr(\u03a3\u2020 B\u03a3W). (NC2) Convergence to Simplex ETF: 1\n  The NC2 Equinorm curve corresponds to the variation of ||\u00b5c\u2212\u00b5G||2 across all labels c, the standard deviation of these c quantities: std(||\u00b5c \u2212\u00b5G||2). The NC2 Equiangular curve corresponds to AVGc\u0338=c\u2032 abs(\u27e8\u02dc\u00b5c, \u02dc\u00b5c\u2032\u27e9+ 1 C\u22121), where abs is the absolute value operator. (NC3) Convergence to self-duality: w\n# (NC4) Simplification to NCC classifier:\n(NC4) Simplification to NCC classifier:\nThe NC4 curve corresponds to the mismatch ratio of these two quantities. NCC-Network Matching Rate:\narg max c\u2032 \u27e8wc\u2032, h\u27e9+ bc\u2032 ?= arg min c\u2032 \u2225h \u2212\u00b5c\u2032\u22252, h \u2208H(S\u2032).  \nThis quantity matches NC4 when S\u2032 = S, where S is the dataset that the classifier was trained on and S\u2032 is the dataset of interest. When we study the presence of Neural Collapse in intermediate layers of an already converged network, we first collect its intermediate representations at this layer. The centers of these representations can be used to form a simple NCC classifier. This allows us to compute the alignment of the predictions of the intermediate-layer NCC and the entire network over S\u2032. In our experiments, we calculate the NC statistics with the code provided by Han et al. (2022)3.\n# B Experimental Details\nCode. For \u2113\u221eand \u21132 PGD attacks with ST and AT, we used the code from Rice et al. (2020a) 4. For TRADES, we adopted the original implementation5. For the 160px ImageNette, all original images have the shortest side resized to 160px. To fulfill the requirement of NC, we use the CenterCrop of 160 to fix the train and test set. We have attached the code for reproducing NC results with ST, AT, and TRADES within a zip file. Plotting. Throughout our paper, we plot all quantities per 5 epochs in all figures. Layerwise NC. We study the layerwise NC1, NC2 and NC4 quantities for both PreActResNet18 (ResNet18) and VGG11. With ResNet18, which consists of one convolutional layer, four residual blocks, and the final linear layer, we use the features after every block for the first five blocks (one convolutional layer and four residual blocks) as representations. With VGG, which consists of eight convolutional blocks (convolutional layer +\nCode. For \u2113\u221eand \u21132 PGD attacks with ST and AT, we used the code from Rice et al. (2020a) 4. For TRADES, we adopted the original implementation5. For the 160px ImageNette, all original images have the shortest side resized to 160px. To fulfill the requirement of NC, we use the CenterCrop of 160 to fix the train and test set. We have attached the code for reproducing NC results with ST, AT, and TRADES within a zip file.\nCode. For \u2113\u221eand \u21132 PGD attacks with ST and AT, we used the code from Rice et al. (2020a) 4. For TRADES, we adopted the original implementation5. For the 160px ImageNette, all original images have the shortest side resized to 160px. To fulfill the requirement of NC, we use the CenterCrop of 160 to fix the train and test set. We have attached the code for reproducing NC results with ST, AT, and TRADES within a zip file. Plotting. Throughout our paper, we plot all quantities per 5 epochs in all figures. Layerwise NC. We study the layerwise NC1, NC2 and NC4 quantities for both PreActResNet18 (ResNet18)\nshortest side resized to 160px. To fulfill the requirement of NC, we use the CenterCrop of 160 to fix the train and test set. We have attached the code for reproducing NC results with ST, AT, and TRADES within a zip file. Plotting. Throughout our paper, we plot all quantities per 5 epochs in all figures. Layerwise NC. We study the layerwise NC1, NC2 and NC4 quantities for both PreActResNet18 (ResNet18) and VGG11. With ResNet18, which consists of one convolutional layer, four residual blocks, and the final linear layer, we use the features after every block for the first five blocks (one convolutional layer and four residual blocks) as representations. With VGG, which consists of eight convolutional blocks (convolutional layer + batch-normalization + max-pooling) and the final linear layer, we use the features after each convolutional block as representations. We apply average-pooling subsampling on representations that are too large for feasible computation of NC1\u2019s pseudo-inverse.\n# C Complementary Results on CIFAR-10, \u21132 adversary.\nHere we complement our main text with robust network experiments on CIFAR-10 for \u21132 adversarial perturbations.\nHere we complement our main text with robust network experiments on CIFAR-10 for \u21132 adversarial perturbations.\nHere we complement our main text with robust network experiments on CIFAR-10 for \u21132 adversaria perturbations. Figure 8 illustrates NC results of Adversarial Training and TRADES training with the \u21132 adversary. All plots are consistent with our findings in the main text: Adversarial Training alters Neural Collapse such that the clean representation simplex overlaps with the perturbed representation simplex, whereas TRADES does not\nHere we complement our main text with robust network experiments on CIFAR-10  perturbations.\n  perturbations. Figure 8 illustrates NC results of Adversarial Training and TRADES training with the \u21132 adversary. All plots are consistent with our findings in the main text: Adversarial Training alters Neural Collapse such that the clean representation simplex overlaps with the perturbed representation simplex, whereas TRADES does not lead to any simplex ETF.\nFigure 8 illustrates NC results of Adversarial Training and TRADES training with the \u21132 adversary. All plots are consistent with our findings in the main text: Adversarial Training alters Neural Collapse such that the clean representation simplex overlaps with the perturbed representation simplex, whereas TRADES does not lead to any simplex ETF.\n# D Complementary Results on CIFAR-100\nIn this section, we reproduce our experiments on CIFAR-100. We illustrate results with (\u2113\u221e, \u21132) adversaries and obtain the same conclusions as those on CIFAR-10. This suggests the universality of the intrinsic adversarial perturbation dynamics that we have detailed in the main text.\n# D.1 CIFAR-100 \u2113\u221eStandard and Adversarial Training Results\nAll results are summarized within Figure 9. Similar to the main text, we plot the untargeted attack illustration in Figure 10. Notably, on CIFAR-100 with ST, adversarial perturbations also push the representation to leap toward the predicted class\u2019s simplex cluster with very small angular deviation.\n# D.2 CIFAR-100 \u2113\u221eTRADES Results\nFor CIFAR-100 \u2113\u221etrained with TRADES, Figure 11 depicts the results, and we observe that no simplex exists, consistent with previous results.\n# D.3 CIFAR-100 \u21132 AT and TRADES Results\nre shown in Figure 12. All observations are consistent with previou\nThese results are shown in Figure 12. All observations are consistent with previous results.\n4https://github.com/locuslab/robust_overfitting 5https://github.com/yaodongyu/TRADES\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33d4/33d4e202-0ef4-43b8-828a-c0d9eb13956c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">robust (perturbed) cla</div>\n<div style=\"text-align: center;\">clean robust (perturbed) classifie</div>\n<div style=\"text-align: center;\">clean robust (perturbed)</div>\n<div style=\"text-align: center;\">robust (perturbed)</div>\nFigure 8: Accuracy, Loss and NC evolution with adversarially trained and TRADES trained networks. Settin CIFAR-10, \u21132 adversary.\n# D.4 CIFAR-100 Simplex Similarity Results\nThe Simplex Similarity and non-centered Angular Distance of the simplices formed by targeted adversarial and clean examples with ST, and the simplices generated by clean and perturbed examples with AT, are depicted in Figure 13. The result is the same as the one for CIFAR-10 in the main text, Figure 4.\n# D.5 CIFAR-100 Layerwise Results\nHere in Figure 14 and Figure 15, we perform the same computations as those in Figure 6 and Figure 7. We arrive at the same conclusions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68a8/68a806bf-0a5c-440d-a6c6-3caa4ecd582b.png\" style=\"width: 50%;\"></div>\nFigure 9: Accuracy, Loss and NC evolution with standardly trained networks. Setting: CIFAR-100, adversary.\n# E Small Epsilon Results\nHere, we illustrate how AT indeed progressively induces more robust NC metrics and simplex ETFs wit respect to the perturbation radius \u03f5. Figure 16 shows the NC metrics over 8/255\u2212perturbed data. Conversel using an ST model, the NC metrics when evaluating on (2/255, 4/255, 8/255)\u2212perturbed data also increase monotonically with adversarial strength. This is illustrated in Figure 17.6\n# F ImageNette Results\nTo further corroborate our experimental conclusions, we append ImageNette results here. Results ar illustrated in Figure 18, 19, 20 and 21.\n6For small radius AT and small radius adversarial attack for ST, we scale the PGD step size \u03b1 linearly with \u03f5 t to work properly.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e162/e162af65-6269-4c74-be4a-baddf2e45e2b.png\" style=\"width: 50%;\"></div>\nFigure 10: Illustration of untargeted adversarial attacks on standardly trained, converged, models that correspond to one random seed. (CIFAR-100, \u2113\u221e). Left: Number of examples with a certain predicted label. Inner Left: The norms of clean class-means. Inner Right: The norms of predicted class-means with perturbed data. Right: Angular distance between clean and predicted class-mean with perturbed data. Upper: ResNet18; Lower: VGG11. For 100 classes, the between-class angular distance is arccos (\u22121 99) = 1.58 rad = 90.58 degrees, while 0.2 rad is only 11.4 degrees.\n# G Test Set Neural Collapse Results\nFor the completeness of our investigation, we illustrate the test set loss, accuracy, and NC evolution in Figures 22, 23, 24, 25, 26 and 27. We put TRADES and AT results together for a straightforward comparison between these two robust optimization algorithms. Still, TRADES leads to a non-collapsed dynamic, and clearly, representations on perturbed data of robust networks do not form a simplex ETF anymore, because the robust test accuracy is less than 40%. An interesting observation is that though both robust optimization algorithms exhibit significant robust overfitting (Rice et al., 2020b), AT\u2019s robust accuracy increases in the final stage, with an increment in the NC2 quantity.\nResNet18, TRADES\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d070/d070e6e9-1c2d-4d46-9d14-c8abda74ac08.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">robust (perturbed) c</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55c0/55c0290e-ab9f-40a1-9017-b06ba81e9632.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">clean robust (perturbed</div>\n<div style=\"text-align: center;\">clean robust (perturbed) classifie</div>\n<div style=\"text-align: center;\">robust (perturbed)</div>\n<div style=\"text-align: center;\">ean robust (perturbed)</div>\nFigure 12: Accuracy, Loss and NC evolution with \u21132 robust models on CIFAR-100. Setting: CIFAR-100, \u21132 adversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/39e2/39e2493b-5e9f-4a1a-88ad-d09c863b9e8e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Angular distance. Left and Inner Left: Average between targeted attack class-means and clean class-means on ST network. Inner Right and Right: Average between perturbed class-means and clean class-means on AT network. Setting: CIFAR-100, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aabc/aabc919f-74d2-48c0-915b-fad617ba9a6b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">clean</div>\nFigure 14: Layerwise evolution of NC1, NC2 and NC4 for ST and AT networks. NC metrics for perturbe data tend to undergo some amount of clustering in the earlier layers. For AT, collapse undergoes a slowe decrease through layers than for ST. Setting: CIFAR-100, \u2113\u221eadversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d2b6/d2b62577-b637-49a1-8f37-d1e17a4b218b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">rturbed NCC-Net Matching Rate NCC Robustness</div>\n<div style=\"text-align: center;\">perturbed NCC-Net Matching Rate</div>\nFigure 15: Layerwise NCC classifier. We measure the performance of the NCC classifier obtained from (training) class means on both train and test data. NCC Robustness refers to NCC Accuracy on perturbed data. Note that, on training data, the NCC Robustness and the perturbed NCC-Net Matching Rate curves overlap. Early layers give a surprisingly robust NCC classifier (NCC Robustness) for both train and test data. Setting: CIFAR-100, \u2113\u221eadversary.\n<div style=\"text-align: center;\">robust (perturbed)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5263/5263baf8-9d71-4c2f-95d1-c54f8c0dae92.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: Progressive NC evolution, AT with varying strength. The color indicates the epsilon used fo training. Setting: CIFAR-10, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f13/7f13ba55-c28b-4482-8ca1-1eb689f0c931.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 17: Progressive NC evolution, ST with varying attacking strength. The color indicates the epsilon used for evaluation. Setting: CIFAR-10, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66a3/66a3d8f6-7fd0-4f28-8015-9e19432cd460.png\" style=\"width: 50%;\"></div>\nFigure 18: Accuracy, Loss and NC evolution with standardly-trained and adversariall Setting: ImageNette, \u2113\u221eadversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0283/028388f3-ae76-45d3-a8de-fe81d8cc7d2a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">robust (perturbed</div>\nFigure 19: Accuracy, Loss and NC evolution with TRADES trained networks. Upper: ResNet18; Lower: VGG11. Results indicate AT boosts Neural Collapse so that it also happens on adversarially-perturbed data. Setting: ImageNette, \u2113\u221eadversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af34/af3448ac-560d-45fc-b6bc-617fbf9b01a5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 20: Angular distance. Left and Inner Left: Average between targeted attack class-means and clean class-means on ST network. Inner Right and Right: Average between perturbed class-means and clean class-means on AT network. Setting: ImageNette, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/90a1/90a1ec86-4d24-4d50-a1b3-d49d25ba7774.png\" style=\"width: 50%;\"></div>\nFigure 21: Accuracy, Loss and NC evolution with standardly-trained and adversarially-trained networks. Setting: ImageNette, \u21132 adversary.\nFigure 21: Accuracy, Loss and NC evolution with standardly-trained and advers Setting: ImageNette, \u21132 adversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/975e/975e86b2-daac-4c1a-96a8-aece1798781c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 22: Test set accuracy, Loss and NC evolution with standardly-trained and adversarially-trained networks. Setting: CIFAR-10, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/32ea/32eabf78-0366-4fa4-86eb-72c30acd17e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 23: Test set accuracy, Loss and NC evolution with adversarially-trained and TRADES-trained networks. Setting: CIFAR-10, \u2113\u221eadversary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2dee/2dee6d95-49d6-4a2b-934a-79aa4ed4bdf5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 24: Test set accuracy, Loss and NC evolution with standardly-trained and adversarially-trained networks. Setting: CIFAR-100, \u2113\u221eadversary.</div>\nFigure 24: Test set accuracy, Loss and NC evolution with standardly-t networks. Setting: CIFAR-100, \u2113\u221eadversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/359d/359db583-741b-4f3d-a113-46c9116f6cca.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">clean robust (perturbed)</div>\nFigure 25: Test set accuracy, Loss and NC evolution with adversarially-trained and TRADES-trained networks. Setting: CIFAR-100, \u2113\u221eadversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48ae/48ae007e-d981-4aa7-95eb-7ea1a3393a79.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 26: Test set accuracy, Loss and NC evolution with standardly-trained and adversarially-trained networks. Setting: ImageNette, \u2113\u221eadversary.</div>\nFigure 26: Test set accuracy, Loss and NC evolution with standardly-t networks. Setting: ImageNette, \u2113\u221eadversary.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c5f/4c5f84a5-0bda-4cc6-a5d4-01759ca93922.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 27: Test set accuracy, Loss and NC evolution with adversarially-trained and TRADES-trained networks. Setting: ImageNette, \u2113\u221eadversary.</div>\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the phenomenon of Neural Collapse in neural networks, where feature vectors and classification weights converge to a simple geometric arrangement known as a simplex. The connection between Neural Collapse and properties such as generalization and robustness of neural networks is explored, particularly in the context of adversarial robustness.",
        "problem": {
            "definition": "The problem centers around understanding how Neural Collapse interacts with adversarial robustness in neural networks, specifically whether the geometric properties of the feature representations are maintained or lost under adversarial attacks.",
            "key obstacle": "A significant challenge is the fragility of the simplex structure when faced with adversarial perturbations, which raises questions about the stability of Neural Collapse."
        },
        "idea": {
            "intuition": "The idea was inspired by empirical observations of the geometric structures formed by neural networks during training and their implications for robustness against adversarial attacks.",
            "opinion": "The authors propose that Neural Collapse can provide insights into the robustness of neural networks, particularly in adversarial settings.",
            "innovation": "The main innovation is the identification of the relationship between the stability of simplices formed during Neural Collapse and the robustness of the neural networks against adversarial attacks."
        },
        "Theory": {
            "perspective": "The theoretical perspective focuses on the geometric properties of feature representations and classification weights in neural networks, particularly during the terminal phase of training.",
            "opinion": "The authors assume that the geometric arrangements of feature representations can inform us about the robustness of the model.",
            "proof": "The proof is derived from empirical results demonstrating that adversarial perturbations disrupt the simplex structure, indicating the fragility of Neural Collapse."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on image classification tasks using CIFAR-10, CIFAR-100, and a subset of ImageNet, employing standard and adversarial training methods.",
            "evaluation method": "The evaluation involved measuring Neural Collapse metrics across different training epochs and under various perturbation settings, including adversarial and Gaussian noise."
        },
        "conclusion": "The experiments conclude that Neural Collapse is sensitive to input perturbations, with adversarial training inducing significant collapse in both clean and perturbed data, while TRADES does not lead to Neural Collapse.",
        "discussion": {
            "advantage": "The paper highlights the robustness of nearest-neighbor classifiers derived from feature representations, suggesting that early layers maintain robustness even under adversarial conditions.",
            "limitation": "A limitation noted is that not all robust training methods exhibit Neural Collapse, indicating that its presence is not universally necessary for robustness.",
            "future work": "Future work should explore theoretical frameworks for understanding Neural Collapse in the context of adversarial robustness, potentially extending beyond current models."
        },
        "other info": [
            {
                "info1": "The authors provide code for reproducing their results, available at https://github.com/JingtongSu/robust_neural_collapse."
            },
            {
                "info2": {
                    "info2.1": "The study includes a comprehensive analysis of the propagation of representations within the network.",
                    "info2.2": "The findings suggest that earlier layers of the network exhibit more reliable simplices compared to later layers."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.4",
            "key information": "The paper explores the connection between Neural Collapse and adversarial robustness, emphasizing how the geometric properties of feature representations are affected by adversarial attacks."
        },
        {
            "section number": "5.2",
            "key information": "The paper discusses how adversarial perturbations disrupt the simplex structure formed during Neural Collapse, indicating the fragility of neural networks under adversarial conditions."
        },
        {
            "section number": "6.1",
            "key information": "The evaluation method in the experiments involved measuring Neural Collapse metrics under various perturbation settings, including adversarial and Gaussian noise, highlighting the importance of quantifying uncertainty in the context of robustness."
        },
        {
            "section number": "7.1",
            "key information": "A significant challenge identified is the fragility of the simplex structure in Neural Collapse when faced with adversarial perturbations, raising questions about the stability of the model."
        },
        {
            "section number": "7.2",
            "key information": "Future work should explore theoretical frameworks for understanding Neural Collapse in the context of adversarial robustness, indicating potential advancements in this area."
        }
    ],
    "similarity_score": 0.6582252052178306,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/On the Robustness of Neural Collapse and the Neural Collapse of Robustness.json"
}