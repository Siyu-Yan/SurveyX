{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2306.04120",
    "title": "MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation",
    "abstract": "We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. However, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost.",
    "bib_name": "tohme2024messyestimationmaximumentropybased",
    "md_text": "Published in Transactions on Machine Learning Research (01/2024)\n# MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation\nTony Tohme\u2217 Massachusetts Institute of Technology, USA.\nMohsen Sadr\u2217 Massachusetts Institute of Technology, USA. Paul Scherrer Institute, Switzerland.\n10 Feb 2024\nKamal Youcef-Toumi Massachusetts Institute of Technology, USA.\nNicolas G. Hadjiconstantinou Massachusetts Institute of Technology, USA.\n# Abstract\nWe introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. However, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost.\n# 1 Introduction\nRecovering probability density functions from samples is one of the fundamental problems in statistics with many applications. For example, the traditional task of discovering the underlying dynamics governing the corresponding distribution function is strongly dependent on the quality of the density estimator (Rudy et al., 2017). Applications include particle physics (Patrignani et al., 2016), boundary conditions for multi-scale kinetic problems (Frezzotti et al., 2005; Kon et al., 2014), and machine learning (Song et al., 2020).\n\u2217Equal contribution.\nngh@mit.edu\nBroadly speaking, two categories of methods have been developed for this task: parametric and nonparametric estimators. While parametric methods assume a restrictive ansatz for the underlying distribution function, non-parametric methods provide a more flexible density estimate by performing a kernel integration locally using nearby samples. Although non-parametric methods do not need any prior knowledge of the underlying distribution, they suffer from the unclear choice of kernel and its support leading to bias and lack of moment matching. Examples of non-parametric density estimators include histogram and Kernel Density Estimation (KDE) (Rosenblatt, 1956; Jones et al., 1996; Sheather, 2004). On the other hand, parametric density estimators may allow matching of moments while introducing modeling error, since a guess for the distribution is required. Parametric distributions include Gaussian, orthogonal expansion with respect to Gaussian using Hermite polynomials (also known as Grad\u2019s ansatz in kinetic theory) (Hermite, 1864; Grad, 1949; Cai et al., 2015), wavelet density estimation (Donoho et al., 1996), and Maximum Entropy Distribution (MED) (Kapur, 1989; Tagliani, 1999; Khinchin, 2013; Hauck et al., 2008) function among others. Given only the mean and variance, information theory provides us with the Gaussian distribution function as the least biased density, which has been used extensively in the literature. However, including higher order moments in a similar way, i.e. moment problem, raises further complications. For example in the context of kinetic theory, Grad proposed a closure that incorporates higher-order moments by considering a deviation from Gaussian using Hermite polynomials. Even though the information from higher moments is incorporated as the parameters of the polynomial expansion in Grad\u2019s ansatz, such a formulation suffers from not guaranteeing positivity of the estimated density along with the introduction of bias. Among parametric density estimators, the Maximum Entropy Distribution (MED) function has been proposed in information theory as the least biased density estimate given a number of moments of the unknown distribution (Kapur, 1989). While MED provides the least biased density estimate, it suffers from two limitations. First, the distribution parameters (Lagrange multipliers) can only be found by solving a convex optimization problem with ill-conditioned Hessian (Dreyer, 1987; Levermore, 1996). The condition number increases either by increasing the order of the matching moments or approaching the limit of physical realizability which motivated the use of adaptive basis functions (Abramov, 2007; 2009). Second, MED only exists and is unique in bounded domains. While existence/uniqueness is guaranteed for recovering the distribution in the subspace occupied by the samples, the computational complexity associated with the direct computation of Lagrange multipliers has prevented researchers from deploying MED in practice. Related methods. The problem of recovering a distribution function from samples has been investigated and studied before. We briefly review some of the work most relevant to our paper: Data-driven maximum entropy distribution function: Several attempts have been made in the literature to speed up the computation of Lagrange multipliers for MED using Neural Networks (Sadr et al., 2021; Porteous et al., 2021; Schotth\u00f6fer et al., 2022) and Gaussian process regression (Sadr et al., 2020). Unfortunately, these approaches are data-dependent with support only on the trained subspace of distributions. Similar to the standard MED and other related closures, the data-driven MED can only handle polynomial moments as input, even though the data may be better represented with moments of other basis functions. Learning an invertible map: The idea is to train an invertible neural network that maps the samples to a known distribution function. Then the unknown distribution function is found by inverting the trained map with the known distribution as the input. This procedure is called the normalizing flow technique (Rezende & Mohamed, 2015; Dinh et al., 2016; Kingma & Dhariwal, 2018; Durkan et al., 2019; Tzen & Raginsky, 2019; Kobyzev et al., 2020; Wang & Marzouk, 2022). This method has been used for re-sampling unknown distributions, e.g. Boltzmann generators (No\u00e9 et al., 2019), as well as density recovery such as AI-Feynmann (Udrescu & Tegmark, 2020; Udrescu et al., 2020). We note that AI-Feynman does not obtain the density from the samples directly; instead it first fits a density to the samples using the normalizing flow technique, constructs an input/output data set, then finds a simpler expression using symbolic regression. While invertible maps can be used to accurately predict densities, they can become expensive since for each problem one has to learn the parameters of the considered map via optimization.\nBroadly speaking, two categories of methods have been developed for this task: parametric and nonparametric estimators. While parametric methods assume a restrictive ansatz for the underlying distribution function, non-parametric methods provide a more flexible density estimate by performing a kernel integration locally using nearby samples. Although non-parametric methods do not need any prior knowledge of the underlying distribution, they suffer from the unclear choice of kernel and its support leading to bias and lack of moment matching. Examples of non-parametric density estimators include histogram and Kernel Density Estimation (KDE) (Rosenblatt, 1956; Jones et al., 1996; Sheather, 2004). On the other hand, parametric density estimators may allow matching of moments while introducing modeling error, since a guess for the distribution is required. Parametric distributions include Gaussian, orthogonal expansion with respect to Gaussian using Hermite polynomials (also known as Grad\u2019s ansatz in kinetic theory) (Hermite, 1864; Grad, 1949; Cai et al., 2015), wavelet density estimation (Donoho et al., 1996), and Maximum Entropy Distribution (MED) (Kapur, 1989; Tagliani, 1999; Khinchin, 2013; Hauck et al., 2008) function among others. Given only the mean and variance, information theory provides us with the Gaussian distribution function as the least biased density, which has been used extensively in the literature. However, including higher order moments in a similar way, i.e. moment problem, raises further complications. For example in the context of kinetic theory, Grad proposed a closure that incorporates higher-order moments by considering a deviation from Gaussian using Hermite polynomials. Even though the information from higher moments is incorporated as the parameters of the polynomial expansion in Grad\u2019s ansatz, such a formulation suffers from not guaranteeing positivity of the estimated density along with the introduction of bias. Among parametric density estimators, the Maximum Entropy Distribution (MED) function has been proposed in information theory as the least biased density estimate given a number of moments of the unknown distribution (Kapur, 1989). While MED provides the least biased density estimate, it suffers from two limitations. First, the distribution parameters (Lagrange multipliers) can only be found by solving a convex optimization problem with ill-conditioned Hessian (Dreyer, 1987; Levermore, 1996). The condition number increases either by increasing the order of the matching moments or approaching the limit of physical realizability which motivated the use of adaptive basis functions (Abramov, 2007; 2009). Second, MED only exists and is unique in bounded domains. While existence/uniqueness is guaranteed for recovering the distribution in the subspace occupied by the samples, the computational complexity associated with the direct computation of Lagrange multipliers has prevented researchers from deploying MED in practice.\nLearning an invertible map: The idea is to train an invertible neural network that maps the samples to a known distribution function. Then the unknown distribution function is found by inverting the trained map with the known distribution as the input. This procedure is called the normalizing flow technique (Rezende & Mohamed, 2015; Dinh et al., 2016; Kingma & Dhariwal, 2018; Durkan et al., 2019; Tzen & Raginsky, 2019; Kobyzev et al., 2020; Wang & Marzouk, 2022). This method has been used for re-sampling unknown distributions, e.g. Boltzmann generators (No\u00e9 et al., 2019), as well as density recovery such as AI-Feynmann (Udrescu & Tegmark, 2020; Udrescu et al., 2020). We note that AI-Feynman does not obtain the density from the samples directly; instead it first fits a density to the samples using the normalizing flow technique, constructs an input/output data set, then finds a simpler expression using symbolic regression. While invertible maps can be used to accurately predict densities, they can become expensive since for each problem one has to learn the parameters of the considered map via optimization.\nDiffusion map: Instead of training for an invertible map, the diffusion map (Coifman et al., 2005; Coifman & Lafon, 2006) constructs coordinates using eigenfunctions of Markov matrices. Using pairwise distances between samples, in this method a kernel matrix is constructed as a generator of the underlying Langevin diffusion process. As shown by Li & Marzouk (2023), one can generate samples of the target distribution using Laplacian-adjusted Wasserstein gradient descent (Chewi et al., 2020). Unfortunately, this approach can become computationally expensive since it requires singular value decomposition of matrices of size equal to the number of samples. Gradient flow: The gradient flow method has gained attention in recent years (Villani, 2009; Song et al., 2020; Song & Ermon, 2020). In particular, a class of sampling methods has been devised for drawing samples from a given distribution function using Langevin dynamics with the gradient of log-density as the driving force (Liu, 2017; Garbuno-Inigo et al., 2020a;b). Yet, this approach does not provide the density of the samples by itself. In our paper, we benefit from this formulation to recover the parameters of a density ansatz. Stein Variational Gradient Descent method (SVGD): Given a target density function, the SVGD method as a deterministic and non-parameterized Gradient method generates samples of the target by carrying out a dynamic system on particles where the velocity field includes the grad-log of the target density (similar to the Gradient flow) and a kernel over particles (Liu & Wang, 2016). SVGD has been derived by approximating a kernelized Wasserstein gradient flow of KL divergence (Liu, 2017). While further steps have been taken to improve this method, e.g. SVGD with moment matching (Liu & Wang, 2018), similar to Gradient flow, this class of method cannot be used for estimating the density itself from samples. Wavelet and Conditional Renormalization Group method: One of the powerful methods in signal processing is the wavelet method (Mallat, 1999), which may be considered as an extension of the Fourier method and domain decomposition. The basic idea is to consider the data on a multiple grids/scales, where the contribution from the smallest frequencies are found on the coarsest mesh and the highest frequencies on the finest mesh. While this method has been extended to finding high dimensional probability density functions (Marchand et al., 2022; Kadkhodaie et al., 2023), it is not clear how much bias is introduced by the orthogonal wavelet bases. KDE via diffusion: In this method, the bandwidth of the kernel density estimation is computed using the minimum of mean integrated squared error and the fact that the KDE is the fundamental solution to a heat (more precisely Fokker-Planck) equation (Botev, 2007; Botev et al., 2010). While improvement has been achieved in this direction, we note that the KDE-diffusion method suffers from smoothing effects which introduce bias. Moreover moments of the unknown distribution are not necessarily matched. Symbolic regression: Symbolic regression (SR) is a challenging task in machine learning that aims to identify analytical expressions that best describe the relationship between inputs and outputs of a given dataset. SR does not require any prior knowledge about the model structure. Traditional regression methods such as least squares (Wild & Seber, 1989), likelihood-based (Edwards, 1984; Pawitan, 2001), and Bayesian regression techniques (Lee, 1997; Leonard & Hsu, 2001; Tohme et al., 2020) use a fixed parametric model structure and only optimize for model parameters. SR optimizes for model structure and parameters simultaneously and hence is thought to be NP-hard, i.e. Non-deterministic Polynomial-time hard, (Udrescu & Tegmark, 2020; Petersen et al., 2021; Virgolin & Pissis, 2022). The SR problem has gained significant attention over recent years (Orzechowski et al., 2018; La Cava et al., 2021), and several approaches have been suggested in the literature. Most methods adopt genetic algorithms (Koza & Koza, 1992; Schmidt & Lipson, 2009; Tohme et al., 2023). Lately, researchers proposed using machine learning algorithms (e.g. Bayesian optimization, nonlinear least squares, neural networks, transformers, etc.) to solve the SR problem (Sahoo et al., 2018; Jin et al., 2019; Udrescu et al., 2020; Cranmer et al., 2020; Kommenda et al., 2020; Burlacu et al., 2020; Biggio et al., 2021; Mundhenk et al., 2021; Petersen et al., 2021; Valipour et al., 2021; Zhang et al., 2022; Kamienny et al., 2022). While most SR methods are concerned with finding a map from the input to the output, very few have addressed the problem of discovering probability density functions from samples (Udrescu et al., 2020).\nOur Contributions. Our work improves the efficiency in determining the maximum entropy result for the unknown distribution. We specifically develop a new method for determining the unknown parameters (Lagrange multipliers) of this distribution without solving the optimization problem associated with this approach. This is achieved by relating the samples to the MED using Gradient flow, with the grad-log of the MED guess distribution serving as the drift. This results in a linear inverse problem for the Lagrange multipliers that is significantly easier to solve compared to the aforementioned optimization problem. We also propose a Monte Carlo search in the space of smooth functions for finding an optimal basis function for describing (the exponent of) the maximum entropy ansatz. As a selection criterion, we rate randomly created basis functions according to the condition number associated with the coefficient (Hessian) matrix of the inverse problem for the Lagrange multipliers. This helps to maintain good conditioning, which allows us to incorporate more degrees of freedom and recover the unknown density accurately. Discontinuous density functions are treated by considering only the domain supported by data and using a multi-level solution process. The paper is organized as follows. In Section 2 we review the concept of Gradient flow with grad-log of a known density as the drift. In Section 3, we show how parameters of a guess MED may be found by computing the relaxation rates of the corresponding Gradient flow. Using the maximum entropy ansatz, in Section 4 we derive a linear inverse problem for finding the Lagrange multipliers without the need for solving an optimization problem. In Section 5, we propose a symbolic regression method for finding basis functions that can be used to increase degrees of freedom while maintaining good conditioning of the problem by construction. In Section 6, we propose a generalization of the maximum entropy ansatz that allows including further degrees of freedom in a multi-level fashion. Section 7 presents the complete MESSY algorithm. In Section 8, we validate MESSY by comparing its predictions to those of benchmark density estimators in recovering distributions featuring discontinuities and bi-modality, as well as distributions close to the limit of realizability. Finally, in Section 9, we offer our conclusions and outlook.\n# 2 Gradient flow and theoretical motivation\nConsider a set of samples of a random variable X from an unknown density distribution function f(x). Let our guess for this distribution function, the \u201cansatz\u201d, be denoted by \u02c6f(x).\nInstead of constructing a non-parametric approximation of the target density numerically from samples of X (like histogram or KDE) and then calculating its difference from the guess density \u02c6f, in this work we suggest measuring the distance using transport. In particular, we use the fact that the steady-state distribution of X(t) which follows the stochastic differential equation (SDE)\n# dX = \u2207x \ufffd log \ufffd\u02c6f \ufffd\ufffd dt + \u221a 2dWt\ndX = \u2207x \ufffd log \ufffd\u02c6f \ufffd\ufffd dt + \u221a 2dWt\n\ufffd \ufffd \ufffd\ufffd is the distribution \u02c6f. Here, Wt is the standard Wiener process of dimension dim(x). We note that Eq. (1) is known as the gradient flow (or Langevin dynamics) with grad-log of density as the force. We note that this drift differs from the score-based generative model in Song et al. (2020) where the drift is a function o time, i.e. \u2207x \ufffd log( \u02c6f(t)) \ufffd .\n\ufffd \ufffd The distance of f from \u02c6f may be measured by the time required for the SDE with X(t = 0) \u223cf to reach steady state. Alternatively, one may compare the moments computed from the solution to X(t) against the input samples to measure this distance. Both these approaches are subject to numerical and statistical noise associated with the numerical scheme deployed in integrating Eq. (1). In the next section, we derive an efficient way of computing the parameters of our approximation \u02c6f based on these ideas. We also show that the transition from f to \u02c6f is monotonic.\n(1)\n# 3 Ansatz as the target density of Gradient flow\ncording to Ito\u2019s lemma (Platen & Bruti-Liberati, 2010) the transition of f to \u02c6f is governed by the Fokkernck equation\nAccording to Ito\u2019s lemma (Platen & Bruti-Liberati, 2010) the transition of f to \u02c6f is governed by the FokkerPlanck equation\nAccording to Ito\u2019s lemma (Platen & Bruti-Liberati, 2010) the transition of f to \u02c6f is governed by the Fokker Planck equation\n\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd Proposition 3.1. The distribution function f(t) governed by the Fokker-Planck Eq. (2) converges to \u02c6f as t \u2192\u221e. Furthermore, the cross entropy distance between f and \u02c6f monotonically decreases during this transition. Proof. Let us multiply both sides of Eq. (2) by log(f/ \u02c6f) and take the integral with respect to x in order to obtain the evolution of the cross-entropy S = \ufffd f log(f/ \u02c6f)dx. It follows that\n\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd Proposition 3.1. The distribution function f(t) governed by the Fokker-Planck Eq. (2) converges to \u02c6f as t \u2192\u221e. Furthermore, the cross entropy distance between f and \u02c6f monotonically decreases during this transition.\nProof. Let us multiply both sides of Eq. (2) by log(f/ \u02c6f) and take the integral with respect to x in order to obtain the evolution of the cross-entropy S = \ufffd f log(f/ \u02c6f)dx. It follows that\nHere, we use the regularity condition that f log(f/ \u02c6f)\u2207x log(f/ \u02c6f) \u21920 as |x| \u2192\u221e. Therefore, given any initial condition for f at t = 0, the cross-entropy distance between f and \u02c6f following the Fokker-Planck in Eq. (2) monotonically decreases until it reaches the steady-state with the trivial fixed point f \u2192\u02c6f as t \u2192\u221e. For details, see (Liu, 2017).\nHere, we use the regularity condition that f log(f/ \u02c6f)\u2207x log(f/ \u02c6f) \u21920 as |x| \u2192\u221e. Therefore, given any initial condition for f at t = 0, the cross-entropy distance between f and \u02c6f following the Fokker-Planck in Eq. (2) monotonically decreases until it reaches the steady-state with the trivial fixed point f \u2192\u02c6f as t \u2192\u221e. For details, see (Liu, 2017). Instead of seeking solutions of Eq. (2), our approach focuses on working with appropriate empirical moments of this equation, which can be evaluated from the available samples. As will be demonstrated below, this approach lends itself to a very effective method for determining \u02c6f. Let us denote a vector of basis functions in Rdim(x) by H(x). By multiplying both sides of Eq. (3) by H(x) and integrating with respect to x, we obtain the evolution equation for the moments, also known as the relaxation rates,\nLet us denote a vector of basis functions in Rdim(x) by H(x). By multiplying both sides of Eq. (3) by H(x) and integrating with respect to x, we obtain the evolution equation for the moments, also known as the relaxation rates,\nAssuming that the underlying density f is integrable in Rdim(x) and fH \u21920 as x \u2192\u221e, which is implied by the existence of moments, we use integration by parts to obtain\nGiven samples of f, one can compute the relaxation rates of moments represented by Eq. (6) as a measure of the difference between \u02c6f and f. These relaxation rates can be used as the gradient in the search for parameters of a given ansatz, i.e. g(t) = d \ufffd H(X(t)) \ufffd = \ufffd \u2207[H(X(t))] \u00b7 \u2207[log( \u02c6f(X(t)))] \ufffd + \ufffd \u22072 [H(X(t))] \ufffd . (7)\nGiven samples of f, one can compute the relaxation rates of moments represented by Eq. (6) as a measure of the difference between \u02c6f and f. These relaxation rates can be used as the gradient in the search for parameters of a given ansatz, i.e.\ng(t) = d dt \ufffd H(X(t)) \ufffd = \ufffd \u2207x[H(X(t))] \u00b7 \u2207x[log( \u02c6f(X(t)))] \ufffd + \ufffd \u22072 x[H(X(t))] \ufffd .\n(2) (3)\n(3)\n(4)\n(5)\n(6)\n(7)\nIn the above, \u27e8\u03d5(X)\u27e9denotes the unbiased empirical measure for the expectation of \u03d5(X) which is computed using samples of Xi, for i = 1, ..., N via \u27e8\u03d5(X)\u27e9= 1 N \ufffdN i=1 \u03d5(Xi). In what follows we develop an approach that uses g(t) as the gradient of an optimization problem to bring computational benefits to the solution of the maximum entropy problem.\nIn the above, \u27e8\u03d5(X)\u27e9denotes the unbiased empirical measure for the expectation of \u03d5(X) which is computed using samples of Xi, for i = 1, ..., N via \u27e8\u03d5(X)\u27e9= 1 N \ufffdN i=1 \u03d5(Xi).\nIn this work, we use the maximum entropy distribution function as our parameterized ansatz for \u02c6f, i.e.\n\u02c6f(x) = Z\u22121 exp \ufffd \u03bb \u00b7 H(x) \ufffd\n\ufffd \ufffd where Z = \ufffd exp(\u03bb \u00b7 H(x))dx is the normalization constant. The motivation for choosing this family of distributions is the fact that this is the least-biased distribution for the moment problem, provided the given moments are matched. Definition 4.1. Moment problem The problem of finding a distribution function f(x) given its moments \ufffd H(x)f(x)dx = \u00b5 for the vector of basis functions H(x) will be referred to as the moment problem.\n\u02c6f(x) = arg min F\u2208K C[F(x)]\nF\u2208K where C[F(x)] := \ufffd F(x) log(F(x))dx \u2212 Nb \ufffd i=1 \u03bbi \ufffd\ufffd Hi(x)F(x)dx \u2212\u00b5i(x) \ufffd .\nHere K denotes the space of probability density functions with measurable moments; see (Kapur, 1989) and Appendix A for more details. In this paper, we denote the number of considered basis functions by Nb, while Nm denotes the highest order of these basis functions. For instance, in the case of traditional one-dimensional random variable where polynomial basis functions are deployed, i.e. H = \ufffd x, x2, ..., xNm\ufffd , we have Nm = Nb. Here, we use the following definition for the growth rate of a basis function. Definition 4.2. Growth rate of n-th order A function \u03c8(x) has the growth-rate of n-th order if |\u03c8(x)| \u2264Cxn for all x \u2265x0 where C \u2208R+ and x0 \u2208R. This is often denoted by \u03c8(x) = O(xn). We note that the moment problem for the MED is reduced to the following optimization problem by substituting the extremum 8 back in the objective functional equation 10, see e.g. Abramov (2006). Definition 4.3. Standard dual optimization problem of Maximum entropy distribution function Given moments \u00b5, the Lagrange multipliers \u03bb of the maximum entropy distribution are the solution to the following unconstrained optimization problem\nHere K denotes the space of probability density functions with measurable moments; see (Kapur, 1989) and Appendix A for more details. In this paper, we denote the number of considered basis functions by Nb, while Nm denotes the highest order of these basis functions. For instance, in the case of traditional one-dimensional random variable where polynomial basis functions are deployed, i.e. H = \ufffd x, x2, ..., xNm\ufffd , we have Nm = Nb. Here, we use the following definition for the growth rate of a basis function. Definition 4.2. Growth rate of n-th order A function \u03c8(x) has the growth-rate of n-th order if |\u03c8(x)| \u2264Cxn for all x \u2265x0 where C \u2208R+ and x0 \u2208R. This is often denoted by \u03c8(x) = O(xn).\nHere K denotes the space of probability density functions with measurable moments; see (Kapur, 1989) and Appendix A for more details. In this paper, we denote the number of considered basis functions by Nb, while Nm denotes the highest order of these basis functions. For instance, in the case of traditional one-dimensional random variable where polynomial basis functions are deployed, i.e. H = \ufffd x, x2, ..., xNm\ufffd , we have Nm = Nb. Here, we use the following definition for the growth rate of a basis function.\nWe note that the moment problem for the MED is reduced to the following optimization problem by substituting the extremum 8 back in the objective functional equation 10, see e.g. Abramov (2006). Definition 4.3. Standard dual optimization problem of Maximum entropy distribution function Given moments \u00b5, the Lagrange multipliers \u03bb of the maximum entropy distribution are the solution to the following unconstrained optimization problem\nClearly, the standard optimization problem for finding Lagrange multipliers is nonlinear and require deploying iterative methods, such as the Newton-Raphson method detailed in A. Instead, using the Gradient flow, we find an alternative optimization problem for finding Lagrange multipliers which is linear and simple to compute from given samples.\n(8)\n(9)\n(10)\n(11)\nSubstituting Eq. (8) for \u02c6f in Eq. (7) results in the relaxation rate\nre \u2297indicates the outer product. Let us define the matrix LME as\nDefinition 4.4. Optimization problem of Maximum entropy distribution function via Gradient flow Given samples X of the target distribution f, Lagrange multipliers \u03bb of maximum entropy distributio estimate \u02c6f is the solution to the following unconstrained optimization problem\nwhere (:) denotes the Frobenius inner (or double dot) product, i.e. A : B = \ufffd i,j AijBij for given two dimensional tensors A and B.\n \ufffd i,j   dimensional tensors A and B. We note that the matrix LME is the Hessian of the optimization problem with gradient given by Eq. (12) which is positive definite, making the underlying optimization problem convex. Proposition 4.5. The Hessian matrix LME is symmetric positive definite. As a result, the optimization problem with gradient given by Eq. (12) and Hessian matrix given by Eq. (13) is strictly convex. Proof. Clearly, the Hessian matrix defined by Eq. (13) is symmetric, i.e. LME i,j = LME j,i \u2200i, j = 1, ..., Nb. We further note that this matrix is positive definite, i.e. for any non-zero vector w \u2208RNb we can write\nGiven the Hessian is symmetric positive definite, we conclude that the underlying optimization problem is convex (Chong & Zak, 2013). When the matrix LME is well-conditioned, we can directly compute the Lagrange multipliers using samples, i.e. a linear solution to the optimization problem in Def. 4.4. This can be achieved by solving Eq. (12) for the Lagrange multipliers\nfor a given relaxation rate g.\nWe proceed by noting that a convenient way for determining the parameters of \u02c6f is to set \u02c6f = f(t = 0) in the above formulation, or in other words, require that the given samples are also samples of \u02c6f as given. This corresponds to the steady solution of Eq. (12), namely g \u21920, which implies the remarkably simple result\n(12)\n(13)\n(14)\n(15)\n(16)\n(17)\n(18)\nwhich implies a closed-form solution for the Lagrange multipliers through the above linear problem. While Eq. (18) analytically recovers the Lagrange multipliers \u03bb directly from samples of X, it still requires inverting the matrix LME which may be ill-conditioned (Abramov, 2010; Alldredge et al., 2014). This means that the resulting Lagrange multipliers may become sensitive to noise in the samples and the choice of the basis functions. In order to cope with this issue, we propose computing \u03bb as outlined below. Orthonormalizing the basis functions. We construct an orthonormal basis function with respect to X \u223cf using the modified Gram-Schmidt algorithm as described in Algorithm 1. We deploy the orthonormal basis functions from the Gram-Schmidt procedure to construct \u2207x[H]\u22a5, i.e. \u2207x[H] is the input to Algorithm 1, and by integration we obtain H\u22a5. This leads to a well-conditioned matrix LME, since the resulting matrix should be close to identity LME \u2248I with condition number cond \ufffd LME\ufffd \u22481 subject to round-off error. We note that the cost of this algorithm is quadratic with the number of basis functions and linear with the number of samples.\nWhile Eq. (18) analytically recovers the Lagrange multipliers \u03bb directly from samples of X, it still requires nverting the matrix LME which may be ill-conditioned (Abramov, 2010; Alldredge et al., 2014). This means that the resulting Lagrange multipliers may become sensitive to noise in the samples and the choice of the basis functions. In order to cope with this issue, we propose computing \u03bb as outlined below.\nAlgorithm 1: Modified Gram-Schmidt: Given a vector of basis functions \u03d5, this algorithm constructs\nan orthonormal basis functions \u03d5\u22a5with respect to f such that \u27e8\u03d5\u22a5(X)\u2297\u03d5\u22a5(X)\u27e9\u2248I using the modified\nGram-Schmidt procedure (Giraud et al., 2002; Abramov, 2010).\nInput: \u03d5\nInitialize \u03d5\u22a5\u2190\u03d5;\nfor i = 1, ..., dim(\u03d5) do\n\u03d5\u22a5\ni = \u03d5\u22a5\ni /\n\ufffd\n\u27e8(\u03d5\u22a5\ni (X))2\u27e9;\nfor j = i + 1, ..., dim(\u03d5) do\n\u03d5\u22a5\nj \u2190\u03d5\u22a5\nj \u2212\u27e8\u03d5\u22a5\ni (X)\u03d5\u22a5\nj (X)\u27e9\u03d5\u22a5\ni ;\nend\nend\nReturn \u03d5\u22a5\nAlgorithm 1: Modified Gram-Schmidt: Given a vector of basis functions \u03d5, this algorithm constructs an orthonormal basis functions \u03d5\u22a5with respect to f such that \u27e8\u03d5\u22a5(X)\u2297\u03d5\u22a5(X)\u27e9\u2248I using the modified Gram-Schmidt procedure (Giraud et al., 2002; Abramov, 2010).\n# 4.1 Comparing the proposed formulation to standard Maximum Entropy Distribution\nHere we point out several advantages of using the proposed loss function compared to the standard maximum entropy closure.\n\u2022 A closed-form solution. By setting the relaxation rate of the moments to zero, the Lagrange multipliers can be computed directly from samples X \u223cf, i.e. by solving the system 18, without the need for the line-search associated with the Newton method of solving the optimization problem of standard MED, i.e. Def. 4.3. This is a significant improvement compared to standard MED, where Lagrange multipliers are estimated iteratively \u2014 see Eq. (A.5) and Algorithm 4 in Appendix A. \u2022 Avoiding the curse of dimensionality in integration. In the proposed method, the computational complexity associated with the integration only depends on the number of samples, and not on the dimension of the probability space. The proposed method takes full advantage of having access to the samples of the unknown distribution function. This is in contrast to the standard Newton-Raphson method of solving optimization problem 4.3 where samples of the initial guessed MED corresponding to the initial guessed \u03bb is not available and one runs to the curse of dimensionality in computation of gradient and Hessian. In particular, we compute the orthonormal basis function, gradient, and Hessian using the samples of X. This use of the Monte Carlo integration method avoids the curse of high dimensionality associated with the conventional method for computing Lagrange multipliers. By deploying the Law of Large Numbers (LLN) in computing integrals, the proposed method benefits from the well-known result that the error of Monte Carlo integration is independent of dimension. In\n\u2022 A closed-form solution. By setting the relaxation rate of the moments to zero, the Lagrange multipliers can be computed directly from samples X \u223cf, i.e. by solving the system 18, without the need for the line-search associated with the Newton method of solving the optimization problem of standard MED, i.e. Def. 4.3. This is a significant improvement compared to standard MED, where Lagrange multipliers are estimated iteratively \u2014 see Eq. (A.5) and Algorithm 4 in Appendix A. \u2022 Avoiding the curse of dimensionality in integration. In the proposed method, the computational complexity associated with the integration only depends on the number of samples, and not on the dimension of the probability space. The proposed method takes full advantage of having access to the samples of the unknown distribution function. This is in contrast to the standard Newton-Raphson method of solving optimization problem 4.3 where samples of the initial guessed MED corresponding to the initial guessed \u03bb is not available and one runs to the curse of dimensionality in computation of gradient and Hessian.\nIn particular, we compute the orthonormal basis function, gradient, and Hessian using the samples of X. This use of the Monte Carlo integration method avoids the curse of high dimensionality associated with the conventional method for computing Lagrange multipliers. By deploying the Law of Large Numbers (LLN) in computing integrals, the proposed method benefits from the well-known result that the error of Monte Carlo integration is independent of dimension. In particular, given N independent, identically distributed d-dimensional random variables X1, ..., XN with probability density f(x), where X \u223cf, the variance of the empirical estimator of a moment\n\u03d5(x) of f, i.e. E\u2206[\u03d5(X)] = \ufffdN i=1 \u03d5(Xi)/N, is\nwhich is independent of the dimension d = dim(x). Therefore, for a required ratio of variance in prediction and variance of the underlying random variable, i.e. Var(E\u2206[\u03d5(X)])/Var(\u03d5(x)), the cost of integration is O(sN). The factor s denotes the cost of computing \u03d5(X) for one sample.\nof integration is O(sN). The factor s denotes the cost of computing \u03d5(X) for one sample. This is a considerable advantage compared to the standard approach of finding the Lagrange multipliers of MED where the cost associated with integration is of order O(N d) where N is the number of discretization points in each dimension. We remind the reader that in the standard Newton-Raphson method of finding the Lagrange multipliers, one updates the guessed \u03bb by\nThis is a considerable advantage compared to the standard approach of finding the Lagrange multipliers of MED where the cost associated with integration is of order O(N d) where N is the number of discretization points in each dimension. We remind the reader that in the standard Newton-Raphson method of finding the Lagrange multipliers, one updates the guessed \u03bb by\n# \u03bb \u2190\u03bb \u2212L\u22121(\u03bb)g(\u03bb)\nwhere the gradient g and Hessian L need to be computed during each iteration, see section A and reference therein for details. Since samples of the guessed distribution, i.e. guessed \u03bb, are not available, computation of the gradient and Hessian can become expensive. Specifically, one has to either generate samples of the guess distribution in each intermediate step of the Newton-Raphson method which adds complexity, or deploy a deterministic integration method with cost O(N d) where N is the number of discretization points in each dimension and d = dim(x).\nWe further point out that since in both the proposed solution and the standard iterative method a matrix needs to be inverted, the cost in both algorithms scales O(N 3 b ) with number of basis functions (moments) regardless of the cost associated with integration. For example, in case of using monomials up to 2nd order in all dimensions as basis functions i.e. H = [x1, ..., xd, x2 1, x1x2, ..., x2 d], there are |H| = d + d(d + 1)/2 unique bases, leading to complexity O \ufffd (d + d(d + 1)/2)3\ufffd for solving the linear system 18.\n Relaxed existence requirements. Since the proposed approach directly finds the Lagrange multipliers for the realizable moment problem linearly using Eq. (18), it avoids the problem of possible non-realizable distribution estimate with intermediate Lagrange multipliers that is present in the iterative methods. This is another advantage compared to the standard MED optimization problem, Eq. (4.3), where the line search Eq. (20) may fail as the distribution associated with the intermediate \u03bb may not exist (not integrable). This is a common problem when finding Lagrange multipliers for the moment problem close to the limit of realizability when the condition number of the Hessian becomes large and the iterative Newton-Raphson method fails, e.g. see Alldredge et al. (2014).\n# 5 Symbolic-Based Maximum Entropy Distribution\nIn the standard moment problem it is common to consider polynomials for the moment functions in H, i.e. H = \ufffd x, x2, . . . \ufffd , even though other basis functions may better represent the unknown distribution. Additionally, such polynomial basis functions are notorious for resulting in ill-conditioned solution processes. For these reasons, we introduce a symbolic regression approach to introduce some diversity and ultimately optimize over our use of basis functions. As we will see in the next section, adding the symbolic search to our MED description improves the accuracy, convergence, and robustness of the density recovery problem.\nGiven a metric L and a dataset D = {xi, yi}N i=1 consisting of N independent identically distributed (i.i.d.) paired samples, where xi \u2208Rdim(x) and yi \u2208R, the SR problem searches in the space of functions S defined by a set of given mathematical functions (e.g., cos, sin, exp, ln) and arithmetic operations (e.g., +, \u2212, \u00d7, \u00f7), for a function \u03c8\u2217(x) which minimizes \ufffdN i=1 L \ufffd yi, \u03c8(xi) \ufffd where \u03c8 \u2208S.\n(19)\n(20)\nIn order to deploy the SR method for the density recovery, we need to restrict the space of functions S to those which satisfy non-negativity, normalization and existence of moments with respect to the vector of linearly independent (polynomial) basis functions R. The space of such distributions can be defined as\n\ufffd\ufffd non-negativity, motivated by the MED formulation, we consider \u02c6f to be exponential, i.e.\n\ufffd In order to ensure non-negativity, motivated by the MED formulation, we consider \u02c6f to be exponential, i.\n\ufffd \ufffd \ufffd \ufffd where G(x) is an analytical (or symbolic) function of x = \ufffd x1, x2, . . . , xdim(x) \ufffd . While the non-negativity is guaranteed, existence of moments needs to be verified when a test function for G(x) is considered. As our focus in this paper is on the maximum entropy distribution function given by Eq. (8), we consider G(x) to have the form\now we proceed to provide a modified formulation for SR tailored to our MED problem. Definition 5.2. Symbolic Regression for the Maximum Entropy Distribution (SR-MED) problem\nNow we proceed to provide a modified formulation for SR tailored to our MED problem. Definition 5.2. Symbolic Regression for the Maximum Entropy Distribution (SR-MED) problem Given a measure of difference between distributions L (e.g. KL Divergence) and a dataset D = {Xi}N i=1 consisting of N i.i.d. samples, where Xi \u2208Rdim(x), the SR-MED problem searches in the space SNb for N basis functions subject to \u02c6f \u2208Sf|R which minimizes L.\nGiven a measure of difference between distributions L (e.g. KL Divergence) and a dataset D = {Xi}N i=1 consisting of N i.i.d. samples, where Xi \u2208Rdim(x), the SR-MED problem searches in the space SNb for Nb basis functions subject to \u02c6f \u2208Sf|R which minimizes L.\nHere, we deploy continuous functions consisting of binary operators (e.g. +, \u2212, \u00d7, \u00f7) or unary functions (e.g. cos, sin, exp, log) to fill the space SNb. As in most of the SR methods, we encode mathematical expressions using symbolic expression trees, a type of binary tree, where internal nodes contain operators or functions and terminal nodes (or leaves) contain input variables of constants. For instance, the expression tree in Figure 1 represents x2 cos(x). In this paper, we perform a Monte Carlo symbolic search in the space of smooth functions (by generating random expression trees) to find a vector of basis functions H that guarantees acceptable cond(LME), by rejecting candidates that do not satisfy this condition. In our search, we do not consider test basis functions with odd growth rates which lead to non-realizable distributions.\n# 6 Multi-level density recovery\nWe further improve our proposed method by introducing a multi-level process that improves our prediction as the distribution becomes more detailed. The goal is to obtain a more generalized MED estimate with the form\n(.)[l] denotes the level index, Z[l] is the normalization factor of density at level l, NL is the number of levels considered and m[l] indicates the portion of total mass that is covered by \u02c6f [l]. We note that this multi-level approach is recursive and can be described as follows:\n(21)\n(22)\n(23)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25f0/25f0a5d6-a459-431d-be9b-c2406fb1e3d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Expression tree for x2 \u00d7 cos(x).</div>\n(24)\n(25)\n\u2022 Step 1: Find MED estimate \u02c6f [l] at level l.\nAt level l, first we pick a basis function H\nby solving the SR-MED problem detailed in Def. 5.2. Then, we orthonormalize the basis functio\nwith respect to the distribution of the samples using Gram\u2013Schmidt\u2019s procedure as outlined i\nAlgorithm 1.\n\u2022 Step 2: Removing subset of samples covered by \u02c6f [l]. Here, we attempt to find and remov\na subset of samples D[l]\nmask \u2013 representing a fraction of the mass, i.e. m[l] = |D[l]\nmask|/|D| \u2013 that ca\nbe estimated by our estimated \u02c6f [l] at this level. To this end, we deploy acceptance/rejection wit\nprobability \u02c6f [l]/ \u02c6f hist to find and remove D[l]\nmask from the remaining samples D[l].\n\u2022 Step 3: Repeat steps 1-2 for the next level l + 1 until almost no samples are left. Repea\nsteps 1-2 with the remaining uncovered samples (which constitutes the next level) until there ar\n(almost) no uncovered samples. The resulting total distribution is a weighted sum of the estimate\nfrom each level.\nIn Algorithm 2, we detail a pseudocode for our devised multi-level process. As we will see in the next section\nour proposed multi-level recursive mechanism improves overall performance, and elegantly describe detai\nof multi-mode distributions.\nAlgorithm 2: Multi-level, symbolic and recursive algorithm for density recovery. Here, D[l] denotes the\nset of samples at level l and u is a random variable that is uniformly distributed in (0, 1), i.e. u \u223cU([0, 1]).\nInput: D[1] = D = {Xi}N\ni=1, N tot\nL\n= NL\nfor l = 1, ..., NL do\nSample random basis functions H[l] that satisfies Def. 5.2 starting from polynomials in level l = 1;\nCompute \u02c6f [l](x) given D[l] using Algorithm 1;\nD[l]\nmask \u2190{D[l] | \u02c6f [l](X)/ \u02c6f hist(X) > u} where u \u223cU([0, 1]);\nm[l] \u2190|D[l]\nmask|/|D|;\nif\n\ufffdl\nj=1 |D[j]\nmask| \u2248|D| then\nD[l]\nmask \u2190D[l];\n// Mask all available samples\nm[l] \u2190|D[l]\nmask|/|D|;\nN tot\nL\n\u2190l;\nbreak;\n// Terminate the process\nelse\nD[l+1] \u2190D[l]\\D[l]\nmask;\n// The uncovered samples are left for the next level\nend\nend\nReturn \u02c6f(x) = \ufffdNtot\nL\nl=1\n\u02c6f [l](x) |D[l]\nmask|/|D|;\n// Ntot\nL\nis the total number of recursive calls\n7\nAlgorithm for MESSY estimation\n\u2022 Step 1: Find MED estimate \u02c6f [l] at level l. At level l, first we pick a basis function H[l] by solving the SR-MED problem detailed in Def. 5.2. Then, we orthonormalize the basis function with respect to the distribution of the samples using Gram\u2013Schmidt\u2019s procedure as outlined in Algorithm 1. \u2022 Step 2: Removing subset of samples covered by \u02c6f [l]. Here, we attempt to find and remove a subset of samples D[l] mask \u2013 representing a fraction of the mass, i.e. m[l] = |D[l] mask|/|D| \u2013 that can be estimated by our estimated \u02c6f [l] at this level. To this end, we deploy acceptance/rejection with probability \u02c6f [l]/ \u02c6f hist to find and remove D[l] mask from the remaining samples D[l]. \u2022 Step 3: Repeat steps 1-2 for the next level l + 1 until almost no samples are left. Repeat steps 1-2 with the remaining uncovered samples (which constitutes the next level) until there are (almost) no uncovered samples. The resulting total distribution is a weighted sum of the estimates from each level.\nIn Algorithm 2, we detail a pseudocode for our devised multi-level process. As we will see in the next section, our proposed multi-level recursive mechanism improves overall performance, and elegantly describe details of multi-mode distributions.\nAlgorithm 2: Multi-level, symbolic and recursive algorithm for density recovery. Here, D[l] denotes the\nset of samples at level l and u is a random variable that is uniformly distributed in (0, 1), i.e. u \u223cU([0, 1]).\nInput: D[1] = D = {Xi}N\ni=1, N tot\nL\n= NL\nfor l = 1, ..., NL do\nSample random basis functions H[l] that satisfies Def. 5.2 starting from polynomials in level l = 1;\nCompute \u02c6f [l](x) given D[l] using Algorithm 1;\nD[l]\nmask \u2190{D[l] | \u02c6f [l](X)/ \u02c6f hist(X) > u} where u \u223cU([0, 1]);\nm[l] \u2190|D[l]\nmask|/|D|;\nif\n\ufffdl\nj=1 |D[j]\nmask| \u2248|D| then\nD[l]\nmask \u2190D[l];\n// Mask all available samples\nm[l] \u2190|D[l]\nmask|/|D|;\nN tot\nL\n\u2190l;\nbreak;\n// Terminate the process\nelse\nD[l+1] \u2190D[l]\\D[l]\nmask;\n// The uncovered samples are left for the next level\nend\nend\nReturn \u02c6f(x) = \ufffdNtot\nL\nl=1\n\u02c6f [l](x) |D[l]\nmask|/|D|;\n// Ntot\nL\nis the total number of recursive calls\n#   7 Algorithm for MESSY estimation\nThe complete MESSY estimation algorithm is summarized in Algorithm 3. Within the iteration loop, following each application of the multi-level, symbolic, and recursive density recovery summarized in Algorithm 2, we introduce a maximum-cross entropy distribution (MxED) correction step (see Appendix B for details) to reduce any bias in our prediction for \u02c6f from the former.\nFinally, after completing the desired number of iterations, the algorithm returns the candidate density with the smallest KL Divergence given by\n\ufffd In other words, we use \u2212 \ufffd log \ufffd\u02c6f(X) \ufffd\ufffd as our selection criterion.\nThe MESSY algorithm comes in two flavors: MESSY-P, which considers only polynomial basis functions for H, and MESSY-S which includes optimization over basis functions using the SR algorithms outlined above. In fact, by convention, the SR algorithm in MESSY-S starts its first iteration using polynomial basis functions up to order Nm as the sample space of smooth functions. In other words, MESSY-P is a special case of MESSY-S with Niter = 1. In the remaining iterations of MESSY-S, we perform the symbolic search in the space of smooth functions of order Nm to find Nb bases that provide manageable cond(LME), as discussed in Section 6. In addition, we provide the option to enforce boundedness of \u02c6f on the support that is specified by the user, i.e. letting \u02c6f(x) = 0 for all x outside the domain of interest. This allows us to recover distributions with discontinuity at the boundary which may have application in image processing. We also provide an option to further reduce the bias by minimizing the cross-entropy given samples of bounded/unbounded multi-level estimate as prior and moments of input samples as the target moments (see Appendix B for more details on the cross-entropy calculation). For this optional step, we generate samples of \u02c6f and match the moments of polynomial basis functions up to order Nm. Since the solution at each level of \u02c6f is close to the exact MED solution, the optimization problem associated with the moment matching procedure of the cross-entropy algorithm converges very quickly, i.e. in a few iterations, providing us with\n(26)\n(27)\n(28)\na correction that minimizes bias along with the weighted samples of our estimate as the by-product. We note that in general the order of the randomly created basis function during the MESSY-S procedure may be different from the one used in the cross-entropy moment matching procedure.\n# 8 Results\nIn this section we demonstrate the effectiveness of the proposed MESSY estimation method in recovering distributions given samples, using a number of numerical experiments, involving a range of distributions ranging from multi-mode to discontinuous. For validation, we provide comparisons with the standard KDE with an optimal bandwidth obtained using K-fold cross-validation with K = 5. We consider a uniform grid with 20 elements for the cross-validation in a range that spans from 0.01 to 10 on a logarithmic scale. We also compare with cross-entropy closure with Gaussian as the prior (MxED) using Newton\u2019s method. We note that while the standard maximum entropy distribution function differs from MxED as the latter incorporates a prior, we intentionally use MxED as a benchmark instead because the standard approach can be extremely expensive. Unless mentioned otherwise, we report error, time, and KL Divergence by ensemble averaging over 25 for different sets of samples. Furthermore, in the case of MESSY-S we perform Niters = 10 iterations, and we consider (+, \u2212, \u00d7) operators and (cos, sin) functions. Here we report the execution time using a single-core CPU for each method. Typical symbolic expressions of density functions recovered by MESSY for the test cases considered here can be found in Appendix E. Furthermore, in Appendix C, we perform an ablation study that shows the importance of each component of the MESSY algorithm.\n# 8.1 Bi-modal distribution function\nFor our first test case, we consider a one-dimensional bi-modal distribution function constructed by mix two Normal distribution functions N(x | \u00b5, \u03c3), i.e.\n# For our first test case, we consider a one-dimensional bi-modal distribution function constructed by mixing two Normal distribution functions N(x | \u00b5, \u03c3), i.e.\nf(x) = \u03b1 N(x | \u00b51, \u03c31) + (1 \u2212\u03b1) N(x | \u00b52, \u03c32),\n= 0.5, means \u00b51 = \u22120.6 and \u00b52 = 0.7, and standard deviations \u03c31 =\nFigure 2 compares results from MESSY, KDE and MxED for three different sample sizes, namely 100, 1000, and 10, 000 samples of f. For MxED and MESSY-P, we use Nb = Nm = 4. In the case of MESSY-S, we randomly create Nb basis functions which are O(x4) (where Nb is sampled uniformly within {2, . . . , 8}). Both MESSY results are subject to a cross-entropy correction step with Nb = 4 polynomial moments. Clearly, the MxED and MESSY methods provide a reasonable estimate when a small number of samples is available; where KDE may suffer from bias introduced by the smoothing kernel. As a reference for the reader, we also compared the outcome density with a histogram in Appendix D. In order to analyze the error further, Fig 3 presents the relative error in low and high order moments, KL Divergence, and single-core CPU time as the measure of computational cost for considered methods. Given optimal bandwidth is found, the KDE error can only be reduced by increasing the number of samples. However, maximum entropy based estimators such as MxED and MESSY provide more robust estimate when less samples are available. We point out that the convergence of the cases where only moments of polynomial basis functions are considered, i.e. MxED and MESSY-P, relies on the degree of the polynomials and not the number of samples. On the other hand, the additional search associated with MESSY-S returns more appropriate basis functions for a given upper bound on the order of the basis functions. Next, we perform a convergence study on 10,000 samples and show that the parametric description converges to the solution when its degrees of freedom are increased. In Fig. 4, we show that both MESSY-P and MESSY-S converge to the true solution by increasing either the order of polynomial basis function, or the number of basis functions, respectively. In the case of MESSY-S, we generated symbolic expressions that are O(x2). The improved agreement compared to the MESSY-P case highlights the benefit derived from non-traditional basis functions that may better represent the data.\n(29)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a98a/a98a61d8-35ff-47a0-b7c6-489e084a8103.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) 100 samples</div>\n<div style=\"text-align: center;\">Figure 2: Density estimation using KDE, MxED, MESSY-P, and MESSY-S given (a) 100, (b) 1,000, and 10,000 samples.</div>\n<div style=\"text-align: center;\">2: Density estimation using KDE, MxED, MESSY-P, and MESSY-S given (a) 100, (b) 1,000, and (c) samples.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1caf/1caf63d1-3fd4-495f-b480-a20233fb1909.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c)</div>\n<div style=\"text-align: center;\">Figure 3: Comparing the relative error in (a) the first four moments, (b) two higher order moments (i.e. fifth and sixth moments), (c) KL Divergence, and (d) the execution time for KDE, MxED, MESSY-P, and MESSY-S in recovering distribution function for different sample sizes. Here, the error bar (in black) corresponds to the standard error of the empirical measurements.</div>\nAs shown in Fig. 5, the MESSY-S procedure results in better-conditioned LME matrices than the MESSY-P for the same degrees of freedom. However, the search for a good basis function increases the computational cost. In each iteration of the search for basis functions, the MESSY-S algorithm may reject symbolic basis candidates based on the condition number of the matrix LME. In other words, the improved performance associated with MESSY-S comes at some increased computational cost.\n<div style=\"text-align: center;\">(d)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ef3/0ef391bc-e896-48d7-84b6-5396ebd2c6ba.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) MESSY-P</div>\nFigure 4: Convergence of MESSY estimation to target distribution function by (a) increasing the order of polynomial basis functions for MESSY-P or (b) increasing the number of randomly selected symbolic basis functions with Nm = 2 for MESSY-S.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b42/0b42abff-68c6-45fe-a183-f164eafbdd24.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: KL Divergence, execution time, and condition number against the degrees of freedom, i.e. the order of polynomial basis functions for MESSY-P or the number of symbolic basis functions with Nm = 2 for the MESSY-S estimate.</div>\n# 8.2 Limit of realizability\nOne of the challenging moment problems for maximum entropy methods is the one involving distributions near the border of physical realizability. In the one-dimensional case with moments of the first four monomials [x, x2, x3, x4] as the input, the moment problem is physically realizable when\nThe moment problem with moments approaching the equality in Eq. (30) is called limit of realizablity (McDonald & Torrilhon, 2013; Akhiezer & Kemmer, 1965). We consider samples from a distribution in this limit as our test case here, since the standard MED cannot be solved due to an ill-conditioned Hessian (see Abramov (2007); Alldredge et al. (2014)). In Fig. 6, we depict the estimated density of a bi-modal distribution in this limit given its samples with moments \u27e8X\u27e9= 0, \u27e8X2\u27e9= 1, \u27e8X3\u27e9= \u22122.10 and \u27e8X4\u27e9= 5.42. Here, we compare the density obtained\nThe moment problem with moments approaching the equality in Eq. (30) is called limit of realizablity (McDonald & Torrilhon, 2013; Akhiezer & Kemmer, 1965). We consider samples from a distribution in this limit as our test case here, since the standard MED cannot be solved due to an ill-conditioned Hessian (see Abramov (2007); Alldredge et al. (2014)).\n<div style=\"text-align: center;\">(b) MESSY-S</div>\n(30)\nusing KDE, MxED, MESSY-P, and MESSY-S to the histogram of samples. In this example, we obtained the MESSY-S estimate by searching in the space of smooth functions with Nb \u2208{2, ..., 8} basis functions and compare polynomial and symbolic basis functions of order 2 and 4. In Fig. 7, we compare the KL Divergence, the execution time, and the condition number for each method. While KDE suffers from over-smoothing and MxED/MESSY-P require at least Nb = 4 (and consequently Nm = 4, resulting in a stiff problem with large condition number), MESSY-S can obtain accurate density estimates by using unconventional basis functions with Nm = 2, thus maintaining a manageable condition number.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a99b/a99bcc05-71b3-44bc-ac57-375cc5bb7439.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Estimating density for a case of distribution near the limit of realizability using KDE, MxED, MESSY-P, and MESSY-S. The solutions of MxED, MESSY-P, and MESSY-S are obtained using basis functions of second (left) and fourth (right) order.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/969b/969bd5cc-2282-40c5-97ae-9e87f95eed2b.png\" style=\"width: 50%;\"></div>\nFigure 7: Comparing KL Divergence, execution time, and condition number of KDE, MxED, MESSY-P, and MESSY-S for an unknown distribution near the limit of the realizability. Here, we consider polynomial basis functions of second and fourth order for MxED and MESSY-P denoted by MxED (2), MxED (4), MESSY-P (2) and MESSY-P (4), respectively. In MESSY-S, we consider symbolic basis functions of second order only which we denote by MESSY-S (2).\n# 8.3 Discontinuous distributions\nWe now highlight the benefits of using MESSY estimation with piecewise continuous capability for recov ering distributions with a discontinuity at the boundary. As an example, let us consider the exponentia distribution with a probability density function given by\nGiven 10, 000 samples of this distribution, in Fig. 8 we compare KDE, MxED, and the proposed MESSY-P and MESSY-S methodologies. In the case of MxED and MESSY-P we consider second-order polynomial basis functions, and for MESSY-S we search the space of smooth functions for Nb \u2208{2, ..., 8} symbolic basis functions of order O(x2). For MESSY-P and MESSY-S, we apply the boundary condition\nBy providing information about the boundedness of the expected distribution, we enable MESSY to accurately predict densities with discontinuity near the boundary. As it can be seen clearly from Fig. 8, in contrast to MxED, both MESSY-P and MESSY-S provide accurate predictions by taking advantage of the information about the boundedness of the target density.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be70/be7056f3-ba04-47d1-bad0-52718e1c9191.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">igure 8: Estimating density of exponential distribution function from its samples using KDE, MxED, MESSY-P, and MESSY-S. For MxED, MESSY-P and MESSY-S, with Nm = 2.</div>\nThe KL Divergence score and execution time for each method is shown in Fig. 9. These figures show tha MESSY-P and MESSY-S provide a more accurate description compared to the KDE estimate, albeit at  higher computational cost.\n(31)\n(32)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/06dd/06dd8562-f936-409d-8fa1-85f23a6d6a03.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: KL Divergence and execution time for KDE, MxED, MESSY-P, and MESSY-S estimation o exponential distribution function given 10,000 samples.</div>\n# 8.4 Two-dimensional distributions\nIn this section the proposed MESSY methodology is applied to the estimation of 2-dimensional distributions. Let us consider a multivariate Gaussian distribution in two dimensions, i.e. X = (X1, X2) \u223cN(\u00b5, \u03a3) with probability density\nLet us also consider a more complex 2D distribution by multiplying two independent 1D distributions, namely the exponential and Gamma distributions, i.e. X1 \u223cExp(\u03bb), X2 \u223c\u0393(k, \u03b8), and X = (X1, X2) \u223c Exp(\u03bb)\u0393(k, \u03b8). In particular, we consider samples of the joint pdf given by\nwhere \u03bb > 0 is the rate parameter, k > 0 is the shape parameter, \u03b8 > 0 is the scale parameter, and \u0393(\u00b7) is the gamma function. In our example, we use \u03bb = 2, k = 3, and \u03b8 = 0.5.\nFig. 10 compares the true distribution against the KDE, MESSY-P and MESSY-S estimate given 104 samples of the distribution. In cases of MESSY-P and MESSY-S, we consider basis functions up to 2nd order in each dimension. The figure shows that KDE, MESSY-P and MESSY-S provide a reasonable estimate for the multivariate normal distribution. However, among all considered estimators for the case of the Gamma-exponential distribution, MESSY-S seems to provide a slightly better estimate, i.e. it captures the peak in the most probable region of the distribution.\n# 8.5 Cost of MESSY-P with respect to dimension\nAs discussed in section 4.1, for a given moment problem (fixed vector of basis functions), the cost of evaluating Lagrange multipliers in the proposed method is linear with respect to number of samples and independent of dimension \u2014 a benefit of Monte Carlo integration. However, similar to the standard MED approach, the proposed MESSY estimate requires the inversion of a Hessian matrix L \u2208RNb\u00d7Nb. Therefore, in addition to integration, a MESSY cost estimate also includes solution of a linear system of equations with\n(33)\n(34)\n(35)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc6a/dc6a5964-5c9b-4b0d-ab29-e6e3c71d6f99.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Estimating the density of samples in two dimensions using KDE, MESSY-P, and MESSY-S. We use 10,000 samples from a Gaussian (top) and the Gamma-exponential density defined in Eq. (35) (bottom). For better visualization, we only show 100 random samples. We also report that the KL-divergence for MESSY-S, MESSY-P and KDE are on the same order.</div>\nFigure 10: Estimating the density of samples in two dimensions using KDE, MESSY-P, and MESSY-S. We use 10,000 samples from a Gaussian (top) and the Gamma-exponential density defined in Eq. (35) (bottom). For better visualization, we only show 100 random samples. We also report that the KL-divergence for MESSY-S, MESSY-P and KDE are on the same order.\ncost of order O(N 3 b ). In this example, we consider the MESSY-P estimator with basis functions H = \ufffd x1, ..., xd, x2 1, x1x2, ..., x2 d \ufffd for estimating a target d-dimensional multivariate normal distribution Eq. (33) with mean \u00b5\nand covariance matrix \u03a3\nUsing 2nd order polynomial basis functions, the number of unique basis functions is Nb = d + d(d + 1)/2, leading to a cost of order O((d + d(d + 1)/2)3) for a d-dimensional probability space. In Fig. 11-(a), we illustrate this by showing the normalized execution time (normalized by the time for d = 1 and given sample size) against the dimension of the target density given N \u2208{100, 200, 400, 800, 1600, 3200} samples. Furthermore, in order to highlight the linear dependence of cost with number of samples N for a fixed moment problem, in Fig. 11-(b) we also report the normalized execution time of computing Lagrange multipliers (normalized by the time for N = 100 and dimension d of target density) as a function of number of samples.\nSince the most expensive component of MESSY, i.e. the computation of Lagrange multipliers, has been carried out on a single computer core with a direct solver, we confirm that MESSY algorithm can be used for density recovery in large number of dimensions at a feasible computational cost, regardless of the scaling with dimension. The limiting factor appears to be the storage associated with solving the linear system. In case a larger system than the one studied here are of interest, one may use iterative solvers, see Saad (2003).\n(36)\n(37)\n(38)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a89e/a89eb863-5461-4db8-8819-37e54e5f6c02.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\n# 9 Conclusion and Outlook\nWe present a new method for symbolically recovering the underlying probability density function of a given finite number of its samples. The proposed method uses the maximum entropy distribution as an ansatz thus guaranteeing positivity and least bias. We devise a method for finding parameters of this ansatz by considering a Gradient flow in which the ansatz serves as the driving force. One main takeaway from this work is that the parameters of the MED ansatz can be computed efficiently by solving a linear problem involving moments calculated from the given samples. We also show that the complexity associated with finding Lagrange multipliers, the most expensive part of the algorithm, scales linearly with the number of samples in all dimensions. On the other hand, the cost of inverting the Hessian matrix with dimension RNb\u00d7Nb is O(N 3 b ), where the number of basis functions Nb grows with dimension. Overall, our numerical experiments show that densities of dimension 10 can be recovered using a single core of a typical workstation. It is worth noting that since the whole probability space is represented with a few parameters, the MESSY representation can reduce the training cost associated with the PDE-FIND Rudy et al. (2017) and SINDy Brunton et al. (2016) method, where in the latter the regression is done on a data set that discretizes the whole space with a large number of parameters that need to be fit. Further detailed analysis of the trade-off between cost and accuracy in inferring densities in high-dimensional probability spaces will be addressed in future work. The second main takeaway from this work is that accurate density recovery does not necessarily require the use of high-order moments. In fact, increasing the number of complex but low-order basis functions leads to superior expressiveness and better assimilation of the data. For this reason, the proposed method is equipped with a Monte Carlo search in the space of smooth functions for finding basis functions to describe the exponent of the MED ansatz, using KL Divergence, calculated from the unknown-distribution samples, as an optimality criterion. Discontinuous densities are treated by considering piece-wise continuous functions with support on the space covered by samples.\nWe validate and test the proposed MESSY estimation approach against benchmark non-parametric (KDE) and parametric (MxED) density estimation methods. In our experiments, we consider three canonical test cases; a bi-modal distribution, a distribution close to the limit of realizability, and a discontinuous\ndistribution function. Our results suggest that MESSY estimation exhibits several positive attributes compared to existing methods. Specifically, while retaining some of the most desirable features associated with MED, namely non-negativity, least bias, and matching the moments of the unknown distribution, it outperforms standard maximum-entropy-based approaches for two reasons. First, it uses samples of the target distribution in the evaluation of the Hessian, which has a linear cost with respect to the dimension of the random variable. Second, the resulting linear problem for finding the Lagrange multipliers from moments is significantly more efficient than the Newton line search used by the classical MED approach. Moreover, our multi-level algorithm allows for recovery of more complex distributions compared to the standard MED approach. Combining the efficient approach of finding maximum entropy density via a linear system with the symbolic exploration for the optimal basis functions paves the way for achieving low bias, consistent, and expressive density recovery from samples. Although the proposed MESSY estimate alleviates a number of numerical challenges associated with finding the MED given samples, it cannot expand the space of the MED solution. In other words, if the moment problem does not permit any MED solutions, e.g. (Junk, 1998), the MESSY estimate of MED fails to infer any densities since there is no MED solution to be found. Secondly, there are no guarantees that the coefficient of the leading term in the exponent of the MED will be negative. Hence, in unbounded domains the MESSY estimate may diverge in the tails of the distribution. Both of these issues can be resolved by regularizing the MED ansatz, e.g. via incorporating a Wasserstein distance from a prior distribution as suggested by Sadr et al. (2023). The most important, perhaps, distinguishing characteristic of the proposed methodology from nonparametric approaches, such as KDE, is the ability to recover a tractable symbolic estimator of the unknown density. This can be beneficial in a number of applications, such as, for example, finding the underlying dynamics of a stochastic process. In such a case, where it is known that the transition probability takes an exponential form, this method can be used for recovering the drift and diffusion terms of an Ito process given a sequence of random variables, see e.g. Risken (1989); Oksendal (2013). Furthermore, the proposed method may be used to find a relation between moments of interest which could be helpful in determining the parameters of a statistical model, such as a closure for a hierarchical moment description of fluid flow far from equilibrium. That said, exploring the efficiency of our algorithm in high-dimensional contexts is a critical next step, considering the complexity of modern machine learning datasets. Other possible directions for future work include: (i) data-driven discovery of governing dynamical laws from samples; and (ii) applications to variance reduction.\n# Acknowledgments\nWe would like to acknowledge Prof. Youssef Marzouk and Evan Massaro for their helpful comments and suggestions. M. Sadr thanks Hossein Gorji for his stimulating and constructive comments about the core ideas presented in this paper. T. Tohme was supported by Al Ahdab Fellowship. M. Sadr acknowledges funding provided by the German Research Foundation (DFG) under grant number SA 4199/1-1.\n# References\nRafail Abramov. A practical computational framework for the multidimensional moment-constrained maximum entropy principle. Journal of Computational Physics, 211(1):198\u2013209, 2006. Rafail V Abramov. An improved algorithm for the multidimensional moment-constrained maximum entropy problem. Journal of Computational Physics, 226(1):621\u2013644, 2007. Rafail V Abramov. The multidimensional moment-constrained maximum entropy problem: A BFGS algorithm with constraint scaling. Journal of Computational Physics, 228(1):96\u2013108, 2009. Rafail V Abramov. The multidimensional maximum entropy moment problem: A review of numerical methods. Communications in Mathematical Sciences, 8(2):377\u2013392, 2010. Naum Il\u2019ich Akhiezer and N Kemmer. The classical moment problem: and some related questions in analysis, volume 5. Oliver & Boyd Edinburgh, 1965.\nGraham W Alldredge, Cory D Hauck, Dianne P O\u2019Leary, and Andr\u00e9 L Tits. Adaptive change of basis in entropy-based moment closures for linear kinetic equations. Journal of Computational Physics, 258: 489\u2013508, 2014. Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural symbolic regression that scales. In International Conference on Machine Learning, 2021. Zdravko Botev. Nonparametric density estimation via diffusion mixing. 2007. Zdravko I Botev, Joseph F Grotowski, and Dirk P Kroese. Kernel density estimation via diffusion. The Annals of Statistics, 38(5), 2010. Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113 (15):3932\u20133937, 2016. Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. Operon C++: An efficient genetic programming framework for symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion, GECCO \u201920, pp. 1562\u20131570, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371278. doi: 10.1145/3377929.3398099. Zhenning Cai, Yuwei Fan, and Ruo Li. A framework on moment model reduction for kinetic equation. SIAM Journal on Applied Mathematics, 75(5):2001\u20132023, 2015. Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, and Philippe Rigollet. SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence. Advances in Neural Information Processing Systems, 33:2098\u20132109, 2020. Edwin KP Chong and Stanislaw H Zak. An introduction to optimization, volume 75. John Wiley & Sons, 2013. Ronald R Coifman and St\u00e9phane Lafon. Diffusion maps. Applied and computational harmonic analysis, 21 (1):5\u201330, 2006. Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps. Proceedings of the national academy of sciences, 102(21):7426\u20137431, 2005. Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases. arXiv preprint arXiv:2006.11287, 2020. Kristian Debrabant, Giovanni Samaey, and Przemys\u0142aw Zielinski. A micro-macro acceleration method for the Monte Carlo simulation of stochastic differential equations. SIAM Journal on Numerical Analysis, 55 (6):2745\u20132786, 2017. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv preprint arXiv:1605.08803, 2016. David L Donoho, Iain M Johnstone, G\u00e9rard Kerkyacharian, and Dominique Picard. Density estimation by wavelet thresholding. The Annals of statistics, pp. 508\u2013539, 1996. Wolfgang Dreyer. Maximisation of the entropy in non-equilibrium. Journal of Physics A: Mathematical and General, 20(18):6505, 1987. Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Advances in neural information processing systems, 32, 2019. Anthony William Fairbank Edwards. Likelihood. CUP Archive, 1984.\nAldo Frezzotti, Livio Gibelli, and Silvia Lorenzani. Mean field kinetic theory description of evaporation of a fluid into vacuum. Physics of Fluids, 17(1):012102, 2005. doi: 10.1063/1.1824111. Alfredo Garbuno-Inigo, Franca Hoffmann, Wuchen Li, and Andrew M Stuart. Interacting Langevin diffusions: Gradient structure and ensemble Kalman sampler. SIAM Journal on Applied Dynamical Systems, 19(1): 412\u2013441, 2020a. Alfredo Garbuno-Inigo, Nikolas Nusken, and Sebastian Reich. Affine invariant interacting Langevin dynamics for Bayesian inference. SIAM Journal on Applied Dynamical Systems, 19(3):1633\u20131658, 2020b. Luc Giraud, Julien Langou, and Miroslav Rozlozn\u0131k. On the round-off error analysis of the gram-schmidt algorithm with reorthogonalization. Technical report, Technical Report TR/PA/02/33, CERFACS, Toulouse, France, 2002. Harold Grad. Note on N-dimensional Hermite polynomials. Communications on Pure and Applied Mathematics, 2(4):325\u2013330, 1949. Cory D Hauck, C David Levermore, and Andr\u00e9 L Tits. Convex duality and entropy-based moment closures: Characterizing degenerate densities. SIAM Journal on Control and Optimization, 47(4):1977\u20132015, 2008. M Hermite. Sur un nouveau d\u00e9veloppement en s\u00e9rie des fonctions. Imprimerie de Gauthier-Villars, 1864. Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. Bayesian symbolic regression. arXiv preprint arXiv:1910.08892, 2019. M Chris Jones, James S Marron, and Simon J Sheather. A brief survey of bandwidth selection for density estimation. Journal of the American statistical association, 91(433):401\u2013407, 1996. Michael Junk. Domain of definition of levermore\u2019s five-moment system. Journal of Statistical Physics, 93: 1143\u20131167, 1998. Zahra Kadkhodaie, Florentin Guth, St\u00e9phane Mallat, and Eero P Simoncelli. Learning multi-scale local conditional probability models of images. arXiv preprint arXiv:2303.02984, 2023. Pierre-Alexandre Kamienny, St\u00e9phane d\u2019Ascoli, Guillaume Lample, and Fran\u00e7ois Charton. End-to-end symbolic regression with transformers. arXiv preprint arXiv:2204.10532, 2022. Jagat Narain Kapur. Maximum-entropy models in science and engineering. John Wiley & Sons, 1989. A Ya Khinchin. Mathematical foundations of information theory. Courier Corporation, 2013. Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of current methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):3964\u20133979, 2020. Michael Kommenda, Bogdan Burlacu, Gabriel Kronberger, and Michael Affenzeller. Parameter identification for symbolic regression using nonlinear least squares. Genetic Programming and Evolvable Machines, 21 (3):471\u2013501, 2020. Misaki Kon, Kazumichi Kobayashi, and Masao Watanabe. Method of determining kinetic boundary conditions in net evaporation/condensation. Physics of Fluids, 26(7):072003, 2014. John R Koza and John R Koza. Genetic programming: on the programming of computers by means of natural selection, volume 1. MIT press, 1992. William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabr\u00edcio Olivetti de Fran\u00e7a, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary symbolic regression methods and their relative performance. arXiv preprint arXiv:2107.14351, 2021.\nPeter M Lee. Bayesian statistics. Arnold Publication, 1997. Thomas Leonard and John SJ Hsu. Bayesian methods: an analysis for statisticians and interdisciplinary researchers, volume 5. Cambridge University Press, 2001. C David Levermore. Moment closure hierarchies for kinetic theories. Journal of statistical Physics, 83(5-6): 1021\u20131065, 1996. Fengyi Li and Youssef Marzouk. Diffusion map particle systems for generative modeling. arXiv preprint arXiv:2304.00200, 2023. Qiang Liu. Stein variational gradient descent as gradient flow. Advances in neural information processing systems, 30, 2017. Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. Advances in neural information processing systems, 29, 2016. Qiang Liu and Dilin Wang. Stein variational gradient descent as moment matching. Advances in Neural Information Processing Systems, 31, 2018. St\u00e9phane Mallat. A wavelet tour of signal processing. Elsevier, 1999. Tanguy Marchand, Misaki Ozawa, Giulio Biroli, and St\u00e9phane Mallat. Wavelet conditional renormalization group. arXiv preprint arXiv:2207.04941, 2022. James McDonald and Manuel Torrilhon. Affordable robust moment closures for CFD based on the maximumentropy hierarchy. Journal of Computational Physics, 251:500\u2013523, 2013. T Nathan Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago, Daniel M Faissol, and Brenden K Petersen. Symbolic regression via neural-guided genetic programming population seeding. arXiv preprint arXiv:2111.00053, 2021. Frank No\u00e9, Simon Olsson, Jonas K\u00f6hler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457):eaaw1147, 2019. Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013. Patryk Orzechowski, William La Cava, and Jason H Moore. Where are we now? a large benchmark study of recent symbolic regression methods. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1183\u20131190, 2018. Claudia Patrignani, K Agashe, G Aielli, C Amsler, M Antonelli, DM Asner, H Baer, Sw Banerjee, RM Barnett, T Basaglia, et al. Review of particle physics. CHINESE PHYSICS C, 2016. Yudi Pawitan. In all likelihood: statistical modelling and inference using likelihood. Oxford University Press, ",
    "paper_type": "method",
    "attri": {
        "background": "Recovering probability density functions from samples is a fundamental problem in statistics with applications in various fields such as particle physics and machine learning. Traditional methods include parametric and nonparametric estimators, each with their limitations. Nonparametric methods like Kernel Density Estimation (KDE) suffer from bias due to kernel choice, while parametric methods can introduce modeling errors. Maximum Entropy Distribution (MED) is a parametric approach that provides the least biased density estimate given moments of the unknown distribution but faces challenges in computational complexity and positivity.",
        "problem": {
            "definition": "The paper addresses the problem of estimating probability density functions from a finite number of samples, particularly focusing on the limitations of existing methods such as MED, which struggles with computational efficiency and positivity constraints.",
            "key obstacle": "Existing methods, particularly the MED, require solving convex optimization problems with ill-conditioned Hessians, leading to computational difficulties, especially as the order of moments increases or when approaching the limit of physical realizability."
        },
        "idea": {
            "intuition": "The authors propose a novel method that leverages Gradient flow dynamics to efficiently compute the parameters of the MED without the need for complex optimization, thus simplifying the computation of Lagrange multipliers.",
            "opinion": "The proposed MESSY estimation method utilizes a Maximum-Entropy based approach combined with symbolic regression to recover probability density functions from samples, enhancing accuracy and computational efficiency.",
            "innovation": "The key innovation lies in transforming the parameter estimation for the MED from a nonlinear optimization problem to a linear problem, which can be solved more efficiently using empirical moments derived from samples."
        },
        "method": {
            "method name": "MESSY estimation",
            "method abbreviation": "MESSY",
            "method definition": "MESSY estimation is a method for recovering probability density functions symbolically from samples using a Maximum Entropy approach, where the parameters are determined via Gradient flow dynamics.",
            "method description": "The method connects samples of the unknown distribution function to a guess symbolic expression by constructing a gradient-based drift-diffusion process.",
            "method steps": [
                "Construct a gradient flow with the guess distribution as the driving force.",
                "Compute relaxation rates of moments from the samples.",
                "Formulate a linear inverse problem to find the Lagrange multipliers of the MED.",
                "Utilize symbolic regression to optimize basis functions for the maximum entropy functional."
            ],
            "principle": "The method is effective because it transforms the problem of estimating distribution parameters into a linear problem, leveraging the properties of the Gradient flow to ensure convergence and stability."
        },
        "experiments": {
            "evaluation setting": "The experiments validate the MESSY method against benchmark methods like KDE and MxED using bi-modal and discontinuous density functions, as well as distributions near the limit of physical realizability.",
            "evaluation method": "Performance was assessed using KL Divergence, execution time, and accuracy of density recovery, with results averaged over multiple runs to ensure reliability."
        },
        "conclusion": "The MESSY estimation method effectively recovers probability density functions from samples, demonstrating superior performance over traditional methods by providing a low-bias, tractable symbolic representation of the unknown density.",
        "discussion": {
            "advantage": "The MESSY method offers significant advantages in terms of computational efficiency, as it scales linearly with the number of samples and avoids the curse of dimensionality associated with traditional optimization methods.",
            "limitation": "Despite its advancements, the MESSY method still relies on the existence of a solution for the moment problem; if no valid MED exists, the method cannot infer a density.",
            "future work": "Future research will focus on enhancing the method's robustness in high-dimensional contexts and exploring its applications in data-driven discovery of governing dynamical laws."
        },
        "other info": {
            "info1": "The MESSY algorithm consists of two variants: MESSY-P (using only polynomial basis functions) and MESSY-S (optimizing over symbolic basis functions).",
            "info2": {
                "info2.1": "The method can handle discontinuous distributions by enforcing boundary conditions.",
                "info2.2": "The implementation of the MESSY method is designed to be computationally efficient, allowing for density recovery in high-dimensional spaces."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the problem of estimating probability density functions from a finite number of samples, particularly focusing on the limitations of existing methods such as Maximum Entropy Distribution (MED), which struggles with computational efficiency and positivity constraints."
        },
        {
            "section number": "1.2",
            "key information": "Traditional methods for recovering probability density functions include parametric and nonparametric estimators, each with their limitations. Nonparametric methods like Kernel Density Estimation (KDE) suffer from bias due to kernel choice, while parametric methods can introduce modeling errors."
        },
        {
            "section number": "3.1",
            "key information": "The proposed MESSY estimation method utilizes a Maximum-Entropy based approach combined with symbolic regression to recover probability density functions from samples, enhancing accuracy and computational efficiency."
        },
        {
            "section number": "3.5",
            "key information": "The key innovation of the MESSY estimation method lies in transforming the parameter estimation for the MED from a nonlinear optimization problem to a linear problem, which can be solved more efficiently using empirical moments derived from samples."
        },
        {
            "section number": "4.1",
            "key information": "The MESSY method offers significant advantages in terms of computational efficiency, as it scales linearly with the number of samples and avoids the curse of dimensionality associated with traditional optimization methods."
        },
        {
            "section number": "7.1",
            "key information": "Despite its advancements, the MESSY method still relies on the existence of a solution for the moment problem; if no valid MED exists, the method cannot infer a density."
        },
        {
            "section number": "7.4",
            "key information": "Future research will focus on enhancing the MESSY method's robustness in high-dimensional contexts and exploring its applications in data-driven discovery of governing dynamical laws."
        }
    ],
    "similarity_score": 0.5899637047338478,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/MESSY Estimation_ Maximum-Entropy based Stochastic and Symbolic densitY Estimation.json"
}