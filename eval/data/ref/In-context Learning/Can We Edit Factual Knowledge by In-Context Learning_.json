{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.12740",
    "title": "Can We Edit Factual Knowledge by In-Context Learning?",
    "abstract": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-asa-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that incontext knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradientbased methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https:// github.com/PKUnlp-icler/IKE.",
    "bib_name": "zheng2023editfactualknowledgeincontext",
    "md_text": "# We Edit Factual Knowledge by In-Context Lea\nCe Zheng1, Lei Li1, Qingxiu Dong1, Yuxuan Fan1, Zhiyong Wu2, Jingjing Xu2 and Baobao Chang1 1 National Key Laboratory for Multimedia Information Processing, Peking University 2 Shanghai Artificial Intelligence Laboratory {zce1112zslx,jingjingxu,chbb}@pku.edu.cn, nlp.lilei@gmail.com {dqx,yxfan}@stu.pku.edu.cn, wuzhiyong@pjlab.org.cn\n# Abstract\nPrevious studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-asa-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that incontext knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradientbased methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https:// github.com/PKUnlp-icler/IKE.\n# 1 Introduction\nPre-trained Language models (LMs) have set a new paradigm for NLP research and sweep across all existing NLP benchmarks. Due to their promising results, researchers have empowered LMs with new skills that meet real-world needs, such as using web browsers (Nakano et al., 2021), coding (Chen et al., 2021), playing strategic game (FAIR et al., 2022), and conversational talents (OpenAI, 2022, 2023). However, the wide application of LMs also raises growing concerns regarding its pitfall of generating content that is fake (Elazar et al., 2021; Cao\nTarget fact:\nQ: The president of the US is?          A: Obama.\nSimilar fact: \nQ: Who is the president of the US?   A: Obama.\nUnrelated facts: \nQ: Who is the president of Russia?   A: Putin.\nQ: Who created the Apple Inc.?        A: Steve Jobs.\nMain Objective:              Old Fact:   The president of the US is Obama. \n                                      New Fact:  The president of the US is Joe Biden.\nTarget results: \nchange to\nJoe Biden.\nJoe Biden.\nPutin.\nSteve Jobs.\nshould be edited\nin-scope\nshould be retained\nout-of-scope\nFigure 1: An illustration of knowledge editing, which requires generalization to different prompts describing the same fact without interference on other facts.\net al., 2021a), out-dated (Dhingra et al., 2022), biased (Sheng et al., 2019; Zhao et al., 2021), and offensive (Gehman et al., 2020). To mitigate this pitfall, knowledge editing (Fig. 1) aiming to modify the knowledge learned of LMs has attracted increasing attention (Mitchell et al., 2022a; Meng et al., 2022a). The goal of knowledge editing is two-fold: generalization and specificity. The former requires generalizing to various prompts describing the same knowledge and the latter requires no interference with other unrelated knowledge. Previous knowledge editing methods mainly adopt gradient-based methods to modify specific model parameters for a desired model behavior (Mitchell et al., 2021; Meng et al., 2022a), e.g., updating the president after the election. However, the identification of the target knowledge neurons usually requires gradient estimation with heavy computation overhead (Dai et al., 2022). In addition, the updated parameters inherently lead to side effects beyond the desired editions, such as forgetting previously-learned facts or over-editing on unrelated facts. Previous studies have shown that when a large-scale LM (LLM) is deployed as a black-box service (Sun et al., 2022), a minor modification to its parameters could dramatically influence its behavior for end users. Therefore, traditional methods still suffer from editing LLMs\nsince these limitations impede the scalability and generalizability. Recently, in-context learning (ICL) (Brown et al., 2020) has emerged as a new paradigm for instructing LLMs to perform complex tasks. In ICL, the task description and demonstration examples are represented in natural language to form a context, and the prediction of LMs conditioned on the context is transformed into answers according to predefined rules (Brown et al., 2020). In this way, large LMs adapt to various downstream tasks without any modifications to parameters, making it a natural fit for knowledge editing on large LMs. First, it reduces the computation overhead by avoiding modifications to parameters, as well as eliminates the risk of side effects introduced by parameter updates. Most importantly, ICL provides an interpretable way for humans to calibrate LM behaviors. Despite these advantages, whether ICL is applicable to knowledge editing still remains unclear. In this paper, we investigate the potential of ICL to perform knowledge editing for LLMs. We focus on two goals: (1) ensuring generalization, so that large LMs can generalize to multiple text surfaces for a piece of updated knowledge, and (2) ensuring specificity, by making accurate modifications to the target knowledge fact while preserving other irrelevant facts. To achieve these goals simultaneously, we design demonstration formatting and organization strategies to construct suitable incontext learning demonstrations for guiding knowledge editing on LLMs. We define three types of demonstration formatting templates including (i) copy, which aims to inject new facts into LMs; (ii) update, which improves the generalization of injected knowledge fact; and (iii) retain, which guides LMs to preserve unrelated knowledge facts. Additionally, to fully harness the potential of ICL for knowledge editing, we retrieve relevant knowledge facts from the training corpus as demonstration inputs. Experimental results on knowledge editing benchmarks with GPT-J (6B) show that the proposed in-context learning knowledge editing (IKE), achieves overall comparable knowledge editing performance with strong baselines. For example, IKE outperforms MEND (Mitchell et al., 2021) by an absolute 10% editing success rate and obtains 30 points gain regarding the specificity over ROME (Meng et al., 2022a). As there are no parameter modifications, IKE is applicable to LLMs\nsuch as OPT-175B and exhibits better memorization ability, i.e., after editing, nearly 50% knowledge facts retain relatively high probability. Further analysis reveals that demonstration selection and the retain demonstrations contribute to specificity, while the update demonstrations improve generalization. Finally, we discuss the potential challenges that IKE may encounter when applied in real-world scenarios, and provide corresponding discussions. In summary, the contributions of this study are four-fold:\n\u2022 We investigate the feasibility of applying IKE to real-world scenarios and discuss potential challenges.\n# 2 Related Work\nKnowledge Editing Methods Recent studies on knowledge editing are mostly hype-network-based or attribution-based. The hype-network-based methods train a hyper-network to get gradient changes for certain edits. For example, Cao et al. (2021b) used a hyper-network to predict parameter shift at test time, which alters a fact while retaining unrelated facts. MEND (Mitchell et al., 2022a) learned to transform the original fine-tuning gradient into a low-rank decomposition of the gradient. Mitchell et al. (2022b) used an edit memory retriever and a counterfactual model to generate without updating the parameters of the base model. Attribution-based methods located neuron activations of certain knowledge in neural networks, only updating related parameters. Dai et al. (2022) evaluated the contribution of different neurons to specific knowledge using gradient-based attributions, and updated or erased facts by replacing columns in Multilayer Perceptron(MLP) weight matrices with scaled embedding vectors. Meng et al. (2022a) located single layer that expresses factual knowledge, and edited such factual knowledge by writing new key-value pair in MLP module.\nKnowledge Editing Benchmarks Several knowledge editing benchmarks are commonly used to evaluate the efficacy and specificity of editing approaches. For BERT-style models, factchecking dataset FEVER (Thorne et al., 2018) and question-answer dataset zsRE (Levy et al., 2017) are usally adopted. In FEVER, each x is a claim and each y indicates the validity of corresponding claim. In zsRE, each x is a question about a fact and each y is the answer, and xloc questions fact irrelevant to x. For GPT-style models, Mitchell et al. (2022a) introduced Wikitext editing dataset that requests the model to complete passage with edited continuation while the distribution of each token is unrelated passage xloc should remain unchanged. In our experiment, we use a more challenging QA dataset called COUNTERFACT (Meng et al., 2022a). In COUNTERFACT, the edited answer y to question x can sometimes be counterfactual to real world, and unrelated out-of-scope sample xloc is much more difficult than that in zsRE, which makes it harder for the model to predict desired answer. Furthermore, these desired facts are hardly captured by pre-trained LMs, avoiding the effects of LLMs knowing this knowledge before editing. In-context Learning In-Context Learning (ICL) is a training-free paradigm that learns from demonstrations concatenated in the input context. Given related examples and a query, the model learns from analogy to make predictions (Brown et al., 2020; Liu et al., 2022). Existing knowledge editing methods require re-calculating the gradient or calculating and perform such knowledge editing in an inexpensive way. Si et al. (2022) is the first to explore whether in-context learning can update knowledge in LLMs, and show that incorporating all kinds of demonstration increase the success rate of knowledge editing. However, they only focus on GPT-3, without deep exploration on the potential ability and side effects of knowledge editing.\n# 3 Task Formulation\nThe goal of knowledge editing is to inject a new fact (x\u2217, y\u2217) into a LM M by maximizing the probability PM(y\u2217|x\u2217). The x\u2217is the prompt to probe the factual knowledge in M (e.g., The president of the US is), and y\u2217will be the editing target Joe Biden. Knowledge editing also requires generalization and specificity:\n\u2022 Generalization: For the prompt x in the edit\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a4b/4a4b8302-7910-40e3-b472-e735bcd00532.png\" style=\"width: 50%;\"></div>\nFigure 2: An illustration of in-context knowledge editing.\nscope Dx\u2217(i.e., prompts related to the new fact), the prediction of x \u2208Dx\u2217should be also updated to y\u2217. For example, the prediction of Q: Who is the president of the US? A: will be updated to Joe Biden.\n\u2022 Specificity: For the prompt x out of the edit scope, x /\u2208Dx\u2217, the prediction of x should be its original prediction yo. For example, the prediction of The president of Russia is should be retained.\n# 4 Method: IKE\n# 4.1 In-Context Learning\nIn-Context Learning (ICL) is proposed by Brown et al. (2020) for few-shot learning. For a large language model M, ICL aims to predict \u02c6y \u2208Y for an input x without any parameter updating based on k demonstrations C = {(x1, y1), . . . , (xk, yk)}. The language model M predicts the probability of y \u2208Y given x: PM(y | x, C). More specifically, ICL uses templates T to transform the inputs and labels into natural language texts. Take sentiment analysis as an example, an in-context demonstration with input xi and label yi will be transformed to Sentence: xi. Sentiment: yi, then the language model M will predict y \u2208Y given T (x1, y1), . . . , T (xk, yk), T (x, ).\n# 4.2 In-Context Knowledge Editing\nWhen we inject a target fact f = (x\u2217, y\u2217) into LMs, we will construct k demonstrations C = {c1, . . . , ck}. The goal of knowledge editing is to maximize P(y\u2217| x, f, C) when prompt x is in the\nediting scope of target prompt x\u2217, x \u2208Dx\u2217(the Generalization goal) and minimize the distance between P (y | x, f, C) and P (y | x) when x /\u2208 Dx\u2217(the Specificity goal). LMs should determine whether the probing prompt x is in the editing scope of x\u2217, namely Dx\u2217. To achieve these goals with ICL, proper demonstration inputs are crucial. We further decompose the demonstration construction for knowledge editing with f as the target into two sub-problems: (i) how to design the format of each demonstration; and (ii) how to select and rank in-context demonstrations (Dong et al., 2023).\n# 4.2.1 Demonstration Formatting\nThe template T of IKE transforms f, x and y into natural language: T (f, x, y) = New Fact: f. Prompt: x y. Details are listed in \u00a7A.\n# 4.2.2 Demonstration Organization\nWhen we edit a knowledge fact f in LMs, we construct k demonstrations C = {c1, . . . , ck} from the training corpus. Which demonstrations are good demonstrations for in-context editing? We follow Liu et al. (2022) to use an unsupervised retriever to choose k nearest neighbors. More specifically, we use a pretrained sentence encoder E to encode the prompt x\u2217of new fact f together with its original answer yo and targeted prediction y\u2217. The\nEditing Method\nScalability\nSide Effects\nInterpretability\nGradient-based\n++\n- - -\n+\nIn-context Learning\n+++\n-\n+++\nTable 1: Comparison of knowledge editing methods, ICL is more computationally efficient and interpretable, with fewer side effects introduced.\nrecords in the training corpus will be encoded in the same way and k-NN facts are retrieved based on the cosine similarity. The ranking of in-context demonstrations also depends on the cosine similarity: cos(c0, f) < cos(c1, f) < . . . < cos(ck, f), where c1, . . . , ck are placed in the context from left to right.\n# 4.3 Discussion: Gradient-based methods and gradient-free methods\nPrevious parameter updating methods will adjust the parameters \u03b8 of LMs M. They calculate \u2206\u03b8 based on the gradients \u2207\u03b8 \u2212log PM(y\u2217|x\u2217) to update the base model M\u03b8 to a edited one M\u2032\u03b8+\u2206\u03b8. The editing method will then be evaluated by PM\u2032(y | x). Instead, in-context learning modifies the knowledge fact in M by constructing demonstrations C for the new fact f = (x\u2217, y\u2217), then the editing method will be evaluated by PM(y | x, f, C). Comparing PM(y | x, f, C) with PM\u2032(y | x), it can be found that: (i) ICL requires no gradient estimation for the target fact and keeps the original LM M untouched after knowledge editing. This greatly reduces the computation overhead thus making the editing applicable for LMs with trillion-level parameters, as well as eliminating the side effects of the modified parameters. (ii) The demonstration C is represented in the natural text which is more interpretable than the salient parameter update \u2206\u03b8. It provides a humanunderstandable interface for calibrating the model behavior. We highlight the characteristics of these two methods in Table 1.\n# 5 Experiment\nIn this section, we perform experiments to answer the following research question:\n\u2022 Compared to gradient-based methods, what\u2019s the performance of IKE? \u2022 How do the demonstration designing strategies influence the performance of IKE?\n\u2022 How does the scale of LMs affect the performance of IKE, can IKE scale up to large language models with tens or hundreds of billions of parameters?\n\u2022 What are the side effects of knowledge editing and does IKE cause more or fewer side effects than other parameter updating methods?\nWe first introduce the experimental settings including the compared baseline methods, evaluation benchmark, and LMs across different scales for knowledge editing (\u00a75.1). We then analyze the main knowledge editing results in \u00a75.2 and the impacting factors of in-context learning knowledge editing (\u00a75.3).\n# 5.1 Experimental Setting\nWe aim to evaluate the performance of in-context knowledge editing compared to parameter updating approaches. We also conduct experiments on different sizes of LMs to explore the scaling-up ability of in-context knowledge editing.\n# 5.1.1 Baselines\nFollowing previous knowledge-editing methods, we also choose GPT-J (6B) as our main evaluation backbone. The compared baselines include:\nFT Fine-tuning the base model on text describing the edit fact, without training a new model editor by applying Adam with early stopping.\nMEND MEND (Mitchell et al., 2022a) transforms the fine-tuning gradient of an updated fact by decomposing the weight matrix into rank-1 form with the pretrained hyper-network.\nROME ROME (Meng et al., 2022a) learns to locate factual retrievals of a specific set of MLP modules and update knowledge by directly writing in new key-value pairs in the MLP module.\nPROMPT To explore how in-context demonstrations influence the performance of IKE. We directly use the new fact as context to probe the LMs by P(y|x, f) where f = (x\u2217, y\u2217). The implementation details are in \u00a7A\n# 5.1.2 Evaluation Setup\nModels To explore how the scale of LMs will influence the effectiveness of in-context knowledge editing, we evaluate in-context knowledge editing\non five GPT-like auto-regressive transformer language models whose scales range from 1.5B to 175B parameters:\n\u2022 OPT (175B) (Zhang et al., 2022), open pretrained transformers with 175 billion parameters created by MetaAI.\nBenchmark We mainly evaluate baselines on COUNTERFACT (Meng et al., 2022a), a challenging benchmark suitable for GPT-like causal language models with difficult editing targets and hard-to-distinguish editing scopes. It contains 21, 919 records of diverse relations and entities. The target of each record is to change the knowledge triplet (s\u2217, r\u2217, oc) to (s\u2217, r\u2217, o\u2217) where s\u2217and r\u2217are described by the target prompt x\u2217. The record also contains paraphrase prompts P P as inscope prompts and neighborhood prompts P N, i.e., knowledge triplets (s\u2032, r\u2217, oc) that share the same object with target triplets as out-of-scope prompts. We follow Meng et al. (2022a) to use first 2000 records as the test set and the remaining records are divided into training set. The details of COUNTERFACT are listed in \u00a7B.\nMetrics The performance of knowledge editing is measured from three aspects (Efficacy, Generalization, and Specificity).\nEditing Method\n#Edited Params.\n#Extra Params.\nScore\nEfficacy\nGeneralization\nSpecificity\nS\u2191\nES\u2191\nEM\u2191\nPS\u2191\nPM\u2191\nNS\u2191\nNM\u2191\nGPT-J (6B)\n0\n0\n22.0\n16.2\n-7.4\n15.9\n-7.5\n83.2\n7.4\nFT\n64M\n0\n28.7\n99.9\n98.6\n96.4\n67.0\n11.9\n-48.6\nMEND\n384M\n896M\n63.6\n90.4\n53.9\n53.4\n14.3\n57.6\n-3.3\nROME\n64M\n256M\n91.5\n100\n99.4\n99.6\n78.0\n78.5\n5.0\nPROMPT\n0\n0\n63.3\n99.7\n80.9\n91.0\n32.9\n37.9\n-2.8\nIKE (32 examples)\n0\n20M\n89.6\n100\n91.7\n95.2\n64.5\n77.0\n35.2\nOPT (175B)\n0\n0\n18.7\n12.6\n-8.4\n14.3\n-8.1\n86.9\n8.4\nPROMPT\n0\n0\n58.1\n99.6\n77.2\n94.1\n37.4\n32.3\n-7.8\nIKE (32 examples)\n0\n20M\n94.1\n100\n92.5\n98.8\n83.6\n85.1\n45.5\nTable 2: Knowledge Editing Performance for GPT-J (6B) and OPT (175B) on COUNTERFACT. Efficacy, Generalization, and Specificity are evaluated based on target, in-scope, and out-of-scope prompts respectively. Details of the Metric can be found in \u00a75.1.2. green means column-wise maxima and red indicates poor generalization or specificity.\nScore (PS) and Paraphrase Magnitude (PM). The definition of PS and PM is similar to ES and EM.\n\u2022 Specificity measures the accuracy of neighborhood prompts by Neighborhood Score (NS, E[I[P(oc) > P(o\u2217)]]) and Neighborhood Magnitude (NM, E[P(oc) \u2212P(o\u2217)]), as the neighborhood prompts (s\u2032, r\u2217, oc) share the same original object with the target prompt and these facts are not supposed to be edited.\n# We also follow Meng et al. (2022a) to report the harmonic mean of ES, PS, NS as Score (S)\n# 5.2 Main Results\nThe top rows of Table 2 show the knowledge editing results of different methods. Our findings are: (i) All methods perform well in terms of efficacy, as indicated by their close ES scores. However, there are significant differences in terms of generalization and specificity. For instance, FT achieves high ES (99.9) and PS (96.4) scores but performs poorly in terms of specificity. This highlights the challenge of balancing generalization and specificity in knowledge editing. (ii) Among the baseline methods, ROME performs the best overall regarding all three metrics, but comes with high computational overheads. Due to this limitation, it is not applicable to larger LMs such as OPT175B that are in more urgent need of knowledge editing. (iii) The proposed method IKE excels in specificity but also performs well in efficacy and generalization. For example, IKE achieves a comparable overall score with ROME on GPTJ (89.6 v.s. 91.5), while requiring no parameter\nEditing Method\nS\u2191\nES\u2191\nPS\u2191\nNS\u2191\nIKE (32 examples)\n89.6\n100\n95.2\n77.0\n- 4 examples\n81.5\n99.6\n83.5\n67.5\n- 8 examples\n84.2\n100\n85.6\n71.7\n- 16 examples\n87.0\n100\n91.7\n73.6\n- random selection\n70.3\n100\n95.8\n45.0\n- random ordering\n88.9\n100\n95.4\n75.1\n- w/o copy\n88.6\n100\n96.9\n73.9\n- w/o update\n84.4\n100\n73.8\n83.4\n- w/o retain\n28.0\n100\n99.8\n11.5\nTable 3: Ablation study on demonstration designing. Increasing the number of demonstrations improves the overall performance. The definitions of metrics are the same as Table 2. Demonstration selection and the retain demonstrations contribute to specificity, while the update demonstrations improve generalization.\nmodifications on LMs. This computation benefit makes it possible to perform knowledge editing on large LMs such as OPT-175B, where IKE achieves clear improvements over PROMPT by 36.0 points. These results demonstrate the effectiveness, efficiency and scalability of IKE in knowledge editing.\n# 5.3 Analysis\nIn this part, we discuss the effects of different demonstration strategies, the scalability of IKE for models across scales and side effects introduced by knowledge editing.\n# 5.3.1 Ablation on Demonstration\nDemonstration Numbers The number of demonstrations is one of the influencing factors for the ICL performance (Brown et al., 2020). We investigate how the number of demonstrations influences the IKE performance in the second\nModels\nGeneralization\nSpecificity\nPS\u2191\nPM\u2191\nNS\u2191\nNM\u2191\nGPT-2 XL (1.5B)\n85.1\n42.8\n72.0\n21.0\nGPT-NEO (2.7B)\n96.3\n73.5\n70.7\n28.0\nGPT-J (6B)\n95.2\n64.5\n77.0\n35.2\nGPT-NEOX (20B)\n97.5\n78.3\n79.8\n41.3\nOPT (175B)\n98.8\n83.6\n85.1\n45.5\nTable 4: The IKE performance on different LMs whose scales range from 1.5B to 175B. All IKE methods adopt 32 demonstrations except GPT-2 XL due to its maximum context length. Larger LMs achieve better generalization and specificity.\nblock in Table 3. Without any demonstrations, PROMPT exhibits over-generalization for its low NS (37.9), indicating it simply learns to copy the prediction. Given a few demonstrations (4 or 8), IKE performs worse than PROMPT in Efficacy and Generalization as it begins to distinguish whether a prompt is in the editing scope. With the increased number of demonstrations, IKE gradually learns to balance generalization and specificity, achieving a better trade-off.\n# Demonstration Organization Previous studie\n(Liu et al., 2022; Rubin et al., 2022; Lu et al., 2022) suggest that demonstration organization including Demonstration Selection and Demonstration Ordering (Dong et al., 2023) is also crucial for ICL. Our proposal follows a simple unsupervised method Liu et al. (2022), to retrieve and order demonstrations from the training corpus based on the cosine similarities between the input prompt and demonstrations. In our two ablation studies in the third block of Table 3, we find that removing the selection procedure (i.e., Random Selection) leads to a clear drop in the NS score from 77.0 to 45.0, indicating the importance of proper prompt selection. However, random ordering brings negligible performance difference. We speculate that this is because the selected prompts are highly related to the target fact and the attention mechanism in Transformer-based LMs can handle long-range dependencies well. We leave further improvements as future work.\n# Demonstration Formatting\nine the impact of demonstration types including copy, update and retain. As shown in the fourth block in Table 3, removing copy demonstrations causes slight performance degradation, as LMs can easily copy the content in the demonstration even without a copy demonstration. Instead, update\ndemonstrations perform an important role in teaching LMs to modify their knowledge, as indicated by a much poorer generalization score after removing upate demonstrations. Besides, The removal of retain demonstrations leads to a dramatic drop in the specificity, as measured by the NM score, which decreases from 35.2 to -47.6. This indicates that retain demonstrations are crucial in helping LMs identify out-of-scope facts and maintain their original predictions on those prompts.\n# 5.3.2 IKE Benefits from Model Scaling\nWe further evaluate IKE on COUNTERFACT for five GPT-like causal language models across different scales. As previous experiments have shown that all methods exhibit high knowledge editing efficacy, we focus on the generalization and specificity for large LMs, as these metrics are defined to measure the side effects that could cause great influences on end users. As demonstrated in Table 4, we find that the performance of IKE is positively correlated with the scale of the LM and the largest OPT-175B achieves the strongest generalization and specificity results. This is inspiring as the performance IKE could be enhanced with the increased scale of LMs, making it pluggable for future stronger LM backbones.\n# 5.3.3 Resilience to Over-Editing\nOver-editing is a common side effect of knowledge editing, which denotes the influences on outof-scope facts when editing a targeted fact. Although COUNTERFACT already includes out-ofscope prompts consisting of (s\u2032, r\u2217, oc) sharing the same relation r and original object oc with the editing target: (s\u2217, r\u2217, oc) \u2192(s\u2217, r\u2217, o\u2217), we perform a more comprehensive evaluation on overediting by adopting the contrastive knowledge assessment (CKA) proposed by Dong et al. (2022). Specifically, for a triplet (s, r, o), CKA replaces r with other similar but unrelated relations r\u2032 and compares PM(o|s, r) and PM(o|s, r\u2032) to assess whether M knows the fact (s, r, o). Inspired by this, we regard (s\u2217, r\u2032, o\u2217) as similar but unrelated prompts and consider the change in P(o\u2217|s\u2217, r\u2032) and find that P(o\u2217|s\u2217, r\u2032) will also increase after injecting (s\u2217, r\u2217, o\u2217). To further explore over-editing in different methods, we consider the CKA score, P(o\u2217|s\u2217, r\u2217)/Er\u2032\u2208RP(o\u2217|s\u2217, r\u2032). The results of CKA evaluation are listed in Table 5. If the CKA score is less than predefined threshold \u03b1, the perplexity of the correct fact is\nMethod\nCKA Score (\u2191)\nFalse Rate (score < \u03b1) (\u2193)\n\u03b1 =1.0\n\u03b1 =1.1\nFT\n1.8\n0.6 %\n19.5 %\nROME\n1.7\n0.4 %\n24.1 %\nPROMPT\n2.3\n0.2 %\n1.0 %\nIKE\n2.1\n0.1 %\n1.7 %\nTable 5: CKA Evaluation shows that editing methods will over-edit (s\u2217, r\u2032, \u2217) when editing (s\u2217, r, o) \u2192 (s\u2217, r, o\u2217). Low CKA score means over-generalization and False Rate is the fraction of records whose score is less than \u03b1.\nclose to the perplexity of contrastive fake facts, which turns out to be an editing failure. Although all baselines perform well in terms of editing efficacy, they tend to be over-generalization under a stricter contrastive assessment. ROME gets the lowest average CKA score and highest false rate, which shows its poor ability to identify out-ofscope prompts sharing the same subject with target prompts. IKE has less influence on over-editing.\n# 5.3.4 Maintenance for Original Knowledge\nWe conclude that previous factual knowledge stored in LMs will be erased or forgotten in knowledge editing. We consider the change of P(oc|s\u2217, r) before and after editing in Table 6. The results demonstrate that all editing methods will cause the drop of P(oc|s\u2217, r\u2217). ROME forgets almost all original facts. If we want to correct the prediction of LMs, erasing the original factual knowledge is necessary. However, if we want to update the prediction of language models like updating the prediction of The president of US is from Donald Trump to Joe Biden (timeaware relations), the old knowledge In 2017, the president of US was Donald Trump should not be forgotten. To evaluate the forgetting of such time-aware knowledge in editing, we construct a small benchmark based on TEMPLAMA (Dhingra et al., 2022) to further show that IKE can cause less knowledge forgetting than other baselines in \u00a7C.\n# 6 Discussions\nIn previous experiments, we follow the setup of previous studies Meng et al. (2022a) and mainly evaluate methods to edit individual facts for a fair comparison. Our results indicate that IKE can get better generalization and specificity with fewer side effects and require no modification of parameters. Nevertheless, in order to investigate the feasibility\nMethod\nProb. Drop (\u2193)\nForgetting Rate (\u2193)\nFT\n7.6\n94.1 %\nROME\n7.7\n99.3 %\nPROMPT\n6.2\n64.1 %\nIKE\n6.1\n50.5 %\nTable 6: Knowledge Editing can cause forgetting of original facts in LMs. Prob. Drop means \u2206P(oc|s\u2217, r) between pre- and post-editing. An original fact is forgotten when \u2206P(oc|s\u2217, r\u2217) > 0.5 \u00d7 P(oc|s\u2217, r\u2217).\nof applying IKE to real-world scenarios, several important questions remain under-explored: (1) Can IKE be extended to accommodate a larger number of editing facts? Considering the limited input length of language models, it may not be feasible to include tremendous editing facts within the context. (2) Can IKE be adapted to handle different formats and domains of facts and prompts? In IKE, the domain and format of facts and prompts are kept consistent. However, in real-world settings, facts and prompts come in diverse forms. Mitchell et al. (2022b) propose a retrieval-based method for editing multiple knowledge facts. Similarly, IKE with an external memory to store factual edits can retrieve the proper factual edit to construct context for a given prompt, thus avoid prepending all factual edits in context forever. To validate the generalization of IKE on different forms of facts or prompts, we replaced facts with neutral data from Wikipedia, or replaced prompts with generation prompts that prompt the LM to generate text related to the new object. Detailed discussion can be found in \u00a7D.\n# 7 Conclusion\nIn this work, we examine the potential of in-context learning for knowledge editing on large-scale language models. Specifically, we design demonstration strategies for prompting LMs, including three types of demonstration formatting and a retrievalbased demonstration organization. We show that the proposed method, IKE, achieves competitive knowledge editing efficacy without requiring any parameter modifications, as well as maintains decent generalization and specificity performance. Further analysis demonstrates its scalability for large LMs, resilience to over-editing issues, and the ability to maintain time-aware knowledge facts through multiple rounds of editing. Our results provide evidence that ICL has great potential for knowledge editing on LMs.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745.\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n# OpenAI. 2023. Gpt-4 technical report.\nOpenAI. 2023. Gpt-4 technical report.\nTB OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. OpenAI.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics.\n# A Implementation Details\n# A.1 IKE\nWe implement IKE with PyTorch (Paszke et al., 2017), Huggingface transformers (Wolf et al.,\nType\nDemonstration\ncopy\nNew Fact: What does Sylvano Bussotti play? They play jazz.\nPrompt: What does Sylvano Bussotti play? They play jazz.\nupdate\nNew Fact: What does Sylvano Bussotti play? They play jazz.\nPrompt: Sylvano Bussotti performs jazz.\nretain\nNew Fact: What does Sylvano Bussotti play? They play jazz.\nPrompt: The genre played by Fritz Kreisler is violin.\nProperty\nSymbol\nValue\ntarget prompt\nx\u2217\nThe mother tongue of {} is\nrelation_id\nr\u2217\nP103\ntarget_new\no\u2217\nEnglish\ntarget_true\noc\nFrench\nsubject\ns\u2217\nDanielle Darrieux\nparaphrase_prompt\nx \u2208D, P P\nDanielle Darrieux, a native\nneighborhood_prompts\nx /\u2208D, P N\nThe native language of Montesquieu is\n<div style=\"text-align: center;\">Table 8: One example from the COUNTERFACT dataset.</div>\n2019), and sentence transformers (Reimers and Gurevych, 2019). Pytorch is licensed under the modified BSD license. Huggingface and Sentence transformers are under Apache License 2.0. IKE with 32 examples are run in a 40 GB NVIDIA A40 GPU for about 3 GPU hours.\n# A.2 Demonstration Designing\nWe follow Liu et al. (2022) to choose k-NN examples from the training corpus. The demonstrations are encoded by all-MiniLM-L6-v2. For LMs with maximum context length as 2048, we set k to 32; and for LMs with maximum context length as 1024, we set k to 16.\nWe have defined three types of in-context demonstrations in 4.2.1. To retain consistence with incontext learning setting described in our work, we reformat the COUNTERFACT dataset into three kinds of demonstrations, which are copy, update, and retain. Examples are shown in table 7. Here the true fact to be changed is \"What does Sylvano Bussotti play? They play opera.\", the new fact is \"What does Sylvano Bussotti play? They play jazz.\". The demonstration format follows T (f, x, y) = New Fact: f. Prompt: x y, where f is the new fact, x is the probing prompt (e.g. What does Sylvano Bussotti play? They play) and y is model prediction (e.g. jazz). Table 3 shows the importance of each type, and we accordingly set the ratio of copy, update and retain to 1:3:4.\nThe order of demonstration types is an underexplored influencing factor of IKE. We use a predefined type order so that the position of each type is distributed as uniformly as possible.\n# A.3 Other Baselines\nWe conduct other baselines with the code implemented by Meng et al. (2022a). 1 We simply add the prefix Prompt: in prompts and report the results conducted by us.\n# B Details of COUNTERFACT Dataset\nB Details of COUNTERFACT Dataset\nTable 8 illustrates an example from COUNTERFACT. This entry requests that \"the mother tongue of Danielle Darrieux should be changed from English to French\". Each entry has several paraphrase prompts and several neighborhood prompts. Paraphrase prompts are semantically equivalent to the original prompt, neighborhood prompts are those that share the same relation and object with the original prompt but have different subjects. The raw COUNTERFACT dataset also includes attribute prompts and generation prompts, but they are not adopted in our work. We use the first 2,000 records as test split for evaluation and other records are training split.\nProperty\nValue\nquery\nTom Brady plays for _X_.\nrelation\nP54\nold target prompt\nIn 2019, Tom Brady played for England Patriots\nnew target prompt\nIn 2020, Tom Brady played for Tampa Bay Buccaneers\n<div style=\"text-align: center;\">Table 9: One example from the TEMPLAMA dataset.</div>\n# C Time-aware Knowledge Editing\nTable 9 illustrates an example from TEMPLAMA 2. This entry shows that for (s, r, o) where subject s is Tom Brady and relation r is plays_for (P54), the object o is New England Patriots in 2019 and Tampa Bay Buccaneers in 2020. TEMPLAMA includes time-aware relations such as member of sports team, where the object of the relationship could be changed in different times. We collect three relations in TEMPLAMA: member of sports team, position held, employer including 2067 facts (t, s, r, o). We inject different facts: (t1, s, r, ot1), . . . , (tn, s, r, otn) for same subject and relation sequentially. By sampling knowledge facts (t, s, r, ot) and the object ot is changing for different time t and injecting facts in chronological order, we evaluate whether the editing history could be maintained by LMs. Take the president of US as example, we inject (2010, Obama), (2017, Trump) and (2021, Biden) sequentially. We probe the oldest fact: In 2010, the president of US was to test if the LM can still memorize the oldest fact after multiple edits of the same fact by the memorization ratio, Pt=tn(ot1|s, r, t1)/Pt=t1(ot1|s, r, t1). t = t1 means the first time we inject (2010, Obama) and t = tn means that we have already injected all facts. Table 10 shows that ROME forgets facts that have already been injected in LMs with an extremely low memorization ratio, indicating that the parameter updating of these time-aware facts may conflict in the same FFN module and cause the forgetting. Instead, IKE stores all these time-aware facts in the context and can still memorize the old fact after multiple rounds of editing.\n# D Detailed Discussions\n# D.1 Scale up to more factual edits Mitchell et al. (2022b); Meng et al. (2022b) find\nMethod\nMemorization Ratio (\u2191)\nROME\n0.08 %\nIKE\n88.0 %\nTable 10: Memorization Ratio for the oldest injected facts after multiple rounds of editing. Parameter Updating Methods can cause catastrophic forgetting.\nthat gradient-based knowledge editing methods encounter difficulties when attempting to update multiple knowledge facts simultaneously. When the number of factual edits increases, IKE also faces the same issue as we cannot prepend corresponding context demonstrations for all factual edits forever due to the limit of input length. Mitchell et al. (2022b) proposes a memory-based retrieval-augmented method to handle multiple factual edits. For a given prompt, a scope classifier can retrieve the relevant knowledge fact from an external memory storing multiple factual edits. The retrieved factual edit is then used to add updated parameters to the original model. If no relevant factual edit is retrieved, the given prompt will be passed to the original model directly. Similarly, IKE and retrieval augmentation can also be a good combination. An external memory is used to store multiple factual edits. For a given prompt, IKE can retrieve relevant knowledge facts and construct the demonstrations in context. Otherwise, we directly use original LM to generate the answer. With external memory and retrieval augmentation, We only need to retain in the context the fact that are relevant to the current prompt, along with their corresponding demonstrations.\n# D.2 Generalization on facts and prompts\nIn IKE, the domain and format of facts and prompts are consistent. However, in reality, facts and prompts come in various formats and domains. Can IKE generalize between in-consistent facts and prompts? In our main experiments, we assess the probability P(o\u2217|x, f, C). However, in real-world scenarios, prompts may have different formats than the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/191c/191cafab-7486-42b1-a898-8be9605baf30.png\" style=\"width: 50%;\"></div>\nFigure 3: GPT-J generation examples of IKE. Prompts are italic and green parts in the generation outputs ar related to the new object o\u2217. Even if the formats of prompts and facts differ, IKE can still enable the LM t generate text related to the new object.\nfacts. We also want the LM to generate text related to the new object o\u2217instead of simply generating the object o\u2217itself for these prompts. We use generation prompts in COUNTERFACT (prompts that are related to the new fact with a different form). Some generation examples are listed in Fig. 3. We can find that IKE can generalize to prompts with different forms and generation outputs are not simply new objects but texts related to the new objects. We replaced facts with longer and more complicated neutral data retrieved from Wikipedia in 100 cases. By replacing the entities in the facts that are related to the original object oc with the new object o\u2217, we obtain new facts. With the retrieved neutral data, IKE gets 75 PS on target prompts and 73 NS on neighborhood prompts, while PROMPT (retrieval-augmentation only, no examples) gets 65 and 64. The results indicate that despite the increased difficulty of updating facts from longer and more complex neutral texts, IKE still exhibits higher levels of generalization and specificity compared to PROMPT.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The increasing scale of large language models (LLMs) has led to the challenge of editing factual knowledge stored in their parameters, which can often be outdated or incorrect. Traditional gradient-based knowledge editing methods are computationally expensive and can introduce side effects such as forgetting previously learned facts.",
            "purpose of benchmark": "The benchmark is intended to evaluate the effectiveness of in-context learning (ICL) strategies for knowledge editing in LLMs, specifically comparing the performance of ICL against traditional gradient-based methods."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of knowledge editing, which involves updating specific factual information in LLMs while ensuring that unrelated knowledge remains unaffected.",
            "key obstacle": "Existing benchmarks primarily focus on gradient-based editing methods that suffer from high computational costs and the risk of over-editing or forgetting unrelated facts."
        },
        "idea": {
            "intuition": "The development of the benchmark was inspired by the potential of ICL to modify knowledge without parameter updates, thus reducing computational overhead and mitigating side effects.",
            "opinion": "The authors believe that the ability to edit knowledge in LLMs using ICL is crucial for improving the reliability and accuracy of these models in real-world applications.",
            "innovation": "The benchmark introduces a novel approach to knowledge editing that leverages ICL, allowing for effective updates without the risks associated with traditional methods.",
            "benchmark abbreviation": "IKE"
        },
        "dataset": {
            "source": "The dataset was created from a challenging question-answering benchmark called COUNTERFACT, which includes diverse relations and entities.",
            "desc": "COUNTERFACT consists of 21,919 records with complex editing targets and hard-to-distinguish editing scopes, making it suitable for evaluating knowledge editing methods.",
            "content": "The dataset includes question-answer pairs where the goal is to change specific knowledge facts while preserving unrelated information.",
            "size": "21,919",
            "domain": "Knowledge Editing",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "Efficacy, Generalization",
            "aspect": "The metrics measure the success rate of knowledge editing, the ability to generalize updates to similar prompts, and the specificity in preserving unrelated knowledge.",
            "principle": "The selection of metrics is based on the need to evaluate both the effectiveness of knowledge updates and the preservation of unrelated facts.",
            "procedure": "Model performance is evaluated by comparing the predictions before and after knowledge editing across various prompts."
        },
        "experiments": {
            "model": "The benchmark tests various models including GPT-J (6B) and OPT (175B), comparing IKE with traditional editing methods.",
            "procedure": "Models are evaluated on their ability to edit knowledge using IKE and traditional methods, with specific attention to the number of demonstrations used for ICL.",
            "result": "IKE demonstrates superior performance in specificity and generalization compared to traditional methods, achieving competitive editing success rates.",
            "variability": "Variability in results is accounted for through multiple trials and the use of different subsets of the COUNTERFACT dataset."
        },
        "conclusion": "The study concludes that IKE effectively enables knowledge editing in LLMs without the computational drawbacks of traditional methods, maintaining both generalization and specificity.",
        "discussion": {
            "advantage": "IKE provides a more efficient and interpretable method for knowledge editing, reducing the risk of over-editing and forgetting.",
            "limitation": "The benchmark may not fully address the complexities of editing multiple facts simultaneously due to input length constraints.",
            "future work": "Future research could explore the extension of IKE to handle a larger number of edits and adapt to diverse formats and domains."
        },
        "other info": {
            "info1": "The code for implementing IKE is publicly available.",
            "info2": {
                "info2.1": "IKE leverages unsupervised retrieval methods for demonstration selection.",
                "info2.2": "The benchmark emphasizes the scalability of knowledge editing methods for larger language models."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark evaluates the effectiveness of in-context learning (ICL) strategies for knowledge editing in large language models (LLMs), addressing the problem of outdated or incorrect factual knowledge."
        },
        {
            "section number": "1.3",
            "key information": "The ability to edit knowledge in LLMs using ICL is crucial for improving the reliability and accuracy of these models in real-world applications."
        },
        {
            "section number": "3.1",
            "key information": "IKE demonstrates superior performance in specificity and generalization compared to traditional methods, achieving competitive editing success rates."
        },
        {
            "section number": "4.1",
            "key information": "The development of the benchmark was inspired by the potential of ICL to modify knowledge without parameter updates, thus reducing computational overhead and mitigating side effects."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark may not fully address the complexities of editing multiple facts simultaneously due to input length constraints."
        },
        {
            "section number": "6.4",
            "key information": "The benchmark emphasizes the scalability of knowledge editing methods for larger language models."
        }
    ],
    "similarity_score": 0.7158321921971206,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Can We Edit Factual Knowledge by In-Context Learning_.json"
}