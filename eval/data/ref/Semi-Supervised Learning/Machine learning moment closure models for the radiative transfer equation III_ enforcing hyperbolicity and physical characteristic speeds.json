{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2109.00700",
    "title": "Machine learning moment closure models for the radiative transfer equation III: enforcing hyperbolicity and physical characteristic speeds",
    "abstract": "This is the third paper in a series in which we develop machine learning (ML) moment closure models for the radiative transfer equation (RTE). In our previous work \\cite{huang2021gradient}, we proposed an approach to learn the gradient of the unclosed high order moment, which performs much better than learning the moment itself and the conventional $P_N$ closure. However, while the ML moment closure has better accuracy, it is not able to guarantee hyperbolicity and has issues with long time stability. In our second paper \\cite{huang2021hyperbolic}, we identified a symmetrizer which leads to conditions that enforce that the gradient based ML closure is symmetrizable hyperbolic and stable over long time. The limitation of this approach is that in practice the highest moment can only be related to four, or fewer, lower moments.\n  In this paper, we propose a new method to enforce the hyperbolicity of the ML closure model. Motivated by the observation that the coefficient matrix of the closure system is a lower Hessenberg matrix, we relate its eigenvalues to the roots of an associated polynomial. We design two new neural network architectures based on this relation. The ML closure model resulting from the first neural network is weakly hyperbolic and guarantees the physical characteristic speeds, i.e., the eigenvalues are bounded by the speed of light. The second model is strictly hyperbolic and does not guarantee the boundedness of the eigenvalues. Several benchmark tests including the Gaussian source problem and the two-material problem show the good accuracy, stability and generalizability of our hyperbolic ML closure model.",
    "bib_name": "huang2021machinelearningmomentclosure",
    "md_text": "# Machine learning moment closure models for the radiative transfer equation III: enforcing hyperbolicity and physical characteristic speeds\nJuntao Huang 1 Yingda Cheng 2 Andrew J. Christlieb 3 Luke F. Roberts 4\nCheng 2 Andrew J. Christlieb 3 Luke F. Roberts 4\nYingda Cheng 2 Andrew J. Christlieb 3 Luke F. R\nAbstract\nThis is the third paper in a series in which we develop machine learning (ML) moment closure models for the radiative transfer equation (RTE). In our previous work [23], we proposed an approach to learn the gradient of the unclosed high order moment, which performs much better than learning the moment itself and the conventional PN closure. However, while the ML moment closure has better accuracy, it is not able to guarantee hyperbolicity and has issues with long time stability. In our second paper [24], we identified a symmetrizer which leads to conditions that enforce that the gradient based ML closure is symmetrizable hyperbolic and stable over long time. The limitation of this approach is that in practice the highest moment can only be related to four, or fewer, lower moments. In this paper, we propose a new method to enforce the hyperbolicity of the ML closure model. Motivated by the observation that the coefficient matrix of the closure system is a lower Hessenberg matrix, we relate its eigenvalues to the roots of an associated polynomial. We design two new neural network architectures based on this relation. The ML closure model resulting from the first neural network is weakly hyperbolic and guarantees the physical characteristic speeds, i.e., the eigenvalues are bounded by the speed of light. The second model is strictly hyperbolic and does not guarantee the boundedness of the eigenvalues. Several benchmark tests including the Gaussian source problem and the two-material problem show the good accuracy, stability and generalizability of our hyperbolic ML closure model.\n# Key Words: radiative transfer equation; moment closure; machine learning; neural\n# Key Words: radiative transfer equation; moment closure; machine learning; neura\n1Department of Mathematics, Michigan State University, East Lansing, MI 48824, USA. E-mail: huangj75@msu.edu 2Department of Mathematics, Department of Computational Mathematics, Science and Engineering, Michigan State University, East Lansing, MI 48824, USA. E-mail: ycheng@msu.edu. Research is supported by NSF grants DMS-2011838 and AST-2008004. 3Department of Computational Mathematics, Science and Engineering, Michigan State University, East Lansing, Michigan 48824, USA. E-mail: christli@msu.edu. Research is supported by: AFOSR grants FA9550-19-1-0281 and FA9550-17-1-0394; NSF grants DMS-1912183 and AST-2008004; and DoE grant DE-SC0017955. 4National Superconducting Cyclotron Laboratory and Department of Physics and Astronomy, Michigan State University, East Lansing, MI 48824, USA and CCS-2, Los Alamos National Laboratory, Los Alamos, NM 87545, USA. E-mail: lfroberts@lanl.gov. Research is supported by: NSF grant AST-2008004; and DoE grant DE-SC0017955.\n# 1 Introduction\nIn this paper, we introduce an extension to our previous works on ML closures for radiative transfer modeling [23, 24]. The new approach enforces the hyperbolicity (and physical characteristic speeds) by ensuring mathematical consistency between the closure and the macroscopic model. Further, the numerical results demonstrate the plausibility of capturing kinetic effects in a moment system with a handful of moments and an appropriate closure model. The study of radiative transfer is of vital importance in many fields of science and engineering including astrophysics [40], heat transfer [29], and optical imaging [28]. The kinetic description of radiative transfer is a integro-differential equation in six dimensions in spatial and angular spaces plus time. While there exist many numerical methods to solve this equation, from Monte Carlo methods to deterministic mesh based schemes, the fundamental fact remains that radiative transfer equation (RTE) is computationally demanding for many problems. An alternative approach is to directly model the observables of the kinetic equations: density, momentum, energy etc. by taking moments of the kinetic equation. However, the resulting system of equations is not closed, since the equation for the pth moment depends on knowledge of the (p + 1)th moment. This is known as the moment closure problem. To obtain a closed system of equations, typically a relationship has to be introduced to eliminate the dependency of the equations on the (p + 1)th moment. This may be as simple as setting the (p + 1)th moment to zero, or involve some other relations relating the (p + 1)th moment to lower order moments. Many moment closure models have been developed, including the PN model [10]; the variable Eddington factor models [32, 38]; the entropy-based MN models [21, 2, 1]; the positive PN models [20]; the filtered PN models [37, 30]; the B2 models [3]; and the MPN model [14, 15, 34]. In moment closure problems, hyperbolicity is a critical issue, which is essential for a system of first-order partial differential equations (PDEs) to be well-posed [45]. The pioneering work on the moment closure for the Boltzmann equation, in the context of gas kinetic theory, was introduced by Grad in [17] and is the most basic one among the moment models. Recent analysis for Grad\u2019s 13-moment model showed that the equilibrium of the model is on the boundary of the region of hyperbolicity in 3D [9]. This instability issue has led to a range of efforts to develop closures that lead to globally hyperbolic moment systems [7, 8, 14, 15, 34]. The traditional trade off in introducing a closure relation and solving a moment model instead of a kinetic equation is generic accuracy verses practical computability. However, thanks to the rapid development of machine learning (ML) and data-driven modeling [6, 42, 18], a new approach to solve the moment closure problem has emerged based on ML [19, 44, 25, 5, 35, 48, 36, 23, 24, 41, 43].\nThis approach offers a path for multi-scale problems that is relatively unique, promising to capture kinetic effects in a moment model with only a handful of moments. For more detailed literature review, we refer readers to [23]. We remark that most of the works mentioned above are not able to guarantee hyperbolicity or long time stability, except the works in [25, 24, 41, 43]. In [25], based on the conservation-dissipation formalism [50] of irreversible thermodynamics, the authors proposed a stable ML closure model with hyperbolicity and Galilean invariance for the Boltzmann BGK equation. Nevertheless, the model is limited to only one extra non-equilibrium variable and it is still not clear how to generalize to an arbitrary number of moments. In [41, 43], the authors constructed ML surrogate models for the maximum entropy closure [33] of the moment system of the RTE and the Boltzmann equation. By approximating the entropy using convex splines and input convex neural networks [4], the ML model preserves the structural properties of the original system and reduces the computational cost of the associated ill-conditioned constrained optimization problem significantly, which needed to be solved at each time step in the original formulation of the maximum entropy closure. This paper is a continuation of our previous work in [23], where we proposed to directly learn a closure that relates the gradient of the highest order moment to the gradients of the lower order moments. This gradient based closure is consistent with the exact closure for the free streaming limit and also provides a natural output normalization. A variety of numerical tests show that the ML closure model in [23] has better accuracy than an ML closure based on learning a relation between the moments, as opposed to a relation between the gradients, and the conventional PN closure. Further, the method was able to accurately model both the optically thin and optically thick regime in a single domain with only six moments and was in good agreement with moments computed from the kinetic solution. However, it is not able to guarantee hyperbolicity and long time simulations are not always satisfactory. In our follow-up work [24], we proposed a method to enforce the global hyperbolicity of the ML closure model. The main idea is to seek a symmetrizer (a symmetric positive definite matrix) for the closure system, and derive constraints such that the system is globally symmetrizable hyperbolic. It was also shown that the hyperbolic ML closure system inherits the dissipativeness of the RTE and preserves the correct diffusion limit as the Knunsden number goes to zero. In the numerical tests, the method preformed as well as our original gradient based ML closure for short time simulations and also has the additional benefit of long time stability. A limitation of our approach in [24] is that in practice it is limited to relating the gradient of the highest moment to the gradient of the next 4 lower moments. However, our analysis in [23] indicated that in the free streaming limit, the gradient of the highest moment should be related to a range of gradients which include the lowest\nIn this paper, to overcome this limitation, we take a different approach to enforce the hyperbolicity of our gradient based ML closure model. The approach is to design a structure preserving neural network that ensures that the desired hyperbolicity is preserved in our ML gradient based closure. The main idea is motivated by the observation that the coefficient matrix of the gradient based closure system [23] is an unreduced lower Hessenberg matrix, see Definition 2.1. Due to this particular mathematical structure, we relate its eigenvalues to the roots of some polynomials associated with the coefficient matrix. Therefore, the hyperbolicity of the closure model is equivalent to the condition that the associated polynomial only has simple and real roots, see Theorem 2.4 and Theorem 3.1. Then, we derive the relation between the eigenvalues and the weights in the gradient based closure using the Vieta\u2019s formula and a linear transformation between monomial basis functions and Legendre polynomials. Based on this relation, we design two new neural network architectures both starting with a fully connected neural network which takes the input as the lower order moments. The first neural network architechture is then followed by a component-wise hyperbolic tangent function to enforce the boundedness of the eigenvalues, while the second one has some postprocessing layers to enforce that the eigenvalues are distinct. Lastly, two sublayers representing the Vieta\u2019s formula and a linear transformation are applied to produce the weights in the gradient based closure as the final output, see Figure 4.1 and Figure 4.2 in Section 4. The resulting ML closure model from the first neural network is weakly hyperbolic and guarantees the physical characteristic speeds, i.e. the eigenvalues lie in the range of the interval [\u22121, 1], see Theorem 4.1, while the symmetrizer approach in [24] usually violates the physical characteristic speeds. The second model is strictly hyperbolic and does not guarantee the boundedness of the eigenvalues, see Theorem 4.2. Nevertheless, in practice, we find the characteristic speeds stay close to the physical bound. Maintaining physical characteristic speeds saves substantial computational efforts by allowing for a larger time step size, as compared to [24] when solving the closure system. We numerically tested that the hyperbolic ML closure model has good accuracy in a variety of numerical examples and, just as with our previous work, can capture accurate solutions to problems which have regions in both the optically thin and optically thick regime with only 6 moments. Further, we numerically demonstrate that as we increase the number of moments in the new approach, the ML closure converges rapidly to the solution of the kinetic equation. Nevertheless, there exists some numerical instability for the current model when a small number of moments are used. For the first neural network, we observe numerically that the eigenvalues get too close, which behaves as if the system is weakly hyperbolic instead of strongly hyperbolic. For the second neural network, we check the linear stability of the system numerically and find that the\nloss of linear stability probably results in the blow up of the numerical solutions. How to stabilize the closure system, while maintaining the accuracy, is a topic to be investigated in the future. The remainder of this paper is organized as follows. In Section, 2, we present some preliminary results about Hessenberg matrixes. In Section 3, we introduce the hyperbolic ML moment closure model. In Section 4, we present the details in the architectures and the training of the neural networks. The effectiveness of our ML closure model is demonstrated through extensive numerical results in Section 5. Some concluding remarks are given in Section 6.\n# 2 Preliminary results about Hessenberg matrix\nIn this section, we review important properties of the Hessenberg matrix. These properties facilitate directly relating the eigenvalues of a Hessenberg matrix to the roots of some associated polynomial and derive some equivalent conditions for a Hessenberg matrix to be real diagonalizable. As the matrix being real diagonalizable is equivalent to enforcing that the first-order system is hyperbolic, this is a critical aspect in the design of our structure-preserving neural network in Sections 3 and 4. We start with the definitions of the (unreduced) lower Hessenberg matrix and the associated polynomial sequence [13]: Definition 2.1 (lower Hessenberg matrix). The matrix H = (hij)n\u00d7n is called lower Hessenberg matrix if hij = 0 for j > i + 1. It is called unreduced lower Hessenberg matrix if further hi,i+1 \u0338= 0 for i = 1, 2, \u00b7 \u00b7 \u00b7 , n \u22121. Definition 2.2 (associated polynomial sequence [13]). Let H = (hij)n\u00d7n be an unreduced lower Hessenberg matrix. The associated polynomial sequence {qi}0\u2264i\u2264n with H is defined as: q0 = 1, and \uf8eb \uf8f6\nwith hn,n+1 := 1.\n# Notice that the recurrence relation in (2.1) can be written as a matrix-vector form:\n# Notice that the recurrence relation in (2.1) can be written as a matrix-vector form:  \u2212\nHqn\u22121(x) = xqn\u22121(x) \u2212qn(x)en,\nwhere qn\u22121(x) = (q0(x), q1(x), \u00b7 \u00b7 \u00b7 , qn\u22121(x))T and en = (0, 0, \u00b7 \u00b7 \u00b7 , 0, 1)T \u2208Rn. From this relation, one can immediately relate the roots of qn to the eigenvalues of H [13]: Theorem 2.3 ([13]). Let H = (hij)n\u00d7n be an unreduced lower Hessenberg matrix and {qi}0\u2264i\u2264n is the associated polynomial sequence with H. The following conclusion holds true:\nwhere qn\u22121(x) = (q0(x), q1(x), \u00b7 \u00b7 \u00b7 , qn\u22121(x))T and en = (0, 0, \u00b7 \u00b7 \u00b7 , 0, 1)T \u2208Rn. From this relation, one can immediately relate the roots of qn to the eigenvalues of H [13]:\n(2.1)\n(2.2)\n1. If \u03bb is a root of qn, then \u03bb is an eigenvalue of the matrix H and a corresponding eigenvector is (q0(\u03bb), q1(\u03bb), \u00b7 \u00b7 \u00b7 , qn\u22121(\u03bb))T ; 2. If all the roots of qn are simple, then the characteristic polynomial of H is precisely \u03c1qn with \u03c1 = \u03a0n\u22121 i=1 hi,i+1, i.e.,\ndet(xIn \u2212H) = \u03c1qn(x), where In denotes the identity matrix of order n.\nBy analyzing the eigenspace of the unreduced lower Hessenberg matrix, we have the following equivalent conditions for an unreduced lower Hessenberg matrix to be real diagonalizable. The proof is included in the appendix. Theorem 2.4. Let H = (hij)n\u00d7n be an unreduced lower Hessenberg matrix and {qi}0\u2264i\u2264n is the associated polynomial sequence with H. The following conditions are equivalent: 1. H is real diagonalizable; 2. all the eigenvalues of H are distinct and real; 3. all the roots of qn are simple and real.\n# 3 Moment closure for radiative transfer equation\nIn this section, we first review the gradient based ML moment closure method for the RTE in slab geometry proposed in [23]. Then, we present our approach to enforce the hyperbolicity of the ML moment closure model. Our method for enforcing hyperbolicity comes from a direct relation we derive in Section 3.2 between the coefficients of the neural network in the gradient based model and the eigenvalues of coefficient matrix. Given this relation, in Section 4 we propose two neural network architectures where we directly learn the eigenvalues of the coefficient matrix A such that the eigenvalues are real. The resulting setup produces distinct eigenvalues and there by guarantees that the learned gradient based closure is hyperbolic.\n# 3.1 Gradient based ML moment closure\nWe consider the time-dependent RTE for a gray medium in slab geometry:\n\u2202tf + v\u2202xf = \u03c3s \ufffd1 2 \ufffd1 \u22121 fdv \u2212f \ufffd \u2212\u03c3af, \u22121 \u2264v \u22641\n\u2202tf + v\u2202xf = \u03c3s \ufffd1 2 \ufffd1 \u22121 fdv \u2212f \ufffd \u2212\u03c3af, \u22121 \u2264v \u22641\nHere, f = f(x, v, t) is the specific intensity of radiation. The variable v \u2208[\u22121, 1] is the cosine of the angle between the photon velocity and the x-axis. \u03c3s = \u03c3s(x) \u22650 and \u03c3a = \u03c3a(x) \u22650 are the scattering and absorption coefficients.\n(2.3)\n(3.1)\n\ufffd \u2212 Multiplying by Pk(v) on both sides of (3.1) and integrating over v \u2208[\u22121, 1], we derive the moment\nMultiplying by Pk(v) on both sides of (3.1) and integrating over v \u2208[\u22121, 1], we derive the moment equations:\nThe above system is clearly not closed due to the existence of \u2202xmN+1 in the last equation. The learning gradient approach proposed in [23] is to find a relation between \u2202xmN+1 and the gradients on lower order moments:\n\ufffd with N = (N0, N1, \u00b7 \u00b7 \u00b7 , NN) : RN+1 \u2192RN+1 approximated by a neural network and learned from data. Plugging (3.4) into the closure system, we derive the moment closure model:\n\ufffd In the numerical tests, this approach is shown to be accurate in the optically thick regime, intermediate regime and the optically thin regime. Moreover, the accuracy of this gradient-based model is much better than the approach based on creating a ML closure directly trained to match the moments, as well as the conventional PN closure. However, this model exhibits numerical instability due to the loss of hyperbolicity [23]. This severely restricts the application of this model, especially for long time simulations.\n# 3.2 Hyperbolic ML moment closure\nIn this work, our main idea to enforce the hyperbolicity is motivated by the observation that the coefficient matrix of the closure system is a lower Hessenberg matrix. We write the closure model\n(3.2)\n(3.3)\n(3.4)\n(3.5)\nwith m = (m0, m1, \u00b7 \u00b7 \u00b7 , mN)T and the coefficient matrix A \u2208R(N+1)\u00d7(N+1\n<div style=\"text-align: center;\">with m = (m0, m1, \u00b7 \u00b7 \u00b7 , mN)T and the coefficient matrix A \u2208R(N+1)\u00d7(N+1):</div>\nwith\nand the source term\nIn what follows, we will use the properties of the Hessenberg matrix in Section 2 to analyze  real diagonalizability of the coefficient matrix A in (3.7). We first write down the associated polynomial sequence of A using the definition (2.1):\nNotice that (3.10b) is exactly the same as the recurrence relation for the Legendre polynomia Thus, we have\nThen from (3.10c), we derive\nwhere we used the recurrence relation for the Legendre polynomial:\nBy Theorem 2.3, it is easy to derive the following theorem:\n(3.6)\n(3.7)\nj \u0338= N \u22121,\n(3.8)\n(3.9)\n(3.10a) (3.10b) (3.10c)\n(3.10a) (3.10b)\n(3.10c)\n(3.11)\n(3.12)\n(3.13)\nhe coefficient matrix A in (3.7), the associated polynomial seq\nwhere Pn(x) denotes the Legendre polynomial of degree n. If all the roots of qN+1(x) are simple, then the characteristic polynomial of A is:\ndet(xIN+1 \u2212A) = \u03c1qN+1(x) = \u03c1 \ufffd 2N + 1PN+1(x) + 2N + 1PN\u22121(x) \u2212 \ufffd k=0 akPk(x) \ufffd (3.15) with \u03c1 = N! (2N\u22121)!!. If further assuming all the roots of qN+1(x) are simple and real, then all the eigenvalues of A are distinct and real. In this case, the moment closure system is strictly hyperbolic. If further assuming all the roots of qN+1(x) are simple, real and lie in the interval [\u22121, 1], then the moment closure system is strictly hyperbolic with physical characteristic speeds. Remark 3.2. From Theorem 2.4, the condition that all the roots of qN+1(x) are simple and real, is also necessary for the moment closure system to be hyperbolic. Next, we will derive the relation between the eigenvalues of A (or the roots of qN+1(x)) and the weights of the gradients in (3.4). In particular, we will represent {Nk}0\u2264k\u2264N in (3.4) using the eigenvalues of A. We denote the distinct real eigenvalues of A by {rk}0\u2264k\u2264N. Then, by Theorem 3.1, we have (x \u2212r0)(x \u2212r1) \u00b7 \u00b7 \u00b7 (x \u2212rN) = \u03c1 \ufffd N + 1 2N + 1PN+1(x) + N 2N + 1PN\u22121(x) \u2212 N \ufffd k=0 akPk(x) \ufffd . (3.16) First, we expand the characteristic polynomial using a set of monomial basis: det(xIN+1 \u2212A) = c0 + c1x + \u00b7 \u00b7 \u00b7 + cNxN + xN+1. (3.17) Using Vieta\u2019s formulas, we relate the coefficients {ck}0\u2264k\u2264N to the sums and products of its roots {rk}0\u2264k\u2264N: r0 + r1 + \u00b7 \u00b7 \u00b7 + rN\u22121 + rN = \u2212cN, (r0r1 + r0r2 + \u00b7 \u00b7 \u00b7 + r0rN) + (r1r2 + r1r3 + \u00b7 \u00b7 \u00b7 + r1rN) + \u00b7 \u00b7 \u00b7 + rN\u22121rN = cN\u22121, ... r0r1 \u00b7 \u00b7 \u00b7 rN\u22121rN = (\u22121)N+1c0, (3.18) or equivalently written as a compact formulation \ufffd 0\u2264i1<i2<\u00b7\u00b7\u00b7<ik\u2264N \ufffd \u03a0k j=1rij \ufffd = (\u22121)kcN+1\u2212k, k = 1, 2, \u00b7 \u00b7 \u00b7 , N + 1. (3.19)\n\ufffd with \u03c1 = N! (2N\u22121)!!. If further assuming all the roots of qN+1(x) are simple and real, then all the eigenvalues of A are distinct and real. In this case, the moment closure system is strictly hyperbolic. If further assuming all the roots of qN+1(x) are simple, real and lie in the interval [\u22121, 1], then the moment closure system is strictly hyperbolic with physical characteristic speeds. Remark 3.2. From Theorem 2.4, the condition that all the roots of qN+1(x) are simple and real, is also necessary for the moment closure system to be hyperbolic. Next, we will derive the relation between the eigenvalues of A (or the roots of qN+1(x)) and the weights of the gradients in (3.4). In particular, we will represent {Nk}0\u2264k\u2264N in (3.4) using the eigenvalues of A. We denote the distinct real eigenvalues of A by {rk}0\u2264k\u2264N. Then, by Theorem 3.1, we have (x \u2212r0)(x \u2212r1) \u00b7 \u00b7 \u00b7 (x \u2212rN) = \u03c1 \ufffd N + 1 2N + 1PN+1(x) + N 2N + 1PN\u22121(x) \u2212 N \ufffd k=0 akPk(x) \ufffd . (3.16) First, we expand the characteristic polynomial using a set of monomial basis: det(xIN+1 \u2212A) = c0 + c1x + \u00b7 \u00b7 \u00b7 + cNxN + xN+1. (3.17) Using Vieta\u2019s formulas, we relate the coefficients {ck}0\u2264k\u2264N to the sums and products of its roots {rk}0\u2264k\u2264N: r0 + r1 + \u00b7 \u00b7 \u00b7 + rN\u22121 + rN = \u2212cN, (r0r1 + r0r2 + \u00b7 \u00b7 \u00b7 + r0rN) + (r1r2 + r1r3 + \u00b7 \u00b7 \u00b7 + r1rN) + \u00b7 \u00b7 \u00b7 + rN\u22121rN = cN\u22121, ... r0r1 \u00b7 \u00b7 \u00b7 rN\u22121rN = (\u22121)N+1c0, (3.18) or equivalently written as a compact formulation \ufffd \ufffd \u03a0k j=1ri \ufffd = (\u22121)kcN+1\u2212k, k = 1, 2, \u00b7 \u00b7 \u00b7 , N + 1. (3.19)\nwith \u03c1 = N! (2N\u22121)!!. If further assuming all the roots of qN+1(x) are simple and real, then all the eigenvalues of A are distinct and real. In this case, the moment closure system is strictly hyperbolic. If further assuming all the roots of qN+1(x) are simple, real and lie in the interval [\u22121, 1], then the moment closure system is strictly hyperbolic with physical characteristic speeds. Remark 3.2. From Theorem 2.4, the condition that all the roots of qN+1(x) are simple and real, is also necessary for the moment closure system to be hyperbolic. Next, we will derive the relation between the eigenvalues of A (or the roots of qN+1(x)) and the weights of the gradients in (3.4). In particular, we will represent {Nk}0\u2264k\u2264N in (3.4) using the eigenvalues of A. We denote the distinct real eigenvalues of A by {rk}0\u2264k\u2264N. Then, by Theorem 3.1, we have (x \u2212r0)(x \u2212r1) \u00b7 \u00b7 \u00b7 (x \u2212rN) = \u03c1 \ufffd N + 1 2N + 1PN+1(x) + N 2N + 1PN\u22121(x) \u2212 N \ufffd k=0 akPk(x) \ufffd . (3.16)\nor equivalently written as a compact formulation\n\ufffd \ufffd \u03a0k j=1rij \ufffd = (\u22121)kcN+1\u2212k, k = 1, 2, \u00b7 \u00b7 \u00b7 , N + 1.\n(3.14a)\n(3.14a) (3.14b)\n(3.14b)\n(3.15)\n(3.16)\n(3.17)\n(3.18)\n(3.19)\nHere the indices ik are sorted in strictly increasing order to ensure each product of k roots is used exactly once. Then, we establish the relationship between {ck}0\u2264k\u2264N to {ak}0\u2264k\u2264N. Using the generating function of Legendre polynomials, one can express the monomial in terms of a summation of Legendre polynomials [47]. We present the conclusion in the following lemma and include the proof in the appendix.\nwith F(m, k) = m!(2m\u22124k+1) 2kk!(2m\u22122k+1)!!. Here Pn(x) is the n-th order Legendre polynomial, and \u230a\u00b7\u230bis the floor function which takes a real number x as input, and gives the greatest integer less than or equal o x as output.\nWe rewrite (3.20) into an equivalent formulation\nwith\n\uf8f3 From this formula, we can expand any polynomial \ufffdn i=0 cixi in terms of Legendre polynomials:\nwith\n\ufffd We apply the above relation to derive the relationship between {ck}0\u2264k\u2264N to {ak}0\u2264k\u2264N: c0 + c1x + \u00b7 \u00b7 \u00b7 + cNxN + xN+1 = \u03c1 \ufffd N + 1 2N + 1PN+1(x) + N 2N + 1PN\u22121(x) \u2212 N \ufffd k=0 akPk(x) \ufffd , \nand obtain\n10\n(3.20)\n(3.21)\n(3.22)\n(3.23)\n(3.24)\n(3.25)\n(3.26a)\n(3.26b)\nwith cN+1 := 1. The last one (3.26c) is automatically satisfied since bN+1,N+1 = F(N + 1, 0)  (N+1)!(2N+3) (2N+3)!! = (N+1)! (2N+1)!!. Lastly, we rewrite (3.26a)-(3.26b) in terms of {Nk}0\u2264k\u2264N using the relation (3.8):\nwith cN+1 := 1. Now, together with (3.18) and (3.27), we have expressed {Nk}0\u2264k\u2264N using the eigenvalues {rk}0\u2264k\u2264N.\nNow, together with (3.18) and (3.27), we have expressed {Nk}0\u2264k\u2264N using the eigenvalues {rk}0\u2264k\u2264N.\n# 4 Architectures and training of the neural network\nIn this section, we provide the architectures and training of the proposed neural networks that enforces the hyperbolicity of the closure system.\n# 4.1 Architectures of the neural network\nWe start with the first neural network architecture. As shown in Figure 4.1, this neural network begins with a fully connected neural network denoted by M\u03b8 : RN+1 \u2192RN+1 with the input being the lower order moments (m0, m1, \u00b7 \u00b7 \u00b7 , mN) and the output denoted by (z0, z1, \u00b7 \u00b7 \u00b7 , zN). Here \u03b8 denotes the collection of all the parameters to be trained in the neural network. It is then followed by a component-wise hyperbolic tangent function to enforce the boundness of the eigenvalues, i.e. ri = tanh(zi) for i = 0, 1, \u00b7 \u00b7 \u00b7 , N. Lastly, two sublayers representing the Vieta\u2019s formula (3.18) and a linear transformation (3.27) are applied to produce the weights (N0, N1, \u00b7 \u00b7 \u00b7 , NN) in the gradient based closure in (3.4) as the final output. For the ML moment closure model resulted by this neural network in Figure 4.1, we have the following conclusion: Theorem 4.1. The ML moment closure model (3.5) resulting from the neural network with bounded eigenvalues shown in Figure 4.1 is weakly hyperbolic. Moreover, it guarantees the physical characteristic speeds, i.e., the eigenvalues lie in the interval [\u22121, 1]. There is a small gap between the implementation of the neural network in Figure 4.1 and the theory presented in previous sections. From Theorem 2.4, all the eigenvalues being distinct and real is a necessary and sufficient condition for the moment closure system to be hyperbolic. However, we do not force all the eigenvalues to be distinct in the current neural network architecture. Thus, this\nThere is a small gap between the implementation of the neural network in Figure 4.1 and the theory presented in previous sections. From Theorem 2.4, all the eigenvalues being distinct and real is a necessary and sufficient condition for the moment closure system to be hyperbolic. However, we do not force all the eigenvalues to be distinct in the current neural network architecture. Thus, this\n(3.27)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d13/6d136a78-6fb5-45c6-aa82-4b910b40a25d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4.1: Schematic of the neural network with bounded eigenvalues. Input: the mo ments (m0, m1, \u00b7 \u00b7 \u00b7 , mN), output: the weights (N0, N1, \u00b7 \u00b7 \u00b7 , NN) in the gradient based closure i (3.4). The Vieta\u2019s formula is given in (3.18). The linear transformation from (c0, c1, \u00b7 \u00b7 \u00b7 , cN) t (N0, N1, \u00b7 \u00b7 \u00b7 , NN) is given in (3.27).</div>\nonly guarantees that the resulting system is theoretically weakly hyperbolic instead of the system being hyperbolic, and might cause an instability issue. In the numerical tests in Section 5, we observe that the ML closure model is numerically stable for N \u22656. Meanwhile, for N \u22645, the model has some stability issues, and these stability issues are associated with the case when two of the eigenvalues are within 10\u22123 of each other on a range of grid points, see the detailed discussion in Figure 5.7 in Section 5. To fix this problem, we tried several approaches by enforcing that the eigenvalues are distinct (or well separated). The first approach is to divide the interval [\u22121, 1] into (N +1) uniform subintervals with some threshold gap between two neighbouring subintervals: Ik = [\u22121+ 2k N+1+\u03b3, \u22121+ 2(k+1) N+1 \u2212\u03b3] for k = 0, \u00b7 \u00b7 \u00b7 , N. Then, we put exactly one eigenvalue into each subinterval, enforced by a scaled hyperbolic tangent function. Here \u03b3 \u22650 is a small number to guarantee a minimum distance of any two eigenvalues. We take \u03b3 = 0 and 10\u22123 in the implementation. This approach is motivated by the fact that, in the PN closure, each subinterval contains exactly one eigenvalue. However, we find that, the neural network results in large training errors in the training process, which are generally larger than 14% with N = 3, 4, \u00b7 \u00b7 \u00b7 , 10. In these tests, we fix the number of nodes to be 64 and the number of layers to be 6. This indicates that the assumption of the uniform distribution of the eigenvalues is too restrictive, so that the approximation power of the neural network is not enough to produce an accurate closure. We will not focus on this neural network in the afterwards. The other approach is to replace the hyperbolic tangent layer in Figure 4.1 with some other postprocessing layers. As illustrated in Figure 4.2, we first applied some positive function, \u03ba, to\n# Here \u03ba = \u03ba(x) \u2265\u03b3 > 0 is a strictly positive function taken as \u03ba(x) = ln(1 + ex) + \u03b3\n\u03ba(x) = ln(1 + ex) + \u03b3\nith \u03b3 = 0.1. Then, it is followed by a linear transformation:\nri = i \ufffd k=0 \u02dczk, i = 0, \u00b7 \u00b7 \u00b7 , N,\nwhich produces the eigenvalues of the closure system. Next, the Vieta\u2019s formula and the linear transformation are imposed as in Figure 4.1. This approach can guarantee that the eigenvalues are distinct, i.e. r0 < r1 < \u00b7 \u00b7 \u00b7 < rN, but may lose the boundness property of the eigenvalues. Nevertheless, in the numerical simulations, we observe that the model has the physical characteristic speeds for most of the time although this constrain is not enforced explicitly, see the discussion in Figure 5.6 in Section 5. For the ML moment closure model resulted from this neural network in Figure 4.2, we have the following conclusion: Theorem 4.2. The ML moment closure model (3.5) resulted from the neural network with distinct eigenvalues shown in Figure 4.2 is strictly hyperbolic. We remark that the the current neural network architectures could not guarantee (strongly) hyperbolicity and the physical characteristic speeds simultaneously. We also tried to enforce the two desired properties by adding some penalty terms into the loss function, but we did not get satisfactory results. Nevertheless, we will more fully explore this direction in our future work.\nwhich produces the eigenvalues of the closure system. Next, the Vieta\u2019s formula and the linear transformation are imposed as in Figure 4.1. This approach can guarantee that the eigenvalues are distinct, i.e. r0 < r1 < \u00b7 \u00b7 \u00b7 < rN, but may lose the boundness property of the eigenvalues. Nevertheless, in the numerical simulations, we observe that the model has the physical characteristic speeds for most of the time although this constrain is not enforced explicitly, see the discussion in Figure 5.6 in Section 5. For the ML moment closure model resulted from this neural network in Figure 4.2, we have the following conclusion:\nTheorem 4.2. The ML moment closure model (3.5) resulted from the neural network with distinc eigenvalues shown in Figure 4.2 is strictly hyperbolic.\neigenvalues shown in Figure 4.2 is strictly hyperbolic. We remark that the the current neural network architectures could not guarantee (strongly) hyperbolicity and the physical characteristic speeds simultaneously. We also tried to enforce the two desired properties by adding some penalty terms into the loss function, but we did not get satisfactory results. Nevertheless, we will more fully explore this direction in our future work.\nWe remark that the the current neural network architectures could not guarantee (strongly) hyperbolicity and the physical characteristic speeds simultaneously. We also tried to enforce the two desired properties by adding some penalty terms into the loss function, but we did not get satisfactory results. Nevertheless, we will more fully explore this direction in our future work.\n# 4.2 Training of the neural network\nFor the training of the neural network, we take 1000 total epochs (the number of iterations in the optimization process). We investigated two activation functions including the hyperbolic tangent (tanh) function and Rectified Linear Unit (ReLU) function. The learning rate is set to be 10\u22123 in the initial epoch and decays by 0.5 every 100 epochs. The L2 regularization is applied with weight 10\u22127. The batch size is taken to be 1024. The training is implemented within the PyTorch framework [39]. We use the same hyperparameters for the two neural networks. Following [23], in the training process, the loss function is taken to be:\nL = 1 Ndata \ufffd j,n \ufffd\ufffd\u2202xmtrue N+1(xj, tn) \u2212\u2202xmappx N+1(xj, tn) \ufffd\ufffd2 .\n(4.1)\n(4.2)\n(4.3)\n(4.4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e6b/7e6b8924-3cc0-4e70-b375-6083265ad4c7.png\" style=\"width: 50%;\"></div>\nFigure 4.2: Schematic of the neural network with distinct eigenvalues. Input: the moments (m0, m1, \u00b7 \u00b7 \u00b7 , mN), output: the weights (N0, N1, \u00b7 \u00b7 \u00b7 , NN) in the gradient based closure in (3.4). Here, \u03ba = \u03ba(x) > 0 is a positive function. The layer connecting (\u02dcz0, \u02dcz1, \u00b7 \u00b7 \u00b7 , \u02dczN) and (r0, r1, \u00b7 \u00b7 \u00b7 , rN) is given in (4.3). The Vieta\u2019s formula is given in (3.18). The linear transformation from (c0, c1, \u00b7 \u00b7 \u00b7 , cN) to (N0, N1, \u00b7 \u00b7 \u00b7 , NN) is given in (3.27).\nHere, \u2202xmtrue N+1(xj, tn) denotes the spatial derivative of (N + 1)-th order moment at x = xj and t = tn computed from the kinetic solver and \u2202xmappx N+1(xj, tn) comes from the evaluation of the neural network using (3.4). Following [23], the training data comes from numerically solving the RTE using the space-time discontinuous Galerkin (DG) method [11, 12] with a range of initial conditions in the form of truncated Fourier series and different scattering and absorption coefficients which are constants over the computational domain, see the details in [23]. We train the neural network with 100 different initial data sets. For each initial data set, we run the numerical solver up to t = 1. The other parameters are the same as in [23]. To evaluate the accuracy in the training process, we define the relative L2 error for the gradient to be \ufffd\ufffd\n\ufffd The depth and width of neural networks (i.e., the number of hidden layers and the number of nodes in the hidden layers) are crucial hyperparameters in a neural network. Here, we perform a grid search to find the optimal hyperparameters of the neural network including the number of layers and the number of nodes in the first fully-connected neural network M\u03b8. In particular, we take the number of layers to be {2, 3, \u00b7 \u00b7 \u00b7 , 10} and the number of nodes to be {16, 32, \u00b7 \u00b7 \u00b7 , 256}. For the first neural network in Figure 4.1, the relative L2 errors in the training data with different\n(4.5)\ndepths and widths, and tanh and ReLU activation functions are shown in Figure 4.3. Here, we only show the cases with the number of moments to be N = 5, 7, 9, the cases with N = 6, 8 are similar. With the ReLU activation function, the error decreases when we increase the number of layers and nodes in hidden layers until it saturates, see Figure 4.3 (b) and Figure 4.3 (d). However, we observe a different phenomenon with the hyperbolic tangent activation function. When we increase the depth and width, the error decreases only when the network stay relatively small widths 16, 32 and 64 in Figure 4.3 (a) and widths 16 and 32 in Figure 4.3 (c). When the neural networks get deeper, the error increases with width. This numerical observation is similar to the well-known vanishing gradient problem. In our current setup, the problem is probably caused by the strong nonlinearity of the Vieta\u2019s formula after the fully connected neural network, which stops the neural network from further training. The hyperbolic tangent function, as the activation function, has gradients in the range of (0, 1), which makes it easy for the neural network to become stuck in a local minimum due to the vanishing gradient problem. ReLU suffers less from the vanishing gradient problem than the hyperbolic tangent function, because it only saturates in one direction, the one with negative inputs. Other solutions to the vanishing gradient problem, such as residual neural networks (ResNet) [22] and batch normalization [26], may also be applied here to achieve better performance. We will explore this direction in our future work. Moreover, these tests indicate that taking number of layers to be 6 and number of nodes to be 64 and ReLU activation function are good hyperparameters for our neural network. As such these are the values used in all the numerical tests in Section 5 unless otherwise stated.\n# 5 Numerical tests\nIn this section, we show the performance of our ML closure model on a variety of benchmark tests, including problems with constant scattering and absorption coefficients, Gaussian source problems and two-material problems. The main focus of the tests is on the comparison of four moment closure models: (i) the symmetrizer based hyperbolic ML closure [24] (termed as \u201chyperbolic (symmetrizer)\u201d); (ii) the hyperbolic ML closure with bounded eigenvalues (termed as \u201chyperbolic (bound)\u201d), see the neural network architechture in Figure 4.1; (iii) the hyperbolic ML closure with distinct eigenvalues (termed as \u201chyperbolic (distinct)\u201d), see the neural network architechture in Figure 4.2; (iv) the classical PN closure [10]. In all the numerical examples, we take the physical domain to be the unit interval [0, 1] and periodic boundary conditions are imposed. To numerically solve the moment closure system, we apply the fifth-order finite difference WENO scheme [27] with a Lax\u2013Friedrichs flux splitting for the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ada/7ada4de0-f113-4fb5-84b1-e01382fd2dd8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5abe/5abe7d8c-03cc-443d-8a24-d0264e1df5ee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2bb6/2bb6603d-b487-4264-93ad-fbe2df8646a0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) tanh, N = 5</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a5a/4a5afafd-0e51-4b9d-8b14-118c5a6b0420.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) tanh, N = 9</div>\nFigure 4.3: Relative L2 error in the training data with different depths and widths of the neural networks. Here, we use the first neural network architecture in Figure 4.1. The number of layers: 2, 3, \u00b7 \u00b7 \u00b7 , 10; the number of nodes in the hidden layers: 16, 32, \u00b7 \u00b7 \u00b7 , 256. Left: hyperbolic tangent activation function; right: ReLU activation function. The number of moments N = 5, 7, 9.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebc6/ebc6e6bb-38c7-49e0-8e00-ef8369b98ec8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) ReLU, N = 5</div>\n<div style=\"text-align: center;\">(d) ReLU, N = 7</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9bf0/9bf0c58e-728e-4cef-a883-b7e3a5ef0c95.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(f) ReLU, N = 9</div>\nspatial discretization, and the third-order strong-stability-preserving Runge-Kutta (RK) scheme [46] for the time discretization. We take the grid number in space to be Nx = 256. The CFL condition is taken to be \u2206t = 0.8\u2206x/c with c being the maximum eigenvalues in all the grid points. Example 5.1 (constant scattering and absorption coefficients). The setup of this example is the same as the data preparation. The scattering and absorption coefficients are taken to be constants over the domain. The initial condition is taken to be a truncated Fourier series, see the details in [23]. In Figure 5.4, we show the numerical solutions of m0 and m1 with seven moments in the closure system (N = 6) in the optically thin regime (\u03c3s = \u03c3a = 1). It is observed that, at t = 0.5 and t = 1, all the hyperbolic ML moment closures agree well the RTE. As a comparison, the PN closure has large deviations from the exact solution at both t = 0.5 and t = 1. In Figure 5.5, we display the log-log scatter plots of the relative L2 error versus the scattering coefficient for N = 6 at t = 1. We observe that, all the hyperbolic ML closures have better accuracy than the PN closure. Moreover, in the optically thick regime, all the closures perform well. It is also observed that the ML hyperbolic closure model with bounded eigenvalues generally has better accuracy than the other two ML closures. In Figure 5.6 (a), we present the L2 errors as a function of time for the solutions of the three hyperbolic ML moment closure systems and the solution generated by the RTE in the optically thin regime (\u03c3s = \u03c3a = 1). We observe that the three hyperbolic closures generate good predictions in the long time simulation up to t = 10. Moreover, the eigenvalue based ML hyperbolic closure models are more accurate than the symmetrizer based model in [24]. This is probably due to the fact that there is only 4 degrees of freedom in [24]. In contrast, the current eigenvalue based approach makes full use of all the degrees of freedom, which results in better approximation results. We also display the maximum eigenvalues of the three hyperbolic ML closure models at all the grid points during the time evolution in Figure 5.6 (b). It is observed that the eigenvalues are always real numbers, which validates the hyperbolicity feature of the closure models. Moreover, the ML closure with bounded eigenvalues always has physical characteristic speeds bounded by 1. For the closure with distinct eigenvalues, it is interesting to see that the model has the physical characteristic speeds for most of the time although this constrain is not enforced explicitly. The largest eigenvalues of this model during the time evolution is 1.12, which is slightly larger than 1. As a comparision, the symmetrizer based closure in [24] usually violates the physical characteristic speeds, which can be as large as 5.05. The physical characteristic speed of the current ML closure model results in larger time step size in the numerical simulations and thus less computational cost. Moreover, to determine the time step size in the symmetrized based ML model [24] during the time evolution\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7ec/a7ec4f09-d570-479d-9998-ed8212a6acc9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) m0 at t = 1</div>\nFigure 5.4: Example 5.1: constant scattering and absorption coefficients, optically thin regime (\u03c3s = \u03c3a = 1), N = 6, t = 0.5 and t = 1.\n<div style=\"text-align: center;\">Figure 5.4: Example 5.1: constant scattering and absorption coefficients, optically thin regime (\u03c3s = \u03c3a = 1), N = 6, t = 0.5 and t = 1.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b720/b7206c57-39b6-4224-ba31-1c04d7bbccfd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e 5.1: constant scattering and absorption coefficients, N = 6 </div>\n<div style=\"text-align: center;\">(d) m1 at t = 1</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e89c/e89cfeba-d615-405e-b115-ecdc3ddebf62.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) L2 errors of m0 and m1</div>\n<div style=\"text-align: center;\">mple 5.1: constant scattering and absorption coefficients, N =</div>\n<div style=\"text-align: center;\">gure 5.6: Example 5.1: constant scattering and absorption co</div>\nbased on the CFL condition, it is required to first compute the coefficient matrix for the closure models and then compute the maximum eigenvalues, which results in additional computational cost. Therefore, the current two ML closure models are better than the symmetrizer based model in [24] in terms of the efficiency. Next, we discuss the instability issue of the hyperbolic ML closure with bounded eigenvalues. The two eigenvalues get too close for small numbers of moments (N = 3, 4, 5), which behaves as if the system is weakly hyperbolic. We simulate the ML closure model with bounded eigenvalues with N = 3 and N = 5 in the optically thin regime (\u03c3s = \u03c3a = 1). The numerical solutions blow up at t = 0.18 for N = 3 and t = 1.25 for N = 5, see Figure 5.7 (b) and Figure 5.7 (d) for the L\u221enorm of the numerical solutions during the time evolution. As a comparison, the solution stays stable for N = 7, see Figure 5.7 (f). To investigate this phenomenon in detail, in each time step, we compute the eigenvalues at each grid point, and compute the number of grid points with two eigenvalues which are closer than a given thresholds \u03b5. The number of grid points with close eigenvalues with different thresholds in the time evolution are presented in Figure 5.7 (a) and Figure 5.7 (c). From the figure, we observe that there are no grid points with close eigenvalues in the beginning. As time evolves, more grid points with non-distinct eigenvalues appear for N = 3 and N = 5. For N = 7, there only exists a couple of grid points with the thresholds 10\u22123 and 10\u22124 and no grid points with the thresholds 10\u22125 and 10\u22126. This does not affect the numerical stability of the simulation. We also observe numerical instability in the hyperbolic ML closure model with distinct eigenvalues for some parameters. The model is numerically stable for N \u22656 but numerically unstable for N = 3, 4, and 5 in the optically thin regime. We show the distributions of the training data and the numerical solution during the time evolution of the ML closure model with N = 3 and N = 6\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3b4a/3b4a475b-2214-4481-a0bd-f4da3152a303.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a0e9/a0e96a44-f358-4302-8b1d-cab9ccf93b94.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c56/7c56f000-fa2a-409d-bda1-54cd3774befb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f5ed/f5edd186-a8ed-45e5-8136-ea1cf8489d82.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) number of grid points with close eigenvalues, N = 3</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1304/13046271-a437-4a03-88af-2f78210687e0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) number of grid points with close eigenvalues, N = 5</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d702/d70239ce-dcc5-4480-9aea-f57dc6d4cd7f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40dc/40dc24cc-be5a-4ea3-846c-8c0a1356ca04.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) number of grid points with close eigenvalues, N = 7</div>\nFigure 5.7: Example 5.1: constant scattering and absorption coefficients. Here, we use the first neural network architecture in Figure 4.1. The number of grid points with imaginary eigenvalues and L\u221enorm of numerical solutions during the time evolution in the optically thin regime (\u03c3s = \u03c3a = 1) with N = 3, 5, 7.\n<div style=\"text-align: center;\">(f) L\u221enorm of numerical solution, N = 7</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc3a/dc3a1b1a-2e83-4afb-8c7c-0eafa738df8e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) m2/m0 vs m1/m0 with N = 3</div>\nFigure 5.8: Example 5.1: constant scattering and absorption coefficients, m2/m0 vs m1/m0 with N = 3 and N = 6. Here, we use the second neural network architecture in Figure 4.2. At each time step, there is a curve composed of 256 points and the plots represent the evolution of the closed curve where the color denotes the evolution time. For N = 3, as the numerical solutions is approaching the steady state, it suddenly has a dramatic change in dynamics of the solution and then proceeds to run outside of the range of the training data. This in contrast to the case N = 6 which is plotted, which clearly shows relaxation to the steady state. The grey points denote the training data and the colorful points denote the numerical solutions solving from the ML moment closure system.\nin Figure 5.8. At each time step, there is a curve composed of 256 points and the plots represent the evolution of the closed curve where the color denotes the evolution time. It can be seen for the N = 3 case, that as the numerical solutions is approaching the steady state, it suddenly undergoes a dramatic change in the dynamics of the solution and then proceeds to run out side of the range of the training data. This in contrast to the case N = 6 which is plotted in Figure 5.8(b), which clearly shows relaxation to the steady state. In the plots, the color bar represents the time of the solution. To investigate the instability of the ML closure model with distinct eigenvalues further, we check the linear stability of the system numerically. We denote the source term of the closure model in (3.5) by S = (\u2212\u03c3am0, \u2212(\u03c3s + \u03c3a)m1, \u00b7 \u00b7 \u00b7 , \u2212(\u03c3s + \u03c3a)mN). Then, the Jacobian matrix of the source term is SU = diag(\u2212\u03c3a, \u2212(\u03c3s + \u03c3a), \u00b7 \u00b7 \u00b7 , \u2212(\u03c3s + \u03c3a)). The model is called linearly stable if all the eigenvalues of (i\u03beA + SU) have non-positive real part for any \u03be \u2208R. Here, A is the coefficient matrix of the closure system given in (3.7) and i is the imaginary unit. Linear stability is essential for the closure system to generate stable results in long time simulations [49]. The symmetrizer based hyperbolic ML moment closure model in [24] satisfies this stability condition. We test for linear stability numerically, by taking \u03be = \u2212100, \u221299, \u00b7 \u00b7 \u00b7 , 99, 100, and computing the eigenvalues\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0819/0819dcf0-2b69-4ccd-bc0d-c479bdfcbc09.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) m2/m0 vs m1/m0 with N = 6</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/07a1/07a1190b-7b66-4506-be38-73c3c128230e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) number of grid points with linear instability in the time evolution</div>\n<div style=\"text-align: center;\">Figure 5.9: Example 5.1: constant scattering and absorption coefficients.</div>\nof (i\u03beA + SU) at all grid points. The number of grid points with eigenvalues with positive real part and the L\u221e-norm of m0 during the time evolution is shown in Figure 5.9. It is observed that for N = 3 and 5, the solution blows up when the grid points with linear instability appear. This indicates that the loss of linear stability probably results in the blow up of the numerical solutions. It is also interesting to see that for N = 9, the model generates stable solution; however, there also exists several grid points with linear instability when the time is around 0.7 and the model returns to stability in the time afterwards. How to stabilize the closure system, while maintaining training accuracy, is a topic to be investigated in the future. Example 5.2 (Gaussian source problem). In this example, we investigate the RTE with the initial condition to be a Gaussian distribution in the physical domain:\nIn this test, we take c1 = 0.5, c2 = 2.5, x0 = 0.5 and \u03b8 = 0.01. We note that this problem is named the Gaussian source problem in the literature [16, 14]. In Figure 5.10, we present the results obtained using various closure models. Here, we take \u03c3s = 1 and \u03c3a = 0. We observe good agreement between the three ML closure models and the kinetic model, while the PN model has large deviations from the kinetic model. These results show the good generalizability of our ML closure models. Moreover, the three hyperbolic ML models have the same level of accuracy in this test. Example 5.3 (two-material problem). The two-material problem models a domain with a discontinuous material cross section [31]. In our problem setup, there exist two discontinuities\n<div style=\"text-align: center;\">(b) L\u221e-norm of m0 in the time evolution</div>\n(5.1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f4b9/f4b9eb9d-e1af-429c-b282-3579b7c18b49.png\" style=\"width: 50%;\"></div>\n\u03c3a(x) = \ufffd\u03c3a1, x1 < x < x2, \u03c3a2, 0 \u2264x < x1 or x2 \u2264x < 1.\n  Specifically, we take x1 = 0.3, x2 = 0.7, \u03c3s1 = 1, \u03c3s2 = 10 and \u03c3a1 = \u03c3a2 = 0. The numerical results are shown in Figure 5.11. The gray part is in the optically thin regime and the other part is in the intermediate regime. We observe that our current closure model agrees well with the kinetic solution over the whole domain at both t = 0.5 and t = 1. We note that this is in contrast to the PN closure, which has large deviations from the kinetic solution in the optically thin portion of the domain, see Figure 5.11. Moreover, the two eigenvalue based hyperbolic closures perform better than the closure in [24] which has some overshoot near the discontinuities, see Figure 5.11 (d). In Figure 5.12, we numerically investigate the convergence of the ML closure model with bounded eigenvalues to the kinetic model as the number of moments increases. We take the number of moment to be N = 6, 8, 10, 12, 14, 16. In Table 5.1, we present the relative L2 errors of m0 and m1 for the same numerical example. We observe that the error between the solution to the ML closure model and the solution to the kinetic equation becomes smaller with an increasing number of moments. This numerically indicates that the ML closure model converges to the kinetic model as the number of moments increases. It is worth noting that the saturation in convergence seen in table 5.1 is of the same order as the training error in the ML Closure model.\n<div style=\"text-align: center;\">(b) m1 at t = 1</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4278/427818af-6360-4108-8e1a-c31af2d58bda.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) m0 at t = 1</div>\n<div style=\"text-align: center;\">Figure 5.11: Example 5.3: two-material problem. Numerical solutions of m0 and m1 at t = 0.5 and t = 1 with N = 6. The gray part in the middle is in the optically thin regime and the other part is in the intermediate regime.</div>\n<div style=\"text-align: center;\">Table 5.1: Example 5.3: two-material problem, convergence with respect to number of moments, the ML closure model with bounded eigenvalues. The relative L2 errors of the numerical solutions</div>\n<div style=\"text-align: center;\">the ML closure model with bounded eigenvalues. The relative L2 errors of the numerical so of m0 and m1 at t = 2 with N = 6, 8, 10, 12, 14, 16.    </div>\n= 2 with N = 6, 8, 10, 12, 14, 16.\nN\nrelative L2 error of m0\nrelative L2 error of m1\n6\n5.79e-4\n7.84e-2\n8\n3.78e-4\n5.69e-2\n10\n3.67e-4\n2.93e-2\n12\n3.66e-4\n3.64e-2\n14\n1.98e-4\n2.94e-2\n16\n1.66e-4\n2.21e-2\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/89bb/89bbee50-e8cc-4ea2-8988-7c53cd212b1e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) m0 at t = 2</div>\nFigure 5.12: Example 5.3: two-material problem, convergence with respect to number of moments, the ML closure model with bounded eigenvalues. Numerical solutions of m0 at t = 2 with N = 6, 8, 10, 12, 14, 16. The gray part in the middle is in the optically thin regime and the other part is in the intermediate regime.\n# 6 Conclusion\nIn this paper, we propose a new method to enforce the hyperbolicity of a ML closure model. Motivated by the observation that the coefficient matrix of the closure system is a lower Hessenberg matrix, we relate its eigenvalues to the roots of an associated polynomial. We design two new neural network architectures based on this relation. The ML closure model resulting from the first neural network is weakly hyperbolic and guarantees the physical characteristic speeds, i.e. the eigenvalues lie in the range of the interval [\u22121, 1]. The second model is strictly hyperbolic, but does not guarantee the boundedness of the eigenvalues, although in practice the eigenvalues lie nearly within the physical range. Having the physical characteristic speeds saves substantial computational expenses when numerically solving the closure system by allowing for a larger time step size compared to [24]. Several benchmark tests including the Gaussian source problem and the two-material problem show the good accuracy and generalizability of our hyperbolic ML closure model. Nevertheless, there exists some numerical instability for the current model when a small number of moments are used. We will try to fix this problem in the future work.\n# Acknowledgment\nWe thank Michael M. Crockatt in Sandia National Laboratories for providing numerical solver for the radiative transfer equation. We acknowledge the High Performance Computing Center (HPCC) at Michigan State University for providing computational resources that have contributed to the\n<div style=\"text-align: center;\">(b) m1 at t = 2</div>\n# research results reported within this paper. JH would like to thank Professor Wen-An Yong in Tsinghua University for many fruitful discussions. This work has been assigned a document release number LA-UR-21-28626.\n# Appendix A Collections of proofs\nAppendix A Collections of proofs\nIn this appendix, we collect some lemma and proofs. We start with a lemma which characterize the eigenspace of unreduced lower Hessenberg matrix.\nLemma A.1. For an unreduced lower Hessenberg matrix H = (hij)n\u00d7n, the geometric multiplicity of any eigenvalue \u03bb is 1 and the corresponding eigenvector is (q0(\u03bb), q1(\u03bb), \u00b7 \u00b7 \u00b7 , qn\u22121(\u03bb))T . Here {qi}0\u2264i\u2264n\u22121 is the associated polynomial sequence defined in (2.1). Proof. By Definition 2.1, we have that hij = 0 for j > i + 1 and hi,i+1 \u0338= 0 for i = 1, \u00b7 \u00b7 \u00b7 , n \u22121. Let r = (r1, r2, \u00b7 \u00b7 \u00b7 , rn) be an eigenvector associated with \u03bb. We write Ar = \u03bbr as an equivalent component-wise formulation:\nand\nHere we used the fact that hij = 0 for j > i + 1. Since hi,i+1 \u0338= 0 for i = 1, \u00b7 \u00b7 \u00b7 , n \u22121, (A.1) is\nHere we used the fact that hij = 0 for j > i + 1. Since hi,i+1 \u0338= 0 for i = 1, \u00b7 \u00b7 \u00b7 , n \u22121, (A.1) is equivalent to \uf8eb \uf8f6\n\uf8ed \uf8f8 From (A.3), we deduce that r1 \u0338= 0, otherwise r2 = \u00b7 \u00b7 \u00b7 = rn = 0. Moreover, ri for i = 2, \u00b7 \u00b7 \u00b7 , n are uniquely determined by r1. Therefore, the geometric multiplicity of \u03bb is 1. Moreover, without loss of generality, we take r1 = 1. In this case, r is exactly the same with (q0(\u03bb), q1(\u03bb), \u00b7 \u00b7 \u00b7 , qn\u22121(\u03bb))T . Here {qi}0\u2264i\u2264n\u22121 is the associated polynomial sequence defined in (2.1). Lemma A.2. Let H = (hij)n\u00d7n be an unreduced lower Hessenberg matrix and {qi}0\u2264i\u2264n is the associated polynomial sequence with H. If \u03bb is an eigenvalue of H, then \u03bb is a root of qn. Proof. From Lemma A.1, we have the geometric multiplicity of \u03bb is 1 and the corresponding eigenvector qn\u22121(\u03bb) = (q0(\u03bb), q1(\u03bb), \u00b7 \u00b7 \u00b7 , qn\u22121(\u03bb))T , i.e. Hqn\u22121(\u03bb) = \u03bbqn\u22121(\u03bb). Plugging \u03bb into (2.2), we immediately have qn(\u03bb) = 0, i.e., \u03bb is a root of qn.\n(A.1)\n(A.2)\n(A.3)\nProof. We start by proving that condition 1 and condition 2 are equivalent. First, it is easy to see that condition 2 implies condition 1. We only need to prove that condition 1 implies condition 2. Since A is real diagonalizable, all the eigenvalues of A are real. Moreover, for any eigenvalue of A, the geometric multiplicity is equal to its algebraic multiplicity. By Lemma A.1, the geometric multiplicity of any eigenvalue of an unreduced lower Hessenberg matrix is 1. Therefore, any eigenvalue of A has algebraic multiplicity of 1, i.e. all the eigenvalues of A are distinct. Next, we prove that the equivalence of condition 2 and condition 3. It is easy to see that, condition 3 implies condition 2 from Theorem 2.3, and condition 2 implies condition 3 from Lemma A.2. This completes the proof.\n# A.2 Proof of Lemma 3.3\nProof. We start from the definition of Legendre polynomials by the generating function \u221e \ufffd\nIntroduce the variable s such that\nwhich is equivalent to\n\u221e \ufffd n=0 tn \ufffd1 \u22121 xmPn(x)dx (A.4) = \ufffd1 \u22121 xmdx \u221a 1 \u22122tx + t2 (A.6) = \ufffd1 \u22121 xm(1 \u2212ts)ds \u221a 1 \u22122tx + t2 (A.5)-(A.6) = \ufffd1 \u22121 \ufffd s + t 2(1 \u2212s2) \ufffdm\nDefine\nBy comparing the coefficients of tn on both sides of (A.7), we find that am,n = 0 if n > m or m, n has different parity. For n = m \u22122k for some integer k \u22650, we have\nBy comparing the coefficients of tn on both sides of (A.7), we find that am,n = 0 if n > m or m, n has different parity. For n = m \u22122k for some integer k \u22650, we have am,m\u22122k = 22k\u2212m \ufffdm 2k \ufffd\ufffd1 \u22121 s2k(1 \u2212s2)m\u22122kds = 22k\u2212m \ufffdm 2k \ufffd\ufffd1 0 2s2k(1 \u2212s2)m\u22122kds (A.9\nam,m\u22122k = 22k\u2212m \ufffdm 2k \ufffd\ufffd1 \u22121 s2k(1 \u2212s2)m\u22122kds = 22k\u2212m \ufffdm 2k \ufffd\ufffd1 0 2s2k(1 \u2212s2)m\u22122kds (A.\n(A.4)\n(A.5)\n(A.6)\n(A.7)\n(A.8)\n(A.9)\nhere in the fourth equality we used the relation between the gamma function and the beta function\nand in the last equality we used the properties of the gamma function: for any integer n \u22650\nLastly, using the orthogonality relation \ufffd1 \u22121 Pm(x)Pn(x) = 2 2m+1\u03b4m,n, we have for any integer\nLastly, using the orthogonality relation \ufffd1 \u22121 Pm(x)Pn(x) = 2 2m+1\u03b4m,n, we h m \u22650,\nxm = \u230am/2\u230b \ufffd k=0 \ufffd2m \u22124k + 1 2 \ufffd am,m\u22122kPm\u22122k(x) = \u230am/2\u230b \ufffd k=0 m!(2m \u22124k + 1) 2kk!(2m \u22122k + 1)!!Pm\u22122k(x) (A.1\nThis completes the proof.\n(A.10)\n(A.11)\n(A.12)\n(A.13)\n[1] G. W. Alldredge, C. D. Hauck, D. P. OLeary, and A. L. Tits. Adaptive change of basis in entropy-based moment closures for linear kinetic equations. Journal of Computational Physics, 258:489\u2013508, 2014. [2] G. W. Alldredge, C. D. Hauck, and A. L. Tits. High-order entropy-based closures for linear transport in slab geometry ii: A computational study of the optimization problem. SIAM Journal on Scientific Computing, 34(4):B361\u2013B391, 2012. [3] G. W. Alldredge, R. Li, and W. Li. Approximating the M2 method by the extended quadrature method of moments for radiative transfer in slab geometry. Kinetic & Related Models, 9(2):237, 2016. [4] B. Amos, L. Xu, and J. Z. Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017. [5] L. Bois, E. Franck, L. Navoret, and V. Vigon. A neural network closure for the Euler-Poisson system based on kinetic simulations. arXiv preprint arXiv:2011.06242, 2020. [6] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 113(15):3932\u20133937, 2016. [7] Z. Cai, Y. Fan, and R. Li. Globally hyperbolic regularization of Grad\u2019s moment system in one-dimensional space. Communications in Mathematical Sciences, 11(2):547\u2013571, 2013. [8] Z. Cai, Y. Fan, and R. Li. Globally hyperbolic regularization of Grad\u2019s moment system. Communications on Pure and Applied Mathematics, 67(3):464\u2013518, 2014. [9] Z. Cai, Y. Fan, and R. Li. On hyperbolicity of 13-moment system. Kinetic & Related Models, 7(3):415, 2014. 10] S. Chandrasekhar. On the radiative equilibrium of a stellar atmosphere. The Astrophysical Journal, 99:180, 1944. 11] M. M. Crockatt, A. J. Christlieb, C. K. Garrett, and C. D. Hauck. An arbitrary-order, fully implicit, hybrid kinetic solver for linear radiative transport using integral deferred correction. Journal of Computational Physics, 346:212\u2013241, 2017.\n[12] M. M. Crockatt, A. J. Christlieb, C. K. Garrett, and C. D. Hauck. Hybrid methods for radiation transport using diagonally implicit runge\u2013kutta and space\u2013time discontinuous galerkin time integration. Journal of Computational Physics, 376:455\u2013477, 2019. [13] M. Elouafi and A. D. A. Hadj. A recursion formula for the characteristic polynomial of hessenberg matrices. Applied mathematics and computation, 208(1):177\u2013179, 2009. [14] Y. Fan, R. Li, and L. Zheng. A nonlinear hyperbolic model for radiative transfer equation in slab geometry. SIAM Journal on Applied Mathematics, 80(6):2388\u20132419, 2020. [15] Y. Fan, R. Li, and L. Zheng. A nonlinear moment model for radiative transfer equation in slab geometry. Journal of Computational Physics, 404:109128, 2020. [16] M. Frank, C. D. Hauck, and E. Olbrant. Perturbed, entropy-based closure for radiative transfer. arXiv preprint arXiv:1208.0772, 2012. [17] H. Grad. On the kinetic theory of rarefied gases. Communications on Pure and Applied Mathematics, 2(4):331\u2013407, 1949. [18] J. Han, A. Jentzen, and W. E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505\u20138510, 2018. [19] J. Han, C. Ma, Z. Ma, and W. E. Uniformly accurate machine learning-based hydrodynamic models for kinetic equations. Proceedings of the National Academy of Sciences, 116(44):21983\u2013 21991, 2019. [20] C. Hauck and R. McClarren. Positive PN closures. SIAM Journal on Scientific Computing, 32(5):2603\u20132626, 2010. [21] C. D. Hauck. High-order entropy-based closures for linear transport in slab geometry. Communications in Mathematical Sciences, 9(1):187\u2013205, 2011. [22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016. [23] J. Huang, Y. Cheng, A. J. Christlieb, and L. F. Roberts. Machine learning moment closure models for the radiative transfer equation I: directly learning a gradient based closure. arXiv preprint arXiv:2105.05690, 2021.\n[24] J. Huang, Y. Cheng, A. J. Christlieb, L. F. Roberts, and W.-A. Yong. Machine learning moment closure models for the radiative transfer equation II: enforcing global hyperbolicity in gradient based closures. arXiv preprint arXiv:2105.14410, 2021. [25] J. Huang, Z. Ma, Y. Zhou, and W.-A. Yong. Learning thermodynamically stable and Galilean invariant partial differential equations for non-equilibrium flows. Journal of Non-Equilibrium Thermodynamics, 2021. [26] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456. PMLR, 2015. [27] G.-S. Jiang and C.-W. Shu. Efficient implementation of weighted ENO schemes. Journal of Computational Physics, 126(1):202\u2013228, 1996. [28] A. D. Klose, U. Netz, J. Beuthan, and A. H. Hielscher. Optical tomography using the timeindependent equation of radiative transfer\u2014part 1: forward model. Journal of Quantitative Spectroscopy and Radiative Transfer, 72(5):691\u2013713, 2002. [29] R. Koch and R. Becker. Evaluation of quadrature schemes for the discrete ordinates method. Journal of Quantitative Spectroscopy and Radiative Transfer, 84(4):423\u2013435, 2004. [30] V. M. Laboure, R. G. McClarren, and C. D. Hauck. Implicit filtered PN for high-energy density thermal radiation transport using discontinuous galerkin finite elements. Journal of Computational Physics, 321:624\u2013643, 2016. [31] E. Larsen and J. Morel. Asymptotic solutions of numerical transport problems in optically thick, diffusive regimes ii. Journal of Computational Physics, 83(1), 1989. [32] C. Levermore. Relating eddington factors to flux limiters. Journal of Quantitative Spectroscopy and Radiative Transfer, 31(2):149\u2013160, 1984. [33] C. D. Levermore. Moment closure hierarchies for kinetic theories. Journal of statistical Physics, 83(5):1021\u20131065, 1996. [34] R. Li, W. Li, and L. Zheng. Direct flux gradient approximation to close moment model for kinetic equations. arXiv preprint arXiv:2102.07641, 2021. [35] C. Ma, B. Zhu, X.-Q. Xu, and W. Wang. Machine learning surrogate models for Landau fluid closure. Physics of Plasmas, 27(4):042502, 2020.\n[36] R. Maulik, N. A. Garland, J. W. Burby, X.-Z. Tang, and P. Balaprakash. Neural network representability of fully ionized plasma fluid model closures. Physics of Plasmas, 27(7):072106, 2020. [37] R. G. McClarren and C. D. Hauck. Robust and accurate filtered spherical harmonics expansions for radiative transfer. Journal of Computational Physics, 229(16):5597\u20135614, 2010. [38] E. Murchikova, E. Abdikamalov, and T. Urbatsch. Analytic closures for M1 neutrino transport. Monthly Notices of the Royal Astronomical Society, 469(2):1725\u20131737, 2017. [39] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. [40] G. C. Pomraning. The Equations of Radiation Hydrodynamics. Pergamon Press, Oxford, UK, 1973. [41] W. A. Porteous, M. P. Laiu, and C. D. Hauck. Data-driven, structure-preserving approximations to entropy-based moment closures for kinetic equations. arXiv preprint arXiv:2106.08973, 2021. [42] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019. [43] S. Schotth\u00a8ofer, T. Xiao, M. Frank, and C. D. Hauck. A structure-preserving surrogate model for the closure of the moment system of the Boltzmann equation using convex deep neural networks. arXiv preprint arXiv:2106.09445, 2021. [44] J. B. Scoggins, J. Han, and M. Massot. Machine learning moment closures for accurate and efficient simulation of polydisperse evaporating sprays. In AIAA Scitech 2021 Forum, page 1786, 2021. [45] D. Serre. Systems of Conservation Laws 1: Hyperbolicity, entropies, shock waves. Cambridge University Press, 1999. [46] C.-W. Shu and S. Osher. Efficient implementation of essentially non-oscillatory shock-capturing schemes. Journal of Computational Physics, 77(2):439\u2013471, 1988.\n[47] G. Szeg. Orthogonal Polynomials, volume 23. American Mathematical Soc., 1939. [48] L. Wang, X. Xu, B. Zhu, C. Ma, and Y.-A. Lei. Deep learning surrogate model for kinetic Landau-fluid closure with collision. AIP Advances, 10(7):075108, 2020. [49] W.-A. Yong. Basic aspects of hyperbolic relaxation systems. In Advances in the theory of shock waves, pages 259\u2013305. Springer, 2001. [50] Y. Zhu, L. Hong, Z. Yang, and W.-A. Yong. Conservation-dissipation formalism of irreversible thermodynamics. Journal of Non-Equilibrium Thermodynamics, 40(2):67\u201374, 2015.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of enforcing hyperbolicity in machine learning (ML) moment closure models for the radiative transfer equation (RTE), which is crucial for ensuring well-posedness in numerical simulations. Previous methods have shown improvements in accuracy but struggled with stability and hyperbolicity, necessitating a new approach.",
        "problem": {
            "definition": "The problem at hand is the moment closure problem in the context of the RTE, where the equations governing higher-order moments depend on knowledge of lower-order moments, leading to an unclosed system.",
            "key obstacle": "The main difficulty is ensuring hyperbolicity in the ML moment closure models, which is essential for the stability of the numerical solutions over long time simulations."
        },
        "idea": {
            "intuition": "The idea is inspired by the mathematical structure of the coefficient matrix in the closure system, which is a lower Hessenberg matrix. This structure allows for a relationship between its eigenvalues and an associated polynomial.",
            "opinion": "The proposed method involves designing neural network architectures that enforce hyperbolicity by ensuring that the eigenvalues derived from the closure model are real and distinct.",
            "innovation": "The innovation lies in the development of two neural network architectures: one that guarantees weak hyperbolicity and physical characteristic speeds, and another that ensures strict hyperbolicity without guaranteeing bounded eigenvalues."
        },
        "method": {
            "method name": "Hyperbolic ML Moment Closure Model",
            "method abbreviation": "HMLMCM",
            "method definition": "A machine learning-based approach to enforce hyperbolicity in moment closure models by relating eigenvalues of the coefficient matrix to roots of an associated polynomial.",
            "method description": "The method utilizes neural networks to learn the relationship between the highest moment's gradient and lower moments, ensuring hyperbolicity through the structure of the coefficient matrix.",
            "method steps": [
                "Define the closure model using a lower Hessenberg matrix.",
                "Relate the eigenvalues of the closure system to the roots of an associated polynomial.",
                "Design neural network architectures that enforce hyperbolicity based on this relation.",
                "Train the networks using data from numerical solutions of the RTE."
            ],
            "principle": "The effectiveness of this method is based on the mathematical properties of lower Hessenberg matrices, ensuring that the closure model remains hyperbolic and stable in numerical simulations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using benchmark tests, including the Gaussian source problem and the two-material problem, with comparisons to existing closure models.",
            "evaluation method": "Performance was assessed by measuring accuracy, stability, and generalizability of the proposed ML closure models against traditional methods."
        },
        "conclusion": "The proposed hyperbolic ML closure models demonstrate good accuracy and stability in various numerical tests, effectively capturing kinetic effects with fewer moments. However, some numerical instability was observed with small moment numbers, indicating areas for future improvement.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved accuracy and stability in long-time simulations, as well as the ability to maintain physical characteristic speeds.",
            "limitation": "The primary limitation is the numerical instability observed when using a small number of moments, which may lead to weak hyperbolicity in the model.",
            "future work": "Future research will focus on stabilizing the closure system while maintaining accuracy, as well as exploring enhancements to the neural network architectures."
        },
        "other info": {
            "acknowledgment": "The authors thank various funding bodies and institutions for their support in this research, including NSF grants and the High Performance Computing Center at Michigan State University."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational idea behind semi-supervised algorithms is to leverage both labeled and unlabeled data, which is similar to the proposed method of using machine learning to enforce hyperbolicity in moment closure models."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of the hyperbolic ML moment closure model is to address the stability and hyperbolicity issues in numerical simulations of the radiative transfer equation."
        },
        {
            "section number": "1.3",
            "key information": "The main goal of the paper is to develop a machine learning approach that enforces hyperbolicity in moment closure models to improve accuracy and stability in numerical simulations."
        },
        {
            "section number": "3.4",
            "key information": "The proposed method, Hyperbolic ML Moment Closure Model (HMLMCM), utilizes neural networks to learn the relationship between the highest moment's gradient and lower moments, ensuring hyperbolicity."
        },
        {
            "section number": "4.1",
            "key information": "The importance of data labeling in the context of the proposed method is crucial as it involves training neural networks using data from numerical solutions of the radiative transfer equation."
        },
        {
            "section number": "7.1",
            "key information": "Challenges related to scalability and computational complexity are highlighted, particularly in maintaining stability and hyperbolicity in long-time simulations."
        },
        {
            "section number": "8",
            "key information": "The conclusion emphasizes that the proposed hyperbolic ML closure models demonstrate good accuracy and stability, but also notes the necessity for future improvements to address numerical instability."
        }
    ],
    "similarity_score": 0.5727572543269785,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Machine learning moment closure models for the radiative transfer equation III_ enforcing hyperbolicity and physical characteristic speeds.json"
}