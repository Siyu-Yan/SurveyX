{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.17890",
    "title": "SLMRec: Empowering Small Language Models for Sequential Recommendation",
    "abstract": "Sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user\u2019s actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in realworld platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs\u2019 depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, our motivational experiments reveal that most intermediate layers of LLMs are redundant, indicating that pruning the remaining layers can still maintain strong performance. Motivated by this insight, we empower small language models for SR, namely SLMREC, which adopt a simple yet effective knowledge distillation method. Moreover, SLMREC is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMREC model attains the best performance using only 13% of the parameters found in LLM-based recommendation models, while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively. Besides, we provide a theoretical justification for why small language models can perform comparably to large language models in SR. The source code and datasets are available at the URL 1.",
    "bib_name": "xu2024slmrecempoweringsmalllanguage",
    "md_text": "# SLMREC: EMPOWERING SMALL LANGUAGE MODELS FOR SEQUENTIAL RECOMMENDATION\nWujiang Xu1, Qitian Wu2, Zujie Liang3, Jiaojiao Han4, Xuying Ning5, Yunxiao Shi6, Wenfang Lin3, Yongfeng Zhang1\u2217 1 Rutgers University 2 Broad Institute of MIT and Harvard 3 Ant Group 4 Dian Diagnostics Group 5 University of Illinois Urbana-Champaign 6 University of Technology Sydney\n 3 Oct 2024\n# ABSTRACT\nSequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user\u2019s actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in realworld platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs\u2019 depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, our motivational experiments reveal that most intermediate layers of LLMs are redundant, indicating that pruning the remaining layers can still maintain strong performance. Motivated by this insight, we empower small language models for SR, namely SLMREC, which adopt a simple yet effective knowledge distillation method. Moreover, SLMREC is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMREC model attains the best performance using only 13% of the parameters found in LLM-based recommendation models, while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively. Besides, we provide a theoretical justification for why small language models can perform comparably to large language models in SR. The source code and datasets are available at the URL 1.\narXiv:2405.17890v2\n# INTRODUCTION\nLearning temporal interest information is fundamental for sequential recommendation models. Traditional sequential recommendation (TSR) methods (Wu et al., 2017; Hidasi et al., 2015; Kang & McAuley, 2018; Sun et al., 2019) focus on the development of intricate sequential encoders, evolving from LSTM and GRU architectures to the self-attention layers and Transformer models. However, the state-of-the-art performance in TSR has hit a plateau, limited by model sizes that usually feature fewer than 0.1 billion parameters. Recently, Large Language Models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Anil et al., 2023) have made significant advancements in various aspects by scaling the size of the training data or the model\u2019s architecture. Building upon the scaling laws delineated in prior research (Kaplan et al., 2020; Hoffmann et al., 2022), it endows LLMs with enhanced expressivity, culminating in superior performance benchmarks. Naturally, a burgeoning trend among contemporary LLM-based recommendation architectures has raised concerns. The current LLM-based recommender system\n\u2217Corresponding author. 1https://github.com/WujiangXu/SLMRec\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99ed/99ed84e9-9796-41c7-9f0f-1d7bd185031a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/030f/030f35ba-d404-4264-8284-87c6d3a8ee14.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">nput tokens Candidate item (a) TSR methods</div>\n<div style=\"text-align: center;\">(b) G-LLMRec, E-LLMRec methods</div>\nFigure 1: This overview compares traditional sequential recommendation (TSR) methods with LLM-based recommendation (LLMRec) methods. Here, hu and hi represent the user and item representations, respectively. In contrast to G-LLMRec methods, E-LLMRec approaches adhere to the TSR prediction framework. These methods leverage LLMs as feature extractors in the manner of BERT, diverging from the generative focus of G-LLMRec.\ncan be classified as 1) generation-based approaches, e.g., P5 (Geng et al., 2022; Xu et al., 2023a), CoLLM (Zhang et al., 2023b) and LLaRa (Liao et al., 2023); 2) embedding-based approaches such as E4SRec (Li et al., 2023a), CLLM4Rec (Zhu et al., 2023) and Lite-LLM4Rec (Wang et al., 2024). As shown in Fig. 1, generation-based approaches (G-LLMRec) encode an item as a token and formulate the sequential recommendation as the next token prediction task. By contrast, embedding-based approaches (E-LLMRec) regard the last hidden representation as user representation and learn an external adapter to compute user-item preference. The adoption of LLMs has vastly driven the development of sequence recommendation tasks, bringing an improvement of nearly 20% against the TSR model on the benchmark (Li et al., 2023a; Liao et al., 2023; Wang et al., 2024). This arouses the following research motivation for this work. \u2022 Some researchers (Ardalani et al., 2022; Zhang et al., 2023a; 2024) have attempted to investigate the scaling laws in the recommendation domain. However, the largest model examined in these studies is less than 1 billion parameters, significantly smaller than the 175 billion parameters of GPT-3 (Brown et al., 2020). Additionally, the focus has been primarily on test loss rather than on ranking-based evaluation metrics, which limits the practical applicability of their findings. Recent studies (Liang et al., 2023; Gromov et al., 2024; Men et al., 2024) on the NLP domain suggest a high degree of redundancy in the LLMs\u2019 model architecture. Since the ID information of the recommendation domain has not been explicitly learned during the LLMs\u2019 training process, we also aim to find out whether increasing the model size of LLMs is beneficial for the SR task. \u2022 Despite the large performance gain, the LLMRec methods also escalate the model size significantly, e.g., nearly 70 times greater parameters compared with TSR models (from 0.1B to 7B+). Even within the parameter-efficient training technique (Hu et al., 2021a), the paradigm still poses a significant challenge for real-world sequential recommendation use cases, where billions of traffic logs every day and potential new items need to be processed constantly. This disparity imposes strict hardware demands and makes it both inefficient and infeasible to deploy the LLMRec model. Our contributions. This paper presents an initial attempt to reassess the need for LLMs in sequential recommendation. To explore the reasons for the significant improvement of LLMRec methods, we conduct a series of experiments on large-scale industry datasets to investigate the effects of reducing the number of parameters during the training and inference stages on overall performance. From the empirical results, we found some profound insights that the improvement of the rise of the model parameters is not consistent. Meanwhile, it reveals that some layers of LLMs are redundant in the recommendation task, similar to findings in NLP domains (Men et al., 2024; Gromov et al., 2024). Motivated by these findings, we empower small language models for the sequential recommendation, named SLMREC. We adopt the vanilla knowledge distillation approaches to align the representation knowledge. Moreover, multiple supervision signals are crafted to steer the student model toward acquiring task-aware knowledge within its hidden representations. Additionally, our model oper-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6611/66112e89-5888-49bd-a607-7d601bfee898.png\" style=\"width: 50%;\"></div>\nFigure 2: We present the relationship between the number of decoder layers and the final recommendation performance, with the performance of SASRec plotted as a baseline. Figures (a)-(c) show the results of directly using representations from the middle layers for inference without training, while (d)-(f) prune the later layers and train a model using only the specified number of layers. From the results, we observe that deeper decoder layers introduce redundancy in recommendation tasks, with models utilizing fewer layers (8-layer) achieving performance nearly equivalent to (24-layer) models. ates without the need for any supplementary model design elements and is compatible with other quantization and pruning techniques utilized within LLMs. Extensive experiments have revealed that SLMRec, with a model size of less than 1 billion parameters, can deliver performance that is remarkably competitive with baselines using LLMs sized over 7 billion parameters. Furthermore, SLMRec achieves up to 6.6x/8.0x speedup in terms of training/inference time costs against LLM-based recommendation models. Besides, we present the results of SLMRec employing online knowledge distillation, demonstrating its competitive performance. Beyond empricial experiment results, we provide a theoretical justification for why small language models can perform comparably to large language models in SR.\n# 2 MOTIVATIONAL EXPERIMENTS\nAs described above, here we try to explore the effectiveness of LLMs in recommendation vi decreasing the parameters of popular LLMs (i.e., LLaMa-7B) and observe the change in performanc\ndecreasing the parameters of popular LLMs (i.e., LLaMa-7B) and observe the change in performance. Evaluation Protocol. In the motivational experiment, we select SASRec as a traditional sequential recommendation baseline due to its performance (Klenitskiy & Vasilev, 2023). We adopt2 embeddingbased method (Li et al., 2023a) as the baseline, named E4SRec, to easily generate the ranking for the full/sampled list of items. As shown in Fig. 2, a pre-trained embedding layer learned from SASRec is used to obtain the sequential item embedding. Then we concatenate the item embeddings with the prompt embeddings obtained after the tokenization. After encoding of stacked attention blocks of LLM, we regard the representation of the last layers as the user representation. Then, we follow the TSR methods to calculate the inner product of user embeddings and item embeddings from the pre-trained embedding layer to serve as the score for the user-item pair. Also, cross-entropy loss and fully candidate item are utilized for the optimization to achieve best results (Xu et al., 2024a; Petrov & Macdonald, 2023). To reduce both computational demands and processing time, LoRA (Hu et al., 2021a) is used to update a comparatively smaller set of parameters. Besides, to generate an unbiased evaluation for fair comparison (Krichene & Rendle, 2020; Zhao et al., 2020), we randomly sampled\n2To accelerate and align with the prediction head of traditional SR methods, we remove the original softma layer and instead use the dot product of the user and item representations to compute the prediction score.\n<div style=\"text-align: center;\">(f) Cloth (Train) - MRR</div>\n999 negative items, which were items not interacted with by the user, along with 1 positive item that served as the ground-truth interaction. To obtain large-scale industry data, we use the Amazon 18 version3 dataset in this paper. More details are shown in Section 5. Evaluation Strategy. To examine the connection between the number of parameters and the performance of LLM-based methods (E4SRec), we have truncated the original LLM architecture\u2014in this case, a 32-layer decoder from the LLaMa 7B model\u2014by pruning the decoder layers during both the inference and the training stages. As a direct inference method, we refrain from additional training using new labels and instead directly employ the output from the final ten layers as user representations to gauge recommendation performance. Instead of direct inference, we focus on conserving the initial layers of the decoder and proceed to train a more lightweight E4SRec model while adhering to the original training protocol. The models resulting from varying levels of layer retention are designated as E4SRecl, with the variable l indicating the number of layers retained. The chosen values of l encompass a spectrum, specifically {1, 2, 4, 8, 16, 24, 32}. Results from both experimental approaches are graphically depicted in Figure 2, providing insight into how the models\u2019 depth influences their recommendation capabilities. Insights. From Figure 2 (a)-(b), we can observe that directly utilizing the representation of other layers without training cannot obtain a comparative performance. Compared to TSR baseline SASRec, Figure 2 (c)-(d) yield the following insightful findings: (1) As the number of layers increases, the performance of the model also improves. Furthermore, even when the model has the same layer number (i.e., l=2) as SASRec, its performance is still superior to that of SASRec. We assume the gains observed in LLM-based methods could likely be attributed to the larger hidden representation size ((i.e., 4096 V.S. 128), the initialization from LLMs, and the introduction of PEFT (Hu et al., 2021a). (2) When l is set ranging from 8-24, the model\u2019s improvement is slight. It reveals that an 8-layer E4SRec8 can obtain nearly as informative user representations as a 24-layer E4SRec24. Considering the two findings above, it naturally inspires us to explore better training methods to obtain a smaller-size LLM-based SR model that is comparable with large models. If we want to learn a E4SRecM that perform similar as E4SRecN (M < N), we should make sure the intermediate representations in E4SRecM to be as closer to those in E4SRecN as possible. Knowledge distillation (KD) is a straightforward idea in this case. Thus, we design a simple yet effective knowledge distillation method to train a tiny LLM-based model with similar performance. For the motivation experiment results in Movie domain, it can be found in Appendix B.1. In Section 6, we provide a theoretical justification that aligns with these empirical insights.\n# 3 PRELIMINARIES\nIn this study, rather than constructing complex additional structures, we slightly modify existing E-LLMRec methods for our purposes. Initially, we delineate the E-LLMRec model that we employ for sequential recommendation tasks.\nfor sequential recommendation tasks. Model structure. The E-LLMRec models capitalize on an ID embedding layer from TSR models such as BERT4Rec, SASRec, and GRU4Rec, which is pre-trained on a designated dataset (Sun et al., 2019; Kang & McAuley, 2018; Hidasi et al., 2015). The objective of sequential recommendation is to forecast subsequent items utilizing the user action sequence S = (i1, i2, ..., iT ), a sequence that is either truncated or padded to maintain uniform length. Through truncation and padding, we derive the user\u2019s action sequence mask, serving as the attention mask in LLMs (Large Language Models). The fixed-length sequence S \u2208RT is translated into a sequential representation S \u2208RT \u00d7d0 via the pre-trained ID embedding layer. A linear transformation is then applied to upscale the representation from a lower dimension d0 to a higher dimension d1 suitable for the hidden layers within the LLMs. Upon defining the prompt template, the tokenization layer within the LLMs processes the natural language input into corresponding text embeddings and their associated attention masks. These embeddings and attention masks, derived from both the ID sequence and the text, are then introduced into the LLM decoder. The final temporal output hM from the last layer of the decoder is inferred as the user representation and subsequently mapped through a linear layer to condense the dimensionality from d1 back to d0. Finally, user-item interaction predictions \u00afp are inferred by executing a dot product between the user and item representations. The learning process of the model is refined through the application of a cross-entropy loss.\n3https://nijianmo.github.io/amazon/index.html\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2375/2375bdca-5099-4077-86b5-133a6c7b1b23.png\" style=\"width: 50%;\"></div>\nFigure 3: The overview of SLMREC. A layer-wise knowledge distillation approach is applied to align the representation knowledge by grouping the layer into serveral blocks. The teacher and student model share a similar E-LLMRec model architecture. Multiple supervision signals are introduced to steer the student model toward acquiring fine-grained task-aware knowledge.\n  Knowledge Distillation. Knowledge distillation is a technique aimed at transferring knowledge from a sophisticated teacher model to a more streamlined student model (Hinton et al., 2015). We represent the teacher by ft(\u0398t) and the student by fs(\u0398s). We aim to solve the following optimization problem:\nHere, Dkd(\u0398t, \u0398s) signifies the knowledge distillation loss, which quantifies the discrepancies between the teacher and the student models. A prevalent method involves employing the KL divergence to evaluate the divergence between the logits produced by both models. One wellestablished training schema is known as offline distillation, wherein the teacher is fully trained beforehand and remains unchanged, while the student is refined based on the criteria outlined in Eq. 6. In the offline knowledge distillation manner, the teacher model \u0398t is initially trained in a designated training set by minimizing the cross-entropy loss Lce.\n# 4 SLMREC\nIn this work, we do not adopt logits-based knowledge distillation, as our goal is for the student model to learn how to encode hidden representations similar to the teacher model, rather than merely replicating its predictions. To achieve this, we perform feature distillation across multiple layers. Specifically, considering that the teacher model consists of M stacked decoder layers and the student model has N stacked decoder layers, we design several feature regularizers to guide the distillation process at regular intervals between the hidden representations of both models. We divide the layers of the teacher and student models into blocks by grouping every m layers of the teacher and every n layers of the student. The number of resulting blocks is B, calculated as B = \ufffdM m \ufffd = \ufffdN n \ufffd . Let the hidden representations from the teacher model be denoted as: Ht = {hm t , . . . , hM t }, where hm t represents the final temporal dimension of the hidden representation from the m-th layer of the teacher. Similarly, the hidden representations from the student model are denoted as: Hs = {hn s , . . . , hN s }.\n(1)\n(2)\nTable 1: Statistics of the Amazon datasets. |U|, |V|, and |E| denote the number of users, items, and ratings, respectively.\nTable 1: Statistics of the Amazon datasets. |U|, |V|, and |E| denote the number of users, items, and ratings, respectively.\nDataset\n|U|\n|V|\n|E|\nDensity\nCloth\n1,219,678\n376,858\n11,285,464\n0.002%\nMovie\n297,529\n60,175\n3,410,019\n0.019%\nMusic\n112,395\n73,713\n1,443,755\n0.017%\nSport\n332,447\n12,314\n146,639\n0.008%\nIn this study, we use a deeper LLM as the teacher model and a shallower LLM as the student model, both sharing the same hidden dimension d, such that Ht, Hs \u2208RB\u00d7d.\nIn this study, we use a deeper LLM as the teacher model and a shallower LLM as the student model, both sharing the same hidden dimension d, such that Ht, Hs \u2208RB\u00d7d. Feature Similarity. To regulate the alignment of feature directions between the teacher and student models, we employ a cosine similarity-based loss term. Formally, it is described by the equation:\nFeature Norm Regularization. In addition, we introduce a straightforward regularization term designed to minimize the L2 distance between the hidden representations of the teacher and student models. It is mathematically formulated as:\n\ufffd Multiple Supervision. Furthermore, we employ multiple supervision strategies to steer the student model toward assimilating specific aspects of recommendation-related knowledge. For each representation, we learn additional adapters ( Wa ) to reduce the dimension. The modified prediction ( \u02c6p(km) t ) can be acquired as described by Eq. 1:\n\ufffd Total Loss. Integrating the aforementioned distillation losses, the composite objective function for training the student model is given by: min [Lce(\u0398s) + \u03bb1Dcos(\u0398t, \u0398s) + \u03bb2Dnorm(\u0398t, \u0398s) + \u03bb3Lms(\u0398s, Wa)]. (6)\n\ufffd Total Loss. Integrating the aforementioned distillation losses, the composite objective function for training the student model is given by:\n\u03bb1, \u03bb2 and \u03bb3 are hyperparameters that control the contribution of each te\nwhere \u03bb1, \u03bb2 and \u03bb3 are hyperparameters that control the contribution of each term.\n# 5 EXPERIMENTS\nIn this section, we present extensive experiments to demonstrate the effectiveness of SLMREC, aiming to answer the following four research questions (RQs). \u2022 RQ1: How does the performance of our proposed SLMREC model compare to LLM-based recommendation models when evaluated on a large-scale industry dataset? \u2022 RQ2: What is the comparative efficiency and runtime of our SLMREC model against the GLLMRec and E-LLMRec models? \u2022 RQ3: Whether the proposed three knowledge regularizers work? \u2022 RQ4: Is it feasible to train our model, SLMREC, simultaneously with an untrained teacher model?\nIn this section, we present extensive experiments to demonstrate the effectiveness of SLMRE aiming to answer the following four research questions (RQs).\naiming to answer the following four research questions (RQs). \u2022 RQ1: How does the performance of our proposed SLMREC model compare to LLM-base recommendation models when evaluated on a large-scale industry dataset?\n# 5.1 EXPERIMENT SETUP\nFor our experimental evaluation, we utilize data from the clothing, movies, music, and sports categories within the extensive, industry-scale Amazon18 dataset4. Statistics of the datasets are\nFor our experimental evaluation, we utilize data from the clothing, movies, music, and sport categories within the extensive, industry-scale Amazon18 dataset4. Statistics of the datasets are\n4https://nijianmo.github.io/amazon/index.html\n(3)\n(4)\n(6)\nTable 2: Experimental results (%) on the Cloth and Movie dataset. The missing MRR value of Open-P5 is unavailable due to the time complexity constrictions. The number on the left of the arrow is the layers N of the student model. The left number on the right of the arrow is the layers M of the teacher model. For Open-P5, we adopt LLaMa as their backbone. We highlight the methods with the first and second best performances. Moreover, E4SRec4, which has the same number of layers as our SLMREC, is also marked.\nModel\nCloth\nMovie\nHR@1\nHR@5\nNDCG@5\nMRR\nHR@1\nHR@5\nNDCG@5\nMRR\nCaser\n9.66\n15.18\n12.66\n13.03\n4.27\n14.96\n9.57\n10.36\nGRU4Rec\n13.79\n15.46\n14.64\n15.15\n10.56\n19.47\n15.11\n15.46\nBERT4Rec\n13.60\n14.66\n14.14\n14.59\n9.68\n14.91\n12.40\n12.74\nSASRec\n13.08\n16.94\n15.01\n15.76\n5.57\n16.80\n11.17\n12.08\nHGN\n15.96\n18.70\n17.30\n18.27\n7.54\n19.20\n13.42\n14.73\nLightSANs\n14.12\n20.32\n17.30\n16.86\n6.08\n17.54\n11.81\n12.82\nOpen-P5\n14.13\n17.68\n17.02\n-\n12.66\n21.98\n17.13\n-\nE4SRec\n16.71\n19.45\n18.09\n18.77\n14.74\n23.79\n19.45\n19.74\nE4SRec8\n15.30\n18.54\n16.91\n17.60\n13.32\n22.49\n17.99\n18.46\nE4SRec4\n14.58\n18.05\n16.32\n17.01\n11.80\n21.54\n16.73\n17.20\nSLMRec4\u21908\n16.69\n19.47\n18.07\n18.74\n15.29\n24.25\n19.90\n20.36\nshown in Table 1. In all datasets, we interpret any rating above 3 as positive feedback, indicating user interaction with the item, and employ timestamps to establish the chronological order of actions. We eliminate users and items that have fewer than 5 associated actions to ensure sufficient data density. The historical sequence of interactions for each user is divided into three segments: (1) the most recent interaction is reserved for testing, (2) the second most recent for validation, and (3) all preceding interactions are used for training. Based on the ranking results, we utilize the typical top-N metrics hit rate (HR@{1, 5, 10}), normalized discounted cumulative gain (NDCG@{5,10}) (J\u00e4rvelin & Kek\u00e4l\u00e4inen, 2002) and Mean Reciprocal Rank (MRR) (Sarwar et al., 2001) to evaluate the model performance. For all the metrics, higher values indicate better performance. Models that achieve the highest MRR performance on the validation set, including ours and other baseline models, will be preserved for subsequent performance evaluation on the test set. In order to ensure an unbiased evaluation, we adopt the methodology employed in previous works (Krichene & Rendle, 2020; Zhao et al., 2020), wherein we randomly select 999 negative items (i.e., items that the user has not interacted with) and combine them with 1 positive item (i.e., a ground-truth interaction) to form our recommendation candidates for the ranking test. Detailed hyperparameters of our model in each dataset are in Appendix B.2.\nCompared Methods. We compare our method with three classes of baselines: (1) Single-domain sequential recommendation methods, i.e., GRU4Rec (Hidasi et al., 2015), Caser (Tang & Wang, 2018), HGN (Ma et al., 2019), BERT4Rec (Sun et al., 2019), SASRec (Kang & McAuley, 2018) and LightSANs (Fan et al., 2021). (2) G-LLMRec method: Open-P5LLaMa5 (Xu et al., 2023a). (3) E-LLMRec method: E4SRec (Li et al., 2023a). A detailed introduction to these baselines can be found in Appendix B.3. It should be noted that we did not select various G-LLMRec methods or E-LLMRec methods as baselines. This is because the differences between each LLM-based method are minimal, and our model is a universal approach that is not confined to a specific model type. Our primary focus is to improve the efficiency of language model utilization. Hence, we opted to select one G-LLMRec method (Open-P5) and one E-LLMRec method (E4SRec) as baselines. Quantitative Results (RQ1). Tables 2\u20133 showcase the quantitative comparison of four large-scale sequential recommendation datasets. From our analysis, we have several insightful observations: (1) LLM-based recommendation methods exhibit substantial improvements over traditional sequential recommendation (TSR) methods, primarily due to their enhanced modeling capacity which adeptly extracts informative sequential interest patterns. (2) Our model, SLMRec4\u21908, outperforms the\n5For Open-P5, we adopt the version of LLaMa as the foundation model in their code repository implementa tion to ensure the best results are achieved.\n<div style=\"text-align: center;\">Table 3: Experimental results (%) on the Music and Sport dataset.</div>\nModel\nMusic\nSport\nHR@1\nHR@5\nNDCG@5\nMRR\nHR@1\nHR@5\nNDCG@5\nMRR\nCaser\n0.71\n3.28\n1.96\n2.29\n1.05\n3.75\n2.39\n2.84\nGRU4Rec\n1.89\n3.22\n2.57\n3.08\n5.26\n7.75\n6.52\n7.08\nBERT4Rec\n2.10\n3.16\n2.64\n3.11\n4.81\n6.70\n5.79\n6.26\nSASRec\n1.82\n5.72\n3.79\n4.51\n4.70\n8.43\n6.59\n7.24\nHGN\n2.01\n5.49\n3.82\n4.17\n3.42\n6.24\n4.83\n5.30\nLightSANs\n1.05\n4.06\n2.54\n3.00\n5.18\n8.94\n7.07\n7.72\nOpen-P5\n4.35\n8.12\n6.74\n-\n5.49\n8.50\n6.92\n-\nE4SRec\n5.62\n9.29\n7.50\n7.98\n6.40\n9.67\n8.05\n8.70\nE4SRec8\n5.46\n8.86\n7.21\n7.74\n5.48\n8.63\n7.06\n7.76\nE4SRec4\n5.33\n8.75\n7.08\n7.59\n5.41\n8.65\n7.04\n7.72\nSLMRec4\u21908\n5.72\n9.15\n7.48\n8.03\n6.62\n9.83\n8.25\n8.89\n<div style=\"text-align: center;\">Table 4: Experiment results (%) of ablation study.</div>\nSLMREC\nCloth\nMovie\nHR@1\nHR@5\nNDCG@5\nMRR\nHR@1\nHR@5\nNDCG@5\nMRR\n+Dcos\n16.10\n18.85\n17.48\n18.17\n14.83\n23.08\n19.08\n19.45\n+Dcos,Dnorm\n16.28\n19.12\n17.69\n18.40\n14.86\n23.89\n19.36\n19.84\n+Dcos,Lms\n16.85\n19.05\n17.96\n18.59\n15.05\n23.48\n19.40\n19.76\n+Dcos,Dnorm,Lms\n16.69\n19.47\n18.07\n18.74\n15.29\n24.25\n19.90\n20.36\nteacher model E4SRec8 by leveraging knowledge distillation within the hidden layers. By refraining from applying this constraint prior to the prediction phase, we enable the final representation to organically gravitate towards the label\u2014yielding an approximate 8% enhancement in performance in comparison to the teacher model. (3) Introducing vanilla knowledge distillation techniques into LLMRec, without altering the model structure, allows SLMRec4\u21908 to achieve a marginally superior performance compared to E4SRec32. This suggests that small language models equipped with efficacious training strategies can rival, or even exceed, larger language models in the sequential recommendation task. This phenomonon is also matched with our therotical justification in Section 6. Model Efficiency (RQ2). We report the time efficiency and parameters of comparative baselines and our model in Table 5. All time and parameter metrics represent the average across the four datasets reported. Inference time evaluates the prediction ranking among 1,000 candidate items for each user. Detailed training and inference times for each dataset are provided in Appendix B.4. The Open-P5, an LLMRec model based on generative methods, offers a reasonable training duration. Yet, during the inference phase, it becomes considerably time-consuming (4942 hours) as it necessitates generating a substantial pool of candidate items (for instance, 1000). Owing to the intrinsic workings of generative LLMs, employing generation-based LLMRec models for the comprehensive ranking of extensive item sets is not advised. Our model outperforms E4SRec with enhanced efficiency, maintaining only 13% and 14% in E4SRec\u2019s parameters for training and inference, respectively. Moreover, our SLMREC demonstrates a remarkable gain in speed, being 6.6 times faster during training and 8.0 times quicker in inference than E4SRec. Ablation Study (RQ3). As shown in Table 4, SLMREC, when enhanced with various knowledge regularizers (namely Dcos, Dnorm and Lms), demonstrates improved performance. The regularizers Dcos and Dnorm aid SLMREC in aligning its intermediate representations with those of the teacher model, thereby endowing it with more potent representational extraction capabilities. Meanwhile, Lms steers the model to assimilate domain knowledge pertinent to recommendation systems within its preliminary layers. The ablation study in Music and Sport domain can be found in Appendix B.5.\n# 5.3 MODEL STUDY\nStudy of Online KD (RQ4). In our methodology, we first train the teacher model on downstream recommendation tasks and then train the student model through knowledge distillation, which is an offline knowledge distillation technology. In this section, we demonstrate that we can train both the\nTable 5: Efficiency comparison of Open-P5, E4SRec, and our SLMREC in terms of epoch-wise training time (hours), inference time (hours), number of training parameters (B) and inference parameters (B). These comparisons were conducted on a machine with an A100 GPU. The training batch size for all models was standardized at 256. During inference, E4SRec and SLMREC utilized a batch size of 512, whereas Open-P5\u2019s inference was performed with a batch size of 1.\nMethod\nTr time(h)\nInf time(h)\nTr params (B)\nInf params (B)\nOpen-P5LLaMa\n0.92\n4942\n0.023\n7.237\nE4SRec\n3.95\n0.415\n0.023\n6.631\nSLMREC4\u21908\n0.60\n0.052\n0.003\n0.944\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0877/0877518c-2ccc-41a9-9da0-725dbcd49c09.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Experiment results (%) of online KD and block number B in the Cloth dataset.</div>\nteacher model and SLMREC together on downstream recommendation tasks, which constitutes an online knowledge distillation. Under this setting, we are able to achieve comparative results. Study of block number B. We also conducted experiments to investigate the effect of block number B. As shown in Figure 4, when B is set to 4, our model achieves the best performance. When B is set to 1 or 2, the feature constraint imitation for each block within SLMREC is diminished relative to the teacher model, resulting in a decline in performance.\n# 6 THEORETICAL JUSTIFICATIONS\nBeyond empirical experiments, we aim to provide insights into why small language models can perform as effectively as large language models in learning desirable user representations. Specifically, we focus on the feature propagation process within a single layer of an LLM, as outlined below:\nwhere H(k) represents the hidden representation of the k-th layer, and A(k) is the attention matrix. In LLaMa, the attention matrix is defined as A = softmax \ufffd Q\u2032K\u2032\u22a4 \u221adk \ufffd , where Q\u2032 and K\u2032 incorporate rotational encoding (Su et al., 2024). Our analysis is hinged on interpreting the stack of propagation layers of Transformers as optimization dynamics for minimizing energies of certain forms (Shuman et al., 2013; Kalofolias, 2016; Fu et al., 2022; Wu et al., 2024). Proposition 1. Given the updating matrix \u02c6A(k) = A(k) + I, Eqn. 7 is equivalent to a gradient descent step with respect to the following optimization problem: \ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd As A(k) changes across layers, multi-layer attention models can be interpreted as a series of iterative descent steps, each focusing on layer-specific denoising objectives. We will show that this multi-layer structure can be simplified into a single-layer model while retaining the same denoising effectiveness. Proposition 2. For any K-layer attention model (where K is an arbitrary positive integer) with the layer-wise updating rule defined by Eqn. 7, there exists C\u2217such that one gradient descent step for the optimization problem (from the initial embeddings H(0)) \ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd here C\u2217associated with A, can yield the output embeddings H(K) of the K-layer model.\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd e C\u2217associated with A, can yield the output embeddings H(K) of the K\n(7)\n(8)\n(9)\nThese findings indicate that for any multi-layer stacked decoder, an equivalent single-layer decoder can be constructed to encode hidden representations in a similar way. Moreover, while the multi-layer model optimizes distinct objectives at each layer, this may introduce redundancy when compared to a single-layer model that achieves its objective in a single step. Consistent with the motivation underlying our framework design, we employ knowledge distillation (KD) to guide the one-layer network, enabling it to streamline the learning process and replicate the feature extraction capabilities of a multi-layer network.\n# 7 RELATED WORK\nIn this section, we introduce the most related background and scientific investigations to this work, which are roughly divided into five categories, i.e., 1) Sequential Recommendation, 2) Knowledge Distillation (KD), 3) Depth-wise Knowledge of LLMs, 4) Model Pruning, and 5) Parameter-Efficient Fine-Tuning (PEFT). For details on sections three through five, please refer to Appendix ??. Sequential Recommendation. Traditional Sequential Recommendation (TSR) methods (Wu et al., 2017; Hidasi et al., 2015; Sun et al., 2019; Kang & McAuley, 2018) primarily focus on developing various temporal encoders to capture short- and long-term user interests. The evolution of temporal sequential encoders has progressed from LSTM units (Wu et al., 2017) and GRU units (Hidasi et al., 2015), to more advanced architectures such as graph neural networks (He et al., 2020; Xu et al., 2023b; 2024c), self-attention layers (Kang & McAuley, 2018; Xu et al., 2024b), and Transformer models (Sun et al., 2019). Following the triumph of large language models (LLMs), researchers have begun leveraging open-source LLMs (Touvron et al., 2023) to construct their recommendation systems (Ji et al., 2024; Bao et al., 2023; Wei et al., 2024). G-LLMRec methods (Geng et al., 2022; Xu et al., 2023a; Zhang et al., 2023b; Liao et al., 2023; Mei & Zhang, 2023) generate the next item based on historical sequences, while E-LLMRec approaches (Li et al., 2023a; Zhu et al., 2023; Wang et al., 2024) use LLMs as feature extractors to learn user representations for prediction. More recently, (Zhai et al., 2024) introduces a generative sequential framework scalable up to GPT-3 dimensions, trained from scratch with 100 billion examples and utilizing 256 H100 GPUs. LLM-based recommendation systems frequently outperform TSR models by a margin of 20% (Li et al., 2023a; Liao et al., 2023; Wang et al., 2024), also increasing the parameters by nearly 100 times compared to TSR models. Therefore, the deployment of LLMRec models in real-world platforms is heavily constrained by computational resources. Knowledge Distillation (KD). Training a smaller \u201cstudent\" model on the distribution predicted by a large \u201cteacher\" model is known as a powerful knowledge distillation technique (Hinton et al., 2015). The fundamental insight behind this is to transform the knowledge and capabilities of the teacher into more compact, compressed, and possibly skill-specific representations (Jiao et al., 2020; Gu et al., 2024). For those cases when the student only has access to the output tokens generated by the teacher, another way of KD is data distillation (Eldan & Li, 2023; Li et al., 2023b; Fu et al., 2023; Hsieh et al., 2023). This technique first generates high-quality synthetic data by prompting the larger teacher model. The synthetic data are then used to enhance the student\u2019s capabilities by fine-tuning. Our work lies in the former series, which performs a simple yet effective layer-skipping approach to transfer those useful representations of the teacher to the student. Depth-wise Knowledge of LLMs The recent community interest stems from how linguistic properties and knowledge are encoded in language models. (Meng et al., 2022; Dai et al., 2022) emphasize that knowledge localizes within the middle or final layers. On the other hand, (Hase et al., 2024) attempts to perform knowledge editing and concludes that information may be stored non-locally across layers. What\u2019s more, (Men et al., 2024; Gromov et al., 2024) share a similar view that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge. By contrast, we are the first to investigate which part of knowledge on the LLMs plays a key role, especially in the sequential recommendation scene. Model Pruning Model Pruning is a fundamental approach for reducing the size of a well-trained large model by removing unimportant parameters (Hassibi & Stork, 1992). Recent work has focused on applying pruning methods to the Transformer architecture (Vaswani et al., 2017). These works have studied different components of the model architecture for pruning, including dropping attention heads (Voita et al., 2019; Michel et al., 2019), dropping layers (Fan et al., 2019; Zhang & He, 2020;\nKim & Awadalla, 2020; Sajjad et al., 2023), dropping hidden states (Hou et al., 2020), replacing sparse weight matrices with smaller dense ones (Ashkboos et al., 2024), and combinations of these solutions. By contrast, our work performs layer removal through simple knowledge distillation, rather than more complex pruning techniques. Parameter-Efficient Fine-Tuning (PEFT) PEFT emerges as a novel technique for tailoring Large Language Models (LLMs) to specific tasks while ensuring minimal computational and memory costs (Houlsby et al., 2019; Lester et al., 2021; Hu et al., 2021b; Liu et al., 2022). In this work, we combine our method with the Low-Rank Adapters (LoRA) (Hu et al., 2021b) to reduce the memory and computation of the knowledge distillation process. Specifically, we freeze the pre-trained model and only tune a small set of additional trainable parameters.\n# 8 CONCLUSIONS AND OUTLOOKS\nThis paper explores the effectiveness of large language models (LLMs) in sequential recommendation. Our motivational experiments reveal that intermediate layers in LLMs are redundant for achieving optimal recommendation performance. Motivated by empirical insights, we adopt vanilla knowledge distillation methods to improve the performance of small language models. Achieving only 13% of the parameters compared to the LLMRec baseline, our SLMREC model yields an 8x acceleration and slightly better performance. On top of our technical contributions, we believe the results in this paper could shed light on a new promising direction for building effective and efficient recommenders based on LLMs, which is largely under-explored. Additionally, we provide theoretical justifications showing that while multi-layer models optimize distinct objectives at each layer, this can introduce redundancy compared to a single-layer model that achieves its objective in one step. These theoretical insights align with the motivation behind our framework design, where we employ knowledge distillation (KD) to guide the one-layer network, enabling it to streamline the learning process and replicate the feature extraction capabilities of a multi-layer network. Future Work. This work concentrates on enhancing the efficiency of Large Language Model (LLM) utilization in the sequential recommendation. A notable limitation is the model\u2019s inability to adapt to new scenarios through few-shot learning. When confronted with a fresh dataset or new traffic logs from the platform, the model requires retraining from the entire dataset. In contrast, LLMs have demonstrated promising results in adapting to downstream language tasks using few-shot learning approaches. Looking ahead, we intend to investigate the incorporation of incremental learning into LLM-based recommendations to bolster the model\u2019s transferability. Additionally, integrating auxiliary linguistic and visual information of users and items into the LLMRec model may offer further improvements in its adaptability to new scenarios.\nREFERENCES\n# REFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and Adnan Aziz. Understanding scaling laws for recommendation models. arXiv preprint arXiv:2208.08489, 2022. Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pp. 1007\u20131014, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8493\u20138502, 2022. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023. Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. Xinyan Fan, Zheng Liu, Jianxun Lian, Wayne Xin Zhao, Xing Xie, and Ji-Rong Wen. Lighter and better: low-rank decomposed self-attention networks for next-item recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pp. 1733\u20131737, 2021. Guoji Fu, Peilin Zhao, and Yatao Bian. p-Laplacian based graph neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 6878\u20136917. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/fu22e.html. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, pp. 10421\u201310430. PMLR, 2023. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022. Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=5h0qf7IBZZ. Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. Advances in Neural Information Processing Systems, 36, 2024.\nBabak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pp. 639\u2013648, 2020. Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33: 9782\u20139793, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pp. 2790\u20132799. PMLR, 2019. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021a. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021b. Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446, 2002. Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, and Yongfeng Zhang. Genrec: Large language model for generative recommendation. In European Conference on Information Retrieval, pp. 494\u2013502. Springer, 2024. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4163\u20134174, 2020. Vassilis Kalofolias. How to learn a graph from smooth signals. In Artificial Intelligence and Statistics, pp. 920\u2013929. PMLR, 2016. Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), pp. 197\u2013206. IEEE, 2018. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Young Jin Kim and Hany Hassan Awadalla. Fastformers: Highly efficient transformer models for natural language understanding. arXiv preprint arXiv:2010.13382, 2020.\nAnton Klenitskiy and Alexey Vasilev. Turning dross into gold loss: is bert4rec really better than sasrec? In Proceedings of the 17th ACM Conference on Recommender Systems, pp. 1120\u20131125, 2023. Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1748\u20131757, 2020. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021. Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation. arXiv preprint arXiv:2312.02443, 2023a. Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023b. Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-aware layer-wise distillation for language model compression. In International Conference on Machine Learning, pp. 20852\u201320867. PMLR, 2023. Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, and Xiangnan He. Llara: Aligning large language models with sequential recommenders. arXiv preprint arXiv:2312.02445, 2023. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61\u201368, 2022. Chen Ma, Peng Kang, and Xue Liu. Hierarchical gating networks for sequential recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 825\u2013833, 2019. Kai Mei and Yongfeng Zhang. Lightlm: a lightweight deep and narrow language model for generative recommendation. arXiv preprint arXiv:2310.17488, 2023. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022. Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. Aleksandr Vladimirovich Petrov and Craig Macdonald. gsasrec: Reducing overconfidence in sequential recommendation trained with negative sampling. In Proceedings of the 17th ACM Conference on Recommender Systems, pp. 116\u2013128, 2023. Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023. Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pp. 285\u2013295, 2001. David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE signal processing magazine, 30(3):83\u201398, 2013.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 1441\u20131450, 2019. Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, pp. 565\u2013573, 2018. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019. Hanbing Wang, Xiaorui Liu, Wenqi Fan, Xiangyu Zhao, Venkataramana Kini, Devendra Yadav, Fei Wang, Zhen Wen, Jiliang Tang, and Hui Liu. Rethinking large language model architectures for sequential recommendations. arXiv preprint arXiv:2402.09543, 2024. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pp. 806\u2013815, 2024. Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing. Recurrent recommender networks. In Proceedings of the tenth ACM international conference on web search and data mining, pp. 495\u2013503, 2017. Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, and Junchi Yan. Simplifying and empowering transformers for large-graph representations. Advances in Neural Information Processing Systems, 36, 2024. Cong Xu, Zhangchi Zhu, Jun Wang, Jianyong Wang, and Wei Zhang. Fairly evaluating large language model-based recommendation needs revisit the cross-entropy loss. arXiv preprint arXiv:2402.06216, 2024a. Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. Openp5: Benchmarking foundation models for recommendation. arXiv preprint arXiv:2306.11134, 2023a. Wujiang Xu, Shaoshuai Li, Mingming Ha, Xiaobo Guo, Qiongxu Ma, Xiaolei Liu, Linxun Chen, and Zhenfeng Zhu. Neural node matching for multi-target cross domain recommendation. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pp. 2154\u20132166. IEEE, 2023b. Wujiang Xu, Xuying Ning, Wenfang Lin, Mingming Ha, Qiongxu Ma, Qianqiao Liang, Xuewen Tao, Linxun Chen, Bing Han, and Minnan Luo. Towards open-world cross-domain sequential recommendation: A model-agnostic contrastive denoising approach. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 161\u2013179. Springer, 2024b. Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, and Junchi Yan. Rethinking cross-domain sequential recommendation under open-world assumptions. In Proceedings of the ACM on Web Conference 2024, pp. 3173\u20133184, 2024c. Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, et al. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. arXiv preprint arXiv:2402.17152, 2024.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 1441\u20131450, 2019. Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, pp. 565\u2013573, 2018. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019. Hanbing Wang, Xiaorui Liu, Wenqi Fan, Xiangyu Zhao, Venkataramana Kini, Devendra Yadav, Fei Wang, Zhen Wen, Jiliang Tang, and Hui Liu. Rethinking large language model architectures for sequential recommendations. arXiv preprint arXiv:2402.09543, 2024. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pp. 806\u2013815, 2024. Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing. Recurrent recommender networks. In Proceedings of the tenth ACM international conference on web search and data mining, pp. 495\u2013503, 2017. Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, and Junchi Yan. Simplifying and empowering transformers for large-graph representations. Advances in Neural Information Processing Systems, 36, 2024. Cong Xu, Zhangchi Zhu, Jun Wang, Jianyong Wang, and Wei Zhang. Fairly evaluating large language model-based recommendation needs revisit the cross-entropy loss. arXiv preprint arXiv:2402.06216, 2024a. Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. Openp5: Benchmarking foundation models for recommendation. arXiv preprint arXiv:2306.11134, 2023a. Wujiang Xu, Shaoshuai Li, Mingming Ha, Xiaobo Guo, Qiongxu Ma, Xiaolei Liu, Linxun Chen, and Zhenfeng Zhu. Neural node matching for multi-target cross domain recommendation. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pp. 2154\u20132166. IEEE, 2023b. Wujiang Xu, Xuying Ning, Wenfang Lin, Mingming Ha, Qiongxu Ma, Qianqiao Liang, Xuewen Tao, Linxun Chen, Bing Han, and Minnan Luo. Towards open-world cross-domain sequential recommendation: A model-agnostic contrastive denoising approach. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 161\u2013179. Springer, 2024b. Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, and Junchi Yan. Rethinking cross-domain sequential recommendation under open-world assumptions. In Proceedings of the ACM on Web Conference 2024, pp. 3173\u20133184, 2024c. Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, et al. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. arXiv preprint arXiv:2402.17152, 2024.\nBuyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, et al. Wukong: Towards a scaling law for large-scale recommendation. arXiv preprint arXiv:2403.02545, 2024. Gaowei Zhang, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and Ji-Rong Wen. Scaling law of large sequential recommendation models. arXiv preprint arXiv:2311.11351, 2023a. Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. Advances in Neural Information Processing Systems, 33:14011\u201314023, 2020. Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. Collm: Integrating collaborative embeddings into large language models for recommendation. arXiv preprint arXiv:2310.19488, 2023b. Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu, and Ji-Rong Wen. Revisiting alternative experimental settings for evaluating top-n item recommendation algorithms. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp. 2329\u20132332, 2020. Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al. Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms. In proceedings of the 30th acm international conference on information & knowledge management, pp. 4653\u20134664, 2021. Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for recommender systems. arXiv preprint arXiv:2311.01343, 2023.\nCONTENTS\n# 1 Introduction\n# 2 Motivational Experiments\n# 3 Preliminaries\n3 Preliminaries\n# 4 SLMRec\n# 5 Experiments\n5.1 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Performance Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Model Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.1 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Performance Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Model Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n# 6 Theoretical Justifications\n# 7 Related Work\n# 8 Conclusions and Outlooks\n# A Proof\nA.1 Proof for Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Proof for Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.1 Proof for Proposition 1 A.2 Proof for Proposition 2\n# B Experiments\nB.1 Motivation Experiment Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Compared methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Model Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nB.1 Motivation Experiment Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 B.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 B.3 Compared methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 B.4 Model Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 B.5 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n# A PROOF\nA.1 PROOF FOR PROPOSITION 1\nProposition 1. Given the matrix \u02c6A(k) = A(k) + I, Eqn. 7 is equivalent to a gradient descent step with step size 1 for the following optimization problem:\n\ufffd\ufffd\ufffd\ufffd Proof. The cost function at the k-th layer is denoted by E(H; H(k \u22121)) = min H \ufffd\ufffd\ufffdH \u2212\u02c6A(k)H(k\u22121)\ufffd\ufffd\ufffd 2 2\n\ufffd\ufffd Proof. The cost function at the k-th layer is denoted by\n\ufffd\ufffd Then, the gradient of E(H; H(k \u22121)) is computed as\nWith the step size 1 of the gradient descent, it minimizes the cost function E(H; H(k \u22121)) at the current layer is\n# A.2 PROOF FOR PROPOSITION 2\nProposition 2. For any K-layer attention model (where K is an arbitrary positive integer) with the layer-wise updating rule defined by Eqn. 7, there exists C\u2217such that one gradient descent step for the optimization problem (from the initial embeddings H(0))\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd where C\u2217associated with A, can yield the output embeddings H(K) of the K-layer model.\n\ufffd\ufffd Proof. Similar to Theorem 1, we define \u02c6A(k) to simpify the Eqn. 7. \u02c6A(k) = I + A(k),\nThen Eqn. 7 can be equivalently written as\nBy stacking K layers of propagation, we can denote the output embeddings as H(K) = \u02c6A(K)H(K\u22121) = \u02c6A(K) \u02c6A(K\u22121)H(K\u22122) = \u00b7 \u00b7 \u00b7 = \u02c6A(K) \u00b7 \u00b7 \u00b7 \u02c6A(1)H(0) = A\u2217H(0), ( where A\u2217defined as multiple matrix production. We can show that solving the denoising problem with gradient step size \u00b5\u2217 2 w.r.t. the objective\nBy stacking K layers of propagation, we can denote the output embeddings as\nBy stacking K layers of propagation, we can denote the output embeddings as\n(10)\n(11)\n(12)\n(13)\n(17)\n(18)\n(19)\n(20)\n(21)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b98e/b98e1bff-2d2d-4c0d-b3a4-820567244801.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Movie (Train) - HR@10</div>\n<div style=\"text-align: center;\">(e) Movie (Train) - NDCG@10</div>\nFigure 5: We present the relationship between the number of decoder layers and the final recommendation performance, with the performance of SASRec plotted as a baseline. Figures (a)-(c) show the results of directly using representations from the middle layers for inference without training, while (d)-(f) prune the later layers and train a model using only the specified number of layers. From the results, we observe that deeper decoder layers introduce redundancy in recommendation tasks, with models utilizing fewer layers (8-layer) achieving performance nearly equivalent to (24-layer) models. Defining C\u2217= 1 \u00b5\u2217(A\u2217\u2212(1 \u2212\u00b5\u2217)I), H(k) = A\u2217H(0) will induce the output embeddings H(K), by noticing that\n# B EXPERIMENTS\n# B.1 MOTIVATION EXPERIMENT RESULTS\n# B.2 TRAINING DETAILS\nIn Table 6, we provide hyper-parameters in our training stage. Our implementation is based on Huggingface Transformers 6. The input and intermediate hidden dimension in the feed-forward network is 4096. We use mixed precision training and train on 1*80G Nvidia A100 GPU.\n# B.3 COMPARED METHODS\n# Tranadtional sequential recommendation methods\n6https://github.com/huggingface/transformers\n<div style=\"text-align: center;\">(f) Movie (Train) - MRR</div>\n(22)\n<div style=\"text-align: center;\">Table 6: Hyper-parameter (HP) settings of our method on each dataset.</div>\nHP\nCloth\nMovie\nMusic\nSport\nadam_beta1\n0.9\n0.9\n0.9\n0.9\nadam_beta2\n0.999\n0.999\n0.999\n9.999\nadam_epsilon\n1e-8\n1e-8\n1e-8\n1e-8\nlearning_rate\n0.003\n0.001\n0.002\n0.002\nlogging_steps\n1\n1\n1\n1\nlr_scheduler_type\ncosine\ncosine\ncosine\ncosine\nmax_grad_norm\n1.0\n1.0\n1.0\n1.0\nmax_steps\n1500\n-1\n800\n2000\noptimizer\nadamw_torch\nadamw_torch\nadamw_torch\nadamw_torch\nsave_strategy\nsteps\nsteps\nsteps\nsteps\nsave_steps\n50\n100\n100\n100\neval_steps\n50\n100\n100\n100\nwarmup_steps\n50\n50\n100\n50\n\u03bb1\n1.0\n1.0\n1.0\n1.0\n\u03bb2\n0.1\n0.1\n0.1\n0.1\n\u03bb3\n1.0\n1.0\n0.01\n0.1\nb\n4\n4\n4\n4\nCaser (Tang & Wang, 2018) introduces a novel approach to sequential recommendation systems by modeling user-item interactions as sequences, which is designed to predict the next item a user may interact with by capturing both short-term and long-term dependencies in user behavior. GRU4Rec (Hidasi et al., 2015) tackles the issue of modeling sparse sequential data while also adapting RNN models to the recommender system. To achieve this, the authors propose a new ranking loss function that is specifically designed for training these models. The implementation of GRU4Rec in PyTorch can be found at the URL 7. BERT4Rec (Sun et al., 2019) designs a bidirectional self-attention network to model user behavior sequences. To prevent information leakage and optimize the training of the bidirectional model, a Cloze objective is used to predict the randomly masked items in the sequence by considering both their left and right context. The implementation of BERT4Rec in PyTorch can be found at the URL 8. SASRec (Kang & McAuley, 2018) is a self-attention based sequential model that addresses the challenge of balancing model parsimony and complexity in recommendation systems. By using an attention mechanism, SASRec identifies relevant items in a user\u2019s action history and predicts the next item based on relatively few actions, while also capturing long-term semantics like an RNN. This enables SASRec to perform well in both extremely sparse and denser datasets. The implementation of SASRec in PyTorch can be found at the URL 9. HGN (Ma et al., 2019) propose a novel hierarchical gating mechanism to effectively capture both short-term and long-term user preferences in sequential recommendation tasks. Their model dynamically selects relevant interaction history at multiple temporal levels, improving next-item prediction accuracy. This approach outperforms state-of-the-art methods while maintaining efficiency and scalability for large-scale recommendation systems. LightSANs (Fan et al., 2021) introduces a low-rank decomposition technique for self-attention networks, reducing their computational complexity while maintaining strong performance. This approach makes the model more efficient and scalable for large-scale recommendation tasks without compromising accuracy. For the code implementation of Caser, HGN and LightSANs, we run the experiment based on the RecBole (Zhao et al., 2021) 10.\n7https://github.com/hungpthanh/GRU4REC-pytorch 8https://github.com/jaywonchung/BERT4Rec-VAE-Pytorch 9https://github.com/pmixer/SASRec.pytorch 10https://github.com/RUCAIBox/RecBole/tree/master\n# 7https://github.com/hungpthanh/GRU4REC-pytorch 8https://github.com/jaywonchung/BERT4Rec-VAE-Pytorch 9https://github.com/pmixer/SASRec.pytorch 10https://github.com/RUCAIBox/RecBole/tree/master\n<div style=\"text-align: center;\">Table 7: Detailed efficiency comparison of Open-P5, E4SRec, and our SLMREC, in terms of training and inference time, on each dataset.</div>\nMethod\nCloth\nMovie\nMusic\nSport\nTr time (h)\nInf time (h)\nTr time (h)\nInf time (h)\nTr time (h)\nInf time (h)\nTr time (h)\nInf time (h)\nOpen-P5LLaMa\n1.36\n3554.43\n0.36\n3504\n0.35\n3692\n1.60\n9017\nE4SRec\n5.27\n0.578\n1.90\n0.208\n1.88\n0.216\n6.75\n0.660\nSLMREC4\u21908\n0.97\n0.070\n0.15\n0.030\n0.30\n0.030\n0.98\n0.078\n<div style=\"text-align: center;\">Table 8: Experiment results (%) of ablation study.</div>\nSLMREC\nMusic\nSport\nHR@1\nHR@5\nNDCG@5\nMRR\nHR@1\nHR@5\nNDCG@5\nMRR\n+Dcos\n5.62\n8.78\n7.23\n7.81\n6.25\n9.25\n7.76\n8.41\n+Dcos,Dnorm\n5.95\n9.26\n7.65\n8.23\n6.61\n9.82\n8.24\n8.87\n+Dcos,Lms\n5.69\n8.94\n7.36\n7.91\n6.51\n9.39\n7.96\n8.62\n+Dcos,Dnorm,Lms\n5.72\n9.15\n7.48\n8.03\n6.62\n9.83\n8.25\n8.89\n# LLM-based recommendation methods:\nOpen-P5 (Xu et al., 2023a) is an open-source platform introduced to catalyze research in LLM-based generative recommender systems. It supports key model architectures like T5 and Llama-2 across diverse public datasets, focusing on sequential and straightforward recommendation tasks. The platform emphasizes the role of item IDs through various indexing methods and offers a customizable, efficient, and standardized environment for developing and assessing recommender systems. The implementation of Open-P5 in PyTorch can be found at the URL 11. E4SRec (Li et al., 2023a) integrate of Large Language Models (LLMs) into sequential recommendation systems, offering a significant leap in handling item IDs and personalization. In the original paper, they use Softmax layer to output each user-item prediction score. The implementation of E4SRec in PyTorch can be found at the URL 12.\nOpen-P5 (Xu et al., 2023a) is an open-source platform introduced to catalyze research in LLM-based generative recommender systems. It supports key model architectures like T5 and Llama-2 across diverse public datasets, focusing on sequential and straightforward recommendation tasks. The platform emphasizes the role of item IDs through various indexing methods and offers a customizable, efficient, and standardized environment for developing and assessing recommender systems. The implementation of Open-P5 in PyTorch can be found at the URL 11.\nE4SRec (Li et al., 2023a) integrate of Large Language Models (LLMs) into sequential recommendation systems, offering a significant leap in handling item IDs and personalization. In the original paper, they use Softmax layer to output each user-item prediction score. The implementation of E4SRec in PyTorch can be found at the URL 12.\nB.4 MODEL EFFICIENCY\n# B.4 MODEL EFFICIENCY\nWe show the running time of Open-P5, E4SRec, and our SLMREC in each dataset. These comparisons were conducted on a machine with an A100 GPU. The training batch size for all models was standardized at 256. During inference, E4SRec and SLMREC utilized a batch size of 512, whereas Open-P5\u2019s inference was performed with a batch size of 1.\n# B.5 ABLATION STUDY\nWe present the remaining ablation study results in Table 8. SLMREC, when enhanced with various knowledge regularizers (namely Dcos, Dnorm and Lms), demonstrates improved performance. The regularizers Dcos and Dnorm aid SLMREC in aligning its intermediate representations with those of the teacher model, thereby endowing it with more potent representational extraction capabilities. Meanwhile, Lms steers the model to assimilate domain knowledge pertinent to recommendation systems within its preliminary layers.\n11https://github.com/agiresearch/OpenP5 12https://github.com/HestiaSky/E4SRec\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of inefficiency in applying large language models (LLMs) for sequential recommendation tasks due to their size and the redundancy found in their intermediate layers. It highlights the limitations of traditional sequential recommendation methods and the necessity for a more efficient approach that leverages the capabilities of smaller models.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of efficiently utilizing LLMs for sequential recommendation tasks, which involves predicting the next item a user is likely to interact with based on their past interactions.",
            "key obstacle": "The core obstacle preventing effective solutions is the large size of LLMs, which makes them impractical for real-world applications that need to process vast amounts of data daily."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that many intermediate layers of LLMs are redundant, and thus, a smaller model could perform comparably to larger models if properly designed.",
            "opinion": "The proposed idea, SLMREC, involves empowering small language models for sequential recommendation by using knowledge distillation to transfer knowledge from larger models while maintaining efficiency.",
            "innovation": "The innovation of SLMREC lies in its ability to achieve competitive performance with only 13% of the parameters of traditional LLMs, alongside significant improvements in training and inference speed."
        },
        "method": {
            "method name": "SLMREC",
            "method abbreviation": "SLMREC",
            "method definition": "SLMREC is a method designed to empower smaller language models for sequential recommendation tasks by leveraging knowledge distillation techniques to align the representations learned by a larger teacher model.",
            "method description": "SLMREC utilizes a simple yet effective knowledge distillation method to transfer knowledge from larger models to smaller ones, ensuring efficient performance.",
            "method steps": "1. Train a larger teacher model on the sequential recommendation task. 2. Use knowledge distillation to align the representations of the student model with those of the teacher. 3. Implement the student model for making predictions in sequential recommendation.",
            "principle": "The effectiveness of SLMREC is based on the observation that deeper layers in LLMs often introduce redundancy, allowing a simpler model to achieve similar performance with fewer parameters."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using the Amazon 18 dataset, focusing on categories such as clothing, movies, music, and sports. Several baseline methods were also compared, including traditional sequential recommendation models and LLM-based methods.",
            "evaluation method": "Performance was assessed using metrics such as hit rate (HR), normalized discounted cumulative gain (NDCG), and mean reciprocal rank (MRR). The models were evaluated based on their ability to predict user-item interactions."
        },
        "conclusion": "The experiments demonstrated that SLMREC can achieve comparable or better performance than larger LLM-based recommendation models while using significantly fewer parameters and reducing training and inference time.",
        "discussion": {
            "advantage": "SLMREC's key advantages include its efficiency in terms of parameter usage and speed, making it suitable for real-world applications that require rapid processing of large datasets.",
            "limitation": "A limitation of SLMREC is its current inability to adapt to new scenarios through few-shot learning, necessitating retraining with new data.",
            "future work": "Future research will explore integrating incremental learning capabilities into SLMREC to enhance its adaptability to new datasets and scenarios, as well as incorporating auxiliary information to improve recommendation performance."
        },
        "other info": {
            "source code": "The source code and datasets are available at the provided GitHub repository.",
            "dataset details": {
                "Cloth": {
                    "users": 1219678,
                    "items": 376858,
                    "interactions": 11285464,
                    "density": "0.002%"
                },
                "Movie": {
                    "users": 297529,
                    "items": 60175,
                    "interactions": 3410019,
                    "density": "0.019%"
                },
                "Music": {
                    "users": 112395,
                    "items": 73713,
                    "interactions": 1443755,
                    "density": "0.017%"
                },
                "Sport": {
                    "users": 332447,
                    "items": 12314,
                    "interactions": 146639,
                    "density": "0.008%"
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The paper highlights the limitations of traditional sequential recommendation methods and the necessity for a more efficient approach that leverages the capabilities of smaller models."
        },
        {
            "section number": "3.3",
            "key information": "The proposed idea, SLMREC, involves empowering small language models for sequential recommendation by using knowledge distillation to transfer knowledge from larger models while maintaining efficiency."
        },
        {
            "section number": "4.1",
            "key information": "SLMREC achieves competitive performance with only 13% of the parameters of traditional LLMs, alongside significant improvements in training and inference speed."
        },
        {
            "section number": "4.2",
            "key information": "SLMREC utilizes a simple yet effective knowledge distillation method to transfer knowledge from larger models to smaller ones, ensuring efficient performance."
        },
        {
            "section number": "10.1",
            "key information": "The core obstacle preventing effective solutions is the large size of LLMs, which makes them impractical for real-world applications that need to process vast amounts of data daily."
        },
        {
            "section number": "10.2",
            "key information": "Future research will explore integrating incremental learning capabilities into SLMREC to enhance its adaptability to new datasets and scenarios."
        }
    ],
    "similarity_score": 0.7781361448835218,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/SLMRec_ Empowering Small Language Models for Sequential Recommendation.json"
}