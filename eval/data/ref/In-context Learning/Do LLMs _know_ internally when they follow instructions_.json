{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.14516",
    "title": "Do LLMs \"know\" internally when they follow instructions?",
    "abstract": "Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs\u2019 internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs\u2019 instruction-following, paving the way for reliable LLM agents.1",
    "bib_name": "heo2024llmsknowinternallyfollow",
    "md_text": "# DO LLMS \u201cKNOW\u201d INTERNALLY WHEN THEY FOLLOW INSTRUCTIONS?\nJuyeon Heo University of Cambridge\u2217 jh2324@cam.ac.uk Christina Heinze-Deml Apple c heinzedeml@apple.com Oussama Elachqar o elachqar@apple.com Shirley Ren Apple c shirleyr@apple.com Udhay Nallasamy Apple udhay@apple.com Andy Miller Apple acmiller@apple.com Kwan Ho Ryan Chan UPenn* ryanckh@seas.upenn.edu Jaya Narain Apple jnarain@apple.com\n# Kwan Ho Ryan Chan UPenn*\nryanckh@seas.upenn.edu\nABSTRACT\n# ABSTRACT\nInstruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs\u2019 internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs\u2019 instruction-following, paving the way for reliable LLM agents.1\narXiv:2410.14516v4\n# INTRODUCTION\nGiven the potential of large language models (LLMs), there has been significant interest in utilizing these models to build personal AI agents. For instance, one could imagine deploying an LLM as a personal healthcare assistant, such as a fitness or nutrition planner, or for psychological counseling (Li et al., 2024b; Wang et al., 2023; Tu et al., 2024). Compared to traditional machine learningbased AI agents, LLMs offer the advantage of being easily adaptable through prompting, allowing users to provide guidelines and personal information without the need to retrain model weights. Instruction-following is critical in the development of personal AI agents with LLMs through prompts because these models must adhere to the constraints and guidelines to ensure safe and trustworthy interactions. For example, suppose an LLM is building a personal fitness plan for a user with knee problems. To avoid knee problems for the user, the LLM must follow the instruction of not recommending knee-intensive movements or any exercises that could lead to potential injury. Similarly, in a nutrition planner, the LLM should avoid generating harmful recommendations, such as suggesting inappropriate food for pregnant women or children with diabetes.\n\u2217This work was done during an Apple internship. 1We will release the data and code on GitHub upon publication.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f951/f951edac-9c95-4711-8482-9d5919b922f6.png\" style=\"width: 50%;\"></div>\nFigure 1: Overview of our paper. Left: Success and failure cases in a personalized AI fitness planner. The task is to generate a warm-up plan while avoiding knee-required positions. The success case follows the instruction, while the failure case violates it. Middle: Linear probing is applied to analyze internal representations from success and failure cases, identifying the instruction-following dimension. The probe is tested on unseen tasks (e.g., writing a CV) and instruction types (e.g., include/exclude keywords). Right: Representation engineering is used to shift failure cases into success by adjusting the representations along the instruction-following dimension, improving adherence without compromising task quality.\n<div style=\"text-align: center;\">Figure 1: Overview of our paper. Left: Success and failure cases in a personalized AI fitness planner. The task is to generate a warm-up plan while avoiding knee-required positions. The success case follows the instruction, while the failure case violates it. Middle: Linear probing is applied to analyze internal representations from success and failure cases, identifying the instruction-following dimension. The probe is tested on unseen tasks (e.g., writing a CV) and instruction types (e.g., include/exclude keywords). Right: Representation engineering is used to shift failure cases into success by adjusting the representations along the instruction-following dimension, improving adherence without compromising task quality.</div>\nHowever, LLMs often fail to follow even non-ambiguous and simple instructions (Zhou et al., 2023; Qin et al., 2024; Xia et al., 2024; Kim et al., 2024; Yan et al., 2024) like including keywords or following formatting guidelines. GPT-4 achieves around an 80% success rate on IFEval (Zhou et al., 2023), an instruction-following benchmark dataset, while smaller models have success rates around 30% to 40%. This raises the question: why do LLMs fail to follow instructions, even when those instructions are clear and familiar? To gain a better understanding of instruction-following outcomes, we analyze the internal state of LLMs, focusing on the differences in representations between success and failure cases of instruction-following across different tokens and layers. Our approach involves disentangling the effects of tasks and instructions in input prompts, where the instruction specifies the action (e.g., \u2018please do not use keywords\u2019) and the task provides the context for executing the instruction (e.g., \u2018please write a resume\u2019). By applying linear probing\u2014a widely used method for interpreting model representations (Alain & Bengio, 2016; Belinkov, 2022; Elazar et al., 2021)\u2014we identify a specific dimension within the input embedding space that is strongly associated with instruction-following. While previous work has primarily used linear probing to explore representations related to truthfulness and reducing hallucinations (Azaria & Mitchell, 2023; Marks & Tegmark, 2023; MacDiarmid et al., 2024), our study extends this method to investigate instruction-following. We demonstrate that this dimension generalizes to unseen tasks, indicating that it captures a fundamental aspect of instruction adherence in LLMs. To validate the significance of the instruction-following dimension, we applied representation engineering techniques to enforce instruction-following based on insights from our linear probes. Our experiments show that adjustments along this specific dimension are more effective in enhancing instruction-following success rates than random modifications, while maintaining the overall quality of the generated responses. These results indicate that the instruction-following dimension plays a crucial role in shaping the model\u2019s behavior, toward better adherence to instructions. To further interpret the meaning of this dimension, we conduct a sensitivity analysis based on three key perturbations to the input prompt: task familiarity, instruction difficulty, and phrasing. Our findings reveal that this dimension is more related to the rephrasing of prompts rather than the inherent difficulty of the task or instructions. This suggest that the way a prompt is encoded within the model\u2019s input representation space plays a significant role in whether the instruction is followed correctly. This observation not only provides a deeper understanding of why LLMs sometimes fail to adhere to straightforward instructions but also offers an explanation for the effectiveness of prompt engineering, even when the content of the prompt remains largely unchanged.\nOverall, this work sheds light on the underlying mechanisms of instruction-following in LLMs by uncovering a critical dimension in the model\u2019s representation space. These insights enhance our understanding of LLM behavior and offer practical approaches to improving instruction adherence, bringing us closer to developing more reliable and trustworthy AI agents.\n# 1.1 CONTRIBUTIONS\n\u2022 We identify a specific dimension within the input embeddings space of LLMs that is closely linked to instruction-following, using linear probes, by carefully designing our setting to disentangle the effects of tasks and instructions in input prompts. \u2022 We demonstrate that this dimension generalizes to unseen tasks and that modifying representations along this dimension effectively converts instruction-following failures into successes without compromising response quality. \u2022 Through a sensitivity analysis, our findings reveal that this dimension is linked to how prompts are rephrased, underscoring that instruction-following in LLMs is influenced by how prompts are encoded within the model\u2019s input embeddings. This explains why LLMs sometimes fail to follow clear, simple instructions and why prompt engineering can enhance instruction adherence, even when the content remains largely unchanged.\n# 2 DO LLMS KNOW WHEN THEY SUCCEED OR FAIL TO FOLLOW INSTRUCTIONS?\nIn this section, we aim to identify the dimension within the models\u2019 representation space that is closely associated with instruction-following. We use linear probes to determine the internal signals that separate successful instruction-following from failures and examine whether this dimension generalizes to different tasks and instruction types. By exploring different tokens and layers within the models, we seek to understand how and when instruction-following information is encoded.\n# 2.1 IFEVAL-SIMPLE\nFirst, we selected the IFEval dataset (Zhou et al., 2023) as a base, due to its objective evaluations with verifiable instructions, thereby minimizing uncertainties from ambiguous evaluation criteria. The IFEval dataset comprises 25 instruction types under 9 categories, with each instruction type paired with a distinct set of tasks \u2014 approximately 20 tasks per instruction type. Because of the relatively small number of tasks per instruction type, internal model states resulting from these prompts contain a mix of both instruction-following and task-specific details. To isolate the dimension related specifically to instruction-following, we generated a modified version of the IFEval data, called IFEval-simple.2 First, we selected 5 instruction types that are likely to be used in real-world applications for AI agents. For example, ensuring the inclusion (keywords:existence) or exclusion (keywords:forbidden) of specific keywords, specifying the frequency of certain keywords (keywords:frequency), generating responses with placeholders (detectable content:place holders), and requiring responses to end with predefined sentences (startend:end checker). We excluded more complex or impractical instructions, such as those requiring omission of punctuation, as they are less relevant for practical use cases. Second, we generated 100 tasks using GPT-4, similar to the original tasks in IFEval, where each instruction type is paired with the same set of 100 tasks. By pairing each instruction type with the same set of 100 tasks, we ensure that linear probes trained on the model\u2019s representations are more likely to capture information solely related to instruction-following, without the confounding influence of varying tasks. The instructions assigned to each task vary in detail based on the context. For example, for an instruction type focused on keyword inclusion or exclusion, a resume-writing task might require keywords like \u2018skills\u2019 and \u2018career\u2019, while a joke about a programmer might involve terms like \u2018syntax\u2019 or \u2018code\u2019. These variations introduce diverse challenges, testing the model\u2019s adaptability in following instructions. Example tasks are provided in Appendix Table 5 and Table 6. The instruction-following accuracy for IFEval-simple datasets is presented in Appendix Table 11.\nwill release the IFEval-simple data along with code on GitHub upon publication\nTask generalization\nInstruction-type generalization\nModel\nFirst token\nMiddle token\nLast token\nFirst token\nMiddle token\nLast token\nLLaMA-2-chat-7B (14 lyr)\n0.77 \u00b1 0.04\n0.55 \u00b1 0.07\n0.73 \u00b1 0.04\n0.52 \u00b1 0.03\n0.50 \u00b1 0.07\n0.52 \u00b1 0.05\nLLaMA-2-chat-13B (16 lyr)\n0.83 \u00b1 0.03\n0.58 \u00b1 0.06\n0.82 \u00b1 0.03\n0.56 \u00b1 0.06\n0.58 \u00b1 0.06\n0.53 \u00b1 0.03\nMistral-7B-inst-v0.3 (14 lyr)\n0.74 \u00b1 0.02\n0.54 \u00b1 0.05\n0.72 \u00b1 0.04\n0.50 \u00b1 0.05\n0.51 \u00b1 0.05\n0.51 \u00b1 0.05\nPhi-3-mini-128k (14 lyr)\n0.88 \u00b1 0.03\n0.56 \u00b1 0.04\n0.86 \u00b1 0.03\n0.55 \u00b1 0.04\n0.48 \u00b1 0.03\n0.50 \u00b1 0.03\nTable 1: Task and instruction-type generalization AUROC scores for task and instruction-type generalization using a 70-30 train-test split for task generalization on unseen tasks, and leave-oneout cross-validation for instruction-type generalization across different instruction types. Standard deviation is calculated from five runs with different random seeds for task generalization and across instruction types for instruction-type generalization.\nEarly layers\nMiddle layers\nLast layers\nModel\nFirst token\nMiddle token\nLast token\nFirst token\nMiddle token\nLast token\nFirst token\nMiddle token\nLast token\nLLaMA-2-chat-7B\n0.77 \u00b1 0.04\n0.55 \u00b1 0.07\n0.73 \u00b1 0.04\n0.75 \u00b1 0.05\n0.51 \u00b1 0.04\n0.76 \u00b1 0.04\n0.73 \u00b1 0.03\n0.54 \u00b1 0.02\n0.70 \u00b1 0.02\nLLaMA-2-chat-13B\n0.83 \u00b1 0.03\n0.58 \u00b1 0.06\n0.82 \u00b1 0.03\n0.81 \u00b1 0.02\n0.56 \u00b1 0.05\n0.80 \u00b1 0.04\n0.78 \u00b1 0.04\n0.49 \u00b1 0.03\n0.79 \u00b1 0.05\nMistral-7B-inst-v0.3\n0.74 \u00b1 0.02\n0.54 \u00b1 0.05\n0.72 \u00b1 0.04\n0.71 \u00b1 0.05\n0.51 \u00b1 0.03\n0.67 \u00b1 0.04\n0.71 \u00b1 0.03\n0.49 \u00b1 0.04\n0.70 \u00b1 0.03\nPhi-3-mini-128k\n0.88 \u00b1 0.03\n0.56 \u00b1 0.04\n0.86 \u00b1 0.03\n0.85 \u00b1 0.03\n0.56 \u00b1 0.03\n0.83 \u00b1 0.02\n0.65 \u00b1 0.05\n0.53 \u00b1 0.03\n0.63 \u00b1 0.04\nTable 2: Task generalization (detailed across layers) AUROC scores for the first, middle, and last tokens across early, middle, and last layers of various models. The layers selected for LLaMA-213B-chat are 16, 32, and 40, while for the other three models, the layers used are 14, 26, and 32.\n# 2.2 METHODS\nRepresentations We analyzed four language models: LLaMA-2-7B-chat (Touvron et al., 2023), LLaMA-2-13B-chat (Touvron et al., 2023), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Phi-3mini-128k-instruct (Abdin et al., 2024). For each model, we looked at the representations between tokens \u2013 the first, middle, and last tokens, representing the LLMs before, during, and after they generate responses. We also examined three layers (early, middle, last) to identify where instructionfollowing information is encoded within the models\u2019 internal state. Specifically, we used layers 16, 32, and 40 and for LLaMA-2-13B-chat and 14, 26, and 32 for other three models. To avoid randomness in decoding, we employed greedy decoding without sampling. Linear Probes We trained linear probes on the representations to identify the instruction-following dimension. A simple linear model was trained on instruction-following success outcome, optimized for 1000 epochs with AdamW, a 0.001 learning rate, and 0.1 weight decay. Train-test split and metric We assessed task generalization and instruction-type generalization by splitting the data into training and testing sets, as shown in Figure 1. IFEval-simple has 5 instruction types, each paired with the same set of 100 tasks. To evaluate task generalization, we split the data by the task dimension, using a 70-30 train-test split across the 100 tasks. To evaluate instruction-type generalization, we applied a leave-one-out approach, over the instruction-type dimension. To evaluate performance, we use the Area Under the Receiver Operating Characteristic Curve (AUC)(Pedregosa et al., 2011), assessing the accuracy of binary predictions for each model on unseen tasks and instruction types.\n# 2.3 RESULTS\nLinear probes generalize across unseen tasks The task generalization results in Table 1 show that linear probes performed well across different tasks when the instruction type remains consistent. The AUROC scores, which range from 0.7 to 0.8 using the first token, suggest that the input embeddings of these models possess a shared geometry related to instruction-following that generalizes well across varied tasks. This is particularly beneficial in the context of buliding AI agents, where a pre-defined consistent set of instructions needs to be followed across different tasks. For example, if a probe is trained on examples of an instruction type like \u201cPlease do not include these keywords\u201d using examples from resume writing and nutrition coaching, the linear probe can predict if the model follows the same instructions type even unseen tasks, such as creating a warm-up plan without knee-intensive exercises. Additionally, we plot the principal components analysis (PCA)\n<div style=\"text-align: center;\">Instruction-type generalization</div>\nInstructions\nLLaMA-2-chat-7B\nLLaMA-2-chat-13B\nMistral-7B-inst-v0.3\nPhi-3-mini-128k\nEarly lyr\nMiddle lyr\nLast lyr\nEarly lyr\nMiddle lyr\nLast lyr\nEarly lyr\nMiddle lyr\nLast lyr\nEarly lyr\nMiddle lyr\nLast lyr\nkey:forbidden\n0.52\n0.51\n0.56\n0.45\n0.45\n0.44\n0.44\n0.41\n0.46\n0.52\n0.54\n0.53\nkey:exist\n0.50\n0.50\n0.51\n0.67\n0.68\n0.66\n0.55\n0.50\n0.50\n0.63\n0.67\n0.68\nkey:freq\n0.57\n0.59\n0.59\n0.57\n0.57\n0.57\n0.56\n0.56\n0.56\n-\n-\n-\nnumber placeholders\n0.56\n0.54\n0.52\n0.58\n0.58\n0.54\n0.50\n0.49\n0.50\n0.50\n0.53\n0.46\nend checker\n0.48\n0.46\n0.47\n0.55\n0.57\n0.56\n0.44\n0.42\n0.45\n0.55\n0.59\n0.57\nAVERAGE\n0.52\n0.52\n0.53\n0.56\n0.57\n0.55\n0.50\n0.48\n0.49\n0.55\n0.58\n0.56\nTable 3: Instruction-type generalization (detailed) AUROC across different models and selected layers on first token representations. A leave-one-out approach was employed, and the standard deviation from training a linear probe is small enough to be omitted from the table. The \u2018-\u2019 mark in \u2018keywords:frequency\u2019 instruction type is due to an insufficient number of data points caused by a 100% success rate, making it impossible to compute reliable AUC scores.\nusing representations from the first token and early layers, fitting the PCA on the training split and visualizing the results on the test split (unseen tasks) in Figure 2. They show clear separability, supporting the idea that the instruction-following dimension is consistently represented across different tasks. Further PCA analysis is provided in Figure 7 in the Appendix. Linear probes do not generalize across unseen instruction types In contrast to task generalization, the models exhibit no clear generalization when tested across unseen instruction types. The AUROC scores for instruction-type generalization are notably lower, ranging from 0.50 to 0.55, close to chance (Table 1). A potential explanation for this poor generalization could be the limited number of instruction types used during training, where the linear probe was trained on just 4 instruction types. To investigate, we expanded the dataset to include 25 instruction types, each paired with 20 tasks. However, as shown in Appendix in Table 8, this expanded experiment yielded similar results, with models still failing to generalize well across unseen instruction types. This indicates that models struggle to generalize instruction-following across different instruction types, implying the absence of a \u2018global\u2019 instruction-following dimension that can be leveraged regardless of the instruction type, which may be due to varying representation geometries. First token is as informative as last token Interestingly, the first and last tokens\u2014representing the model\u2019s state before and after response generation\u2014show high AUROC scores, implying that LLMs may already \u201cknow\u201d whether they will follow instructions even before they start generating their responses. This early indication of instruction following is valuable, since early intervention or correction could be applied. In contrast, the middle tokens showed lower AUROC scores, likely because the representation contains information about next token generation more than information about instruction-following. Layer-wise performance is similar, with early layers slightly better for task generalization The performance across different layers shows only slight variations, with early layers marginally outperforming middle and last layers, as detailed in Table 2. For example, in the 13B model, the early layers achieve an AUROC of 0.83 for the early token, which is slightly better than the performance of middle and last layers. This suggests that the instruction-following dimension may be more prominently represented in the earlier stages of the model\u2019s processing. However, for instructiontype generalization, there is no clear pattern across layers (Table 3), indicating that the challenges associated with generalizing across different instruction types are pervasive throughout layers.\n# 3 REPRESENTATION ENGINEERING\nWe identified a dimension within the input embedding space associated with instruction-following. To evaluate whether this dimension significantly impacts the models\u2019 behavior, we manipulated the representations along this direction using representation engineering (Marks & Tegmark, 2023; Zou et al., 2023). An increase in the models\u2019 instruction-following success rate tied to manipulations along the identified direction validates the role of the dimension in shaping the models\u2019 generation outcomes toward instruction adherence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a97/9a979292-539a-4732-9117-5aa0d197b062.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Llama-2-13b-chat-hf (b) Llama-2-7b-chat-hf (c) Mistral-7B-Inst-v0.3</div>\n<div style=\"text-align: center;\">at-hf (b) Llama-2-7b-chat-hf (c) Mistral-7B-Inst-v0.3 (d) P</div>\nFigure 2: PCA plot of first token representations from early layers across four LLMs. PCA is fitted on the training split and visualized on the test split (unseen tasks). The PCA shows separability, suggesting the consistent capture of the instruction-following dimension across tasks. The analysis includes three instruction types from the keyword category in IFEval-simple. Additional PCA results for all five instruction types across different categories are provided in Appendix Figure 7.\nModel\nOriginal SR\nRandom SR\nInst-follow SR\nOriginal QR\nRandom QR\nInst-follow QR\nLLaMA-2-chat-7B\n0.57 \u00b1 0.00\n0.55 \u00b1 0.00\n0.59 \u00b1 0.00\n0.87 \u00b1 0.09\n0.85 \u00b1 0.10\n0.87 \u00b1 0.08\nLLaMA-2-chat-13B\n0.61 \u00b1 0.00\n0.54 \u00b1 0.12\n0.65 \u00b1 0.02\n0.92 \u00b1 0.00\n0.91 \u00b1 0.02\n0.94 \u00b1 0.00\nMistral-7B-inst-v0.3\n0.58 \u00b1 0.00\n0.56 \u00b1 0.02\n0.64 \u00b1 0.02\n0.95 \u00b1 0.02\n0.86 \u00b1 0.02\n0.98 \u00b1 0.06\nPhi-3-mini-128k\n0.71 \u00b1 0.00\n0.63 \u00b1 0.04\n0.74 \u00b1 0.01\n0.76 \u00b1 0.01\n0.76 \u00b1 0.01\n0.78 \u00b1 0.00\nTable 4: Representation Engineering results on the last layer across four models. Success rate (SR) for instruction-following and quality ratio (QR) for task quality are compared across the original outputs, outputs using the instruction-following dimension, and outputs using a random directions. RE along the instruction-following dimension improves SR while maintaining or enhancing QR, unlike random adjustments which often reduce both SR and QR. Standard deviations are across three runs with different random seeds.\n# 3.1 SETTINGS\nMethod For each input representation Roriginal, we applied a transformation in the identified direction D using the formula Rupdated = Roriginal + \u03b1 \u00d7 D, where \u03b1 is a scaling hyper-parameter. We applied this transformation to all input representations, including both success and failure cases, to evaluate whether RE could improve instruction following universally, without disrupting cases where the model was already successful. This adjustment was applied to the representations in the last layer of the model, as it was more robust to variations in \u03b1. We focused on the representation of the first token, which corresponds to the input embedding before any response generation, since the goal of representation engineering (RE) is to adjust internal representations before the response is generated to improve the model\u2019s instruction adherence. The direction D is the weight of a linear probes trained on all IFEval-simple dataset. 3 Metric We evaluated the success rate (SR) of instruction-following using predefined evaluation functions from the IFEval (Zhou et al., 2023). Additionally, we assessed the quality of the responses using GPT-4, scoring each response on a scale from 0 to 9 based on its relevance to the given task. We defined quality ratio (QR) as the number of responses scoring above 7 divided by the total number of responses that successfully follow instructions (this cutoff was defined based on the distribution of quality scores). F2T (False to True) and T2T (True to True) show how many failed responses became successful and how many successful ones remained so after modification. The Success conversion ratio (SCR) := F 2T (F 2T +F 2F ) indicates the proportion of originally failed responses that became successful after modification, while Success preservation ratio (SPR) := T 2T (T 2T +T 2F ) reflects the proportion of originally successful responses that remained successful. Baseline and hyperparameter selection To demonstrate the effectiveness of the identified instruction-following dimension, we compared it against random directions. Each model and in-\n3We also experimented with training the linear probe on 70% of the IFEval-simple dataset and applying RE to the remaining 30% test set. The results were similar but slightly worse than when the linear probe was trained and RE was applied to the entire dataset. Since our primary focus is on analyzing the variance caused by RE itself, rather than variance from train-test splits, we present the results using the full dataset here.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f4c/1f4ccb37-4b4b-43f2-a2a9-3850dc290ea3.png\" style=\"width: 50%;\"></div>\nFigure 3: Transition metric for Representation Engineering on the last layer of four models Success rate (SR) only on high quality responses in task execution (scoring above 7 by GPT-4, scale from 0 to 9). The Success conversion ratio (SCR) indicates the proportion of originally failed responses that became successful after modification, while Success preservation ratio (SPR) reflects the proportion of originally successful responses that remained successful.\nstruction type required a different \u03b1 value based on their specific geometry. If \u03b1 is too large, it can degrade the quality of responses; if too small, it may not effectively improve instruction-following. We selected \u03b1 for each model and instruction type using a validation set comprising 10% of the instruction data. The selected \u03b1 values were: 0.3 for Llama-2-chat-13b and Llama-2-chat-7b, 0.1 for Phi-3, and 0.15 for Mistral-7B.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d9a9/d9a9f7a7-e8e6-44f9-ae75-ed202d587205.png\" style=\"width: 50%;\"></div>\nYou are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to score a given output for the given instruction. You should give an overall score (an integer) on a scale of 0 to 9, where a higher score indicates better overall performance. Do NOT provide any explanation for your evaluation.\n# 3.2 RESULTS\nRE on instruction-following direction improves success rate while maintaining quality Our experiments demonstrate that applying the RE direction generally improves the instruction-following success rate (SR) across most models and instruction types. As shown in Table 4, the SR with the instruction-following direction usually outperforms the original success rate and is lower bounded by the the original SR \u2013 that is, the instruction-following dimension does not lead to worse than original SRs. Additionally, the QR remains equal to or higher than the original, indicating that RE can be applied with minimal risk of reducing response quality. Figure 5 in the Appendix provides an illustrative example of modified responses. In this case, the task was to write a resume with the instruction to include three specific keywords. The original response only included one keyword, whereas the modified response, guided by the instruction-following direction, successfully incorporated all three keywords, demonstrating the effectiveness of RE in enhancing instruction adherence. Instruction-following direction is better than random directions When comparing RE direction to random directions, RE consistently outperforms random directions in increasing the success rate across all instruction types and models, as illustrated in Table 4 and Figure 3. The ratios of Trueto-True (T2T) and False-to-True (F2T) transitions are typically larger for the instruction-following direction than for random directions, indicating a more reliable improvement in success rates.\n# INTERPRETING THE INSTRUCTION-FOLLOWING DIMENSION\nWhile manipulating representations along the instruction-following dimension reveals that it influences a model\u2019s behavior, the meaning behind this manipulation remains unclear. To interpret the meaning of the instruction-following dimension, we conduct a sensitivity analysis to investigate the relative of perturbations on the internal state of LLMs, compared to our identified direction. We consider three perturbation types: task familiarity, instruction difficulty, and phrasing. We (1) systematically alter the original input prompts in IFEval-simple dataset for each perturbation, (2) compute the resulting difference in internal state representation space before and after the perturbation, and (3) compute the cosine similarity between the perturbation-induced difference vector and the instruction-following dimension we identified. We designed prompt changes for each perturbation: (1) Task Familiarity: We investigated whether the instruction-following dimension might be related to how familiar the model is with a given task. For example, the task \u201cwrite a resume for software engineer\u201d might be more familiar to the model than \u201cwrite a summary about current events\u201d, if it was more common in the data used to train the LLMs. If a task is more familiar to a model, it may be easier for the model to follow instructions regarding that task. To perturb the model on task familiarity, we kept the instruction constant while changing the task to one with lower perplexity (Jelinek et al., 1977). Perplexity measures the probability of tokens in generation, reflecting task familiarity (Gonen et al., 2022), where high perplexity indicates a familiar task and vice versa. (2) Instruction Difficulty: We investigated the relationship of the instruction-following dimension with the complexity of the instructions. We perturbed the instruction difficulty by simplifying instructions by relaxing instruction-related constraints. For example, in the original instruction \u201cplease include keywords: coding, Python, computer, experience\u201d, we reduced the complexity by reducing the number of keywords required in the instruction to \u201cplease include the keywords: coding, Python\u201d. (3) Phrasing Modification: Finally, we examined whether the instruction-following dimension was correlated to how the prompt is phrased. We rephrased the prompts while keeping the meaning of the task and the instruction unchanged. For example, we modified \u201cWrite a resume for software engineer. Please include keywords such as coding, Python, computer, experience\u201d to \u201cI want you to write about software engineer resume including four words coding, Python, computer, or experience\u201d. We used GPT-4 to rephrase both the task and instruction in the input prompt, and applied GPT-4 again to validate that the meaning of the contents remained the same after rephrasing. We selected 20 prompts, each containing a task and an instruction from the \u2018forbidden keyword\u2019 instruction type in IFEval-simple dataset. For each perturbation type, we created five modified versions of each prompt. We then averaged the representations of these modified prompts and calculated the difference between this averaged representation and the representation of the original prompt. Finally, we assessed how well this difference vector aligned with the instruction-following dimension by computing the cosine similarity. Our findings, illustrated in Figure 4, show the sensitivy analysis results for two models: Llama-213b-chat and Llama-2-7b-chat. In both models, the results indicated that phrasing modifications have a stronger correlation with the instruction-following dimension than task familiarity or instruction difficulty. These results support the hypothesis that the instruction-following dimension is more closely tied to how prompts are phrased rather than the inherent difficulty of the task or the complexity of the instruction. This suggests that how prompts are phrased plays a critical role in determining whether LLMs will successfully follow the instructions, aligned to observations Lu et al. (2023); Sclar et al. (2023) showing LLMs are sensitive to prompt formatting.\n# 5 RELATED WORK\nInstruction-following in LLMs Recent research has introduced various benchmark datasets to evaluate the instruction-following capabilities of LLMs across different contexts(Zhou et al., 2023; Qin et al., 2024; Yan et al., 2024; Xia et al., 2024). Among these, we selected the IFEval dataset (Zhou et al., 2023) as the foundation for our study, due to its structured and objective evaluation framework that minimizes ambiguity by using verifiable instructions. Beyond evaluation, several approaches have been proposed to improve instruction-following performance, such as modifying\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/41b9/41b9410a-0c89-4629-bd26-d59f89bb0822.png\" style=\"width: 50%;\"></div>\nFigure 4: Cosine similarity alignment for modified data in the \u2018forbidden keyword\u2019 instruction type across two models (Llama-2-7b-chat (Left) and Llama-2-13b-chat (Right)). The figure shows the cosine similarity between the instruction-following dimension and the difference vector (computed as the difference between the original prompt\u2019s representation and the average representation of five modified prompts) across 20 sampled prompts. Modifications include changes in task familiarity, instruction difficulty, and phrasing. The results indicate that phrasing modifications align more closely with the instruction-following dimension, suggesting that how prompts are phrased plays a crucial role in determining instruction adherence.\nattention mechanisms (Zhang et al., 2023) and applying fine-tuning strategies (He et al., 2024; Sun et al., 2024). In contrast to prior work that primarily focuses on evaluating or enhancing instructionfollowing, our study aims to understand why LLMs sometimes fail to follow instructions by analyzing their internal representations.\nLinear Probing and Representation engineering on LLMs Linear probes have been widely used for interpreting and analyzing the representations of neural networks (Alain & Bengio, 2016) and language models (Belinkov, 2022; Elazar et al., 2021). Specifically, probing for the trustworthiness of LLMs has been an active area of research (Azaria & Mitchell, 2023; Marks & Tegmark, 2023; MacDiarmid et al., 2024; Li et al., 2024a; Burns et al., 2022; Zou et al., 2023; Rimsky et al., 2023; Li et al., 2022; Nanda et al., 2023; Subramani et al., 2022; Tigges et al., 2023; Todd et al., 2023; Farquhar et al., 2024; Ahdritz et al., 2024; Duan et al., 2024). These probing methods are closely related to representation engineering and editing techniques aimed at modifying model knowledge and behavior (Zou et al., 2023; Rimsky et al., 2023; Li et al., 2024a; Park et al., 2023; Chen & Yang, 2023; Luo et al., 2024; Turner et al., 2023). Our work is distinct from these previous efforts, which primarily focus on representations related to truthfulness and reducing hallucinations. In contrast, our study centers on representations related to instruction-following, highlighting the importance of understanding how models internally handle instructions.\n# 6 DISCUSSION AND CONCLUSION\n6.1 LLMS INTERNALLY RECOGNIZE WHETHER THEY WILL FOLLOW INSTRUCTIONS\nOur findings suggest that LLMs may possess an inherent ability to predict whether they will successfully follow instructions, even before the generation process begins. This capability is supported by several key observations: LLMs generalize well across tasks but struggle with different instruction types We find that while LLMs can generalize across different tasks, they struggle with generalization across different instruction types. This suggests that distinct instruction categories may have unique geometries within the models\u2019 internal representation space, making it more challenging for the model to generalize across them. LLMs can predict instruction success from the first token We observe that the model\u2019s inter-\nOur findings suggest that LLMs may possess an inherent ability to predict whether they will suc cessfully follow instructions, even before the generation process begins. This capability is supporte by several key observations:\nLLMs can predict instruction success from the first token We observe that the model\u2019s internal representations are separable from the very first token, which corresponds to the embedding of the input prompt. This indicates that the likelihood of instruction-following success can be determined early in the process, before the model generates any responses. This highlights the critical\nrole of how the input prompt is encoded and the importance of input representations in predicting instruction-following outcomes. Representation engineering increases instruction-following success We further validate the significance of the identified instruction-following dimension by adjusting the model\u2019s representations. By moving failure cases into the success class along this dimension and comparing the results to random adjustments, we observe a significant increase in the success rate while keeping the task quality. This demonstrates that the identified dimension is both meaningful and can be used practically. The instruction-following dimension is closely tied to prompt phrasing Our findings, in Figure 4, reveal that the instruction-following dimension is most closely associated with the phrasing of prompts, rather than the inherent difficulty of the task or the specific details of the instructions. This suggests that how instructions are phrased plays a crucial role in whether LLMs will follow them and is consistent with our finding on the separability of representations from the early token.\nOur findings highlight the role of representation of the input prompt in determining instructionfollowing success in LLMs. We discover that the instruction-following dimension identified in our analysis is sensitive to changes in how the input prompt is phrased. This sensitivity explains several behaviors of LLMs: Why LLMs fail in following instructions LLMs may fail to follow even simple, clear instructions because the encoding of the input prompt within the models\u2019 internal representation space can be easily disrupted. Our findings suggest that small variations in how a prompt is phrased can result in significant differences in how the model processes the instruction, leading to failures in adherence. This issue arises not from ambiguity in the instruction itself, but from the LLM\u2019s sensitivity to the exact structure and phrasing of the input, which influences how the instruction is embedded and processed internally. As a result, the model might not consistently follow instructions, even when they are clear and familiar. Why Prompt Engineering (PE) works PE operates by slightly altering the phrasing of a prompt, which in turn changes how the input is encoded within the model. This subtle shift in encoding can move a representation from a failure class to a success class in terms of instruction-following within the input embedding space. Our work with representation engineering achieves a similar outcome, but instead of modifying the input text, we make adjustments directly in the representation space. Both approaches influence the model\u2019s internal states, highlighting the importance of the input encoding process. Our observations align with prior research showing LLM sensitivity to prompt formatting (Lu et al., 2023; Sclar et al., 2023; Gonen et al., 2022). Semantic sensitivity of LLM input embedding space The fact that instruction-following success or failure can be altered by slight prompt rephrasing shows that the LLM\u2019s input embedding space is semantically sensitive. This sensitivity suggests that the model\u2019s internal representation of prompts is brittle, making LLMs vulnerable to small changes in how an input is framed or phrased. This fragility, likely driven by the model\u2019s large size and the complexity of its training dynamics, creates challenges in ensuring robust instruction adherence. Given this sensitivity, future efforts should focus on making LLMs\u2019 input embedding space more robust and reliable. One potential approach is to fine-tune models with an explicit focus on stabilizing instruction-following by utilizing the identified instruction-following dimension. Our findings highlight the crucial role of prompt encoding in instruction-following success for LLMs. The sensitivity of the input embedding space to slight changes in phrasing explains why LLMs may fail to follow even clear instructions and why prompt engineering is effective. By adjusting the representations directly, as we did with representation engineering, we show that it is possible to significantly improve instruction adherence. Going forward, improving the robustness of LLMs\u2019 input embeddings through training can make models more reliable and consistent in following instructions across a variety of tasks. This is crucial for building trustworthy AI systems, especially in real-world applications where accuracy and reliability are essential.\nOur analysis was primarily focused on a specific set of tasks and models. Although our current results are consistent across the models we studied, future work could extend these findings by evaluating additional models to determine whether the identified instruction-following dimension generalizes across different LLM architectures. Additionally, expanding the dataset to include a wider variety of instruction-following cases could enrich the analysis and improve the generalizability of our findings. We focused our investigation on simple modeling approaches to identify an instruction-following dimension and evaluate its practical significance. Future work could include additional methods train linear probes, particularly in handling domain shifts. Similarly, better approaches to representation engineering (Zou et al., 2023) could further improve the success rate of instruction-following modifications. Finally, unambiguously interpreting the meaning of the instruction-following dimension remains an open question. We considered three hypotheses and found that phrasing modification was most closely related to the dimension associated with instruction-following using a perturbation-based approach. Additional investigations to develop systematic approaches to interpret the dimension could add to a deeper understanding of its meaning and implications.\n# REFERENCES\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable from the unknowable with language models. arXiv preprint arXiv:2402.03563, 2024. Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016. Amos Azaria and Tom Mitchell. The internal state of an llm knows when it\u2019s lying. arXiv preprint arXiv:2304.13734, 2023. Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207\u2013219, 2022. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms. arXiv preprint arXiv:2310.20150, 2023. Hanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of llm\u2019s hidden states. arXiv preprint arXiv:2402.09733, 2024. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160\u2013175, 2021. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625\u2013630, 2024. Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022. Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, and Yanghua Xiao. From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. arXiv preprint arXiv:2404.15846, 2024. Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity\u2014a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1): S63\u2013S63, 1977.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, and Chanjun Park. Evalverse: Unified and accessible library for large language model evaluation. arXiv preprint arXiv:2404.00943, 2024. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022. Kenneth Li, Oam Patel, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024a. Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024b. Sheng Lu, Hendrik Schuff, and Iryna Gurevych. How are prompts different in terms of sensitivity? arXiv preprint arXiv:2311.07230, 2023. Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, and Ren\u00b4e Vidal. Pace: Parsimonious concept engineering for large language models. arXiv preprint arXiv:2406.04331, 2024. Monte MacDiarmid, Timothy Maxwell, Nicholas Schiefer, Jesse Mu, Jared Kaplan, David Duvenaud, Sam Bowman, Alex Tamkin, Ethan Perez, Mrinank Sharma, Carson Denison, and Evan Hubinger. Simple probes can catch sleeper agents, 2024. URL https://www.anthropic. com/news/probes-catch-sleeper-agents. Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023. Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658, 2023. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\u2019 sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023. Nishant Subramani, Nivedita Suresh, and Matthew E Peters. Extracting latent steering vectors from pretrained language models. arXiv preprint arXiv:2205.05124, 2022. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. Conifer: Improving complex constrained instruction-following ability of large language models. arXiv preprint arXiv:2404.02823, 2024.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, and Chanjun Park. Evalverse: Unified and accessible library for large language model evaluation. arXiv preprint arXiv:2404.00943, 2024. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022. Kenneth Li, Oam Patel, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024a. Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024b. Sheng Lu, Hendrik Schuff, and Iryna Gurevych. How are prompts different in terms of sensitivity? arXiv preprint arXiv:2311.07230, 2023. Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, and Ren\u00b4e Vidal. Pace: Parsimonious concept engineering for large language models. arXiv preprint arXiv:2406.04331, 2024. Monte MacDiarmid, Timothy Maxwell, Nicholas Schiefer, Jesse Mu, Jared Kaplan, David Duvenaud, Sam Bowman, Alex Tamkin, Ethan Perez, Mrinank Sharma, Carson Denison, and Evan Hubinger. Simple probes can catch sleeper agents, 2024. URL https://www.anthropic. com/news/probes-catch-sleeper-agents. Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023. Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. arXiv preprint arXiv:2309.00941, 2023. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658, 2023. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\u2019 sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023. Nishant Subramani, Nivedita Suresh, and Matthew E Peters. Extracting latent steering vectors from pretrained language models. arXiv preprint arXiv:2205.05124, 2022. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. Conifer: Improving complex constrained instruction-following ability of large language models. arXiv preprint arXiv:2404.02823, 2024.\nCurt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representations of sentiment in large language models. arXiv preprint arXiv:2310.15154, 2023. Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards conversational diagnostic ai. arXiv preprint arXiv:2401.05654, 2024. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, 2023. Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, and Kam-Fai Wong. Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms. arXiv preprint arXiv:2305.11792, 2023. Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. Fofo: A benchmark to evaluate llms\u2019 format-following capability. arXiv preprint arXiv:2402.18667, 2024. Jianhao Yan, Yun Luo, and Yue Zhang. Refutebench: Evaluating refuting instruction-following for large language models. arXiv preprint arXiv:2402.13463, 2024. Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention steering for llms. arXiv preprint arXiv:2311.02262, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023.\n# A APPENDIX\n# A.1 EXAMPLES OF IFEVAL-SIMPLE DATASET\nThe IFEval-simple dataset is created to focus specifically on instruction-following, removing the confounding influence of varying tasks present in the IFEval dataset (Zhou et al., 2023). In this modified version, we select 5 instruction types commonly used in real-world AI applications, such as including or excluding keywords, generating responses with placeholders, and ensuring specific phrases are present in the generated text. These instructions are paired with the same set of 100 tasks to help isolate the instruction-following dimension. By using the same set of tasks across all instruction types, we ensure that any differences in model behavior are attributed to instructionfollowing rather than task-specific features. This allows us to more effectively probe the model\u2019s internal representations and evaluate how well it can follow instructions across various scenarios. Table 5 presents examples from the IFEval-simple dataset, such as tasks like writing a resume or creating a joke about programmers. The instructions assigned to each task vary, requiring the model to follow specific guidelines such as including or excluding certain keywords, ensuring word usage meets a specific frequency, and adhering to formatting rules. The keywords that must be included or excluded differ based on the task. For instance, in the resume task, keywords might include \u201cresume\u201d, \u201csoftware\u201d, or \u201cengineer\u201d, whereas in the joke task, the focus may shift to terms like \u201csyntax\u201d or \u201ccode\u201d. These varied instructions introduce diverse challenges for the model in instruction-following.\nType\nExample\nTask\nWrite a resume for a software engineer with 5+ years of experience in the Bay Area, CA.\nInstruction\nkeywords:existence\nMake sure to include the keywords: \u201cskills\u201d, \u201ctechnology\u201d, \u201ccareer\u201d.\nkeywords:forbidden\nDo not include the following keywords: resume, software, engineer, experience.\nkeywords:frequency\nMake sure to use the word \u201cqualifications\u201d at least 2 times.\nstartend:end checker\nYour resume must end with the exact phrase \u201cLooking forward to contributing to innovative projects.\u201d\ndetectable content:number placeholders\nMake sure to include at least 5 placeholders represented by square brackets, such as [name].\nTask\nWrite a joke about programmers.\nInstruction\nkeywords:existence\nMake sure to include the keywords: \u201chumor\u201d, \u201ccode\u201d, \u201clife\u201d.\nkeywords:forbidden\nDo not include the following keywords: joke, programmers.\nkeywords:frequency\nMake sure to use the word \u201csyntax\u201d at least 3 times.\nstartend:end checker\nYour programmer joke must end with the exact phrase \u201cAnd that\u2019s the real bug in the code of life.\u201d\ndetectable content:number placeholders\nMake sure to include at least 3 placeholders represented by square brackets, such as [name].\nTable 5: Examples from the IFEval-simple dataset. This table shows two tasks: writing a resume and crafting a joke about programmers. Each task is paired with multiple instruction types, such as including/excluding keywords, ensuring word frequency, and adhering to specific content formatting rules. The uniform set of tasks across different instruction types helps isolate the instructionfollowing dimension by removing task-specific variations.\nIndex\nTask\n1\nWrite a story about the importance of understanding the truths that are not obvious.\n2\nWrite a serious riddle about trips and stitches in a poem style.\n3\nWrite a rubric for teenagers on how to review a book.\n4\nWrite a persuasive email to a teenager who lives in Aberdeen, Scotland.\n5\nWrite a resume for a software engineer with 5+ years of experience in the Bay Area, CA.\n6\nWrite a song about regrets in the style of Taylor Swift.\n7\nWrite an essay about Alvin and the Chipmunks.\n8\nThe Legend of the Sword and the Fairy is a movie in which Wan Wan is a villain. Write a story\nabout Wan Wan\u2019s character.\n9\nWrite a story about a family that goes camping in the woods.\n10\nWrite an obviously fake news article saying that aliens have invaded earth. Make it funny.\n11\nWrite a song about the benefits of eating your vegetables.\n12\nWrite a startup pitch for \u201dWard and Guerre\u201d.\n13\nIs Seoul a good place to live?\n14\nWrite a letter to a friend asking them to go and vote.\n15\nWrite a resume for a fresh high school graduate who is seeking their first job.\n16\nIs praying for someone\u2019s health a good idea?\n17\nWhat\u2019s the difference between a 2-stroke and a 4-stroke motor?\n18\nExplain to a group of elementary school students why we have seasons.\n19\nCan you re-create a story from a fictional newspaper with the title: \u201dA man mysteriously died in his\nhouse, and police are investigating\u201d?\n20\nCome up with a proposal for a new research project on how to improve the quality of life for people\nwith disabilities.\n21\nWrite a blog post about the benefits of meditation for busy professionals.\n22\nCreate a recipe for a vegan gluten-free chocolate cake.\n23\nDraft a comprehensive guide on how to start a podcast.\n24\nDevelop a character sketch for a villain in a fantasy novel.\n25\nCompose a haiku about a sunset over the ocean.\n26\nSummarize the plot of the film \u201dInception\u201d.\n27\nExplain the theory of relativity in simple terms.\n28\nWrite a review of the latest iPhone model.\n29\nDescribe the lifecycle of a butterfly.\n30\nPropose a business plan for a sustainable fashion brand.\n31\nOutline the steps for training a puppy.\n32\nDiscuss the impact of social media on teenage mental health.\n33\nDraft a speech for a climate change conference.\n34\nWrite a joke about programmers.\n35\nExplain how to change a car tire.\n36\nDevelop a fitness routine for beginners.\n37\nCompose a sonnet about the city of Venice.\n38\nWrite a user manual for a smartwatch.\n39\nDescribe a typical day in ancient Rome.\n40\nProvide advice on how to improve public speaking skills.\n41\nDiscuss the effects of global warming on polar bears.\n42\nDraft a letter of recommendation for a student.\n43\nSummarize the story of \u201dThe Great Gatsby\u201d.\n44\nExplain the process of photosynthesis.\n45\nWrite a critique of a famous painting.\n46\nDevelop a marketing strategy for a new video game.\n47\nCompose a limerick about a mischievous cat.\n48\nDescribe the benefits of yoga for athletes.\n49\nWrite instructions for assembling a desk.\n50\nDiscuss the history of the internet.\nTable 6: Sample of 50 tasks from the IFEval-simple dataset. This table provides a subset of 50 tasks from the IFEval-simple dataset, which includes a total of 100 tasks designed to evaluate instruction-following performance.\nTable 6: Sample of 50 tasks from the IFEval-simple dataset. This table provides a subset of 50 tasks from the IFEval-simple dataset, which includes a total of 100 tasks designed to evaluate instruction-following performance.\n\nFigure 5: RE example An illustrative example of modified responses. In this case, the task was to write a resume with the instruction to include three specific keywords. The original response only included one keyword, whereas the modified response, guided by the instruction-following direction, successfully incorporated all three keywords, demonstrating the effectiveness of RE in enhancing instruction adherence.\nA.3 INSTRUCTION GENERALIZATION ON EXPANDED EXPERIMENT\n# A.3 INSTRUCTION GENERALIZATION ON EXPANDED EXPERIMENT\nIn the main paper, we observed that models struggle to generalize across unseen instruction types, with AUC scores ranging from 0.50 to 0.55, which is close to random chance, as shown in Table 1 and Table 3 of the main paper. One hypothesis for this poor generalization is the limited number of instruction types used in the initial experiments, where the linear probe was trained on just 4 instruction types. To further investigate this, we expanded the dataset to include 23 instruction types across 8 categories, each paired with 20 tasks. Unlike the IFEval dataset, which contains 25 instruction types across 9 categories, we omitted the \u2018combination\u2019 category, which includes the \u2018combination: Repeat Prompt\u2019 and \u2018combination: Two Responses\u2019 instruction types. This is because combined instructions can lead to conflicting signals in our analysis, where success in one instruction type but failure in another may produce mixed representations. By focusing on single instruction types, we aim to more clearly capture the representations associated with instruction-following success and failure. In comparison to IFEval-simple, which features 5 instruction types across 3 categories, this expanded dataset includes 23 instruction types across 8 categories, helping to prevent overfitting to a small number of instructions. The results from this expanded experiment, shown in Table 7 for different layers and Table 8 for different tokens, reveal that despite increasing the number of instruction types, the models still demonstrate limited generalization across unseen instruction types. The AUC scores remain close to chance levels, similar to the initial experiments. As shown in Table 7 and 8, the results indicate\nthat adding more instruction types does not significantly improve instruction generalization. These findings reinforce the conclusion that models struggle to generalize instruction-following across different instruction types. This suggests that a \u201cglobal\u201d instruction-following dimension, applicable across diverse instruction types, may not exist.\nModels\nLLaMA-2-chat-7B\nLLaMA-2-chat-13B\nMistral-7B-inst-v0.3\nPhi-3-mini-128k\nInstructions\nEarly lyr\nMiddle lyr\nLast lyr\nEarly lyr\nMiddle lyr\nLast lyr\nEarly lyr\nMiddle lyr\nLast lyr\nEarly lyr\nMiddle lyr\nLast lyr\nstartend\n0.70\n0.61\n0.57\n0.47\n0.54\n0.52\n0.56\n0.62\n0.59\n0.60\n0.46\n0.48\nkeywords\n0.39\n0.49\n0.48\n0.53\n0.46\n0.45\n0.42\n0.43\n0.45\n0.59\n0.48\n0.47\ndetectable format\n0.52\n0.45\n0.42\n0.50\n0.47\n0.47\n0.49\n0.45\n0.41\n0.81\n0.79\n0.70\nlength constraints\n0.40\n0.30\n0.33\n0.60\n0.50\n0.52\n0.44\n0.57\n0.56\n0.69\n0.52\n0.52\npunctuation\n-\n-\n-\n0.47\n0.37\n0.35\n0.94\n0.95\n0.92\n-\n-\n-\nchange case\n0.59\n0.40\n0.35\n0.28\n0.26\n0.29\n0.61\n0.43\n0.39\n0.40\n0.34\n0.29\ndetectable content\n0.65\n0.62\n0.61\n0.59\n0.53\n0.57\n0.49\n0.37\n0.34\n0.13\n0.11\n0.10\nlanguage\n0.38\n0.49\n0.47\n0.12\n0.13\n0.17\n0.41\n0.60\n0.62\n0.78\n0.77\n0.80\nAVERAGE\n0.52\n0.48\n0.46\n0.44\n0.41\n0.42\n0.54\n0.55\n0.54\n0.57\n0.50\n0.48\nTable 7: Instruction-type generalization on IFEval-simple-expanded across layers AUC scores across different models and instruction types from IFEval-simple-expanded. The \u2018punctuation\u2019 instruction type is marked with \u2018-\u2019 due to an insufficient number of data points caused by a low success rate, making it impossible to compute reliable AUC scores.\nLLaMa2-chat-7b\nLLaMa2-chat-13b\nMistral-7B-inst-v0.3\nPhi-3-mini-128k\ninstructions\nEarly token\nMiddle token\nLast token\nEarly token\nMiddle token\nLast token\nEarly token\nMiddle token\nLast token\nEarly token\nMiddle token\nLast token\nstartend\n0.70\n0.42\n0.29\n0.47\n0.53\n0.55\n0.56\n0.56\n0.60\n0.60\n0.70\n0.64\nkeywords\n0.39\n0.69\n0.66\n0.53\n0.32\n0.40\n0.42\n0.60\n0.50\n0.59\n0.37\n0.47\ndetectable format\n0.52\n0.45\n0.49\n0.50\n0.58\n0.52\n0.49\n0.60\n0.57\n0.81\n0.56\n0.62\nlength constraints\n0.40\n0.57\n0.55\n0.60\n0.61\n0.56\n0.44\n0.55\n0.56\n0.69\n0.44\n0.49\npunctuation\n-\n-\n-\n0.47\n0.47\n0.49\n0.94\n0.65\n0.43\n-\n-\n-\nchange case\n0.59\n0.52\n0.51\n0.28\n0.58\n0.45\n0.61\n0.47\n0.48\n0.40\n0.45\n0.37\ndetectable content\n0.65\n0.53\n0.56\n0.59\n0.47\n0.55\n0.49\n0.54\n0.45\n0.13\n0.38\n0.33\nlanguage\n0.38\n0.46\n0.36\n0.12\n0.56\n0.51\n0.41\n0.59\n0.75\n0.78\n0.40\n0.46\nAVERAGE\n0.52\n0.52\n0.49\n0.44\n0.51\n0.50\n0.54\n0.57\n0.54\n0.57\n0.47\n0.48\nTable 8: Instruction-type generalization on IFEval-simple-expanded across tokens AUC scores across early, middle, and late token representations, showing instruction-type generalization performance on IFEval-simple-expanded. The results indicate that despite expanding the number of instruction types, models continue to struggle with unseen instruction types, with scores close to chance levels across different token positions. The \u2018punctuation\u2019 instruction type is marked with \u2018-\u2019 due to an insufficient number of data points caused by a low success rate, making it impossible to compute reliable AUC scores.\n# A.4 SUCCESS RATE\nThis section presents the success rate for instruction-following, which measures the accuracy of responses adhering to instructions. The success rates for the IFEval dataset(Zhou et al., 2023) are shown in Table 9, for our IFEval-simple dataset in Table 10, and for IFEval-simple-extended in Table 11, which is used in Section A.3 of the Appendix. The IFEval dataset consists of 25 instruction types categorized under 9 broader categories, with approximately 20 tasks per instruction type. For details on IFEval and IFEval-simple, please refer to Section 2.1 of the main paper. We use the success rate (loose) metric from Zhou et al. (2023). To ensure consistent results without randomness in decoding, we used greedy decoding without sampling when calculating the success rate.\nIFEval inst\nLLaMa2-chat-7b\nLLaMa2-chat-13b\nMistral-7B-inst-v0.3\nPhi-3-mini-128k\nchange case\n0.48\n0.52\n0.62\n0.29\ndetectable content\n0.85\n0.89\n0.79\n0.89\ndetectable format\n0.66\n0.68\n0.78\n0.67\nkeywords\n0.68\n0.71\n0.73\n0.75\nlanguage\n0.68\n0.58\n0.87\n0.97\nlength constraints\n0.46\n0.48\n0.55\n0.41\npunctuation\n0.24\n0.14\n0.17\n0.11\nstartend\n0.67\n0.58\n0.63\n0.22\ncombination\n0.24\n0.22\n0.17\n0.22\nTable 9: Success rate on the IFEvalZhou et al. (2023) across 9 categories of instruction types\nTable 9: Success rate on the IFEvalZhou et al. (2023) across 9 categories of instruction types\nIFEval inst\nLLaMa2-chat-7b\nLLaMa2-chat-13b\nMistral-7B-inst-v0.3\nPhi-3-mini-128k\nkeywords:existence\n0.79412\n0.87255\n0.86275\n0.94118\nkeywords:forbidden words\n0.18627\n0.28431\n0.36275\n0.32353\nkeywords:frequency\n0.86275\n0.92157\n0.91176\n1.0000\nstartend:end checker\n0.23529\n0.16667\n0.27451\n0.13725\ndetectable content:number placeholders\n0.76471\n0.80392\n0.5098\n0.87255\nIFEval inst\nLLaMa2-chat-7b\nLLaMa2-chat-13b\nMistral-7B-inst-v0.3\nPhi-3-mini-128k\nchange case\n0.53\n0.70\n0.46\n0.31\ndetectable content\n0.65\n0.90\n0.75\n0.94\ndetectable format\n0.67\n0.72\n0.72\n0.64\nkeywords\n0.80\n0.91\n0.90\n0.96\nlanguage\n0.40\n0.10\n0.94\n0.83\nlength constraints\n0.53\n0.56\n0.69\n0.40\npunctuation\n0.15\n0.25\n0.06\n0.00\nstartend\n0.98\n0.93\n0.69\n0.28\nTable 11: Success rate on IFEval-simple-extended across 8 categories of instruction types (exclud ing the \u2018combination\u2019 category)\nA.5 MORE RESULT ON REPRESENTATION ENGINEERING\n# A.5 MORE RESULT ON REPRESENTATION ENGINEERING\nIn the main paper, we presented the average performance of Representation Engineering (RE) across all instruction types and models. Here, we provide more detailed insights through figures, highlighting the variation in RE effectiveness across different types of instructions. Figure 3 shows the transition metrics for high-quality responses, including F2T (False to True) and T2T (True to True). These results demonstrate the robustness of the instruction-following dimension in improving success rates. The proportion of F2T and T2T transitions is consistently higher when using the instruction-following dimension compared to random directions across various instruction types. This indicates that RE not only enhances success rates but also effectively converts failed responses into successful ones, while maintaining overall task quality.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f765/f7652a09-817f-4aa0-80fd-77b427aa613c.png\" style=\"width: 50%;\"></div>\nFigure 6: Transition metric for Representation Engineering on the last layer of four models Success rate (SR) only on high quality responses (scoring above 7 by GPT4) in task execution. The Success conversion ratio (SCR) indicates the proportion of originally failed responses that became successful after modification, while Success preservation ratio (SPR) reflects the proportion of originally successful responses that remained successful.\n# A.6 PCA ACROSS ALL FIVE INSTRUCTION TYPES\nIn this section, we extend the PCA analysis to include all five instruction types used in our experiments. This analysis contrasts with the PCA plot in Figure 2 of the main paper, where we focus on three instruction types within the keyword category. In the main paper, the PCA plot show a clear\ntendency towards separability of the instruction-following dimension across tasks, even though the data points were not perfectly linearly separable. However, in this extended analysis with all five instruction types in Figure 7, the representations are less linearly separable in the 2-dimensional PCA plot. This highlights that different instruction types (or categories) may exhibit distinct geometries in the representation space. The lack of clear separability further supports our findings in the main paper that linear probes trained on one set of instruction types struggle to generalize to unseen instruction types in Section 2.3. This suggests that there is no \u201cglobal\u201d instruction-following dimension that can be applied across different types of instructions, likely due to the varying internal geometries of these categories.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5211/5211bd6f-3b89-42bb-a2f8-934d0f58e00a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ma-2-13b-chat-hf (b) Llama-2-7b-chat-hf (c) Mistral-7B-Inst-v0.3</div>\nFigure 7: PCA plot of representations from four LLMs across all five instruction types. This PCA plot of first-token representations from early layers shows that the inclusion of all five instruction types results in less separability compared to the three instruction types in the main paper in Figure 2. This indicates that different instruction types possess distinct geometries, supporting the conclusion that linear probes do not generalize well to unseen instruction types.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of instruction-following in large language models (LLMs), highlighting the challenges faced in adhering to user-provided constraints and guidelines. Previous methods have struggled with clear and simple instructions, necessitating a breakthrough to enhance instruction-following behavior and prevent undesirable outputs.",
        "problem": {
            "definition": "The problem is the inability of LLMs to consistently follow instructions, even when they are clear and familiar, leading to failures in generating appropriate responses.",
            "key obstacle": "The core obstacle is the lack of understanding of how LLMs' internal representations relate to instruction adherence, which hinders the effectiveness of existing methods."
        },
        "idea": {
            "intuition": "The idea emerged from the observation that certain dimensions in the input embedding space of LLMs are linked to successful instruction-following.",
            "opinion": "The proposed idea involves modifying representations along a specific dimension identified through linear probing to improve instruction-following success rates.",
            "innovation": "This method differs from existing approaches by focusing on a newly discovered instruction-following dimension, which enhances adherence without compromising response quality."
        },
        "method": {
            "method name": "Instruction-Following Dimension Modification",
            "method abbreviation": "IFDM",
            "method definition": "IFDM is a technique that modifies the internal representations of LLMs along a specific dimension associated with instruction-following to enhance adherence to user instructions.",
            "method description": "The core of the method involves adjusting representations within the input embedding space to improve instruction adherence.",
            "method steps": [
                "Identify the instruction-following dimension using linear probing.",
                "Apply representation engineering to modify embeddings along this dimension.",
                "Evaluate the impact on instruction-following success rates."
            ],
            "principle": "The effectiveness of this method is rooted in the understanding that the identified dimension significantly influences the model's behavior regarding instruction adherence."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized the IFEval dataset and a modified version called IFEval-simple, focusing on five common instruction types across a consistent set of tasks.",
            "evaluation method": "Performance was assessed using linear probes to measure the success rates of instruction-following before and after applying representation modifications."
        },
        "conclusion": "The experiments demonstrated that modifying representations along the identified instruction-following dimension significantly improves adherence to instructions, providing a deeper understanding of LLM behavior and paving the way for more reliable AI agents.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to enhance instruction-following success rates without degrading response quality, offering a practical solution to a significant problem.",
            "limitation": "A limitation of the method is its reliance on the specific instruction-following dimension, which may not generalize well across all instruction types.",
            "future work": "Future research should explore ways to improve the robustness of LLMs' input embeddings and investigate additional instruction-following dimensions to enhance generalization across diverse tasks."
        },
        "other info": {
            "data release": "The IFEval-simple dataset and code will be released on GitHub upon publication.",
            "contributions": {
                "contribution1": "Identified a specific dimension within the input embeddings space of LLMs closely linked to instruction-following.",
                "contribution2": "Demonstrated that modifying representations along this dimension effectively improves instruction-following success rates.",
                "contribution3": "Revealed that prompt phrasing significantly influences instruction-following behavior in LLMs."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of instruction-following in large language models (LLMs), highlighting the challenges faced in adhering to user-provided constraints and guidelines."
        },
        {
            "section number": "1.2",
            "key information": "The inability of LLMs to consistently follow instructions leads to failures in generating appropriate responses, emphasizing the relevance of in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Instruction-Following Dimension Modification (IFDM), modifies internal representations of LLMs along a specific dimension associated with instruction-following to enhance adherence."
        },
        {
            "section number": "3.3",
            "key information": "The core of the IFDM method involves adjusting representations within the input embedding space to improve instruction adherence."
        },
        {
            "section number": "4.1",
            "key information": "Prompt phrasing significantly influences instruction-following behavior in LLMs, highlighting the importance of effective prompt design."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the proposed method is its reliance on the specific instruction-following dimension, which may not generalize well across all instruction types."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrated that modifying representations along the identified instruction-following dimension significantly improves adherence to instructions, providing insights for future research."
        }
    ],
    "similarity_score": 0.6979393458240576,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Do LLMs _know_ internally when they follow instructions_.json"
}