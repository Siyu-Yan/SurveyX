{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.09748",
    "title": "Large Language Model-Aware In-Context Learning for Code Generation",
    "abstract": "Large language models (LLMs) have shown impressive in-context learning (ICL) ability in code generation. LLMs take a prompt consisting of requirement-code examples and a new requirement as input, and output new programs without any parameter updates. Existing studies have found that the performance of ICL is highly dominated by the quality of selected examples and thus arises research on example selection: given a new requirement, several examples are retrieved from a candidate pool for ICL. However, existing approaches are mostly based on heuristics. They randomly select examples or only consider the textual similarity of requirements to retrieve, leading to sub-optimal performance. In this paper, we propose a novel learning-based selection approach named LAIL (LLM-Aware In-context Learning) for code generation. Given a candidate example, we exploit LLMs themselves to estimate it by considering the generation probabilities of ground-truth programs given a requirement and the example. We then label candidate examples as positive or negative through the probability feedback. Based on the labeled data, we import a contrastive learning objective to train an effective retriever that acquires the preference of LLMs in code generation. We argue that considering the feedback from LLMs themselves is logical and our approach can select suitable examples for ICL since it is consistent with the fact that LLMs have different prior knowledge and preferences. To evaluate our approach, we apply LAIL to three LLMs and evaluate it on three representative datasets (e.g., MBJP, MBPP, and MBCPP). Extensive experiments demonstrate that LATA outperforms the state-of-theart baselines by 11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in terms of Pass@1, respectively. Human evaluation further verifies the superiority of our approach in",
    "bib_name": "li2023largelanguagemodelawareincontext",
    "md_text": "# Large Language Model-Aware In-Context Learning for Code Generation\nJia Li Key Lab of HCST (PKU), MOE, SCS, Peking University Beijing, China lijiaa@pku.edu.cn Ge Li Key Lab of HCST (PKU), MOE, SCS, Peking University Beijing, China lige@pku.edu.cn\nJia Li\nGe Li\nJia Li \u2642, Huangzhao Zhang Key Lab of HCST (PKU), MOE, SCS, Peking University Beijing, China lijia@stu.pku.edu.cn zhang_hz@pku.edu.cn\n# ABSTRACT\nLarge language models (LLMs) have shown impressive in-context learning (ICL) ability in code generation. LLMs take a prompt consisting of requirement-code examples and a new requirement as input, and output new programs without any parameter updates. Existing studies have found that the performance of ICL is highly dominated by the quality of selected examples and thus arises research on example selection: given a new requirement, several examples are retrieved from a candidate pool for ICL. However, existing approaches are mostly based on heuristics. They randomly select examples or only consider the textual similarity of requirements to retrieve, leading to sub-optimal performance. In this paper, we propose a novel learning-based selection approach named LAIL (LLM-Aware In-context Learning) for code generation. Given a candidate example, we exploit LLMs themselves to estimate it by considering the generation probabilities of ground-truth programs given a requirement and the example. We then label candidate examples as positive or negative through the probability feedback. Based on the labeled data, we import a contrastive learning objective to train an effective retriever that acquires the preference of LLMs in code generation. We argue that considering the feedback from LLMs themselves is logical and our approach can select suitable examples for ICL since it is consistent with the fact that LLMs have different prior knowledge and preferences. To evaluate our approach, we apply LAIL to three LLMs and evaluate it on three representative datasets (e.g., MBJP, MBPP, and MBCPP). Extensive experiments demonstrate that LATA outperforms the state-of-theart baselines by 11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in terms of Pass@1, respectively. Human evaluation further verifies the superiority of our approach in\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, July 2017, Washington, DC, USA \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/XXXXXXX.XXXXXXX\nZhi Jin Key Lab of HCST (PKU), MOE, SCS, Peking University Beijing, China zhijin@pku.edu.cn\n# three aspects. We also find our LAIL has surprising transferability across LLMs and datasets.\nCode generation, in-context-learning, large language model ACM Reference Format: Jia Li, Ge Li, Chongyang Tao, Jia Li \u2642, Huangzhao Zhang, Fang Liu, and Zh Jin. 2018. Large Language Model-Aware In-Context Learning for Code Gen eration. In Proceedings of ACM Conference (Conference\u201917). ACM, New York NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format: Jia Li, Ge Li, Chongyang Tao, Jia Li \u2642, Huangzhao Zhang, Fang Liu, and Zhi Jin. 2018. Large Language Model-Aware In-Context Learning for Code Generation. In Proceedings of ACM Conference (Conference\u201917). ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX\n# 1 INTRODUCTION\nCode generation aims to automatically generate the source code given a natural language requirement [7, 18, 30]. It plays an important role in improving the productivity of software development. With the emergence of large language models (LLMs), in-context learning (ICL) [11] becomes a prevailing paradigm and achieves impressive performance in code generation [16, 31, 32]. Given limited examples as the prompt, ICL imitates the human ability to leverage prior knowledge to generate programs without parameter updates. However, ICL comes along with the robustness problem [33], which is sensitive to the selected examples in prompts, resulting in the performance usually varying from almost random to near state-of-the-art performance. Despite the importance of selecting examples, only a few studies attempt to investigate example selection in code generation [10, 14, 31]. One line of work is to randomly select examples from a candidate pool [14, 26]. The selected examples are usually improper to the test requirement and their semantic distributions vary a lot, resulting in unstable performance. Another line is based on learning-free heuristics [31], which leverages off-the-shelf retrievers such as BM25 [42] to select examples. They calculate the textual similarity between a test requirement and the requirement of examples, then select candidate items with high similarities. Despite the improved performance, these approaches only consider textual matching among requirements and thus are easily biased by exterior lexicon features. Figure 1 reports the top-3 examples by random selection and the BM25 approach for the test requirement \u201cwrite a function to check whether the entered number is greater than the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ab7/2ab7122b-171a-4c4f-a0c2-32aff240e1da.png\" style=\"width: 50%;\"></div>\n\u00d8 Write a function to convert camel case string to  snake case string by using regex. \u00d8 Write a function to calculate volume of a  tetrahedron. \u00d8 Write a Java function to find the sum of fourth  power of first n even natural number.\n# : Exhibition of the selected top-3 examples by random, BM25, and our LAIL approaches.\n<div style=\"text-align: center;\">Figure 1: Exhibition of the selected top-3 examples by random, BM25, and our LAIL approache</div>\nelements of the given array\u201d in MBPP dataset [9]. To accomplish this requirement, LLMs need to perform traversal operations in an array. Randomly selected examples are irrelevant to the test requirement. Although retrieved examples by BM25 have a large amount of textual overlap with the test item labeled by underline, the overlapping words are trivial such as \u201c write a function\u201d. Meanwhile, the operations of non-overlapping words in red color, such as \u201clines are parallel or not\u201d, can provide less information to LLMs. Examples in ICL are supposed to provide information that LLMs require, and then guide them to generate programs, but the existing approaches leave a huge gap. They do not consider the prior knowledge in LLMs and ignore their preferences. Although one may remedy it by inputting more examples in the prompt, it is highly impractical due to the limited input length of LLMs. Therefore, selecting applicable examples for ICL becomes more urgent. In this paper, we propose a novel learning-based approach dubbed LAIL to select in-context examples. Instead of selecting examples via textual similarity, given a candidate example, our approach leverages LLMs themselves (LLM-aware) to measure it by incorporating the prediction probability of ground truth given a requirement and the candidate item. To quantify the probability feedback, we design a new metric and label candidate examples based on their metric scores. We treat examples with higher metric scores as positive examples, meanwhile, label examples equipped with lower scores as negative examples since good examples should be beneficial for LLMs to generate correct programs. Based on labeled data, we train a neural retriever through a contrastive learning objective to acquire the preference of LLMs in code generation. From Figure 1, we can find that although not textually similar to the test requirement, the selected examples of LAIL contain the \u201ctraversal\u201d operation and are related to the array as shown in purple color tokens. We evaluate LAIL on three representative LLMs including ChatGPT [1], GPT-3.5 [2], and CodeGen (16B) [47]. We conduct extensive experiments on three datasets (i.e., MBJP (Java) [8], MBPP (Python) [9], and MBCPP (C++) [8]). We use a widely used evaluation metric Pass@k (k = 1, 3, 5) to measure the performance of different approaches. We obtain some findings from experimental results. \u2776In terms of Pass@1, LAIL significantly outperforms the state-of-the-art (SOTA) baselines by 11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5, respectively. \u2777 We conduct a human evaluation to measure generated programs\n\u00d8 Write a function to check if any list element is  present in the given list.  \u00d8 Write a function to find the cumulative sum of  all the values that are present in the tuple list. \u00d8 Write a function to find the difference between  largest and smallest value in a given array. \n\uff08c\uff09LAIL TOP-3\nin three aspects (e.g. code correctness, quality, and maintainability) and further prove the superiority of LAIL. \u2778Our approach has surprising transferability across LLMs and datasets and brings satisfying improvements to them. \u2779We investigate two plausible choices to estimate candidate examples by LLMs themselves and demonstrate the effectiveness of our design. We summarize our contributions in this paper as follows: \u25cfOur paper investigates example selection for ICL and argues that a good approach should consider what knowledge LLMs themselves require. \u25cfWe propose a novel learning-based approach dubbed LAIL. Given an example, LAIL exploits LLMs themselves to estimate it by calculating the generation probabilities of ground truth given a requirement and the example. We then label candidate examples via their probabilities. Based on labeled data, we import contrastive learning to optimize a retriever. \u25cfWe evaluate our approach on three LLMs and conduct extensive experiments on three datasets. Qualitative and quantitative experiments reveal that LAIL significantly outperforms the state-of-the-art baselines.\n# 2 BACKGROUND\n# 2.1 Large Language Models\nIn this section, we focus on large language models (LLMs) for source code. LLMs are neural networks that aim to learn the statistical patterns in programming languages [6]. LLMs are pre-trained on a large-scale unlabeled code corpus with the next tokens prediction objective. Specifically, given a program with the token sequence \ud835\udc36= {\ud835\udc501,\ud835\udc502, \u22ef,\ud835\udc50\ud835\udc5b}, LLMs are trained to predict the next token based on some previous tokens:\n(1)\n\u2112() = \u2212\u2211 (\u22c3\ufe00\u2212 \u22ef\u2212) where \ud835\udc57is the window length of previous tokens and \u0398 means parameters of the LLM. After being pre-trained, LLMs are adapted to a specific downstream task. At the beginning stage, LLMs are used in a fine-tuning manner, which are continually optimized on specific code generation datasets. With the size of LLMs growing rapidly such as ChatGPT [1] and CodeGen [47], fine-tuning is neither economical nor practical, in contrast, a convenient solution ICL arises.\n# 2.2 In-Context Learning\nIn-context Learning (ICL) refers to an emerging ability of LLMs. Formally, given a set of requirement-code examples\ud835\udc47= {\ud835\udc65\ud835\udc58,\ud835\udc66\ud835\udc58}\ud835\udc5a \ud835\udc58=1, a test requirement \ud835\udc65\ud835\udc61and a LLM with frozen parameters \u0398, ICL defines the generation of a program \ud835\udc66\ud835\udc61as follows:\n(2)\n)\ufe01\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02]\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02\u230a\ufe02)\ufe02 where \u223crepresents decoding strategies such as greedy decoding and nuclear sampling [22] in code generation. The generation procedure is attractive as the parameters of LLMs are not need to be updated when executing a new task, which is efficient and practical. As demonstrated in Formula 2, LLMs learn the task and generate programs depending on the selected examples. The performance of ICL can vary from almost random to near the state-of-the-art approach due to the different qualities of in-context examples. Thus, selecting appropriate examples is a significant research topic. In this paper, we propose a novel learning-based approach to select in-context examples for code generation.\n# 3 METHOD: LAIL\nIn this section, we propose an effective learning-based approach LAIL to select examples for ICL in code generation, as shown in Figure 2. For a candidate example, LAIL exploits LLMs themselves to estimate it by incorporating the prediction probability of the groundtruth program given a requirement and the example, then labels examples as positive and negative according to their probability feedback (Section 3.1). Our approach imports a contrastive learning objective to optimize a neural retriever, resulting in alignment with the preference of LLMs (Section 3.2). In the inference stage, given a test requirement, LAIL selects a set of examples as a prompt and a LLM generates programs with the prompt (Section 3.3).\n# 3.1 Estimate and Label Examples\nDifferent LLMs are equipped with diverse prior knowledge, thus they have disparate preferences for candidate examples in code generation. To mitigate this phenomenon, we argue that a good selection approach should align with the preference of LLMs. Given a candidate example, we exploit LLMs themselves (LLM-aware) to measure it by incorporating the prediction probability of the ground-truth program given a requirement and the candidate item. Based on the probability feedback, we label candidate examples as positive and negative that are then used to train the retriever in Section 3.2. Formally, given a requirement in the candidate pool, we use LLMs themselves to estimate whether each other candidate in the pool is beneficial to generate the ground truth of the requirement, and label each candidate based on its beneficial degree. Following previous studies [19, 31], this paper uses the training set as the candidate pool. The goal of this procedure can be formulated as follows:\n\ud835\udc9f= {( { })} = where \ud835\udc52\ud835\udc56= (\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) is the \ud835\udc56-th examples in the training set. \ud835\udc46\ud835\udc5d \ud835\udc56and \ud835\udc46\ud835\udc5b \ud835\udc56are the positive and negative example set of \ud835\udc52\ud835\udc56, respectively. \ud835\udc41 is the number of examples in the training set \u211b.\n(A) Estimate Examples\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/978f/978f568a-ae49-4c2e-b0ed-e0006b517f7f.png\" style=\"width: 50%;\"></div>\nFigure 2: The overview of our LAIL. LAIL use LLMs themselves to estimate candidate examples and label them as positive and negative (A). Based on the label date, LAIL then trains a retriever to align with the preference of LLMs with a contrastive loss (B). Given a test requirement, the optimized retriever selects several examples as a prompt that is inputted to LLMs for code generation (C).\nModeling on the full space of the training set is quadratic and thus prohibitive. To mitigate this limitation, we introduce a twostage process to estimate examples in the training set. In the first stage, for each example \ud835\udc52\ud835\udc56, we use an off-the-shelf retriever to select a set of examples \ud835\udc46\ud835\udc56= {(\ud835\udc65\ud835\udc56\ud835\udc5e,\ud835\udc66\ud835\udc56\ud835\udc5e)} \ud835\udc61 \ud835\udc5e=1 from \u211bwhere \ud835\udc61\u226a\ud835\udc41. In this paper, we apply BM25 1 [42] to construct \ud835\udc46\ud835\udc56, which measures textual n-gram matching between \ud835\udc65\ud835\udc56and \ud835\udc65\ud835\udc56\ud835\udc5e. According to BM25 scores, the top-\ud835\udc5fexamples with higher BM25 scores constitute the set \ud835\udc46\ud835\udc56. In the second stage, we leverage LLMs themselves to estimate each example in \ud835\udc46\ud835\udc56. Concretely, we feed each example (\ud835\udc65\ud835\udc56\ud835\udc5e,\ud835\udc66\ud835\udc56\ud835\udc5e) in \ud835\udc46\ud835\udc56and \ud835\udc65\ud835\udc56into the LLM, and acquire the prediction probability of ground-truth program \ud835\udc66\ud835\udc56. To quantify LLMs\u2019 probability feedback, we design a metric \u2133which is defined as follows:\n(4)\n1We also use random and semantics-based approaches to filter examples and find that BM25 is more effective and efficient.\n(\u22c3\ufe00 ) = \u2211 = ((\u22c3\ufe00 <)) where \ud835\udc66\ud835\udc56= {\ud835\udc611, \u22ef,\ud835\udc61\ud835\udc4f} and \ud835\udc61\ud835\udc62is the \ud835\udc62-th token in \ud835\udc66\ud835\udc56. The metric can reflect how helpful the example (\ud835\udc65\ud835\udc56\ud835\udc5e,\ud835\udc66\ud835\udc56\ud835\udc5e) is for generating the target program \ud835\udc66\ud835\udc56, which represents the preference of the LLM. We then rank all examples in the set \ud835\udc46\ud835\udc56according to their metric scores \u2133from high to low. The top-\ud835\udc67examples with higher scores are inputted into the positive example set \ud835\udc46\ud835\udc5d \ud835\udc56, meanwhile, the bottom-\ud835\udc63examples constitute the negative example set \ud835\udc46\ud835\udc5b \ud835\udc56since good examples should be beneficial for LLMs to generate correct programs. We apply this two-stage procedure to the entire training set and finally acquire the labeled data \ud835\udc9f. In our labeled data, the more helpful examples are labeled as positive examples and the examples with lower metric scores are treated as negative examples, which can reflect the preference of LLMs to candidate examples given a specific requirement.\n# 3.2 Training Neural Retriever\nAs described in Section 3.1, the labeled data can reflect the preference of LLMs. In this section, we use the labeled data \ud835\udc9fto train a neural retriever, aiming to align with the preference of LLMs. After being trained, given a test requirement, the retriever can select a set of candidate examples as a prompt that is beneficial for LLMs to generate correct programs. To optimize the retriever, we import a contrastive learning objective. The objective targets to draw a test requirement with helpful training examples together and push the given requirement with knowledge-limited examples apart. Formally, for an example \ud835\udc52\ud835\udc56in the labeled data \ud835\udc9f, we randomly select an item \ud835\udc52\ud835\udc5d \ud835\udc56from \ud835\udc46\ud835\udc5d \ud835\udc56as its corresponding positive example. Meanwhile, we randomly pick a training example \ud835\udc52\ud835\udc5b \ud835\udc56from the set \ud835\udc46\ud835\udc5b \ud835\udc56and select \u210eexamples from the set \u210b(e.g., \u210b= \u211b\u2216\ud835\udc46\ud835\udc56) as its negative example set N\ud835\udc56. Next, We apply GraphCodeBERT [21] to encode their requirements and acquire corresponding representations. Then, we model the relations of these requirements with contrastive loss. Following SimCLR [15], the learning objective \u2112is formulated as:\n(6)\n\u2211 (\ufe00\u230b\ufe00\u2208 where \ud835\udf0fis a temperature parameter. \ud835\udc38(\ufe00\ud835\udc36\ud835\udc3f\ud835\udc46\u230b\ufe00means the entire representation of a requirement. s(\u00b7) represents the cosine similarity of two vectors. Based on the objective, our retriever learns the preference of LLMs, which will be effective to select helpful examples from the training set given a test requirement, resulting in promoting LLMs generating more correct programs.\n# 3.3 Inference\nDuring inference, instead of using heuristical approaches, we apply LAIL to select limited examples from the training set, which can retrieve examples with high metric scores given a test requirement and thus are helpful for LLMs in code generation. Specifically, we first feed requirements in the training set \u211bto our trained retriever\nrespectively and acquire their representational vectors {\ud835\udc38\ud835\udc56 \ud835\udc36\ud835\udc3f\ud835\udc46} \ud835\udc41 \ud835\udc56=1. Given a test requirement \ud835\udc65\ud835\udc61, we obtain its representation \ud835\udc38\ud835\udc61 \ud835\udc36\ud835\udc3f\ud835\udc46by the retriever. Next, we match \ud835\udc38\ud835\udc61 \ud835\udc36\ud835\udc3f\ud835\udc46and \ud835\udc38\ud835\udc56 \ud835\udc36\ud835\udc3f\ud835\udc46and thus lead to \ud835\udc41 pairs of representations {(\ud835\udc38\ud835\udc61 \ud835\udc36\ud835\udc3f\ud835\udc46, \ud835\udc38\ud835\udc56 \ud835\udc36\ud835\udc3f\ud835\udc46)} \ud835\udc41 \ud835\udc56=1. We calculate their cosine similarities {\ud835\udc50\ud835\udc56}\ud835\udc41 \ud835\udc56=1 of all pairs and rank candidate examples according to similarity scores from high to low. Then, the top-\ud835\udc5f examples are selected for ICL. We concatenate the top-\ud835\udc5fexamples as a prompt {\ud835\udc521, \u22ef,\ud835\udc52\ud835\udc56, \u22ef,\ud835\udc52\ud835\udc5f} where their similarity scores gradually decrease (e.g., \ud835\udc501 < \ud835\udc50\ud835\udc56< \ud835\udc50\ud835\udc5f). We feed the prompt and the test requirement into LLMs and make LLMs generate programs with nuclear sampling [22] as described in Formula 2. Note that LAIL only needs to encode training examples once. Given a test requirement, LAIL just calculates cosine similarities between it and all candidates. Thus, the efficiency of our approach is acceptable compared to other heuristic approaches.\n# 4 STUDY DESIGN\nTo investigate the effectiveness of our LAIL, we perform a largescale study to answer three research questions. In this section, we describe the details of our study, including datasets, evaluation metrics, baselines, base LLMs, and experimental details.\n# 4.1 Research Questions\nOur study aims to answer the following research questions. RQ1: How does LAIL perform compared to the state-ofthe-art baselines? This RQ aims to verify that LAIL can generate more correct programs than state-of-the-art (SOTA) baselines. We apply three LLMs to evaluate our approach. We compare LAIL to 7 baselines in three code generation datasets and employ unit tests to check the correctness of generated programs. RQ2: Do developers prefer programs generated by LAIL? The ultimate goal of a code generation model is to assist developers in writing programs. In this RQ, we hire 10 developers to manually estimate the programs generated by our LAIL and baselines. We evaluate these programs in three aspects, including correctness, code quality, and maintainability. RQ3: What is the better design choice to estimate examples? In section 3.1, we leverage the prediction probability of ground-truth programs to estimate candidate examples. In this RQ, we explore other design choices to measure examples by LLMs themselves and compare them to our design.\n# 4.2 Datasets\nWe conduct experiments on three code generation datasets, including MBJP [8] in Java, MBPP [9] in Python, and MBCPP [8] in C++. The statistics of these datasets are given in Table 1. MBPP [9] contains 974 Python programming problems constructed by crowd-sourcing. Each example consists of a brief description, a single self-contained function solving the problem specified, and three test cases to evaluate the correctness of the generated programs. The problems range from simple numeric manipulations or tasks that require the basic usage of standard library functions to tasks that demand nontrivial external knowledge. MBJP [8] and MBCPP [8] are drived from MBPP [9]. MBJP and MBCPP contain 966 and 848 crowd-sourced programming problems\nTable 1: Statistics of three datasets on the different split sets.\n<div style=\"text-align: center;\">Table 1: Statistics of three datasets on the different split sets.</div>\nMBJP\nMBPP\nMBCPP\nLanguage\nJava\nPython\nC++\n#Train\n383\n384\n413\n#Dev\n90\n90\n\u2013\n#Test\n493\n500\n435\nAvg. tests per example\n3\n3\n3\nAvg. tokens in requirement\n16.71\n16.50\n17.38\nAvg. tokens in code\n247.79\n92.68\n113.94\nin Java and C++, respectively. Each problem consists of a description, an individual function, and three test cases. These problems cover programming fundamentals, standard library functionality, etc. We follow previous studies [8, 9] to split three datasets into the training set, the valid set, and the test set, respectively. We measure the performance of different ICL approaches on the test set.\n# 4.3 Evaluation Metrics\nFollowing prior code generation studies [8, 14], we leverage Pass@k to evaluate our approaches. Pass@k evaluates the functional correctness of the generated programs by executing test cases. Precisely, given a test requirement, we generate \ud835\udc58programs using the sampling strategy. If any of the generated \ud835\udc58programs passes all test cases, we think the requirement is solved. Finally, the percentage of solved requirements in all test requirements is treated as Pass@k. In this paper, we set \ud835\udc58to 1, 3, and 5. We notice that previous studies [20, 21] also use some matchbased metrics such as BLEU [37] and CodeBLEU [41]. These metrics focus on the textual similarity or syntactic similarity between reference programs and generated programs. However, existing work [24] has shown that match-based metrics can not effectively measure the functionality of programs. In this paper, we follow recent studies [24, 29, 38] use execution-based metrics (e.g. Pass@k).\n# 4.4 Baselines\nThere are a few studies to investigate in-context example selection for code generation such as zero-shot learning [11], random selection [14], and AceCoder [31], where AceCoder [31] is the stateof-the-art (SOTA) baseline. Zero-Shot Learning [11] directly inputs a requirement into LLMs without any examples as the prompt. LLMs generate programs for the given requirement. Random [14] randomly select a few examples from the training set and construct a prompt. Then, LLMs predict the source code based on the prompt. AceCoder [31] uses BM25 [42] to calculate the textual similarities between a test requirement and the requirements of candidates, and retrieve a set of examples with high similarities. In inference, it first generates test cases and then predicts programs. For a fair comparison, we use the same pipeline to LAIL and all baselines. To extensively evaluate the effectiveness of our LAIL, we also transfer some advanced ICL approaches in natural language processing to the source code.\nTOP-k-SBERT [35] leverages Sentence-BERT [40], a representative sentence encode, to encode all requirements in the training set. Given a test requirement, we first encode it and computer semantic similarities between it and the training requirements. Next, we select the top-k similar examples. TOP-k-GraphCodeBERT [21] is a variant of TOP-k-SBERT. It applies GraphCodeBERT to encode requirements and retrieve a few examples from the training set based on semantic similarity. TOP-k-VOTE [43] is a graph-based method [43], to votes examples. It first encodes each example by GraphCodeBERT and each example is as a vertice in the graph. Each vertice connects with its k nearest vertices based on their semantic similarities. Finally, it treats the k nearest examples as a prompt. Uncertainty-Target [17] assumes examples with higher uncertainty have a greater impact on LLMs. It defines uncertainty as the perplexity when LLMs generate ground truths. The approach computes the uncertainty of each candidate and selects k items with high perplexity as a prompt.\n# 4.5 Base Large Language Models\nThis paper focuses on code generation with LLMs. Thus, we select three recently proposed LLMs for code generation as the base models. The details of the base models are shown as follows. ChatGPT [1] is the state-of-the-art language model for code generation. It is trained on a large amount of natural language text and programming data. Then, ChatGPT is continually trained with reinforcement learning and learns to align with human instructions. We leverage OpenAI\u2019s APIs to access ChatGPT (i.e., gpt-3.5-turbo). GPT-3.5 [2] is a powerful large language model and is trained on large of unlabeled corpus. In this paper, we use OpenAI\u2019s APIs to access the latest version with 175 billion parameters (i.e., textdavinci-003). CodeGen [47] is a family of language models for code generation. CodeGen is training with a 635GB code corpus and 1159GB English text data. In this paper, we leverage the largest version with 16 billion parameters (i.e. CodeGen-Multi-16B).\n# 4.6 Implementation Details\nEstimate and Label Examples. We use greedy decoding to generate programs and collect the predicted probabilities of ground-truth programs. Following the previous generation works [31], we set the max generated length to 500 tokens. The number of examples in the set \ud835\udc46\ud835\udc56is 50 for efficiency. Note that the size of \ud835\udc46\ud835\udc56larger the better performance of LLMs and we leave other sizes in our future work. We set \ud835\udc67and \ud835\udc63as 5 respectively and analyze their effects on performance in Section 6.3. Thus, the positive set \ud835\udc46\ud835\udc5d \ud835\udc56contains 5 examples with higher metric scores and the negative set \ud835\udc46\ud835\udc5b \ud835\udc56has 5 items with lower scores. Training Neural Retriever. In this procedure, we set the number of the negative example set N\ud835\udc56as 64. In other words, \u210eis set to 63. For each epoch, we randomly select 63 examples from the set \u210b. We also attempt to set the size of N\ud835\udc56to 32 and 128 in Section 6.3. The learning rate is 5e-5 and the batch size is 32. We train the retriever for about 1 hour on 4 NVIDIA A100. Inference. We treat a large language model as a black-box generator and sample programs from it. The input of LLMs only contains\nTable 2: Evaluation results of our LAIL and baselines on CodeGen at the three code generation datasets. Numbers in bo indicate that the improvement is significant and the percentages in red color mean the relative improvements compared to t SOTA baseline.\nMBJP\nMBPP\nMBCPP\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nZero-Shot Learning [11]\n14.27\n23.69\n27.83\n8.80\n20.60\n25.60\n15.39\n25.97\n30.94\nRandom [14]\n15.74\n24.25\n28.14\n15.20\n25.40\n28.80\n15.70\n28.72\n32.25\nAceCoder [31]\n17.65\n27.18\n30.63\n16.80\n26.40\n29.40\n17.47\n30.11\n33.78\nTOP-k-SBERT [35]\n16.63\n26.77\n29.21\n15.40\n25.00\n29.00\n18.09\n29.43\n33.33\nTOP-k-GraphCodeBERT [21]\n17.44\n24.34\n28.40\n17.40\n25.80\n28.40\n18.16\n30.48\n35.47\nTOP-k-VOTE [43]\n15.42\n21.70\n23.73\n17.40\n26.00\n29.40\n17.01\n30.03\n35.63\nUncertainty-Target [17]\n17.09\n23.06\n27.93\n14.60\n24.20\n28.80\n17.70\n30.12\n34.62\nLAIL\n21.30\n28.49\n32.05\n18.60\n27.80\n30.60\n19.08\n31.36\n37.94\nRelative Improvement\n11.58%\n4.82%\n4.64%\n6.89%\n6.92%\n4.08%\n5.07%\n2.89%\n6.96%\na prompt and a test requirement without any natural language instructions. During the sampling, we use nuclear sampling [22] to decode, where the temperature is 0.8 and the top-p is 0.95. The maximum generated length is 500 tokens. For each test requirement, we generate 5 programs. For a fair comparison, we set the same parameters to generate programs for all baselines and our LAIL.\n# 5 RESULTS AND ANALYSIS\n# RQ1: How does LAIL perform compared to the state-of-theart baselines?\nIn RQ1, we apply our LAIL and baselines to two LLMs and measure the correctness of generated programs. Setup. We compare our LAIL and baselines (Section 4.4) on three code generation datasets (Section 4.2). The evaluation metric is Pass@k as described in Section 4.3. For the metric, higher scores indicate better performance of approaches. Results. Table 2 and Table 3 report the Pass@k (k \u2208[1, 3, 5]) of different approaches on CodeGen and GPT-3.5, respectively. Numbers in bold mean that the improvement is significant compared with baselines, and the percentages in red color represent the relative improvements compared to the state-of-the-art (SOTA) baseline. Analysis. (1) LAIL achieves the best performance among all approaches with significant improvements. In all datasets, LAIL generates more correct programs than baselines. Compared to the SOTA baseline, LAIL outperforms it by 11.58%, 6.89%, and 5.07% on CodeGen, and acquires 4.38%, 2.85%, and 2.74% improvement on GPT-3.5 in Pass@1, respectively. Note that Pass@1 is a very strict metric and is different to be improved. The significant improvements prove the superiority of our LAIL in code generation. (2) Selecting proper examples is critical to the performance of ICL. Compared to random selection, LAIL acquires 8.31%, 30.60%, and 32.86% absolute improvements in Pass@1 on GPT-3.5. AceCoder and other heuristic approaches further improve code generation performance by selecting textually or semantically similar examples. LAIL exploits LLMs themselves to estimate and trains a neural retriever to align with the preference of LLMs, then achieves the best results among all approaches. That demonstrates the importance of examples in ICL and verifies the reasonability of using\nLLMs themselves to label examples. (3) LAIL is effective in LLMs with different sizes and different programming languages. As above described, our approach on CodeGen and GPT-3.5 both achieve the best performance among all approaches. Table 2 and Table 3 also show that LAIL can generate more correct programs on all datasets including MBPP (Python), MBJP (Java), and MBCPP (C++). This reveals LAIL is generalized and can be applied to different LLMs and languages.\nAnswer to RQ1: LAIL achieves the best results among all baselines. In three datasets, LAIL acquires 11.58%, 6.89%, and 5.07% improvements on CodeGen, and achieves 4.38%, 2.85%, and 2.74% improvements on GPT-3.5 at Pass@1. The significant improvements prove our approach can align with the preference of LLMs. Thus, it is able to select suitable examples for ICL and generates more correct programs.\nRQ2: Do developers prefer programs generated by LAIL? The goal of a code generation approach is to assist developers in writing programs. Thus, a good program not only satisfies the requirement but also is easy to read and maintain. In this question, we manually verify generated programs in three aspects. Setup. Following previous work [29], we manually measure programs generated by different approaches in three aspects (e.g. correctness, code quality, maintainability). Correctness measures whether a program satisfies the given requirement, code quality verifies whether a program does not contain bas code smell, and maintainability measures whether the implementation is standardized and easy to read. For each aspect, the score is an integer and ranges from 0 to 2. The higher score, the better code is. Specifically, we randomly select 50 test samples from MBPP, MBJP, and MBCPP, respectively 2. Then, we use LAIL and baselines to generate programs for these examples. Finally, we obtain 400 (50 \u00d7 8) programs for human evaluation. The 400 programs are randomly divided into 5 groups. We hire 10 developers to verify\n<div style=\"text-align: center;\">Table 3: The performance of our LAIL and the existing approaches on GPT-3.5 at the three code generation datasets. Th numbers in red color represent the relative improvements compared to the SOTA baseline.</div>\nMBJP\nMBPP\nMBCPP\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nZero-Shot Learning [11]\n44.83\n53.05\n59.72\n20.00\n27.00\n29.60\n21.85\n37.94\n49.90\nRandom [14]\n47.87\n59.83\n63.69\n43.00\n56.40\n60.80\n50.02\n61.43\n65.28\nAceCoder [31]\n50.91\n61.25\n65.15\n47.40\n61.00\n64.20\n53.25\n63.22\n66.21\nTOP-k-SBERT [35]\n50.30\n60.46\n64.70\n48.00\n61.40\n64.00\n52.87\n63.21\n67.13\nTOP-k-GraphCodeBERT [21]\n50.30\n60.85\n64.50\n49.20\n58.20\n64.20\n52.64\n63.19\n67.28\nTOP-k-VOTE [43]\n50.52\n60.45\n63.49\n47.80\n59.20\n63.40\n51.03\n62.98\n65.51\nUncertainty-Target [17]\n47.67\n59.23\n63.69\n37.80\n51.40\n57.80\n42.87\n52.60\n57.18\nLAIL\n53.14\n62.87\n66.72\n50.60\n62.40\n65.20\n54.71\n65.98\n69.67\nRelative Improvement\n4.38%\n2.64%\n2.41%\n2.85%\n1.63%\n1.53%\n2.74%\n4.37%\n3.55%\nTable 4: The results of human evaluation. The numbers in red represent LAIL\u2019 relative improvements compared to the SOTA baseline. \u201cCR\u201d, \u201cQL\u201d, and \u201cMT\u201d mean correctness, code quality, and maintainability, respectively.\nApproach\nCR\nQL\nMT\nZero-Shot Learning [11]\n0.472\n1.079\n1.183\nRandom [14]\n0.925\n1.416\n1.520\nAceCoder [31]\n1.523\n1.652\n1.647\nTOP-k-SBERT [35]\n1.569\n1.640\n1.665\nTOP-k-GraphCodeBERT [21]\n1.582\n1.738\n1.609\nTOP-k-VOTE [43]\n1.325\n1.624\n1.631\nUncertainty-Target [17]\n0.736\n1.297\n1.324\nLAIL\n1.764\n1.839\n1.782\nRelative Improvement\n11.504%\n5.811%\n7.027%\nthese programs. These developers are computer science students or industrial practitioners and write programs at least for 3 years. Each group is measured by two developers and the final score is the average of two developers\u2019 scores. Results. The results of the human evaluation are shown in Table 4. The percentages in parentheses are the relative improvements compared to the SOTA baseline. Analyses. (1) Developers prefer programs generated by LAIL over all baselines. LAIL substantially outperforms all baselines in three aspects, which demonstrates the effectiveness of our approach. (2) In addition to satisfying requirements, programs provided by our approach are easier to read and have less bad smell. Particularly, our LAIL surpasses the SOTA approach by 5.811% in code quality and 7.027% in maintainability. (3) Some heuristic methods that select the same examples as prompts for all test requirements are not optimal for ICL. As shown in Figure 4, Uncertainty-Target and Vote-K perform comparably to the random approach, which indicates that applying the same prompt to all test data is not a good choice, and different test requirements should use their appropriate prompts.\nAnswer to RQ2: Human evaluation shows that LAIL significantly outperforms SOTA baselines on all three aspects. Programs generated by LAIL can better satisfy requirements, be easier to read, and have little bad code smell. That is, developers prefer programs generated by LAIL among all approaches.\nRQ3: What is the better design choice to estimate examples? In this paper, we apply the predicted probability of ground truths to estimate examples. We further investigate two plausible choices to measure candidates by LLMs themselves. Setup. We design two plausible approaches including the MatchBLEU and the Match-CodeBLEU formats. The Match-BLEU format uses the BLEU score of the generated program to measure examples. The Match-CodeBLEU approach applies the CodeBLEU score of the predicted program to label examples. In this RQ, we select a representative LLM (i.e. GPT-3.5) as the base model and evaluate their performance on three datasets. Results. The results of different feedback scores are represented in Table 5. For comparison, we also present the results of the existing SOTA baseline in Table 5. Analyses. (1) Our probability-based approach is better than Match-BLEU and Match-CodeBLEU. In all datasets, our LAIL outperforms them by 4.05%, 5.86%, and 3.77% on three datasets at Pass@1, respectively. We argue that the predicted probability of ground truth accurately reflects the certainty of LLMs for correct programs. The BLEU-based and CodeBLEU-based methods only reflect the literal accuracy of LLMs\u2019 prediction, but can not provide how certain LLMs are when predicting programs. (2) MatchCodeBLEU approach is more effective than Match-BLEU method. Match-CodeBLEU outperforms Match-BLEU on all datasets. The reason might be that CodeBLEU measures the n-gram match, the semantic match, and the syntactic match between the hypothesis programs and the reference code tokens, which is better to evaluate the generated programs than BLEU.\nAnswer to RQ3: Probability-based approach is better than Match-CodeBLEU and Match-BLEU formats. It can effectively reflect the LLMs\u2019 preference for candidate examples given a test requirement. .\n# 6 DISCUSSION\n# 6.1 Transferability\nWe explore whether our retriever based on one LLM\u2019s feedback and specific dataset can be transferred to other LLMs or code generation datasets without further tuning. This is a significant research question since the retriever for each LLM and dataset needs to be trained in real applications.\n6.1.1 Transfer across LLMs. We consider transferring the retriever based on one LLM\u2019s feedback to another LLM. Specifically, we use a source LLM (e.g. CodeGen or GPT-3.5) to estimate examples for training a retriever and then apply the retriever to another target LLM (e.g. ChaTGPT) in generating programs. Table 6 shows the performance of ChaTGPT in three datasets. We surprisingly find our retriever based on CodeGen and GPT-3.5 can bring obvious improvements to ChaTGPT. In particular, in terms of Pass@1, ChaTGPT achieves 2.56% improvements from CodeGen\u2019s feedback and 4.31% enhancements from GPT-3.5\u2019s feedback compared to the SOTA baseline. The phenomenons demonstrate that our approach has satisfying transfer ability across different LLMs. Note that ChaTGPT can not provide the prediction probability of ground truths in practice, thus LAIL is a quite meaningful approach, especially for LLMs whose parameters are unavailable. Besides, the performance of ChaTGPT from GPT-3.5\u2019s feedback is higher than the counterpart from CodeGen\u2019s feedback. The reason might be that GPT-3.5 and ChatGPT have comparable abilities in code generation, thus their preference is similar and GPT-3.5 can provide more proper examples as prompts. To verify the transfer ability among LLMs with different sizes, we further evaluate the performance of CodeGen based on GPT3.5\u2019s feedback and the results of GPT-3.5 from CodeGen\u2019s feedback, where the parameter size of CodeGen is much smaller than the counterpart of GPT-3.5. As shown in Table 6, compared to the random approach, the retriever learned from CodeGen achieves 8.25%, 12.09%, and 4.32% relative improvements to GPT-3.5 on Pass@1, meanwhile our retriever under GPT-3.5\u2019s feedback brings 22.30%, 14.47%, and 16.94% improvements to CodeGen, respectively. This indicates our approach has satisfying transfer ability among LLMs and can bring improvements to target LLMs.\n6.1.2 Transfer across datasets. Considering that the compositional features of natural language are general, the retriever trained on one dataset may apply to other datasets and exploit similar knowledge in different datasets. In this section, we further investigate whether a retriever trained on one dataset can transfer to others. We transfer the retriever among three datasets (e.g., MBJP, MBPP, and MBCPP) and Figure 3 demonstrates the transferring results on Pass@1. We find that most retrievers can successfully transfer to other datasets and bring improvements compared to their SOTA baselines. Concretely, the retriever trained on MBJP (MBCPP)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6fb3/6fb3ef24-8e9a-45bd-86e1-bd1cd861ee0a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Results of transferring the retriever trained on one dataset (row) to others (column) on GPT-3.5 in MBPP dataset.</div>\nachieves 1.28% (1.07%) absolute improvements when it migrates to MBCPP (MBJP). Meanwhile, the MBCPP-trained retriever hardly transfers to MBPP and the MBPP-based retriever suffers the generation performance on MBPP. The reason might be that Java and C++ are object-oriented programming languages, and their syntax and code morphology are similar. Exploring a retriever that is suitable for many datasets is a challenging but meaningful research question, and we leave this topic as our future work.\n# 6.2 Impacts of In-Context Example Numbers\nMost of the LLMs are trained with a limited input length, which restricts the number of examples in the prompt. Previous studies [19, 35] also find that LLMs are affected by the number of in-context examples. Here we explore the impacts of the number of examples in the prompt on baselines and our approach. Figure 4 reports how the performance of LAIL and GraphCodeBET change with respect to different in-context example numbers inMBPP dataset. Note that GraphCodeBET is the best baseline in this dataset, thus we choose it to compare with our approach. We observe that the performances of GraphCodeBET and our approach monotonically increase in rough with the increase of in-context example numbers. In addition, our LAIL always outperforms GraphCodeBERT in all cases, which further proves the superiority of our approach even with different in-context example numbers.\n# 6.3 Effects of Contrastive Learning Parameter\nThe number of negative examples N\ud835\udc56(named N\ud835\udc56for convenience in Figure 7) is an important element in contrastive learning for training a neural retriever. We investigate how the element affects the performance of our approach. As shown in Figure 7, with the increasing of negative example numbers, LAIL achieves consistent improvements and can generate more correct programs. Besides, as described in 3.2, we randomly select an example \ud835\udc52\ud835\udc5b \ud835\udc56from \ud835\udc46\ud835\udc5b \ud835\udc56into N\ud835\udc56. We also explore the effect of the number of selected examples from \ud835\udc46\ud835\udc5b \ud835\udc56(dubbed \ud835\udf0f\ud835\udc5b\ud835\udc52). We can find that the more hard negative examples, the worse our approach performs. We argue that the examples of \ud835\udc46\ud835\udc5b \ud835\udc56are with high BM25 scores among all candidate examples and thus \ud835\udc46\ud835\udc5b \ud835\udc56may contain some false negative examples. Selecting more examples from \ud835\udc46\ud835\udc5b \ud835\udc56will affect a retriever learning for choosing suitable in-context examples. Between the two factors,\nTable 5: The comparison of different formats to estimate examples on three datasets. Th improvements. \u201cProbability-Based \u201d represents our LAIL.\n<div style=\"text-align: center;\">Table 5: The comparison of different formats to estimate examples on three datasets. The values in parentheses mean signif improvements. \u201cProbability-Based \u201d represents our LAIL.</div>\nMBJP\nMBPP\nMBCPP\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nRandom [14]\n47.87\n59.83\n63.69\n43.00\n56.40\n60.80\n50.04\n61.45\n65.28\nMatch-BLEU\n50.19\n59.87\n63.92\n46.20\n60.20\n62.40\n50.26\n61.47\n65.73\nMatch-CodeBLEU\n51.07\n60.04\n64.58\n47.80\n59.20\n63.80\n52.72\n62.03\n66.80\nProbability-Based (LAIL)\n53.14\n62.87\n66.72\n50.60\n62.40\n65.20\n54.71\n65.98\n69.67\nRelative Improvement\n4.05%\n4.71%\n3.31%\n5.86%\n3.65%\n2.19%\n3.77%\n6.37%\n4.30%\n<div style=\"text-align: center;\">Table 6: Results of transferring a retriever learned on one LLM to others on three datasets. \u2663means the source LLM.</div>\n \u2663\n\u2663ChaTGPT\nMBJP\nMBPP\nMBCPP\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nPass@1\nPass@3\nPass@5\nZero-Shot Learning [11]\n16.63\n34.48\n44.21\n26.60\n32.00\n34.60\n30.34\n57.01\n63.68\nRandom [14]\n53.34\n62.27\n65.72\n50.80\n60.60\n63.40\n40.15\n60.06\n66.28\nAceCoder [31]\n54.46\n64.01\n66.75\n54.40\n62.40\n65.20\n42.53\n62.29\n68.32\nTOP-k-SBERT [35]\n54.26\n63.89\n66.53\n54.80\n63.00\n65.40\n43.37\n61.45\n67.11\nTOP-k-GraphCodeBERT [21]\n53.95\n62.67\n66.32\n54.20\n62.60\n65.20\n44.08\n61.61\n68.27\nTOP-k-VOTE [43]\n51.93\n62.88\n65.72\n42.00\n56.00\n61.00\n42.74\n62.68\n67.73\nUncertainty-Target [17]\n49.69\n60.45\n54.50\n36.40\n51.80\n56.80\n42.46\n61.84\n66.67\nCodeGen\nLAIL\n54.79\n64.70\n67.43\n55.60\n63.80\n66.20\n45.21\n63.45\n68.93\nRelative Improvement\n0.61%\n1.08%\n1.02%\n1.44%\n1.27%\n1.22%\n2.56%\n1.23%\n0.89%\nGPT-3.5\nLAIL\n55.97\n64.97\n68.27\n56.20\n64.60\n66.80\n45.98\n63.84\n70.35\nRelative Improvement\n2.77%\n1.50%\n2.28%\n2.56%\n2.54%\n2.14%\n4.31%\n1.85%\n2.97%\n\u2663GPT-3.5\nCodeGen\nRandom [14]\n47.87\n59.83\n63.69\n43.00\n56.40\n60.80\n50.02\n61.43\n65.28\nLAIL\n51.82\n61.75\n64.89\n48.20\n59.00\n64.20\n52.18\n64.54\n67.90\nRelative Improvement\n8.25%\n3.21%\n1.88%\n12.09%\n4.61%\n5.59%\n4.32%\n5.06%\n4.01%\n\u2663CodeGen\nGPT-3.5\nRandom [14]\n15.74\n24.25\n28.14\n15.20\n25.40\n28.80\n15.70\n28.72\n32.25\nLAIL\n19.25\n27.53\n30.98\n17.40\n26.00\n30.00\n18.36\n30.41\n35.54\nRelative Improvement\n22.30%\n13.53%\n10.09%\n14.47%\n2.36%\n4.17%\n16.94%\n5.88%\n10.20%\nthe number of selected examples from \ud835\udc46\ud835\udc5b \ud835\udc56has a greater influence on the performance of our approach.\n# 6.4 Threats to Validity\nThere are two main threats to the validity of our work. The generalizability of our experimental results. For the datasets, we follow previous studies [8, 9, 14] and leverage three representative code generation datasets. The three datasets cover different programming languages (e.g., Java, Python, and C++) and\ncome from real-world software communities. To verify the superiority of LAIL, we consider seven existing ICL approaches in both the code generation task and many maintain natural language tasks. In addition, to effectively evaluate our approach, we select a series of advanced pre-trained large language models (CodeGen [47], GPT-3.5, and ChatGPT) as base models in the past three years. We apply our approach and baselines to base models and evaluate their performance in code generation. For the metric, following existing works [13, 47], we select a widely used Pass@k metric to evaluate\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d43b/d43bfc1d-e295-4a34-bce9-492873f46e2c.png\" style=\"width: 50%;\"></div>\nFigure 4: The performance of the different number of incontext examples on GPT-3.5 in MBPP datasets.\n<div style=\"text-align: center;\">Table 7: Effects of parameters.</div>\nPass@1\nPass@3\nPass@5\nNegative examples (N\ud835\udc56)\n32\n50.20\n61.80\n65.00\n64\n50.60\n62.40\n65.20\n128\n50.80\n62.40\n65.40\n\ud835\udf0f\ud835\udc5b\ud835\udc52\n1\n50.60\n62.40\n65.20\n5\n50.20\n62.20\n65.00\n10\n49.40\n61.80\n64.40\nall approaches. It is an execution-based metric that utilizes test cases to check the correctness of generated programs. To ensure fairness, we execute each method three times and report the average experimental results. The implementation of models and prompts. It is widely known that deep neural models are sensitive to the implementation details. In this paper, we need to execute all baselines and our approach on three base models. For baselines, we apply the source code and parameters published by their original papers [21, 40, 47]. For LLMs (e.g., GPT-3.5 and ChatGPT), the hyper-parameters (e.g., temperature) of sampling will impact their outputs. In experiments, we follow previous studies [29, 47] to set hyper-parameters for all approaches. In addition, the performance of LLMs heavily depends on the prompts, including the instruction and the number of examples. To alleviate this threat, we leverage the same number of examples for all approaches and directly construct prompts without any natural language instructions. Besides, it\u2019s worth noting that the candidate pool for example selection also influences ICL performance. A large-scale study on 13.2 million real code files showed the proportion of reused code is up to 80% [36]. Thus, we use the training set of each dataset as the candidate set for all approaches in this paper and believe that the training set can provide supporting examples for test requirements. We do not tune the prompt and hyper-parameters experimentally and set them empirically. Thus, there might be room to tune hyper-parameter settings of our approach for more improvements.\n# 7 RELATED WORK 7.1 Pre-trained Language Models\n# 7.1 Pre-trained Language Models\nCode generation aims to automatically generate programs given natural language requirements. Pre-trained language models have achieved state-of-the-art results in code generation. They are pretrained on large-scale unlabeled code corpus and then transferred to code generation. Existing pre-trained models can be divided into encoder-decoder models and decoder-only models. Encoder-decoder models consist of an encoder and a decoder. An encoder takes a requirement as the input, and a decoder outputs a program. Many popular encoder-decoder architectures in natural language processing have been applied to source code, which often use denoising-based pre-training objectives to pre-train, such as masked sequence prediction, identifier tagging, and masked identifier prediction. For example, CodeT5 [44] is the variant of T5 model [39] to support programs. [5] is adapted from the BART model [28], which is pre-trained on a number of programs. Besides, some studies [12] further consider the naturalized feature of code and train models to generate natural programs based on noised programs. Decoder-only models contain decoder networks, which are pre-trained with the next token prediction objective. Inspired by the success of GPT series [38] in natural language processing, researchers attempt to adapt similar models to source code. CodeGPT [34] is pre-trained on CodeSearchNet [23] corpus with the same setting and structure as GPT-2. CodeX is continually fine-tuned on GPT-3 [11] on code corpus from GitHub. CodeX is proficient in over a dozen languages (e.g., JavaScript and Python) and supports a commercial application (e.g., Copilot). Since its parameters are not available, many researchers try to replicate them and bring CodeParrot [3], GPT-CC [4], PyCodeGPT [45] and CodeGen [47]. CodeGeeX [46] is pre-trained on a large code corpus and can generate more than 20 programming languages. Lately, ChatGPT is proposed by OpenAI and achieves impressive performance in code generation. In this paper, we select three representative networks including CodeGen [47], GPT-3.5 [2], and ChatGPT [1] as our base models.\n# 7.2 In-Context Learning\nIn-context learning (ICL) is an emerging approach to using large language models (LLMs). By providing limited examples as a prompt, ICL empowers LLMs to learn a specific downstream task. ICL [11] is first proposed in natural language processing and achieves impressive performance in many tasks [19, 27] such as text generation and sentiment classification. Inspired by the success of ICL in natural language processing, researchers attempt to adapt ICL to source code [10, 14, 16]. They design task-specific instructions with a set of examples to prompt LLMs and improve the performance on many tasks (e.g., code generation [16] and code repair [25]). The popularity of ICL also introduces instability in performance: given different sets of examples as prompts, LLMs\u2019 performance usually varies from random to near state-of-the-art [33]. Some studies [16, 31] make efforts to mitigate this issue. Researchers [16] randomly select limited examples for ICL and verify the results in code generation. AceCoder [31] uses BM25 to select n-gram matching examples and further generates more correct programs. However, these approaches are based on\nlexical features, which require developers to design heuristics and lead to sub-optimal performance. Compared to the existing methods, LAIL is a learning-based approach for selecting in-context examples. Instead of considering textual similarity, it applies LLMs themselves to measure candidate examples and uses contrastive learning to learn LLMs\u2019 preferences in code generation.\n# 8 CONCLUSION\nDue to the instability of ICL, there is an increasing demand for in-context example selection in code generation. This paper proposes a novel learning-based ICL approach LAIL. We exploit LLMs themselves to estimate examples by considering the generation probabilities of ground-truth programs given a requirement, and label these examples based on LLMs\u2019 feedback. We then introduce a contrastive learning objective to train a retriever, resulting in acquiring the preferences of LLMs. Experimental results on three code generation datasets demonstrate that our proposed approach achieves excellent performance. LAIL also shows satisfactory transferability across LLMs and datasets, showing that it is an efficient and practical approach to code generation. In the future, we will apply our approach to other LLMs and datasets.\n# REFERENCES\n[1] Available. https://openai.com/chatgpt. [2] Available. https://platform.openai.com/docs/models/gpt-3-5. [3] Available. https://huggingface.co/codeparrot/codeparrot. [4] Available. https://github.com/CodedotAl/gpt-code-clippy. [5] WU Ahmad, S Chakraborty, B Ray, and KW Chang. 2021. Unified pre-training for program understanding and generation.. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. [6] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1\u201337. [7] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton. 2018. A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv. 51, 4 (2018), 81:1\u201381:37. https://doi.org/10.1145/3212695 [8] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual Evaluation of Code Generation Models. arXiv preprint arXiv:2210.14868 (2022). [9] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021). [10] Patrick Barei\u00df, Beatriz Souza, Marcelo d\u2019Amorim, and Michael Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335 (2022). [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901. [12] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar T Devanbu, and Baishakhi Ray. 2022. Natgen: generative pre-training by \u201cnaturalizing\u201d source code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 18\u201330. [13] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 (2022). [14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597\u20131607. [16] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128 (2023).\n[17] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. 2019. Selection via proxy: Efficient data selection for deep learning. arXiv preprint arXiv:1906.11829 (2019). [18] Li Dong and Mirella Lapata. 2016. Language to Logical Form with Neural Attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 33\u201343. [19] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234 (2022). [20] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020). [21] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020). [22] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019). [23] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019). [24] Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnaci\u00f3n, Shuvendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao. 2022. Faultaware neural code rankers. Advances in Neural Information Processing Systems 35 (2022), 13419\u201313432. [25] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of code language models on automated program repair. arXiv preprint arXiv:2302.05020 (2023). [26] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023. Self-planning code generation with large language model. arXiv preprint arXiv:2303.06689 (2023). [27] Itay Levy, Ben Bogin, and Jonathan Berant. 2022. Diverse demonstrations improve in-context compositional generalization. arXiv preprint arXiv:2212.06800 (2022). [28] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019). [29] Jia Li, Yongmin Li, Ge Li, and Zhi Jin. 2023. Structured Chain-of-Thought Prompting for Code Generation. CoRR (2023). https://arxiv.org/abs/2305.06599 [30] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. SkCoder: A Sketch-based Approach for Automatic Code Generation. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2124\u20132135. https://doi.org/10.1109/ICSE48619.2023.00179 [31] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. AceCoder: Utilizing Existing Code to Enhance Code Generation. CoRR (2023). https://arxiv.org/abs/ 2303.17780v3 [32] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092\u20131097. [33] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? arXiv preprint arXiv:2101.06804 (2021). [34] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021). [35] Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr. ICL: Demonstration-Retrieved In-context Learning. arXiv preprint arXiv:2305.14128 (2023). [36] Audris Mockus. 2007. Large-scale code reuse in open source software. In First International Workshop on Emerging Trends in FLOSS Research and Development (FLOSS\u201907: ICSE Workshops 2007). IEEE, 7\u20137. [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311\u2013318. [38] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018). [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1\u201367. [40] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). [41] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for\nautomatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020). [42] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval 3, 4 (2009), 333\u2013389. [43] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975 (2022). [44] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifieraware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).\n[45] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. 2022. CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation. arXiv preprint arXiv:2206.06888 (2022). [46] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568 (2023). [47] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, and Mingwen Wang. 2022. CodeGen-Test: An Automatic Code Generation Model Integrating Program Test Information. arXiv preprint arXiv:2202.07612 (2022).\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) for code generation using large language models (LLMs), highlighting the limitations of existing heuristic methods for example selection that lead to sub-optimal performance. A new learning-based approach is proposed to improve the selection of examples for ICL, which is crucial for enhancing code generation accuracy.",
        "problem": {
            "definition": "The problem is the ineffective selection of examples in ICL for code generation, which leads to inconsistent performance of LLMs. Current methods rely heavily on heuristics and textual similarity, failing to capture the nuanced preferences of LLMs.",
            "key obstacle": "The main challenge is that existing methods do not consider the prior knowledge and unique preferences of LLMs, resulting in the selection of irrelevant or low-quality examples for generating code."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that LLMs can provide feedback on the quality of examples based on their own generation probabilities, leading to better-informed selection of examples.",
            "opinion": "The proposed idea, named LAIL (LLM-Aware In-context Learning), involves using LLMs to evaluate candidate examples and label them as positive or negative based on their effectiveness in generating correct code.",
            "innovation": "LAIL innovates by employing a learning-based approach for example selection rather than relying on heuristic methods, thus aligning the selection process with the inherent preferences of LLMs."
        },
        "method": {
            "method name": "LAIL",
            "method abbreviation": "LAIL",
            "method definition": "LAIL is a learning-based method that utilizes LLMs to evaluate and select examples for ICL in code generation by leveraging generation probabilities of ground-truth programs.",
            "method description": "LAIL selects in-context examples for code generation by incorporating LLM feedback to estimate the relevance of each candidate example.",
            "method steps": [
                "Estimate the generation probabilities for candidate examples using LLMs.",
                "Label examples as positive or negative based on their predicted usefulness.",
                "Train a neural retriever using a contrastive learning objective to align with LLM preferences.",
                "During inference, select the most relevant examples based on the trained retriever."
            ],
            "principle": "The effectiveness of LAIL lies in its ability to align example selection with the specific preferences and prior knowledge of LLMs, leading to improved code generation outcomes."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three datasets (MBJP, MBPP, MBCPP) using three LLMs (ChatGPT, GPT-3.5, CodeGen) to evaluate the performance of LAIL against several baseline methods.",
            "evaluation method": "Performance was assessed using the Pass@k metric, which measures the functional correctness of generated programs based on executing test cases against generated outputs."
        },
        "conclusion": "The experimental results demonstrate that LAIL significantly outperforms existing state-of-the-art methods in code generation, achieving notable improvements across various metrics and showcasing its effectiveness and transferability across different LLMs and datasets.",
        "discussion": {
            "advantage": "LAIL offers significant advantages by effectively utilizing LLM feedback for example selection, leading to improved performance in code generation tasks compared to heuristic methods.",
            "limitation": "One limitation of LAIL is its reliance on the specific characteristics of the LLMs used for training the retriever, which may affect its generalizability across different LLM architectures.",
            "future work": "Future research will focus on enhancing the transferability of the retriever across different LLMs and datasets, as well as exploring further optimizations in the example selection process."
        },
        "other info": {
            "key findings": "LAIL consistently outperformed state-of-the-art baselines, showing improvements in Pass@1 metrics of 11.58%, 6.89%, and 5.07% across different models.",
            "datasets": {
                "MBJP": {
                    "language": "Java",
                    "train_size": 383,
                    "dev_size": 90,
                    "test_size": 493
                },
                "MBPP": {
                    "language": "Python",
                    "train_size": 384,
                    "dev_size": 90,
                    "test_size": 500
                },
                "MBCPP": {
                    "language": "C++",
                    "train_size": 413,
                    "dev_size": 0,
                    "test_size": 435
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is crucial for enhancing code generation accuracy using large language models (LLMs)."
        },
        {
            "section number": "1.3",
            "key information": "Large language models facilitate in-context learning by providing feedback on the quality of examples based on their generation probabilities."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method, LAIL (LLM-Aware In-context Learning), innovates by employing a learning-based approach for example selection rather than relying on heuristic methods."
        },
        {
            "section number": "3.4",
            "key information": "LAIL selects in-context examples for code generation by incorporating LLM feedback to estimate the relevance of each candidate example."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of LAIL is its reliance on the specific characteristics of the LLMs used for training the retriever, which may affect its generalizability across different LLM architectures."
        },
        {
            "section number": "5.2",
            "key information": "LAIL has been applied to code generation tasks, demonstrating significant improvements in performance metrics compared to existing methods."
        }
    ],
    "similarity_score": 0.7116811284654503,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Large Language Model-Aware In-Context Learning for Code Generation.json"
}