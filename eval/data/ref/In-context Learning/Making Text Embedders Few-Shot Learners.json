{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.15700",
    "title": "Making Text Embedders Few-Shot Learners",
    "abstract": "Large language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding.",
    "bib_name": "li2024makingtextembeddersfewshot",
    "md_text": "# MAKING TEXT EMBEDDERS FEW-SHOT LEARNERS\nChaofan Li1,2\u2217, MingHao Qin1,3,\u2217, Shitao Xiao1, Jianlyu Chen1,4, Kun Luo1,3, Yingxia Shao2, Defu Lian4, Zheng Liu1\u2020\n1: Beijing Academy of Artificial Intelligence 2: Beijing University of Posts and Telecommunications 3: Chinese Academy of Sciences 4: University of Science and Technology of China\n{cfli, shaoyx}@bupt.edu.cn qinminghao24@ia.ac.cn stxiao@baai.ac.cn chenjianlv@mail.ustc.edu.cn liandefu@ustc.edu.cn {luokun695, zhengliu1026}@gmail.com\n# ABSTRACT\nLarge language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding.\n# INTRODUCTION\nText embeddings are vector representations that capture the semantic and contextual meaning of natural language text. They play a pivotal role in natural language processing (NLP) tasks, facilitating a wide range of applications such as information retrieval, text classification, item recommendation, and question answering (Karpukhin et al., 2020; Xiong et al., 2020; Lu et al., 2020). Pre-trained bidirectional encoder and encoder-decoder architectures have been widely adopted as backbone models for embedding model, owing to their effectiveness in producing high-quality vector embeddings for text thanks to their extensive pre-training (Xiao et al., 2022; Gao et al., 2021). Recent advancements in LLMs have significantly shifted the focus towards embedding models that rely primarily on decoder-only architectures (Ma et al., 2023; Li et al., 2024; Wang et al., 2023). These LLM-based embedding models have demonstrated remarkable improvements in in-domain accuracy and generalization, particularly when trained using supervised learning approaches (Wang et al., 2023). However, despite these advances, embedding models still struggle to follow unseen task instructions and execute complex retrieval tasks Su et al. (2024); Weller et al. (2024). This limitation stems from a mismatch between the relatively narrow range of instructions encountered during training and the broader variety of real-world text embedding tasks. In-context learning (ICL) is a core capability of LLMs, enabling them to incorporate task-specific examples directly into input prompts to generate desired outputs (Radford et al., 2019; Brown, 2020;\n\u2217Co-first authors \u2020Corresponding author\nGao et al., 2020). The scope of ICL extends beyond tasks seen during training; it enables LLMs to generalize to new and complex tasks by learning patterns from the provided examples. This allows LLMs to adapt dynamically to novel tasks without additional training, making them highly applicable to large-scale, real-world scenarios (Wei et al., 2022; Yao et al., 2022; Dong et al., 2022). Recognizing the robust ICL abilities of LLMs, in this study, we propose to generate more adaptable text embeddings with ICL strategy. Specifically, we guide the model by including task-specific examples directly within the query prompt. By doing so, we leverage the ICL capabilities of LLMs to produce embeddings that are not only more relevant to the specific domain but also more generalizable across various contexts. Moreover, LLMs are predominantly utilized for text generation tasks, and adapting them for embedding representation tasks requires specific fine-tuning strategies. Recent studies have introduced various approaches, including the generation of high-quality training data through LLMs (Wang et al., 2023), modifications to attention mechanisms, and changes in pooling methods (Ma et al., 2023; Li et al., 2024). Following previous works (Muennighoff et al., 2024; BehnamGhader et al., 2024), we investigate how to effectively utilize LLMs as embedding models by modifying various architectures, e.g., bidirectional attention, meaning pooling, etc. Our experimental findings indicate that in the ICL scenario, making complex modifications to the models does not lead to significant improvements. Surprisingly, the best results are obtained using the original, unmodified architecture. By employing only the ICL strategy, our model bge-en-icl achieves state-of-the-art (SOTA) results on both the MTEB and AIR-Bench benchmarks. We have also released a multilanguage embedding model bge-multilingual-gemma2 and a lightweight reranker bge-rerankerv2.5-gemma2-lightweight. The lightweight reranker also serves as the teacher model for training embedding models through distillation. Further details are provided in Appendices C and D.\n\u2022 We propose bge-en-icl, which incorporate few-shot examples into the query side to enhance the query embeddings. This integration leverages the in-context learning (ICL) capabilities of large language models (LLMs) in text embedding tasks. \u2022 We rethink and explore how to effectively utilize LLMs as embedding models by evaluating various attention mechanisms, pooling methods, and the incorporation of passage prompts. Our findings highlight that simplicity is best; simply combining ICL capabilities with embedding models can achieve excellent performance. \u2022 In contrast to other leading models on the MTEB benchmark, we provide open access to our model checkpoint, dataset, and training scripts.\n\u2022 We propose bge-en-icl, which incorporate few-shot examples into the query side to enhance the query embeddings. This integration leverages the in-context learning (ICL) capabilities of large language models (LLMs) in text embedding tasks. \u2022 We rethink and explore how to effectively utilize LLMs as embedding models by evaluating various attention mechanisms, pooling methods, and the incorporation of passage prompts. Our findings highlight that simplicity is best; simply combining ICL capabilities with embedding models can achieve excellent performance. \u2022 In contrast to other leading models on the MTEB benchmark, we provide open access to our model checkpoint, dataset, and training scripts.\n# 2 RELATED WORK\nText embedding is a critical research direction in the field of information retrieval, with wide-ranging applications including web search, question answering, and dialogue systems. The fundamental principle involves encoding both queries and documents into embedding vectors within the same latent space. By calculating similarity scores between these vectors, effective retrieval is achieved. In recent years, numerous studies have leveraged pre-trained language models such as BERT (Devlin, 2018), T5 (Raffel et al., 2020), and RoBERTa (Liu, 2019) as the backbone for embedding models. These models have consistently demonstrated superior performance compared to traditional sparse retrieval methods. The capability of the backbone is a crucial determinant in the effectiveness of retrieval systems. (Luo et al., 2024) have demonstrated that performance improves with increased scale and extensive pretraining. Currently, numerous studies have explored the effectiveness of utilizing LLMs as backbone encoders for text embedding tasks. Repllama (Ma et al., 2023) fine-tuned Llama-2 to serve as both a dense retriever and a reranker, demonstrating the effectiveness of applying large language models (LLMs) in text embedding tasks. To further align LLMs with text embedding tasks, Llama2Vec (Li et al., 2024) introduced two pretraining tasks specifically designed to enhance the model\u2019s performance in such tasks, which led to significant improvements on the BEIR benchmark. E5-mistral and Gecko (Wang et al., 2023; Lee et al., 2024b) advanced the training of LLM-based embedding models through the use of syn-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/436d/436d986f-341c-469f-8645-d62a2485aa69.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The architecture of the ICL-based model.</div>\nthetic data, markedly boosting their performance across a diverse range of retrieval and non-retrieval tasks. NV-Embed (Lee et al., 2024a) innovatively proposed a latent attention layer to replace conventional pooling methods and implemented a two-stage training strategy to address the challenge of false negatives in non-retrieval tasks. This model has shown strong performance in both retrieval and non-retrieval domains. Additionally, GRIT (Muennighoff et al., 2024) successfully integrated text embedding and generation within a single LLM, achieving performance levels on par with specialized models focused solely on either embedding or generation. In the exploration of LLMs as embedding models from an unsupervised perspective, LLM2Vec (BehnamGhader et al., 2024) presented a novel unsupervised method to transform decoder-only LLMs into embedding models. This approach demonstrated significant potential for modifying LLM backbone encoders to perform retrieval without any supervision. Similarly, PromptReps (Zhuang et al., 2024) leveraged chat-based LLMs aligned with human preferences to generate high-quality dense representations in an unsupervised manner. The LLM-based embedding models mentioned above exhibit commendable performance across both retrieval and non-retrieval tasks. However, much of the existing work has disproportionately focused on altering model architectures, thereby neglecting the intrinsic capabilities of LLMs. Even models like GritLM, which integrate generation and embedding functionalities, fail to fully exploit the potential ICL capabilities of LLMs within the embedding process. By leveraging the innate ICL capabilities of LLMs, embedding models can be more versatile and adapt to diverse scenarios without necessitating additional fine-tuning. Our model not only achieves SOTA results on the MTEB and AIR-Bench benchmarks but also effectively utilizes the inherent strengths of LLMs across tasks.\n# 3 METHOLOGY\n# 3.1 IN-CONTEXT LEARNING FOR EMBEDDING MODELS\nPrevious embedding models often involve directly inputting the query into the model to generate target embeddings. However, this method struggles to handle tasks with different intents, limiting the model\u2019s adaptability and generalization capabilities. To address this, researchers have introduced task instructions (Su et al., 2022) appended to queries, enabling a single embedding model to generalize across tasks in various domains by altering the instructions.\nDespite these advances, studies such as Su et al. (2024); Weller et al. (2024) reveal that embedding models have a limited ability to follow unseen embedding task instructions and conduct complex retrieval tasks. This limitation arises from a gap between the limited diversity of instructions seen during training and the vast range of real-world scenarios. Inspired by the ability of LLMs to generalize to unseen tasks through in-context learning (ICL), we explore whether embedding models can be enhanced by leveraging ICL, thereby significantly improving their generalization and versatility across diverse embedding tasks with various user intents. In this work, we demonstrate the potential of embedding models to benefit from ICL through fewshot contrastive training. Consider a query-passage pair (qi, pi) in an embedding task. We first construct an example template as follows:\n\u27e8Instruct\u27e9{task definition} \u27e8query\u27e9{qi} \u27e8response\u27e9{pi}\nHere, \u201dtask definition\u201d represents the description of the specific embedding task. This example template is applied to new input queries for each embedding task (Figure 1). For a relevant querypassage pair (q+, p+), the modified query q+ exp is constructed as follows:\nexample 1} ... {example n} \u27e8Instruct\u27e9{task definition} \u27e8query\u27e9{q+}\nAll modified queries and passages in the corpus are encoded using the same LLM to obtain their embedding representations. Specifically, we append an [EOS] token to the end of the input modified queries and passages, feeding them into the LLM to obtain embeddings (hq+ exp, hp+) by extracting the final layer\u2019s [EOS] vector. We employ the standard InfoNCE (Izacard et al., 2021) loss function L, utilizing both in-batch negatives and hard negatives for training:\n  p\u2212 j denotes the set of negative passages, and s(q, p) is the scoring function between the query and passage. In this work, we adopt a temperature-scaled cosine similarity function defined as:\nwhere \u03c4 is a temperature hyperparameter, which is fixed at 0.02 during tr\n# 3.2 REPRESENTATION METHOD\nThe attention mechanism in LLM-based embedding models is typically unidirectional, aligned with the next-token prediction task fundamental to their pre-training (Touvron et al., 2023).\nHowever, recent studies indicate that unidirectional attention may limit the model\u2019s capacity for representation learning. Evidence suggests that bidirectional attention is more effective at integrating contextual information, resulting in improved performance on certain tasks. For example, LLM2Vec (BehnamGhader et al., 2024) introduces an additional training phase with a masked token prediction task, preconditioning the model for bidirectional attention. Approaches such as NV-Embed (Lee et al., 2024a) and GritLM (Muennighoff et al., 2024) replace unidirectional attention with bidirectional attention during the embedding training phase, often employing mean pooling or more sophisticated latent attention layers to obtain representations for queries and passages. Despite these advances, we argue that incorporating bidirectional attention during embedding finetuning creates a mismatch with the model\u2019s pre-training design, potentially undermining its incontext learning and generative properties. To address the trade-off between enhancing embedding representations for specific tasks and preserving the model\u2019s inherent generative properties for deep semantic pattern understanding, our approach retains the unidirectional attention mechanism, consistent with the majority of existing embedding methods. We use the [EOS] token\u2019s output embedding as the vector representation for queries and passages, positioning it at the end of inputs to capture both semantic and ICL patterns through causal attention\n(1)\n(2)\n(3)\n(4)\n# mechanisms, thereby aligning with the foundational pretraining methodology of LLMs. Specifically, given the tokenized input sequence T: [BOS], t1, ..., tN is sent into the LLM (Figure 1):\nht = LLM(T)[EOS]\nThe text embedding is taken from the output embedding of the special token [EOS].\n3.3 ICL-BASED INSTRUCTION-TUNING\nWhile previous works (Wang et al., 2023; Lee et al., 2024a) have proposed the training method of instruction-tuning, which incorporates a large number of task-specific instructions during the training process, enabling the model to adapt to various downstream retrieval tasks based on different instructions, it is not applicable to the ICL strategy. As demonstrated by GRIT (Muennighoff et al., 2024), directly supplying few-shot examples when generating embeddings can actually degrade model performance. To incorporate ICL capabilities into models, we need to modify the conventional instruction tuning strategy. Our approach involves integrating ICL abilities during the training phase. Specifically, we provide task-relevant examples to the query throughout the training process, allowing the model to develop ICL capabilities as it learns. Recognizing the risk of compromising zero-shot capabilities if examples are consistently provided during training, we propose a dynamic training process. In each training step, queries are supplied with a variable number of few-shot examples, ranging from zero to n, determined by a sampling function. This approach maintains a balance between developing ICL abilities and preserving zeroshot performance. To further enhance the model\u2019s ICL capabilities, we introduce an innovative technique for examples selection. By incorporating in-batch pairs as few-shot examples, we train the model to better differentiate between examples and inputs, aims to improve the model\u2019s ability to generate reliable embeddings based on the provided examples.\n# 4 EXPERIMENTENS\nhis section, we examine the effectiveness of the ICL training pipeline and reconsider the training thodologies for LLM-based embedding models. \u2022 RQ 1: What is the effectiveness of the ICL training strategy for both zero-shot and few-shot learning scenarios? \u2022 RQ 2: How does the ICL training strategy impact performance compared to traditional training methods? \u2022 RQ 3: How does the integration of in-batch examples affect the performance of the ICL training strategy. \u2022 RQ 4: What are the implications of replacing a causal attention mask with a bidirectional attention mask within the framework of LLMs? \u2022 RQ 5: What is the impact of various representation strategies, including last token pooling and mean pooling, on model performance? \u2022 RQ 6: Do passage-based prompts enhance performance in the ICL training strategy?\n# 4.1 SETUP\nLLM. Following E5-Mistral (Wang et al., 2023), SFR, and NV-Embedder (Lee et al., 2024a), we have adopted Mistral-7B (Jiang et al., 2023) as the backbone for our framework. Evaluation. We evaluate the performance of our model on MTEB (Muennighoff et al., 2022) and AIR-Bench. MTEB is a comprehensive benchmark designed to evaluate the performance of text embedding models. AIR-Bench is dedicated to the evaluation of retrieval performance, its testing data is automatically generated by large language models without human intervention.\nraining Data. To ensure a fair comparison, we use the same public datasets from E5-Mistral Wang et al., 2023), which includes ELI5 (Fan et al., 2019), HotpotQA (Yang et al., 2018), FEVER Thorne et al., 2018), MIRACL (Zhang et al., 2023), MSMARCO passage and document ranking Nguyen et al., 2016), NQ (Karpukhin et al., 2020), NLI (Gao et al., 2021), SQuAD (Karpukhin t al., 2020), TriviaQA (Karpukhin et al., 2020), Quora Duplicate Questions (DataCanary et al., 017), MrTyDi (Zhang et al., 2021), DuReader (Qiu et al., 2022), and T2Ranking (Xie et al., 2023), ll of which are also used for LLM2Vec (BehnamGhader et al., 2024). However, methods that typically perform exceptionally well, such as NV-Embedder (Lee et al., 024a) and SFR, often require more training data. Additionally, some of these methods, such as GTE-Qwen2 (Li et al., 2023), do not disclose their sources of training data. In response, we have eveloped an enhanced version of our model that leverages a more comprehensive dataset, which ncludes the following training sets: \u2022 Retrieval: ELI5, HotpotQA, FEVER, MSMARCO passage and document ranking, NQ, NLI, SQuAD, TriviaQA, Quora Duplicate Questions, Arguana (Wachsmuth et al., 2018), and FiQA (Maia et al., 2018). \u2022 Reranking: SciDocsRR (Cohan et al., 2020) and StackOverFlowDupQuestions (Liu et al., 2018). \u2022 Classification: AmazonReviews-Classification (McAuley & Leskovec, 2013), AmazonCounterfactual-Classification (O\u2019Neill et al., 2021), Banking77Classification (Casanueva et al., 2020), Emotion-Classification (Saravia et al., 2018), TweetSentimentExtraction-Classification (Maggie, 2020), MTOPIntent-Classification (Li et al., 2020), IMDB-Classification (Maas et al., 2011), ToxicConversations-Classification (Adams et al., 2019). \u2022 Clustering: {Arxiv/Biorxiv/Medrxiv/Reddit/StackExchange}-Clustering-{S2S/P2P}, TwentyNewsgroups-Clustering (Lang, 1995). \u2022 STS: STS12 (Agirre et al., 2012), STS22 (Chen et al., 2022), STS-Benchmark (Cer et al., 2017).\nTraining Detail. We fine-tune the Mistral-7B model using a contrastive loss and conduct the process over a single epoch. For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2021), setting the LoRA rank to 64 and the LoRA alpha to 32, with a learning rate of 1e-4. For retrieval tasks, we use in-batch negatives, a strategy not adopted for other tasks. Each dataset incorporates 7 hard negatives. The batch size is set to 512 for retrieval tasks and 256 for other types of tasks. We maintain consistency by using the same dataset throughout one training step, and the maximum sequence length is set at 512 tokens. To distill the score from reranker in retrieval tasks, we use the bge-reranker model as the teacher. For in-context learning training, we implement a randomized sampling method. For each query, we select between 0 to 5 examples from the in-batch training data. The maximum allowable lengths for example queries and documents are set to 256 tokens each, and the combined length for a query with examples is set at 2048 tokens. Evaluation. We evaluate the performance of our model under both zero-shot and few-shot conditions. In the few-shot scenario, a consistent set of in-context examples is applied to each query. The examples utilized for evaluation are sourced from training datasets. In cases where training datasets are unavailable, examples are generated using ChatGPT.\n# 4.2 MAIN RESULTS\nMTEB. Table 1 presents the performance of our model, bge-en-icl, evaluated on the MTEB benchmark. This evaluation contrasts the results obtained from using the full dataset with those obtained from using only the public dataset. When leveraging the full dataset, our model demonstrates strong capabilities in both zero-shot and few-shot settings, achieving SOTA results in few-shot scenarios. However, it is important to note that the use of full datasets may introduce inconsistencies, as different models often rely on varying datasets. Notably, many of these models do not disclose the specific datasets they use, leading to potential unfair comparisons. For a fairer comparison and to better understand the impact of in-context learning, we conducts an evaluation using only the public dataset. Under these constraints, our model\u2019s performance in\nTask\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\n# of datasets \u2192\n15\n4\n11\n3\n12\n10\n1\n56\nw/ full data\nE5-mistral-7b-instruct\n56.90\n60.21\n50.26\n88.34\n78.47\n84.66\n31.40\n66.63\nGritLM-7B\n57.41\n60.49\n50.61\n87.16\n79.46\n83.35\n30.37\n66.76\nSFR-Embedding\n59.00\n60.64\n51.67\n88.54\n78.33\n85.05\n31.16\n67.56\nLinq-Embed-Mistral\n60.19\n60.29\n51.42\n88.35\n80.20\n84.97\n30.98\n68.17\nvoyage-large-2-instruct\n58.28\n60.09\n53.35\n89.24\n81.49\n84.31\n30.84\n68.23\nNV-Embed-v1\n59.36\n60.59\n52.80\n86.91\n87.35\n82.84\n31.20\n69.32\nbge-multilingual-gemma2\n59.24\n59.72\n54.65\n85.84\n88.08\n83.88\n31.20\n69.88\nstella en 400M v5\n58.97\n60.16\n56.70\n87.74\n86.67\n84.22\n31.66\n70.11\ngte-Qwen2-7B-instruct\n60.25\n61.42\n56.92\n85.79\n86.58\n83.04\n31.35\n70.24\nSFR-Embedding-2 R\n60.18\n60.14\n56.17\n88.07\n89.05\n81.26\n30.71\n70.31\nstella en 1.5B v5\n61.01\n61.21\n57.69\n88.07\n87.63\n84.51\n31.49\n71.19\nbge-en-icl (zero-shot)\n61.67\n59.66\n57.51\n86.93\n88.62\n83.74\n30.75\n71.24\nbge-en-icl (few-shot)\n62.16\n59.82\n57.89\n88.14\n88.95\n84.24\n30.77\n71.67\nw/ public data only\nE5-mistral-7b-instruct\n52.78\n60.38\n47.78\n88.47\n76.80\n83.77\n31.90\n64.56\nGritLM-7B\n53.10\n61.30\n48.90\n86.90\n77.00\n82.80\n29.40\n64.70\nLLM2Vec-Mistral-supervised\n55.99\n58.42\n45.54\n87.99\n76.63\n84.09\n29.96\n64.80\nbge-en-icl (zero-shot)\n59.59\n56.85\n42.61\n87.87\n75.47\n83.30\n29.52\n64.67\nbge-en-icl (few-shot)\n60.08\n56.67\n46.55\n88.51\n77.31\n83.69\n30.68\n66.08\n<div style=\"text-align: center;\">Table 1: Top MTEB leaderboard models as of August 27, 2024.</div>\nDomain\nwiki\nweb\nnews\nhealthcare\nlaw\nfinance\narxiv\nmsmarco\nAvg.\n# of datasets \u2192\n1\n1\n1\n1\n1\n1\n1\n1\n8\nw/ full data\nE5-mistral-7b-instruct\n61.67\n44.41\n48.18\n56.32\n19.32\n54.79\n44.78\n59.03\n48.56\nSFR-Embedding\n63.46\n51.27\n52.21\n58.76\n23.27\n56.94\n47.75\n58.99\n51.58\nNV-Embed-v1\n62.84\n50.42\n51.46\n58.53\n20.65\n49.89\n46.10\n60.27\n50.02\nLinq-Embed-Mistral\n61.04\n48.41\n49.44\n60.18\n20.34\n50.04\n47.56\n60.50\n49.69\ngte-Qwen2-7B-instruct\n63.46\n51.20\n54.07\n54.20\n22.31\n58.20\n40.27\n58.39\n50.26\nstella en 1.5B v5\n61.99\n50.88\n53.87\n58.81\n23.22\n57.26\n44.81\n61.38\n51.53\nbge-en-icl (zero-shot)\n64.61\n54.40\n55.11\n57.25\n25.10\n54.81\n48.46\n63.71\n52.93\nbge-en-icl (few-shot)\n64.94\n55.11\n56.02\n58.85\n28.29\n57.16\n50.04\n64.50\n54.36\nw/ public data only\nbge-en-icl (zero-shot)\n64.82\n54.96\n55.82\n57.06\n28.87\n54.46\n49.60\n63.25\n53.60\nbge-en-icl (few-shot)\n66.98\n56.38\n57.17\n59.54\n32.03\n58.81\n51.36\n65.05\n55.92\nTable 2: QA (en, nDCG@10) performance on AIR-Bench 24.04.\nthe zero-shot scenario is on par with, or slightly below, that of other models such as LLM2Vec and GritLM. However, in the few-shot settings, our model show significant enhancements (\u21911.41), particularly in the classification and clustering tasks that were not part of the training data. These improvements underscore the potential advantages of in-context learning, emphasizing its efficacy in adapting to tasks beyond the direct scope of initial training parameters. Furthermore, in contrast to training exclusively with public datasets, the utilization of full training data effectively familiarizes the model with these datasets. As a result, the model\u2019s ability to generalize effectively is compromised, leading to only a modest improvement in few-shot settings (\u21910.43). AIR-Bench. The performance of our model is also evaluated using the AIR-Bench dataset. As illustrated in Tables 2 and 3, the model demonstrates superior performance compared to prior models in both zero-shot and few-shot scenarios, excelling across qa and long-doc tasks. Notably, there is no overlap between the training dataset and the evaluation data for these tasks, highlighting the robustness of the model in scenarios with limited prior exposure. In the few-shot setting, the model exhibits significant improvements over the zero-shot scenario, achieving gains of 1.43 points in the qa task and 1.08 points in the long-doc task. This improvement underscores the efficacy of in-context learning in enhancing the model\u2019s generalization capabilities.\nDomain\narxiv\nbook\nhealthcare\nlaw\nAvg.\n# of datasets \u2192\n4\n2\n5\n4\n15\nw/ full data\ntext-embedding-3-large\n74.53\n73.16\n65.83\n64.47\n68.77\nE5-mistral-7b-instruct\n72.14\n72.44\n68.44\n62.92\n68.49\nSFR-Embedding\n72.79\n72.41\n67.94\n64.83\n69.00\nNV-Embed-v1\n77.65\n75.49\n72.38\n69.55\n73.45\nLinq-Embed-Mistral\n75.46\n73.81\n71.58\n68.58\n72.11\ngte-Qwen2-7B-instruct\n63.93\n68.51\n65.59\n65.26\n65.45\nstella en 1.5B v5\n73.17\n74.38\n70.02\n69.32\n71.25\nbge-multilingual-gemma2\n71.77\n76.46\n73.96\n70.86\n72.88\nbge-en-icl (zero-shot)\n78.30\n78.21\n73.65\n67.09\n73.75\nbge-en-icl (few-shot)\n79.63\n79.36\n74.80\n67.79\n74.83\nw/ public data only\nbge-en-icl (zero-shot)\n79.73\n78.66\n72.88\n70.59\n74.86\nbge-en-icl (few-shot)\n79.82\n80.37\n74.60\n71.66\n75.98\nTable 3: Long-Doc (en, Recall@10) performance on AIR-Bench 24.04.\nHowever, when the model is trained exclusively using public data, it achieves better results compared to training with the full dataset. This could be attributed to the presence of an excessive amount of MTEB-related data, such as clustering and classification, within the full dataset. Such data might introduce the risk of overfitting, thereby potentially hampering the model\u2019s generalization performance on the AIR-Bench dataset.\n# 4.3 IN-CONTEXT LEARNING\n<div style=\"text-align: center;\">4.3 IN-CONTEXT LEARNING</div>\nTask\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\n# of datasets \u2192\n15\n4\n11\n3\n12\n10\n1\n56\nw/ full data\nw/o in-context learning\n59.11\n57.02\n42.60\n87.99\n76.27\n83.93\n30.50\n64.83\nw/ fix examples (zero-shot)\n48.98\n56.48\n41.84\n85.94\n74.38\n84.31\n29.68\n61.50\nw/ fix examples (few-shot)\n59.00\n56.90\n45.75\n88.54\n75.56\n84.67\n30.66\n65.46\nw/ in-batch examples (zero-shot)\n59.59\n56.85\n42.61\n87.87\n75.47\n83.30\n29.52\n64.67\nw/ in-batch examples (few-shot)\n60.08\n56.67\n46.55\n88.51\n77.31\n83.69\n30.68\n66.08\nTo evaluate the impact of the ICL strategy, we conduct a series of ablation studies using the MTEB benchmark. In these studies, we compare the performance of models fine-tuned with the ICL strategy against those fine-tuned without it. Specifically, for ICL training, we employ two distinct training approaches: fixed examples and in-batch examples. In the fixed examples approach, each task was trained using three predetermined examples. In Table 4, we present various results from our experiment. When the model is trained without ICL strategy, its average performance is 64.83. This performance is comparable to GritLM (Muennighoff et al., 2024), LLM2Vec (BehnamGhader et al., 2024), etc. When fixed examples are used during ICL training, there is a significant decline in zero-shot evaluation performance, with a decrease of 3.33 points. This decline is attributed to the model\u2019s consistent exposure to specific training examples, which can impair its zero-shot capabilities. On the other hand, in few-shot scenarios, the model demonstrates improved performance, exceeding zero-shot results by 3.96 points and surpassing models trained without ICL by 0.63 points. This also confirms the effectiveness of the ICL strategy. Meanwhile, the use of in-batch examples, where training may involve zero examples, has preserved the zero-shot capability of the model. There is a modest decrease of 0.16 points compared to the model trained without ICL. Notably, in few-shot scenarios, the performance of the model employing in-batch examples rises to 66.08 (\u21911.25), indicating a robust improvement. Furthermore, when compared with the model utilizing fixed examples, the model trained with in-batch examples displays\nsuperior performance in tasks that diverge significantly from the training data, such as classification and clustering tasks.\n# 4.4 ATTENTION\n<div style=\"text-align: center;\">4.4 ATTENTION</div>\nTask\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\n# of datasets \u2192\n15\n4\n11\n3\n12\n10\n1\n56\ncausal attention & last token pooling\nw/o in-context learning\n59.11\n57.02\n42.60\n87.99\n76.27\n83.93\n30.50\n64.83\nw/ in-context learning (zero-shot)\n59.59\n56.85\n42.61\n87.87\n75.47\n83.30\n29.52\n64.67\nw/ in-context learning (few-shot)\n60.08\n56.67\n46.55\n88.51\n77.31\n83.69\n30.68\n66.08\ncausal attention & mean pooling\nw/o in-context learning\n58.50\n53.74\n36.82\n82.14\n72.37\n77.62\n29.10\n61.03\nbidirectional attention & last token pooling\nw/o in-context learning\n59.59\n56.96\n44.34\n87.61\n74.77\n83.81\n30.12\n64.96\nw/ in-context learning (zero-shot)\n59.77\n58.09\n44.04\n87.87\n75.35\n83.97\n29.75\n65.19\nw/ in-context learning (few-shot)\n60.23\n57.81\n44.45\n88.64\n77.00\n83.77\n29.99\n65.74\nbidirectional attention & mean pooling\nw/o in-context learning\n59.13\n57.03\n43.44\n87.25\n75.03\n84.08\n29.17\n64.73\nw/ in-context learning (zero-shot)\n59.53\n57.48\n43.88\n88.12\n74.86\n83.64\n29.58\n64.90\nw/ in-context learning (few-shot)\n59.42\n57.29\n44.93\n88.36\n75.26\n83.75\n29.60\n65.18\nRecent studies have explored modifying causal attention in LLMs to adopt bidirectional attention and employ mean pooling for embedding generation. Notably, models such as GritLM (Muennighoff et al., 2024), NV-Embed (Lee et al., 2024a), and LLM2Vec (BehnamGhader et al., 2024) have utilized these techniques with considerable experimental success. Motivated by these advancements, we explore the potential benefits of implementing bidirectional attention in the ICL scenario. Specifically, we investigate the impacts of various attention and pooling mechanisms, including causal and bidirectional attention, coupled with last token pooling and mean pooling. In a causal attention framework, each token is limited to accessing only preceding tokens\u2019 information and not the subsequent ones. Consequently, employing mean pooling tends to yield suboptimal results because of this restriction. We find that the model could not be trained effectively under the ICL setting. Therefore, only results from experiments without ICL are presented in this specific configuration. Table 5 presents our experimental setup and results in both non-ICL and ICL scenarios. It demonstrates that in non-ICL scenarios, most methods yield consistent performance, aside from the combination of causal attention with mean pooling. In contrast, within ICL scenarios, the integration of causal attention and last token pooling emerges as the superior approach. This configuration appears to resonate with the competencies fostered during the initial training phase of the model, suggesting a strong alignment with the foundational strategies employed during pre-training. Moreover, shifting from causal attention to bidirectional attention does not result in significant improvements, and mean pooling is not necessary for the implementation of bidirectional attention. Additionally, configurations utilizing bidirectional attention paired with last token pooling are notably effective, excelling in both non-ICL and zero-shot scenarios. This configuration\u2019s performance is also pronounced in few-shot reranking tasks, highlighting its adaptability and potential applicability across various demands.\n# 4.5 PROMPT\nRecently, most LLM-based embedding models have incorporated instruction-based prompts on the query side. However, there has been limited investigation into the efficacy of utilizing prompts on the passage side. To address this gap, our study introduces and explores the use of prompts on the passage side. The specific prompt employed in our study is as follows:\n# {passage} Summarize the above passage:\nTable 6 presents the results obtained using passage prompts. The results demonstrate that the integration of passage prompts leads to a significant decline in performance across all tasks except\n(6)\nTask\nRetr.\nRerank.\nClust.\nPairClass.\nClass.\nSTS\nSumm.\nAvg.\n# of datasets \u2192\n15\n4\n11\n3\n12\n10\n1\n56\nw/o passage prompt (zero-shot)\n59.59\n56.85\n42.61\n87.87\n75.47\n83.30\n29.52\n64.67\nw/o passage prompt (few-shot)\n60.08\n56.67\n46.55\n88.51\n77.31\n83.69\n30.68\n66.08\nw/ passage prompt (zero-shot)\n59.50\n46.84\n39.57\n81.25\n71.41\n80.38\n30.26\n61.61\nw/ passage prompt (few-shot)\n59.93\n46.39\n39.40\n82.25\n72.00\n79.81\n30.97\n61.74\nretrieval. This indicates that further exploration and experimentation are needed when employing prompts at the passage level.\n# 5 CONCLUSION\nIn this paper, we explore the utilization of in-context learning (ICL) derived from large language models (LLMs) for generating text embeddings and investigate various methods of LLMs as embedding models. Specifically, we examine the integration of attention mechanisms, different pooling methods, and passage prompts. We advocate for maintaining the model\u2019s original architecture while embedding in-context learning capabilities into the dense retrieval process. Our approach necessitates no modifications to the model\u2019s architecture; instead, it involves altering the prompt on the query side to include in-context learning features in the embedding generation task. Despite its simplicity, our method proves highly effective on the MTEB and AIR-Bench benchmarks.\n# REFERENCES\nC.J. Adams, Daniel Borkan, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and Nithum Thain. Jigsaw unintended bias in toxicity classification, 2019. URL https://kaggle.com/ competitions/jigsaw-unintended-bias-in-toxicity-classification. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. in* sem 2012: The first joint conference on lexical and computational semantics\u2013volume 1: Proceedings of the main conference and the shared task, and volume 2: Proceedings of the sixth international workshop on semantic evaluation (semeval 2012). Association for Computational Linguistics. URL http://www. aclweb. org/anthology/S12-1051, 2012. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. I\u02dcnigo Casanueva, Tadas Tem\u02c7cinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u00b4c. Efficient intent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807, 2020. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. Xi Chen, Ali Zeynali, Chico Q Camargo, Fabian Fl\u00a8ock, Devin Gaffney, Przemyslaw A Grabowicz, Scott A Hale, David Jurgens, and Mattia Samory. Semeval-2022 task 8: Multilingual news article similarity. 2022. Mathieu Ciancone, Imene Kerboua, Marion Schaeffer, and Wissam Siblini. Mteb-french: Resources for french sentence embedding evaluation and analysis. arXiv preprint arXiv:2405.20468, 2024.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180, 2020.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180, 2020. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00b4an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440\u20138451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/ 2020.acl-main.747. DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. Quora question pairs, 2017. URL https://kaggle.com/competitions/quora-question-pairs. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur C\u00b8 elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay\nMenon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm\u00b4an, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, V\u00b4\u0131tor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. Ken Lang. Newsweeder: Learning to filter netnews. In Machine learning proceedings 1995, pp. 331\u2013339. Elsevier, 1995. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024a. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, et al. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024b. Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, and Defu Lian. Llama2vec: Unsupervised adaptation of large language models for dense retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3490\u2013 3500, 2024. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark. arXiv preprint arXiv:2008.09335, 2020. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang Zhai. Linkso: a dataset for learning to retrieve similar question answer pairs on software development forums. In Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering, pp. 2\u20135, 2018. Yinhan Liu. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu, Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang, Luxi Xing, and Ping Yang. Multi-cpr: A multi domain chinese dataset for passage retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201922, pp. 3046\u20133056, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450387323. doi: 10.1145/3477495.3531736. URL https: //doi.org/10.1145/3477495.3531736. Wenhao Lu, Jian Jiao, and Ruofei Zhang. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp. 2645\u20132652, 2020. Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, and Kang Liu. Large language models as foundations for next-gen dense retrieval: A comprehensive empirical assessment. arXiv preprint arXiv:2408.12194, 2024. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319, 2023.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 142\u2013150, 2011. Wei Chen Maggie, Phil Culliton. Tweet sentiment extraction, 2020. URL https://kaggle. com/competitions/tweet-sentiment-extraction. Macedo Maia, Siegfried Handschuh, Andr\u00b4e Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www\u201918 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pp. 1941\u20131942, 2018. Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pp. 165\u2013172, 2013. Sepideh Mollanorozy, Marc Tanti, and Malvina Nissim. Cross-lingual transfer learning with {P}ersian. In Lisa Beinborn, Koustava Goswami, Saliha Murado uglu, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Edoardo M. Ponti, Ryan Cotterell, and Ekaterina Vylomova (eds.), Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pp. 89\u201395, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.sigtyp-1.9. URL https://aclanthology.org/2023.sigtyp-1.9. Niklas Muennighoff, Nouamane Tazi, Lo\u00a8\u0131c Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906, 2024. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human-generated machine reading comprehension dataset. 2016. James O\u2019Neill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. I wish i would have loved this one, but i didn\u2019t\u2013a multilingual dataset for counterfactual detection in product reviews. arXiv preprint arXiv:2104.06893, 2021. Rafa\u0142 Po\u00b4swiata, S\u0142awomir Dadas, and Micha\u0142 Pere\u0142kiewicz. Pl-mteb: Polish massive text embedding benchmark. arXiv preprint arXiv:2405.10138, 2024. Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, and Haifeng Wang. Dureader retrieval: A large-scale chinese benchmark for passage retrieval from web search engine. arXiv preprint arXiv:2203.10232, 2022. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. Carer: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 3687\u20133697, 2018. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741, 2022. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S Siegel, Michael Tang, et al. Bright: A realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883, 2024.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00b4eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00b4e, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024. Nandan Thakur, Nils Reimers, Andreas R\u00a8uckl\u00b4e, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 241\u2013251, 2018. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions. arXiv preprint arXiv:2403.15246, 2024. Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. arXiv preprint arXiv:2205.12035, 2022. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 641\u2013649, 2024. Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. T2ranking: A large-scale chinese benchmark for passage ranking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2681\u20132690, 2023. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00b4eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00b4e, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024. Nandan Thakur, Nils Reimers, Andreas R\u00a8uckl\u00b4e, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 241\u2013251, 2018. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions. arXiv preprint arXiv:2403.15246, 2024. Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. arXiv preprint arXiv:2205.12035, 2022. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 641\u2013649, 2024. Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. T2ranking: A large-scale chinese benchmark for passage ranking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2681\u20132690, 2023. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\nLi Yudong, Zhang Yuqing, Zhao Zhe, Shen Linlin, Liu Weijie, Mao Weiquan, and Zhang Hui. Csl: A large-scale chinese scientific literature dataset. arXiv preprint arXiv:2209.05034, 2022. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval, 2024. Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: A multi-lingual benchmark for dense retrieval. arXiv preprint arXiv:2108.08787, 2021. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Miracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114\u20131131, 2023. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Promptreps: Prompting large language models to generate dense and sparse representations for zero-shot document retrieval. arXiv preprint arXiv:2404.18424, 2024.\n# A INSTRUCTION\nTask Name\nInstruction Template\nArguAna\nGiven a claim, find documents that refute the claim.\nClimateFEVER\nGiven a claim about climate change, retrieve documents that support or refute\nthe claim.\nCQADupStack\nGiven a question, retrieve detailed question descriptions from Stackexchange that are\nduplicates to the given question.\nDBPedia\nGiven a query, retrieve relevant entity descriptions from DBPedia.\nFEVER\nGiven a claim, retrieve documents that support or refute the claim.\nFiQA2018\nGiven a financial question, retrieve user replies that best answer the question.\nHotpotQA\nGiven a multi-hop question, retrieve documents that can help answer the question.\nMSMARCO\nGiven a web search query, retrieve relevant passages that answer the query.\nNFCorpus\nGiven a question, retrieve relevant documents that best answer the question.\nNatural Question\nGiven a question, retrieve Wikipedia passages that answer the question.\nQuoraRetrieval\nGiven a question, retrieve questions that are semantically equivalent to the given\nquestion.\nSCIDOCS\nGiven a scientific paper title, retrieve paper abstracts that are cited by the given paper.\nSciFact\nGiven a scientific claim, retrieve documents that support or refute the claim.\nTouche2020\nGiven a question, retrieve detailed and persuasive arguments that answer the question.\nTREC-COVID\nGiven a query, retrieve documents that answer the query.\nSTS*\nRetrieve semantically similar text.\nSummEval\nGiven a news summary, retrieve other semantically similar summaries.\nAmazonCounterfactualClassification\nClassify a given Amazon customer review text as either counterfactual\nor not-counterfactual.\nAmazonPolarityClassification\nClassify Amazon reviews into positive or negative sentiment.\nAmazonReviewsClassification\nClassify the given Amazon review into its appropriate rating category.\nBanking77Classification\nGiven a online banking query, find the corresponding intents.\nEmotionClassification\nClassify the emotion expressed in the given Twitter message into one of the six\nemotions: anger, fear, joy, love, sadness, and surprise.\nImdbClassification\nClassify the sentiment expressed in the given movie review text from\nthe IMDB dataset.\nMassiveIntentClassification\nGiven a user utterance as query, find the user intents.\nMassiveScenarioClassification\nGiven a user utterance as query, find the user scenarios.\nMTOPDomainClassification\nClassify the intent domain of the given utterance in task-oriented conversation.\nMTOPIntentClassification\nClassify the intent of the given utterance in task-oriented conversation.\nToxicConversationsClassification\nClassify the given comments as either toxic or not toxic.\nTweetSentimentExtractionClassification\nClassify the sentiment of a given tweet as either positive, negative, or neutral.\nArxivClusteringP2P\nIdentify the main and secondary category of Arxiv papers based on the titles\nand abstracts.\nArxivClusteringS2S\nIdentify the main and secondary category of Arxiv papers based on the titles.\nBiorxivClusteringP2P\nIdentify the main category of Biorxiv papers based on the titles and abstracts.\nBiorxivClusteringS2S\nIdentify the main category of Biorxiv papers based on the titles.\nMedrxivClusteringP2P\nIdentify the main category of Medrxiv papers based on the titles and abstracts.\nMedrxivClusteringS2S\nIdentify the main category of Medrxiv papers based on the titles.\nRedditClustering\nIdentify the topic or theme of Reddit posts based on the titles.\nRedditClusteringP2P\nIdentify the topic or theme of Reddit posts based on the titles and posts.\nStackExchangeClustering\nIdentify the topic or theme of StackExchange posts based on the titles.\nStackExchangeClusteringP2P\nIdentify the topic or theme of StackExchange posts based on the given paragraphs.\nTwentyNewsgroupsClustering\nIdentify the topic or theme of the given news articles.\nAskUbuntuDupQuestions\nRetrieve duplicate questions from AskUbuntu forum.\nMindSmallReranking\nRetrieve relevant news articles based on user browsing history.\nSciDocsRR\nGiven a title of a scientific paper, retrieve the titles of other relevant papers.\nStackOverflowDupQuestions\nRetrieve duplicate questions from StackOverflow forum.\nSprintDuplicateQuestions\nRetrieve duplicate questions from Sprint forum.\nTwitterSemEval2015\nRetrieve tweets that are semantically similar to the given tweet.\nTwitterURLCorpus\nRetrieve tweets that are semantically similar to the given tweet.\nAIR-Bench\nGiven a question, retrieve passages that answer the question.\nTable 7: The instruction we used on the MTEB and AIR-Bench benchmarks.\nTable 7: The instruction we used on the MTEB and AIR-Bench benchmarks.\nDataset\nNV-Em\nbed-v1\nbge-multilin\ngual-gemma2\ngte-Qwen2-\n7B-instruct\nSFR-Embe\ndding-2 R\nstella en\n1.5B v5\nbge-en-icl\n(zero-shot)\nbge-en-icl\n(few-shot)\nArguAna\n68.21\n77.37\n64.27\n62.34\n65.27\n82.76\n83.08\nClimateFEVER\n34.72\n39.37\n45.88\n34.43\n46.11\n45.35\n45.43\nCQADupStack\n50.51\n47.94\n46.43\n46.11\n47.75\n47.23\n47.31\nDBPEDIA\n48.29\n51.37\n52.42\n51.21\n52.28\n50.42\n51.63\nFEVER\n87.77\n90.38\n95.11\n92.16\n94.83\n91.96\n92.83\nFiQA2018\n63.10\n60.04\n62.03\n61.77\n60.48\n58.77\n59.67\nHotpotQA\n79.92\n83.26\n73.08\n81.36\n76.67\n84.98\n85.14\nMSMARCO\n46.49\n45.71\n45.98\n42.18\n45.22\n46.72\n46.79\nNFCorpus\n38.04\n38.11\n40.60\n41.34\n42.00\n40.69\n41.85\nNatural Question\n71.22\n71.45\n67.00\n73.96\n71.80\n73.85\n73.88\nQuoraRetrieval\n89.21\n90.04\n90.09\n89.58\n90.03\n91.02\n90.95\nSCIDOCS\n20.19\n26.93\n28.91\n24.87\n26.64\n25.25\n25.26\nSciFact\n78.43\n72.05\n79.06\n85.91\n80.09\n78.33\n79.09\nTouche2020\n28.38\n30.26\n30.57\n28.18\n29.94\n29.67\n30.48\nTREC-COVID\n85.88\n64.27\n82.26\n87.28\n85.98\n78.11\n79.08\nBIOSSES\n85.59\n85.74\n81.37\n87.60\n83.11\n86.35\n86.47\nSICK-R\n82.80\n82.66\n79.28\n77.01\n82.89\n83.87\n83.87\nSTS12\n76.22\n77.71\n79.55\n75.67\n80.09\n77.73\n78.14\nSTS13\n86.30\n87.45\n88.83\n82.40\n89.68\n85.98\n86.59\nSTS14\n82.09\n83.48\n83.87\n79.93\n85.07\n82.34\n82.83\nSTS15\n87.24\n87.63\n88.54\n85.82\n89.39\n87.35\n87.77\nSTS16\n84.77\n86.70\n86.49\n84.50\n87.15\n86.54\n87.04\nSTS17\n87.42\n91.18\n88.73\n88.93\n91.35\n91.25\n91.25\nSTS22\n69.85\n69.02\n66.88\n67.10\n68.10\n68.08\n70.07\nSTSBenchmark\n86.14\n87.25\n86.85\n83.60\n88.23\n87.92\n88.42\nSummEval\n31.20\n31.20\n31.35\n30.71\n31.49\n30.75\n30.77\nSprintDuplicateQuestions\n95.94\n90.94\n92.82\n97.62\n96.04\n95.06\n97.23\nTwitterSemEval2015\n78.73\n79.64\n77.96\n78.57\n80.58\n78.54\n79.34\nTwitterURLCorpus\n86.05\n86.95\n86.59\n88.03\n87.58\n87.19\n87.84\nAmazonCounterfactual\n95.12\n89.48\n91.31\n92.72\n92.87\n92.88\n93.15\nAmazonPolarity\n97.14\n96.90\n97.50\n97.31\n97.16\n96.86\n96.98\nAmazonReviews\n55.47\n61.60\n62.56\n61.04\n59.36\n61.28\n61.46\nBanking77\n90.34\n92.53\n87.57\n90.02\n89.79\n91.42\n91.49\nEmotion\n91.71\n92.97\n79.45\n93.37\n84.29\n93.31\n93.36\nImdb\n97.06\n96.66\n96.75\n96.80\n96.66\n96.91\n96.91\nMassiveIntent\n80.07\n82.05\n85.41\n85.97\n85.83\n82.26\n82.93\nMassiveScenario\n81.74\n84.40\n89.77\n90.61\n90.20\n83.92\n85.60\nMTOPDomain\n96.51\n98.61\n99.04\n98.58\n99.01\n97.99\n98.42\nMTOPIntent\n89.77\n95.51\n91.88\n91.30\n92.78\n93.56\n94",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of generating high-quality text embeddings using large language models (LLMs) and their in-context learning (ICL) capabilities. Previous methods have primarily relied on bidirectional encoder architectures, which may not effectively generalize to unseen tasks. The introduction of decoder-only architectures has shown promise, but challenges remain in adapting these models for complex retrieval tasks without additional training.",
        "problem": {
            "definition": "The problem this paper aims to solve is the difficulty embedding models face in following unseen task instructions and executing complex retrieval tasks due to a mismatch between the narrow range of instructions encountered during training and the broader variety of real-world text embedding tasks.",
            "key obstacle": "The main difficulty associated with this problem is the limited ability of existing embedding models to adapt to new tasks dynamically, which restricts their applicability in various real-world scenarios."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is to leverage the ICL capabilities of LLMs by incorporating task-specific examples directly into input prompts, thereby enhancing the adaptability of text embeddings.",
            "opinion": "The proposed idea entails using a novel model, bge-en-icl, which integrates few-shot examples into the query side to produce high-quality text embeddings, making the model more relevant and generalizable across various contexts.",
            "innovation": "The primary innovation of this method lies in its simplicity and effectiveness; it retains the original architecture of LLMs while embedding ICL capabilities into the dense retrieval process, achieving state-of-the-art performance without complex modifications."
        },
        "method": {
            "method name": "bge-en-icl",
            "method abbreviation": "BGE-ICL",
            "method definition": "bge-en-icl is a model that enhances text embedding generation by incorporating few-shot examples into the query prompts, thereby utilizing the in-context learning capabilities of large language models.",
            "method description": "The core of the method involves appending task-specific examples to the input queries, allowing the model to generate embeddings that are contextually relevant.",
            "method steps": "1. Construct example templates with task definitions and query-passage pairs. 2. Encode modified queries and passages using the LLM. 3. Apply a contrastive loss function for training. 4. Evaluate the model's performance on benchmarks.",
            "principle": "This method is effective in solving the problem because it allows the model to generalize better to new tasks by learning from the provided examples, thus enhancing its adaptability and performance in diverse scenarios."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved using the MTEB and AIR-Bench benchmarks to assess the performance of the bge-en-icl model against various baseline methods, utilizing a range of public datasets for training.",
            "evaluation method": "The evaluation method included zero-shot and few-shot learning scenarios, where the model's performance was measured using standard metrics such as accuracy and F1 score across different tasks."
        },
        "conclusion": "The experiments demonstrated that the bge-en-icl model achieves state-of-the-art results on the MTEB and AIR-Bench benchmarks, effectively leveraging ICL strategies to enhance text embedding generation while maintaining simplicity in its architecture.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to achieve significant improvements in text embedding performance by simply integrating ICL capabilities, which enhances the model's adaptability without the need for complex modifications.",
            "limitation": "A limitation of the method is that while it performs well in few-shot scenarios, its performance may not be as robust in zero-shot situations compared to models specifically fine-tuned for those tasks.",
            "future work": "Future work should explore further enhancements to the ICL strategy, such as optimizing the selection of task-specific examples and investigating the impact of different attention mechanisms on model performance."
        },
        "other info": {
            "repository": "The model, code, and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding.",
            "additional findings": {
                "finding1": "The best results were achieved using the original architecture of LLMs without complex modifications.",
                "finding2": "The integration of in-batch examples during training preserved the model's zero-shot capabilities while enhancing performance in few-shot scenarios."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of generating high-quality text embeddings using large language models (LLMs) and their in-context learning (ICL) capabilities."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea entails using a novel model, bge-en-icl, which integrates few-shot examples into the query side to produce high-quality text embeddings, making the model more relevant and generalizable across various contexts."
        },
        {
            "section number": "3.1",
            "key information": "The main difficulty associated with this problem is the limited ability of existing embedding models to adapt to new tasks dynamically, which restricts their applicability in various real-world scenarios."
        },
        {
            "section number": "3.2",
            "key information": "The primary innovation of this method lies in its simplicity and effectiveness; it retains the original architecture of LLMs while embedding ICL capabilities into the dense retrieval process."
        },
        {
            "section number": "4.1",
            "key information": "The key advantage of the proposed approach is its ability to achieve significant improvements in text embedding performance by simply integrating ICL capabilities."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the method is that while it performs well in few-shot scenarios, its performance may not be as robust in zero-shot situations compared to models specifically fine-tuned for those tasks."
        }
    ],
    "similarity_score": 0.7094343395223897,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Making Text Embedders Few-Shot Learners.json"
}