{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.08008",
    "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
    "bib_name": "sivarajkumar2023empiricalevaluationpromptingstrategies",
    "md_text": "# An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing  \n# Sonish Sivarajkumar1, Mark Kelley2, Alyssa Samolyk-Mazzanti2, Shyam Visweswaran, MD, PhD1,3,  Yanshan Wang, PhD1,2,3* \n1Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA   2Department of Health Information Management, University of Pittsburgh, Pittsburgh, PA   3Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA  \nLarge language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP),  especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the  clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform  specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an  art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering  approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering  for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference  Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed  in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and  introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the  performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted  zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for  LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of  different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will  inspire and inform future research in this area. \nClinical information extraction (IE) is the task of identifying and extracting relevant information from clinical narratives, such as clinical notes, radiology reports, or pathology reports. Clinical IE has many applications in healthcare, such as improving diagnosis, treatment, and decision making, facilitating clinical research, and enhancing patient care(1, 2). However, clinical IE faces several challenges, such as the scarcity and heterogeneity of annotated data, the complexity and variability of clinical language, and the need for domain knowledge and expertise. \nZero-shot IE is a promising paradigm that aims to overcome these challenges by leveraging large pre-trained language models (LMs) that can perform IE tasks without any task-specific training data(3). In-context learning is a framework for zero-shot and few-shot learning, where a large pre-trained LM takes a context, and directly decodes the output without any  retraining or fine-tuning(4). In-context learning relies on prompt engineering, which is the process of crafting informative and contextually relevant instructions or queries as inputs to language models to guide their output for specific tasks(5). The utility of prompt engineering lies in its ability to leverage the powerful capabilities of LLMS, such as GPT-3.5(6), LLAMA-2(7), even in scenarios where limited or no task-specific training data is available. In clinical NLP, where labeled datasets tend to be scarce, expensive and time-consuming to create, splintered across institutions, and constrained by data use agreements, prompt engineering becomes even more crucial to unlock the  potential of state-of-the-art language models for clinical NLP tasks. \nWhile prompt engineering has been widely explored for general NLP tasks, its application and impact in clinical NL remain relatively unexplored. Most of the existing literature on prompt engineering in the healthcare domain focuse on biomedical NLP tasks, rather than clinical NLP tasks that involve processing real-world clinical notes. For instance\nChen et al.(8) used a fixed template as the prompt to measure the performance of LLMs on biomedical NLP tasks, but  did not investigate different kinds of prompting methods. Wang et al.(9) gave a comprehensive survey of prompt  engineering for healthcare NLP applications such as question-answering systems, text summarization, and machine  translation. However, they did not compare and evaluate different types of prompts for specific clinical NLP tasks and  how the performance varies across different LLMs. There is a lack of systematic and comprehensive studies on how  to engineer prompts for clinical NLP tasks, and the existing literature predominantly focuses on general NLP  problems. This creates a notable gap in the research, warranting a dedicated investigation into the design and  development of effective prompts specifically for clinical NLP. Currently, researchers in the field lack a  comprehensive understanding of the types of prompts that exist, their relative effectiveness, and the challenges  associated with their implementation in clinical settings.    The main research question and objectives of our study are to investigate how to engineer prompts for clinical NLP  tasks, identify best practices, and address the challenges in this emerging field. By doing so, we aim to propose a  guideline for future prompt-based clinical NLP studies. In this work, we present a comprehensive empirical evaluation  study on prompt engineering for five diverse clinical NLP tasks, namely Clinical Sense Disambiguation, Biomedical  Evidence Extraction, Coreference Resolution,Medication Status Extraction, and Medication Attribute Extraction (10,  11). By systematically evaluating different types of prompts proposed in recent literature, including prefix (12),  cloze(13), chain of thought (14), and anticipatory prompts(15), we gain insights into their performance and suitability  for each task.We also introduce two new types of prompting approaches: 1) Heuristic prompts and 2) Ensemble.The  rationale behind these novel prompts is to leverage the existing knowledge and expertise in rule-based NLP, which  has been prominent and has shown significant results in the clinical (16) We hypothesize that heuristic prompts, which  are based on rules derived from domain knowledge and linguistic patterns, can capture the salient features and  constraints of the clinical IE tasks. We also conjecture that ensemble prompts, which are composed of multiple types  of prompts, can benefit from the complementary strengths and mitigate the weaknesses of each individual prompt.     One of the key aspects of prompt engineering is the number of examples or shots that are provided to the model along  with the prompt. Few-shot prompting is a technique that provides the model with a few examples of input-output  pairs, while zero-shot prompting does not provide any examples(3, 17). By contrasting these strategies, we aim to  shed light on the most efficient and effective ways to leverage prompt engineering in clinical NLP. Finally, we propose  a prompt engineering framework to build and deploy zero-shot NLP models for clinical domain. Our study covers  three state-of-the-art language models, including GPT-3.5, BARD (Chatbot by Google based on PALM-2), and  LLAMA 2, to assess the generalizability of the findings across various models. Our work yields novel insights and  guidelines for prompt engineering specifically for clinical NLP tasks.  \n2. Methods  2.1 Tasks  We selected five distinct clinical NLP tasks representing diverse categories of natural language understanding: Clinica Sense Disambiguation (text classification)(18), Biomedical Evidence Extraction (NER)(19), Coreferenc Resolution(20),Medication Status Extraction (NER + classification)(21), and Medication Attribute Extraction (NER + RE)(22). Table 1 provides a succinct overview of each task, an example scenario, and the corresponding promp type employed for each task.  \nWe selected five distinct clinical NLP tasks representing diverse categories of natural language understanding: Clinical  Sense Disambiguation (text classification)(18), Biomedical Evidence Extraction (NER)(19), Coreference  Resolution(20),Medication Status Extraction (NER + classification)(21), and Medication Attribute Extraction (NER  + RE)(22). Table 1 provides a succinct overview of each task, an example scenario, and the corresponding prompt  type employed for each task.  \n<div style=\"text-align: center;\">Table 1: Task Descriptions</div>\nTable 1: Task Descriptions \nTask \nNLP Task \nCategory \nDescription \nDataset \nExample \nExample \nPrompt \nClinical Sense \nDisambiguation \nText \nClassification \nThis task \ninvolves \nidentifying the \ncorrect \nmeaning of \nclinical \nabbreviations \nwithin a given \ncontext. \nThe \nabbreviation \n\"CR\" can refer \nto \"cardiac \nresuscitation\" or \n\"computed \nradiography\". \nWhat is the \nmeaning of the \nabbreviation CR \nin the context of \ncardiology? \nBiomedical \nEvidence \nExtraction \nText \nExtraction \nIn this task, \ninterventions \nare  extracted \nfrom \nbiomedical \nabstracts. \nIdentifying \npanic, \navoidance, and \nagoraphobia \n(psychological \ninterventions) \nIdentify the \npsychological \ninterventions in \nthe given text? \nCoreference \nResolution \nCoreference \nResolution \nThe goal here \nis to identify all \nmentions in \nclinical text \nthat refer to the \nsame entity. \nResolving \nreferences to \n\"the patient\" or \n\"the study\" \nwithin a clinical \ntrial report. \nIdentify the \nantecedent for \nthe patient in \nthe clinical note. \nMedication \nStatus Extraction \nNamed Entity \nRecognition + \nclassification \nThis task \ninvolves \nidentifying \nwhether a \nmedication is \ncurrently being \ntaken, not \ntaken, or \nunknown. \nIdentifying that \na patient is \ncurrently taking \ninsulin for \ndiabetes. \nWhat is the \ncurrent status of \n[medication \nname] in the \ntreatment of \n[medical \ncondition]? \nMedication \nAttribute \nExtraction \nNamed Entity \nRecognition + \nRelation \nExtraction \nThe objective \nhere is to \nidentify \nspecific \nattributes of a \nmedication, \nsuch as dosage \nand frequency. \nIdentifying \ndosage, \nfrequency, and \nroute of a \nmedication for a \npatient. \nWhat is the \nrecommended \ndosage of \n[medication \nname] for \n[patient \npopulation] and \nhow often? \n \n2.2 Datasets and Evaluation  The prompts were evaluated on three language models, GPT-3.5, BARD(which is a chatbot based on PALM-2) LLAMA2, under both zero-shot and few-shot prompting conditions, employing precise experimental settings and parameters. To simplify the evaluation process and facilitate clear comparisons, we adopted accuracy as the sole evaluation metric for all tasks. Accuracy is defined as the proportion of correct outputs generated by the language model for each task, using a resolver that maps the output to the label space.  Table 3 shows the datasets and evaluation metrics for each clinical NLP task. The datasets are:  \u2022  Clinical Abbreviation Sense Inventories(CASI): This is a dataset of clinical abbreviations, senses, and instances(23).It contains 41 acronyms from 18,164 notes, along with their expanded forms and contexts. We used a randomly sampled subset from this dataset for clinical sense disambiguation, coreference resolution medication status extraction, and medication attribute extraction tasks(Table 2).  \u2022  EBM-NLP: This is a dataset of evidence-based medicine (EBM) annotations for NLP (24). It contains 187 abstracts and 20 annotated abstracts, with interventions extracted from the text. We used this dataset for biomedical evidence extraction task. \n<div style=\"text-align: center;\">Table 2: Evaluation Datasets, and Samples for Different Tasks</div>\nTable 2: Evaluation Datasets, and Samples for Different Tasks \nTask \nDataset \nSamples \nMetric \nClinical Sense \nDisambiguation \nCASI  \n11 acronyms from \n55 notes \nAccuracy: proportion of \ncorrect senses extracted \nBiomedical Evidence \nExtraction \nEBM-NLP  \n187 abstracts and \n20 annotated \nabstracts \nAccuracy: proportion of \ncorrect entities or relations \nextracted \nCoreference \nResolution \nCASI  \n105 annotated \nexamples \nAccuracy: proportion of \ncorrect coreference \nresolutions \nMedication Status \nExtraction \nCASI  \n105 annotated \nexamples with 340 \nmedication-status \npairs \nAccuracy: proportion of \ncorrect status extracted for \neach medication \nMedication Attribute \nExtraction \nCASI  \n105 annotated \nexamples with 313 \nmedications and \n533 attributes \nAccuracy: proportion of \ncorrect attribute extracted \nfor each medication \n \n2.2 Prompt Creation Process  We followed a rigorous process to create suitable prompts for each task. These prompts were carefully crafted to match the specific context and objectives of each task. There is no established method for prompt design and selection as of now. Therefore, we adopted an iterative approach where prompts, which are created by healthcare experts, go through a verification and improvement process in an iterative cycle, which involved design, experimentation, and evaluation, as depicted in Figure 2. \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aea1/aea1514a-f369-4aff-be97-6473cc6392c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 2: Iterative Prompt Design Process: A schematic diagram of the iterative prompt creation process for clinical  NLP tasks. The process consists of three steps: sampling, prompt designing, and deployment. The sampling step  involves defining the task and collecting data and annotations. The prompt designing step involves creating and  refining prompts using different types and language models. The deployment step involves selection of the best  model and deploying the  model for clinical use. \nFigure 2 illustrates the three main steps of our prompt creation process: sampling, prompt designing, and deployment.  In the sampling step(step 1), we defined the clinical NLP task (e.g., NER, RE, text classification, etc.) and collected a  sample of data and annotations as an evaluation for the task. In the prompt designing step(step 2), we designed a  prompt for the task using one of the prompt types (e.g., simple prefix prompt, simple cloze prompt, heuristic prompt,  chain of thought prompt, question prompt, anticipatory prompt, etc.). We also optionally performed few-shot  prompting by providing some examples along with the prompt. We then configured the LLMs and the evaluation  metrics for the experiment setup. We ran experiments with various prompt types and language models and evaluated  their performance on the task. Based on the results, we refined or modified the prompt design until we achieved  satisfactory performance or reached a limit. In the deployment step(step 3), we the best prompt-based models based  on their performance metrics and deployed the model for the corresponding task.  \n2.3 Prompt Engineering Techniques  Prompt engineering is the process of designing and creating prompts that elicit desired responses from language  models. Prompts can be categorized into different types based on their structure, function, and complexity.   Each prompt consists of a natural language query that is designed to elicit a specific response from the pre-trained language model. The prompts are categorized into seven types, as illustrated in Figure 3. Prefix prompts are the simplest type of prompts, which prepend a word or phrase indicating type/format/tone of response, for control and relevance. Cloze prompts are based on the idea of fill-in-the-blank exercises, which create a masked token in the input text and ask the language model to predict the missing word or phrase. Anticipatory prompts are the prompts anticipating the next question/command based on experience or knowledge, guiding conversation.  \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c3e8/c3e83afd-3bd7-4932-b7bf-6164338b3591.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Types of prompts: Examples of six types of prompts that we used to query the pre-trained language model  or different clinical IE tasks. [X]: context , [Y]: Abbreviation, [Z]: Expanded Form.(All prompts have been </div>\n<div style=\"text-align: center;\">Figure 3: Types of prompts: Examples of six types of prompts that we used to query the pre-trained language mod for different clinical IE tasks. [X]: context , [Y]: Abbreviation, [Z]: Expanded Form.(All prompts have been  included in Appendix A.) </div>\nIn addition to the existing types of prompts, we also designed two new novel prompts: heuristic prompts and ensemble  prompts. Heuristic prompts are rule-based prompts that break down complex queries into smaller parts for  comprehensive answers. They use a set of predefined rules to guide the language model to expand the abbreviations  in a given context. For example, a heuristic prompt can use the rule that an abbreviation is usually capitalized, followed  by a period, and preceded by an article or a noun. Unlike chain of thought prompts which explain the reasoning or  logic behind the output,  heuristic prompts use a set of predefined rules to guide the language model to perform a task.    Ensemble prompts are prompts that combine multiple prompts using majority voting for aggregated outputs. They  use different types of prompts to generate multiple outputs for the same input, and then select the most common output  as the final answer. For example, an ensemble prompt can use three different prefix prompts or combination of all  other types of prompts to generate three possible expansions for an abbreviation, and then choose the one that appears  most frequently. For simplicity, we combined all the five different prompt outputs using a majority voting  approach.The intuition behind an ensemble prompt is that by combining multiple types of prompts, we can leverage  the strengths and compensate for the weaknesses of each individual prompt. For example, some prompts may be more \neffective for certain tasks or certain models than others, or some prompts may be more robust to noise or ambiguity than others. By using majority voting, we can select the output that is most likely to be correct or coherent among the outputs generated by different types of prompts.  \n3. Results  In this section, we present the results of our experiments on prompt engineering for zero-shot clinical information  extraction. We evaluated various prompt types across five clinical NLP tasks, aiming to understand how different  prompts influence the accuracy of different language models. We also compared zero-shot and few-shot prompting  strategies, exploring how the addition of context affects the model performance. Furthermore, we tested an ensemble  approach that combines the outputs of different prompt types using majority voting. Finally, we analyzed the impact  of different language models on the task performance and observed some interesting patterns. Table 3 illustrates that  different prompt types have different levels of effectiveness for different tasks and language models. We can also  observe some general trends across the tasks and models. \n<div style=\"text-align: center;\">3.1 Prompt Optimization and Evaluation  Table 3: Performance Comparison of Different Prompt Types and Language Models</div>\n<div style=\"text-align: center;\">3.1 Prompt Optimization and Evaluation  Table 3: Performance Comparison of Different Prompt Types and Language Models</div>\n<div style=\"text-align: center;\">able 3: Performance Comparison of Different Prompt Types and Language Models</div>\n \nTask \nLanguage  \nModel \nSimple \nPrefix \nSimple \nCloze \nAnticip- \natory  \nHeuristic \nChain of  \nThought \nEnsemble \nFew-shot \n  \nClinical Sense \nDisambiguation \n  \nGPT 3.5 \n0.88 \n0.86 \n0.88 \n0.96 \n0.9 \n0.9 \n0.82 \nBARD \n0.76 \n0.68 \n0.71 \n0.75 \n0.72 \n0.71 \n0.67 \nLLAMA2 \n0.88 \n0.76 \n0.82 \n0.82 \n0.78 \n0.82 \n0.78 \n  \nBiomedical \nEvidence \nextraction \n  \nGPT 3.5 \n0.92 \n0.82 \n0.88 \n0.94 \n0.94 \n0.88 \n0.96 \nBARD \n0.89 \n0.89 \n0.91 \n0.9 \n0.91 \n0.9 \n0.88 \nLLAMA2 \n0.85 \n0.88 \n0.87 \n0.88 \n0.87 \n0.88 \n0.86 \n  \nCoreference \nResolution \n  \nGPT 3.5 \n0.78 \n0.6 \n0.74 \n0.94 \n0.94 \n0.74 \n0.74 \nBARD \n0.69 \n0.81 \n0.73 \n0.67 \n0.71 \n0.69 \n0.7 \nLLAMA2 \n0.8 \n0.64 \n0.74 \n0.76 \n0.8 \n0.78 \n0.68 \n  \nMedical Status \nExtraction \n  \nGPT 3.5 \n0.76 \n0.72 \n0.75 \n0.74 \n0.73 \n0.75 \n0.72 \nBARD \n0.67 \n0.51 \n0.65 \n0.55 \n0.59 \n0.58 \n0.55 \nLLAMA2 \n0.58 \n0.48 \n0.52 \n0.64 \n0.52 \n0.58 \n0.42 \n  \nMedical Attribute \nExtraction \n  \nGPT 3.5 \n0.88 \n0.84 \n0.9 \n0.96 \n0.96 \n0.9 \n0.96 \nBARD \n0.68 \n0.72 \n0.88 \n0.7 \n0.74 \n0.76 \n0.88 \nLLAMA2 \n0.6 \n0.66 \n0.58 \n0.66 \n0.72 \n0.64 \n0.6 \nFor clinical sense disambiguation, the heuristic and prefix prompts consistently achieved the highest performance  across all language models. This indicates that these prompts were able to provide enough guidance and context for  the model to disambiguate the word in the clinical note. Among the language models, GPT 3.5 excelled with this  prompt type, achieving an accuracy of 0.96. For biomedical evidence extraction, the heuristic and chain-of-thought  prompts excelled across all language models in zero-shot setting. This indicates that these prompt types were able to  provide enough information and constraints for the model to extract the evidence from the clinical note. GPT 3.5  achieved an accuracy of 0.94 with these prompt types, which was higher than any other model or prompt type  combination. For coreference resolution, the chain of thought prompt type performed best among all prompt types  with two language models-GPT3.5 and LLAMA2. This indicates that this prompt type was able to provide enough  structure and logic for the model to resolve the coreference in the clinical note. GPT 3.5 displayed high accuracy with  this prompt type, achieving an accuracy of 0.94. For medication status extraction, simple prefix and heuristic prompts  yielded good results across all language models. These prompt types were able to provide enough introduction or rules  for the model to extract the status of the medication in relation to the patient or condition. GPT 3.5 excelled with these \nprompt types, achieving an accuracy of 0.76 and 0.74 respectively. For medication attribute extraction, we found that  the chain of thought and heuristic prompts were effective across all language models. These prompt types were able  to provide enough reasoning or rules for the model to extract and label the attributes of medications from clinical  notes. Anticipatory prompts, however, had the best accuracy for BARD among all the prompts. GPT-3.5 achieved an  accuracy of 0.96 with these prompt types, which was higher than any other model or prompt type combination.     Thus, we can see that task-specific prompt tailoring is crucial for achieving high accuracy. Different tasks require  different levels of information and constraints to guide the language model to produce the desired output. The  experiments show that heuristic, prefix and chain of thought prompts are generally very effective for guiding the  language model to produce clear and unambiguous outputs. We have also shown that GPT 3.5 is a superior and  versatile language model that can handle various clinical NLP tasks in zero-shot settings, outperforming other models  in most cases. \n# 3.2 Zero-shot vs Few-shot Prompting \nFrom these results, we can infer that LLMs can be effectively used for clinical NLP in a no-data scenario, where we  don\u2019t have much publicly available datasets, by using appropriate zero-shot prompt types that guide the language  model to produce clear and unambiguous outputs. However, few-shot prompting can also improve the performance  of LLMs by providing some context that helps the language model to handle complex scenarios. \n# 3.3 Other Observations Ensemble Approaches \nWe experimented with an ensemble approach by combining outputs from multiple prompts using majority voting.  The ensemble approach was not the best performing strategy for any of the tasks, but it was better than the low  performing prompts. The ensemble approach was able to benefit from the diversity and complementarity of different prompt types, and avoid some of the pitfalls of individual prompts. For example, for clinical sense disambiguation,  the ensemble approach achieved an accuracy of 0.9 with GPT 3.5, which was the second best performing prompt  type. Similarly, for medical attribute extraction, the ensemble approach achieved an accuracy of 0.9 with GPT 3.5  and 0.76 with BARD, which were close to the best single prompt type (anticipatory). However, the ensemble  approach also suffered from some drawbacks, such as inconsistency and noise. For tasks that required more specific  or consistent outputs, such as coreference resolution, the ensemble approach did not improve the accuracy over the  best single prompt type, and sometimes even decreased it. This suggests that the ensemble approach may introduce  ambiguity for tasks that require more precise or coherent outputs. \n# Impact of Language Models \nWe observed variations in performance among different language models(Table 3). We found that GPT 3.5 generally outperformed BARD and LLAMA2 on most tasks. This suggests that GPT 3.5 has a better generalization ability and can handle a variety of clinical NLP tasks with different prompt types. However, BARD and LLAMA2 also showed some advantages over GPT 3.5 on certain tasks and prompt types. For example, BARD achieved the highest accuracy of 0.81 with simple cloze prompts and LLAMA2 achieved the highest accuracy of 0.8 with simple prefix prompts for\ncoreference resolution. This indicates that BARD and LLAMA2 may have some domain-specific knowledge that ca benefit certain clinical NLP tasks for specific prompt types. \nPersona Patterns  Persona patterns are a way of asking the language model to act like a persona or a system that is relevant to the task  or domain. For example, one can ask the language model to \u201cact as a clinical NLP expert\u201d. This can help the language  model to generate outputs that are more appropriate and consistent with the persona or system.  For example, one can  use the following prompt for clinical sense disambiguation:  Act as a clinical NLP expert. Disambiguate the word \u201ccold\u201d in the following sentence: \u201cShe had a cold for three days.\u201d  We experimented with persona patterns for different tasks and language models, and found that they can improve the  accuracy and quality of the outputs. Persona patterns can help the language model to focus on the relevant information  and constraints for the task, and avoid generating outputs that are irrelevant or contradictory to the persona or system.    Randomness in Output: \nPersona Patterns  Persona patterns are a way of asking the language model to act like a persona or a system that is relevant to the task  or domain. For example, one can ask the language model to \u201cact as a clinical NLP expert\u201d. This can help the language  model to generate outputs that are more appropriate and consistent with the persona or system.  For example, one can  use the following prompt for clinical sense disambiguation:  Act as a clinical NLP expert. Disambiguate the word \u201ccold\u201d in the following sentence: \u201cShe had a cold for three days.\u201d  We experimented with persona patterns for different tasks and language models, and found that they can improve the  accuracy and quality of the outputs. Persona patterns can help the language model to focus on the relevant information  and constraints for the task, and avoid generating outputs that are irrelevant or contradictory to the persona or system.    Randomness in Output:  Most LLMs do not produce the output in the same format every time. There is inherent randomness in the outputs the  language models produce. Hence, the prompts need to be specific in the way they are done for the task. Prompts are  powerful when they are specific and if we use them in the right way.  Randomness in output can be beneficial or detrimental for different tasks and scenarios. In clinical domain,  randomness can introduce noise and errors in the outputs, which can make them less accurate and reliable for the  users. For example, for tasks that involve extracting factual information, such as biomedical evidence extraction,  medical status extraction, etc., randomness can cause the language model to produce outputs that are inconsistent or  contradictory with the input or context. \nIn this section, we synthesize the main findings from our experiments and offer some practical advice for prompt engineering for zero-shot and few-shot clinical information extraction. We propose the following steps for selecting optimal prompts for different tasks and scenarios:  Define the goal and scope of the prompt.    \nThe first step is to identify the type of the clinical NLP task, which can be broadly categorized into three types: 1)  Classification, 2) Extraction, and 3) Resolution. Classification tasks involve assigning a label or category to a word,  phrase, or sentence in a clinical note, such as clinical sense disambiguation or medication status extraction. Extraction  tasks involve identifying and extracting relevant information from a clinical note, such as biomedical evidence  extraction or medication attribute extraction. Resolution tasks involve linking or matching entities or concepts in a  clinical note, such as coreference resolution.    The second step is to choose the prompt type that is most suitable for the task type. We found that different prompt  types have different strengths and weaknesses for different task types, depending on the level of information and  constraints they provide to the language model. Table 4a summarizes our findings and recommendations for optimal  prompt selection for each task type.   \n<div style=\"text-align: center;\">Table 4a: Optimal prompt types for different clinical NLP task types </div>\nTable 4a: Optimal prompt types for different clinical NLP task types \nTask Type \nPrompt Type \nClassification \nHeuristic or Prefix \nExtraction \nHeuristic of Chain-of-thought \nResolution \nChain-of-thought \n \nTable 4b: Optimal language models for different prompt types \nPrompt Type \nLanguage Model \nHeuristic \nGPT-3.5 \nPrefix \nGPT-3.5 or LLAMA2 \nCloze \nBARD or LLAMA2 \nChain of Thought \nGPT-3.5 \nAnticipatory \nBARD \n \nThe third step is to choose the language model that is most compatible with the chosen prompt type. We found that  different language models have different capabilities and limitations for different prompt types, depending on their  generalization ability and domain-specific knowledge. Table 4b summarizes our findings and recommendations for  optimal language model selection for each prompt type.    The fourth step is to evaluate the performance of the chosen prompt type and language model combination on the  clinical NLP task, using appropriate metrics such as accuracy, precision, recall, or F1-score. If the performance is  satisfactory, then the prompt engineering process is complete. If not, then the process can be repeated by choosing a  different prompt type or language model, or by modifying the existing prompt to improve its effectiveness. \nThe third step is to choose the language model that is most compatible with the chosen prompt type. We found that different language models have different capabilities and limitations for different prompt types, depending on their generalization ability and domain-specific knowledge. Table 4b summarizes our findings and recommendations for optimal language model selection for each prompt type. \nThe fourth step is to evaluate the performance of the chosen prompt type and language model combination on t clinical NLP task, using appropriate metrics such as accuracy, precision, recall, or F1-score. If the performance satisfactory, then the prompt engineering process is complete. If not, then the process can be repeated by choosing different prompt type or language model, or by modifying the existing prompt to improve its effectiveness. \n# 4. Discussion\nIn this paper, we have presented a novel approach to zero-shot and few-shot clinical information extraction using  prompt engineering. We have evaluated various prompt types across five clinical NLP tasks: clinical sense  disambiguation, biomedical evidence extraction, coreference resolution, medical status extraction, and medical  attribute extraction. We have also compared the performance of different language models: GPT 3.5, BARD, and  LLAMA2. Our main findings are as follows:  (1) Task-specific prompt tailoring is crucial for achieving high accuracy. Different tasks require different levels of  information and constraints to guide the language model to produce the desired output. Therefore, it is important  to design prompts that are relevant and specific to the task at hand, and avoid using generic or vague prompts that  may confuse the model or lead to erroneous outputs.  (2) Heuristic prompts are generally very effective for guiding the language model to produce clear and unambiguous  outputs. These prompts use a rule-based approach to generate sentences that contain definitions or examples of  the target words or concepts, which help the model to understand and match the context. Heuristic prompts are  especially useful for tasks that involve disambiguation, extraction, or classification of entities or relations.  (3) Chain of thought prompts are also effective for guiding the language model to produce logical and coherent  outputs. These prompts use a multi-step approach to generate sentences that contain a series of questions and  answers that resolve the task in the context. Chain of thought prompts are especially useful for tasks that involve  reasoning, inference, or coreference resolution.  (4) Few-shot prompting can improve the performance of language models by providing some context that helps the  model to handle complex scenarios. Few-shot prompting can be done by adding some examples or explanations  to the input, depending on the complexity and variability of the task. Few-shot prompting can enhance accuracy  by providing limited context that aids complex scenario understanding. The improvement is more pronounced  compared to simple prefix and cloze prompts, which had lower accuracy in most of the tasks.  (5) Ensemble approaches can also improve the performance of language models by combining outputs from multiple  prompts using majority voting. Ensemble approaches can leverage the strengths of each prompt type and reduce  the errors of individual prompts. Ensemble approaches are especially effective for tasks that require multiple types  of information or reasoning, such as biomedical evidence extraction and medical attribute extraction. \nIn this paper, we have benchmarked different prompt engineering techniques for zero-shot clinical NLP tasks. We  also conceptualized and proposed two new types of prompts, heuristic and ensemble prompts, we have demonstrated  that prompt engineering can enable the use of pre-trained language models for various clinical NLP tasks without  requiring any fine-tuning or additional data. We have shown that task-specific prompt tailoring, heuristic prompts,  chain of thought prompts, few-shot prompting, and ensemble approaches can improve the accuracy and quality of the  outputs. We have also shown that GPT 3.5 is very adaptable and precise across all tasks and prompt types, while  BARD and LLAMA2 may have some domain-specific advantages for certain tasks and prompt types.    We believe that prompt-based approach has several benefits over existing methods for clinical information extraction.  It reduces the cost and time of developing clinical NLP applications, as it does not require any labeled data or finetuning. It is flexible and adaptable, as it can be applied to various clinical NLP tasks with different prompt types and  language models. It is interpretable and explainable, as it uses natural language prompts that can be easily understood  and modified by humans. It is scalable and portable, as it can leverage the power of pre-trained language models that  are widely available and accessible. \n# 6. Limitations and Future Work:\n# 6. Limitations and Future Work: Limitations: \nOur approach also has some limitations that we acknowledge in this work. First, it relies on the quality and availability  of pre-trained language models, which may vary depending on the domain and task. As language models are rapidly  evolving, some parts of prompt engineering discipline may be timeless, while some parts may evolve and adapt over  time as different capabilities of models evolve. Second, it requires a lot of experimentation and iteration to optimize  prompts for different applications, which may be tedious and time-consuming. We may not have explored all the  possible combinations and variations of prompts that could potentially improve the performance of the clinical NLP  tasks. Third, the LLMs do not release the details of the dataset which they were trained on. Hence, the high accuracy  could be because the models would have already seen the data during training, and not because of the effectiveness of  the prompts.   \n#  Future Work:\n Future Work:  We plan to address these challenges and limitations in our future work. We aim to develop more systematic and  automated methods for prompt design and evaluation, such as using prompt-tuning or meta-learning techniques. We also aim to incorporate more domain knowledge or external resources into the prompts or the language models, such as using ontologies, knowledge graphs, or databases. We also aim to incorporate more quality control or error  correction mechanisms into the prompts or the language models, such as using adversarial examples, confidence  scores, or human feedback.  \n# Author Information\n# Contributions \nContributions   SS conceptualized, designed, and organized this study, analyzed the results, and wrote, reviewed, and revised the  paper. MK, AS analyzed the results, and wrote, reviewed, and revised the paper. SV wrote, reviewed, and revised  the paper.  Y.W. conceptualized, designed, and directed this study, wrote, reviewed, and revised the paper. \n# Corresponding author   Correspondence to Yanshan Wang.\nCompeting Interests   All authors declare no competing interests.\nData availability Not applicable. \nCode availability  Not applicable. \n# Acknowledgements\nThis work was supported by the National Institutes of Health under award number U24 TR004111 and R0 LM014306. The content is solely the responsibility of the authors and does not necessarily represent the official view of the National Institutes of Health. \n# References\n# Appendix A: Prompts for Clinical NLP Tasks \nIn the appendix, we present the prompts that we used for each of the five clinical NLP tasks and each of the five prompt types in our experiments(Table A1). The prompts are written in natural language and formatted according to the input specifications of each language model. The prompt types are: Prefix, Cloze,  Anticipatory, Heuristic, and Chain of Thought. We used seven types of prompts in our experiments, but two of them were not independent prompt types.  Ensemble prompt type was a combination of the outputs from all the other prompt types, using majority voting to decide the final output. Few-shot prompting  was a variation of the other prompt types, where we added two examples(2-shot) to the input to provide some context to the model. The clinical NLP tasks are:  Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction.  \n# Table A1: Prompts for each task and prompt type combination\n<div style=\"text-align: center;\">Table A1: Prompts for each task and prompt type combination </div>\n \nTas\nk \nPrefix  \nCloze \nAnticipatory \nHeuristic \nChain-of-thought \nClin\nical \nSens\ne \nDisa\nmbi\nguat\nion \nDetermine the \nmeaning to the \nabbreviation \n<abbreviation> in the \nclinical note. Clinical \nnote: \n \nThe abbreviation \n<abbreviation> \nstands for ___ in the \nclinical note. Clinical \nNote:  \n \n[Input 1:] what is \nclinical sense \ndisambiguation? \n[Input 2:] how to \ndetermine what a \nclinical \nabbreviation \nmeans in a given \ncontext? \n[Input 3:] Context: \n<text>, \nAbbreviation: \n<abbreviation>, \nWhat does the \nabbreviation mean \nin this context? \n \n[Input 1:] First, store these rules in \nmemory: \n1. If the terms \"hemoglobin,\" \"ekg,\" \nor \"valve\" appear in the text, \"ab\" \nlikely stands for \"atrioventricular\", \n\"avr\" likely stands for \"aortic valve \nreplacement\",\"av\" likely stands for \n\"atrioventricular\",\"cvp\" likely stands \nfor \"central venous pressure\",\"cvs\" \nlikely stands for cardiovascular \nsystem\",\"la\" likely stands for \"left \natrial\", and \"sa\" likely stands for \n\"sinuatrial\" \n2. If the text is discussing \ncardiology, cardiovascular issues, or \nthe hearts electrical system, \"ab\" \nlikely stands for \"atrioventricular\", \n\"avr\" likely stands for \"aortic valve \nreplacement\",\"av\" likely stands for \n\"atrioventricular\",\"cvp\" likely stands \nfor \"central venous pressure\",\"cvs\" \nlikely stands for cardiovascular \nsystem\",\"la\" likely stands for \"left \natrial\", and \"sa\" likely stands for \n\"sinuatrial\" \nEXAMPLE: Expand the \nabbreviation \"<abbreviation>\" in \nthe clinical note. Clinical Note:  \nANSWER: \"<abbreviation>\" \nstands for \"<expanded form>\" in \nthe text because the context is \nreferring to cardiology and the \nheart. \nQUESTION: Expand the \nabbreviation \"<abbreviation>\" in \nthe clinical note. Clinical Note. \n \n3. If the text is relating to the heart, \nheart disease, or any anatomy \nrelating to the heart, \"ab\" likely \nstands for \"atrioventricular\", \"avr\" \nlikely stands for \"aortic valve \nreplacement\",\"av\" likely stands for \n\"atrioventricular\",\"cvp\" likely stands \nfor \"central venous pressure\",\"cvs\" \nlikely stands for cardiovascular \nsystem\",\"la\" likely stands for \"left \natrial\", and \"sa\" likely stands for \n\"sinuatrial\" \n[Input 2:] Using the stored rules, \nexpand the abbreviation \n\"<abbreviation>\" in the text: \n\u201c<text>\" \n \nBio\nmed\nical \nEvid\nence \nExtr\nactio\nn \nLabel each token in \nthe clinical note that \nreferences an \nintervention.  If the \ntoken is an \nintervention, specify \nthe type of \nintervention as: \n1=surgical, \n2=physical, \n3=pharmacological,4\n=educational,5=psyc\nhological,6=other,7=\ncontrol \nFor each token in the \nclinical note below \nusing white space \ntokenization, the \ntoken relates to a __ \nintervention. Your \nchoices are: surgical, \nphysical, \npharmacological, \neducational, \npsychological, other \ntype, control, or not \nan intervention. \n \n[Input 1:] what is \nmedical \nintervention. \n[Input 2:] how to \nidentify medical \ninterventions: \n[Input 2:]   What \nintervention \ncategory does each \ntoken in the clinical \nnote belong to? \nSurgical (1), \nphysical (2), \npharmacological \n(3), educational (4), \npsychological (5), \nother (6), control \n(7), or none (0). Use \nwhitespace \ntokenization. \nClinical Note:  \n \n[Input 1:] First, store these rules in memory: \n \n \n[Input 2:] Rule 1: Review each token \nand decide if the token can be \nclassified as a \u201cclinical intervention,\u201d \n. Rule 2: If the token is not an \nintervention, assign \u201c0\u201d to the token, \nRule 3. If the token is an \nintervention, use the following sub-\ncategory to classify the type of \nintervention: 1 = surgical, 2 = \nphysical, 3 = pharmacological, 4 = \neducational, 5 = psychological, 6 = \nother, 7 = control. \n \n[Input 3:] Using the above rules, \nextract the medical interventions in \nthe clinical note. Clinical Note: \n \nEXAMPLE: Assign each token in the \nclinical note to an intervention \ncategory: surgical (1), physical (2), \npharmacological (3), educational (4), \npsychological (5), other (6), control \n(7), or none (0). Use whitespace \ntokenization. Clinical Note: \"Report \nfrom Pain Study . Patients with \nopioid use responded better to \ntreatment . Predicted outcome with \nmorphine or vicodin .\"  \nANSWER: Tokenized text using \nwhitespace tokenization: \"Report\" \n\"from\" \"Pain\" \"Study\" \".\" \"Patients\" \n\"with\" \"opioid\" \"use\" \"responded\" \n\"better\" \"to\" \"treatment\" \".\" \nPredicted\" \"outcome\" \"with\" \n\"morphine\" \"or\" \"vicodin\" \".\" A \nstudy is educational so \"Pain\" and \n\"Study\" will be assigned \"4.\" A \ntreatment and drug names are \npharmacological so \"treatment\" \n\"morphine\" and \"vicodin\" will be \nassigned \"3.\" \nQUESTION: Assign each token in \nthe clinical note to an intervention \ncategory: surgical (1), physical (2), \npharmacological (3), educational (4), \npsychological (5), other (6), control \n(7), or none (0). Use whitespace \ntokenization. Clinical Note: \n \nCore\nfere\nnce \nRes\noluti\non \nIdentify the \nantecedent for the \npronoun <pronoun> \nin the clinical note. \nClinical Note:  \nThe pronoun \n<pronoun> refers to \n___ in the clinical \nnote. Clinical Note:  \n[Input 1:] what is \ncoreference \nresolution? \n[Input 2:] what is \nan antecedent? \n[Input 3:] What is \nthe antecedent for \nthe pronoun \n<pronoun> in the \nclinical note? \nClinical Note: \n \n[Input 1:] First, store these rules in \nmemory.provided rules: identify the \npronoun mentioned in the task, \nlocate the pronoun in the text, \ndetermine the noun or noun phrase \nthat the pronoun refers to by \nconsidering the context.> \n \n[Input 2:] Identify the pronoun \n<pronoun> in the clinical text.  \n[Input 3:]identify the reference for \nthe pronoun in the clinical note. \nClinical Note:  \n \nEXAMPLE\u201d Identify the antecedent \nthe pronoun \"this\" refers to in the \nclinical note. Clinical Note: \"5. \nZosyn. This was stopped today, \n_%#MMDD2003#%_. 6. \nMultivitamin one tablet p.o. q.d. 7. \nLasix 80 mg p.o. q.d. 8. Paxil 40 mg \np.o. q.d. 9. Bactrim SS one tablet \np.o. Mondays and Thursdays. 10. \nRanitidine 150 mg p.o. b.i.d. 11. \nMetoprolol 25 mg p.o. b.i.d. 12. \nColace 100 mg p.o. b.i.d.\" \nANSWER: The pronoun \"this\" is \nreferring to \"Zosyn\" because it is the \nmost recent noun or noun phrase \nmentioned previously. \nQUESTION: identify the antecedent \nthe pronoun \"[pronoun]\" refers to in \nthe clinical note. Clinical Note:  \n \nMed\nicati\non \nStat\nus \nExtr\nactio\nn \n \nIn the clinical note, \nextract the \nmedication and it\u2019s \ncurrent status as \nactive, discontinued, \nor neither. Clinical \nnote: \nThe active \nmedications in the \ntext are ___, the \ndiscontinued \nmedications are ___, \nand the medications \nthat are neither active \nnor discontinued in \nthe clinical note are \n___. Clinical Note:  \n \n[Input 1:] how to \nidentify contextual \nlevel information in \na text? \n[Input 2:] For each \nmedication \nmentioned in the \ngiven clinical note, \nis the current status \n\u201cactive,\u201d \n\u201cdiscontinued,\u201d or \n\u201cneither?\u201d \n \n[Input 1:] First, store these rules in \nmemory: \n <10 rules> \n \nEXAMPLE: Label any \nmedications in the clinical note as \nactive, discontinued, or neither. \nText: \"2. Prior major leg infection. \n3. Penicillin allergy. PLANS: Add \nFortaz to vancomycin and DC \nclindamycin. Will also give one \ndose of tobramycin. If we do \nanother I&D or dressing change, it \nwould be of value to do swab \nculture of the wound to see what is \ncolonizing it superficially.\" \n[Input 2:] Given the clinical note, \nextract the medication and its \ncurrent status as active, \ndiscontinued, or neither, using the \nrules mentioned above. Clinical \nNote: \n \nANSWER: 1. Penicillin: Neither \n(doesn't specifically mention active \nor discontinued, if it's an allergy \nit's labeled as neither), 2. Fortaz: \nActive (states it was added to \nvancomycin, adding indicates it's \nan active medication), 3. \nVancomycin: Active (text states \nFortaz was added to Vancomycin, \nadding indicates it's an active \nmedication), 4. Clindamycin: \nDiscontinued (DC stands for \ndiscontinued). \nQUESTION: Using the stored \nexample, label any medications in \nthe text as active, discontinued, or \nneither. Text: <text> \n \nMed\nicati\non \nAttri\nbute \nExtr\nactio\nn \nExtract the \nmedications \nmentioned in the \nclinical note, and any \nattributes such as \nroute, frequency, \ndosage, duration, or \nreason for \nprescribing if \nmentioned. Clinical \nnote:  \nThe medications in \nthe clinical note are \n___. The name, \ndosage, frequency, \nand reason of each \nmedication is ___. \nClinical Note: \n[Input 1:] how to \nextract contextual \nlevel information in \na text such as status \nabout medications? \n[Input 2:] how to \nidentify specific \nmedication \nattributes, such as \nroute, frequency, or \nduration? \n[Input 3:] In the \ngiven clinical note, \nfor each medication \nmentioned, what \nare the attributes \nsuch as route, \nfrequency, dosage, \nduration, or reason \nfor prescription? \nClinical note: \n \n[Input 1:] Store these rules in \nmemory: <10 rules> \n \n[Input 2:] First extract the \nmedications mentioned in the \nclinical note. Second extract any \nattributes such as route, \nfrequency, dosage, duration, or \nreason for prescribing. \n \n \nEXAMPLE: Identify any \nmedications in the text and list the \nname and if applicable the dosage, \nfrequency, and reason. Text: \"The \npatient tells me that she was taking \n325 mg of aspirin per day for three \nyears for a transient ischemic attack \nbefore she had her AVR. Now she \nis off this and on Coumadin, \nalthough the AVR is said to be a \ntissue valve. She takes no other \nnonsteroidal anti-inflammatory \ndrugs except occasional Aleve in \nthe past.\" \nANSWER: 1. aspirin 325 mg per \nday for three years for a transient \nischemic attack, 2. Coumadin, 3. \nnonsteroidal anti-inflammatory \ndrugs, 4. Aleve occasional. \nQUESTION: Using the stored \nexample, identify any medications \nin the clinical note and list the \nname and if applicable the dosage, \nfrequency, and reason. Clinical \nNote:  \n \n \n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The clinical domain faces challenges due to the scarcity and heterogeneity of annotated data, the complexity of clinical language, and the need for domain expertise. Existing benchmarks do not adequately address these issues, necessitating a dedicated evaluation of prompt engineering for clinical NLP tasks.",
            "purpose of benchmark": "This benchmark aims to systematically evaluate various prompting strategies for large language models in clinical NLP tasks, focusing on improving model performance without task-specific training data."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of clinical information extraction (IE), which involves identifying and extracting relevant information from clinical narratives.",
            "key obstacle": "Existing benchmarks often lack a systematic approach to prompt engineering for clinical NLP tasks, resulting in ineffective evaluations and limited understanding of prompt effectiveness."
        },
        "idea": {
            "intuition": "The development of this benchmark was inspired by the need to evaluate the effectiveness of different prompting strategies in clinical NLP, particularly in a zero-shot context where labeled data is scarce.",
            "opinion": "The authors believe that effective prompt engineering is crucial for unlocking the potential of large language models in clinical NLP, and this benchmark will provide valuable insights for future research.",
            "innovation": "This benchmark introduces new prompting strategies, including heuristic prompting and ensemble prompting, which differ from traditional methods by leveraging domain knowledge and combining outputs from multiple prompts.",
            "benchmark abbreviation": "CLIP"
        },
        "dataset": {
            "source": "The dataset was created using a combination of real-world clinical notes and existing annotated datasets, such as Clinical Abbreviation Sense Inventories (CASI) and EBM-NLP.",
            "desc": "The dataset includes clinical narratives that cover various clinical NLP tasks, with a focus on both structured and unstructured data.",
            "content": "The dataset contains text data related to clinical notes, including abbreviations, interventions, and medication statuses.",
            "size": "18,164",
            "domain": "Clinical Information Extraction",
            "task format": "Named Entity Recognition"
        },
        "metrics": {
            "metric name": "Accuracy",
            "aspect": "Model performance in extracting relevant information from clinical narratives.",
            "principle": "Accuracy was chosen as it provides a straightforward measure of the proportion of correct outputs generated by the model for each task.",
            "procedure": "Model performance was evaluated by comparing generated outputs against a labeled dataset, calculating the proportion of correct predictions."
        },
        "experiments": {
            "model": "The benchmark evaluated three state-of-the-art language models: GPT-3.5, BARD, and LLAMA2.",
            "procedure": "Models were trained and tested under zero-shot and few-shot conditions, employing various prompt types to assess performance across multiple tasks.",
            "result": "The experiments showed that different prompt types significantly influenced model accuracy, with heuristic and chain-of-thought prompts performing particularly well.",
            "variability": "Variability was accounted for by conducting multiple trials and analyzing performance across different prompt types and model configurations."
        },
        "conclusion": "The benchmark demonstrates that tailored prompt engineering can enhance the performance of large language models in clinical NLP tasks, providing a framework for future research in the field.",
        "discussion": {
            "advantage": "The benchmark offers a systematic approach to evaluating prompt engineering in clinical NLP, highlighting the importance of task-specific prompts for achieving high accuracy.",
            "limitation": "The approach relies on the quality of pre-trained language models, which may vary, and the need for extensive experimentation to optimize prompts.",
            "future work": "Future research will focus on developing automated methods for prompt design, incorporating domain knowledge, and refining evaluation techniques."
        },
        "other info": {
            "acknowledgements": "This work was supported by the National Institutes of Health under award number U24 TR004111 and R0 LM014306."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The clinical domain faces challenges due to the scarcity and heterogeneity of annotated data, the complexity of clinical language, and the need for domain expertise."
        },
        {
            "section number": "1.2",
            "key information": "Effective prompt engineering is crucial for unlocking the potential of large language models in clinical NLP."
        },
        {
            "section number": "1.3",
            "key information": "This benchmark aims to systematically evaluate various prompting strategies for large language models in clinical NLP tasks."
        },
        {
            "section number": "1.4",
            "key information": "The benchmark introduces new prompting strategies, including heuristic prompting and ensemble prompting, which leverage domain knowledge."
        },
        {
            "section number": "2",
            "key information": "The benchmark addresses the problem of clinical information extraction (IE), which involves identifying and extracting relevant information from clinical narratives."
        },
        {
            "section number": "3.1",
            "key information": "Models were trained and tested under zero-shot and few-shot conditions, employing various prompt types to assess performance across multiple tasks."
        },
        {
            "section number": "6.1",
            "key information": "The approach relies on the quality of pre-trained language models, which may vary, impacting the effectiveness of prompt engineering."
        },
        {
            "section number": "7",
            "key information": "The benchmark demonstrates that tailored prompt engineering can enhance the performance of large language models in clinical NLP tasks."
        }
    ],
    "similarity_score": 0.7261634409174851,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing.json"
}