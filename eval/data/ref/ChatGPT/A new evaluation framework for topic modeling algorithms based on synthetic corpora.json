{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1901.09848",
    "title": "A new evaluation framework for topic modeling algorithms based on synthetic corpora",
    "abstract": "Topic models are in widespread use in natural language processing and beyond. Here, we propose a new framework for the evaluation of probabilistic topic modeling algorithms based on synthetic corpora containing an unambiguously defined ground truth topic structure. The major innovation of our approach is the ability to quantify the agreement between the planted and inferred topic structures by comparing the assigned topic labels at the level of the tokens. In experiments, our approach yields novel insights about the relative strengths of topic models as corpus characteristics vary, and the first evidence of an \u201cundetectable phase\u201d for topic models when the planted structure is weak. We also establish the practical relevance of the insights gained for synthetic corpora by predicting the performance of topic modeling algorithms in classification tasks in real-world corpora.",
    "bib_name": "shi2019newevaluationframeworktopic",
    "md_text": "# A new evaluation framework for topic modeling algorithms based on synthetic corpora\nHanyu Shi 1 Martin Gerlach1 Isabel Diersen1 Doug Downey2 Lu\u00b4\u0131s A. N. Amaral1,\u2217 1Department of Chemical and Biological Engineering, 2Department of Electrical Engineering and Computer Science Northwestern University Evanston, Illinois 60208, USA \u2217amaral@northwestern.edu\n28 Jan 2019\n  28 Jan 201\n# Abstract\nTopic models are in widespread use in natural language processing and beyond. Here, we propose a new framework for the evaluation of probabilistic topic modeling algorithms based on synthetic corpora containing an unambiguously defined ground truth topic structure. The major innovation of our approach is the ability to quantify the agreement between the planted and inferred topic structures by comparing the assigned topic labels at the level of the tokens. In experiments, our approach yields novel insights about the relative strengths of topic models as corpus characteristics vary, and the first evidence of an \u201cundetectable phase\u201d for topic models when the planted structure is weak. We also establish the practical relevance of the insights gained for synthetic corpora by predicting the performance of topic modeling algorithms in classification tasks in real-world corpora.\n[cs.CL]\narXiv:1901.09848v1\n# 1 Introduction\nTopic modeling is a powerful natural language processing tool for the unsupervised inference of the latent topics of a collection of texts [1, 2]. A variety of topic modeling algorithms have been proposed to cope with a broad set of technical challenges and diverse types of written documents [3, 4, 5, 6, 7]. Due to the large number of topic models in the literature and their widespread use, it is crucial to benchmark\nPre-print. To appear in Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019. Do not distribute.\nPre-print. To appear in Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019. Do not distribute.\navailable algorithms. The need for such approaches is exacerbated by the increase of topic modeling applications in computational social science, where the purpose of the models is not to predict documents (in which case held-out likelihood would suffice) but instead to help understand the corpus, which requires an evaluation of the inferred topics themselves [8]. Our analysis is grounded on the assumption that a hidden topic structure exists in the texts (i.e. a latent variable leading to deviations from the random usage of words). Under this assumption, a topic modeling algorithm can be viewed as an instrument for the measurement of the hidden structures. Crucial to measurement is the existence of a standard that provides ground truth [9, 10]. For example, the use of synthetic datasets has become standard in order to probe machine learning algorithms in fields such as clustering [11] or community detection [12]. Currently employed evaluation methods for topic models are often subjective, and can lack theoretical justification. Indeed, the debate is ongoing as to which evaluation method is best [13, 14, 15]. From a practical perspective, the relative performance of topic modeling algorithms varies substantially across different corpora with different characteristics (see e.g. Fig. 1, which compares several topic modeling algorithms on classification tasks). While we would expect that certain algorithms or settings are better suited to particular document characteristics (e.g., corpus size, document length, number of topics, burstiness, etc.), it remains unclear how such properties affect the performance of topic modeling algorithms, beyond a certain measure of machine learning \u201cfolklore\u201d [16]. In this work, we present a new framework for topic model evaluation relying on generating a synthetic corpus containing an unambiguous ground truth. First, we propose a novel way to generate synthetic corpora that generalizes upon previous approaches. Our approach allows us to isolate the impact of various cor-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c125/c1258d96-3f99-4718-b644-1f7fccaac235.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Performance of topic models is inconsistent across diverse real-world corpora. Normalized mutual information of four topic modeling algorithms in unsupervised document classification for 8 real-worl corpora. See Supplementary Material, Secs. S1, S2, and S4 for details on the corpora (and the pre-processin steps), the topic modeling algorithms, and the comparison metric, respectively.</div>\nFigure 1: Performance of topic models is inconsistent across diverse real-world corpora. Normalized mutual information of four topic modeling algorithms in unsupervised document classification for 8 real-world corpora. See Supplementary Material, Secs. S1, S2, and S4 for details on the corpora (and the pre-processing steps), the topic modeling algorithms, and the comparison metric, respectively.\npus characteristics, such as size, number of topics, the signal-to-noise ratio, burstiness, or fraction of stopwords, which in real-world corpora are either unknown or impossible to tone. Second, we propose a new evaluation metric based on the normalized mutual information that compares the agreement between planted and inferred topics on the level of individual word tokens. Our approach yields an absolute measure of topic modeling accuracy, eliminating the need for post-inference heuristics such as \u201ctopic matching\u201d [7]. While synthetic ground truth has been used for topic model evaluation in the past, ours is the first framework for evaluating how well topic modeling algorithms perform the key task of inferring per-token topic assignments. Altogether, the formalization of synthetic corpora allows us to probe more accurately the ability of different topic modeling algorithms to resolve a wide range of topic structures, beyond simplistic assumptions of LDA. We present experiments showing how different popular topic modeling algorithms fare as these characteristics change, for one type of synthetic corpus. We show how our measurement framework leads to new insights, including evidence of an \u201cundetectable region\u201d for sufficiently weak topic structures, or how the choice of hyperparameters can bias the inference result. Finally, we show that our approach is predictive of the performance of topic modeling algorithms in classification tasks in real-world corpora.\n# 2 Background\nA popular approach for evaluating topic models is to inspect their output manually [17], but this ap-\nproach is expensive and subjective. The most common quantitative approaches to evaluate topic modeling algorithms rely on intrinsic evaluation methods, such as held-out likelihood [13], and topic coherence [18, 19], or on extrinsic tasks such as document classification [20, 21] and information retrieval [22, 23]. However, these approaches allow only for limited insights into why topic modeling algorithms fail or succeed. For example, perplexity and topic coherence can only provide a relative measure of performance: how well does a topic model do in relation to another model? In contrast, extrinsic evaluation tasks allow for the formulation of absolute measures based on the prediction of document metadata, often considered as \u201cground truth\u201d labels in the literature. However, extrinsic evaluation approaches, and the latter identification in particular, are also problematic because: (i) manual labeling is subjective and error prone; (ii) they evaluate the topic structure only indirectly (e.g. via the fraction of correctly classified documents); and (iii) they implicitly assume that the manually generated labels are truly encoded in the topic structure of the documents. The latter assumption has been shown to be surprisingly unsupported in other domains [24, 25]. It has been recently shown that topic modeling can be formally mapped to the problem of community detection in networks [26, 27]. The formulation of benchmark corpora pursued here follows the idea of benchmark graphs in community detection. There, the basic approach is to build synthetic networks with known (planted) community structure and evaluate an algorithm by comparing the overlap between\nthe planted and the inferred community structures [12, 28, 29, 30, 31, 32, 33]. This approach allowed researchers to gain new insights into community detection algorithms such as (i) the spurious appearance of large values of modularity in random networks [34]; (ii) the existence of a resolution limit concerning the minimum size of the groups that can be inferred [35]; or (iii) the existence of an undetectable phase in which no algorithm is able to infer a structure [36]. The use of synthetic corpora has appeared sporadically in the context of topic modeling (see Supplementary Materials, Table S3). In most cases, the synthetic data comes from the generative process of LDA and is tested only on intrinsic evaluation methods such as held-out likelihood [13] or topic coherence [37]. Comparison between planted and inferred structure is usually done by visual inspection [4, 38], focuses only on either the word-topic or topic-document distribution requiring \u201cmatching of topics\u201d [39, 40, 7], or evaluate very specific hypothesis of the fitted model (such as the independence of words and documents in individual topics [19]). Our work formalizes and generalizes these ideas: (i) by developing a framework to investigate a wide range of topical structures and including a number of realistic features that might be of interest to practitioners; and (ii) proposing a measure that compares the planted and inferred structure (i.e. the topic labels) on the level of individual word tokens.\n# 3 Evaluating topic modeling algorithms using synthetic corpora\nOur approach to comparing the performance of topic modeling algorithms using synthetic corpora consists of two main steps (Fig. 2) 1. First, we generate a synthetic benchmark with a planted ground truth structure; and second, we quantify the overlap between the planted and inferred structures.\n# 3.1 Generating synthetic corpora\nOur approach to generating synthetic corpora with a planted structure is based on the formulation of the generative process employed by probabilistic topic models [1, 2]. Consider a corpus of d = 1, . . . , D documents each with length md (and N = \ufffd d md words in total) generated from K topics and V unique words defining the vocabulary V. The statistical characteristics of the corpus are determined by two sets of conditional probabilities: P(t|d), indicating the probability of topic t within document d; and P(w|t), indicating the probability with which word w is used by topic t. Specifically, for each token w(id), defined as the\n1Code to generate synthetic corpora is available at: https://github.com/amarallab/synthetic_benchmark_topic_model\nword at position id = 1, . . . , md in document d, we first draw a topic z(id) = t with probability P(t|d) and then a word w(id) = w is chosen with probability P(w|t = z(id)). Typically, one makes assumptions about these probabilities in the form of prior distributions. For example, in the case of Latent Dirichlet allocation (LDA), it is assumed that P(t|d) and P(w|t) are drawn from Dirichlet distributions with hyperparameters \u03b1 and \u03b2, respectively. Given an observed corpus, the aim in topic modeling is then to determine the most likely distributions \u02c6P(t|d) and \u02c6P(w|t) by inferring the latent topic variables \u02c6z(id) (Fig. 2A). Here, we take the inverse approach by a priori fixing the distributions P(t|d) and P(w|t) and using the generative process to produce a synthetic corpus. Formally, our generation process includes the following steps. First, we assign each word w \u2208V from the vocabulary to either the stopwords set VS (VS \u2261|VS|) or topical word set VT (VT \u2261|VT |) such that V = VS + VT . Second, we fix the global word distribution P(w) (\ufffd w P(w) = 1 ) and the number of topical words assigned to each topic Vt (\ufffd t Vt = VT ). Here, we consider a uniform or power-law functional form for their distributions. Third, we assume that each word w belongs uniquely to one topic t denoted by tw assigned randomly (such that we have Vt words in topic t). This assignment determines the topic distribution P(t) over the entire\n   Second, we fix the global word distribution P(w) (\ufffd w P(w) = 1 ) and the number of topical words assigned to each topic Vt (\ufffd t Vt = VT ). Here, we consider a uniform or power-law functional form for their distributions.\nThird, we assume that each word w belongs uniquely to one topic t denoted by tw assigned randomly (such that we have Vt words in topic t). This assignment determines the topic distribution P(t) over the entire corpus \ufffd\n\ufffd where \u03b4i,j is Kronecker delta function, i.e., \u03b4i,j = 1 only if i = j. Assuming that each document d belongs uniquely to one topic denoted by td which is randomly assigned with probability P(t). Fourth, we define the word-topic distribution P(w|t) with structure parameter cw as\n\ufffd where \u03b4i,j is Kronecker delta function, i.e., \u03b4i,j = 1 only if i = j. Assuming that each document d belongs uniquely to one topic denoted by td which is randomly assigned with probability P(t).\nFourth, we define the word-topic distribution P(w|t) with structure parameter cw as\nP(w|t) = \uf8f1 \uf8f2 \uf8f3 cw \u03b4tw,t P(w) P(t) + (1 \u2212cw) P(w), if w \u2208VT P(w), if w \u2208VS . (2)\nP(w), if w \u2208VS\n\uf8f3 While the topical words (w \u2208VT ) are characterized by a linear combination of a structured term and a random, unstructured term, the stopwords (w \u2208VS) appear randomly in all topics. Similarly, we define the topic-document distribution P(t|d) with structure parameter cd as\nP(t|d) = cd \u03b4td,t + (1 \u2212cd) P(t),\n(3)\nwhere the first term is the structured part and the second is the random, unstructured part. The resulting synthetic corpus contains a fully known planted structure since we know the topic label z(id) of each individual token w(id) (Fig. 2A). The general formulation not only allows us to investigate a wide range of topical structures, but also to incorporate statistical laws observed in real-world corpora [41], such as Zipfian word-frequency distribution, stopwords, or burstiness (Fig. 2B-E), see Supplementary Material Sec. S5.\n# 3.2 Comparing planted and inferred structure\nTypically, the results of topic modeling algorithms are evaluated either at the level of the topic-document distribution P(t|d) in applications such as document classification, or at the level of the word-topic distribution P(w|t) to judge the topic quality such as in topic coherence [42]. Here, we propose a new approach by quantifying the overlap between the planted and the inferred structure based on the comparison of the topic labels of each individual token.\nSpecifically, for each token w(id) we record the planted topic label as zpl(id) and the inferred topic label as zinf(id) and construct a confusion matrix pt,t\u2032 , which counts the fraction of tokens having a planted topic label t and an inferred topic label t\u2032\nFrom this we calculate the normalized mutual information, \u02c6I, a commonly used metric to quantify the overlap between different partitions [29] defined as:\n(5)\nwhere I is the mutual information and H (and H\u2032) are the respective entropies\n(6)\nWe thus obtain a measure between \u02c6I = 0 indicating no overlap, and \u02c6I = 1 indicating perfect overlap. Note that \u02c6I takes into account that the number of topics in the inference results does not have to match the number of planted classes (Fig. S1). The major advantage of the NMI is its easy interpretability: it quantifies the average amount of information one gains about the planted label of a token upon learning its inferred topic\nlabel. Furthermore, \u02c6I is invariant with respect to permutation of the topic labels; thus we avoid the issue of finding the \u201cbest match\u201d between planted and inferred topics, typically addressed by non-trivial heuristic approaches [7] (See [43] for advantages of \u02c6I over other measures, such as Jaccard index). This measure is related to the Variation of Information proposed in [44], i.e. V OI = const.\u00d7(1\u2212\u02c6I); however, while [44] compare different outputs of a topic modeling algorithm under different pre-processing steps, here we use the measure to compare the planted ground truth against the output of the topic modeling algorithm.\n# 4 Results\nWe report three different experiments that illustrate how synthetic corpora can yield new insights on topic modeling algorithms. As a representative sample, we evaluate four topic modeling algorithms on these corpora: LDA using Gibbs sampling (LDAGS) [4, 45], LDA using variational inference (LDAVB) [3, 46], Hierarchical Dirichlet Processes (HDP) [47, 48], and TopicMapping (TM) [7, 49] (see Supplementary Material Sec. S2 for details) using default parameter settings of the corresponding implementations unless stated otherwise.\n# 4.1 Degree of structure\nOur first experiment evaluates how modeling accuracy varies with the degree of topic structure in the synthetic corpus. Here, we consider a simple version of the synthetic corpus described in Sec. 3.1 with a single parameter for the degree of structure c = cw = cd such that we can vary between a trivial (c = 1) and an impossible (c = 0) inference problem (as shown in Fig. S2). More specifically, a smaller value of c corresponds to a higher level of noise in the synthetic corpus. In addition, we fix that there are no stopwords (Vs = 0), and that the global word-distribution and the topic-size distribution are uniform; P(w) = 1/V and Vt = V/K.\nIn Fig. 3 we compare the overlap between planted and inferred structure as a function of c for synthetic corpora with K = 10 planted topics.\nIn general, the performance of all algorithms increases non-linearly with the degree of structure c (Fig. 3A). We observe substantial differences between algorithms for both the mean (identifying TM as a systematically more accurate algorithm) and the standard deviation (identifying HDP as a systematically less reproducible algorithm). We also observe a region ( c < c\u2217with c\u2217\u2248 0.3), where none of algorithms are able to recover any\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/284b/284b1e88-c3ce-4fab-8a10-49eb11d40595.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b17a/b17a8e8b-ef70-4df3-9bea-986bad85b9e0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Generative process</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/37fb/37fb0266-f949-4ef0-9f65-c614b8261d99.png\" style=\"width: 50%;\"></div>\nFigure 2: Proposed framework for the evaluation of topic models based on synthetic corpora. (A) Evaluation framework. (B-E) Examples of synthetic corpora with different statistical features observed in real-world corpora showing the number of occurrences of each word in each document with D = 1000 and V = 100.\nstructure (\u02c6I = 0) despite the fact that the synthetic corpus contains some small degree of structure (c > 0). The latter suggests the existence of an \u201cundetectable phase\u201d, a phenomenon recently reported in the context of community detection [36]. For the LDA algorithms in Fig. 3A, we assume the number of topics (a parameter which has to be specified a priori in LDA) is Ka = 100, which is a common choice for real-world corpora in the literature [13, 23, 50, 51]. Not surprisingly, in Fig. 3B we observe a substantial improvement in performance when considering the unlikely case of guessing the correct number of topics (Ka = K = 10). We find that the performance of LDA algorithms is typically reduced by choosing both too many or too few topics highlighting how uninformed modeling assumptions can strongly affect performance (Fig. S3). We further investigate how accurately non-parametric topic models such as HDP and TopicMapping can infer the number of topics (Fig. 3C). TopicMapping finds the correct number of topics even for only moderately structured corpora, but it completely fails for very unstructured corpora by overfitting the data reflecting the intrinsic difficulty when the signal-to-noise ratio is low. In contrast, HDP tends to overestimate the number of topics in this experiment, even more so as the degree of structure becomes large. This suggests that the model is arbitrarily splitting ground truth topics\nstructure (\u02c6I = 0) despite the fact that the synthetic corpus contains some small degree of structure (c > 0). The latter suggests the existence of an \u201cundetectable phase\u201d, a phenomenon recently reported in the context of community detection [36].\nFor the LDA algorithms in Fig. 3A, we assume the number of topics (a parameter which has to be specified a priori in LDA) is Ka = 100, which is a common choice for real-world corpora in the literature [13, 23, 50, 51]. Not surprisingly, in Fig. 3B we observe a substantial improvement in performance when considering the unlikely case of guessing the correct number of topics (Ka = K = 10). We find that the performance of LDA algorithms is typically reduced by choosing both too many or too few topics highlighting how uninformed modeling assumptions can strongly affect performance (Fig. S3).\nWe further investigate how accurately non-parametric topic models such as HDP and TopicMapping can infer the number of topics (Fig. 3C). TopicMapping finds the correct number of topics even for only moderately structured corpora, but it completely fails for very unstructured corpora by overfitting the data reflecting the intrinsic difficulty when the signal-to-noise ratio is low. In contrast, HDP tends to overestimate the number of topics in this experiment, even more so as the degree of structure becomes large. This suggests that the model is arbitrarily splitting ground truth topics\n<div style=\"text-align: center;\">Inference process</div>\ninto distinct topics, a hypothesis that is corroborated by the relatively low reproducibility of the method (in terms of the average overlap between two different inferred solutions on the same data, as shown in Fig. S4). Thus, in this experiment we do not find the number of topics inferred by HDP to be reliable.\n# 4.2 Impact of LDA-implementation and hyperparameter values\nDespite the considerable advances in our understanding of LDA since its original formulation [3], we still lack a systematic understanding of the impact of different approximation techniques on the performance [52]. While some groups have investigated the advantages of Collapsed Variational Bayes over mean-field Variational Bayes [53] or the effect of hyperparameter choice [54, 55], to our knowledge there have been no systematic studies exploring the inferred solutions in terms of the corresponding topic distributions and how they depend on the hyperparameters or inference algorithms.\nIn order to understand the differences between the Variational Bayes (VB) and Gibbs Sampling (GS) implementations of LDA observed in Fig. 3, we investigate in detail the planted and inferred P(t|d) and P(w|t) for both algorithms (Fig. 4). We find that neither can accurately infer the ground truth topic distributions endowed with a mixed structure in both\nIn order to understand the differences between the Variational Bayes (VB) and Gibbs Sampling (GS) implementations of LDA observed in Fig. 3, we investigate in detail the planted and inferred P(t|d) and P(w|t) for both algorithms (Fig. 4). We find that neither can accurately infer the ground truth topic distributions endowed with a mixed structure in both\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f701/f70155f1-a6c4-421c-96d1-ff20679b501e.png\" style=\"width: 50%;\"></div>\nFigure 3: Performance of topic modeling algorithms in synthetic corpora with varying degree of structure. (A) Normalized mutual information, \u02c6I, between planted and inferred structures for different topic modeling algorithms as a function of the structure parameter c. (B) Relative performance of different topic modeling algorithms against TopicMapping, the best performing algorithm in A. (C) Number of inferred topics for non-parametric topic modeling algorithms. Synthetic corpora were generated with K = 10 topics, D = 104 documents, document length m = 100, and vocabulary size V = 103. The lines (error bars) denote averages (one standard deviation) estimated from 10 realizations.\nP(t|d) and P(w|t). With default hyperparameters, the GS implementation infers a pure word-topic distribution and places the fluctuations almost exclusively on P(t|d) (Fig. 4, 1st column). In contrast, the VB implementation infers a pure topic-document distribution and places the fluctuation mainly on P(w|t) (Fig. 4, 2nd column). However, these differences can be explained, in part, by different default values for the hyperparameters. Assuming the correct number of topics (Ka = 10) and using the same hyperparameters (default values from VB implementation) for both the GS and the VB inference, we obtain almost identical results from the two LDA algorithms (Fig. 4, 2nd & 4th columns). In contrast, the VB implementation is virtually unable to infer any meaningful structure when using the default hyperparameters of Gibbs Sampling (Fig. 4, 5th column). Interestingly, when the true number of topics is unknown, we observe substantial differences in how the two algorithms overfit the ground truth structure (Fig. S5).\nTo ensure the reliability of these findings, we repeated our analyses increasing the number of iterations 10fold for each algorithm, obtaining identical results (Figs. S6, S7).\nThese results confirm that the choice of default hyperparameters can bias the output of topic modeling algorithms. More generally, however, they show how our approach can reveal intricate differences in performance which are inaccessible in standard evaluation approaches such as document classification, where only partial information on the inferred structure is used,\ne.g., the maximum in the topic-document distribution P(t|d) (Fig. S8).\n# 4.3 Insights on real world corpora\nThe synthetic corpora discussed earlier constitute a simplified abstraction of the topic structure of realworld corpora. Thus, it may not be obvious that the insights drawn from synthetic corpora will be generalizable to real-world corpora. Therefore, we next investigate two examples supporting the hypothesis that despite its simplicity the synthetic corpus not only allows to make predictions on the performance of topic modeling algorithms in similar real-world corpora, but it also provides additional insights as to why different algorithms perform differently on distinct corpora (Fig. 5).\nWe measure performance in real-world corpora in an unsupervised classification task using human-assigned document labels as a ground truth proxy. In analogy to the approach in Eqs. (4,5) we quantify the correspondence between external and inferred document labels using the normalized mutual information (see Supplementary Material Sec. S4).\nStopwords. While many practitioners remove stopwords from corpora prior to analysis, there is no consensus on the effect of stopwords on the performance of topic modeling algorithms [56, 57]. We thus investigate the effect of stopwords using the 20 News Group (20NG) dataset motivated by the fact that it exhibited the strongest dependence of performance on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9c5/b9c5f692-1572-450b-a226-8c45294d9d22.png\" style=\"width: 50%;\"></div>\nFigure 4: Default hyperparameter settings bias the inferred topic structure of different LDA implementations. Comparison of topic distributions P(t|d) (top row) and P(w|t) (bottom row) from the planted and inferred structure from LDAGS and LDAVB using two different sets of hyperparameters: original defaults as defined in each implementation (left panels) and defaults from the other implementation, respectively (right panels). Ground truth is displayed in the middle column. Same parameters as in Fig. 3 fixing c = 0.7 and using Ka = 10.\nstopword shown in Fig. 1. Using the English stopword list from MALLET [45], we estimate that about 43% of word tokens in the 20NG corpus are stopwords. For our analysis, we remove at random a given fraction of these tokens. We find that performance of topic modeling algorithms varies but generally increases as we decrease the fraction of stopwords (Fig. 5A). We construct a synthetic corpus with c = 0.7 (we obtain similar results with different values, Fig. S9), K = 40, and a varying fraction Ps of stopwords. Measuring performance by unsupervised document classification, we find the same pattern as for the real corpus (Fig. 5B). In contrast, measuring performance as the overlap between planted and inferred structure yields substantial differences, which reflect the additional detail provided by the structure overlap (Fig. 5C). Considering the inferred topic distributions (Fig. S10), we find that LDAVB infers a pure topic-document distribution, assigning most of the uncertainty to the wordtopic distribution and correctly identifying most of the stopwords, while LDAGS assigns most of the uncertainty to the topic-document distribution, trying to infer a pure word-topic distribution resulting in overfitting the stopwords and assigning them to inferred topics. In document classification, most of this information remains invisible, leading to indistinguishable results for the two algorithms.\nstopword shown in Fig. 1. Using the English stopword list from MALLET [45], we estimate that about 43% of word tokens in the 20NG corpus are stopwords. For our analysis, we remove at random a given fraction of these tokens. We find that performance of topic modeling algorithms varies but generally increases as we decrease the fraction of stopwords (Fig. 5A).\nWe construct a synthetic corpus with c = 0.7 (we obtain similar results with different values, Fig. S9), K = 40, and a varying fraction Ps of stopwords. Measuring performance by unsupervised document classification, we find the same pattern as for the real corpus (Fig. 5B). In contrast, measuring performance as the overlap between planted and inferred structure yields substantial differences, which reflect the additional detail provided by the structure overlap (Fig. 5C). Considering the inferred topic distributions (Fig. S10), we find that LDAVB infers a pure topic-document distribution, assigning most of the uncertainty to the wordtopic distribution and correctly identifying most of the stopwords, while LDAGS assigns most of the uncertainty to the topic-document distribution, trying to infer a pure word-topic distribution resulting in overfitting the stopwords and assigning them to inferred topics. In document classification, most of this information remains invisible, leading to indistinguishable results for the two algorithms.\nDocument length. It has been reported that topic models have low performance on corpora of short document, such as Twitter posts [58]. However, the effect of document length on the performance of topic models\nis still not well characterized [16]. We thus investigate the effect of text length by considering only the first md words of each document in the Web of Science (WOS) dataset, a collection of 40,526 scientific articles (title and abstract) from 7 academic areas. Prior to analysis, we removed all stopwords (using the stopword list from MALLET [45]). We find that performance improves with increasing document length (Fig. 5D); yet, the ranking of the models\u2019 performance remains virtually unchanged. We construct a synthetic corpus with similar properties fixing c = 0.7 (we obtain similar results with different values, Fig. S11) and K = 10 and varying the length md of each document. For both measures of performance, classification (Fig. 5E) and structure overlap (Fig. 5F) we qualitatively reproduce the findings on the real corpus. In particular, we recover the same ranking for the performance of topic models.\nis still not well characterized [16]. We thus investigate the effect of text length by considering only the first md words of each document in the Web of Science (WOS) dataset, a collection of 40,526 scientific articles (title and abstract) from 7 academic areas. Prior to analysis, we removed all stopwords (using the stopword list from MALLET [45]). We find that performance improves with increasing document length (Fig. 5D); yet, the ranking of the models\u2019 performance remains virtually unchanged.\nWe construct a synthetic corpus with similar properties fixing c = 0.7 (we obtain similar results with different values, Fig. S11) and K = 10 and varying the length md of each document. For both measures of performance, classification (Fig. 5E) and structure overlap (Fig. 5F) we qualitatively reproduce the findings on the real corpus. In particular, we recover the same ranking for the performance of topic models.\n# 5 Discussion\nOur study illustrates how the use of synthetic corpora can lead to new insights on topic model performance unattainable when only studying real-world corpora. Our approach allows us to systematically investigate the effect of both individual properties of the corpus (document length, stopwords, etc.) and parameters of the topic modeling algorithms (assumed number of topics, hyperparameters, etc.). For example, our analysis reveals that (i) the number of topics determined by popular non-parametric approaches (such as HDP) cannot be relied upon; (ii) there exist fundamental\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4023/40237132-4082-4d4f-b5de-874bf4446c87.png\" style=\"width: 50%;\"></div>\nFigure 5: Performance in synthetic corpora is strongly correlated to performance in real-world corpora. Comparison between 20 News Group data and synthetic corpora with K = 40, D = 104 md = 100, V = 103, c = 0.7 varying the fraction of stopwords Ps (top row) using Ka = 100, and WOS data and a synthetic corpus with K = 10, D = 104, V = 103, c = 0.7 varying the document length md (bottom row). (A, D) NMI from unsupervised document classification in real-world corpora. (B, E) NMI from unsupervised document classification in synthetic corpora. (C, F) NMI from structure overlap in synthetic corpora. While each case measures NMI (in bits), panels (A,B,D,E) compare labels of documents and panels (C,F) compare labels of word tokens.\nlimits to algorithms\u2019 ability to infer a topic structure. and (iii) the default hyperparameter settings induce a substantial bias in the inferred solutions of different implementations of the same topic model. Most importantly, we demonstrate the practical relevance of our approach by showing that relative performance in synthetic corpora predicts relative performance in real-world corpora. While these results raise more questions than they can answer, we believe that our proposed framework offers a complimentary approach to gain a better understanding of topic modeling algorithms. In particular, it allows us to systematically identify strengths and weaknesses of topic modeling algorithms in different applications and under different conditions allowing for more informed choices among a large number of available algorithms.\nlimits to algorithms\u2019 ability to infer a topic structure. and (iii) the default hyperparameter settings induce a substantial bias in the inferred solutions of different implementations of the same topic model. Most importantly, we demonstrate the practical relevance of our approach by showing that relative performance in synthetic corpora predicts relative performance in real-world corpora.\nWhile these results raise more questions than they can answer, we believe that our proposed framework offers a complimentary approach to gain a better understanding of topic modeling algorithms. In particular, it allows us to systematically identify strengths and weaknesses of topic modeling algorithms in different applications and under different conditions allowing for more informed choices among a large number of available algorithms.\nUnarguably, the presented synthetic corpora are far from the complexity of real-world corpora. However, our framework provides enough flexibility to accommodate different features such as burstiness, syntax, or structures beyond the bag-of-words model (phrases, sentences, etc.) in future studies with increasing complexity of the synthetic corpora.\n# Acknowledgements\nL.A.N.A. thanks the John and Leslie McQuown Gift and support from Department of Defense Army Research Office (Grant Number W911NF-14-1-0259).\nReferences [1] Blei DM. Probabilistic topic models. Communications of the ACM. 2012;55(4):77. [2] Crain SP, Zhou K, Yang SH, Zha H. Dimensionality reduction and topic modeling: From Latent Semantic Indexing to Latent Dirichlet allocation and beyond. In: Mining Text Data. Springer US; 2012. p. 1\u2013522. [3] Blei DM, Ng AY, Jordan MI. Latent Dirichlet allocation. Journal of Machine Learning Research. 2003;3:993\u20131022. [4] Griffiths TL, Steyvers M. Finding scientific topics. Proceedings of the National Academy of Sciences. 2004;101:5228\u20135235. [5] Blei DM, Lafferty JD. A correlated topic model of Science. The Annals of Applied Statistics. 2007;1(1):17\u201335. [6] Buntine WL, Mishra S. Experiments with nonparametric topic models. In: Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM Press; 2014. p. 881\u2013890. [7] Lancichinetti A, Irmak Sirer M, Wang JX, Acuna D, Kording K, Amaral LAN. Highreproducibility and high-accuracy method for automated topic classification. Physical Review X. 2015;5(1):011007. [8] Boyd-Graber J, Hu Y, Mimno D. Applications of Topic Models. Foundations and Trends in Information Retrieval. 2017;11:143\u2013296. [9] Bandalos DL. Measurement Theory and Applications for the Social Sciences. Guilford Publications; 2018. 10] Allen MJ, Yen WM. Introduction to Measurement Theory. Waveland Press; 2001. 11] Jain AK. Data clustering: 50 years beyond Kmeans. Pattern Recognition Letters. 2010;31:651\u2013 666. 12] Lancichinetti A, Fortunato S, Radicchi F. Benchmark graphs for testing community detection algorithms. Physical Review E. 2008;78:046110. 13] Wallach HM, Murray I, Salakhutdinov R, Mimno D. Evaluation methods for topic models. In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM Press; 2009. p. 1105\u20131112.\n[14] Chang J, Gerrish S, Wang C, Boyd-graber J, Blei DM. Reading Tea Leaves: How Humans Interpret Topic Models. In: Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2009. p. 288\u2013296. [15] R\u00a8oder M, Both A, Hinneburg A. Exploring the Space of Topic Coherence Measures. In: Proceedings of the Eighth ACM International Conference on Web Search and Data Mining. ACM; 2015. p. 399\u2013408. [16] Tang J, Meng Z, Nguyen X, Mei Q, Zhang M. Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis. In: Proceedings of the 31st International Conference on Machine Learning. PMLR; 2014. p. 190\u2013198. [17] Murakami A, Thompson P, Hunston S, Vajn D. What is this corpus about?\u2019: Using topic modelling to explore a specialised corpus. Corpora. 2017;12(2):243\u2013277. [18] Newman D, Lau JH, Grieser K, Baldwin T. Automatic Evaluation of Topic Coherence. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics; 2010. p. 100\u2013 108. [19] Mimno D, Wallach HM, Talley E, Leenders M, McCallum A. Optimizing Semantic Coherence in Topic Models. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics; 2011. p. 262\u2013272. [20] Lu Y, Mei Q, Zhai C. Investigating task performance of probabilistic topic models: An empirical study of PLSA and LDA. Information Retrieval. 2011;14(2):178\u2013203. [21] Xie P, Xing EP. Integrating Document Clustering and Topic Modeling. In: Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence. AUAI Press; 2013. p. 694\u2013703. [22] Sch\u00a8utze H, Manning CD, Raghavan P. Introduction to Information Retrieval. Cambridge University Press; 2008. [23] Wei X, Croft WB. LDA-based document models for ad-hoc retrieval. In: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM; 2006. p. 178\u2013185.\n[24] Hric D, Darst RK, Fortunato S. Community detection in networks: Structural communities versus ground truth. Physical Review E. 2014;90(6):062805. [25] Peel L, Larremore DB, Clauset A. The ground truth about metadata and community detection in networks. Science Advances. 2017;3(5):e1602548. [26] Karrer B, Newman MEJ. Stochastic blockmodels and community structure in networks. Physical Review E. 2011;83(1):016107. [27] Gerlach M, Peixoto TP, Altmann EG. A network approach to topic models. arXiv: 170801677. 2017;p. 1\u201319. [28] Girvan M, Newman MEJ. Community structure in social and biological networks. Proceedings of the National Academy of Sciences. 2002;99(12):7821\u20137826. [29] Danon L, Daz-Guilera A, Duch J, Arenas A. Comparing community structure identification. Journal of Statistical Mechanics: Theory and Experiment. 2005;2005(09):P09008. [30] Sales-Pardo M, Guimer`a R, Moreira AA, Amaral LAN. Extracting the hierarchical organization of complex systems. Proceedings of the National Academy of Sciences. 2007;104(39):15224\u201315229. [31] Sawardecker EN, Sales-Pardo M, Amaral LAN. Detection of node group membership in networks with group overlap. The European Physical Journal B. 2009;67(3):277\u2013284. [32] Lancichinetti A, Fortunato S. Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities. Physical Review E. 2009;80:016118. [33] Guimer`a R, Sales-Pardo M, Amaral LAN. Module identification in bipartite and directed networks. Physical Review E. 2007;76:036102. [34] Guimer`a R, Sales-Pardo M, Amaral LAN. Modularity from fluctuations in random graphs and complex networks. Physical Review E. 2004;70(2):025101. [35] Fortunato S, Barthelemy M. Resolution limit in community detection. Proceedings of the National Academy of Sciences. 2007;104(1):36\u201341. [36] Decelle A, Krzakala F, Moore C, Zdeborov\u00b4a L. Inference and Phase Transitions in the Detection of Modules in Sparse Networks. Physical Review Letters. 2011;107(6):065701.\n[37] AlSumait L, Barbar\u00b4a D, Gentle J, Domeniconi C. Topic Significance Ranking of LDA Generative Models. In: Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg; 2009. p. 67\u201382. [38] Andrzejewski D, Zhu X, Craven M. Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM; 2009. p. 25\u201332. [39] Taddy M. On Estimation and Selection for Topic Models. In: Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics. PMLR; 2012. p. 1184\u20131193. [40] Arora S, Ge R, Halpern Y, Mimno D, Moitra A, Sontag D, et al. A Practical Algorithm for Topic Modeling with Provable Guarantees. In: Proceedings of the 30th International Conference on Machine Learning. PMLR; 2013. p. 280\u2013288. [41] Altmann EG, Gerlach M. Statistical Laws in Linguistics. In: Creativity and Universality in Language. Springer International Publishing; 2016. p. 7\u201326. [42] Bhatia S, Lau JH, Baldwin T. An Automatic Approach for Document-level Topic Model Evaluation. arXiv:170605140. 2017;. [43] Fortunato S. Community detection in graphs. Physics Reports. 2010;486:75\u2013174. [44] Schofield A, Mimno D. Comparing apples to apple: The effects of stemmers on topic models. Transactions of the Association for Computational Linguistics. 2016;4:287\u2013300. [45] McCallum AK. Mallet: A machine learning for language toolkit. http://malletcsumassedu. 2002;. [46] \u02c7Reh\u02dau\u02c7rek R, Sojka P. Software Framework for Topic Modelling with Large Corpora. In: Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA; 2010. p. 45\u201350. [47] Teh YW, Jordan MI, Beal MJ, Blei DM. Hierarchical Dirichlet processes. Journal of the American Statistical Association. 2006;101(476):1566\u2013 1581. [48] Wang C. Hierarchical Dirichlet process. https://githubcom/blei-lab/hdp. 2010;. [49] Lancichinetti A. TopicMapping. https://bitbucketorg/andrealanci/topicmapping. 2016;.\n[50] Aletras N, Baldwin T, Lau JH, Stevenson M. Evaluating topic representations for exploring document collections. Journal of the Association for Information Science and Technology. 2017;68(1):154\u2013167. [51] Steyvers M, Griffiths T. Probabilistic topic models. Handbook of latent semantic analysis. 2007;427(7):424\u2013440. [52] Zhang J, Zeng J, Yuan M, Rao W, Yan J. LDA Revisited. In: Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM Press; 2016. p. 1763\u20131772. [53] Mukherjee I, Blei DM. Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation. In: Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2009. p. 1129\u20131136. [54] Asuncion A, Welling M, Smyth P, Teh YW. On Smoothing and Inference for Topic Models. In: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press; 2009. p. 27\u201334. [55] Wallach HM, Mimno D, McCallum A. Rethinking LDA: Why priors matter. In: Advances in Neural Information Processing Systems; 2009. . [56] Zaman ANK, Matsakis P, Brown C. Evaluation of stop word lists in text retrieval using Latent Semantic Indexing. In: International Conference on Digital Information Management. IEEE; 2011. p. 133\u2013136. [57] Schofield A, Magnusson M, Mimno D. Pulling Out the Stops: Rethinking Stopword Removal for Topic Models. In: Proceedings of Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics; 2017. p. 432\u2013436. [58] Hong L, Davison BD. Empirical Study of Topic Modeling in Twitter. In: Proceedings of the First Workshop on Social Media Analytics. ACM; 2010. p. 80\u201388.\n# Supplementary Material for the manuscript: \u201cA new evaluation framework for topic modeling algorithms based on synthetic corpora\u201d\nMartin Gerlach1 Isabel Diersen1\nIsabel Diersen1\nDepartment of Chemical and Biological Engineering, 2Department of Electrical Engineering and Computer Science Northwestern University Evanston, Illinois 60208, USA\n 28 Jan 2019\n# S1 Real world corpora\nWe use 4 datasets and apply different filtering strategies yielding 8 different real world corpora each with D documents, N tokens, and C category labels, as shown in Table S1.\n[cs.CL]\n# S2 Topic model algorithms\nWe use the following topic modeling algorithms (shown in Table S2) with default parameter settings unless stated otherwise.\narXiv:1901.09848v1\nLDA requires hyperparameter values for the topicdocument distribution, a Ka-dimensional vector \u20d7\u03b1 = (\u03b1j)j=1,...,Ka (where Ka is the assumed number of topics), and the word-topic distribution, a V -dimensional vector \u20d7\u03b2 = (\u03b2w)w=1,...,V (where V is the size of the vocabulary). We assume symmetric priors, i.e. \u03b1j = \u03b1 and \u03b2w = \u03b2, such that the hyperparameters are fully determined by the scalar parameters \u03b1 and \u03b2. For LDAVB we use the default values of the gensim implementations. For LDAGS we use the default values of the gensim-wrapper of the mallet implementation.\n# S3 Usage of synthetic corpora in previous studies\nDifferent types of synthetic corpora in previous studies are given in Table S3. As we can see, in previous research a large portion of synthetic corpora are generated directly from LDA.\n# S4 Document classification\nIn practical applications, topic models are often used to find documents of similar topical content in an unsupervised fashion. In this spirit we quantify the performance of topic models by checking how much the\nDoug Downey2 Lu\u00b4\u0131s A. N. Amaral1\nDoug Downey2 Lu\u00b4\u0131s A\ninferred topic distributions reflect the assignment of human-labeled categories in real world corpora.\nMore specifically, we fit a topic model to the entire corpus and obtain the inferred topic-distribution of each document, P(t|d). Identifying each topic with a category, we predict the category-membership of each document, sd, from the topic with maximum probability [S24]\n(S1)\nComparing this with the given metadata-category, rd, we can construct a confusion matrix\n(S2)\n\ufffd which yields the fraction of documents that have metadata-category r and predicted category s. With confusion matrix ps,r, we can quantify the performance of the topic model in the classification task using the normalized mutual information.\n# S5 Generation of synthetic benchmark corpora\nWith the distributions P(w|t) and P(t|d) described in the main text Sec. 3.1, we can generate the synthetic benchmark corpora from distributions P(w|t) and P(t|d) according to the generative process. One major advantage of our work is that our approach allows for the inclusion of many realistic features, such as Zipfian distribution, stopword, and burstiness, as described below.\nZipfian word-frequency distribution. One of the most well-known statistical laws in language is the socalled Zipf\u2019s law [S25], which states that the frequency f of the r-th most frequent word is given by a powerlaw with exponent \u03b3 > 1:\nf(r) \u221dr\u2212\u03b3\n(S3)\n<div style=\"text-align: center;\">Table S1: Details for real-world corpora.</div>\nDataset\nVariation\nFiltering\nCharacteristics\nReuters-21578 [S1]\n1\nOnly documents from the 10\nlargest categories\nD=7,518; N=775,771; C=10\n2\nOnly documents from cate-\ngories with more than 10 doc-\numents\nD=8,559; N=936,004; C=41\nRCV1 [S2]\n1\nOnly documents with one cat-\negory label; subsample\n10%\nof documents from the largest\ncategory\nD=3,070; N=574,249; C=4\n2\nOnly documents with two cat-\negory labels; subsample 10% of\ndocuments\nD=20,474;\nN=2,917,939;\nC=54\nWeb of Science [S3]\n1\nNone\nD=40,526; N=3,828,735; C=7\n2\nOnly keep the first 20 tokens of\neach document\nD=40,526; N=808,672; C=7\n20 News Group [S4]\n1\nRemove all words with less\nthan 3 characters\nD=18,803;\nN=3,831,559;\nC=20\n2\nSame as in variation 1 and re-\nmove all stopwords from list\ngiven in Ref. [S5]\nD=18,799;\nN=2,654,710,\nC=20;\n<div style=\"text-align: center;\">Table S2: Details for topic modeling algorithms.</div>\nTopic Model\nImplementation\nDefault hyperparameter values\nGibbs Sampling LDA (LDAGS)\n[S6]\nMallet [S5]\n\u03b1 = 5/Ka, \u03b2 = 0.01\nVariational Bayes LDA (LDAVB)\n[S7]\ngensim [S8]\n\u03b1 = 1/Ka, \u03b2 = 1/Ka\nHierarchical\nDirichlet\nProcesses\n(HDP) [S9]\nFrom work of [S10]\nn.a.\nTopicMapping (TM)\n[S11]\nFrom work of [S3]\nn.a.\nWe incorporate a Zipfian distribution by accommodating any global word-frequency distribution P(w) as an average over all topics, i.e. P(w) = \ufffd t P(w, t) = \ufffd t P(w|t)P(t) where P(t) is the size of topic t.\nStopwords. An important statistical property of real texts is the generic appearance of stopwords. While there is no general agreed-upon definition, in the context of topic modeling this usually refers to very common words (such as \u201cthe,\u201d \u201cand,\u201d etc.) which are considered not informative in inferring topical structure related to semantics. In practice, these words are typically removed from a corpus using a pre-specified list of stopwords, however, there exist differing opinions on the effect of stopwords in the result of the quality of inferred topic models [S26,S27].\nWe model stopwords as words which have the same probability of appearance in any topic, i.e. P(w|t) = P(w). Varying the fraction of stopwords (of unique words in the vocabulary) by a parameter Ps \u2208[0, 1] allows us to investigate the robustness of a topic model with respect to these non-informative words.\nBurstiness. The phenomenon of burstiness refers to non-stationarity in the usage of words [S28,S29], that is a word is more likely to occur in a text after its first occurrence. We incorporate burstiness following approaches proposed in Refs. [S30,S31] using Dirichlet-distributions. Given a topic t, instead of drawing from a fixed wordtopic distribution P(w|t), we obtain a different wordtopic distribution in each document which is drawn\nBurstiness. The phenomenon of burstiness refers to non-stationarity in the usage of words [S28,S29], that is a word is more likely to occur in a text after its first occurrence.\nfrom a V -dimensional Dirichlet distribution with concentration parameter ac, i.e. Pd(w|t) \u223cDirV (ac \u00b7 P(w|t)). This means that the smaller ac the more \u201cbursty\u201d the synthetic corpora. For example, in the limiting case ac \u21920 (ac \u2192\u221e) the word-topic distribution in each document will contain only one word with non-zero probability (the original global wordtopic distribution from the non-bursty case).\n# S6 Supplementary figures\nAs an example in Fig. S1, consider two planted classes in the synthetic benchmark, where 50% of the tokens belong to each planted class, we obtain different values \u02c6I depending on the inferred structure: (1) If all tokens are correctly assigned into two inferred classes yielding a perfectly diagonal confusion matrix pt,t\u2032, this leads to I = log(2) and \u02c6I = 1; (2) In case one of the inferred classes gets split into two equal-sized classes, we get the same I, but a smaller value for \u02c6I; (3) If one of the smaller inferred classes is uninformative with respect to the planted classes, this leads to a further reduction of I and \u02c6I; (4) If the tokens are just randomly assigned to two inferred classes, this yields I = 0 and \u02c6I = 0. In Fig. S2 we show the resulting synthetic corpora for the random (c = 0), mixed (c = 0.5), and ordered (c = 1) case. While for c = 0 the topics are not distinguishable in the ground truth topic distributions, c > 0 yields a block-diagonal structure (Fig. S2A). Looking at the empirically observed corpus in the form of the counts n(d, w), i.e., the number of times word w appears in document d, words are distributed randomly across all documents for c = 0, while increasing c leads to a higher concentration of words in certain documents reflecting the increasing degree of structure (Fig. S2B). For algorithms such as LDA one needs to specify the number of topics for fitting the topic model. While in the synthetic corpus we know the true number of topics, in practice, this value is unknown. Therefore, we investigate the effect of over- and under-fitting by varying the assumed number of topics, Ka, for LDA in a synthetic corpus with 10 planted topics (Fig. S3). Fig. S4 compares the reproducibility of HDP and TM. There are 10 repetitions for each data points. In each repetition, only one synthetic benchmark corpus is generated. A topic model will be run on this corpus twice. The inferred token topics form the two experiments of the topic model will be compared. In Fig. S5, we show the planted and inferred P(t|d) and P(w|t) by the implementation of different algorithms for LDA, using Ka = 100 as the assumed number of topics.\nIn Fig. S6 and Fig. S7, we confirm that the solutions for LDA obtained in Fig. 4 and Fig. S5 have converged with respect to the number of iterations in each topic model. In Fig. S8 we compare the planted and inferred topic distributions P(t|d) and P(w|t) and show how they provide a more detailed view on the performance of a topic model than obtained from document classification tasks. In Fig. S9 we show that the results from Fig. 5 (B, C) investigating the effect of stopwords on the performance of topic models in synthetic corpora remain qualitatively similar when varying the parameter of the degree of structure, c. In Fig. S10 we show differences between the planted and inferred topic distributions P(t|d) and P(w|t) for synthetic corpora in the case of stopwords. In Fig. S11 we show that the results from Fig. 5 (E, F) investigating the effect of document length on the performance of topic models in synthetic corpora remain qualitatively similar when varying the parameter of the degree of structure, c.\nIn Fig. S6 and Fig. S7, we confirm that the solutions for LDA obtained in Fig. 4 and Fig. S5 have converged with respect to the number of iterations in each topic model.\nIn Fig. S8 we compare the planted and inferred topic distributions P(t|d) and P(w|t) and show how they provide a more detailed view on the performance of a topic model than obtained from document classification tasks.\nIn Fig. S9 we show that the results from Fig. 5 (B, C) investigating the effect of stopwords on the performance of topic models in synthetic corpora remain qualitatively similar when varying the parameter of the degree of structure, c.\nIn Fig. S10 we show differences between the planted and inferred topic distributions P(t|d) and P(w|t) for synthetic corpora in the case of stopwords.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8989/898952de-0aa2-4637-9472-675a09ef3d79.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure S1: Quantifying overlap using the normalized mutual information. Unnormalized, I, and normalized mutual information, \u02c6I, for different examples of confusion matrices pt,t\u2032 (cases 1-4). Number indicate the values of the confusion matrix according to color.</div>\n<div style=\"text-align: center;\">Table S3: Usage of synthetic corpora in previous studies</div>\nReference\nSynthetic Corpora\nCorresponding evaluation metric\n[S12]\nGenerated from LDA\nLikelihood; Variational free energy\n[S13]\nGenerated from LDA\nL1-norm between true and inferred\nword-topic distribution\n[S14]\nGenerated from LDA\nHeld-out likelihood\n[S15]\nGenerated from LDA\nCheck hypothesis that words and\ndocuments are independent given\nthe topic using mutual information\n[S16]\nGenerated from LDA\nCompare entries (and residuals) in\n\u03b8k (defined as the distribution over\nwords for each topic) between true\ntopics and inferred topics\n[S17]\nGenerated from LDA\nPosterior contraction analysis of\nthe topic polytope\n[S18]\nGenerated from LDA\nUse the synthetic corpora to test\ninferred number of topics\n[S19]\nMultinomial with 5 equiprobable\nwords\nLikelihood; Classification\n[S6]\nBar-data (5x5 grid)\nVisual comparison\n[S20]\nSmall size synthetic corpora based\non\ntheir\nproposed\ntopic\nmodel\n(LDA with Dirichlet Forest Priors)\nVisual\ninspection\nof\nthe\nword-\ndocument matrix\n[S21]\nSmall size synthetic corpora:\n6\nsamples of 16 documents from\nthree static equally weighted topic\ndistributions. On average, the doc-\nument size was 16 words.\nTopic signficance score (similar to\ntopic coherence)\n[S22,S23]\nSemi-synthetic data (train param-\neters of a model on a real cor-\npus, then use the model to gener-\nate synthetic data)\nTraining time; L1-error between\ntrue and inferred matrix A (defined\nas the word-topic matrix).\n[S11]\nLanguage\ndata\nwith\nnon-\noverlapping topics\nL1-norm between true and inferred\np(d|t)\nThis work\nA flexible framework that could in-\nclude a range of topic structure and\nrealistic features\nMeasure the overlap between the\nplanted and the inferred topic la-\nbels on the token level\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe2e/fe2e845a-4ca7-4117-a48b-6133db592949.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">100 500 900 100 500 900 Document number 100 500 900</div>\nFigure S2: Synthetic benchmarks with known ground truth from the generative process of topic models. Three synthetic benchmark corpora with equal size but different degrees of structure c \u2208{0, 0.5, 1} (left, middle, right column). (A) Ground truth topic distributions P(t|d) and P(w|t). (B) Resulting observable corpus showing the number of times word w appears in document d, n(d, w). For all panels, the number of topics is K = 5; there are V = 100 words in the vocabulary; and each corpus contains D = 1, 000 document with length of md = 100.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca4b/ca4b3942-4841-4a69-ad20-563255ba6a9e.png\" style=\"width: 50%;\"></div>\nFigure S3: Both overfitting and underfitting reduce the performance of LDA models. Normalized mutual information, \u02c6I, between planted and inferred structure as a function of the structure parameter c varying the assumed number of topics Ka \u2208{5, 10, 20, 50, 100}. (A) Gibbs Sampling LDA. (B) Variational Bayes LDA. For the synthetic benchmark corpora, we set the planted number of topics as K = 10, the vocabulary as V = 1, 000, and the number of documents as D = 10, 000 each with length md = 100. In the experiment we use different assumed numbers of topics (5, 10, 20, 50, 100) for LDA methods. The curves denote averages (and \u00b1 one standard deviation) over 10 realizations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d06/1d066a26-984a-471e-8ed9-e1f9d41d8d30.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure S4: Compare the reproducibility of two nonparametric topic modeling algorithms, HD and TM, based on token labeling comparison. Synthetic corpora were generated with K = 10 topic D = 104 documents, document length m = 100, and vocabulary size V = 103. The lines (error bars) deno averages (one standard deviation) estimated from 10 realizations.</div>\nFigure S4: Compare the reproducibility of two nonparametric topic modeling algorithms, HDP and TM, based on token labeling comparison. Synthetic corpora were generated with K = 10 topics, D = 104 documents, document length m = 100, and vocabulary size V = 103. The lines (error bars) denote averages (one standard deviation) estimated from 10 realizations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a036/a0362518-1098-458c-868d-5bdfad8042c3.png\" style=\"width: 50%;\"></div>\nFigure S5: Hyperparameters bias the inferred topic structure of different algorithms of LDA models. Comparison of topic distributions P(t|d) (top row) and P(w|t) (bottom row) from the planted and inferred structure from LDAGS and LDAVB using two different sets of hyperparameters: original defaults as defined in each implementation (middle panels) and defaults from the other implementation, respectively (right panels). Same parameters as in Fig. 3 fixing c = 0.7 and using Ka = 100.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d717/d717f1b5-db66-4a07-8309-6d8f0dc5050e.png\" style=\"width: 50%;\"></div>\nFigure S6: Topic models converge with the default iteration setting for Ka = 10. Inferred topic distributions P(t|d) and P(w|t) as in Fig. 4 for Gibbs Sampling LDA and Variational Bayes LDA with different hyperparameter settings comparing the case where we use the default number of iterations (top two columns) with the case where we increase the number of iterations 10-fold (bottom two columns).\n<div style=\"text-align: center;\">Figure S6: Topic models converge with the default iteration setting for Ka = 10. Inferred topic distributions P(t|d) and P(w|t) as in Fig. 4 for Gibbs Sampling LDA and Variational Bayes LDA with different hyperparameter settings comparing the case where we use the default number of iterations (top two columns) with the case where we increase the number of iterations 10-fold (bottom two columns).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3906/39069537-2b08-4ff6-9661-62f5c5fe5855.png\" style=\"width: 50%;\"></div>\nFigure S7: Topic models converge with the default iteration setting for Ka = 100. Inferred topic distributions P(t|d) and P(w|t) as in Fig. 4 for Gibbs Sampling LDA and Variational Bayes LDA with different hyperparameter settings comparing the case where we use the default number of iterations (top two columns) with the case where we increase the number of iterations 10-fold (bottom two columns).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5947/5947a218-72bd-4e50-8e84-98c785d39e87.png\" style=\"width: 50%;\"></div>\nFigure S8: Document classification overlooks information of the inferred topic structure. Comparison of the topic-document distribution P(t|d) (top row), the word-topic distribution P(w|t) (middle row), and the predicted topic in unsupervised document classification arg max t P(t|d) (bottom row) for three cases: Ground truth as planted in the synthetic corpus (left column), inferred from Gibbs Sampling LDA (middle column), and inferred from Variational Bayes LDA (right column). For the synthetic benchmark corpora, we set parameters as K = 10 topics, D = 10, 000 documents each of length md = 100, V = 103 as vocabulary size, and c = 0.5 for the degree of structure. For LDA models, we use Ka = 100 as the assumed number of topics.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98e8/98e80d7c-b0d6-491d-881c-3919eb13145a.png\" style=\"width: 50%;\"></div>\nFigure S9: Varying the degree of structure does not effect results on stopword dependency in synthetic corpora. Normalized mutual information, \u02c6I, as measured by structure overlap (left column) and unsupervised document classification (right column) as in Fig. 5 (E, F) varying the degree of structure: c = 0.6 (top row), c = 0.7 (middle row), and c = 0.8 (bottom row).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f06/8f061530-f663-4582-b23a-5642be597ed9.png\" style=\"width: 50%;\"></div>\nFigure S10: Different LDA models lead to qualitatively different solutions in the case of stopwords. Comparison of the topic-document distribution (top row), P(t|d) , word-topic distribution (middle row), P(w|t), and predicted topic in unsupervised document classification (bottom row), arg max t P(t|d) for three cases: Ground truth as planted in the synthetic corpus (left column), inferred from Gibbs Sampling LDA (middle column), and inferred from Variational Bayes LDA (right column). Same parameters as in Fig. 5 (E, F) setting the fraction of stopwords Ps = 0.65 and the degree of structure c = 0.7.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c29/0c292f02-c56e-45bc-a49e-dc28bebddb96.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e98d/e98d3573-55cf-45a2-9701-bda0b26da4f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure S11: Varying the degree of structure does not effect results on document length dependency in synthetic corpora. Normalized mutual information, \u02c6I, as measured by structure overlap (left column) and unsupervised document classification (right column) as in Fig. 5 (B, C) varying the degree of structure: c = 0.6 (top row), c = 0.7 (middle row), and c = 0.8 (bottom row).</div>\nFigure S11: Varying the degree of structure does not effect results on document length dependency in synthetic corpora. Normalized mutual information, \u02c6I, as measured by structure overlap (left column) and unsupervised document classification (right column) as in Fig. 5 (B, C) varying the degree of structure: c = 0.6 (top row), c = 0.7 (middle row), and c = 0.8 (bottom row).\n# Supplementary References\n[S1] Hettich S, Bay S. The UCI KDD archive. http://kdd ics uci edu, Irvine, CA: University of California, Department of Information and Computer Science. 1999;.\n[S2] Lewis DD, Yang Y, Rose TG, Li F. RCV1: A New Benchmark Collection for Text Categorization Research. Journal of Machine Learning Research. 2004;5:361\u2013397.\n[S2] Lewis DD, Yang Y, Rose TG, Li F. RCV1: A New Benchmark Collection for Text Categorization Research. Journal of Machine Learning Research. 2004;5:361\u2013397.\n[S3] Lancichinetti A. TopicMapping. https://bitbucketorg/andrealanci/topicmapping. 2016;.\n[S3] Lancichinetti A. TopicMapping. https://bitbucketorg/andrealanci/topicmapping. 2016;.\n[S4] Cachopo AMdJC. Improving methods for singlelabel text categorization. Instituto Superior T\u00b4ecnico, Portugal. 2007;.\n[S5] McCallum AK. Mallet: A machine learning for language toolkit. http://malletcsumassedu. 2002;.\n[S6] Griffiths TL, Steyvers M. Finding scientific topics. Proceedings of the National Academy of Sciences. 2004;101:5228\u20135235.\n[S7] Blei DM, Ng AY, Jordan MI. Latent Dirichlet allocation. Journal of Machine Learning Research. 2003;3:993\u20131022.\n[S8] \u02c7Reh\u02dau\u02c7rek R, Sojka P. Software Framework for Topic Modelling with Large Corpora. In: Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA; 2010. p. 45\u201350.\n[S9] Teh YW, Jordan MI, Beal MJ, Blei DM. Hierarchical Dirichlet processes. Journal of the American Statistical Association. 2006;101(476):1566\u2013 1581.\n10] Wang C. Hierarchical Dirichlet process. https://githubcom/blei-lab/hdp. 2010;.\n[S11] Lancichinetti A, Irmak Sirer M, Wang JX, Acuna D, Kording K, Amaral LAN. Highreproducibility and high-accuracy method for automated topic classification. Physical Review X. 2015;5(1):011007. [S12] Mukherjee I, Blei DM. Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation. In: Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2009. p. 1129\u20131136. [S13] Newman D, Asuncion A, Smyth P, Welling M. Distributed Algorithms for Topic Models. J Mach Learn Res. 2009 dec;10:1801\u20131828. [S14] Wallach HM, Murray I, Salakhutdinov R, Mimno D. Evaluation methods for topic models. In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM Press; 2009. p. 1105\u20131112. [S15] Mimno D, Blei D. Bayesian Checking for Topic Models. Proceedings of the Conference on Empirical Methods in Natural Language Processing. 2011;p. 227\u2013237. [S16] Taddy M. On Estimation and Selection for Topic Models. In: Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics. PMLR; 2012. p. 1184\u20131193. [S17] Tang J, Meng Z, Nguyen X, Mei Q, Zhang M. Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis. In: Proceedings of the 31st International Conference on Machine Learning. PMLR; 2014. p. 190\u2013198. [S18] Hsu WS, Poupart P. Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics. In: Lee DD, Sugiyama M, Luxburg UV, Guyon I, Garnett R, editors. Advances in Neural Information Processing Systems 29. Curran Associates, Inc.; 2016. p. 4536\u2013 4544. [S19] Minka T, Lafferty J. Expectation-Propagation for the Generative Aspect Model. Uncertainty in Artificial Intelligence. 2002;p. 352\u2013359. [S20] Andrzejewski D, Zhu X, Craven M. Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM; 2009. p. 25\u201332. [S21] AlSumait L, Barbar\u00b4a D, Gentle J, Domeniconi C. Topic Significance Ranking of LDA Generative Models. In: Machine Learning and Knowl-\nedge Discovery in Databases. Springer Berlin Heidelberg; 2009. p. 67\u201382. [S22] Arora S, Ge R, Halpern Y, Mimno D, Moitra A, Sontag D, et al. A Practical Algorithm for Topic Modeling with Provable Guarantees. In: Proceedings of the 30th International Conference on Machine Learning. PMLR; 2013. p. 280\u2013288. [S23] Arora S, Ge R, Koehler F, Ma T, Moitra A. Provable Algorithms for Inference in Topic Models. In: Proceedings of the 33rd International Conference on Machine Learning. PMLR; 2016. p. 2859\u20132867. [S24] Xie P, Xing EP. Integrating Document Clustering and Topic Modeling. In: Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence. AUAI Press; 2013. p. 694\u2013 703. [S25] Zipf GK. The Psychobiology of Language. London: Routledge; 1936. [S26] Zaman ANK, Matsakis P, Brown C. Evaluation of stop word lists in text retrieval using Latent Semantic Indexing. In: International Conference on Digital Information Management. IEEE; 2011. p. 133\u2013136. [S27] Schofield A, Magnusson M, Mimno D. Pulling Out the Stops: Rethinking Stopword Removal for Topic Models. In: Proceedings of Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics; 2017. p. 432\u2013436. [S28] Katz SM. Distribution of content words and phrases in text and language modelling. Natural Language Engineering. 1996;2(1):15\u201359. [S29] Altmann EG, Pierrehumbert JB, Motter AE. Beyond word frequency: Bursts, lulls, and scaling in the temporal distributions of words. PloS one. 2009;4(11):e7678. [S30] Madsen RE, Kauchak D, Elkan C. Modeling word burstiness using the Dirichlet distribution. In: Proceedings of the 22nd Annual International Conference on Machine Learning. ACM Press; 2005. p. 545\u2013552. [S31] Doyle G, Elkan C. Accounting for burstiness in topic models. In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM Press; 2009. p. 281\u2013288.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The need for a standardized evaluation framework for topic modeling algorithms has arisen due to the increasing applications of topic models in diverse fields such as computational social science, where understanding the corpus is more critical than merely predicting documents. Existing evaluation methods often lack theoretical justification and can be subjective, leading to inconsistencies in assessing model performance across different datasets.",
            "purpose of benchmark": "The benchmark is intended to provide a systematic way to evaluate and compare the performance of probabilistic topic modeling algorithms using synthetic corpora with a clearly defined ground truth topic structure."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of accurately inferring latent topics from collections of texts, particularly in evaluating how well topic modeling algorithms can recover known topic structures from synthetic datasets.",
            "key obstacle": "Existing benchmarks often rely on subjective evaluations or indirect measures, which do not provide clear insights into the strengths and weaknesses of different topic modeling algorithms or how they perform across varying corpus characteristics."
        },
        "idea": {
            "intuition": "The creation of the benchmark was inspired by the need to quantify the agreement between planted and inferred topic structures, addressing the limitations of current evaluation methods that often lack clarity and objectivity.",
            "opinion": "The authors emphasize the importance of this benchmark in enhancing the understanding of topic modeling algorithms, suggesting that it will lead to more informed choices among available algorithms.",
            "innovation": "This benchmark introduces a novel evaluation metric based on normalized mutual information that compares the agreement between planted and inferred topics at the token level, offering a more absolute measure of topic modeling accuracy compared to previous methods.",
            "benchmark abbreviation": "NMI"
        },
        "dataset": {
            "source": "The dataset is generated synthetically based on probabilistic topic models, allowing for the inclusion of various realistic features such as Zipfian distribution and burstiness.",
            "desc": "The synthetic corpora are designed to have a known ground truth structure, enabling precise evaluations of topic modeling algorithms. The framework allows for the manipulation of corpus characteristics to explore their impact on model performance.",
            "content": "The dataset includes text data where each token is assigned a topic label, facilitating the evaluation of how well algorithms can recover these labels.",
            "size": "1,000,000",
            "domain": "Topic Modeling",
            "task format": "Document Classification"
        },
        "metrics": {
            "metric name": "Normalized Mutual Information (NMI)",
            "aspect": "The aspect being measured is the accuracy of topic assignments at the token level, specifically how well the inferred topics match the planted topics.",
            "principle": "The choice of NMI as a metric is based on its ability to provide a clear, interpretable measure of overlap between the planted and inferred topic structures, avoiding the complications of heuristic approaches.",
            "procedure": "The evaluation involves generating synthetic corpora, running topic modeling algorithms on these datasets, and calculating NMI to quantify the agreement between the planted and inferred topic labels."
        },
        "experiments": {
            "model": "The benchmark evaluates several topic modeling algorithms, including Latent Dirichlet Allocation (LDA) using both Gibbs sampling and variational inference, as well as Hierarchical Dirichlet Processes (HDP) and TopicMapping (TM).",
            "procedure": "Models are trained on synthetic corpora with varying characteristics, and their performance is assessed using the NMI metric to determine how well they recover the planted topic structures.",
            "result": "The experiments reveal significant differences in performance among the algorithms, with novel insights into the conditions under which each algorithm excels or fails.",
            "variability": "Variability is accounted for by conducting multiple trials with different synthetic corpora and analyzing the consistency of results across these trials."
        },
        "conclusion": "The study concludes that the proposed benchmark framework for evaluating topic modeling algorithms offers valuable insights into their performance, revealing strengths and weaknesses that are not apparent through traditional evaluation methods.",
        "discussion": {
            "advantage": "The benchmark provides a systematic and objective means of evaluating topic modeling algorithms, leading to better understanding and selection of models based on specific corpus characteristics.",
            "limitation": "One limitation noted is that the synthetic corpora may not fully capture the complexity of real-world data, which could affect the generalizability of the insights gained.",
            "future work": "Future research could focus on extending the framework to incorporate more complex structures and features observed in real-world corpora, enhancing its applicability."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge support from the John and Leslie McQuown Gift and the Department of Defense Army Research Office."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The benchmark provides a systematic way to evaluate and compare the performance of probabilistic topic modeling algorithms using synthetic corpora with a clearly defined ground truth topic structure."
        },
        {
            "section number": "2.2",
            "key information": "The study addresses the challenge of accurately inferring latent topics from collections of texts, particularly in evaluating how well topic modeling algorithms can recover known topic structures from synthetic datasets."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark evaluates several topic modeling algorithms, including Latent Dirichlet Allocation (LDA) using both Gibbs sampling and variational inference, as well as Hierarchical Dirichlet Processes (HDP) and TopicMapping (TM)."
        },
        {
            "section number": "6.4",
            "key information": "One limitation noted is that the synthetic corpora may not fully capture the complexity of real-world data, which could affect the generalizability of the insights gained."
        },
        {
            "section number": "7.1",
            "key information": "Future research could focus on extending the framework to incorporate more complex structures and features observed in real-world corpora, enhancing its applicability."
        }
    ],
    "similarity_score": 0.5696497889294972,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0746_artif/papers/A new evaluation framework for topic modeling algorithms based on synthetic corpora.json"
}