{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.00880",
    "title": "Compressing VAE-Based Out-of-Distribution Detectors for Embedded Deployment",
    "abstract": "Out-of-distribution (OOD) detectors can act as safety monitors in embedded cyber-physical systems by identifying samples outside a machine learning model's training distribution to prevent potentially unsafe actions. However, OOD detectors are often implemented using deep neural networks, which makes it difficult to meet real-time deadlines on embedded systems with memory and power constraints. We consider the class of variational autoencoder (VAE) based OOD detectors where OOD detection is performed in latent space, and apply quantization, pruning, and knowledge distillation. These techniques have been explored for other deep models, but no work has considered their combined effect on latent space OOD detection. While these techniques increase the VAE's test loss, this does not correspond to a proportional decrease in OOD detection performance and we leverage this to develop lean OOD detectors capable of real-time inference on embedded CPUs and GPUs. We propose a design methodology that combines all three compression techniques and yields a significant decrease in memory and execution time while maintaining AUROC for a given OOD detector. We demonstrate this methodology with two existing OOD detectors on a Jetson Nano and reduce GPU and CPU inference time by 20% and 28% respectively while keeping AUROC within 5% of the baseline.",
    "bib_name": "bansal2024compressingvaebasedoutofdistributiondetectors",
    "md_text": "# Compressing VAE-Based Out-of-Distribution Detectors for Embedded Deployment\nAditya Bansal,1 Michael Yuhas,1,2 Arvind Easwaran1 1College of Computing and Data Science 2Energy Research Institute @ NTU, Interdisciplinary Graduate Program Nanyang Technological University, Singapore aditya018@e.ntu.edu.sg, michaelj004@e.ntu.edu.sg, arvinde@ntu.edu.sg\nAbstract\u2014Out-of-distribution (OOD) detectors can act as safety monitors in embedded cyber-physical systems by identifying samples outside a machine learning model\u2019s training distribution to prevent potentially unsafe actions. However, OOD detectors are often implemented using deep neural networks, which makes it difficult to meet real-time deadlines on embedded systems with memory and power constraints. We consider the class of variational autoencoder (VAE) based OOD detectors where OOD detection is performed in latent space, and apply quantization, pruning, and knowledge distillation. These techniques have been explored for other deep models, but no work has considered their combined effect on latent space OOD detection. While these techniques increase the VAE\u2019s test loss, this does not correspond to a proportional decrease in OOD detection performance and we leverage this to develop lean OOD detectors capable of real-time inference on embedded CPUs and GPUs. We propose a design methodology that combines all three compression techniques and yields a significant decrease in memory and execution time while maintaining AUROC for a given OOD detector. We demonstrate this methodology with two existing OOD detectors on a Jetson Nano and reduce GPU and CPU inference time by 20% and 28% respectively while keeping AUROC within 5% of the baseline. 1  [cs.LG]  2 Sep 2024\n# I. INTRODUCTION\nDeep learning models are usually trained with a closedworld assumption \u2013 the data on which the model is trained is entirely representative of real-world data. However, when the models are deployed in real-world settings, the test data distribution might be drastically different from the training set, resulting in poor performance. Although this loss in accuracy might be tolerable for non-critical applications, for missioncritical applications like autonomous driving, such accuracy drops cannot be tolerated. Hence, to ensure safety in machine learning models, it is necessary to detect out-of-distribution (OOD) samples at test time and relay this information to a high level controller, which can put the system into a safe state or hand control back to a human operator if possible. One of the key applications of OOD detection is in embedded systems where a deep neural network (DNN) has to be deployed to a resource constrained device while still being capable of making online inferences. Thus, it is critical to ensure that such models are efficient, with low inference times and storage arXiv:2409.0088\nThis research is part of the programme DesCartes and is supported by the National Research Foundation, Prime Minister\u2019s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. This research was funded in part by MoE, Singapore, Tier-2 grant number MOE2019-T2-2-040.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7d3/d7d33377-72ca-4f67-9bf1-996702b464b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: Our design methodology for compressing a VAE-based OOD detector using pruning informed knowledge distillation and quantization.</div>\nrequirements, along with high classification accuracy. Consider a mobile robot navigating an environment using a DNN. If most of the training data was gathered during normal brightness conditions, performance may degrade in dark or extremely bright environments. An OOD detector designed to protect this system needs to meet a minimum accuracy requirement, not exceed a maximum execution time, and achieve minimal memory usage so as not to interfere with other mission-critical tasks. While many OOD detection architectures have been explored in previous literature [1], the variational autoencoder (VAE) based OOD detection architecture has received interest in the context of embedded systems due to its ability to learn interpretable representations in latent space [2]. Previous works have explored knowledge distillation [3], pruning [4], and quantization [5] for OOD detection, but none have focused on the combination of all three techniques. VAEbased OOD detectors present an additional challenge as there is no direct relationship between loss and detection performance, unlike other models where compression was explored [3], [4]. We show that these compression techniques applied to a VAE-based OOD detector can yield significant reductions in inference time and memory footprint without loss of accuracy for the OOD detection task. This is particularly useful for memory-intensive models in embedded systems [6]. Moreover, we observe that low training loss in the VAE does not necessarily translate to high OOD detection accuracy, and vice-versa. This implies that the VAE-based OOD detector can maintain some level of detection performance, even when its loss is\ndegraded by compression. Based on our experiments, we propose the design methodology shown in Fig. 1 to compress an existing VAE-based OOD detector for deployment in an embedded system while maintaining acceptable classification accuracy. Our framework relies on three stages: 1) quantization, 2) pruning informed knowledge distillation, and 3) final pruning. As many modern SoCs are equipped with both CPU and GPU cores, our methodology generates two models: a floating point version tailored to GPU inference and a quantized version tailored to CPU inference. We demonstrate the methodology on two VAE-based OOD detectors: \u03b2-VAE, which learns an interpretable representation of latent space [2] and optical flow (OF), which learns two latent representations for horizontal and vertical flows [7]. With the \u03b2-VAE model we reduce execution time by 38% while AUROC only falls 1% below baseline. With the OF model, we reduce execution time by 20% and 28% on GPU and CPU respectively while maintaining AUROC within 5% of baseline.\ndegraded by compression. Based on our experiments, we propose the design methodology shown in Fig. 1 to compress an existing VAE-based OOD detector for deployment in an embedded system while maintaining acceptable classification accuracy. Our framework relies on three stages: 1) quantization, 2) pruning informed knowledge distillation, and 3) final pruning. As many modern SoCs are equipped with both CPU and GPU cores, our methodology generates two models: a floating point version tailored to GPU inference and a quantized version tailored to CPU inference. We demonstrate the methodology on two VAE-based OOD detectors: \u03b2-VAE, which learns an interpretable representation of latent space [2] and optical flow (OF), which learns two latent representations for horizontal and vertical flows [7]. With the \u03b2-VAE model we reduce execution time by 38% while AUROC only falls 1% below baseline. With the OF model, we reduce execution time by 20% and 28% on GPU and CPU respectively while maintaining AUROC within 5% of baseline. II. RELATED WORK Quantization has proven successful in both training [8] and neural network inference [9]. Quantization consists of techniques for reducing the precision of weights and activations in a neural network. By storing tensors at a lower bit-width than floating-point precision, drastic execution time improvements can be made in model inference and the storage space requirements of such models. Broadly, quantization can be categorized into three different algorithms: dynamic quantization, post-training static quantization, and quantization aware training (QAT) [10]. The algorithms primarily differ in when and how the quantization parameters are determined. Dynamic quantization computes quantized weights offline, but activations dynamically during inference time, quantization-aware training trains a model with quantized parameters, and posttraining static quantization performs an additional calibration step after training. Quantized networks have shown impressive performance on various benchmark datasets such as MNIST, CIFAR-10 and ImageNet [11]. Neural network pruning removes the redundant and inconsequential parameters from a neural network, by either removing individual parameters or removing groups of parameters such as entire filters or channels from a convolutional layer. Based on the importance of the neurons or weights in a model, the ones that contribute the least to the output are removed, until the desired compression is achieved. Several pruning algorithms have been proposed in the literature to improve neural network efficiency (e.g., [12]\u2013[14]). Knowledge distillation aims to train a simpler and smaller \u2018student model\u2019 from a larger complex model or an ensemble of models with minimal loss in accuracy [15]. As noted by [16], distillation algorithms typically differ in the form of knowledge, the distillation algorithm, and the training technique. Wu et al., considered knowledge distillation for OOD detectors [3] and Koda et al. considered pruning [4], but only in cases where the OOD detector was integrated with a larger classification DNN. In these studies the classification accuracy of the DNN was considered, not the interpretability of the\nQuantization has proven successful in both training [8] and neural network inference [9]. Quantization consists of techniques for reducing the precision of weights and activations in a neural network. By storing tensors at a lower bit-width than floating-point precision, drastic execution time improvements can be made in model inference and the storage space requirements of such models. Broadly, quantization can be categorized into three different algorithms: dynamic quantization, post-training static quantization, and quantization aware training (QAT) [10]. The algorithms primarily differ in when and how the quantization parameters are determined. Dynamic quantization computes quantized weights offline, but activations dynamically during inference time, quantization-aware training trains a model with quantized parameters, and posttraining static quantization performs an additional calibration step after training. Quantized networks have shown impressive performance on various benchmark datasets such as MNIST, CIFAR-10 and ImageNet [11]. Neural network pruning removes the redundant and inconsequential parameters from a neural network, by either removing individual parameters or removing groups of parameters such as entire filters or channels from a convolutional layer. Based on the importance of the neurons or weights in a model, the ones that contribute the least to the output are removed, until the desired compression is achieved. Several pruning algorithms have been proposed in the literature to improve neural network efficiency (e.g., [12]\u2013[14]). Knowledge distillation aims to train a simpler and smaller \u2018student model\u2019 from a larger complex model or an ensemble of models with minimal loss in accuracy [15]. As noted by [16], distillation algorithms typically differ in the form of knowledge, the distillation algorithm, and the training technique. Wu et al., considered knowledge distillation for OOD detectors [3] and Koda et al. considered pruning [4], but only in cases where the OOD detector was integrated with a larger classification DNN. In these studies the classification accuracy of the DNN was considered, not the interpretability of the\nOOD detector, which is a driving factor behind VAE-based OOD detection. Furthermore, these studies did not consider how to combine compression techniques and did not feature quantization.\nDesigning accurate OOD detectors has been explored in previous literature. In order to deal with the high dimensionality of image data, OOD detectors have commonly been implemented as DNNs. For example, in [17], multi-head CNNs were used to quantify uncertainty in a model\u2019s prediction, allowing an image to be classified as in-distribution (ID) or OOD, while in [18] generative models with normalizing flows were used to perform OOD detection given some domain knowledge of ID characteristics. Another common method is to use a VAE, which learns a latent distribution of input samples and attempts to reconstruct them from this representation. Reconstruction based methods [19] use this information bottleneck to ensure OOD samples are reconstructed poorly and can be identified easily during test. Recently, detection in the latent representation space has become popular as it only requires executing the VAE encoder during test and yields better detection results [20]. The \u03b2-VAE OOD detector shown in Fig. 2a is one such latent space detection methodology [2], and allows reasoning about OOD samples based on their latent distributions. ID training data is divided into partitions based on generative factors, e.g., precipitation or ambient brightness. All partitions are used to train a VAE using a modified loss function that combines reconstruction loss with a Kullback-Leibler (KL) divergence term multiplied by factor \u03b2, which enforces a normal distribution of samples in the latent space [21]. After training, a set of calibration data containing samples from each data partition is fed into the network and the latent variables that show the most variance with respect to a particular generative factor are selected as reasoners for that factor. At inference, the KL divergence between a latent reasoner and the standard normal distribution is fed into an inductive conformal prediction (ICP) algorithm [22], which assigns a confidence level as to whether a sample belongs to the same distribution as the calibration set. A martingale [23] and cumulative sum are then used to smooth these confidence scores over time as environmental conditions like precipitation and brightness are unlikely to change instantaneously between frames. Another example of latent space OOD detection is the optical flow OOD detector shown in Fig. 2b [7]. This OOD detector is built on the notion that certain motions in a vehicle\u2019s environment have been encountered during training and other OOD motions indicate the presence of hazardous conditions. First, the Farneb\u00a8ack optical flow is calculated between two sequential input frames, which generates a vector field of flows with horizontal and vertical components [24]. The vector fields for six successive frames are concatenated and used as input to two VAEs: one for the vertical flow components and another for the horizontal ones. During training these VAEs use the evidence lower bound (ELBO) loss function to reconstruct the original flow vectors and enforce normality in the the latent space [21]. At test time, KL divergence is used to measure how\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/613d/613d8ea9-1f05-4b5f-acd0-985e69ddaeb9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a78/4a782660-adb3-420f-98ec-0dc4ef65c3bb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Optical flow OOD detector operating on horizontal and vertical flow vectors.</div>\nfar a sample deviates from the expected distribution. Because a sample can be OOD with respect to horizontal or vertical flows, the KL divergence for both the encoders are summed and compared against a specified threshold.\n# IV. DESIGN METHODOLOGY\nVAE-based OOD detection methods require inferencing an encoder network for each sample at run time. This becomes a challenge in systems with hard deadlines where these models must process large input images. Previous works have analyzed reducing the dimensionality of these models\u2019 inputs [5], however, this may discard potentially useful information in an input sample. Our methodology formulates the problem differently: given an input with fixed dimensionality and data format, how do we design an OOD detector that minimizes execution time and memory usage while maintaining accuracy above a given threshold. The search space for this design problem is prohibitively large (quantization level, pruning amount, and student architecture for knowledge distillation). To address this, our methodology starts with a greedy search through potential student architectures, testing those most likely to lead to a reduction in execution time while maintaining accuracy first. Afterward, a binary search through pruning levels is used to select the minimum memory configuration for a given architecture that satisfies our accuracy constraints. All these steps require access to a limited number of OOD samples for cross-validation purposes, even though the OOD detectors are trained using only ID samples. The components of our methodology from Fig. 1 are explained in detail below. Target Aware Model Generation - Different embedded hardware have different requirements for the minimum quantization level and precision that can be used efficiently at run time. For example, an embedded GPU may only support floating point operations, while a CPU may require quantized neural operations in order to achieve a reasonable execution time. Furthermore, previous works targeted at embedded deployment require a model to have two versions: one for embedded CPUs and another for embedded GPUs depending on resource availability at run time [25]. Our methodology achieves this by separately designing a 16-bit floating point (fp16) model\nfor an embedded GPU and a quantized 8-bit (qint8) model for CPU. The design process is separate for each model as different precision levels may tolerate different amounts of pruning and knowledge distillation before violating accuracy constraints. Pruning Aware Knowledge Distillation - Training a student network that requires fewer operations than its teacher model has the largest potential to reduce execution time. While the impacts of quantization and precision reduction are hardware dependent, gains from knowledge distillation are universally applicable. However, the search space of possible student models is prohibitively large, so we propose pruning aware knowledge distillation to define the subset of models to be explored and greedily searched within this space. First, we propose that all student models should be based on the teacher architecture and that execution time decreases should come from the removal of layers. First, 50% of the weights and biases with the lowest L2 mean in the teacher model are pruned and set to zero. Then, the layer with the highest percentage of zero weights is removed and the resulting model becomes the student. If a layer\u2019s bias vector has the highest percentage of zeros, then the bias vector is removed. When a layer is removed, the output of the preceding layer must match the shape of the next layer in the original model. To accomplish this for convolutional layers without causing an explosion in the number of learnable parameters, stride and dilation are increased while kernel size and number of input and output channels remain unchanged. For linear layers, the number of outputs is simply set to the the number of inputs for the next layer. We take a layer\u2019s sparsity after pruning as a heuristic for its contribution to the network\u2019s output; in this way we perform greedy search, removing layers we believe correspond the least to the model\u2019s output first, to quickly arrive at the architecture that best satisfies the design requirements. Obtaining the Sparsest Model - Pruning aware knowledge distillation stops when the pruned model no longer satisfies the accuracy constraint. We take the last student model from the previous step that satisfied all constraints and perform a binary search across sparsity (from 0 \u2212100%) to find a model with the greatest possible sparsity that still satisfies all constraints. First, the model is pruned to the desired sparsity, s. If the accuracy is greater than the constraint, the model is pruned again such that its new sparsity is (s+(100\u2212s)/2)%, otherwise the unpruned model is pruned to (s/2)% sparsity and the process repeats. The assumption is that sparser models will have worse accuracy than less-sparse models, but there will exist some sparsity level at which the student model will still satisfy the accuracy constraint, but benefit from the the storage size reduction resulting from pruning.\n# V. CASE STUDY: \u03b2-VAE OOD DETECTOR\nA miniature robotic platform, called Duckietown [26], was used as a test-bed to simulate autonomous driving. To validate the performance of the VAE-based OOD Detector, a test vehicle, called a Duckiebot (Jetson Nano quad-core ARM Cortex-A57 64-bit CPU @ 1.42 GHz with 2 GB RAM), was deployed in the Duckietown environment to collect data in different brightness conditions. The data was collected on four\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7434/743499de-e223-4202-bc5c-76a187ca5f46.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Reconstructed Images Fig. 3: Sample images reconstructed using the \u03b2-VAE model</div>\n<div style=\"text-align: center;\">Fig. 3: Sample images reconstructed using the \u03b2-VAE model</div>\ndifferent tracks (three indoor and one outdoor track), under three ambient lighting conditions \u2013 low, medium, and high \u2013 as illustrated in Figure 3a. Low and medium brightness levels were considered as in-distribution (ID), and high brightness was considered as OOD data. A calibration set was generated from the same distribution as that of ID data. This set consisted of rapidly fluctuating brightness levels between low and medium ID partitions to allow the model to identify the latent variables corresponding to brightness and rank them based on the amount of variance. Similar to [2], a 2:1 training to calibration ratio was used during calibration. A beta variational auto-encoder (\u03b2-VAE) architecture, as proposed by [2], was trained for 200 epochs using ELBO loss and optimised using the Adam optimizer. The encoder consisted of four convolution blocks each having a 2D convolution layer followed by 2D batch normalisation, leaky ReLU activation function, and finally a 2D max pooling layer. At the end, four fully-connected layers were used with leaky ReLU activation functions. The decoder block, typically an inverse of the encoder block, consisted of four fully connected linear layers with leaky ReLU activation functions, with an increasing number of neurons in each subsequent layer. The fully-connected layers were followed by four deconvolution blocks, each comprising of max unpooling layer, transpose convolution, batch normalization, and leaky ReLU activation function. The number of latent dimensions (30) and \u03b2 value (1.4) were determined using Bayesian optimization to maximize the VAE\u2019s mutual information gain as recommended by [2]. As discussed in Section III, the \u03b2-VAE model is first trained on ID data gathered in different lighting conditions, and the weights of the decoder part are discarded and the encoderonly model is used for OOD detection. Figure 3b illustrates sample images reconstructed using the baseline \u03b2-VAE model. The baseline model achieved a true positive rate of 0.955 and a false positive rate of 0.250. The area under the ROC curve (AUROC) for the optimum decay values was 0.852.\n# A. Compressing the \u03b2-VAE OOD Dectector\nThe compression techniques of quantization, pruning, and knowledge distillation were independently explored on the \u03b2VAE OOD detector, and the results are presented in figure 5.\n<div style=\"text-align: center;\">TABLE I: Memory consumption and execution time of the model after Dynamic Quantization, Static Quantization and Quantization Aware Training</div>\nTABLE I: Memory consumption and execution time of the model after Dynamic Quantization, Static Quantization and Quantization Aware Training (QAT)\nModel\nModel Size\n(MB)\nForward pass\nsize (MB)\nExecution\nTime (ms)\nDynamic\nQuantization\n18.34\n90.56\n90.39\u00b14.05\nStatic Quantization\n17.75\n40.47\n56.14\u00b11.56\nQAT\n17.76\n40.48\n57.48\u00b12.27\nQuantization - Table I summarises the memory usage and execution times of three different quantization techniques. All quantization methods result in a memory improvement of about 3.7 times due to the conversion of weight precision from float32 to int8 in the quantized form. Dynamic quantization incurs the longest execution time, which could be attributed to the additional overhead of computing the scaling factors during runtime. As observed in Figure 5, dynamic quantization results in the least total loss, since the quantization takes place during runtime (\u2018on-the-fly\u2019), and hence it can effectively compute the quantization scaling factor for each instance. Further, quantization aware training results in the highest AUROC value, among the three techniques. Pruning - A global unstructured pruning was performed on the \u03b2-VAE model, where individual nodes with the lowest L2 mean weights were pruned away. Figure 4 illustrates the reconstruction loss of the network across different sparsity levels. It can be observed that the total loss does not deteriorate until about 60% of the nodes are pruned away. However, a significant drop in the AUROC is observed after pruning about 50%. Such a drop could be attributed to the unstructured nature of the pruning technique used, where any neuron in the entire architecture could be pruned. As discussed in section VI a structured layer-wise pruning strategy has been opted to mitigate such a drastic drop in AUROC.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/77b1/77b1de0b-0158-48d0-99c8-a200c824ddb3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4: AUROC and total reconstruction loss for the \u03b2-VAE detector a different sparsity levels using pruning</div>\nKnowledge Distillation - Knowledge distillation was performed on a student architecture with one of the fully-connected layers removed, resulting in a reduction of more than 50% in the number of parameters in the network. As demonstrated in table II, the resultant student architecture had approximately 7 million parameters as opposed to the initial 17 million parameters. A second student architecture was tested with the removal of the last convolution layer in the encoder and the first deconvolution layer in the decoder part of the network. This resulted in a significant drop in the number of parameters,\nModel\nNumber of Parameters\nModel Size (MB)\nBase Model\n\u223c17 M\n67.42\nStudent Architecture 1\n\u223c7 M\n26.71\nStudent Architecture 2\n\u223c400K\n1.61\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e2ba/e2badf16-e11e-4aa0-b324-eed5037562ea.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Reconstruction loss and AUROC of the \u03b2-VAE detector across different compression techniques</div>\nwith just 400 thousand parameters in the entire network due to the reduced output shape in the absence of a convolution layer, which cascades to all the following fully connected layers. The first student architecture led to a relatively better AUROC performance, as compared to the second architecture, which reiterates the importance of the convolutional layer in encoding crucial information necessary for OOD detection. The ablation studies shed light onto the influence of individual compression techniques on the overall system design. The experiments reveal that low training loss does not necessarily translate to a high OOD accuracy. Comparison between dynamic quantization and quantization aware training reveal that although dynamic quantization leads to a lower reconstruction loss, the AUROC achieved by quantization aware training is higher than that of dynamic quantization. Overall, quantization results in the best OOD performance among all compression techniques. Interestingly, quantization aware training surpassed the baseline AUROC, which could be attributed to the higher generalizability of quantized networks. Moreover, although the \u03b2-VAE network in our model was trained with a combination of reconstruction loss and KL loss, only KL divergence in the latent space is used at test time, which may have contributed to these compression techniques maintaining relatively high OOD detection accuracy (and sometimes leading to an improvement), despite causing an overall decrease in reconstruction loss.\n# VI. CASE STUDY: OPTICAL FLOW OOD DETECTOR\nWe tested our strategy on the optical flow OOD detector [7]. Using the same dataset as the previous case study, we artificially added rain and snow to 50% of the images in the test set using the same method as [5], such that samples with no precipitation were considered ID, while samples with precipitation were considered OOD. Both encoder networks (Fig. 2b) were trained with the same initial architecture referred to as the baseline. This consists of 4 convolutional layers with stride 3, kernel size 5, and depths 32/64/128/256. Each convolutional layer is followed by a batch norm and ReLU activation function. Finally, a fully connected layer with linear activation leads to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f57/1f573741-efd1-4544-98a5-05e9b68d73cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6: AUROC, number of parameters, execution time (ET), and mean KL divergence loss for the optical flow OOD detector under five different knowledge distillation configurations: (1) baseline; (2) layer 4 removed; (3) layers 4 and 5 removed; (4) layers 3, 4, and 5 removed; (5) layers 2, 3, 4, and 5 removed.</div>\na latent space with 12 dimensions. Both horizontal and vertical flow vectors were sized 224 \u00d7 224 with 6 flows concatenated to form one input. During training, the decoder was the mirror image of the encoder. Each network was trained for 100 epochs using the Adam optimizer with learning rate capped at 1\u00d710\u22125. First, we performed our pruning aware knowledge distillation with the results shown in Fig. 6. Configuration (1) corresponds to the baseline model, in configuration (2) we retain convolutional layers with depths 32/64/256 and the linear layer, in configuration (3) we have convolutional layer depths 32/64/256 with no linear layer, in configuration (4) we have convolution depths 32/64, and in configuration (5) we only have a single convolution layer that takes input space to 12 output latent dimensions. As expected, execution time (ET) decreases as layers are systematically removed, however, AUROC does not sharply fall off for the fp16 model until only one layer is left. The qint8 model has the largest improvement in execution time for removing layers, but its AUROC also falls off much faster. Surprisingly, for very few layers (configurations 4 and 5) it ends up performing better than some configurations with more free parameters. We hypothesize that the initial effective capacity of the architecture was much less than its representational capacity, and for the configurations with fewer layers, even though they have less representational capacity, their effective capacity increases due to the architecture. We also show that while removing layers greatly impacts the VAE\u2019s loss, good OOD detection performance is maintained because the mean KL loss on OOD samples remains above that of ID samples; as long as loss increases equally for all samples, performance is preserved. Since AUROC dips below 0.9 after configuration (4), we choose this model to analyze the effects of further pruning on performance. Fig. 7 shows AUROC and corresponding loss on ID and OOD samples across varying sparsity levels for the fp16 and qint8 models. These sparsity levels are calculated across all layers in the model. We observe that increasing sparsity leads to an increase in loss, but because mean loss increases proportionally for ID and OOD samples, performance is preserved. However, for very sparse models (e.g. 90%) AUROC and loss both drop sharply. This collapse in loss is explained by the fact that nearly all the weights are now 0,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5687/5687d6e2-6347-4709-ae29-569755407eed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7: AUROC and corresponding test loss for quantized and fp16 optical flow models under increasing sparsity.</div>\n<div style=\"text-align: center;\">for quantized and fp16 optical</div>\nmaking it less likely that any given sample will lead to a high mean and variance, which would increase KL divergence in latent space. Because the KL loss collapses for all samples regardless of ID or OOD, AUROC is also severely impacted.\n# VII. CONCLUSION\nWe explored different neural network compression techniques on \u03b2-VAE and optical flow OOD detectors using a mobile robot powered by a Jetson Nano. Based on our analysis of results for quantization, knowledge distillation, and pruning, we proposed a design strategy to find the model with the best execution time and memory usage while maintaining some accuracy metric for a given VAE-based OOD detector. We successfully demonstrated this methodology on an optical flow OOD detector and showed that our methodology\u2019s ability to aggressively prune and compress a model is due to the unique attributes of VAE-based OOD detection. Despite our methodology\u2019s good performance, it requires access to OOD samples at design time to act as a crossvalidation set. In our case study, we assume OOD samples arise from a particular generating distribution, but this may not be the case in general. Furthermore, it only guides the search for a faster architecture, but does not guarantee the optimum result. Nevertheless, we believe having a design methodology that combines quantization, knowledge distillation, and pruning allows engineers to exploit the combined powers of these techniques instead of considering them individually.\n# REFERENCES\n[1] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek, M. Kloft, T. G. Dietterich, and K.-R. M\u00a8uller, \u201cA unifying review of deep and shallow anomaly detection,\u201d Proceedings of the IEEE, vol. 109, no. 5, pp. 756\u2013795, 2021. [2] S. Ramakrishna, Z. Rahiminasab, G. Karsai, A. Easwaran, and A. Dubey, \u201cEfficient out-of-distribution detection using latent space of \u03b2-vae for cyber-physical systems,\u201d ACM Transactions on Cyber-Physical Systems (TCPS), vol. 6, no. 2, pp. 1\u201334, 2022. [3] Q. Wu, H. Jiang, H. Yin, B. Karlsson, and C.-Y. Lin, \u201cMulti-level knowledge distillation for out-of-distribution detection in text,\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). ACL, 2023, pp. 7317\u20137332. [4] S. Koda, A. Zolfit, E. Grolman, A. Shabtai, I. Morikawa, and Y. Elovici, \u201cPros and cons of weight pruning for out-of-distribution detection: An empirical survey,\u201d in 2023 International Joint Conference on Neural Networks (IJCNN). IEEE, 2023, pp. 1\u201310. [5] M. Yuhas, D. J. X. Ng, and A. Easwaran, \u201cDesign methodology for deep out-of-distribution detectors in real-time cyber-physical systems,\u201d in 2022 IEEE 28th International Conference on Embedded and RealTime Computing Systems and Applications (RTCSA). IEEE, 2022, pp. 180\u2013185.\n[6] Y. Chen, B. Zheng, Z. Zhang, Q. Wang, C. Shen, and Q. Zhang, \u201cDeep learning on mobile and embedded devices: State-of-the-art, challenges, and future directions,\u201d ACM Computing Surveys (CSUR), vol. 53, no. 4, pp. 1\u201337, 2020. [7] Y. Feng, D. J. X. Ng, and A. Easwaran, \u201cImproving variational autoencoder based out-of-distribution detection for embedded real-time applications,\u201d ACM Transactions on Embedded Computing Systems (TECS), vol. 20, no. 5s, pp. 1\u201326, 2021. [8] R. Banner, I. Hubara, E. Hoffer, and D. Soudry, \u201cScalable methods for 8-bit training of neural networks,\u201d in Advances in Neural Information Processing Systems, vol. 31. Curran Associates, Inc., Dec. 2018. [9] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, \u201cA survey of quantization methods for efficient neural network inference,\u201d in Low-Power Computer Vision. Chapman and Hall/CRC, 2022, pp. 291\u2013326. [10] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen, and T. Blankevoort, \u201cA white paper on neural network quantization,\u201d arXiv preprint arXiv:2106.08295, 2021. [11] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, \u201cQuantized neural networks: Training neural networks with low precision weights and activations,\u201d The Journal of Machine Learning Research, vol. 18, no. 1, pp. 6869\u20136898, 2017. [12] J. Zhang, X. Chen, M. Song, and T. Li, \u201cEager pruning: Algorithm and architecture support for fast training of deep neural networks,\u201d in 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2019, pp. 292\u2013303. [13] P. Ponnapalli, K. Ho, and M. Thomson, \u201cA formal selection and pruning algorithm for feedforward artificial neural network optimization,\u201d IEEE Transactions on Neural Networks, vol. 10, no. 4, pp. 964\u2013968, 1999. [14] M. G. Augasta and T. Kathirvalavakumar, \u201cA novel pruning algorithm for optimizing feedforward neural network of classification problems,\u201d Neural Processing Letters, vol. 34, no. 3, pp. 241\u2013258, 2011. [15] G. Hinton, O. Vinyals, J. Dean et al., \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015. [16] J. Gou, B. Yu, S. J. Maybank, and D. Tao, \u201cKnowledge distillation: A survey,\u201d International Journal of Computer Vision, vol. 129, no. 6, pp. 1789\u20131819, 2021. [17] J. Linmans, J. van der Laak, and G. Litjens, \u201cEfficient out-of-distribution detection in digital pathology using multi-head convolutional neural networks.\u201d in MIDL, 2020, pp. 465\u2013478. [18] M. M. A. Valiuddin, C. G. A. Viviers, R. J. G. van Sloun, P. H. N. de With, and F. van der Sommen, \u201cEfficient out-of-distribution detection of melanoma with wavelet-based normalizing flows,\u201d in Cancer Prevention Through Early Detection. Singapore: Springer Nature Switzerland, Sep. 2022, pp. 99\u2013107. [19] J. An and S. Cho, \u201cVariational autoencoder based anomaly detection using reconstruction probability,\u201d Special lecture on IE, vol. 2, no. 1, pp. 1\u201318, 2015. [20] A. Vasilev, V. Golkov, M. Meissner, I. Lipp, E. Sgarlata, V. Tomassini, D. K. Jones, and D. Cremers, \u201cq-space novelty detection with variational autoencoders,\u201d in Computational Diffusion MRI: MICCAI Workshop, Shenzhen, China, October 2019. Springer, 2020, pp. 113\u2013124. [21] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, \u201cbeta-vae: Learning basic visual concepts with a constrained variational framework,\u201d 2016. [22] R. Laxhammar and G. Falkman, \u201cInductive conformal anomaly detection for sequential detection of anomalous sub-trajectories,\u201d Annals of Mathematics and Artificial Intelligence, vol. 74, pp. 67\u201394, 2015. [23] V. Fedorova, A. Gammerman, I. Nouretdinov, and V. Vovk, \u201cPlug-in martingales for testing exchangeability on-line,\u201d in Proceedings of the 29th International Coference on International Conference on Machine Learning, ser. ICML\u201912. Omnipress, 2012, p. 923\u2013930. [24] G. Farneb\u00a8ack, \u201cTwo-frame motion estimation based on polynomial expansion,\u201d in Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29\u2013July 2, 2003 Proceedings 13. Springer, 2003, pp. 363\u2013370. [25] N. Ling, X. Huang, Z. Zhao, N. Guan, Z. Yan, and G. Xing, \u201cBlastnet: Exploiting duo-blocks for cross-processor real-time dnn inference,\u201d in Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems, 2022, pp. 91\u2013105. [26] L. Paull, J. Tani, H. Ahn, J. Alonso-Mora, L. Carlone, M. Cap, Y. F. Chen, C. Choi, J. Dusek, Y. Fang et al., \u201cDuckietown: an open, inexpensive and flexible platform for autonomy education and research,\u201d in 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 1497\u20131504.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of out-of-distribution (OOD) detection in embedded systems, highlighting the limitations of existing deep neural network (DNN) based methods due to their high memory and execution time requirements, and the necessity for more efficient solutions.",
        "problem": {
            "definition": "The problem involves detecting samples that fall outside the training distribution of machine learning models during real-time inference in resource-constrained environments.",
            "key obstacle": "Existing methods often struggle to meet the stringent memory and execution time constraints of embedded systems while maintaining acceptable accuracy in OOD detection."
        },
        "idea": {
            "intuition": "The proposed idea stems from the observation that while compression techniques can degrade model performance, they do not necessarily correlate with a proportional decrease in OOD detection accuracy.",
            "opinion": "The idea involves using a combination of quantization, pruning, and knowledge distillation to create leaner VAE-based OOD detectors that maintain performance while being suitable for embedded deployment.",
            "innovation": "This approach innovatively combines three distinct compression techniques, which have not been previously applied together in the context of VAE-based OOD detection, leading to significant improvements in execution time and memory usage."
        },
        "method": {
            "method name": "Compressed VAE-based OOD Detector",
            "method abbreviation": "C-VAE-OOD",
            "method definition": "This method integrates quantization, pruning informed knowledge distillation, and final pruning to compress VAE-based OOD detectors for efficient embedded deployment.",
            "method description": "The core of the method involves a systematic application of compression techniques to reduce the model size and execution time while preserving OOD detection accuracy.",
            "method steps": [
                "1) Perform quantization of model weights.",
                "2) Apply pruning informed by knowledge distillation to reduce model complexity.",
                "3) Conduct final pruning to achieve the desired memory footprint."
            ],
            "principle": "The effectiveness of this method lies in its ability to reduce model complexity and execution time without significantly sacrificing detection performance, leveraging the unique characteristics of VAE-based models."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using two VAE-based OOD detectors (\u03b2-VAE and optical flow) on a Jetson Nano platform, comparing performance metrics such as execution time and AUROC before and after applying the compression techniques.",
            "evaluation method": "The evaluation involved measuring execution time and AUROC under various configurations of the compression techniques, analyzing the trade-offs between model size, speed, and detection accuracy."
        },
        "conclusion": "The study demonstrates that the proposed design methodology effectively compresses VAE-based OOD detectors, achieving substantial reductions in memory and execution time while maintaining high detection performance, thus enabling their deployment in resource-constrained environments.",
        "discussion": {
            "advantage": "The key advantages include significant reductions in execution time and memory usage without a corresponding drop in OOD detection accuracy, making the method well-suited for embedded systems.",
            "limitation": "The method requires access to OOD samples during the design phase for cross-validation, which may not always be feasible, and it does not guarantee the absolute optimal configuration.",
            "future work": "Future research could explore alternative architectures and further refine the compression techniques to enhance performance and adaptability to various embedded systems."
        },
        "other info": {
            "info1": "The research was supported by the National Research Foundation, Prime Minister\u2019s Office, Singapore under its CREATE programme.",
            "info2": {
                "info2.1": "The methodology was validated using a mobile robotic platform, Duckietown.",
                "info2.2": "The experiments revealed that low training loss does not necessarily correlate with high OOD detection accuracy."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The problem involves detecting samples that fall outside the training distribution of machine learning models during real-time inference in resource-constrained environments."
        },
        {
            "section number": "2.1",
            "key information": "The paper defines OOD detection as a critical task for identifying samples that do not conform to the training distribution, especially in embedded systems."
        },
        {
            "section number": "3.4",
            "key information": "The proposed method is a Compressed VAE-based OOD Detector (C-VAE-OOD) that integrates quantization, pruning informed by knowledge distillation, and final pruning to compress VAE-based OOD detectors for efficient embedded deployment."
        },
        {
            "section number": "4.1",
            "key information": "The study highlights challenges in OOD detection for embedded systems, particularly the need for methods that require low memory and execution time while maintaining accuracy."
        },
        {
            "section number": "5.2",
            "key information": "The key advantages of the proposed method include significant reductions in execution time and memory usage without a corresponding drop in OOD detection accuracy, making it suitable for embedded systems."
        },
        {
            "section number": "7.1",
            "key information": "A key obstacle in existing methods is their struggle to meet stringent memory and execution time constraints while maintaining acceptable accuracy in OOD detection."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore alternative architectures and further refine the compression techniques to enhance performance and adaptability to various embedded systems."
        }
    ],
    "similarity_score": 0.6180916990568426,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Compressing VAE-Based Out-of-Distribution Detectors for Embedded Deployment.json"
}