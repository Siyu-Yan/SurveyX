{
    "from": "google",
    "scholar_id": "DijuQwjzRYQJ",
    "detail_id": null,
    "title": "Tcra-llm: Token compression retrieval augmented large language model for inference cost reduction",
    "abstract": "Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop.",
    "bib_name": "liu2023tcra",
    "md_text": "# TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction Junyi Liu , Liangzhi Li , Tong Xiang , Bowen Wang ,\ntyou AI Lab, Xiamen Key Laboratory of Women\u2019s Internet Health Management, Osaka University, Agency for Science, Technology and Research (A*STAR) {liujunyi, liliangzhi, xiangtong}@xiaoyouzi.com, bowen.wang@is.ids.osaka-u.ac.jp, qiany@ihpc.a-star.edu.sg\n# Abstract\nSince ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop.\n# 1 Introduction\nWith the increase in computing power and accumulation of enormous text data, large language models (LLMs) such as ChatGPT (OpenAI, 2023b) and GPT-4 (OpenAI, 2023a) have shown impressive performance in dialogue-based question-answering (QA), allowing them to interact with users fluently.\nIn open-domain QA where the models are engaged in casual conversations with users, LLMs exhibit astonishing performance by leveraging strong incontext learning ability. However LLMs may produce vague responses or incorrect answers in certain specialized domains, owing to the absence of relevant knowledge or a restricted scope of information acquired during the training stage, which might potentially result in untruthful answers and even cause physical damages to users (Xiang et al., 2023). For QA in such domains, retrievalaugmented generation (RAG) (Lewis et al., 2020), where the system retrieves external knowledge beforehand and then utilizes LLMs to generate answers leveraging retrieved knowledge, can greatly reduce the hallucinations generated (Shi et al., 2023; Shuster et al., 2021). Many current commercial LLMs are black-box models, where the model architectures and the weight information are not disclosed. These LLMs own superior text comprehension abilities, yet in many cases they can only output desired answers through complicated prompt engineering. On the other hand, deploying open-source LLMs to local servers is resource-intensive, in contrast to deploying smaller models such as T5 (Raffel et al., 2020). Some commercial LLMs like GPT-3.5-turbo (OpenAI, 2023c) and GPT-4 offer access through API calls; however, these models charge users based on the size of input and output1. For individuals or companies looking to create their own services using LLMs through API calls, utilizing commercial ones can be resource-consuming if requests are made frequently. Therefore, it is necessary to minimize the number of input tokens while maintaining optimal performance during the API calls.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33e9/33e99b37-bbb4-445b-9b45-f1cfa6a223a5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Illustration of different ways to utilize LLMs for QA. Top: directly using LLM. Middle: using a retrieval-augmented LLM. Bottom: using retrieval-augmented LLM with our proposed token compression methods.</div>\nIn this work, we propose a token compression scheme specifically designed for the retrievalaugmented LLMs (shown in Figure 1), namely, Token Compression Retrieval Augmented Large Language Model (TCRA-LLM). Our proposed scheme can reduce up to 65% of the token size with additional 0.3% improvement on accuracy when doing QA on our proposed dataset called FoodRecommendation DB (FRDB). We propose two approaches to reduce the token size of the LLMs\u2019 input: summarization compression and semantic compression. For summarization compression, we leverage self-instruct (Wang et al., 2022) scheme to build multiple summarization datasets with varying lengths to fine-tune the mT5 model (Xue et al., 2020). The samples from the summarization datasets are generated by GPT-3.5-turbo (OpenAI, 2023c), which is instructed to shorten the summary of the input sentences in an iterative manner. The semantic compression approach is based on a simple yet effective intuition, that removing semantically less important words in a sentence won\u2019t drastically change its semantic. Here, we deploy a multi-lingual sentence-transformer (Reimers and Gurevych, 2020) to encode sentences into embeddings where the distances between original and perturbed embeddings are used to measure the semantic deviation from the original meaning. Larger semantic deviation indicates that the corresponding word owns more important semantic in the sentence. We conduct an iterative process that measures the semantic importance of each word in the\n3. We propose two token compression methods (Section 4.2), both of which can reduce the number of input tokens during the API calls of retrieval-augmented commercial LLMs while maintaining optimal performance.\n# 2 Related Work\nLLMs such as GPT-3 (Brown et al., 2020), PALM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), Bloom (Scao et al., 2022), and LLaMA (Touvron et al., 2023) are trained on massive amounts of data and have demonstrated powerful comprehension capabilities. These models have been deployed in a breadth of tasks and achieve promising results (Zhang et al., 2023; Ashok and Lipton, 2023; Lu et al., 2023; Wang et al., 2023; Xiang et al., 2023). One major barrier that prevents more people from participating in commercial deployment of the LLMs is\ntheir training and hosting costs. A way to reduce such costs is through training smaller domainspecific models such as BioMedLM (Bolton et al., 2022), BloombergGPT (Wu et al., 2023), and LawGPT (Nguyen, 2023). Such domain-specific training enables smaller LLMs to be applied to certain fields but still requires huge investment. For instance, BloombergGPT is trained on 512 40GB A100 GPUs with the total budget being approximately $2.7 million (Sheikh, 2023). Alternatively, LLMs can be used without finetuning through retrieval augmentation leveraging external data sources, where the retrieved data is used as supplementary information to help LLMs improve logical reasoning and language generation (Thorne et al., 2021; Izacard et al., 2022). Previous experiments (Ram et al., 2023) show that additional information can be beneficial for LLMs across different model sizes. Retrieval augmentation eliminates the cost of tuning an in-house LLM on new data, and can be easily integrated with commercial LLM services such as ChatGPT (OpenAI, 2023b) from OpenAI or Bard (Pichai, 2023) from Google. Many studies have shown, applying retrieval augmentation to the commercial LLMs such as ChatGPT allow the models to gain knowledge in specific domains such as natural science and medicine (Soong et al., 2023; Inaba et al., 2023) which is not revealed during their training and retrieval augmentation can be further improved by applying more sophisticated retrievers (Shi et al., 2023). However, commercial LLMs all have a limitation on input lengths which put an upper ceiling on the amount of information that can be fed into a LLM. Later models such as GPT-4 has looser restriction but the inference cost increases drastically in comparison with other models. Some previous work applies template-based prompt optimization (Santra et al., 2023), which select retrieved context (Mallen et al., 2022) in an adaptive manner, or uses cascading of LLMs with different sizes (Chen et al., 2023) to reduce the inference costs. Our proposed method has no conflict with these methods and can be used with them simultaneously.\n# 3 FRDB\nWe build a Food Recommendation Dataset in Chinese called FRDB, for recommending foods that are safe to consume for women before/during/after their pregnancy as well as infants. It contains two\nFood type\nCount\u2193\nEntr\u00e9e\n31\nVegetables\n31\nSeafood\n22\nSweets\n22\nMedicine/Health supplement\n20\nFruit\n17\nGrains\n16\nSoft drink\n13\nCondiment\n10\nMeat/Eggs\n10\nSoybeans/Dried fruit\n6\nDairy products\n2\nTotal\n200\nparts: multiple-choice (MC) QA pairs and a knowledge database. The QA pairs contain 1,000 samples that cover 200 types of food. The categories of foods are shown in Table 1. The possible answers to the question falls into three choices based on the increasing degree of recommendations ranging from 1 (avoid) to 3 (highly recommend). Each type of food has five recommendation rating corresponding to five groups: pre-pregnancy, pregnancy, postpartum, lactation, and infant. Additionally, we build a knowledge database that contains 7,588 entries; details of the entries are shown in Table 2. The distribution of sentence length in the knowledge database is shown in Figure 2.\nMean\nMax\nMin\nStd.\n# of words\n88\n248\n12\n27\nAll the information has been verified by the health-domain professionals. During the verification, we remove the text that is ambiguous to the human annotators. Two samples of knowledge are shown in Table 3. Sample questions are available in the Appendix A.\n# 4 Method\nTypically, a retrieval-augmented LLM consists of three components (shown in Figure 1), a knowledge database, a retriever, and the LLM. The knowledge\nHigh quality knowledge\nAmbiguous knowledge\nConsuming mushrooms after childbirth is benefi-\ncial for postpartum recovery, constipation relief,\nand promoting lactation due to their rich B vi-\ntamins, protein, and amino acids. A moderate\namount of intake based on the recovery status is\nrecommended.\nPostpartum mothers are safe to consume a moderate amount of cake.\nCakes are easier to digest and absorb for postpartum mothers with\nweaker gastrointestinal systems.\nHowever, cakes have relatively\nsmaller nutritional diversity and they should be consumed together\nwith vegetables, fruits, and meats to make the nutrition more balanced.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/878b/878b4e83-998a-44df-88e0-e47f5d32ea1a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The distribution of sentence length in FRDB knowledge database.</div>\ndatabase contains all available domain-specific knowledge. The retriever applies the question as a query to search for the relevant information from the knowledge database. The retrieved information is then formulated as the context packaged together with questions as a prompt for LLM to generate an answer. Our proposed methods are able to compress the retrieved information and formulate shorter context but maintain the effectiveness of retrieval augmentation. In this section, we go through the pipeline of a retrieval-augmented LLM system for QA and introduce our proposed token compression methods.\n# 4.1 Information Retrieval\nGenerally, the first step for LLM\u2019s retrieval augmentation is knowledge retrieval. Given an user query x, the retriever extracts k pieces of information from the knowledge database D = {d1, d2, \u00b7 \u00b7 \u00b7 , dm} that are most likely to be relevant to x. There are two mainstream retrieval methods: dense retrieval (Karpukhin et al., 2020; Ni et al., 2021) and sparse retrieval (Robertson et al., 2009). Dense retrieval first encodes queries and documents into dense embeddings (Huang et al., 2013; Yi et al., 2019) using pre-trained neural en-\ncoders and then finds a query\u2019s nearest neighbors in the embedding space using a relevancy measure such as cosine similarity (Yu et al., 2021). Sparse retrieval, on the other hand, maps queries and documents into a high-dimensional space with methods such as TF-IDF (Sparck Jones, 1972; Jones, 1973) and the most relevant documents are returned to the user as the answer. Typical example of sparse retrieval is BM25 (Robertson et al., 1995). Here we evaluate both dense and sparse retrieval methods. For dense retrieval, we follow a similar process from Huang et al. (2013): we first encode the text using the GPT-embedding (OpenAI, 2022) provided by OpenAI, then deploy vector database FAISS Index (Johnson et al., 2019) to store the embeddings, enabling faster manipulation on them. For sparse retrieval, we deploy BM25 (Robertson et al., 1995) which is considered the standard way.\n# 4.1.1 Next Sentence Prediction\nThe retrieved top-k results Dk = {dk 1, dk 2, \u00b7 \u00b7 \u00b7 , dk k} (dk i are the i-th retrieved elements from the original set D where 1 \u2264i \u2264k) are deemed as the most relevant ranked by the retrieval method. Using only the top-1 result as the context is not always reliable, but using more retrieved results will consume more space in the input tokens, leading to higher costs; therefore, to improve reliability without incurring additional costs, we propose to use next-sentence prediction (NSP) as a second-stage ranking method. It is based on an intuitive assumption that the retrieved information is more likely to be predicted as the next sentence of the question if it is more related to the question. The implementation of this approach is based on the pre-trained NSP module from BERT (Devlin et al., 2018); the selected sentence s with maximum probability from NSP is selected as the best result (See Equation 1).\nWe conduct experiments to evaluate the impact of including NSP into the retrieval-augmented LLM. Here we use OpenAI\u2019s GPT-3.5-turbo as the base LLM and evaluate it on the FRDB dataset using the top-1 retrieval results as the context. The result is shown in Table 4. There is a minor performance gain using NSP with both GPT-embedding and BM25 and thus we keep this NSP module in all our later experiments.\nMethod\nAcc. (%)\nEmbedding\n89.1\nEmbedding +NSP\n90.2\nBM25\n83.4\nBM25+NSP\n84.9\nTable 4: Performance comparison of retrieval methods with and without NSP. The retrieved sentences are directly used as context and fed into GPT-3.5-turbo. Acc. stands for accuracy.\nFrom the experiment, we also see that the combination of dense retrieval with the NSP approach obtain the highest accuracy. We tune the value of k by searching it from 1 to 10 and perform corresponding evaluation on the FRDB dataset. The experiment results are shown in Figure 3. We find that k = 3 is the optimal choice and we will adhere to this value in all our subsequent experiments.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4bda/4bda7131-c5e4-4568-ab27-86fe8b56c7ba.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Evaluation of the retrieval performance using GPT-embedding and NSP with different choices of k.</div>\n# 4.2 Token Compression\nThe retrieval text is usually long and easily consume a huge amount of space from the input tokens during the API calls while using commercial LLMs. In order to mitigate this, we propose two methods to compress the retrieved text. The first one is the summarization compression which produces shorten the original text by rephrasing. The second method is semantic compression which we perturb the original sentence and rank the impact of the semantic change from each word in the sentence. The\nwords with lower semantic impact on the sentence are removed.\n# 4.2.1 Summarization Compression\nSummarization models like the mT5 model (Xue et al., 2020) have been widely used in many applications to shorten the input text, but they could not output summary with arbitrary length due to the constraint of its training data. To solve this, we propose to build a summarization model that is able to output summary with various lengths. To build such a model, we leverage the power of self-instruct (Wang et al., 2022) where we use GPT3.5-turbo to generate training datasets. The procedure of the data generation is shown in Figure 4. First, we start with a text x from the dataset, then pack it with additional prompt instruction as we illustrated in Figure 4 and send it to GPT-3.5-turbo to generate a summary. If the length of the summary meets requirements, the procedure is ended; otherwise, a follow-up prompt will instruct GPT3.5-turbo to further shorten the summary to the desired length. By doing this, we build a collection of training datasets with different summary length. We build three datasets that are 30%, 50%, and 70% of their original length. Each dataset is used to fine-tune one summary model independently. We randomly extract from FRDB and generate 400, 50, and 50 samples for training, validation, and testing respectively. Training on the generated datasets not only enables the model to produce summaries of the desired length, but also familiarizes the model with domain-specific information by doing further domain adaptation (Gururangan et al., 2020).\nWe propose another compression method based on perturbations of the original sentence and ranking the impact of the semantic importance for each word in the sentence where words with less importance will be removed. We deploy a multi-lingual sentence-transformer (Reimers and Gurevych, 2020) to encode a sentence into embedding \u03c70. Then we iteratively remove one word in the sentence and obtain an updated embedding \u03c7i where i is the index of the word in the sentence and n is the number of words in the sentence. We have a new set L that tracks the Euclidean distance between the original and perturbed embedding L = {L2(\u03c70, \u03c71), . . . , L2(\u03c70, \u03c7n)}. We denote pj as the value of the j-th percentile in L. The Lj is the new subset that removed the bottom\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd61/cd61be7f-1730-4f85-8d2c-f94e9f911baa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Self-instruct training data generation.</div>\nj-th percentile elements:\n(2)\nThe words corresponding to the elements in set Lj are extracted as the context for the LLM.\n# 5 Evaluation\n# 5.1 Experiment Setup\nWe conduct studies on the FRDB dataset. The summarization compression is based on the pre-trained mT5-multilingual-XLSum (Xue et al., 2020). The maximum input and output length is set to 512, the learning rate is 2e-5, the number of epochs is set to 10, the batch size is 2, and the rest of the settings follow the default settings from the models. The training of the mT5 models are conducted on the server that contains an AMD EPYC 7763 CPU, 256GB RAM, and NVIDIA 4090 GPU with 24GB memory. Following the method described in section 4.2.1, three shortened versions of the summarization dataset with 30%, 50%, and 70% of its original length are generated. Each version is used to fine-tune an mT5 summarization model. The pre-trained multi-lingual sentencetransformer (Reimers and Gurevych, 2020) is used as an embedding encoder for semantic compression. Three compressed sentences in 30%, 50%,\nand 70% of their original length are generated. The GPT-3.5-turbo (OpenAI, 2023c) is used for processing prompts generated by our methods.\n# 5.2 Token Compression Results\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb09/cb09acca-0d67-4f5d-a2c1-2ddd2c6c7f6d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Percentage of orignal length (%)</div>\nFigure 5: Performance comparison of token compression methods. The horizontal dashed lines represent accuracy from the methods that do not output variable token lengths.\nWe conduct experiments on FRDB which contains 1,000 maternity/infant food related domainspecific multiple-choice questions, each of which only contains one correct answer. Three sentencecompression methods are evaluated in our experiments: 1) random deletion which randomly deletes words from a sentence, 2) summarization compression, and 3) semantic compression. The experiment results are shown in Figure 5. To construct the baseline, we evaluate the GPT-3.5turbo performance in two configurations: without retrieval and with retrieval but without token compression. We observe from the results that, once additional information is fed into the model as context, the accuracy immediately improves, from 51% to 90.2%. This shows the benefit that retrieving domain information brings to the LLM. Next, we compare using the original pre-trained mT5 based model with using the version where the model is fine-tuned on the datasets generated by self-instruction. With fine-tuning, the accuracy improves dramatically from 60% to 90.6%. The summarization model without fine-tuning output has an average length of 15 words, compare with 88, 46, and 21 words for our 70%, 50%, and 30% length dataset fine-tuned model respectively. It shows that the summarization model might remove critical information by mistake if the input text\nis on a topic that the summarization model is not familiar with. A small fine-tuning set (400 samples) is enough to help the summarization model adapting to the new domain. The second compression method we propose is semantic compression. It has better flexibility to generate variable lengths of tokens but delivers lower performance than the summarization compression method. The baseline for our experiment is random deletion where we randomly delete a certain percentage of words. This random deletion consistently scores lower performance than both of our proposed algorithms.\n# 5.3 Cost Reduction Comparison\nOur proposed summarization compression method is tested on a server with one NVIDIA 4090 GPU (24GB), 32GB RAM and Intel i7 CPU. The runtime for such a system is on average 0.383s per sample. A similar server on the AWS, i.e., g5.2xlarge, has an A10G GPU, 32GB RAM, and 8-core vCPU and the hourly price for such a system is $0.485. In one hour, such a system can process approximately 9,400 summarizations, so the cost per summarization is $5.16e-5. Assume the system utilization rate is only 50%, it means that the cost per summarization is $1.0319e-04. The GPT-3.5-turbo itself does not have enough knowledge to precisely answer the question from our dataset (51% accuracy if without additional context in the prompt). Thus, both the common retrieval-augmented GPT-3.5-turbo and our system (retrieval-augmented GPT-3.5-turbo with additional token compression) require dense retrieval to reach acceptable performance, and the retrieval cost, which is $0.0001 per 1,000 query tokens, is the same for both systems. Since the original questions are typically short, we can assume that the average length of them is about 128 tokens, which translates into $1.2500e-05 per question for the retrieval. Assuming at full size the input has 512 tokens and the output has 64 tokens, the total cost for the common retrieval-augmented GPT-3.5-turbo (for both retrieval and QA using API calls) is about $8.9050e-04 per question. In comparison, our algorithm can compress the retrieved context of GPT3.5-turbo to 35% of its original length, which translates into an averagely 50% of reduction during the API calls. Thus, The cost using our token compression system is around $6.1869e-04 per question. It reduces the overall costs by 30%.\n# 5.4 Information Entropy vs. Accuracy\nIn the next experiment, we investigate the impact of the information entropy on the accuracy of different token compression methods. We measure the word frequency from the FRDB dataset and use it as a probabilistic model to calculate the information entropy of each word. The mean word information entropy is calculated on each sentence to normalize the entropy. The results for the three token compression methods is shown in Figure 6. The random deletion method removes words randomly which leads to the average information entropy for different sentence lengths approximately the same. On the other hand, the semantic compression algorithm removes the words that have less semantic meaning. Our experiment shows that, the average information entropy goes lower as sentences become shorter, indicating that the sentence becomes less compressible. Additionally, the average word information entropy is positively correlated with the accuracy when semantic compression is used, showing that higher information will benefit the model performance. On the contrary, the summarization compression shows distinct phenomenon. Instead of naively removing words, the summarization compression compresses the original sentences into different lengths by rephrasing sentences. By doing this, the shortened sentences obtain lower average information entropy but the accuracy stays at a similar level in comparison with the original sentences. The lower average information entropy indicates that sentences become more condensed but the semantic of the sentences stays approximately the same.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4567/4567d4bf-45a2-444d-961b-87212de2993e.png\" style=\"width: 50%;\"></div>\nFigure 6: Impact of average information entropy on accuracy.\nNext, we investigate the impact of cosine similarity between original and compressed sentences on accuracy and we find a positive correlation between accuracy and cosine similarity value. It indicates\ncloser to the original semantic meaning would produce better accuracy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8856/88566c60-3755-4b65-83cd-098a865b3684.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Impact of cosine similarity between original and compressed sentence on accuracy.</div>\nOur summarization model is built based on the pre-trained mT5 model and fine-tuned on our selfinstruct generated dataset. We generate three datasets which are 30%, 50%, and 70% of the length compared to their original text. Three different models are fine-tuned independently. Figure 8 shows the distribution of sentence length from our three fine-tuned models. At 70% compression, the summary text shifts from an average of 88 words to 46 words for 50% length and 21 words for 30% length.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7668/76682a24-e63b-4764-9a27-5a4050bce1d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Distribution of sentence lengths with different summarization lengths.</div>\n# 5.6 Compression Rate vs. Entropy\nWe conduct a study on the compression rate of our mT5 model fine-tuned with the 30% length dataset. Our input consists of 1) the original length of the sentence, 2) average word entropy, and 3) accumulated word entropy. We deploy a simple linear regression algorithm to predict the compression\nrate. The result is shown in Fig. 9. We find that, there is a positive correlation (0.31) between our selected input and compression rate with an RMSE of 11.4% and R-squared of 9.6%. This indicates the compression rate of each sentence can be estimated prior to the summarization process.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b125/b1250939-e3df-40b9-a51b-44c53cd4244a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Visualization of multi-variable linear regression on predicting compression rate.</div>\n# 5.7 Ablation Study\nFrom our experiments, we find the summarization compression method delivers the best performance. Here we compare different retrieval methods and investigate what the optimal settings are. Four configurations are evaluated: embedding only, BM25 only, embedding first then BM25, and BM25 first then embedding. The first two configurations are straightforward; in the third configuration, we apply the embedding-based method to extract the topq results, then apply BM25 to extract top-k results from the q results where q \u2265k. In the fourth configuration, we reverse the order where we first extract the top-q results using BM25 and then extract the top-k results from the q results using embeddingbased methods. We set k = 3 based on previous experiments. The evaluation results are shown in Table 5. We find the straightforward dense retrieval approach achieves the best performance.\n# 6 Conclusion\nIn this paper, we propose two methods that reduce the token size for retrieval-augmented LLM. Additionally, we propose a food recommendation dataset contains domain-specific knowledge to benchmark the performance of retrieval-augmented LLM performance on the GPT-3.5-turbo. We carefully select a subset that focuses on 200 types of food recommendations for maternity and infant people. Without retrieval augmentation, the commercial GPT-3.5-turbo model is only able to get\nMethod\nTop-q\nAcc. (%)\nEmbedding\nn/a\n90.9\nEmbedding+BM25\n10\n89.7\nEmbedding+BM25\n100\n88.0\nBM25\nn/a\n74.7\nBM25+Embedding\n10\n89.3\nBM25+Embedding\n100\n89.8\nTable 5: Evaluation of different retrieval algorithm configurations. The top-q column indicates the q value used in the search. Acc. stands for accuracy.\n51% percent of the question right, compared to 90.2% with retrieved context. We use this 90.2% as our goal and compare the performance of different token compression algorithms. Our proposed summarization compression achieves the best performance, reaching 90.5% of accuracy with 65% token reduction on the retrieved context. It indicates that the summarized text maintains a similar level of critical information but with a significantly shorter length, showing promising ability of our proposed method. The semantic compression method can further remove the words that have lower semantic meaning and provides a more flexible way to trade-off the length of sentences with accuracy.\n# Limitations\nThe goal of our model is to reduce the token size for retrieval-augmented LLMs. The compression rate is determined based on the methods\u2019 ability to condense sentences while preserving as much of their essential information as possible. If the sentences are already short and compacted in meaning, the compression rate won\u2019t be able to be low if we want to maintain most of the critical information within the sentence. Our algorithm is designed for large commercial LLMs and smaller open-source LLMs may not experience similar levels of performance gain.\n# Ethics Statement\nOur proposed methods are designed solely for the goal of reducing cost while maintaining the performance using commercial LLMs through API calls. The algorithms are not designed for activities that are in violation of human rights or with military purposes. The data we collected for our proposed dataset does not contain any privacy information.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083.\n# Jamiel Sheikh. 2023. Bloomberg Uses Its Vast Data To Create New Finance AI.\nFaisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n# Appendix\n# B Sentence compression examples Examples are shown in Table 6.\nC QA examples Examples of QA are shown in Table 7.\n# D Knowledge examples\nExamples of knowledge utilized for answering the questions are shown in Table 8.\nDataset\nOriginal\nCompressed\nFRDB\nBabies are strictly prohibited from consuming\ndonkey meat buns. The reason is that don-\nkey meat buns usually contain a large amount\nof spices and seasonings, with high salt con-\ntent, which will increase the metabolic burden\non the baby\u2019s kidneys. Furthermore, infants\nand young children are in a critical period for\ndeveloping taste preferences, and frequently\nconsuming donkey meat buns may affect the\nformation of their taste buds. It is not recom-\nmended to feed babies donkey meat buns.\nIn the first-instance procedure of\na public prosecution case, if the\nparticipants or observers disrupt\nthe courtroom order, the presid-\ning judge shall handle the situa-\ntion according to the law. Unau-\nthorized recording, videotaping,\nphotographing, or disseminating\ntrial proceedings through mail,\nblog, or microblog may lead to\nthe temporary seizure of storage\nmedia or relevant equipment.\n<div style=\"text-align: center;\">Questions (English translation)</div>\n\u5bf9\u4e8e\u2019\u6839\u636e\u201c\u4ea7\u5987\u5c11\u5403\u9e45\u809d\u3002\n\u539f\u56e0\u662f\uff1a\u4ea7\u540e\u5988\u5988\u9002\u91cf\u98df\n\u7528\u9e45\u809d\u6709\u5229\u4e8e\u4fc3\u8fdb\u4f24\u53e3\u7684\u6108\n\u5408\uff0c\u5f25\u8865\u751f\u4ea7\u5931\u8840\u548c\u91cd\u5efa\u809d\n\u810f\u94c1\u50a8\u5907\uff0c\u63d0\u9ad8\u4e73\u6c41\u8d28\u91cf\u3002\n\u8fd8\u80fd\u63d0\u9ad8\u5988\u5988\u7684\u514d\u75ab\u529b\uff0c\n\u5177\u6709\u6297\u6c27\u5316\u3001\u7f13\u89e3\u8870\u8001\u7684\u4f5c\n\u7528\u3002\u5efa\u8bae\u4ea7\u540e\u5988\u5988\u53ef\u9002\u91cf\u98df\n\u7528\u3002\u201d\u4ea7\u5987\u98df\u7528/\u996e\u7528\u9e45\u809d\u2019\uff0c\n\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u6700\u9002\u5408\u7528\u4e8e\u63cf\n\u8ff0\u4e0a\u8ff0\u884c\u4e3a\uff1f\u9009\u9879\uff1a(1) \u63a8\n\u8350\uff0c(2) \u4e2d\u7acb\uff0c(3) \u907f\u514d\nAccording to \"Maternity should eat less foie\ngras.\nThe reason is:\nmoderate consump-\ntion of foie gras after childbirth can promote\nwound healing, make up for blood loss during\nchildbirth, rebuild liver iron reserves, improve\nbreast milk quality. It can also improve the\nimmunity of mothers, and has an antioxidant\nand anti-aging effect. It is recommended that\npostpartum mothers can consume it in moder-\nation.\" Which of the following options is most\nsuitable to describe the behavior of postpartum\nmothers consuming/eating foie gras? Options:\n(1) Recommend, (2) Neutral, (3) Avoid\nNeutral\n\u5bf9\u4e8e\u2019\u6839\u636e\u201c \u5907\u5b55\u5973\u6027\u53ef\u4ee5\n\u5403\u5496\u55b1\u3002\u539f\u56e0\u662f\uff1a\u5496\u55b1\u5c5e\u4e8e\n\u6df7\u5408\u8c03\u5236\u7684\u9999\u6599\uff0c\u80fd\u8c03\u8282\u80a0\n\u80c3\u8815\u52a8\uff0c\u63d0\u9ad8\u98df\u6b32\uff0c\u5176\u8f9b\u8fa3\n\u7a0b\u5ea6\u6839\u636e\u914d\u6599\u800c\u53d8\uff0c\u8fc7\u4e8e\u8f9b\n\u8fa3\u7684\u5496\u55b1\u5bf9\u80c3\u6709\u4e00\u5b9a\u7684\u523a\u6fc0\n\u6027\uff0c\u5907\u5b55\u671f\u5973\u6027\u53ef\u4ee5\u6839\u636e\u81ea\n\u5df1\u7684\u53e3\u5473\u559c\u597d\u9009\u62e9\u5408\u9002\u8fa3\u5ea6\n\u7684\u5496\u55b1\u3002\u201d\u5907\u5b55\u5973\u6027\u98df\u7528/\u996e\n\u7528\u5496\u55b1\u2019\uff0c\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u6700\n\u9002\u5408\u7528\u4e8e\u63cf\u8ff0\u4e0a\u8ff0\u884c\u4e3a\uff1f\u9009\n\u9879\uff1a(1) \u63a8\u8350\uff0c(2) \u4e2d\u7acb\uff0c(3)\n\u907f\u514d\nRegarding \"Females preparing for pregnancy\ncan eat curry. The reason is that curry is a\nmixed seasoning that can regulate intestinal\nmovement, increase appetite, and its spiciness\nvaries according to the ingredients. Curry that\nis too spicy can stimulate the stomach to some\nextent. Females preparing for pregnancy can\nchoose curry with appropriate spiciness based\non their own taste preferences.\" Which of the\nfollowing options is most suitable to describe\nthe behavior of females preparing for preg-\nnancy consuming/eating curry? Options: (1)\nRecommend, (2) Neutral, (3) Avoid\nRecommend\n\u5bf9\u4e8e\u2019\u6839\u636e\u201c6\u6708\u5927\u7684\u5b9d\u5b9d\u5c11\u5403\n\u7389\u7c73\u6c41\u3002\u539f\u56e0\u662f\uff1a\u7389\u7c73\u6c41\u5bcc\n\u542b\u7ef4\u751f\u7d20\u3001\u77ff\u7269\u8d28\u548c\u78b3\u6c34\u5316\n\u5408\u7269\uff0c\u53ef\u4e3a\u5b9d\u5b9d\u63d0\u4f9b\u80fd\u91cf\uff0c\n\u4fc3\u8fdb\u5176\u751f\u957f\u53d1\u80b2\uff0c\u4e14\u5438\u6536\u7387\n\u8f83\u9ad8\u30026\u6708\u9f84\u4ee5\u540e\u7684\u5b9d\u5b9d\u53ef\n\u5c11\u91cf\u98df\u7528\uff0c\u6ce8\u610f\u9c9c\u69a8\u7389\u7c73\u6c41\n\u4e0d\u8981\u6dfb\u52a0\u7cd6\uff0c\u4ee5\u9632\u6444\u5165\u8fc7\u591a\n\u7684\u7cd6\uff0c\u4e0d\u5229\u4e8e\u5b9d\u5b9d\u7684\u53e3\u8154\u5065\n\u5eb7\u3002\u201d6\u6708\u9f84\u7684\u5b9d\u5b9d\u98df\u7528/\u996e\n\u7528\u7389\u7c73\u6c41\u2019\uff0c\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u6700\n\u9002\u5408\u7528\u4e8e\u63cf\u8ff0\u4e0a\u8ff0\u884c\u4e3a\uff1f\u9009\n\u9879\uff1a(1) \u63a8\u8350\uff0c(2) \u4e2d\u7acb\uff0c(3)\n\u907f\u514d\nRegarding \"Babies at the age of 6 months\nshould consume less corn juice. The reason\nis that corn juice is rich in vitamins, minerals\nand carbohydrates, which can provide energy\nfor babies, promote their growth and develop-\nment, and has a high absorption rate. Babies\nover 6 months old can consume it in modera-\ntion, but be mindful that freshly squeezed corn\njuice should not contain added sugar to pre-\nvent excessive intake of sugar, which could\nbe detrimental to baby\u2019s oral health.\" Which\nof the following options is most suitable to\ndescribe the behavior of a 6-month-old baby\nconsuming/drinking corn juice? Options: (1)\nRecommend, (2) Neutral, (3) Avoid\nNeutral\n\u5bf9\u4e8e\u2019\u6839\u636e\u201c6\u6708\u5927\u7684\u5b9d\u5b9d\u5c11\u5403 \u7389\u7c73\u6c41\u3002\u539f\u56e0\u662f\uff1a\u7389\u7c73\u6c41\u5bcc \u542b\u7ef4\u751f\u7d20\u3001\u77ff\u7269\u8d28\u548c\u78b3\u6c34\u5316 \u5408\u7269\uff0c\u53ef\u4e3a\u5b9d\u5b9d\u63d0\u4f9b\u80fd\u91cf\uff0c \u4fc3\u8fdb\u5176\u751f\u957f\u53d1\u80b2\uff0c\u4e14\u5438\u6536\u7387 \u8f83\u9ad8\u30026\u6708\u9f84\u4ee5\u540e\u7684\u5b9d\u5b9d\u53ef \u5c11\u91cf\u98df\u7528\uff0c\u6ce8\u610f\u9c9c\u69a8\u7389\u7c73\u6c41 \u4e0d\u8981\u6dfb\u52a0\u7cd6\uff0c\u4ee5\u9632\u6444\u5165\u8fc7\u591a \u7684\u7cd6\uff0c\u4e0d\u5229\u4e8e\u5b9d\u5b9d\u7684\u53e3\u8154\u5065 \u5eb7\u3002\u201d6\u6708\u9f84\u7684\u5b9d\u5b9d\u98df\u7528/\u996e \u7528\u7389\u7c73\u6c41\u2019\uff0c\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u6700 \u9002\u5408\u7528\u4e8e\u63cf\u8ff0\u4e0a\u8ff0\u884c\u4e3a\uff1f\u9009 \u9879\uff1a(1) \u63a8\u8350\uff0c(2) \u4e2d\u7acb\uff0c(3) \u907f\u514d\n<div style=\"text-align: center;\">Knowledge (English translation)</div>\nKnowledge\nKnowledge (English translation)\n\u5907\u5b55\u5973\u6027\u53ef\u4ee5\u5403\u571f\u8c46\u6ce5\u3002\u539f\n\u56e0\u662f\uff1a\u5907\u5b55\u4eba\u7fa4\u53ef\u4ee5\u98df\u7528\uff0c\n\u4e0d\u8fc7\u571f\u8c46\u6ce5\u5347\u7cd6\u8f83\u5feb\uff0c\u5efa\u8bae\n\u4e00\u6b21\u4e0d\u8981\u5403\u592a\u591a\u3002\nFemales preparing for pregnancy can eat mashed pota-\ntoes. The reason is that it is safe for this group to con-\nsume mashed potatoes. However, mashed potatoes have\na high glycemic index, so it is recommended not to eat\ntoo much at once.\n\u5b9d\u5b9d\u4e0d\u80fd\u5403\u751f\u9c7c\u7247\u3002\u539f\u56e0\n\u662f\uff1a\u9c7c\u8089\u4e2d\u542b\u6709\u5927\u91cf\u7684\u4e0d\u9971\n\u548c\u8102\u80aa\u9178\uff08\u5c24\u5176\u662fDHA\uff09\uff0c\n\u6709\u52a9\u4e8e\u4fc3\u8fdb\u5b9d\u5b9d\u5927\u8111\u53ca\u89c6\n\u529b\u53d1\u80b2\u3002\u4f46\u751f\u9c7c\u7247\u5982\u679c\u5904\n\u7406\u4e0d\u5f53\uff0c\u5bb9\u6613\u611f\u67d3\u75c5\u83cc\u548c\u5bc4\n\u751f\u866b\u3002\u4e0d\u5efa\u8bae\u7ed9\u5b9d\u5b9d\u5403\u751f\u9c7c\n\u7247\u3002\nBabies should not eat raw fish slices. The reason is\nthat fish contains a large amount of unsaturated fatty\nacids (especially DHA), which can help promote the\ndevelopment of the baby\u2019s brain and vision. However, if\nraw fish slices are not properly processed, they can easily\nbecome contaminated with bacteria and parasites, posing\nhealth risks to babies. Therefore, it is not recommended\nto feed raw fish slices to babies.\n\u5b55\u5987\u53ef\u4ee5\u5403\u7f57\u975e\u9c7c\u3002\u539f\u56e0\n\u662f\uff1a\u7f57\u975e\u9c7c\u4e2d\u542b\u6709\u975e\u5e38\u4e30\u5bcc\n\u7684\u4e0d\u9971\u548c\u8102\u80aa\u9178\u3001\u86cb\u767d\u8d28\u548c\n\u591a\u79cd\u6c28\u57fa\u9178\uff0c\u5bb9\u6613\u88ab\u4eba\u4f53\u6d88\n\u5316\u5438\u6536\u3002\u80fd\u4e3a\u5b55\u5988\u63d0\u4f9b\u591a\u79cd\n\u8425\u517b\u7269\u8d28\uff0c\u5e2e\u52a9\u80ce\u513f\u9aa8\u9abc\u751f\n\u957f\u548c\u795e\u7ecf\u7cfb\u7edf\u53d1\u80b2\u3002\nPregnant women can eat tilapia. The reason is that tilapia\nis rich in unsaturated fatty acids, protein and various\namino acids, which are easily digested and absorbed by\nthe human body. It can provide pregnant women with\nvarious nutrients to help promote fetal bone growth and\ndevelopment of the nervous system.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of high inference costs associated with deploying commercial retrieval-augmented large language models (LLMs) due to the increased input token size from retrieved context. Previous methods have not adequately addressed the need for cost-effective solutions, making a new breakthrough necessary.",
        "problem": {
            "definition": "The problem is the excessive cost of using retrieval-augmented LLMs, which arises from the large number of input tokens generated by retrieved context.",
            "key obstacle": "The main challenge is that existing methods do not effectively reduce the input token size without compromising performance, leading to high operational costs."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that not all words in a sentence contribute equally to its semantic meaning, allowing for potential compression without significant loss of information.",
            "opinion": "The proposed idea involves a token compression scheme that utilizes summarization and semantic compression to reduce token size while maintaining accuracy.",
            "innovation": "The key innovation lies in the dual approach of summarization compression and semantic compression, which differentiates it from existing methods that do not combine these strategies."
        },
        "method": {
            "method name": "Token Compression Retrieval Augmented Large Language Model",
            "method abbreviation": "TCRA-LLM",
            "method definition": "TCRA-LLM is a method designed to reduce the token size of input for retrieval-augmented LLMs through summarization and semantic compression.",
            "method description": "It compresses the retrieved context to minimize the number of input tokens while preserving critical information.",
            "method steps": [
                "Retrieve relevant information based on user queries.",
                "Apply summarization compression to shorten the retrieved text.",
                "Use semantic compression to remove less important words based on their semantic impact."
            ],
            "principle": "The effectiveness of this method stems from the reduction of unnecessary tokens while retaining the essential semantic content, thus lowering inference costs."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the Food-Recommendation DB (FRDB) dataset, which focuses on food recommendations for women during pregnancy and infants, comparing the performance of the proposed methods against baseline retrieval methods.",
            "evaluation method": "Performance was measured through accuracy assessments on multiple-choice questions, analyzing the impact of token compression on the retrieval-augmented LLM's output."
        },
        "conclusion": "The proposed methods demonstrated significant improvements in reducing token size by up to 65% while maintaining accuracy levels close to those of the original retrieval-augmented LLM, highlighting their effectiveness in cost reduction.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to significantly reduce inference costs while maintaining or improving accuracy through effective token compression.",
            "limitation": "A limitation is that the compression rate may be constrained when dealing with already compact sentences, which may not allow for further reduction without loss of critical information.",
            "future work": "Future research could explore further optimizations of the compression methods and their applicability to other domains or smaller LLMs."
        },
        "other info": {
            "Ethics Statement": "The methods are designed solely for cost reduction and performance maintenance in commercial LLMs and do not violate human rights or military purposes.",
            "Dataset Info": {
                "Dataset Name": "Food-Recommendation DB",
                "Purpose": "To provide domain-specific knowledge for food recommendations during pregnancy and for infants."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommendation algorithms are significant in modern applications, particularly in addressing high inference costs associated with deploying commercial retrieval-augmented large language models (LLMs)."
        },
        {
            "section number": "1.2",
            "key information": "The paper highlights the contribution of retrieval-augmented LLMs in enhancing recommendation systems while addressing the excessive cost of input token size from retrieved context."
        },
        {
            "section number": "1.3",
            "key information": "NLP and AI techniques, such as the proposed token compression scheme, contribute to improving user experience by reducing inference costs while maintaining accuracy."
        },
        {
            "section number": "2.3",
            "key information": "The paper discusses advancements in LLMs through the introduction of the Token Compression Retrieval Augmented Large Language Model (TCRA-LLM), which reduces token size while preserving critical information."
        },
        {
            "section number": "4.1",
            "key information": "TCRA-LLM is designed to reduce the token size of input for retrieval-augmented LLMs through summarization and semantic compression, showcasing the capabilities of LLMs in processing and understanding natural language."
        },
        {
            "section number": "4.2",
            "key information": "The integration of TCRA-LLM into recommendation systems enhances personalization and user interaction by minimizing operational costs associated with input tokens."
        },
        {
            "section number": "10.1",
            "key information": "The paper identifies the existing challenge of high operational costs in traditional LLMs due to excessive input token sizes, necessitating innovative solutions."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions include exploring further optimizations of the token compression methods and their applicability to other domains or smaller LLMs."
        }
    ],
    "similarity_score": 0.7277486323873074,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33e9/33e99b37-bbb4-445b-9b45-f1cfa6a223a5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/878b/878b4e83-998a-44df-88e0-e47f5d32ea1a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4bda/4bda7131-c5e4-4568-ab27-86fe8b56c7ba.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd61/cd61be7f-1730-4f85-8d2c-f94e9f911baa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb09/cb09acca-0d67-4f5d-a2c1-2ddd2c6c7f6d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4567/4567d4bf-45a2-444d-961b-87212de2993e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8856/88566c60-3755-4b65-83cd-098a865b3684.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7668/76682a24-e63b-4764-9a27-5a4050bce1d3.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b125/b1250939-e3df-40b9-a51b-44c53cd4244a.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Tcra-llm_ Token compression retrieval augmented large language model for inference cost reduction.json"
}