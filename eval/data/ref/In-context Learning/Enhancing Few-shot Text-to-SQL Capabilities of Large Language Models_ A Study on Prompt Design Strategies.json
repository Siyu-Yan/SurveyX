{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.12586",
    "title": "Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies",
    "abstract": "In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example\u2019s SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-toSQL task, and we present an analysis of the factors contributing to the success of our strategy.",
    "bib_name": "nan2023enhancingfewshottexttosqlcapabilities",
    "md_text": "# Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies\nLinyong Nan1 Yilun Zhao1 Weijin Zou1 Narutatsu Ri2 Jaesung Tae1 Ellen Zhang1 Arman Cohan1,3 Dragomir Radev1\n# Abstract\nIn-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example\u2019s SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-toSQL task, and we present an analysis of the factors contributing to the success of our strategy.\n# 1 Introduction\nQuestion answering using structured knowledge source is a critical function of information retrieval systems that act as an interface between humans and vast structured data repositories. Extracting and aggregating information accurately is a fundamental requirement of these systems and is thus a primary goal in their design. In recent years, the neural symbolic design approach (Berant et al., 2013; Yao and Van Durme, 2014; Liang et al., 2017; Gardner et al., 2018; Yu et al., 2018; Cheng et al., 2023) has become the preferred choice for such\nsystems for two main reasons. First, neural models have inherent limitations, including a limited working memory that is costly to access during inference and a long-term memory that is unreliable to read from or write to, making it impractical to have them directly read from large-scale knowledge sources. Second, understanding how a system decides which information to retrieve and how to aggregate it is crucial for assessing its reliability and robustness. Recent investigations have demonstrated the effectiveness of the neural symbolic approach in producing transparent reasoning process in formal language sequence (such as Text-to-SQL) for question answering tasks based on databases or knowledge graphs (Berant et al., 2013; Zhong et al., 2017; Yu et al., 2018; Yin and Neubig, 2018; Yu et al., 2019; Ren et al., 2021; Cheng et al., 2023). A typical system comprises a neural semantic parsing module that translates user queries in natural language to formal language sequences (e.g., logical forms or executable code) and a symbolic reasoner module, such as database management system (DBMS), that executes the code on structured knowledge sources to extract the result. The primary objective of this work is to improve the semantic parsing module, as it is essential in extracting answers from relational databases using SQL as the formal language. Current semantic parsing modules can be broadly categorized based on their learning strategies. State-of-the-art systems involve fine-tuning a pretrained language models on a large corpus of {question, SQL} pairs, enabling the model to generate code (Wang et al., 2020; Yin et al., 2020; Scholak et al., 2021; Xie et al., 2022). Alternatively, the in-context learning (ICL) approach exploits the inherent capabilities of large language models (LLMs) to directly produce SQL code by providing a well-defined task prompt (Xie et al., 2022; Chen et al., 2022; Rajkumar et al., 2022; Ni et al., 2023). Existing research indicates that LLMs using\nprompt-based semantic parsing underperform their fine-tuned counterparts (Liu et al., 2023), while recent studies also suggest that performance of ICLtrained LLMs is significantly affected by the structure of the demonstration prompt (Liu et al., 2022a; Rubin et al., 2022; Lu et al., 2022; Wei et al., 2022; Fu et al., 2023; Ye et al., 2023). This motivates us to examine various prompt configurations for semantic parsing tasks, taking advantage of the latest advancements of LLMs pertaining to our domain of interest. Our study focused on exploring various prompt design strategies for semantic parsing tasks in the Text-to-SQL domain. We conducted a systematic investigation into different demonstration example selection criteria and instruction formats on Text-to-SQL datasets. Specifically, we propose to employ an example\u2019s SQL syntactic structure as the basis for retrieving demonstrations, thereby facilitating a more accurate representation of the problem structure. Our approach revealed that selecting demonstration examples with a dual emphasis on diversity and similarity objectives yields maximized gain in performance. Our study also showed that LLMs benefit from database-related knowledge augmentation in certain circumstances. Through experiments, we identified the most effective strategy, which resulted in an Execution Accuracy score of 84.4 on the Spider dataset (Yu et al., 2018). This score is 2.5 points higher than the current state-of-the-art system (Ni et al., 2023) and 5.1 points higher than the best fine-tuned system (Scholak et al., 2021). These results demonstrate the effectiveness of our in-context learning scheme in adapting LLMs to our target task. Furthermore, we present the empirical findings and analysis on the factors that contributed to the success of our strategy.1\n# 2 Methods\nTo design prompts for in-context learning in zeroshot or few-shot settings, it is important to find an optimal way to represent, augment, and arrange all resources in the input-output mapping. Additionally, the task instructions should be formulated to align with these resources. When few-shot learning is employed, the selection of a subset of demonstrations from a pool of annotated examples for each test instance is another critical design choice\nthat can impact the ICL performance. We proposed enhancements for each of these components and evaluated them against existing methods.\n# 2.1 Demonstration Selection\nThe goal is to select a subset of annotated examples from a pool that offers the best context for solving the test problem. While random selection from the pool is one option, Liu et al. (2022a) proposed kNN-augmented example selection (KATE), which retrieves k nearest neighbors from the pool based on the input of the compared instances. To achieve this, all the pool instances are first transformed into continuous vectors using a sentence encoder. During inference, the input of a test instance is projected into a latent space using the same encoder and then compared to the pool of vectors using a similarity measure, such as negative Euclidean distance or cosine similarity. Finally, the top k most similar annotated examples are selected from the pool.\n# Structured Prediction as Basis for Retrieval\nWe propose utilizing the output SQL queries to select the demonstration examples, rather than using the input questions. This is because, unlike many tasks where the output is a classification label or extracted entity with little information about the problem structure, Text-to-SQL demands structured prediction which contains more explicit information about the problem structure than that provided in the input question. Furthermore, unlike natural language questions that can only be converted into continuous semantic vectors, SQL queries can be easily transformed into discrete feature vectors based on their syntax, making their comparison more efficient and transparent. To implement our proposal, we begin by converting the SQL queries of all pool instances into discrete syntax vectors. This is done by parsing the queries and identifying their syntactic elements, including keywords, operators, and identifiers. These elements are then mapped to binary features that indicate their presence in the query. During inference, we first generate a draft of the SQL query using a preliminary predictor. We then apply the same process to convert this draft query into a discrete vector, which is used to represent the test instance for retrieving demonstration examples.\nBalancing Diversity and Similarity We propose a new demonstration selection strategy that differs from Liu et al. (2022a), which retrieves\nthe most similar examples with continuous-valued measurements for each test instance. In contrast, our strategy seeks to balance similarity and diversity of the demonstrations. This is achieved by changing the representation of the given example from a continuous-valued vector denoting the question semantics to a discrete-valued vector that captures the SQL syntax. To obtain demonstration examples that are similar to the given example, we first split the pool of annotated examples into disjoint partitions that represent different categories. Specifically, we use the difficulty-level based categorization derived from the Spider dataset (Yu et al., 2018). While alternative categorization options may exist, we leave this for exploration in future work. Given a test instance, we use a preliminary predictor to generate a draft SQL query and, based on its category, retrieve candidate examples that belong to the relevant partition. Next, to select diverse examples from the candidate partitions, we implement k-means clustering on the discrete vectors of examples, selecting k diverse examples that are closest to each centroid of the cluster. The resulting examples exhibit similarity to the test example by sharing the same category, yet maintain diversity in problem structures. These demonstrations are then used to construct the prompt. The procedure for our demonstration selection strategy is outlined in Algorithm 1.\n# 2.2 Schema Representation in Instruction\nInstructions are crucial to designing prompts, as they define the task by clarifying how provided resources can aid the inference process (Dong et al., 2023). Our primary focus lies in determining the optimal way to represent a structured knowledge source within the instruction and identifying supplementary resources that can enhance the inference process.\n# Linearization of Structured Knowledge\nWe\nbegin by altering the linearization of structured knowledge. In prior research (Xie et al., 2022), structured knowledge sources such as databases or tables have been linearized into a \u201ctext\u201d sequence. Instead, we propose representing the database using a \u201ccode\u201d sequence, specifically the CREATE query employed to construct the table initially, as illustrated in listing 1 and 2 of the Appendix. This linearization approach provides data type information for each column and encompasses all foreign key constraint details within the database. More-\nAlgorithm 1: Similarity-Diversity Demonstration Selection Input: Set of annotated examples A, test examples T, # demonstrations k, categorization {\u03b1, \u03b2, ...} Result: Set of prompts P, where Pi is the prompt for test example Ti /* Split A into disjoint partitions A\u03b1,\u03b2,... */ for Ai in annotated set A do ci = get_category(Ai.SQL); Aci.append(Ai); Vi = get_syntax_vectors(Ai.SQL); end /* Prepare demonstrations Dj for each partition Aj */ for partition Aj in A\u03b1,\u03b2,... do M = k-Means_clustering(V j, k); /* V j is set of discrete vectors for examples in Aj, M has k centroids \u00b51, ..., \u00b5k */ for \u00b5i in M do Dj.append(get_nearest(A, \u00b5i)); end end /* Build test prompts */ for Ti in test set T do Ti.SQL = initial_predictor(Ti); ci = get_category(Ti.SQL); Pi = build_prompt(Dci, Ti); end return P\n*/\nover, we modify other resources in the instructions, such as the question and example entries in the database, to conform to the code sequence style by appending them as comments.\nThe ontology of a database delineates the structure and semantics of the database by offering definitions for a set of classes (tables), their attributes (columns), and the relationships among them. We initially enhance the semantics of each class and attribute by elaborating on their meanings within the context of the entire database. Specifically, we employ OpenAI\u2019s gpt-3.5-turbo engine2 to generate a natural language definition for each column in every table, considering all its values\nand other columns. We then incorporate these definitions into the input either by appending them as a block comment or inserting them within the CREATE query as inline comments. Furthermore, we suggest augmenting the representation of the database structure by providing an Entity-Relationship summary that outlines the connections between tables and specifies how they can be joined. As depicted in Figure 9 of the Appendix, an Entity-Relationship diagram of a database is utilized to enumerate all possible paths between distinct tables. These paths are subsequently arranged in descending order based on their respective lengths. The resulting summary has shown to be useful in our experiments for test instances where multiple tables need to be combined. Listing 5 further demonstrates our augmentations and how we arrange them to construct the prompt.\n# 2.3 Integrated Strategy for Text-to-SQL\nUpon examination, we found that models trained with ICL exhibit sensitivity to the number of demonstration examples, resulting in noticeable variance in performance across models provided with various numbers of demonstrations. To establish substantial conclusions when comparing distinct prompting approaches, we present the mean and standard deviation for models sharing identical configurations except for the varying number of demonstrations. In addition, we employ a majority vote on these models exhibiting diverse performances. Specifically, we obtain the execution results of different models\u2019 greedy decoding predictions, eliminate those with execution errors by deterministic database management system (DBMS), and choose the prediction that receives the majority vote. Alternative integration methods, such as the self-consistency sampling (Wang et al., 2023), are also available, but we reserve their exploration for future research. The comprehensive results are available in Figures 10, 11, 12 of the Appendix for reader\u2019s perusal. We propose the following procedure for constructing prompts for the Text-to-SQL task. Given a set A of annotated examples, we first establish a categorization that divides the pool into disjoint partitions A\u03b1, A\u03b2, . . . ,, with each partition containing examples whose SQL queries share a relatively similar syntax structure. Next, we apply the kMeans strategy detailed in Section 2.1 to obtain\nAlgorithm 2: Integrated Strategy\nInput: Set of annotated examples A, test\nexamples T, # demonstrations k,\ncategorization {\u03b1, \u03b2, ...}, and from\nAlgorithm 1: disjoint partitions\n{A\u03b1, A\u03b2, ...} and corresponding\ndemonstrations {D\u03b1, D\u03b2, ...}\nResult: Set of SQL predictions SP, where\nSPi is the final prediction for test\nexample Ti\nfor Ti in test set T do\nTi.SQL = initial_predictor(Ti);\nci = get_category(Ti.SQL);\nfor n = 4 to k do\nP n\ni = build_prompt(Dci[: n], Ti);\nP n\u2217\ni\n= augment_schema(P n\ni );\nSPn\ni = Model(P n\u2217\ni );\nERn\ni = DBMS(SPn\ni );\nend\nER\u2217\ni = Remove_Exec_Errors(ERi);\nSPi = Majority_Vote(ER\u2217\ni );\nend\nreturn SP\ndiverse demonstration examples Dj for partition Aj. For each example, the demonstration is constructed by transforming the database into multiple CREATE queries and augmenting with schemarelated knowledge. During inference, we employ a preliminary model to generate a draft SQL query, which is used to determine the problem category and thus the corresponding Dj for building the prompt. We obtain multiple predictions using various numbers of shots in Dj and perform majority voting to arrive at the final prediction. Details of this approach are shown in Algorithm 2.\n# 3 Experiments\nDataset We conduct comprehensive experiments on the following four semantic parsing datasets:\n\u2022 Spider (Yu et al., 2018) is a cross-domain semantic parsing dataset that contains complex Text-to-SQL problems. The data originates from 200 databases covering 138 different domains. We use the 7,000 training data as our pool of annotated examples. \u2022 Spider-Syn (Gan et al., 2021a) replaced schema-related words in the questions of Spi-\n Spider-Realistic (Deng et al., 2021) removed explicit mentions of column names from Spider examples to reflect more realistic texttable alignment settings, and selected eight existing Text-to-SQL datasets for cross-domain evaluation.\nBaselines We compare the following prompting strategies for generating SQL queries in few-shot and zero-shot settings.\n# Few-shot\n# 3.2 Main Results\nIn this section, we present a comprehensive analysis of various prompting strategies, assessing their efficacy across multiple datasets. The evaluation of demonstration sampling strategies in a fewshot setting testing on code-davinci-002 is illustrated in Figure 1a, and more few-shot results of gpt-3.5-turbo are shown in Figure 2. We compared different demonstration selection strategies, including random selection, k-nearest neighbors selection (similarity sampling)4, k-means selection (diversity sampling), and our proposed approach, which combines both similarity and diversity. Moreover, we examined the impact of augmenting schema representation within the task instructions and assessed the performance of our integrated strategy. Our findings indicate that employing similarity and diversity objectives in the sampling process leads to better performance on average across all datasets. Furthermore, incorporating schema representation within the instructions enhances performance, and the implementation of voting of models with different shot results in a marked improvement in overall performance. The efficacy of schema augmentation is further supported by experiments in a zero-shot setting, as illustrated in Figure 1b. We compared systems using different linearization methods for prompts: one that transforms the database into a text sequence, and another that uses multiple CREATE queries to represent the database. The latter method shows noticeable improvement in performance. We also contrasted two separate techniques for augmenting schema representation: one that adds semantic information to each column within each table, and another that incorporates entity-relationship knowledge into the schema. The results suggest that structural augmentation 4\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2c99/2c99be79-3215-42cf-878b-eadb8bfe6108.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd4e/cd4ea083-bdf6-4068-8e14-429d38bc6bb1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Few-shot results</div>\n<div style=\"text-align: center;\">(b) Zero-shot results</div>\nFigure 1: Few-shot and zero-shot results of Codex for all datasets. In the few-shot setting, error bars indicate means and standard deviations over performances of systems provided with prompts ranging from 4-shot to 10shot. To obtain the error bars for the random sampling approach, we conducted 3 independent runs using different random seeds. Schema augmentation utilized for the reported results in (a) is structure augmentation - add ontology summary. In the zero-shot setting, the error bars indicate means and standard deviations over 3 independent runs. Our results suggest that 1) using similarity and diversity objectives in the sampling process, 2) including schema representation in instructions, and 3) employing model voting with different shot outcomes both contribute to the improvement of ICL performance.\n(add ontology summary) brings a slight greater improvement in the few-shot setting for Codex (shown in Figure 6), while semantic augmentation (add column summary as block comments) proves more beneficial in the zero-shot setting for Codex and also the few-shot setting for ChatGPT (gpt-3.5-turbo). We hypothesize that this difference may arise from the less descriptive nature of structural augmentation, which calls for more demonstrations in order to effectively understand and utilize the provided information. In future study, we will explore how to adjust structural schema augmentation to better align with the zeroshot setting.\n# 4 Analysis\n# 4.1 Prediction-Syntax based Retrieval\nThe existing method for selecting demonstrations relies on the semantic representations of the question and the database. We propose an alternative method specifically for code generation tasks, which focuses on the syntax of the solution code. We examined syntax coverage and syntax similarity of the prompts produced with different strategies. Syntax coverage is computed by counting the occurrence of syntactic elements (keywords, operators, and identifiers), and dividing it by the total number of all syntactic elements. Syntax similarity, on the other hand, is measured by the mean Euclidean distance between the discrete vector representation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/35aa/35aace95-754a-4925-982d-1625814ff234.png\" style=\"width: 50%;\"></div>\nFigure 2: Few-shot results of gpt-3.5-turbo for Spider. Error bars indicate means and standard deviations over performances of systems provided with 1-shot to 5-shot prompts. Schema augmentation utilized for the reported results is semantic augmentation - add column summary as block-comment.\nof the predicted SQL and vectors that represent the gold SQLs of the demonstrations selected. As indicated in Table 1, both of these metrics contribute to the quality of the examples selected. Furthermore, a simple summation of the two measurements suggests a correlation with the system\u2019s performance, as illustrated in Figure 3. We argue the efficacy of our strategy through the following rationale: (1) in cases where the pool of annotated examples is limited in diversity of the problem structures, certain test problems may lack similar examples available for retrieval; and (2) neither the semantic representation of the question/database nor the distance metric inherently support encapsulation and comparison of different problem structures, whereas SQL syntax provides direct measurement of the problem structures. Given these constraints, the optimal strategy is to select similar examples while ensuring the coverage of as many syntax demonstrations as feasible to mitigate potential failures in similarity-based retrieval.\n# 4.2 Comparative Analysis of Retrieval Methods\nWe conducted an examination of various similaritybased retrieval methods and presented a comparative analysis of their performance in Figure 4. The primary variable in this investigation was the\nCov.\nSim.\nScore\nRandom\n0.38\n0.24\n76.03\nSimilarity\n0.35\n0.30\n78.33\nDiversity\n0.43\n0.23\n78.64\nSimilarity-Diversity\n0.50\n0.26\n80.32\nTable 1: Average syntax coverage and similarity measures of the prompt for different demonstration selection strategies and the corresponding execution accuracies.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98b4/98b4511d-9b0a-493e-a02e-948e707b269b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 3: Correlation between syntax coverage and similarity measures of prompts and execution accuracy.\nrepresentation extracted for each example, with a focus on extracting and comparing the following embedding types: (1) question embeddings generated by Sentence-BERT (Reimers and Gurevych, 2019)5 , RoBERTa-base (Liu et al., 2020), and OpenAI\u2019s text-embedding-ada-002; (2) combined question and database embeddings obtained by (i) employing a single model (i.e., T5-base (Raffel et al., 2020) finetuned on the Spider training split and text-embedding-ada-002) with the database linearized as a text sequence or CREATE queries, and (ii) utilizing separate models, specifically RoBERTa-base for encoding questions and CodeT5-base (Wang et al., 2021) or CodeBERTbase (Feng et al., 2020) for encoding databases; (3) syntactic embeddings of predicted SQL, generated by either binary coding to indicate the presence of SQL syntactic elements or by quantifying their occurrences; and finally, (4) embeddings that encode questions, databases and predicted SQL using text-embedding-ada-002. The following conclusions can be drawn\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d36/3d36d42f-aeda-4e4c-90a5-9f768c0be62d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Comparison between various similarity based demonstration selection methods. Q indicates the embedding model employed to extract representation for the question; D stands for database, and S stands for SQL query.</div>\nabout the similarity-based retrieval methods for Text-to-SQL task: (1) questions alone effectively represent distinct examples for retrieval purposes; (2) RoBERTa-base provides better embeddings for comparisons relative to text-embedding-ada-002; (3) it is feasible to employ models that have not been fine-tuned on Text-to-SQL examples for similarity-based retrieval, while still achieving comparable performance to fine-tuned models; (4) the linearization of databases as SQL queries facilitates the extraction of enhanced embeddings. Additionally, we conducted a comparison between multiple embeddings utilized for diversitybased demonstration selection, encompassing embeddings that encode the semantics of questions, databases and predicted SQL, as well as embeddings that capture the syntactic features of predicted SQL. As depicted in Figure 5, the syntactic embeddings of predicted SQL serve as the most effective basis for contrasting different examples for diversity-based retrieval purposes.\n# 4.3 Schema Augmentation\nFigure 6 presents the outcomes of various schema augmentations applied to the instruction. It is observed that improvement is not apparent in the fewshot setting; however, in the zero-shot setting, the semantic augmentation incorporating descriptions of all table columns proves to be beneficial.\n# 4.4 Effectiveness Analysis\nIn order to determine the problem types that benefit most or least from our proposed methods, we also evaluate the performance of different models across various problem categories within the Spider\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dd28/dd28c0fb-8849-4805-b58a-d333758e0ef9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Comparison between various diversity based demonstration selection methods.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8969/89697cd5-4fff-47a8-9c04-339ec9878d07.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Comparison between various schema augmentations in few-shot and zero-shot settings.</div>\ndataset. As indicated in Figure 7, our similaritydiversity strategy proves beneficial for most problem types, with the exception of the medium split, which includes the most diverse problems. This is the case where similarity-based retrieval fails and syntax coverage becomes more crucial. Furthermore, we observe that augmenting schema semantics is more effective for the easy and medium splits (albeit with high variance), while augmenting schema structure is more effective for more complex problems. This obvervation leads us to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a97c/a97cd79d-226d-4ba7-9cd4-ed172d15d34b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Effects of various prompting strategies on Text-to-SQL problems of different difficulty levels.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c93/1c93e7c5-eb49-4118-a8d3-012381ba7f4f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 8: Effects of preliminary model on proposed strategies.\nhypothesize that challenging problems necessitate addressing a higher number of tables, thus requiring a more comprehensive understanding of the entire database structure. Lastly, the integrated approach is effective across all examples, offering increased benefits especially for those difficult problems.\n# 4.5 Preliminary Models\nTo assess the impact of the choice of preliminary model used to generate the draft SQL on our approach, we conducted tests involving our methods for preliminary models with varying performance levels. Figure 8 reveals that the preliminary models have a relatively minor effect on the performance of the similarity-diversity or integrated\napproaches, exhibiting gradual improvements as higher-performing preliminary models are utilized.\n# 5 Related Work\n# 5.1 In-Context Learning\nExisting literature indicates the ability of large language models to adapt to new tasks at inference time by learning from a few example demonstrations (Brown et al., 2020; Radford et al., 2019). This new capability has been referred to as incontext learning. In this paper, we expand on previous works that investigate the optimal representations for prompt inputs.\n# 5.1.1 Prompt Organization\nPrompt organization investigates the task of selecting and organizing in-context examples, a critical aspect of enhancing model performance. Several studies (Sorensen et al., 2022; Gonen et al., 2022; Wu et al., 2022; Hu et al., 2022; Lu et al., 2022) have proposed metrics to measure the suitability of examples with respect to the target objective and to determine the optimal ordering of them. Liu et al. (2022a) suggest selecting examples that are semantically similar to the test example by employing a k-NN approach in the embedding space. Rubin et al. (2022) train a prompt retriever based on contrastive learning, wherein examples are classified as either positive or negative if they are ranked among the top-k or bottom-k probabilities of a language model generating the target output, conditioned on\nthe retrieved example and the input. Zhang et al. (2022) suggests to actively select demonstrations using Q-Learning. Su et al. (2023) introduces the Vote-k approach to selectively annotate diverse and representative examples for pool construction, then retrieve based on the similarity. In contrast, our approach retrieve a diverse set of examples given a pre-established pool. As the authors demonstrate that having a diverse and representative pool is important for the success of ICL, we posit that a similar characteristic is equally important when composing the prompt, as this approach increases the likelihood of including various syntactical usages or similar problem structures within the prompt.\n# 5.1.2 Prompt Formatting\nPrompt engineering is concerned with investigating the impact of prompt structure on downstream task performance. For tasks that involve multi-step reasoning and higher complexity, Chain-of-thought prompting has been developed (Wei et al., 2023; Kojima et al., 2023). This approach involves laying out the generation process over multiple steps and using the model\u2019s own intermediate process as input. Wang et al. (2023) proposes to sample multiple different chain-of-thoughts then selects the most consistent answer through marginalization of all possible reasoning paths. Press et al. (2023) suggests that prompting LLMs to ask follow-up questions is an effective way to construct the chainof-thoughts process. Zhou et al. (2023) proposes an automatic approach to identify the optimal prompt by searching over a pool of model generated instructions, assigning scores to them, and selecting the prompt with the highest score.\n# 5.2 Table-related task Encoding\nEncoding structured data is fundamental for various table-related tasks, including Table QA and Textto-SQL. In the case of Table QA, a commonly used method is to first employ a weakly-supervised table parser to extract relevant table cells and, optionally, apply a corresponding aggregation operator to the retrieved data. For example, TAPAS (Herzig et al., 2020) incorporates additional embedding layers into a BERT model to capture both the table structure and numerical information. To obtain an answer for a given question, TAPAS uses two classification layers that predict aggregation functions and corresponding table cells. More recent works (Liu et al., 2022b; Jiang et al., 2022; Zhao et al., 2022; Chen, 2023) have considered Table QA as a se-\nquence generation task. They flatten the table into a text sequence and use special tokens to indicate the table structure while encoding the tabular data. Text-to-SQL is a task that aims to convert natural language questions into SQL queries that can be executed on a database (Yu et al., 2018; Gan et al., 2021b; Deng et al., 2021). In this task, structured data in the form of a table schema is also provided as input. The encoder should be able to align entity mentions in the NL question to the schema, and also understand schema structure information (e.g., foreign/primary keys and the column types).\nIn this study, we investigated various prompt design approaches for semantic parsing tasks in the Text-to-SQL domain. We proposed an approach that leverages an example\u2019s SQL syntactic structure for demonstration examples selection, emphasising both diversity and similarity as the sampling objectives. Additionally, We found that LLMs gain benefits from database-related knowledge augmentations. Future research can build upon our findings to examine the transferability of our approach to other domains. Through ongoing improvement of LLMs\u2019 capabilities in semantic parsing, we aim to contribute to the development of QA systems that are more accurate, robust and comprehensible.\n# Limitations\nOne of the main limitations of this study is the reproducibility problem. The experiments presented in this paper relied on the use of OpenAI APIs, which were available at the time of our research but have since been or will be deprecated. This means that the results of our experiments cannot be replicated using the same APIs, which hinders the reproducibility of our findings. To address this limitation, we will focus on providing experiments results that are based on open-sourced LLMs (Touvron et al., 2023; Taori et al., 2023; Chiang et al., 2023) for greater transparency and reproducibility. Another limitation is that it is not clear how our approach will benefit LLMs given smaller or more constrained pools of annotated examples. Although we postulate that our approach offers the advantage of providing a prompt with maximal coverage of similar problem structures when identically structured problems cannot be found in the pool, we could not substantiate this due to our limited budget and access to the OpenAI APIs.\n# References\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA. Association for Computational Linguistics.\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nonathan Herzig, Pawel Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4320\u20134333, Online. Association for Computational Linguistics.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2627\u20132643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nZhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and Weizhu Chen. 2022. OmniTab: Pretraining with natural and synthetic data for few-shot tablebased question answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 932\u2013942, Seattle, United States. Association for Computational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners.\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. 2017. Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23\u201333, Vancouver, Canada. Association for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022a. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022b. TAPEX: Table pre-training via learning a neural SQL executor. In International Conference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Ro{bert}a: A robustly optimized {bert} pretraining approach.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. 2021. CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696\u20138708, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.\nao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019. CoSQL: A conversational text-to-SQL challenge towards crossdomain natural language interfaces to databases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1962\u2013 1979, Hong Kong, China. Association for Computational Linguistics.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/53ea/53ea06ed-0b5f-4746-a9a7-ecaa302efdb0.png\" style=\"width: 50%;\"></div>\n# Appendix\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c50/7c50bc15-0c6c-437c-b750-ff1f8b1ba2ac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9f58/9f58a1b2-fa26-43f4-b9c4-61c62e7483ba.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fc6f/fc6fe195-79f4-4e05-b58f-2c9d134f77fe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec52/ec5252a6-f5aa-4a3d-970c-0030d62e7cb4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31ff/31ffb1a9-a75e-4608-9d92-6a8df8f9dc20.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/457f/457fe498-bb59-431f-8b11-f4e3ad3853f1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Examples of schema structure representation construction.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e888/e888d9a4-8f38-4bec-a178-2d64951ed725.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b856/b856bea3-1304-4d84-bacc-6aaeda22acf5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f64d/f64d7d95-d1ad-44f8-9fda-2f0e0a359989.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/04ce/04ce599f-2190-4e2d-9fdd-79fc981e1ea9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Few-shot results for comparing different schema representation augmentation methods with differen number of demonstration examples selected for the prompt.</div>\n<div style=\"text-align: center;\">Figure 11: Few-shot results for comparing different schema representation augm number of demonstration examples selected for the prompt.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1df7/1df7e4c6-c8e6-4196-8dbf-2dc7ca7d278b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">igure 12: Few-shot results for comparing different sampling strategies on Text-to-SQL problems of different ifficulty levels, with different number of demonstration examples selected for the prompt.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of existing methods in question answering tasks that utilize structured knowledge sources, particularly in improving Text-to-SQL systems through effective prompt design strategies for large language models (LLMs).",
        "problem": {
            "definition": "The problem focuses on enhancing the performance of Text-to-SQL systems, which convert natural language questions into SQL queries, particularly in few-shot learning contexts.",
            "key obstacle": "Current methods underperform due to inadequate prompt configurations and the inability to effectively utilize the structured nature of SQL queries for demonstration selection."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that structured prediction in SQL queries contains more explicit information about the problem structure than the input questions.",
            "opinion": "The proposed idea involves using SQL syntactic structure to retrieve demonstration examples, balancing diversity and similarity in selection to improve performance.",
            "innovation": "The key innovation lies in the dual emphasis on both similarity and diversity in demonstration selection, contrasting with previous methods that focused solely on similarity."
        },
        "method": {
            "method name": "Similarity-Diversity Demonstration Selection",
            "method abbreviation": "SDDS",
            "method definition": "SDDS is a method for selecting a subset of annotated examples based on the syntactic structure of SQL queries to improve the context for solving test problems in Text-to-SQL tasks.",
            "method description": "The method employs a balanced approach to retrieve similar and diverse demonstration examples from a pool based on SQL syntax.",
            "method steps": [
                "Transform SQL queries into discrete syntax vectors.",
                "Categorize the pool of annotated examples based on difficulty levels.",
                "Retrieve candidate examples from relevant partitions using a preliminary predictor.",
                "Implement k-means clustering on the discrete vectors to select diverse examples."
            ],
            "principle": "The effectiveness of this method is grounded in the structured nature of SQL, which allows for a more efficient and transparent comparison of problem structures."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on the Spider dataset, utilizing a pool of 7,000 annotated examples along with comparisons to baseline methods in few-shot and zero-shot settings.",
            "evaluation method": "Performance was assessed through execution accuracy, comparing various prompting strategies, and utilizing majority voting for final SQL predictions."
        },
        "conclusion": "The proposed strategies significantly enhance the performance of Text-to-SQL systems, with the most effective approach achieving an execution accuracy score of 84.4, surpassing both state-of-the-art and fine-tuned systems.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to leverage both similarity and diversity in demonstration selection, leading to improved accuracy in SQL query generation.",
            "limitation": "A notable limitation is the reproducibility issue due to reliance on OpenAI APIs, which may hinder future validation of results.",
            "future work": "Future research should explore the applicability of these strategies to smaller or more constrained pools of annotated examples and investigate additional schema augmentation techniques."
        },
        "other info": {
            "info1": "The study highlights the importance of prompt design in enhancing the capabilities of LLMs for structured tasks.",
            "info2": {
                "info2.1": "The integrated strategy for Text-to-SQL tasks includes augmenting schema representation and majority voting for predictions.",
                "info2.2": "The findings suggest potential for further improvements in the design of prompts to better align with the complexity of the tasks."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The study highlights the importance of prompt design in enhancing the capabilities of large language models (LLMs) for structured tasks."
        },
        {
            "section number": "4.2",
            "key information": "Current methods underperform due to inadequate prompt configurations and the inability to effectively utilize the structured nature of SQL queries for demonstration selection."
        },
        {
            "section number": "5.2",
            "key information": "The problem focuses on enhancing the performance of Text-to-SQL systems, which convert natural language questions into SQL queries, particularly in few-shot learning contexts."
        },
        {
            "section number": "3.3",
            "key information": "The method employs a balanced approach to retrieve similar and diverse demonstration examples from a pool based on SQL syntax."
        },
        {
            "section number": "6.1",
            "key information": "A notable limitation is the reproducibility issue due to reliance on OpenAI APIs, which may hinder future validation of results."
        }
    ],
    "similarity_score": 0.729128676507574,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models_ A Study on Prompt Design Strategies.json"
}