{
    "from": "google",
    "scholar_id": "9KYMfTovTlIJ",
    "detail_id": null,
    "title": "Code-style in-context learning for knowledge-based question answering",
    "abstract": " Abstract\n\nCurrent methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the few-shot setting. The code and supplementary files are released at https://github.com/ Arthurizijar/KB-Coder.\n\n# Introduction\n\nKnowledge-Based Question Answering (KBQA) (Yih et al. 2016; Lan and Jiang 2020; Yu et al. 2022; Li et al. 2023b) is a long-studied problem in the NLP community. Due to the complexity and diversity of the questions, the models with good performance usually have complex modeling frameworks or special training strategies. However, these designs also lead to models that require a lot of labeled data to help the parameters converge, making them difficult to apply in the new domains. Recently, Large Language Models (LLMs) have strong generalization capabilities, benefiting from pre-training on vast amounts of natural language corpus and open source code (Figure 1 Top). In addition, the emergence of In-Context Learning (ICL) capabilities (Brown et al. 2020) allows LLMs to accomplish even",
    "bib_name": "nie2024code",
    "md_text": "# Code-Style In-Context Learning for Knowledge-Based Question Answerin\n\nchool of Computer Science and Engineering, Beihang University, Beijing, 2 Zhongguancun Laboratory, Beijing, China 3 Shen Yuan Honors College, Beihang University, Beijing, China {niezj,zhangrc,wangzy23,liuxd} @act.buaa.edu.cn\n\n# Abstract\n\nCurrent methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the few-shot setting. The code and supplementary files are released at https://github.com/ Arthurizijar/KB-Coder.\n\n# Introduction\n\nKnowledge-Based Question Answering (KBQA) (Yih et al. 2016; Lan and Jiang 2020; Yu et al. 2022; Li et al. 2023b) is a long-studied problem in the NLP community. Due to the complexity and diversity of the questions, the models with good performance usually have complex modeling frameworks or special training strategies. However, these designs also lead to models that require a lot of labeled data to help the parameters converge, making them difficult to apply in the new domains. Recently, Large Language Models (LLMs) have strong generalization capabilities, benefiting from pre-training on vast amounts of natural language corpus and open source code (Figure 1 Top). In addition, the emergence of In-Context Learning (ICL) capabilities (Brown et al. 2020) allows LLMs to accomplish even complex reasoning tasks while observing a small amount of labeled data (Wei et al. 2022; Cheng et al. 2022). Recently, Pangu (Gu, Deng, and Su 2023) first proposed a KBQA method based on the ICL paradigm, which con\n\n* Corresponding author Copyright \u00a9 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/72b0/72b01aeb-75f1-4f0a-97b6-d4f22f7feed0.png\" style=\"width: 50%;\"></div>\nFigure 1: A comparison between our proposed ICL method and the existing method. Intuitively, our method achieves better performance by transforming the original KBQA task into a more familiar code form for the LLM.\n\nsists of a symbolic agent and a language model. The agent is responsible for constructing effective programs step-bystep, while the language model is responsible for guiding the search process to evaluate the plausibility of candidate plans. KB-BINDER (Li et al. 2023b) proposes a trainingfree paradigm, which uses multiple (question, logical form) pairs as the demo examples, and leads the LLM to generate the correct logic form for the new question based on these demo examples (Figure 1 Middle). However, we find that this latest method still has several serious problems: (1) High format error rate. Due to the highly specialized nature of the logic forms, all of them can hardly ever appear in the training corpus of LLM. As a result, it is challenging to generate the logical form with the correct formatting with a few demo examples. In our preliminary experiments, the logic forms generated by the powerful GPT3.5-turbo have a more than 15% format error rate on GrailQA (Gu et al.\n\nror correction, the logic form generated cannot even provide a final answer to the question; (2)  Low zero-shot generalization performance. In the zero-shot generalization scenario, the prior knowledge of the KB domain schema related to the test question is unable to be obtained through the training set (Gu et al. 2021). In other words, it means that demo examples created from the training set cannot provide enough information for question answering under this scenario. Considering the labeled data usually covers only a small fraction of knowledge in KB, this problem will inevitably affect the performance in practice. To address these problems, the key is to deeply understand the behavior of the LLMs. Thankfully, some empirical observations on LLMs have provided desirable insights, which include (1) converting original tasks to code generation tasks will reduce the difficulty of post-processing (Wang, Li, and Ji 2022); (2) reasoning step-by-step can improve LLMs performance on complex reasoning tasks (Wei et al. 2022); (3) retrieval augmentation is helpful for LLMs in dealing with uncommon factual knowledge (Mallen et al. 2023). Inspired by the above valuable observations, we propose a novel code-style in-context learning method for KBQA, which converts the one-step generation process of the logic forms to the progressive generation process of the function calls in Python (Figure 1 Bottom). Specifically, we first define seven meta-functions for S-Expression (Gu et al. 2021), a popular logic form for KBQA, and implement these functions in Python. Note that this step enables all S-Expressions to be generated by a finite number of calls of pre-defined meta-functions. For a test question, we sample a few numbers of (question, S-Expression) pairs in the training set and convert them into (question, function call sequence), where the function call sequence can be executed in the Python interpreter to output S-Expression. Finally, the Python implementation of the meta-functions, all (question, function call sequences) pairs, and the test question are reformatted in code form as input to the LLM. And the LLM is expected to complement a complete function call sequence for the new question to obtain the correct logic form. In addition, we find a simple and effective way to improve the performance in the zero-shot generalization scenario: Regard the test question as the query to retrieve a related relation in KB, and provide LLM with the relation for reference. Our contribution can be summarized as follows:\n\u2022 We propose a novel code-style in-context learning method for KBQA. Compared to the existing methods, our method can effectively reduce the format error rate of the logic form generated by LLMs while providing additional intermediate steps during reasoning. \u2022 We find that providing a question-related relation as a reference to LLMs in advance can effectively improve the performance in the zero-shot generalization scenario. \u2022 We design a training-free KBQA model, KB-Coder, based on the proposed ICL method. Extensive experiments on WebQSP, GrailQA, and GraphQ show that our model achieves SOTA under the few-shot setting. While allowing access to the full training set, the training-free\n\nKB-Coder achieves competitive or better results compared with current supervised SOTA methods.\n\n# Related Work\n\nComplex Reasoning with Code-LLMs Codex (Chen et al. 2021) first introduces a code corpus to train LLMs and finds that the obtained code-LLMs have excellent logical reasoning capabilities. Subsequently, code-LLMs have been used for a variety of complex tasks in two ways. The works in the first way only implicitly use the reasoning power that derives from code pre-training and proposes techniques such as chain-of-thought (Wei et al. 2022) and problem decomposition (Zhou et al. 2022), etc. And the works in another way convert the task form into code generation and guide the LLMs to achieve the original task goal by creating instances (Wang, Li, and Ji 2022), complementary code (Li et al. 2023a) or generating SQL (Cheng et al. 2022) or logic forms (Li et al. 2023b) directly.\nQuestion Answering with LLMs We distinguish the different methods according to the type of LLM-generated content. The first class of methods guide LLMs to generate answer directly (Li et al. 2023c; Baek, Aji, and Saffari 2023). In these methods, the knowledge in the external knowledge source is converted into the index, then the questions are used as queries to obtain relevant knowledge from the index through sparse or dense retrieval. Then the questions and related knowledge are spliced together and fed into the LLM to generate the answers directly. The second class of methods (Izacard et al. 2022; Gu, Deng, and Su 2023; Tan et al. 2023) views the LLM as a discriminator and leads LLMs to choose the correct answer or action from a candidate set. The third class of methods (Li et al. 2023b; Cheng et al. 2022) views the LLM as a semantic parser and guides the model to generate intermediate logic forms. Compared to generating the answer directly, the other two classes of methods can eliminate the risk of generating fake knowledge in principle but cannot achieve an end-to-end method. Beyond the above three classes, DECAF (Yu et al. 2022) is a special case that directs LLM to generate both logic forms and the final answer by changing the prompt.\n\n# Method\n\n# Overview\n\nIn general, our proposed KBQA method is a method based on semantic parsing: Given a natural language question q and KB G = {(h, r, t), h, t \u2208E, r \u2208R}, where E  is the entity set and R is the relation set, our method can be viewed as a function F, which maps q to a semantically consistent logic form l = F (q). Then l  is converted into a query to execute, and the queried results are regarded as the answers to q. Specifically, we first design seven meta-functions, which can cover all atomic operations of a specific logic form, and re-define these meta-functions in Python. Finally, for a new question, the following three steps are adopted to get the answer (Figure 3): (1) An LLM is adopted to obtain its function call sequence with the code-style in-context learning method; (2) A dense retriever is utilized to link entities for\n\nFunction\nDomain\nRange\nMapping Descriptions\nSTART\n{E|E \u2208P(E)}\n{E\u2032|E \u2208P(E)}\nStart from E and return the same set E\u2032 = E\nJOIN\n{(r, E)|r \u2208R, E \u2208P(E)}\nReturn E\u2032 that pointed by r from E\nAND\n{(E1, E2)|E1, E2 \u2208P(E)}\nReturn the intersection E\u2032 of E1 and E2\nCMP\n{(c, r, v)|r \u2208R, v \u2208V)}\nReturn E\u2032 \u2282E whose value pointed by r </>/\u2264/\u2265v\nARG\n{(a, E, r)|E \u2208P(E), r \u2208R}\n{e|e \u2208E}\nReturn e \u2208E whose value pointed by r is largest / smallest\nCOUNT\n{E|E \u2208P(E)}\n{n|n \u2208N}\nReturn the element number n of E\nSTOP\n{E|E \u2208P(E)}\n\u2212\nStop at E and E is regarded as the predicted answer set\nTable 1: Seven meta-functions with their domain, range, and mapping descriptions. c \u2208{<, >, \u2264, represents the power set of a given set, V is a subset of E containing all entities in value type, and N numbers. For the other notations, please refer to the Overview section.\n\nmeta-functions with their domain, range, and mapping descriptions. c \u2208{<, >, \u2264, \u2265}, a \u2208{min, max}, P (. power set of a given set, V is a subset of E containing all entities in value type, and N represents the set of natura he other notations, please refer to the Overview section.\n\nthe entity mentions extracted from the function calls, while another dense retriever is utilized to match relations for the relation name extracted from the function calls; (3) A program interpreter is used to execute the generated function call sequence to get the logical form l, which will be executed further to get the answer a.\n\n# Meta-Function Design\n\nIn practice, we use S-Expression defined by Gu et al. (2021) as the logical form l due to its simplicity. S-Expression (McCarthy 1960) is a name-like notation for the nested list (tree-structured) data, which conforms to the grammar of \u201cprefix notation\u201d. Specifically, S-Expression usually consists of parentheses and several space-separated elements within them, where the first element is the function name and all remaining elements are the attributes of the function. For example, for the question \u201chow many American presenters in total\u201d, its S-Expression is\n\n(COUNT (AND (JOIN nationality m.09 c7w0) (JOIN profession m.015 cjr)))\n\nwhere m.09c7w0 and m.015cjr  are the unique identifiers of entity United States of America and Presenter. The corresponding tree structure of this SExpression is shown on the left side of Figure 2. Based on the original syntax of S-Expression (Gu et al. 2021), we define seven meta-functions in total (Table 1), which include their name, domain, range, and mapping description. Compared to the original grammar, we omit the R function, remove a call way for the JOIN function, and add the START and STOP functions.\n\n# Code-Style In-Context Learning\n\nA typical in-context learning paradigm (Brown et al. 2020) generally includes an instruction I, K demo examples D = {d 1, d 2, ..., d K}, and a new query Q. If the output of the LLM is denoted as C, we can express the process of the incontext learning as\n\n(1)\n\nwhere f LLM represents a specific LLM. In our method, we use the code style to construct I, D, and Q and expect the\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/81c0/81c0e638-2890-47a0-9f0a-4805cc3e1558.png\" style=\"width: 50%;\"></div>\nFigure 2: The tree structure (left) and the corresponding function call sequence (right) of S-Expression (COUNT (AND (JOIN nationality m.09c7w0) (JOIN profession m.015cjr))).\n\nmodel to generate a piece of code - a sequence of metafunction calls - for Q, following the demo example. Next, we describe in detail how to construct each part respectively.\nInstruction I Similar to previous works on code-LLMs (Cheng et al. 2022; Li et al. 2023a), the instruction consists of two parts: a prompt comment for describing the task and the code implementation of seven meta-functions. Due to the successful practice of Codex (Chen et al. 2021) in Python, we select Python to implement these functions. Then, the complete contents of I are shown in the code from line 1 to line 26 in Figure 4. All seven functions are the implementation of the same-name function in Table 1.\nDemo Examples D Each demo example is created from a (question, S-Expression) pair in the training set and contains two parts: a variable named question and a function call sequence. Specifically, string variable question  is assigned by the question, while the function call sequence is converted by the S-Expression. The right side of Figure 2 provides an example of the function call sequence, where each function call can correspond to one non-leaf node in the tree structure. Thus, the function call sequence can also be regarded as the result of a bottom-up parsing of the tree structure. If the meta-function definitions in Instruction are spliced together with the function call sequence as a whole code into the Python interpreter, we can get the complete SExpression by visiting the value of variable expression. Since the entity identifiers in KB, such as m.03yndmb, lose\n\nmodel to generate a piece of code - a sequence of metafunction calls - for Q, following the demo example. Next, we describe in detail how to construct each part respectively.\nInstruction I Similar to previous works on code-LLMs (Cheng et al. 2022; Li et al. 2023a), the instruction consists of two parts: a prompt comment for describing the task and the code implementation of seven meta-functions. Due to the successful practice of Codex (Chen et al. 2021) in Python, we select Python to implement these functions. Then, the complete contents of I are shown in the code from line 1 to line 26 in Figure 4. All seven functions are the implementation of the same-name function in Table 1.\n\nDemo Examples D Each demo example is created from a (question, S-Expression) pair in the training set and contains two parts: a variable named question and a function call sequence. Specifically, string variable question  is assigned by the question, while the function call sequence is converted by the S-Expression. The right side of Figure 2 provides an example of the function call sequence, where each function call can correspond to one non-leaf node in the tree structure. Thus, the function call sequence can also be regarded as the result of a bottom-up parsing of the tree structure. If the meta-function definitions in Instruction are spliced together with the function call sequence as a whole code into the Python interpreter, we can get the complete SExpression by visiting the value of variable expression. Since the entity identifiers in KB, such as m.03yndmb, lose\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/86d3/86d33bb8-042f-4079-bb6e-daccdfd1e252.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ure 3: An illustration of the inference process of KB-Code\n</div>\ntheir semantic information, we map all entity identifiers to their surface name to help the LLM obtain the semantic information of these entities. Finally, the demo example corresponding to Figure 2 is reformulated as the code from line 28 to line 35 in Figure 4. To obtain D, we sample K  (question, S-Expression) in the training set in total, convert each pair to the above form, and split them using line breaks.\nNew Query Q Compared to the form of demo examples, the new query is an incomplete piece of code and only contains the part of the variable question, For example, the question \u201chow many religious texts does syncretism have\u201d will be reformulated the code of line 42 in Figure 4. For each Q, the LLM is expected to complement the remaining function call sequence corresponding to the question with the capability of in-context learning. In practice, I remains constant. For D, we can provide a consistent D for each Q in the dataset to follow the few-shot setting, or we can select a different D for each Q based on the similarity for better performance. Both two settings will be studied in subsequent experiments.\n\ntheir semantic information, we map all entity identifiers to their surface name to help the LLM obtain the semantic information of these entities. Finally, the demo example corresponding to Figure 2 is reformulated as the code from line 28 to line 35 in Figure 4. To obtain D, we sample K  (question, S-Expression) in the training set in total, convert each pair to the above form, and split them using line breaks.\n\n# Reasoning\n\nIn this section, we describe how to post-process the function call sequence generated by the LLM so that they can be converted into the query, obtaining the predicted answer.\n\nEntity Linking & Relation Matching Recall that the LLM can not have a direct view of the information in KB, making it difficult to generate completely correct entity names and relationship names. However, We believe that the names generated have a high level of semantic similarity with the correct ones, so the names generated can be treated as mentions for entity linking and relation matching. Benefiting from our strict definition of the domain of meta-functions, both entity mentions and relation mentions\n\ncan be parsed easily. Specifically, for entity linking, we first convert all surface names of all entities in the KB into representations by the off-the-shelf embedding model, SimCSE (Gao, Yao, and Chen 2021), and build the entity index with Faiss (Johnson, Douze, and J\u00b4egou 2019). When linking for an entity mention, we first use the HARD MATCH strategy to obtain all entities that have the same surface name as the mention as the candidate set. If the size of the candidate set is larger than a hyper-parameter M e, we will only retain the most popular M e entities referring to FACC1 (Gabrilovich, Ringgaard, and Subramanya 2013). In contrast, if the size is less than M e, we will search the most similar entities from the existing entity index to fill the candidates up to M e. Similarly, we use the same techniques to index all relations in the KB and retrieve M r most semantically similar relations as candidates for each generated relation name.\n\nAnswer Prediction Without loss of generality, we assume that the function call sequence contains p entities to be linked and q relations to be matched, denote the candidate set of the i-th entity mention as C i e and the candidate set of the j-th relation mention as C j r. Then we can obtain the ordered tuple set {(c 1 e, ..., c p e, c 1 r, ..., c q r)} = C 1 e \u00d7... \u00d7 C p e \u00d7 C 1 r \u00d7... \u00d7 C q r, where \u00d7 represents the Cartesian product. For each ordered tuple in the set, let each element in the ordered tuple replace the corresponding generated names in the function call sequence one by one, we can obtain a candidate for the whole function call sequence (Figure 3). Finally, there will be (M e) p. (M r) q candidate items for each function call sequence, which can be a huge number for some special cases. Therefore, we will execute the sequence of function calls one by one to get the S-Expression, and then convert the SExpression to SPARQL to execute it. Once the queried result is not null, we will just terminate the process of trying one by one and consider the result of the queried result as the answer to the question.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b675/b675a218-e258-40fd-846f-4ed31de54ef9.png\" style=\"width: 50%;\"></div>\n\nFigure 4: An example of contextual learning of a code style. The part of demo examples is shown only one example due to space constraints.\n\n# One Related Relation for Zero-shot Generalizatio\n\nCode-Style in-context learning allows the LLM to be more adaptable to the task form, resulting in a lower format error rate. However, when the queried domain is not involved in the demo examples, which is called the zero-shot generalization scenario by (Gu et al. 2021), the performance of the LLM is still poor. This is not difficult to understand, as the change in the prompt form of the task does not bring new additional knowledge of the queried domain to the LLM. After analyzing the bad cases from the preliminary experiments, we found that the biggest problem with the error was relation matching, where the relation mentions generated by the LLM usually cannot hit the correct relation under the zeroshot generalization scenario. Subsequently, we mitigate this problem by providing an additional relation name for LLM. Specifically, we use the entire test question as a query to re\n\ntrieve the similar relation from the relation index, and the relation with the highest similarity is inserted between demo examples D and the new question Q  with the comment format like the code from line 37 to line 40 in Figure 4. Based on preliminary experiments, it is observed that one relation brings the best performance and more relations have little improvement on the performance. We will analyze the effect of the number of relations on the results in detail in the Experiment section.\n\n# Experiment\n\n# Experiment Setup\n\nDataset We use three mainstream datasets in KBQA, WebQSP (Yih et al. 2016), GraphQ (Su et al. 2016), and GrailQA (Gu et al. 2021), which represent the three generalization capabilities of i.i.d, compositional, and zero-shot, respectively, to evaluate the effect of KB-Coder.\n\nLLM Due to the deprecation of the Codex family of models, we select gpt-3.5-turbo  from OpenAI for our experiments. In all experiments, we used the official API 1 to obtain model results, where temperature is set to 0.7, max tokens is set to 300, and other parameters are kept at default values.\n\nobtain model results, where temperature is set to 0.7, max tokens is set to 300, and other parameters are kept at default values.\nBaseline We mainly compare our model with KBBINDER (Li et al. 2023b), the SOTA model on WebQSP, GrailQA, and GraphQ under the few-shot setting. The original results of KB-BINDER are obtained with the deprecated code-davinci-002, and we reproduce their method with gpt-3.5-turbo while the other setting remains consistent with us for a fair comparison. Some results obtained by training on the whole training dataset (Ye et al. 2022; Shu et al. 2022; Gu et al. 2021; Sun, Bedrax-Weiss, and Cohen 2019) are also reported for reference.\nEvaluation Metric Consistent with previous works (Yu et al. 2022; Li et al. 2023c), we report F1 Score on WebQSP and GraphQ, while Exact Match (EM) and F1 Score on GrailQA as performance metrics. At the same time, we use the Format Error Rate (FER) to evaluate the proportion of logical forms generated by different methods that conform to the grammar of S-Expresssion.\n\n# Implementation Details\n\nWithout special instructions, we reported the experiment results with M e = 15 and M r = 100. SimCSE instance sup-simcse-bert-base-uncased is used to obtain the dense representations. Consistent with KB-BINDER (Li et al. 2023b), we conduct 100-shot for WebQSP and GraphQ, and 40-shot for GrailQA. In the following sections, we use different notations to represent different variants: \u2022 KB-Coder (K): The fixed questions sampled randomly from the training set are selected to be the demo examples. The LLM generates K candidates and uses the majority vote strategy (Wang et al. 2022) to select the final answer. The performance is expected to be further improved thanks to the self-consistency of the LLM.\n\n1 https://openai.com/api\n\nMethod\nI.I.D\nCompositional\nZero-Shot\nOverall\nEM\nF1\nEM\nF1\nEM\nF1\nFER\u2193\nEM\nF1\nFull Supervised on the Entire Training Set\nRnG-KBQA (Ye et al. 2022)\n86.7\n89.0\n61.7\n68.9\n68.8\n74.7\n-\n69.5\n76.9\nDecAF (Yu et al. 2022)\n88.7\n92.4\n71.5\n79.8\n65.9\n77.3\n-\n72.5\n81.4\nTIARA (Shu et al. 2022)\n88.4\n91.2\n66.4\n74.8\n73.3\n80.7\n-\n75.3\n81.9\nIn-Context Learning (Training-Free)\nKB-BINDER (1)\n40.02.3\n43.32.7\n33.92.7\n36.62.6\n40.13.6\n44.04.1\n20.02.4\n38.73.0\n42.23.3\nKB-Coder (1)\n40.63.3\n45.52.8\n34.53.6\n38.63.5\n42.25.9\n47.35.4\n3.00.9\n40.13.7\n44.93.4\nKB-BINDER (6)\n43.62.1\n48.32.5\n44.52.3\n48.82.7\n37.52.4\n41.82.8\n8.10.4\n45.72.3\n50.82.8\nKB-Coder (6)\n43.63.7\n49.33.3\n44.02.2\n49.61.9\n37.72.6\n43.22.6\n0.60.2\n45.93.9\n51.73.3\nKB-BINDER (1)-R\n74.70.1\n79.70.1\n44.60.4\n48.50.5\n37.10.2\n40.80.1\n16.40.2\n47.60.0\n51.70.1\nKB-Coder (1)-R\n76.23.0\n80.21.9\n50.40.7\n54.80.7\n45.80.4\n50.60.9\n3.10.4\n54.01.0\n58.51.0\nKB-BINDER (6)-R\n75.80.1\n80.90.1\n48.30.4\n53.60.4\n45.40.2\n50.70.3\n5.20.1\n53.20.1\n58.50.1\nKB-Coder (6)-R\n76.93.1\n81.01.8\n52.70.9\n57.81.0\n48.90.2\n54.10.6\n1.50.1\n56.30.9\n61.31.0\nMethod\nFER\u2193\nF1\nFull Supervised on the Entire Training Set\nArcaneQA (Gu and Su 2022)\n-\n75.6\nTIARA (Shu et al. 2022)\n-\n76.7\nDecAF (Yu et al. 2022)\n-\n78.7\nIn-Context Learning (Training-Free)\nKB-BINDER (1)\n3.91.1\n52.61.1\nKB-Coder (1)\n1.92.3\n55.71.3\nKB-BINDER (6)\n0.30.4\n56.61.7\nKB-Coder (6)\n0.10.1\n60.51.9\nKB-BINDER (1)-R\n1.90.3\n68.90.3\nKB-Coder (1)-R\n1.70.3\n72.20.2\nKB-BINDER (6)-R\n0.70.0\n71.10.2\nKB-Coder (6)-R\n0.30.2\n75.20.5\nTable 3: 100-shot results on WebQSP. The subscript is the standard deviation of the three runs.\n\n\u2022 KB-Coder (K)-R: Compared to KB-Coder (K), the questions most similar to every test question are selected as their demo examples.\nNote that KB-Coder (K) is strictly under the few-shot  setting, while KB-Coder (K)-R is to explore the upper bound on performance when the whole training set can be accessed. We report results for K = 1 and 6 for fair comparison with KB-BINDER. For each setting, we rerun it three times and report the mean and standard deviation.\n\nWe report the performance of KB-Coder and other baselines on GrailQA, WebQSP, and GraphQ in Tables 2, 3 and 4. Next, we will analyze the format error rate (FER) and the performance metrics (EM and F1) respectively. Recall that one of the motivations for converting logical form generation to code generation is to allow LLM to do a more familiar task to ensure the correctness of the format of the generated content. the performance on all three datasets successfully verifies the effectiveness of our method under\n\nMethod\nFER\u2193\nF1\nFull Supervised on the Entire Training Set\nSPARQA (Sun et al. 2020)\n-\n21.5\nBERT + Ranking (Gu et al. 2021)\n-\n25.0\nArcaneQA (Gu and Su 2022)\n-\n31.8\nIn-Context-Learning (Training-Free)\nKB-BINDER (1)\n15.41.6\n27.10.5\nKB-Coder (1)\n6.32.5\n31.11.3\nKB-BINDER (6)\n2.80.6\n34.50.8\nKB-Coder (6)\n0.60.2\n35.80.6\nKB-BINDER (1)-R\n19.10.1\n26.70.3\nKB-Coder (1)-R\n10.00.2\n30.00.3\nKB-BINDER (6)-R\n5.70.5\n32.50.5\nKB-Coder (6)-R\n0.90.1\n36.60.2\nTable 4: 100-shot results on GraphQ. The subscript is the standard deviation of the three runs.\n\nthe few-shot setting: (1) On WebQSP, where the questions are simpler, the existing method generates logical forms with a low FMR, but KB-Coder can still further reduce the FMR to even lower levels; (2) On GrailQA and GraphQ, where the questions are more complex, KB-Coder improves dramatically compared to the two methods generating logic form directly; (3) Benefiting from self-consistency in LLMs, the majority vote strategy will help alleviate the problem of KBBINDER format error rates. While KB-Coder can work with the majority vote strategy to promote new lows in FMR. Benefiting from the lower FER, the F1 Scores (or EM) on the three datasets are both significantly improved compared to KB-BINDER under almost all settings, especially with two settings that do not introduce the majority vote strategy. (1) On WebQSP and GraphQ, the training-free KB-Coder(6)-R achieves competitive results compared to full-supervised methods, while our method further narrows down the performance with those of the fully-supervised model On GrailQA; (2) Compared to KB-BINDER, KBCoder usually obtains a substantial lead under no dependence, reflecting the better underlying performance of our\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9ce8/9ce82848-1c8f-4daa-a07c-6a144fafddbd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Effect of the shot number\n</div>\n<div style=\"text-align: center;\">(b) Effect of the relation number\n</div>\ne factors in ICL with a subset of 500 questions from GrailQA local dev set, where the solid d the dashed line is used to indicate FER.\n\nmethod; (3) The performance fluctuations of KB-Coder are more drastic compared to KB-BINDER, which is an issue we should focus to solve in the future work.\n\n# Ablation Study\n\nTo explore the necessity of all parts of our method, we consider three settings in the ablation experiments: (1) removing related relation (-w/o relations) (2) removing instructions (-w/o instruction); (3) removing demo examples (-w/o examples). We report the results of three generalization levels in GrailQA separately in Figure 6. From the results, it can be seen that removing relations drastically reduces the F1 performance of the questions for zero-shot generalization, while having little impact on i.i.d and compositional questions. On top of that, removing instruction would bring about a weak degeneration in the F1 and FER. And removing demo examples would render the entire paradigm nearly invalid, meaning that it would be difficult for LLMs to understand the task requirements based on instructions alone.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c396/c396eee7-937b-4426-b7ea-cbf0ae285c7d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) The ablation study on F1 Score.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8d97/8d97e047-68b9-4335-a739-5f240cecef64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) The ablation study on Format Error Rate.\n</div>\n<div style=\"text-align: center;\">Figure 6: Ablation Study on GrailQA.\n</div>\n<div style=\"text-align: center;\">(c) Effect of the vote number\n</div>\n# Analysis on In-Context Learning\n\nIn this section, we explore the impact of three factors in ICL: the number of demo examples, the number of related relations, and the number of participating majority votes on the performance. For cost reasons, we performed this experiment on a subset of the 500-size scale on GrailQA.\nThe Effects of Demo Examples We analyze the effect of the shot number on the results. Specifically, we set K = {10, 20, 30, 40, 50} and report the performance trend with the shot number in Figure 5a. The results show that boosting the number of shot numbers has a stabilizing effect on F1, whereas FER was maintained at a low level throughout.\nThe Effects of Related Relation Number Similarly, we explore the effect of the number of correlations on the results. We set the related relation number as {1, 3, 5, 7, 9}  respectively and report the performance trend in Figure 5b. The results show that the introduction of more relations has little effect on both F1 and FER. Instead, too many relations make the performance degrade.\nThe Effects of Answer Number We explore the effect of the answer number of participating in the majority vote on the performance. Specifically, we set the answer number as {1, 2, 3, 4, 5, 6} respectively and report the performance trend in Figure 5c. The results show that FER improves significantly as the number of results participating in the vote rises, and there was an upward trend in F1 as well.\n\n# Conclusion\n\nIn this paper, we design a training-free KBQA framework, KB-Coder, which centers on a code-style in-context learning method for reducing formatting errors in generated logic forms and a retrieval-augment method for boosting zeroshot generalization capability. Extensive experimental results demonstrate that variants of our model, KB-Coder(1) and KB-Coder(6), achieve the SOTA performance under the few-shot setting, while the other two variants, KB-Coder(1)R and KB-Coder(6)-R, achieves competitive performance compared to fully-supervised methods in a training-free premise. In general, KB-Coder demonstrates the potential of code-style ICL in KBQA and offers a training-free but effective baseline for the community.\n\n# Acknowledgments\n\nThis work was supported by the National Key R&D Program of China under Grant 2022ZD0120200, in part by the National Natural Science Foundation of China (No. U23B2056), in part by the Fundamental Research Funds for the Central Universities, and in part by the State Key Laboratory of Software Development Environment.\n\n# References\n\nBaek, J.; Aji, A. F.; and Saffari, A. 2023. KnowledgeAugmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. arXiv preprint arXiv:2306.04136. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners.  Advances in neural information processing systems, 33: 1877\u2013 1901. Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cheng, Z.; Xie, T.; Shi, P.; Li, C.; Nadkarni, R.; Hu, Y.; Xiong, C.; Radev, D.; Ostendorf, M.; Zettlemoyer, L.; et al. 2022. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875. Gabrilovich, E.; Ringgaard, M.; and Subramanya, A. 2013. FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 2013-06-26, Format version 1, Correction level 0). Gao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Empirical Methods in Natural Language Processing (EMNLP). Gu, Y.; Deng, X.; and Su, Y. 2023. Don\u2019t Generate, Discriminate: A Proposal for Grounding Language Models to RealWorld Environments. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 4928\u20134949. Toronto, Canada: Association for Computational Linguistics. Gu, Y.; Kase, S.; Vanni, M.; Sadler, B.; Liang, P.; Yan, X.; and Su, Y. 2021. Beyond IID: three levels of generalization for question answering on knowledge bases. In Proceedings of the Web Conference 2021, 3477\u20133488. Gu, Y.; and Su, Y. 2022. ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. In  Proceedings of the 29th International Conference on Computational Linguistics, 1718\u2013 1731. Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299. Johnson, J.; Douze, M.; and J\u00b4egou, H. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3): 535\u2013547.\n\nLan, Y.; and Jiang, J. 2020. Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 969\u2013974. Li, P.; Sun, T.; Tang, Q.; Yan, H.; Wu, Y.; Huang, X.; and Qiu, X. 2023a. CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. In Rogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 15339\u201315353. Association for Computational Linguistics. Li, T.; Ma, X.; Zhuang, A.; Gu, Y.; Su, Y.; and Chen, W. 2023b. Few-shot In-context Learning on Knowledge Base Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 6966\u20136980. Toronto, Canada: Association for Computational Linguistics. Li, X.; Zhao, R.; Chia, Y. K.; Ding, B.; Bing, L.; Joty, S.; and Poria, S. 2023c. Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases. arXiv preprint arXiv:2305.13269. Mallen, A.; Asai, A.; Zhong, V.; Das, R.; Khashabi, D.; and Hajishirzi, H. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and NonParametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 9802\u20139822. Toronto, Canada: Association for Computational Linguistics. McCarthy, J. 1960. Recursive functions of symbolic expressions and their computation by machine, part I.  Communications of the ACM, 3(4): 184\u2013195. Shu, Y.; Yu, Z.; Li, Y.; Karlsson, B.; Ma, T.; Qu, Y.; and Lin, C.-Y. 2022. TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 8108\u20138121. Su, Y.; Sun, H.; Sadler, B.; Srivatsa, M.; G\u00a8ur, I.; Yan, Z.; and Yan, X. 2016. On generating characteristic-rich question sets for qa evaluation. In  Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 562\u2013572. Sun, H.; Bedrax-Weiss, T.; and Cohen, W. 2019. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2380\u20132390. Sun, Y.; Zhang, L.; Cheng, G.; and Qu, Y. 2020. SPARQA: skeleton-based semantic parsing for complex questions over knowledge bases. In Proceedings of the AAAI conference on artificial intelligence, 8952\u20138959. Tan, C.; Chen, Y.; Shao, W.; and Chen, W. 2023. Make a Choice! Knowledge Base Question Answering with InContext Learning. arXiv preprint arXiv:2305.13972.\n\nWang, X.; Li, S.; and Ji, H. 2022. Code4struct: Code generation for few-shot structured prediction from natural language. arXiv preprint arXiv:2210.12810. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022. SelfConsistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837. Ye, X.; Yavuz, S.; Hashimoto, K.; Zhou, Y.; and Xiong, C. 2022. RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 6032\u20136043. Yih, W.-t.; Richardson, M.; Meek, C.; Chang, M.-W.; and Suh, J. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 201\u2013206. Yu, D.; Zhang, S.; Ng, P.; Zhu, H.; Li, A. H.; Wang, J.; Hu, Y.; Wang, W. Y.; Wang, Z.; and Xiang, B. 2022. DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. In  The Eleventh International Conference on Learning Representations. Zhou, D.; Sch\u00a8arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang, X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V.; et al. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In  The Eleventh International Conference on Learning Representations.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. The emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate.",
        "problem": {
            "definition": "The problem addressed in this paper is the high format error rate in generating logical forms for Knowledge-Based Question Answering (KBQA) using Large Language Models (LLMs).",
            "key obstacle": "The main difficulty is that the highly specialized nature of logic forms means they rarely appear in the training corpus of LLMs, making it challenging to generate correctly formatted logic forms with only a few demo examples."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that converting original tasks to code generation tasks reduces the difficulty of post-processing, and that reasoning step-by-step can improve LLM performance on complex reasoning tasks.",
            "opinion": "The proposed method transforms the generation process of unfamiliar logical forms into a more familiar code generation process for LLMs, specifically using Python function calls.",
            "innovation": "The primary difference between the proposed method and existing approaches is the conversion of the one-step generation process of logic forms into a progressive generation process of function calls, which reduces formatting errors."
        },
        "method": {
            "method name": "Code-Style In-Context Learning for Knowledge-Based Question Answering",
            "method abbreviation": "KB-Coder",
            "method definition": "KB-Coder is a training-free KBQA model that utilizes code-style in-context learning to reduce formatting errors in generated logic forms and enhance zero-shot generalization capabilities.",
            "method description": "The method involves defining meta-functions for S-Expressions in Python and reformulating demo examples and test questions into code form for LLM processing.",
            "method steps": [
                "Define seven meta-functions for S-Expressions in Python.",
                "Convert demo examples into function call sequences.",
                "Input the test question alongside the demo examples into the LLM for generating the function call sequence."
            ],
            "principle": "The method is effective because it leverages the LLM's familiarity with code generation to improve the accuracy of logic form generation and reduce formatting errors."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three mainstream datasets: WebQSP, GrailQA, and GraphQ, which represent different generalization capabilities.",
            "evaluation method": "Performance was assessed using F1 Score and Exact Match (EM) metrics, as well as Format Error Rate (FER) to evaluate the correctness of generated logic forms."
        },
        "conclusion": "The experiments demonstrated that KB-Coder achieves state-of-the-art performance under few-shot settings and competitive results compared to fully-supervised methods, highlighting the effectiveness of code-style ICL in KBQA.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include a significant reduction in formatting error rates and improved zero-shot generalization performance by providing relevant relations as references.",
            "limitation": "The method may still face challenges in scenarios where the demo examples do not cover the queried domain, leading to potential inaccuracies in relation matching.",
            "future work": "Future research could focus on enhancing the model's robustness in zero-shot scenarios and exploring additional methods for improving formatting accuracy."
        },
        "other info": {
            "acknowledgments": "This work was supported by the National Key R&D Program of China under Grant 2022ZD0120200 and the National Natural Science Foundation of China (No. U23B2056).",
            "code_link": "The code and supplementary files are released at https://github.com/Arthurizijar/KB-Coder."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provide a simple and training-free semantic parsing paradigm for Knowledge-Based Question Answering (KBQA)."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, KB-Coder, is a training-free KBQA model that utilizes code-style in-context learning to reduce formatting errors in generated logic forms and enhance zero-shot generalization capabilities."
        },
        {
            "section number": "3.1",
            "key information": "KB-Coder leverages the LLM's familiarity with code generation to improve the accuracy of logic form generation and reduce formatting errors."
        },
        {
            "section number": "3.3",
            "key information": "The method involves defining meta-functions for S-Expressions in Python and reformulating demo examples and test questions into code form for LLM processing."
        },
        {
            "section number": "5.2",
            "key information": "The experiments demonstrated that KB-Coder achieves state-of-the-art performance under few-shot settings in Knowledge-Based Question Answering tasks."
        },
        {
            "section number": "6.1",
            "key information": "The method may still face challenges in scenarios where the demo examples do not cover the queried domain, leading to potential inaccuracies in relation matching."
        }
    ],
    "similarity_score": 0.7031400340195125,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/72b0/72b01aeb-75f1-4f0a-97b6-d4f22f7feed0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/81c0/81c0e638-2890-47a0-9f0a-4805cc3e1558.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/86d3/86d33bb8-042f-4079-bb6e-daccdfd1e252.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b675/b675a218-e258-40fd-846f-4ed31de54ef9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9ce8/9ce82848-1c8f-4daa-a07c-6a144fafddbd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c396/c396eee7-937b-4426-b7ea-cbf0ae285c7d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8d97/8d97e047-68b9-4335-a739-5f240cecef64.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Code-style in-context learning for knowledge-based question answering.json"
}