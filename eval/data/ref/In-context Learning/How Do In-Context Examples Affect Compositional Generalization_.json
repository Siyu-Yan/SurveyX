{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.04835",
    "title": "How Do In-Context Examples Affect Compositional Generalization?",
    "abstract": "Compositional generalization\u2014understanding unseen combinations of seen primitives\u2014is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2014the prevailing few-shot paradigm based on large language models\u2014exhibits compositional generalization. In this paper, we present COFE, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the incontext examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
    "bib_name": "an2023incontextexamplesaffectcompositional",
    "md_text": "Shengnan An\u2217\u2020, Zeqi Lin\u2021, Qiang Fu\u2021, Bei Chen\u2021, Nanning Zheng\u2020, Jian-Guang LOU\u2021, Dongmei Zhang\u2021 \u2020 Institute of Artificial Intelligence and Robotics, Xi\u2019an Jiaotong University \u2021 Microsoft Corporation {an1006634493@stu, nnzheng@mail}.xjtu.edu.cn {Zeqi.Lin, qifu, beichen, jlou, dongmeiz}@microsoft.com\n# Abstract\nCompositional generalization\u2014understanding unseen combinations of seen primitives\u2014is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2014the prevailing few-shot paradigm based on large language models\u2014exhibits compositional generalization. In this paper, we present COFE, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the incontext examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.\n# 1 Introduction\nCompositional generalization is an essential capability of human intelligence. It means to understanding and producing novel expressions by recombining known components in language (Chomsky, 1957; Montague, 1974; Fodor and Lepore, 2002). Taking examples in Figure 1, after learning the combination \u201cbaby in a room\u201d, human intelligence can easily generalize to \u201cJackson in a room\u201d. On exploring this human-like capability \u2217Work done during an internship at Microsoft Research.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1be3/1be314d9-04a6-43f8-bee9-a503a9fdf596.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">In-Context Learning Paradigm</div>\nLarge Language Model\nSampled Sequence\nFigure 1: Test compositional generalization under incontext learning. This case belongs to Phrase Recombination in COFE. The phrases modify the objects in examples but are recombined with subject in test input. in deep learning models, several benchmarks such as SCAN (Lake and Baroni, 2018), CFQ (Keysers et al., 2019) and COGS (Kim and Linzen, 2020) have been proposed based on semantic parsing1 tasks. In these benchmarks, the training set cover all the primitives while lacking certain combinations, and the test set focuses on these missing combinations. By fine-tuning generic neural models on these benchmarks, much work reported that these models exhibit poor compositional generalization (Furrer et al., 2020; Shaw et al., 2021; Bogin et al., 2022). Recently, in-context learning with large language models exhibits impressive performance on various tasks (Brown et al., 2020; Rae et al., 2021; Wei et al., 2022). By conditioning on few-shot incontext examples, the pre-trained language model, with extremely large model size and pre-trained\ncorpus, can perform downstream tasks without any update on pre-trained parameters.\ncorpus, can perform downstream tasks without any update on pre-trained parameters. Behind the impressive performance of in-context learning, we are curious whether this prevailing paradigm can take a step towards compositional generalization. To investigate this, we first take an initial exploration: for each test case in COGS, we select in-context examples from its training set and ensure that all primitives in each test case are covered by the equipped in-context examples. Our initial exploration suggests that compositional generalization can be easily affected by in-context examples: with only covering primitives, davinci 175B lags behind fine-tuned GPT2-Large with 24.2% accuracy (similar to the observation in Qiu et al. (2022)); with also covering some local structures (inspired by Bogin et al. (2022)), davinci outperforms fine-tuned GPT2-Large with 3.9% accuracy. Based on these initial observations, we raise and investigate the question: How do in-context examples affect compositional generalization? We construct the test suite COFE (based on COGS) to facilitate our systematic investigation. Taking the coverage of primitives as a basic principle in COFE, we further define and inject three factors in selecting in-context examples: similarity, diversity, and complexity. Similarity is considered as the matching of hidden structures behind concrete expressions. Diversity reflects whether the context presents repeated patterns or not. Complexity portrays the amount of information contained in each example. By controlling these factors in constructing COFE, we can systematically investigate how would in-context examples influence the performance on compositional generalization. Our experiments demonstrate that all three factors matter for in-context compositional generalization. We leverage six large language models in GPT series: davinci, code-cushman-001, codecushman-002, text-davinci-002, text-chat-davinci002, and code-davinci-002. The observations are consistent across models: to better perform compositional generalization, all backbone models prefer in-context examples with higher structural similarity to the test case, higher diversity among different examples, and lower complexity in each individual example. Furthermore, beyond the influence from these factors, in-context compositional generalization still faces two challenges. One is that in-context learning has difficulty recombining fictional words (e.g., random tokens) rather than com-\nmonly used ones. The other one is that in-context examples are still required to cover the linguistic structures in NL expressions, even though the backbone model has been pre-trained on large corpus. Our contributions are three-fold: 1) to answer the research question posed, we investigate three factors in selecting in-context examples and draw consistent conclusions across models; 2) we construct COFE to conduct our systematic investigation, and will release it to facilitate further exploration of in-context compositional generalization; 3) we also point out two remaining challenges that in-context learning still struggles to handle. We hope our analysis would provide insights on how to select proper in-context examples, and to shed light on the future research of in-context compositional generalization. COFE is publicly available at https://github.com/microsoft/Contextua lSP/tree/master/cofe.\n# 2 In-Context Compositional Generalization\nIn-context compositional generalization refers to understand and produce novel combinations through recombining the building blocks presented by in-context examples. We first introduce some basic settings for testing this desired capability, then show our initial observations.\n# 2.1 Principles for Measuring In-Context Compositional Generalization\nTo measure in-context compositional generalization under a test suite, each test case and its equipped in-context examples should satisfy two principles.\ngeneralization on certain combinations, incontext examples should exclude these combinations while test cases contain them.\n\u2022 Primitive coverage principle: the primitives contained in each test case should be fully covered by in-context examples. Primitives are the minimum indivisible units in expressions. In this work, we mainly consider primitives as lexical items (e.g., the noun \u201cbaby\u201d and the verb \u201cobserved\u201d in Figure 1).\nWe say that a model exhibits in-context compositional generalization if it performs well on a test suite that satisfies these two principles.\nCategory\nIn-Context Examples\nTest Case\nIllustration of Combination\nPrimitive\nSubstitution\nPrimitive\nStructural Alternation\nPhrase\nRecombination\nLonger\nChain\nDeeper\nNesting\ninput:   shark\noutput: SHARK\ninput:   A girl drew the boy .\noutput: DRAW ( GIRL , BOY , NONE )\ninput:   The shark drew a boy .\noutput: DRAW ( SHARK , BOY , NONE )\n\ud835\udc17\ud835\udc41\n\ud835\udc17\ud835\udc3f+\n\ud835\udc17\ud835\udc46\n1\ninput:   The goose baked .\noutput: BAKE ( GOOSE , NONE , NONE )\ninput:   A teacher noticed a chicken .\noutput: NOTICE ( TEACHER , CHICKEN , NONE )\ninput:   A teacher baked the chicken .\noutput: BAKE ( TEACHER , CHICKEN , NONE )\n+\n\ud835\udc17\ud835\udc46\n1\n\ud835\udc17\ud835\udc46\n2\ninput:   Logan mailed Stella the cake in the pile .\noutput: MAIL ( LOGAN , IN ( CAKE , PILE ) , STELLA )\ninput:   The goose rolled a baby in a room .\noutput: ROLL ( GOOSE , IN ( BABY , ROOM ) , NONE )\ninput:   A visitor in the pile rolled a resident .\noutput: ROLL ( IN ( VISITOR , PILE ) , RESIDENT , NONE )\n\ud835\udc17\ud835\udc46\n1\n+\n\ud835\udc17\ud835\udc46\n2\ninput:   The boy admired that Noah confessed that \\\nEmma was given a cookie .\noutput: ADMIRE ( BOY , NONE , NONE ) \\\nCCOMP CONFESS ( NOAH , NONE , NONE ) \\\nCCOMP GIVE ( NONE , COOKIE , EMMA )\ninput:   The girl wished that a crocodile declared that \\\nthe boy admired that Emma liked that \\\nEvelyn was passed a drink .\noutput: WISH ( GIRL , NONE , NONE ) \\\nCCOMP DECLARE ( CROCODILE , NONE , NONE ) \\\nCCOMP ADMIRE ( BOY , NONE , NONE ) \\\nCCOMP LIKE ( EMMA , NONE , NONE ) \\\nCCOMP PASS ( NONE , DRINK , EVELYN )\n\ud835\udc18\ud835\udc46\n\ud835\udc5b\n\ud835\udc5btimes\nrecursion\ninput: Noah appreciated a girl in a house \\\nbeside the chair .\noutput: APPRECIATE ( NOAH , \\\nIN ( GIRL , \\\nBESIDE ( HOUSE , CHAIR\\\n) ) , NONE )\ninput:   A dog painted the girl beside the chair \\\nin a house beside a road on a dish .\noutput: PAINT ( DOG , \\\nBESIDE ( GIRL , \\\nIN ( CHAIR , \\\nBESIDE ( HOUSE , \\\nON ( ROAD , DISH\\\n) ) ) ) , NONE )\n\ud835\udc18\ud835\udc46\n\ud835\udc5b\n\ud835\udc5btimes\nrecursion\n\ud835\udc17\ud835\udc3f\n\ud835\udc17\ud835\udc41\n<div style=\"text-align: center;\">Figure 2: Five categories of aiming combinations. The key parts in combinations are marked with underlines and colors (blue in NL-side and purple in code-side). The last column follows the notations defined in Section 3.2.</div>\n# 2.2 COGS (Under In-Context Learning)\nCOGS is a compositional generalization benchmark designed for the fine-tuning paradigm: based on a semantic parsing task, the training set of COGS covers all primitives in this task, while several combinations of primitives in the test set are excluded from the training set. We term these excluded combinations as aiming combinations. We measure in-context compositional generalization based on COGS, by converting it from the original fine-tuning paradigm to the in-context learning paradigm. For each COGS test case, we select in-context examples from the training set B, ensuring that the two principles are satisfied. Note that, for each test case, there are usually different collections of in-context examples satisfying the two principles. Our basic setting is to use a random one among them, and we show that this casual strategy could lead to an underestimation of in-context compositional generalization (Section 2.3). To facilitate testing on more complex logical forms, we reconstruct some target-side clauses from the chain structure into the nested-function format (illustrated in Figure 2). This reconstruction follows An et al. (2023) and is similar to the conversion from Lambda calculus to FunQL in Geo domain(Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2012). Moreover, to improve human readability, we omitted two types of details: the special marker for definite descriptions and the Skolem constants. These details do not affect the testing of compositional generalization.\nApart from these omitted details, the logical forms in COFE unambiguously represent the main semantics in the domain of COGS, such as semantic roles, modifications, and orders among clauses and modifications. More details about COFE logical forms are contained in Appendix A. Categories of aiming combinations. The aiming combinations in COGS can be divided into five categories, of which two are low-level combinations (i.e., focusing on specific primitives) and three are high-level combinations (i.e., focusing on high-level structures), illustrated in Figure 2. \u2022 Primitive Substitution (PrimSubs): Compose a primitive (e.g., \u201cshark\u201d) with a grammatical role (e.g., \u201csubject\u201d). \u2022 Primitive Structural Alternation (PrimAlte): Compose a primitive (e.g., \u201cbaked\u201d) with a sentence structure (e.g., \u201csubj. verb obj.\u201d). \u2022 Phrase Recombination (PhraReco): Compose a prepositional phrase (e.g., \u201cA in B\u201d) with a grammatical role (e.g., \u201csubject\u201d). \u2022 Longer Chain (LongChain): Extend the tail of the logical form with CCOMP clauses \u2208Y1 S. The max recursive times of CCOMP clauses in B is 2, while in test case it is 12. \u2022 Deeper Nesting (DeepNest): Expand the arguments in functions with IN/ON/BESIDE clauses \u2208Y1 S. The max recursive times in B and test cases are the same with LongChain. Note that PrimSubs and PrimAlte are low-level combinations while others are high-level ones.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/01f3/01f36e94-7005-46b7-bbd5-1601511787ce.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Initial observations on PrimSubs: casual selection leads to low performance while adding preference brings considerable gains.</div>\n# 2.3 In-Context Learning vs Fine-Tuning\nCompositional generalization under the fine-tuning paradigm has been widely studied (Furrer et al., 2020; Shaw et al., 2021; Bogin et al., 2022), while there is little observation under in-context learning. To first get a general sense about in-context compositional generalization, we conduct an initial exploration to compare with a fine-tuning baseline. Models and setups. We test in-context compositional generalization with six large models in GPT series: davinci, code-cushman-001 (cuchman001), code-cushman-002 (cuchman002), text-davinci002 (text002), text-chat-davinci-002 (chat002), and code-davinci-002 (code002). The sampling temperature is 0 (i.e., greedy decoding), and the max decoding length is 500. The reported metric is exact-match accuracy. To set a fine-tuning baseline, we take GPT2-Large with 0.7B parameters. We fine-tune it on the whole B and test without in-context examples. We set learning rate as 1e-5 and batch size as 8 during fine-tuning, and set beam size as 5 for inference. Appendix B includes more details.\n# Casual selection leads to low performance of in-context compositional generalization. For\nselecting in-context examples, we first take a casual selection: while satisfying the primitive coverage principle, we randomly select 10 examples without other preference. We conduct initial exploration on PrimSubs category. Figure 3 shows that under the casual selection, all six models lag behind the fine-tuned GPT2-Large on PrimSubs. In particular, although the size of davinci is more than 200 times that of GPT2-Large, there is a 24.2% accuracy gap between davinci and the fine-tuned GPT2-Large. These observations are close to Qiu et al. (2022). However, we suppose the potential of in-context learning is still not fully revealed. Specifically, the selection of in-context examples does not yet take full advantage of available examples in B. In next try, while still following the primitive coverage\nprinciple, we consider injecting some additional preference in the selection of in-context examples.\nPreference in selection could bring huge improvement on PrimSubs. Inspired by Bogin et al. (2022) that suggests the influence of unobserved local structures, we consider to prioritize examples that have similar hidden structures to the test case. Figure 3 shows that with this preference in selection, results on PrimSubs hugely change: davinci now outperforms the fine-tuned GPT2-Large; codedavinci-002 even performs near-perfectly. These changes strongly suggest that the selection of incontext examples can significantly affect in-context compositional generalization. Based on these initial results, to further reveal the potential of in-context learning, we perform in-depth investigations on how the selection of incontext examples affects compositional generalization.\n# 3 Factors Under In-Context Examples\nTo facilitate our systematic investigation, we construct COFE (COmpositional generalization with FEw-shot examples), which is derived from COGS. For selecting in-context examples in constructing COFE, we identify, inject, and control three potential factors: similarity, diversity, and complexity.\n# 3.1 Conceptual Definitions\nWe first give conceptual definitions of our considered factors and discuss our intuitions behind them.\nSimilarity has been widely considered as the main factor in selecting in-context examples (Liu et al., 2022; Shin et al., 2021; Rubin et al., 2021; Poesia et al., 2021). The primitive coverage principle can be regarded as a basic lexical similarity on the surface of expressions. Beyond this surface similarity, we consider that the structural similarity hidden behind expressions could be a beneficial factor. From the view of syntactic structure, the recombination of primitives is equivalent to the reconstruction of the parse tree. Similar structures would ease the difficulty of recombination because the model does not need to completely reconstruct the entire structure of in-context examples. Moreover, some work has suggested that the challenge of compositional generalization under fine-tuning lies in unobserved structures (Keysers et al., 2019; Shaw et al., 2021; Bogin et al., 2022).\nDiversity concerns the repetitiveness among incontext examples. It portrays the property among in-context examples. Specifically, the context is under low diversity if it contains many repeating patterns among in-context examples, otherwise it is under high diversity. Under in-context learning, the low diversity can easily lead to biased observations on the full task space, as there are only few examples for the model to learn. Thus, we suppose that the low diversity among examples could block in-context compositional generalization. Moreover, some work also demonstrated that the diversity in training data could affect compositional generalization under fine-tuning (Oren et al., 2021). Complexity reflects the amount of information contained in each individual in-context example. The higher complexity means that the example could provide more information to the model, but these information could be redundant. In addition, the difficulty in directly learning from complex examples has been flagged at the intersection of cognitive science and machine learning (Elman, 1993; Bengio et al., 2009). Such difficulty may be more severe for in-context learning, since the parameters of the model cannot be updated to fit these complex examples. Thus, we suppose that too high complexity might hinder performance.\n# 3.2 Incorporate Three Factors Into Test Suite\nTo inject these factors in selecting in-context examples, we design a matching score based on the parse trees behind concrete expressions. Formally, considering the primitive coverage, structural similarity, diversity and complexity, the matching score of two parse trees T and T\u2032 is defined as follows,\nMatch(T, T\u2032) =wp \u00b7 |P(T) \u2229P(T\u2032)|+ ws \u00b7 |S(T) \u2229 \ufffd S(T\u2032)\u2212S(C) \ufffd |\u2212 wc \u00b7 depth(T\u2032),\n(1)\nin which P(\u00b7) contains primitives, S(\u00b7) contains partial structures (defined later), C contains already selected examples, S(T\u2032) \u2212S(C) means to exclude already covered parts in S(C) from S(T\u2032), and depth(\u00b7) reflects the complexity of the tree. The meaning of three factors in Equation 1 is that: the structural similarity means covering S(T), the high diversity means to avoid repeatedly covering the same element in S(T), and the low complexity is to prioritize low-depth structures.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb4d/eb4df494-3704-4995-ba15-08f648b62071.png\" style=\"width: 50%;\"></div>\n# Based on this matching score, the overall ranking score between the test case (X, Y) and a candidate (Xc, Yc) is calculated as follows,\nBased on this matching score, the overall ranking score between the test case (X, Y) and a candidate (Xc, Yc) is calculated as follows,\nscorec = Match(X, Xc) + Match(Y, Yc), (2)\nin which both the matching of source side (i.e., NL expressions) and target side (i.e., logical forms) are considered. Poesia et al. (2021) has demonstrated the importance of target-side similarity in semantic parsing and code generation tasks, and this work will further investigates the necessity of source-side matching. In the following, we will give a more detailed description of notations in Equation 1. Detailed description: Figure 4 shows an illustration of notations. Considering an expression e with the parse tree T, TL represents leaf nodes (e.g., \u201cJackson\u201d) and TN contains internal nodes (e.g., \u201csubject\u201d). T1 S contains one-depth sub-structures in T. Each T1 s \u2208T1 S (e.g., \u2460in Figure 4) contains one parent node (e.g., \u201croot\u201d) and a set of child nodes (e.g., \u201csubject\u201d, \u201cverb\u201d and \u201cobject\u201d). T>1 S contains deeper sub-structures that are composed from several one-depth sub-structures in T1 S (e.g., \u2460+\u2461+\u2463in Figure 4). In Equation 1, the primitives P(T) = TL, and the partial structures S(T) = T1 S \u222aT>1 S . Note that aiming combinations \u2282S(T). Appendix E includes more details.\ndetailed description of notations in Equation 1. Detailed description: Figure 4 shows an illustration of notations. Considering an expression e with the parse tree T, TL represents leaf nodes (e.g., \u201cJackson\u201d) and TN contains internal nodes (e.g., \u201csubject\u201d). T1 S contains one-depth sub-structures in T. Each T1 s \u2208T1 S (e.g., \u2460in Figure 4) contains one parent node (e.g., \u201croot\u201d) and a set of child nodes (e.g., \u201csubject\u201d, \u201cverb\u201d and \u201cobject\u201d). T>1 S contains deeper sub-structures that are composed from several one-depth sub-structures in T1 S (e.g., \u2460+\u2461+\u2463in Figure 4). In Equation 1, the primitives P(T) = TL, and the partial structures S(T) = T1 S \u222aT>1 S . Note that aiming combinations \u2282S(T). Appendix E includes more details.\n# 4 Experiments and Analysis 4.1 Experimental Settings and Hyper-Parameters\n# 4.1 Experimental Settings and Hyper-Parameters\nWe take a greedy-search algorithm to sequentially select 10 examples for each test case. Models and setups follow our initial explorations in Section 2.3. For the investigation of each factor, hyperparameters in Equation 1 are set as follows2.\n<div style=\"text-align: center;\">Table 1: Results with (and without) structural similarity. Grey boxes mark the significantly better performan compared to the fine-tuned GPT2-Large.</div>\nModel\nSetting\nPrimSubs\nPrimAlte\nPhraReco\nLongChain\nDeepNest\nAvg. Acc\ncode-davinci-002\nPrimitive Coverage\n92.2\n77.1\n60.8\n62.1\n12.3\n60.9\n+ Structural Similarity\n99.8\n99.7\n65.3\n87.0\n26.0\n75.6\ntext-chat-davinci-002\nPrimitive Coverage\n92.2\n75.4\n47.0\n65.0\n6.3\n57.2\n+ Structural Similarity\n99.5\n99.3\n53.4\n87.7\n18.9\n71.8\ntext-davinci-002\nPrimitive Coverage\n88.5\n66.4\n38.7\n46.5\n2.9\n48.6\n+ Structural Similarity\n99.7\n99.4\n39.4\n80.2\n12.7\n66.3\ncode-cushman-002\nPrimitive Coverage\n82.6\n55.6\n21.3\n29.3\n5.0\n38.8\n+ Structural Similarity\n98.9\n99.0\n28.5\n64.0\n15.1\n61.1\ncode-cushman-001\nPrimitive Coverage\n76.6\n60.7\n16.9\n5.0\n1.0\n32.0\n+ Structural Similarity\n99.1\n98.4\n20.7\n11.1\n8.9\n47.6\ndavinci\nPrimitive Coverage\n69.4\n52.3\n9.4\n2.3\n0.2\n26.7\n+ Structural Similarity\n97.5\n95.4\n12.3\n13.4\n1.4\n44.0\nFine-Tuning Baseline\n-\n93.6\n97.9\n14.0\n5.4\n0.0\n42.2\nIn all settings, we prioritize the matching of primitives (i.e., |P(T) \u2229P(T\u2032)| in Equation 1) since the primitive coverage principle should be firstly satisfied. Concretely, we set wp = 100 and ensure wp \u226bws and wc in all settings. For investigating structural similarity3, we set ws = 1 and wc = 0, and exclude S(C) term. For investigating the effect of higher diversity, we add the S(C) term and keep other settings. For complexity, we set |wc|\u00b7max(depth(T\u2032)) < ws, such that the of preference of complexity will not influence the priority of structural similarity. Concretely, as max(depth(T\u2032)) = 12 in COFE, we set wc = 0.01 for the low-complexity experiments and wc = \u22120.01 for the high-complexity experiments, and exclude S(C) term. Some basic statistics for COFE under full similarity setting are listed in Table 2, and Appendix C.5 contains statistics under other settings. These statics show that the primitive coverage principle is well satisfied, since the cover rates of TL are almost 100%. Note that the coverage on T1 S \u222aT>1 S must be lower than 100% since the aiming combination must be excluded.\nStructural similarity brings significant gains. Table 1 shows the performance with structural similarity. Compared to the results without structural similarity (i.e., only with the coverage on primitives), there are considerable gains on all five categories and across all six models. These gains clearly demonstrate that beyond primitive coverage, the structural similarity under in-context examples\n# are essential for compositional generalization.\nMore precise structural similarity brings larger gains. As mentioned in Section 3.2, the structural similarity considers to match S(T) which contains two parts, T1 S and T>1 S . Specifically, we regard that T1 S describes the rough structure of T, and T>1 S determines a more precise structure. Based on the results in Table 1, we are curious about whether a rough structural similarity is enough. To verify this, we remove T>1 S from S(T), which means that now we do not restrict the selected incontext examples to match precise structures in test cases. Figure 5 shows that the performances on four categories significantly drop with only a rough structural similarity, indicating that matching the precise structure of test case is still required for in-context examples. The only exception lies in PhraReco. It suggests that similarity is not the only influential factor for in-context compositional generalization. In Section 4.3, we will show that the low diversity and high complexity potentially cause this exception.\ntions are almost solved while high-level combinations still have large room for improvement. Specifically, for code-davinci-002, which exhibits the best performance among all backbone models, it performs near-perfectly on low-level combinations (i.e., PrimSubs and PrimAlte) while still does not achieve >95% accuracy on high-level combinations (i.e., PhraReco, LongChain and DeepNest). Although in-context learning greatly exceeds the fine-tuning baseline on high-level combinations, we suppose there is still potential for improvement. Compared to low-level combinations, han-\nStatistics\nNumber of Instances\nAverage Coverage\nAverage Length\nTL\nTN\nT1\nS\nT>1\nS\nContext\nCase Input\nCase Output\nTest Cases\n4,785\n99.7%\n100%\n88.9%\n49.3%\n297.7\n17.8\n33.7\n- PrimSubs\n1,100\n100%\n100%\n79.8%\n45.1%\n236.7\n7.1\n11.5\n- PrimAlte\n700\n100%\n100%\n96.6%\n59.7%\n269.4\n7.9\n13.8\n- PhraReco\n1,000\n100%\n100%\n84.4%\n19.8%\n254.0\n10.7\n16.9\n- LongChain\n1,000\n99.8%\n100%\n97.8%\n76.7%\n370.6\n32.4\n76.7\n- DeepNest\n985\n98.8%\n100%\n89.0%\n48.6%\n356.4\n29.0\n46.3\nExample Bank\n24,155\n-\n-\n-\n-\n-\n7.5\n10.5\ndling high-level ones requires more creation than imitation, thus just considering similarity for incontext examples is not enough. In the following, we will further investigate these high-level combinations from the view of diversity and complexity.\n# 4.3 Diversity and Complexity\nHigh diversity brings considerable gains on PhraReco. Figure 6 shows how diversity among in-context examples affects generalization on highlevel combinations. It shows that increasing the diversity could bring considerable gains in PhraReco, while not affecting the other two categories. For the performance on PhraReco, the improvements from higher diversity are in line with our speculations in Section 3.1, that low diversity leads to biased observations, thus blocking high-level structural generalization. For LongChain and DeepNest, beyond biased structures, their difficulty also lies in length generalization, thus just increasing structural diversity brings less effect to them.\n# Low complexity brings considerable gains on\nPhraReco. Figure 7 shows how the complexity in each individual example affects generalization on high-level combinations. For PhraReco, there are \u223c10% gains in accuracy when the high complexity setting is changed to low complexity setting. We suppose the reason behind this gain is that simple examples could reduce the learning difficulty for the model. Moreover, simple examples also contain less redundant information thus would not confuse the model4. For LongChain and DeepNest, there is still less change on performance. Note that the max depth in these two categories is 13 while the max depth in the whole example bank is only 3. Therefore, changing the complexity of in-context examples would bring negligible influence for test cases in LongChain and DeepNest.\nTable 3: Results under different prompt orders (full similarity setting). \u2206represents the max difference in performance for each model.\nModel\nStructure Closer Atom Closer Random Order\n\u2206\ncode-davinci-002\n75.6\n74.2\n74.5\n1.4\ntext-davinci-002\n66.3\n66.0\n66.3\n0.3\ncode-cushman-002\n61.1\n60.0\n60.1\n1.1\ncode-cushman-001\n47.6\n48.2\n47.3\n0.9\ndavinci\n44.0\n43.6\n42.5\n1.5\n# 4.4 Analysis: Robustness to Prompt Order\nSome previous work on in-context learning showed that the order of exemplars in prompt could sometimes hugely influences the performance of LLMs (Zhao et al., 2021; Lu et al., 2022). Here, we examine whether our observations above are sensitive to the prompt order. Based on the full similarity setting (Section 4.2), we consider three different strategies for ordering exemplars: 1) random order; 2) atom closer: exemplars with higher coverage on atomic blocks are placed closer to the test input; 3) structure closer (default): examples with higher similarity on linguistic structures are placed closer to the test input. Implementations of different strategies for prompt order are detailed in Appendix C.3. Results in Table 3 show that the performance only slightly changes under different prompt orders. These results indicate that the main results revealed by COFE is consistent and reliable. It also indicates that in-context learning could be less sensitive to the prompt order when the in-context examples are chosen properly.\n# 4.5 Discussion: Difficulty in DeepNest\nAmong all five categories, in-context learning performs worst on DeepNest. Compared to LongChain which also test recursive structures, the results on DeepNest still lag far behind. There is an interesting observation from the study of error cases (such as Figure 10): in-context learning frequently makes word-level mistakes, while the overall nested struc-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ee4/1ee45460-a521-45c0-83a4-ac48b088b47a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ure 5: Performance of code-davinci-002 and text-davinci-002 with different levels of structural similarity.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c97e/c97e47c0-cdc2-47fb-a55d-ba244b67f779.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Performance under different diversity settings (on high-level combinations).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1244/1244aa16-25ad-4503-99fe-9aa91624e153.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Performance under different complexity settings (on high-level combinations).</div>\nture in the prediction is close to the ground truth. It suggests that the performance bottleneck in DeepNest is to correctly fill the details in the complex structure, rather than generating the sketch of the structure. Appendix F.1 provides further analysis.\n# 5 Remaining Challenges\nOur investigation has revealed a huge potential of in-context learning on performing compositional generalization5. Despite this potential, for achieving the ideal in-context compositional generalization, there remains the following two challenges.\nOur investigation has revealed a huge potential of in-context learning on performing compositional generalization5. Despite this potential, for achieving the ideal in-context compositional generalization, there remains the following two challenges. In-context examples are still required to match linguistic structures in NL expressions. Since all backbone models have been pre-trained on large natural language corpus, we expect that these models could already handle the high variety in NL expressions without further hints from in-context examples. Motivated by this, we conduct experiments on another variant of COFE: the source-side term Match(X, Xc) is removed from Equation 2, and the coverage of S(X) is limited (detailed in Appendix C.6). Figure 8 shows that on all five categories, the performance consistently drops if in5Appendix G shows the results of assembling factors.\nIn-context examples are still required to match linguistic structures in NL expressions. Since all backbone models have been pre-trained on large natural language corpus, we expect that these models could already handle the high variety in NL expressions without further hints from in-context examples. Motivated by this, we conduct experiments on another variant of COFE: the source-side term Match(X, Xc) is removed from Equation 2, and the coverage of S(X) is limited (detailed in Appendix C.6). Figure 8 shows that on all five categories, the performance consistently drops if in5Appendix G shows the results of assembling factors.\ncontext examples do not match the NL-side structure. It suggests that even having been pre-trained on large corpus, in-context learning still struggles to effectively recognize the semantic equivalence among different linguistic structures behind NL expressions (detailed in Appendix F.3). In-context learning has difficulty leveraging fictional words6. The ideal compositional generalization requires that the recombination of primitives should be independent of the surface form in primitives. In COFE, we set the target-side primitives as the uppercase of source-side ones (e.g., \u201ccat\u201d\u2192\u201cCAT\u201d). Such case conversion is commonly used in semantic parsing tasks. To test whether in-context learning could use fictional words, we replace each target-side word with random characters (e.g., replace \u201cCAT\u201d with \u201cMXR\u201d, detailed in Appendix C.7). Figure 9 shows the huge drops after changing words. Moreover, we investigate the structural accuracy by only keeping the structural terminals (e.g., parentheses and commas) in predictions. Figure 9 shows that the structural accuracy is also affected by fictional words. It indicates that on performing in-context compositional generalization, the prediction of structural sketch is not decoupled with word-level patterns.\n# 6 Related Work\nCompositional generalization (CG) has attracted much attention in NLP field. Most existing benchmarks measured CG under fine-tuning with synthetic semantic parsing tasks, suggesting the limitations of general-purpose neural networks (Lake and Baroni, 2018; Keysers et al., 2019; Kim and Linzen, 2020). Many approaches were proposed to enhance the CG on general-purpose models (Andreas, 2020; Aky\u00fcrek et al., 2020; Guo et al., 2021; Oren et al., 2021; Shaw et al., 2021; Zhu et al., 2021) or design task-specific methods (Liu et al., 2020; Herzig\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6fb1/6fb158e9-ec71-4c26-ab77-5d12d68bf57a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Performance with or without matching linguistic structures in NL expressions.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0166/01661f03-3391-435e-8f3d-15454dc72bbd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Average exact-match accuracy and structural accuracy with fictional words.</div>\nand Berant, 2021; Chen et al., 2020; Liu et al., 2021). Some influential factors that affect CG have been revealed, such as the length bias (Csord\u00e1s et al., 2021), target-side format (Furrer et al., 2020; Herzig et al., 2021) and local structures (Bogin et al., 2022). Most existing work explored CG under the fine-tuning paradigm, while our work advances the exploration under the in-context learning paradigm.\nIn-context learning (ICL) along with large language models (LLMs) has shown surprising performance in many NLP tasks (Brown et al., 2020; Hendrycks et al., 2020; Patel and Pavlick, 2021; Rae et al., 2021; Zhang et al., 2022a; Hoffmann et al., 2022; Srivastava et al., 2022; Chowdhery et al., 2022; Smith et al., 2022; Wei et al., 2022). Most related to our work, Qiu et al. (2022) and Drozdov et al. (2022) also explored ICL on CG challenges. Qiu et al. (2022) utilized the targetside similarity on structural fragments and reported that LLMs still exhibited much poorer CG than finetuned small models on COGS, which is close to our initial observations. Drozdov et al. (2022) designed task-specific inference pipelines for performing CG under a least-to-most manner. Our work provides more general understandings on how to improve CG performance by revealing several factors in selecting in-context examples. In addition, some more recent work has similar observations on the potential of LLMs on CG (Hosseini et al., 2022), gains from diversity (Levy et al., 2022), and challenges under fictional words (Kim et al., 2022) Selection of in-context examples is an essential\nFigure 10: An error case in DeepNest (full similarity setting) with wrong local words and redundant parts.\npart for the utilization of ICL. Most existing work considered the similarity as the major metric during selection. Liu et al. (2022) selected k-nearest neighbors with similar sentence embeddings; Shin et al. (2021) regarded the conditional probability from a pre-trained LLM as the similarity score; Rubin et al. (2021) and Zhang et al. (2022b) separately trained a retriever to score the similarity; Poesia et al. (2021) and Madaan et al. (2022) estimated the target-side similarity. This work demonstrates the necessity of structural similarity in achieving CG, and also reveals the importance of two other factors beyond similarity, i.e., diversity and complexity.\n# 7 Conclusion and Future Work\nThis work investigates how in-context compositional generalization is affected by the selection of examples. The test suite COFE is constructed to study three factors. Experiments show the effects of structural similarity, higher diversity and lower complexity. Two challenges under in-context compositional generalization are further revealed. To apply our revealed factors outside the COFE test suite, one main challenge for future work is to determine the hidden structures behind expressions without knowing the exact generative grammar. Here, we consider two potential approaches. One is to use a pre-trained parser to generate a parse tree for the input query and then measure tree similarity. The other approach is to pre-train an embedding model with a structure-aware training objective and then compute embedding similarity.\n# Limitations\nGPU resources. This work utilizes extremely large language models and thus has a high cost on GPU resources. Concretely, experiments are conducted on the 8 x NVIDIA A100 GPU station. The maximum inference time on each version of COFE (containing 4,785 test cases) is \u223c8 hours. The maximum estimation of costed computing resources in this study is \u223c500 x 8 GPU hours.\nSynthetic data. As in most previous work on compositional generalization (Lake and Baroni, 2018; Keysers et al., 2019; Kim and Linzen, 2020), the COFE dataset is constructed using synthetic data rather than natural one. The source-side sentences in COFE are from COGS, which account for 70\u201380% of naturally-occurring English sentences (Kim and Linzen, 2020; Roland et al., 2007). Thus, this synthetic test suite could be close to the real-world application scenarios.\nSingle run. Due to the high cost on computing resources, we do not take multiple runs with different sets of examples, nor did we take multiple samples with temperature > 0. Observations under different prompt orders (in Appendix 4.4) imply that with desired factors in selecting in-context examples, there could be low variance in experiments.\n# Ethics Statement\nDue to the utilization of pre-trained language models, this work could be exposed to some potential risks of ethical issues on general deep learning models (such as social bias and privacy breaches). As explored in this work that the model behavior can be hugely influenced by the provided context, we call for further investigation into how ethical issues can be avoided by controlling the provided context.\n# Acknowledgments\nWe thank all the anonymous reviewers for their valuable comments. Shengnan An and Nanning Zheng were supported in part by NSFC under grant No. 62088102.\n# References\nBen Bogin, Shivanshu Gupta, and Jonathan Berant. 2022. Unobserved local structures make compositional generalization hard. arXiv preprint arXiv:2201.05899.\nNoam Chomsky. 1957. Syntactic structures (the hague: Mouton, 1957). Review of Verbal Behavior by BF Skinner, Language, 35:26\u201358.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nR\u00f3bert Csord\u00e1s, Kazuki Irie, and Juergen Schmidhuber. 2021. The devil is in the detail: Simple tricks improve systematic generalization of transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 619\u2013634. Andrew Drozdov, Nathanael Sch\u00e4rli, Ekin\nZheng, and Dongmei Zhang. 2020. Compositional generalization by learning analytical expressions. Advances in Neural Information Processing Systems, 33:11416\u201311427.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128.\nR Montague. 1974. English as a formal language. Formal Philosophy: Selected Papers of Richard Montague.\nInbar Oren, Jonathan Herzig, and Jonathan Berant. 2021. Finding needles in a haystack: Sampling structurally-diverse training sets from synthetic data for compositional generalization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10793\u201310809.\nRoma Patel and Ellie Pavlick. 2021. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations.\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2021. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations.\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. 2022. Evaluating the impact of model scale for compositional generalization in semantic parsing. arXiv preprint arXiv:2205.12253.\nack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\nDouglas Roland, Frederic Dick, and Jeffrey L Elman. 2007. Frequency of basic english grammatical structures: A corpus analysis. Journal of memory and language, 57(3):348\u2013379.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 922\u2013938.\nRichard Shin, Christopher Lin, Sam Thomson, Charles Chen Jr, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7699\u20137715.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatronturing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.\nJohn M Zelle and Raymond J Mooney. 1996. Learning to parse database queries using inductive\nLuke S Zettlemoyer and Michael Collins. 2012. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. arXiv preprint arXiv:1207.1420.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486.\nThis is the Appendix of the paper: How Do In-Context Examples Affect Compositional Generalization?\n# A Grammar\nPart of the grammar used in constructing COFE is listed in Table 4. Note that the max recursive times of R-Production Rules is 2 in prompting examples and 12 in test cases. The target-side grammar follows the reconstruction in An et al. (2023). Overall, the original target grammar of COGS is reconstructed to be chain-structured. Concretely, first, the original output tokens in COGS are capitalized; then, the variables (e.g., \u201cx_1\u201d) in the original grammar are aligned and replaced with their corresponding terminals; finally, the output clauses are grouped as the function format, in which the function name belongs to \u201cPRED-FUNC\u201d and the arguments are ordered as \u201cAGENT\u201d, \u201cTHEME\u201d, and \u201cRECIPIENT\u201d. Moreover, if \u201cPRED-FUNC\u201d does not contain one or some arguments, the positions of these arguments are filled with \u201cNONE\u201d terminal. For the two R-Production rules in Table 4, the first is in chain structure and the second is in nested structure. Moreover, the whole nested \u201cPPFUNC\u201d will be filled into the \u201cPRED-FUNC\u201d as an argument, rather than concatenated to the tail of the \u201cCLAUSE\u201d.\n# B Details of Fine-Tuning\nThe fine-tuned GPT2-Large contains 762M parameters. For fine-tuning, we take 50,000 training steps with 8 batch size and 1e-5 learning rate (without warm-up strategy). We set weight decay as 1e-2 and label smoothing factor as 1e-1. For inference with GPT2-Large, we set beam size as 5 and set max length as 1,024.\n# C Details of Implementation\n# C.1 Algorithm\nAlgorithm 1 shows the greedy searching algorithm for constructing COFE.\n# C.2 Key Designs\nWe give detailed descriptions of some key designs in Algorithm 1.\n\u2022 P(T): Return the leaf nodes TL on the tree; \u2022 S(T): Return the structural combinations on the tree, i.e., T1 S \u222aT>1 S ;\nstructing COFE\nGiven:\n(X, Y): Source and target parse trees in one test case;\nB: Example bank;\n(Xi, Yi) \u2208B: One candidate case in example bank;\nXA and YA: Aiming combination;\nwp, ws, wc: Weights for primitive coverage, structural\nsimilarity, and complexity penalty;\nP(\u00b7): primitives;\nS(\u00b7): structural combinations;\nReturn:\nC: Selected in-context examples;\n1: C = {}\n2: while |C| < n do\n3:\nmax_score = 0\n4:\ncandidate = None\n5:\nfor (Xi, Yi) \u2208B do\n6:\nAssert XA /\u2208S(Xi)\n7:\nAssert YA /\u2208S(Yi)\n8:\nprim_score = 0\n9:\nstru_score = 0\n10:\nfor element \u2208P(Xi) \u222aP(Yi) do\n11:\nif element \u2208P(X) \u222aP(Y) then\n12:\nprim_score += wp\n13:\nend if\n14:\nend for\n15:\nfor element \u2208S(Xi) \u222aS(Yi) do\n16:\nif element \u2208S(X) \u222aS(Y) and element /\u2208S(C)\nthen\n17:\nstru_score += ws\n18:\nend if\n19:\nend for\ncomp_penalty = wp \u00b7 depth(Xi)\nscore = prim_score + stru_score - comp_penalty\n20:\nif score > max_score then\n21:\nmax_score = score\n22:\ncandidate = (Xi, Yi)\n23:\nend if\n24:\nend for\n25:\nC.add(candidate)\n26: end while\n\u2022 depth(T): return the depth of the tree. Note that depth(Xi) = depth(Yi) in COFE.\n# C.3 Prompt Order\nWe take the structure-closer order, i.e., the examples in C with a higher stru_score are placed closer to the test case. In Section 4.4, we show the robustness to the other two orders: random order, i.e., all selected in-context examples in C are randomly\n<div style=\"text-align: center;\">Table 4: Part of the grammar used in constructing COFE.</div>\nFormal English Grammar\nSemantic Representation\nType\nactive-verb / passive-verb \u21a0Sv\nPRED-FUNC \u21a0SP\nT-Production Rule\nsubject / direct-object / indirect-object \u21a0Sn\nAGENT / THEME / RECIPIENT \u21a0SE\npp-mod / pp-s \u21a0Sn\nPP-FUNC / PP-S \u21a0SE\nconj \u21a0that\nCP-CONCAT \u21a0CCOMP\nprep \u21a0in / on / beside\nPP-CONCAT \u21a0IN / ON / BESIDE\nsentence \u21a0subj active-verb\nCLAUSE \u21a0PRED-FUNC ( AGENT, NONE, NONE )\nN-Production Rule\nsentence \u21a0subj active-verb direct-obj indirect-obj\nCLAUSE \u21a0PRED-FUNC ( AGENT, THEME, RECIPIENT )\nsubject / direct-object / indirect-object \u21a0pp-mod\nAGENT / THEME / RECIPIENT \u21a0PP-FUNC\nsentence \u21a0sentence conj sentence\nCLAUSE \u21a0CLAUSE CP-CONCAT CLAUSE\nR-Production Rule\npp-mod \u21a0pp-s prep pp-mod\nPP-FUNC \u21a0PP-CONCAT ( PP-S, PP-FUNC )\nshuffled, and atom-closer order, i.e., the examples in C with a higher prim_score are placed closer to the test case.\n# C.4 Max Depth in T>1 S\nSince the max repetition times for LongChain and DeepNest are 2 (as described in Section 2.2), we set the max depth in T>1 S as 2 in S(T).\n# C.5 Similarity Under Diversity and Complexity Settings\n<div style=\"text-align: center;\">Table 5: Statistics of different versions of COFE (PhraReco category).</div>\nTable 5: Statistics of different versions of COFE (PhraReco category).\nSetting\nAverage Coverage\nTL\nTN\nT1\nS\nT>1\nS\nDefault (Low Diversity, Mid Complexity)\n100%\n100%\n84.4%\n19.8%\nHigh Diversity\n100%\n100%\n84.4%\n19.8%\nLow Complexity\n100%\n100%\n84.4%\n19.8%\nHigh Complexity\n100%\n100%\n84.4%\n19.8%\nWhile changing diversity and complexity in variants of COFE in Section 4.3, the primitive coverage and structural similarity are still satisfied. Table 5 shows that onPhraReco, the statistics of coverage in different diversity and complexity settings are kept identical to the full similarity setting in COFE.\n# C.6 Excluding NL-Side Matching\nFor excluding source-side matching in Section 5, besides removing the first term in Equation 2, we also limit the matching of X1 S. Concretely, we require that the sentence rule in test case should not be covered by in-context examples. The sentence rule is an N-Production rule that contains the nonterminal \u201csentence\u201d as the left hand. To achieve this, we filter out test cases that can not meet this constraint. Finally, 1,037 out of 4,785 test cases are kept in this variant of COFE.\n# C.7 Fictional Words\nFor each target-side word that contain l characters, we sequentially and randomly sample l characters from alphabet as a fictional word to replace the original word. In addition, for the experiments on fictional words, we take the atom-closer prompt order, since the model with this order performs better the default structure-closer order.\n# D Excluding Target-Side Matching\nIn Section 5, we show that the performance drops with excluding the source-side matching. Here, we examine the effect of target-side matching. For constructing data, we directly remove the second term in Equation 2. As shown in Table 6, the performances with or without target-side matching are nearly identical. Such an observation is similar to the comparison between oracle and non-oracle settings in Qiu et al. (2022) that also utilized COGS benchmark, but different from Poesia et al. (2021) which suggested the importance of target-side similarity in code generation tasks. We suppose there are mainly two reasons that could cause this difference. On the one hand, different from general code generation tasks, the test suite for compositional generalization requires the exclusion of certain aiming combinations. Therefore, the performance bottleneck in compositional generalization benchmarks mainly lies in the lacked aiming combinations. On the other hand, in most compositional generalization benchmarks, the source-side matching could largely take over the target-side matching, since the terminals and rules in source grammar in these benchmarks are mapped many-to-one to the target grammar. Therefore, when seeking for the source-side matching, the target-side matching is also improved.\n<div style=\"text-align: center;\">Table 6: Performances under only matching source side.</div>\nModel\nSetting\nPrimSubs\nPrimAlte\nPhraReco\nLongChain\nDeepNest\nAverage\ncode-davinci-002\nmatching both side\n99.8\n99.7\n65.3\n87.0\n26.0\n75.6\nonly matching source side\n99.3\n99.7\n63.2\n88.9\n25.8\n75.4\ntext-davinci-002\nmatching both side\n99.7\n99.4\n39.4\n80.2\n12.7\n66.3\nonly matching source side\n98.8\n99.6\n35.6\n81.1\n12.5\n65.5\ncode-cushman-002\nmatching both side\n98.9\n99.0\n28.5\n64.0\n15.1\n61.1\nonly matching source side\n98.6\n99.4\n26.7\n66.8\n16.3\n61.6\ncode-cushman-001\nmatching both side\n99.1\n98.4\n20.7\n11.1\n8.9\n47.6\nonly matching source side\n99.2\n99.6\n17.4\n13.1\n8.6\n47.6\ndavinci\nmatching both side\n97.5\n95.4\n12.3\n13.4\n1.4\n44.0\nonly matching source side\n97.7\n94.7\n7.2\n14.7\n2.1\n43.3\n# E Illustration of Defined Notations\nFigure 11 illustrates the notations defined in Section 3.2 based on a concrete expression \u201cJackson in a room observed a baby\u201d. Note that for all sub-structures in T1 S \u222aT>1 S , we require them to be complete sub-structures.\nDefinition: Complete sub-structure (CSS). A CSS is a subgraph in a tree T, satisfying that if an internal node in T and one of its child nodes are covered in this CSS, all other child nodes must be also covered in this CSS.\n# F Case Study\nWe provide case study to further understanding the performance of compositional generalization observed in the main text. For ease of reading, we include the following contents in the caption of figures.\n# F.1 Two Types of Errors in DeepNest\nFigure 12 shows two error cases in DeepNest with code-davinci-002 model and full similarity setting. The overall structure of predictions are close to the ground truth, but the model makes mistakes on some local parts. Concretely, some local semantics are incorrect (in red), and some words are redundant (in gray). Moreover, we also calculate the word-level coverage in predictions. Besides the instance-level accuracy, we further investigate a word-level error rate on DeepNest. We find that in DeepNest, 96.8% of the words in the ground truth are contained by the predictions from code-davinci-002 (while only 48.8% for GPT2-Large). It indicates that the low instance-level accuracy is mainly caused by the wrong positions of words and redundant words.\n# F.2 Structural Errors with Fictional Words\nFigure 13 shows the comparison of performance between fictional words (left) and commonly used words (right). For the provided contexts on the left and right, the only difference is that the target-side words on the left are randomly selected characters while on the right they are uppercase of the source-side words. It shows that by changing only the target-side words, the model not only makes word-level errors (i.e., missing two words \u201cES\u201d and \u201cNVCWI\u201d in prediction), it also generates the wrong parentheses structure (i.e., generate a 2-depth structure while in ground truth it is 3-depth).\n# F.3 Fail to Recognize Semantic Equivalence\nFigure 14 shows the comparison of performances between excluding NL-side matching (left) and containing NL-side matching (right). For the test input \u201cMatthew shipped the professor a chair .\u201d, it contains the sentence structure \u201csubject verb object_1 object_2\u201d behind the NL expression. Context on the left does not explicitly contain this sentence structure, but it contains a semantically equivalent structure (i.e., \u201csubject verb object_2 to object_1\u201d). However, the model generates the correct prediction on the right while fails on the left. Concretely, according to the wrong prediction on the left, the model perhaps considers that the semantics of \u201csubject verb object_1 object_2\u201d is equivalent with \u201csubject verb object_1 to object_2\u201d.\n# F.4 Low Diversity Block Generalization\nFigure 15 shows the comparison of performances on PhraReco under high diversity (left) and low diversity (right). For the test input \u201cA girl in the house slept\u201d, \u201csubject slept\u201d is one element contained in T>1 S . This element is repeatedly covered in the context on the right (low diversity) while only covered once on the left (high diversity). However,\nunder high repetitiveness, the model fails on the test case, but succeed when there is low repetitiveness.\n# F.5 High Complexity Block Generalization\nFigure 16 shows the comparison of performance on PhraReco under low complexity (left) and high diversity (right). With low complexity, the test case is covered by simple and short in-context examples, and the model succeeds on the test case. With high complexity, the test case is covered by more complex and longer examples, and the model fails on the test case.\n# G Full Results\nDue to the page limitation for main text, here we list our full results in Section 4. The results in Assembling are the best performance under each category among all combinations of factors.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8af0/8af020fa-1a3f-4107-bc01-af76724c2d68.png\" style=\"width: 50%;\"></div>\nGIVE ( DOG , ON ( CAKE , ON ( TABLE , IN ( STOOL , ON ( CONTAINER , ON ( BENCH , PLATE ) ) ) ) ) , OLIVIA )\n# pred:\n# pred: \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a8a7/a8a72eee-347e-47c3-8eb2-8527bb0efbbe.png\" style=\"width: 50%;\"></div>\nlabel:\nFigure 13: Comparison of performance between fictional words (left) and commonly used words (right). For the provided contexts on the left and right, the only difference is that the target-side words on the left are randomly selected characters while on the right they are uppercase of the source-side words. It shows that by changing only the target-side words, the model not only makes word-level errors (i.e., missing two words \u201cES\u201d and \u201cNVCWI\u201d in prediction), it also generates the wrong parentheses structure (i.e., generate a 2-depth structure while in ground truth it is 3-depth).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1802/18020534-d60d-4b83-b595-5b202f04ee83.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c197/c197b8f2-00d3-429a-aa44-e2aa8d195f59.png\" style=\"width: 50%;\"></div>\nFigure 14: Comparison of performances between excluding NL-side matching (left) and containing NL-side matching (right). For the test input \u201cMatthew shipped the professor a chair .\u201d, it contains the sentence structure \u201csubject verb object_1 object_2\u201d behind the NL expression. Context on the left does not explicitly contain this sentence structure, but it contains a semantically equivalent structure (i.e., \u201csubject verb object_2 to object_1\u201d). However, the model generates the correct prediction on the right while fails on the left. Concretely, according to the wrong prediction on the left, the model perhaps considers that the semantics of \u201csubject verb object_1 object_2\u201d is equivalent with \u201csubject verb object_1 to object_2\u201d.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f15/0f153059-e75e-451c-b707-71b1f8feb202.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a3e/4a3efdb5-949f-4ad2-9d91-7be2670e7b89.png\" style=\"width: 50%;\"></div>\nlabel: SLEEP ( IN ( GIRL , HOUSE ) , NONE , NONE ) pred:  SLEEP ( IN ( GIRL , HOUSE ) , NONE , NONE )\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a075/a075e009-420e-4bf8-b9ab-0d079512cb0b.png\" style=\"width: 50%;\"></div>\nlabel: SLEEP ( IN ( GIRL , HOUSE ) , NONE , NONE ) pred:  SLEEP ( IN ( GIRL , HOUSE ) , NONE , NONE )\nFigure 16: Comparison of performance on PhraReco under low complexity (left) and high diversity (right). With low complexity, the test case is covered by simple and short in-context examples, and the model succeeds on the test case. With high complexity, the test case is covered by more complex and longer examples, and the model fails on the test case.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/95d5/95d54177-d383-462e-bb0a-0b6d80d6d587.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 7: Full results.</div>\nModel\nPrimitive\nSimilarity\nDiversity\nComplexity\nPrimSubs\nPrimAlte\nPhraReco\nLongChain\nDeepNest\nAverage\nRough\nPrecise\nLow\nHigh\nLow\nMid\nHigh\ncode-davinci-002\n\u2713\n92.2\n77.1\n60.8\n62.1\n12.3\n60.9\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n99.8\n99.7\n65.3\n87.0\n26.0\n75.6\n\u2713\n\u2713\n\u2713\n\u2713\n97.7\n92.1\n77.6\n80.4\n18.3\n73.2\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n80.0\n87.6\n26.2\n64.6\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n67.6\n87.3\n25.6\n60.2\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n56.9\n87.6\n26.0\n56.8\nAssembling Desired Factors\n99.8\n99.7\n80.0\n87.6\n26.2\n78.7\ntext-chat-davinci-002\n\u2713\n92.2\n75.4\n47.0\n65.0\n6.3\n57.2\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n99.5\n99.3\n53.4\n87.7\n18.9\n71.8\n\u2713\n\u2713\n\u2713\n\u2713\n96.1\n89.7\n62.9\n80.1\n11.7\n68.1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n69.2\n87.6\n18.2\n58.3\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n55.1\n87.6\n19.0\n53.9\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n45.1\n88.2\n19.2\n50.8\nAssembling Desired Factors\n99.5\n99.3\n69.2\n88.2\n19.2\n75.1\ntext-davinci-002\n\u2713\n88.5\n66.4\n38.7\n46.5\n2.9\n48.6\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n99.7\n99.4\n39.4\n80.2\n12.7\n66.3\n\u2713\n\u2713\n\u2713\n\u2713\n94.9\n86.7\n55.9\n66.3\n8.1\n62.4\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n60.6\n78.7\n12.3\n50.5\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n43.2\n79.9\n12.9\n45.3\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n33.5\n80.2\n12.8\n42.2\nAssembling Desired Factors\n99.7\n99.4\n60.6\n80.2\n12.9\n70.6\ncode-cushman-002\n\u2713\n82.6\n55.6\n21.3\n29.3\n5.0\n38.8\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n98.9\n99.0\n28.5\n64.0\n15.1\n61.1\n\u2713\n\u2713\n\u2713\n\u2713\n94.0\n77.7\n31.4\n44.7\n10.3\n51.6\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n40.8\n62.4\n14.9\n39.4\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n31.9\n64.3\n15.8\n37.3\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n22.6\n64.5\n14.6\n33.9\nAssembling Desired Factors\n98.9\n99.0\n40.8\n64.5\n15.8\n63.8\ncode-cushman-001\n\u2713\n76.6\n60.7\n16.9\n5.0\n1.0\n32.0\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n99.1\n98.4\n20.7\n11.1\n8.9\n47.6\n\u2713\n\u2713\n\u2713\n\u2713\n92.5\n86.0\n24.7\n8.0\n3.5\n42.9\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n31.4\n12.8\n8.4\n17.5\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n23.2\n12.7\n8.9\n14.9\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n18.6\n11.5\n8.7\n12.9\nAssembling Desired Factors\n99.1\n98.4\n31.4\n12.8\n8.9\n50.1\ncode-cushman-001\n\u2713\n69.4\n52.3\n9.4\n2.3\n0.2\n26.7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n97.5\n95.4\n12.3\n13.4\n1.4\n44.0\n\u2713\n\u2713\n\u2713\n\u2713\n79.4\n66.6\n18.8\n4.3\n1.3\n34.1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n20.0\n10.2\n1.3\n10.5\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n14.7\n13.8\n1.4\n10.0\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n-\n-\n7.8\n13.5\n1.3\n7.5\nAssembling Desired Factors\n97.5\n95.4\n20.0\n13.8\n1.4\n45.6\nFine-Tuned GPT2-Large\n-\n93.6\n97.9\n14.0\n5.4\n0.0\n42.2\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Compositional generalization is a fundamental aspect of human reasoning, allowing the understanding and production of novel expressions by recombining known components. The AI community has primarily focused on this capability through fine-tuning neural networks on extensive training datasets, but there remains uncertainty about the effectiveness of in-context learning\u2014especially in large language models\u2014on compositional generalization.",
            "purpose of benchmark": "The benchmark COFE is designed to investigate in-context compositional generalization to understand how in-context examples influence model performance in generating unseen combinations of known primitives."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of understanding how in-context examples affect compositional generalization, specifically focusing on the selection of examples that can enhance the model's ability to generalize to new combinations.",
            "key obstacle": "Existing benchmarks primarily focus on semantic parsing tasks but often overlook the influence of in-context examples, leading to poor performance in compositional generalization."
        },
        "idea": {
            "intuition": "The creation of COFE stems from the observation that the performance of compositional generalization is significantly affected by the selection of in-context examples, raising the question of what factors contribute to effective example selection.",
            "opinion": "The authors believe that understanding the factors influencing in-context learning is crucial for advancing research in compositional generalization and improving the performance of language models.",
            "innovation": "COFE introduces a structured approach to selecting in-context examples based on three key factors: similarity, diversity, and complexity, which differ from previous benchmarks that did not account for these aspects.",
            "benchmark abbreviation": "COFE"
        },
        "dataset": {
            "source": "The dataset COFE is derived from the COGS benchmark, which includes a variety of test cases designed to evaluate compositional generalization under in-context learning.",
            "desc": "COFE consists of 4,785 test cases that cover various combinations and structures necessary for evaluating in-context compositional generalization.",
            "content": "The dataset includes natural language expressions and their corresponding logical forms, facilitating the evaluation of models on their ability to generalize from learned examples.",
            "size": "4,785",
            "domain": "Semantic Parsing",
            "task format": "Compositional Generalization"
        },
        "metrics": {
            "metric name": "Accuracy",
            "aspect": "Model performance in generating correct outputs based on in-context examples.",
            "principle": "The choice of accuracy as a metric is driven by its relevance in assessing the correctness of model predictions in compositional tasks.",
            "procedure": "Model performance is evaluated by comparing the generated outputs against the expected outputs for each test case, with accuracy calculated as the ratio of correct predictions to total predictions."
        },
        "experiments": {
            "model": "The benchmark tests six large language models from the GPT series: davinci, code-cushman-001, code-cushman-002, text-davinci-002, text-chat-davinci-002, and code-davinci-002.",
            "procedure": "Models are evaluated using a greedy search algorithm to select in-context examples for each test case, with a focus on structural similarity, diversity, and complexity.",
            "result": "The results indicate significant performance variations across models, with some models outperforming fine-tuned baselines when in-context examples are selected effectively.",
            "variability": "Variability in results is accounted for by conducting multiple trials with different sets of in-context examples and assessing the robustness of the findings."
        },
        "conclusion": "The experiments reveal that the selection of in-context examples significantly influences compositional generalization performance, highlighting the importance of structural similarity, diversity, and complexity in enhancing model capabilities.",
        "discussion": {
            "advantage": "The benchmark provides insights into effective in-context example selection, which can lead to improved model performance in compositional generalization tasks.",
            "limitation": "Despite its contributions, the benchmark is limited by its reliance on synthetic data and the high computational costs associated with testing large language models.",
            "future work": "Future research could explore more naturalistic datasets and further investigate the hidden structures behind expressions to enhance the understanding of in-context learning."
        },
        "other info": [
            {
                "info1": "COFE is publicly available for further research and exploration."
            },
            {
                "info2": {
                    "info2.1": "The benchmark aims to facilitate advancements in understanding in-context learning.",
                    "info2.2": "The authors emphasize the need for continued exploration of factors affecting compositional generalization."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Compositional generalization is a fundamental aspect of human reasoning, allowing the understanding and production of novel expressions by recombining known components."
        },
        {
            "section number": "1.2",
            "key information": "The AI community has primarily focused on compositional generalization through fine-tuning neural networks, but there remains uncertainty about the effectiveness of in-context learning on compositional generalization."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark COFE is designed to investigate in-context compositional generalization to understand how in-context examples influence model performance in generating unseen combinations of known primitives."
        },
        {
            "section number": "3.2",
            "key information": "The creation of COFE stems from the observation that the performance of compositional generalization is significantly affected by the selection of in-context examples."
        },
        {
            "section number": "3.3",
            "key information": "COFE introduces a structured approach to selecting in-context examples based on three key factors: similarity, diversity, and complexity."
        },
        {
            "section number": "5.1",
            "key information": "The dataset COFE consists of 4,785 test cases that cover various combinations and structures necessary for evaluating in-context compositional generalization."
        },
        {
            "section number": "6.2",
            "key information": "The benchmark is limited by its reliance on synthetic data and the high computational costs associated with testing large language models."
        }
    ],
    "similarity_score": 0.7028810418998086,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/How Do In-Context Examples Affect Compositional Generalization_.json"
}