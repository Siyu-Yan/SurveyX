{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2303.07971",
    "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction",
    "abstract": "Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformer LMs can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LMs in a miniature setup, in-context learning emerges when scaling parameters and data, and LMs perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input\u2019s compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models. Large language models (LLMs), trained only on next-word prediction, can perform n s by completing a prompt consisting of example demonstrations, without any param ating [Brown et al., 2020]. This ability, termed in-context learning (ICL), is emergent in se that it arises without specialized training data or objectives, simply by scaling mo  computation [Wei et al., 2022a]. This phenomenon has recently been the focus of m arch, but theoretical understanding is limited. Aiming to build theoretical understand nt work has studied in-context learning in miniaturized controlled settings, investiga w transformers could learn to solve simple classification or regression tasks in context  l., 2022, Aky\u00fcrek et al., 2022, Garg et al., 2022, Chan et al., 2022, von Oswald et al., 2 et al., 2022]. However, existing studies do not take into account the highly compositi ure of language data, modeling the pretraining data either in terms of an unstructured HMMs [Xie et al., 2022], or as consisting of prompts formatted analogously to the test ta h setups make it hard to account for a lot of the remarkable flexibility that real-world L w: They can perform broad ranges of in-context tasks with varying prompt formats,  y can be prompted to provide intermediate steps leading to an answer, often dramatic roving performance [e.g. Wei et al., 2022b, Nye et al., 2021, Wang et al., 2022, Suzgun e 2]. The emergence of such behavior remains largely mysterious. We argue that these abilities can arise through recombination of compositional struc nd in linguistic data, which we formalize in terms of grammar formalisms long stud",
    "bib_name": "hahn2023theoryemergentincontextlearning",
    "md_text": "# A Theory of Emergent In-Context Learning as Implicit Structure Induction\nMichael Hahn Saarland University mhahn@lst.uni-saarland.de Navin Goyal Microsoft Research India navingo@microsoft.com\nMichael Hahn Saarland University\nmhahn@lst.uni-saarland.de\n# Abstract\nScaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformer LMs can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LMs in a miniature setup, in-context learning emerges when scaling parameters and data, and LMs perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input\u2019s compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models. Large language models (LLMs), trained only on next-word prediction, can perform n s by completing a prompt consisting of example demonstrations, without any param ating [Brown et al., 2020]. This ability, termed in-context learning (ICL), is emergent in se that it arises without specialized training data or objectives, simply by scaling mo  computation [Wei et al., 2022a]. This phenomenon has recently been the focus of m arch, but theoretical understanding is limited. Aiming to build theoretical understand nt work has studied in-context learning in miniaturized controlled settings, investiga w transformers could learn to solve simple classification or regression tasks in context  l., 2022, Aky\u00fcrek et al., 2022, Garg et al., 2022, Chan et al., 2022, von Oswald et al., 2 et al., 2022]. However, existing studies do not take into account the highly compositi ure of language data, modeling the pretraining data either in terms of an unstructured HMMs [Xie et al., 2022], or as consisting of prompts formatted analogously to the test ta h setups make it hard to account for a lot of the remarkable flexibility that real-world L w: They can perform broad ranges of in-context tasks with varying prompt formats,  y can be prompted to provide intermediate steps leading to an answer, often dramatic roving performance [e.g. Wei et al., 2022b, Nye et al., 2021, Wang et al., 2022, Suzgun e 2]. The emergence of such behavior remains largely mysterious. We argue that these abilities can arise through recombination of compositional struc nd in linguistic data, which we formalize in terms of grammar formalisms long stud\nScaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformer LMs can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LMs in a miniature setup, in-context learning emerges when scaling parameters and data, and LMs perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input\u2019s compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.\nLarge language models (LLMs), trained only on next-word prediction, can perform novel tasks by completing a prompt consisting of example demonstrations, without any parameter updating [Brown et al., 2020]. This ability, termed in-context learning (ICL), is emergent in the sense that it arises without specialized training data or objectives, simply by scaling models and computation [Wei et al., 2022a]. This phenomenon has recently been the focus of much research, but theoretical understanding is limited. Aiming to build theoretical understanding, recent work has studied in-context learning in miniaturized controlled settings, investigating how transformers could learn to solve simple classification or regression tasks in context [Xie et al., 2022, Aky\u00fcrek et al., 2022, Garg et al., 2022, Chan et al., 2022, von Oswald et al., 2022, Dai et al., 2022]. However, existing studies do not take into account the highly compositional nature of language data, modeling the pretraining data either in terms of an unstructured set of HMMs [Xie et al., 2022], or as consisting of prompts formatted analogously to the test tasks Such setups make it hard to account for a lot of the remarkable flexibility that real-world LLMs show: They can perform broad ranges of in-context tasks with varying prompt formats, and they can be prompted to provide intermediate steps leading to an answer, often dramatically improving performance [e.g. Wei et al., 2022b, Nye et al., 2021, Wang et al., 2022, Suzgun et al., 2022]. The emergence of such behavior remains largely mysterious. We argue that these abilities can arise through recombination of compositional structure found in linguistic data, which we formalize in terms of grammar formalisms long studied\nin the linguistic literature. We first investigate when an idealized predictor performing nexttoken prediction can perform in-context learning from demonstrations. Theorem 1 describes how broad ICL skills arise when the pretraining distribution contains a sufficient amount of compositional structure. Based on this result, we introduce a novel controlled scenario in which in-context learning from demonstrations emerges from next-token prediction. We define a suite of few-shot test tasks, defined in first-order logic relative to a logical world model, and evaluate language models (LMs) trained on training datasets with varying amounts of diverse compositional structure. Unlike training datasets closely mirroring constrained scenarios proposed in previous work, text generated by compositional processes leads to broad ICL capabilities. While pretraining cross-entropy decreases continuously, a wide variety of tasks emerge suddenly after varying amounts of pretraining. Our theory also explains why prompting LLMs to provide intermediate steps makes ICL more effective (Theorem 2). We probe the LM\u2019s inner workings and argue that representation learning supports the ICL ability. Taken together, our key contributions are 1. a theoretical analysis of the conditions under which generic next-token prediction leads to in-context learning from demonstrations in an idealized predictive model, 2. a controlled setup for studying in-context learning, in which in-context learning skills emerge for a broad set of tasks, including prompting LMs for providing intermediate steps.\nTheory\nCOMP.\nLLM\nimproves with prompt length\n\u2713\n\u2713\n\u2713\ngets harder with D[\u03c4\u03c6]\n\u2713\n\u2713\n\u27131\nCHAINOFTHOUGHT > Raw\n\u2713\n\u2713\n\u27132\nCHAINOFTHOUGHT > EXPLANATION\n\u2713\n\u2713\n\u27132\ngets harder with |F|\n\u2713\n\u2713\n?\ndoes not get harder with |\u2126|\n\u2713\n\u2713\n?\nrecombining skills never seen together\nn.a.\n\u27133\n?\nworks for natural & unnatural prompts\n\u2713\nn.a.\n\u2713\nTable 1: Schematic properties of ICL as predicted by our theory, as exhibited by real transformers trained on the COMPOSITIONAL data, and observed for real-world LLMs. Evidence for the first group of properties comes from all settings. A signature prediction of our theory is that ICL success depends on the complexity D[\u03c4\u03c6] of a task\u2019s compositional description. We further derive a benefit of prompting LMs for intermediate steps before the answer (chain-of-thought prompting). Properties in the second group are hard to establish for real-world LLMs, but can be cleanly studied in our controlled setup. The theory predicts scaling with the number of functions |F| and objects |\u2126|; we further experimentally observe recombination of skills never seen together in finite training data. Robustness to unnatural prompt formats (third group) is shared by our theory and real-world LLMs; it does not apply to our miniaturized training data which has no notion of \u201cnaturalness\u201d.\n# 1 A Formal Learnability Bound for Learning from Demonstrations\nIn order to understand when broad ICL capabilities emerge from generic next-token prediction, we take the perspective of idealized learners with access to infinite training data and infinite\n1Figure 10 provides evidence from InstructGPT. 2 [e.g. Wei et al., 2022b, Lampinen et al., 2022]; Figure 10 3Figure 6\ncapacity to fit the data distribution. We show that very general linguistically-motivated assump tions about the generative process underlying the pretraining data are sufficient to guarantee ICL capabilities for an idealized language model performing ordinary next-token prediction.\n# 1.1 Setup\nWorld Model. Both the pretraining data and the few-shot tasks are generated on the basis of some finite universe \u2126of objects. The pretraining corpus consists of a collection of finite strings d \u2208\u03a3\u2217, referred to as documents; \u03a3 is a finite set serving as the alphabet. A \u201cspellout\u201d map \u03c9 \ufffd\u2192\u03c9 maps objects \u03c9 \u2208\u2126to their names \u03c9 \u2208\u03a3.\nFormalizing compositional document generation. We start by theoretically analyzing when ICL is possible for a predictor reflecting a linguistically-plausible generative process. Over the past decades, the linguistics literature has proposed a substantial number of grammar formalisms intended to describe the compositional structure and distribution of sentences and text [e.g. Pollard, 1984, Joshi, 1985, Abney, 1996, Stabler, 1996, Steedman, 2001, Kallmeyer, 2010b]. Rather than committing to any individual one of them, we eclectically condense key aspects into a simple formalism and analyze that one. Our results then transfer to other formalisms in the literature; see Appendix H. The intuitive scope of our formalism is described in Figure 1A\u2013B: Text is produced from an inventory of building blocks through composition operations [e.g. Chomsky, 1957, Goldberg, 2006], and properties and operations can be recombined and applied to different objects [e.g. Montague, 1973, Marcus, 1998] (instantiated by attributes named x, y in Figure 1A). Our formalization of linguistically-plausible compositional generative processes, Compositional Attribute Grammar (CAG), consists of two components: (1) a probabilistic context-free grammar (PCFG) probabilistically generatomg derivation trees over finite sets of terminals and nonterminals by applying a finite set of production rules, (2) a yield operation Y recursively mapping trees to strings. As outlined below, to sample a random string from the CAG, we first generate a derivation tree from the PCFG and then apply the yield operation on it. We discuss the relation between this definition and the landscape of grammar formalisms, and how our theoretical results transfer to those, in Appendix H. CAGs go beyond PCFGs in (i) conditioning string generation on attributes passed across subtrees (such as the entities \u201cFrance\u201d or \u201c95 m2\u201d in Figure 1A), and (ii) allowing operations other than simple concatenation of strings derived by subtrees (Figure 1A.4, 6). In a derivation tree, each node is associated with a list of attributes taking values in \u2126; its length given by the arity an \u2208N of the node\u2019s nonterminal n. The yield operation Y recursively maps trees to strings. It takes into account a tree, attributes from \u2126, and a source r of randomness (Figure 1B):\nwhere \u03c4 \u2208T (the set of all derivation trees), xi \u2208\u2126. For a tree consisting of only a terminal, (1) is arbitrarily defined. For a tree with children, the yield is defined recursively: The yield of \u03c8[\u03c41, ..., \u03c4\u2113]\u2014i.e., the tree rooted by a production rule \u03c8 with children trees \u03c41, ..., \u03c4\u2113\u2208T \u2014is some arbitrary concatenation of yields of children trees, Y(\u03c4i, \u03b7j, rj), where each \u03b7j is a tuple of attributes and rj are independent. Children may appear multiple times with different attributes; their ordering, multiplicity, and attributes may depend on \u03c8, r, and the attributes x1, . . . , xan of the parent (formal definition in Appendix F.1.2). Besides grammatical knowledge that is typically the focus in linguistic work on grammar formalisms, Y must also incorporate world knowledge that shapes the sentence distribution: In Figure 1B (top), Y as applied to the node labeled y=capital(x) is responsible for passing the attribute y satisfying y = capital(x) to a\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f983/f9831245-a24f-4bf1-9a8b-07c58747b25b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/349f/349f525e-e321-4cad-857e-565089a985d8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Unemployment reached 7.3% in California, 6.8% in New Mexico, 5.9% in Michigan, 5.6% in Mississippi, [...]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1bb/d1bb37f9-793c-4d34-9ac5-55e1f69cc2d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f20/0f200383-b09a-41be-b079-313edaa2189a.png\" style=\"width: 50%;\"></div>\nFigure 1: (A) Natural language text is generated by a compositional process. Following linguistic research, we assume that each document can be described in terms of a compositional description generated from a formal grammar. We highlight some examples of operations that are re-used and re-composed across documents and applied to different objects. (B) We formalize this generative processes in terms of a probabilistic grammar combining operations into derivation trees paired with a yield operation expressing these into strings. Going beyond PCFGs, variables can be shared across subtrees (top), and subtrees can be iterated (bottom). (C) When faced with a prompt, recombining compositional operations found in the training corpus provides a parsimonious explanation of the input (here, naming countries with their capitals), allowing an optimal predictor to correctly infer the response. Noncompositional explanations (bottom) are not parsimonious. Other compositional explanations (such as alternatingly naming largest cities and capitals, top right) can also explain the input, but are disfavored because they are less parsimonious. (D) This perspective extends to chained reasoning: the most parsimonious explanation of the prompt is in terms of a generative process combining reasoning steps. We illustrate this at the example of numerical reasoning (unit conversion), which can be solved by recombining two functions observed in the training data. In the chain-of-thought version, the intermediate step is made explicit. (E) Theorem 1 guarantees convergence of errors to zero, as prompt length increases, when the compositional process can express n-fold repetition (as in (B) bottom). For a composite task as in (D), Theorem 2 provides individual error bounds for the two steps in chain-of-thought prompting (D right), providing faster convergence than for direct prompting (D left).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7a60/7a609192-58f5-407a-b73a-d34e9c8c5df6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ded6/ded60dbc-6e39-41f1-a311-b5c4efab3749.png\" style=\"width: 50%;\"></div>\nsubtree; in Figure 1B (bottom), it is\u2014when applied to the loop node\u2014responsible for repeating a subtree applied to different US states. Each document d in the corpus is generated by sampling a tree \u03c4 \u2208T whose root nonterminal is a designated start symbol START, with arity 0, and a random r and setting d := Y(\u03c4, \u27e8\u27e9, r) \u2208\u03a3\u2217. We write p(d) for the resulting distribution on \u03a3\u2217. We refer to the number of nodes in a derivation tree \u03c4 \u2208T as its description length D[\u03c4]. For some constant \u03c1 > 0, P(\u03c4 | START) \u2265exp(\u2212\u03c1 \u00b7 D[\u03c4]) (Lemma 5).\nRegularity Assumptions. We make general regularity assumptions about the CAG, conceptually similar to those made in the HMM model of Xie et al. [2022]: The set of derivation trees is closed under projection onto variable values, concatenation of yields (as in a standard CFG), and marginalizing of variables. Documents have finite expected length, and all nonterminals can be used in generating some documents at probability bounded away from zero. See Appendix F.2 for formal definition. These assumptions ensure that all strings in \u03a3\u2217can be constructed at some (albeit small) nonzero probability, so that an idealized predictor makes well-defined next-token predictions on any input.\nIteration Complexity. Key to our learning bound will be the ability of CAGs to generate repetition of the same operation applied to different objects. Natural language has various such operations (Figure 1A), including lists (Figure 1A.4) or the gapping construction (Figure 1A.64); Ross [1970]), the latter highly prominent in linguistic research and thought to elude context-free syntax [Steedman, 1990]. Not all CAGs will have loop-like operations as in Figure 1A, but they may have other compositional means of generating structures repeating an operation on different attributes. We formalize this by associating to each CAG its Iteration Complexity Rn: For each n \u2264|\u2126|, let Rn be the smallest number such that the following holds for all \u03b8 \u2208T , and all pairwise distinct x1, ..., xn \u2208\u2126. We consider all trees \u03c4 \u2208T (a\u03c4 = a\u03b8 + 1) such that for all \u03be \u2208\u2126a\u03c4, Y(\u03c4, \u03be, r) with probability at least p\u03c4 > 0 has an infix whose distribution matches\nhere is always at least one such tree (Lemma 6). We define Rn by the requirement that, for at\nThere is always at least one such tree (Lemma 6). We define Rn by the requirement that, fo least one of these \u03c4,  1 \ufffd \ufffd\u2126 \ufffd\ufffd\nIntuitively, Rn indicates how much more complex repetition is compared to a single occurrence; the third term accounts for the number of different choices of x1, . . . , xn; it disappears in the simple case where the yield of \u03c4 contains (2) for each sequence x1, . . . , xn at equal probabilities p\u03c4 = (|\u2126| n ) \u22121. A simple way of achieving Rn = 1 uses a production rule \u03c8 mapping a nonterminal to a single nonterminal, and a corresponding yield Y(\u03c8[\u03b8], \u27e8\u27e9, r) of the form Y(\u03b8, \u27e8x1\u27e9, r1) . . . Y(\u03b8, \u27e8xn\u27e9, rn), with the permutation determined by r, as in Figure 1B bottom.5\n# 1.2 Learnability Bound\nWe now provide in-context learning guarantees for an idealized predictor reflecting the distribution of documents sampled from a CAG. This autoregressive predictive distribution, over\n4A linguistically faithful analysis of gapping is slightly more complex than in Figure 1A.6, see more in Appendix H.3. 5More generally, if the number n of iterations produced by this nonterminal depends on r, with some probability distribution p(n), then Rn \u22641 \u22121 \u03c1 log \u2211\u221e k=n p(k) (Appendix, Example 4).\n(3)\nM(xn|x1...n\u22121) = \u2211d\u2208\u03a3\u2217p(d) \u00b7 #d(x1...n) \u2211d\u2208\u03a3\u2217p(d) \u00b7 #d(x1...n\u22121)\nwhere #d(x1...n) is the number of times x1...n appears in $d$ (with $ serving as beginning and end of sequence token). For longer strings, M(xn...n+\u2206|x1...n\u22121) := \u220fn+\u2206 i=n M(xi|x1...i\u22121). Our learning bound is in terms of the description length of defining a function within the CAG. Formally, we say that a function \u03c6 : \u2126\u2192\u2126\u2217is expressed by a derivation tree \u03c4\u03c6 with description length D[\u03c4\u03c6] if:\nY(\u03c4\u03c6, \u27e8x\u27e9, r) \u2261\u03c6(x), \u2200r, \u2200x \u2208\u2126\nFor instance, \u201ccapital\u201d or unit conversion are expressed by subtrees of description length 2  the examples in Figure 1A\u2013B. With these notions in place, we state our first theorem:\nTheorem 1 (Single-Step Prompting). Let any CAG be given, satisfying the regularity assumptions, including the associated trees T , yield map Y, and predictive distribution M, with the associated quantities Rn. Let \u03c6 : \u2126\u2192\u2126d be a function expressed by a derivation tree \u03c4\u03c6 \u2208T . Let \u03be := x1, x2, ..., xn \u2208\u2126(n \u2264|\u2126|) be a sequence with the xi pairwise distinct, and let s \u2208\u03a3. For m = 1, . . . , n, consider the prompt Pm given by\narg max\u03c9\u2208\u03a3d M(\u03c9s|Pm),\nwith ties broken arbitrarily. On average across the choice of the sequence x1, x2, ..., xn (picked uniformly at random from length-n sequences with pairwise-distinct entries), the summed zero-one loss on completing\nwith ties broken arbitrarily. On average across the choice of the sequence x1, x2, ..., xn (picked uniformly at random from length-n sequences with pairwise-distinct entries), the summed zero-one loss on completing P1, ..., Pn, is bounded by \ufffd \ufffd\n\ufffd \ufffd where O(\u00b7) absorbs constants depending on the PCFG, s, and the average document length E[|d|], bu not otherwise on |\u2126|, \u03c6, or n. We provide the proof in Appendix F.4.\n\ufffd \ufffd where O(\u00b7) absorbs constants depending on the PCFG, s, and the average document length E[|d|], bu not otherwise on |\u2126|, \u03c6, or n.\nWe provide the proof in Appendix F.4.\nRemarks. Dependence of (8) on Rn cannot in general be avoided (Appendix F.6). The bound (8) is information-theoretic in nature, considering the idealized predictor (4). We take up the empirical behavior of real transformers pretrained on finite data in Section 2. Equation 8 absorbs constants that depend on the PCFG backbone, but not on |\u2126|. Thus, while the bound might end up vacuous when |\u2126| (and thus the maximum prompt length) is small, it will always become nonvacuous when taking |\u2126| to infinity while fixing the PCFG. Various variants and extensions can be proven with the same approach. We assumed that the response has a fixed, known length d, because we did not make any assumptions about the separator. An analogous theorem holds if the length of the response is unknown a priori, but the separator s does not occur in any \u03c6(x). Even the length of x may be taken as flexible if their set is prefix-free. The bound is robust to changes in the prompt format, such as adding symbols between x and \u03c6(x). The function \u03c6 can also be taken as stochastic; in this case, a regret bound\n6This is defined for all x1...n except when x1...n\u22121 \u2208\u03a3+$\u03a3\u2217, which can never be followed by any symbol inside a document.\n6This is defined for all x1...n except when x1...n\u22121 \u2208\u03a3+$\u03a3\u2217, which can neve document.\n(4)\n(5)\n(7)\n(8)\ncomparing to an oracle predictor that knows the task from the start holds, with an analogous proof (Appendix F.5). An analogous statement further holds for functions \u03c6 with multiple arguments, though an adapted definition of Rn is then needed; we include such functions in our experiments (Section 2.3).\nProof Intuition. The intuition of the proof is that an optimal predictive model M implicitly identifies the generative process \u03c4 \u2208T underlying the prompt, in order to predict the next token (Figure 1C). One possibility is that the prompt was generated in some unstructured manner as a random concatenation of symbols (Figure 1C bottom); another possibility is that the recurrence of pairs (x, \u03c6(x)) throughout the string is no coincidence, and that a generative process generating a prompt-like structure underlies it (Figure 1C center). If M was trained on an unstructured corpus, there is no reason to prefer the second hypothesis: appearance of structure is likely to be a coincidence under the corpus-generating process, and there is no reason to extrapolate it to future tokens. On the other hand, when the pretraining data was generated by a compositional process (as in Figure 1A), the most parsimonious explanation of the very peculiar format of the prompt Pn is as structured repetition of a single operation, leading M to predict \u03c6(xm+1). The key quantities modulating the preference for the second explanation are D[\u03c4\u03c6] and Rn: the smaller these are, the greater the advantage in parsimony of a structured explanation, and the more strongly M will predict the pattern to continue, i.e., predict \u03c6(xm+1) as the next token. A more complex \u03c6, as in Figure 1D (left) may take more examples, but will nonetheless ultimately be learned: every prediction error provides some information about the function \u03c6; the number of errors is thus bounded by the complexity of the structure underlying the prompt.\nmappings (\u201cvolleyball: animal \\n onions: sport \\n broccoli: sport...\u201d) [Rong, 2021, Wei et al., 2023]. Indeed, the proof of Theorem 1 provides more or less favorable bounds for more or less natural prompts: the bound in Equation 8 is derived by bounding the cross-entropy that M incurs on predicting all tokens in the prompt Pn+1. Higher probability of natural examples compared to less natural or even unnaturally permuted ones propagates to increased probability assigned by M to the entire prompt, and thereby faster convergence of ICL. The theory thus predicts correctly that naturalistic prompts are more successful than unnatural ones, but simultaneously that sufficiently strong predictive models can ultimately override semantic priors favoring natural completions [Wei et al., 2023]. The relation of the error bound (8) to the cross-entropy on the prompt also explains the empirical observation that prompts assigned higher LLM likelihood tend to lead to better ICL results [Gonen et al., 2022].\n# 1.3 Chain-of-Thought Prompting\nEmpirical research has observed that ICL for complex tasks benefits when models are prompted to provide intermediate steps before the answer [e.g. Nye et al., 2021, Wei et al., 2022b, Suzgun et al., 2022, chain of thought prompting]. We formally study this in the simple context of computing composed functions \u03c61 \u25e6\u03c62. Here, chain-of-thought prompting conceptually corresponds to prompting the model to output both an intermediate step \u03c61(xn) and the result \u03c62(\u03c61(xn)) (Figure 1D). Applying Theorem 1 to either direct prompting or a version with the intermediate step results in a bound depending on D[\u03c4\u03c61\u25e6\u03c62]. We now show a better bound for the chain-ofthought version, where the intermediate step is provided before the answer: the error in each of the two steps can be bounded individually by the description of only one function. While one cannot, without further assumptions, expect a bound that holds pointwise for each pair \u03c61, \u03c62, we prove a bound that holds pointwise on the component of interest and on-average on the other component:\nTheorem 2 (Chain-of-Thought Prompting). Let any CAG be given, satisfying the regularity assumptions, including the associated trees T , yield map Y, and predictive distribution M, with the associated quantities Rn. Let \u03c61 : \u2126\u2192\u2126be a function expressed by a derivation tree \u03c4\u03c61 \u2208T . Let \u03c62 : \u2126\u2192\u2126. Let s \u2208\u03a3. Consider the prompt\nwith expected completion \u03c61(xm+1), or\nP(2) m = x1\u03c62(x1)\u03c61(\u03c62(x1))s . . . xm\u03c62(xm)\u03c61(\u03c62(xm))sxm+1\u03c62(xm+1)\nwith expected completion \u03c61(\u03c62(xm+1)). On average across arbitrary functions \u03c62 and across pairwise distinct sequences x1, . . . , xn \u2208\u2126, and summed over m = 1, . . . , n, the zero-one-error on each of the two prompts is bounded by\nwith constants depending on the PCFG, s, and E[|d|], but not \u03c61, |\u2126|, or n.\nWe prove this in Appendix G. The proof idea is that in each step, the other function can be effectively ignored in inferring the compositional process. While we focus on the composition of two functions, an analogous statement and proof hold for longer composition chains. In the worst case, the two components could make errors on disjoint sets of inputs, so that the errors would add up, giving the same asymptotics as without chain-of-thought prompting. But in the more realistic situation where both errors are high for short prompts and then\n(9)\n(10)\n(11)\ngo to zero once the tasks are identified, the overall error will just be the larger one of the two component tasks\u2019 errors (Figure 1E right). This indeed is close to what happens in our experiments (Figure 9). This theoretical benefit is not available when providing the intermediate step after the solution, as an explanation rather than a chain-of-thought. In this version, the first step amounts to solving the composed task in one go, leading to an error bound only in terms of D[\u03c4\u03c61\u25e6\u03c62]. Indeed, in real-world LLMs, providing intermediate steps before the answer seems to be much more effective than providing it after the answer [Wei et al., 2022b, Lampinen et al., 2022].\n# 1.4 Comparison to Xie et al. [2022]\nWe discuss how this theoretical analysis of ICL relates to and differs from the analysis proposed by Xie et al. [2022]. They model the pretraining data as a mixture of HMMs, and cast ICL as Bayesian identification of one of these mixture components (an analogous idea is sketched by Wang et al. [2023]). Each HMM mixture component is thought to describe some type of text, e.g., Wikipedia biographies, newspaper articles, or tweets. A prompt (e.g., Albert Einstein was German \\n Mahatma Gandhi was Indian \\n Marie Curie was) is then identified as a concatenation of text samples resembling some of these components (e.g., biographies). Our analysis likewise can be understood in terms of Bayesian inference, in that M implicitly identifies a generative process \u03c4 underlying the prompt. The most important difference between our analysis and that of Xie et al. [2022] is that we aim to account for the flexible and openended nature of prompting capabilities in LLMs by leveraging the compositional nature of natural-language data: Whereas Xie et al. [2022] analyzed the task of recovering one of a fixed space of HMMs which made up the training corpus, we explain ICL as identifying a task from an open-ended hypothesis space of tasks compositionally recombining operations found in the training corpus. Whereas Xie et al. [2022] focused their discussion of ICL on entity-property associations (e.g., nationalities), our approach makes it possible to study ICL on tasks of varying complexity and structure within a single framework (Figure 1D), including variants such as chain-of-thought prompting (Section 1.3). The theorems in Xie et al. [2022] study general recoverability of HMM mixture components from a single sample in the presence of repeated low-probability transitions (caused by the prompt structure), with few assumptions about the HMM. However, the application of these theorems to explaining ICL of classification tasks (e.g. mapping famous people to their nationalities) relies on an important additional assumption, which closely relates to Iteration Complexity: HMM transitions happen within each mixture component (e.g., Wikipedia articles about people), but not between different mixture components (e.g., articles about people; articles about cities; tweets). This encodes an implicit modeling assumption that similar text about different entities (e.g., biographies of different people) tends to appear contiguously in the pretraining corpus. This assumption is a specific form of the more general idea that the generative process underlying natural language can produce repetition of the same operation applied to different entities, formalized by Iteration Complexity Rn. Similar assumptions implicitly underlie other work aiming to empirically induce ICL in controlled setups by training on prompt-like inputs [Garg et al., 2022, Chan et al., 2022, Aky\u00fcrek et al., 2022]. We will experimentally compare such training data with CAG-based pretraining data.\na\nb\nc\nd\ne\nf\ng\nFunction f1\na\nb\nc\nd\ne\nf\ng\nFunction f2\n...\n...\nUniverse \u03a9\nloop over x do\n  for some y such that f1(x)=y\n      if f2(y)=x then\n         print x\n      else\n         print y\n      print x\n   endfor\nendfor\nggdfbbeebcddaa\nejgbkxjdfdkajdfhzjdfhv\nsjdkghakjdvnxkjbhakdjh\ngsgbhndfjbnfjbgkfjbhf\n...\na, b, c, d, e, ...\nFunctions f1, f2, f3, f4, ...\nWorld Model\ngenerate\nDocument Scripts\ngfafdabeaeaacbadgaad\nFVPrompt Dataset\nHMM Datasets\nf1\nf1\nf1\nf1\nx\nf\nf(x)\ng g g f f b b b b e c c \nf1f2f1f1f2f1f2f2f1f1f1f2\nf f d b e c c c a b b f \ngfwfdwbeweawcbwdgwad\nf2\nf2\nf2\nf2\nCompositional Dataset\nA\nB\nC\nD\nseparators\nemits\nFigure 2: Setup and datasets. (A) We assume a logical model M consisting of a universe \u2126 and a set of functions, visualized here as directed graphs. (B\u2013D) We consider three types of generative processes for the training dataset used in pretraining. (B) The FVPROMPT dataset directly encodes the functions in a prompt-based format. (C) The HMM datasets, adapting GINC [Xie et al., 2022], are generated by HMMs whose state space consists of object-function pairs. (D) In the COMPOSITIONAL dataset, documents are generated by scripts that have access to the world model and contain arbitrary but compositionally structured instructions; they correspond to a very simple CAG.\n# 2 Experiments\n# 2.1 Training Datasets\nWe have described an information-theoretic analysis characterizing when broad ICL capabilities become possible for an idealized predictive model reflecting a linguistically motivated string distribution. We now empirically verify whether the predicted behavior can emerge in transformer models pretrained on finite data sampled from a CAG. We define a suite of in-context learning tasks, and benchmark transformers pretrained on several types of controlled miniature datasets: some modeled closely on prior work, and one representing a minimal CAG.\nWorld Models. All training datasets are based on a world model M consisting of a finite universe \u2126and a set F of functions f : \u2126\u2192\u2126. We focus on functions of arity 1 for simplicity, and will logically define more complex functions in terms of these. In creating documents, we make the simple assumption that \u2126= \u03a3 and \u03c9 = \u03c9.\nFunction Value Prompts. Our first dataset (FVPROMPT, Figure 2B) has a prompt-based format: pairs of x and f (x), for functions f \u2208F (fixed within a document), with an intervening separator randomly chosen (fixed within a document) from \u2126. Each prompt includes all x \u2208\u2126 in random order. Analogous training datasets have been used in prior work experimentally inducing ICL in constrained setups [Chan et al., 2022, Garg et al., 2022, Aky\u00fcrek et al., 2022, Li et al., 2023].\nHMM5 and HMMPERDOC. Our next dataset (HMM5, Figure 2C) closely follows the GINC dataset proposed by Xie et al. [2022]: the dataset is generated by a mixture of five HMMs; each HMM state has the form \u27e8x, f \u27e9\u2208\u2126\u00d7 F and emits f (x); the two components evolve independently according to separate transition matrices. The transition matrices are defined as in Xie et al. [2022]; we provide these in Appendix J for reference. This dataset has more diversity than FVPROMPT, but still less than the unbounded state space arising from a PCFGbased compositional system. We thus considered a variant, HMMPERDOC, where separate permutation matrices were sampled for each document, and no mixing or averaging was applied (Appendix J). By assuming a fixed finite state space, the generative processes underlying HMM5 and HMMPERDOC cannot express unbounded composition. However, due to the specific factorized\ndesign, they incorporate a bias towards repeating the same operation (in this case, sequences of evaluations fi(x)) on different objects; it can be viewed as a simple instantiation of loop operations. Indeed, this modeling choice is key to Xie et al. [2022]\u2019s argument for this as a model of ICL, whereby ICL arises because similar text about different entities (e.g., Wikipedia articles about different people) tends to appear contiguously in the pretraining corpus. Compositional Document Scripts. Finally, we define a minimal CAG (COMPOSITIONAL, Figure 2D), including minimal language features needed for our theoretical analysis: a loop construct, a construct introducing new attributes standing in functional relationships to existing attributes\u2014so that functions f \u2208F can be defined, a terminal that outputs the value of an attribute, and a conditional construct (\u201cif-then-else\u201d). We ablate each component below. This can be intuitively described as a minimalistic programming language; we\u2019ll refer to the derivation trees as document scripts and write them as programs (Figure 22). Attributes correspond to variables in a script. Loops are executed for 10 random objects \u03c9 \u2208\u2126. The \u201cfor some\u201d statement selects a random satisfying object if more than one exists, and is skipped if none exist. The syntax tree of a script corresponds to the derivation tree \u03c4, and the stochastic map from scripts to output strings corresponds to the yield function Y in a CAG (see Appendix I). Due to the \u201cfor all\u201d construct, Rn = 1 for n \u226410. In order to sample scripts for document generation, we defined a PCFG-like distribution over syntactically valid scripts (corresponding to the PCFG backbone of CAGs), favoring scripts generating documents within our LM\u2019s context length (64). See Appendix I for details, We provide examples in Appendix B. Intuitively, this generative process represents agents that have access to the world model and produce text according to arbitrary but compositionally structured instructions. Unlike natural language, the documents do not share systematic generalizations such as vocabulary or grammar, nor do they have function words indicating structural relations between words. They make up for this lack of structure by containing increased amounts of repetition within a document. We discuss the relation to and differences from natural language in Section 3. Our research questions are: 1. Does ICL appear when training real-world transformers on finite data generated from a minimal CAG? How do models trained on CAG data compare to models trained on the other datasets? 2. Are the predictions of Theorems 1\u20132 borne out, i.e., effect of description length, dependence on |F|, independence from |\u2126|, advantage of chain-of-thought prompting? 3. In ICL, can transformers recombine operations never seen together during pretraining? 4. What are the dynamics of emergence?\ndesign, they incorporate a bias towards repeating the same operation (in this case, sequences of evaluations fi(x)) on different objects; it can be viewed as a simple instantiation of loop operations. Indeed, this modeling choice is key to Xie et al. [2022]\u2019s argument for this as a model of ICL, whereby ICL arises because similar text about different entities (e.g., Wikipedia articles about different people) tends to appear contiguously in the pretraining corpus.\n# 2.2 Training Setup\nModels. We train GPT2-like [Radford et al., 2019] models for next-token prediction using the HuggingFace transformers library [Wolf et al., 2019]. Varying the numbers of dimensions, layers, and heads (Table 2), we considered models with 14M (small, 2 layers), 21M (medium, 3 layers), 42M (large, 6 layers), and 85M (XL, 12 layers) parameters. See Appendix C for training details.\nd\nHeads\nLayers\nParameters\nSmall\n64\n2\n2\n14M\nMedium\n128\n2\n3\n21M\nLarge\n256\n8\n6\n42M\nXL\n768\n12\n12\n85M\n<div style=\"text-align: center;\">Table 2: Model Sizes. The XL model has the same architecture as GPT2-Small, but a sma vocabulary.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a87/4a872f88-b38f-4e16-a1f6-fcafded0ce0e.png\" style=\"width: 50%;\"></div>\nFigure 3: Test tasks. (A) The FUNCTIONEVALUATION task prompts the model to apply a function to a given element. (B) PROPOSITIONAL tasks generalize this by asking the model to find a (unique) z satisfying a relation \u03d5(x, y, z), where x, y are the inputs. (C) BINARY tasks prompt the model to decide which of two funcional relationships holds between a pair of objects. (D) The COMPOSITION Task prompts the model to compute the composition of two functions. (E) In the CHAIN-OF-THOUGHT version, the model is prompted to also produce an intermediate step. Across (A\u2013E), the models need to identify the task from the prompt without any further training or instruction. Note that there are no separate types reserved for labels or separators, and the models have no access to markup indicating the components of the prompt. Similar to large-scale LMs, the models need to figure out the structures of different prompts on-the-fly. Worlds. We focus on |\u2126| = 30 and |F| = 10, and additionally varied |F| = 5, 10, 20, 30 and |\u2126| = 30, 100, 300. Functions were created randomly; f1 was the identity function. Furthermore, we designated functions f2, f3 such that no script included both f2 and f3: we did this in order to test whether models could generalize to contexts simultaneously requiring knowledge of both functions, which cannot be generated by scripts in the training distribution. For each world M = \u27e8\u2126, F\u27e9, we generated 500M tokens of training data, for each of the four data-generating procedures. In the main setup, we additionally created five more worlds and trained medium-size models on compositional data to verify robustness to sampling of the world (Appendix D). Documents were concatenated, separated by a STARTOFSEQUENCE symbol. Data was fed to the model in portions of 64 tokens, across document boundaries. Training was performed for up to 20 epochs, or until held-out cross-entropy stopped improving, which happened earlier for noncompositional datasets.\nlarge-scale LMs, the models need to figure out the structures of different prompts on-the-fly. Worlds. We focus on |\u2126| = 30 and |F| = 10, and additionally varied |F| = 5, 10, 20, 30 and |\u2126| = 30, 100, 300. Functions were created randomly; f1 was the identity function. Furthermore, we designated functions f2, f3 such that no script included both f2 and f3: we did this in order to test whether models could generalize to contexts simultaneously requiring knowledge of both functions, which cannot be generated by scripts in the training distribution. For each world M = \u27e8\u2126, F\u27e9, we generated 500M tokens of training data, for each of the four data-generating procedures. In the main setup, we additionally created five more worlds and trained medium-size models on compositional data to verify robustness to sampling of the world (Appendix D). Documents were concatenated, separated by a STARTOFSEQUENCE symbol. Data was fed to the model in portions of 64 tokens, across document boundaries. Training was performed for up to 20 epochs, or until held-out cross-entropy stopped improving, which happened earlier for noncompositional datasets.\n# 2.3 Test Tasks\nIn order to evaluate ICL capabilities, we collected a suite of test tasks that are definable relative to the world M = (\u2126, F) (Figure 3). Tasks are defined using first-order logic formulas \u03d5 with literals of the form f (X) = Y (f \u2208F), with free variables matching the inputs x \u2208\u2126(or x, y \u2208 \u2126) and the expected output z. Given inputs x \u2208\u2126(or x, y), the expected output \u03c6(x) (or \u03c6(x, y)) is the z satisfying the formula. All prompts are chosen so that z is uniquely determined for all included (x, y), including the examples. The simplest task is the FUNCTIONEVALUATION task (Figure 3A): given x \u2208\u2126, compute \u03c6(x) := fi(x)\u2014i.e., the z such that \u03d5(x, y, z) \u2261( fi(x) = z) holds.\n# Prompt Format. We encoded all test tasks into a prompt-based format for evaluating ICL Across tasks, examples are separated by a random (but fixed within a prompt) separator s \u2208\u2126 When there are two input variables, each prompt is of the form\nPrompt Format. We encoded all test tasks into a prompt-based format for evaluating ICL Across tasks, examples are separated by a random (but fixed within a prompt) separator s \u2208\u2126 When there are two input variables, each prompt is of the form\nx1y1z1sx2y2z2s . . . xkykzksxk+1yk+1\nwhere each tuple (xi, yi, zi) satisfies the formula \u03d5(xi, yi, zi) defining the task. When there is on input variable, each yi is omitted.\nPropositional. Our next set of tasks is defined by first-order formulas without quantifiers (see Appendix A for list). Besides the function evaluation task, its INVERSE (given x \u2208\u2126, output some z such that f (z) = x) is also definable using one literal. We next constructed more complex formulas with two input variables x, y. Each formula was in DNF, such that each term had 2 literals. We choose 2 literals because this allows encoding the functional relationships between the three variables x, y, z. The number of literals in \u03d5 provides a proxy for the description length D[\u03c4\u03c6] in the minimal CAG (Appendix B.4). For instance, one task (\u201cmissing link\u201d, Figure 3B, #2 in Appendix A) assumes that either x = fi( fj(y)) or y = fi( fj(x)); z then is the intermediate element (either fj(y) or fj(x)); defined by \u03d5(x, y, z) \u2261( fj(x) = z \u2227fi(z) = y) \u2228( fj(y) = z \u2227fi(z) = x).\nComposed. We furthermore considered a set of tasks that require reasoning about an unobserved variable, or, equivalently, require evaluating composed functions in one go (see Appendix A for list). One example (COMPOSITION, Figure 3C) is defined as \u03d5(x, z) \u2261\u2203a : a = fi(x) \u2227z = fj(a); here, z = fj( fi(x)).\nBinary Classification. Beyond multi-class classification, LLMs can solve novel classification and reasoning tasks. To test for the emergence of such abilities, we created tasks that require discriminating between two types of examples and output a binary label \u21131, \u21132 \u2208\u2126(fixed within a prompt). For instance, the RELATIONCLASSIFICATION Task (Figure 3D) asks the model to decide whether y = fi(x) (z = \u21131) or y = fj(x) (z = \u21132), assuming exactly one of these is true. Experimental Details. All functions fi, fj, ... are varied across prompts, but fixed within a prompt. We first exclude the designated functions f2, f3, and later evaluate performance on them separately. We also excluded the identity function f1. Each input x or (x, y) appears at most once within a prompt. Inputs where the answer is either ambiguous or undefined (e.g., in the INVERSE task, if z is the image of zero or two elements under fi) were excluded. We evaluate on prompts with an even number of examples, ranging from 2 to 14; this exhausts the LM\u2019s context size for some tasks. In the binary classification tasks, the number of examples was balanced between the classes. In the PROPOSITIONAL or COMPOSED tasks involving disjunction, all disjuncts were represented equally, up to a difference of at most one to make up for non-divisibility. Importantly, there are no separate types reserved for labels or separators, and the model has no access to markup indicating the components of the prompt: like real-world LLMs, models are asked to figure out the structures of different prompts on-the-fly.\n# 2.4 Results\nCompositional training dataset enables ICL on composed tasks. Figure 4 shows results across tasks for the four training datasets, as a function of the training steps (# of processed\n(12)\ntokens), for the models with 85M parameters. As expected, models trained on HMM5 cannot solve the ICL tasks, and models trained on FVPROMPT can only solve the function evaluation task. Models trained on HMMPERDOC achieve above-chance performance on the PROPOSITIONAL tasks, though not on the BINARY or COMPOSED tasks. For models trained on COMPOSITIONAL near-perfect accuracy is achieved on FUNCTIONEVALUATION and other tasks with few literals, but not on BINARY tasks for which above-chance accuracy is achieved. BINARY tasks and tasks with many literals are the most difficult. Focusing on models trained on COMPOSITIONAL, we next investigated how accuracy scales with various parameters, focusing on the diverse PROPOSITIONAL tasks. Scaling with prompt length. Theorem 1 provides a bound on the summed errors across prompt lengths, guaranteeing faster convergence for functions with small description length. Figure 5A shows that accuracy increases with prompt length. In agreement with the theorem, longer prompts tend to be necessary for tasks with more literals (i.e., higher description length). Increasing |F| makes ICL harder, increasing |\u2126| does not. Another prediction of Theorem 1 concerns the size of the world model. Recall that Equation 8 depends on the PCFG, Rn, s, and D[\u03c4\u03c6]. As each f \u2208F has its own production (Figure 22), dependence on the PCFG is less favorable when |F| increases even if D[\u03c4\u03c6] stays the same. On the other hand, in our experimental setup, none of these parameters change with |\u2126|. We thus expect that increasing |F| should make tasks harder, but increasing \u2126should not.8 This was borne out when re-fitting at |F| = 20, 30 (Figure 5B), and at |\u2126| = 100, 300 (Figure 5C). Indeed, ICL accuracy improved on PROPOSITIONAL (no improvement on other tasks, see Figure 25) when increasing |\u2126|; most remarkably, tasks with eight literals are now learnt at almost perfect accuracies at the same prompt length (see Appendix K for explanation). Emergence with data and model size. We next evaluated the role of model size (Figure 5E). Two observations are salient. First, increasing model size leads to smaller gains on tasks with few literals (accuracy was already high for these tasks), and large gains on tasks with many literals. Second, Figure 5 show that accuracy follows a pattern of sudden emergence over the course of pretraining\u2014in particular, in those cases where very high accuracy is ultimately reached (as when |\u2126| = 300, Figure 5B): a period of flat at-chance performance precedes a sudden increase in accuracy, followed by a mostly flat phase. This stands in contrast with the evolution of pretraining cross-entropy, which decreases continuously (Figure 5D). LMs can recombine functions. Next, for each of the PROPOSITIONAL tasks with at least two different functions, we created a variant where two of the functions involved were replaced by the two functions that never appeared in a single document script (f2,f3). Solving these tasks thus requires recombining knowledge acquired from different portions of the training data. We compare accuracy on these versions with the previous results in Figure 6. The effect on the accuracies is small, with a significant drop affecting only some tasks with 8 literals. Explanations help recombine abilities, but Chain-of-thought prompting helps more. To test our predictions from Section 1.3, we compared two variants of the COMPOSITION task where the model was prompted to generate not only the answer, but also the unobserved\ntokens), for the models with 85M parameters. As expected, models trained on HMM5 cannot solve the ICL tasks, and models trained on FVPROMPT can only solve the function evaluation task. Models trained on HMMPERDOC achieve above-chance performance on the PROPOSITIONAL tasks, though not on the BINARY or COMPOSED tasks. For models trained on COMPOSITIONAL near-perfect accuracy is achieved on FUNCTIONEVALUATION and other tasks with few literals, but not on BINARY tasks for which above-chance accuracy is achieved. BINARY tasks and tasks with many literals are the most difficult. Focusing on models trained on COMPOSITIONAL, we next investigated how accuracy scales with various parameters, focusing on the diverse PROPOSITIONAL tasks.\nExplanations help recombine abilities, but Chain-of-thought prompting helps more. To test our predictions from Section 1.3, we compared two variants of the COMPOSITION task where the model was prompted to generate not only the answer, but also the unobserved variable. In the CHAINOFTHOUGHT (CoT) version, the model was prompted to output the\n8Equation 8 depends on s. As our experiments randomize s over \u2126, the PCFG is decoupled from s, removing this dependence. See Remark 2 in Appendix F.4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a082/a0823fbb-bd3e-4cab-bf55-6a97e2d832c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Literals: \u2014 1 \u2014 2 \u2014 4 \u2014 6 \u2014 8 Pretraining Steps [Millions of Tokens]</div>\nFigure 4: Compositional training data enables emergent in-context learning. For each pretraining dataset (rows) and each group of test tasks (columns), we show accuracy (y-axis) as a function of the number of tokens processed in pretraining (x-axis, in millions); one epoch corresponds to 500M tokens. Models were trained until held-out loss stopped improving but at most for 20 epochs (1010 tokens). All results at 85M parameters and the longest prompt length (14 examples). Dotted lines indicate chance accuracy. The HMM5 dataset does not lead to ICL. The FVPROMPT dataset does not lead to generalization beyond the simple function evaluation task. With HMMPERDOC and COMPOSITIONAL, above-chance performance on composed tasks becomes possible. COMPOSITIONAL achieves above-chance accuracy on all task groups.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb4e/eb4e0a7f-c9b5-4ca0-b1a1-90e5bff3029f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Literals: \u2014 1 \u2014 2 \u2014 4 \u2014 6 \u2014 8</div>\nFigure 5: Scaling of accuracy on the PROPOSITIONAL tasks. As in Figure 4, the x-axis denotes the number of tokens processed in pretraining. otherwise stated, prompt length is 14, |\u2126| = 30 |F| = 10, model size 85M parameters.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4314/43144d34-b9cf-4173-99f1-89a0791fe8c8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Literals: \u2014 1 \u2014 2 \u2014 4 \u2014 6 \u2014 8</div>\nFigure 6: Recombination: For PROPOSITIONAL tasks with at least two different functions, we show results when all function pairs appeared in overlapping sets of document scripts om pretraining (left), and when two functions appeared in disjoint sets of document scripts (right).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fd15/fd1555e8-5461-405d-88a8-43398a2095fa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Parameters: \u2014 14M \u2014 21M \u2014 42M \u2014 84M</div>\nFigure 7: Computing the composition of functions is hard, as this involves an unobserved variable (A), in particular, when it requires recombining functions fi, fj that never appeared together in training (B). Prompting the model to provide the intermediate step after the answer (EXPLANATION) or before it (CHAINOFTHOUGHT) facilitates this. CHAINOFTHOUGHT transforms the task into a sequence of two simple tasks, and enables even the smallest model to solve the task. All results at the longest prompt length (14 examples).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5098/5098f5eb-9061-4d6c-9b65-367ec20fd368.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Parameters: \u2014 14M \u2014 21M \u2014 42M \u2014 84M</div>\nFigure 8: Chain-of-thought reasoning for compositions of several functions; in the setting where fi, fj never appeared in the same training document script (B in Figure 7). For fair comparison, we show results at the largest length (8 examples) that fits into the transformer\u2019s context length for all tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0af3/0af38cb4-cd0c-4dea-8dac-08e92b59f727.png\" style=\"width: 50%;\"></div>\nFigure 9: Accuracy for function composition task by prompt length and model size, at the end of pretraining. A and B are as in Figure 7. As predicted by our theory, CHAINOFTHOUGHT achieves the best results. EXPLANATION only helps for large models (as found for real-world LLMs by Lampinen et al. [2022]), and does not work as well as CHAINOFTHOUGHT even for those.\n<div style=\"text-align: center;\">Figure 9: Accuracy for function composition task by prompt length and model size, at the end of pretraining. A and B are as in Figure 7. As predicted by our theory, CHAINOFTHOUGHT achieves the best results. EXPLANATION only helps for large models (as found for real-world LLMs by Lampinen et al. [2022]), and does not work as well as CHAINOFTHOUGHT even for those.</div>\nunobserved variable before the answer (Figure 3E). In the EXPLANATION version, the model was prompted to output the unobserved variable after the answer. In these versions, the models are prompted to produce two tokens. We generated using greedy decoding, and counted responses as correct when the correct sequence was produced. Theorem 2 provides an improved bound in CHAINOFTHOUGHT, but not in EXPLANATION. Results, by data and model size, are shown in Figure 7A. Providing an explanation leads to somewhat earlier emergence of the task, but it only benefits large models (in line with empirical findings for real-world LLMs by Lampinen et al. [2022]). In line with the theoretical prediction, gains from CHAINOFTHOUGHT are much stronger: it leads to early emergence even in the small model, not much later than the emergence of the simple function evaluation task (Figure 4). This difference between producing intermediate reasoning steps before or after the answer mirrors the behavior of real-world LLMs [Wei et al., 2022b, Lampinen et al., 2022]. We next considered composing f2, f3, which never appeared in the same script used for the pretraining corpus (Figure 7B). The raw task is now inaccessible even to the largest model. Both ways of including the intermediate step make the task accessible to the large models; CoT succeeds even on the smallest model and with short prompts (Figure 9B). This suggests that prompting the model to include the intermediate step helps it compositionally recombine abilities that were never used together in pretraining. CHAINOFTHOUGHT continues to succeed for the composition of three and four functions, where single-step prompting is too difficult for all models (Figure 8).\nAblating CAG Features. We created variants of the training data with (i) loops, (ii) variable introduction via \u201cfor some\u201d, (iii) conditions (if-then-else) ablated. See Appendix L for details. We trained medium-size LMs (21M parameters) on each variant (Figure 24). Ablating loops or variable introduction made ICL impossible; indeed, these are necessary to provide learning guarantees via Theorem 1 for any nontrivial task. On the other hand, ablating conditions had no discernible negative impact even on PROPOSITIONAL or BINARY tasks whose definitions involve disjunction.\nHeldout analysis. We additionally trained the 21M parameter LM on a pretraining set where any document containing a substring that would be a valid prompt (with at least 4 examples)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6da4/6da463f2-313c-4052-a828-2f626dc72ec9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Results from InstructGPT (text-davinci-003) on a family of synthetic string manipulation tasks: FUNCTIONEVALUATION and PROPOSITIONAL tasks (A), BINARY tasks (B), FUNCTIONCOMPOSITION (C) with direct prompting, EXPLANATION and CHAINOFTHOUGHT.</div>\nfor any test task (including chain-of-thought) was removed, affecting < 0.02% of documents.9 We considered not just the sample prompts used for evaluation, but any sequence matching the specification of any of the tasks. We considered substrings appearing in any place in a document, not just substrings following the StartOfSequence symbol. Accuracies were highly correlated between the two versions (Figure 29).\nReal-World LLMs. We investigated whether key tenets of our theory apply to a real-world LLM in the InstructGPT family (text-davinci-003), in a synthetic setup of strings over a,...,z of length 10, with length-preserving operations from Hupkes et al. [2020] as basic functions (reverse; shift; swap first and last letter). Almost none of the > 1014 strings are likely to have appeared in the training data. See Appendix O for details. Results are shown in Figure 10. For PROPOSITIONAL tasks, accuracy increases with prompt length, with earlier increases for tasks defined with fewer literals. BINARY tasks are solved successfully, better than in our other experiments. Finally, the FUNCTIONCOMPOSITION task is difficult; in line with Theorem 2 and our other experiments, CHAINOFTHOUGHT is more helpful in facilitating it than EXPLANATION.\n# 2.5 Representation learning supports ICL\nOur theoretical analysis in Section 1 argues that ICL relies on identifying the compositional generative process underlying a prompt. Here, we provide evidence from the LM\u2019s activations and attention patterns that they indeed induce the compositional structure underlying documents and prompts. We target the 21M parameters model as it has a small number of heads and layers and yet is successful on almost all tasks. We first visualize attention patterns in a chainof-thought example, focusing on the final tokens for visibility, in Figure 11A (see Appendix,\n9Of a sample of 1K excluded documents, 27% FUNCTIONEVALUATION or INVERSE, 71% COMPOSITION, 3% Task 12, 1.5% COMPOSITION with CoT or explanation; 0% other PROPOSITIONAL or BINARY tasks. We believe that most matches to the composed functions are chance matches, i.e. to functions that are simple and happen to match on some inputs. Upon closer inspection, 66% of the COMPOSITION examples were cases where z = x, which match COMPOSITION in the (a priori rare) case where fj( fi(x)) = x, so that the function collapses to the much simpler identity function.\n9Of a sample of 1K excluded documents, 27% FUNCTIONEVALUATION or INVERSE, 71% COMPOSITION, 3% Task 12, 1.5% COMPOSITION with CoT or explanation; 0% other PROPOSITIONAL or BINARY tasks. We believe that most matches to the composed functions are chance matches, i.e. to functions that are simple and happen to match on some inputs. Upon closer inspection, 66% of the COMPOSITION examples were cases where z = x, which match COMPOSITION in the (a priori rare) case where fj( fi(x)) = x, so that the function collapses to the much simpler identity function.\nFigure 17 for other tasks). The two heads in the lowest layer attend to the three preceding token to decreasing degree (1.1), or recent tokens with the same type (1.2), respectively. The two heads in the second layer attend to diffuse average of recent tokens (2.1) or sharply the immediately preceding tokens (2.2). In the third layer, one head (3.1) attends to the preceding tokens that are in structurally corresponding positions to the upcoming token\u2014e.g. the head attends to previous delimiters at the end of each prompt example. Head (3.2) is similar if a bit more diffuse. The pattern was more diffuse but analogous when removing the separator, i.e., the model does not rely on a recurring symbol to induce the structure (Appendix, Figures 15 and 17). We next analyzed attention patterns across a sample of 300 random documents in the training corpus. The attention head patterns generalized: heads in the lower layers attended in the same way as described (Figure 11B). Head (3.1) was best explained as attending to tokens that were structurally corresponding\u2014those tokens produced by executing the same line in the document script as the token now being predicted. Head (3.2) was explained best\u2014depending on random seed\u2014either by the same or by the same shifted by one. In addition to this correlational study, we next performed an interventional one: we intervened on the top-level attention heads in the trained model by masking out attention logits to non-structurally-corresponding positions. If the function of the top-layer attention heads is indeed to attend to structurally corresponding positions, this intervention should improve performance, by effectively providing oracle access to the document\u2019s structure. Indeed, the intervention consistently improved cross-entropy on the pretraining task when applied to the top layer heads, and hurt when applied to other layers (Appendix, Figure 12). Similarly, it improved accuracy on the chain-of-thought task, in particular for short prompts (Figure 11D). Towards extracting the learned algorithm. This attention pattern suggests a general algorithmic approach: first, the lower layers identify the structure of the document. The third layer then collects information from all structurally matching positions, and predicts the structurally required token based on that information. In this algorithm, there are two key tasks: first, identify the structure underlying the input in the lower layers; second, perform analogical reasoning to predict the next token in the top layer. Importantly, this algorithm equally works on simple FUNCTIONEVALUATION and chain-of-thought-prompting: once the structure has been induced, both tasks amount to predicting fi(x) for the last token x. This helps explain why chain-of-thought prompting emerges so quickly and works even in the smallest model. The remaining question is how the lower layers identify and represent document structure. We hypothesize that the model learns to encode the logical relations holding among the tokens close by, using information gained from the three heads attending to recent tokens. We can formalize the logical relations as a set of graphs with the immediately preceding tokens as vertices ({t \u2212K + 1, ...t}), and edges describing which functional relations hold. Encoding this in a set of adjacency matrices, we obtain a tensor Mi,j,k (1 \u2265i, j \u2265K, 1 \u2265k \u2265|F|) where Mi,j,k holds iff f (wt\u2212i) = wt\u2212j. We set the context size to K = 3, and fitted a set of log-linear probes to decode M from the activations in each layer (Figure 11). M can be decoded with highest accuracy from the second layer. Probe complexity as assessed using a prequential code [Voita and Titov, 2020] was \u224850% of that for a control task [Hewitt and Liang, 2019] where functions were randomized (but consistently within a probe), showing that the layer encodes nontrivial information. Interestingly, probe accuracy undergoes a rapid increase at about \u224870M tokens of training, at a similar time as the emergence of many PROPOSITIONAL tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b583/b5830821-720e-4858-8942-013f7e9ce47e.png\" style=\"width: 50%;\"></div>\nFigure 11: (A) Attention Maps: chain-of-thought prompting, 21M parameter model. Columns: heads; rows: layers, bottom layer is closer to the input. The prompt included 15 examples, we only show the last 10 tokens for clarity. Within each attention map, the y-axis indexes queries and the x-axis indexes keys. Orange background indicates the separator. (B) Correlation between attention scores and four attention pattern predictors across 300 documents. (C) Local relations among immediately preceding tokens can be summarized into a set of adjacency matrices with binary entries, one for each function fi. A log-linear probe recovers these matrices at high F1 from the intermediate layer. (D) Intervening on attention heads in the top layer, so that they can only attend to structurally-matching positions (without any other change to the trained model) improves model performance for short prompts (here, the same chain-of-thought task as in A).\n# 3 Discussion and Related Work\nQualitative Properties of ICL. We summarize schematic properties of ICL in Table 1. Properties in the first group are supported by our theory, real transformers pretrained on a minimal CAG, and real-world LLMs like GPT-3. Improvement of ICL with longer prompts is arguably predicted by all theoretical or experimental approaches to understanding ICL. On the other hand, the advantage for prompting LMs to provide intermediate steps, and effects of task complexity formalized by description length, are new predictions of our theory. The first one is by now very well established empirically [e.g. Wei et al., 2022b, Suzgun et al., 2022]; we found evidence for the second one by prompting InstructGPT on our test tasks (Figure 10). The second group in Table 1 includes properties of ICL which are easy to manipulate in controlled setups. The fact that ICL gets harder with increased |F| is intuitive. Invariance to, or even improvement with, increased |\u2126| is consistent with empirical observations about benefits of increased vocabulary size [Xie et al., 2022] and increased numbers of objects [Chan et al., 2022] We empirically also found that transformers could recombine their knowledge of functions fi, fj never seen together during training. While it is likely that emergent behavior in LLMs involves a substantial amount of recombination of abilities, this has been hard to directly verify in real-world LLMs.\nTheoretical Guarantees for In-Context Learning. We have proven information-theoretic bounds for in-context learning in an optimal predictor, under linguistically motivated assumptions on the pretraining dataset, stated in terms of a task\u2019s description length in the compositional structure underlying the pretraining data (Theorem 1). As discussed in Section 1.4 this differs from bounds by Xie et al. [2022] in guaranteeing learning in an open-ended hypothesis space recombining compositional operations found in the pretraining data; we used this to prove a benefit for prompting models to provide intermediate steps (Theorem 2). These results highlight the usefulness of considering linguistically motivated compositional generative processes in analyzing the theoretical foundations of LLMs. Our bounds apply to an optimal predictor; extending them to finite-capacity models pretrained on finite data (as in our experiments) is an interesting problem for future research. Our findings on representation learning suggests ways in which the transformer\u2019s architecture might be helpful.\nICL in Controlled Setups. Our work joins a line of recent work [Aky\u00fcrek et al., 2022, Garg et al., 2022, Chan et al., 2022, Xie et al., 2022, Li et al., 2023] studying ICL in controlled miniature setups. Garg et al. [2022], Aky\u00fcrek et al. [2022] study the in-context learning abilities of transformers by directly meta-training on prompts from a candidate function class, finding that they can be meta-trained to learn, among others, linear functions and decision trees, from example prompts. Li et al. [2023] provide generalization bounds for such a meta-training setup. Chan et al. [2022] empirically study a simple image-based multiclass classification dataset, where the training data also had a prompt-based format but the task was fixed across pretraining prompts and test trials targeted heldout classes. Conceptually closest to our work, Xie et al. [2022] cast ICL as recovery of an HMM mixture component (see detailed discussion in Section 1.4). The key innovation in our study is that we account for compositional recombination of skills into composed tasks, attributing its emergence to the compositional structure of linguistic pretraining data. This allows us to induce ICL on tasks of varying complexity, and account for the benefit of providing intermediate steps. Another line of work has studied the ability of real-world LLMs to in-context-learn synthetic or even unnatural tasks resembling natural tasks but with exchanged labels [Rong, 2021, Min et al., 2022, Wei et al., 2023]. The ability of models both to infer the prompt structure and\nunnatural input-label mappings, at least when they are sufficiently large [Wei et al., 2023], is conceptually well compatible with the idea that in-context learning relies on identifying the compositional structure underlying a prompt.\nconceptually well compatible with the idea that in-context learning relies on identifying the compositional structure underlying a prompt. Emergence and Grokking. In many cases, we observed sudden emergence of abilities when a certain threshold of training steps had been crossed, while the pretraining loss decreased continuously. This has commonalities with the grokking phenomenon observed in supervised learning [Power et al., 2022]. Grokking is thought to relate to the build-up of generalizable representations [Liu et al., 2022, Chughtai et al., 2023]; in a similar vein, we found that emergence of many tasks coincided with improvement in structural representations. Mechanisms of ICL. A recent line of work [Aky\u00fcrek et al., 2022, von Oswald et al., 2022, Dai et al., 2022] provides in-principle proofs that transformers can implement optimization algorithms in context, hypothesizing that this underlies real-world ICL, but leaving open how generic natural-language text data would give rise to such abilities. In our controlled setup, ICL relied in part on attention heads attending to structurally related positions earlier in the sequence, allowing the model to \u201ccopy\u201d behavior. This may be related to a hypothesized role of induction heads [Olsson et al., 2022], heads that copy symbols from past contexts matching a certain pattern-like description.\nEmergence and Grokking. In many cases, we observed sudden emergence of abilities when a certain threshold of training steps had been crossed, while the pretraining loss decreased continuously. This has commonalities with the grokking phenomenon observed in supervised learning [Power et al., 2022]. Grokking is thought to relate to the build-up of generalizable representations [Liu et al., 2022, Chughtai et al., 2023]; in a similar vein, we found that emergence of many tasks coincided with improvement in structural representations.\nRole of Pretraining Data in ICL. Shin et al. [2022] empirically investigate the role of pretraining corpora for ICL in real-world LLMs. Potentially relevant to our attributes on the role of recombination of compositional behavior, they found that ICL can result by combining corpora which individually do not give rise to it. Razeghi et al. [2022] studied numerical reasoning capabilities in GPT-based LLMs, finding that term frequencies in the pretraining dataset had a strong impact on ICL performance. Chan et al. [2022] empirically investigated the role of the training dataset in a simple image classification task where the pretraining data consisted of prompt-like sequences, finding that data-distributional properties such as a Zipfian distribution over classes were beneficial to ICL of held-out classes. Our setup differs in that we target an open-ended space of compositionally created test tasks and the benefit of providing intermediate steps. However, relevant to our findings, they found that varying the assignment of classes to labels in pretraining improved ICL; this may encourage a model towards flexible recombination of knowledge.\nAlgorithmic Information Theory and Minimum Description Length. Theorems 1 and 2 suggest that ICL can work because prompts are compressible into compositional generation processes. Our theoretical analysis has links to Algorithmic Information Theory [Li and Vit\u00e1nyi, 2008] and the Minimum Description Length (MDL) principle [Rissanen, 2010], and to statistical estimators based on MDL [Barron and Cover, 1991]. Informally, Theorem 1 is based on the idea that a predictor trained on compositionally generated data will show an implicit MDL-like bias. In the case where documents are generated by a Turing-complete generative process, D[\u00b7] corresponds to Kolmogorov Complexity and M corresponds to the universal prior used in Solomonoff induction ([Solomonoff, 1964]; Definition 4.5.6 in Li and Vit\u00e1nyi [2008]). In that setting, our theorem is similar to the completeness theorem for Solomonoff induction (Theorem 5.2.1 in Li and Vit\u00e1nyi [2008]). Turing-complete generative processes might be linguistically unrealistic as a model of text data; hence, we here explicitly derive ICL guarantees using a restricted and linguistically motivated class of generative processes. However, there\nis an intriguing link to proposals conceptualizing intelligence as universal prediction akin t Solomonoff induction [Hutter, 2004].\nPretraining Data and Natural Language. While our theoretical results are grounded in a long tradition of research on language, the COMPOSITIONAL pretraining dataset only aims to provide a minimal CAG, and has key differences from real language. Most prominently, document scripts repeat a lot of structure within a document, whereas natural language repeats a lot of structure across documents, which our scripts cannot model (recall that there are no grammatical or lexical conventions across documents). This limitation is shared with the existing work aiming to induce ICL in controlled setups [Garg et al., 2022, Chan et al., 2022, Xie et al., 2022, Aky\u00fcrek et al., 2022], though our experiments advance by incorporating compositional recursive structure and multi-step tasks. Importantly, our theoretical analysis in Section 1 is valid independently of such restrictions, because it holds for a very broad class of linguistically realistic generative processes, and is (up to constants) robust to extending the CAG, even extensions adversarial to ICL (Appendix F.6). Designing CAGs incorporating more realistic features of natural language is an interesting future research direction, and should enable more detailed miniaturized models of the emergence of ICL. Linguistically richer CAGs could also pave the way to accounting for the role of instructions, which boost prompt effectiveness when prepended to demonstrations [Brown et al., 2020]; arguably, they provide additional cues regarding the generative process.\nLimitations and Future Work. Our theory and experiments focus on ICL for functions applying to objects from a fixed finite universe. Real-world ICL is also successful on more complex and open-ended input, such as mapping a sentence to its sentiment, or mapping a text and a question to an answer. Formalizing this within our framework and deriving appropriate ICL bounds is an interesting problem for further research. Relatedly, the analysis of chain-of-thought prompting in Theorem 2 focuses on the composition of a fixed sequence of functions. Real LLMs may also be able to find an appropriate sequence of reasoning steps that may itself be dependent on the input, and there will typically be multiple acceptable chains of reasoning leading to a correct answer. Generalizing Theorem 2 in this respect is an intriguing problem for future research. In our theoretical guarantees, the measure of task difficulty is the description length in the compositional generative process underlying the pretraining data. Quantifying it for naturalistic real-world tasks could provide a useful measure of ICL task difficulty for real-world LLMs.\n# 4 Conclusion\nWe have provided an information-theoretic analysis of how generic next-token prediction can enable a model to learn from demonstrations in context. We show a general error bound for the in-context learning of functions, under linguistically motivated assumptions about the\ngenerative process underlying the pretraining corpus. Using this framework, we further prove a benefit for prompting models to provide intermediate steps towards an answer. To validate the theoretical analysis, we introduce a new way of inducing in-context learning in a controlled miniature setup. In this setup, we found sudden emergence of in-context learning when scaling parameters and data, recombination of abilities found in different parts of the pretraining data, and the theoretically predicted benefit of prompting for intermediate steps. Taken together, these results provide a step towards theoretical understanding of emergent properties in large language models.\n# Acknowledgments\nM.H. gratefully acknowledges Saarland University\u2019s Department of Language Science and Technology and Saarland Informatics Campus for compute resources.\n# References\n# Steven P. Abney. Stochastic attribute-value grammars. ArXiv, cmp-lg/9610003, 1996.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. CoRR, abs/2211.15661, 2022. doi: 10.48550/arXiv.2211.15661. URL https://doi.org/10.48550/arXiv.2211. 15661. Andrew R. Barron and Thomas M. Cover. Minimum complexity density estimation. IEEE Trans. Inf. Theory, 37:1034\u20131054, 1991.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What lea algorithm is in-context learning? investigations with linear models. CoRR, abs/2211. 2022. doi: 10.48550/arXiv.2211.15661. URL https://doi.org/10.48550/arXiv. 15661.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. CoRR, abs/2211.15661, 2022. doi: 10.48550/arXiv.2211.15661. URL https://doi.org/10.48550/arXiv.2211.\nAndrew R. Barron and Thomas M. Cover. Minimum complexity density estimation. IEEE Trans Inf. Theory, 37:1034\u20131054, 1991.\nDami\u00e1n E. Blasi, Ryan Cotterell, Lawrence Wolf-Sonkin, Sabine Stoll, Balthasar Bickel, and Marco Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3938\u20133943. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1384. URL https://doi.org/10.18653/v1/ p19-1384.\nPierre Boullier. Chinese numbers, mix, scrambling, and range concatenation grammars. I Conference of the European Chapter of the Association for Computational Linguistics, 1999.\nJoan Bresnan. Lexical Functional Syntax. 2000.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. CoRR, abs/2205.05055, 2022. doi: 10.48550/ arXiv.2205.05055. URL https://doi.org/10.48550/arXiv.2205.05055. Zhiyi Chi. Statistical properties of probabilistic context-free grammars. Comput. Linguistics, 25: 131\u2013160, 1999. Noam Chomsky. Syntactic structures. Mouton, The Hague, 1957. Noam Chomsky. The minimalist program. 1992. Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. CoRR, abs/2302.03025, 2023. doi: 10. 48550/arXiv.2302.03025. URL https://doi.org/10.48550/arXiv.2302.03025. Alexander Clark. Strong learning of some probabilistic multiple context-free grammars. In Mathematics of Language, 2021. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. CoRR, abs/2212.10559, 2022. doi: 10.48550/arXiv.2212.10559. URL https://doi.org/10. 48550/arXiv.2212.10559. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? A case study of simple function classes. CoRR, abs/2208.01066, 2022. doi: 10.48550/arXiv.2208.01066. URL https://doi.org/10.",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of emergent in-context learning (ICL) in large language models (LLMs), emphasizing the limited theoretical understanding of how these models learn from example demonstrations without specialized training data.",
        "problem": {
            "definition": "The problem is understanding how LLMs can perform in-context learning through next-token prediction when trained on compositional structures in natural language data.",
            "key obstacle": "The main challenge is the lack of theoretical frameworks that adequately explain the emergent capabilities of ICL in LLMs."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that LLMs can recombine compositional operations found in natural language data to perform tasks.",
            "opinion": "The authors propose that ICL arises from the generative process of language models predicting the next token based on compositional structures.",
            "innovation": "The main difference compared to previous methods is the introduction of an information-theoretic bound that links ICL to the compositional structure of the pretraining data."
        },
        "Theory": {
            "perspective": "The theory is grounded in linguistically motivated assumptions about the generative processes underlying the pretraining data, formalized through compositional attribute grammars.",
            "opinion": "The authors assume that the ability to learn in context is tied to the structural properties of the language data used in pretraining.",
            "proof": "The paper provides a formal proof demonstrating how broad ICL capabilities emerge from next-token prediction when the pretraining distribution contains sufficient compositional structure."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized a controlled setup with datasets generated from compositional attribute grammars, comparing the performance of transformer models across various tasks.",
            "evaluation method": "The evaluation involved testing LLMs on a suite of tasks designed to assess their ICL capabilities, particularly focusing on how well they could perform with varying prompt lengths and configurations."
        },
        "conclusion": "The experiments confirmed that ICL capabilities emerge when scaling parameters and data, highlighting the importance of prompting models to provide intermediate steps for improved performance.",
        "discussion": {
            "advantage": "The advantages of this paper include a novel theoretical framework that provides insights into the emergent behavior of LLMs and the validation of this theory through controlled experiments.",
            "limitation": "A limitation is that the theoretical analysis is based on idealized models, which may not fully capture the complexities of real-world LLMs.",
            "future work": "Future work could explore extending the theoretical framework to more complex tasks and datasets, incorporating more realistic features of natural language."
        },
        "other info": [
            {
                "info1": "The study emphasizes the role of representation learning in supporting ICL."
            },
            {
                "info2": {
                    "info2.1": "The paper discusses the implications of the findings for understanding emergent properties in LLMs.",
                    "info2.2": "It suggests that further research is needed to explore the generalization of these results to more complex real-world tasks."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of emergent in-context learning (ICL) in large language models (LLMs), emphasizing the limited theoretical understanding of how these models learn from example demonstrations without specialized training data."
        },
        {
            "section number": "1.3",
            "key information": "The authors propose that ICL arises from the generative process of language models predicting the next token based on compositional structures."
        },
        {
            "section number": "2",
            "key information": "The problem is understanding how LLMs can perform in-context learning through next-token prediction when trained on compositional structures in natural language data."
        },
        {
            "section number": "3.2",
            "key information": "The theory is grounded in linguistically motivated assumptions about the generative processes underlying the pretraining data, formalized through compositional attribute grammars."
        },
        {
            "section number": "3.4",
            "key information": "The experiments confirmed that ICL capabilities emerge when scaling parameters and data, highlighting the importance of prompting models to provide intermediate steps for improved performance."
        },
        {
            "section number": "6.1",
            "key information": "A limitation is that the theoretical analysis is based on idealized models, which may not fully capture the complexities of real-world LLMs."
        },
        {
            "section number": "6.4",
            "key information": "Future work could explore extending the theoretical framework to more complex tasks and datasets, incorporating more realistic features of natural language."
        }
    ],
    "similarity_score": 0.755091210074326,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/A Theory of Emergent In-Context Learning as Implicit Structure Induction.json"
}