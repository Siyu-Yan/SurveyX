{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2110.07814",
    "title": "Meta-learning via Language Model In-context Tuning",
    "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose in-context tuning (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to metatrain the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks. We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x. 1",
    "bib_name": "chen2022metalearninglanguagemodelincontext",
    "md_text": "Yanda Chen1\u2217 Ruiqi Zhong2 Sheng Zha3 George Karypis3 He H 1Columbia University, 2University of California, Berkeley, 3AWS AI 4New York University\nyc3384@columbia.edu, ruiqi-zhong@berkeley.edu, {zhasheng, gkarypis, hehea}@amazon.com\n# Abstract\nThe goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose in-context tuning (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to metatrain the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks. We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x. 1\n# 1 Introduction\nFew-shot learning (FSL) refers to a system\u2019s ability to quickly adapt to new tasks when very few labeled examples are available for training. FSL is a key feature of human learning (Lake et al., 2016), but current machine learning systems often rely on large amounts of labeled training data (Silver et al., 2016; He et al., 2016; Adiwardana et al., 2020). Recently, prompting large pre-trained language models (LMs) for FSL has achieved remarkable\nprogress (Brown et al., 2020; Schick and Sch\u00fctze, 2021a). LM prompting with in-context learning reduces the \u201ctask learning and predict\u201d process to a simple sequence prediction problem. To perform a new task, Brown et al. (2020) prompt a raw LM (i.e., a pre-trained LM not fine-tuned on any labeled data) with the concatenation of the task instruction, some input-output examples, and the target input to be predicted on; then they extract the answer from the LM\u2019s continuation of the concatenated sequence (Figure 1 left). For example, to coax the model into performing sentiment classification on the target input \u201cThis movie is a waste of time\u201d, we prompt the LM with the sequence \u201cI like the movie! Positive review? Yes. Horrible Movie! Positive review? No. This movie is a waste of time. Positive review? ___\u201d, and predict \u201cpositive\u201d if the next word is more likely to be \u201cYes\u201d rather than \u201cNo\u201d. However, raw LMs are not optimized for incontext FSL during pre-training, and exhibit undesirable behavior when used for FSL. For example, Zhao et al. (2021) observed that LMs suffer from the \u201crecency bias\u201d, which assigns higher probability to labels that appear closer to the target input. As a result, the accuracy becomes extremely sensitive to the ordering of the in-context examples. Previous work has also shown that prompting raw LMs is often oversensitive to example choices and instruction wording (Schick and Sch\u00fctze, 2021a; Jiang et al., 2020; Gao et al., 2021; Liu et al., 2021). We address this weakness through a metalearning lens and directly fine-tune the LM for FSL. Under the meta-learning framework, we metatrain a model to learn to adapt to new tasks from a few examples on a wide range of tasks, so that it learns to leverage the few-shot examples to adapt to new tasks at test time. Since LM prompting already reduces the \u201ctask learning and predict\u201d process to a simple sequence prediction problem, we meta-train a LM by directly fine-tuning it to optimize for this sequence prediction problem on a\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7035/7035cea5-3ba5-48ba-9ca4-e5fe4a3760e4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Few-shot Adaptation via Gradient Descent</div>\nFew-shot Adaptation via  In-context Learning\nFigure 1: MAML (right): MAML aims to learn a task-agnostic model initialization \u03b8 that can adapt fast to new tasks. To adapt the model initialization to a new task \u02dcT, a task-specific model \u03b8\u2032 initialized with \u03b8 is updated with gradient descent using task examples from \u02dcT. Meta-training of MAML involves bi-level optimization, where the inner optimization learns a task-specific model \u03b8\u2032 using task examples from \u02dcT, and the outer optimization learns a meta-initialization \u03b8 to minimize few-shot prediction loss of \u03b8\u2032 on task \u02dcT. In-context Tuning (ours) (left): our approach adapts to new tasks via in-context learning, and learns a single model \u03b8 shared across all tasks that is directly optimized with the FSL objective (Section 2.2). Because model parameters are frozen during task adaptation, our approach does not involve bi-level optimization during meta-training.\nwide range of tasks (Figure 1 left). Since we finetune our model to learn in-context learning, we call our approach in-context tuning (ICT). Unlike optimization-based meta learning approaches such as MAML (Finn et al., 2017), in-context tuning adapts to new tasks through in-context learning where model parameters are frozen, thus it avoids the challenging nested optimization problem in MAML (Figure 1). We benchmark our algorithm on LAMA (Petroni et al., 2019), a dataset for testing models\u2019 factual knowledge, and BinaryClfs (Zhong et al., 2021), a wide range of binary classification tasks each annotated with a few language descriptions of the task. Compared to prompting raw LMs, in-context tuning improves performance by 7.6 Precision@1 points on LAMA and 10.6% AUC-ROC score on BinaryClfs. In addition, in-context tuning mitigates the over-sensitivity of raw LM prompting, significantly reducing the variance of the performance with respect to example ordering (by 68% on LAMA and 83% on BinaryClfs), example choices (by 56% on LAMA and 40% on BinaryClfs), and instruction wording (by 19% on LAMA). Our approach also out-performs MAML, which adapts the model by gradient descent on a few examples and learns an initialization that can adapt to a new task through a few gradient steps (Finn et al., 2017; Nichol et al., 2018). Since our approach better takes advantage of the inductive bias of LMs to extrapolate from in-context examples, our approach out-performs first-order MAML by\n<div style=\"text-align: center;\">Meta-Update: Optimize     to minimize the loss. \u03b8</div>\n2.8 points on LAMA and 5.1 points on BinaryClfs, with increasing advantage as models become larger. Given the empirical effectiveness of in-context tuning (Section 4.1), we conjecture that the fewshot learning potential of large LMs (e.g., GPT-3) may be broadly underestimated if prompted without any direct optimization for FSL. We also conjecture that in-context tuning can mitigate various undesirable properties of LM prompting, such as over-sensitivity to example ordering, example choices, and instruction wording (Section 4.2).\n# 2 Approach\nWe introduce the problem setup (Section 2.1), describe our in-context tuning algorithm (Section 2.2), compare our algorithm to gradient-based adaptation methods (Section 2.3) and other baselines (Section 2.4).\n# 2.1 Problem Setup\nWe focus on the few-shot classification problem, where the model first learns from a set of training tasks T \u2208Ttrain, each associated with its natural language instructions IT and a large amount of task input-output examples DT = {(xi T , yi T )} (see Figure 1 left for examples). At test time, we ask the model to learn a new task \u02dcT given its instruction and only a few (K) labeled examples, i.e. S \u02dcT \u2286 D \u02dcT , |S \u02dcT | = K. We denote the task input to be predicted at test time as xtarget \u02dcT . Note that \u201ctask input\u201d is different from \u201cmodel input\u201d. For example, on the left panel of Figure 1,\nthe task input is \u201cGood movie!\u201d while the model input can be a concatenation of the instruction, task inputs and task outputs.\n# 2.2 In-context Tuning Algorithm\nIn-context tuning directly optimizes pre-trained LMs with the few-shot in-context learning objective (Brown et al., 2020): task-agnostic LMs are meta-trained to perform few-shot in-context learning on a wide variety of training tasks. Similar to in-context learning, LMs trained with in-context tuning adapt to a new task by using few-shot training examples as the input prefix. Formally, during meta-training, we build the model input by concatenating the task instruction IT , task input-output pairs ST \u2286DT , and the task input xtarget T 2 to be classified. We then fine-tune a pre-trained LM to predict ytarget T and hope that the model learns to use the in-context examples ST . Here is the few-shot in-context tuning objective L:\n(1) (2)\nTo adapt to a new task \u02dcT at test time, we directly concatenate the few-shot examples S \u02dcT with the instruction I \u02dcT and the target task input xtarget \u02dcT to be classified to form the model input, and ask the model to predict its corresponding output. No gradient update is performed during adaptation.\nWe compare in-context tuning with two classical few-shot learning methods: multi-task fine-tuning (instruction tuning + fine-tuning) and MAML. Both methods adapt the model parameters to new tasks by gradient descent on few-shot examples.\ntasks and \u25e6represents the concatenation operation. During the few-shot adaptation phase, the model is presented with a new task \u02dcT, its natural language instruction I \u02dcT and a small set of (K) task inputoutput examples S \u02dcT = {(xi \u02dcT , yi \u02dcT )|i \u2208[K]}. We then fine-tune the model to predict the task output yi \u02dcT from the new task given I \u02dcT \u25e6xi \u02dcT and update \u03b8 with a few gradient steps to get \u03b8 \u02dcT . Finally, we use the updated model \u03b8 \u02dcT to predict the output from the task input xtarget \u02dcT and the instruction I \u02dcT under the test task \u02dcT.\nMAML The few-shot adaptation stage of MAML is the same as instruction tuning + finetuning, where we update the model parameters (initialized with \u03b8) by gradient descent on K examples S \u02dcT \u2286D \u02dcT . However, during meta-training, MAML aims to learn a task-agnostic model initialization \u03b8 such that, \u03b8T , which is to be found by initializing with \u03b8 and performing gradient descent on ST , would lead to good performance (Finn et al., 2017). Therefore, MAML involves two levels of optimization, an inner optimization to learn \u03b8T given \u03b8 and ST \u2286DT , and an outer optimization to learn \u03b8 given \u03b8T . Due to the bi-level structure in this optimization problem, MAML has been found to be empirically unstable, sensitive to hyperparameters, and computationally expensive (Finn et al., 2017; Nikolaev et al., 2020). Even worse, few-shot task adaptation is known to be highly sensitive to optimization hyperparameters (Antoniou et al., 2019), while a large labeled validation set for hyperparameter tuning may not be available under a FSL setting (Perez et al., 2021). In comparison, in-context tuning simplifies the two-stage process of (1) few-shot task adaptation and (2) task-specific prediction as one sequence prediction problem, where task-specific examples are concatenated to the model input to provide information about the task. Hence, in-context tuning removes the bi-level optimization during metatraining, which can be empirically unstable and expensive. Additionally, since model weights are frozen during task adaptation, it is not sensitive to adaptation hyperparameters.\n# 2.4 Other Baselines\nRaw In-context Learning (Raw IC-L) We directly evaluate a raw LM on a new task using the same evaluation set-up for in-context tuning, without fine-tuning the LM on any labeled data.\n<div style=\"text-align: center;\">Adaptation Meta-train</div>\nMethod\nAdaptation\nMeta-train\nIn-context Tuning\nIn-context\nFew-shot\nMAML\nGradient\nFew-shot\nInsT\nNone\nZero-shot\nInsT + FT\nGradient\nZero-shot\nRaw IC-L\nIn-context\nLM\nTable 1: We categorize our approach and the baselines according to 1) how the few-shot examples (if any) are used for adaptation, and 2) the meta-training objective. Ins-T refers to instruction tuning.\nInstruction Tuning (InsT) The model learns to predict the target output only based on the instruction and the target input. Only the instruction is available during the adaptation phase, and this setup is also known as zero-shot learning. We categorize all approaches in our paper based on their meta-training objective and how they use task-specific examples in Table 1. In-context tuning is the only method that directly optimizes the FSL objective without gradient-based adaptation.\n# 3 Experimental Setup\n# 3.1 Datasets and Metrics\nWe experiment with two meta-datasets that contain a wide range of tasks, LAMA and BinaryClfs. Each task is associated with several different natural language descriptions, and we call them instructions for convenience, even though some of them are realized as questions.\nLAMA LAnguage Model Analysis (Petroni et al., 2019) is a dataset that tests the factual and commonsense knowledge learned by LMs. In our experiments, we use the TREx-UHN portion of LAMA (Poerner et al., 2020), which consists of (subject, relation, object) triples from Wikidata. LAMA is an entity prediction task, where a model is asked to predict the object entity given the subject entity and the relation. In our experiments, we treat one relation as a task as in Perez et al. (2021). Initial experiments on LAMA showed that LMs take significant advantage of \u201cmajority label bias\u201d (Zhao et al., 2021), where they assign higher probability to object entities that have appeared in the in-context examples, thus inflating the accuracy. To reflect the improvement due to few-shot learning rather than this simple heuristic to copy answers, for all tasks we prune the LAMA dataset so that all object entities appear less than 2.5% of times. Our\nfinal filtered LAMA dataset consists of 29 relations (tasks) and 12k (subject, relation, object) examples. We use task instructions from two datasets: LAMA and LPAQA (Jiang et al., 2020). LAMA contains one task instruction for each task, and the auxiliary LPAQA dataset contains on average 10 additional instructions for each LAMA task. We use the same evaluation protocol as in Petroni et al. (2019): 1) the object entity is predicted from a pre-defined vocabulary set of 21k words (each LAMA task is 21k-way classification); 2) we compute mean precision at one (P@1) for each task, and report the average across tasks. Because LAMA does not have an official trainvalidation-test split, we use 8-fold cross-validation in our experiments. We randomly partition the 29 tasks into 8 groups of similar sizes. For each cross-validation split, we use six groups for training, one group for validation, and one group for testing. The test sets of the eight folds are disjoint and their union is the set of all tasks. BinaryClfs This dataset contains a wide range of binary classification tasks, and each task can be described by 1-4 \u201cyes/no\" questions, which we concatenate to the input context as instructions. There are in total 204 different tasks, and 73 of them are used for testing, which include sentiment classification, topic classification, definition detection, stance classification, etc. We use the same evaluation protocol as in Zhong et al. (2021): 1) we group the tasks by similarity and do not allow training tasks to be similar to testing tasks; 2) we treat \u201cYes\u201d answer as the positive class and calculate the AUC-ROC score for each instruction of each task. To fit model inputs (concatenation of in-context examples and task input to classify) within the maximum context length (1024) of our LMs, we leave out five evaluation tasks where the maximum task input length exceeds 230 BPE tokens. We also leave out the spam classification task due to its small test set. BinaryClfs does not come with an official validation set. To perform hyperparameter tuning, for each testing group, we randomly sample another testing group as its validation group.\n# 3.2 Implementation Details\nArchitecture We use BERT models for LAMA (BERT-Base [110M parameters], BERT-Large [340M] and DeBERTa-XLarge-V2 [900M]) and GPT2 models for BinaryClfs (GPT2-Medium [345M] and GPT2-Large [774M]). We use the Hug-\n<div style=\"text-align: center;\">LAMA</div>\nLAMA\nBinaryClfs\nBERT-Base\nBERT-Large\nDeBERTa-xlarge\nGPT2-M\nGPT2-L\n0-S\n1-S\n2-S\n5-S\n0-S\n1-S\n2-S\n5-S\n0-S\n1-S\n2-S\n5-S\n0-S\n5-S\n0-S\n5-S\nRaw IC-L\n10.3\n8.5\n10.8\n14.1\n12.7\n12.1\n15.4\n18.6\n11.2\n12.6\n20.6\n23.7\n50.5\n57.8\n51.0\n58.3\nInsT + FT\n/\n17.5\n18.6\n20.0\n/\n21.6\n22.6\n23.9\n/\n24.7\n25.6\n27.0\n/\n67.0\n/\n69.4\nICT\n14.6\n16.3\n17.6\n19.6\n18.0\n21.6\n23.4\n24.3\n21.9\n26.0\n27.5\n28.8\n62.9\n67.4\n66.3\n69.8\nRaw IC-L w/o Ins\n1.5\n4.9\n8.7\n12.3\n1.4\n3.5\n7.0\n12.5\n2.7\n13.0\n19.5\n22.6\n/\n/\n/\n/\nICT w/o Ins\n7.1\n14.6\n17.0\n18.2\n9.3\n19.4\n19.9\n22.9\n10.6\n23.5\n26.0\n27.6\n/\n/\n/\n/\nTable 2: Few-shot learning accuracy of our in-context tuning approach (ICT) compared to in-context learning with raw LMs (Raw IC-L) and instruction tuning + fine-tuning (InsT + FT). K-S: K-shot learning. GPT2-M: GPT2-Medium. GPT2-L: GPT2-Large. Task instructions are used except the last two rows labeled with \u201cw/o Ins\u201d. By definition, InsT + FT is the same as ICT for 0-S. We only experiment with the no-instruction setting on the LAMA dataset. Since we modify the LAMA dataset and BinaryClfs dataset (Section 3.1), the numbers reported in our work are not directly comparable to other work.\nLAMA\nBinaryClfs\nBB\nBL\nGPT2-M\nGPT2-L\nMAML\n16.9\n21.4\n63.3\n63.9\nICT\n19.6\n24.3\n67.4\n69.8\nTable 3: In-context tuning consistently out-performs MAML on both datasets and all model sizes under the 5-shot setting. BB: BERT-Base. BL: BERT-Large. GPT2-M: GPT2-Medium. GPT2-L: GPT2-Large.\n# gingface implementation (Wolf et al., 2020).\ngingface implementation (Wolf et al., 2020).\nHyperparameters We select hyperparameters based on few-shot classification accuracy on validation tasks. Our validation tasks and testing tasks are disjoint, so hyperparameter tuning on validation tasks does not use extra labeled examples on the testing tasks (Perez et al., 2021). See Appendix A for the hyperparameters we tuned.\nSampling Different instructions and few-shot example choices can lead to different predictions (Section 2.2). At training time, we expose the model to diverse task instructions and few-shot choices by randomly sampling task instructions and few-shot examples for each target example. At test time, we report the average accuracy across task instructions and few-shot choices. Since computing the average across all few-shot choices is intractable (there are combinatorically many distinct few-shot choices), we thus calculate the average accuracy of multiple random samplings of few-shot choices as approximation.\n<div style=\"text-align: center;\">BinaryClfs</div>\n# 4 Results\nIn-context tuning out-performs MAML and various baselines on the two text classification metadatasets (Section 4.1). It also significantly reduces model sensitivity to instruction wording, example choices, and example ordering compared to prompting raw LMs (Section 4.2).\n# 4.1 Few-shot Learning Performance\nIn-context tuning improves in-context learning accuracy over raw LMs. We compare ICT with Raw IC-L in Table 2. In-context tuning consistently out-performs raw LM prompting by 7.6 points on LAMA and 10.6 points on BinaryClfs (averaged across model size and number of few-shots). As expected, directly optimizing the few-shot in-context learning objective (Section 2.2) improves the fewshot in-context learning accuracy.\n# Few-shot examples lead to more effective task\nadaptation. We compare few-shot in-context tuning with instruction tuning (equivalent to 0shot ICT) in Table 2. Few-shot in-context tuning consistently out-performs instruction tuning on both LAMA and BinaryClfs, with increasing performance gains as number of shots increases. Specifically, we observe that 5-shot in-context tuning out-performs instruction tuning by 6.1 points on LAMA and 4.0 points on BinaryClfs. Results show that demonstration examples besides task instructions facilitate more effective task adaptation.\nIn-context tuning better leverages the inductive bias for pattern matching. By comparing MAML (the first row of Table 3) to instruction\ntuning (equivalent to 0-shot ICT) of Table 2, we see that MAML out-performs instruction tuning in most evaluation settings, which indicates that MAML is indeed able to take advantage of the few-shot task examples for task adaptation. However, Table 3 shows that our approach of 5-shot in-context tuning out-performs 5-shot MAML consistently on both datasets with an accuracy gain of 2.8 points on LAMA and 5.1 points on BinaryClfs (averaged across model size). We argue that in-context tuning out-performs MAML because in-context tuning better leverages the existing inductive bias of pre-trained LMs to perform pattern matching with in-context examples. We also compare in-context tuning to the pipeline of instruction tuning + task-specific finetuning (Table 2). Surprisingly, fine-tuning an instruction-tuned model on as few as one taskspecific example significantly improves task accuracy, without over-fitting to the few labeled examples. We observe that instruction tuning + 1-shot fine-tuning out-performs instruction tuning (equivalent to 0-shot ICT) by 3.1 points on LAMA (Table 2). Our in-context tuning approach performs comparable or better than instruction tuning + finetuning, with increasing accuracy gains as models get bigger (Table 2). For DeBERTa-XLarge-v2 (the largest models we use in this work), in-context tuning out-performs InsT + FT across all numbers of shots, achieving an accuracy gain of 1.7 points on LAMA (averaged across all numbers of shots). We conjecture that in-context tuning will be increasingly effective for bigger models that have a stronger inductive bias of pattern matching.\n# In-context tuning reduces the need of task in-\nstructions. As coming up with good task instructions can be hard (Schick and Sch\u00fctze, 2021a; Jiang et al., 2020), we further investigate the effectiveness of in-context tuning without task instructions (Table 2). In-context tuning is effective in the no-instruction setting as well, consistently out-performing raw in-context learning with no instructions by an average margin of 9.5 points on LAMA. Comparing raw in-context learning with (Raw IC-L) and without instructions (Raw IC-L w/o Ins) (Table 2), we observe that task instructions yield the most significant performance gains when model size is relatively small (+2.5 points on BERT-Base, +7.7 points on BERT-Large, only +0.6 points on DeBERTa-xlarge). We conjecture that smaller models may be weaker at inferring patterns\nLAMA\nBinaryClfs\nBB\nBL\nGPT2-M\nGPT2-L\nRaw IC-L\n1.82\n2.14\n9.26\n8.84\nICT\n0.66\n0.61\n1.41\n1.58\nTable 4: In-context tuning is significantly less sensitive to example ordering compared to in-context learning with raw LMs.\nfrom in-context examples alone compared to larger models, which is why instructions yield larger performance gains on smaller models. On BERT-Base and BERT-Large models where task instructions are most helpful, in-context tuning reduces the improvement gain from task instructions from 5.1 points (raw in-context learning) to 1.8 points (averaged across BERT-Base and BERT-Large), which indicates that in-context tuning reduces the need of task instructions compared to raw in-context learning. However, we note that instructions still yield performance improvement even if in-context tuning is applied.\n# 4.2 Sensitivity Analysis\nWe analyze the sensitivity of in-context tuning accuracy with respect to example ordering, example choices, and instruction wording, and compare it with prompting raw LMs. Let I denote a random selection of task instruction, ST a random unordered set of few-shot training examples with size K, \u03c3 a random permutation of K examples. The accuracy \u00b5 is a function of these three random variables, i.e. \u00b5 : (ST , \u03c3, I) \ufffd\u2192[0, 1]. We can decompose the total variance of \u00b5 into its variance w.r.t. each of the three random variables, since they are independent (order variance is independent to choice variance because ST is unordered):\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b780/b780e2b7-9ce4-466e-9ad5-1c08a99a9a1b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\ufffd \ufffd\ufffd \ufffd We analyze each type of variance below.</div>\nIn-context tuning is significantly less sensitive to example ordering. We compare the variance with respect to example ordering for in-context\nLAMA\nBinaryClfs\nBB\nBL\nGPT2-M\nGPT2-L\nRaw IC-L\n3.74\n6.30\n18.52\n20.33\nICT\n1.78\n2.57\n11.46\n11.62\nTable 5: In-context tuning is significantly less sensitive to example choices compared to in-context learning with raw LMs.\nBERT-Base\nBERT-Large\nRaw IC-L\nICT\nRaw IC-L\nICT\n1-shot\n35.38\n26.31\n34.03\n28.78\n2-shot\n33.79\n25.40\n17.71\n19.35\n5-shot\n24.90\n15.64\n6.36\n5.16\nTable 6: In-context tuning is much less sensitive to task instruction wording compared to in-context learning with raw LMs.\ntuning and in-context prompting with raw LMs in Table 4. Results show that in-context tuning is significantly less sensitive to ordering of in-context examples compared to in-context prompting with raw LMs, reducing the sensitivity by 68% on LAMA and 83% on BinaryClfs. In-context tuning is significantly less sensitive to example choices. We compare the variance with respect to example choices for in-context tuning and in-context prompting with raw LMs in Table 5. Results show that in-context tuning is significantly less sensitive to selection of in-context examples compared to in-context prompting with raw LMs across both datasets and all model sizes, reducing the sensitivity by 56% on LAMA and 40% on BinaryClfs (averaged across model sizes). We conjecture that in-context tuning is significantly less sensitive to example ordering and selection because the model is exposed to various example orderings and selections during in-context tuning.\nwording. We report the variance with respect to instruction wording for in-context tuning and incontext prompting with raw LMs in Table 6. Results show that in-context tuning is less sensitive to instruction wording compared to in-context prompting with raw LMs in five out of six evaluation settings, reducing the variance by 19% on LAMA (averaged across model size and number of shots). We also observe that in-context tuning is especially effective on task instructions with low accu-\nracy under raw in-context learning. For each task, we compute the Pearson correlation between the raw in-context learning accuracy and the accuracy gain from in-context tuning (over raw in-context learning) on all instructions. On the LAMA dataset, we see a strong negative correlation of -0.563 (averaged across all tasks), with p-value < 0.05 on 63% of the tasks. We conjecture that in-context tuning is much less sensitive to instruction wording because the model is exposed to a wide variety of different task instructions during in-context tuning. In-context examples are complementary to instructions. We observe that in-context tuning is especially effective on task instructions with low accuracy under instruction tuning. For each task, we compute the Pearson correlation between the instruction tuning accuracy and the accuracy gain from in-context tuning (over instruction tuning) on all instructions. On the LAMA dataset, we see a strong negative correlation of -0.910 (averaged across all tasks), with p-value < 0.01 on 91% of the tasks. We conjecture that in-context tuning is much less sensitive to instruction wording because few-shot in-context examples provide additional task information besides the task instructions.\n# 5 Related Work\nLM Prompting for FSL Pre-trained LMs can be used to perform various FSL tasks when prompted with a natural language task instruction and several task examples (Radford et al., 2019; Brown et al., 2020; Schick and Sch\u00fctze, 2021b; Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021). However, prompting pre-trained LMs directly for FSL is known to be sensitive to various artifacts, such as the wording of the task instruction and the selection and ordering of few-shot training examples (Schick and Sch\u00fctze, 2021a; Jiang et al., 2020; Zhao et al., 2021; Gao et al., 2021; Liu et al., 2021). Our work is the first to show that meta-learning with an explicit FSL objective significantly reduces the sensitivity of LM prompting with respect to the in-context examples and instruction wording. Meta-learning for FSL Meta-learning is a widely used technique in NLP to improve crossdomain transfer (Yu et al., 2018; Geng et al., 2019; Holla et al., 2020; Deng et al., 2020) and crosstask transfer (Gu et al., 2018; Bansal et al., 2020; Dou et al., 2019). Existing optimization-based meta-learning methods mostly perform task adap-\ntation by fine-tuning a task-agnostic model on taskspecific examples using gradient descent (Finn et al., 2017; Jiang et al., 2019; Nichol et al., 2018). However, fine-tuning on few-shot task examples is sensitive to hyperparameters (Antoniou et al., 2019) and nested optimization during meta-training is often unstable (Nichol et al., 2018; Antoniou et al., 2019; Rajeswaran et al., 2019). In contrast, our approach performs few-shot task adaptation by using task-specific examples as part of the model input while keeping the model parameters frozen and task-agnostic during the adaptation stage.\nMulti-task Learning In multi-task learning, a single model is trained on the union of training sets of multiple tasks to learn a shared representation (Liu et al., 2019). The multi-task model is then fine-tuned on task-specific examples to adapt to new tasks. Multi-task learning is shown to improve performance on various downstream tasks, especially tasks with small training sets (Khashabi et al., 2020; Ye et al., 2021; Aghajanyan et al., 2021). Compared to meta-learning, multi-task learning does not optimize task adaptation directly.\nFine-tuned LMs for Instruction Learning Recent work shows that fine-tuning LMs to learn task instructions on a wide variety of tasks can further leverage the inductive bias of LMs to perform instruction learning (Zhong et al., 2021; Mishra et al., 2021; Wei et al., 2021). Our work is partially inspired by this line of work, but we work under the more generic few-shot meta-learning setting, and show that our approach out-performs both instruction tuning and existing few-shot meta-learning methods (e.g., MAML). While previous work focuses on the accuracy improvement gained from instruction fine-tuning, our work also looks into the well-known over-sensitivity issue of FSL and shows that in-context tuning effectively reduces the sensitivity of FSL with respect to various factors. Concurrent to our work, Min et al. (2021) also explores in-context tuning under more general Seq2Seq tasks. In comparison, our work compares in-context tuning to a meta-learning baseline MAML, and shows that in-context tuning mitigates the well-known oversensitivity issue of LM prompting. Contrary to our paper, Min et al. (2021) finds that in-context tuning under-performs InsT + FT. This might be because they use many more shots (16-shot), which could give gradient-based methods more advantage.\n# 6 Future Directions\nMeta-learning for Robustness Our work assumed that the few-shot training examples come from the same distribution as the test examples, but this assumption does not necessarily hold in practice. For example, the test distribution might constitute new input compositions (Lake and Baroni, 2018), rare subgroups (Sagawa et al., 2019), other types of distribution shifts (Hendrycks and Dietterich, 2019), or even adversarial examples (Kang et al., 2019). More effective meta-learning methods might learn a more robust learning mechanism and combat these generalization challenges.\nproperties of in-context learning are still unknown. Is in-context learning more robust to distribution shift (Lester et al., 2021)? Can we combine in-context learning and gradient learning to get the benefit of both worlds (Wortsman et al., 2021)?\n# 7 Conclusion\nIn this work, we propose meta-learning via incontext tuning, which recasts the few-shot learning process of task adaptation and task-specific prediction as a simple sequence prediction problem, where few-shot labeled examples are concatenated with the target example to form the model input. In-context tuning out-performs a wide variety of baselines in terms of accuracy, including raw LM prompting, MAML and instruction tuning. Meanwhile, sensitivity study shows that our FSL approach of in-context tuning is significantly\nless sensitive to few-shot examples and instruction wording compared to raw LM prompting. Given the empirical effectiveness of in-context tuning, we conjecture that the few-shot learning potential of large LMs (e.g., GPT-3) might be broadly underestimated, and that in-context tuning can eliminate well-known artifacts of few-shot LM prompting such as over-sensitivity to example ordering, example selection and instruction wording.\n# References\nDaniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. 2020. Towards a human-like opendomain chatbot.\nTrapit Bansal, Rishikesh Jha, and Andrew McCallum. 2020. Learning to few-shot learn across diverse natural language classification tasks. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5108\u20135123, Barcelona, Spain (Online). International Committee on Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\nAnnual Meeting of the Association for Computational Linguistics, pages 2978\u20132988, Florence, Italy. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, Online. Association for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3?\nCongying Xia, Wenpeng Yin, Yihao Feng, and Philip Yu. 2021. Incremental few-shot text classification with multi-round new classes: Formulation, dataset and system. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1351\u20131360, Online. Association for Computational Linguistics.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang, and Bowen Zhou. 2018. Diverse few-shot text classification with multiple metrics. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1206\u20131215, New Orleans, Louisiana. Association for Computational Linguistics.\n# A Hyperparameters\nIn this section, we report the hyperparameters we tuned for our approach and each baseline.\nIn this section, we report the hyperparameters we tuned for our approach and each baseline. In-Context Tuning (ours) We tune number of training epochs ([10, 15, 30] for LAMA and [1e-7, 3e-7, 1e-6, 3e-6] for BinaryClfs) and learning rate ([1e-7, 3e-7, 1e-6, 3e-6] for LAMA and [3e-6, 1e-5, 3e-5, 1e-4] for BinaryClfs). MAML We assume that inner optimization and outer optimization use the same learning rate. We tuned number of adapt steps ([1, 2, 4] for both datasets) and learning rate ([3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3] for LAMA and [3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3] for BinaryClfs). Instruction-Tuning + Fine-tuning For instruction tuning we tuned the same set of hyperparameters as in in-context tuning. The instruction tuning model with the highest validation performance are used for downstream task fine-tuning. For task finetuning, we tuned number of training epochs ([5, 10, 15, 30, 40] for LAMA and [5, 10, 15, 30, 40] for BinaryClfs) and learning rate ([1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5] for LAMA and [3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3] for BinaryClfs).\nIn-Context Tuning (ours) We tune number of training epochs ([10, 15, 30] for LAMA and [1e-7, 3e-7, 1e-6, 3e-6] for BinaryClfs) and learning rate ([1e-7, 3e-7, 1e-6, 3e-6] for LAMA and [3e-6, 1e-5, 3e-5, 1e-4] for BinaryClfs).\ntion tuning we tuned the same set of hyperparameters as in in-context tuning. The instruction tuning model with the highest validation performance are used for downstream task fine-tuning. For task finetuning, we tuned number of training epochs ([5, 10, 15, 30, 40] for LAMA and [5, 10, 15, 30, 40] for BinaryClfs) and learning rate ([1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5] for LAMA and [3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3] for BinaryClfs).\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Few-shot learning (FSL) is essential for adapting to new tasks with limited labeled examples, contrasting with traditional machine learning that often requires extensive labeled datasets. Recent advancements in large language models (LMs) have demonstrated promising results in FSL, yet raw LMs remain sensitive to example ordering and instruction wording, necessitating a more robust approach.",
            "purpose of benchmark": "The benchmark aims to evaluate the performance of in-context tuning (ICT) against existing methods like MAML, focusing on its effectiveness in few-shot learning tasks across diverse datasets."
        },
        "problem": {
            "definition": "The benchmark addresses the few-shot classification problem, where models must learn to classify new tasks based on a limited number of labeled examples.",
            "key obstacle": "Existing benchmarks suffer from sensitivity to example ordering, choice of examples, and instruction wording, leading to unstable performance in few-shot learning scenarios."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need to enhance few-shot learning through a more stable and efficient method that leverages the inductive bias of pre-trained LMs.",
            "opinion": "The authors emphasize the importance of ICT in significantly improving few-shot learning performance and reducing sensitivity to various artifacts inherent in traditional prompting methods.",
            "innovation": "ICT differs from previous benchmarks by integrating few-shot examples directly into the model input without gradient updates during task adaptation, simplifying the learning process.",
            "benchmark abbreviation": "ICT"
        },
        "dataset": {
            "source": "The datasets, LAMA and BinaryClfs, were sourced from existing tasks that test models' capabilities in factual knowledge and binary classification respectively.",
            "desc": "LAMA consists of 12,000 examples across 29 relations, while BinaryClfs includes 204 binary classification tasks with a variety of natural language instructions.",
            "content": "The datasets contain text data, specifically structured examples for classification tasks.",
            "size": "12,000",
            "domain": "Text Classification",
            "task format": "Binary Classification"
        },
        "metrics": {
            "metric name": "AUC-ROC, P@1",
            "aspect": "Accuracy",
            "principle": "The selected metrics are based on their ability to effectively measure model performance in classification tasks, providing insights into both predictive accuracy and ranking quality.",
            "procedure": "Model performance is evaluated by calculating AUC-ROC scores for binary tasks and mean precision at one (P@1) for multi-class tasks, using cross-validation and disjoint test sets."
        },
        "experiments": {
            "model": "The models tested include BERT and GPT2 variants, representing both state-of-the-art and baseline models.",
            "procedure": "Models were trained on the datasets using few-shot examples, with hyperparameters tuned based on validation performance across various configurations.",
            "result": "ICT consistently outperformed MAML and raw LM prompting, achieving significant gains in accuracy across both datasets.",
            "variability": "Variability was accounted for through multiple trials and random sampling of instructions and few-shot examples during both training and evaluation."
        },
        "conclusion": "The experiments demonstrated that in-context tuning effectively enhances few-shot learning performance while mitigating sensitivity issues associated with traditional prompting methods, suggesting its potential as a robust approach in the field.",
        "discussion": {
            "advantage": "ICT's strength lies in its ability to simplify the learning process and reduce sensitivity to various factors, leading to more reliable performance in few-shot scenarios.",
            "limitation": "The benchmark may still face challenges related to distribution shifts in test examples, which were not fully addressed in the current setup.",
            "future work": "Future research should explore the robustness of ICT under varying distributions and investigate the integration of gradient-based methods with in-context learning."
        },
        "other info": {
            "info1": "The benchmark highlights the few-shot learning potential of large LMs like GPT-3, which may be underestimated when not optimized for FSL.",
            "info2": {
                "info2.1": "The benchmark includes comprehensive evaluations across different model sizes.",
                "info2.2": "Variability in results was systematically analyzed to ensure robustness of findings."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Few-shot learning (FSL) is essential for adapting to new tasks with limited labeled examples, contrasting with traditional machine learning that often requires extensive labeled datasets."
        },
        {
            "section number": "1.2",
            "key information": "Recent advancements in large language models (LMs) have demonstrated promising results in FSL, yet raw LMs remain sensitive to example ordering and instruction wording, necessitating a more robust approach."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark aims to evaluate the performance of in-context tuning (ICT) against existing methods like MAML, focusing on its effectiveness in few-shot learning tasks across diverse datasets."
        },
        {
            "section number": "3.2",
            "key information": "ICT differs from previous benchmarks by integrating few-shot examples directly into the model input without gradient updates during task adaptation, simplifying the learning process."
        },
        {
            "section number": "4.1",
            "key information": "The authors emphasize the importance of ICT in significantly improving few-shot learning performance and reducing sensitivity to various artifacts inherent in traditional prompting methods."
        },
        {
            "section number": "6.1",
            "key information": "Existing benchmarks suffer from sensitivity to example ordering, choice of examples, and instruction wording, leading to unstable performance in few-shot learning scenarios."
        },
        {
            "section number": "6.4",
            "key information": "Future research should explore the robustness of ICT under varying distributions and investigate the integration of gradient-based methods with in-context learning."
        }
    ],
    "similarity_score": 0.7232479701431237,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Meta-learning via Language Model In-context Tuning.json"
}