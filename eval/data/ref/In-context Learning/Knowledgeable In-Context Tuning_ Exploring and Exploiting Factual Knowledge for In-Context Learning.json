{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.14771",
    "title": "Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning",
    "abstract": "Large language models (LLMs) enable incontext learning (ICL) by conditioning on a few labeled training examples as a text-based prompt, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets: the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting knowledge into LLMs during continual self-supervised pretraining, 2) judiciously selecting the examples for ICL with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on autoregressive models (e.g., GPTstyle LLMs) over multiple text classification and question-answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines and improves by more than 13% and 7% on text classification and question-answering tasks, respectively 1.",
    "bib_name": "wang2024knowledgeableincontexttuningexploring",
    "md_text": "# Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning\nJianing Wang1\u2217, Chengyu Wang2\u2217, Chuanqi Tan2, Jun Huang2, Ming Gao1,3\u2020 1 School of Data Science and Engineering, East China Normal University, Shanghai, China 2 Alibaba Group, Hangzhou, China 3 KLATASDS-MOE, School of Statistics, East China Normal University, Shanghai, China lygwjn@gmail.com,{chengyu.wcy,chuanqi.tcq}@alibaba-inc.com huangjun.hj@alibaba-inc.com,mgao@dase.ecnu.edu.cn\n# Abstract\nLarge language models (LLMs) enable incontext learning (ICL) by conditioning on a few labeled training examples as a text-based prompt, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets: the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting knowledge into LLMs during continual self-supervised pretraining, 2) judiciously selecting the examples for ICL with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on autoregressive models (e.g., GPTstyle LLMs) over multiple text classification and question-answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines and improves by more than 13% and 7% on text classification and question-answering tasks, respectively 1.\narXiv:2309.14771v2 \n# 1 Introduction\nLarge language models (LLMs) have become an imperative infrastructure in the natural language processing (NLP) community (Zhao et al., 2023b). To enable pre-trained LLMs to perform well without any parameter updates, in-context learning (ICL) has emerged as one of the flourishing research topics in many few-shot NLP tasks. It aims to generate predictions for target examples by conditioning on a few labeled samples (Brown et al.,\n\u2217J. Wang and C. Wang contributed equally to this work. \u2020 Corresponding author. 1The code and datasets are released in HugNLP (Wang et al., 2023a): https://github.com/HugAILab/ HugNLP.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d14/6d14e6da-0ba6-4a07-ac3b-b10c96d87690.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: An example of in-context learning (ICL)</div>\n2020). As shown in Figure 1, the key component of ICL is the text-based prompt (containing labeled examples) that functions as the demonstration. Previous works have explored multiple aspects that affect the performance of ICL (Dong et al., 2023), such as input-output mapping (Min et al., 2022b; Kim et al., 2022), extensive data resources (Mishra et al., 2022; Chen et al., 2022b; Min et al., 2022a), prediction calibration (Zhao et al., 2021), and self-improvment (Chen et al., 2023; Lyu et al., 2023). Liu et al. (2022); Lu et al. (2022) have investigated others, such as prompt format (e.g., \u201cInput:\u201d, \u201cOutput:\u201d), the selection of labeled data, and example permutation. Wang et al. (2023a); Wu et al. (2023) have developed toolkits for LLMs to reason with ICL prompts. In addition, to better elicit the LLM to reason on complex tasks, chain-of-thought (CoT) has been introduced to extend the ICL with multiple rationales to express the thinking process (Wei et al., 2022; Dhuliawala et al., 2023; Wang et al., 2023c,b; Zhao et al., 2023a; Zhang et al., 2023; Liang et al., 2023). However, these works pay little attention to the influence of factual knowledge in ICL, which is a non-negligible factor in NLP (Hu et al., 2022). To this end, we explore the effectiveness of ICL from the perspective of factual knowledge. As seen in Figure 2, when entities and labels in text-based prompts are randomly replaced or removed, the average accuracy decreases significantly, indicating that performance degradation is universal across different model scales. Further analysis reveals that: 1) more intrinsic factual knowledge acquired during the pre-training stage is typically beneficial\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9ee/b9ee3b82-7607-4d83-aece-489ef287487c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Results of different scales of GPT-2 and OPT models over 8 text classification tasks and 4 question answering tasks in various component destruction settings. For each target example, we have K = 8 labeled samples as the demonstration. Results indicate that factual knowledge is crucial to the performance of ICL.</div>\nfor LLMs to improve effectiveness; 2) The factual knowledge (e.g., entities and labels) derived from selected in-context examples is crucial for the performance of ICL; 3) LLMs tend to generate common words that may have high frequencies in the training corpora, resulting in biased predictions. After analyzing these knowledge facets, a natural question arises: How can we fully employ factual knowledge to further improve the performance of ICL? To achieve this goal, we focus on causal autoregressive LLMs (e.g., GPT-2 (Radford et al., 2019) and OPT (Zhang et al., 2022a)) and present a novel Knowledgeable In-Context Tuning (KICT) framework, which involves knowledgeable guidance in pre-training, prompting, and prediction of these models. Specifically, to endow LLMs with enhanced text generation abilities by better leveraging inherent knowledge, we introduce several knowledgeable self-supervised tasks during the pretraining stage to inject knowledge into LLMs. For text-based prompting, we propose a knowledgeable example retrieval algorithm to judiciously select in-context examples that have relevant knowledge to the target example. Finally, during prediction, we utilize the knowledge-wise priors of label words from an underlying knowledge base (KB) to calibrate the prediction distributions generated by LLMs. Each of the proposed techniques is plugand-play and can be freely combined, facilitating users to exploit knowledge for improving ICL. To evaluate the effectiveness of the KICT framework, we employ LLMs (e.g., GPT-style models) to conduct extensive experiments over multiple text classification and question-answering tasks. Results demonstrate that each proposed procedure achieves substantial improvements. To sum up, we make the following main contributions:\n\u2022 Extensive experiment results show that our approach attains more impressive performance over classification and QA tasks.\n# 2 Impact of Knowledge on ICL\nIn this section, we investigate whether factual knowledge affects the performance of ICL.\n# 2.1 Preliminary Experimental Settings\nFollowing Min et al. (2022b) and Kim et al. (2022), we perform empirical experiments through component destruction. Specifically, given a target example text Xtgt, we randomly select K training samples \ufffdD = {(Xtrn i , ytrn i )}K i=1 to form a text-based prompt. We identify all entities in the prompt and then devise several destruction settings as follows: 1) Shuffle Entity involves randomly replacing all entities with others from the KB; 2) Shuffle Non-Entity entails replacing some non-entity words (e.g., \u201cIt\u201d, \u201chave\u201d) with others from the vocabulary; 3) Shuffle Label consists of replacing all the golden labels with incorrect ones; 4) Remove Entity and Remove Label aim to remove all entities and labels from the prompt, respectively; 5) No Demonstration represents a typical zero-shot\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/acc8/acc8746a-2d23-4215-a952-67ae53609853.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: 4-shot results of GPT-2 (urge) over AGNews and TREC. For each frequency region, we sample top-5 label words for each category and report the accuracy for all label mapping permutations.</div>\nmethod where no labeled data is used (Min et al., 2022b). We employ various scales of GPT-2 (0.1B-1.5B) and OPT (Zhang et al., 2022a) (2.7B-6.7B) models to evaluate 8 text classification tasks and 4 question answering tasks. 2 By default, we randomly sample K = 8 labeled samples for each task and conduct the experiments with 5 different random seeds. Further details are presented in Appendix A. The findings are summarized below.\n# 2.2 Findings\nThe inherent knowledge in the LLM itself is beneficial for the performance of downstream tasks. As shown in Figure 2, models can achieve remarkable few-shot performance with increased scale. We hypothesize that larger models can learn more valuable semantics in the pre-training corpus, which contributes to this improvement. To test this hypothesis, we perform zero-shot inference without any text-based prompts (i.e., No Demonstration), relying solely on the intrinsic knowledge acquired during pre-training to guide the predictions. We observe that the performance gap between the 6.7B and 0.1B models is about 20% on both text classification and questionanswering tasks. This observation supports the idea that the inherent knowledge learned during pre-training is critical (Yang et al., 2021). The factual knowledge in selected in-context examples is crucial for ICL. As shown in Figure 2, the original setting (Origin) outperforms other configurations across all model scales. We observe that altering non-entity words does not significantly reduce performance, whereas replacing or removing entities leads to a considerable decrease in\n2Due to resource constraints, we do not use larger models. Nevertheless, our findings are generally consistent across different model scales.\naverage accuracy for both text classification and question-answering tasks. This demonstrates that factual knowledge embedded in text-based prompts is a critical factor for LLMs to understand the task. Furthermore, we find that labels are also essential for ICL, echoing similar observations presented in (Kim et al., 2022). Differing from Min et al. (2022b), we posit that labels can be regarded as a form of factual knowledge that guides the LLM to grasp semantics during inference. LLMs tend to generate common label words due to knowledge bias. To investigate whether predictions are biased, we select two knowledgeintensive tasks (i.e., AGNews (Zhang et al., 2015), and TREC (Voorhees and Tice, 2000)). We first retrieve the top-5 predictions at the output position for each training example3 and compute frequency statistics for each generated label word. Subsequently, we select 4 labeled examples from the training set for each category. From each frequency region, we randomly choose 2 label words and calculate the average accuracy across all label mapping permutations.4 The results, as presented in Figure 3, reveal that performance is highly contingent on label word frequency, suggesting that the frequency with which factual knowledge is learned by LLMs plays a critical role in prediction outcomes. Similar observations have been reported by Zhao et al. (2021).\n# 3 The Proposed KICT Framework\nThe preliminary experiments demonstrate that factual knowledge has a substantial effect on ICL. This suggests that we can exploit this knowledge to enhance performance across various processes in ICL, including pre-training, prompting, and prediction. To achieve this goal, we introduce the KICT framework, a novel Knowledgeable In-Context Tuning framework designed to better leverage knowledge and unleash the power of LLMs in answer generation. Within this framework, we introduce Knowledgeable Pre-Training (KPT) with three carefully designed self-supervised tasks to infuse LLMs with factual knowledge. We then present a Knowledgeable Example Retrieval (KER) algorithm to judiciously select in-context examples that are relevant to the given knowledge. Finally, we employ a\n3The training set is larger than the testing set, thereby providing a more robust statistical representation. 4Considering AGNews as an example, which has 4 classes with 2 label words each, there are 24 = 16 possible label mapping permutations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3679/3679ee1e-c52e-453b-ac20-60968063f257.png\" style=\"width: 50%;\"></div>\nFigure 4: The overview of the KICT framework. We introduce multiple plug-and-play knowledgeable techniques to enhance the utilization of knowledge for improving ICL performance. Left: We propose three knowledge-aware self-supervised learning tasks that infuse factual knowledge into LLMs during pre-training. Middle: We utilize entity-related information to select in-context examples that exhibit high knowledge relevance to the target example. Right: For prediction, we derive prior information from large-scale corpora to calibrate the predictions.\nKnowledgeable Prediction Calibration (KPC) technique to adjust the prediction distribution using prior information derived from a KB. An overview of the framework is depicted in Figure 4.\n# .1 Knowledgeable Pre-Training\nThis section describes three knowledge-aware selfsupervised learning tasks designed to infuse factual knowledge into LLMs, namely, Masked Entity Prediction (MEP), Entity Description Generation (EDG), and Knowledgeable Question Answering (KQA). Differing from Chen et al. (2022a), we leverage an external KB to enrich the models\u2019 language generation abilities with respect to important entities. The input consists of a training corpus {X} and a KB G = (E, R, T ), where E denotes a set of entities, R a set of relations, and T a set of triples representing factual knowledge. Masked Entity Prediction (MEP). MEP requires the model to predict missing entities within a text, enhancing its capability to learn explicit knowledge. This task is akin to Masked Language Modeling employed in BERT-style models (Devlin et al., 2019; Liu et al., 2019). Given a text composed of tokens X = {xi}, we identify all entities EX = {e|e \u2208G, e \u2208X} using an entity linking toolkit. Each entity e = {xj|xj \u2208X}, which may span multiple tokens, is either replaced with special tokens (e.g., \u201c_\u201d) or random tokens with equal probability. This process generates a modified text \u02c6X = {\u02c6xi}. A label mask vector M \u02c6 X is created to indicate training positions, where M \u02c6 Xi = I(\u02c6xi \u2208EX) and I(\u00b7) is an indicator function. Figure 4 (left) illustrates this with highlighted\nwords. Entity Description Generation (EDG). EDG tasks the model with producing a text description for a given entity. For a text X and associated entity set EX, we construct a prefix text using the template \u201cEntities:\u201d, followed by a list of entities and the template \u201cText:\u201d. The original text X serves as the suffix. This forms the modified example \u02c6X and corresponding label mask vector M \u02c6 X, where M \u02c6 Xi = 1 if \u02c6xi is part of the suffix string. Knowledgeable Question Answering (KQA). KQA leverages relation triples from the KB to facilitate question answering. Given a text X and entity set EX, we select a pair of entities eh, et \u2208EX linked by a 1-hop relation r \u2208R to form a triple (eh, r, et) \u2208T . Inspired by Wang et al. (2022), we create a question template for each triple, prompting the model to predict the tail entity et. Training examples \u02c6X and label mask vectors are generated accordingly, with M \u02c6 Xi = 1 designating tokens belonging to the tail entity. During pre-training, we randomly compile examples from the same task into a training batch X = { \u02c6X} until the maximum sequence length is reached. The cross-entropy loss for prediction positions (where M \u02c6 X = 1) is computed as follows:\n (1)\nwhere yi is the ground truth token, p(\u00b7) is the predicted probability, and T \u02c6 X = \ufffd \u02c6xi\u2208\u02c6 X M \u02c6 Xi is the number of tokens the model is required to predict.\n# 3.2 Knowledgeable Example Retrieval\nDespite having a powerful and knowledgeable LLM at our disposal, the efficacy of ICL is significantly influenced by the selection and ordering of labeled examples (Brown et al., 2020). Previous studies (Liu et al., 2022; Lu et al., 2022; Rubin et al., 2022) have demonstrated that LLMs can autonomously generate suitable text-based prompts, yet they largely overlook the importance of factual knowledge from KBs. To address this gap, we introduce a novel Knowledgeable Example Retrieval (KER) algorithm that utilizes knowledge to select the most relevant in-context examples. This process is illustrated in Figure 4 (middle) and detailed in Algorithm 1 in Appendix C. Concisely, given a training set Dtrn = {(Xtrn i , ytrn i , Etrn i )} and a testing set Dtgt = {(Xtgt j , Etgt j )}, where Xtrn i and Xtgt j are input texts, ytrn i are labels, and Etrn i and Etgt j are the corresponding entity sets, KER\u2019s objective is to select a subset of training examples as demonstrations that exhibit high knowledge relevance to the testing set. A straightforward approach is to retrieve examples containing entities that cover a higher number of target examples. We use the Jaccard similarity to assess the similarity between two examples:\n(2)\n  However, since the Jaccard similarities for most example pairs are zero, we further employ pre-trained knowledge embeddings to retrieve training examples that are semantically similar to the target set. We compute the average representations ei and ej of all entities in Etrn i and Etgt j , respectively. The semantic difference is quantified using the Euclidean distance dsem(i, j) between ei and ej. The overall knowledge relevance between two examples is calculated as follows:\n(3)\nwhere \u03b1 \u2208[0, 1] and \u03b3 > 0 are tunable hyperparameters. The sampling weight for each training example Xtrn i is given by:\n(4)\nwhere s(Xtrn i ) is computed as the average relevance score to the testing set:\ns(Xtrn i ) = 1 |Dtgt| \ufffd Xtgt j \u2208Dtgt d(Xtrn i , Xtgt j ). (\nAn example with a higher weight signifies greater knowledge relevance across all target examples. Ultimately, we sample K training examples based on these weights to serve as in-context examples.\n# 3.3 Knowledgeable Prediction Calibration\nFollowing model pre-training and in-context example selection, we can proceed to generate predictions for the target example Xtgt \u2208Dtgt using the following equation:\n\u02c6y = arg max v\u2208V p(y = e|X, Xtgt),\n(6)\nwhere V is a verbalizer that maps label words to their corresponding classes 5. \ufffdD represents the set of in-context examples used for prediction. However, as discussed in Section 2, the frequency of label words (in classification tasks) or entities (in question answering tasks) can bias the prediction probabilities. To mitigate this issue, we utilize the prior information of label words to refine the prediction for each target example. Specifically, we select a subset of training data S from the KQA task and estimate the contextual prior probability for each candidate label word or entity v \u2208V at the output position:\n(7)\nwhere \u02c6X denotes a training example, and P(v) represents the estimated prior probability of candidate v. Following this, we discard any label word or entity v whose prior probability falls below a specific threshold (Hu et al., 2022). Consequently, we enhance the final output by applying calibrated prediction:\n(8)\n \ufffd Remarks. While most related works (Hu et al., 2022; Zhao et al., 2021) concentrate on prediction calibration, our approach distinguishes itself by\n5For classification tasks, V is the set of label words; for question answering tasks, V is the entire vocabulary.\nleveraging a priori knowledge from a large-scale corpus to debias outputs. This contrasts with methods that rely solely on in-domain data or utilize task-agnostic, content-free inputs (e.g., \u201cN/A\u201d).\n# 4 Experiments\n# 4.1 Implementation Settings and Baselines\nFor the pre-training corpus, we use Wikipedia Dumps (2020/03/01)6, which consists of 25,933,196 sentences. Further, the KB we used is WikiData5M (Wang et al., 2021b), which includes 3, 085, 345 entities and 822 relation types. By default, we choose GPT-2 (large) with 0.8B parameters as the backbone. For downstream tasks, we consider 8 text classification tasks and 4 question answering tasks. The details of corpora and downstream benchmarks are shown in Appendix B. The implementation details of pre-training, prompting, and prediction can be found in Appendix C. We consider the following baselines: 1) InContext Learning (ICL) is the vanilla version proposed by GPT-3. 2) Calibrate Before Use (CBU) (Zhao et al., 2021) is a typical method that aims to de-bias the prediction via content-free prompts. 3) KATE (Liu et al., 2022) uses the CLS embeddings of a RoBERTa-large model as sentence representations, and retrieves the nearest K neighbors for each target example as the final incontext examples. 4) MetaICL (Min et al., 2022a) improves ICL by meta-learning the objective of ICL in cross-task settings. 5) SelfSup. (Chen et al., 2022a) improves ICL by multiple self-supervised learning tasks. We also choose RoBERTa-large to perform fully Fine-tuning to demonstrate the ceiling performance of each task.\n# 4.2 Main Results\nTable 1 and Table 2 respectively report the results over text classification and question answering tasks in the 8-shot setting. We thus make the following observations: 1) Our proposed framework outperforms strong baselines and achieves substantial improvements over all benchmarks. Specifically, compared with ICL, the average result over the text classification task is improved by 13.70%, which is larger than that of other baselines. The average gain over question answering tasks is also more than 7%, although there is still room for improvement on unseen target domains, likely because they\n6https://dumps.wikimedia.org/enwiki/\nrequire more challenging generalization and commonsense abilities. 2) Compared with ICL, KER and KCP make significant contributions to the performance. Particularly, KER and KCP also respectively outperform strong baselines KATE and CBU, indicating the indispensable merit of factual knowledge at the inference stage. 3) The performance of KPT exceeds that of meta-learning (MetaICL) and self-supervised learning (SelfSup.) approaches by around 4%, which also focus on continual pretraining. This demonstrates that explicitly injecting knowledge into LLMs is more effective for ICL, which is imperative and plays a dominant role. 4) Our method attains more impressive performance when combining all of these knowledgeable techniques, highlighting the necessity of factual knowledge in ICL. We provide a detailed analysis in Section 4.3. 5) We also evaluate other scales for GPT-2 and OPT in 8-shot settings. Results in Appendix F show that the improvements are consistent across different LLMs.\n# 4.3 Ablation Study\nWe further investigate how these proposed knowledgeable techniques contribute to the final performance with different combinations. As shown in Table 3, the results demonstrate that any combination greatly promotes the overall performance of vanilla ICL. An interesting observation is that KPT is particularly important for performance improvement, achieving higher scores than KER and KCP. This indicates that the most effective way to unleash the power of LLMs is to inject knowledge into the model parameters. Nonetheless, the combination of KER and KCP also improves ICL by about 8% for each task, respectively. This suggests that KER and KCP are critical to ICL because ultralarge LLMs cannot be continuously pre-trained or tuned in real-world scenarios to save computational resources. Furthermore, results from Table 1 to Table 3 show that our method has significantly improved classification tasks. We believe that the benefits of injecting knowledge are more pronounced for simple language understanding tasks than for question answering.\n# 4.4 Further Analysis\nEffectiveness of KPT. To investigate what makes a high performance for KPT, we test the effectiveness of each knowledgeable self-supervised task. For a fair comparison, we also choose two baselines: 1) None is that we do not use any self-\nBaselines\nSST-2\nMRPC\nMNLI\nQNLI\nRTE\nCB\nTREC\nAGNews\nAvg.\nacc\nf1\nacc\nacc\nacc\nacc\nacc\nacc\nFull Data\nFine Tuning (RoBERTa-large)\n95.00\n91.40\n89.80\n93.30\n80.90\n90.50\n97.40\n94.70\n91.63\nFew-shot Labeled Data (8-shot)\nICL (Brown et al., 2020)\n76.18\u00b17.2\n54.46\u00b12.3\n56.85\u00b12.4\n52.93\u00b13.2\n53.94\u00b15.0\n42.50\u00b11.8\n51.56\u00b14.1\n45.67\u00b16.6\n54.26\nCBU (Zhao et al., 2021)\n82.71\u00b14.4\n63.07\u00b13.9\n57.93\u00b12.8\n53.19\u00b13.9\n54.87\u00b12.8\n51.34\u00b11.7\n54.61\u00b13.7\n55.42\u00b12.8\n59.14\nKATE (Liu et al., 2022)\n81.33\u00b13.8\n58.04\u00b13.9\n59.40\u00b12.4\n53.57\u00b13.5\n53.17\u00b12.7\n45.48\u00b12.1\n54.69\u00b12.8\n50.28\u00b13.4\n57.00\nMetaICL\u2020 (Min et al., 2022a)\n87.40\u00b15.0\n62.91\u00b12.0\n60.22\u00b13.4\n55.18\u00b11.9\n57.06\u00b12.8\n49.20\u00b12.5\n56.09\u00b11.8\n55.80\u00b12.4\n60.48\nSelfSup.\u2020 (Chen et al., 2022a)\n87.94\u00b13.0\n62.33\u00b12.0\n62.00\u00b12.2\n54.77\u00b11.8\n57.27\u00b12.6\n45.80\u00b12.5\n55.59\u00b12.5\n57.44\u00b13.2\n60.39\nKICT\u2020\n91.21\u00b12.9\n69.96\u00b10.7\n69.59\u00b11.0\n60.66\u00b11.2\n63.74\u00b14.2\n56.07\u00b13.8\n63.52\u00b15.5\n68.89\u00b15.7\n67.96\nonly w. KPT\u2020\n90.04\u00b13.5\n66.65\u00b11.9\n67.39\u00b12.6\n58.97\u00b13.0\n58.26\u00b13.3\n55.43\u00b12.0\n60.16\u00b12.2\n59.74\u00b14.4\n64.58\nonly w. KER\n84.05\u00b12.7\n59.26\u00b12.5\n59.93\u00b11.0\n57.23\u00b11.2\n53.79\u00b14.0\n51.36\u00b13.8\n55.52\u00b15.1\n52.70\u00b13.3\n59.23\nonly w. KPC\n85.52\u00b13.9\n64.77\u00b10.7\n63.13\u00b11.2\n57.69\u00b12.4\n55.94\u00b11.2\n54.07\u00b12.8\n56.92\u00b12.7\n57.24\u00b15.5\n61.91\nTable 1: The 8-shot performance (%) on GPT-2 (large) of different learning settings with standard deviations ove text classification benchmarks. Compared with other baselines, our framework achieves consistent improvement.  denotes the method involves parameters update for ICL. \u201conly w.\u201d means we only use one technique in KICT.\nBaselines\nComQA\nQuartz\nSQuAD\nQuoref\nAvg.\nacc\nacc\nem\nem\nFull Data\nFine Tuning (RoBERTa-large)\n72.10\n76.90\n86.50\n78.70\n78.55\nFew Labeled Data (8-shot)\nICL (Brown et al., 2020)\n27.93\u00b14.8\n54.49\u00b13.5\n46.93\u00b13.0\n40.31\u00b12.7\n42.42\nCBU (Zhao et al., 2021)\n29.88\u00b13.9\n55.40\u00b11.8\n49.32\u00b14.0\n44.05\u00b14.0\n44.66\nKATE (Liu et al., 2022)\n29.02\u00b14.0\n55.10\u00b13.9\n47.25\u00b13.4\n42.77\u00b13.8\n43.54\nMetaICL\u2020 (Min et al., 2022a)\n31.16\u00b13.2\n55.64\u00b12.9\n50.46\u00b12.6\n46.72\u00b12.7\n46.00\nSelfSup.\u2020 (Chen et al., 2022a)\n31.32\u00b13.0\n54.88\u00b13.0\n49.97\u00b12.7\n47.50\u00b13.5\n45.92\nKICT\u2020\n36.17\u00b11.8\n58.11\u00b12.4\n54.23\u00b12.6\n50.46\u00b13.3\n49.74\nonly w. KPT\u2020\n34.21\u00b14.3\n57.32\u00b12.2\n52.79\u00b13.0\n49.93\u00b11.9\n48.56\nonly w. KER\n29.56\u00b12.3\n55.82\u00b11.2\n48.11\u00b12.4\n43.58\u00b12.1\n44.27\nonly w. KCP\n33.60\u00b13.7\n57.77\u00b12.4\n51.63\u00b12.9\n46.09\u00b13.1\n47.27\nTable 2: The 8-shot performance (%) on GPT-2 (large) of different learning settings with standard deviations o question answering benchmarks.\nsupervised task, which is the same as vanilla ICL proposed in (Brown et al., 2020), 2) GPT-2 represents conventional autoregressive language modeling (ALM) pre-training tasks. As shown in Table 4, KPT can make substantial improvements for ICL. Particularly, all the self-supervised learning tasks in KPT are complementary for pre-training and outperform the baseline with or without the conventional objective of GPT-2. In addition, the MEP and KQA tasks are most critical for classification and question answering, respectively, which demonstrates that different pre-training objectives possess different advantages in downstream tasks. Sample Effectiveness. To investigate the influence of the number of in-context examples K, we choose multiple classification and question answering tasks and vary K from 0, 1, 4, 8 to 16. From Figure 5, we find that increasing K generally helps across both classification and question answering\ntasks, demonstrating that more in-context examples may bring more knowledge to better guide the LLM to make predictions. When K > 8, the performance of the most tasks will decrease, because the maximum length limit causes information loss. The suitable value K is set around 8. Visualization of Selected Examples in KER. In addition, for explicitly seeing the performance in semantic space, we obtain the t-SNE (Van der Maaten and Hinton, 2008) visualization of each training example over AGNews via averaged representations of all corresponding entities. We choose KATE as our strong baseline, which is also focused on the example selection. Here, we do not finetune RoBERTa on the training set. Figure 6 demonstrates that our method can build better semantic representations toward factual knowledge.\nPermutations of In-Context Examples. We also compare different permutations of these selected\nBaselines\nSST-2\nMRPC\nMNLI\nRTE\nAGNews\nTREC\nComQA\nQuartz\nSQuAD\nQuoref\nacc\nf1\nacc\nacc\nacc\nacc\nacc\nacc\nem\nem\nICL\n76.18\u00b17.2\n54.46\u00b12.3\n56.85\u00b12.4\n53.94\u00b15.0\n45.67\u00b16.6\n51.56\u00b14.1\n27.93\u00b14.8\n54.49\u00b13.5\n46.93\u00b13.0\n40.31\u00b12.7\nKPT+KER\n91.04\u00b13.3\n67.93\u00b13.0\n68.47\u00b12.9\n61.30\u00b13.3\n62.18\u00b13.9\n61.52\u00b13.1\n35.17\u00b14.0\n57.64\u00b12.6\n52.23\u00b13.4\n50.20\u00b13.1\nKPT+KCP\n90.65\u00b13.7\n68.44\u00b12.5\n68.89\u00b13.4\n62.38\u00b12.3\n63.88\u00b13.5\n62.12\u00b12.9\n36.38\u00b12.2\n58.03\u00b12.0\n54.17\u00b11.8\n50.18\u00b12.2\nKER+KCP\n86.45\u00b13.0\n64.07\u00b12.4\n66.60\u00b12.9\n57.39\u00b13.2\n58.95\u00b13.6\n58.60\u00b13.5\n34.26\u00b12.2\n57.88\u00b13.1\n52.20\u00b12.3\n47.92\u00b12.7\nAll (KICT)\n91.21\u00b12.9\n69.96\u00b10.7\n69.59\u00b11.0\n63.74\u00b14.2\n68.89\u00b15.7\n63.52\u00b15.5\n36.17\u00b11.8\n58.11\u00b12.4\n54.23\u00b12.6\n50.46\u00b13.1\nTable 3: The 8-shot performance (%) of different combinations of the knowledgeable modules.\nMethods\nSST-2\nAGNews\nTREC\nComQA\nSQuAD\nacc\nacc\nacc\nacc\nem\nNone (ICL)\n76.18\u00b17.2\n45.67\u00b16.6\n51.56\u00b14.1\n27.93\u00b14.8\n46.93\u00b13.0\nGPT-2\n81.35\u00b13.0\n48.72\u00b12.7\n52.36\u00b13.3\n28.61\u00b13.8\n47.14\u00b13.1\nKPT\n90.04\u00b13.5\n59.74\u00b14.4\n60.16\u00b12.0\n34.21\u00b14.3\n52.79\u00b13.0\nw/o. MEP\n84.40\u00b14.0\n51.29\u00b13.9\n54.72\u00b13.1\n33.01\u00b17.7\n52.23\u00b12.8\nw/o. EDG\n87.19\u00b12.9\n56.40\u00b14.3\n55.91\u00b13.1\n31.95\u00b15.9\n50.80\u00b13.9\nw/o. KQA\n85.30\u00b13.3\n53.03\u00b13.6\n53.46\u00b12.4\n30.08\u00b15.8\n49.71\u00b14.6\n<div style=\"text-align: center;\">Table 4: The 8-shot performance (%) of each selfsupervised task. GPT-2 denotes the vanilla objective.</div>\nBaselines\nSST-2\nMRPC\nMNLI\nRandom\n79.42\u00b12.7\n59.26\u00b12.5\n59.93\u00b11.0\nAscending\n78.29\u00b12.2\n58.05\u00b12.6\n59.31\u00b11.5\nDescending\n79.61\u00b13.0\n58.16\u00b13.0\n59.58\u00b11.3\nTable 5: The 8-shot averaged results (%) of KICT (only w. KER) for different permutations.\nTable 5: The 8-shot averaged results (%) of KICT (only w. KER) for different permutations.\nexamples according to the sample weight computed in Eq. 4. In Table 5, Random means to randomly choose an order. Ascending and Descending respectively denote that the example order is ascending or descending by weight. From the results, we find no tangible relationship between the sampling weight and order. Effectiveness of KPC. We finally conduct analysis on prediction calibration. We choose AGNews and TREC tasks and follow the same settings in the preliminary experiments (we randomly choose two label words from different frequency regions). Results in Figure 7 demonstrate that calibrating the prediction consistently achieves improvements to the vanilla approach. In addition, we find that the prediction results highly depend on the label frequency, which is similar to Figure 3. However, our KPC still outperforms the strong baseline Calibrate Before Use (CBU) with arbitrary label frequency, which only transforms the input into content-free prompts. It underscores that the prior information of each label word in KB is non-negligible. In other words, calibration by the prior information can alleviate the impact of label frequency.\nexamples according to the sample weight computed in Eq. 4. In Table 5, Random means to randomly choose an order. Ascending and Descending respectively denote that the example order is ascending or descending by weight. From the results, we find no tangible relationship between the sampling weight and order.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/39d3/39d386ae-55cd-41e5-8080-61e31ffc26a7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5447/54474e62-d6de-4867-adea-3018411fb3f3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 1 4 8 16 Example Number (K) Question Answering</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c340/c340390b-7712-4e6b-9d5e-905b2b7f527c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 1 4 8 16 Example Number (K)</div>\nFigure 5: GPT-2 (large) sample effectiveness (%) of KICT (only w. KER) with different values of K.\n# 5 Related Work\n# 5.1 Pre-trained/Large Language Models\nPre-trained Language Models (PLMs) aim to learn representations from texts and have made significant progress in NLP. PLMs can be divided into three main categories: encoder-only (Devlin et al., 2019; Liu et al., 2019; He et al., 2021; Yang et al., 2019; Lan et al., 2020; Zhang et al., 2022b), decoder-only (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022a), and encoderdecoder (Lewis et al., 2020; Raffel et al., 2020). To incorporate factual knowledge into PLMs, a branch of knowledge-enhanced PLMs has been proposed (Zhang et al., 2019; Sun et al., 2020a; Wang et al., 2021b,a, 2022; Pan et al., 2022; Zhang et al.,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/54a8/54a86d5d-49c1-4f86-ac36-759baf9dd973.png\" style=\"width: 50%;\"></div>\nFigure 6: Visualizations of each AGNews\u2019s training example. KATE (left) uses CLS embeddings of RoBERTa. Ours (right) utilizes averaged knowledge embeddings.\n<div style=\"text-align: center;\">Figure 6: Visualizations of each AGNews\u2019s training example. KATE (left) uses CLS embeddings of RoBERTa. Ours (right) utilizes averaged knowledge embeddings.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/56dd/56dd9401-f5ac-4db9-a95b-f79ba19b981e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: GPT-2 (large) 4-shot performance of calibration over difference word frequencies.</div>\n2022c), enabling PLMs to capture rich semantic knowledge from KBs. Since the introduction of ChatGPT, a variety of decoder-only LLMs have been released. Popular open-source LLMs include LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022a), Galactica (Taylor et al., 2022), Pythia (Biderman et al., 2023), among others. Our work concentrates on decoder-only LLMs and aims to infuse them with factual knowledge to enhance their ICL performance.\n# 5.2 Prompt Learning\nPrompt-based learning aims to add natural language prompts to guide PLMs to solve downstream tasks. A series of works focus on tunable discrete prompt tuning (Gao et al., 2021; Raffel et al., 2020) and continuous prompt tuning (Liu et al., 2021b; Gu et al., 2021; Xu et al., 2023). For LLMs, GPT3 (Brown et al., 2020) enables In-Context Learning (ICL) with a text-based prompt in zero-shot scenarios, bypassing parameter updates (Dong et al., 2023). To explore the factors affecting ICL, previous works have focused on input-output mapping (Min et al., 2022b; Kim et al., 2022), metalearning (Chen et al., 2022b; Min et al., 2022a), prompt engineering (Liu et al., 2022, 2021a), and prediction calibration (Zhao et al., 2021; Hu et al.,\n2022), among others. Recently, the Chain-ofThought (CoT) approach has been presented to leverage reasoning and interpretable information to guide LLMs in generating reliable responses (Si et al., 2022; Zhang et al., 2022d; Wei et al., 2022; Yan et al., 2023). Different from these approaches, we exploit factual knowledge to further improve ICL in pre-training, prompting, and prediction phases.\n# 6 Conclusion\nIn this paper, we investigate and harness factual knowledge in ICL, including inherent knowledge embedded in LLMs, pertinent knowledge derived from selected training examples, and knowledge biases affecting predictions. We introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further enhance ICL performance by comprehensively exploiting factual knowledge throughout the processes of pre-training, prompting, and prediction. Experiments demonstrate that each introduced technique significantly improves upon strong baselines across classification and questionanswering tasks. Future work will focuses on 1) exploring the reasoning capabilities and interpretability of knowledge within ICL, and 2) extending our approach to encoder-decoder models.\n# Acknowledgements\nThis work has been supported by the National Natural Science Foundation of China under Grant No. U1911203, Alibaba Group through the Alibaba Innovation Research (AIR) Program, the National Natural Science Foundation of China under Grant No. 61877018, the Research Project of Shanghai Science and Technology Commission (20dz2260300) and the Fundamental Research Funds for the Central Universities.\n# Limitations\nThis work presents several limitations: 1) It concentrates on decoder-only LLMs, as traditional incontext learning primarily targets decoder-only generation models such as GPT-2, GPT-3, OPT, etc. Nevertheless, we envision potential extensions to encoder-decoder architectures used in tasks such as translation and conditional generation. 2) Due to computational resource constraints, we do not experiment with ultra-large LLMs exceeding 10 billion parameters. 3) Our investigation centers on factual knowledge in three specific areas: pre-training,\nprompting, and prediction. We acknowledge that knowledge may influence additional aspects such as reasoning and interpretability, and we intend to explore these in future research.\n# Ethical Considerations\nThe contributions of this work are methodological, focusing on a Knowledgeable In-Context Tuning (KICT) framework to augment the capabilities of LLMs with factual knowledge. Nonetheless, transformer-based models may perpetuate negative biases, including gender and social biases. As such, these issues are inherent to our work as well. We advise caution and recommend addressing potential risks when KICT models are deployed in real-world applications.\n# References\nGalen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In Proceedings of the 24th International Conference on Machine Learning, pages 33\u201340. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 2397\u20132430. PMLR. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS. Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. 2022a. Improving in-context few-shot learning via self-supervised training. In NAACL, pages 3558\u20133573. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen. 2023. Self-icl: Zero-shot incontext learning with self-generated demonstrations. In EMNLP, pages 15651\u201315662. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022b. Meta-learning via language model in-context tuning. In ACL, pages 719\u2013730.\nGalen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In Proceedings of the 24th International Conference on Machine Learning, pages 33\u201340. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 2397\u20132430. PMLR. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS. Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. 2022a. Improving in-context few-shot learning via self-supervised training. In NAACL, pages 3558\u20133573. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen. 2023. Self-icl: Zero-shot incontext learning with self-generated demonstrations. In EMNLP, pages 15651\u201315662. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022b. Meta-learning via language model in-context tuning. In ACL, pages 719\u2013730.\nPradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In EMNLP, pages 5924\u2013 5931. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In AAAI. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL, pages 4171\u20134186. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234. Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia entities). In CIKM, pages 1625\u20131628. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In ACL, pages 3816\u20133830. Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. PPT: pre-trained prompt tuning for few-shot learning. CoRR, abs/2109.04332. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In ICLR. Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong Sun. 2022. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. In ACL, pages 2225\u20132240. Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. CoRR, abs/2205.12685. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In ICLR. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, pages 7871\u20137880.\nYuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Weining Qian, and Yunshi Lan. 2023. Prompting large language models with chain-of-thought for few-shot knowledge base question generation. In EMNLP, pages 4329\u20134343. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for gpt-3? In ACL, pages 100\u2013114. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021a. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. CoRR, abs/2107.13586. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: enabling language representation with knowledge graph. In AAAI, pages 2901\u20132908. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In ACL, pages 8086\u2013 8098. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-ICL: zero-shot in-context learning with pseudo-demonstrations. In ACL, pages 2304\u20132317. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022a. Metaicl: Learning to learn in context. In NAACL, pages 2791\u20132809. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? CoRR, abs/2202.12837. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In ACL, pages 3470\u20133487. Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. 2022. Knowledge-incontext: Towards knowledgeable semi-parametric language models. CoRR, abs/2210.16433. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. In ACL, pages 784\u2013789. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In NAACL, pages 2655\u20132671. Association for Computational Linguistics. Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan L. Boyd-Graber, and Lijuan Wang. 2022. Prompting GPT-3 to be reliable. CoRR, abs/2210.09150. Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. 2020a. Colake: Contextualized language and knowledge embedding. In COLING, pages 3660\u20133670. Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: enhanced representation through knowledge integration. CoRR, abs/1904.09223. Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020b. ERNIE 2.0: A continual pre-training framework for language understanding. In AAAI, pages 8968\u20138975. Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. 2019. Quartz: An open-domain dataset of qualitative relationship questions. In EMNLP, pages 5940\u20135945. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL-HLT, pages 4149\u20134158. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. CoRR, abs/2211.09085. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288.\nLaurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(11). Ellen M. Voorhees and Dawn M. Tice. 2000. Building a question answering test collection. In SIGIR, pages 200\u2013207. ACM. Jianing Wang, Nuo Chen, Qiushi Sun, Wenkang Huang, Chengyu Wang, and Ming Gao. 2023a. Hugnlp: A unified and comprehensive library for natural language processing. In CIKM, pages 5111\u20135116. ACM. Jianing Wang, Wenkang Huang, Qiuhui Shi, Hongbin Wang, Minghui Qiu, Xiang Li, and Ming Gao. 2022. Knowledge prompting in pre-trained language model for natural language understanding. CoRR, abs/2210.08536. Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and Ming Gao. 2023b. Boosting language models reasoning with chain-of-knowledge prompting. CoRR, abs/2306.06427. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. 2021a. K-adapter: Infusing knowledge into pre-trained models with adapters. In ACL, pages 1405\u20131418. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b. KEPLER: A unified model for knowledge embedding and pre-trained language representation. TACL, 9:176\u2013194. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves chain of thought reasoning in language models. In ICLR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903. Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, and Yu Qiao. 2023. Openicl: An open-source framework for in-context learning. In ACL, pages 489\u2013498.\nZiyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo, Runxin Xu, Songfang Huang, and Jun Huang. 2023. Making pre-trained language models end-to-end fewshot learners with contrastive prompt tuning. In WSDM, pages 438\u2013446. ACM. Junbing Yan, Chengyu Wang, Taolin Zhang, Xiaofeng He, Jun Huang, and Wei Zhang. 2023. From complex to simple: Unraveling the cognitive tree for reasoning with small language models. In EMNLP (Findings), pages 12413\u201312425. Association for Computational Linguistics. Jian Yang, Gang Xiao, Yulong Shen, Wei Jiang, Xinyu Hu, Ying Zhang, and Jinghui Peng. 2021. A survey of knowledge enhanced pre-trained models. CoRR, abs/2110.00269. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, pages 5754\u20135764. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022a. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068. Taolin Zhang, Junwei Dong, Jianing Wang, Chengyu Wang, Ang Wang, Yinghui Liu, Jun Huang, Yong Li, and Xiaofeng He. 2022b. Revisiting and advancing chinese natural language understanding with accelerated heterogeneous knowledge pre-training. In EMNLP, pages 560\u2013570. Association for Computational Linguistics. Taolin Zhang, Chengyu Wang, Nan Hu, Minghui Qiu, Chengguang Tang, Xiaofeng He, and Jun Huang. 2022c. DKPLM: decomposable knowledgeenhanced pre-trained language model for natural language understanding. In AAAI, pages 11703\u201311711. AAAI Press. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649\u2013657. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced language representation with informative entities. In ACL, pages 1441\u20131451. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022d. Automatic chain of thought prompting in large language models. CoRR, abs/2210.03493. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In ICLR.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023a. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. In ACL, pages 5823\u20135840. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b. A survey of large language models. CoRR, abs/2303.18223. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\n# A Details of Preliminary Experiments A.1 Details of Destruction Settings\n# A Details of Preliminary Experiments\n# A.1 Details of Destruction Settings\nFor our preliminary experiments, we selected 8 classification tasks and 4 question-answering tasks. The specifics of these datasets are detailed in Appendix B. To explore the influence of factual knowledge, we posit that entities (and their associated labels in text classification tasks) embody factual knowledge (Wang et al., 2021b, 2022, 2021a; Sun et al., 2019; Zhang et al., 2019). We identify all entities using the open-source TagMe entity linking tool7 (Ferragina and Scaiella, 2010). In the case of classification tasks, labels are treated as special types of entities. We follow the methodologies of Min et al. (2022b) and Kim et al. (2022) to create various destruction settings that either remove or replace entities (and labels), thereby demonstrating the impact of factual knowledge. Additionally, for each task, we randomly select K = 8 examples as in-context examples and concatenate them with each test example to form an input sequence, capped at a maximum sequence length of 256 tokens. With 5 different random seeds (i.e., 12, 24, 42, 90, and 100), each dataset yields 5 unique test results for a given LLM. Consequently, for each LLM, we collate 8 \u00d7 5 = 40 results for classification and 4 \u00d7 5 = 20 results for question-answering tasks. The aggregated results are presented in Figure 2, underscoring factual knowledge as a pivotal component in the performance of ICL.\n7https://sobigdata.d4science.org/ group/tagme\n# A.2 Details of Frequency Settings\nIn our preliminary assessment of label word frequency\u2019s impact, we focused on two wellestablished tasks: AGNews and TREC. Selecting K = 4 examples from the training corpus to construct the in-context prompt, we then used the remaining training examples as targets to generate predictions. Development or test sets were not utilized due to their insufficient scale for demonstrating frequency effects clearly. During prediction, we recorded the top-4 words with the highest prediction probabilities, facilitating the computation of frequency statistics for each label word. Figure 8 depicts the top-8 label word frequency statistics for each AGNews category. To examine frequency influences, we randomly selected two label words per frequency range (e.g., (0, 200], (200, 400], (400, 600], and > 600) for predictions. For instance, in AGNews, labels like \u201cteams\u201d and \u201cgroups\u201d could be chosen from the > 600 frequency region to represent the \u201csports\u201d category. Accordingly, we generated 24 = 16 and 26 = 64 permutations for AGNews and TREC, respectively. We report the average results using GPT-2 (urge) with 1.5B parameters and present the findings in box plot format in Figure 3.\n# A.3 Analysis of Knowledge Relevance in In-Context Examples\nOur preliminary experiments indicated that factual knowledge in selected in-context examples is crucial for ICL. To substantiate this, we conducted further analyses on two datasets, SST-2 and TREC. Employing our KER technique, we calculated a knowledge relevance score for each training example. For each defined score interval (i.e., (0, 15], (15, 30], (30, 45], (45, 60], (60, 75]), we sampled K = 4 examples to compose the in-context prompt. We then assessed the average performance across all 4! = 24 permutations for each interval and visualized the results in Figure 9. The findings corroborated the significance of selecting examples with high knowledge relevance for enhancing ICL performance.\n# B Details of the Corpus and Downstream Benchmarks\n# B.1 Corpora and Knowledge Base\nWe propose knowledgeable pre-training (KPT), which is similar to the current flourishing research\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b81/9b81cb51-a542-47dc-b0d8-9787a488825d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Label word frequency statistics for the AGNews dataset.</div>\nFigure 8: Label word frequency statistics for the AGNews dataset.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8a0f/8a0f884e-98fe-4234-94ef-e3d9ad7936ba.png\" style=\"width: 50%;\"></div>\nFigure 9: The 4-shot performance (%) with different knowledge relevance over SST-2 and TREC.\n<div style=\"text-align: center;\">Figure 9: The 4-shot performance (%) with different knowledge relevance over SST-2 and TREC.</div>\nof knowledge-enhanced pre-trained language models (KEPLMs) (Liu et al., 2020; Sun et al., 2019, 2020b; Wang et al., 2022). Different from them, we focus on auto-regressive PLMs, such as GPT2. We collect training corpora from Wikipedia (2020/03/01)8, and use WikiExtractor9 to process the pre-training data. The knowledge base (KB) G we choose is WikiData5M (Wang et al., 2021b), which is an urge-large structural data source based on Wikipedia. The entity linking toolkit we used is TagMe. In total, we have 3,085,345 entities and 822 relation types in G, and 25,933,196 training sentences. As mentioned above, KPT consists of three self-\n8https://dumps.wikimedia.org/enwiki/. 9https://github.com/attardi/ wikiextractor.\ntraining tasks, i.e., masked entity prediction, entity description generation, and knowledgeable question answering. For each task, we randomly select multiple sentences to form a training instance until reaching the maximum sequence length (i.e., 2048). Finally, we have sampled 100k training instances for each task. In average, we have 8 examples for each instance.\n# B.2 Downstream Task Datasets\nTo evaluate the effectiveness of our framework, we choose 8 text classification tasks and 4 question answering tasks. For the text classification, we directly choose 8 tasks from (Gao et al., 2021; Zhao et al., 2021). All the classification tasks involve sentiment analysis, natural language inference (NLI), question classification, and topic classification. For the question answering tasks, we choose four widely used tasks, including CommonsenseQA (ComQA) (Talmor et al., 2019), Quartz (Tafjord et al., 2019), SQuAD (Rajpurkar et al., 2018) and Quoref (Dasigi et al., 2019), where ComQA and Quartz are multi-choice QA, SQuAD and Quoref are extractive QA. The statistics of each dataset are shown in Table 6.\n# C Implementation Details\n# C Implementation Details C.1 Pre-training Details\n# C.1 Pre-training Details\nIn the pre-training stage, we choose different scales of GPT-2 (0.1B, 0.3B, 0.8B, 1.5B) (Brown et al., 2020) and OPT (Zhang et al., 2022a) (2.7B, 6.7B) from HuggingFace10 as the underlying LLMs. We do not use larger GPT-3 models because of the computation resource limitations. Because all three kinds of pre-training tasks share the same format, we can directly mix up all the pre-training examples to form a cross-task pre-training paradigm. We find that it is suitable for the LLM to learn crosstask knowledge. We train our model by AdamW algorithm with \u03b21 = 0.9, \u03b22 = 0.98. The learning rate is set as 1e-5 with a warm-up rate 0.1. We also leverage dropout and regularization strategies to avoid over-fitting. The models are trained on 8 NVIDIA A100-80G GPUs.\n# C.2 Prompting Details\nWe describe the implementation details with knowledgeable example retrieval (KER). Given a training\n10https://huggingface.co/transformers/ index.html.\nCategory\nDataset\n#Class\n#Train\n#Test\nType\nLabels (classification tasks)\nSST-2\n2\n6,920\n872\nsentiment\npositive, negative\nMRPC\n2\n3,668\n408\nparaphrase\nequivalent, not_equivalent\nMNLI\n3\n392,702\n9,815\nNLI\nentailment, neutral, contradiction\nText\nQNLI\n2\n104,743\n5,463\nNLI\nentailment, not_entailment\nClassification\nRTE\n2\n2,490\n277\nNLI\nentailment, not_entailment\nCB\n3\n250\n57\nNLI\nentailment, neutral, contradiction\nTREC\n6\n5,452\n500\nquestion cls.\nabbr., entity, description, human, loc., num.\nAGNews\n4\n120,000\n7,600\ntopic cls.\nworld, sports, business, technology\nComQA\n-\n9,741\n1,221\nmulti-choice\n-\nQuestion\nQuartz\n-\n2,696\n384\nmulti-choice\n-\nAnswering\nSQuAD\n-\n87,599\n10,570\nextractive QA\n-\nQuoref\n-\n19,399\n2,418\nextractive QA\n-\nTable 6: The statistics of multiple text classification and question answering datasets. Since the original test data i unavailable, we use the development sets as our test sets.\ndataset and a testing set, we aim to choose K examples from the training set which have a high knowledge relevant to all testing examples. To reach this goal, we utilize both Jaccard similarity and Euclidean distance in terms of pre-trained knowledge embeddings. For pre-trained knowledge embeddings, we choose the ConVE (Dettmers et al., 2018) algorithm to pre-train over wikidata5m and obtain the embeddings of entities and relations. We set its dimension as 768, the negative sampling size as 64, the batch size as 128 and the learning rate as 0.001. Finally, we only store the embeddings of all the entities. The KER algorithm for the prompting is shown in Algorithm 1.\n# C.3 Prediction Details\nWe first provide the details of the prompt formats and label mapping rules. Specifically, for the classification task, we need to define a template and label mapping to guide the model to generate results toward pre-defined classes. The prompt formats and label words are shown in Table 8. For the question answering task, we only need to define the template format, shown in Table 9. During the prediction, we calibrate the prediction probability. We thus provide the implementation details. We obtain a subset of training corpora from the KQA pre-training task, which consists of many question answer pairs. Thus, for each question, we can generate an answer (may be an entity or a label word) at the output position, and obtain the contextualized prior via Eq. 7. The value P(v) means the prior information of the generated entity or label word. Intuitively, if the value P(v) is higher, the entity or label word v is more likely\n<div style=\"text-align: center;\">Labels (classification tasks)</div>\nAlgorithm 1 Knowledgeable Example Retrieval Require: Training set Dtrn, Target (testing) set Dtgt, number of in-context examples K. 1: Randomly sampling a subset D\u2032 trn from Dtrn; 2: for each target example (Xtgt j ) \u2208Dtgt do 3: Extract entities Etgt j from this target example; 4: for each training example (Xtrn i , ytrn i ) \u2208 D\u2032 trn do 5: Extract entities Etrn i from this training example; 6: Calculate Jaccard similarity djac(i, j) and Euclidean distance dsem(i, j); 7: end for 8: Conditioning on the target example Xtgt j , obtain the knowledge relevance score d(Xtrn i , Xtgt j ) for the training example Xtrn i ; 9: end for 10: Calculate the final sampling weight s\u2032(Xtrn i ) for each training example Xtrn i in Eq. 4; 11: Sampling K training examples via the weight s\u2032(Xtrn i ); 12: return The selected K training examples.\n# 9: end for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6fad/6fadfbdc-32c9-46ae-a515-c29b2cd3e452.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Entity Description Generation (c) Knowledge Question Answe</div>\nFigure 10: The curves of the pre-training loss on GPT-2 (large) for each self-supervised learning task.\nFigure 10: The curves of the pre-training loss on GPT-2 (large) for each self-supervised learning task.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/697b/697b4ac8-84aa-4391-9e98-e4f6fbac188d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Question Answering</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e628/e6281962-c8ca-45c3-bb6b-9ca858430382.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Alpha ( )</div>\nFigure 11: The 8-shot performance (%) of GPT-2 (large) with different \u03b1 over text classification and question answering tasks.\nto be generated. We can save these prior values before prediction for downstream tasks. During the prediction, we can use the prior information of each pre-defined label word or entity to calibrate the prediction probability via Eq. 8.\n# D Analysis of Settings of Model Variants\nWe conduct some detailed analysis of our proposed technique.\nAnalysis of Pre-training Efficiency. To show the efficiency of pre-training, we choose GPT-2\nHyper-parameter\nValue\nBatch Size\n{2, 4, 8, 16, 32, 64}\nSeed\n{12, 24, 42, 90, 100}\nK\n{0, 1, 4, 8, 16}\n\u03b1\n{0.1, 0.3, 0.5, 0.7, 0.9}\n\u03b3\n{0.001, 0.01, 0.05, 0.1, 0.5, 1.0}\nTable 7: The searching scope for each hyper-parameter.\n(large) draw the pre-training loss for each selfsupervised learning task. From Figure 10, we can see that as the training process proceeds, each selfsupervised learning task has reached the convergence of the model through the entire pre-training process. Effectiveness of Hyper-parameters. In KICT, we investigate the effectiveness of the hyperparameter \u03b1 in KER, which aims to balance the relevance scores between Jaccard similarity and Euclidean distance. Results shown in Figure 11 demonstrate that the hyper-parameter \u03b1 is key to the performance. We can see that the suitable value is around 0.3. Effectiveness of the Template. We believe that the model performances rely on the format of the template, which has been investigated in (Liu et al., 2022; Min et al., 2022b). We choose some other templates for evaluation. For example, when we change the prefix string (e.g., \u201cQuestion:\u201d, \u201cAnswer:\u201d) to others (e.g., \u201cQ:\u201d, \u201cA:\u201d), the performance improvement of KICT is consistent. In addition, we also find that the text split character \u201c \\n\u201d between each sentence or example is important to support the generation, which is also found in (Dong et al., 2023; Andrew and Gao, 2007; Kim et al., 2022; Si et al., 2022).\nEffectiveness of the Template. We believe that the model performances rely on the format of the template, which has been investigated in (Liu et al., 2022; Min et al., 2022b). We choose some other templates for evaluation. For example, when we change the prefix string (e.g., \u201cQuestion:\u201d, \u201cAnswer:\u201d) to others (e.g., \u201cQ:\u201d, \u201cA:\u201d), the performance improvement of KICT is consistent. In addition, we also find that the text split character \u201c \\n\u201d between each sentence or example is important to support the generation, which is also found in (Dong et al., 2023; Andrew and Gao, 2007; Kim et al., 2022; Si et al., 2022).\nTask\nPrompt\nLabel Words\nSST-2\nReview: This movie is amazing!\nSentiment: Positive\nReview: Horrific movie, don\u2019t see it.\nSentiment:\nPositive, Nega-\ntive\nMRPC\nWhether the two questions are similar?\nQuestion 1: How much is this book? Question 2: How many books?\nOutput: No\nQuestion 1: Do you know the reason? Question 2: What\u2019s the reason?\nOutput:\nYes, No\nMNLI\nIs entailment, neutral, or contradiction between two texts?\nText 1: We sought to identify practices within the past 5 years. Text 2: We want to identify\npractices commonly used by agencies in the last 5 years.\nOutput: entailment\nText 1: yeah well you\u2019re a student right Text 2: Well you\u2019re a mechanics student right?\nOutput:\nentailment, neu-\ntral, contradic-\ntion\nQNLI\nWhether the answer is entailed to the question?\nText 1: In what year did the university first see a drop in applications? Text2: In the early\n1950s, student applications declined as a result of increasing crime and \u00b7 \u00b7 \u00b7\nOutput: Yes\nText1: When did Tesla move to Gospic? Text2: Tesla was the fourth of five children.\nOutput:\nYes, No\nRTE\nOthers argue that Mr. Sharon should have negotiated the Gaza pullout - both to obtain at\nleast some written promises of \u00b7 \u00b7 \u00b7\nQuestion: Mr. Abbas is a member of the Palestinian family. True or False?\nAnswer: False\nThe program will include Falla\u2019s \"Night in the Gardens of Spain,\" Ravel\u2019s Piano \u00b7 \u00b7 \u00b7\nQuestion: Beatrice and Benedict is an overture by Berlioz. True or False?\nAnswer:\nTrue, False\nCB\nBut he ended up eating it himself. I was reluctant to kiss my mother, afraid that somehow\nher weakness and unhappiness would infect me. \u00b7 \u00b7 \u00b7\nQuestion: her life and spirit could stimulate her mother. True, False, or Neither?\nAnswer: Neither\nValence the void-brain, Valence the virtuous valet. Why couldn\u2019t the figger choose his own\nportion of titanic anatomy to shaft? Did he think he was helping?\nQuestion: Valence was helping. True, False, or Neither?\nAnswer:\nTrue,\nFalse,\nNeither\nTREC\nClassify the questions based on whether their answer type is a Number, Location, Person,\nDescription, Entity, or Abbreviation.\nQuestion: How did serfdom develop in and then leave Russia?\nAnswer Type: Description\nQuestion: When was Ozzy Osbourne born?\nAnswer Type:\nNumber,\nLo-\ncation, Person,\nDescription,\nEntity, Abbrevi-\nation\nAGNews\nArticle: USATODAY.com - Retail sales bounced back a bit in July, and new claims for\njobless benefits fell last week, the government said Thursday, indicating \u00b7 \u00b7 \u00b7\nAnswer: Business\nArticle: New hard-drive based devices feature color screens, support for WMP 10.\nAnswer:\nWorld, Sports,\nBusiness, Tech-\nnology\nTable 8: The prompts used for text classification. We show one training example per task for illustration purpose The right column shows the label words (aiming to map the word to the original label class).\nTask\nPrompt\nComQA\nAnswer the question through multiple-choice.\nQuestion: When people want to watch a new move, the often go see it at the? (A) town (B) conference (C)\nbathroom (D) theater (E) train station\nAnswer: theater\nQuestion: Where is known to always have snow? (A) africa (B) north pole (C) roof (D) canada (E) surface\nof earth north pole\nAnswer:\nQuartz\nAnswer the question through multiple-choice.\nQuestion: Eric pushes an electron closer to the nucleus of an atom. The electron _____ energy.As you go\nfarther from the nucleus of an atom, the electron levels have more and more energy. (A) loses (B) gains\nAnswer: gains\nQuestion: When something is very lightweight what does it need to move?Objects with greater mass have\ngreater inertia. (A) more inertia (B) less inertia\nAnswer:\nSQuAD\nRead the question and find an answer in the context.\nQuestion: Where was the first figure skating championship held?\nContext: The tourism industry began in the early 19th century when foreigners visited the Alps, traveled to\nthe bases of the mountains to enjoy the scenery, and stayed at the spa-resorts. Large hotels were built during\nthe Belle \u00c9poque; cog-railways, built early in the 20th century, brought tourists to ever higher elevations,\nwith the Jungfraubahn terminating at the Jungfraujoch, well above the eternal snow-line, after going through\na tunnel in Eiger. During this period winter sports were slowly introduced: in 1882 the first figure skating\nchampionship was held in St. Moritz, and downhill skiing became a popular sport with English visitors\nearly in the 20th century, as the first ski-lift was installed in 1908 above Grindelwald.\nAnswer: St. Moritz\nQuestion:",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving in-context learning (ICL) in large language models (LLMs) by leveraging factual knowledge, which has been underexplored in previous research.",
        "problem": {
            "definition": "The problem is that existing methods for in-context learning do not adequately utilize factual knowledge, leading to suboptimal performance in tasks requiring few-shot learning.",
            "key obstacle": "The main challenge is the reliance on the intrinsic knowledge of LLMs without effectively incorporating external factual knowledge from selected examples."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that factual knowledge significantly impacts the performance of LLMs in ICL scenarios.",
            "opinion": "The proposed Knowledgeable In-Context Tuning (KICT) framework aims to enhance ICL by integrating factual knowledge during pre-training, prompting, and prediction.",
            "innovation": "The innovation lies in the introduction of specific techniques for knowledge injection, example selection, and prediction calibration, which differ from traditional ICL approaches."
        },
        "method": {
            "method name": "Knowledgeable In-Context Tuning",
            "method abbreviation": "KICT",
            "method definition": "KICT is a framework designed to improve the performance of in-context learning by effectively leveraging factual knowledge throughout the learning process.",
            "method description": "The KICT framework enhances ICL by incorporating knowledgeable pre-training, example retrieval, and prediction calibration.",
            "method steps": [
                "Inject knowledge into LLMs during continual self-supervised pretraining.",
                "Select in-context examples with high knowledge relevance.",
                "Calibrate prediction results based on prior knowledge."
            ],
            "principle": "KICT is effective because it systematically integrates factual knowledge into the ICL process, addressing the limitations of existing methods."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using various scales of GPT-2 and OPT models over multiple text classification and question-answering tasks, utilizing datasets from HugNLP.",
            "evaluation method": "The performance of the KICT framework was assessed by comparing it against strong baseline methods across different tasks, measuring improvements in accuracy and effectiveness."
        },
        "conclusion": "The experimental results demonstrate that the KICT framework achieves substantial improvements over baseline methods, confirming the importance of integrating factual knowledge in ICL.",
        "discussion": {
            "advantage": "The key advantages of KICT include enhanced performance in ICL tasks and the ability to effectively utilize factual knowledge, leading to more accurate predictions.",
            "limitation": "The method currently focuses on decoder-only LLMs and may not generalize to encoder-decoder architectures without further adaptation.",
            "future work": "Future research will explore the reasoning capabilities of KICT and its application to encoder-decoder models, as well as addressing potential biases in LLMs."
        },
        "other info": {
            "acknowledgements": "This work was supported by various grants and initiatives, including the National Natural Science Foundation of China and the Alibaba Innovation Research Program.",
            "limitations": "The study is limited by computational constraints, as it does not evaluate ultra-large LLMs over 10 billion parameters."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of improving in-context learning (ICL) in large language models (LLMs) by leveraging factual knowledge, which has been underexplored in previous research."
        },
        {
            "section number": "1.3",
            "key information": "The proposed Knowledgeable In-Context Tuning (KICT) framework aims to enhance ICL by integrating factual knowledge during pre-training, prompting, and prediction."
        },
        {
            "section number": "2",
            "key information": "The problem is that existing methods for in-context learning do not adequately utilize factual knowledge, leading to suboptimal performance in tasks requiring few-shot learning."
        },
        {
            "section number": "3.1",
            "key information": "KICT is effective because it systematically integrates factual knowledge into the ICL process, addressing the limitations of existing methods."
        },
        {
            "section number": "4.1",
            "key information": "The innovation lies in the introduction of specific techniques for knowledge injection, example selection, and prediction calibration, which differ from traditional ICL approaches."
        },
        {
            "section number": "6.1",
            "key information": "The method currently focuses on decoder-only LLMs and may not generalize to encoder-decoder architectures without further adaptation."
        },
        {
            "section number": "7",
            "key information": "The experimental results demonstrate that the KICT framework achieves substantial improvements over baseline methods, confirming the importance of integrating factual knowledge in ICL."
        }
    ],
    "similarity_score": 0.7389757128310606,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Knowledgeable In-Context Tuning_ Exploring and Exploiting Factual Knowledge for In-Context Learning.json"
}