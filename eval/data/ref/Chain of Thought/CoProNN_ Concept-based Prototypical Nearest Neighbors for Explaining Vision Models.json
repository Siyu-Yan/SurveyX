{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.14830",
    "title": "CoProNN: Concept-based Prototypical Nearest Neighbors for Explaining Vision Models",
    "abstract": "Mounting evidence in explainability for artificial intelligence (XAI) research suggests that good explanations should be tailored to individual tasks and should relate to concepts relevant to the task. However, building task specific explanations is time consuming and requires domain expertise which can be difficult to integrate into generic XAI methods. A promising approach towards designing useful task specific explanations with domain experts is based on compositionality of semantic concepts. Here, we present a novel approach that enables domain experts to quickly create concept-based explanations for computer vision tasks intuitively via natural language. Leveraging recent progress in deep generative methods we propose to generate visual concept-based prototypes via text-to-image methods. These prototypes are then used to explain predictions of computer vision models via a simple k-Nearest-Neighbors routine. The modular design of CoProNN is simple to implement, it is straightforward to adapt to novel tasks and allows for replacing the classification and text-to-image models as more powerful models are released. The approach can be evaluated offline against the ground-truth of predefined prototypes that can be easily communicated also to domain experts as they are based on visual concepts. We show that our strategy competes very well with other concept-based XAI approaches on coarse grained image classification tasks and may even outperform those methods on more demanding fine grained tasks. We demonstrate the effectiveness of our method for human-machine collaboration settings in qualitative and quantitative user studies. All code and experimental data can be found in our GitHub $\\href{https://github.com/TeodorChiaburu/beexplainable}{repository}$.",
    "bib_name": "chiaburu2024copronnconceptbasedprototypicalnearest",
    "md_text": "# CoProNN: Concept-based Prototypical Nearest Neighbors for Explaining Vision Models\nTeodor Chiaburu1[0009\u22120009\u22125336\u22122455], Frank Hau\u00dfer1[0000\u22120002\u22128060\u22128897], and Felix Bie\u00dfmann1,2[0000\u22120002\u22123422\u22121026]\n1 Berliner Hochschule f\u00a8ur Technik, Berlin, Germany {chiaburu.teodor;frank.hausser;felix.biessmann}@bht-berlin.de 2 Einstein Center Digital Future, Berlin, Germany\nAbstract. Mounting evidence in explainability for artificial intelligence (XAI) research suggests that good explanations should be tailored to individual tasks and should relate to concepts relevant to the task. However, building task specific explanations is time consuming and requires domain expertise which can be difficult to integrate into generic XAI methods. A promising approach towards designing useful task specific explanations with domain experts is based on compositionality of semantic concepts. Here, we present a novel approach that enables domain experts to quickly create concept-based explanations for computer vision tasks intuitively via natural language. Leveraging recent progress in deep generative methods we propose to generate visual concept-based prototypes via text-to-image methods. These prototypes are then used to explain predictions of computer vision models via a simple k-NearestNeighbors routine. The modular design of CoProNN is simple to implement, it is straightforward to adapt to novel tasks and allows for replacing the classification and text-to-image models as more powerful models are released. The approach can be evaluated offline against the ground-truth of predefined prototypes that can be easily communicated also to domain experts as they are based on visual concepts. We show that our strategy competes very well with other concept-based XAI approaches on coarse grained image classification tasks and may even outperform those methods on more demanding fine grained tasks. We demonstrate the effectiveness of our method for human-machine collaboration settings in qualitative and quantitative user studies. All code and experimental data can be found in our GitHub repository: https://github.com/TeodorChiaburu/beexplainable.\nKeywords: XAI \u00b7 visual concepts \u00b7 prototypes \u00b7 nearest neighbors\n# 1 Introduction\nEmpirical evidence in the field of eXplainable Artificial Intelligence (XAI) suggests that good explanations for Machine Learning (ML) predictions should be designed for each task individually [35,26,12,20]. This requires understanding of both the ML model and the domain of the respective task. These findings\nhighlight the importance of human experts in the development of XAI methods. Integrating knowledge of domain experts in XAI methods is challenging. Not only because of potential cognitive bias [3], but also because of the time needed to design task specific XAI methods. Especially when dedicated training and expert knowledge is needed for a task, such as in classification problems with large label spaces, rare classes and small differences between classes, it is often difficult to scale the human part in the XAI algorithm development. To address these challenges, we propose a novel approach to XAI by leveraging recent advancements in multimodal foundation models and deep generative models. In order to enable human experts to easily develop explanations in natural language, we make use of text-to-image methods, in particular Stable Diffusion [39]. Importantly, the proposed approach allows to improve the efficiency of human experts when developing explanations for a novel task and when evaluating individual XAI methods. We demonstrate the effectiveness of our approach in experiments on heterogeneous tasks requiring domain expertise for designing explanations. Our results show that CoProNN is simple to adapt to a broad range of tasks. Compared to existing similar XAI approaches our methodology compares favourably both in terms of offline metrics as well as in qualitative and quantitative user studies. We note that we aimed at embodying into our method the benefits of two worlds: concept- and prototype-based explanations. As discussed in the following sections, the common approach for concept-related methods is to infer semantic significance from a pool of many general pre-annotated concepts, while prototypical models search for representative fragments/patches in the training samples that are meant to capture the defining traits of the class prototypes. To the best of our knowledge, no other XAI framework is currently designing task-specific prototypes separate from the training set, based on higher level concepts.\n# 2 Related Work\nEarly attempts at post-hoc XAI approaches focused on feature attribution maps that identify relevant pixels in the input image [43,46,44,38,42] or visualizing what features the network learns during training [29]. These methods have the key advantage that they do not require task specific adaptations. As such, these methods are simple to scale to a large number of tasks with minimal manual effort. On the other hand, they can be difficult to use in practice, as they rarely provide useful explanations other than some localization of features relevant for a given prediction. Another challenge with most attribution based post-hoc explanation methods is that attribution maps are often noisy, non-robust and unfaithful to the predictive behavior of the model [31,22,19,4]. Complementing this work, other researchers have focused on concept-based explanations, leveraging principles of semantic compositionality of intuitive visual concepts [18,11,7,25,10,51,37], prototypes [6,9,28,40], examples from the training set [24,50,45] or captioning [15,41], in order to generate explanations that are comprehensible for human subjects. Here, we build on this work in order to com-\nbine the scalability of post-hoc attribution methods with the intuitive comprehensibility of semantic explanations by leveraging recent progress in generative deep learning methods. Our contribution is similar to prior work on prototype methods and concept-based methods. Prototype methods usually search for prototypes or prototypical parts in the training set of the classifier and often aim at upgrading the given classifier into an interpretable model by retraining it. This retraining step is usually not necessary for post-hoc explanation methods, like CoProNN. Another disadvantage of classical prototype methods is that integrating task relevant concepts is often not straightforward. Such concepts, however, can be useful if not required to relate an explanation to expert knowledge for a given task. Two popular post-hoc methods to integrate domain expertise via task relevant concepts into explanations are Testing with Concept Activation Vectors (TCAV) [18] and Interpretable Basis Decomposition (IBD) [51]. TCAV fits a linear classifier in one of the model\u2019s layers, that separates a concept\u2019s examples from random counterexamples. Hence, the CAVs are the vectors orthogonal to these decision boundaries. To quantify how sensitive the class prediction is towards each concept, TCAV computes the directional derivatives of the class members w.r.t. the CAVs multiple times for different partitions of the random dataset. The TCAV score for a whole class w.r.t. a concept is defined as the fraction of class members having positive directional derivatives throughout all of that concept\u2019s CAVs. For a single sample, the score would only count the positive derivatives throughout the concept\u2019s CAVs (not class-wise). IBD decomposes the neural activations of the input into semantically interpretable components. It achieves this by fitting logistic regressors to separate the concept examples from counterexamples. The resulting \u2019concept weight vectors\u2019 will construct a concept basis onto which the rows in the Dense layer matrix responsible for predicting the class probabilities (called \u2019class weight vectors\u2019) are projected. The resulting class concept scores represent the concept contributions to the prediction of a classifier. The IBD concept relevance scores for a single sample are the dot products between the sample\u2019s feature vector and each of the concept components that approximate its class weight vector, see Appendix 6.\n# 3 Methods\nIn this section we first describe the overall workflow of the proposed approach followed by a more detailed description of the key components and conclude with presenting the datasets and evaluation methods.\n# 3.1 CoProNN\nFigure 1 depicts the inner workings of the CoProNN approach, which consists of two main stages:\nFigure 1 depicts the inner workings of the CoProNN approach, which consists of two main stages: 1) Training the Classifier: A Deep Neural Network (DNN) is trained (or a standard computer vision backbone is fine tuned) on the given dataset of\n1) Training the Classifier: A Deep Neural Network (DNN) is trained (or a standard computer vision backbone is fine tuned) on the given dataset of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/87c0/87c061d3-e6f6-46d1-9799-112c627453ef.png\" style=\"width: 50%;\"></div>\nFig. 1: Overview of the CoProNN framework, illustrated on the subset of wild bee images scraped from iNaturalist. Classifier (blue boxes): images of the five pre-selected species are fed along with their labels assigned by entomologists into a standard DNN. Concept based explanations (red boxes): domain experts define a set of intuitive concepts to discriminate the five species in a hierarchical manner (the decision tree block). These concepts are used as prompts to generate prototype images with Stable Diffusion. The prototype images along with a set of random images are mapped into the latent feature space of the DNN\u2019s frozen backbone, where they will be fed into kNN. At inference (bottom row): the DNN classifier predicts a species label and an explanation is retrieved based on the kNN-computed similarities between the extracted features of the test sample and the prototype images in the latent space.\nimages until convergence. For our experiments, we have used a ResNet50 [14], but any DNN able to extract image features can be inserted. More details about the training process can be found in the Appendix 6. 2) Fitting the Post-Hoc Explanation Method: Depending on the knowledge and skills required to solve the classification task, domain experts define a minimal set of relevant concepts a layman would need to be able to classify the images. For example, for our wild bee dataset, entomologists came up with the simple taxonomic tree sketched in Figure 1. The nodes represent intuitive visual concepts related to the color and the texture of the bees\u2019 body parts. In a hypothetical medical application, radiologists could similarly draw a decision tree to classify different types of cancerous cells, using features such as cell shape or size. Once the concepts are agreed upon, they are turned into prompts fed into a generative text-to-image model that produces prototypical images for each concept. These prototypes are passed\nthrough the frozen backbone of the previously trained DNN and mapped into its latent feature space. A set of random images is similarly transformed and fed along with the prototypical vectors into a kNN. The check against random images ensures that CoProNN is also able to predict the absence of all the concepts in an image.\nThe explanations are generated post-hoc by comparing the test sample in the feature space of the classifier against the prototype and the random vectors. The kNN probabilities are interpreted as relevance scores for the prototypes. Concepts associated with a prediction are presented to the end-user next to the DNN label prediction. The general explanation format is, hence: \u201dThis image is class A, because concepts X, Y ... are present and concepts Z, W ... are absent\u201d. To expand the explanation modality, the textual formulation can be accompanied by one prototype image per concept. More details on the generation of the prototypes and the prediction of the explanation follow in the next subsections.\n# 3.2 Prototype Images via Stable Diffusion\nFor creating the prototypical images characterized by representative concepts, a Stable Diffusion (SD) v1.5 model from the diffusers library [1] in PyTorch was used. A key advantage of modern generative models is that these concepts can be retrieved by human domain experts using short natural language prompts for the diffusion model. For instance, a well known animal such as cheetah is simple to describe with intuitive concepts i.e. \u2019animal with yellow fur and black dots\u2019. When transitioning to a more fine-grained context, expert knowledge will often be required. For example, the basic concepts to distinguish the five wild bees (Fig. 7) were proposed by the entomologist we collaborated with. By following his suggestions and the prompting guidelines in [49,34,32], we came up with the successful prompts for generating the prototypes. Their shorthand version can be found in Fig. 2. More details on the extra prompt modifiers that we used to increase image resolution and fidelity can be found in our code repository.\n# 3.3 Nearest Neighbors as Explanations\nWe leverage k Nearest Neighbors (kNN) in the feature space of a DNN classifier to retrieve the concepts relevant to class prediction. Other XAI methods [30,17,33] also make use of kNN to explain a model\u2019s decision. However, in contrast to CoProNN, their search space for explanations resides in the training set of images the classifier was trained on. Algorithm 1 in the Supplement describes our approach to formulating explanations. For each concept j, 1 \u2264j \u2264m, all prototype images obtained via SD as described in Section 3.2 along with a set of random images are mapped into the feature space of the last layer of the trained DNN classifier. We denote by \u2126j the set of feature vectors of the prototype images for concept j. Given a new image x, we first compute a feature vector f(x) (the activation of the last layer of the DNN for image x) and pass this feature vector to the\nkNN model trained on the concept prototype images to obtain the likelihood of that feature vector f(x) being associated with each concept image set \u2126j. In order to improve the robustness of the kNN model we also include a set of random images of the same size as each of the prototype image sets. We randomly sample the set of random images as a subset/partition of a large image set, kNN is run for all the different random partitions and the scores pj are averaged. While this inclusion of a random concept image set would not be required, we perform this step for the sake of a fair comparison with the TCAV and IBD studies. Both of these methods we compare with incorporate a set of counterexamples paired with positive concept examples. To decide which of the m concepts are shown to the end-user as relevant, either the top-N predicted concepts (sorted according to their kNN scores p) are selected, or a threshold t \u2208(0, 1] is defined; only prototype sets for which pj \u2265t would then be considered for formulating an explanation of the type \u201dthe model predicts class A because concept X is present and concept Y is absent.\u201d Note that the absence of a concept is also of importance for hierarchical classification routines like the decision tree drawn in Figure 1. The decision for top-N or threshold based concept predictions is similar to that of multiclass vs multilabel paradigms. CoProNN can be readily adapted to both settings, depending on the application specific requirements, such as the question of how many concepts should be used for an explanation.\n# 3.4 Coarse and Fine Grained Classification Tasks\nTo illustrate the effectiveness of CoProNN to generate task-specific explanations via concepts relevant to a given domain we employ three different data sets:\n1) Animals from ImageNet [8]: only a subset of easily distinguishable classes was downloaded: cheetah, garter snake, tiger, mud turtle, zebra (150 images each). The choice of these classes was inspired by the experiments carried out in the paper introducing TCAV [18], where the authors tested their algorithm on straightforward concepts such as \u2019striped\u2019, \u2019dotted\u2019 or \u2019chequered\u2019. These concepts also made the key words in the prompts for generating the concept based prototypes (one prompt per class, see Fig. 2 (a) - (e)). 2) Wild Bees from iNaturalist [2]: 30k images of the top 25 most frequent wild bee species with their natural habitat in Germany were scraped from the iNat online database3. To keep label noise in check, only images awarded a \u2019research grade\u2019 (and possessing a CC-BY-NC copyright license) were downloaded. The 25 species were identified in collaboration with our entomologist. After preliminary experiments, we have reduced the subset down to five particularly difficult and frequently confounded species: Andrena bicolor/flavipes/fulva and Bombus lucorum/pratorum (see Fig. 7). This resulted in a dataset totalling 7595 images of wild bees. The concept keywords used to distinguish the five bee species (also highlighted in the decision tree in Fig. 1) are: 3 https://www.inaturalist.org/\n https://www.inaturalist.org/\n* \u2019fuzzy orange\u2019 - A. fulva (completely orange), A. bicolor (orange thorax) and B. pratorum (orange sting region) * \u2019fuzzy yellow with black stripes\u2019 - both Bombus on thorax and abdomen * \u2019smooth shiny dark brown\u2019 - A. bicolor and A. flavipes on abdomen. The prompts for generating the corresponding concept based prototypes and some examples are given in Fig. 2 (f) - (h). ) Broden [5]: the standard dataset of pre-annotated concept images; used for comparison against our approach defining task-specific concepts and for sampling the set of random images (1000 samples).\n* \u2019fuzzy orange\u2019 - A. fulva (completely orange), A. bicolor (orange thorax) and B. pratorum (orange sting region) * \u2019fuzzy yellow with black stripes\u2019 - both Bombus on thorax and abdomen * \u2019smooth shiny dark brown\u2019 - A. bicolor and A. flavipes on abdomen. The prompts for generating the corresponding concept based prototypes and some examples are given in Fig. 2 (f) - (h). 3) Broden [5]: the standard dataset of pre-annotated concept images; used for comparison against our approach defining task-specific concepts and for sampling the set of random images (1000 samples).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9e56/9e56b0ef-6cff-406f-8adc-3ff1f736df59.png\" style=\"width: 50%;\"></div>\nFig. 2: Examples of prototype images generated by SD along with their (abbreviated) prompts underneath. Prototypes (a) to (e) correspond to the ImageNet animal classes, prototypes (f) to (h) to the iNat wild bees.\n<div style=\"text-align: center;\">Fig. 2: Examples of prototype images generated by SD along with their (abbreviated) prompts underneath. Prototypes (a) to (e) correspond to the ImageNet animal classes, prototypes (f) to (h) to the iNat wild bees.</div>\nNote the distortions and unrealistic appearance of the examples in Figure 2. To fix this, one could simply generate concept images by using very straightforward prompts such as \u201dzebra, highly detailed\u201d. The generated images would resemble very much real images of the considered class, improving the kNN search. However, this would miss the point of our work. The aim here is to show that using concept key words such as \u201dwhite animal with black stripes\u201d and generating not perfectly looking prototypes allows our method to generalize better without sacrificing accuracy (more details in Section 4). In the case of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3ed/b3ed3b1d-eb78-44bc-9623-686100412028.png\" style=\"width: 50%;\"></div>\n(h) fuzzy dark orange bee\nfiner grained classes, such as the wild bees, a text-to-image model would struggle to understand a prompt devoid of context such as \u201dAndrena fulva\u201d (this would just generate a common honey bee).\n# 3.5 Evaluation without Humans-in-the-Loop\nAll XAI methods considered in this work output a vector of relevance scores for every concept. One key advantage of concept-based explanations is that they can be tested against ground-truth labels of concepts associated with a category. As an example, we know that zebras are white with black stripes or that A. bicolor bees are both fuzzy orange and shiny dark brown. We can, therefore, encode the true explanation labels as a binary vector, so that, for instance, an A. bicolor bee is represented as a vector [1, 0, 1] - translated as IS fuzzy orange, IS NOT fuzzy yellow, IS shiny brown. To investigate to what extent the predicted concept relevance vectors match the ground truth concept-label association, we measure their similarity via Cosine Similarity4 (CS) sample-wise and report the averaged values per class. We are using CS for comparing the methods (and not Euclidean distance, for instance) so that only the direction between the true and predicted explanation vectors is taken into consideration. Magnitude should not play a role, since i.e. scores in the TCAV explanation vectors can each range between 0 and 1, while scores in the CoProNN and IBD explanations range cumulatively (when added up) from 0 to 1 (see Fig. 3).\n# 3.6 Quantitative User Study\nWe evaluated whether our proposed explanation method facilitates human-machine collaboration in a web based user study using jspsych [27]. The app was deployed on the platform https://www.cognition.run; a total of 80 subjects took part in our investigation. The participants were informed that the data gathered would solely be used for research purposes. The subjects\u2019 identities were anonymous and before starting they were given a detailed description of what they were required to do. The experiment was approved by our institutional research board. Subjects were randomly divided into two groups: Users in the Control Group would only receive the model predictions as an aid for solving the tasks, but not the explanations, and users in the CoProNN Group could see the model predictions as well as the CoProNN explanation for the prediction. The task interface for the CoProNN users is shown in Fig. 7 in the Supplement; a demo can be inspected at: https://hgyl4wmb2l.cognition.run. Users are presented with a random selection of 10 out of a pool of 24 image samples from the iNat wild bee dataset. The images were selected such that every user would be shown 2 images wrongly classified by our model (out of 4 total misclassifications in the pool of 24). The remaining 8 images were samples correctly classified by\n4 https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n4 https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\nour model. The 4 misclassified samples were also chosen such that the CoProNNexplanation would match the wrong species predicted by the model. Since the wild bees can have more than one correct concept label - in the case of A. bicolor and B. pratorum - a threshold t = 0.4 was chosen to decide which concepts to show the users (refer to Algorithm 1). With k = 10 neighbors to choose from5 and m = 3 concepts competing against one another, the number 0.4 = 1 k\u2308k m\u2309 appeared to be a good default threshold. Based on a pilot study, we determined a lower bound of 4 images, corresponding to the 20% percentile in the distribution of correct answers, that should be classified correctly by each user. Subjects who did not meet this requirement did not qualify for analysis of the user study. In total, this resulted in 41 subjects in the Control Group and 34 in the CoProNN Group.\n# 3.7 Qualitative User Study\nTo better understand qualitatively how subjects used and perceived the explanations provided, we asked users in the CoProNN Group to fill in a standard survey, the System Causability Scale [16]. Subjects in the CoProNN Group were asked to answer the following questionnaire items with one of five responses that range from \u2019strongly agree\u2019 to \u2019strongly disagree\u2019:\n1. I did not need support to understand the model\u2019s explanations. 2. I found the model\u2019s explanations helped me to understand causality. 3. I was able to use the model\u2019s explanations with my knowledge base. 4. I think that most people would learn to understand the model\u2019s explanations very quickly.\n# 4 Results\nIn this section we describe the results from our quantitative and qualitative comparisons to existing concept-based XAI methods (TCAV and IBD). More details on how exactly TCAV and IBD were applied to our use-cases can be found in the Supplement 6.\n# 4.1 Explanations via Task-Specific Concept-Based Prototypes\nWe formulated higher-level concept-based prototype images in SD, tailored specifically for the tasks at hand. Figure 2 shows examples of such prototypes for every class in the two datasets along with the (abbreviated) prompts used to generate them in SD. The results in Figure 3 demonstrate that prototypical images capturing higher-level concepts enable CoProNN to retrieve more faithful and robust explanations6 than TCAV and IBD. Ideally, the ground-truth concept(s) for every\nWe formulated higher-level concept-based prototype images in SD, tailored specifically for the tasks at hand. Figure 2 shows examples of such prototypes for every class in the two datasets along with the (abbreviated) prompts used to generate them in SD. The results in Figure 3 demonstrate that prototypical images capturing higher-level concepts enable CoProNN to retrieve more faithful and robust explanations6 than TCAV and IBD. Ideally, the ground-truth concept(s) for every 5 Value set before hypertuning, more details in the Appendix. 6 We have verified the robustness and faithfulness of our explanations by running multiple iterations w.r.t. various random partitions. More advanced techniques such as adversarial attacks could also be employed.\n5 Value set before hypertuning, more details in the Appendix. 6 We have verified the robustness and faithfulness of our explanations by running multiple iterations w.r.t. various random partitions. More advanced techniques such as adversarial attacks could also be employed.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7866/786650de-523a-4c84-93f8-351ee33bafe5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) CoProNN</div>\n<div style=\"text-align: center;\">(e) TCAV</div>\nFig. 3: CoProNN finds the relevant concepts for discriminating categories better than alternative XAI methods (TCAV and IBD). Top row (plots (a) to (c)) depicts the five ImageNet animal classes. The expected explanations are: \u201dyellow fur black dots\u201d for cheetah, \u201delongated chequered\u201d for snake, \u201dorange fur black stripes\u201d for tiger, \u201dchequered shell\u201d for turtle and \u201dwhite black stripes\u201d for zebra. Bottom row (plots (d) to (f)) refers to the iNat wild bees. The expected explanations are (in shorthand): \u201dorange\u201d and \u201dbrown\u201d for A. bicolor, \u201dbrown\u201d for A. flavipes, \u201dorange\u201d for A. fulva, \u201dyellow\u201d for B. lucorum, \u201dyellow\u201d and \u201dorange\u201d for B. pratorum. A diamond \u22c4marks a zero-value. All the plots display the concept relevance scores averaged per class. An extra note on the normalization of the IBD scores can be found in the Appendix and the corresponding concept contribution scores for the whole classes are given in Fig. 8. Our CoProNN method identifies relevant concepts with high certainty in both datasets, which is made clear by the peaks in plots (a) and (d).\nspecies should achieve high relevance scores and should consistently stand out as the most relevant of the predicted concepts. Note that IBD allows for negative relevance scores, meaning that the concept is considered irrelevant for that class; for the sake of comparability we mapped IBD negative scores to 0 when displaying them in Figures 3, 9 and computing the metrics in Tables 1, 2. The ImageNet classes, indeed, exhibit high relevance scores when explained by CoProNN and IBD (see Fig. 3 a, c), with CoProNN attaining higher certainty in assigning scores to the relevant concepts. On the other hand, TCAV-computed relevance scores do not single out relevant concepts for any of the classes, oftentimes even predicting the wrong concept (see Fig. 3 b).\n<div style=\"text-align: center;\">(c) IBD</div>\n<div style=\"text-align: center;\">(f) IBD</div>\nAlso on the wild bee dataset, CoProNN assigns high relevance scores to the correct concepts (see Fig. 3 d). We notice, however, that species requiring more than one concept to be uniquely characterized remain difficult to fully explain: A. bicolor, which is both fuzzy orange and shiny dark brown and B. pratorum, which is mainly fuzzy yellow with black stripes, but also has a fuzzy orange terminal segment on the abdomen. For both of them, CoProNN confidently predicts only one of the concepts. For A. bicolor, TCAV does assign more importance to the secondary concept \u2019fuzzy orange\u2019 but also to the incorrect concept \u2019fuzzy yellow\u2019. This culminates in predicting with highest confidence the single wrong available concept for B. pratorum, namely \u2019shiny brown\u2019. IBD has the most difficulty in recognizing relevant concepts for the wild bees. The CS values in Table 1 stem from comparing the ground truth concepts associated with a class with the predicted concept relevance scores. CoProNN compares very well against TCAV and IBD on the animal classes from the standard (overfitted by many XAI methods) coarse grained classification dataset ImageNet (all scores above 0.9). On more difficult fine grained tasks in the wild bee dataset, our method outperforms the competitors in explaining most classes.\nCosine Similarity\nClass\nCoProNN\nTCAV\nIBD\nCheetah\n0.9177 \u00b1 0.2126\n0.2306 \u00b1 0.001\n1.0 \u00b1 0.0\nSnake\n0.9956 \u00b1 0.022\n0.5101 \u00b1 0.001\n0.9986 \u00b1 0.0017\nTiger\n0.9715 \u00b1 0.1509\n0.3953 \u00b1 0.001\n0.9532 \u00b1 0.3013\nTurtle\n0.9589 \u00b1 0.103\n0.2203 \u00b1 0.001\n1.0 \u00b1 0.0\nZebra\n0.9364 \u00b1 0.1022\n0.0709 \u00b1 0.001\n1.0 \u00b1 0.0\nA. bicolor\n0.7831 \u00b1 0.0763\n0.7142 \u00b1 0.1527\n0.66 \u00b1 0.2539\nA. flavipes\n0.9926 \u00b1 0.0043\n0.667 \u00b1 0.3386\n0.6667 \u00b1 0.7454\nA. fulva\n0.7373 \u00b1 0.2118\n0.9223 \u00b1 0.1183\n0.9247 \u00b1 0.3596\nB. lucorum\n0.7929 \u00b1 0.2027\n0.6549 \u00b1 0.0827\n0.7644 \u00b1 0.4016\nB. pratorum\n0.7639 \u00b1 0.1412\n0.5882 \u00b1 0.144\n0.66 \u00b1 0.2539\nTable 1: CoProNN reliably finds concepts relevant for a given class. Cosine similarity (CS) is computed between the true concept labels and the predicted concept relevance scores from each method. A high CS outlines a concept relevance vector very similar to the concept ground-truth. CoProNN outperforms competitors TCAV and IBD on 4 out of 5 wild bee classes and achieves very high scores on the ImageNet animal classes (above 0.9, at least second best).\n# 4.2 Explanations via Task-Unspecific Concepts\nFor the ImageNet animal classes described in Section 3.4 we also investigated the efficiency of pre-annotated generic and task unspecific concept images from the broden dataset when computing explanations with our method, TCAV and IBD. Deeper scrutiny of the broden concepts revealed that they were sampled from very different distributions. While the concept set for \u2019striped\u2019 consisted of image patches from the animal world (tiger stripes, fish stripes and so on), the sets for \u2019dotted\u2019 and \u2019chequered\u2019 were either images of clothing, materials or generated with a drawing software. This makes it difficult for the concepts to faithfully represent task relevant features in a given data set or task. To illustrate this effect, we also generated concepts similar to the generic broden concepts using their label names as prompts. Yet, these concepts also proved inappropriate for capturing the semantic essence of the animal classes. The results are summarised in Fig. 9 and Table 2 from the Supplement. They confirm previous findings [36] highlighting that concept-based explanation methods are sensitive to the concept dataset and that different probe datasets can yield very different explanations.\n# 4.3 Quantitative User Study\nWhen analyzing the performance in both groups, we notice higher accuracy in the CoProNN group than in the Control Group (see Fig. 4). For orientation, 79% of the CoProNN participants gave at least 8 correct answers, as opposed to only 63% in the Control Group. On average, CoProNN users had an accuracy of 8.24 \u00b1 1.28 correct clicks (median at 9 correct clicks), while Control users had an accuracy of 8.05 \u00b1 1.64 (median at 8 correct clicks). Recall that two of the ten samples shown to the participants were misclassified by the model. Therefore, users in the Control Group would receive a misleading species suggestion, while CoProNN users would also be shown a misleading explanation for that wrong prediction. We observe an average of 0.59 \u00b1 0.59 clicks on the wrong model suggestion in the Control Group (median at 1 click) and 0.47 \u00b1 0.61 clicks in the CoProNN Group (median at 0). Users in the Control Group agreed on 29.27% of the images with wrong model predictions and users from the CoProNN Group with only 23.53%. This demonstrates that CoProNN explanations enable users to identify wrong model predictions.\n# 4.4 Results Qualitative User Study\nThe results of the survey with 34 participants shown in Figure 5 demonstrate that subjects generally found the explanations delivered by our model helpful and easy to understand. 58.82% claimed that they understood them without further instructions. 61.76% could grasp causal relationships through the explanations i.e. \u201dif concept X and Y, but not Z, then species A\u201d; roughly a quarter of the respondents found that difficult. 61.76% were able to use the explanations with their prior knowledge (about basic taxonomies in nature) and about two thirds agree that the explanations are accessible to learn by most non-experts (in AI or entomology).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d0fe/d0fe1e85-ea31-495d-8d43-4db7fc86e4eb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFig. 4: Explanations improve human performance. There is a trend towards higher classification accuracy when subjects are given explanations for the AI\u2019s prediction as opposed to only being shown the unexplained model prediction.\n# 5 Discussion\nWe discuss here the results presented in the previous section. We first take a look at why CoProNN prototypes are more task specific than low-level concepts. Afterwards, we analyze the results gathered from our user study and outline several recommendations for users interested in adapting our method to their own use case. Finally, we point out some of CoProNN\u2019s limitations.\n# 5.1 Improved Task Specificity of CoProNN Concepts\nComparing the concepts used in existing methods with the proposed CoProNN approach, we find that concepts created with text-to-image generative Deep Learning methods are simpler to adapt to novel tasks. Concepts such as \u2019striped\u2019 or \u2019dotted\u2019 used in existing methods are often not task-specific and, as such, not representative for the key traits of the considered classes. We demonstrated in our experiments that custom-tailored concepts associated with relevant prototypes are better suited for generating task-specific explanations (see Fig. 6). Concepts generated in the CoProNN approach are easily adapted via prompting with specific context to novel scenarios, making them well-suited for versatile use.\n# 5.2 Interpretation User Study Results\n1) CoProNN explanations facilitate human-AI collaboration. The results presented in Section 4.3 and Figure 4 suggest that CoProNN explanations improved human performance on the classification task. 2) CoProNN explanations help spotting wrong predictions. One possible reason for the increased annotation performance in the CoProNN Group\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6893/68938469-e6ac-4f43-afaa-aed183eeedaf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Results of a survey for qualitative evaluation of CoProNN show that subjects generally found the explanations delivered by our model helpful and easy to understand.</div>\nFig. 5: Results of a survey for qualitative evaluation of CoProNN show that subjects generally found the explanations delivered by our model helpful and easy to understand.\nwould be that the explanations helped users spot wrong predictions more easily. Based on the visual inconsistency of the explanations with the visual appearance of the test sample, CoProNN users manage to identify false predictions more often. It appears, therefore, that explanations enable subjects to better calibrate their trust in the assistive AI, increasing users\u2019 critical assessment of the validity of the model\u2019s proposals. This contributes to fewer wrong answers in the CoProNN Group. ) CoProNN explanations are easy to understand and use. As the survey answers in Figure 5 show, respondents generally found the explanations delivered by our CoProNN intuitive and helpful for solving the tasks.\n# 5.3 Applying CoProNN to Your Own Use Case\nIn accordance to the recommendations in [36], we advise practitioners interested in applying our method to concentrate on a relatively small set of intuitive and easily learnable concepts, in order to avoid overwhelming the end-user with the generated explanations. The form in which the explanation is presented\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0cf4/0cf4366a-73a8-443a-aa0d-15fb01c4c9cc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8401/840120f6-9b75-473d-8ac2-a75826934720.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) SD concept</div>\n<div style=\"text-align: center;\">(a) Broden</div>\nFig. 6: High-level concept-based prototypes allow for higher task specificity. Image (a) is taken from the broden dataset under the concept set \u2019chequered\u2019. Image (b) is generated with SD by using a simple prompt condensed in the name of the concept. Image (c) is generated with SD by inputting a more specific prompt, namely \u2019elongated chequered reptile\u2019. The concept sets from which these examples come were supposed to explain the ImageNet class \u2019snake\u2019.\nto the user is also of paramount importance. We refer the reader to the user study in [21], where the authors \u201d...found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details.\u201d\n# 5.4 Limitations and Extensions\nNonetheless, CoProNN has several limitations. First and foremost, our method requires visual concepts that are known and easy to pinpoint. These are key to a successful usage of CoProNN. If the curated concept set is insufficient, our method will have difficulties in producing accurate or useful explanations of model predictions. The concept set is insufficient if not all classes can be uniquely recognized based solely on those concepts. It would mean that the underlying hierarchy does not end in one class per leaf. In that case, the explainer would need to communicate that ambiguity in a proper way to the user. Secondly, CoProNN currently works best for tree-structured class domains with conjunctive (\u201dand\u201d) characteristics. Especially in a highly complex use-case such as recognizing rare or very similar insect species, disjunctive (\u201dor\u201d) relationships are also sometimes necessary for accurate classification. For instance, very often the males and females of the same species have very different characteristics. Accurately assigning them to the correct class would require modelling more complex relationships between these characteristics. Furthermore, not all concepts are necessarily equally important for discriminating a class. For instance, one can say that the \u201dfuzzy orange\u201d abdomen tip of the Bombus pratorum is a weaker concept than the dominating trait \u201dyellow black stripes\u201d. There are several approaches for addressing this issue such as weighting concepts differently for each class, reformulating CoProNN into a multi-label explainer, precisionrecall thresholds and so on.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66a5/66a51afe-c236-4291-9be5-19605d140e32.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) SD prototype</div>\nWhile the DNN does not require any retraining for CoProNN to work, the explanation method itself does have several hyperparameters that may be used for fine-tuning and adapting the method to other usecases: number of neighbors k, threshold t, choice of random images, number of random partitions \u03b1, size of random partitions \u03b2, number of prototypes per concept nj, text prompts for generating the prototypes. Finally, CoProNN explanations are better and more robust if the concept sets are well separated/clustered in the search space. One might consider applying a suitable dimensionality reduction technique to the latent space to increase the efficiency of the kNN search.\n# 6 Conclusion\nIn this paper we proposed a novel XAI approach capable of generating intuitive explanations via task-specific concepts. Our method performs very well in comparison to similar concept-based methods, facilitates human-AI collaboration and offers a high degree of flexibility when applied in other scenarios, too. All in all, we are confident that CoProNN offers a viable extension of the current state-of-the-art concept-based XAI approaches and has the potential to be further improved, in order to reliably handle more diverse tasks.\n# Acknowledgments\nWe thank Christian Schmid-Egger for valuable expert advice on entomological classification of wild bee species.\n# References\n1. Diffusers. https://github.com/huggingface/diffusers, accessed: 2023-02-25 2. inaturalist challenge datase. https://github.com/visipedia/inat_comp/tree/ master/2021, accessed: 2023-03-1 3. The promise and peril of human evaluation for model interpretability. NeurIPS 2017 Symposium on Interpretable Machine Learning (2017), http://arxiv.org/ abs/1711.07414 4. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., Kim, B.: Sanity checks for saliency maps (2020) 5. Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network dissection: Quantifying interpretability of deep visual representations (2017) 6. Chen, C., Li, O., Tao, C., Barnett, A.J., Su, J., Rudin, C.: This looks like that: Deep learning for interpretable image recognition (2019) 7. Chen, Z., Bei, Y., Rudin, C.: Concept whitening for interpretable image recognition. Nature Machine Intelligence 2(12), 772\u2013782 (Dec 2020). https://doi.org/10.1038/s42256-020-00265-z, http://dx.doi.org/10.1038/ s42256-020-00265-z 8. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A LargeScale Hierarchical Image Database. In: CVPR09 (2009)\n9. Donnelly, J., Barnett, A.J., Chen, C.: Deformable protopnet: An interpretable image classifier using deformable prototypes (2022) 10. Fel, T., Picard, A., Bethune, L., Boissin, T., Vigouroux, D., Colin, J., Cad`ene, R., Serre, T.: Craft: Concept recursive activation factorization for explainability (2023) 11. Ghorbani, A., Wexler, J., Zou, J., Kim, B.: Towards automatic concept-based explanations (2019) 12. Hase, P., Bansal, M.: Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior? (2020). https://doi.org/10.18653/v1/2020.acl-main.491, https://github.com/ peterbhase/ 13. He, K., Gkioxari, G., Doll\u00b4ar, P., Girshick, R.B.: Mask R-CNN. CoRR abs/1703.06870 (2017), http://arxiv.org/abs/1703.06870 14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015). https://doi.org/10.48550/ARXIV.1512.03385, https://arxiv.org/ abs/1512.03385 15. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.: Generating visual explanations (2016) 16. Holzinger, A., Carrington, A., M\u00a8uller, H.: Measuring the Quality of Explanations: The System Causability Scale (SCS). Comparing Human and Machine Explanations. KI - K\u00a8unstliche Intelligenz 34(2), 193\u2013198 (Jun 2020) 17. Keane, M.T., Kenny, E.M.: How Case-Based Reasoning Explains Neural Networks: A Theoretical Analysis of XAI Using Post-Hoc Explanation-by-Example from a Survey of ANN-CBR Twin-Systems, p. 155\u2013171. Springer International Publishing (2019). https://doi.org/10.1007/978-3-030-29249-2 11, http://dx.doi.org/ 10.1007/978-3-030-29249-2_11 18. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., Sayres, R.: Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav) (2018) 19. Kim, B., Seo, J., Jeon, S., Koo, J., Choe, J., Jeon, T.: Why are saliency maps noisy? cause of and solution to noisy saliency maps (2019) 20. Kim, S.S.Y., Meister, N., Ramaswamy, V.V., Fong, R., Russakovsky, O.: HIVE: Evaluating the human interpretability of visual explanations. In: European Conference on Computer Vision (ECCV) (2022) 21. Kim, S.S.Y., Watkins, E.A., Russakovsky, O., Fong, R., Monroy-Hern\u00b4andez, A.: \u201chelp me help the ai\u201d: Understanding how explainability can support human-ai interaction. In: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI \u201923, ACM (Apr 2023). https://doi.org/10.1145/3544548.3581001, http://dx.doi.org/10.1145/ 3544548.3581001 22. Kindermans, P.J., Hooker, S., Adebayo, J., Alber, M., Sch\u00a8utt, K.T., D\u00a8ahne, S., Erhan, D., Kim, B.: The (un)reliability of saliency methods (2017) 23. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Doll\u00b4ar, P., Girshick, R.: Segment anything (2023) 24. Koh, P.W., Liang, P.: Understanding black-box predictions via influence functions (2020) 25. Koh, P.W., Nguyen, T., Tang, Y.S., Mussmann, S., Pierson, E., Kim, B., Liang, P.: Concept bottleneck models (2020)\n26. Lage, I., Chen, E., He, J., Narayanan, M., Kim, B., Gershman, S., Doshi-Velez, F.: An evaluation of the human-interpretability of explanation. In: arXiv (Jan 2019), http://arxiv.org/abs/1902.00006, iSSN: 23318422 eprint: 1902.00006 27. de Leeuw, J.: jspsych: A javascript library for creating behavioral experiments in a web browser. Behavior Research Methods (2015). https://doi.org/10.3758/s13428014-0458-y 28. Nauta, M., van Bree, R., Seifert, C.: Neural prototype trees for interpretable finegrained image recognition (2021) 29. Nguyen, A., Yosinski, J., Clune, J.: Understanding neural networks via feature visualization: A survey (2019) 30. Nguyen, G., Taesiri, M.R., Nguyen, A.: Visual correspondence-based explanations improve ai robustness and human-ai team accuracy (2023) 31. Nie, W., Zhang, Y., Patel, A.: A theoretical explanation for perplexing behaviors of backpropagation-based visualizations (2020) 32. Oppenlaender, J.: A taxonomy of prompt modifiers for text-to-image generation (2022). https://doi.org/10.48550/ARXIV.2204.13988, https://arxiv.org/ abs/2204.13988 33. Papernot, N., McDaniel, P.: Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning (2018) 34. Pavlichenko, N., Zhdanov, F., Ustalov, D.: Best prompts for text-to-image models and how to find them (2022). https://doi.org/10.48550/ARXIV.2209.11711, https://arxiv.org/abs/2209.11711 35. Poursabzi-Sangdeh, F., Goldstein, D.G., Hofman, J.M., Vaughan, J.W., Wallach, H.: Manipulating and measuring model interpretability (2018) 36. Ramaswamy, V.V., Kim, S.S.Y., Fong, R., Russakovsky, O.: Overlooked factors in concept-based explanations: Dataset choice, concept learnability, and human capability (2023) 37. Ramaswamy, V.V., Kim, S.S.Y., Meister, N., Fong, R., Russakovsky, O.: Elude: Generating interpretable explanations via a decomposition into labelled and unlabelled features (2022) 38. Ribeiro, M.T., Singh, S., Guestrin, C.: \u201dwhy should i trust you?\u201d: Explaining the predictions of any classifier (2016) 39. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models (2021). https://doi.org/10.48550/ARXIV.2112.10752, https://arxiv.org/abs/2112. 10752 40. Rymarczyk, D., \ufffdLukasz Struski, G\u00b4orszczak, M., Lewandowska, K., Tabor, J., Zieli\u00b4nski, B.: Interpretable image classification with differentiable prototypes assignment (2022) 41. Sammani, F., Mukherjee, T., Deligiannis, N.: Nlx-gpt: A model for natural language explanations in vision and vision-language tasks (2022) 42. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-CAM: Visual explanations from deep networks via gradient-based localization. International Journal of Computer Vision 128(2), 336\u2013359 (oct 2019). https://doi.org/10.1007/s11263-019-01228-7, https://doi.org/10.1007% 2Fs11263-019-01228-7 43. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps (2014) 44. Smilkov, D., Thorat, N., Kim, B., Vi\u00b4egas, F., Wattenberg, M.: Smoothgrad: removing noise by adding noise (2017)\n45. Sui, Y., Wu, G., Sanner, S.: Representer point selection via local jacobian expansion for post-hoc classifier explanation of deep neural networks and ensemble models. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 23347\u201323358. Curran Associates, Inc. (2021), https://proceedings.neurips.cc/paper_files/ paper/2021/file/c460dc0f18fc309ac07306a4a55d2fd6-Paper.pdf 46. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks (2017) 47. Tkachenko, M., Malyuk, M., Holmanyuk, A., Liubimov, N.: Label Studio: Data labeling software (2020-2022), https://github.com/heartexlabs/label-studio, open source software available from https://github.com/heartexlabs/label-studio 48. Van Horn, G., Cole, E., Beery, S., Wilber, K., Belongie, S., Mac Aodha, O.: Benchmarking representation learning for natural world image collections. In: Computer Vision and Pattern Recognition (2021) 49. Witteveen, S., Andrews, M.: Investigating prompt engineering in diffusion models (2022). https://doi.org/10.48550/ARXIV.2211.15462, https://arxiv.org/abs/ 2211.15462 50. Yeh, C.K., Kim, J.S., Yen, I.E.H., Ravikumar, P.: Representer point selection for explaining deep neural networks (2018) 51. Zhou, B., Sun, Y., Bau, D., Torralba, A.: Interpretable basis decomposition for visual explanation. In: Proceedings of the European Conference on Computer Vision (ECCV) (September 2018)\n# Appendix\n# Training the Classifier and Optimizing the Explainer\nFor all three XAI methods (CoProNN, TCAV, IBD), the same random set of 1000 images from the broden dataset was used. The random images are not generated with SD, in order to ensure a fair comparison between all three methods, instead of overfitting/fine-tuning on our own method. Indeed, there is plenty of room for improvement when choosing a tailored random dataset for CoProNN. One may argue that generating the random images with SD, too, guarantees a more uniform distribution of all the images used for searching for prototypical neighbors. Inspired by the TCAV approach, we implemented the partitioning of this random dataset (\u03b1 = 100 samplings of size \u03b2 = 30 without replacement for CoProNN; \u03b1 = 30 samplings of size \u03b2 = 500 without replacement for TCAV and IBD) and run the methods multiple times w.r.t. to these partitions, averaging the relevance scores in the end. For CoProNN, the parameter \u03b2 matched the size of every prototype set. In the TCAV, when computing the Concept Activation Vectors for the \u2019random concept\u2019, we split the random partitions in two further sub-partitions of size 250 (one for the positive class, one for the negative one). In IBD, we left out the concept localization part via feature attribution maps and only used the scores resulted from projecting the feature vector a of a test sample on the concept components qcj that approximate their class weight\nvectors \u03c9k of class k. For the sake of comparability to the other two methods, we normalized the scores for every test sample by the corresponding class logits wT k a predicted by the classifier (refer to Equations 7-97 in [51]):\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c96c/c96cef9b-3516-4e24-ab9c-d639dc6b641b.png\" style=\"width: 50%;\"></div>\n\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd We mention here that CoProNN may also be implemented as a multi-label predictor version of the algorithm (each prototype set tested separately against the random set in the kNN, more similar to the implementation of the traditional TCAV), as well as a version without any random set at all (only the concept sets form the search space for the kNN, similar to the \u2019relative TCAV\u2019 implementation). Moreover, we tried generating prototypical images in the spirit of the broden concepts categorisation (in our case, color = {brown, yellow, orange} and texture = {smooth, fuzzy} for the wild bees) and fit two kNNs for each concept category. It turned out that this alternative version would not perform as well as the CoProNN version described in this paper on the aforementioned metrics. More details on these experimental trials are available in our code repository.\nImageNet Animals: For the ImageNet classes, a frozen ResNet50 [14] initialized with ImageNet weights from the TensorFlow library was used. For every class, 150 images were downloaded, out of which 100 were randomly selected as a validation set for optimizing the k parameter in the kNN-explainer. The best k-value that would maximize the CS over all classes was 8 when the concept set consisted of SD-generated prototypes and 36 when it consisted of low-level concept images (from broden or SD-generated). The remaining 50 images per class made up the test set on which the XAI experiments were conducted.\nImageNet Animals: For the ImageNet classes, a frozen ResNet50 [14] initialized with ImageNet weights from the TensorFlow library was used. For every class, 150 images were downloaded, out of which 100 were randomly selected as a validation set for optimizing the k parameter in the kNN-explainer. The best k-value that would maximize the CS over all classes was 8 when the concept set consisted of SD-generated prototypes and 36 when it consisted of low-level concept images (from broden or SD-generated). The remaining 50 images per class made up the test set on which the XAI experiments were conducted. iNat Wild Bees: From all the 30k scraped images, an extra subset of 30 images per species was set apart and used for manual annotation8 via segmentation masks in Label Studio [47]. Our annotated dataset is freely available online at https://zenodo.org/record/6642157. The masks were then used to train a MaskRCNN [13], with which the remaining scraped images were automatically segmented9. The MaskRCNN-segmented images belonging only to the five species of interest were then cropped down to their minimal bounding box, split into a training and validation set (2/3 - 1/3) and fed into a ResNet50 initialized with a backbone [48] pretrained on the 2021 iNat Challenge Dataset [2]. The\niNat Wild Bees: From all the 30k scraped images, an extra subset of 30 images per species was set apart and used for manual annotation8 via segmentation masks in Label Studio [47]. Our annotated dataset is freely available online at https://zenodo.org/record/6642157. The masks were then used to train a MaskRCNN [13], with which the remaining scraped images were automatically segmented9. The MaskRCNN-segmented images belonging only to the five species of interest were then cropped down to their minimal bounding box, split into a training and validation set (2/3 - 1/3) and fed into a ResNet50 initialized with a backbone [48] pretrained on the 2021 iNat Challenge Dataset [2]. The\n7 Note that in this paper we used m as the number of considered concepts, which corresponds to n in the cited paper. 8 We note that at the time of building this annotated dataset, powerful off-the-shelf segmenters such as SAM [23] were not yet released. 9 The pipeline for training the segmenter was implemented via a forked repository: https://github.com/lucasjinreal/yolov7 d2.\nimages of the five species manually annotated in Label Studio were set aside as test set for the classifier. We report a 0.85 top-1 test accuracy. Similar to [48], we conducted first an experiment to test which input format - raw image, segmented or segmented + cropped to bounding box - leads to the highest model performance. The third format proved to deliver the best results. As far as the kNN-explainer is concerned: we searched for an optimal k on the combined training and validation set. The best value for k w.r.t. CS over all classes was found to be 18.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b39/2b399992-9f5c-4006-8a5c-ee002e0eee5e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a810/a8103b31-4289-4955-b52d-13de994e9302.png\" style=\"width: 50%;\"></div>\nFig. 7: Example of one task an XAI user may receive. To answer the question, the subjects need to click on one of the five species on the right. Beneath the test sample on the left there is zoom-in pane to magnify certain portions of the image.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7985/79851a8f-26e7-4b13-9d1b-2874a75687cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8: IBD class scores for the ImageNet animal classes (a) and the iNat wild bees with concept prototypes (b). A diamond \u22c4marks a zero-value.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/90c8/90c8acd6-6ada-47c1-8369-89525a67df2c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e01/2e01adef-7390-4545-8fba-f25f0bb0c327.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) TCAV</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c6b/5c6b760b-84b1-4b31-9a3e-d7d0b34dc3de.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) CoProNN</div>\n<div style=\"text-align: center;\">(e) TCAV</div>\nFig. 9: Low-level task-unspecific concept images do not work well with any XAI method. Top row: methods were applied w.r.t. broden concept images. Bottom row: methods were applied w.r.t. SD-computed concept images. A diamond \u22c4 marks a zero-value. All plots show the average concept relevance scores per class.\n<div style=\"text-align: center;\">(f) IBD</div>\nAlgorithm 1 CoProNN (Concept-based Prototypical Nearest Neighbors) Input: 1: feature extractor f : Rh\u00d7w\u00d73 \u2192RD, mapping an input image of height h and width w to a feature vector of length D 2: m prototypes (explanations), where each prototype j is represented by a set Cj = {c(1) j , ..., c (nj) j } of nj prototypical images c of dimension h \u00d7 w \u00d7 3 3: nm+1 random images Cm+1 = {c(1) m+1, ..., c (nm+1) m+1 } to extend the search space for kNN 4: s sample images {x1, ..., xs} for which to predict the explanations (relevant prototypes) 5: number of neighbors k \u2208N to be considered in kNN 6: threshold t \u2208(0, 1] to select the concept candidates for formulating the final explanation 7: number of partitions \u03b1 \u2208N to sample the random dataset 8: size of each random partition \u03b2 < nm+1 Output: R = {R1, ..., Rs}, where Ri \u2282{1, ..., m} is the set of relevant prototype indices for the test sample xi\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4890/48901539-3980-4c85-bb15-6962dfed5149.png\" style=\"width: 50%;\"></div>\nExtract features of all prototype and random images 9: for j in 1 ... m+1 do 10: \u2126j \u2190f(Cj) = {f(c(1) j ), . . . , f(c (nj) j )} 11: end for\nRun kNN for every random partition\n12: Initialize zero-matrix P \u2208Rs\u00d7(m+1)\n13: for a in 1 ... \u03b1 do\n14:\nSample random partition \u2126a\nm+1 of size \u03b2\n15:\nFit kNN on \u21261 \u222a.... \u222a\u2126m \u222a\u2126a\nm+1\n16:\nfor i in 1 ... s do\n17:\nN(f(xi)) \u2190kNN.predict( f(xi) ) \u25b7set of k nearest proto./random vectors\n18:\nfor j in 1 ... m do\n19:\nPi,j \u2190|N(f(xi)) \u2229\u2126j|\nk\n\u25b7ratio of neighbors belonging to j-th proto. set\n20:\nend for\n21:\nend for\n22: end for\n23: P \u21901\n\u03b1P\n\u25b7average scores over all partitions\nSelect relevant prototypes for final explanation\n24: Initialize empty set R\n25: for i in 1 ... s do\n26:\nInitialize empty set Ri\n27:\nfor j in 1 ... m do\n28:\nif Pij \u2265t then\n29:\nAppend j to Ri\n30:\nend if\n31:\nend for\n32:\nAppend Ri to R\n33: end for\n34: return R\nCosine Similarity\nClass\nCoProNN\nTCAV\nIBD\nBroden Concepts\nCheetah\n0.1207 \u00b1 0.1242\n0.4225 \u00b1 0.001\n0.7316 \u00b1 0.6209\nSnake\n0.3274 \u00b1 0.0866\n0.6572 \u00b1 0.001\n0.0 \u00b1 0.0\nTiger\n0.9973 \u00b1 0.0084\n0.7448 \u00b1 0.001\n0.9535 \u00b1 0.3014\nTurtle\n0.2307 \u00b1 0.1097\n0.5367 \u00b1 0.001\n0.0 \u00b1 0.0\nZebra\n0.9978 \u00b1 0.0035\n0.3906 \u00b1 0.001\n1.0 \u00b1 0.0\nSD Concepts\nCheetah\n0.168 \u00b1 0.164\n0.8835 \u00b1 0.001\n0.0 \u00b1 0.0\nSnake\n0.664 \u00b1 0.1403\n0.5958 \u00b1 0.001\n0.0 \u00b1 0.0\nTiger\n0.718 \u00b1 0.1231\n0.6058 \u00b1 0.001\n0.1628 \u00b1 0.9867\nTurtle\n0.7696 \u00b1 0.0941\n0.6046 \u00b1 0.001\n0.0 \u00b1 0.0\nZebra\n0.8915 \u00b1 0.0561\n0.4634 \u00b1 0.001\n0.9999 \u00b1 0.0001\nTable 2: Low-level task-unspecific concept images do not work well with any XAI method, neither on pre-annotated broden concepts, nor on similar concepts generated with SD. CoProNN appears to efficiently explain the striped animals (tiger and zebra) on the broden concepts, but plot (a) in Fig. 9 reveals that the \u2019striped\u2019 concept is always predicted as dominant regardless of the class. Also, on the SD concepts, TCAV only explains the cheetah class well, fact highlighted in plot (e) of Fig. 9 where only the cheetah has a clear peak for the correct \u2019dotted\u2019 concept.\n<div style=\"text-align: center;\">Cosine Similarity</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of creating task-specific explanations for AI predictions in computer vision, highlighting the challenges of integrating domain expertise into existing generic XAI methods. It emphasizes the need for novel approaches that can facilitate the development of tailored explanations, particularly in complex classification tasks.",
        "problem": {
            "definition": "The problem at hand is the difficulty in generating intuitive and accurate explanations for AI predictions in computer vision tasks, particularly when domain expertise is required to interpret the results.",
            "key obstacle": "The main challenge is the time-consuming process of designing task-specific explanations that necessitate domain knowledge, which is often difficult to scale in the development of XAI methods."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of generative models to create visual representations of concepts that can be easily understood by domain experts, enabling them to contribute effectively to the explanation process.",
            "opinion": "The proposed method, CoProNN, allows domain experts to generate concept-based explanations quickly and intuitively using natural language prompts, bridging the gap between AI predictions and human understanding.",
            "innovation": "CoProNN distinguishes itself from existing methods by generating task-specific prototypes that are not tied to the training set, leveraging higher-level concepts to enhance the interpretability of AI predictions."
        },
        "method": {
            "method name": "CoProNN",
            "method abbreviation": "Concept-based Prototypical Nearest Neighbors",
            "method definition": "CoProNN is a method that generates intuitive explanations for computer vision model predictions by creating task-specific concept-based prototypes using a text-to-image generative model.",
            "method description": "The method utilizes k-Nearest Neighbors to relate generated prototypes to new input images, providing explanations based on the presence or absence of visual concepts.",
            "method steps": [
                "Train a Deep Neural Network (DNN) on the dataset.",
                "Domain experts define relevant concepts for the task.",
                "Generate prototype images using a text-to-image model.",
                "Map prototypes and random images into the DNN's latent feature space.",
                "Use kNN to compare the test sample with prototypes to retrieve relevant concepts."
            ],
            "principle": "The effectiveness of CoProNN lies in its ability to generate high-quality visual representations of concepts that are directly relevant to the task, allowing for meaningful comparisons in the feature space."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using three datasets: a subset of animals from ImageNet, wild bees from iNaturalist, and the Broden dataset for comparison. Each dataset was carefully selected to evaluate the method's performance in both coarse and fine-grained classification tasks.",
            "evaluation method": "The performance of CoProNN was assessed using Cosine Similarity to compare predicted concept relevance scores against ground truth labels, along with qualitative user studies to gauge user understanding and trust in the explanations."
        },
        "conclusion": "CoProNN successfully generates intuitive and task-specific explanations for AI predictions, demonstrating its effectiveness in enhancing human-AI collaboration and showing promising results compared to existing XAI methods.",
        "discussion": {
            "advantage": "CoProNN offers significant advantages in terms of adaptability to various tasks and the ability to generate explanations that are easily understood by users, thus improving classification accuracy.",
            "limitation": "The method relies heavily on the availability of well-defined visual concepts; insufficient or ambiguous concepts can lead to ineffective explanations.",
            "future work": "Future research could focus on refining the concept generation process, exploring multi-label classification scenarios, and enhancing the robustness of explanations in more complex classification tasks."
        },
        "other info": {
            "code repository": "All code and experimental data can be found in our GitHub repository: https://github.com/TeodorChiaburu/beexplainable.",
            "acknowledgments": "We thank Christian Schmid-Egger for valuable expert advice on entomological classification of wild bee species."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of creating task-specific explanations for AI predictions in computer vision, highlighting the challenges of integrating domain expertise into existing generic XAI methods."
        },
        {
            "section number": "1.2",
            "key information": "The significance of studying cognitive processes, logical reasoning, and problem-solving strategies is underscored by the need for novel approaches that facilitate tailored explanations in complex classification tasks."
        },
        {
            "section number": "2.1",
            "key information": "The concept of task-specific explanations in AI, particularly in computer vision, requires a clear understanding of cognitive processes involved in interpreting AI predictions."
        },
        {
            "section number": "2.3",
            "key information": "The theoretical foundation of the paper revolves around the integration of domain knowledge into explainable AI (XAI) methods, emphasizing the need for innovative approaches like CoProNN."
        },
        {
            "section number": "5.3",
            "key information": "CoProNN utilizes a text-to-image generative model to create task-specific prototypes, demonstrating the application of machine learning techniques in innovative problem-solving models."
        },
        {
            "section number": "6.1",
            "key information": "The paper discusses how the CoProNN method allows domain experts to generate concept-based explanations quickly, impacting decision-making processes in AI-related tasks."
        },
        {
            "section number": "7.2",
            "key information": "The application of CoProNN in AI systems showcases how cognitive processes and logical reasoning can enhance human-AI collaboration and improve classification accuracy."
        },
        {
            "section number": "8",
            "key information": "CoProNN demonstrates its effectiveness in enhancing human-AI collaboration, suggesting areas for future research in refining explanation methods and exploring multi-label classification scenarios."
        }
    ],
    "similarity_score": 0.5561172826214337,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/CoProNN_ Concept-based Prototypical Nearest Neighbors for Explaining Vision Models.json"
}