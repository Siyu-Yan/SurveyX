{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.11511",
    "title": "Reasoning with Large Language Models, a Survey",
    "abstract": "Scaling up language models to billions of parameters has opened up possibilities for in-context learning, allowing instruction tuning and few-shot learning on tasks that the model was not specifically trained for. This has achieved breakthrough performance on language tasks such as translation, summarization, and question-answering. Furthermore, in addition to these associative \"System 1\" tasks, recent advances in Chain-of-thought prompt learning have demonstrated strong \"System 2\" reasoning abilities, answering a question in the field of artificial general intelligence whether LLMs can reason. The field started with the question whether LLMs can solve grade school math word problems. This paper reviews the rapidly expanding field of prompt-based reasoning with LLMs. Our taxonomy identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. Finally, we highlight the relation between reasoning and prompt-based learning, and we discuss the relation between reasoning, sequential decision processes, and reinforcement learning. We find that self-improvement, self-reflection, and some metacognitive abilities of the reasoning processes are possible through the judicious use of prompts. True self-improvement and self-reasoning, to go from reasoning with LLMs to reasoning by LLMs, remains future work.",
    "bib_name": "plaat2024reasoninglargelanguagemodels",
    "md_text": "# Reasoning with Large Language Models, a Survey\nAske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas B\u00a8ack LIACS, Leiden University, Netherlands\n# Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas B\u00a8ack LIACS, Leiden University, Netherlands\n# July 17, 2024\nAbstract\nScaling up language models to billions of parameters has opened up possibilities for in-context learning, allowing instruction tuning and few-shot learning on tasks that the model was not specifically trained for. This has achieved breakthrough performance on language tasks such as translation, summarization, and question-answering. Furthermore, in addition to these associative \u201cSystem 1\u201d tasks, recent advances in Chain-of-thought prompt learning have demonstrated strong \u201cSystem 2\u201d reasoning abilities, answering a question in the field of artificial general intelligence whether LLMs can reason. The field started with the question whether LLMs can solve grade school math word problems. This paper reviews the rapidly expanding field of prompt-based reasoning with LLMs. Our taxonomy identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. Finally, we highlight the relation between reasoning and prompt-based learning, and we discuss the relation between reasoning, sequential decision processes, and reinforcement learning. We find that self-improvement, self-reflection, and some metacognitive abilities of the reasoning processes are possible through the judicious use of prompts. True self-improvement and self-reasoning, to go from reasoning with LLMs to reasoning by LLMs, remains future work.\n# 1 Introduction\nTransformer-based Large Language Models (LLMs) that are trained on large datasets have achieved breakthrough performance at next token prediction [Vaswani et al., 2017, Radford et al., 2019, Wei et al., 2022a]; they are very good at natural language understanding (GLUE, SQUAD, Xsum) [Wang et al., 2018, 2019, Rajpurkar et al., 2016, Narayan et al., 2018], translation [Kocmi et al., 2022, Papineni et al., 2002, Sennrich et al., 2015], question answering [Tan et al., 2023], and other System 1 tasks [Kahne-\nman, 2011].1 The success of ChatGPT [Ouyang et al., 2022] has taken the world by storm. Transformer-based generative language models whose size is beyond hundreds of billions parameters are not only very good at language generation, they also enable new type of machine learning, called in-context learning [Brown et al., 2020]. In-context learning, also known as prompt-based learning, occurs only in LLMs beyond a certain size (hundreds of billions of parameters) that are sufficiently rich [Wei et al., 2022a]. In-context learning is inference time, prompt-based, few-shot learning, where model parameters are not trained or fine-tuned. System 1 tasks, such as associative language tasks, are easily solved by LLMs with prompt-based learning, as the many school children around the world that use ChatGPT daily can attest. (Although the problems are too often not solved correctly, just fluently, when the model\u2019s association powers lead to hallucination [Huang et al., 2023].) On the other hand, System 2 tasks, such as grade school math word problems, are more difficult for LLMs[Cobbe et al., 2021]. To solve math word problems we need to break down the problem in multiple reasoning steps. Spurred-on by the impressive performance on System 1 tasks, much research has focused on understanding the reason for the poor performance of LLMs on System 2 tasks, and how it can be improved. Among this research, the Chain-of-thought experiment [Wei et al., 2022b] stands out. This work, and subsequently Kojima et al. [2022], showed that adding a simple instruction to the prompts, Let\u2019s think step by step, can provoke an LLM to perform the required intermediate reasoning steps, achieving a surprising jump in performance. The Chain-of-thought paper is a breakthrough in the field of reasoning with LLMs. Much exciting work has been published that builds on this work. Grade school math word problems started the research into LLM-reasoning, with the GSM8K benchmark [Cobbe et al., 2021]. In our survey we discuss papers based on this benchmark, and directly-related follow up work on reasoning. We focus on prompt-based approaches. We survey the recent literature using a straightforward taxonomy. Although the field has only recently started, the jump in performance on reasoning has excited artificial intelligence and society alike. We provide a research agenda with opportunities for future research. At the end of this survey, we also discuss connections to other fields, such as self-reflection, metacognition (or thinking about thinking, see for example Dunlosky and Metcalfe [2008]), and the motivation towards artificial general intelligence. Our contributions are:\n\u2022 A survey of relevant approaches in prompt-based reasoning (grade school math word problems and closely related domains) in large language models, including\n\u2022 A survey of relevant approaches in prompt-based reasoning (grade school math word problems and closely related domains) in large language models, including a research agenda.\n1In his book Thinking, fast and slow, a bestseller on human psychology, Daniel Kahneman described System 1 thinking as a near-instantaneous process; it happens automatically, intuitively, and with little effort. It is driven by instinct and experiences. System 2 thinking is slower and requires more effort. It is conscious and logical. The automatic operations of System 1 generate surprisingly complex patterns of ideas, but only the slower System 2 can construct thoughts in an orderly series of steps. In the LLM literature the terms are often used as shorthand to distinguish single-step associative tasks, from multi-step reasoning tasks, despite the fact that language tasks such as question answering and translation may require some \u201cslow\u201d thinking.\n1In his book Thinking, fast and slow, a bestseller on human psychology, Daniel Kahneman described System 1 thinking as a near-instantaneous process; it happens automatically, intuitively, and with little effort. It is driven by instinct and experiences. System 2 thinking is slower and requires more effort. It is conscious and logical. The automatic operations of System 1 generate surprisingly complex patterns of ideas, but only the slower System 2 can construct thoughts in an orderly series of steps. In the LLM literature the terms are often used as shorthand to distinguish single-step associative tasks, from multi-step reasoning tasks, despite the fact that language tasks such as question answering and translation may require some \u201cslow\u201d thinking.\n\u2022 A taxonomy based on regular reasoning literature (step generation, step evaluation, and control of reasoning steps).\nThis survey is organized as follows. Section 2 summarizes the most relevant developments in LLMs, including in-context learning. Of great importance are the benchmarks that are used in this field. We discuss these in Section 3, followed by our method for scoping and selecting of papers in Section 4. Next, in Section 5 we provide a taxonomy of the field, where we discuss the approaches in detail. Then, in Section 6 we discuss our findings in a broader perspective. We also discuss the relation between reasoning and work on self-reflection and metacognition. This section concludes with an agenda for future research. Finally, Section 7 concludes the survey.\n# 2 Background: Reasoning with LLMs\nBefore we dive into the works on reasoning, we review some background terminology on LLMs. Our overview is brief. Excellent surveys on LLMs are, for example, Minaee et al. [2024] and Zhao et al. [2023]. We discuss the generic training pipeline for LLMs, we discuss how in-context learning works, and we discuss the reasoning pipeline. We start with the generic language model training pipeline.\n# 2.1 Training Pipeline Language Model\nLLMs are typically constructed in a sequence of stages, from data preparation, through training, to inference. The training pipeline for most LLMs is quite elaborate. We will now list a pipeline of the most common stages, based on the survey by Minaee et al. [2024]. 1. Acquire a large, general, unlabeled, high-quality text corpus. Some considerations on the selection of the texts are discussed in Brown et al. [2020]. 2. Pretrain the transformer model [Vaswani et al., 2017] on this large corpus. This step yields a generalist model. The pretraining is done using a self-supervised approach on the unlabeled dataset (text corpus). 3. Finetune the general model to a specific (narrow) task. This can be done using supervised-learning with a new labeled dataset consisting of prompts and answers (supervised finetuning, SFT) [Wei et al., 2022a, Minaee et al., 2024], specific for the task at hand. (A small number of papers in this survey work in the finetuning stage.) 4. Instruction tuning is a form of finetuning on a labeled dataset of instruction prompts and corresponding outputs, to improve instruction following, and thus the usefulness of models. 5. Align the finetuned model with user expectations (preference alignment). The goal of this stage is to improve the model to give more ethically and socially acceptable answers. The machine learning method that is used in this stage can\n2022] or Direct Preference Optimization [Rafailov et al., 2024]. 6. Optimize training to improve cost-effectiveness, for example, with low-rank optimization [Hu et al., 2021], mixed precision training [Micikevicius et al., 2017], quantization [Jacob et al., 2018], or knowledge distillation [Xu et al., 2024, Gu et al., 2023]. 7. Inference & In-context learning can be used to train the model to provide the correct answers without changing parameters [Dong et al., 2022, Brown et al., 2020]. By providing a prompt that contains a small number of examples together with a question, prompt learning is a form of few-shot learning. This is the stage in which most of the papers of this survey work, and that is familiar to all general users of ChatGPT.\nMost of the reasoning methods that we discuss in this survey work in stage 7: in-context learning, using prompts for the LLM to perform a complex multi-step reasoning task. The following section provides a brief introduction to in-context learning.\n# 2.2 In-Context Learning\nIn LLMs beyond hundreds of billions of parameters a new kind of learning has emerged, that is called in-context learning or prompt-learning [Brown et al., 2020]. It occurs at inference time, and is often able to give good results with few examples; it is a form of few-shot learning. The large size of the model, containing rich and general knowledge is enabling this new type of few-shot learning (see Dong et al. [2022] for a survey). A prompt, consisting of a piece of demonstration context, is concatenated with a query question, and is given to the language model for prediction [Liu et al., 2023]. For example, when the task is emotion recognition in a social media post, \u201cI missed the bus today,\u201d can be followed by \u201cI felt so [ ]\u201d. Alternatively, for translation, we could follow \u201cI missed the bus today,\u201d by \u201cFrench: [ ]\u201d [Liu et al., 2023]. The prompt contains background information that is recognized by the model, selecting the desired model context. In-context learning works when language models contain enough knowledge, allowing them to generalize on the examples provided in the prompt. Prompts that contain a few examples are said to perform few-shot learning. Prompts that contain only instructions without examples are said to perform zero-shot learning. In-context learning takes place at inference time, after the computationally intensive stages where parameters have been pretrained and finetuned, when the model is queried by the user to provide answers. No parameters are changed anymore with in-context learning. This is quite different from the common approach in supervised deep learning\u2014or self-supervised deep learning\u2014where large datasets are used during training to update model parameters with backward propagation in lengthy and costly training epochs [Goodfellow et al., 2016]. Common approaches to few-shot learning, such as metalearning, do include training and finetuning of parameters to achieve generalization, and are computationally expensive (see, for example, Finn et al. [2017] or Huisman et al. [2021], Hospedales et al. [2021] for a survey).\nPrompts provide a user-friendly interface to LLMs. The success of in-context learning tends to be quite sensitive to the way a prompt is formulated; a new field called prompt engineering has emerged to optimize the usefulness of in-context learning by learning how to make them do what we want [Radford et al., 2019, Wei et al., 2022a, Giray, 2023, Sahoo et al., 2024].\n# 2.3 Reasoning Pipeline\nReasoning problems are also solved with a pipeline of stages. A typical approach to solving a complex problem is to subdivide it into smaller steps and solve those. This approach is related to divide and conquer [Bellman, 1966]. New steps are (1) generated, (2) evaluated, and the number of steps that are generated and searched is (3) controlled in some way. The in-context reasoning approaches that we survey follow a general three-stage pipeline [Madaan et al., 2023]: 1. Generate: generation of steps by the model, 2. Evaluate: evaluation of the predicted steps by an evaluator, 3. Control: control of the number of steps that are generated and how deep ahead the reasoning process will look. This three-stage pipeline will be the basis of our taxonomy. But first, we will look at benchmarks.\n# 3 Benchmarks\nProgress in artificial intelligence is measured by benchmarks. Benchmarks define the goal that researchers aim to achieve in their experiments. In natural language processing, a wide array of benchmarks exists to measure progress, such as on question answering (for example, CommonsenseQA [Talmor et al., 2018]), word prediction (for example, LAMBADA [Paperno et al., 2016]), translation (for example, WMT\u201922 [Kocmi et al., 2022]), language understanding (for example, GLUE [Wang et al., 2018, 2019]), and text summarization (for example, Xsum [Narayan et al., 2018]). Transformer architectures were first popularized by encoder models such as BERT [Devlin et al., 2018], for named entity recognition and classification tasks. Subsequently, decoder models such as GPT 2-4 [Radford et al., 2019, Brown et al., 2020, Achiam et al., 2023] showed impressive progress on natural language benchmarks. The field of LLMs is quite active. Many different benchmarks exist, and listing a comprehensive overview of all relevant benchmarks is beyond the scope of this survey. We will mention relevant benchmarks for testing the reasoning abilities of LLMs. Following Wei et al. [2022b], these are all math word problem benchmarks. The benchmark that is most frequently associated with reasoning by LLMs is a dataset of grade school math word problems GSM8K [Cobbe et al., 2021]. GSM8K was created by humans, with an aim of high quality, high diversity, moderate difficulty, and solutions in natural language. Other benchmarks are the SVAMP varying structures benchmarks [Patel et al., 2021], the ASDiv dataset of diverse math problems [Miao et al., 2021],\nthe AQuA dataset of algebraic word problems [Ling et al., 2017], and the MAWPS benchmark [Koncel-Kedziorski et al., 2016]. We will now briefly discuss these benchmarks; the baseline performance that we quote is from Wei et al. [2022b].\nGSM8K To test reasoning skills, the Grade School Math problem dataset (GSM8K) was developed for testing LLMs [Cobbe et al., 2021]. It consists of 8500 humanwritten math problems. Language models struggled to achieve good performance on this dataset (pre Chain-of-thought). An example of a math word task is:\nProblem: Beth bakes 4, two dozen batches of cookies in a week. If these cookies are shared amongst 16 people equally, how many cookies does each person consume? Answer: 4 \u00d7 2 \u00d7 12/16 = 6.\nThe baseline performance of GPT-3 175B is 15.6% accuracy. In comparison, the performance of Chain-of-thought is 46.9% accuracy.\nASDiv The Academia Sinica Diverse MWP Dataset (ASDiv) [Miao et al., 2021] is specifically designed for high diversity in problem types, formats and difficulty levels. It consists of 2305 problems. An example problem is:\nProblem: A sandwich is priced at 0.75. A cup of pudding is priced at 0.25. Tim bought 2 sandwiches and 4 cups of pudding. How much money should Tim pay? Answer: 0.75 \u00d7 2 + 0.25 \u00d7 4 = 2.5.\nThe baseline performance of GPT-3 175B is 70.3% accuracy. The performance of Chain-of-thought is 71.3% accuracy.\nMAWPS The Math Word Problem Repository (MAWPS) [Koncel-Kedziorski et al., 2016] allows for the construction of datasets with particular characteristics by selecting different categories of problems. The dataset consists of 3320 problems. An example is:\nProblem: Rachel bought two coloring books. One had 23 pictures and the other had 32. After one week she had colored 44 of the pictures. How many pictures does she still have to color? Answer: 55 \u221244 = 11.\nThe baseline performance of GPT-3 175B is 72.7% accuracy. The performance of Chain-of-thought is 87.1% accuracy.\nSVAMP The Simple Variations on Arithmetic Math word Problems dataset (SVAMP) was designed by Patel et al. [2021]. It consists of 1000 problems, curated from variations of ASDiv-a [Miao et al., 2021] and MAWPS [Koncel-Kedziorski et al., 2016]. An example problem is:\n# Problem: Jack had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. How many pens does Jack have now? Answer: 8 \u22123 = 5.\nThe baseline performance of GPT-3 175B is 65.7% accuracy. In comparison, the performance of Chain-of-thought is 68.9% accuracy.\nAQuA The Algebraic Question Answering dataset [Ling et al., 2017] is a large dataset of 100,949 questions, answers, and rationales. The dataset is based on a combination of a smaller seed dataset and crowdsourcing. An example question is:\nQuestion: Two trains running in opposite directions cross a man standing on the platform in 27 seconds and 17 seconds respectively and they cross each other in 23 seconds. The ratio of their speeds is: Options: A) 3/7 B) 3/2 C) 3/88 D) 3/8 E) 2/2 Answer: B.\nThe baseline performance of GPT-3 175B is 24.8% accuracy. The performance of Chain-of-thought is 35.8% accuracy. There is a wide variety of benchmarks, and there is a wide variety of performance in benchmarks. Some are easily solvable by current LLMs, and some are significantly harder. Benchmark design is an important part of the field of reasoning in LLMs. Currently the GSM8K benchmark is popular; baseline model performance is weak, and reasoning prompts can substantially improve performance. As performance on GSM8K improves, different (harder) benchmarks will become popular.\n# 4 Selection of Papers\nThe papers in this survey were selected as follows. Baseline LLMs have difficulty solving math word problems, specifically on benchmarks listed in the previous section. We take the ability to solve those benchmarks as a proxy for reasoning ability. We initially performed a literature search for papers that use these benchmarks, and that contain the search terms reasoning and large language model in their title or abstract. We also searched for papers that referenced the Chain-of-thought paper. The resulting papers were curated based on recency, relevance, substance, and novelty. We favor recent papers (two years prior to the writing of the survey), related to the Chain-of-thought approach of generating intermediate reasoning steps, that solve tasks such as math word problems, and that work by prompt-based in-context learning. We also include some papers that work by finetuning or supervised learning that relate to, or inspire, the Chain-of-thought approaches. Furthermore, we include approaches outside math word problems that showed interesting approaches to reasoning, such as applications in coding and autonomous agents, because of their approach to grounding.\n# 5 Prompt Generation, Evaluation and Control\nThis survey examines how an architecture that is good at System 1 tasks can be prompted to solve System 2 tasks. The Chain-of-thought paper showed how a simple command could prompt an LLM to perform reasoning steps, yielding much better performance in math word problems. Since then much research has further explored this approach, trying to build the ultimate general problem solver for System 1 and System 2 problems. Following the pipeline of Section 2.3, the prompts must (1) generate the reasoning steps, (2) evaluate the answer to the steps, and (3) control the number of steps that are generated, the shape (or complexity) of the reasoning process must be controlled. We will now briefly discuss the three stages. Please refer to Figure 1 for a diagram of the different approaches for the generation, evaluation, and control of reasoning steps, and to Table 1.2\nPrompt for Step Generation The first order of business is to create a prompt that instructs the LLM to generate reasoning steps. The problem must be split into substeps. This can be achieved with a problem-specific prompt that contains elements of the problem, such as: \u201cFirst calculate how many marbles Mary had originally, then how many her friend had, and finally how many they had together.\u201d In general, it is possible to prompt an LLM to fill in the blanks in a step-by-step fashion. In the papers that we will discuss, there are three main approaches for generating the step-by-step prompt. The prompt may be (1) handcrafted for the problem by the researchers (hand-written prompt), or (2) the prompt or prompts may come from an source that is external to the model, such as another model or a dataset (prompt using external knowledge), or (3) the model itself can be prompted to generate a (series of) prompt(s) to analyze the problem (model-generated prompt). As we will see, all three approaches have their advantages and disadvantages. Generating the subproblem-steps is the first stage that is necessary for in-context learning to perform reasoning. Each paper in our survey performs at least this stage of the reasoning pipeline. In some of the early papers (around 2022) it is the only stage of the pipeline that is performed.\nPrompt for Result Evaluation After the prompt has been generated and the model has answered it, the next step in the reasoning pipeline is to evaluate the answer. Again, we see three main approaches for substep evaluation. First, the steps may be evaluated by (1) the model itself (self-assessment). Second, (2) an external program can be used to evaluate the steps. For example, when the steps are expressed as computer code, an external interpreter or compiler can be used to check the validity and the outcome (tool-based evaluation). Finally, (3) an external model can be used, LLM or otherwise. For example, in robotics, an external physics model can determine if certain actions are physically possible (external model validation).\n2We show the approaches in the Figure in their main category only. Some approaches show innovations in two categories, and are shown twice. (Since all approaches have a generation, an evaluation, and a control aspect, all could in principle occur three times\u2014all three columns can be found in Table 1).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6df2/6df20317-5de7-461f-b6e9-d8d0f7a0e131.png\" style=\"width: 50%;\"></div>\nFigure 1: Taxonomy of LLM-Reasoning Approaches: Prompt Generation, Evaluation and Control\nPerform Control of Reasoning Steps A reasoning process that consists of multiple steps is a sequential decision process [Littman, 1996]. When a single chain of reasoning steps is generated, the control flow of the reasoning process is simple: greedily evaluate the first step and then the next one, if present. The control flow of the reasoning process may also be more intricate. Some reasoning problems can be divided into multiple subproblems. To execute, evaluate and combine the results of all substeps, a separate controller may be needed. This controller can be a prompt or an external algorithm. Again, we distinguish three approaches. Most papers use (1) a greedy selection approach: a single prompt with a single chain of steps is generated, and these steps are directly executed and followed. The second approach (2) is to generate an ensemble strategy of reasoning steps, evaluate them, combine the individual results, and present them as the result of the ensemble. Finally, (3) a full tree-search or a reinforcement learning (RL) algorithm can be used as scaffolding. In this case, when a step is followed and evaluated, the LLM can roll back and try a different reasoning step. This is a breadth-first search approach [Plaat, 2020]. Going further, a full reinforcement learning approach can be used [Sutton and Barto, 2018, Plaat, 2022] to find an optimal policy for the sequential decision process. A full Markov Decision Process of state, action, transition, and reward function is specified, and step control can become a process where prompts are generated dynamically.\nDomain Many papers are applied to math word problems (natural language descriptions of math problems). Math problems were the original inspiration for the experiments with reasoning in LLMs. Other application domains include autonomous agents, robotic movement, generating computer programs, and playing computer games. We will discuss these in more detail with the individual approaches.\nTaxonomy Table Table 1 lists the papers of this survey. They are listed by the domain they work on, the type of prompt generation, the evaluation of the result, and the control method. The approaches in the table are grouped, divided by horizontal lines. The first group, from Scratchpad to Self-ask, focuses on creating a prompt that generates the reasoning steps. The entries in the cells of this column are shown in bold, highlighting the focus of the approaches. The approaches in this group can be considered to be the start of the field of LLM-reasoning. The Chain-of-thought approach is especially an inspiration for many works. The prompts are often written \u201cmanually\u201d by the researchers, the steps are encoded in one prompt, and step control is greedy. There is no specific evaluation of the steps, other than comparing results to the benchmark. The Scratchpad approach is special in that it uses supervised learning, not promptlearning; the work showed that LLMs can be made to generate internal reasoning steps by supervised learning, paving the way for the later prompt-based papers. The second group, from Self-verification to Self-taught-reasoner, focuses on evaluation of the reasoning steps in the prompt. This column is shown in bold in the table. The approaches in this group aim to improve the Chain-of-thought results by reducing the error accumulation that occurs when multiple steps are taken in a reasoning chain. A variety of step control methods is used by these approaches, which is dis-\n<div style=\"text-align: center;\">Table 1: Taxonomy of approaches: Generation, Evaluation, and Contro</div>\nTable 1: Taxonomy of approaches: Generation, Evaluation, and Control\nApproach\nDomain\nStep generation\nStep evaluation\nStep control\nScratchpad [Nye et al., 2021]\nmath word\nhand-wr/supervised\n-\ngreedy/1prompt\nChain-of-thought [Wei et al., 2022b]\nmath word\nhand-written\n-\ngreedy/1prompt\nZS-CoT [Kojima et al., 2022]\nmath word\nhand-written\n-\ngreedy/1prompt\nAuto-CoT [Zhang et al., 2022]\nmath word\nmodel-generated\n-\nclustering\nComplexity [Fu et al., 2022]\nmath word\nhand-written\nself-consistency\ngreedy/1prompt\nSelf-ask [Press et al., 2022]\nmath word\nexternal knowledge\nLLM\nmulti-hop questions\nSelf-verification [Weng et al., 2022]\nmath word\nhand-written\nback-verify\nensemble\nSelf-consistency [Wang et al., 2022b]\nmath word\nhand-written\nmajority\nensemble\nCodex [Chen et al., 2021]\ncode\n-\ntool-based\n-\nSelf-debugging [Chen et al., 2023]\ncode\nhand-written\ntool-based\ngreedy\nFun-search [Romera-Paredes et al., 2024]\ncode\nhand-written\ntool-based\nevolutionary algorithm\nLLaMEa [van Stein and B\u00a8ack, 2024]\ncode\nhand-written\ntool-based\nevolutionary algorithm\nMathPrompter [Imani et al., 2023]\nmath\nhand-written\ntool-based\nensemble\nProgram-of-thoughts [Chen et al., 2022]\nmath word\nhand-written, Codex\nPython+Consist.\ndecouple reason/compute\nProgram-aided-language [Gao et al., 2023]\nmath word\nhand-written, Codex\nNLP/Python\nensemble\nRefiner [Paul et al., 2023]\nmath word\nfinetune\ncritic model\ngen/crit feedback\nSelf-corrector [Welleck et al., 2022]\nmath word\nfinetune\ncorrector model\ngen/corr feedback\nSelf-improvement [Huang et al., 2022a]\nmath word\nfinetune\nself-assessment\nCoT/consistency\nSay-can [Ahn et al., 2022]\nrobot\nmodel-generated\nexternal model\ngreedy\nInner-monologue [Huang et al., 2022b]\nrobot\nhand-written\nvarious\ngreedy\nSelf-taught-reasoner [Zelikman et al., 2022]\nmath word\nfinetune\naugmentation\ngreedy/feedback\nLeast-to-most [Zhou et al., 2022]\nmath word\nhand-written\nself-assessment\ncurriculum\nProgressive-hint [Zheng et al., 2023]\nmath word\nmodel-generated\nself-assessment\nstable prompt\nSelf-refine [Madaan et al., 2023]\nmath word\nmodel-generated\nself-assessment\ngreedy/feedback\nTree-of-thoughts [Yao et al., 2024]\npuzzles\nmodel-generated\nself-assessment\nBFS/DFS\nBuffer-of-thoughts [Yang et al., 2024]\nmath word\nthought template\nself-assessment\nbuffer manager\nBeam-search [Xie et al., 2024]\nmath word\nmodel-generated\nself-assessment\nBeam Search\nReAct [Yao et al., 2022]\naction\nexternal knowledge\nself-assessment\nreinforcement learning\nReflexion [Shinn et al., 2024]\ndecision\nmodel-generated\next model\nreinforcement learning\nVoyager [Wang et al., 2023]\nMinecraft\nmodel-generated\nMinecraft\nreinforcement learning\ncussed in more detail later. Note that not all approaches use natural language problems (often math word problems). For example, the subgroup of Codex to Program-aidedlanguage focuses on formal languages. They generate code or math equations, typically in Python, to formalize the steps of the reasoning problem, or as result of the task. LLMs are quite good at code generation, and these approaches typically achieve good performance. The use of code also allows the approaches to call external programs such as interpreters and debuggers to evaluate the correctness of the reasoning steps that are generated. There is also a special subgroup, Refiner to Self-improvement, that does not use prompt learning but finetuning. Here, new data is generated based on reasoning exemplars, which is then used to further train the model. The extra data is often generated as a separate dataset, sometimes called critic or corrector. There are two approaches, Say-can and Inner-monologue, whose application domain is control of robot movement. Robotic movement is constrained by the laws of physics (both in the body of the robot as in aspects of its environment). The laws of physics are learned and used to ground the reasoning steps in reality (to reduce hallucination). The third group, Least-to-most to Voyager, addresses step control (approaches shown in bold in this column). Whereas in the previous approaches the reasoning steps are written in a single, static, prompt, these approaches generate the steps in multiple, dynamic, prompts. This allows control of the space of reasoning steps. Various search control approaches are used, all in the form of an external algorithm that performs calls to the LLM with different prompts. The control methods range from simple greedy and depth-first search to elaborate beam search and reinforcement learning schemes. In summary, we see a diverse array of methods that often achieve high performance in reasoning about their respective domains. To better understand the approaches, let us discuss the techniques in more detail, starting with the generation of steps.\n# 5.1 Generation of Steps\nOriginally, LLMs performed poorly on math word problems (GSM8K [Cobbe et al., 2021]). Some different approaches were tried, for example scaling up the size of the LLM [Rae et al., 2021]. The LLM architecture, based on transformers, is designed to produce a single token. When we prompt such an architecture to produce an answer, it does so. What we should do is prompt it to follow intermediate steps, answer those, and thus work towards the final answer, just as a student is taught to break down a complex problem into smaller steps. We should take the model by its hand and teach it to write down the intermediate steps, and combine the intermediate results [Nye et al., 2021]. This idea was used by Nye et al. [2021] in Scratchpads, a transformer model that performs multi-step computations by asking it to emit intermediate computation steps into a scratchpad. They train the model by supervised learning (not prompt-based in-context learning). Figure 2 shows an example. On experiments with addition, polynomial evaluation, and Python code execution, versions that produced the intermediate steps on a scratchpad performed considerably better than versions that did not.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3adf/3adf7b03-4139-42fc-aebd-89b761ac1f87.png\" style=\"width: 50%;\"></div>\nFigure 2: Example of input and target for supervised learning on a long addition problem of adding two numbers. The carry is recorded in the C: digit. Comments (after #) are not part of the learning target [Nye et al., 2021]\nIf supervised learning can produce intermediate steps, would prompt-learning be able to do so too?\n# If supervised learning can produce intermediate steps, would prompt-learning be able to do so too?\n# 5.1.1 Hand-written Prompt\nThis question was studied by Wei et al. [2022b], amongst others. A basic way to instruct an LLM to generate steps by prompt-learning is to manually write a prompt for the large language model to follow the reasoning steps. They showed in their Chain-ofthought paper that with such a prompt the LLM follows such intermediate steps. When the LLM is prompted to rephrase information from the question as intermediate reasoning steps in its answer, the LLM performed much better than when it was prompted to answer a math problem directly, without reproducing the information from the question in its answer. The example from the Chain-of-thought paper is shown in Figure 3 Wei et al. [2022b]. Performance figures were given in Section 3 on benchmarks. The substantial performance improvement by Chain-of-thought has caused much excitement and has opened up further research on reasoning with LLMs. In the original Chain-of-thought paper the prompts were handwritten by the researchers for the individual types of problems, and evaluations are conducted with five different benchmarks (not by an LLM).3 In a later work the prompts were generated automatically by the LLM [Zhang et al., 2022]. Kojima et al. [2022] go a step further. They show that the simple addition of a single text to the prompt (Let\u2019s think step by step) significantly improves performance. Since this text does not contain problem-related elements, this can be considered as a form of zero-shot learning. Figure 4 compares the approaches. Experiments further show that with this addition to the prompt, significant performance gains are achieved\n3The Chain-of-thought idea is about prompt generation, not about the evaluation or the search control of the reasoning steps. Hence, in Table 1 Chain-of-thought is labeled as greedy without an evaluation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/677b/677bb21c-b1c8-4f25-8489-1894c604c9c5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Chain of Though Prompting. In blue at the top the prompt, in green at the bottom the answer. When shown the longer example prompt, the LLM follows the longer example when answering the question [Wei et al., 2022b].</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c23a/c23a86a6-568a-43d4-bd91-1340e907f536.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">4: Zero-shot Chain-of-thought: Let\u2019s think step by step [Kojima et al., 20</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f067/f0673467-6f9e-4c06-a4d6-7de907abb511.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bc02/bc0234b0-ae9e-4428-9acd-a48a3addfa5a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bbb0/bbb01712-efd6-47db-90d5-f3c5b7207585.png\" style=\"width: 50%;\"></div>\nFigure 5: Self-Ask asks follow-up questions, and uses an external search engine [Pres et al., 2022]\non a diverse set of reasoning benchmarks, including arithmetic, symbolic, and logical reasoning. The Chain-of-thought idea itself is inspired by earlier work where natural language steps are generated for arithmetic reasoning [Ling et al., 2017, Cobbe et al., 2021], and the use of formal languages for reasoning [Roy and Roth, 2016, Chiang and Chen, 2018, Amini et al., 2019, Chen et al., 2019].\n# 5.1.2 Prompt using External Knowledge\nChain-of-thought shows that an LLM gives better answers to complex problems when it is guided to take individual steps. Prompts are written manually, from scratch, by the researchers. We can use external information about the problem to improve the prompt. Press et al. [2022] study how subproblems are related to the main problem, which they call compositional reasoning. They study how often a model is able to answer the subproblems, but not the overall problem. This difference is called the compositionality gap. They find that in GPT-3, as model size increases, the compositionality gap does not decrease: the single-hop question-answering performance improves faster than the multi-hop performance. This shows that while more powerful models memorize and recall more factual knowledge, no improvement in their ability to perform compositional reasoning occurs. They find that the ability to reason does not depend on the size of the model. Subsequently, a method called Self-ask is proposed, that asks elicitive follow-up\nquestions (like Chain-of-thought, but with the follow up: prompt), see Figure 5. The model is then used to answer these follow-up questions. Self-ask can also use an external search engine to answer intermediate prompts, instead of the model. The model takes as input a compositional question which it decomposes. The initial subquestion is fed into the search engine, and the answer is processed by the model, which generates another subquestion, and so on, until it produces the final answer. The approach performs a few percentage points better than vanilla Chain-of-thought on three benchmarks that were specifically designed for multi-hop questions.\n# 5.1.3 Model-Generated Prompt\nIn addition to manually writing prompts or using external information, we can also try to let the LLM itself study the problem to write the best reasoning-prompt, a form of self-improvement. An example of this approach is Auto-chain-of-thought [Zhang et al., 2022]. This approach builds on the observation by Kojima et al. [2022] that large language models are zero-shot reasoners. First, Auto-chain generates specific questions for a given dataset and partitions them into clusters. Then an external algorithm uses the model to generate examples that are sampled for diversity. The constructed demonstrations augment the in-context prompt. The automatically generated prompts are reported to perform as well or better than the hand-written Chain-of-thought prompts on ten benchmarks using GPT-3. Fu et al. [2022] introduce Complexity-based prompting. Inspired by Chain-ofthought and Self-consistency, this work studies which prompts achieve the best results on math word and other reasoning problems. Their work specifically studies the impact of the complexity of the reasoning chain, and introduces a related reasoning approach (Complexity-based prompting). They find that prompts with the largest complexity (the most reasoning steps) perform best. Further, they find that outputs (answers) with the highest complexity are the best. Complexity-based prompting achieves high performance on three math reasoning benchmarks. Another approach that uses model-generated prompts is Buffer-of-thoughts. We will discuss this approach in Section 5.3.3.\n# 5.2 Evaluation of Steps\nAfter discussing prompts for the generation of reasoning steps, the next stage in the reasoning pipeline (Section 2.3) is evaluation of the results of the steps, to reduce the error of multi-step reasoning chains. We will start with approaches where the same model performs step-generation and step-evaluation.\n# 5.2.1 Self-Assessment\nWhen LLMs are prompted to perform reasoning steps, they perform a sequence of steps and predict multiple tokens. Performing a sequence of steps makes them sensitive to mistakes and vulnerable to error accumulation [Weng et al., 2022, Xiao et al., 2023a]. Several methods have been developed to prevent error accumulation. One approach is\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05e5/05e549d0-f735-4db0-8c27-80596d17b76a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Self-Consistency [Wang et al., 2022b]</div>\nto create a new model to separately evaluate the results. Shen et al. [2021] and Li et al. [2022b] train an external verifier to check results. In contrast, Weng et al. [2022] propose an automated approach using evaluation by the same LLM, called Self-verification. They note that human reasoning also suffers from the problem of accumulating errors, and that in human reasoning we frequently revisit our thought process to verify the accuracy of our reasoning steps. Thus, they propose to apply such a backwards self-verification approach. The LLM is prompted to use the conclusion of the Chain-of-thought reasoning chain as a condition for solving the original problem and then compare the answer going back to the original question. The LLM is given variations of its own conclusion and is instructed to choose the one with the highest similarity to the original question. (Note that there can be feedback issues using an LLM to evaluate itself, for a discussion see Zheng et al. [2024].) Experiments are reported on GPT-3 [Chen et al., 2021] and on Instruct-GPT [Ouyang et al., 2022]. The performance of Chain-of-thought was improved by a few percentage points on arithmetic and general reasoning tasks. A popular related approach is called Self-consistency [Wang et al., 2022b]. Selfconsistency is a straightforward ensemble approach. Greedy single-path decoding is replaced by sampling diverse reasoning paths, evaluating them, and selecting the most consistent answer. Self-consistency asks the LLM to simply perform the same query multiple times, and takes the majority-vote of the answers. Self-consistency works since complex reasoning problems typically allow different reasoning paths that lead to the correct answer. Figure 6 summarizes the approach. Self-consistency has been evaluated on arithmetic reasoning, commonsense reasoning and symbolic reasoning, on a variety of LLMs, including GPT-3 [Tay et al., 2022, Brown et al., 2020, Thoppilan et al., 2022, Chowdhery et al., 2023]. Self-consistency improves the performance of Chain-of-thought typically by 10-20 percentage points, and has been used as a baseline in many of the other approaches in this survey. (Selfverification also reports that performance is improved when used in combination with\nSelf-consistency [Wang et al., 2022b] and with Program-aided-language [Gao et al., 2023].)\n# 5.2.2 Tool-based Validation\nAnother possibility to improve the accuracy of evaluating the reasoning steps is to switch from a natural to a formal language. The advantage of a formal language is that it is less ambiguous than a natural language. Examples are computer languages, such as Python, or mathematical equations. Using a formal language for reasoning is a popular approach, and we discuss seven papers. Many approaches generate the steps in Python, and the code can then be evaluated by a formal evaluator, such as a compiler, debugger, or interpreter. LLMs have been quite successful in generating computer code from natural language prompts. Chen et al. [2021] introduced Codex, a GPT model that was trained on publicly available code in the repository GitHub. A production version of this work was introduced under the name GitHub Copilot. Codex is able to generate correct programs from descriptions in natural language, such as commentary strings. Figure 7 shows examples that are produced by Codex. The work on Codex is used as a basis for further research on reasoning in LLMs. Human programmers, when writing code, typically follow a cycle of writing some code, executing it to look for errors, and then using the feedback to improve the code. This same approach is followed in the Self-debugging work [Chen et al., 2023]. Selfdebugging teaches a large language model to debug its generated program code via few-shot demonstrations. It follows the same steps of (1) code generation, (2) code execution, and (3) code explanation (see Figure 8). Self-debugging is able, without human feedback on the code\u2019s correctness or error messages, to identify mistakes in the code that was generated by itself from investigating the execution results. Self-debugging can also provide an explanation of the generated code in natural language. It achieves strong performance on text-to-SQL generation, C++-to-Python transcoding, and text-to-Python generation. Several works use self-debugging to generate working code tuned for solving specific problems automatically, without human feedback. Romera-Paredes et al. [2024] introduced FunSearch, an approach that integrates formal methods and LLMs to enhance mathematical reasoning and code generation. FunSearch is capable of producing functionally correct programs that adhere to specified requirements. It uses a genetic algorithm approach with multiple populations of candidate solutions (programs), which are automatically evaluated (using tools depending on the problem specification). In addition to the problem specification in the form of an evaluate function, also an initial program is given to the LLM in the first prompt. After evaluating a number of generated programs from the starting prompt, a new prompt using \u2018best-shot prompting\u2019 is created in an iterative fashion, combining a selection of k sampled programs in a sorted list (ascending according to their evaluation score), and the LLM is requested to generate program k + 1. Another work leverages evolutionary computation methods to generate and optimize evolutionary algorithms [van Stein and B\u00a8ack, 2024]. This approach, LLaMEA (Large Language Model Evolutionary Algorithm), utilizes LLMs to design and optimize evolutionary algorithms. The approach uses LLMs to generate ini-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d8b5/d8b586da-0c5a-4009-bb1b-1af7a7320579.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Codex [Chen et al., 2021]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d97f/d97f1e99-029b-4116-b58d-60e9326e8f32.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Self-Debugging [Chen et al., 2023]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/05d6/05d64fcb-5f54-450c-befb-0fc1b9801dac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Program-aided-language [Gao et al., 2023]</div>\ntial algorithmic structures, which are then refined through mutation and selection. This enhances the efficiency of algorithm design, particularly in fields requiring innovative and adaptive solutions. A key difference between FunSearch and LLaMEA is that LLaMEA uses a sample-efficient elitism strategy by iteratively improving the best-sofar solution, requiring significantly fewer prompt evaluations than the large-population strategy proposed in FunSearch. To improve prompt-based reasoning, Codex is used in an ensemble approach named MathPrompter [Imani et al., 2023]. This approach generates multiple algebraic expressions or Python functions, which then solve the same math problem. The results are compared, just like in Self-consistency and Self-verification, raising the confidence level in the results. MathPrompter achieved state-of-the-art results on the MultiArith dataset (78.7% \u219292.5%), evaluated on GPT-3 175B. Two other approaches that use a formal language are Program-of-thought (PoT) [Chen et al., 2022] and Program-aided-language (PAL) [Gao et al., 2023]. Both approaches use the LLM to generate Python and then use a Python interpreter to evaluate the result. PoT and PAL are similar approaches. PoT uses benchmark-specific prompts; PAL uses generic prompts, and has been tested on more benchmarks and has been used in other approaches. Figure 9 illustrates the PAL approach. When the evaluation of the reasoning steps is offloaded to the Python interpreter, decomposing the natural language problem into executable code-steps remains the only task for the LLM. (Earlier work in mathematical word problems, such as Ling et al. [2017], showed how to decompose a problem and reach an answer.) Gao et al.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b53f/b53f603c-eb28-4550-ba03-10418b5062d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Refiner [Paul et al., 2023]</div>\n[2023] provide extensive experimental evidence about the synergy between the neural LLM and the symbolic interpreter. Experiments are performed over 13 mathematical, symbolic, and algorithmic reasoning tasks, achieving more accurate results than much larger models.\n# 5.2.3 External Model Validation\nWe have seen many successful examples of prompt-based in-context reasoning and evaluation. We will now look at related reasoning approaches that follow a more traditional parameter learning approach. We describe three natural language approaches that follow this route. All approaches evaluate the output of the model and generate corrective data. That data is then added to the training pipeline, and the model is subsequently finetuned.\nFinetuning The Refiner approach [Paul et al., 2023] uses a generator model and a critic model to provide fine-grained feedback on reasoning errors. The generator generates multiple reasoning hypotheses, the critic evaluates results by randomly selecting a hypothesis for feedback. The generator model is finetuned based on its reasoning errors. A small supervised model is used to overcome the cold-start problem. Figure 10 shows an example of how the critic provides feedback to the generator. The approach is reported to work well on math word problems and synthetic natural language reasoning.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6ca/b6cacf17-7613-4737-aca6-d09a7d7fd1e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Self-Taught-Reasoner [Zelikman et al., 2022]</div>\nWelleck et al. [2022] follow a similar approach in their Self-correction approach. The corrector is a separate model specialized in refining the outputs of the generator. Unlike Refiner, where the generator is finetuned based on the critic feedback, Selfcorrection finetunes the corrector to rectify errors in the hypotheses produced by the generator. A third finetuning approach is Self-improvement, by Huang et al. [2022a]. Here too the base model data is augmented by LLM-generated rationales, and then finetuned. Noteworthy in all three finetuning approaches is that LLMs are capable of improving themselves by training on their own generated output, and that stability problems inherent in feedback loops are overcome.\nDataset Augmentation The final finetuning approach that we discuss uses dataset augmentation. An explicit intermediate reasoning is called a rationale. Rationale generation has been shown to be valuable for LLMs across diverse tasks such as mathematical and commonsense reasoning, code evaluation, social bias inference, and natural language inference [Zelikman et al., 2022]. Zelikman et al. [2022] describe how reasoning steps are used to create rationales, that are then used to augment the dataset on which the model is finetuned. The approach is called Self-taught-reasoner. Figure 11 illustrates the approach. In Self-taught-reasoner, an augmentation dataset is created by attempting to solve the original dataset using the current model\u2019s rationale generation ability in each iteration. Next, the dataset is augmented using rationalizations, using ground-truth answers to problems the model failed to solve. Finally, the large language model is finetuned on the combined dataset.\n# Reasoning about Robot Behavior\nReasoning about Robot Behavior In addition to math word problems, prompt-based reasoning has also been used to reason about robot behavior. Language models contain a large amount of information about the real world [Ahn et al., 2022]. In theory, this should allow the model to exhibit realistic reasoning about robotic behavior. However, the models do not have knowledge about particular embodied aspects of a particular robot. If we could compare a Scratchpad-like list of intermediate reasoning steps with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a0ef/a0efe9b9-dcd0-4ae0-973c-b6b615b660f8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Say-Can compared to other language models [Ahn et al., 2022]</div>\na list of possible movements of the robot in its environment, then we could prevent the model from suggesting impossible joint movements and actions, and prevent accidents. Such an approach has been tried in the Say-can paper [Ahn et al., 2022]. Saycan learns a value function [Kaelbling et al., 1996] of the behavior of a robot in an environment using temporal difference reinforcement learning Sutton [1988]. This value function is then combined with prompt-based reasoning by the language model, to constrain it from suggesting impossible or harmful actions. The goal of Say-can is to ground language in robotic affordances. In contrast to Scratchpad, which used supervised learning, the affordance model is learned interactively by reinforcement learning, and then applied using prompt-based learning by the LLM. The robot can act as the language model\u2019s hands and eyes, while the language model has high-level semantic knowledge about the task. The LLM (Say) provides a task-grounding to find the actions to achieve the high-level goal. The learned affordance function (Can) provides a world-grounding to allow what is possible. Say-can is evaluated on 101 real-world robotic tasks, such as how to solve tasks in a kitchen environment (see Figure 12). Where Say-can learns affordance as a separate function, another approach, Innermonologue [Huang et al., 2022b] formulates robotic planning directly as part of the language prompt. This approach incorporates environmental information into the prompt, linguistically, as an inner monologue. As in Say-can, the information comes as feedback from different sources. Unlike Say-can, the information of physics and the world is inserted directly into the prompt. Inner-monologue consists of many elements: it uses InstructGPT [Brown et al., 2020] for multi-step planning, scripted modules for object recognition, success detection, task-progress scene description, and language-conditioned pick-and-place primitives, similar to CLIPort [Shridhar et al., 2022]. These elements generate textual descriptions that are used in prompt-based learning. Figure 13 gives an example of the working of Inner-monologue. The language feedback that is thus generated significantly improves performance on three domains, such as simulated and real table top rearrangement tasks and manipulation tasks in a kitchen environment. There are many studies into robotic behavior. A recent approach related to Inner-monologue is Chain-of-tools, which proposes a plan-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/381b/381b3f85-9c7c-4d70-be74-9b0de5d59da7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: Inner-Monologue [Huang et al., 2022b]</div>\nexecute-observe pipeline to ground reasoning about tool behavior [Shi et al., 2024a,b]. This concludes our discussion of the second stage of the reasoning pipeline, evaluation of the reasoning steps.\n# 5.3 Control of Steps\nThe third stage in the reasoning pipeline in Section 2.3 is reasoning control. This stage controls how many sub-steps are generated, and how deep into the future the reasoning chain is generated. There are three main approaches: (1) greedy selection, which generates a step and then follows it, (2) ensemble strategy, which generates a set of possible next steps, and (3) a full tree-shaped search which generates multiple options for the step, and follows them multiple steps into the future, traversing a search tree with backtracking, controlling an exponential search space. We include reinforcement learning approaches, that interactively learn an optimal policy for such a reasoning space.\n# 5.3.1 Greedy Selection\nMost earlier works on prompt-based reasoning follow the greedy approach: generate a single prompt with a sequence of steps and follow them. Among the greedy reasoners are Chain-of-thought, Auto-CoT, and Zero-shot CoT. Inner Monologue and Say-Can also use greedy reasoning. In Least-to-most prompting [Zhou et al., 2022], the key idea is to break down a complex problem into simpler subproblems and then solve these in sequence, explicitly encoding them in the prompt. It is related to Complexity-based prompting. In Least-tomost, finding the answer to each subproblem is facilitated by the answers to previously solved subproblems, as in a curriculum [Bengio et al., 2009]. The authors find that on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d770/d770900b-6b47-4804-80a6-2d59aa6087a7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Least-to-most prompting [Zhou et al., 2022]</div>\nsymbolic manipulation, compositional generalization, and math reasoning, the Leastto-most prompting is capable of generalizing to more difficult problems than those that are given in the prompts. Figure 14 illustrates the idea.\n# 5.3.2 Ensemble Strategy\nThe second kind of reasoning control is based on an ensemble of (sequences of) reasoning steps. The ensemble approach is a well-known technique in machine learning to make a strong learner out of multiple weaker learners [Sagi and Rokach, 2018, Breiman, 2001]. For most problems, multiple different options for the next step exist. When all or some of these are generated and evaluated, then the best result or the consensus result can be reported as the outcome of an ensemble of steps. Various approaches have been proposed. We already mentioned Self-consistency [Wang et al., 2022b] and Self-verification [Weng et al., 2022] in Section 5.2.1. They are popular ensemble approaches to evaluate the results of reasoning steps in prompt learning. The greedy single-path decoding used in Chain-of-thought prompting is replaced by sampling a diverse set of reasoning paths, evaluating them, and selecting the most consistent answer. In another domain Chain-of-experts builds on Chain-of-thought with a mixture of experts ensemble for complex combinatorial operations research problems [Xiao et al., 2023b]. PAL and MathPrompter also use the ensemble approach. They generate multiple steps, which are evaluated and whose answer is combined, or the best step is chosen. The ensemble approach is a popular approach in LLM-reasoning.\n# 5.3.3 Reinforcement Learning\nIn the greedy approach, a single reasoning path is generated and traversed. In reasoning, often multiple valid reasoning steps are possible, but pursuing all possibilities over multiple reasoning steps may lead to an infeasible number of possibilities. The third kind of reasoning control is to use a full-fledged controller that can traverse a tree, or even perform reinforcement learning to do so [Sutton and Barto, 2018, Kaelbling et al., 1996, Plaat, 2022]. This group of control approaches enables the most elaborate control of the reasoning process, and is used by many works, as we will see. When decomposing the problem, multiple alternative steps are generated that can be searched multiple steps into the future. Then, backtracking can be performed, allowing alternative steps to be tried. Where greedy and ensemble processes can be controlled with a prompt by the LLM, this third group is more complex, and an external algorithm is used to control the reasoning process. The external algorithms call the LLM as a subroutine prompting it to perform its tasks. This allows more complex reasoning control, but we are no longer performing prompt-based self-reasoning; control has been given to an algorithm that is external to the LLM and external to prompt-learning. We start our discussion of control strategies with depth-first and breadth-first search, then go to beam search, and then to full reinforcement learning.\nBreadth first search A complex reasoning space can be traversed with a search algorithm. Tree-of-thoughts includes a search algorithm to dynamically follow different reasoning steps [Yao et al., 2024]. When one reasoning path has been traversed, a search algorithm can backtrack, and try an alternative path. The paper describes both a breadth-first-search and a depth-first-search controller. The evaluation part in Tree-of-thoughts is performed with a prompt by the LLM. Together, the trio of generation, evaluation, and control allow systematic exploration of the space of reasoning steps with look-ahead and backtracking. The authors compare their approach to Chain-of-thought and Self-consistency. Chain-of-thought builds a reasoning out of a path of thoughts, Self-consistency creates an ensemble of thoughts, and Tree-of-thoughts constructs a tree structure. Figure 15 illustrates the different reasoning structures.4 Another approach, Buffer-of-thoughts [Yang et al., 2024], goes a step further towards meta-reasoning. It introduces a meta-buffer that stores high-level thought-template These universal thought-templates are derived from a variety of tasks. Figure 16 compares the Buffer-of-thoughts approach to other approaches such as Chain-of-thought and Tree-of-thoughts. Buffer-of-thoughts outperforms other methods in puzzles such as Game of 24 and checkmating. Thought templates are related to metacognition (thinking about thinking), which is further discussed in Section 6.2.3.\nBeam search A related search method is Beam-search. Beam-search-for-reasoning [Xie et al., 2024] focuses on control of the space of possible reasoning paths. In some\n4A similarly named approach is Graph-of-thoughts [Besta et al., 2024]. Graph-of-thoughts allows mor general reasoning graphs, providing a formal framework, where the different elements can then be specified manually.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/169c/169ce0a9-3f26-48b2-a69e-106999a957f0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 15: Reasoning structure of Chain-of-Thought, Self-Consistency, and Tree-ofThoughts [Yao et al., 2024]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/64dc/64dc715f-a914-4820-a0f6-1ed1235ac274.png\" style=\"width: 50%;\"></div>\nFigure 16: Chain-of-Thought, Self-Consistency, and Buffer of Thoughts [Yang e 2024]\n<div style=\"text-align: center;\">Figure 16: Chain-of-Thought, Self-Consistency, and Buffer of Thoughts [Yang et al.,</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4bf1/4bf1c29f-b302-48a0-b416-9658052c5a8f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/566d/566d4d43-d576-4225-9205-938c2cac2c74.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a645/a6452c95-75e8-408b-af06-bf400aeb847e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 18: Reinforcement Learning [Sutton and Barto, 2018]</div>\nreasoning problems, this space can be very large. Beam-search solves this challenge by searching only a promising part of this space. It uses self-evaluation to control exploration and to evaluate (decode) reasoning steps. Figure 17 shows how Beamsearch self-evaluation is used in multi-step reasoning. Beam search uses Programaided-language models for math word problems [Gao et al., 2023]. Using a Codex backbone [Chen et al., 2021], it surpasses the few-shot baselines by 6.34%, 9.56%, and 5.46% on the GSM8K, AQuA, and StrategyQA benchmarks, respectively.\nReinforcement learning Reinforcement learning (RL) methods are another step in the sophistication of optimization algorithms. RL learns by interactive sampling, improving its policy based on rewards from the environment [Sutton and Barto, 2018]. To use reinforcement learning, the reasoning problem must be formulated as a Markov Decision Process: the agent-algorithm creates a prompt (an action), to sample a step (t) and get an answer (state, reward) from the environment-model (see Figure 18). The answer can then be used to improve the prompt (next action), just like reinforcement learning uses rewards to improve its policy of best actions for each state. The approaches that use reinforcement learning do so in the form of an external algorithm. No prompt has been created that performs RL by itself. Progressive-hint-prompting (PHP) uses reinforcement learning to interactively improve prompts [Zheng et al., 2023]. Figure 19 illustrates the approach. PHP is an\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7697/7697ceb6-eebb-4ba1-86c0-17ffe6000949.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 19: Progressive Hint Prompting [Zheng et al., 2023]</div>\nexternal algorithm that calls the LLM with dynamic prompts, using previously generated answers as hints to progressively prompt the LLM toward the correct answers. It works as follows: (1) given a question (prompt), the LLM provides a base answer, and (2) by combining the question and answer, the LLM is queried and we obtain a subsequent answer. We (3) repeat operation (2) until the answer becomes stable, like a regular policy-optimizing reinforcement learning algorithm. The authors have combined PHP with Chain-of-thought and with Self-consistency. Using GPT-4, state-of-the-art performance was achieved in grade school math questions (95%), simple math word problems (91%) and algebraic question answering (79%). Another approach that is motivated by improving answers from feedback, is Selfrefine [Madaan et al., 2023]. In this method, initial outputs from LLMs are improved through iterative feedback and refinement. Like PHP, the LLM generates an initial output and provides feedback for its answer, using it to refine itself, iteratively. Figures 20 and 21 illustrate the approach. Self-refine prompts the LLM in three ways: (1) for initial generation, (2) for feedback, and (3) for refinement. Note that Self-refine follows a greedy reasoning chain, learning from feedback. Self-refine has been used with GPT-3.5 and GPT-4 as base LLMs, and has been benchmarked on dialogue response generation [Askari et al., 2024], code optimization, code readability improvement, math reasoning, sentiment reversal, acronym generation, and constrained generation, showing substantial improvements over the base models. Another approach that combines reinforcement learning and LLMs is ReAct [Yao et al., 2022]. Most works so far have focused on reasoning by the LLM, not on actions by an agent. A key element of reinforcement learning is that it learns a policy for\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1798/17984823-d8eb-4c7a-bdc8-9b8c30d0bfc2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 20: Self-Refine [Madaan et al., 2023]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b65/2b654d4d-43f7-4303-99c9-a360fed943cd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 21: Self-Refine [Madaan et al., 2023]</div>\nan environment. The goal of ReAct is to combine progress in reasoning with action plan generation. (Or, to put it differently, most approaches use RL to improve LLMreasoning, ReAct uses LLMs to improve RL agent policies.) ReAct uses Chain-ofthought prompt-learning as part of an RL framework that also uses external knowledge sources (Wikipedia) and finetuning, for error reduction, grounding, and for reducing hallucination. The framework allows hand-written prompts. Figure 22 shows four different prompting strategies. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with one or two in-context examples. The ReAct work has been developed further. Reflexion [Shinn et al., 2024] is built on top of ReAct. The goal is to create AI agents that learn by reflecting on failures and enhancing their results, much like humans do. Reflexion uses three language models: actor, evaluator, and reflector. It works as follows: (1) an actor generates text and actions, (2) an evaluator model scores the outputs produced by the actor, and (3) a selfreflection model generates verbal reinforcement cues to assist the actor to self-improve (see Figure 23). For the actor, Chain-of-thought [Wei et al., 2022b] and ReAct [Yao et al., 2022] can be used. Reflexion is evaluated on decision-making, reasoning, and coding tasks. Improvements of 10-20 percentage points are reported. Figure 24 shows three different prompting applications. To conclude this overview of reinforcement learning approaches, we discuss an application in the games domain. Voyager [Wang et al., 2023] is an agent for the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/761a/761a02af-6bbe-495a-ad13-4508ee3554fd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 22: Comparison of four prompting strategies: Standard, Chain of Thought (reason), Action only, and ReAct [Yao et al., 2022]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/44e2/44e24f8e-1b09-4946-9f7b-9022f77c3f21.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 23: Architecture of Reflexion [Shinn et al., 2024]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/87d2/87d2fbd1-5c20-4083-890b-869e662125ff.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 24: Comparison of three application areas [Shinn et al., 2024]</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7103/7103aba2-4475-49f4-bc59-a2d1e64656a1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 25: Performance of Voyager in Minecraft [Wang et al., 2023]</div>\ngame of Minecraft that uses an iterative prompting mechanism that generates code for embodied control. The mechanism includes Self-verification [Shinn et al., 2024]. The agent has a skill library and an automatic curriculum to maximize exploration. Voyager interacts with GPT-4 through prompts. The goal of Voyager\u2019s prompts is to discover as many diverse items in Minecraft as possible, a form of novelty search [Eysenbach et al., 2018]. Voyager performs well, it shows in-context lifelong learning capability and reaches high scores by acquiring many tools (see Figure 25).\n# 6 Discussion\nWe have reviewed approaches for prompt-based reasoning by LLMs, highlighting techniques that have achieved a breakthrough in reasoning performance. It is time for reflection on limitations in the approaches, suggesting promising areas of future work. First we discuss issues concerning hallucination, faithful reasoning, and scaling. Then we discuss what LLMs can and cannot do. Then, we highlight connections with sequential decision processes and metacognition, and end with a research agenda.\n# 6.1 Hallucination, Faithfulness and Scaling\nMost works on reasoning in LLMs are experimental in nature. The success of incontext learning and Chain-of-thought reasoning is attracting the attention of work providing deeper insight into the reasoning processes in language models. Saparov and He [2022] introduce a synthetic question/answer dataset designed to evaluate the reasoning abilities of LLMs. The work showed that LLMs are capable of\nreasoning to a certain degree, but that Chain-of-thought struggles with proof trees with a wide branching factor. In another study, Wang et al. [2022a] also aim to increase our understanding of how Chain-of-thought works. The authors find that it continues to work even with invalid steps in the reasoning chain. They also find that the order of the reasoning steps is important for good results. Prompts should be relevant to the question, and coherent (steps should be in the correct order). Jin et al. [2024] study the impact of reasoning step length on LLMs, finding a strong positive correlation between the length of the prompt and reasoning abilities. These works highlight ways in which LLM-reasoning can see things that are not there. Next, we discuss works on failure modes of the Chain-of-thought approach, studying whether the reasoning of the LLM is faithful, or that it gives the right answer for the wrong reason.\n# 6.1.1 Faithfulness\nChain-of-thought and other approaches prompt a language model to take certain steps to solve the problem that the prompt specifies. One can ask the question, whether those steps are indeed the steps that the model has followed (faithful reasoning) or whether it took another road to arrive at the correct answer (unfaithful reasoning). A few studies measure the faithfulness of reasoning by LLMs. Lanham et al. [2023] notes that just like organic reasoners, a model\u2019s reasoning may be post-hoc, it may be constructed after a certain conclusion has been found. By deliberately adding mistakes to the chain of thought, the authors measure the faithfulness of the model. They find a wide variation of post-hoc reasoning, with a tendency of larger models to be less faithful. Like regular LLMs, when not properly grounded, (Chain-of-thought) reasoning suffers from hallucination. Another study adds deliberate bias to the prompt. For example, in a multiplechoice setting, they always make answer (a) the correct answer [Turpin et al., 2024]. They find that a bias towards wrong answers can cause significant drops in accuracy, and that models frequently generate Chain-of-though explanations rationalizing wrong answers. The authors further note that, insofar as language models are trained on human-written explanations, that explanations may be incomplete or wrong. Human explanations may omit crucial steps of the causal chain, may provide an unfaithful account of the human reasoning process, or may be aimed at convincing others, instead of providing the true causes of a decision. To address issues of faithfulness, Lyu et al. [2023] propose Faithful-chain-of-though This approach involves two stages. First, the natural language query is translated into a formal symbolic language. Second, the problem-solving stage processes the formal language, and can explain the reasoning steps it has thus taken. For the symbolic language, Python, Datalog, or PDDL is suggested. Faithfulness studies tell us more about how models reason. Further surveys on this topic are Mondorf and Plank [2024], Chuang et al. [2024], Luo et al. [2023], Paul et al. [2024],\n# 6.1.2 Scaling\nThe emergent abilities of LLMs have prompted research into the nature of scaling and reasoning with LLMs, and, specifically, how reasoning capabilities can be transferred to smaller language models. Scaling laws of LLMs are an active area of study, see for example Kaplan et al. [2020], Henighan et al. [2020], Hoffmann et al. [2022]. Given the computational cost of LLMs, there is much interest in transferring knowledge to small language models. Comprehensive surveys on knowledge distillation are Xu et al. [2024], Gu et al. [2023]. For reasoning specifically, Magister et al. [2022] have studied reasoning in small language models, using a student model that learns from a teacher model, by finetuning. Another study related to Self-taught-reasoner [Li et al., 2022a] focuses on explanation in small language models, achieving similar results. Other works focus on prompt distillation for retrieval Dai et al. [2022], recommendation [Li et al., 2023], distillation to embodied agents of Chain-of-thought reasoning [Choi et al.], and distillation of LLM graph reasoning [Zhang et al., 2024]. Distillation of reasoning to smaller models can work surprisingly well in situations with more explicit instructions. Distillation is also proposed for bringing results of System 2 reasoning to System 1 Yu et al. [2024], which brings us to the topic of metacognition (see Section 6.2.3).\n# 6.2 Limitations: What LLMs Can and Cannot do\nThe capabilities of LLMs are impressive. LLMs can be seen as large text-based surrogate models of the world (or the world how we describe it on the internet), and thus allow us to reason in a way that we can understand about a large variety of contexts and problems. Reasoning tasks, such as math word problems, were one of the capabilities that LLMs could not achieve, until recently. Let us look more closely at what language models can and cannot do.\nWith the right prompt LLMs are able to solve many of the problems in reasoning grade school math word benchmarks. Prompt-based learning is able to perform reasoning tasks such as math word problems, robotic movement, and Python code generation, at inference time, without expensive parameter training. We note that a simple taxonomy of generate-evaluate-control is able to describe the structure of the current LLM reasoning literature well. Furthermore, the accuracy of the reasoning chains can be improved with ensemble methods, or self-verification. Hallucination can be reduced by grounding the model with external models, such as for robotic affordances, and information retrieval from search engines and Wikipedia. Going a step further, using external control algorithms (such as search or RL) as scaffolding, dynamic prompts can use the LLMs to perform complex and interactive reasoning patterns. Note that the reasoning control is now two layers away from the core LLM: an external control algorithm, on top of in-context-learning, dynamically generating prompts for the LLM. This is reasoning with prompts with LLMs, not by.\nAt this point, it is interesting to note the confluence of the two schools of classical artificial intelligence (AI), symbolic and connectionist.5 Search and reinforcement learning are rooted in the symbolic AI tradition, while LLMs are rooted in the connectionist tradition. The literature in this survey combines the two traditions. High performance reasoning is created with a (symbolic) searcher/learner on top of a (connectionist) LLM. In other fields similar combinations can be seen (for example, AlphaFold Bryant et al. [2022], Jumper et al. [2021] and retrosynthesis of molecules Segler et al. [2018]). The LLM helps ground symbolic reasoning methods in language; symbolic methods help create prompts that let the LLM perform reasoning. How the two traditions will continue to improve eachother, we will see in further research. We note that benchmarks such as GSM8K have been central for the progress in the field, and that while reasoning started with math word problems, the field has extended to robotics, autonomous agents, games, and most emphatically computer code. Formal languages play an important role in the intermediate multi-step reasoning chains. A side effect from the work on reasoning is the emergence of a new few-shot learning approach for sequential decision-making processes (SDP)[Littman, 1996]. Traditionally these processes are solved with reinforcement learning (such as DQN Mnih et al. [2015], PPO [Schulman et al., 2017] and SAC Haarnoja et al. [2018]), achieving good results, but suffering from high sample complexity for larger problems Plaat et al. [2023]. The emergence of few-shot in-context learning for solving SDPs opens a research avenue to find out what SDPs few-shot prompt-learning will be able to solve.\nNow that grade school math word problems are largely solvable, harder reasoning benchmarks in other domains are appearing [Ahn et al., 2024]. Another line of research argues that LLMs cannot reason, providing examples where LLMs fail, and discussing potential reasons. Berglund et al. [2023] show that LLMs can fail to generalize in surprising ways. They provide the example that if a model is trained to report that \u201dValentina Tereshkova was the first woman to travel to space\u201d, it will not automatically be able to answer the question, \u201dWho was the first woman to travel to space?\u201d pointing to a lack in semantic understanding of LLMs. Other work suggests that results are less generalizable and transferable than often assumed, showing how base-10 arithmetic skills do not transfer to base-9 arithmetic problems Wu et al. [2024]. The question which problems LLMs can and cannot solve will continue to motivate researchers. Other works study the dangers of the size of LLMs. Bender et al. [2021] mention the environmental risks associated with the large computational training demands, as well as the difficulty of understanding the training data, for example in the context of bias. Furthermore, there are ethical, legal, and copyright concerns regarding the data that LLMs are trained on. Finally, to prevent putting too much trust in the outcome\n5Reasoning and planning have been studied since the start of artificial intelligence, starting with logic and reasoning [Newell and Simon, 1961], search algorithms in puzzles and board games [Korf, 1999, Plaat, 2020], robot planning [Fikes and Nilsson, 1971], classical machine learning such as decision trees and support vector machines [Flach, 2012, Breiman, 2001, Cortes and Vapnik, 1995], through knowledge representation and the semantic web [Van Harmelen et al., 2008]. Ever since the success of the connectionist approach LeCun et al. [2015], Goodfellow et al. [2016] (deep learning, including LLMs) researchers have tried to join the two approaches.\n5Reasoning and planning have been studied since the start of artificial intelligence, starting with logic and reasoning [Newell and Simon, 1961], search algorithms in puzzles and board games [Korf, 1999, Plaat, 2020], robot planning [Fikes and Nilsson, 1971], classical machine learning such as decision trees and support vector machines [Flach, 2012, Breiman, 2001, Cortes and Vapnik, 1995], through knowledge representation and the semantic web [Van Harmelen et al., 2008]. Ever since the success of the connectionist approach LeCun et al. [2015], Goodfellow et al. [2016] (deep learning, including LLMs) researchers have tried to join the two approaches.\nof LLMs, we should understand their failure modes better, such as the well-publicized problems of hallucination (inventing facts that look right but are not). Most of the reasoning capabilities exhibited by LLMs are due to the great representational powers of the transformer architecture, and how in-context learning is able to harness them. Prompt engineering and prompt control play a crucial role in the kind of reasoning that we have seen in the papers. Models can be instructed to write their own reasoning prompts; however, such Auto-GPT or Auto-CoT prompts need evaluation, verification, and grounding in the real world, to prevent degeneration into a hallucinatory world of their own. Models can also be instructed to interact with the world, and become the tool of external scaffolding that evaluates, controls and improves the prompts. Some of what we experience as reasoning by the LLM, is controlled by the prompt or the scaffolding algorithm. It is an open question if prompt learning is able get the LLM to create a prompt to exhibit non-trivial reasoning by itself. From the symbolic planning field there is also a critical view on the reasoning and planning abilities of LLMs [Valmeekam et al., 2023] giving examples of planning failures. They argue that LLMs can be used instead to improve heuristic elements of traditional planners, such as PDDL [Kambhampati et al., 2024], to strengthen traditional symbolic planning approaches. Some of the names of the approaches surveyed in this paper are suggestive of selfawareness and self-reflective capabilities. True self-reflection, or metacognition, is still largely outside the capabilities of current LLMs. LLMs can be prompted to reason, to take small steps, to self-evaluate, and their search process can be controlled by an external algorithm. The self-reflective type of \u201cintelligence\u201d is written into the prompt by the prompt engineer or the interactive algorithm. We are unaware of any LLM that has been made to reflect on, or even control, its reasoning processes, controlling how many reasoning steps it should take, or limiting its reasoning once the answer had become good enough. True self-reflection remains future work, although some steps have been taken, as we will discuss next.\n# 6.2.3 Reasoning towards Metacognition\nHuman thought exhibits the ability to reason about self, we are able to think about our own thinking processes. Metacognition studies these topics [Veenman et al., 2006]. Prompted by the success of Chain-of-thought and the works that we have surveyed, metacognition has also been studied in the context of LLMs [Toy et al., 2024]. Many reasoning approaches highlight self-reflective aspects in their names and in how they work. The prompts that prompt the models to reason are being improved with the outcome of the reasoning process, and in Buffer-of-thoughts thought-templates are used that are derived from other reasoning processes. Wang and Zhao [2023] study Metacognitive-prompting. Inspired by Chain-of-thought and Self-consistency, they create manually designed prompts to increase the understanding of language models. Figure 26 illustrates the relation between metacognitive human thought processes and metacognitive LLM prompting. Another work, again inspired by Chain-of-thought and Self-consistency, connects psychology and LLMs. Didolkar et al. [2024] study metacognitive capabilities of LLMs in mathematical problem solving, both on GSM8K and on the",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to explore the rapidly expanding field of reasoning with Large Language Models (LLMs), specifically focusing on how prompt-based approaches can enhance reasoning capabilities, particularly in solving grade school math word problems. It addresses the knowledge gaps regarding the effectiveness of various reasoning methodologies and proposes a research agenda for future exploration.",
            "scope": "The survey encompasses prompt-based reasoning methods applied to LLMs, particularly in the context of grade school math word problems. It excludes other types of reasoning not directly related to LLMs or those that do not utilize prompts, as the focus is on understanding and categorizing prompt-based reasoning techniques."
        },
        "problem": {
            "definition": "The core problem explored in this survey is the difficulty that LLMs face in performing multi-step reasoning tasks, particularly in the context of math word problems. It examines how these models can be prompted to improve their reasoning abilities.",
            "key obstacle": "The primary challenge is the error accumulation in multi-step reasoning processes, where small mistakes can lead to incorrect final answers. Additionally, LLMs often struggle with compositional reasoning, where the ability to solve subproblems does not necessarily translate to solving the overall problem."
        },
        "architecture": {
            "perspective": "The survey introduces a taxonomy of reasoning approaches based on the pipeline of generation, evaluation, and control of reasoning steps. It categorizes existing research into these stages to better understand the effectiveness of various methodologies.",
            "fields/stages": "The survey organizes the current methods into three main stages: 1) Step Generation (how prompts are created to elicit reasoning steps), 2) Step Evaluation (how the generated steps are assessed for accuracy), and 3) Control of Reasoning Steps (how the reasoning process is managed and directed)."
        },
        "conclusion": {
            "comparisions": "The comparative analysis reveals that methods utilizing Chain-of-thought prompting significantly outperform baseline models in solving math word problems. Approaches that incorporate self-assessment and ensemble strategies further enhance performance, indicating that diverse reasoning paths can lead to more reliable outcomes.",
            "results": "The survey concludes that while LLMs have made significant strides in reasoning capabilities through prompt-based learning, challenges remain in ensuring faithful reasoning and reducing hallucination. Future research should focus on developing more robust evaluation methods and exploring the potential of metacognitive reasoning."
        },
        "discussion": {
            "advantage": "Current research has demonstrated that LLMs can effectively solve a variety of reasoning tasks, particularly when guided by well-structured prompts. The Chain-of-thought approach has shown substantial improvements in performance on complex reasoning problems.",
            "limitation": "Despite advancements, limitations persist in the form of hallucination, where models generate plausible but incorrect outputs. Additionally, the models often lack true understanding and may produce results based on surface-level patterns rather than genuine reasoning.",
            "gaps": "Unanswered questions remain regarding the scalability of reasoning capabilities to smaller models and the transferability of learned reasoning strategies across different domains. Furthermore, the exploration of true self-reflective reasoning in LLMs is still in its infancy.",
            "future work": "Future research should focus on enhancing the robustness of reasoning methods, exploring the integration of external knowledge sources, and investigating the potential of metacognitive prompting to improve reasoning accuracy and reliability."
        },
        "other info": {
            "authors": "Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas B\u00e4ck",
            "affiliation": "LIACS, Leiden University, Netherlands",
            "date": "July 17, 2024"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The survey explores the rapidly expanding field of reasoning with Large Language Models (LLMs), focusing on how prompt-based approaches can enhance reasoning capabilities, particularly in solving grade school math word problems."
        },
        {
            "section number": "1.2",
            "key information": "The significance of in-context learning is highlighted through the examination of the difficulty LLMs face in performing multi-step reasoning tasks, particularly in the context of math word problems."
        },
        {
            "section number": "3.1",
            "key information": "The primary challenge in in-context learning is the error accumulation in multi-step reasoning processes, where small mistakes can lead to incorrect final answers."
        },
        {
            "section number": "3.2",
            "key information": "The survey introduces a taxonomy of reasoning approaches based on the pipeline of generation, evaluation, and control of reasoning steps, categorizing existing research into these stages."
        },
        {
            "section number": "4.1",
            "key information": "The Chain-of-thought prompting method has shown substantial improvements in performance on complex reasoning problems, indicating the influence of effective prompt design."
        },
        {
            "section number": "6.1",
            "key information": "Despite advancements, limitations persist in the form of hallucination, where models generate plausible but incorrect outputs, indicating issues related to model bias and context sensitivity."
        },
        {
            "section number": "6.4",
            "key information": "Unanswered questions remain regarding the scalability of reasoning capabilities to smaller models and the transferability of learned reasoning strategies across different domains."
        }
    ],
    "similarity_score": 0.7190351587213668,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Reasoning with Large Language Models, a Survey.json"
}