{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.03498",
    "title": "In-Context Exemplars as Clues to Retrieving from Large Associative Memory",
    "abstract": "Recently, large language models (LLMs) have made remarkable progress in natural language processing. The most representative ability of LLMs is in-context learning (ICL), which enables LLMs to learn patterns from in-context exemplars without training. The performance of ICL greatly depends on the exemplars used. However, how to choose exemplars remains unclear due to the lack of understanding of how in-context learning works. In this paper, we present a novel perspective on ICL by conceptualizing it as contextual retrieval from a model of associative memory. We establish a theoretical framework of ICL based on Hopfield Networks. Based on our framework, we look into how in-context exemplars influence the performance of ICL and propose more efficient active exemplar selection. Our study sheds new light on the mechanism of ICL by connecting it to memory retrieval, with potential implications for advancing the understanding of LLMs.",
    "bib_name": "zhao2023incontextexemplarscluesretrieving",
    "md_text": "# In-Context Exemplars as Clues to Retrieving from Large Associative Memory\nJiachen Zhao\nUniversity of Massachusetts Amherst jiachenzhao@umass.edu\n# Abstract\nRecently, large language models (LLMs) have made remarkable progress in natural language processing. The most representative ability of LLMs is in-context learning (ICL), which enables LLMs to learn patterns from in-context exemplars without training. The performance of ICL greatly depends on the exemplars used. However, how to choose exemplars remains unclear due to the lack of understanding of how in-context learning works. In this paper, we present a novel perspective on ICL by conceptualizing it as contextual retrieval from a model of associative memory. We establish a theoretical framework of ICL based on Hopfield Networks. Based on our framework, we look into how in-context exemplars influence the performance of ICL and propose more efficient active exemplar selection. Our study sheds new light on the mechanism of ICL by connecting it to memory retrieval, with potential implications for advancing the understanding of LLMs.\n# 1 Introduction\nIn recent years, large language models (LLMs) have garnered significant attention due to their ability to revolutionize natural language processing (NLP) by demonstrating impressive language understanding and reasoning capabilities (7; 6; 45; 56; 44). LLMs are first pretrained on extensive data using the language modeling technique where the model predicts the next token given a context. Without finetuning on task-specific data, LLMs leverage in-context learning (ICL), also referred to as few-shot prompting, to make predictions. Through ICL, LLMs can find underlying patterns of the input query through given in-context exemplars, such as a set of input/output pairs, and use them to complete the response. However, the effects of in-context exemplars on downstream performance via ICL and guidelines for formulating those exemplars (e.g., how to select exemplars and how many exemplars to use) remain unclear. Because the understanding of how ICL works is currently intuitive and lacks theoretical foundation. Past works to understand ICL mainly focus on empirical investigation (32; 14; 28; 59). Therefore, this work are motivated to build a theoretical framework to analyze ICL and then to provide implications to formulating in-context exemplars for stronger performance. More specifically, we adopt a novel and distinct perspective by conceptually reframing ICL as contextual retrieval rather than a learning problem (10; 53; 12), as there is no actual weight update involved. We conceptualize LLM as a biologically plausible model of associative memory (18), also known as content-addressable memory. In psychology, associative memory refers to the aptitude for linking and recollecting numerous sets of unconnected items, also known as memories. If furnished with a subset of items derived from a specific memory, an organism possessing associative memory can\nretrieve the remaining items associated with that particular memory. Such memory retrieval in human brains is also triggered by cue/ prompt. Contextual cues are essential to successful memory recall in brain and increase the accessibility of memory (51). In the realm of machine learning, memory models (18; 24; 48; 23; 37; 25) have been widely studied for a long time. Two fundamental models are Hopfield Network (18) and its extension, sparse distributed memory (24). Generally, the memory retrieval process in these models is executed by updating neuron configurations through a predetermined rule to minimize the network\u2019s energy. This retrieval process can also be viewed as pattern recognition (25). An input is perceived as a query related to patterns stored in the model\u2019s memory, and the model generates a prediction by recalling its memory based on the query. Through the lens of memory models, we demonstrate that ICL with self-attention (52) in LLMs can be interpreted as retrieving patterns from associative memory of Hopfield Networks with context. Correspondingly, we establish a theoretical framework and analyze retrieval error, i.e., downstream performance via ICL. Within our contextual retrieval framework, we look into the influence of in-context exemplars on performance of ICL both theoretically and empirically. We justify why randomly choosing exemplars can work especially given enough exemplars. Our analysis also suggests that different from supervised learning, ICL will not necessarily have better performance with more exemplars, which depends on the chosen exemplars. Moreover, we further propose efficient Active Exemplar Selection which achieves better performance with much fewer exemplars than random selection.\n# 2 Brief Review of Hopfield Networks\nHopfield Networks (HNs) (18) were introduced to store and retrieve information. The standard HN (18) consists of a neural network of N neurons that can in total store M binary patterns of dimension D. Memory \u03be is denoted as an array of stored pattern vectors, i.e., \u03be = [m1, ..., mM], where \u03be \u2208RM\u00d7D. During the retrieval process, the configuration of neurons is fixed to the query pattern (e.g., incomplete mi), and an update rule f for \u03c3 is defined to retrieve the similar or the same pattern to the query. Each update lowers the energy function E of the network, which belongs to the Ising spin-glass model (43) in physics. The energy is expected to converge to an attractor state (local minimum) through repeated updates. Eventually, HNs will return the pattern from its memory that is the most similar to the input. Additionally, HNs are similar to humans\u2019 memory system. The neuron\u2019s state corresponds to the firing rate or activity level of biological neurons. The weights of the network correspond to the strength of the synaptic connections between neurons in the brain. Similar to HNs, memories in brains are stored in a distributed manner across many regions and neurons. There are associative areas storing relations between features. Complex memories can then be recalled to generate predictions based on partial cues or associations (4; 3) just like HNs.\n# 3 ICL as Contextual Retrieval\nThis section presents a formulation of ICL as pattern retrieval based on context from memories of modern Hopfield Networks (MHNs)(37; 25; 31). Brief overview of Hopfield Networks is provided in Appendix 2. In this section, we first give a formal setup of ICL. For a pre-trained language model whose parameters are denoted as \u03b8, given an input x, the model will predict \u02dcy for ground truth by conditioning on the query and a context sequence containing K exemplars that are drawn from an accessible labeled dataset D(x,y) (each exemplar ei = (xi, yi)). Formally, we denote the sequence of\n\u02dcy = argmaxyP(y|e, x, \u03b8).\nFrom the perspective of HNs, the input string [e,x] is a cue to the associative memory. The feed-forwarding process in the language model is to reconstruct the completion \u02dcy of x that aligns with patterns of context e by recalling information stored into the model\u2019s memory during pretraining. For LLM, the pretraining is implemented as predicting masked/ next tokens of sentences, which is essentially teaching the model to reconstruct completion based on context like HNs. To demonstrate the close relation between ICL and retrieving from HNs, we first extend the model definitions discussed by Ramsauer et al. (37); Millidge et al. (31) to construct a Hopfield Network with Context (HN-C). We then show that contextual retrieval from HN-C is equivalent to self-attention in LLMs. To incorporate context in HNs, we consider stored patterns in memory as applying a linear transformation to raw vectors with a memory matrix, which is different from past frameworks (37; 25; 31) that assume a static array of stored patterns. Thus, in our case, context patterns are dynamically defined depending on the input context. It is also important to note that retrieval does not necessarily mean extracting the exact stored patterns in memory without loss, but rather involves the induction of completion based on the input patterns that are typically not fully identical to the stored memories (25). Actually, contextual retrieval setting is indeed how the human brain retrieves episodic memory (38). Formally, we denote some underlying query vector of input strings by \u03c3 \u2208Rdm. We define there are M context vectors \u03bbi \u2208Rdm which are represented by a matrix \u039b \u2208Rdm\u00d7M. We define memory matrix \u03beQ \u2208Rdm\u00d7dq and \u03beK \u2208Rdm\u00d7dq respectively for query vector \u03c3 and context vector \u03bb. We further define Z := \u03beT K\u039b and each column vector z is context pattern. Accordingly, we have u := \u03c3\u03beQ as query pattern. We then define the update rule for u of the model based on the Universal Hopfield Network (31) as follows:\nwhere \u03b3 is a scalar value, sim is a similarity function and sep is a separation function. We set sim as dot production and sep as softmax function. Then the update rule can be further specified as Eq. 4.\nThis formulation can be converted to self-attention by applying a linear transformation to unew, i.e. unewWv = softmax(\u03b3QKT )V, where we write \u03c3\u03beQ = Q, \u03beT K\u039b = KT and \u039bT \u03beKWv = V. Therefore the update rule for contextual retrieval from HNs can be equivalent to self-attention through simple conversion. For self-attention, query pattern is Q and the context pattern is namely K.\nPattern Retrieval From the perspective of memory models, ICL can be reinterpreted as retrieving underlying patterns of input based on context vectors \u03bb following the update rule. This interpretation is focused on the association among neurons in some middle layer of the model, where the hidden states at each token position may encode some unique information (1). Query and context are thus assumed to be encoded into separate vectors. The retrieval process consists of the following stages. (1) Query vector \u03c3 and context vectors \u03bb are mapped to the associative space through linear transformation with \u03beQ and \u03beK to reveal underlying patterns. (2) Then a similarity score between u and z is computed to measure their mutual closeness in the associative space. Dot product is\n(1)\n(2)\n(3) (4)\nconsidered as the similarity function for self-attention. (3) An exponential separation function, i.e., Softmax is computed to stress the prominent context patterns that have higher similarity scores. (4) After separation, unew is computed as a weighted sum of context patterns. There can be repetition in context patterns (which means some zi = zj). So more frequent context patterns might thus have a larger contribution to the weighted summation.\nefinition 1 (Query-Context Separation) For zi, \u03b4 := uzi \u2212uzj, where zj \u0338= {zi|i \u2208[1, M]} an\n \u2208 We then establish the distinction in similarity scores between context patterns and query patterns as a metric for evaluating the degree of separation between two context patterns with respect to the query pattern. The larger \u03b4 is between zi and some other patterns zj, the easier it will be for zi to be matched by the query pattern u. Moreover, we define the pattern retrieval error as \u2225f(u) \u2212u\u22c6\u2225, where f is the update rule for the query pattern and u\u22c6is the corresponding underlying ground-truth pattern of y in the same associative space to u. It is assumed that both the query vectors and context vectors follow some distribution within the pre-trained model, allowing the model to effectively capture and represent their patterns. Different from the defined error of HNs in (37), we consider a general case where uT is not necessarily in {zi|i \u2208[1, M]}.\n# 3.1 Prompting Performance as Retrieval Error\nTheorem 1 (Retrieval Error) For some zi that has t instances, i.e., t = \ufffdM j=1 1{zj = zi}. The ground-truth pattern u\u22c6= (zi + \u2206z)T . We define c := exp(\u2212\u03b3(uzi \u2212maxzi\u0338=zjuzj)) = exp(\u2212\u03b3\u03b4min), and zmax = max(z1, ..., zM). The retrieval error \u03f5 := \u2225f(u) \u2212u\u22c6\u2225is then bounded by [0, \u2225\u2206z\u2225+ \u03b2\u2225zmax\u2225], where \u03b2 = \ufffd 1 \u2212 \ufffd 1 + c(M\u2212t) t \ufffd\u22121 + c(M \u2212t) \ufffd and \u03b2 \u221dc M t The proof is displayed in Appendix A. We can see the upper bound consists of two parts, i.e., \u2225\u2206z\u2225and \u03b2\u2225zmax\u2225. We name \u2225\u2206z\u2225as Instance Error which directly reflects the match between a context pattern zi and the target pattern u\u22c6. On the other hand, \u03b2\u2225zmax\u2225is named as Contextual Error that mainly indicates the separation of zi from other context patterns (remind that \u03b2 \u221dc M t = exp(\u2212\u03b3\u03b4min) M t ), i.e., how easy for the model to rely on zi more for the retrieval. Additionally, when t = 1 and ||\u2206z|| = 0, we are directly retrieving the pattern from context patterns stored in the HN. We next discuss two primary questions on ICL, utilizing our retrieval framework as a foundation, and offer some theoretical predictions.\n\ufffd \ufffd \ufffd \ufffd The proof is displayed in Appendix A. We can see the upper bound consists of two parts, i.e., \u2225\u2206z\u2225and \u03b2\u2225zmax\u2225. We name \u2225\u2206z\u2225as Instance Error which directly reflects the match between a context pattern zi and the target pattern u\u22c6. On the other hand, \u03b2\u2225zmax\u2225is named as Contextual Error that mainly indicates the separation of zi from other context patterns (remind that \u03b2 \u221dc M t = exp(\u2212\u03b3\u03b4min) M t ), i.e., how easy for the model to rely on zi more for the retrieval. Additionally, when t = 1 and ||\u2206z|| = 0, we are directly retrieving the pattern from context patterns stored in the HN. We next discuss two primary questions on ICL, utilizing our retrieval framework as a foundation, and offer some theoretical predictions.\nHow does the number of context patterns influence retrieval error? With the increase of M, M t and c may change accordingly depending on newly introduced context patterns. Given the instance error, this fluctuation leads to the different tendencies of the upper bound, which means varied potential of the actual error. When context vectors are randomly sampled from the distribution, larger M is often observed to enable generally better performance of ICL (32; 6; 58). However, chances are that if one context pattern zi already has minimum instance error, larger M\nmay lead to declined performance due to the introduced contextual error from other context patterns (i.e., increased M t and c). We empirically show this in Sec. 4.2. Thus, the influence of M can be uncertain depending on chosen context patterns.\n# 3.2 Implications on Exemplar Selection\nThis section analyzes the default random selection based on our retrieval framework. We first detai the definition of exemplar selection.\nDefinition 2 (Exemplar Selection) For an input query x and output y sampled from distribution p(Dte) of task T , a set of K exemplars Scontext is selected from training data Dtr (x,y) to minimize \u2113(y, \u02dcy), where \u02dcy = argmaxyP(y|e1, ..., eK, x), ei = (xei, yei), ei \u2208Scontext. We assume p(Dte) \u2248p(Dtr (x,y)) \u2248p\u2217that is the population distribution. We also regard patterns as latent variables that underlie string sequences.\nRandom Selection. Random selection is the default method (6) that can be considered as sampling exemplars from p(Dtr (x,y)). When K is large enough, we assume p(Scontext) \u2248p(Dtr (x,y)) \u2248p\u2217. Accordingly, the mode of context patterns, i.e., argmaxz \ufffdM j=1 1{zj = z} may approximate the mode (denoted by \u02c6z) of the pattern distribution of samples from p\u2217. Then for the upper bound of retrieval error \u03f5 with zi = argmaxz \ufffdM j=1 1{zj = z}, the instance error can be approximated to the error given by \u02c6z, i.e., \u2225\u2206z\u2225\u2248\u2225\u02c6z \u2212(u\u2217)T \u2225. Because \u02c6z may more or less be relevant to (u\u2217)T when they follow the same pattern distribution, with sufficiently large K, random selection may give a decent retrieval error. On the other hand, when K is small, random selection may perform poorly and have great variance depending on sampled exemplars. We provide empirical verification of our theoretical prediction in Fig. 1 of Sec. 4.2.\nMetric-based Selection. For this approach, a metric function is employed to measure the closeness of query x and xe that is the query in an exemplar and to choose more similar e accordingly (47; 17; 28; 42). The choice of the metric function is heuristic. One of such approaches is to measure semantic closeness (28; 42), which assumes that semantically similar queries imply similar patterns as well for a task T . But this assumption is likely to collapse for some tasks (see experiments in Sec. 4.2).\n# 3.2.1 Active Exemplar Selection\nAs has been discussed, the problem with random selection is its great variance, which requires sufficiently large number of exemplars to be effective. However, the number of exemplars is constrained by token limit of the model, making random selection fail in some cases. Thus, we are motivated to directly sample exemplars with lower expected value of instance error to reduce the upper bound of \u03f5. Inspired by active learning (41), we propose Active Exemplar Selection that is task-agnostic, parameter-free and fast at inference. First of all, a value function is defined for an exemplar ei from Dtr,\ns is a task-specific score function (e.g., F1 score) and F stands for LLM. The value function is meant to measure the expected value of the instance error of patterns at the data level (i.e., assuming\n(5)\nv(ei) \u221dE\u2225\u2206z\u2225). However, it is hard to directly compute v(ei), we then use Monte Carlo Method to estimate it in the training dataset.\nwhere N is the size of training data Dtr and (xj, yj) \u223cp(Dtr). Finally, K exemplars with the highest v(ei) on Dtr are selected as the context. In this way, exemplars with high v(ei) will be directly chosen that tend to contain the mode pattern of the distribution. Therefore, active selection can be less noisy and require smaller K to reach \u2225\u2206z\u2225\u2248\u2225\u02c6z \u2212(u\u2217)T \u2225than random selection (see Fig. 1). To make the algorithm more tractable when |Dtr| is very large, instead of using all data, a sufficiently large pool of candidates for comparison can be formed by random sub-sampling of Dtr. All in all, active exemplar selection is expected to be a more reliable replacement of default random selection.\n# 4 Experiments\n# 4.1 Experimental Setup\nDatasets. We conduct experiments on two types of common tasks in NLP, each of which has two different datasets. For constituency parsing, we use the Penn Treebank corpus (29) with the standard splits (2-21) for training containing 39832 sentences, 22 for validation, 23 for test). PTB is an often-used benchmark for constituency parsing in English. The corpus is collected from a variety of sources including stories, news, and scientific abstracts. To evaluate our approach in a specific domain, we employ the Colorado Richly Annotated Full-Text (CRAFT) corpus (9) that consists of biomedical journal articles and contains 21121 sentences in total. We randomly split the CRAFT corpus into a training set and a test set following the ratio of 6:4. For question answering, we use challenging MedMCQA (34) and MedQA (20) that involve multiple choices on professional biomedical knowledge. We expect those two datasets are harder for LLM to answer with zero-shot prompting, making the effects of in-context exemplars more noticeable. Implementation. We employ Code-Davinci-002, known as Codex (8) and GPT-3.5-turbo, i.e., engine for ChatGPT. For evaluating parsing tasks, in all cases, we report sentence-level unlabeled parsing F1 that is computed separately for each sentence and then averaged across the dataset. For question answering, we report average accuracy over all test data. In terms of configuration of prompting, we use 20 exemplars for constituency parsing, while 5 exemplars for medical QA given longer context and queries. We employ GPT-3.5-turbo for QA given its more updated training data and ability of following instructions. For active exemplar selection, we randomly sample 100 cases from training data for our estimation of expectance. The score function is F1 for parsing and accuracy for QA. Baselines. To evaluate the effects of our proposed active exemplar selection on ICL, we consider\nBaselines. To evaluate the effects of our proposed active exemplar selection on ICL, we consider four baselines for comparison: (1) the default random exemplar selection (6) that randomly samples exemplars for each test case; (2) semantics-based approach (28; 42) where the relatedness of an exemplar to the input query is decided by computing distance between embeddings and the exemplars with the closest embedding will be chosen; (3) language-modeling-based approach (44) that employs probability p(u|ui) output by a large language model where u is a test input and ui is an exemplar. At inference time, both the LM-based method requires traversing partial or all training exemplars to\n(6)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f046/f0467c57-ab0b-48c3-a9b3-bc1104e25126.png\" style=\"width: 50%;\"></div>\nFigure 1: The relation between the performance of ICL with different exemplar selection method and the number of exemplars (i.e., K). Apart from random selection and proposed active selection we assume the oracle access to ground truth so as to select the best exemplars for each query (denoted as \u201cinstance best\u201d).\nchoose the one giving the highest probability, which can be very time-consuming. (4) BM25 (40 that is a bag-of-words retrieval function to calculate and select exemplars more relevant to the input query.\n# 4.2 Results\nDataset\nPTB\nCRAFT\nModel\nCodex\nChat\nCodex\nChat\nRandom\n71.23\n69.68\n58.44\n57.23\nSemantic\n72.11\n70.02\n57.36\n57.11\nLM\n71.02\n69.73\n57.23\n56.84\nActive Selection\n73.92\n73.61\n62.98\n61.65\nTable 1: F1 score on PTB and CRAFT with different methods of exemplars selection.\nDownstream Performance. The results are shown in Table 1 and Table 2. Our active selection method enables both LLMs to greatly outperform the other baselines. In Table 1, for ChatGPT, the improvement can achieve around 4%. This demonstrates that choosing useful in-context exemplars\nMethod\\Dataset\nMedQA\nMedMCQA\nZero-shot\n43.45\n41.59\nRandom\n60.01\n54.59\nBM25\n60.43\n56.29\nSemantic\n60.33\n56.14\nActive Selection\n62.51\n59.86\nTable 2: Accuracy on MedMCQA and MedQA with different methods of exemplars selection. Fo zero-shot prompting, no exemplars are used.\nhelp LLM better induce patterns based on its memory. Additionally, semantic-based selection and LM-based selection give similar performance to the random selection. The assumption of semantic-based selection is that semantically similar data shall contain related task-specific patterns. However, this assumption seems to fail in our evaluation on constituency parsing. In comparison, our proposed active selection requires no such pattern-wise assumption and can directly choose exemplars containing common patterns for a dataset. In Table 2, the performance on both datasets can be improved by 1% to 2% compared with using random exemplars. On MedQA, BM25 and semantic-based selection perform slightly better than random selection. Additionally, using exemplars significantly increases the performance of zero-shot prompting. This demonstrates that LLM may have encoded answers during training and in-context exemplars help increase the accessibility of encoded knowledge through ICL. Effects of K. We conduct experiments with different number of in-context exemplars. The results are shown in Fig. 1. The performance of ICL increases with more exemplars for random selection and active selection. Recall in our theory in Sec. 3.2, we consider the drawback of random selection is it needs larger K to reach an optimal status where the mode of exemplars approximates the major patterns of the population. Fig. 1 verifies our reasoning and demonstrates the random selection can achieve a decent performance when K is large enough. In comparison, our proposed active selection gives much better results even with smaller K. This verifies our thoery that active selection can directly capture exemplars that may contain the common patterns. Active selection is thus especially helpful to cases where a large amount of labeled data are available, while the toke limit only allows a few exemplars to be fed to LLM.\nhelp LLM better induce patterns based on its memory. Additionally, semantic-based selection and LM-based selection give similar performance to the random selection. The assumption of semantic-based selection is that semantically similar data shall contain related task-specific patterns. However, this assumption seems to fail in our evaluation on constituency parsing. In comparison, our proposed active selection requires no such pattern-wise assumption and can directly choose exemplars containing common patterns for a dataset. In Table 2, the performance on both datasets can be improved by 1% to 2% compared with using random exemplars. On MedQA, BM25 and semantic-based selection perform slightly better than random selection. Additionally, using exemplars significantly increases the performance of zero-shot prompting. This demonstrates that LLM may have encoded answers during training and in-context exemplars help increase the accessibility of encoded knowledge through ICL. Effects of K. We conduct experiments with different number of in-context exemplars. The results are shown in Fig. 1. The performance of ICL increases with more exemplars for random selection and active selection. Recall in our theory in Sec. 3.2, we consider the drawback of random selection is it needs larger K to reach an optimal status where the mode of exemplars approximates the major patterns of the population. Fig. 1 verifies our reasoning and demonstrates the random selection can achieve a decent performance when K is large enough. In comparison, our proposed active selection gives much better results even with smaller K. This verifies our thoery that active selection can directly capture exemplars that may contain the common patterns. Active selection is thus especially helpful to cases where a large amount of labeled data are available, while the toke limit only allows a few exemplars to be fed to LLM. Comparison with oracle exemplars. For each query instance, we also assume the access to ground truth and rank training data that can give the best performance when used as the only exemplar for the query. We report the average result in Fig. 1. When K = 1, such oracle method gives the best result, while the performance drops immediately with additional exemplars and starts to stagnate. This can be due to the increased contextual error. Including more optimal exemplars turns out to give similar performance to random selection and active selection. Therefore, different from supervised tuning, for ICL more exemplars do not always guarantee a better performance, which depends on added exemplars. Additionally, the observation indicates when knowing the optimal exemplar for a query (which is likely to be impossible in practice), we do not need many-shot prompting. However, for cases with no access to such information, simply increasing K may actually be a good strategy for better performance.\nComparison with oracle exemplars. For each query instance, we also assume the access to ground truth and rank training data that can give the best performance when used as the only exemplar for the query. We report the average result in Fig. 1. When K = 1, such oracle method gives the best result, while the performance drops immediately with additional exemplars and starts to stagnate. This can be due to the increased contextual error. Including more optimal exemplars turns out to give similar performance to random selection and active selection. Therefore, different from supervised tuning, for ICL more exemplars do not always guarantee a better performance, which depends on added exemplars. Additionally, the observation indicates when knowing the optimal exemplar for a query (which is likely to be impossible in practice), we do not need many-shot prompting. However, for cases with no access to such information, simply increasing K may actually be a good strategy for better performance.\n# 5 Biological Plausibility of ICL as Memory Retrieval\nThe process of ICL in LLMs exhibits similarities to the memory retrieval process in the human brain, both of which involves the use of prompts or cues related to targeted information to retrieve. This section shows some phenomenon-level biological plausibility of ICL. Similar to LLMs, human memory retrieval also heavily depends on contextual cues for successful recall (51; 13; 16; 46). Human\u2019s memories can actually exist in a state of being available but inaccessible (50) (compare performance of \u201cZero-shot\u201d with other methods in Table 2). When some information cannot be recalled with internal cues (i.e., without external hints), such as in free recall tasks, it is considered inaccessible. However, external cues, e.g., category cues related to the target items to recall, can greatly increase the accessibility of memory. Likewise, LLMs can provide answers to questions that they initially fail in zero-shot prompting scenarios when given related in-context exemplars. The\nquery together with in-context exemplars can also be viewed as partial information cues for memory retrieval, providing incomplete or fragmented versions of the target (22; 21). Additionally, the cue-to-target similarity, also known as encoding specificity, is critical to the likelihood of successful recall for human brain (51; 33). Similarly, LLMs that are trained through language modeling may exhibit such requirements for in-context exemplars (54). For humans, prompts are typically extralist cues, originating from a different list of stored memories to be retrieved. But extralist cues can still be effective if they are relevant to the target (51; 33). Similarly, in the case of ICL, it is uncommon to encounter context and target output that exactly match the training data. However, by providing relevant exemplars, LLMs may still capture underlying patterns of query with the guide of in-context demonstrations and generalize to unseen cases.\n# 6 Related Work\nAssociative Memory Models. Models of associative memory have the capability of recalling its learnt patterns stored in memory for an incomplete input and then recovering it. Hopfield Network (HN) (18) was the first introduced type of artificial neural network used for associative memory. It consists of one neural network layer and has binary memories. Memory retrieval is performed through a forward pass following an update rule, which can decrease the energy of the network. An extension of HN is Sparse Distributed Memory (SDM) (24), which stores memories in an \u2019Address\u2019 matrix and a \u2019Pattern\u2019 matrix as well. Self-attention of Transformer (52) is shown to work similarly to SDM (5). Recently, the memory capacity and capabilities of memory models have been raised to a new level. Krotov and Hopfield (25) developed Dense Associative Memory for pattern recognition and showed the duality between the associative memory model and a feedforward neural network. Ramsauer et al. (37) generalized HNs to continus inputs, whose model is known as Modern Hopfield Network (MHN). Ramsauer et al. (37) demonstrates retrieving patterns from MHN is the same as a feed-forward pass in self-attention (52). On the other hand, the recent work (19) explores sparse MHN and concurrently, Anonymous (2) investigate the application of (sparse) Hopfield-based deep learning model to multivariate time series prediction. In-Context Learning. In-Contex Learning (ICL) is the ability of language models to induce answers from given demonstrations without weights updating in supervised tuning. ICL is shown to exist in both small language models (e.g., a vanilla Transformer) (14) and large language models (6). Explaining how ICL works for LLM is a fundamental while challenging topic. Currently, there is still no consensus on the mechanism of ICL despite the popularity of LLM in applications. Some of past works (53; 10) propose that ICI is doing implicit gradient descent and theoretically showing that the hidden states of some neurons can be approximately the same as gradients in back-propagation during training. Pan (35) empirically investigates ICL by disentangling it into task recognition and task learning. Some other works cast ICL to Bayesian Inference (57). But those works are mainly tested on a small language model with simple regression tasks, which might not work for LLMs. Language Models as Memory Networks. Language Models (LMs) have been shown to encode extensive knowledge in their weights (36; 30; 39) through pretraining and can answer factual questions with zero shots or few shots (27; 45). The output of feed-forward layer in Transformer is demonstrated to consist of its stored memories (15). There are works (55; 49) that leverage LM for Information Retrieval (IR) by mapping IR tasks to sequence-to-sequence generation tasks and can obtain superior\nperformance. There are also works (26; 11) that construct memory networks for natural language processing tasks and achieve decent performance as well.\n# 7 Conclusion\nIn this work, we investigate the influence of in-context exemplars on ICL via viewing in-context learning (ICL) of large language models through the lens of models of associative memory. We have shown that in-context learning can be theoretically equivalent to contextual retrieval from a Hopfield Network. We then give theoretical and empirical analysis of formulating exemplars for better downstream performance via ICL and propose more efficient Active Exemplar Selection approach. All in all, our work interprets ICL as contextual retrieval from memory which enables the theoretical analysis of correlation between exemplars and performance of ICL. By linking recent LLMs to biologically plausible Hopfield Networks, our work may shed new light on understanding LLMs.\n# 8 Limitations\nAlthough we show some convergences between ICL and HNs, our theoretical framework exhibits a high level of abstraction, primarily focusing on a single layer of self-attention and neglecting the influence of order of exemplars. The connection between our theory and proposals may not be strong enough. We aim to provide more theoretical and empirical evidence in linking ICL and HN as our future work. This work aims to provide explanations rather than precise predictions regarding the impact of context in ICL and may serve as a framework to conceptually analyze in-context learning of LLMs.\n# References\n[1] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. A review on language models as knowledge bases. arXiv preprint arXiv:2204.06031, 2022. [2] Anonymous. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In Submitted to The Twelfth International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6iwg437CZs. under review. [3] Helen C Barron, Ryszard Auksztulewicz, and Karl Friston. Prediction and memory: A predictive coding account. Progress in neurobiology, 192:101821, 2020. [4] Leonardo Bonetti, Elvira Brattico, Francesco Carlomagno, Giovanni Donati, Joana Cabral, Niels Trusbak Haumann, Gustavo Deco, Peter Vuust, and Morten L Kringelbach. Rapid encoding of musical tones discovered in whole-brain connectivity. NeuroImage, 245:118735, 2021. [5] Trenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory. Advances in Neural Information Processing Systems, 34:15301\u201315315, 2021. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[2] Anonymous. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In Submitted to The Twelfth International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6iwg437CZs. under review.\n[7] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Kevin Bretonnel Cohen, Karin M. Verspoor, Kar\u00ebn Fort, Christopher S. Funk, Michael Bada, Martha Palmer, and Lawrence E. Hunter. The colorado richly annotated full text (craft) corpus: Multi-model annotation in the biomedical domain. 2017. [10] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. [11] Micha\u0142 Daniluk, Tim Rockt\u00e4schel, Johannes Welbl, and Sebastian Riedel. Frustratingly short attention spans in neural language modeling. arXiv preprint arXiv:1702.04521, 2017. [12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. [13] James Eric Eich. The cue-dependent nature of state-dependent retrieval. Memory & Cognition, 8:157\u2013173, 1980. [14] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. [15] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021. [16] Duncan R Godden and Alan D Baddeley. Context-dependent memory in two natural environments: On land and underwater. British Journal of psychology, 66(3):325\u2013331, 1975. [17] Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022. [18] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982. [19] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. arXiv preprint arXiv:2309.12673, 2023. [20] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. [21] Yong Sang Jo and June-Seek Choi. Memory retrieval in response to partial cues requires nmda receptor-dependent neurotransmission in the medial prefrontal cortex. Neurobiology of learning and memory, 109:20\u201326, 2014.\n[22] Yong Sang Jo, Eun Hye Park, Il Hwan Kim, Soon Kwon Park, Hyun Kim, Hyun Taek Kim, and June-Seek Choi. The medial prefrontal cortex is involved in spatial memory retrieval under partial-cue conditions. Journal of Neuroscience, 27(49):13567\u201313578, 2007. [23] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? Advances in Neural Information Processing Systems, 29, 2016. [24] Pentti Kanerva. Sparse distributed memory. MIT press, 1988. [25] Dmitry Krotov and John J Hopfield. Dense associative memory for pattern recognition. Advances in neural information processing systems, 29, 2016. [26] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International conference on machine learning, pages 1378\u20131387. PMLR, 2016. [27] Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d\u2019Autume, Phil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense knowledge in large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2022. [28] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Association for Computational Linguistics, 2022. [29] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2), 1993. [30] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022. [31] Beren Millidge, Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Universal hopfield networks: A general framework for single-shot associative memory models. In International Conference on Machine Learning, pages 15561\u201315583. PMLR, 2022. [32] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064. Association for Computational Linguistics, December 2022. [33] Douglas L Nelson, Cathy L McEvoy, and Martha A Friedrich. Extralist cuing and retrieval inhibition. Journal of Experimental Psychology: Learning, Memory, and Cognition, 8(2):89, 1982. [34] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, pages 248\u2013260. PMLR, 2022. [35] Jane Pan. What In-Context Learning \u201cLearns\u201d In-Context: Disentangling Task Recognition and Task Learning. PhD thesis, Princeton University, 2023.\n[36] Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2019. [37] Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. [38] Charan Ranganath and Robert T Knight. Prefrontal cortex and episodic memory: Integrating findings from neuropsychology and functional brain imaging. The cognitive neuroscience of memory: Encoding and retrieval, 1:83, 2002. [39] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2020. [40] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009. [41] Nicholas Roy and Andrew McCallum. Toward optimal active learning through monte carlo estimation of error reduction. ICML, Williamstown, 2:441\u2013448, 2001. [42] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for incontext learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, July 2022. [43] Kirkpatrick Scott and Sherrington David. Infinite-ranged models of spin-glasses. Physical Review B, 17(11):4384\u20134403, 1978. [44] Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. Constrained language models yield few-shot semantic parsers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021. [45] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022. [46] Steven M Smith. Remembering in and out of context. Journal of Experimental Psychology: Human Learning and Memory, 5(5):460, 1979. [47] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, May 2022. [48] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015.\n[49] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems, 35:21831\u201321843, 2022. [50] Endel Tulving and Zena Pearlstone. Availability versus accessibility of information in memory for words. Journal of verbal learning and verbal behavior, 5(4):381\u2013391, 1966. [51] Endel Tulving and Donald M Thomson. Encoding specificity and retrieval processes in episodic memory. Psychological review, 80(5):352, 1973. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. [54] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095, 2023. [55] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al. A neural corpus indexer for document retrieval. Advances in Neural Information Processing Systems, 35:25600\u201325614, 2022. [56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. [57] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. [58] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. arXiv preprint arXiv:2302.05698, 2023. [59] Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. Association for Computational Linguistics, 2022.\nProof :\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8a4c/8a4c5b3f-4c38-48b3-829e-a276884467d2.png\" style=\"width: 50%;\"></div>\n \ufffd For zj and zj \u0338= zi,\n \ufffd Then, for the retrieval error, we can have,\nThus, we have proved the upper bound of the retrieval error. For the lower bound, if u\u22c6is etrieved without loss, the error will be naturally zero.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in large language models (LLMs), which has shown remarkable progress in natural language processing. The performance of ICL is heavily influenced by the choice of exemplars, but the mechanism behind exemplar selection remains unclear. This work aims to build a theoretical framework to analyze ICL and provide guidelines for selecting effective exemplars.",
        "problem": {
            "definition": "The main problem is the lack of understanding of how in-context learning operates, particularly regarding the selection of exemplars that influence performance.",
            "key obstacle": "The main challenge lies in the intuitive understanding of ICL without a solid theoretical foundation, making it difficult to formulate effective exemplar selection strategies."
        },
        "idea": {
            "intuition": "The idea was inspired by the concept of associative memory, where memory retrieval is triggered by contextual cues.",
            "opinion": "The authors propose that ICL should be viewed as contextual retrieval rather than a learning problem, as there are no actual weight updates involved in the process.",
            "innovation": "The primary improvement over previous methods is the introduction of a theoretical framework based on Hopfield Networks, which connects ICL to memory retrieval, leading to more efficient active exemplar selection."
        },
        "Theory": {
            "perspective": "The theoretical perspective is that ICL can be conceptualized as contextual retrieval from a model of associative memory, specifically Hopfield Networks.",
            "opinion": "The view is that the performance of ICL depends on the retrieval of patterns from memory based on contextual cues.",
            "proof": "The paper provides a proof of retrieval error bounds, demonstrating the relationship between exemplar selection and performance in ICL."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using two types of NLP tasks: constituency parsing (using the Penn Treebank and CRAFT datasets) and question answering (using MedMCQA and MedQA datasets).",
            "evaluation method": "The evaluation involved comparing the performance of different exemplar selection methods, including random selection and the proposed active exemplar selection, across various models."
        },
        "conclusion": "The study concludes that in-context learning can be understood as contextual retrieval from memory, and that active exemplar selection significantly improves performance compared to random selection, highlighting the importance of effective exemplar formulation.",
        "discussion": {
            "advantage": "The advantage of this paper lies in its theoretical framework that connects ICL to associative memory, providing insights into how to formulate exemplars for improved performance.",
            "limitation": "A limitation is that the theoretical framework is highly abstract and focuses on a single layer of self-attention, potentially neglecting the influence of exemplar order.",
            "future work": "Future work should aim to strengthen the theoretical connection between ICL and Hopfield Networks and explore the impact of exemplar order on performance."
        },
        "other info": [
            {
                "info1": "The paper discusses the biological plausibility of ICL as similar to human memory retrieval processes."
            },
            {
                "info2": {
                    "info2.1": "The authors utilize Monte Carlo methods for estimating the expected value of instance error in active exemplar selection.",
                    "info2.2": "The paper presents empirical results showing that active exemplar selection outperforms other methods in various experimental settings."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning (ICL) in large language models (LLMs), highlighting the importance of understanding how ICL operates, particularly regarding the selection of exemplars that influence performance."
        },
        {
            "section number": "1.3",
            "key information": "The performance of ICL is heavily influenced by the choice of exemplars, which is a critical aspect of how large language models facilitate in-context learning."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective presented in the paper conceptualizes ICL as contextual retrieval from a model of associative memory, specifically Hopfield Networks."
        },
        {
            "section number": "3.3",
            "key information": "The paper introduces a theoretical framework based on Hopfield Networks that leads to more efficient active exemplar selection, which can enhance in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "The study concludes that active exemplar selection significantly improves performance compared to random selection, emphasizing the influence of effective prompt design in ICL."
        },
        {
            "section number": "6.1",
            "key information": "A limitation discussed in the paper is that the theoretical framework is highly abstract and focuses on a single layer of self-attention, potentially neglecting the influence of exemplar order."
        },
        {
            "section number": "7",
            "key information": "Future work should aim to strengthen the theoretical connection between ICL and Hopfield Networks and explore the impact of exemplar order on performance, reflecting on the current state and future directions of in-context learning."
        }
    ],
    "similarity_score": 0.7067474043051293,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Exemplars as Clues to Retrieving from Large Associative Memory.json"
}