{
    "from": "google",
    "scholar_id": "0KZw1a_uwsIJ",
    "detail_id": null,
    "title": "Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction",
    "abstract": " Abstract\n\nIn this paper, we focus on the challenging task of reliably estimating factual knowledge that is embedded inside large language models (LLMs). To avoid reliability concerns with prior approaches, we propose to eliminate prompt engineering when probing LLMs for factual knowledge. Our approach, called  Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the in-context learning ability of LLMs to communicate both the factual knowledge question as well as the expected answer format. Our knowledge estimator is both conceptually simpler (i.e., doesn\u2019t depend on meta-linguistic judgments of LLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ZP-LKE. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts. 1\n\n# CCS Concepts\n\n# CCS Concepts\n\u2022 Computing methodologies \u2192 Information extraction.\n\n# \u2022 Computing methodologies \u2192 Information extraction.\n\n1 Code available at: https://github.com/QinyuanWu0710/ZeroPrompt_LKE\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License. WSDM \u201925, March 10\u201314, 2025, Hannover, Germany \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1329-3/25/03 https://doi.org/10.1145/3701551.3703562\n\nSaarbruecken, Germany\n\nLarge language models; Knowledge extraction; In-context learning\n\nACM Reference Format: Qinyuan Wu, Mohammad Aflah Khan,",
    "bib_name": "wu2024towards",
    "md_text": "# Towards Reliable Latent Knowledge Estimation in LLMs: ro-Prompt Many-Shot Based Factual Knowledge Extraction\n\n# Laurent Bindschaedler MPI-SWS Saarbruecken, Germany\n\nEvimaria Terzi Boston University Boston, Massachusetts, United States\n\n# Abstract\n\nIn this paper, we focus on the challenging task of reliably estimating factual knowledge that is embedded inside large language models (LLMs). To avoid reliability concerns with prior approaches, we propose to eliminate prompt engineering when probing LLMs for factual knowledge. Our approach, called  Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the in-context learning ability of LLMs to communicate both the factual knowledge question as well as the expected answer format. Our knowledge estimator is both conceptually simpler (i.e., doesn\u2019t depend on meta-linguistic judgments of LLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ZP-LKE. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts. 1\n\n# CCS Concepts\n\n# CCS Concepts\n\u2022 Computing methodologies \u2192 Information extraction.\n\n# \u2022 Computing methodologies \u2192 Information extraction.\n\n1 Code available at: https://github.com/QinyuanWu0710/ZeroPrompt_LKE\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License. WSDM \u201925, March 10\u201314, 2025, Hannover, Germany \u00a9 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1329-3/25/03 https://doi.org/10.1145/3701551.3703562\n\nSaarbruecken, Germany\n\nLarge language models; Knowledge extraction; In-context learning\n\nACM Reference Format: Qinyuan Wu, Mohammad Aflah Khan, Soumi Das, Vedant Nanda, Bishwamittra Ghosh, Camila Kolling, Till Speicher, Laurent Bindschaedler, Krishna Gummadi, and Evimaria Terzi. 2025. Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction. In  Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining (WSDM \u201925), March 10\u201314, 2025, Hannover, Germany. ACM, New York, NY, USA, 26 pages. https://doi.org/10. 1145/3701551.3703562\n\n# 1 Introduction\n\nConversational chatbots (e.g., OpenAI\u2019s ChatGPT) built around large language models (e.g., OpenAI\u2019s GPT) are increasingly being used for a variety of information retrieval tasks such as searching for information or seeking recommendations related to real-world entities like people or places [42, 51]. A worrisome concern in such scenarios is the factual correctness of information generated by the LLMs [7, 17, 18, 21, 32, 36, 40, 43, 44, 48]. The latent knowledge estimation problem: To avoid making false assertions about a real-world entity, an LLM first needs to have factual (true) knowledge about the entity. Given a prompt like \u201cEinstein was born in the year\u201d, LLMs may generate both the correct answer (\u201c1879\u201d) and wrong answers (e.g., \u201c1878\u201d or \u201c1880\u201d) with some probabilities. If an LLM knows the fact, one can hope that the probability with which it would generate the correct answer would be much higher than the wrong answers [19]. As LLMs are typically pretrained over a Web corpus (including Wikipedia data) with millions of facts about real-world entities, they have the opportunity to learn factual knowledge about our world and latently embed the knowledge in their parameters. But, how can we estimate the extent of LLMs\u2019 knowledge of real-world facts? Reliability of latent knowledge estimates: Following [33], many prior works [4, 20, 35] represent factual knowledge in the\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f99/6f99b9e3-fd6c-4945-8375-bf7a945da649.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Overview of how Latent Knowledge Estimators (LKEs) work\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d429/d429d5ed-f35b-4f1d-87ca-453a649ad3a6.png\" style=\"width: 50%;\"></div>\nform of triplets \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u27e9, where the subject \ud835\udc65 has a relation of type \ud835\udc5f with the object \ud835\udc66 (e.g., \u27e8 Einstein, birth-year, 1879 \u27e9). The central challenge of latent knowledge estimation is to infer \ud835\udc66 given \ud835\udc65 and \ud835\udc5f by only using information extracted from the LLM. Typically, the inference relies on probing the LLM with prompt templates, \ud835\udf0e (\ud835\udc65,\ud835\udc5f), constructed to communicate the information of \ud835\udc65 and \ud835\udc5f and analyzing the generated responses for presence of \ud835\udc66 (see Figure 1). Current approaches [16, 19\u2013 21, 35, 37] allow unrestricted choice of prompt templates with few well-defined rules (see Figure 2). As a result, they are vulnerable to prompt engineering and prompt hacking, which raises serious concerns about the reliability of their estimates [7]. Against this background, in this paper, we make four primary contributions: 1. A structured reliable latent knowledge estimator (LKE) based on zero prompting (ZP): Our latent knowledge estimator, called ZP-LKE, is based on the following simple, yet novel and powerful insight. Rather than engineer input prompts \ud835\udf0e (\ud835\udc65,\ud835\udc5f) that best communicate \ud835\udc5f to an LLM, we let the LLM infer \ud835\udc5f by simply providing multiple examples of \u27e8 \ud835\udc65,\ud835\udc66 \u27e9 pairs that share the same relation \ud835\udc5f (see Figure 2). The key distinguishing feature of ZPLKE is its adherence to zero prompting, i.e., the input \ud835\udf0e (\ud835\udc65,\ud835\udc5f) is highly structured and contains no prompt tokens outside other similarly related \u27e8 \ud835\udc65,\ud835\udc66 \u27e9 pairs. Thus, ZP-LKE avoids the reliability risks associated with prompt engineering such as side-channels and over-fitting. (We discuss these reliability concerns in Section 2.1.) 2. ZP-LKE requires many-shots and is fundamentally different from few-shot prompting: A recent work [21] shows that few-shot prompting (FS-LKE in Figure 2) can yield improved knowledge estimates compared to zero-shot prompting (ZS-LKE\n\nin Figure 2) by providing the LLM with examples to  format generated answers. In contrast, ZP-LKE also uses examples to effectively communicate the question at hand. The different modes in which examples are being used in FS-LKE and ZP-LKE is illustrated further in Table 1. Adding a few examples to the zero-shot prompt\"Peter Gr\u00fcnberg was born in\" results in the model generating the correct answer \"1939\" right-away. However, it does not appear to matter whether the examples provided are reflecting correct information or information about subjects known to the LLM (consistent with our hypothesis that the examples are being used to infer answer format). In contrast, Table 1 suggests that for ZP-LKE not only is the number of examples needed larger, but it also matters whether they are correct and known to the LLM (consistent with our hypothesis that examples are being used to infer the question). The two distinct ways in which examples are being used by FS-LKE and ZP-LKE map well to the dual modes of in-context learning namely, task recognition and task learning, respectively, that have been identified in a recent work[31]. We systematically investigate how factors such as how many examples are provided in an ZP-LKE, whether some of those examples are unknown to the model or simply incorrect, as well as how the examples are ordered affect knowledge estimation. We find that ZP-LKE requires many-shots, which make it relatively robust to unknown examples, but ZP-LKE remains vulnerable to incorrect examples. Our findings represent a nuanced exploration of in-context learning[5], where the dominant learning mode is task learning rather than task recognition. 3. ZP-LKE significantly outperforms previous prompt-based approaches across different open-source models and different types of factual relations: We empirically compared the\n\nTowards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n\n<div style=\"text-align: center;\">Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n</div>\nPrompt based LKEs (ZS-LKE & FS-LKE)\nZero-Prompt based LKE (ZP-LKE)\nInput\nOutput: Next 10 tokens\nInput\nOutput: Next 10 tokens\nPeter Gr\u00fcnberg was born in\nMunich in 1939 and obtained\nAlbert Einstein 1879, Peter Gr\u00fcnberg\n2007 Mohammed Hanif\nAlbert Einstein was born in 1879, Brian Kobilka\nwas born in 1955, Peter Gr\u00fcnberg was born in\n1939 and has a birthday\nAlbert Einstein 1879, Brian Kobilka 1955, Stefan\nW. Hell 1962, Ivan Pavlov 1849, Peter Gr\u00fcnberg\n1939, Albert Szent-\nAlbert Einstein was born in 1872, Brian Kobilka\nwas born in 1965, Peter Gr\u00fcnberg was born in\n1939 and has a birthday\nAlbert Einstein 1872, Brian Kobilka 1927, Stefan\nW. Hell 1962, Ivan Pavlov 1878, Peter Gr\u00fcnberg\n2007, Albert Szent-\nAuthor-1 was born in 1872, Author-2 was born\nin 1965, Peter Gr\u00fcnberg was born in\n1939 and has a life expect\nAuthor-2 1879, Author-3 1955, Author-4 1962,\nAuthor-1 1849, Peter Gr\u00fcnberg\n2022, Rossitza\nTable 1: Comparison of latent knowledge estimators for the test fact \u27e8Peter Gr\u00fcnberg Birth Year 1939\u27e9using Llama2-7B. Correct\n<div style=\"text-align: center;\">Prompt based LKEs (ZS-LKE & FS-LKE)\n</div>\nperformance of ZP-LKE against prior approaches that relied on a variety of human-generated prompts (HGPs) as well as machinemined prompts (MMPs)[20]. Across a large set of facts spanning different types of relations from the widely-used T-Rex dataset [10], we find that ZP-LKE improves the fraction of facts accurately extracted from four open-source models by an average of 35% for HGPs (from 0.45 to 0.61) and 90% for MMPs (from 0.32 to 0.61). These performance gains of ZP-LKE arise from a better comprehension of the question as well as the expected answer format. To quantify the performance gains from only better question comprehension, we propose in Section 2.2 a multiple-choice testing that accounts for answer formats. We find that ZP-LKE still outperforms existing approaches by an average of 9.41% for HGPs (from 0.71 to 0.78) and 57% for MMPs (from 0.50 to 0.78), with improvements for specific relations like \"position played on team/specialty\" varying from 152% for HGPs (from 0.17 to 0.43) and 310% for MMPs (from 0.10 to 0.43). Thus, ZP-LKE represents a better way to retrieve knowledge stored internally within an LLM, surpassing the model\u2019s ability to follow instructions in prompt templates. 4. Being model-agnostic, ZP-LKE enables a systematic comparison of latent knowledge of open source LLMs at scale: In contrast to prompt-based LKEs [20, 35], which are tailored to specific relations and models, ZP-LKE creates a single input to test for facts pertaining to a relation that can be used flexibly for any model. This simplicity and versatility allows for cross-LLM comparisions of factual knowledge. Using ZP-LKE, we evaluated the knowledge of 49 open-source LLMs from various families like Llama(2), Gemma, Mistral, OPT, and Pythia. These models vary in size and were tested with and without instruction-finetuning on 50 different relations and 20,000 facts from Wikidata. We found that models from families such as Llama2, Mistral, and Gemma, as well as larger models, know more facts. Models within the same family differ in the specific facts they know, even if trained on the same data. Additionally, instruction fine-tuning reduces the amount of factual knowledge that can be extracted from these models. Our findings will likely be of interest to developers that wish to train models with lots of embedded factual knowledge. Related Work: Researchers have proposed several approaches [44] to estimate latent knowledge from LLMs, which can be categorized into two main methods: (i) Model-internals based approaches: These methods use various internal aspects of the LLM, such as attention maps [39], activation functions [6], or model parameters [22], to determine whether factual information can be extracted\n\n<div style=\"text-align: center;\">Zero-Prompt based LKE (ZP-LKE)\n</div>\nfrom the model. (ii) Model-responses-based approaches, which are generally applicable to a wide range of LLM models and there are two key parts of the model-responses-based approach: constructing the input and evaluating the output from the LLM. Input construction: There are different prompting techniques to verify if a target fact is stored in the model. These prompt-based methods differ in their choice of prompts, which can be divided into human-generated prompts (HGPs) [7, 9, 19\u2013 21, 29, 33, 37, 39] and machine-mined prompts (MMPs) [20, 35]. All the promptingbased methods try to find the best template for the question that the model can comprehend best; however, the optimization of the prompting template can be unreliable [2, 7, 34, 46]. Instead of finding the best comprehensive template, our approach proposes using structured triplet examples to communicate and prob the tested fact, uncover deeper relations and knowledge in the LLM. This method communicates the question through in-context examples, a strategy that, to our knowledge, has not been explored before. A similar approach is to use few-shot prompting. For example, [21] first found the best prompt template and then applied few-shot prompting to guide and limit the model\u2019s response format. However, this approach still holds the limitation of template searching and relies on the model\u2019s comprehension of the template, which is fundamentally different from our approach. Output evaluation: The evaluation methods of early works are LLM specific, limiting the evaluated objects to single-token outputs [4, 20, 33, 35]. More recent works evaluate the generation by checking the next \ud835\udc58 generated tokens to see whether the potentially multi-token ground truth appears in the \ud835\udc58 generated tokens [7, 9, 19, 21, 29, 37, 39]. However, the final performance is significantly influenced by the choice of \ud835\udc58, and the generation quality also relies heavily on various sampling parameters, which introduces uncertainty [27]. In order to respond more fundamentally to the model\u2019s level of knowledge, without focusing on metrics such as the fluency of generation, we constructed a multiple-choice dataset with 100 unique possible choices for each evaluated fact and judged whether or not the model knew the fact by comparing the relative probabilities of these 100 objects. Factual knowledge datasets: Different from existing knowledge evaluation benchmarks like TruthfulQA [25] and MMLU [13], which already provide templates of questions, our approach considers facts from existing knowledge graphs for performing knowledge\n\nestimation of LLMs. As a test bed [10, 16, 23, 33, 37, 52], we utilize knowledge graphs, allowing our method to be applied to any knowledge graph database without additional effort.\n\n# 2 Designing Reliable LKEs\n\nToday, there exist many general-purpose as well as domain-specific factual knowledge bases that contain a very large number (millions to billions) of facts. The facts can be encapsulated as triplets, represented as \u27e8 subject (\ud835\udc65), relation (\ud835\udc5f), object (\ud835\udc66) \u27e9. These triplets offer a general way to represent factual knowledge about real-world entities in knowledge graphs or other structured knowledge bases. The goal of latent knowledge estimation is to infer what fraction of the facts are known to an LLM. We call methods that estimate the amount of latent knowledge inside an LLM latent knowledge estimators (LKEs).\n\n# 2.1 Reliability concerns with existing LKEs\n\nExisting approaches to estimating latent knowledge in LLMs use various factual knowledge tests. We identify several reliability concerns (RCs) with current designs that motivate our new LKE design. While some related works address some of these concerns, none have comprehensively solved all the issues [7, 21]. RC 1. Reliance on unrestricted prompt engineering: Many past works have attempted to use test prompts without any restrictions, including both human-generated or machine-mined prompts [2, 20, 34, 46]. They typically intersperse the subject \ud835\udc65 and object \ud835\udc66 between additional relationship context-communicating tokens. Some analyze the performance of a variety of prompts and then pick the best-performing or use an ensemble of the best-performing prompts [11, 20, 29]. However, unrestricted prompt engineering risks introducing side-channels and over-fitting. First, the generated prompts, particularly those that are machine-mined, may include tokens that can implicitly or explicitly introduce additional (sidechannel) information that makes it easier to answer the question. As a specific example, in a prior work [20], for the relation \u201cposition held\", the prompt \u201c\ud835\udc65 has the position of \ud835\udc66\" performed worse than \u201c\ud835\udc65 is elected \ud835\udc66\". But, note that the second prompt potentially introduces a side-channel: it implicitly rules out answer choices for unelected positions like Professor and favors elected positions like President. Second, selecting from an unbounded number of potential prompt choices raises concerns about the complexity of LKEs (the size of the set of all considered prompts) and the associated risks of over-fitting, which in turn affect the reliability of estimates. RC 2. Reliance on LLMs\u2019 meta-linguistic judgments: Prior works used prompt templates with instructions [7, 9, 19\u2013 21, 29, 33, 37, 39, 44, 49] for communicating the question as well as the expected format of answers. But, the scores (estimates) resulting from such prompt-based testing conflate an LLM\u2019s latent knowledge of the facts with the LLM\u2019s meta-linguistic judgments, i.e., the LLM\u2019s ability to comprehend the prompt, understand the question embedded within the prompt and output the answer in some expected format [15]. The impact on meta-linguistic judgments can be seen from the fact that multiple semantically-equivalent prompts result in different responses from an LLM and thereby, different estimates of latent knowledge [15].\n\nRC 3. Reliance on LLM-specific prompts: Many prior works [20, 33, 35] limit the choice of facts that can be used in tests to those where the surface form of the objects (\ud835\udc66) is represented by a single token by the LLM\u2019s tokenizer. Even though some works are able to evaluate multiple-token objects, prompt-based approaches need careful prompt engineering for different LLMs to get the best prompt template [7, 9, 19, 21, 29, 37, 39], which makes it hard to estimate and compare factual knowledge across a large number of LLMs and the prompt optimisation would be very expensive and inefficient for large models. Motivated by the above, we derive the following three design principles (DPs) for LKEs. A reliable LKE design should:\n\u2022 DP1: limit prompt hacking to avoid over-fitting & side-channels \u2022 DP2: minimize reliance on meta-linguistic prompts. \u2022 DP3: avoid LLM-specific prompts.\n\n# 2.2 A new Z eroP rompt based LKE (ZP-LKE)\n\nOur goal is to estimate whether an LLM knows a fact \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u27e9. The challenge is to probe the LLM and evaluate its responses in a way compatible with the design principles defined in Section 2.1. The key idea here is to eliminate prompts meant to capture the relation \ud835\udc5f (zero-prompt) and instead rely on examples of similarly related \u27e8 \ud835\udc65,\ud835\udc66 \u27e9 pairs to probe the internal knowledge. LLMs have been shown to exhibit In-Context Learning (ICL) abilities [5] that allow them to infer and extrapolate patterns in their inputs. We leverage this ability to communicate information about relation \ud835\udc5f without additional instructions to the LLMs (DP1 and DP2) by providing it with a list of facts based on \ud835\udc5f.\nExample 1. Assume that we want to probe for whether an LLM knows the fact \u27e8 Einstein, birth-year, 1879 \u27e9. We can use other facts for the birth-year relation such as \u27e8 Feynman, birth-year, 1918 \u27e9, \u27e8 Heisenberg, birth-year, 1901 \u27e9 to construct an input \u201cFeynman 1918 Heisenberg 1901 Einstein\u201d. By providing such zero-prompt in-context examples to the model, we expect to communicate the underlying relation between subjects and objects. To correctly extrapolate the pattern, the model needs to retrieve Einstein\u2019s birth-year as the completion of the sequence.\n\nMore formally, given a training dataset of facts F \ud835\udc5f = {\u27e8 \ud835\udc65 \ud835\udc56,\ud835\udc5f,\ud835\udc66 \ud835\udc56 \u27e9} \ud835\udc5b \ud835\udc56 = 1 for relation \ud835\udc5f, as well as a test fact \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u27e9, we leverage ICL to construct prompts that elicit information about \ud835\udc53 as\n\n# More formally, given a training dataset of facts F \ud835\udc5f = {\u27e8 \ud835\udc65 \ud835\udc56,\ud835\udc5f,\ud835\udc66 \ud835\udc56 \u27e9} \ud835\udc5b \ud835\udc56 = 1 for relation \ud835\udc5f, as well as a test fact \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u27e9, we leverage ICL to construct prompts that elicit information about \ud835\udc53 as\n\nWe use \ud835\udc5f to pick facts from F \ud835\udc5f and concatenate the tokens corresponding to the subjects and objects, but do not include any other information about \ud835\udc5f. We use space \u201c \u201d as the separator token and discuss this choice in detail in Section 4.1. We discuss other design choices for the construction of ZP-LKE in Section 3. When further details are not needed, we simply refer to some input as \ud835\udf0e. ZP-LKE design satisfies all our design principles (DPs):\n\u2022 DP1: by construction, zero-prompting eliminates prompt hacking and thus, risks of over-fitting and side-channels.\n\u2022 DP2: it only relies on the in-context learning abilities and not meta-linguistic judgments of an LLM.\n\u2022 DP3: by construction, input \ud835\udf0e (\ud835\udc65,\ud835\udc5f) is LLM-agnostic and hence, enables cross-LLM latent knowledge comparisons.\n\nTowards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n\n# 2.3 Evaluating model outputs\n\nWe evaluate the output of model \ud835\udf03 for input \ud835\udf0e (\ud835\udc65,\ud835\udc5f) in two ways: (1) Open-ended generation that lets the model generate till \ud835\udc58 tokens [21, 45] after which the presence of the ground truth is checked within the response, (2) Multiple-choice test that forces the model to predict from a list of options [20]. (1) Response testing in open-ended generation. Given a fact \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u2217 \u27e9 and a model \ud835\udf03, we provide the input \ud835\udf0e (\ud835\udc65,\ud835\udc5f) to the model and let it generate for \ud835\udc58 tokens \ud835\udc61 1,\ud835\udc61 2, ..\ud835\udc61 \ud835\udc58. We consider the answer to be correct if \ud835\udc66 \u2217 \u2286{\ud835\udc61 1,\ud835\udc61 2, ...,\ud835\udc61 \ud835\udc58} leading to the prediction \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 \ud835\udf03 (\ud835\udc53) = \ud835\udc66 \u2217. (2) Multiple-choice testing. In the multiple-choice testing, we extract the answer based on the probabilities \ud835\udf03 assigned to the tokens of the corresponding object \ud835\udc66. To allow for objects \ud835\udc66 consisting of multiple tokens and to be independent of the specific tokenization scheme or LLM (DP3), we compute the object probability over multiple tokens as follows:\n\n(2)\n\nwhere | \ud835\udc66 | denotes the number of tokens in \ud835\udc66 and \ud835\udc43 \ud835\udf03 (\ud835\udc66 (\ud835\udc56) | \ud835\udc66 [\ud835\udc56 \u2212 1:1] \ud835\udf0e) is the conditional probability of predicting the \ud835\udc56-th token \ud835\udc66 (\ud835\udc56) of \ud835\udc66 given the preceding tokens \ud835\udc66 (\ud835\udc56 \u2212 1), . . . ,\ud835\udc66 (1), and \ud835\udf0e. To determine whether model \ud835\udf03 knows a fact \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u2217 \u27e9, we test whether given an input \ud835\udf0e (\ud835\udc65,\ud835\udc5f), \ud835\udf03 can choose the correct object \ud835\udc66 \u2217\nfrom among a set of \ud835\udc40 unique alternatives. Specifically, given fact \ud835\udc53, we redefine it as \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u2217, Y\u27e9, where Y is a set of \ud835\udc40 plausible but incorrect alternatives. We discuss the choice of Y in Section 4.\n\n(3)\n\ndenotes the prediction of \ud835\udf03 for the fact \ud835\udc53 = \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u2217, Y\u27e9. The predicted object has the maximal object probability within {\ud835\udc66 \u2217} \u222aY. We evaluate the factual knowledge of model \ud835\udf03 over a dataset of test facts D = {\ud835\udc53 \ud835\udc56} \ud835\udc5a \ud835\udc56 = 1 using accuracy as a metric for both the response test and multiple-choice test:\n\ufffd \ufffd\ufffd\n\n|D\nwhere \ud835\udeff (\u00b7) is the indicator function.\n\n# 3 Exploring the design space of ZP-LKE\n\nOur ZP-LKE design avoids many of the reliability concerns of prior works [7, 14, 20, 21, 33, 45]. However, ZP-LKE also introduce a few design choices for the input i.e., \ud835\udf0e (\ud835\udc65,\ud835\udc5f) in Equation (1). One must decide the right \ud835\udc5b, the number of in-context examples included in \ud835\udf0e (\ud835\udc65,\ud835\udc5f). Furthermore, it is unclear how ZP-LKE would be affected if some chosen examples are unknown to the model or are incorrect or appeared in a different order. We study this by varying \ud835\udc5b and introducing unknown or incorrect examples within these \ud835\udc5b examples. While many prior works investigated the number of in-context examples needed for various tasks [1, 5, 8, 26, 31], it is worth re-examining them for ZP-LKE for three reasons: (i) prior works report differing results, with some reporting that increasing the number of examples improves performance [1], while others argue the opposite [26], (ii) most don\u2019t\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/660f/660fceef-41aa-4ab2-b090-5787ab7a5ea3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Number of Examples\n</div>\nFigure 3: Impact of in-context example count on multiplechoice accuracy across LLMs. The dashed line marks the number needed for 95% stable accuracy with 50 examples.\n\ncarefully distinguish between the two learning modes of in-context learning (as noted in Section 1, ZP-LKE relies on one of the modes), and (iii) only a few [8, 28] studied the influence of incorrect examples and none studied the impact of unknown examples.\nOur experiments help us understand the number of in-context examples needed, as well as how the in-context example\u2019s generation probability changes with different types of noise. We perform an in-depth empirical analysis on a Nobel Laureate dataset for the relation \u2018birth year\u2019 (details in Appendix A.1). The dataset consists of facts formatted as \u27e8 Person (\ud835\udc65), birth-year (\ud835\udc5f), YYYY (\ud835\udc66)\u27e9. The number of required in-context samples for communicating both the question and answer format varies across LLMs. In Figure 3, we report multiple choice accuracy (Eq. (4)) for different LLMs evaluated on 900 test samples, with varying numbers of in-context examples (\ud835\udc5b) that are randomly sampled from a separate training set using 5 random seeds. As the number of in-context examples increases, the mean accuracy rises while the standard deviation decreases across different LLMs, indicating that the models gradually converge to stable performance. The dashed vertical lines show the minimum number of examples required by different LLMs to achieve 95% of the accuracy reached with 50 in-context examples. Interestingly, LLMs with higher estimation accuracy require fewer in-context examples than those with lower accuracy to effectively interpret the underlying question. This maybe attributed to the amount of internal knowledge contained in the LLMs. To enable ZP-LKE across all the LLMs, we set \ud835\udc5b = 50 for the following experiments. We delve deeper to further investigate which individual facts may be known or unknown to a model. We examine the generation probability of in-context objects in 200 correct subject (\ud835\udc65)-object (\ud835\udc66) pairs using the Mistral-7B model. We can see in Figure 4athat Mistral-7B model shows a gradual increase in the probability of generating correct objects from left to right on the x-axis (where points on the right have more context to leverage) stabilizing at a mean probability of approximately 0.85. Some objects at later positions, however, have a lower generation probability, suggesting that the LLM may be less confident about its knowledge of the facts corresponding to those pairs. Thus, we can leverage the generation probability as a signal of the LLM\u2019s confidence when evaluating\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/156d/156dd6fb-09fb-40d3-83d0-91f33dccd01c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Example Position\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f08/6f081d2e-3ed5-4e1e-b04a-0522b4c3ea77.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Distributed unknown examples\n(c) Continuous unknown ex\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d09f/d09f130e-cbd2-4178-96fe-89719b712004.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Distributed incorrect examples\n(e) Continuous incorrect examples\n</div>\nFigure 4: Variation in Nobel laureate data probabilities using Mistral-7B. Figure 4a illustrates object probabilities at various positions in the prompt. Figures 4b and 4c show impacts of unknown objects at random and continuous positions, while Figures 4d and 4e show effects of incorrect examples. The dashed line indicates average correct probabilities (blue dots).\n\nLKEs (see Appendix D). Similar results for additional models are presented in Appendix E. Models are robust to unknown examples.  Next, we investigate the robustness of estimates to the occurrence of unknown examples. We insert unknown examples in two distinct ways: randomly distributed throughout \ud835\udf0e (\ud835\udc65,\ud835\udc5f)  and in a more extreme scenario, where a continuous block of examples is replaced with unknown ones. We selected 40 out of the 200 examples and replaced them with unknown examples created using fictitious names and birth years 2. Our findings are shown in Figures 4b and 4c for\n\n2 generated via https://en.namefake.com/api\n\ndistributed and continuous replacement respectively. Unknown examples are marked by red dots, examples immediately following unknown ones in cyan dots and the rest in blue dots. The unknown examples show generation probabilities close to zero, confirming the LLM\u2019s tendency to assign low probabilities to unknown data. However, interestingly, unknown examples minimally impact the generation probability of the surrounding data in both settings. Models are vulnerable to incorrect examples. Similar to the setup for unknown examples, we also insert 40 (out of 200) incorrect examples randomly (Figure 4d) and simultaneously (Figure 4e). In our experiments, these incorrect examples are created by altering the birth years of known Nobel laureates and are marked by red dots in the plots. In contrast to inserting unknown examples, the LLM significantly struggles with the injection of incorrect examples. It detrimentally affects the LLM\u2019s performance in both settings thus revealing the vulnerability of the models towards incorrect examples. We highlight one randomly marked yellow star example in Figure 4a, Figure 4b, and Figure 4d to show how the presence of incorrect samples significantly brings down the probability of the neighboring points. Summary: The key takeaways while exploring the design space of ZP-LKE are - (a) Different LLMs take varying numbers of incontext samples to comprehend both the question and format of the answer alongside, with 50 being an optimal number for our setup. (b) The models are robust to unknown examples but vulnerable to incorrect examples. To the best of our knowledge, we are the first to distinguish between examples unknown to the model and incorrect examples known to the model and study their impact on in-context learning. As ZP-LKE relies on many examples, this distinction and understanding is important in practice. Also, while [1] found that the order of examples has varying effects in different domains, we identify the distribution of unknown and incorrect examples as a crucial underlying factor.\n\n# 4 Experiments and Results\n\nAs ZP-LKE inputs are model-agnostic and easy to adapt for a large variety of relations, it can be used to very effectively to conduct cross-LLM latent knowledge comparisons. We leverage ZP-LKE to estimate latent knowledge across 49 open-source (pre-trained and fine-tuned) LLMs, spanning different LLM families (Llama (2), Mistral, Mixtral, Gemma, Falcon, Pythia, Bloom, and OPT) and sizes (from 70M to 8 \u00d7 22B). To the best of our knowledge, we are the first to evaluate a knowledge estimation framework across a large number of models. We list the models and their simplified names used in this paper in Appendix G, Table 7, and provide a leaderboard of models based on ZP-LKE in Appendix G, Table 8. We hope our results and the framework can help future LLM developers reliably and efficiently estimate the latent knowledge of their models. Dataset: We evaluate the knowledge of models on a large set of facts from the T-REx dataset [10]. We selected relations from T-REx with at least 500 samples that are linked to a minimum of 100 unique objects. We create a list of multiple choices for each sample and ensure that instances with multiple correct objects do not have any of their correct answers in their multiple-choice list. This filtering leads to 50 distinct relations spanning categories like birth dates, directorial roles, parental relationships, and educational\n\nTowards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f27/0f27e895-c3c1-4899-aa59-e5582857a844.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Response Accuracy\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e52f/e52f6a43-84de-4d55-883a-da4a6f62e6a8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Multiple-choice Accuracy\n</div>\nFigure 5: Comparison of LKEs using response and multiplechoice accuracy across 12 relations from T-REx-MC. ZPLKE is evaluated against the baseline method [20].\n\nlineage. The resulting T-REx Multiple Choice (T-REx-MC) dataset comprises 5,000 training and 20,000 test facts. Appendix A contains detailed information on the dataset and relations. Choosing the set Y & its impact on test difficulty: For each fact \u27e8 subject (\ud835\udc65), relation (\ud835\udc5f), object (\ud835\udc66 \u2217) \u27e9, we generate alternative objects Y to create multiple choices. Note that the alternative objects in Y  are viable choices and cannot be easily eliminated. Therefore, for each fact \u27e8 \ud835\udc65,\ud835\udc5f,\ud835\udc66 \u2217 \u27e9 we select \ud835\udc66 \u2208Y from other facts in the dataset that share the same relationship \ud835\udc5f. For computational feasibility, we sample |Y| = 99 alternative objects per fact, so that a random guess between {\ud835\udc66 \u2217} \u222aY has a 0. 01 probability of being correct.\n\n# 4.1 ZP-LKE vs. prompt-based approaches\n\nWe compare the performance of ZP-LKE with the existing promptbased approaches [20] using both the response accuracy and the multiple-choice accuracy defined in Section 2.2. ZP-LKE outperforms prompt-based approaches.  We randomly sample three human-generated prompts (HGP) and machinemined prompts (MMP) from [20] for 12 common relations between T-REx-MC and [20]. We show that ZP-LKE outperforms HGP and MMP in terms of the accuracy measures by a large margin, across different models and 12 relations in Figure 5; the detailed accuracy for each relation can be found in the Appendix G.2, Figure 14 and 15. Figure 5a shows that ZP-LKE improves the fraction of facts accurately extracted from four open-source models by an average\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6336/63365ad4-1d6e-4cc1-8a28-b119a051e74c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Knowledge Extraction Method\n</div>\nFigure 6: Impact of separators on the relation \u2019original broadcaster.\u2019 Subject-object pairs are separated by humangenerated prompts (HGP, red background) or machine-mined prompts (MMP, blue background).\n\nof 35% for HGPs (from 0.45 to 0.61) and 90% for MMPs (from 0.32 to 0.61). In the case of multiple-choice accuracy, having controlled the influence of the answer format, we observe that all the knowledge estimation methods improve in their performance. Alongside, ZPLKE still outperforms existing approaches by an average of 9.41% for HGPs (from 0.71 to 0.78) and 57% for MMPs (from 0.50 to 0.78). The multiple-choice accuracy metric disentangles the answer format from the question leading to better factual knowledge estimation across the board. Hence, we primarily report the multiple-choice accuracy metric for the experiments in the rest of the paper. ZP-LKE performs better than FS-LKE with the same number of examples. We adapt ZP-LKE by replacing the separator token \u201c \u201d between subjects and objects with three prompts each from HGPs and MMPs for the relation \u2018original broadcaster\u2019 and report the multiple choice accuracy in Figure 6. We intend to understand whether the additional prompt tokens can help in communicating the question better. The ZP-LKE with \u201c \u201d token performs equally or even better for some models compared to the semantically meaningful prompts from HGP and MMP, which now correspond to the FS-LKE. Thus, relation-specific separators (or prompts) have limited impact on factual knowledge estimation if subject-object pairs are correctly presented. Additionally, finding relation-specific prompts requires hand-crafted efforts or additional computation [35], unlike our zero-prompt many-shot approach using (subject, object) pairs. Therefore, ZP-LKE can potentially extend to any fact from knowledge graphs over any LLM, while HGPs and MMPs require additional supervision and relation-specific validation.\n\n# 4.2 Evaluating Diverse Models and Relations\n\nWe investigate the performance of 35 pre-trained LLMs and 14 fine-tuned LLMs across 50 relations using the ZP-LKE framework. Our analysis aims to uncover nuanced insights into the knowledge levels within these models. We will examine the results through two primary lenses: (1) the variations in knowledge across different model families, and (2) the influence of model size and fine-tuning within the same model family on their knowledge attributes.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/795f/795f380d-724a-4ba9-b347-f57a8560cb58.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Model\n</div>\n<div style=\"text-align: center;\">Figure 7: Multiple-choice accuracy for 35 pre-trained LLMs on 50 relations from T-REx-MC. Models are grouped by family, ordered by their average accuracy, and arranged from left to right based on proximity to 7 billion parameters. Within each family, models are ordered by their average accuracy.\n</div>\nFigure 7: Multiple-choice accuracy for 35 pre-trained LLMs on 50 relations from T-REx-MC. Models are grouped by family, ordered by their average accuracy, and arranged from left to right based on proximity to 7 billion parameters. Within each family, models are ordered by their average accuracy.\n\n4.2.1 Comparing different LLM families. Some model families are consistently more knowledgeable than the rest. We sort the model families based on the performance of the model closest to 7B parameters 3, and the models within each family based on average accuracy across 50 relations. Figure 7 shows that Mistral, Llama2, Gemma, and Llama families have higher performance on most of the relations than Pythia, Bloom, and OPT, indicating the latter\u2019s lower factual knowledge. Different model families align in their relative factual knowledge.  Although different model families have different knowledge levels, they have similar knowledge structures. We investigate the correlations between each model pair\u2019s performance over 50 relations to assess the agreement in their knowledge levels. We compute the average correlations within each model family (e.g. Llama2 7B, 13B, 70B) in Figure 8. Despite differences in architecture and training datasets among model families, there is a significant consensus (correlation > 0.6) regarding the hierarchy of knowledge across various relations. We also compile the three best and worst-performing relations for each model in Table 10, illustrating the consensus among all models. The consistent underperformance across specific relations also suggests that certain types of knowledge are universally less well-represented across different models, regardless of their architecture or size. This consistency in less-known knowledge across models highlights a potential vulnerability that could be exploited if these weaknesses are not addressed. Figure 16 shows the correlations between all the models within each family.\n\n3 7B parameters is a good reference point since all model families except GPT-NEO-X have models within a gap of \u2264 1B parameters: Mistral-7B, Gemma-7B, Llama-7B, Falcon-7B, MPT-7B, OPT-6.7B, GPT-J-6B, Pythia-6.9B, and Bloom-7.1B.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c83b/c83b43b8-c6a4-47dc-9be0-da1bbd9e3063.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Pearson correlation coefficients between model families. We compute pairwise Pearson correlations between models and calculate the average score within each family.\n</div>\nFigure 8: Pearson correlation coefficients between model families. We compute pairwise Pearson correlations between models and calculate the average score within each family.\n\n4.2.2 Comparing within the same LLM family.  Larger models embed more knowledge with certain exceptions. Figure 7 shows that within each model family, larger models (e.g., Llama-65B) generally outperform smaller ones (e.g., Llama-13B). Models within the same family are typically pre-trained on the same datasets [3, 38, 47]. The results suggest that, when trained on identical datasets, larger models in general capture a broader set of facts. An exception lies in the OPT group of models [47] that have also been trained on the Pile dataset [12] like some of the other models [3]. This may call for investigating the ways of knowledge injection and validating if that can be attributed to such a performance deviation. Despite being trained on the same data, models might remember different facts. From the above results, it is not clear if the larger models are subsuming smaller models in their factual knowledge, i.e., do the larger models correctly identify the facts that the smaller models are correct on? To assess this, we compute the subsumption rate \ud835\udf02:\n\nthat measures how much of the fraction of facts from F known by smaller model \ud835\udf03 1 are also recognised by the larger model \ud835\udf03 2. A subsumption rate of \u223c 1 indicates that all of the smaller model\u2019s knowledge is also contained in the larger model. Table 2 shows the average subsumption rate (\ud835\udf02) between the largest and smallest models in a family, as well as the average accuracy, over all relations for different model families. Interestingly, \ud835\udf02 is relatively low (< 0.5) for OPT, Pythia and Bloom (i.e., the larger models know less than 50% of what the smaller models know) and only reaching up to 0.8 for Gemma, Llama and Llama-2. Therefore, even though models within each family are trained on the same datasets and generally agree on the relative knowledge of different relations (Figure 8), there are differences in the knowledge of specific facts they retain from their training data. These discrepancies suggest that simply increasing the model size may not be sufficient\n\nTowards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n\nTable 2: Average subsumption rate (\ud835\udf02) for different model families over the relations in T-REx-MC. Accuracy corresponds to the multiple-choice accuracy.\n\n<div style=\"text-align: center;\">Largest Model\n</div>\nSmallest Model\nLargest Model\nFamily\n#Parameters\nAccuracy\n#Parameters\nAccuracy\n\ud835\udf02\nLlama\n7B\n0.699\n65B\n0.836\n0.769\nLlama-2\n7B\n0.741\n70B\n0.846\n0.801\nGemma\n2B\n0.666\n7B\n0.750\n0.710\nOPT\n125m\n0.430\n30B\n0.588\n0.481\nPythia\n70m\n0.334\n12B\n0.648\n0.403\nBloom\n560m\n0.410\n7.1B\n0.548\n0.498\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba0e/ba0e2f30-39df-4fd3-a9d1-e4fc223282db.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Multiple-choice accuracy of base vs. chat-finetuned models. Finetuned models (lighter shades) show lower accuracy across T-REx-MC relations compared to pre-trained models (darker shades).\n</div>\nto enhance factual knowledge, thus requiring the need for proper factual knowledge injection into the models. Instruction fine-tuning reduces latent knowledge. Finally, we investigate the effects of chat-based instruction fine-tuning on the factual knowledge of models. Base language models are often fine-tuned (using a mix of supervised and reinforcement learning [30]) to improve their ability to follow instructions. While previous studies have shown that fine-tuning enhances performance on various benchmarks, its impact on latent knowledge is unclear. Figure 9 illustrates the comparative accuracy of pre-trained models and their fine-tuned counterparts. In almost all cases, the fine-tuned models obtain lower accuracy than their base versions, suggesting that fine-tuning reduces the amount of latent knowledge estimation.  A similar observation was made by [45]. To further assess if fine-tuned models acquire new knowledge, we compute the subsumption rate between pre-trained and fine-tuned versions (Table 11). We find that most latent knowledge in fine-tuned models is already present in base models (high \ud835\udf02). This outcome highlights the need for caution when fine-tuning models, as these adjustments might inadvertently compromise with the existing internal knowledge.\n\n# 5 Concluding Discussion\n\nIn this work, we investigate a new way to estimate latent factual knowledge from an LLM. Unlike prior approaches, our method does not engineer prompts (zero-prompting). Rather it relies on LLMs\u2019 in-context learning ability to infer the factual knowledge\n\nquestion and the expected answer format. Our method not only addresses many reliability concerns with prompting, but it also recollects significantly more factual knowledge than prompting. In contrast to prompting, which requires relationship-specific and LLM-specific prompt engineering, Our method can be applied with minimal effort to test factual knowledge of relations across a variety of structured knowledge bases and LLMs. This ability enables us to compare the latent knowledge captured by many different families of open-source LLMs; we expect our results to be of interest to the designers of these LLMs. Finally, to design our zero-prompt manyshot LKE, we explore the impact of the number and order of correct, incorrect, and unknown examples used as inputs; our findings may be of independent interest to developing a better understanding of different learning modes of in-context learning. A fundamental question posed by our and prior work on estimating latent knowledge in LLMs: What does it mean for an LLM to know a fact? Suppose we tried to infer if an LLM knows the capital of Germany using the input \"France Paris; Spain Madrid; Germany\" and suppose the answer was Berlin. What we have learned is that the LLM knows that the relationship \ud835\udc5f between Germany and Berlin is similar to that between France and Paris or Spain and Madrid. What we have not learned is whether the LLM knows that the relation \ud835\udc5f is called \"capital\" in English or \"hauptstadt\" in German. The latter is revealed by prompts such as \"The capital of Germany is \". But, such prompts don\u2019t reveal whether the LLM knows that what Berlin means to Germany is similar to what Paris means to France. Is one type of knowing facts better than another? It is difficult to answer in general. Neither type of knowing guarantees that the knowledge can be put to use in different contexts and tasks, such as when we ask the LLM where the parliament of Germany is located. However, they lead to different strategies for getting LLMs to generate correct outputs. With the first type of knowing, we can use a list of facts as input such as \"The parliament of France is in Paris; The parliament of Spain is in Madrid; The parliament of Germany is in \". With the second type of knowing, we can hope to use a chain of thought prompts such as \"The parliament of a country is in its capital. The parliament of Germany is in \". Nevertheless, one clear takeaway from our study is related to how factual knowledge is latently embedded in an LLM. We show that more factual knowledge can be recollected using in-context learning, i.e., the representations of subjects and objects that share the same relationship, than by prompting with the name of their relationship.\n\n# 6 Ethical Considerations\n\nOur research utilizes public datasets and open-source LLMs, which mitigates immediate privacy concerns. However, our findings on the factual knowledge capabilities of various LLMs could influence their deployment in real-world applications, potentially leading to over-reliance on models for tasks requiring factual accuracy. We encourage users of our methodology to consider these implications and to use the knowledge estimation techniques responsibly, with appropriate safeguards against potential misuse. Furthermore, as our work may reveal biases or gaps in the factual knowledge of LLMs, we urge developers to address these issues to ensure fair and equitable AI systems.\n\n[1] Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. 2024. Many-shot in-context learning. arXiv preprint arXiv:2404.11018 (2024).\n[2] Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel J. Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher R\u00e9. 2023. Ask Me Anything: A simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=bhUPJnS2g0X\n[3] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning. PMLR, 2397\u20132430.\n[4]  Zied Bouraoui, Jose Camacho-Collados, and Steven Schockaert. 2020. Inducing relational knowledge from BERT. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 7456\u20137463.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.\n[6] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering Latent Knowledge in Language Models Without Supervision. https://doi.org/10. 48550/arXiv.2212.03827 arXiv:2212.03827 [cs].\n[7] Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021. Knowledgeable or educated guess? revisiting language models as knowledge bases. arXiv preprint arXiv:2106.09231 (2021).\n[8]  Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023. How Many Demonstrations Do You Need for In-context Learning? arXiv preprint arXiv:2303.08119 (2023).\n[9] I.-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. FacTool: Factuality Detection in Generative AI \u2013 A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. http://arxiv.org/abs/2307.13528 arXiv:2307.13528 [cs] version: 2.\n[10] Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).\n[11] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt\u00e4schel. 2023. Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. arXiv:2309.16797 [cs.CL]\n[12] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).\n[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020).\n[14] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d\u2019Amato, Gerard De Melo, Claudio Gutierrez, Sabrina Kirrane, Jos\u00e9 Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, et al. 2021. Knowledge graphs. ACM Computing Surveys (Csur) 54, 4 (2021), 1\u201337.\n[15] Jennifer Hu and Roger Levy. 2023. Prompting is not a substitute for probability measurements in large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 5040\u20135060.\n[16] Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S Yu, and Zhijiang Guo. 2023. Do Large Language Models Know about Facts? arXiv preprint arXiv:2310.05177 (2023).\n\n[17]  Xiangkun Hu, Dongyu Ru, Qipeng Guo, Lin Qiu, and Zheng Zhang. 2023. RefChecker for Fine-grained Hallucination Detection. (2023). https://github.com/ amazon-science/RefChecker\n[18] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1\u201338.\n[19] Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics 9 (2021), 962\u2013977.\n[20] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know?  Transactions of the Association for Computational Linguistics 8 (2020), 423\u2013438.\n[21] Jan-Christoph Kalo and Leandra Fichtel. [n. d.]. Kamel: Knowledge analysis with multitoken entities in language models.\n[22] Amirhossein Kazemnejad, Mehdi Rezagholizadeh, Prasanna Parthasarathi, and Sarath Chandar. 2023. Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. arXiv preprint arXiv:2305.14775 (2023).\n[23] Wojciech Kry\u015bci\u0144ski, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Evaluating the Factual Consistency of Abstractive Text Summarization. http: //arxiv.org/abs/1910.12840 arXiv:1910.12840 [cs].\n[24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz, Germany) (SOSP \u201923). Association for Computing Machinery, New York, NY, USA, 611\u2013626. https://doi.org/10.1145/3600006.3613165\n[25] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 (2021).\n[26] Ziqian Lin and Kangwook Lee. 2024. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819 (2024).\n[27] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187 (2023).\n[28] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837 (2022).\n[29]  Benjamin Newman, Prafulla Kumar Choubey, and Nazneen Rajani. 2022. PAdapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. In International Conference on Learning Representations. https: //openreview.net/forum?id=DhzIU48OcZh\n[30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[31] Jane Pan. 2023. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. Master\u2019s thesis. Princeton University.\n[32] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 (2023).\n[33] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 (2019).\n[34] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying Language Models\u2019 Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324 (2023).\n[35] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 (2020).\n[36] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. 2023. On Early Detection of Hallucinations in Factual Question Answering. arXiv preprint arXiv:2312.14183 (2023).\n[37] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs? http://arxiv.org/abs/2308.10168 arXiv:2308.10168 [cs].\n[38]  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[39] Chenguang Wang, Xiao Liu, and Dawn Song. 2020. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967 (2020).\n[40] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. 2023. Survey on factuality in large language models: Knowledge, retrieval and domainspecificity. arXiv preprint arXiv:2310.07521 (2023).\n\nTowards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n\n[41] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Qun Liu and David Schlangen (Eds.). Association for Computational Linguistics, Online, 38\u201345. https://doi.org/10. 18653/v1/2020.emnlp-demos.6\n[42] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023).\n[43] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023. LLM lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 (2023).\n[44] Paul Youssef, Osman Alperen Kora\u015f, Meijie Li, J\u00f6rg Schl\u00f6tterer, and Christin Seifert. 2023. Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models. arXiv preprint arXiv:2310.16570 (2023).\n[45] Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Kaifeng Yun, Linlu GONG, Nianyi Lin, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Xu Bin, Jie Tang, and Juanzi Li. 2024. KoLA: Carefully Benchmarking World Knowledge of Large Language Models. In The Twelfth International Conference on Learning\n\nRepresentations. https://openreview.net/forum?id=AqN23oqraW\n[46] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny can\u2019t prompt: how non-AI experts try (and fail) to design LLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 1\u201321.\n[47] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).\n[48] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. arXiv preprint arXiv:2309.01219 (2023).\n[49] Xin Zhao, Naoki Yoshinaga, and Daisuke Oba. 2024. What Matters in Learning Facts in Language Models? Multifaceted Knowledge Probing with Diverse MultiPrompt Datasets. arXiv preprint arXiv:2406.12277 (2024).\n[50] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2023. Efficiently Programming Large Language Models using SGLang. arXiv:2312.07104 [cs.AI]\n[51]  Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for Information Retrieval: A Survey. arXiv preprint arXiv:2308.07107 (2023).\n[52] Zeyuan Allen Zhu and Yuanzhi Li. 2023. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316 (2023).\n\n# A Dataset\n\n# A.1 Creation of Nobel laureates dataset from Wikidata\n\nhe Nobel Dataset is a collection of biographical information about all Nobel laureates up until the year 2022, totaling 954 individuals. Th ataset was curated using data obtained from Wikidata\u2019s querying service 4. The following attributes are included for each laureate:\n\u2022 Name: The full name of the Nobel laureate. \u2022 Birth Year: The year in which the laureate was born. \u2022 Award Year: The year(s) in which the laureate was awarded the Nobel Prize. \u2022 Nature of Award: A brief description of the reason for the award, including the field of the Nobel Prize (e.g., Physics, Peace). \u2022 Gender: The gender of the laureate.\nHere are some examples from the Nobel Dataset:\n\n<div style=\"text-align: center;\">Table 3: Excerpt from the Nobel Dataset\n</div>\nName\nBirth Year\nAward Year\nNature of Award\nGender\nAlbert Einstein\n1879\n1921\nPhysics\nmale\nLouis de Broglie\n1892\n1929\nPhysics\nmale\nCarl D. Anderson\n1905\n1936\nPhysics\nmale\nPolykarp Kusch\n1911\n1955\nPhysics\nmale\nMelvin Schwartz\n1932\n1988\nPhysics\nmale\nJerome I. Friedman\n1930\n1990\nPhysics\nmale\n# Creation of multiple choices from T-REx: TRE\n\nT-REx [10] is a large-scale alignment dataset that aligns between Wikipedia abstracts and Wikipedia triples. We have utilized the processed version of T-REx available on HuggingFace 5 for our experiments. We filtered out the relations that have more than 500 facts and 100 unique object entities. The unique objects ensure having 100 feasible multiple choices for each fact in each relation. We also manually filtered out relations with multiple correct objects (e.g. \u201cAmerica\", \u201cUSA\", \u201cAmerican\") to avoid ambiguity. Additionally for relations that have objects in the form of partial matches (e.g. \u201cFrench\", \u201cFrench language\"), the respective objects have been standardized to uniform values (e.g. \u201cFrench\"). We curated 50 relations for our dataset TREx-MC that essentially consists of <subject, relation, multiple choices>. The multiple choices comprise the correct answer along with 99 other potential choices. We list the 50 relations in Table 4 below. The following attributes are included in TREx-MC dataset for each relation:\n\u2022 Subject: The subject entity for each fact. \u2022 Object: The object entity or the correct answer for each fact. \u2022 Multiple choices: The list of other potential choices for each fact. \u2022 Title: The Wikipedia title for each fact. \u2022 Text: The Wikipedia abstract corresponding to each fact.\nSome examples from the T-REx-MC dataset for 2 relations are listed in Table 5\n4 https://query.wikidata.org/\n\n4 https://query.wikidata.org/ 5 https://huggingface.co/datasets/relbert/t_rex\n\n<div style=\"text-align: center;\">Table 4: List of 50 relations from T-REx-MC\n</div>\ndate of\nbirth\ndate of\ndeath\ndirector\nfather\nspouse\nchild\nsibling\ncomposer\nis a\ntributary of\nstudent of\ninstance of\ncast\nmember\ngenre\ncontains the\nadministrative\nterritorial\nentity\neducated\nat\nparent\ntaxon\nscreen\nwriter\nperformer\ncapital\nproducer\nis made by\nnamed\nafter\ndeveloper\npublisher\nfounded\nby\ndrafted\nby\nhas\nplayed\nat\npart of\nthe series\nmanufacturer\nproduction\ncompany\nmother\ncause of\ndeath\nhas\nsubsidiary\ncreates\npoint in\ntime\ninception\npublication\ndate\nlanguages\nspoken,\nwritten\nor signed\noriginal\nlanguage\nof film or\nTV show\nofficial\nlanguage\nnative\nlanguage\nposition\nplayed\non team /\nspeciality\noriginal\nbroadcaster\nrecord\nlabel\nauthor\ndiscoverer\nor inventor\ncharacters\nlyrics by\ndistributed by\nhome venue\n<div style=\"text-align: center;\">Table 5: Excerpts from T-REx-MC Dataset\n</div>\nSubject\nObject\nMultiple choices\nTitle\nText\nDate of birth\nGiovanni Bia\n24 October 1968\n[\u201926 September 1981\u2019,\n\u201920 February 1981\u2019,\n..,\u201920 September 1960\u2019]\nGiovanni Bia\nGiovanni Bia\n(born 24 October 1968)\nis a former\nItalian footballer...\nBrian May\n19 July 1947\n[\u201924 December 1931\u2019,\n\u20191 December 1976\u2019,\n... \u201923 August 1964]\nBrian May\nBrian Harold May, CBE\n(born 19 July 1947)\nis an English musician...\nComposer\nMexico Trilogy\nRobert Rodriguez\n[\u2019Fred Schneider\u2019, \u2019Brandy\u2019,\n.., \u2019Tommaso Traetta\u2019]\nMexico Trilogy\nThe Mexico Trilogy or\nMariachi Trilogy\n(also Desperado Trilogy\non some DVD releases)\nis a series of American..\nChelsea Walls\nJeff Tweedy\n[\u2019Carmine Coppola\u2019,\n\u2019Jimmy Chi\u2019,\n...\u2019Maurice Ravel\u2019]\nChelsea Walls\nChelsea Walls is a 2001\nindependent film\ndirected by Ethan Hawke\nand released by Lions Gate\nEntertainment.\n# B Inference Setup\n\nWe experiment with and use three different inference setups:\n(1) Transformers Based Setup: This setup utilizes the utilities present in the transformers library [41] to obtain the log probabilities for generating the different options.\n(2) vLLM Based Setup: vLLM ([24]) is a fast inference library for large language models (LLMs). It efficiently manages attention key and value memory using PagedAttention. We observed considerable speed boosts for all 3 LKEs compared to the standard Transformers API.\n(3) SGLang Based Setup: SGLang [50] is a structured generation language designed for large language models (LLMs). It speeds up LLM interactions and provides enhanced control through tight integration of its frontend language and backend runtime system. SGLang also leverages Radix Attention to cache common components across queries in the KV cache, enabling substantial speedups. We observed sizable speed boosts for ZP-LKE over vLLM. However, we are constrained by SGLang\u2019s limited model family support at the moment, and only utilize it for the Llama, Mistral, and Mixtral families.\n\n# C Implementation Details\n\nZP-LKE leverages 50 randomly chosen samples from the training data as in-context examples but does not use the relation name. The base prompt is now composed of 50 different examples followed by the name of the entity being tested. A sample would be \u201cAlbert Einstein 14 March 1879 Ernest Rutherford 30 August 1871 ... J.J. Thomson 18 December 1856 Max Planck.\" A single forward pass is conducted for each sequence, generating log probabilities for the entire sequence. The common part, represented by the tokens for the base prompt is then removed from the tokens of the concatenated base prompt and option resulting in the log probabilities for the option. If the option is tokenized into multiple tokens, a single probability value is obtained by multiplying the individual token probabilities. The resulting values are normalized across multiple choices, and the option with the highest probability is selected as the correct answer. We use the vLLM Based & SGLang Based Setup for this LKE.\n\n# D Different Metrics\n\nThe evaluation metric can readily be adapted to existing classification metrics. For example, we introduced the metric Accuracy@K, calibrated measure that assesses a model\u2019s confidence in its predictions. This metric quantifies how accurately the model identifies knowledg at specified confidence levels for a given relation. We filter the instances that have their confidence levels > threshold \ud835\udc3e and form the se D \ud835\udc3e = {\ud835\udc50 \ud835\udc56 | pred \ud835\udf03 (\ud835\udc50 \ud835\udc56) \u2265 \ud835\udc3e \u2200 \ud835\udc50 \u2208D}. Following this, we use our accuracy measure to compute Accuracy@K for varying values of \ud835\udc3e, th results of which are shown in Figure 10.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afeb/afeb33c6-9b09-4638-9890-0195d8ff74e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Multiple-choice Accuracy@K for different models. We evaluated five models on the Nobel dataset, which 50 examples. Each model\u2019s performance was measured using the Accuracy@K metric at various thresholds.\n</div>\n<div style=\"text-align: center;\">oice Accuracy@K for different models. We evaluated five models on the Nobel dataset, which consists of del\u2019s performance was measured using the Accuracy@K metric at various thresholds.\n</div>\nuracy@K for different models. We evaluated five models on the Nobel dataset, which consists of rformance was measured using the Accuracy@K metric at various thresholds.\n\nTowards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction\n\n# E Probabilities of objects in sequence\n\nWe first consider 200 correct examples (subject-object pairs) and report the absolute generation probability of objects in corresponding examples. We showed the results for Llama2-7B, Falcon-7B, and Pythia-12B in Figure 11, Figure 12 and Figure 13. Figure 11a, Figure 12a and Figure 13a illustrates the probability of each object at various sequence positions; Figure 11b, Figure 12b and Figure 13b shows the impact on probabilities after substituting 40 objects dispersed within the sequence with incorrect ones. Figure 11c, Figure 12c and Figure 13c visualizes the effect of replacing objects at simultaneous positions. Figure 11d, Figure 12d, Figures 13d, Figure 11e, Figure 12e and Figure 13e present the outcomes of using unknown subject-object pairs as replacements. We used a horizontal dashed line showing an average probability of the correct examples. The yellow star marks the example at position 114 in the sequence.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ae0/1ae0105e-cee2-47af-8279-5f2e287b43fe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98a2/98a249cd-9bc1-4a42-bc86-caf1d1a9dccc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f44b/f44b4591-dcdc-429d-84f6-d9f93819fc88.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Correct Subject-Object Pairs\n</div>\n<div style=\"text-align: center;\">(b) Distributed incorrect examples\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ee9/4ee96511-b162-41dd-ad77-539daab304c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) Distributed unknown examples\n</div>\nF Details about the human-generated prompts and machine-mined prompts\nWe list the used human-generated and machine-mined prompts from [20] in Table 6 with subjects denoted as <\u2018subject\u2019>\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea7d/ea7df0e1-e823-4e35-ae13-4f6a66366baa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Simultaneous incorrect examples\n</div>\n<div style=\"text-align: center;\">Correct Unknown\n</div>\n<div style=\"text-align: center;\">(e) Simultaneous unknown examples\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65b3/65b3ebc7-7c52-4a97-8439-f7269583e305.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d259/d259de70-745f-42d2-81b8-594cf4b8604e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Correct Subject-Object Pairs\n</div>\n<div style=\"text-align: center;\">(b) Distributed incorrect examples\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/62d7/62d700ed-8b69-46d5-acd3-638305ecf007.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Correct Unknown Following\n</div>\n<div style=\"text-align: center;\">Figure 12: Analysis of object probability in one sequence of Nobel laureate data using Pythia-12B\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fb6d/fb6df3c5-65d7-47c4-98cc-b008f8766333.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5c2/e5c2601a-9b92-42e7-9597-ae984794ab0a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b5d9/b5d96264-dd1a-48ba-8d72-920aefc4ff69.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Distributed incorrect examples\n</div>\n<div style=\"text-align: center;\">(a) Correct Subject-Object Pairs\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8723/87238b61-ef8e-4ce4-89a1-e3ab3f9f665a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ffe/3ffe842d-822c-4b60-8ed5-7ae2fca7e648.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b7e/9b7e2812-af0d-47ae-a57a-b172b83b8a6f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Simultaneous incorrect examples\n</div>\n<div style=\"text-align: center;\">(e) Simultaneous incorrect examples\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9de/e9de2a3b-e012-4d5e-aa18-d2337eca9a3f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Simultaneous incorrect examples\n</div>\n<div style=\"text-align: center;\">Table 6: Templates for Selected Relations\n</div>\nRelation Name\nIndex\nHGP Template\nMMP Template\n1\n{subject} means\n{subject} is a small\nInstance of\n2\n{subject} is one\n{subject} and liberal\n3\n{subject} is a\n{subject} artist\n1\n{subject} is playing music\n{subject} series of\nGenre\n2\n{subject} play\n{subject} favorite\n3\n{subject} performs\n{subject} is an american\n1\n{subject} plays in position\n{subject} substitutions :\nPosition played on team / speciality\n2\n{subject} plays at position\n{subject} substitutes :\n3\n{subject} is in the position\n1\nThe original language of {subject} is\n{subject} a. r. rahman\nOriginal language of film/TV show\n2\nThe source language of {subject} is\n3\nThe default language of {subject} is\n1\nThe capital of {subject} is\n{subject} united states embassy in\nCapital\n2\nThe capital city of {subject} is\n{subject} representative legislature\n3\nIts capital {subject} is\n{subject} rock band from\n1\n{subject} is a native language of\n{subject} descent\nNative language\n2\nThe mother tongue of {subject} is\n{subject} speak the\n3\n{subject} means\n{subject} population or a widely spoken\n1\n{subject} is named after\n{subject} and produces\nNamed after\n2\n{subject} is named for\n{subject} variety of standard )\n3\n{subject} is called after\n{subject} official\n1\nThe official language {subject} is\n{subject} professor of\nOfficial language\n2\n{subject} is\n{subject} is the official language in\n3\n{subject} is officially\n{subject} is the official language spoken in\n1\n{subject} is developed by\n{subject} was developed by\nDeveloper\n2\n{subject} is created by\n{subject} 2008\n2\n{subject} is designed by\n{subject} references external links\n1\n{subject} was originally aired on\n{subject} premiered on\nOriginal broadcaster\n2\n{subject} was originally broadcast on\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of reliably estimating factual knowledge embedded in large language models (LLMs), highlighting the limitations of prior methods that rely on prompt engineering, which raises reliability concerns.",
        "problem": {
            "definition": "The problem defined is the challenge of estimating the extent of factual knowledge that LLMs possess about real-world entities, focusing on the reliability of these estimates.",
            "key obstacle": "The main challenge is the reliance on prompt engineering, which can lead to side-channels and overfitting, thereby compromising the reliability of knowledge estimates."
        },
        "idea": {
            "intuition": "The proposed idea stems from the observation that LLMs can leverage their in-context learning ability without the need for engineered prompts, thus simplifying the probing process.",
            "opinion": "The proposed method, Zero-Prompt Latent Knowledge Estimator (ZP-LKE), eliminates prompt tokens and instead uses many-shot examples of subject-object pairs to communicate the relationship.",
            "innovation": "ZP-LKE differs from existing methods by avoiding prompt engineering altogether, which enhances its reliability and applicability across various models."
        },
        "method": {
            "method name": "Zero-Prompt Latent Knowledge Estimator",
            "method abbreviation": "ZP-LKE",
            "method definition": "ZP-LKE is defined as a method that uses structured examples of subject-object pairs to probe LLMs for factual knowledge without relying on prompts.",
            "method description": "The core of ZP-LKE involves using many examples to communicate the factual relationship to the LLM.",
            "method steps": [
                "Collect a training dataset of facts related to a specific relation.",
                "Provide multiple examples of subject-object pairs sharing the same relation as input to the model.",
                "Allow the model to generate a response based on the input examples.",
                "Evaluate the model's output for correctness against the expected fact."
            ],
            "principle": "ZP-LKE is effective because it leverages the in-context learning capabilities of LLMs, allowing them to infer relationships without additional prompt instructions."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on 49 open-source LLMs across different families and sizes, using a dataset of 20,000 facts from the T-REx dataset.",
            "evaluation method": "The performance of ZP-LKE was evaluated using accuracy metrics based on both response generation and a multiple-choice testing framework."
        },
        "conclusion": "ZP-LKE demonstrates significant improvements in extracting factual knowledge from LLMs compared to traditional prompt-based approaches, highlighting its effectiveness and versatility across different models.",
        "discussion": {
            "advantage": "The key advantages of ZP-LKE include its model-agnostic design, improved reliability, and the ability to extract more factual knowledge than previous methods.",
            "limitation": "A limitation of ZP-LKE is its vulnerability to incorrect examples, which can adversely affect its performance.",
            "future work": "Future research should focus on refining the method to mitigate the impact of incorrect examples and explore additional ways to enhance knowledge extraction."
        },
        "other info": {
            "info1": "The dataset used for evaluation includes various relations from the T-REx dataset, ensuring a comprehensive assessment of knowledge extraction.",
            "info2": {
                "info2.1": "ZP-LKE improves factual knowledge extraction by an average of 35% for human-generated prompts and 90% for machine-mined prompts.",
                "info2.2": "The method is expected to aid developers in understanding and improving the factual knowledge embedded in LLMs."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of reliably estimating factual knowledge embedded in large language models (LLMs), highlighting the limitations of prior methods that rely on prompt engineering."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the in-context learning ability of LLMs without the need for engineered prompts."
        },
        {
            "section number": "3.1",
            "key information": "ZP-LKE is effective because it leverages the in-context learning capabilities of LLMs, allowing them to infer relationships without additional prompt instructions."
        },
        {
            "section number": "4.2",
            "key information": "The main challenge identified is the reliance on prompt engineering, which can lead to side-channels and overfitting, thereby compromising the reliability of knowledge estimates."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of ZP-LKE is its vulnerability to incorrect examples, which can adversely affect its performance."
        },
        {
            "section number": "7",
            "key information": "ZP-LKE demonstrates significant improvements in extracting factual knowledge from LLMs compared to traditional prompt-based approaches."
        }
    ],
    "similarity_score": 0.6964201286183805,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f99/6f99b9e3-fd6c-4945-8375-bf7a945da649.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d429/d429d5ed-f35b-4f1d-87ca-453a649ad3a6.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/660f/660fceef-41aa-4ab2-b090-5787ab7a5ea3.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/156d/156dd6fb-09fb-40d3-83d0-91f33dccd01c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f08/6f081d2e-3ed5-4e1e-b04a-0522b4c3ea77.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d09f/d09f130e-cbd2-4178-96fe-89719b712004.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f27/0f27e895-c3c1-4899-aa59-e5582857a844.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e52f/e52f6a43-84de-4d55-883a-da4a6f62e6a8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6336/63365ad4-1d6e-4cc1-8a28-b119a051e74c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/795f/795f380d-724a-4ba9-b347-f57a8560cb58.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c83b/c83b43b8-c6a4-47dc-9be0-da1bbd9e3063.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba0e/ba0e2f30-39df-4fd3-a9d1-e4fc223282db.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afeb/afeb33c6-9b09-4638-9890-0195d8ff74e8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ae0/1ae0105e-cee2-47af-8279-5f2e287b43fe.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/98a2/98a249cd-9bc1-4a42-bc86-caf1d1a9dccc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f44b/f44b4591-dcdc-429d-84f6-d9f93819fc88.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ee9/4ee96511-b162-41dd-ad77-539daab304c9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea7d/ea7df0e1-e823-4e35-ae13-4f6a66366baa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65b3/65b3ebc7-7c52-4a97-8439-f7269583e305.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d259/d259de70-745f-42d2-81b8-594cf4b8604e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/62d7/62d700ed-8b69-46d5-acd3-638305ecf007.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fb6d/fb6df3c5-65d7-47c4-98cc-b008f8766333.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5c2/e5c2601a-9b92-42e7-9597-ae984794ab0a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b5d9/b5d96264-dd1a-48ba-8d72-920aefc4ff69.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8723/87238b61-ef8e-4ce4-89a1-e3ab3f9f665a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ffe/3ffe842d-822c-4b60-8ed5-7ae2fca7e648.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b7e/9b7e2812-af0d-47ae-a57a-b172b83b8a6f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9de/e9de2a3b-e012-4d5e-aa18-d2337eca9a3f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c61/4c614667-27d0-45df-a5d1-09708b29b90a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/edd2/edd2ed12-45b3-4baf-b7a0-f7c8928341fc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a54d/a54d9b1b-d83a-4af9-b701-37685f0217bd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/39ce/39ce7012-638b-42aa-ad95-553a215bac4f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ebb/4ebb5b6f-ff72-45fc-9213-045b5f8a279f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9ec/b9ecbef9-8579-4ba6-82b0-6ebd110c6fa6.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e598/e598b0d6-6aa5-422b-a580-02bd36582d2a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a21/0a2139d6-bbe5-4f8e-9d11-b2a2496220e8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a446/a446221c-79ee-48ea-b93f-dbaa9184ea4b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7cc/b7cc2863-0d97-46d6-9125-94fe2bc42a17.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6b1c/6b1cdebe-0151-4081-9bef-b19d9604edb9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03fd/03fdfb7d-7697-478e-8b9c-88018aae8b25.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d14a/d14a903b-bc9f-491b-8897-1f66cd4a6750.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe00/fe0072e4-b6cf-4896-9aa0-21d9b24d0027.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/83b4/83b400ae-7d5d-4a4f-9c9d-f4301502a04d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b25/9b257655-2a3b-4d54-81a8-747dd1aca0f5.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Towards Reliable Latent Knowledge Estimation in LLMs_ In-Context Learning vs. Prompting Based Factual Knowledge Extraction.json"
}