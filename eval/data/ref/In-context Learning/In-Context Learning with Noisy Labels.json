{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.19581",
    "title": "In-Context Learning with Noisy Labels",
    "abstract": "In-context learning refers to the emerging ability of large language models (LLMs) to perform a target task without additional training, utilizing demonstrations of the task. Recent studies aim to enhance in-context learning performance by selecting more useful demonstrations. However, they overlook the presence of inevitable noisy labels in task demonstrations that arise during the labeling process in the real-world. In this paper, we propose a new task, in-context learning with noisy labels, which aims to solve real-world problems for in-context learning where labels in task demonstrations would be corrupted. Moreover, we propose a new method and baseline methods for the new task, inspired by studies in learning with noisy labels. Through experiments, we demonstrate that our proposed method can serve as a safeguard against performance degradation in in-context learning caused by noisy labels.",
    "bib_name": "kang2024incontextlearningnoisylabels",
    "md_text": "# In-Context Learning with Noisy Labels\n\n# Junyong Kang jykang@kaist.ac.kr KAIST Seoul, South Korea\nDo happ Seoul N Seou\n\n# Junyong Kang jykang@kaist.ac.kr KAIST Seoul, South Korea\n\nHwanjun Song songhwanjun@kaist.ac.kr KAIST Deajeon, South Korea\n\n# ABSTRACT\n\nIn-context learning refers to the emerging ability of large language models (LLMs) to perform a target task without additional training, utilizing demonstrations of the task. Recent studies aim to enhance in-context learning performance by selecting more useful demonstrations. However, they overlook the presence of inevitable noisy labels in task demonstrations that arise during the labeling process in the real-world. In this paper, we propose a new task, in-context learning with noisy labels, which aims to solve real-world problems for in-context learning where labels in task demonstrations would be corrupted. Moreover, we propose a new method and baseline methods for the new task, inspired by studies in learning with noisy labels. Through experiments, we demonstrate that our proposed method can serve as a safeguard against performance degradation in in-context learning caused by noisy labels.\n\n# CCS CONCEPTS\n\u2022 Computing methodologies \u2192 Natural language processing\n\n# \u2022 Computing methodologies \u2192 Natural language processing.\nKEYWORDS\n\nKEYWORDS\n\nin-context learning, learning with noisy labels, large language mo els\n\nACM Reference Format: Junyong Kang, Donghyun Son, Hwanjun Song, and Buru Chang \u2217. 2018. In-Context Learning with Noisy Labels. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym \u2019XX). ACM, New York, NY, USA, 5 pages. https://doi.org/ XXXXXXX.XXXXXXX\n\n# 1 INTRODUCTION\n\nHave you ever doubted whether the labels in your exemplars could be incorrect?\n\nThe corresponding author.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY \u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX\n\nDonghyun Son happydh1@snu.ac.kr Seoul National University Seoul, South Korea\n\nBuru Chang \u2217\nburu@sogang.ac.kr Sogang University Seoul, South Korea\n\n(a) Noisy labels included in demonstrations for in-context learning\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/58c5/58c56790-18a8-427b-8c61-dd4b4ad6b91c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Performance Degradation by noisy labels\n</div>\n<div style=\"text-align: center;\">(b) Performance Degradation by noisy labels\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c58/3c58eb17-3408-49fc-a914-f35f4462a0db.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">10% 20%30%40% 50% NoiseRate\n</div>\nFigure 1: (a) The Tweet dataset [3] contains noisy labels. Thus, the demonstrations collected from the dataset would include noisy labels. (b) These noisy labels degrade the performance of in-context learning and decrease the stability.\n\nIn-context learning [6, 27] is an emerging property of Large Language Models (LLMs) trained on vast amounts of training data. Without the need for additional fine-tuning on the target task, LLMs can perform the target task by taking task descriptions and a few demonstrations (exemplars) relevant to the task. Recently, many studies have been proposed to enhance the performance of in-context learning. These studies aim to select more suitable demonstrations from the predefined retrieval set for a given test data [13, 22, 28] or enable the utilization of more demonstrations by alleviating the limitation of input length in LLMs [7, 11]. However, these studies overlook the real-world scenarios that the retrieved demonstrations would include noisy labels. In real-world labeled datasets, such as hate speech detection datasets [3], corrupted labels often emerge due to the subjective\n\nnature of annotators and label ambiguity [20]. Given attempts to scale up the usage of demonstrations to the order of 1,000 [11], prompts unavoidably include noisy labels. In addition to previous works [19, 26], our experimental results in Figure 1 show that these noisy labels not only severely degrade the performance of in-context learning but also reduce the stability of the predictive results (See details in \u00a7 4. 4). This observation underscores the need to handle noisy labels, as LLMs actually learn tasks from exemplars through in-context learning [2, 25], and such labels can interfere with this learning process. To address this issue, we introduce a novel task, named  \"incontext learning with noisy labels,\" by establishing a connection between the existing\"learning with noisy labels\" problem [16, 20] and\"in-context learning\" problem in LLMs. The former problem aims to build a robust model from the corrupted dataset having noisy labels. The new task assumes that a certain proportion of labels in a retrieval set are corrupted and evaluates the performance of in-context learning accordingly. As the first comprehensive study on the new task, we present baseline methods for the new task. These baseline methods are adapted versions of representative methods proposed in existing research on learning with noisy labels [17, 29, 31], tailored to the in-context learning scenario. Furthermore, we propose a new rectifying method that is more universally applicable to the in-context learning with noisy label setting. Experiments on classification tasks demonstrate the importance of handling noisy labels in the in-context learning setting, and the proposed method shows its ability to serve as a safeguard against performance degradation caused by noisy labels.\n\n# 2 PROBLEM FORMULATION\n\nIn this section, we formally present a new task, denoted as  incontext learning with noisy labels. Firstly, we formulate the problem of in-context learning, then extend the problem to the new task to address the real-world scenarios where the demonstrations would include noisy labels.\n\nIn-context learning. In-context learning is a paradigm where the LLM performs a new task without additional training by leveraging its powerful knowledge. For this purpose, the LLM predicts the target label \ud835\udc66 along with the input query \ud835\udc65 and \ud835\udc5b-ordered demonstrations for the target task. Following the previous studies [22, 28], we retrieve the most relevant demonstrations [(\ud835\udc65 1,\ud835\udc66 1), \u00b7 \u00b7 \u00b7, (\ud835\udc65 \ud835\udc5b,\ud835\udc66 \ud835\udc5b)] from the pre-defined retrieval set based on the input query \ud835\udc65. Subsequently, the demonstrations are concatenated to the input query \ud835\udc65 as follows [\ud835\udc65 1,\ud835\udc66 1, \u00b7 \u00b7 \u00b7,\ud835\udc65 \ud835\udc5b,\ud835\udc66 \ud835\udc5b,\ud835\udc65], and then it is fed to the frozen LLM to compute the probability distribution. Note that we focus on text classification tasks where the target label \ud835\udc66 is in a set of \ud835\udc5a candidate labels {\ud835\udc50 1, \u00b7 \u00b7 \u00b7,\ud835\udc50 \ud835\udc5a} to simplify our problem. Thus, we decode the prediction \ud835\udc66 by comparing the negative log-likelihood of the candidate labels and choose the minimal one.\n\nIn-context learning with noisy labels. In real-world labeled datasets, labels would be corrupted due to the nature of labeling process [20]. Therefore, we present a new task, in-context learning with noisy labels, by extending the problem of in-context learning to consider the real-world scenarios.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb2b/cb2b4416-5cf0-4c8f-9b48-423c744e75ab.png\" style=\"width: 50%;\"></div>\n# Figure 2: An example of the prompt format of the rectification method. The examples are collected from SST-5.\n\nFigure 2: An example of the prompt format of the rectification method. The examples are collected from SST-5.\n\nWe assume that the pre-defined retrieval set D is corrupted by various noise rates \ud835\udc5f \ud835\udc56 \u2208[0, 1], yielding noisy retrieval set. Specifically, we select \ud835\udc5f \ud835\udc56 \u00d7 |D| demonstrations and flip each of label \ud835\udc66 to \u00af \ud835\udc66 \u2208{\ud835\udc50 1, ...,\ud835\udc50 \ud835\udc5a} \u2212{\ud835\udc66} with uniform transition probability, following the previous study [20]. As in traditional learning with noisy labels problem [12, 16, 24], we assume that a small size of clean dataset is accessible. Such a small-sized clean set can be constructed at a relatively low cost and is effective in significantly building a robust model to noisy labels. We sample a small clean subset D \u2032 and use it to make models more robust to label noise.\n\n# 3 METHOD\n\nWe present several methods to perform the new task making incontext learning more robust in noisy labels. In contrast to the learning with noisy labels problem, a significant challenge in the new task is that parameters of the LLM must be kept frozen instead of updating them to perform the task. Hence, we focus on manipulating the labels of task demonstrations.\n\n# 3.1 Baseline Methods\n\nWe present four baseline methods for standard classification tasks named correction, weighting, reordering, and selection. In learning with noisy labels problem, methods such as loss adjustment and sample selection [24] manipulate the effect of a noisy example by loss reweighting and sampling strategy based on its confidence. Motivated from these approaches, our baseline methods utilize a classifier (e.g. fine-tuned BERT) trained on each target dataset to give auxiliary information to LLM using its output probabilities.\n\nCorrection. is the simplest method which aims to directly correct noisy labels. This approach overwrites all demonstration labels with the output decision of the classifier.\n\nWeighting. puts additional information next to the each prompt label, showing its confidence value from the classifier. We verbalize\n\nthe confidence value as either\"high\" or\"low\" rather than the real number, as classifiers can produce extremely skewed scores [10].\n\nReordering. utilizes the confidence value to replace position of each demonstration, placing low-confidence demonstrations proceeding to high-confidence ones. LLMs puts different importance to each demonstrations depending on its position, typically higher weights to last sentences [18, 30]. Therefore, reordering can be interpreted as an implicit weighting by the order bias of LLMs.\n\nSelection.  also utilizes the confidence value to reduce the influences of noisy labels by discarding demonstrations with low confidence labels in the prompt. As an example, we take the threshold \ud835\udf03 = 0. 3 for the experiments.\n\n# 3.2 Rectification\n\nWe now propose our main method rectification, which processes multiple noisy demonstrations at a time. As shown in Figure 2, rectification receives a sequence of noisy demonstrations and outputs a sequence of corrected labels. To achieve this, we fine-tune a pre-trained generative model (e.g., GPT-2 [21]) using negative log likelihood loss on the label tokens. Unlike baseline methods that perform classification independently for each demonstration, rectification gets a sequence of demonstrations as an input. Hence, it can reference all retrieved demonstrations and utilize them for noisy label rectification. In \u00a7 4.4, we demonstrate the effectiveness of our design choice in improving model performance.\n\n# 4 EXPERIMENTS\n\nWe conduct experiments on three datasets: MRPC (paraphrase detection) [9], SST-5 (sentiment analysis) [23] and Tweet hate speech detection [4]. The statistics of the datasets are summarized in Table 2. We employ the training set to build the corrupted retrieval set D and sample a clean subset D \u2032 corresponding to 10% of the training set. Then, we evaluate the performance of our methods for in-context learning with noisy labels on the validation set.\n\n# 4.1 Implementation Details\n\nWe adopt multiple LLMs of varying sizes, including GPT2-Neo 2.7B [5], Llama2-7B [1], and Mistral-7B [15], as inference LLMs, and use 10 task demonstrations for both training and inference. For baseline methods, we fine-tune a BERT model [8] on the clean subset D \u2032, initialized with the bert-base-uncased checkpoint. We also employ the oracle classifier, which is trained on clean retrieval set D. These classification models are fine-tuned for 30 epochs with batch size of 64 and learning rate of 5e-5. For the rectification method, we fine-tune a pre-trained gpt2-large, one of pre-trained versions of GPT-2 [21]. To construct training dataset for the rectification method, we collect task demonstrations from the clean subset D \u2032 with the EPR retriever [22], and then add label noise to the demonstrations by sampling \ud835\udc5f \u2208{0. 1, 0. 2, 0. 3, 0. 4, 0. 5}. We trained the model for 10 epochs with a batch size of 2 and a learning rate of 1e-4, employing LoRA [14] for memory-efficient training. We split the training set for these methods to create a validation set.\n\n# 4.2 Main results\n\nTable 1 shows the performance of our baseline methods and the rectification method. We report the performance using the EPR retriever and the TopK-similarity retriever based on BERT embedding of demonstration inputs. In the GPT2-Neo results, without any label manipulation, we observe that the performance of incontext learning degrades as the noise rate increases. In contrast, the correction method acts as a robust baseline, maintaining consistent performance across all noise rates. The weighting method shows some improvement compared to no manipulation, but fails to achieve consistent performance as the noise rate increases and sometimes performs worse in the Tweet. The reordering and the selection method shows rapid performance degradation in MRPC due to the imperfect classifier. In SST-5, the selection is relatively steady, and the reordering retains high accuracy at lower noise rates. Although these methods successfully defends against performance drop in Tweet with the EPR retriever, we observe failures when using the TopK-BERT retriever. On the other hand, the rectification method shows superior performance over baseline methods, effectively defending the performance against noisy labels. For Llama2-7B and Mistral-7B, we report the performance of the correction and the rectification method. These LLMs show better noise robustness at lower noise rates, Llama2-7B on SST-5 or Mistral-7B on Tweet with the EPR retriever for instance. However, their robustness is not consistent across datasets or retrievers. For example, we found that the EPR mostly retrieves non-hate demonstrations for evaluation queries, which induce lower performance but higher robustness. On the other hand, the TopK-BERT retrieves more balanced demonstrations, resulting in a clear drop in accuracy as the noise rate increases. Regardless of these variations, the rectification method consistently maintains accuracy across noise rates and outperforms the correction method.\n\n# 4.3 Analysis\n\nStability. With our method, not only accuracy but also stability can be improved. In Figure 1, we visualize the in-context learning performance of the no manipulation version and the rectification method with 10 different random seeds. We also summarize the mean and standard deviation of performance for each dataset in Table 3, averaging the mean and the standard deviation across noise rates. To solely measure the variability due to each noise rate, we retrieve demonstrations from D and then randomly corrupt them by rate \ud835\udc5f \ud835\udc56 in this experiment. Overall, the rectification method shows smaller variance compared to no manipulation within each noise rate, which indicates that the stability is improved. Rectification Accuracy. The performance of in-context learning with noisy labels depends on how well it rectifies the noisy labels. To measure the rectification performance, we define the rectification accuracy as \ud835\udf0f = 1\n\ud835\udc41\ud835\udc3e \ufffd \ud835\udc41 \ud835\udc5b = 1 \ufffd \ud835\udc3e \ud835\udc58 = 1 1 (\ud835\udc66 \ud835\udc58\ud835\udc5b = \u02dc \ud835\udc66 \ud835\udc58\ud835\udc5b), where \ud835\udc41, \ud835\udc3e denote the number of sets of demonstrations and the number of demonstrations in a set, \ud835\udc66 \ud835\udc3e \ud835\udc5b and \u02dc \ud835\udc66 \ud835\udc58 \ud835\udc5b denote \ud835\udc58 \u2019th label and output of \ud835\udc5b \u2019th set, respectively. In Table 4, we compare rectification accuracy between classification models and our rectification method. Specifically, we adopt the BERT classifier from previous sections and a classifier trained from gpt2-large to match model capacity. The rectification\n\n<div style=\"text-align: center;\">Table 1: Experimental results on GPT2-Neo (2.7B), Llama2-7B, and Mistral-7B\n</div>\nModel\nRetriever\nMethod\nMRPC\nSST-5\nTweet\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nGPT2-Neo\nEPR\nw/o manipulation\n78.4\n76.0\n75.5\n67.9\n47.8\n51.5\n46.9\n44.1\n41.6\n38.5\n36.0\n31.8\n58.4\n59.7\n55.5\n54.7\n53.6\n47.4\nCorrection\n72.1\n38.9\n57.3\nWeighting\n78.2\n77.2\n74.8\n70.8\n53.4\n58.3\n48.5\n46.8\n44.5\n42.5\n39.2\n35.9\n58.1\n61.4\n57.2\n55.4\n54.0\n46.8\nReordering\n79.4\n68.1\n62.0\n49.0\n35.0\n36.8\n46.6\n46.3\n45.3\n42.5\n38.5\n35.9\n57.7\n58.3\n57.5\n58.7\n58.0\n55.4\nSelection\n56.4\n49.3\n43.6\n40.7\n35.0\n36.3\n45.6\n44.7\n44.4\n43.2\n43.2\n38.5\n57.3\n57.3\n57.3\n57.3\n57.3\n56.4\nRectification\n78.4\n77.7\n77.2\n77.2\n75.5\n76.2\n44.7\n44.8\n44.4\n45.3\n45.5\n44.5\n58.3\n58.3\n58.3\n58.4\n58.5\n58.7\nTopK-\nBERT\nw/o manipulation\n70.3\n65.4\n62.5\n61.0\n53.4\n52.5\n35.6\n33.6\n31.7\n33.0\n29.0\n26.4\n65.7\n63.3\n60.1\n58.8\n52.8\n50.3\nCorrection\n67.0\n33.8\n57.3\nWeighting\n71.6\n69.1\n67.2\n65.4\n56.4\n56.4\n36.9\n36.5\n35.5\n32.7\n33.9\n30.8\n65.3\n63.8\n59.6\n57.0\n50.2\n48.7\nReordering\n52.7\n46.6\n42.2\n40.2\n34.6\n34.3\n36.4\n36.1\n35.1\n33.8\n31.9\n29.1\n69.7\n66.1\n62.7\n60.4\n56.4\n58.5\nSelection\n44.6\n41.4\n40.0\n35.5\n33.8\n32.8\n34.3\n33.8\n34.9\n33.2\n32.4\n32.9\n61.5\n60.6\n59.9\n59.5\n56.3\n57.7\nRectification\n71.1\n71.1\n70.3\n71.3\n68.2\n67.4\n36.6\n36.8\n36.0\n35.6\n36.0\n36.2\n64.4\n64.2\n63.7\n63.1\n63.5\n64.0\nLlama2-7B\nEPR\nw/o manipulation\n77.7\n75.5\n76.5\n74.3\n62.3\n62.3\n50.2\n49.3\n51.0\n47.6\n45.3\n45.4\n59.5\n61.5\n58.6\n60.0\n57.5\n52.9\nCorrection\n72.5\n43.7\n57.3\nRectification\n78.2\n78.4\n78.4\n78.4\n76.7\n75.8\n50.3\n50.5\n50.3\n50.2\n50.6\n49.4\n59.9\n59.8\n59.9\n59.6\n60.3\n59.4\nTopK-\nBERT\nw/o manipulation\n70.3\n70.6\n69.1\n66.9\n59.1\n60.0\n49.8\n47.2\n49.0\n47.3\n43.5\n40.4\n71.4\n69.7\n67.5\n64.7\n56.9\n55.8\nCorrection\n69.6\n46.3\n57.4\nRectification\n73.5\n71.6\n72.3\n70.6\n71.3\n71.1\n51.2\n50.8\n51.1\n50.2\n50.6\n49.4\n71.7\n71.7\n71.3\n69.7\n71.3\n70.3\nMistral-7B\nEPR\nw/o manipulation\n77.7\n77.0\n76.5\n76.7\n61.3\n64.0\n51.9\n50.1\n48.5\n45.5\n44.3\n40.0\n66.3\n68.2\n63.3\n65.1\n61.8\n55.8\nCorrection\n74.5\n44.8\n57.3\nRectification\n78.4\n77.9\n77.9\n78.9\n77.0\n77.0\n49.4\n49.9\n49.5\n49.9\n49.8\n49.8\n66.0\n65.8\n65.8\n65.8\n65.8\n65.8\nTopK-\nBERT\nw/o manipulation\n72.3\n72.1\n70.3\n67.9\n62.3\n62.3\n47.7\n46.4\n44.8\n42.7\n40.3\n39.2\n74.6\n71.5\n67.5\n65.9\n57.5\n57.5\nCorrection\n71.6\n43.6\n57.2\nRectification\n73.0\n72.3\n72.5\n71.8\n71.8\n71.6\n49.0\n48.7\n49.7\n49.1\n50.2\n48.4\n72.1\n71.5\n71.4\n70.2\n70.3\n69.1\n<div style=\"text-align: center;\">Table 2: Dataset statistics and input formats.\n</div>\nDataset\n# of train data\n# of validation data\nFormat\nMRPC\n3,668\n408\n{sentence1} Can we say\n\"{sentence2}\"? {No, Yes}\nSST-5\n8,534\n1,101\n{question} It is {terrible,\nbad,OK,good,great}\nTweet\n9,000\n1,000\nTweet: {question}\nHate: {No, Yes}\nTable 3: Experimental results on stability. For MRPC and SST5, we used the EPR retriever and the TopK-BERT for Tweet.\n\n<div style=\"text-align: center;\">Table 3: Experimental results on stability. For MRPC and SST5, we used the EPR retriever and the TopK-BERT for Tweet.\n</div>\nModel\nRectification\nMRPC\nSST-5\nTweet\nAcc.\nStd.\nAcc.\nStd.\nAcc.\nStd.\nGPT2-Neo\nNo manipulation\n65.23\n1.98\n37.56\n1.24\n57.50\n1.28\nRectification\n76.02\n0.83\n44.39\n0.53\n63.79\n0.72\nLlama2-7B\nNo manipulation\n71.99\n1.35\n47.21\n0.99\n63.71\n1.01\nRectification\n77.15\n0.80\n49.49\n0.64\n70.21\n0.65\nmethod achieves higher rectification accuracy compared to classifiers across all datasets, showing that it can effectively leverages the context of demonstrations in correcting labels. Data efficiency. As observed in previous sections, the rectification method maintains the performance using only a small portion of the data. To explore the data efficiency of this method, we train the oracle model (the rectification method trained on the full dataset D), which we denote as Full. As shown in Table 5, our method\n\nTable 4: The rectification accuracy comparison. (\u2217) symbol indicates the oracle classifier trained on the full training data.\n\nRectification Method\nMRPC\nSST-5\nTweet\nBERT Classifier\u2217\n97.5\n59.8\n91.5\nBERT Classifier\n81.9\n49.6\n91.6\nGPT-2 Classifier\u2217\n80.9\n49.2\n87.5\nGPT-2 Classifier\n72.1\n42.3\n68.4\nRectification (\ud835\udc5f=0.1)\n90.3\n67.3\n95.1\nRectification (\ud835\udc5f=0.2)\n88.6\n66.3\n95.1\nRectification (\ud835\udc5f=0.3)\n86.1\n65.2\n94.8\nRectification (\ud835\udc5f=0.4)\n83.5\n64.0\n94.6\nRectification (\ud835\udc5f=0.5)\n83.8\n62.4\n94.1\nTable 5: Experimental results with GPT2-Neo to investigate data efficiency. The results on MRPC with the EPR retriever are reported.\n\nTraining Method\nNoise rate\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nFull\n77.7\n78.4\n77.5\n76.7\n76.7\n77.0\n2-shot\n73.5\n73.8\n73.2\n73.0\n73.5\n72.1\n5-shot\n73.3\n73.8\n73.3\n71.3\n70.4\n70.1\nOurs\n78.4\n77.7\n77.2\n77.2\n75.5\n76.2\nformance to Full, though it only uses 10%\n\nreaches comparable performance to Full, though it only uses 10% of the total data.\n\nWhy is the rectification method data-efficient? To investigate this question, we train the rectification method with fewer input demonstrations, specifically 2 and 5. During the inference, these models are executed multiple times (e.g. 5 and 2) to rectify total 10 demonstrations. Table 5 shows that these method are less effective than our approach, where 10 demonstrations are given as an input. From this observation, we conclude that the rectification method benefits from referring to the given context (demonstrations), which we believe to be crucial for the data efficiency.\n\n# 5 CONCLUSION\n\nIn this study, we propose a new task called in-context learning with noisy labels. Additionally, we introduce baseline methods capable of performing the new task and propose a novel method to address their limitations. Given the recent surge in attempts to generate labeled data using large language models, noisy labels are inevitable. Our research highlight a new research direction that must be addressed in the era of LLM.\n\n# REFERENCES\n\n[1] 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. ArXiv abs/2307.09288 (2023).\n[2] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. In The Eleventh International Conference on Learning Representations.\n[3] Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, and Vasudeva Varma. 2017. Deep learning for hate speech detection in tweets. In Proceedings of the 26th international conference on World Wide Web companion. 759\u2013760.\n[4] Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. 2019. SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter. In Proceedings of the 13th International Workshop on Semantic Evaluation. Association for Computational Linguistics, Minneapolis, Minnesota, USA, 54\u201363. https://doi.org/10.18653/v1/S19-2007\n[5] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021.  GPTNeo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. https: //doi.org/10.5281/zenodo.5297715\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.\n[7]  Tianle Cai, Kaixuan Huang, Jason D Lee, and Mengdi Wang. 2023. Scaling InContext Demonstrations with Structured Attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics.\n[9] William B. Dolan and Chris Brockett. 2005. Automatically Constructing a Corpus of Sentential Paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). https://aclanthology.org/I05-5002\n[10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICML\u201917). JMLR.org, 1321\u20131330.\n[11] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713 (2022).\n[12] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. 2018. Using trusted data to train deep networks on labels corrupted by severe noise. Advances in neural information processing systems 31 (2018).\n[13] SU Hongjin, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective Annotation Makes Language Models Better Few-Shot Learners. In The Eleventh International Conference on Learning Representations.\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https: //openreview.net/forum?id=nZeVKeeFYf9\n[15]  Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n\nGuillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023).\n[16]  Seong Min Kye, Kwanghee Choi, Joonyoung Yi, and Buru Chang. 2022. Learning with noisy labels by efficient transition matrix estimation to combat label miscorrection. In European Conference on Computer Vision. Springer, 717\u2013738.\n[17]  Tongliang Liu and Dacheng Tao. 2015. Classification with noisy labels by importance reweighting. IEEE Transactions on pattern analysis and machine intelligence 38, 3 (2015), 447\u2013461.\n[18] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 8086\u20138098. https://doi.org/10.18653/v1/2022.acllong.556\n[19] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? 11048\u201311064. https://doi.org/10.18653/ v1/2022.emnlp-main.759\n[20] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. 2013. Learning with noisy labels. Advances in neural information processing systems 26 (2013).\n[21] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.\n[22] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve Prompts for In-Context Learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2655\u20132671.\n[23] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In  Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (Eds.). Association for Computational Linguistics, Seattle, Washington, USA, 1631\u20131642. https://aclanthology.org/D13-1170\n[24] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. 2023. Learning From Noisy Labels With Deep Neural Networks: A Survey. IEEE Transactions on Neural Networks and Learning Systems 34, 11 (2023), 8135\u20138153. https://doi.org/10.1109/TNNLS.2022.3152527\n[25] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. PMLR, 35151\u201335174.\n[26] Xindi Wang, Yufei Wang, Can Xu, Xiubo Geng, Bowen Zhang, Chongyang Tao, Frank Rudzicz, Robert E Mercer, and Daxin Jiang. 2023. Investigating the learning behaviour of in-context learning: a comparison with supervised learning. arXiv preprint arXiv:2307.15411 (2023).\n[27] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An Explanation of In-context Learning as Implicit Bayesian Inference. In International Conference on Learning Representations.\n[28] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional Exemplars for In-context Learning. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 39818\u201339833. https: //proceedings.mlr.press/v202/ye23c.html\n[29] Kun Yi and Jianxin Wu. 2019. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 7017\u20137025.\n[30] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate Before Use: Improving Few-shot Performance of Language Models. In Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 12697\u2013 12706. https://proceedings.mlr.press/v139/zhao21c.html\n[31] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Dumais. 2021. Meta label correction for noisy label learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 11053\u201311061.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning refers to the emerging ability of large language models (LLMs) to perform a target task without additional training, utilizing demonstrations of the task. However, previous studies overlook the presence of inevitable noisy labels in task demonstrations, which can severely degrade performance. This paper introduces a new task, in-context learning with noisy labels, to address this issue and proposes methods to mitigate the impact of noisy labels on performance.",
        "problem": {
            "definition": "The problem addressed is the degradation of in-context learning performance due to the presence of noisy labels in task demonstrations, which arise from subjective labeling processes in real-world datasets.",
            "key obstacle": "The main obstacle is the challenge of effectively managing and rectifying noisy labels without the ability to update the parameters of the large language model, which must remain frozen."
        },
        "idea": {
            "intuition": "The idea is inspired by the recognition that noisy labels are a common issue in real-world datasets, and that existing methods do not adequately address this challenge in the context of in-context learning.",
            "opinion": "The proposed idea involves developing a new task framework for in-context learning that specifically accounts for noisy labels, enabling LLMs to learn more robustly from corrupted demonstrations.",
            "innovation": "The key innovation lies in the introduction of rectification methods that process multiple noisy demonstrations simultaneously, allowing for improved label correction compared to existing individual correction methods."
        },
        "method": {
            "method name": "Rectification",
            "method abbreviation": "Rect",
            "method definition": "The rectification method processes a sequence of noisy demonstrations and outputs corrected labels, leveraging the context provided by all demonstrations to improve accuracy.",
            "method description": "The core of the method involves fine-tuning a generative model to rectify noisy labels based on the context of multiple demonstrations.",
            "method steps": [
                "Collect a set of noisy demonstrations.",
                "Feed the sequence of demonstrations into the generative model.",
                "Output a sequence of corrected labels based on the model's predictions."
            ],
            "principle": "This method is effective because it utilizes the collective information from multiple demonstrations to inform the correction process, thus reducing the impact of individual noisy labels."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three datasets: MRPC (paraphrase detection), SST-5 (sentiment analysis), and Tweet hate speech detection, with a focus on assessing performance under varying noise rates.",
            "evaluation method": "The performance of the proposed rectification method was compared against baseline methods by measuring accuracy across different noise rates, utilizing a validation set for assessment."
        },
        "conclusion": "The proposed methods demonstrate that addressing noisy labels in in-context learning is crucial for maintaining performance. The rectification method shows superior performance compared to baseline approaches, highlighting the importance of this research direction in the context of large language models.",
        "discussion": {
            "advantage": "The proposed rectification method consistently outperforms baseline methods in terms of accuracy and stability, making it a robust solution for in-context learning with noisy labels.",
            "limitation": "One limitation is that the method relies on the availability of a small clean dataset to aid in the rectification process, which may not always be feasible in practice.",
            "future work": "Future research could explore further refinements to the rectification algorithm, investigate additional strategies for noise management, and extend the approach to other tasks and datasets."
        },
        "other info": {
            "info1": "The paper includes experiments that validate the effectiveness of the proposed methods.",
            "info2": {
                "info2.1": "The rectification method is particularly effective in maintaining performance across various noise rates.",
                "info2.2": "The research underscores the need for robust methodologies in the era of increasing reliance on large language models for real-world applications."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning refers to the emerging ability of large language models (LLMs) to perform a target task without additional training, utilizing demonstrations of the task."
        },
        {
            "section number": "1.2",
            "key information": "The degradation of in-context learning performance due to the presence of noisy labels in task demonstrations is a significant issue, highlighting the relevance of in-context learning within NLP."
        },
        {
            "section number": "3.1",
            "key information": "The main obstacle is the challenge of effectively managing and rectifying noisy labels without the ability to update the parameters of the large language model, which must remain frozen."
        },
        {
            "section number": "3.4",
            "key information": "The rectification method processes a sequence of noisy demonstrations and outputs corrected labels, leveraging the context provided by all demonstrations to improve accuracy."
        },
        {
            "section number": "4.1",
            "key information": "The proposed rectification method consistently outperforms baseline methods in terms of accuracy and stability, making it a robust solution for in-context learning with noisy labels."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the proposed method is that it relies on the availability of a small clean dataset to aid in the rectification process, which may not always be feasible in practice."
        },
        {
            "section number": "7",
            "key information": "The research underscores the need for robust methodologies in the era of increasing reliance on large language models for real-world applications."
        }
    ],
    "similarity_score": 0.7020137534163129,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning with Noisy Labels.json"
}