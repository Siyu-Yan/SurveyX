{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2205.09310",
    "title": "Mitigating Neural Network Overconfidence with Logit Normalization",
    "abstract": "Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)\u2014a simple fix to the cross-entropy loss\u2014by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output\u2019s norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30% on common benchmarks.",
    "bib_name": "wei2022mitigatingneuralnetworkoverconfidence",
    "md_text": "# Mitigating Neural Network Overconfidence with Logit Normalization\nHongxin Wei 1 Renchunzi Xie 1 Hao Cheng 1 2 Lei Feng 3 Bo An 1 Yixuan Li 4\n# Abstract\nDetecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)\u2014a simple fix to the cross-entropy loss\u2014by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output\u2019s norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30% on common benchmarks.\narXiv:2205.09310v2\n# 1. Introduction\nModern neural networks deployed in the open world often struggle with out-of-distribution (OOD) inputs\u2014samples from a different distribution that the network has not been exposed to during training, and therefore should not be predicted with high confidence at test time. A reliable classifier should not only accurately classify known in-distribution (ID) samples, but also identify as \u201cunknown\u201d any OOD input. This gives rise to the importance of OOD detection, which determines whether an input is ID or OOD and allows the model to take precautions in deployment.\n1Nanyang Technological University, Singapore 2Nanjing University, Nanjing, Jiangsu, China 3Chongqing University, Chongqing, China 4University of Wisconsin-Madison, Wisconsin, United States. Correspondence to: Renchunzi Xie <XIER0002@e.ntu.edu.sg>.\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\nA naive solution uses the maximum softmax probability (MSP)\u2014also known as the softmax confidence score\u2014for OOD detection (Hendrycks & Gimpel, 2016). The operating hypothesis is that OOD data should trigger relatively lower softmax confidence than that of ID data. Whilst intuitive, the reality shows a non-trivial dilemma. In particular, deep neural networks can easily produce overconfident predictions, i.e., abnormally high softmax confidences, even when the inputs are far away from the training data (Nguyen et al., 2015). This has cast significant doubt on using softmax confidence for OOD detection. Indeed, many prior works turned to define alternative OOD scoring functions (Liang et al., 2018; Lee et al., 2018; Liu et al., 2020; Sastry & Oore, 2020; Sun et al., 2021; Huang et al., 2021; Sun et al., 2022). Yet to date, the community still has a limited understanding of the fundamental cause and mitigation of the overconfidence issue. In this work, we show that the overconfidence issue can be mitigated through a simple fix to the cross-entropy loss\u2014the most commonly used training objective for classification\u2014 by enforcing a constant norm on the logit vector (i.e., presoftmax output). Our method, Logit Normalization (dubbed LogitNorm), is motivated by our analysis on the norm of the neural network\u2019s logit vectors. We find that even when most training examples are classified to their correct labels, the softmax cross-entropy loss can continue to increase the magnitude of the logit vectors. The growing magnitude during training thus leads to the overconfidence issue, despite having no improvement on the classification accuracy. To mitigate the issue, our key idea behind LogitNorm is to decouple the influence of output\u2019s norm from the training objective and its optimization. This can be achieved by normalizing the logit vector to have a constant norm during training. In effect, our LogitNorm loss encourages the direction of the logit output to be consistent with the corresponding one-hot label, without exacerbating the magnitude of the output. Trained with normalized outputs, the network tends to give conservative predictions and results in strong separability of softmax confidence scores between ID and OOD inputs (see Figure 4). Extensive experiments demonstrate the superiority of LogitNorm over existing methods for OOD detection. First, our method drastically improves the OOD detection perfor-\nmance using the softmax confidence score. For example, using CIFAR-10 dataset as ID and SVHN as OOD data, our approach reduces the FPR95 from 50.33% to 8.03%\u2014 a 42.30% of improvement over the baseline (Hendrycks & Gimpel, 2016). Averaged over a diverse collection of OOD datasets, our method reduces the FPR95 by 33.87% compared to using the softmax score with cross-entropy loss. Beyond MSP, we show that our method not only outperforms, but also boosts more advanced post-hoc OOD scoring functions, such as ODIN (Liang et al., 2018), energy score (Liu et al., 2020), and GradNorm score (Huang et al., 2021). In addition to the OOD detection task, our method improves the calibration performance on the ID data itself by way of post-hoc temperature scaling. Overall, using LogitNorm loss achieves strong performance on OOD detection and calibration tasks while maintaining the classification accuracy on ID data. Our method can be easily adopted in practice. It is straightforward to implement with existing deep learning frameworks, and does not require sophisticated changes to the loss or training scheme. Code and data are publicly available at https: //github.com/hongxin001/logitnorm_ood.\nwhere L is the commonly used cross-entropy loss with the softmax activation function:\nWe summarize our contributions as follows:\n1. We introduce LogitNorm \u2013 a simple and effective alternative to the cross-entropy loss, which decouples the influence of logits\u2019 norm from the training procedure. We show that LogitNorm can effectively generalize to different network architectures and boost different post-hoc OOD detection methods. 2. We conduct extensive evaluations to show that LogitNorm can improve both OOD detection and confidence calibration while maintaining the classification accuracy on ID data. Compared with the cross-entropy loss, LogitNorm achieves an FPR95 reduction of 33.87% on common benchmarks with the softmax confidence score. 3. We perform ablation studies that lead to improved understandings of our method. In particular, we contrast with alternative methods (e.g., GODIN (Hsu et al., 2020), Logit Penalty) and demonstrate the advantages of LogitNorm. We hope that our insights inspire future research to further explore loss function design for OOD detection.\n# 2. Background\n# 2.1. Preliminaries: Out-of-distribution Detection\nSetup. We consider a supervised multi-class classification problem. We denote by X the input space and Y = {1, . . . , k} the label space with k classes. The training dataset Dtrain = {xi, yi}N i=1 consists of N data points, sam-\npled i.i.d. from a joint data distribution PXY. We use Pin to denote the marginal distribution over X, which represents the in-distribution (ID). Given the training dataset, we learn a classifier f : X \u2192Rk with trainable parameter \u03b8 \u2208Rp, which maps an input to the output space. An ideal classifier can be obtained by minimizing the following expected risk:\n\ufffd Here, fy(x; \u03b8) denotes the y-th element of f(x; \u03b8) corresponding to the ground-truth label y, and p(y|x) is the corresponding softmax probability.\nProblem statement. During the deployment time, it is ideal that the test data are drawn from the same distribution Pin as the training data. However, in reality, inputs from unknown distributions can arise, whose label set may have no intersection with Y. Such inputs are termed out-of-distribution (OOD) data and should not be predicted by the model.\nThe OOD detection task can be formulated as a binary classification problem: determining whether an input x \u2208X is from Pin (ID) or not (OOD). OOD detection can be performed by a level-set estimation:\n(1)\nwhere S(x) denotes a scoring function and \u03b3 is commonly chosen so that a high fraction (e.g., 95%) of ID data is correctly classified. By convention, samples with higher scores S(x) are classified as ID and vice versa. In Section 4.2, we will consider a variety of popular OOD scoring functions including MSP (Hendrycks & Gimpel, 2016), ODIN (Liang et al., 2018), energy score (Liu et al., 2020) and GradNorm (Huang et al., 2021).\n# 3. Method: Logit Normalization\n3.1. Motivation\nIn the following, we investigate why neural networks trained with the common softmax cross-entropy loss tend to give overconfident predictions. Our analysis suggests that the large magnitude of neural network output can be a culprit.\nFor notation shorthand, we use f to denote the network output f(x; \u03b8) for an input x. f(x; \u03b8) is also known as the logit, or pre-softmax output. Without loss of generality, the logit vector f can be decomposed into two components:\nf = \u2225f\u2225\u00b7 \u02c6 f,\nLCE(f(x; \u03b8), y) = \u2212log p(y|x) = \u2212log e\u2225f\u2225\u00b7 \u02c6 fy \ufffdk i=1 e\u2225f\u2225\u00b7 \u02c6 fi .\nLCE(f(x; \u03b8), y) = \u2212log p(y|x) = \u2212log e\u2225\u2225\u00b7 \ufffdk  e\u2225f\u2225\u00b7 \u02c6 fi .\n\ufffd We can find that the training loss depends on the magnitude \u2225f\u2225and the direction \u02c6 f. By keeping the direction unchanged, we analyze the influence of the magnitude \u2225f\u2225 on the training loss. When y = arg maxi(fi), we can see that increasing \u2225f\u2225would increase p(y|x). It implies that, for those training examples that are already classified correctly, the optimization on the training loss would further increase the magnitude \u2225f\u2225of the network output to produce a higher softmax confidence score, thus obtaining a smaller loss.\nTo provide a straightforward view, we show in Figure 1 the dynamics of logit norm during training. Indeed, softmax cross-entropy loss encourages the model to produce logits with increasingly larger norms for both ID and OOD examples. The large norms directly translate into overconfident softmax scores, leading to difficulty in separating ID vs. OOD data. We proceed by introducing our method, targeting this problem.\n# 3.2. Method\nIn our previous analysis, we show that the softmax crossentropy loss encourages the network to produce logits with larger magnitudes, leading to the overconfidence issue that makes it difficult to distinguish ID and OOD examples. To\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/359a/359a07c2-f217-4f98-9069-0970ea02488a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. The mean magnitudes of logits under different training epochs. Model is trained on CIFAR-10 with WRN-402 (Zagoruyko & Komodakis, 2016). OOD examples are from SVHN dataset.</div>\nalleviate this issue, our key idea is to decouple the influence of logits\u2019 magnitude from network optimization. In other words, our goal is to keep the L2 vector norm of logits a constant during the training. Formally, the objective can be formulated as:\nPerforming constrained optimization in the context of modern neural networks is non-trivial. As we will show later in Section 5, simply adding the constraint via Lagrange multiplier (Forst & Hoffmann, 2010) does not work well. To circumvent the issue, we convert the objective into an alternative loss function that can be end-to-end trainable, which strictly enforces a constant vector norm.\nLogit Normalization. We employ Logit Normalization (dubbed LogitNorm), which encourages the direction of the logit to be consistent with the corresponding one-hot label, without optimizing the magnitude of the logit. In particular, the logit vector is normalized to be a unit vector with a constant magnitude. The softmax cross-entropy loss is then applied on the normalized logit vector instead of the original output. Formally, the objective function of LogitNorm is given by:\n(3)\n\ufffd \ufffd \ufffd\ufffd where \u02c6f(x; \u03b8) = f(x; \u03b8)/\u2225f(x; \u03b8)\u2225is the normalized logit vector. In practice, a small positive value (e.g., 10\u22127) is added to the denominator to ensure numerical stability. Equivalently, the new loss function can be defined as:\n(4)\n\ufffd where the temperature parameter \u03c4 modulates the magnitude of the logits. Interestingly, our loss function can be viewed\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31d8/31d886ba-7385-484c-b8ec-88551b071e4f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">a) Cross-entropy Loss (b) LogitNorm Loss (ours)</div>\n<div style=\"text-align: center;\">(b) LogitNorm Loss (ours)</div>\nFigure 2. The softmax outputs of two examples on a CIFAR-10 pre-trained WRN-40-2 (Zagoruyko & Komodakis, 2016) with (a) cross-entropy loss and (b) logit normalization loss. For the crossentropy loss, the softmax confidence scores are 1.0 and 1.0 for the ID and OOD examples. In contrast, the softmax confidence scores of the network trained with LogitNorm loss are 0.66 and 0.14 for the ID and OOD examples. While using cross-entropy loss produces an extremely confident prediction for the OOD example, our method produces almost uniform softmax probabilities (near 0.1), which benefits OOD detection.\nas having an input-dependent temperature, \u03c4\u2225f(x; \u03b8)\u2225, which depends on the input x.\nBy way of logit normalization, the magnitudes of the output vectors are strictly constant (i.e., 1/\u03c4). Minimizing the loss in Eq. (4) can only be achieved by adjusting the direction of the logit output f. The resulting model tends to give conservative predictions, especially for inputs that are far away from Pin. We illustrate with an example in Figure 2, where training with logit normalization leads to softmax outputs that are more distinguishable between inand out-of-distribution samples (right), as opposed to using cross-entropy loss (left). In Figure 3, we present a t-SNE visualization of the softmax outputs using Cross-entropy vs. LogitNorm Loss, where LogitNorm leads to more meaningful information to differentiate ID and OOD samples in the softmax output space. Below we further provide a lower bound of this new loss function in Eq. (4).\nProposition 3.3 (Lower Bound of Loss). For any input x and any positive number \u03c4 \u2208R+, the per-sample loss defined in Eq. (4) has a lower bound: Llogit norm \u2265 log \ufffd 1 + (k \u22121)e\u22122/\u03c4\ufffd , where k is the number of classes.\nThe proof of Proposition 3.3 is provided in Appendix C. From Proposition 3.3, we find that the LogitNorm loss has a lower bound that depends on \u03c4 and number of classes k. In particular, it implies that the lower bound of the loss value increases with the value of \u03c4. For example, when k = 10 and \u03c4 = 1, the norm of logits would be linearly scaled to 1 and the lower bound is about 0.7966. The high lower bound can cause optimization difficulty. For this reason, we found it is desirable to have a relatively small \u03c4 < 1. We will analyze the effect of \u03c4 in detail in Section 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78cb/78cb11da-3872-493c-a23e-1c6b6248fc7d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Cross-entropy Loss (b) LogitNorm Loss (ours)</div>\nFigure 3. t-SNE visualization (Van der Maaten & Hinton, 2008) of the softmax outputs from WRN-40-2 (Zagoruyko & Komodakis, 2016) trained on CIFAR-10 with (a) cross-entropy loss and (b) LogitNorm loss. All colors except for brown indicate 10 different ID classes. Points in brown denote out-of-distribution examples from SVHN. Trained with logit normalization, the softmax outputs provide more meaningful information to differentiate in- and outdistribution samples.\n# 4. Experiments\nIn this section, we verify the effectiveness of LogitNorm loss in OOD detection with several benchmark datasets.\n# 4.1. Experimental Setup\nIn-distribution datasets. In this work, we use the CIFAR10 and CIFAR-100 (Krizhevsky et al., 2009) datasets as in-distribution datasets, which are common benchmarks for OOD detection. Specifically, we use the standard split with 50,000 training images and 10,000 test images. All the images are of size 32 \u00d7 32.\nOut-of-distribution datasets. For the OOD detection evaluation, we use six common benchmarks as OOD test datasets Dtest out : Textures (Cimpoi et al., 2014), SVHN (Netzer et al., 2011), Places365 (Zhou et al., 2017), LSUN-Crop (Yu et al., 2015), LSUN-Resize (Yu et al., 2015), and iSUN (Xu et al., 2015). For all test datasets, the images are of size 32 \u00d7 32. The detail information of the six datasets is presented in Appendix D.\nEvaluation metrics. We evaluate the performance of OOD detection by measuring the following metrics: (1) the false positive rate (FPR95) of OOD examples when the true positive rate of in-distribution examples is 95%; (2) the area under the receiver operating characteristic curve (AUROC); and (3) the area under the precision-call curve (AUPR).\nTraining details. For main results, we perform training with WRN-40-2 (Zagoruyko & Komodakis, 2016) on CIFAR-10/100. The network is trained for 200 epochs using SGD with a momentum of 0.9, a weight decay of 0.0005, a dropout rate of 0.3, and a batch size of 128. We set the initial learning rate as 0.1 and reduce it by a factor of 10 at 80 and 140 epochs. The hyperparameter \u03c4 is selected from the range {0.001, 0.005, 0.01, . . . , 0.05}. We set 0.04 for CIFAR-10 by default. For hyperparameter tuning, we use\n<div style=\"text-align: center;\">detection performance comparison using softmax cross-entropy loss and LogitNorm loss. We use WRN-40-2 (Zagoruyko  2016) to train on the in-distribution datasets and use softmax confidence score as the scoring function. All values are indicates larger values are better, and \u2193indicates smaller values are better. Bold numbers are superior results.</div>\nTable 1. OOD detection performance comparison using softmax cross-entropy loss and LogitNorm loss. We use WRN-40-2 (Zagoruyko\n& Komodakis, 2016) to train on the in-distribution datasets and use softmax confidence score as the scoring function. All values are\npercentages. \u2191indicates larger values are better, and \u2193indicates smaller values are better. Bold numbers are superior results.\nID datasets\nCIFAR-10\nCIFAR-100\nOOD datasets\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nCross-entropy loss / LogitNorm loss (ours)\nTexture\n64.13 / 28.64\n86.29 / 94.28\n96.28 / 98.63\n84.11 / 70.67\n74.05 / 78.65\n93.45 / 93.66\nSVHN\n50.33 / 8.03\n93.48 / 98.47\n98.70 / 99.68\n79.09 / 45.98\n78.62 / 92.48\n94.99 / 98.45\nLSUN-C\n33.34 / 2.37\n95.29 / 99.42\n99.04 / 99.88\n67.94 / 13.93\n83.60 / 97.56\n96.32 / 99.48\nLSUN-R\n42.52 / 10.93\n93.74 / 97.87\n98.66 / 99.59\n82.21 / 68.68\n69.45 / 84.77\n91.45 / 96.52\niSUN\n46.56 / 12.28\n93.13 / 97.73\n98.55 / 99.56\n84.50 / 71.47\n69.29 / 83.79\n91.44 / 96.24\nPlaces365\n60.23 / 31.64\n87.36 / 93.66\n96.62 / 98.49\n81.09 / 80.20\n75.61 / 77.14\n93.74 / 94.16\naverage\n49.52 / 15.65\n91.55 / 96.91\n97.98 / 99.31\n79.82 / 58.49\n75.10 / 85.73\n93.57 / 96.42\n\u2018\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d207/d2073272-a1fc-456d-9ccb-bc58209b4966.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Cross-entropy Loss</div>\nFigure 4. Distribution of softmax confidence score from WRN-40-2 (Zagoruyko & Komodakis, 2016) trained on CIFAR-10 with (a) cross-entropy loss and (b) LogitNorm loss.\nGaussian noises as the validation set. All experiments are repeated five times with different seeds, and we report the average performance. We conduct all the experiments on NVIDIA GeForce RTX 3090 and implement all methods with default parameters using PyTorch (Paszke et al., 2019).\nHow does logit normalization influence OOD detection performance? In Table 1, we compare the OOD detection performance on models trained with cross-entropy loss and LogitNorm loss respectively. To isolate the effect of the loss function in training, we keep the test-time OOD scoring function to be the same, i.e., softmax confidence score:\n\ufffd A salient observation is that our method drastically improves the OOD detection performance by employing LogitNorm loss. For example, on the CIFAR-10 model, when evaluated against SVHN as OOD data, our method reduces the FPR95 from 50.33% to 8.03%\u2014a 42.3% of direct improvement. Averaged across six test datasets, our approach reduces the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f346/f346629b-20af-4e41-8c08-dbff898ced51.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) LogitNorm Loss (Ours)</div>\nFPR95 by 33.87% compared with using MSP on the model trained with cross-entropy loss. On CIFAR-100, our method also improves the performance by a meaningful margin. To further illustrate the difference between the two losses on OOD detection, we visualize and compare the distribution of softmax confidence score for ID and OOD data, derived from networks trained with cross-entropy vs. LogitNorm losses. With cross-entropy loss, the softmax scores for both ID and OOD data concentrate on high values, as shown in Figure 4a. In contrast, the network trained with LogitNorm loss produces highly distinguishable scores between ID and OOD data. From Figure 4b, we observe that the softmax confidence score for most OOD examples is around 0.1, which indicates that the softmax outputs are close to a uniform distribution. Overall the experiments show that training with LogitNorm loss makes the softmax scores more distinguishable between in- and out-of-distributions and consequently enables more effective OOD detection.\n<div style=\"text-align: center;\">Table 2. OOD detection performance comparison with various scoring functions using softmax cross-entropy loss and logit normalization loss. We use WRN-40-2 to train on the in-distribution datasets. All values are percentages. \u2191indicates larger values are better, and \u2193 indicates smaller values are better. Bold numbers are superior results.</div>\nindicates smaller values are better. Bold numbers are superior results.\nID datasets\nCIFAR-10\nCIFAR-100\nScore\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nCross-entropy loss / LogitNorm loss (ours)\nSoftmax\n49.52 / 15.65\n91.55 / 96.91\n97.98 / 99.31\n79.82 / 58.35\n75.10 / 85.75\n93.57 / 96.44\nODIN\n40.32 / 12.95\n87.21 / 97.37\n96.24 / 99.40\n70.71 / 58.13\n81.38 / 85.65\n95.37 / 96.36\nEnergy\n26.82 / 19.14\n93.07 / 96.09\n98.02 / 99.14\n70.87 / 65.46\n81.45 / 84.84\n95.38 / 96.27\nGradNorm\n58.98 / 17.78\n72.29 / 96.34\n90.75 / 99.14\n87.01 / 61.89\n52.84 / 81.41\n84.12 / 94.85\nArchitecture ID Accuracy \u2191\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nCross-entropy loss / LogitNorm loss (ours)\nWRN-40-2\n94.75 / 94.69\n49.52 / 15.65 91.55 / 96.91 97.98 / 99.31\nResNet-34\n95.01 / 95.14\n47.74 / 15.82 91.15 / 97.01 97.72 / 99.32\nDenseNet\n94.55 / 94.37\n50.41 / 18.57 91.48 / 96.16 98.04 / 99.10\nered are originally developed based on models trained with cross-entropy loss, hence are natural choices of comparison. In particular, we consider: 1) MSP (Hendrycks & Gimpel, 2016) uses the softmax confidence score to detect OOD samples. 2) ODIN (Liang et al., 2018) employs temperature scaling and input perturbation to improve OOD detection. Following the original setting, we use the temperature parameter T = 1000 and \u03f5 = 0.0014. 3) Energy score (Liu et al., 2020) utilizes the information in logits for OOD detection, which is the negative log of the denominator in softmax function: E(x; f) = \u2212T \u00b7 log \ufffdk i=1 efi(x)/T . With LogitNorm loss, we set T = 0.1 for CIFAR-10 and T = 0.01 for CIFAR-100. 4) GradNorm (Huang et al., 2021) detects OOD inputs by utilizing information extracted from the gradient space.\nOur results in Table 2 suggest that logit normalization can benefit a wide range of downstream OOD scoring functions. Due to space constraints, we report the average performance across six test OOD datasets. The OOD detection performance on each OOD test dataset is provided in Appendix F. For example, we observe that the FPR95 of the ODIN method is reduced from 40.32% to 12.95% when employing logit normalization, establishing strong performance. In addition, we find that logit normalization enables the energy score and GradNorm score to achieve decent OOD detection performance as well.\n<div style=\"text-align: center;\">Table 4. Comparison results of ECE (%) with M = 15, using WRN-40-2 trained on CIFAR-10. For temperature scaling (TS), T is tuned on a hold-out validation set by optimization methods.</div>\nis tuned on a hold-out validation set by optimization methods.\nDataset\nCross Entropy\nLogitNorm (ours)\nCIFAR-10\nw/o TS\n3.20\n66.95\nw/ TS\n0.66\n0.41\nCIFAR-100\nw/o TS\n11.69\n70.45\nw/ TS\n2.18\n1.67\ntures. In Table 3, we show that LogitNorm is effective on a diverse range of model architectures. The results are based on softmax confidence score as test-time OOD score. In particular, our method consistently improves the performance using WRN-40-2 (Zagoruyko & Komodakis, 2016), ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) architectures. For example, on DenseNet, using LogitNorm loss reduces the average FPR95 from 50.41% to 18.57%. Logit normalization maintains classification accuracy. We also verify whether LogitNorm loss affects the classification accuracy. Our results in Table 3 show that LogitNorm can improve the OOD detection performance, and at the same time, achieve similar accuracy as the cross-entropy loss. For example, when trained on ResNet-34 with CIFAR10 as the ID dataset, using LogitNorm loss achieves a test accuracy of 95.14% on CIFAR-10, on par with the test accuracy 95.01% using cross-entropy loss. On CIFAR-100, LogitNorm loss and cross-entropy loss also achieves comparable test accuracies (75.12% vs. 75.23%). Overall LogitNorm loss maintains comparable classification accuracy on the ID data while leading to significant improvement in OOD detection performance.\n# Logit normalization enables better calibration. Whi\nthe OOD detection task concentrates on the separability between ID and OOD data, the calibration task focuses solely on the ID data\u2014softmax confidence score should represent the true probability of correctness (Guo et al., 2017). In practice, Expected Calibration Error (ECE) (Naeini et al., 2015) is commonly used to measure the calibration performance from finite samples. In Figure 4, we observe that LogitNorm loss leads to a smoother distribution of softmax\n<div style=\"text-align: center;\">Table 5. OOD detection performance comparison with different loss functions. We use WRN-40-2 trained on CIFAR-10. Bold numbers are superior results. We report the average norm value of logits as L2 Norm, where we use SVHN as OOD test dataset.</div>\n \nLoss\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nL2 Norm (ID / OOD)\nCrossEntropy\n49.52\n91.55\n97.98\n12.80 / 10.91\nLogitPenalty\n57.62\n73.27\n87.62\n1.90 / 1.74\nLogitNorm (ours)\n15.65\n96.91\n99.31\n1.48 / 0.49\nTable 6. Comparison between GODIN and LogitNorm. We use WRN-40-2 (Zagoruyko & Komodakis, 2016) trained on CIFAR-10. For a fair comparison, we use the ODIN score for LogitNorm loss. Bold numbers are superior results.\n<div style=\"text-align: center;\">Table 6. Comparison between GODIN and LogitNorm. We use WRN-40-2 (Zagoruyko & Komodakis, 2016) trained on CIFAR-10. For a fair comparison, we use the ODIN score for LogitNorm loss. Bold numbers are superior results.</div>\nBold\nLoss\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nGODIN\n25.24\n95.07\n98.93\nLogitNorm (ours)\n15.65\n96.91\n99.31\nconfidence score for ID examples, in contrast to the spiky distribution induced by cross-entropy loss (i.e., values concentrate around 1). It implies that the model trained with LogitNorm loss preserves distinguishable information for different ID samples, indicating its potential in improving model calibration. Indeed, our results in Table 4 show that the model trained with LogitNorm achieves better calibration performance by way of post-hoc temperature scaling.\n# 5. Discussion\nLogit normalization vs. Logit penalty. While our logit normalization has demonstrated strong promise, a question arises: can a similar effect be achieved by imposing a penalty on the L2 norm of the logits? In this ablation, we show that explicitly constraining logit norm via a Lagrangian multiplier (Forst & Hoffmann, 2010) does not work well. Specifically, we consider the alternative loss:\nwhere \u03bb denotes the Lagrangian multiplier that controls the trade-off between the cross-entropy loss and the regularization term.\nOur results in Table 5 show that both logit penalty and logit normalization lead to logits with small L2 norms, compared with using cross-entropy loss. However, unlike LogitNorm, the logit penalty method produces a large L2 norm for OOD data as well, resulting in the inferior performance of OOD detection. In practice, we notice that the network trained with the logit penalty can suffer from optimization difficulty and sometimes fail to converge if \u03bb is too large (which is needed to regularize the logit norm effectively). Overall, we show that simply constraining the logit norm during training cannot help the OOD detection task while our LogitNorm loss significantly improves the performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea50/ea50c30d-6ab9-40ab-94b7-f129a47fc410.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5. Effect of \u03c4 in LogitNorm with CIFAR-10.</div>\ntion 3, the logit normalization can be viewed as an inputdependent temperature on the logits. Related to our work, previous work ODIN (Liang et al., 2018) proposes a variant of softmax score by employing temperature scaling with a constant T in the testing phase:\n\ufffd where the temperature is the same for all inputs. In contrast, our method bares two key differences: (1) the effective temperature in LogitNorm is input-dependent rather than a global constant, and (2) the temperature in LogitNorm can be enforced during the training stage. Our method is compatible with test-time temperature scaling in OOD detection and can improve calibration performance with temperature scaling, as we show in Section 4.2. In Figure 5, we further ablate how the parameter \u03c4 in our method (cf. Eq. 4) affects the OOD detection performance. The analysis is based on CIFAR-10, with FPR95 averaged across six test datasets. Our results echo the analysis in Proposition 3.3, where a large \u03c4 would lead to a large lower bound on the loss, which is less desirable from the optimization perspective. Relations to other normalization methods. In the literature, normalization method is applied for OOD detection in the form of cosine similarity (Techapanurak & Okatani, 2019; Hsu et al., 2020). In particular, Cosine loss (Techapanurak & Okatani, 2019) and Generalized ODIN (GODIN) (Hsu et al., 2020) decompose the logit fi(x; \u03b8) for class i as: fi(x; \u03b8) = hi(x;\u03b8) g(x;\u03b8) , where\n\uf8f3 \ufffd \ufffd \ufffd\ufffd In the testing phase, the maximum cosine similarity is used as the scoring function. Contrastingly, LogitNorm has two key differences: (1) the cosine similarity in hi(x) applies L2 normalization on the last-layer weight w and the learned feature \u03c6p(x), while our LogitNorm normalizes the network output f(x; \u03b8); (2) our LogitNorm can boost the performance of common OOD scoring functions, while GODIN\nand Cosine loss detect OOD examples with their specific scoring functions. In Table 6, we show our LogitNorm with the MSP score achieves better performance than GODIN in OOD detection. More discussion on future work is provided in Appendix. E.\n# 6. Related Work\nOOD detection. OOD detection is an increasingly important topic for deploying machine learning models in the open world and has attracted a surge of interest in two directions.\nworld and has attracted a surge of interest in two directions. 1) Some methods aim to design scoring functions for OOD detection, such as OpenMax score (Bendale & Boult, 2016), maximum softmax probability (Hendrycks & Gimpel, 2016), ODIN score (Liang et al., 2018; Hsu et al., 2020), Mahalanobis distance-based score (Lee et al., 2018), Energybased score (Liu et al., 2020; Wang et al., 2021b; Morteza & Li, 2022), ReAct (Sun et al., 2021), GradNorm score (Huang et al., 2021), and non-parametric KNN-based score (Sun et al., 2022; Zhu et al., 2022). In this work, we first show that logit normalization can drastically mitigate the overconfidence issue for OOD data, thereby boosting the performance of existing scoring functions in OOD detection. 2) Some works address the out-of-distribution detection problem by training-time regularization (Lee et al., 2017; Bevandi\u00b4c et al., 2018; Hendrycks et al., 2019; Geifman & ElYaniv, 2019; Malinin & Gales, 2018; Mohseni et al., 2020; Jeong & Kim, 2020; Liu et al., 2020; Chen et al., 2021; Wei et al., 2021; 2022; Ming et al., 2022a). For example, models are encouraged to give predictions with uniform distribution (Lee et al., 2017; Hendrycks et al., 2019) or higher energies (Liu et al., 2020; Du et al., 2022b; Ming et al., 2022a; Du et al., 2022a; Katz-Samuels et al., 2022) for outliers. The energy-based regularization has a direct theoretical interpretation as shaping the log-likelihood, hence naturally suits OOD detection. Contrastive learning methods are also employed for the OOD detection task (Tack et al., 2020; Sehwag et al., 2021; Ming et al., 2022b), which can be computationally more expensive to train than ours. In this work, we focus on exploring classification-based loss functions for OOD detection, which only requires indistribution data in training. LogitNorm is easy to implement and use, and maintains the same training scheme as standard cross-entropy loss. Normalization in deep learning. In the literature, normalization has been widely used in metric learning (Sohn, 2016; Wu et al., 2018; van den Oord et al., 2018), face recognition (Ranjan et al., 2017; Liu et al., 2017; Wang et al., 2017; 2018; Deng et al., 2019; Zhang et al., 2019), and self-supervised learning (Chen et al., 2020). L2-constrained softmax (Ranjan et al., 2017) applies the L2 normalization on features and SphereFace (Liu et al., 2017) normalizes\n1) Some methods aim to design scoring functions for OOD detection, such as OpenMax score (Bendale & Boult, 2016), maximum softmax probability (Hendrycks & Gimpel, 2016), ODIN score (Liang et al., 2018; Hsu et al., 2020), Mahalanobis distance-based score (Lee et al., 2018), Energybased score (Liu et al., 2020; Wang et al., 2021b; Morteza & Li, 2022), ReAct (Sun et al., 2021), GradNorm score (Huang et al., 2021), and non-parametric KNN-based score (Sun et al., 2022; Zhu et al., 2022). In this work, we first show that logit normalization can drastically mitigate the overconfidence issue for OOD data, thereby boosting the performance of existing scoring functions in OOD detection.\nNormalization in deep learning. In the literature, normalization has been widely used in metric learning (Sohn, 2016; Wu et al., 2018; van den Oord et al., 2018), face recognition (Ranjan et al., 2017; Liu et al., 2017; Wang et al., 2017; 2018; Deng et al., 2019; Zhang et al., 2019), and self-supervised learning (Chen et al., 2020). L2-constrained softmax (Ranjan et al., 2017) applies the L2 normalization on features and SphereFace (Liu et al., 2017) normalizes\nthe weights of the last inner-product later only. Cosine loss (Wang et al., 2017; 2018) normalizes both the features and weights to achieve better performance for face verification. LayerNorm (Xu et al., 2019) normalizes the distributions of intermediate layers for better generalization accuracy. In self-supervised learning, SimCLR (Chen et al., 2020) adopts cosine similarity to measure the feature distances between positive pair of examples. A recent study (Kornblith et al., 2021) shows that several loss functions, including logit normalization and cosine softmax, lead to higher accuracy on ImageNet but degrade the performance of transfer tasks. In addition, GODIN (Hsu et al., 2020) and Cosine loss (Techapanurak & Okatani, 2019) adopt cosine similarity for better performance on OOD detection. As discussed in Section 5, our method is superior to these cosine-based methods, since it is applicable to existing scoring functions and achieves strong performance in OOD detection. Confidence calibration. Confidence calibration has been studied in various contexts in recent years. Some works address the miscalibration problem by post-hoc methods, such as Temperature Scaling (Platt et al., 1999; Guo et al., 2017) and Histogram Binning (Zadrozny & Elkan, 2001). Besides, some regularization methods are also proposed to improve the calibration quality of deep neural networks, like weight decay (Guo et al., 2017), label smoothing (Szegedy et al., 2016; M\u00a8uller et al., 2019), and focal loss (Lin et al., 2017; Mukhoti et al., 2020). Conformal prediction based method (Lei et al., 2013) outputs the empty set as prediction in case of too high \u201catypicality\u201d. Top-label calibration aims to calibrate the reported probability for the predicted class label (Gupta & Ramdas, 2022). Recent work (Wang et al., 2021a) shows that these regularization methods make it harder to further improve the calibration performance with post-hoc methods. LogitNorm loss yields better calibration performance with Temperature Scaling than cross-entropy.\n# 7. Conclusion\nIn this paper, we introduce Logit Normalization (LogitNorm), a simple alternative to the cross-entropy loss that enhances many existing post-hoc methods for OOD detection. By decoupling the influence of logits\u2019 norm from the training objective and its optimization, the model tends to give conservative predictions for OOD inputs, resulting in a stronger separability from ID data. Extensive experiments show that LogitNorm can improve both OOD detection and confidence calibration while maintaining the classification accuracy on ID data. This method can be easily adopted in practical settings. It is straightforward to implement with existing deep learning frameworks, and does not require sophisticated changes to the loss or training scheme. We hope that our insights inspire future research to further explore loss function design for OOD detection.\n# Acknowledgements\nThis research is supported by MOE Tier-1 project RG13/19 (S). LF is supported by the National Natural Science Foundation of China (Grant No. 62106028). YL is supported by Wisconsin Alumni Research Foundation (WARF), Facebook Research Award, and a Google-Initiated Focused Research Award.\n# References\nZhang, X., Zhao, R., Qiao, Y., Wang, X., and Li, H. Adacos: Adaptively scaling cosine logits for effectively learning deep face representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10823\u201310832, 2019.\nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A. Places: A 10 million image database for scene recognition. IEEE transactions on Pattern Analysis and Machine Intelligence, 40(6):1452\u20131464, 2017.\nZhu, Z., Dong, Z., and Liu, Y. Detecting corrupted labels without training a model to predict. In International Conference on Machine Learning (ICML). PMLR, 2022.\nThus Proposition 3.1 is proved.\nFrom Proposition 3.1, we have\nLet fc = maxi(fi), and t = s \u22121, then we have,\n\u03c3c(sf) \u2265 efc \ufffdn j=1 efj = \u03c3c(f).\nThus Proposition 3.2 is proved.\n# C. Proof of Proposition 3.3\n\u22121/\u03c4 \u2264\ufffd f i \u22641/\u03c4, \u2200i \u22081, . . . , k.\nLet \u03c3( \ufffd f) = e \ufffd fy \ufffdk i=1 e \ufffd fi , then we have\n# D. Descriptions of OOD Datasets\nFollowing the prior literature, we use six OOD test datasets: Textures (Cimpoi et al., 2014) is a dataset of describable textural images. SVHN dataset (Netzer et al., 2011) contains 32 \u00d7 32 color images of house numbers, which has ten classes comprised of the digits 0-9. LSUN (Yu et al., 2015) is another scene understanding dataset with fewer classes than Places365. Here we use LSUN-C and LSUN-R to denote the cropped and resized version of the LSUN dataset respectively. iSUN (Xu et al., 2015) is a large-scale eye tracking dataset, selected from natural scene images of the SUN database (Xiao et al., 2010). Places365 (Zhou et al., 2017) consists in images for scene recognition rather than object recognition.\n# E. Future Work\nIn this paper, we introduce a simple fix to the cross-entropy loss that enhances existing post-hoc methods for detecting OOD instances. We expect the observations and analyses in this work could inspire the future design of loss functions for OOD detection. Some future works include:\nTheoretical understanding. In this work, we empirically show that Logit Normalization can significantly improve OOD detection performance. On the theoretical side, we only present an analysis to show why the softmax cross-entropy loss encourages to produce logits with larger magnitudes, leading to the overconfidence issue that makes it challenging to distinguish ID and OOD examples. In the future work, we hope to provide a more rigorous theoretical justification to analyze how the LogitNorm loss improves OOD detection.\nHyperparameter tuning. In our experiments, we tune the hyperparameter \u03c4 with a validation set \u2013 Gaussian noises. Although the proposed method can achieve significant improvement after tuning, the tuning process is computationally expensive because it needs to train multiple models. Therefore, we expect that future work will be able to automatically adjust \u03c4 during training.\n# F. Detailed Experimental Results\nWe report the performance of OOD detectors on each OOD test dataset in Table 7, 8, and 9. In particular, Table 7 shows the detail performance of LogitPenalty and GODIN methods. Table 8 shows the detail performance of different scoring functions with CE loss and LogitNorm loss. Table 9 shows the detail performance of CE loss and LogitNorm loss with different model architectures.\n<div style=\"text-align: center;\">Table 7. OOD detection performance comparison with Logit Penalty (\u03bb = 0.05) and GODIN methods. We train WRN-40-2 (Zagoruyko & Komodakis, 2016) on CIFAR-10 dataset. All values are percentages.</div>\n& Komodakis, 2016) on CIFAR-10 dataset. All values are percentages.\nMethod\nLogitPenalty\nGODIN\nOOD dataset\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nTexture\n62.26\n72.00\n87.29\n34.90\n92.84\n98.41\nSVHN\n62.02\n71.87\n87.21\n25.17\n95.48\n99.04\nLSUN-C\n54.71\n66.23\n82.29\n15.79\n96.88\n99.33\nLSUN-R\n51.31\n79.24\n90.64\n16.82\n97.00\n99.40\niSUN\n51.91\n79.70\n91.17\n23.25\n95.85\n99.15\nPlaces365\n63.51\n70.60\n87.09\n35.51\n93.39\n98.23\n<div style=\"text-align: center;\">Table 8. OOD detection performance comparison using cross-entropy loss and LogitNorm loss. We use WRN-40-2 (Zagoruyko & Komodakis, 2016) to train on the in-distribution datasets and use softmax confidence score as the scoring function. All values are percentages. \u2191indicates larger values are better, and \u2193indicates smaller values are better. Bold numbers are superior results.</div>\nKomodakis, 2016) to train on the in-distribution datasets and use softmax confidence score as the scoring function. All values are\npercentages. \u2191indicates larger values are better, and \u2193indicates smaller values are better. Bold numbers are superior results.\nID dataset\nCIFAR-10\nCIFAR-100\nOOD dataset\nScore\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nCross-entropy loss / LogitNorm loss (ours)\nTexture\nODIN\n59.86 / 26.38\n76.35 / 94.66\n92.09 / 98.68\n80.23 / 70.64\n75.60 / 78.19\n93.49 / 93.35\nEnergy\n51.02 / 35.13\n84.60 / 92.94\n94.90 / 98.29\n80.89 / 79.17\n75.59 / 76.94\n93.48 / 93.29\nGradNorm\n78.50 / 27.26\n55.15 / 93.11\n82.96 / 98.11\n86.51 / 69.07\n55.76 / 75.21\n84.28 / 91.34\nSVHN\nODIN\n53.92 / 9.17\n82.98 / 98.29\n95.17 / 99.63\n83.52 / 45.41\n79.60 / 92.59\n95.34 / 98.48\nEnergy\n21.27 / 10.35\n95.70 / 97.73\n99.05 / 99.55\n84.97/ 62.41\n79.41 / 89.58\n95.31 / 97.86\nGradNorm\n58.95 / 5.51\n76.75 / 98.91\n93.01 / 99.76\n97.7 / 39.79\n54.65 / 93.21\n86.6 / 98.57\nLSUN-C\nODIN\n13.31 / 1.65\n97.14 / 99.59\n99.35 / 99.91\n37.45 / 13.08\n93.01 / 97.66\n98.50 / 99.50\nEnergy\n9.63 / 4.15\n98.01 / 98.64\n99.56 / 99.73\n33.5 / 28.68\n93.74 / 95.33\n98.64 / 99.04\nGradNorm\n22.03 / 1.14\n93.04 / 99.70\n98.12 / 99.94\n43.43 / 8.95\n90.72 / 98.28\n97.84 / 99.63\nLSUN-R\nODIN\n27.21 / 4.71\n93.52 / 98.86\n98.41 / 99.78\n69.69 / 68.3\n83.51 / 84.78\n96.1 / 96.52\nEnergy\n16.5 / 13.90\n96.31 / 97.31\n99.1 / 99.49\n70.45 / 68.84\n83.57 / 85.84\n96.11 / 96.84\nGradNorm\n52.41 / 16.31\n77.70 / 97.18\n92.85 / 99.45\n98.34 / 83.51\n34.18 / 76.32\n76.53 / 94.15\niSUN\nODIN\n33.31 / 5.65\n92.03 / 98.79\n98.07 / 99.76\n74.47 / 71.11\n81.01 / 83.82\n95.43 / 96.24\nEnergy\n19.74 / 16.00\n95.69 / 97.10\n98.98 / 99.45\n75.66/ 72.94\n80.99 / 84.48\n95.43 / 96.49\nGradNorm\n59.08 / 13.76\n74.64 / 97.58\n91.77 / 99.53\n99.41 / 82.51\n32.47 / 77.39\n75.58 / 94.38\nPlaces365\nODIN\n54.32 / 30.12\n81.25 / 94.04\n94.33 / 98.63\n78.93/ 80.23\n75.55 / 76.84\n93.37 / 94.04\nEnergy\n42.75 / 35.31\n88.09 / 92.84\n96.5 / 98.30\n79.72 / 80.73\n75.4 / 76.86\n93.34 / 94.08\nGradNorm\n82.86 / 42.67\n56.46 / 91.55\n85.79 / 98.04\n96.67 / 87.52\n49.29 / 68.02\n83.88 / 91.05\n<div style=\"text-align: center;\">Table 9. OOD detection performance comparison using cross-entropy loss and LogitNorm loss with ResNet-34 and DenseNet-B In-distribution dataset is CIFAR-10. All values are percentages. Bold numbers are superior results.</div>\n Bold\nModel architecture\nResNet-34\nDenseNet\nOOD dataset\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nFPR95 \u2193\nAUROC \u2191\nAUPR \u2191\nCross-entropy loss / LogitNorm loss (ours)\nTexture\n56.38 / 30.68\n87.82 / 94.05\n96.32 / 98.52\n66.14 / 39.79\n85.47 / 91.46\n96.05 / 97.78\nSVHN\n52.34 / 4.72\n89.76 / 98.98\n97.34 / 99.79\n57.26 / 18.66\n91.09 / 96.81\n98.16/ 99.36\nLSUN-C\n32.10 / 0.51\n95.64 / 99.77\n99.14 / 99.95\n32.30 / 2.73\n95.59 / 99.35\n99.12 / 99.87\nLSUN-R\n43.31 / 14.19\n93.41 / 97.65\n98.57 / 99.54\n43.85 / 5.41\n94.04 / 98.70\n98.82 / 99.75\niSUN\n45.80 / 14.83\n92.62 / 97.47\n98.36 / 99.51\n43.08 / 5.73\n94.13 / 98.68\n98.83 / 99.74\nPlaces365\n56.48 / 29.98\n87.57 / 94.16\n96.50 / 98.63\n60.26 / 38.70\n88.26 / 91.98\n97.19/ 98.09\n",
    "paper_type": "method",
    "attri": {
        "background": "Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. This work shows that this issue can be mitigated through Logit Normalization (LogitNorm)\u2014a simple fix to the cross-entropy loss\u2014by enforcing a constant vector norm on the logits in training.",
        "problem": {
            "definition": "The problem addressed is the overconfidence of neural networks in predicting both in-distribution (ID) and out-of-distribution (OOD) inputs, leading to unreliable classifications.",
            "key obstacle": "The main challenge is that existing methods produce high softmax confidence scores even for OOD inputs, which undermines their effectiveness in distinguishing between ID and OOD samples."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that the norm of the logit vector increases during training, leading to overconfident predictions.",
            "opinion": "Logit Normalization decouples the influence of the logit's norm from the training objective, keeping the logit vector's norm constant during training.",
            "innovation": "The key innovation of LogitNorm is its ability to enforce a constant norm on the logits, thus reducing overconfidence and improving the distinction between ID and OOD data."
        },
        "method": {
            "method name": "Logit Normalization",
            "method abbreviation": "LogitNorm",
            "method definition": "LogitNorm is a loss function that normalizes the logit vector to have a constant norm during training, thereby mitigating the overconfidence issue.",
            "method description": "LogitNorm applies a normalization process to the logit outputs, ensuring their magnitudes do not increase during training.",
            "method steps": "1. Compute the logits from the neural network. 2. Normalize the logits to unit length. 3. Apply the cross-entropy loss on the normalized logits.",
            "principle": "The effectiveness of LogitNorm lies in its ability to maintain a consistent output magnitude, which leads to more reliable softmax confidence scores for OOD detection."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using CIFAR-10 and CIFAR-100 as in-distribution datasets and six common benchmarks (Textures, SVHN, Places365, LSUN-C, LSUN-R, iSUN) as OOD datasets.",
            "evaluation method": "Performance was assessed using metrics such as FPR95, AUROC, and AUPR, comparing LogitNorm against traditional cross-entropy loss."
        },
        "conclusion": "The experiments demonstrated that LogitNorm significantly improves OOD detection and calibration performance while maintaining classification accuracy on ID data, making it a practical and effective solution for neural network training.",
        "discussion": {
            "advantage": "LogitNorm allows for better separability between ID and OOD inputs, leading to improved detection rates.",
            "limitation": "While effective, the method may require careful tuning of hyperparameters, such as the temperature parameter \u03c4.",
            "future work": "Future research could focus on automating the tuning process for hyperparameters and providing a more rigorous theoretical understanding of the method."
        },
        "other info": {
            "acknowledgements": "This research is supported by MOE Tier-1 project RG13/19 (S).",
            "code availability": "Code and data are publicly available at https://github.com/hongxin001/logitnorm_ood."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed is the overconfidence of neural networks in predicting both in-distribution (ID) and out-of-distribution (OOD) inputs, leading to unreliable classifications."
        },
        {
            "section number": "2.5",
            "key information": "Logit Normalization decouples the influence of the logit's norm from the training objective, keeping the logit vector's norm constant during training."
        },
        {
            "section number": "3.4",
            "key information": "LogitNorm is a loss function that normalizes the logit vector to have a constant norm during training, thereby mitigating the overconfidence issue."
        },
        {
            "section number": "4.1",
            "key information": "LogitNorm allows for better separability between ID and OOD inputs, leading to improved detection rates."
        },
        {
            "section number": "7.1",
            "key information": "While effective, the method may require careful tuning of hyperparameters, such as the temperature parameter \u03c4."
        },
        {
            "section number": "7.2",
            "key information": "Future research could focus on automating the tuning process for hyperparameters and providing a more rigorous theoretical understanding of the method."
        }
    ],
    "similarity_score": 0.6899292565671563,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Mitigating Neural Network Overconfidence with Logit Normalization.json"
}