{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2111.13188",
    "title": "BioLeaF: A Bio-plausible Learning Framework for Training of Spiking Neural Networks",
    "abstract": "Our brain consists of biological neurons encoding information through accurate spike timing, yet both the architecture and learning rules of our brain remain largely unknown. Comparing to the recent development of backpropagation-based (BP-based) methods that are able to train spiking neural networks (SNNs) with high accuracy, biologically plausible methods are still in their infancy. In this work, we wish to answer the question of whether it is possible to attain comparable accuracy of SNNs trained by BP-based rules with bio-plausible mechanisms. We propose a new bio-plausible learning framework, consisting of two components: a new architecture, and its supporting learning rules. With two types of cells and four types of synaptic connections, the proposed local microcircuit architecture can compute and propagate error signals through local feedback connections and support training of multi-layers SNNs with a globally defined spiking error function. Under our microcircuit architecture, we employ the SpikeTiming-Dependent-Plasticity (STDP) rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, We interpret the proposed framework from an optimization point of view and show the equivalence between it and the BP-based rules under a special circumstance. Our experiments show that the proposed framework demonstrates learning accuracy comparable to BP-based rules and may provide new insights on how learning is orchestrated in biological systems.",
    "bib_name": "yang2021bioleafbioplausiblelearningframework",
    "md_text": "# BIOLEAF: A BIO-PLAUSIBLE LEARNING FRAMEWORK FOR TRAINING OF SPIKING NEURAL NETWORKS\nYukun Yang \u2217, Peng Li Department of Electrical & Computer Engineering University of California, Santa Barbara Santa Barbara, CA 93106, USA {yukunyang,lip}@ucsb.edu\n# ABSTRACT\nOur brain consists of biological neurons encoding information through accurate spike timing, yet both the architecture and learning rules of our brain remain largely unknown. Comparing to the recent development of backpropagation-based (BP-based) methods that are able to train spiking neural networks (SNNs) with high accuracy, biologically plausible methods are still in their infancy. In this work, we wish to answer the question of whether it is possible to attain comparable accuracy of SNNs trained by BP-based rules with bio-plausible mechanisms. We propose a new bio-plausible learning framework, consisting of two components: a new architecture, and its supporting learning rules. With two types of cells and four types of synaptic connections, the proposed local microcircuit architecture can compute and propagate error signals through local feedback connections and support training of multi-layers SNNs with a globally defined spiking error function. Under our microcircuit architecture, we employ the SpikeTiming-Dependent-Plasticity (STDP) rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, We interpret the proposed framework from an optimization point of view and show the equivalence between it and the BP-based rules under a special circumstance. Our experiments show that the proposed framework demonstrates learning accuracy comparable to BP-based rules and may provide new insights on how learning is orchestrated in biological systems.\nThanks to greater computing power, deep learning has gained remarkable achievements in recent years (Hinton et al., 2006; Bengio & LeCun, 2007; Schmidhuber, 2015; Goodfellow et al., 2016). However, learning by backpropagation (BP) (Rumelhart et al., 1986) is still the most popular method, which is generally believed impossible to be implemented in our brains (Illing et al., 2019). As compared to deep neural networks (DNNs), our brain, the only known true intelligence system, is more energy efficient (Von Neumann, 2012), robust (Den`eve et al., 2017; Qiao et al., 2019), and capable of achieving life-long learning (Parisi et al., 2019), online learning (Lobo et al., 2020), logic reasoning (Monti & Osherson, 2012), and has many other advantages (Raichle et al., 2001). The development of artificial intelligence (AI) may benefit from investing in how our brain works. Our brain is a complex system consisting of neurons that communicate with each other through spikes. Therefore, people tried to use simplified spiking neurons to form a network that mimic the function of our brain. Such spiking neural networks (SNNs) can naturally exploit spatio-temporal data with each neuron\u2019s internal temporal dynamics (Yang et al., 2021), and save orders of magnitude of less energy when running on neuromorphic hardwares (Davies et al., 2018; Kim et al., 2020b; Davies et al., 2021). However, the training of SNNs is difficult.\nRecent developments of the direct training methods of SNNs mainly diverge into two streams: BPbased rules and bio-plausible rules (Hao et al., 2020). BP-based learning rules include: the activation-based surrogate gradient methods (Zenke & Ganguli, 2018; Shrestha & Orchard, 2018; Wu et al., 2018), the timing-based methods (Zhang & Li, 2020), the combination of both (Kim et al., 2020a), and the recently proposed neighborhood aggregation method - NA (Yang et al., 2021). These BP-based methods gained great performance improvement and helped SNNs to be implemented on real-world problems, yet their biological plausibility remains unresolved: the co-existence of both forward and backward signals requires a neuron to fire two sets of uncorrelated signals from the same neuron body, which is not bio-plausible. While the other branch, the bio-plausible learning rules, represented by the STDP (Taylor et al., 1973; Levy & Steward, 1983) and the Widrow-Hoff (WH) (Widrow & Hoff, 1960) rules, adjusts parameters using local plasticity only. The STDP learning rule is built upon the Hebbian learning rule, which can be informally described as: \u201dCells that fire together, wire together\u201d (Hebb, 1949). Following this rule, STDP adjusts synaptic weights by evaluating the timing correlation: If a presynaptic neuron fires a few milliseconds before a postsynaptic neuron, meaning this presynaptic spike contributes to the firing of the postsynaptic neuron, their connection is strengthened (causal), or called long-term potentiation. Whereas the opposite temporal order results in long-term depression (acausal). Although STDP demonstrates its potential usefulness in both supervised and unsupervised manners, it is unlikely that STDP works alone: Strengthened connection makes the firing activity of a pair of neurons more synchronized, and vice versa. Due to the existence of the positive feedback loop, one needs to introduce additional tricks to stabilize the learning process - such as winner-takes-all (WTA) (Nessler et al., 2013; Diehl & Cook, 2015; Kheradpisheh et al., 2018; Saunders et al., 2018), weights normalization (Ferr\u00b4e et al., 2018), weights clamping (Diehl & Cook, 2015; Kheradpisheh et al., 2018; Lee et al., 2018; Saunders et al., 2018), layer-by-layer training (Kheradpisheh et al., 2018), and others (Panda & Roy, 2016). In comparison, the WH-based learning algorithms, represented by ReSuMe(Ponulak & Kasi\u00b4nski, 2010) and SPAN (Mohemmed et al., 2012), are able to train a spiking neuron to generate spikes with accurate timing, and do not need additional tricks as STDP does. The WH learning rule (Widrow & Hoff, 1960) is a special case of the gradient descent rule where the least mean square loss is applied. Ponulak & Kasi\u00b4nski (2010) presented a spiking analogy to the classical WH rule for spiking neuron models, and their rule can be interpreted as an STDP-like process between a presynaptic spike train and a postsynaptic error signal. However, previous WH-based methods are constrained to train a single layer SNN since it has difficulty in propagating the teaching signal to previous layers. Sporea & Gr\u00a8uning (2013) extend the single layer WH-based learning rule - ReSuMe (Ponulak & Kasi\u00b4nski, 2010) onto multi-layers networks through BP-liked error propagation, which is practical but deviates from the original intention of exploring bio-plausible mechanisms. In this work, we propose a bio-plausible learning framework - BioLeaF, underpinned by two key components: 1) a microcircuit architecture consisting of two types of spiking neurons and four types of synapses as shown in Figure 1, and 2) the STDP-based learning rules built upon our architecture. The architecture is inspired from the predict-coding-based algorithms (Rao & Ballard, 1999; Stefanics et al., 2014). Previous works proposed several predictive-coding-inspired microcircuit architectures to realize BP-liked learning on rate-based neurons (Bastos et al., 2012; Whittington & Bogacz, 2017; Sacramento et al., 2018), where all neurons communicate with each others through continuous currents, and no explicit temporal point processes or spiking behaviors are included. This simplified setting limits the discussion of the widely used bio-plausible learning rules defined by the spike-timing correlation like STDP rule and WH rule. Our architecture differs from them in both the neuron and the synapse models. We include the more bio-plausible spiking leaky integrateand-fire (LIF) neuron (Gerstner & Kistler, 2002) and synapses models that transmit discontinuous spikes into our architecture. The architecture consists of two types of spiking cells - pyramidal cells and somatostatin-expressing (SOM) cells (Petreanu et al., 2009; Larkum, 2013). Each pyramidal cell i has a paired SOM cell ip to predict its firing activity one-on-one through the same current inputs aj, j = 1 \u00b7 \u00b7 \u00b7 N, where N is the total number of presynaptic neurons. The prediction mismatch, also interpreted as a surprise or free energy (Friston, 2010), is transmitted through top-down connections to all presynaptic neurons\u2019 apical dendrites, and acts as their error signals. A pyramidal cell\u2019s top-down output \u02dca is modulated\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba14/ba14142e-d720-40e2-94c8-d0bc8e82311f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Our proposed Microcircuit architecture.</div>\nby its error signals, whereas a SOM cell\u2019s top-down output ap is not. Therefore, without knowing the signal in i\u2019s apical dendrite, SOM cell ip\u2019s prediction can only cancel out part of i\u2019s output, which leaves the error-related signal on j\u2019s apical dendrites. The summation of all top-down signals will cancel out each others pair by pair, and leaves the total error-related signals onto j, from where the layer-by-layer error backpropagation is realized. The learning rule built upon our microcircuit architecture is the standard STDP rule as defined in Levy & Steward (1983) with specialized choices of pre/postsynaptic signals. As comparing to typical SNNs\u2019 architectures (Shrestha & Orchard, 2018; Wu et al., 2018; Yang et al., 2021), which only have forward connected weights, we introduce three additional types of weights: forward predict, top-down, and top-down predict as in Figure 1 to support the bio-plausible learning. Therefore, to update different types of synaptic weights, our STDP updating rules need to be defined between pairs of presynaptic and postsynaptic signals locates in different components of our microcircuit architecture. A presynaptic signal is an output spike train located in the presynaptic neuron like sj for both weights wij and wipj, or sip for wjipe following Levy & Steward (1983), and a postsynaptic signal is an error signal located in the postsynaptic neuron like ei for weights wij and ej for wjipe. More generally, we analytically show that the the proposed framework is equivalent to the BPbased learning rules under certain settings. To derive a BP-based learning rules which propagates continuous-valued loss signal through discontinuous all-or-none firing activity, some approximation methods are applied following previous works (Shrestha & Orchard, 2018; Wu et al., 2018; Yang et al., 2021). Yet such approximation surprisingly aligned with the standard STDP rules under our microcircuit architecture with only minor differences. We empirically build a 2-layers toy example to evaluate the learning ability of BioLeaF. Deeper than a single layer breaks the limit of the previous WH-based learning rules. Then, by benchmarking on the datasets including MNIST (LeCun, 1998) and CIFAR10 (Krizhevsky et al., 2009), the proposed BioLeaF also exhibits comparable accuracy with other BP-based methods when extended to multi-layers deep SNNs.\n# 2 MICROCIRCUIT ARCHITECTURE\n# 2.1 SPIKING NEURON MODEL\nBoth pyramidal cells and SOM cells are modeled by the leaky integrate-and-fire (LIF) neuron mode which is one of the most prevalent choices for describing dynamics of spiking neurons.\n(1)\nwhere I(l) i is the total input of synaptic currents, and \u03b7(l) i (t) denotes the reset function. A spiking neuron reset its membrane potential from threshold \u03d1 to the resting potential vrest (we set vrest = 0) each time when it fires a spike. We model \u03b7(l) i (t) as the time convolution (*) between a reset kernel \u03bd and the neuron\u2019s output spike train s(l) i : \u03b7(l) i (t) = (\u03bd \u2217s(l) i )(t). The reset kernel \u03bd(t) = \u2212\u03d1\u03b4(t). The amount of resetting is equal to the threshold \u03d1 (we set \u03d1 = 1), and \u03b4(t) is the Dirac delta function. The neuron\u2019s output spike train is also modeled by a serious of delta functions as:s(l) i = \ufffd f \u03b4(t \u2212t(l) i(f)). Here, t(l) i(f) represent the firing time of the f th spike of neuron i in layer (l). An output spike is generated once the membrane potential reaches the threshold \u03d1. Following Shrestha & Orchard (2018), we define the spike function as: fs(u) : u \u2192s, s(t) := s(t) + \u03b4(t \u2212t(f+1)), t(f+1) = min \ufffd t : u(t) = \u03d1, t > t(f) \ufffd . (2)\n# 2.2 SYNAPTIC CURRENTS\nWe model the general total input current on neuron i as: Ii(t) = \ufffd j wij \ufffd f \u03b1ij(t \u2212tj(f)).  modeled total input is the weighted sum over all current pulses: \ufffd \ufffd \ufffd \ufffd\n# We model the general total input current on neuron i as: Ii(t) = \ufffd j wij \ufffd f \u03b1ij(t \u2212tj(f)). The modeled total input is the weighted sum over all current pulses: \ufffd \ufffd \ufffd \ufffd\n \ufffd \u03b1ij \ufffd t \u2212tj(f) \ufffd = gij \ufffd t \u2212tj(f) \ufffd \u00b7 [Esyn \u2212ui(t)] ,\n\ufffd \ufffd \ufffd \ufffd   where Esyn is the reversal potential for the synaptic current. We set Esyn \u226b\u03d1 in all types of synapses (Destexhe et al., 1998), so the membrane potential dependency can be neglected, and the term [Esyn \u2212ui(t)] can be treated as a constant and absorbed into weights. gij(t \u2212tj(f)) is the synaptic conductance change. We modeled it following Eyal et al. (2018), but simplified the double exponential function to a single exponential decaying function and have: \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd\ngij \ufffd t \u2212tj(f) \ufffd = Bi(t) \u00b7 (1/\u03c4s) \u00b7 exp \ufffd \u2212 \ufffd t \u2212tj(f) \ufffd /\u03c4s \ufffd ,\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd where Bi(t) is a membrane potential dependent gating function. The synapses in different location have different properties, where the two types of synapse we apply are named as the forward-relatedtype (F-type) and the error-related-type (E-type). We introduce them one by one as following:\n# F-Type Synapses:\nThe connections from the outputs of pyramidal cells in one layer to the basal dendrites of pyramidal cells in their next layer carrying important feature information build up the main architecture in a spiking neural networks. When training is finished, only these forward connections are needed to realize inference. We implement these forward connections w(l) ij together with their paired forward predict connections w(l) ipj with the F-type synapses.\nBi(t) is set to 1 (voltage independent conductance) as a general setting for AMPA-based condu tance (Eyal et al., 2018). We follow this setting and the input current is then simplified to a widely used alpha function with time constant \u03c4s: \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd\n\ufffd  \u2212 \ufffd  \u00b7 \ufffd \u2212 \ufffd  \u2212 \ufffd \ufffd Under which, the synaptic current is independent from neuron i, and all postsynaptic current (PSC generated from neuron j can be uniformly expressed by one variable aj as:\nwhere (*) represents the time convolution. \u03f5(\u00b7) is the impulse response. H(\u00b7) represents the H side step function: H(t) = 1, t \u22650 and H(t) = 0, t < 0. A fully connected layer can be described through the current flows as:\n\ufffd \ufffd \ufffd Other layers like convolution layers can be easily converted to fully connected layers. Similarly, the total current of SOM cell I(l) ip is modeled as: I(l) ip (t) = \ufffdN (l\u22121) j=1 w(l) ipja(l\u22121) j , where the footnote p represent the predict-related, or the SOM-related variables.\n(3)\n(4)\n(5)\n(6)\n(7)\n# E-Type Synapses:\nAll other connections are modeled by E-type synapses, which differs from the F-type synapses by their postsynaptic voltage dependent property. We model the voltage dependent gating function Bi(t) like how prior works model the NMDA-based synapses (Eyal et al., 2018). Bi(t) has a shape that peaks when the postsynaptic cell\u2019s membrane potential ui(t) reaches the threshold \u03d1, and decrease as ui(t) moves away from \u03d1. Such a shape acts as surrogate derivative function when compared to BP-based methods, which will be fully discussed in the following section.\n\u2212 \u2212 \u00b7 \u00b7 where the extracellular magnesium concentration [Mg2+] was 1 mM in the model. We shift the voltage dependency of Bi(t) by u0 = \u03d1, and tune the parameters gmax, k and n to fits the function B into our simplified setting where vrest = 0, and \u03d1 = 1. The error backpropagation is achieved by the corporation between pairs of pyramidal cells and SOM cells. Higher level pyramidal cells\u2019 top-down output currents are coupled by the error signals located in their apical dendrites. We model this coupling effect by the current sum of both a neuron\u2019s PSC and its error signal as: \ufffda(l) i (t) = a(l) i (t)+e(l) i (t). A pyramidal cell\u2019s top-down connection contributes positively with weights w(l) jie onto previous layers\u2019 pyramidal cells\u2019 apical dendrites, and its paired SOM cell contribute negatively with weights w(l) jipe, where the footnotes p stands for SOM-related, and the footnotes e stands for error-related. We express the total error signals on the pyramidal cell j\u2019s apical dendrites as: \uf8ee \uf8f9\n\uf8f0 \ufffd \ufffd \uf8fb For the output layer, the apical dendrites receives the one-on-one error signal from higher brain areas to realize supervised learning. \ufffd \ufffd\n\ufffd  \u2212 \ufffd In our framework, SOM cells mimic the behavior of the same layer\u2019s pyramidal cells one-on-one. Therefore, a one-to-one nudging signal from a pyramidal cell to its corresponding SOM cell (as the dashed purple connections in Figure 1) are needed. Together with the negative feedback output currents that SOM cells generated themselves, we get the local error signals e(l) ip of an SOM cell i in layer (l):\n \u2212 \ufffd  \u2212 \ufffd Although this simplified one-to-one setting impose special constrains on the neural network\u2019s connectivity, the recent monosynaptic experiments confirm that the SOM cells do receive top-down connections which may encode such teaching information (Leinweber et al., 2017). The SOM cells differs from the pyramidal cells by the sign of their output currents, but it does not mean that we fix the type of a cell to be excitatory or inhibitory. Instead, we allow the synapses\u2019 connection weights to move across zero freely and change the sign of its current, which is a general setting in previous works (Ponulak & Kasi\u00b4nski, 2010; Sacramento et al., 2018). When currents from both SOM cells and pyramidal cells summed together into the SOM cells or the apical dendrites as in (11) and (9), a minus sign is added for SOM cell\u2019s outputs.\n# 3 BIO-PLAUSIBLE LEARNING RULES\n# 3.1 SPIKE-TIMING-DEPENDENT-PLASTICITY (STDP)\n3.1 SPIKE-TIMING-DEPENDENT-PLASTICITY (STDP)\nWe first introduce a popular version of the STDP rules: \u2206wij = \ufffd m \ufffd n \u03baSTDP(tm i \u2212tn j ), whe \u03baSTDP is the STDP kernel function, which is modeled by the two-sides exponential decaying fun tion defined as: \ufffd  \n(8)\n(9)\n(10)\nMeantime, we define a reversed STDP kernel function \u02dc\u03baSTDP(t) = \u03baSTDP(\u2212t). Recalling that all spike trains are a serious of delta function s = \ufffd f \u03b4(t \u2212t(f)), and considering the delta function\u2019s sampling property \ufffd [f(t)\u03b4(t \u2212T)] dt = f(T), we rewrite the STDP updating rule into two equivalent forms:\nwhere \u03b7ij is the learning rate. Although \u03b7ij can be absorbed into A+ and A\u2212, we explicitly define it for clarity. Since the WH rule can be interpreted as an STDP-like process between the presynaptic spike trains sj and an error signal ei := steach i \u2212si. In this work, we represent both the STDP rule and the WH rule uniformly as STDP(\u00b7, \u00b7), where the STDP rule is: STDP(spre, spost), and the WH rule is: STDP(spre, epost). In addition, WH rule\u2019s kernel function \u03baWH is usually equal to \u03baSTDP as in (12). Defining the reverse kernel function \u02dc\u03baWH(t) = \u03baWH(\u2212t), we have the weight updating rule of the WH rule as: \ufffd \ufffd\n\ufffd \ufffd It is noteworthy that the WH-based rules provides a fixed point in the weight space, \u2206wij = 0 when ei = 0, which means steach i = si. It is a global positive attractor under certain conditions (Ponulak & Kasi\u00b4nski, 2010).\n\ufffd\ufffd \ufffd where \u03b7ij, \u03b7ipj, and \u03b7jipe are three different learning rates for these three types of synaptic connections. In the above three equations, all the adjusting rules of a synapse can be described as a STDP process between a presynaptic current and a postsynaptic error signal, which exploits great biological plausibility. The top-down synaptic weights w(l) jie are set to be equal to the bottom-up forward weights w(l) ij for simplicity. One can also try to fixed the top-down weights w(l) jie as randomly initialized values following the idea of the feedback alignment (Sacramento et al., 2018). We interpret each type of synapses\u2019 learning rule as following: In (15), w(l) ij represents the forward weights. It\u2019s fixed point e(l) i (t) = B(lN) i (t)[atarget i (t) \u2212 a(lN) i (t)] = 0 forms a positive attractor to minimize the error signal. When training is converged, we expect a(lN) i \u2248atarget i for all pyramidal cells (i = 1, . . . , N (l)), (l = 1, . . . , lN). In (16), w(l) ipj stands for the predictive connections. Its fixed point in our learning rule is e(l) ip (t) = B(l) ip (t)[a(l) i (t) \u2212a(l) ip (t)] = 0, which means a(l) i = a(l) ip , and s(l) i = s(l) ip . As compared to (15), the learning rule of (16) does not have a fixed target signal, but needs to follow the continuously changing behavior of each SOM cell\u2019s corresponding pyramidal cell during training. The learning rule (17) minimizes the norm of e(l\u22121) j (t) in (9) through adjusting w(l) jipe, which is equivalent to solving a linear equation: \ufffdN (l) i=1 w(l) jipea(l) ip (t) = \ufffdN (l) i=1 w(l) jie(a(l) i (t) + e(l) i (t)). Considering the Current-Time as a continuous f(X) \u2194X function space, where all SOM cells\u2019 PSCs a(l) ip , (i = 1, . . . Nl) form a basis of the space. Then the goal here is to restore the summed\n(14)\n(15)\n(16)\n(17)\ncurrents of all pyramidal cells in layer (l) through these basis functions. When a predictive weight w(l) ipj is well adjusted, a(l) i approximately equals to a(l) ip , then an obvious solution to the equation above will be letting w(l) jipe equal to w(l) jie, if we consider the error currents e(l) i (t) as the orthogonal uncorrelated signal to the basis.\n# 4 RELATIONSHIP TO THE BP ALGORITHM\n# 4.1 BACKPROPAGATION FLOW\nSince the SOM cells are auxiliary in our architecture, we only introduce how backpropagation works in a general SNN without SOM cells. Consider a general loss function L = \ufffd t E(t)dt, which is defined on the output layers\u2019 PSC a(lN) i (t), i = 1, . . . , N (l), where lN is the total number of layers. The differentiable property of a loss function requires that \u2202L/\u2202a(lN)(t) exist. We name the partial derivative on the layer (l)\u2019s PSC as:d(l) i (t) := \u2202L \u2202a(l) i (t). Taking the l2 distance between two PSCs, or the van Rossum distance (van Rossum, 2001) between two spike trains, as an example: \ufffd \ufffd\nWhen computing the gradient of loss with respect to the synaptic weight of the last layer w(lN) ij : \u2202L/\u2202w(lN) ij = \ufffd d(lN) i (t) \ufffd \u03f5 \u2217\u2202s(lN) i /\u2202w(lN) ij \ufffd (t)dt, one may find the derivative \u2202s(lN) i /\u2202w(lN) ij is ill-defined due to spiking neurons\u2019 discontinuous all-or-none firing activities. Following (Zenke & Ganguli, 2018), we substitute this term by: (\u03c3\u2032(u(lN) i ) \u00b7 \u2202u(lN) i /\u2202w(lN) ij ), and further approximate the term (\u2202u(lN) i /\u2202w(lN) ij ) \u2248a(lN\u22121) j by omitting the temporal dependency of membrane potentials. We have the weights updating rule as:\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd where \u03b7 is the learning rate, e(lN) i := \u2212d(lN) i \u03c3\u2032(u(lN) i ), and the kernel function of BP algorithm \u03baBP(t) := (\u03f5 \u2217\u03f5)(t) = \ufffd (t/\u03c4 2 s ) \u00b7 exp(\u2212t/\u03c4s) \u00b7 H(t) \ufffd . In the second line, we approximately switch the order of time convolution and product to separate variables of presynaptic neurons and postsynaptic neurons. Then to further propagate the gradient to previous layers, we calculate the partial derivative of loss with respect to hidden layer\u2019s PSCs as:\n  Here we omit the temporal dependency (\u2217\u03f5) for simplicity. Since BP is not our contribution, we discuss different omitting methods of all previous works and the detailed derivatives including the full dependency in the appendix. With d(l\u22121) j (t) calculated, an hidden layer follows the same rule to propagate from d(l\u22121) j to (\u2202L/\u2202w(l\u22121) jm ) as the output layer.\nnalyses our bio-plausible learning rule from the optimization point of vie\n(18)\n(19)\n(20)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df16/df16ef06-ff75-4c54-a4ee-d734296d1672.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b708/b708a86a-eb1b-482b-96e4-a8a11df005eb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Comparison of \ud835\udf05!\" & the positive side of \ud835\udf05#$%\"</div>\nFigure 2: (a) \u03c3\u2032(u) = 1/(1 + |u \u22121|)2) is defined following Zenke & Ganguli (2018), and the parameters of B(u) used here are: u0 = \u03d1 = 1, gmax = 109.45, k = 1.18 and n = 124.33. (b) \u03baBP = \u03f5 \u2217\u03f5, with \u03c4s = 30 ms. The positive side of \u03baSTDP = A+exp(\u2212t/\u03c4+)H(t) has parameters: A+ = 0.0124, and \u03c4+ = 89.73 ms. In the output layer, the error signal in (10) corresponds to \ufffd \u2212d(lN) i \u03c3\u2032(u(lN) i ) \ufffd , which is the postsynaptic part in (19). The term \u2212d(lN) i corresponds to [atarget i (t) \u2212a(lN) i (t)] in (10), which implies that the equivalent loss function we apply for the bio-plausible rule is also the van Rossum distance. For other loss functions, one can safely substitute [atarget i (t) \u2212a(lN) i (t)] with their own (\u2212d(lN) i ). The auxiliary \u03c3\u2032(u(lN) i ) function corresponds to the voltage dependent gating function B(lN) i . One may find that with proper parametrization they can almost overlap as shown in Figure 2 (a). Both of them reshapes the error signal depending on the neuron\u2019s membrane potential, where the gradients farther from the threshold are weakened. With error signals clearly defined, we find both the BP-based learning rule (19) and the previously described bio-plausible STDP-based learning rule (15) follow a surprisingly similar form:\nFigure 2: (a) \u03c3\u2032(u) = 1/(1 + |u \u22121|)2) is defined following Zenke & Ganguli (2018), and the parameters of B(u) used here are: u0 = \u03d1 = 1, gmax = 109.45, k = 1.18 and n = 124.33. (b) \u03baBP = \u03f5 \u2217\u03f5, with \u03c4s = 30 ms. The positive side of \u03baSTDP = A+exp(\u2212t/\u03c4+)H(t) has parameters: A+ = 0.0124, and \u03c4+ = 89.73 ms.\nAs shown in Figure 2 (b), the shape of BP\u2019s kernel function \u03baBP is highly similar to the positive side of the STDP kernel \u03baSTDP, where the value peaks near zero, and decays gradually. Such equivalence gives theoretical analysis of what should the kernel looks like from the optimization point of view and provides possibly explanations of why the negative side of STDP learning rule usually dampen the performance, and usually been omitted in previous works to boost the performance (Ponulak & Kasi\u00b4nski, 2010). In the experiment part, we also ignored the negative side of STDP kernel. When further propagating the gradient to previous layers, the more complex predictive-codinginspired architectures are involved. As in Sacramento et al. (2018), we also name the ideal state where w(l) ij = w(l) ipj = w(l) jipe = w(l) jie as self-predicting (self-predicting is needed theoretically, but not experimentally as shown in the next section). Under which, the summed error signal on the apical dendrites of a pyramidal cell in (9) are simplified to:\ne(l\u22121) j (t) = B(l\u22121) j (t) N (l) \ufffd i=1 \ufffd w(l) ij (a(l) i (t) + e(l) i (t)) \u2212w(l) ij a(l) i (t) \ufffd = B(l\u22121) j (t) N (l) \ufffd i=1 w(l) ij e(l) i (t). (22)\nComparing to (20), which propagates the negative weighted sum of e(l) i to d(l\u22121) j , and further calculate e(l\u22121) j = \u2212d(l\u22121) j \u03c3\u2032(u(l) j ), our rule yields more symmetry, where the error signals e flow through layers without any intermediate variables. These two rules are equivalent when pairing B(u) to \u03c3\u2032(u) as in Figure 2 (a). We conclude their similarity and difference as following: 1) Both of the learning rules share a same form: \u2206wij \u221d \ufffd (spre \u2217\u03ba) \u00b7 epost. 2) The surrogate derivatives \u03c3\u2032(u(l) i ) in BP is correspondingly achieved by B(l) i of the E-type synapses in our framework. 3) The shape of BP\u2019s kernel function \u03baBP is similar to the positive side of \u03baSTDP. 4) For both rules, the error backpropagation between two layers are equivalent when our network in its self-predicting state. And the one difference these two methods have is: The kernel function \u03baBP only has the positive side (t > 0), but \u03baSTDP has double sides.\n(21)\n(22)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0e77/0e77939e-6372-4c37-a932-dbda9a90cf4d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Universal spike train approximator experiment 5 EXPERIMENTAL RESULTS</div>\n# 5.1 UNIVERSAL SPIKE TRAIN APPROXIMATOR\n5.1 UNIVERSAL SPIKE TRAIN APPROXIMATOR\nTo test the learning ability of our proposed bio-plausible learning rules, we build up a 2-layers SNN to fit a random target spike train (PSC) from randomly generated inputs as shown in Figure 3 (a). One SOM cell paired with the output pyramidal cell helps it propagate error backwards and update the input weights. More detail settings can be found in the appendix. In Figure 3 (b) and (c), the upper two sub-figures exploit that both the output ai and the predict output aip are able to fit our randomly assigned sinusoidal target PSC atarget i . The lower two sub-figures shows the difference between paired weights, where the two backward weights w(l) jipe \u2248w(l) jie after training, yet the other pair remains different. Such difference would not hinder the training because the success of error backpropagation only requires ai \u2248aip.\n# 5.2 THE RESULTS ON THE MNIST DATASET AND THE CIFAR-10 DATASET\nThe proposed framework is compared with other BP-based rules on two widely used real-world datasets: MNIST (LeCun, 1998) and CIFAR-10 (Krizhevsky et al., 2009). Previous works usually use the fixed-step first-order forward Euler method to discretize continuous membrane voltage updates over a set of discrete time steps, we also following this setting and take several measures to guarantee a fair comparison: 1) Mirroring B(u) with \u03d1 to get B(u)\u2019s value when u > \u03d1. 2) Setting \u03baSTDP[t] = 1 when t = 0, and \u03baSTDP[t] = 0 when t \u0338= 0. 3) All the comparisons are made under a SNN\u2019s self-predicting state. The results are concluded in table 1. Our method gains comparable performance as compared to BP-based works. Table 1: Performances comparison of different methods on the MNIST and CIFAR10 datasets\nble 1: Performances comparison of different methods on the MNIST and CIFAR10 datase\nMNIST\nCIFAR10\nMethod\n#Steps\nBestAcc\n#Steps\nBestAcc\nSLAYER (Shrestha & Orchard, 2018)\n300\n99.41%\nnull\nnull\nTSSL-BP (Zhang & Li, 2020)\n5\n99.53%\n5\n89.22%\nNA (Yang et al., 2021)\n5\n99.69%\n5\n91.76%\nThis work\n5\n99.46%\n5\n86.88%\nMNIST SNN structure: 15C5-P2-40C5-P2-300\nCIFAR10 SNN structure: 96C3-256C3-P2-384C3-P2-384C3-256C3-1024-1024\n# 6 CONCLUSION\nWe proposed a new bio-plausible learning framework, BioLeaF, consisting of two key components: an architecture, and its paired learning rules. BioLeaF leverages previous bio-plausible works\u2019 limitation, and bridges the gap between the bio-plausible approach and the BP-based approach both analytically and experimentally. The equivalence of these two approaches are demonstrated under a special setting, and the comparable experimental performance of them are benchmarked on MNIST and CIFAR10 datasets. This work may provide new insights on both approaches.\nREFERENCES\n# A APPENDIX\nWe start from backpropagating the error signal from the PSC - a to the spike train - s. A causa impulse response only has value when t > 0 as the \u03f5 in (6). So the partial derivative of a neuron\u2019 output spike train on time t is the integration on the derivative of all future error with t\u2032 > t, which can be expressed by:\nwhere T is the length of a simulation time window, and \u02dc\u03f5(t) = \u03f5(\u2212t) is the time-reverse impulse response kernel of the synapse. In Figure 4, we draw the full dependency in both (a) - forward and (b) - backward propagation under infinitesimal discrete time steps dt. Under which, the gradient on u can be described by:\nwhich does not have a closed form solution for two reasons: \u2022 The derivative \u2202s(l) i (t)/\u2202u(l) i (t) is zero when u(l) i (t) \u0338= \u03d1, and is \u221ewhen u(l) i (t) = \u03d1, which is ill defined. \u2022 The complex temporal dependency of u(l) i (t) and u(l) i (t + dt) brings difficulty for calculation. Following previous works (# cite): by introducing surrogate gradient as a substitution for \u2202s(l) i (t)/\u2202u(l) i (t), we approx the first term in (24) as:\nThe other term in (24) describes the temporal dependency of the u. Define a membrane potential\u2019s impulse response kernel function \u03b6(t) as: \u03b6(t) = 1 \u03c4m e(\u2212t/\u03c4m)H(t) By ignoring part of the dependency, as shown by the gray colored dash lines in Figure 4 (b), and applying the same trick as in (23), we have:\n\ufffd\ufffd \ufffd \ufffd where \u02dc\u03b6(t) = \u03b6(\u2212t) is the time-reverse impulse response kernel of the membrane potential.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0914/0914a8b1-fc0c-462e-9500-a506c181c509.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/84d6/84d66158-bbb9-4970-bcbc-9c5e73be540b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">a) Forward propagation</div>\nFigure 4: (a) Full forward dependency between u, s, and a in the discrete simulation. (b) Full backward dependency in the discrete simulation. The dashed lines are usually been omitted when doing backpropagation. (c) BP\u2019s kernel functions with different time constants\n(23)\n(24)\n(25)\n(26)\nThen the final step is to propagate the gradient from u(l) i to w(l) ij according to (1):\nwhere e(l) i (t) = (g(l) i \u00b7 \u03c3\u2032(u(l) i ))(t), and \u03baBP(t) = (\u03f5 \u2217\u03f5 \u2217\u03b6)(t). We show the shape of \u03baBPin Figure 4 (c), and calculate its close form expression as:\nWe then provide the rules to further propagate the gradient to previous layers (take layer (l \u22121) a an example). Following (1), we have:\nBP in hidden layers also following the similar steps: from g(l\u22121) j to \u2202L/\u2202u(l\u22121) j following (26), and further propagating to weights following (27).\n# A.2 DETAILED EXPERIMENTAL SETUPS\nA.2.1 UNIVERSAL SPIKE TRAIN APPROXIMATOR\nThis section concludes all settings used in the 2-layers SNN experiment. All parameters used are\nThis section concludes all settings used in the 2-layers SNN experiment. All parameters used are summarized in Table 2.\n<div style=\"text-align: center;\">Table 2: Parameters of the 2-layers network</div>\nTable 2: Parameters of the 2-layers network.\nNT\n\u03c4m\n\u03c4s\n#inputs\n#hidden\npin\nA+\n500ms\n50ms\n20ms\n50\n100\n0.05\n0.00004\n\u03c4+\nA\u2212\nvrest\n\u03d1\n#iters\nscale\nbias\n30ms\n0\n0\n1\n5000\n0.3\n0.01\nThe total simulation time Nt is 500ms. \u03c4m and \u03c4s are the time constants in (1) and (4). The total number of randomly generated inputs is 50. The number of hidden pyramidal cells is 100. We sample the total 500ms by 1ms when doing this experiment, which means there are 500 time steps totally. As shown in Figure 5, the randomly generated input currents are produced by a twosteps process.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/706d/706d9ecc-d17e-4bae-a272-725487922bc2.png\" style=\"width: 50%;\"></div>\nFigure 5: Visualization of all 50 input currents, the network architecture, and the process to generat the target output current\n(27)\n(28)\n(29)\nThe first step is to generate spike trains s(t) following Bernoulli distribution on each time step with probability pin = 0.05. Then the input currents are defined using F-type synapses a(t) = s(t) \u2217\u03f5 as in (6). A+, \u03c4+, and A\u2212are parameters of our STDP function. vrest and \u03d1 are fixed to 0 and 1 in all experiment of our work. All weights are initialized by a scaled Gaussian distribution w \u223c (scale \u00d7 N(0, 1) + bias). The output target signal is defined by a sinusoidal probability function ptarget(t) = 0.3 + \u22120.3cos(0.03t), and ptarget(t) is converted to spike trains starget(t) following the Bernoulli distribution on each time step, further, starget(t) is converted to continuous currents following a(t) = s(t) \u2217\u03f5. The reason we do such conversion is to guarantee that the single output neuron is possible to fit the target output signal perfectly.\nA.2.2 MNIST\nThe experiment runs on a single RTX-3090 GPU. The MNIST dataset (LeCun, 1998) has 60,000 training images and 10,000 testing images. The training parameters are: batch size = 64, number of training epochs = 200, and learning rate = 0.0005 for the adopted AdamW optimizer (Loshchilov & Hutter, 2017). The images were converted to continuous-valued multi-channel currents. Moreover, data augmentations including RandomCrop and RandomRotation were applied to improve performance (Shorten & Khoshgoftaar, 2019).\n# A.2.3 CIFAR10\nA.2.3 CIFAR10\nThe experiment runs on a single RTX-3090 GPU. The CIFAR10 dataset (Krizhevsky et al., 2009) has 50,000 training images and 10,000 test images. We trained our SNN using BioLeaF for 1200 epochs with a batch size of 50 and a learning rate 0.0005 for the AdamW optimizer (Loshchilov & Hutter, 2017). The same input image coding strategy as the MNIST dataset was adopted. Moreover, data augmentations including RandomCrop, ColorJitter, and RandomHorizontalFlip (Shorten & Khoshgoftaar, 2019) were applied. The convolutional layers were initialized using the kaiming uniform initializer (He et al., 2015), and the linear layers were initialized using the kaiming normal initializer (He et al., 2015).\n# A.3 ARCHITECTURE ILLUSTRATION\nWe provide more figures to illustrate our proposed microcircuit architectures. Figure 6 (a) is another example of our spiking neural networks\u2019 architecture: Two types of cells, Pyramidal cells and Somatostatin (SOM) cells, are needed to realize the forward propagation and the local synaptic plasticity. This example three layers fully-connected (FC) neural network has 2-3-2 pyramidal cells in each layer. The input current signals are from four photoreceptor cells in this example simulating a vision related learning task. (b) to (d) are the disassembled explanation of all synaptic connections. Each neuron has been indexed by its layer and the footnoted position in its layer. The pyramidal cells and SOM cells are one-on-one in each layer except for the 1st layer, where no SOM cells are needed. \u2022 (b) The connections in this sub-Figure (green colored solid arrows) are all feed-forward connections as the same as the weight connections in the more conventional non-spiking artificial neural networks (ANNs). \u2022 (c) The output currents of the pyramidal cells in a layer are connected to the SOM cells in the\nWe provide more figures to illustrate our proposed microcircuit architectures. Figure 6 (a) is another example of our spiking neural networks\u2019 architecture: Two types of cells, Pyramidal cells and Somatostatin (SOM) cells, are needed to realize the forward propagation and the local synaptic plasticity. This example three layers fully-connected (FC) neural network has 2-3-2 pyramidal cells in each layer. The input current signals are from four photoreceptor cells in this example simulating a vision related learning task. (b) to (d) are the disassembled explanation of all synaptic connections. Each neuron has been indexed by its layer and the footnoted position in its layer. The pyramidal cells and SOM cells are one-on-one in each layer except for the 1st layer, where no SOM cells are needed. \u2022 (b) The connections in this sub-Figure (green colored solid arrows) are all feed-forward connections as the same as the weight connections in the more conventional non-spiking artificial neural networks (ANNs). \u2022 (c) The output currents of the pyramidal cells in a layer are connected to the SOM cells in the next layer (orange colored solid arrows). The SOM cells use these signals to predict the firing activity of pyramidal cells in the next layer, so we name these connections as predict connections, and use the footnote p to represent all of the predict-related parameters. The adjustment of these predict connections requires all SOM cells to receive the one-to-one teaching current signal from each SOM cell\u2019s corresponding pyramidal cell (purple colored dashed arrows). \u2022 (d) The top-down feed backward signals (red colored solid arrows), carrying the superposition of output current from the next layer and error information, are connected to the apical dendrites of previous layer\u2019s pyramidal cells. The next layer\u2019s output current signals should be canceled out by the local predicting signals that the next layer\u2019s SOM cells provided (blue colored dashed arrows),\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bb40/bb409770-2bf6-424e-adb7-00595d72703c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Our proposed Microcircuit architecture.</div>\nwhich will leave only error signals on the pyramidal cells\u2019 apical dendrites. Since these two types of connections together generate pyramidal cells\u2019 error information, we use the footnote e to mark all the error related variables. Importantly, the output layer\u2019s pyramidal cells needs additional error signals ei = atarget i \u2212ai connected to its apical dendrites. Such error signals may come from higher to lower brain areas (Leinweber et al., 2017).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2137/21371dd0-4a8b-43ef-a8d8-d9898693bf45.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Our proposed Microcircuit architecture.</div>\nFigure 7 zoomed into one pair of Pyramidal-SOM cells, which has three input pyramidal cells from its previous layer, and forward connected to two pairs of Pyramidal-SOM cells in its next layer. Cells in a same layer are indexed, which is used to indicate synapses connections.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of training spiking neural networks (SNNs) with biologically plausible methods, contrasting with the backpropagation-based (BP-based) methods that dominate the field. Previous approaches have struggled to achieve the same level of accuracy as BP-based methods, necessitating a new breakthrough to enhance the training of SNNs.",
        "problem": {
            "definition": "The primary problem is the difficulty in training spiking neural networks effectively using biologically plausible methods, which have not matched the performance of BP-based techniques.",
            "key obstacle": "The main challenge lies in the biological implausibility of BP-based learning rules, which require neurons to fire two sets of uncorrelated signals from the same neuron body, making it difficult to develop a method that is both effective and biologically plausible."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to reconcile the high accuracy of BP-based methods with a biologically plausible learning framework that mimics the workings of the brain.",
            "opinion": "The proposed idea, BioLeaF, integrates a new microcircuit architecture and learning rules, allowing for the training of multi-layer SNNs in a way that is consistent with biological processes.",
            "innovation": "The key innovation of this work lies in the introduction of a microcircuit architecture that utilizes two types of spiking neurons and four types of synapses, enabling error signal propagation and synaptic weight updates in a biologically plausible manner."
        },
        "method": {
            "method name": "BioLeaF",
            "method abbreviation": "BLF",
            "method definition": "BioLeaF is a bio-plausible learning framework for training spiking neural networks, using a microcircuit architecture and Spike-Timing-Dependent-Plasticity (STDP) based learning rules.",
            "method description": "The core of BioLeaF is a microcircuit architecture that supports supervised learning through local feedback connections and a globally defined spiking error function.",
            "method steps": "The method involves defining the microcircuit architecture, implementing STDP for synaptic weight updates, and propagating error signals through layers to enable learning.",
            "principle": "The effectiveness of BioLeaF is based on the principle that it aligns closely with biological learning mechanisms, allowing for the training of SNNs while maintaining biological plausibility."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using a 2-layer SNN model to fit random target spike trains and benchmarked against datasets including MNIST and CIFAR10, comparing performance with existing BP-based methods.",
            "evaluation method": "Performance was assessed by measuring accuracy on the MNIST and CIFAR10 datasets, with specific training parameters and conditions detailed in the paper."
        },
        "conclusion": "The proposed BioLeaF framework demonstrates comparable learning accuracy to BP-based methods while providing insights into biologically plausible learning mechanisms. The framework bridges the gap between bio-plausible approaches and BP-based techniques, offering new perspectives on neural learning.",
        "discussion": {
            "advantage": "The main advantage of BioLeaF is its ability to achieve high learning accuracy while adhering to biologically plausible principles, potentially providing insights into how learning occurs in biological systems.",
            "limitation": "One limitation of the proposed method is that it may not generalize as well to all types of learning tasks, given its reliance on specific architectures and learning rules.",
            "future work": "Future research could explore further enhancements to the framework, including the integration of additional types of neurons and synapses, as well as testing on a broader range of tasks and datasets."
        },
        "other info": {
            "info1": "The architecture includes pyramidal cells and somatostatin-expressing (SOM) cells, which work together to facilitate error propagation and learning.",
            "info2": {
                "info2.1": "Experiments showed that BioLeaF achieved accuracy comparable to existing BP-based methods on standard datasets.",
                "info2.2": "The framework also emphasizes the importance of understanding biological learning processes to inform artificial intelligence development."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of training spiking neural networks (SNNs) with biologically plausible methods, contrasting with the backpropagation-based (BP-based) methods that dominate the field."
        },
        {
            "section number": "2.1",
            "key information": "The primary problem is the difficulty in training spiking neural networks effectively using biologically plausible methods, which have not matched the performance of BP-based techniques."
        },
        {
            "section number": "4.1",
            "key information": "BioLeaF is a bio-plausible learning framework for training spiking neural networks, using a microcircuit architecture and Spike-Timing-Dependent-Plasticity (STDP) based learning rules."
        },
        {
            "section number": "5.1",
            "key information": "The proposed BioLeaF framework demonstrates comparable learning accuracy to BP-based methods while providing insights into biologically plausible learning mechanisms."
        },
        {
            "section number": "6.2",
            "key information": "One limitation of the proposed method is that it may not generalize as well to all types of learning tasks, given its reliance on specific architectures and learning rules."
        },
        {
            "section number": "7.1",
            "key information": "Future research could explore further enhancements to the framework, including the integration of additional types of neurons and synapses, as well as testing on a broader range of tasks and datasets."
        }
    ],
    "similarity_score": 0.5424329844414789,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0928_,arti/papers/BioLeaF_ A Bio-plausible Learning Framework for Training of Spiking Neural Networks.json"
}