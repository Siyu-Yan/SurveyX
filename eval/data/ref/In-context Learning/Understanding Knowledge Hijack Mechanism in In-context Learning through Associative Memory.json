{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2412.11459",
    "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory",
    "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without fine-tuning by leveraging contextual information provided within a prompt. However, ICL relies not only on contextual clues but also on the global knowledge acquired during pretraining for the next token prediction. Analyzing this process has been challenging due to the complex computational circuitry of LLMs. This paper investigates the balance between in-context information and pretrained bigram knowledge in token prediction, focusing on the induction head mechanism, a key component in ICL. Leveraging the fact that a two-layer transformer can implement the induction head mechanism with associative memories, we theoretically analyze the logits when a two-layer transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of a two-layer transformer align with the theoretical results.",
    "bib_name": "wang2024understandingknowledgehijackmechanism",
    "md_text": "# Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory\n# Shuo Wang* and Issei Sato\u2020 The University of Tokyo\nDecember 17, 2024\n# Abstract\nIn-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without fine-tuning by leveraging contextual information provided within a prompt. However, ICL relies not only on contextual clues but also on the global knowledge acquired during pretraining for the next token prediction. Analyzing this process has been challenging due to the complex computational circuitry of LLMs. This paper investigates the balance between in-context information and pretrained bigram knowledge in token prediction, focusing on the induction head mechanism, a key component in ICL. Leveraging the fact that a two-layer transformer can implement the induction head mechanism with associative memories, we theoretically analyze the logits when a two-layer transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of a two-layer transformer align with the theoretical results.\n# 1 Introduction\nIn recent years, transformer-based models, such as BERT [13] and GPT [42, 43, 5], have achieved remarkable success in natural language processing. Especially, in-context learning (ICL) [14] has emerged as a groundbreaking capability within large language models (LLMs), enabling them to adapt to new tasks without traditional fine-tuning. Instead, these models leverage patterns from a prompt or input sequence, effectively learning \"in context\" by interpreting examples or instructions provided in real-time [33, 56]. The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire\n*wang-shuo3112@g.ecc.u-tokyo.ac.jp \u2020sato@g.ecc.u-tokyo.ac.jp\nthe ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]. In the context of ICL, Jiang et al. [25] discovered a phenomenon called context hijacking. This phenomenon occurs when altering the context disrupts fact recall, causing the model to generate incorrect outputs influenced by the in-context knowledge provided in the prompt. Similarly, ICL can exhibit oversight of in-context knowledge provided in the prompt, where the model relies solely on the knowledge acquired during pretraining (global knowledge) while ignoring the in-context knowledge. These types of knowledge hijacking are thought to arise from either discarding part of the prompt\u2019s information or overly trusting the information in the prompt. Conversely, if the model can (i) comprehensively use the information in the prompt and (ii) balance in-context knowledge and global knowledge, hijacking can be prevented. In relation to the induction head, we demonstrate that employing relative positional encoding (RPE) allows the transformer to comprehensively use the information in the prompt. Furthermore, we show that even a three-layer transformer without positional encoding can achieve comprehensive use of information. Understanding how to avoid knowledge hijacking is essential for the safe and reliable use of ICL capabilities in LLMs and is a matter of significant societal importance. In this paper, we theoretically and experimentally carry out evaluation of oversight of in-context information in a transformer, and the model\u2019s prioritization between in-context knowledge, derived from information within the context or the prompt, and global knowledge from the training data generated from a bigram model with triggered transitions. To summarize, we make the following contributions.\n\u2022 We theoretically show that the induction head learned by a transformer using RPE can avoid oversight of in-context knowledge within the\n\u2022 We experimentally confirmed that the model with RPE can comprehensively use in-context knowledge for the bigram model. We also demonstrated its ability to effectively infer on sequences where the transformer using absolute positional encoding (APE) fails in next-token prediction.\n\u2022 We investigated how the model prioritizes incontext knowledge versus global knowledge when generating outputs, using both theoretical analysis and experimental evaluation.\n# 2 Related work\nAssociative memory Associative memory refers to the storage and retrieval of patterns, and its computational model was designed to retrieve complete memories from sufficient fragments of information in neuroscience [20, 21]. After the emergence of modern Hopfield networks [30], which store patterns in an energy function and retrieve them using an updating rule. Moreover, many studies employ the concept of associative memory in various ways. For instance, Zhang et al. [58] propose a memory unit that estimates a conditional probability distribution with Gaussian kernel smoothing. Some work considers attention blocks as memory because they read and write information flowing through the residual stream [39, 17]. Meng et al. [35] locate factual association patterns in the weight matrix of an MLP. Recently, it has become increasingly popular to regard a matrix as associative memory from a theoretical perspective, as shown in Tab. 1. Previous studies examine scaling laws in relation to model capacity [6], learning dynamics of linear model in terms of particle system [7], and the effect of low-rank approximation of weight matrix to classification involving a noisy token [8] for a linear model. Additionally, Jiang et al. [25] demonstrate that a one-layer transformer can recover latent concept from long enough context. Most relevant to our research is the work by Bietti et al. [4], which mainly discusses the learning dynamics of induction heads for a two-layer transformer, but our primary focus is on the reasoning process.\n# In-context learning and induction head Since\nBrown [5] introduced ICL, theoretical understanding of this phenomenon has been a major interest within the community. For example, Aky\u00fcrek et al. [2] showed that transformer can implement a learning algorithm that solves linear regression. Von Oswald et al. [51] demonstrated that linear self-attention layer can emulate gradient descent. Similarly, many studies\n[1, 34, 59, 11] examined the theoretical connections between ICL and gradient descent. Mechanistic interpretability, which aims to understand the model behavior by reverse-engineering its internal components [37, 53, 10], is another perspective for analyzing ICL [40, 16]. In particular, Elhage et al. [16] discovered that two-layer attention-only transformers understand the previous use of the token and look for similar situations in the context. In addition, Olsson et al. [40] provided empirical evidence that induction heads are the mechanism behind ICL. Moreover, Bansal et al. [3] identified that many induction heads are also among the attention heads deemed critical for ICL. The most relevant study to ours is by Bietti et al. [4], who investigated the learning dynamics of the induction head mechanism in two-layer transformers from the perspective of associative memory. In contrast, our study focuses on the length generalization of the induction head mechanism and the influence of knowledge acquired during training and that obtained in context on the final output.\n# 3 Preliminaries\n# 3.1 Transformer architecture\nA decoder-only transformer consists of a stack of identical layers, each composed of several sub-components that enable it to generate sequential outputs in an autoregressive manner. The decoder generates sequences by predicting one token at a time, conditioned on the previously generated tokens. This process continues until an end-of-sequence token is generated or the maximum sequence length is reached. Large language models commonly adopt this kind of architecture and are trained with next token prediction. In this work, we focus on a two-layer transformer architecture analyzed by Bietti et al. [4], while we use simple relative positional encoding instead of absolute positional encoding.\nCalculation Given a sequence of tokens z1:T of length T \u2208N from the vocabulary set V, each token zt is embedded into a d-dimensional vector and the calculation proceeds as follows:\n(1)\n\u02c6zt+1 = arg max v\u2208V \u03c3(WUxt)v,\nPaper\nModel\nObjective\nTheoretical analysis\nCabannes et al. [6]\nlinear model\nscaling laws\nyes\nCabannes et al. [7]\nlinear model\nlearning dynamics\nyes\nJiang et al. [25]\none-layer transformer\nconcept association\nyes\nChen et al. [8]\nlinear model\nnoisy classification\nyes\nBietti et al. [4]\ntwo-layer transformer\nlearning dynamics\nyes\nMeng et al. [35]\nGPT\nknowledge editing\nno\nOURS\ntwo-layer transformer\nlength generalization\n&\nglobal v.s. in-context\nyes\nble 1: Comparisons among related work and ours in terms of associative memory. The concept of associative emory is widely adopted for theoretical analyses of various phenomena.\nwhere WE \u2208 Rd\u00d7|V| is an embedding matrix, \u03c3 : Rt \u2192 Rt is the softmax function, x(0,k), x(0,q), and x(0,v) are the token embeddings, which may or may not include positional information, \u03a61 \u2208Rd\u00d7d represents W 1 OW 1 V , WF \u2208Rd\u00d7d is a linear projection, WU = (wU(v1), . . . , wU(v|V|))\u22a4\u2208 R|V|\u00d7d is an unembedding matrix, and vi represents the i-th vocabulary in V. Note that Bietti et al. [4] incorporate absolute positional encoding:\nx(0,k) t = x(0,q) t = x(0,v) t = wE(zt) + pt,\nwhere pt is the t-th absolute positional encoding. In contrast, we use a simplified relative positional encoding:\nwhere positional information R1\u2212t:0 = (r1\u2212t, . . . , r\u22121, r0) \u2208 Rd\u00d7t and r\u2212i represents the positional information of the i-th previous token zt\u2212i from the current one zt.\n# 3.2 Bigram model with triggered transitions\nThis section describes the bigram model with triggered tokens used in Bietti et al. [4]. In this bigram language model, transitions between certain \"trigger\" tokens (denoted as qk) are modified in each sequence. Specifically, these trigger tokens are always followed by specific output tokens ok. The model operates over a vocabulary of size V , with the following distributions: \u03c0b(\u00b7 | i) for bigram conditionals, \u03c0u for the unigram distribution, and \u03c0o(\u00b7 | i) to generate output tokens, along with a distribution \u03c0q for trigger tokens. The process involves randomly sampling trigger tokens from \u03c0q. Then, output tokens ok are sampled from \u03c0o, and the sequence z1:T is generated as follows:\n<div style=\"text-align: center;\">Theoretical analysis</div>\n\u2022 Sample q1, . . . , qK \u223c\u03c0q, without replacement (random triggers). \u2022 Sample ok \u223c\u03c0o(\u00b7 | qk), with replacement. \u2022 Generate z1 \u223c\u03c0u and for each subsequent token zt (for t = 2, . . . , T), sample zt | zt\u22121 as follows:\npn(j | i) = \ufffd \u03c0b(j | i), if i /\u2208{qk}, 1{j = ok}, if i = qk.\nMotivation for the model The induction head is a computational mechanism capable of detecting repeated patterns in context and facilitating the output of the tokens that complete these patterns when a trigger token appears. In written texts centered on a particular theme, it is common for identical patterns of tokens to recur multiple times. For instance, in texts about Harry Potter or machine learning, the token \"Avada\" is often followed by \"Kedavra,\" and \"neural\" is typically followed by \"network.\" In a pure bigram model, the token following a given token z is determined by conditional probabilities. As a result, consistent repeated patterns may not emerge, or multiple distinct patterns beginning with z may appear, making it difficult to thoroughly evaluate the function of the induction head. However, by associating specific output tokens with designated trigger tokens, it becomes possible to generate multiple instances of the same pattern within the input sequence. Thus, this approach enables a more effective analysis of the induction head mechanism in scenarios that closely resemble real-world text.\n# 3.3 Associative memory\nAssociative memory plays an important role in analyzing the behavior of memory recall at induction head in the next section. Before defining associative memory, we first adopt a technical assumption commonly\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d9e4/d9e42926-31cb-4adb-9380-9c9e9503796e.png\" style=\"width: 50%;\"></div>\nFigure 1: The visualization of induction head mechanism.\nused in theoretical studies on transformers, that is, that embeddings are high-dimensional random vectors, allowing them to be nearly orthogonal. Mathematically, we impose the following assumption. Assumption 1 (Near-orthogonality [22, 31, 4]). The embeddings (ui)i are d-dimensional vectors with Gaussian entries, each having a unit norm \u2225ui\u2225= 1 and u\u22a4 i uj = 0 for i \u0338= j. Also, W0ui forms a new vector that has a near-unit norm and near-orthogonal to (ui)i if W0 \u2208Rd\u00d7d is a Gaussian random matrix with appropriate entries. With the assumption, we define associative memory. Definition 1 (Associative Memory). Suppose (ui)i and (vj)j are the embeddings satisfying Assumption 1. Then, A matrix is called associative memory when W is expressed as the sum of the products of ui and vj:\nwhere \u03b1i,j \u2208R is the score for the pair (ui, vj).\n  Thanks to orthogonality, we can derive the score \u03b1i,j for the pair (ui, vj) using the operation u\u22a4 i Wvj. Henceforth, we consider the weight matrices in the attention layer and the feed-forward network of the transformer as matrices that implement associative memory.\n# 3.4 Induction head\nInduction heads are a specific type of attention mechanism observed in transformer models, responsible for detecting patterns in token sequences and utilizing previous occurrences of the same token to improve predictions. The behavior of induction heads is particularly\nrelevant for tasks that require understanding and leveraging repeated patterns, such as in language modeling. In Fig. 1, we summarize the process by which the induction head identifies and outputs repetitive patterns in the input. When the prompt ABC . . . A is given, the first-layer attention uses W 1 K to copy information from the previous token to the current token. Consequently, the token B retains the information of A in the form of \u03a61A. Then, in the second-layer attention, the associative memory W 2 K matches the pair \u03a61A with A, attending to the position of token B. As a result, the information of B is stored in the last token A as W 2 V B. Finally, the output matrix W 2 O transforms W 2 V B into WU(B), producing an output that follows the repetitive pattern.\n# 4 Theoretical result\nWe analyze two key aspects of induction heads: the impact of positional encoding on the oversight, and the contributions of in-context information and pretrained knowledge during inference.\n# 4.1 Oversight of in-context knowledge\nIn this section, we address the theoretical aspects of oversight mechanism in the induction head mechanism. In short, for transformers with APE, it can be noted that learning of the previous token head fails, resulting in prediction errors. This issue arises not only with input sequences of lengths that were encountered during training, but also with longer sequences. Formally, we observe the representation of the weight matrix \u02dcW 1 K when weight matrices are sequentially trained, in the order of W 2 O, W 2 K, and W 1 K, through one step of gradient descent. We refer to Theorem 3 from Bietti et al. [4]:\nTheorem 1 (Theorem 3, Bietti et al. [4]). For any t > 1, the learned matrix achieves the following associative memory:\n(6)\nwhere pt is the absolute positional encoding for token position t. From Eq. 6, we can see that the coefficient of pt\u22121p\u22a4 t which constitutes the associative memory \u02dcW 1 K is inversely proportional to t. Therefore, as t increases \u2013 meaning as we move toward the latter part of the\n# 4.2 Global knowledge v.s. in-context knowledge\nsequence\u2013 the function of the associative memory di4.2\nIn contrast, we show that transformers employing RPE consistently direct the previous-token head to attend to the preceding token, regardless of the input sequence length. We first calculate how the transformer\u2019s weight matrices are represented by following the same learning procedure as when using a transformer with APE. Here, we present the training outcome of the matrix W 1 K as associative memory, which plays the most significant role in length generalization by constructing the previous token head. We provide the complete statement and proof in Appendix B.\nTheorem 2 (informal). Under the setup described in Sec. B.2, a two-layer attention-only transformer with relative positional encoding learns the associations by one step of gradient descent for any vocabulary v \u2208V:\nwhere \u03b7 \u2208R is the learning rate, \u03b1, \u03b1\u2032 \u2208R are constants, tq and T are the first and second occurrence positions of trigger token, and V is the size of the vocabulary set V.\nTheorem 2 suggests that the association between r\u22121 and wE(v) is independent of both the vocabulary v and the token position t. Therefore, regardless of the input sequence length, the previous token head will always attend to previous tokens with consistent strength.\nRemark 1. A transformer using RPE learns to attend to the previous token regardless of its position within the sequence. However, if the input sequence contains out-of-distribution tokens, the induction head mechanism does not activate. In such cases, only a transformer with APE can effectively attend to the previous token.\nWe also present the following Proposition that shows a three-layer transformer without positional encoding can implement the induction head mechanism.\nProposition 1. A construction exists for a three-layer transformer without positional encoding that successfully achieves the induction head mechanism.\nHere, we discuss whether to prioritize statistical information obtained from the training data or the information provided in-context. By calculating the logits that determine the output probabilities of the vocabulary in an associative memory transformer, we clarify the mechanisms through which outputs are generated.\nAssociative memory construction achieving induction head We first introduce the following lemma, which states that the induction head mechanism is achievable by setting the appropriate weights for the matrices in a two-layer transformer with APE.\nLemma 1 (Bietti et al. [4]). Define Uv = {u \u2208V | \u03c0b(u | v) > 0} as the support of \u03c0b(u | v) and Q = supp(\u03c0q). The induction head can be achieved by constructing matrices in the following manner.\nwhere I is the identity matrix. The other matrices W 1 O, W 1 V , and W 2 O are a Gaussian random matrix.\nIt is straightforward to prove that a two-layer transformer with RPE also has associative memory construction that achieves induction head by modifying W 1 K = \ufffd k\u2208Q wE(k)r\u22a4 \u22121. Now, using the transformer with RPE architecture, we define an associative memory transformer : a model that incorporates an induction head and a feed-forward network storing knowledge learned through pretraining.\nDefinition 2 (associative memory transformer). A twolayer transformer with RPE is called associative memory transformer if its weight matrices are set as in Lemma 1 except for\nNote that when the probability \u03c0b(u | v) approaches +0, its logarithm log \u03c0b(u | v) diverges to \u2212\u221e. If we admit log \u03c0b(u | v) = \u2212\u221efor u /\u2208Uv in the construction of WF , the transformer does not output u after the token v regardless of the context. This does not align with the actual behavior of the induction head. Also, due to the limitations of floating-point representation in computers, it is not possible to represent \u221edirectly. Thus, we consider \u03f5 as threshold and set \u03c0b(u | v) = \u03f5.\n# Associative memory transformer on limited input\nsequences. First, we present the following proposition, which focuses on two token patterns in the input sequence.\nProposition 2. Suppose a two-layer transformer is an associative memory transformer. Given a length-T sequence z1:T where the last token is zT = q, let the t1-th token be zt1 = v1 following zt1\u22121 = q, and let the t2-th token be zt2 = v2 following zt2\u22121 = q, where v1 \u0338= v2 and (3 \u2264)t1 < t2 without loss of generality. Also assume that there exists only one occurrence for each of v1 and v2 in the context z1:T . Then, the logits \u03bev1 and \u03bev2 can be expressed as follows:\n(7)\n(8)\nWe can observe a general trend from Eqs. 7 and 8. First, if the model contains global knowledge of both tokens v1 and v2, the output token is influenced by the knowledge, in addition to the in-context knowledge. This implies that the occurrence of tokens that rarely appear in the training dataset is suppressed. In our associative memory transformer model, global knowledge never promotes the occurrence of tokens but only suppresses inappropriate occurrence, because we always have log \u03c0b(\u00b7 | q) \u22640. Second, if token patterns, such as q v1 and q v2, do not appear in the training data, global knowledge affects the token logits to the same extent, due to the way WF is defined using the threshold \u03f5 in Def. 2. This indicates that the model intensively suppresses token patterns that were not seen during training. It is worth noting that an associative memory transformer exhibits a significant difference in scale when handling global knowledge and in-context knowledge. For instance, if \u03c0b(v | q) = 0.1, then log \u03c0b(v | q) \u2248 \u22122.3. Even with the functionality of an induction head, it is unlikely to predict the token patterns from the\ncontext. This is avoidable by reformulating W 2 O as in Eq. 12 and adjusting the strength of induction head. We consider this setting in the next paragraph, along with more general input sequences. If neither token appears during training, the difference between the two logits before applying the softmax function \u03be\u2032 v2 \u2212\u03be\u2032 v1 is calculated as follows:\n(9)\nThe result implies that the token v2 is more likely to be generated if t1 is large and the distance between the tokens v1 and v2, i.e., t2 \u2212t1 is small. This trend arises from the first layer of the attention block, where the attention is not fully concentrated on the previous token, resulting in a diffusion of attention across other tokens as well. In other words, the more tokens that precede the current token, the lower the attention score assigned to the current token.\nquences. Next, we introduce a more sophisticated analysis of in-context and global knowledge by relaxing assumptions in the sequence z1:T . The transformer considered here has associative memory with larger scores, characterized by a weighted sum of the individual terms of the weight matrices in Sec. 3.4, where each term is scaled by an appropriate coefficient. We consider a special case in which each term has the same coefficient \u03c41, \u03c42, \u03c43 \u2208R. Definition 3 (stronger associative memory transformer). A stronger associative memory transformer is equipped with the following weight matrices:\nDefinition 3 (stronger associative memory transformer). A stronger associative memory transformer is equipped with the following weight matrices:\n(10)\n(11)\n(12)\nwith the parameters \u03c41, \u03c42, \u03c43 \u2208R+. The other weight matrices remain the same.\nwith the parameters \u03c41, \u03c42, \u03c43 \u2208R+. The other weight matrices remain the same. Proposition 3. Suppose a two-layer transformer is a stronger associative memory transformer as defined in Eqs. 10, 11, and 12. Given a length-T sequence z1:T where the last token is zT = q, let f(v) be the number of token pattern \"q v\" appearing in the sequence for vocabulary v \u2208V. For sufficiently large \u03c41 and \u03c42, the logits \u03bev can be expressed as follows:\n(13)\n\ufffd\ufffd \ufffd It can be observed that the transformer equipped with a general associative memory is affected by the proportions of each token pattern present in the context as well as by the global knowledge. In this case, it can be concluded that the information regarding the positions where token patterns are observed does not influence the final logit. We can easily derive the following properties. Corollary 1. Given a length-T sequence z1:T where the last token is zT = q and the only token patterns qv1 and qv2 appear f(v1) and f(v2) times, respectively. The transformer outputs one of the following: arg maxv\u2208V\\{v1,v2} \u03c0b(v | q), v1, or v2. For vocabulary v \u0338= v1, v2, the logit \u03bev is determined only by log \u03c0b(v | q) and this means that the candidate for the next token is arg maxv\u2208V\\{v1,v2} \u03c0b(v | q). On the other hand, The logits for v1 and v2 have the other term, which makes them other candidates for the next token. Overall, Corollary 1 states that the model either uses global bigram knowledge or in-context knowledge, depending on the bigram conditionals \u03c0b(\u00b7 | q) and the frequency of token patterns qv1 and qv2 in the context.\n\ufffd\ufffd \ufffd It can be observed that the transformer equipped with a general associative memory is affected by the proportions of each token pattern present in the context as well as by the global knowledge. In this case, it can be concluded that the information regarding the positions where token patterns are observed does not influence the final logit. We can easily derive the following properties.\n# 5 Experiments\nHenceforth, we denote by TFape, TFrpe and TForc twolayer transformer with APE, RPE, and oracle two-layer transformer parameterized by \u03c41, \u03c42 as in Def. 3, respectively. Note that we construct TForc by setting appropriate weights for each matrix in the transformer. We used the two-layer transformer architecture TFape constructed in Sec. 3.1.\n# 5.1 Oversight of in-context knowledge\nTraining process of previous token head We investigated whether the training of transformers directs attention towards the previous token. Specifically, TFrpe and TFape were trained using inputs of length 256 generated according to the sequence generation rule. Details regarding the hyperparameters and sequence generation process are provided in Appendix D. For an associative memory \ufffd (i,j)\u2208M uiv\u22a4 j , we employed the memory recall metric [12, 18, 4]:\nFor example, TFrpe calculates the attention with W 1 K = \ufffd k\u2208Q wE(k)r\u22a4 \u22121, and W 1 Q = I in the first\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6c3e/6c3e769e-0084-4002-99f1-fea21ab5010c.png\" style=\"width: 50%;\"></div>\nFigure 2: Comparison of a two-layer transformer with APE and RPE in capturing previous token information. Even when trained with sequences of length 256, the attention block in the first layer of TFAPE fails to attend to the previous token at positions t > 128, while TFRPE attends to previous tokens regardless of the positions.\nlayer attention block. We want each vocabulary wE(k) to be paired with the information of the previous position r\u22121, not other positions r\u2212i where i \u0338= 1. Hence, R(W 1 K) is computed as\nFor TFape, we similarly have\nwhere T is the maximum input length for TFape. As we can see from Fig. 2, over 90% of desired associations are stored in W 1 K for TFrpe, while TFape fails to remember most of the associations at positions 128 < t < 256 even though the model was trained with sequences of length 256.\nDiscussion One of the worst-case scenarios for TFape occurs when a trigger token does not appear by chance in the range t \u2264128 and only appears for the first time at t > 128. Although such a sequence is contrived, its probability of occurrence is non-zero. For such sequences, models with an incomplete previous token head fail to associate the trigger token\u2019s information with the output token, resulting in incorrect predictions without leveraging in-context information. In contrast, TFrpe, which links words to their relative positions, ensures that the in-context information is not overlooked, regardless of where the trigger token first appears, thereby enabling correct predictions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9159/9159b1b9-536f-4d95-addb-56efea1e9641.png\" style=\"width: 50%;\"></div>\nFigure 3: Ratio of predicting B1 and B2 given the prompt \"A B1 , . . . , A B1 , A B2 , . . . , A B2 , A\". The transformer shows an increasing tendency to complete the context with B1 as the frequency of token pattern A B1 in the context grows.\n# 5.2 Global knowledge v.s. in-context knowledge\nTo validate the results derived from the theory, we conducted experiments using real data as well as data generated by the bigram model.\nAnalogical reasoning task We utilize the analogical reasoning task, one of the most practical tasks for bigram analysis. Specifically, we focus on the \"capital-world\" analogy type of questions from the Google Analogy Dataset [36], which contains a total of 19,544 questions. This subset includes 232 unique vocabulary items. Denoting a word A (e.g., Tokyo) and its corresponding analogy counterpart A\u2217 (e.g., Japan), the prompts were constructed in the form A A\u2217, B B\u2217, C C\u2217, . . . , with pairs separated by commas. Using these generated prompts, we trained TFrpe. For details on the training and dataset setup, please refer to Appendix D.\nCollision of context information As shown in Corollary 1, the model predicts either a token based on global knowledge or the token that appears in the context. To confirm this, we randomly sampled A from the capitals in the Google Analogy Dataset and constructed prompts using word pairs A B1 and A B2 that were not learned as global knowledge. The resulting prompts were of the form A B1 , . . . , A B1 , A B2 , . . . , A B2 , A, which were then used for predicting the next token. We generated 1,000 prompts and calculated the proportion of cases where either B1 or B2 was predicted, while controlling\nthe frequency of B1 and B2 in the context. The result is shown in Fig. 3. As the number of token pattern A B1 increases, the model predicts B1 as the next token more frequently. Furthermore, it can be observed that the prediction trend reverses when the frequency of B1 and B2 in the context is nearly equal. Additionally, we observe from global ratio that the model predicted some vocabulary B from global knowledge. Especially, the figure illustrates that it is more likely to output global knowledge when the number of B1 and B2 are the same, or when the number of B1 or B2 is almost maximum. Please refer to Appendix E.2 for further discussion on these phenomena.\n# 6 Conclusion\nWe analyzed the influence of interaction between global bigram and in-context knowledge to the twolayer transformer through the lens of associative memory. We theoretically and empirically verified that relative positional encoding enables transformers to generalize longer sequences, and that how the next token prediction is conducted within the transformer that was trained to store global knowledge and to have induction head.\n# 7 Limitations\nOur study has certain limitations that should be acknowledged. First, the analysis presented in this work is conducted using a two-layer transformer model. While this allows for a controlled and interpretable exploration of the underlying mechanisms, the findings may not directly generalize to LLMs, which typically involve significantly more layers and complex interactions. Second, while our focus on induction heads provides valuable insights into in-context learning ICL, it is important to note that ICL leverages additional computational circuits beyond induction heads. As such, our explanation captures only part of the broader mechanisms underlying ICL, leaving room for further investigation into other contributing factors.\n# 8 Ethics statement\nThis work is purely theoretical, supported by controlled experiments designed to validate the proposed analyses. No personal or sensitive data were involved at any stage of this research. All experiments were conducted using datasets that are publicly available, ensuring compliance with ethical standards for data usage.\n[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36:45614\u201345650, 2023.\n[3] Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, and Dan Roth. Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11833\u201311856, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.660. URL https://aclanthology. org/2023.acl-long.660.\n[4] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. Advances in Neural Information Processing Systems, 36, 2024. [5] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [6] Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti. Scaling laws for associative memories. arXiv preprint arXiv:2310.02984, 2023. [7] Vivien Cabannes, Berfin Simsek, and Alberto Bietti. Learning associative memories with gradient descent. arXiv preprint arXiv:2402.18724, 2024. [8] Lei Chen, Joan Bruna, and Alberto Bietti. How truncating weights improves reasoning in language models. arXiv preprint arXiv:2406.03068, 2024. [9] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. arXiv preprint arXiv:2409.10559, 2024.\n[9] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. arXiv preprint arXiv:2409.10559, 2024.\n[10] Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri\u00e0 GarrigaAlonso. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems, 36:16318\u2013 16352, 2023. [11] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 4005\u20134019, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.findings-acl.247. URL https:// aclanthology.org/2023.findings-acl.247. [12] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. arXiv preprint arXiv:2209.02535, 2022. [13] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. A survey on in-context learning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1107\u20131128, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.64. URL https://aclanthology. org/2024.emnlp-main.64. [15] Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: Incontext learning markov chains. arXiv preprint arXiv:2402.11004, 2024. [16] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. [17] Dan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs, 2023. URL https://arxiv.org/abs/2306.01128.\n[18] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023. [19] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. [20] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982. [21] John J Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the national academy of sciences, 81(10):3088\u20133092, 1984. [22] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023. [23] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position embeddings. arXiv preprint arXiv:2009.13658, 2020. [24] Samy Jelassi, St\u00e9phane d\u2019Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran\u00e7ois Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023. [25] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Do llms dream of elephants (when told not to)? latent concept association and associative memory in transformers. arXiv preprint arXiv:2406.18400, 2024. [26] Andrej Karpathy. The unreasonable effectiveness of recurrent neural networks, May 2015. URL https://karpathy.github.io/ 2015/05/21/rnn-effectiveness/. Accessed: 2024-12-15. [27] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024. [28] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv preprint arXiv:2006.15595, 2020.\n[29] Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. arXiv preprint arXiv:2109.05644, 2021. [30] Dmitry Krotov and John J Hopfield. Dense associative memory for pattern recognition, 2016. URL https://arxiv.org/abs/1606.01164. [31] Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of next token prediction with self-attention. In International Conference on Artificial Intelligence and Statistics, pages 685\u2013693. PMLR, 2024. [32] Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. Cape: Encoding relative positions with continuous augmented positional embeddings. Advances in Neural Information Processing Systems, 34:16079\u201316092, 2021.\n Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Eneko Agirre, Marianna Apidianaki, and Ivan Vuli\u00b4c, editors, Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https:// aclanthology.org/2022.deelio-1.10.\n[38] Masato Neishi and Naoki Yoshinaga. On the relation between position information and sentence length in neural machine translation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 328\u2013 338, 2019. [39] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024. [40] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. [41] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. [42] Alec Radford. Improving language understanding by generative pre-training. 2018. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified textto-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020. [45] Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Quanshi Zhang, Xipeng Qiu, and Dahua Lin. Identifying semantic induction heads to understand in-context learning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 6916\u20136932, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.412. URL https://aclanthology. org/2024.findings-acl.412. [46] Anian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023.\n[46] Anian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023.\n[47] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018. [48] Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, and Adina Williams. The curious case of absolute position embeddings. arXiv preprint arXiv:2210.12574, 2022. [49] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [50] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [51] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023. [52] Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding word order in complex embeddings. arXiv preprint arXiv:1912.12333, 2019. [53] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. [54] Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9840\u20139855, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.609. URL https: //aclanthology.org/2023.emnlp-main.609. [55] Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. Advances in Neural Information Processing Systems, 36, 2024. [56] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In\n[56] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423\u20131436, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.79. URL https:// aclanthology.org/2023.acl-long.79. [57] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. [58] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, and L\u00e9on Bottou. Memory mosaics, 2024. URL https://arxiv.org/abs/ 2405.06394. [59] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models incontext. arXiv preprint arXiv:2306.09927, 2023. [60] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does incontext learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023. [61] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. Length extrapolation of transformers: A survey from the perspective of position encoding. arXiv preprint arXiv:2312.17044, 2023.\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423\u20131436, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.79. URL https:// aclanthology.org/2023.acl-long.79.\n# A Additional related work\nIn-context learning and induction head Bayesian inference is also a popular perspective for comprehending ICL. Xie et al. [57] and Wies et al. [55] investigated how a pretrained transformer implicitly discovers the underlying concept or latent task of a prompt by leveraging a mixture of distributions. An alternative line of work [60] relates the Bayesian model averaging to the attention mechanism. Wang et al. [54] studies how information is aggregated to label words in ICL. In terms of two-layer transformer, Chen et al. [9] studied ICL on n-gram Markov chain, proving that gradient flow optimization under the cross-entropy loss leads to a model that exhibits a generalized induction head mechanism. Edelman et al. [15] showed two-layer attention-only transformers learn induction heads that estimate the next token\u2019s conditional probability based on prior occurrences, enabling near Bayes-optimal performance. Similarly, Nichani et al. [39] showed that two-layer transformers learn causal structure via gradient descent, suggesting that induction head is a special case of this structure. Ren et al. [45] discovered a more generalized version of induction heads, which are referred to as semantic induction heads.\n# Length generalization and positional encoding\nLength generalization and positional encoding Although we point out the oversight of in-context information by a transformer with APE, this can be considered as the failure of length generalization. Since the introduction of the Transformer architecture [50], there has been a wealth of research on positional encoding [61]. Originally, Vaswani [50] adopted sinusoidal positional encoding that transforms the tokens\u2019 absolute positions. Subsequently, Kiyono et al. [29] and Likhomanenko et al. [32] proposed shifting absolute sinusoidal positional encoding during training to achieve shift invariance. Wang et al. [52] extends word embedding as continuous functions with respect to token position, leading to smooth word representations. Meanwhile, it is demonstrated that RPE is superior to APE for longer sequence generalization in various tasks [38, 32, 23, 24]. Additionally, Sinha et al. [48] questioned whether models can learn the relative distance between words when equipped with APE. The first RPE was formulated by Shaw et al. [47], where a trainable encoding is added to the key before computing the dot product with the query. The transformer in our work also adopts this type of positional encoding, while we fix the positional encodings to randomly initialized vectors. A variety of different RPE methods were presented [23, 28] until now, but DeBERTa [19], RoPE [49], T5-Bias [44], and ALiBi [41] are the\nmost popular choices. Ruoss et al. [46] suggested that randomizing absolute and relative positional encoding enhance the model\u2019s ability to generalize to input sequences of lengths not encountered during training, but they used the encoder-ony Transformer, which is different from ours.\n# B Learning of ideal transformer\n# B.1 Notation table\nWe prepare tab. 2, which provides a concise summary of the notations used throughout the appendices.\n# B.2 Setup\nHere we consider a simplified setting to analyze the influence of relative positional encoding in the training dynamics of our two-layer attention only transformer. Specifically,\n1. Input Sequence We consider an input sequence z1:T \u2208VT which has one trigger token q appearing twice, and ends with the trigger token. In other words, let tq be the first occurrence position. Then, we have ztq = zT = q. From the bigram generation rule in Sec.3.2, ztq+1 and zT+1 are the output token.\n2. Probability Distribution Assumptions: The bigram is generated by uniform distributions over [V ] for any index i, i.e., \u03c0u, \u03c0q, \u03c0o, and \u03c0b(\u00b7 | i) are uniformly distributed.\n3. Simplification of Loss Function: Consider the loss only for sequences of length T where the final input token zT is the second occurrence of the trigger token, and the label y is zT+1, the corresponding output token.\n4. Simplification for Learning Focus: Our approach involves sequentially training W O 2 , W K 2 , and W K 1 from top to bottom. We employ zeroinitialization and carry out a single gradient descent step.\n# 5. Initialization and Freezing: To achieve our goal\nof showing that W 2 O, W 2 K and W 1 K learn to be an associative memory, we zero-initialize the three matrices. For other matrices such as W 2 V , W 1 V and W 1 O are randomly initialized from Gaussian distribution. We set W 1 Q and W 2 Q to identity matrix. All these matrices except for W 2 O, W 2 K and W 1 K are freezed during the training process.\nSymbol\nDescription\nd\nDimensionality of the positional encoding\nV\nVocabulary set\nV\nVocabulary size\nwE(zt)\nEmbedding of token zt\nrs\u2212t\nRelative positional encoding expressing the relation between s-th token and current one\n\u03c3\nsoftmax function\n\u03a61\nproduct of two randomly initialized matrices W 1\nO and W 1\nV\ntq\nIndex of the first trigger token\nto\nIndex of the output token corresponding to the first trigger token, to = tq + 1\nT\nIndex of the second trigger token, or the input length\n[N]\na set of natural number up to N, {1, 2, . . . , N}\nl\ncross entropy loss function\n\u03b7\nlearning rate\n\u03c4\nconstant, defined by Eto[\ufffdT\nt=to 1/t]\n\u03b1\nconstant, \u03b1 = \u03b7/V T\n\u03b1\u2032\nconstant, \u03b1 = \u03b7\u03c4/V T\nq\na variable representing the trigger token\n\u03c0u\na uniform distribution from which z1 is sampled.\n\u03c0q\na uniform distribution by which the trigger token is determined.\n\u03c0o\na uniform distribution by which the output token is determined.\n\u03c0b\na uniform distribution from which z2:T is sampled conditioned on the previous token.\n<div style=\"text-align: center;\">Table 2: Table of Notations</div>\n# B.3 Theoretical analysis of learning\nTheorem 3 (formal). Under the setup described in Sec. B.2, a two-layer attention only transformer with relative positional encoding learns associations of the associative memory transformer, i.e., the memories W 2 O, W 2 K and W 1 K can be written as the sum of the terms that constitute the associative memory transformer.\n(14)\n(15)\nW 2 O\n(16)\nwhere \u03c7, \u03c8 : V \u2192R and \u03c9 : N \u2192R denotes the learned score of each association, and (W 1 K)\u2032, (W 2 K)\u2032 and (W 2 O)\u2032 are the other associations. Proof. We will demonstrate that associative memory can be learned by performing one gradient descent step in a top-down manner, starting with W 2 O, followed by\nProof. We will demonstrate that associative memory can be learned by performing one gradient descent step in a top-down manner, starting with W 2 O, followed by\nO We begin with the dynamics of W 2 O. The input to the second layer attention, x(1) t , is composed of the sum of the values from the first layer attention, x(1,0) t , and the values from the residual connection, x(1,1) t . Since W 1 K is initialized to zero, the attention of the first layer is evenly distributed across all the tokens z1:t and we have\n\ufffd where \u03a61 = W 1 OW 1 V . Additionally, the residual stream carries the token embedding:\nSince the matrix W 2 K is also zero-initialized, the value of the second layer attention can be computed through\nNow the logit prediction is given by WU(W 2 Ox(2) T + x(1) T ) with the residual connection. We note that when d is large, thanks to the near-orthogonality, WUx(1) T = WU \ufffd\ufffdT s=1 \u03a61wE(zs)/T + wE(zT ) \ufffd is negligible because x(1) T does not contain wU(v) for\nany v \u2208V. This enables us to use Lemma 2 and one gradient step with learning-rate \u03b7 yields\nW 2 O\n(17)\nThis transformation comes from the assumption that every token is sampled from a uniform distribution (p(y = k) = 1/V ) and all the logits are 0 when W 2 O = Od\u00d7d, which means that \u02c6pW (k | x) = 1/V . Given that y = k, it holds that the first output token zto = k. For zi (i \u0338= to), the uniform distribution can possibly generate any token, independently of the condition y = k. Since we have\n(18)\n(19)\nwhere \u03c4 := E \ufffd\ufffdT t=to 1 t \ufffd . Since each token is generated from a uniform distribution, the expected value of wE(zto), conditioned on nothing, is given by:\n\ufffd\ufffd \ufffd ated from a uniform distribution, the expected value of wE(zto), conditioned on nothing, is given by:\nCombining eq. 17 and 18, the near-orthogonality of Gaussian vectors gives us\n(20)\n(21)\n\ufffd \ufffd This is the same result as in the proof from Bietti et al. [4]. So far, the weight matrix W 2 O is trained to perform as an associative memory so that\n(22)\nwith a constant \u03b1 = \u03b7 TV . Step2: Training of W 2 K Since the training of W 2 O is finished, we have Eq. 20 When W 1 K = W 2 K = Od\u00d7d, Lemma 4 demonstrates that the next token prediction is distributed among all tokens, which means that \u02c6p(v | x) = 1/V for any vocabulary v. According to Lemma 5, one step of gradient descent yields\nW 2 K\n(23)\n\ufffd with \u00afx(1) = 1 T \ufffdT t=1 x(0) t .\n\ufffd We simplify our transformer architecture by setting x(1,1) t as the queries and values, and x(1,0) t as the keys. Furthermore, we add the condition that the trigger token ztq = j and rewrite eq. 23. This conditioning leverages the fact that each token in ztq is sampled from a uniform distribution. Now we have W 2\nW 2 K\n\ufffd where \u00afx(1,0) = 1 T \ufffdT t=1 x(1,0) t . Using the formula 22 we obtain\nW 2 K\n(24)\n\ufffd The sum in \u2206k,j can be partitioned into three distinct groups: (i) \u2206o k,j, where t is the index of the output token; (ii) \u2206q k,j, where t corresponds to the indices of the trigger tokens; and (iii) \u2206r k,j, which includes all other cases. Mathematically, let to \u22652 be a random variable, and tq = to \u22121, we write\nwhere Tq = {tq, T} and Tr = {1 \u2264i \u2264T | i \u2208 N, i \u0338= to, i \u0338= tq, i \u0338= T}. F n\nFirst, we will take a look at \u2206o k,j. Note that E[1{zto = k} | y = k, ztq = j] = 1 and E[1{zto = k} | ztq = j] = 1/N, so we find that\n(25)\nwhere ak,j,i is the coefficient of \u03a61wE(i) by calculating\nThe analysis of ak,j,i is done by the work [4]:\n\ufffd \ufffd Thus, we have the following approximation:\n(26)\n(27)\nStep3: Training of W 1 K So far, the gradient descent step as to W 2 O and W 2 K gives them the ability to behave as an associative memory:\n(28) (29)\nFrom Lemma 3, we admit that the model predicts the next token almost randomly, i.e., \u02c6p(v | x) = 1/V for\nany vocabulary v, when W 0 K = Od\u00d7d. This enables us to employ Lemma 5 and W 1 K after one gradient descent step gives the following:\nW 1 K\n\ufffd with qs,t = (wE(zs) + rs\u2212t) \u2212( \u00afwE(z1:t) + \u00afr1\u2212t:0) and \u00afwE(z1:t) = 1 t \ufffdt i=1 wE(zi). Now, fix an arbitrary vocabulary v \u2208V. Given that wU(k)\u22a4\u03a62wE(zt) = \u03b11{zt = k} and that (\u03a61wE(zs))\u22a4W 2 KwE(zT ) = \u03b1\u20321{zs = zT }, which equals \u03b1\u2032 when s = tq or s = T, we manipulate the expression:\nW 1 K\n(30)\n(31)\nWe will further split these four terms based on relative positional encoding and token embedding that are contained in qtq,t. For example, we decompose At,k into AR t,k and Az t,k by defining AR t,k\nWe will further split these four terms based on relative positional encoding and token embedding that are contained in qtq,t. For example, we decompose At,k into AR t,k and Az t,k by defining\n E \ufffd { }{ \u2264} \ufffd \u2212 \u2212\u2212 \ufffd \u00d71{zt = v} | y = k] , Az t,k = E \ufffd 1{zt = k}1{tq \u2264t} \ufffd wE(ztq) \u2212\u00afwE(z1:t) \ufffd \u00d71{zt = v} | y = k] . (i) when v = k We begin with AR t,k: AR t,k = E \ufffd 1{zt = k}1{tq \u2264t} \ufffd rtq\u2212t \u2212\u00afr1\u2212t:0 \ufffd \u00d71{zt = v} | y = k] = t \ufffd s=1 P(tq = s | y = k) \u00d7 E [1{zt = k} | y = k, tq = s] (rs\u2212t \u2212\u00afr1\u2212t:0) = P(tq = t \u22121)(r\u22121 \u2212\u00afr1\u2212t:0) + 1{t = T}P(tq = t)(r0 \u2212\u00afr1\u2212t:0) + 1{t = T} V \ufffd s\u2208[t\u22122]\u222a{t} P(tq = s)(rs\u2212t \u2212\u00afr1\u2212t:0) = P(tq = t \u22121)(r\u22121 \u2212\u00afr1\u2212t:0) + 1{t = T}P(tq = t)(r0 \u2212\u00afr1\u2212t:0) + O \ufffd1 V \ufffd . Here, we used Lemma 6 and 7 for the calculation of the expectation E[1{zt = k} | \u00b7]. Similarly, we will examine BR t,k, CR t,k and DR t,k as well. BR t,k = E [1{zt = k}1{tq \u2264t} \u00d7 \ufffd rtq\u2212t \u2212\u00afr1\u2212t:0 \ufffd 1{zt = v} \ufffd = t \ufffd s=1 P(tq = s) \u00d7 E[1{zt = k} | tq = s](rs\u2212t \u2212\u00afr1\u2212t:0) = t \ufffd s=1 P(tq = s) V (rs\u2212t \u2212\u00afr1\u2212t:0) = O \ufffd1 V \ufffd . For the term CR t,k, we make use of Lemma 8 and 9. CR t,k = E \ufffd 1 T T \ufffd t\u2032=1 1{zt\u2032 = k}1{tq \u2264t} \u00d7 \ufffd rtq\u2212t \u2212\u00afr1\u2212t:0 \ufffd 1{zt = v} | y = k \ufffd = 1 T t \ufffd s=1 P(tq = s)(rs\u2212t \u2212\u00afr1\u2212t:0)\n= E \ufffd 1{zt = k}1{tq \u2264t} \ufffd wE(ztq) \u2212\u00afwE(z1:t) \ufffd \u00d71{zt = v} | y = k] .\nHere, we used Lemma 6 and 7 for the calculation of the expectation E[1{zt = k} | \u00b7]. Similarly, we will examine BR t,k, CR t,k and DR t,k as well.\nFor the term CR t,k, we make use of Lemma 8 and 9.\n= E \ufffd 1{zt = k}1{tq \u2264t}(xtq \u2212\u00afx1:t)1{zt = v} \ufffd = E \ufffd 1{zt = k}1{tq \u2264t}xtq \ufffd \u2212E [1{zt = k}1{tq \u2264t}\u00afx1:t] = P(zt = k)E[1{tq \u2264t}xtq | zt = k] \u2212P(zt = k)E [1{tq \u2264t}\u00afx1:t | zt = k] \ufffd \ufffd\nNow we move to Cz t,k. From lemma 12 and 13, it follows that\nFor Dz t,k, we have\nbecause of the term 1{zt = v} under no condition. So far, we have calculated At,k, Bt,k, Ct,k and Dt,k for v = k. Now, we turn our attention to v \u0338= k.\n(ii) when v \u0338= k In this case, it is guaranteed that 1{zt = k} \u0338= 1{zt = v}, and thus, we have At,k = Bt,k = 0. We only need to examine Ct,k and Dt,k. We use the fact that the token zt has to be sampled uniformly from a potentially reduced vocabulary space with some words excluded depending on the position of t, we obtain\nand\nSo far, we have seen the content of At,k, Bt,k, Ct,k and Dt,k. Now, we will shift our focus to A\u2032 T,k, B\u2032 T,k, C\u2032 T,k and D\u2032 T,k. We recall that qs,t = (wE(zs) + rs\u2212t) \u2212( \u00afwE(z1:t) + \u00afr1\u2212t:0). By substituting s = T and t = T, we have qT,T = (wE(zT ) + r0) \u2212( \u00afwE(z1:T ) + \u00afr1\u2212T:0). Also, 1{zT = v} implies that the trigger token is v. we use Lemma 14 and obtain the following: A\u2032 T,k = E [1{zT = k}qT,T 1{zT = v} | y = k] = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 P(tq = T \u22121) (r0 \u2212\u00afr1\u2212T:0) +P(tq = T \u22121) T\u22122 T wE(k) +O \ufffd1 V \ufffd if v = k, 0 otherwise.\nSimilarly to the argument of Bt,k and Dt,k, the trigger token is chosen uniformly, and thus, we have P(zT = v) = 1 V . This gives\nFinally, according to Lemma 15, it is established that C\u2032 T,k\nif v = k, otherwise.\n\uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \ufffd \ufffd To sum up, we get the following associative memory behaviors by examining Eq. 30. We point out that \u00afr1\u2212t:0 also contains r\u22121 and r0.\nand\n\ufffd Also, for any j \u2208V, we have\n\nThe proof is finished.\n# C Other theoretical proofs\nLemma 2 (Lemma 2, Bietti et al. [4]). Given a data distribution p over pairs (x, y) \u2208Rd \u00d7 [V ] and a fixed matrix WU \u2208Rd\u00d7d, minimizing the loss L(W) = E(x,y)\u223cp [\u2113(y, WUWx)] gives the following gradient:\nwhere \u00b5k = E[x | y = v] and \u02c6\u00b5v = Ex[ \u02c6pW (v|x) p(y=v) x], and \u02c6pW (v | x) is the probability of generating v. Lemma 3. Consider a two-layer attention only transformer that meets the initialization condition in Sec. B.2, and the input sequence of length T ends with the trigger token zT = q at its second occurrence. Even after the gradient descent step of W 2 O and W 2 K, the prediction \u02c6p(v | z1:T ) = 1/V for any v \u2208V. Proof. From the way the transformer is initialized, we have W 1 K = Od\u00d7d. Therefore, the output of the first attention block for any t is:\n\ufffd In the second layer, the key matrix is trained such that (\u03a61wE(zt))\u22a4W 2 KwE(zT ) = \u03b1\u20321{zt = zT }. Thus, we have \u03c3 \ufffd (x(1) 1:T )\u22a4(W 2 K)\u22a4W 2 Qx(1) T \ufffd \uf8eb \uf8eb \uf8f6\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1d7/d1d76522-3c7b-40a4-9f62-b37ab348b6be.png\" style=\"width: 50%;\"></div>\nWhen V is large enough, the attention is spread across the whole sequence evenly, and we obtain\n<div style=\"text-align: center;\">When V is large enough, the attention is spread across the whole sequence evenly, and we obtain</div>\n(32)\n(33)\nwhere we focus on the terms represented by wU(v) for vocabulary v by =wU . Taking the expectation over z1:T while keeping the trigger token zT = zto\u22121 = q fixed yields\nThe coefficients of wU(v) are the same for any v \u2208V and this concludes the proof.\nLemma 4. Consider a two-layer attention only transformer that meets the initialization condition in Sec. B.2, and the input sequence of length T ends with the trigger token zT = q at its second occurrence. After the gradient descent step of W 2 O, the prediction \u02c6p(v | z1:T ) = 1/V for any v \u2208V.\nProof. Since W 2 K = Od\u00d7d, it is guaranteed that we have Eq. 33 as an equation. Thus, the result follows directly from Lemma 3.\n# Lemma 5. Suppose the loss function is given by\nL(W) = E(X,y)[l(y, \u03be(X))] \u03be(X) = WU\u03a62X\u00af\u03c3(Z(W)\u22a4W2xT ),\nwhere Z(W) = (z1(W), . . . , zT (W)) with zt(W) = \ufffdt s=1 \u03a61xs\u03c3((x1:t + R1\u2212t:0)\u22a4Wxt)s, and \u00af\u03c3(u1:T )t = 1 T (1 + ut \u2212 1 T \ufffdT s=1 us) is the linearization of the softmax function around 0. Then the gradient at W = 0 has the following form.\n\u2207W L(W) \ufffd\nwhere qs,t = (xs + rs\u2212t) \u2212(\u00afx1:t \u2212\u00afr1\u2212t:0) and \u00afx1:t = 1 t \ufffdt s=1 xs. Proof. The result is obtained by replacing p\u22a4 1:t with (x1:t + R1\u2212t:0)\u22a4and pt with xt in zt from the proof of Lemma 5 from Bietti et al. [4]. It is worth noting that xi = x(0) i = wE(zi) and that if we directly follows the transformer architecture the vector in the linearized version of\nsoftmax \u00af\u03c3 should be Z\u2032(W)\u22a4W2z\u2032 T (W), where Z\u2032(W) = (z\u2032 1(W), . . . , z\u2032 T (W)) with z\u2032 t(W) = xt + \ufffdt s=1 \u03a61xs\u03c3((x1:t + R1\u2212t:0)\u22a4Wxt)s. Since W 2 K is learned so that it only cares the ordered pair of \u03a61wE(zt) and wE(zt), the loss function is simplified and ignores the other terms. This also applies to \u03be(X), where \u03be\u2032(X) = WU\u03a62Z\u2032\u00af\u03c3(Z\u2032(W)\u22a4W2z\u2032 T ) is the authentic form of the output. Since we know W 2 O (and therefore \u03a62) is already trained, we can simplify the expression.\n# C.1 Supporting lemmas\nThis section provides useful lemmas to show the learning of matrices, especially for W 1 K.\nLemma 6. Given a sequence z1:T that satisfies Sec. B.2. For any t \u2208[1, T], and any vocabulary k \u2208V, we have\nE[1{zt = k} | y = k, tq = t \u22121] = 1.\nProof. Since the trigger token is ztq = zt\u22121, the next token zt is the output token. The condition y = k gives us zt = k. Therefore, 1{zt = k} = 1 and the equation holds.\nProof. Since the trigger token is ztq = zt\u22121, the next token zt is the output token. The condition y = k gives us zt = k. Therefore, 1{zt = k} = 1 and the equation holds. Lemma 7. Given a sequence z1:T that satisfies Sec. B.2. Fix some t \u2208[1, T] \u2282N. For any s \u2208[1, t \u22122] \u222a{t}, and vocabulary k \u2208V, we have\nLemma 7. Given a sequence z1:T that satisfies Sec. B.2. Fix some t \u2208[1, T] \u2282N. For any s \u2208[1, t \u22122] \u222a{t}, and vocabulary k \u2208V, we have\n\uf8f4 \uf8f3 Proof. First, consider the case s = t. Under the condition tq = s, we have tq = s = t, and thus, zt is the trigger token. The only possible case allowing the same token for trigger and output is when tq = T \u22121. This yields zT\u22121 = zT = k because zT not only works as trigger, but also output. Therefore, when s = t, we have\n(35)\nNext, we focus on s \u2264t\u22122. Suppose we are given t = T. The token zt is then the second trigger token, and 1{zt = k} = 1 implies that ztq = k. However, since the first trigger position is tq = s \u2264t\u22122 and the output token is also y = k, zT (= k) cannot be the second trigger occurrence. Hence, it suffices to consider t \u0338= T.\nWe now have s \u2264t \u22122 < T \u22122. Assuming ztq = k will also lead to contradiction because the trigger token k appears more than twice at positions s, s + 1 and T. With this in mind, we obtain\n(36)\nHere we use P(ztq = q | y = k, tq = s) = 1/(V \u22121) because the trigger token is uniformly sampled from V \\ {k}. Also, we have P(zt = k | y = k, tq = s, ztq = q) = 1/(V \u22121) because zt for t \u0338= T is neither the trigger token nor the output token, and thus, sampled from V \\ {q}. These give us the following when s \u2264t \u22122:\n(37)\nLemma 8. Given a sequence z1:T that satisfies Sec. B.2. For any vocabulary k \u2208V, we have\n(38)\n(39)\n\ufffd \ufffd Proof. By the linearity of expectation, we can decompose the expectation as follows:\nwhere we abbreviate the conditions of the expectations to enhance readability. Note that we always have 1{zt = k} = 1 when tq = t\u22121 and y = k. In the case of t = T, the trigger token appears at positions T \u22121 and T given tq = t \u22121. With the condition y = k, it follows that\nIn the above, we point out that we have tq + 1 = T in this situation. Additionally, we get\n\uf8f0 \uf8ed \uf8f8 \uf8fb where the final equation follows from the fact that zt\u2032 is not a trigger token k. For the case where t \u0338= T, the trigger token is not k because otherwise there will be more than two triggers in the input sequence at positions tq, tq+1 and T. Thus, we have\nWe prove the statement by combining these results. Lemma 9. Given a sequence z1:T that satisfies Sec. B.2. Fix some t \u2208[1, T] \u2282N. For any s \u2208[1, t \u22122] \u222a{t}, and vocabulary k \u2208V,\n\uf8f4 \uf8f3 Proof. First, we look at the scenario s = t. Since tq = s = t, the input sequence z1:T will contain more than two trigger tokens unless t = T \u22121, which does not satisfy the condition of input sequence, we examine\nthe case of t = T \u22121. This condition ensures that 1{zT\u22121 = k} = 1{zT = k} = 1, and therefore, we can use the instance t = T from Lemma 8.\nNext, we require s \u2264t\u22122. It is impossible to have t = T because this violates the input sequence condition, where the trigger token only appears twice. Now, by the linearity of expectation, we can decompose the expectation as follows:\n\uf8f0 \uf8ed \uf8f8 \uf8fb Since tq = s \u2264t \u22122 and t \u0338= T, the token zt is neither trigger nor output. This yields the following:\nFinally, for t\u2032 \u0338= tq, tq + 1, t and T, the tokens zt\u2032 and zt are independent. With Lemma 7, we obtain\nIntegrating the equations above allows us to arrive at the conclusion.\nLemma 10. Given a sequence z1:T that satisfies Sec. B.2. For any vocabulary k \u2208V, and t \u2208[1, T] \u2282 N, we have\nProof. We split 1{tq \u2264t} into 1{tq = t\u22121}+1{tq = t} + 1{tq \u2264t \u22122}. Then, the expectation can be decomposed as:\n\ufffd \ufffd Note that when tq = t \u22121, zt is the output token. In addition, if we have t = T, zt works as the second output token as well, which means that zt = k and xtq = wE(k). If we find t \u0338= T, the trigger token is not k. Thus, we derive\nNext, we discuss tq = t. Since tq is the first trigger position, we always observe that tq \u2264T \u22121. If two trigger tokens are adjacent to each other, i.e., tq = T \u22121, we can state that zT\u22121 = zT = k because of the equation y = k. Otherwise, the trigger token must belong to V \\ {k}, and 1{zt = k} = 0. This gives\nFor tq \u2264t \u22122, we again analyze the cases of t = T and t \u0338= T. Provided that t = T, 1{zt = k} = 1 implies that the trigger token is decided to be k, while the output token is also k. This leads to more than two occurrences of trigger token, which violates the input sequence condition. Now, we assume t \u2264T \u22121. This time, the trigger token ztq is not k, and zt is not a trigger token, which yields\nE \ufffd 1{zt = k}1{tq \u2264t \u22122}xtq | y = k \ufffd = P(tq \u2264t \u22122)E[wE(ztq) \u00b7 E[1{zt = k} | ztq, y = k, tq \u2264t \u22122] | y = k, tq \u2264t \u22122]\n\nThis concludes the proof. Lemma 11. Given a sequence z1:T that satisfies Sec. B.2. For any vocabulary k \u2208V, and t \u2208[1, T] \u2282 N, we have\nLemma 11. Given a sequence z1:T that satisfies Sec. B.2. For any vocabulary k \u2208V, and t \u2208[1, T] \u2282 N, we have\n\ufffd (41)\nProof. As in the proof of Lemma 10, we analyze each scenario depending on the value of tq. Firstly, let us consider tq = t \u22121. This leads to the fact that zt is the output token. Given y = k, we have zt = k.\nIn the event that t = T is valid, xtq = xT = wE(k) holds and the other T \u22122 tokens are not k. Thus, it gives\n\uf8f3 \uf8fe When t \u0338= T holds, we can say that xtq \u0338= wE(k), while xtq+1 = wE(k) because of y = k. This gives us\nAlso, we can derive the following equations for i \u2208 [1, t] \\ {tq, tq + 1}:\nIn the above, we use the fact that P(xi = j, xtq = q) = 0 if j = q or q = k holds. To summarize, we have\n(42)\n\ufffd \ufffd Next, assume tq = t. In this case, t = T implies tq = T which is a contradiction. Furthermore, tq = t < T \u22121 implies tq < tq+1 < T. Under this condition, we cannot have zt = k, because it produces three appearances of the trigger token. Hence, it suffices to consider t = T \u22121. If tq = t = T \u22121 is true, we can state that xt = k, leading to the formulation below:\nwhere the second equation follows from the same argument in the instance t = T in Eq. 42, but this time xT is not contained in x1:t. Lastly, we focus on tq = s \u2264t \u22122. We argue that the trigger token ztq = zT is not k, due to the condition of the number of trigger token in the input sequence. This means that t \u0338= T and zt does not serve as a trigger token. Now, we split \u00afx1:t into 1 t (xtq + xtq+1 + xt + \ufffd t\u2032 xt\u2032) and obtain\nIn short, we have\nSumming up all the related terms leads to Eq. 41, which finishes the proof. Lemma 12. Given a sequence z1:T that satisfies Sec. B.2. For any vocabulary k \u2208V, and t \u2208[1, T] \u2282 N, we have\n(43)\nProof. We consider the following cases: Case 1: tq = t \u22121. If t = T holds, this gives ztq = zT = y = k and 1{zt\u22121 = k} = 1{zt = k} = 1. Thus, we have E \ufffd 1{zt\u22121 = k}xtq1{zt = k} | y = k, tq = t \u22121 \ufffd\n(44)\nIn summary, we find that\n(45)\nCase 2: tq = t. For this scenario, 1{zt = k} = 1 holds only when tq = t = T \u22121. Thus, we can state that\nE[1{ztq = k}xtq1{zt = k} | y = k, tq = t] = E[xtq | y = k, tq = t] = wE(k), E[1{zT = k}xtq1{zt = k} | y = k, tq = t] = E[xtq | y = k, tq = t] = wE(k).\nFor the other t\u2032, we have 1{zt\u2032 = k} = 0 because these tokens are not a trigger token. This means that\nCase 3: tq \u2264t \u22122. We first let t = T. If 1{zt = k} = 1 holds, then the trigger token becomes k, but we find that ztq = ztq+1 = zT = k, meaning the trigger token appears more than two times. Therefore, we only consider t \u0338= T. Given ztq(= zT ) = k, it also fails to satisfy the required conditions for the sequence, hence it follows that\nThese equations give us that for 1 \u2264s \u2264t \u22122,\n(47)\n(48)\nGathering all the equations of 1 \u2264tq \u2264t leads to Eq. 43 and we have proven the statement. Lemma 13. Given a sequence z1:T that satisfies Sec. B.2. For any vocabulary k \u2208V, and t \u2208[1, T] \u2282 N, we have\nGathering all the equations of 1 \u2264tq \u2264t leads to Eq. 43 and we have proven the statement.\n(49)\nProof. Considering the linearity of expectation, we focus on each term xi contained in \u00afx1:t. We consider the following cases: Case 1: tq = t \u22121. We already have the result as to xtq in Eq. 45:\n\ufffd \ufffd Now we deal with xi for 1 \u2264i \u2264t \u22122. Note that zi \u0338= k for these i under the conditions t = T and y = k.\nif t = T,\nCase 2: tq = t. For this scenario, we have seen that tq = t = T \u22121 is the only possible way for the expectation to take non-zero value. For xtq, we can observe Eq. 46 and obtain\nThe other terms related to xi for 1 \u2264i \u2264t \u22121 can be calculated\nWe note that 1{zt = k} = 1 and 1{zt\u2032 = k} = 0 fo t\u2032 < tq = t because of the input sequence condition. Case 3: tq = s \u2264t \u22122. Again, we have the result for xtq in Eq. 47.\nWe have xtq+1 = k under y = k, and Lemma 9 tells us that\nNext, we consider xt. Note that t = T forces xtq to be wE(k) for the expectation to take non-zero value, which contradicts the input sequence condition. Hence we assume t \u0338= T so that ztq = zT \u0338= k.\nLastly, regarding xi for i \u0338= t, tq, tq + 1, we also consider t \u0338= T and it follows that\n\n (50)\n(51)\n\nLemma 15. Given a sequence z1:T that satisfies Sec. B.2. Then, we have\nFinally, for t\u2032 = T \u22121, we note that the two conditions y = k and zT = k imply zT\u22121 = k.\nwhere the second-to-last expression follows from Eq. 50. Now, we look at the case v \u0338= k. Different from v = k, we have\nE [1{zT\u22121 = k}qT,T 1{zT = v} | y = k] = 0, E [1{zT = k}qT,T 1{zT = v} | y = k] = 0.\nThis leads to the following expression for t\u2032 \u2208[T \u22122\nThis concludes the proof.\n# C.2 global v.s. in-context\nProposition 4 (Restated). Suppose a two-layer transformer is an associative memory transformer. Given a length-T sequence z1:T where the last token is zT = q, let the t1-th token be zt1 = v1 following zt1\u22121 = q, and let the t2-th token be zt2 = v2 following zt2\u22121 = q, where v1 \u0338= v2 and (3 \u2264)t1 < t2 without loss of generality. Also assume that there exist only one v1 and v2 in the context z1:T . Then, the logit \u03bev1 and \u03bev2 can be expressed as follows:\nProof. We consider the output of the first layer of attention for zt. Since we use the associative memory transformer, we substitute W 1 Q = I and W 1 K =\n\ufffd k\u2208Q wE(k)r\u22a4 \u22121 into Eq. 1, we have\n(52)\n(53)\nwhere e is Euler\u2019s number.",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of knowledge hijacking in in-context learning (ICL) within large language models (LLMs), emphasizing the significance of balancing in-context information and pre-trained knowledge for accurate token prediction.",
        "problem": {
            "definition": "The primary problem is the phenomenon of knowledge hijacking, where the model either ignores in-context knowledge or overly trusts it, leading to incorrect outputs.",
            "key obstacle": "The main challenge lies in understanding the complex computational mechanisms of LLMs and how they balance in-context and global knowledge during token prediction."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that transformers can implement an induction head mechanism that leverages both in-context and pre-trained information.",
            "opinion": "The proposed model aims to enhance the ability of transformers to utilize in-context knowledge effectively while minimizing the risk of knowledge hijacking.",
            "innovation": "The main innovation is the introduction of relative positional encoding (RPE), which allows a two-layer transformer to better utilize in-context knowledge compared to traditional absolute positional encoding (APE)."
        },
        "Theory": {
            "perspective": "The theoretical perspective focuses on the induction head mechanism and its role in managing the interplay between in-context and global knowledge.",
            "opinion": "The theory posits that transformers with RPE can consistently attend to relevant previous tokens, regardless of their position in the sequence.",
            "proof": "The paper provides formal proofs demonstrating that RPE enhances the model's ability to generalize over longer sequences and effectively utilize in-context knowledge."
        },
        "experiments": {
            "evaluation setting": "The experiments involve a two-layer transformer architecture with RPE and APE, utilizing a dataset generated by a bigram model to evaluate knowledge utilization.",
            "evaluation method": "Specific prompts were designed to test the model's outputs against theoretical predictions, focusing on the model's ability to recall and utilize in-context information."
        },
        "conclusion": "The study concludes that RPE significantly improves the performance of transformers in balancing in-context and global knowledge, thus enhancing their predictive capabilities.",
        "discussion": {
            "advantage": "The primary advantage of this work is its theoretical and empirical validation of RPE's effectiveness in mitigating knowledge hijacking.",
            "limitation": "A limitation noted is the focus on a two-layer transformer, which may not generalize directly to larger, more complex models.",
            "future work": "Future research could explore the implications of these findings in larger transformer architectures and investigate additional mechanisms that contribute to ICL."
        },
        "other info": [
            {
                "info1": "The work does not involve personal or sensitive data, adhering to ethical standards for data usage."
            },
            {
                "info2": {
                    "info2.1": "The study emphasizes the societal importance of understanding ICL mechanisms for safer AI applications.",
                    "info2.2": "The paper contributes to the growing body of research on the interpretability of transformer models."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses knowledge hijacking in in-context learning (ICL) within large language models (LLMs), emphasizing the significance of balancing in-context information and pre-trained knowledge for accurate token prediction."
        },
        {
            "section number": "1.2",
            "key information": "The study emphasizes the societal importance of understanding ICL mechanisms for safer AI applications."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective focuses on the induction head mechanism and its role in managing the interplay between in-context and global knowledge."
        },
        {
            "section number": "3.4",
            "key information": "The main innovation is the introduction of relative positional encoding (RPE), which allows a two-layer transformer to better utilize in-context knowledge compared to traditional absolute positional encoding (APE)."
        },
        {
            "section number": "4.1",
            "key information": "The proposed model aims to enhance the ability of transformers to utilize in-context knowledge effectively while minimizing the risk of knowledge hijacking."
        },
        {
            "section number": "6.1",
            "key information": "The primary problem is the phenomenon of knowledge hijacking, where the model either ignores in-context knowledge or overly trusts it, leading to incorrect outputs."
        },
        {
            "section number": "7",
            "key information": "The study concludes that RPE significantly improves the performance of transformers in balancing in-context and global knowledge, thus enhancing their predictive capabilities."
        }
    ],
    "similarity_score": 0.7285223052028831,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory.json"
}