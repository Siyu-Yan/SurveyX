{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2303.03846",
    "title": "Larger language models do in-context learning differently",
    "abstract": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input\u2013label mappings. We investigate two setups\u2014ICL with flipped labels and ICL with semantically-unrelated labels\u2014across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input\u2013label mappings shown in incontext exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input\u2013label mappings, but more of the former.",
    "bib_name": "wei2023largerlanguagemodelsincontext",
    "md_text": "# LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY\n# ABSTRACT\nWe study how in-context learning (ICL) in language models is affected by semantic priors versus input\u2013label mappings. We investigate two setups\u2014ICL with flipped labels and ICL with semantically-unrelated labels\u2014across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input\u2013label mappings shown in incontext exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input\u2013label mappings, but more of the former.\narXiv:2303.03846v2\nLanguage models can perform a range of downstream NLP tasks via in-context learning (ICL), where models are given a few exemplars of input\u2013label pairs as part of the prompt before performing the task on an unseen example (Brown et al., 2020, inter alia). To successfully perform ICL, models can (a) mostly use semantic prior knowledge to predict labels while following the format of in-context exemplars (e.g., seeing \u201cpositive sentiment\u201d and \u201cnegative sentiment\u201d as labels and performing sentiment analysis using prior knowledge) and/or (b) learn the input\u2013label mappings from the presented exemplars (e.g., finding a pattern that positive reviews should be mapped to one label, and negative reviews should be mapped to a different label). Prior work on which of these factors drives performance is mixed. For instance, although Min et al. (2022b) showed that presenting random ground truth mappings in-context does not substantially affect performance (suggesting that models primarily rely on semantic prior knowledge), other work has shown that transformers in simple settings (without language modeling pretraining) implement learning algorithms such as ridge regression and gradient descent (Aky\u00fcrek et al., 2023; von Oswald et al., 2022; Dai et al., 2022).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae6a/ae6a5765-e94a-4f26-94ed-29ae0043c061.png\" style=\"width: 50%;\"></div>\nIn this paper, we study how these two factors\u2014semantic priors and input\u2013label mappings\u2014interact in several experimental settings (see Figure 1 for an example of each setting):\n1. In regular ICL, both semantic priors and input\u2013label mappings can allow the model to perform in-context learning successfully.\n2. In flipped-label ICL, all labels in the exemplars are flipped, which means that semantic prior knowledge and input\u2013label mappings disagree. Labels for the evaluation set stay the same, so for binary classification tasks, performing better than 50% accuracy in this setting means that the model is unable to override semantic priors, and performing below 50% accuracy means that the model is able to learn input\u2013label mappings and override semantic priors.\n3. In semantically-unrelated label ICL (SUL-ICL), the labels are semantically unrelated to the task (e.g., for sentiment analysis, we use \u201cfoo/bar\u201d instead of \u201cnegative/positive\u201d). Since the semantic priors from labels are removed, the model can only perform ICL by using input\u2013label mappings.\nWe run experiments in these settings spanning multiple model families with varying sizes, training data, and instruction tuning (GPT-3, InstructGPT, Codex, PaLM, Flan-PaLM) in order to analyze the interplay between semantic priors and input\u2013label mappings, paying special attention to how results change with respect to model scale. First, we examine flipped-label ICL, where we find that small models do not change their predictions when seeing flipped labels, but large models can flip their predictions to follow flipped exemplars (Section 3). This means that the ability to override semantic priors with input\u2013label mappings emerges with model scale, which should not be taken for granted because larger models presumably have stronger priors that are more challenging to override. Second, we compare the SUL-ICL setting to regular ICL (Section 4). We find that small language models experience a large performance drop when semantic priors are removed, whereas large language models can perform the task well even without semantic priors from the labels. For some datasets, doing better than random in the SUL-ICL setting required substantial scaling (e.g., only PaLM-540B achieves above-random performance). We also found this to be true for high-dimensional linear classification tasks (Section 6). This means that learning input\u2013label mappings without being given semantic priors is also an emergent ability of large language models for those tasks. Finally, we study the effect of instruction tuning (Min et al., 2022a; Wei et al., 2022a; Chung et al., 2022) on ICL abilities (Section 5). We find that instruction-tuned models achieve better performance than pretraining-only models on SUL-ICL settings, which means that instruction tuning increases the model\u2019s ability to learn input\u2013label mappings. On the other hand, we also see that instruction-tuned models are more reluctant to follow flipped labels, which means that instruction tuning decreases the model\u2019s ability to override semantic priors more than it increases its ability to learn input\u2013label mappings. Overall, our work aims to shed light on the interaction between semantic prior knowledge and input\u2013label mappings while considering the effects of scaling and instruction tuning.\n# 2 EXPERIMENTAL SETUP\n# 2.1 EVALUATION TASKS\nWe experiment on seven NLP tasks that have been widely used in the literature (Kim, 2014; Wang et al., 2018; 2019). These evaluation tasks and an example prompt/target pair are shown in Figure 9 in the Appendix; additional dataset details are described in Appendix A. The seven tasks are: Sentiment Analysis (Socher et al., 2013, SST-2); Subjective/Objective Sentence Classification (Conneau & Kiela 2018, SUBJ); Question Classification (Li & Roth, 2002, TREC); Duplicated-Question Recognition (Chen et al., 2017; Wang et al., 2018, QQP); Textual Entailment Recognition (Dagan et al., 2006; Wang et al., 2019, RTE); Financial Sentiment Analysis (Malo et al., 2014, FP); and Hate Speech Detection (Mollas et al., 2020, ETHOS).1\n# 2.2 MODELS\nWe perform experiments on five language model families as shown in Table 1. We use three families of OpenAI language models accessed via the OpenAI API: GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), and Codex (Chen et al., 2021). For GPT-3 models, ada, babbage, curie, and davinci seem to correspond to the following model sizes: 350M, 1.3B, 6.7B, and 175B (Gao et al., 2021). For InstructGPT and Codex, however, it is not publicly known what the sizes of these language models are, but we assume\nModel Family\nModel Name (Abbreviation)\nGPT-3\nada (a), babbage (b), curie (c), davinci (d)\nInstructGPT\ntext-ada-001 (a-1), text-babbage-001 (b-1),\ntext-curie-001 (c-1), text-davinci-001 (d-1),\ntext-davinci-002 (d-2)\nCodex\ncode-cushman-001 (c-c-1), code-davinci-001\n(c-d-1), code-davinci-002 (c-d-2)\nPaLM\nPaLM-8B, PaLM-62B, PaLM-540B\nFlan-PaLM\nFlan-PaLM-8B, Flan-PaLM-62B, Flan-\nPaLM-540B\nthat they are in increasing model scale for some scaling factor.\nWe also experiment on three different sizes of PaLM (Chowdhery et al., 2022) (8B, 62B, and 540B) and their instruction-tuned variants (Chung et al., 2022, Flan-PaLM). PaLM models have the same training data and protocol and only differ by model size (Chowdhery et al., 2022), which provides an additional data point for the effect of scaling model size specifically.\n2.3 ADDITIONAL EXPERIMENTAL DETAILS\nAs additional experimental details, we follow the prior literature on in-context learning and use a different set of few-shot exemplars for each inference example (Brown et al., 2020; Chowdhery et al., 2022; Wang et al., 2023, inter alia). By default, we use k = 16 in-context exemplars per class, though we also experiment with varying number of exemplars in Section 4 and Appendix C.2. We also use the \u201cInput/Output\u201d template for prompts shown in Figure 9, with ablations for input format shown in Appendix B.4 and Appendix B.5, and the semantically-unrelated \u201cFoo\u201d/\u201cBar\u201d targets as shown in Figure 9 (ablations for target type are shown in Appendix B.3). Finally, to reduce inference costs, we use 100 randomly sampled evaluation examples per dataset, as it is more beneficial to experiment with a more-diverse range of datasets and model families than it is to include more evaluation examples per dataset, and our research questions depend more on general behaviors than on small performance deltas (note that all y-axes in our plots go from 0\u2013100).\n1In preliminary experiments (Appendix B.3), we also tried two additional tasks: Question\u2013Answering (Rajpurkar et al., 2016; Wang et al., 2018, QNLI) and Coreference Resolution (Levesque et al., 2012; Wang et al., 2019, WSC), but even the largest models had very weak performance on these tasks in many settings, so we do not include them in further experimentation.\n<div style=\"text-align: center;\">Table 1: Models used in this paper.</div>\n3 INPUT\u2013LABEL MAPPINGS OVERRIDE SEMANTIC PRIORS IN LARGE MODELS To what extent are models able to override semantic priors from pretraining in favor of input\u2013label mappings presented in-context? When presented in-context exemplars with flipped labels, models that are able to override priors and learn input\u2013label mappings in-context should experience a decrease in performance to below random guessing (assuming ground-truth evaluation labels are not flipped). To test this, we randomly flip an increasing proportion of labels for in-context exemplars. As shown in Figure 1, for example, 100% flipped labels for the SST-2 dataset would mean that all exemplars labeled as \u201cpositive\u201d will now be labeled as \u201cnegative,\u201d and all exemplars that were labeled as \u201cnegative\u201d will now be labeled as \u201cpositive.\u201d Similarly, 50% flipped labels is equivalent to random labels, as we use binary classification datasets (we exclude TREC from this experiment since it has six classes). We do not change the labels of the evaluation examples, so a perfect model that can override semantic priors should achieve 0% accuracy when presented with 100% flipped labels. Figure 2 shows average model performance for each of the model families across all tasks with respect to the proportion of labels that are flipped (per-dataset results are shown in Figure 16). We see that there is a similar trend across all model families\u2014at 0% flipped labels (i.e., no labels are changed), larger models have better performance than small models, which is expected since larger models should be more capable than smaller models. As more and more labels are flipped, however, the performance of small models remains relatively flat and often does not dip below random guessing, even when 100% of labels are flipped. Large models, on the other hand, experience performance drops to well-below random guessing (e.g,. text-davinci-002 performance drops from 90.3% with 0% flipped labels to just 22.5% with 100% flipped labels). Note that GPT-3 models can remove semantic priors (i.e., perform at guessing accuracy) but cannot override them (i.e., perform significantly worse than guessing), even when presented with 100% flipped labels. For this reason, we consider all GPT-3 models to be \u201csmall\u201d models because they all behave similarly to each other this way. These results indicate that large models can override prior knowledge from pretraining with input\u2013 label mappings presented in-context. Small models, on the other hand, do not flip their predictions and thus are unable to override semantic priors (consistent with Min et al. (2022b)). Because this ability to override prior knowledge with input\u2013label mappings only appears in large models, we conclude that it is an emergent phenomena unlocked by model scaling (Wei et al., 2022b).\n0\n25\n50\n75 100\n0\n20\n40\n60\n80\n100\n% flipped labels\nAccuracy (%)\nPaLM\nPaLM-540B\nPaLM-62B\nPaLM-8B\nRandom\n0\n25\n50\n75 100\n0\n20\n40\n60\n80\n100\n% flipped labels\nCodex\ncode-davinci-002\ncode-davinci-001\ncode-cushman-001\nRandom\n0\n25\n50\n75 100\n0\n20\n40\n60\n80\n100\n% flipped labels\nInstructGPT\ntext-davinci-002\ntext-davinci-001\ntext-curie-001\ntext-babbage-001\ntext-ada-001\nRandom\n0\n25\n50\n75 100\n0\n20\n40\n60\n80\n100\n% flipped labels\nGPT-3\ndavinci\ncurie\nbabbage\nada\nRandom\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6edc/6edc4a18-c368-481b-922d-639683ceb5c7.png\" style=\"width: 50%;\"></div>\nFigure 2: The ability to override semantic priors when presented with flipped in-context exemplar labels emerges with model scale. Smaller models cannot flip predictions to follow flipped labels (performance only decreases slightly), while larger models can do so (performance decreases to well below 50%). Ground truth labels for evaluation examples are not flipped, so if a model learns to follow flipped labels, its accuracy should be below 50% when more than 50% of labels are flipped. For example, a model with 80% accuracy at 0% flipped labels will have 20% accuracy at 100% flipped labels if it learns to perfectly flip its predictions. Accuracy is computed over 100 evaluation examples per dataset with k = 16 in-context exemplars per class and averaged across all datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e3b2/e3b254f9-de7d-41fc-b55e-d219ea4d0133.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Semantically-unrelated targets (SUL-ICL)</div>\nFigure 3: Small models rely more on semantic priors than large models do, as performance decreases more for small models than for large models when using semantically-unrelated targets instead of natural language targets. For each plot, models are shown in order of increasing model size (e.g., for GPT-3 models, a is smaller than b, which is smaller than c). We use k = 16 in-context exemplars per class, and accuracy is calculated over 100 evaluation examples per dataset and averaged across all datasets. A per-dataset version of this figure is shown in Figure 17 in the Appendix.\n4 IN-CONTEXT LEARNING WITH SEMANTICALLY UNRELATED LABELS EMERGES WITH SCALE\nAnother way to examine how much models use semantic priors from pretraining versus input\u2013label mappings is to replace natural language targets with semantically-unrelated targets. If a model mostly relies on semantic priors for in-context learning, then its performance should significantly decrease after this change, since it will no longer be able to use the semantic meanings of targets to make predictions. A model that learns input\u2013label mappings in-context, on the other hand, would be able to learn these semantically-unrelated mappings and should not experience a major drop in performance. We use an experimental setup that we call Semantically-Unrelated Label In-Context Learning (SULICL) to test model behavior in these scenarios.2 In this setup, all natural language targets are swapped with semantically-unrelated targets (we use \u201cFoo\u201d and \u201cBar\u201d by default, although we get similar results with other semantically-unrelated targets\u2014see Appendix B.3). For example, SUL-ICL relabels examples labeled as \u201cnegative\u201d as \u201cfoo\u201d and examples labeled as \u201cpositive\u201d as \u201cbar\u201d for the SST-2 dataset (Figure 1). We then examine model performance in the SUL-ICL setup (in Appendix B, we investigate other aspects of the SUL-ICL setup such as remapping inputs, formatting prompts differently, changing target types, and using out-of-distribution datasets). In Figure 3, we examine average model accuracy across all tasks on the SUL-ICL setup compared with a regular in-context learning setup (per-dataset results are shown in Figure 17). As expected, we see that increasing model scale improves performance for both regular in-context learning and SUL-ICL. The performance drop from regular ICL to SUL-ICL, however, is far more interesting. We find that using semantically-unrelated targets results in a greater performance drop from using natural language targets for small models compared with large models. Because small models are heavily affected when the semantic meaning of targets is removed, we conclude that they primarily rely on the semantic meaning of targets for in-context learning rather than learn the presented input\u2013label mappings. Large models, on the other hand, experience very small performance drops after this change, indicating that they have the ability to learn input\u2013label mappings in-context when the semantic nature of targets is removed.3 Hence, the ability to learn input\u2013label mappings in-context without being given semantic priors can also be seen as an emergent ability of model scale.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66b3/66b3e218-6e8f-4880-92ca-11cfcb865b91.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: In the SUL-ICL setup, larger models benefit more from additional exemplars than smaller models do. Accuracy is calculated over 100 evaluation examples per dataset and averaged across all datasets. A per-dataset version of this figure is shown in Figure 18 in the Appendix.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea4a/ea4a0eac-5d41-4077-8c24-eaaf579f518a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Codex Models PaLM Models</div>\n<div style=\"text-align: center;\">Codex Models</div>\nFigure 5: Some tasks in the SUL-ICL setting emerge with scale and can only be successfully performed by large-enough models. These experiments use k = 8 in-context exemplars per class. Accuracy is calculated over 100 evaluation examples.\nWe next analyze how models perform on a SUL-ICL setup when presented with an increasing number of in-context exemplars, and we show these data in Figure 4 (per-dataset results are shown in Figure 18). We find that for the three model families that we tested,4 including more in-context exemplars results in a greater performance improvement for large models than it does for small models. This indicates that large models are better at learning from in-context exemplars than small models are, implying that large models are more capable of using the additional input\u2013label mappings presented in context to better learn the correct relationships between inputs and labels. Finally, looking at the per-dataset performance reveals how the ability to perform some benchmark tasks in the SUL-ICL setting emerges with scale. In Figure 5, we highlight two tasks (RTE and ETHOS) that seem particularly emergent in the SUL-ICL setting by plotting model performance at each model size for Codex and PaLM models (Figure 18 shows how each model performs for each dataset). We see that performance on the RTE dataset is around random for PaLM-8B and PaLM-62B, yet increases to well above random for PaLM-540B. Similarly, the performance on both the RTE and ETHOS datasets is around random for code-cushman-001 and code-davinci-001, then jumps to 80%+ for code-davinci-002. PaLM models seem to emerge earlier on the ETHOS dataset, however, as the performance spikes when scaling from PaLM-8B to PaLM-62B. For many datasets that do not show emergence, even small models can outperform random guessing without many in-context exemplars (e.g., on SST-2, TREC, SUBJ, FP). These results show another example of how, for some tasks, the ability to learn input\u2013label mappings in-context without being given semantic priors is only emergent in large-enough language models.\n5 INSTRUCTION TUNING WITH EXEMPLARS IMPROVES INPUT\u2013LABEL MAPPINGS LEARNING AND STRENGTHENS SEMANTIC PRIORS\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f220/f220d6bb-d4f2-4749-98ac-0c3b027a3d8b.png\" style=\"width: 50%;\"></div>\nA popular technique for improving the performance of pretrained language models is to finetune them on a collection of NLP tasks phrased as instructions, with few-shot exemplars as part of the finetuning inputs (Min et al., 2022a; Wei et al., 2022a; Chung et al., 2022; Longpre et al., 2023). Since instruction tuning uses natural language targets, however, an open question is whether it improves the ability to learn input\u2013label mappings in-context or whether it strengthens the ability to recognize and apply semantic priors, as both would lead to an improvement in performance on standard ICL tasks.\nFigure 6: Instruction-tuned language models are better at learning input\u2013label mappings than pretraining-only language models are. Accuracy is calculated using 100 evaluation examples per dataset and averaged across six datasets. A per-dataset version of this figure is shown in Figure 19 in the Appendix.\nTo study this, we run the same experiments from Section 3 and Section 4, and we now compare PaLM models to their\ninstruction-tuned versions (Chung et al., 2022, Flan-PaLM). We do not compare InstructGPT against GPT-3 models in this experiment because we cannot determine if the only difference between these model families is instruction tuning (e.g., we do not even know if the base models are the same).\nFigure 6 shows the average model performance across all datasets with respect to the number of in-context exemplars for PaLM and Flan-PaLM models. We see that Flan-PaLM performs better in the SUL-ICL setting than PaLM does, an effect that is most prominent in small models, as FlanPaLM-8B outperforms PaLM-8B by 9.6%, almost catching up to PaLM-62B. This trend suggests that instruction tuning strengthens the ability to learn input\u2013label mappings (an expected outcome). In Figure 7, we show model performance with respect to the proportion of labels that are flipped for each PaLM and Flan-PaLM model. We find that, compared to pretraining-only models, instructiontuned models are worse at flipping their predictions\u2014Flan-PaLM models were unable to override their semantics more than what could be achieved by random guessing, even with 100% flipped labels. Standard PaLM models, on the other hand, could achieve as low as 31% accuracy when presented with 100% flipped labels. These results indicate that instruction tuning either increases the extent to which models rely on semantic priors when they are available or gives models more semantic priors, as instruction-tuned models are less capable of flipping their natural language targets to follow the flipped labels that were presented. Combined with the result from Figure 6, we conclude that although instruction tuning improves the ability to learn input\u2013label mappings, it concurrently strengthens the usage of semantic priors, similar to the findings in Min et al. (2022a).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7b11/7b110429-d8a2-44f8-b27d-f6e9828db4b7.png\" style=\"width: 50%;\"></div>\nFigure 7: Instruction-tuned models are worse than pretraining-only models are at learning to override semantic priors when presented with flipped labels in-context. We use k = 16 in-context exemplars per class, and accuracy is calculated using 100 evaluation examples per dataset and averaged across six datasets. A per-dataset version of this figure is shown in Figure 20 in the Appendix.\n<div style=\"text-align: center;\"># exemplars per class</div>\nIn addition to the natural language reasoning abilities that we studied throughout the rest of the paper, we also seek to learn about how model scale affects the ability to perform other tasks. Specifically, we look at the linear classification task, where large models should perform better than small models (especially at high dimensions) if their greater capacity to learn input\u2013label mappings as shown in Section 4 also holds for non-natural-language tasks.\nTo analyze this, we create N-dimensional linear classification datasets and examine model behavior with respect to the number of dimensions in the SUL-ICL setup. In these datasets, we provide k N-dimensional points above a threshold and k N-dimensional points below that same threshold as in-context exemplars, and the model must determine whether an N-dimensional evaluation point is above or below the threshold (we do not tell the model the equation or the threshold). When selecting random N-dimensional points, we use random integers between 1 and 1000 for each coordinate value. Algorithm 1 in the Appendix shows the precise dataset generation procedure. In Figure 8, we show Codex model performance on N = 16 dimensional linear classification (per-dimension results are shown in Figure 21 in the Appendix). We find that the largest model outperforms random guessing by 19% on this task, while smaller models cannot outperform random guessing by more than 9%. These results suggest that there exists some scaling factor that allows large-enough language models to perform high-dimensional linear classification.\n# 7 RELATED WORK\nThere has been a growing body of work on in-context learning that suggests that good performance is primarily driven by semantic priors and other factors such formatting and inducing intermediate token generation. For instance, Min et al. (2022b) showed the surprising result that using random groundtruth labels in exemplars barely hurts performance, suggesting that performance is instead mainly driven by the label space, distribution of input text, and overall format of the sequence. Along the same lines, Madaan & Yazdanbakhsh (2022) and Wang et al. (2022) show that for chain-of-thought prompting (Wei et al., 2022c), logically-incorrect prompts do not hurt performance on multi-step reasoning tasks. On a theoretical level, Xie et al. (2022) provide an explanation of in-context learning in which transformers infer tasks from exemplars because they are trained to infer latent concepts during pretraining, and prior knowledge obtained from pretraining data can then be applied to incontext examples. Finally, Reynolds & McDonell (2021) showed that clever zero-shot prompts can outperform few-shot prompts, which implies that some NLP tasks benefit more from leveraging the model\u2019s existing knowledge than from learning about the task from in-context exemplars. In this paper, we do not contest the claim that language models can benefit greatly from semantic prior knowledge\u2014our results instead add nuance to the understanding of ICL by showing that, when semantic prior knowledge is not available, large-enough language models can still do ICL using input\u2013label mappings. Our experiments are consistent with Min et al. (2022b) for models scaling up to davinci, and we show that learning input\u2013label mappings only emerges with larger models (e.g., PaLM-540B, text-davinci-002, and code-davinci-002).\n# 7.2 LEARNING INPUT\u2013LABEL MAPPINGS\nOther recent work has suggested to some degree that language models can actually learn input\u2013label mappings from exemplars given in-context, which is a more-attractive ability than using semantic\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/812a/812a5a18-f9f9-4074-9c94-9fdacc45bfa7.png\" style=\"width: 50%;\"></div>\nFigure 8: Successfully performing 16-dimensional linear classification emerges with model scale for Codex models. Accuracy is calculated over 100 evaluation examples with k = 16 in-context exemplars per class. Per-dimension results are shown in Figure 21 in the Appendix.\npriors because it means that the model would be able to perform a wide range of tasks even if those tasks are not seen in or even contradict pretraining data. For instance, transformers trained from scratch can perform in-context learning on linear-regression datasets with performance that is comparable to the least-squares estimator (Garg et al., 2022), and recent work has shown that transformers can do so by implementing standard learning algorithms such as ridge regression and gradient descent (Aky\u00fcrek et al., 2023; von Oswald et al., 2022; Dai et al., 2022). In the natural language setting, Webson & Pavlick (2022) showed that language models learn just as fast with irrelevant or misleading prompts during finetuning or prompt-tuning. Our work makes similar claims about the ability for language models to learn tasks via input\u2013label mappings only, though it differs crucially in that we observe frozen pretrained transformers without any additional learning.\nIn this paper we have also focused on the effect of scaling on in-context learning, which relates to a nascent body of work showing that scaling language models leads to qualitatively-different behavior (Ganguli et al., 2022; Wei et al., 2022b; Srivastava et al., 2022). For instance, it has recently been shown that scaling up language models can allow them to perform a variety of challenging tasks that require reasoning (Wei et al., 2022c; Chowdhery et al., 2022; Kojima et al., 2022; Zhou et al., 2023). Our experimental findings on the flipped-label ICL setup show that language models can learn input\u2013label mappings even when the input\u2013label mapping contradicts the semantic meaning of the label, demonstrating another type of symbolic reasoning where language models can learn input\u2013label mappings regardless of the actual identity of the labels. Although we have shown that this behavior is emergent with respect to model scale, the investigation of why scaling unlocks such behaviors (Xie et al., 2022; Chan et al., 2022) is still an open question that we leave for future work.\n# 8 CONCLUSIONS\nIn this paper, we examined the extent to which language models learn in-context by utilizing prior knowledge learned during pretraining versus input\u2013label mappings presented in-context. We first showed that large language models can learn to override semantic priors when presented with enough flipped labels (i.e., input\u2013label mappings that contradict prior knowledge), and that this ability emerges with model scale. We then created an experimental setup that we call SemanticallyUnrelated Label In-Context Learning (SUL-ICL) which removes semantic meaning from labels by replacing natural language targets with semantically-unrelated targets. Successfully doing ICL in the SUL-ICL setup is another emergent ability of model scale. Additionally, we analyzed instructiontuned language models and found that instruction tuning improves the capacity to learn input\u2013label mappings but also strengthens semantic priors. Finally, we examined language model performance on linear classification tasks, finding that successfully performing high-dimensional linear classification emerges with model scale. These results underscore how the in-context learning behavior of language models can change depending on the scale of the language model, and that larger language models have an emergent ability to map inputs to many types of labels, a form of true symbolic reasoning in which input\u2013label mappings can be learned for arbitrary symbols.\n# ACKNOWLEDGEMENTS\nWe thank Sewon Min for detailed suggestions and feedback. Thank you to Percy Liang for providing feedback on the initial results.\n# REFERENCES\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Conference on Neural Information Processing Systems (NeurIPS), 2022. URL https://arxiv.org/abs/2205.11916. Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning (KR), 2012. URL http://commonsensereasoning.org/2011/papers/Levesque. pdf.\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 2021. URL https://arxiv.org/abs/2102.07350. Frieda Rong. Extrapolating to unnatural language processing with GPT-3\u2019s in-context learning: The good, the bad, and the mysterious, 2021. URL https://ai.stanford.edu/blog/ in-context-learning/. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013. URL https://www.aclweb.org/anthology/D13-1170. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022. URL https://arxiv.org/abs/2206.04615. Dustin Tran, Jeremiah Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, Neil Band, Tim G. J. Rudner, Karan Singhal, Zachary Nado, Joost van Amersfoort, Andreas Kirsch, Rodolphe Jenatton, Nithum Thain, Honglin Yuan, Kelly Buchanan, Kevin Murphy, D. Sculley, Yarin Gal, Zoubin Ghahramani, Jasper Snoek, and Balaji Lakshminarayanan. Plex: Towards reliability using pretrained large model extensions, 2022. URL https://arxiv.org/abs/2207.07411. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2022. URL https://arxiv.org/abs/2212.07677. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018. URL https://aclanthology.org/W18-5446. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. Conference on Neural Information Processing Systems (NeurIPS), 2019. URL https://arxiv.org/abs/1905.00537. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022. URL https://arxiv.org/abs/2212.10001. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR), 2023. URL https: //openreview.net/forum?id=1PL1NIMMrw. Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies, 2022. URL https://aclanthology.org/2022.naacl-main.167. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. International Conference on Learning Representations (ICLR), 2022a. URL https://openreview.net/ forum?id=gEZrGCozdqR. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research (TMLR), 2022b. URL https: //openreview.net/forum?id=yzkSU5zdwD.\n# Appendix\nTable of Contents\nA Dataset Creation\nB Investigating the SUL-ICL setup B.1 SUL-ICL is easier than flipped-label ICL . . . . . . . . . . . . . . . . . . . . . B.2 Remapping inputs hurts performance . . . . . . . . . . . . . . . . . . . . . . . B.3 Many target types work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Prompt templates showing input\u2013label relationships work . . . . . . . . . . . . B.5 Semantic prompt templates yield varying results depending on model size . . . . B.6 Large models are robust to out-of-distribution datasets . . . . . . . . . . . . . .\n# C Full experimental results\nC.1 The flipped labels setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 The SUL-ICL setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Instruction tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Linear Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8334/8334522c-bdd2-41de-9c97-c0ca8b54105f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5670/56701ca8-d55b-458a-83aa-1efec176c8a6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Prompt formatting for all datasets. We use varying number of in-context exemplars per class in our experiments, but we show one in-context exemplar per class in this figure for conciseness.</div>\n# A DATASET CREATION\nFigure 9 shows example prompts with inputs and targets from each dataset that we tested (full prompt examples for the seven datasets used in the main paper are shown in Appendix D). For each natural language task, we use the version of the dataset that is available on HuggingFace (Lhoest et al., 2021), and we randomly choose in-context exemplars from the training set and evaluation examples from the validation set, following Min et al. (2022b). For datasets without existing train/validation splits, we use a random 80/20 train/validation split. For the FP dataset, we use the sentences_allagree subset. We also use the binary subset of the ETHOS dataset. Additionally, we use the six coarse labels for the TREC dataset.\nFigure 9 shows example prompts with inputs and targets from each dataset that we tested (full prompt examples for the seven datasets used in the main paper are shown in Appendix D). For each natural language task, we use the version of the dataset that is available on HuggingFace (Lhoest et al., 2021), and we randomly choose in-context exemplars from the training set and evaluation examples from the validation set, following Min et al. (2022b). For datasets without existing train/validation splits, we use a random 80/20 train/validation split.\nFor the FP dataset, we use the sentences_allagree subset. We also use the binary subset of the ETHOS dataset. Additionally, we use the six coarse labels for the TREC dataset.\n# B INVESTIGATING THE SUL-ICL SETUP\n# B.1 SUL-ICL IS EASIER THAN FLIPPED-LABEL ICL\nA natural question about the SUL-ICL setup is whether it is more difficult than the flipped labels setup. Intuitively, one would expect that the SUL-ICL setting is easier than the flipped-label setting because while the model needs to override contradiction labels in the flipped-label setting, it does not need to do so in the SUL-ICL setting.\nA natural question about the SUL-ICL setup is whether it is more difficult than the flipped labels setup. Intuitively, one would expect that the SUL-ICL setting is easier than the flipped-label setting because while the model needs to override contradiction labels in the flipped-label setting, it does not need to do so in the SUL-ICL setting. We investigate this question by analyzing model outputs in the SUL-ICL and flipped-label settings. We use the same results from Section 4 to show model performance in the SUL-ICL setting (specifically, we use the per-dataset results from Figure 3). For the flipped-label setting, we use model outputs and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c0d0/c0d00004-9309-4f99-a1da-346359106bbb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Models perform better in the SUL-ICL setting than they do in the flipped-label setting. Accuracy calculated over 100 evaluation examples with k = 16 in-context exemplars per class.</div>\n<div style=\"text-align: center;\">Figure 10: Models perform better in the SUL-ICL setting than they do in the flipped-label setting. Accuracy calculated over 100 evaluation examples with k = 16 in-context exemplars per class.</div>\nevaluation examples with 100% flipped labels (see Section 3), and we then flip evaluation examples (i.e., higher accuracy means the model can follow flipped predictions) to make comparison easier.5 In Figure 10, we compare model performance in the SUL-ICL setting with model performance in the flipped-label setting. We find that performance is almost always higher in the SUL-ICL setting than it is in the flipped-label setting. In particular, medium-sized models perform much worse in the flipped-label setting than they do in the SUL-ICL setting, with performance differing by up to 74% (text-curie-001 on SST-2). Small and large models, on the other hand, see smaller but still significant performance drops when using flipped-labels compared to SUL-ICL labels. These results suggest that the SUL-ICL setting is indeed easier than the flipped-label setting, and that this trend is particularly true for medium-sized models. Small and large models are still affected by the setting, though perhaps to a lesser degree because small models often do not outperform guessing anyway and large models are more capable of overriding semantic priors (i.e., perform better in flipped-label settings). This may be an indication that the flipped-label setting\u2019s requirement of overriding priors is more difficult than learning mappings to semantically-unrelated labels.\n5The accuracy shown in this section is not always equivalent to 100% minus the accuracy shown in Section 3 because models, particularly small ones, will occasionally return a prediction that is not one of the inputted labels (e.g., trying to answer a question in QQP instead of labeling questions as duplicate/non-duplicate).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a72/4a72977f-1f2e-421e-af01-990fc49410fd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: An overview of remapped inputs, where words are remapped to other words to reduce the semantic meaningfulness of inputs. We use prompts with k = 16 in-context exemplars per class in our experiments, but we show k = 1 in-context exemplar per class in this figure for conciseness.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3b5e/3b5ef21e-745a-4c71-95e6-3956c210343b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Language models fail in the SUL-ICL setting when input words are remapped. Accuracy is calculated over 100 evaluation examples with k = 16 in-context exemplars per class.</div>\n# B.2 REMAPPING INPUTS HURTS PERFORMANCE\nAs a sanity check, we want to show that even large models cannot succeed in the SUL-ICL setup in all environments. For example, when presented with semantically-meaningless inputs, even the largest models should not be able to perform the task because there are no longer any semantics that can be used to learn what the task is (the SUL-ICL setup already removes semantics from labels). To show this, we remap an increasing percentage of input words to other input words at a per-prompt level. We first compile the set of all words used in the inputs for a given prompt, and we then map a randomly selected proportion of those words to other randomly selected words, thereby reducing the semantic meaningfulness of inputs. In this setup, 0% remapped words means that no input words have been changed (i.e., regular SUL-ICL), and 100% remapped words means that every input word has been remapped (i.e., inputs are now a concatenation of random words from other inputs, making them essentially meaningless). An example of this procedure is shown in Figure 11. In Figure 12, we show model performance with respect to the proportion of remapped words. We find that small models generally approach guessing performance at 25%\u201350% remapped words, while large models see linear performance drops, usually reaching guessing accuracy at 75%\u2013100% remapped words. At 100% remapped input words, even the largest models (code-davinci-002 and PaLM-540B) are unable to beat random guessing on almost all datasets.6 These results suggest that larger models are more robust to input noise, but only to some extent because they still cannot consistently learning the required mappings to unscramble the words when a large enough proportion of words have been remapped. Indeed, 100% remapped words is most likely too difficult of a task to learn for these models, as the only way to solve the task reliably would be to unscramble most mapped words back to their original words, which would be difficult for even a human to do given the large number of input words per prompt.\n6TREC is the exception, though it is unclear why large models can outperform random guessing on TREC given that 100% remapped input words is equivalent to completely-scrambled inputs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea20/ea203b91-c034-49c5-a681-ce246ac88d1b.png\" style=\"width: 50%;\"></div>\nFigure 13: SUL-ICL works with many types of semantically-unrelated targets. All tasks are binary classification except TREC, which is six-way classification and uses (Foo/Bar/Iff/Roc/Ket/Dal), (0/1/2/3/4/5/6), (A/B/C/D/E/F), and (Apple/Orange/Banana/Peach/Cherry/Kiwi). Reversed targets such as (0/1) and (1/0) means that, for example, if (0/1) assigns 0 = negative and 1 = positive for sentiment analysis, then (1/0) assigns 1 = negative and 0 = positive. \u201cNatural language\u201d indicates that natural language targets are used (i.e., regular ICL). Accuracy is calculated over 250 evaluation examples inputted to code-davinci-002 with k = 16 in-context exemplars per class.\n# B.3 MANY TARGET TYPES WORK\nIn Section 4, we showed that large language models can learn input\u2013label mappings for one set of semantically-unrelated targets (\u201cFoo\u201d and \u201cBar\u201d), but can they still learn these mappings for other types of semantically-unrelated targets? To test this, we evaluate models in the SUL-ICL setup using varying semantically-unrelated targets in addition to Foo/Bar targets: numerical targets, alphabetical targets, and fruit targets.7 For each target format, we also reverse the targets (e.g., 0 \u21921 and 1 \u21920) to verify that labels can be interchanged, at least within each set of labels. We experiment using natural language targets (i.e., regular ICL) for comparison. Figure 13 shows model performance for each target type used.8 We see that, in most cases, model performance stays relatively constant with respect to the target that is used. Additionally, there is no consistent difference between using natural language targets and using semantically-unrelated targets,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/12b8/12b8f071-29f3-41d0-90d1-e2087a63043c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14: Model accuracy stays relatively consistent with respect to the input format used for SUL-ICL. Accuracy is calculated over 100 evaluation examples inputted to code-davinci-002 with k = 16 in-context exemplars per class.</div>\nwhich may suggest that given a large enough model and enough in-context exemplars, input\u2013label mappings alone are enough to drive model performance. These findings demonstrate that for many types of semantically-unrelated targets, large models can still learn input\u2013label mappings. We can also see that some tasks are too difficult for the model to learn, regardless of whether natural language targets or SUL-ICL targets were used. Specifically, the model cannot significantly outperform random guessing on the QNLI and WSC datasets for any target type, and for this reason, we remove the QNLI and WSC datasets from other experiments.\nCan any prompt format be used for SUL-ICL as long as it clearly presents inputs and their respective labels? We explore this question by comparing the default Input/Output prompt template shown in Figure 9 with five additional formats, where [input] and [label] stand for the inputs and labels respectively (templates are shown in quotes).\n\u2022 Input \u2192Output: \u201c[input]->[label]\u201d \u2022 (Input, Output): \u201c[input], [label]\u201d \u2022 Question/Answer: \u201cQuestion: [input] \\n Answer: [label]\u201d \u2022 Student/Teacher: \u201cStudent: [input] \\n Teacher: [label]\u201d \u2022 Q/A: \u201cQ: [input] \\n A: [label]\u201d\nIn Figure 14, we show model performance for each of the input formats that we tested. We find that no input format is significantly better than any other input format, as the mean accuracy across all NLP tasks for all input formats (which ranges from 77.9% to 87.7%) is within \u00b16.3% of the mean (84.2%). These findings suggest that SUL-ICL may work across many simple formats that present input\u2013label mappings, which may indicate that a factor to succeed in a SUL-ICL setup is that prompt templates should show a clear mapping between an input and its respective label.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a826/a826836a-73b8-4eaa-96d7-a6e3f70e3560.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\u201cInput/Output\u201d Prompt Template Semantic Prompt Template</div>\nFigure 15: Small models do worse than large models do in the SUL-ICL setting when presented with semantically-relevant prompt templates. Accuracy is calculated over 100 evaluation examples inputted to Codex models with k = 16 in-context exemplars per class.\nIn Appendix B.4, we did not test any prompt templates that include semantic information that is relevant to the task (e.g., using \u201cReview: [input] \\n Sentiment: [label]\u201d for SST-2). We thus want to explore this setting in order to investigate whether models use semantic priors more or input\u2013label mappings more they are given a semantically-relevant template. We investigate this by using semantic prompt formats from Zhao et al. (2021) in the SUL-ICL setting and compare these results to the results from using our default \u201cInput/Output\u201d prompt template. We run these experiments on the SST-2, TREC, and RTE datasets\u2014the datasets in our paper that intersect with those used in Zhao et al. (2021)\u2014and we evaluate on the Codex model family. As shown in Figure 15, we find that the smallest Codex model (code-cushman-001) sees performance drop across all tested datasets when switching to semantically-relevant prompt templates. The largest Codex model (code-davinci-002), on the other hand, is relatively unaffected by the change, while the middle Codex model (code-davinci-001) experiences performance changes that vary across datasets. These results suggest that small models get worse at learning input\u2013label mappings when presented with semantically-relevant prompts, perhaps because seeing semantically-charged words encourages the model to try to utilize semantic priors rather than learn input\u2013label mappings in-context. We also see that large models may be more robust to these inputs\u2014their performance being unaffected by the change indicates that despite seeing the semantic prompt templates, they are still able to learn the semantically-unrelated input\u2013label mappings in-context.\n# B.6 LARGE MODELS ARE ROBUST TO OUT-OF-DISTRIBUTION DATASETS\nTran et al. (2022) previously showed that model scale improves robustness to out-of-distribution (OOD) datasets where the input distribution of text for a given task changes. We aim to analyze whether this behavior is present in the SUL-ICL setting. In this experiment, we combine examples from SST-2 and the Rotten Tomatoes dataset (Pang & Lee, 2005, RT)\u2014which is also a sentiment analysis dataset\u2014and prompt the model with in-context exemplars from one dataset while evaluating it on examples from the other dataset. We then test InstructGPT models in a SUL-ICL environment using these varied input distributions. As shown in Table 2, we see that small models (e.g., text-ada-001 and text-babbage-001) suffer from significant performance drops of up to 36% when OOD datasets are used. Large models (e.g., text-curie-001 and text-davinci-001), on the other hand, do not suffer from these drops, with text-curie-001 only seeing a 4% decrease in accuracy and text-davinci-001 seeing no significant change in accuracy. These results suggest that robustness to OOD datasets emerges with scale in the SUL-ICL setup, implying that this behavior could be related to the presentation of input\u2013label mappings (something that both regular in-context learning and SUL-ICL share) and not necessarily the availability of semantic targets (which SUL-ICL lacks).\nDataset\na-1\nb-1\nc-1\nd-1\nSST-2 Only (Baseline)\n80\n91\n94\n93\nSST-2 (In-Context) + RT (Eval)\n54\n63\n90\n93\nRT (In-Context) + SST-2 (Eval)\n44\n61\n90\n92\nTable 2: Robustness to out-of-distribution datasets in the SUL-ICL setup emerges with model scale. Accuracy is calculated over 100 evaluation examples with k = 16 in-context exemplars per class. \u201cIn-Context\u201d: examples used as in-context exemplars. \u201cEval\u201d: examples used as evaluation examples.\n# C FULL EXPERIMENTAL RESULTS\n# C.1 THE FLIPPED LABELS SETTING\nHere, we present per-dataset results for each model family after flipping labels for in-context exemplars, as described in Section 3. In Figure 16, we plot model accuracy with respect to the proportion of labels that we flip for each dataset and for each model family. We exclude the RTE dataset for PaLM models because the prompts from this dataset at k = 16 in-context exemplars per class consistently exceed the maximum-allowable context length. For many model families, we see that large models have better performance than small models do at 0% flipped labels, but that flipping more labels results in performance drops for large models but not for small models. This trend is especially true for the InstructGPT model family and, to a lesser extent, the Codex and PaLM model families. The base GPT-3 model family, on the other hand, does not see this trend happen for most tasks, which is likely due to the fact that even the large models in this model family have trouble outperforming random guessing for many tasks. For example, the largest GPT-3 model (davinci) only achieves guessing accuracy on the QQP and RTE datasets, while the largest InstructGPT and Codex models both achieve 80%+ accuracy on these two tasks. We find that many model families exhibit this behavior on the FP, RTE, and ETHOS datasets. Conversely, the SUBJ dataset seems to show that model performance drops across all model families and for all models within each model family, a result that suggests that it is easier for models to flip their predictions to follow flipped labels for this task, even if the model is small. It is unclear why this task in particular encourages flipping predictions to follow flipped labels more than other tasks do.\n# C.2 THE SUL-ICL SETTING\nIn this section, we show per-dataset results for each model family after converting prompts to our SUL-ICL setup described in Section 4. Figure 17 gives a per-dataset overview of the performance differences between using SUL-ICL labels and using natural language labels as described in Section 4. We exclude the RTE dataset for PaLM models because the prompts from this dataset at k = 16 in-context exemplars per class consistently exceed the maximum allowable context length. We find that for InstructGPT, Codex, and PaLM models, large models see less of a performance drop than small models do when switching from natural language targets to semantically-unrelated targets, implying that they are more capable of learning input\u2013label mappings when semantic priors are unavailable. Conversely, base GPT-3 models do not seem to follow the same trend, specifically in the case of davinci, which (on many tasks) sees the largest performance drops when using SUL-ICL targets despite being the largest model in the family. It is unclear why davinci seems to be the only large model that is not capable of learning input\u2013label mappings in the SUL-ICL setup, though this behavior is consistent with davinci behaving similarly to small models as described in Section 3. In Figure 18, we show per-dataset results for model accuracy with respect to the number of in-context exemplars provided. We do not run experiments on InstructGPT models and davinci in order to reduce cost. Lines do not always extend to k = 32 due to context-length constraints. These results indicate that for many datasets and model families, larger models are better at utilizing in-context exemplars in a SUL-ICL setup than small models are. This suggests that larger language models are more capable than small language models are at learning input\u2013label mappings using the exemplars presented in-context rather than using prior knowledge from pretraining.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/133b/133bfb6a-949b-4673-bf7b-785338d027b9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16: Larger models are better able to override semantic meanings when presented with flipped labels than smaller models are for many datasets and model families. Accuracy is calculated over 100 evaluations examples per dataset with k = 16 in-context exemplars per class.</div>\nFigure 16: Larger models are better able to override semantic meanings when presented with flipped labels than smaller models are for many datasets and model families. Accuracy is calculated over 100 evaluations examples per dataset with k = 16 in-context exemplars per class.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b2ef/b2efcb84-5772-4594-94a1-12454fec8e89.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 17: For many datasets and model families, performance decreases more for small models than it does for large models when using semantically-unrelated targets instead of natural language targets. Accuracy is calculated over 100 evaluation examples with k = 16 in-context exemplars per class.</div>\nFigure 17: For many datasets and model families, performance decreases more for small models than t does for large models when using semantically-unrelated targets instead of natural language targets. Accuracy is calculated over 100 evaluation examples with k = 16 in-context exemplars per class.\nWe compare PaLM and Flan-PaLM model behaviors on a per-dataset level as an extension of Section 5. First, we show model behavior in the SUL-ICL setting in Figure 19, finding that for the SST-2, QQP, RTE, and ETHOS datasets, Flan-PaLM models achieve higher performance than their respective PaLM models. On the SST-2 dataset in particular, Flan-PaLM-8B outperforms PaLM-8B by 28% and even outperforms PaLM-62B by 2%. There are some datasets, however, for which instruction tuning seemed to decrease performance (e.g., PaLM-8B outperforms Flan-PaLM-8B on SUBJ by 23%). These results indicate that for many tasks, instruction tuning increases the model\u2019s capacity to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2023/2023e79d-1a60-456a-ab62-3e933b0e5763.png\" style=\"width: 50%;\"></div>\nFigure 18: For many datasets and model families, large language models are better at using in-context exemplars to learn input\u2013label mappings than small language models are. Accuracy is calculated over 100 examples in the SUL-ICL setup.\n<div style=\"text-align: center;\">Figure 18: For many datasets and model families, large language models are better at using in-context exemplars to learn input\u2013label mappings than small language models are. Accuracy is calculated over 100 examples in the SUL-ICL setup.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e02/3e022616-97f6-42de-8df8-ae74a95c2f61.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"># exemplars per class # exemplars per class # exemplars per class</div>\nFigure 19: For many datasets, instruction-tuned language models are better at learning input\u2013label mappings than pretraining-only language models are. Accuracy is calculated over 100 evaluation examples in the SUL-ICL setup.\nlearn input\u2013label mappings in-context (though there are some exceptions), which follows the findings from Section 5. We also found that across most datasets, Flan-PaLM does worse than PaLM and scores close to 0% accuracy when given one in-context exemplar per class, yet this does not seem to be the case when two or more in-context exemplars per class are presented. Why this occurs is unknown, but it may indicate that Flan-PaLM does not give a response that is part of the target set of responses (e.g., does not output \u201cFoo\u201d or \u201cBar\u201d) in a 1-shot SUL-ICL setting. In Figure 20, we show results for PaLM and Flan-PaLM in the flipped-label setting. For all datasets,9 we find that every Flan-PaLM model achieves better performance than its respective PaLM model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5614/5614821c-c91b-485f-9c2a-fe0392686761.png\" style=\"width: 50%;\"></div>\nPaLM models notably have lower accuracy when more labels are flipped, which means that PaLM models are better than Flan-PaLM models are at learning flipped input\u2013label mappings presented in context, suggesting that it is harder for Flan-PaLM models to override semantic priors. This suggests that instruction tuning reinforces the model\u2019s semantic priors or gives it more semantic priors, making it more difficult for the model to override its prior knowledge.\nAlgorithm 1 Generating one evaluation example for N-dimensional linear classification (y =\na1x1 + ... + aNxN) with k in-context exemplars per class. Random N-D vectors are generated using\nnp.random.randint().\n1: procedure GENERATEEVAL(N, k)\n2:\na \u2190random N-D vector\n\u25b7Ground-truth coefficients\n3:\np \u2190random N-D vector\n\u25b7A pivot point\n4:\nt = \u27e8a, p\u27e9\n\u25b7Threshold between positive and negative examples\n5:\nxtrain \u2190[ ], ytrain \u2190[ ]\n6:\nfor i \u21901 to k do\n\u25b72k in-context exemplars\n7:\nx+ \u2190random N-D vector conditioned on \u27e8x+, a\u27e9> t\n\u25b7Positive example\n8:\nx\u2212\u2190random N-D vector conditioned on \u27e8x\u2212, a\u27e9\u2264t\n\u25b7Negative example\n9:\nxtrain \u2190xtrain + [x+, x\u2212]\n10:\nytrain \u2190ytrain + [1, \u22121]\n11:\nend for\n12:\nxeval \u2190random N-D vector\n13:\nyeval \u21901 if \u27e8xeval, a\u27e9> t, else \u22121\n14:\nreturn xtrain, ytrain, xeval, yeval\n15: end procedure\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3cc8/3cc84f68-122a-421a-a260-bb9e4af9d380.png\" style=\"width: 50%;\"></div>\nFigure 21: The largest Codex model (code-davinci-002) can perform linear classification up to 64 dimensions, while smaller Codex models do not outperform random guessing at 16 dimensions. PaLM models can all perform linear classification up to 8 dimensions with little difference in performance with respect to model scale. Standard SVM algorithm performance shown for comparison. Accuracy is calculated over 100 evaluation examples per dataset with k = 16 in-context exemplars per class.\nIn Figure 21, we show model performance for Codex and PaLM models versus an exponentially increasing number of dimensions N (the data generation procedure is shown in Algorithm 1). We also include results from a standard polynomial SVM implemented via scikit-learn (svm.SVC(kernel=\u2018poly\u2019)) for comparison. We find that for the Codex model family, the largest model can successfully perform linear classification up to N = 64, while the smaller models reach guessing performance at approximately N = 16. For PaLM models, on the other hand, model scale does not seem to significantly correlate with the number of dimensions to which the model can perform linear classification, though all PaLM models can perform linear classification up to at least N = 8.10 Neither PaLM models nor Codex models can outperform an SVM baseline. These results suggest that model size alone does not necessarily unlock the ability to perform linear classification at high dimensionality (since PaLM-540B does not outperform PaLM-8B or PaLM62B), but instead imply that there is another scaling factor seen in the Codex models that allows this ability to emerge. Because we do not know the particular scaling factors of the Codex model family, we leave exploration as to what factors unlock this ability to future work.\nIn Appendix D.1\u2013Appendix D.7, we include an example of a full few-shot prompt for each of the seven datasets used in the main paper. We show prompts with k = 16 in-context exemplars per class and the Input/Output prompt template from Appendix B.4 (our default experimental setup) and natural language targets (i.e., regular ICL). Prompts in a SUL-ICL and flipped-label ICL setup can be obtained by swapping labels with the desired labels (e.g., replacing \u201cNegative Sentiment\u201d with \u201cFoo\u201d and \u201cPositive Sentiment\u201d with \u201cBar\u201d to convert SST-2 in a regular ICL setup to SST-2 in a SUL-ICL setup). Prompts (especially from the ETHOS dataset) may contain offensive language\u2014note that all examples are directly taken from the existing datasets as referenced in Appendix A. In Appendix D.8, we provide an example of a full prompt for the linear classification task from Section 6 and Appendix C.4. This prompt uses the same default experimental setup as the prompts from Appendix D.1\u2013Appendix D.7 but uses SUL-ICL targets since we only used this dataset in SUL-ICL settings. For reference, negative examples are labeled \u201cFoo\u201d and positive examples are labeled \u201cBar\u201d (see Algorithm 1 for details about negative and positive examples).\n# D.1 SST-2\nPrompt: Input: a pale imitation Output: Negative Sentiment Input: carries you along in a torrent of emotion Output: Positive Sentiment Input: trashy time Output: Negative Sentiment Input: all the complexity and realistic human behavior of an episode of general hospital Output: Negative Sentiment Input: hold dear about cinema , Output: Positive Sentiment Input: inauthentic Output: Negative Sentiment Input: feels like very light errol morris , focusing on eccentricity but failing , ultimately , to make something bigger out of its scrapbook of oddballs Output: Negative Sentiment Input: with purpose and finesse Output: Positive Sentiment Input: feel a nagging sense of deja vu Output: Positive Sentiment Input: and mawkish dialogue Output: Negative Sentiment Input: , but i believe a movie can be mindless without being the peak of all things insipid . Output: Negative Sentiment Input: it does elect to head off in its own direction\nOutput: Negative Sentiment Input: the sheer joy and pride Output: Positive Sentiment Input: so larger than life Output: Positive Sentiment Input: to its superior cast Output: Positive Sentiment Input: one of the more intelligent children \u2019s movies to hit theaters this year . Output: Answer: Positive Sentiment\n# D.2 SUBJ\n# D.3 TREC\n# Prompt:\nInput: What is the real name of the singer , Madonna ? Output: Human Being Input: What snack food has ridges ? Output: Entity Input: How do you correctly say the word \u2018 qigong \u2019 ? Output: Description and Abstract Concept Input: Which Bloom County resident wreaks havoc with a computer ? Output: Human Being\nOutput: Entity Input: What does Ms. , Miss , and Mrs. stand for ? Output: Abbreviation Input: What is the abbreviation of the company name \u2018 General Motors \u2019 ? Output: Abbreviation Input: What was the name of the orca that died of a fungal infection ? Output: Entity Input: When did the Carolingian period begin ? Output: Numeric Value Input: What architect originated the glass house designed the Chicago Federal Center had a philosop of \u201c less is more , \u201d and produced plans that were the forerunner of the California ranch house ? Output: Human Being Input: How high must a mountain be to be called a mountain ? Output: Numeric Value Input: What does snafu stand for ? Output: Abbreviation Input: Who shared a New York City apartment with Roger Maris the year he hit 61 home runs ? Output: Human Being Input: What is the location of McCarren Airport ? Output: Location Input: How many people die of tuberculosis yearly ? Output: Numeric Value Input: What is IOC an abbreviation of ? Output: Abbreviation Input: What is HTML ? Output: Abbreviation Input: What does the \u201c blue ribbon \u201d stand for ? Output: Abbreviation Input: What does the term glory hole mean ? Output: Description and Abstract Concept Input: What does the abbreviation cwt. ? Output: Abbreviation Input: How many students attend the University of Massachusetts ? Output: Numeric Value Input: Who was the captain of the tanker , Exxon Valdez , involved in the oil spill in Prince Willia Sound , Alaska , 1989 ?\nInput: What should the oven be set at for baking Peachy Oat M Output: Entity Input: What bread company used to feature stickers of the Cis Output: Human Being Input: Why do airliners crash vs. gliding down ? Output: Description and Abstract Concept Input: What is a fear of fish ? Output: Entity Input: Which country did Hitler rule ? Output: Location Input: What does A&W of root beer fame stand for ? Output: Abbreviation Input: How does a hydroelectric dam work ? Output: Description and Abstract Concept Input: What year did the Vietnam War end ? Output: Numeric Value Input: What are some children \u2019s rights ? Output: Description and Abstract Concept Input: What is Colin Powell best known for ? Output: Description and Abstract Concept Input: What is the largest island in the Mediterranean Sea ? Output: Location Input: What is a fear of weakness ? Output: Entity Input: What \u2019s the world \u2019s most common compound ? Output: Entity Input: Why do people in the upper peninsula of Michagin say Output: Description and Abstract Concept Input: Why do many Native American students not complete  Output: Description and Abstract Concept Input: When are the Oscars Academy Awards in 1999 ? Output: Numeric Value Input: Where can I get cotton textiles importer details ? Output: Location Input: What is a fear of childbirth ? Output: Entity\nOutput: Human Being Input: What are differences between 1980 and 1990 ? Output: Description and Abstract Concept Input: What 2 statues did France give to other countries ? Output: Entity Input: Whose biography by Maurice Zolotow is titled Shooting Star ? Output: Human Being Input: What kind of gas is in a fluorescent bulb ? Output: Answer: Entity\n# D.4 QQP\n# Prompt:\nInput: Why did Indian Government introduced 2000 note instead of the new 1000 note? Meanwhile, they introduced the new 500 note for old 500 note. If 500 and 1000 notes are banned then why are new 500 and 2000 notes being introduced? Output: Duplicate Input: Where can I get a free iTunes gift card without doing a survey or download? How can I download the Itunes gift card generator with no surveys? Output: Not a duplicate Input: Is petroleum engineering still a good major? Is the petroleum engineering major still worthy to choose today? And how about in the future 2020-2025? Output: Duplicate Input: Is Minecraft Turing complete? Why is Minecraft so popular? Output: Not a duplicate Input: What are some HR jobs in Mumbai? How do I get a HR job in Bangalore? Output: Not a duplicate Input: To which caste and category does the surname Saini belong to? \u201cWhich caste (General/OBC/SC/ST) does \u201c\u201cBera\u201d\u201d surname belongs to?\u201d Output: Not a duplicate Input: Who are burning the schools in Kashmir and why? Why are separatists burning schools in Kashmir? Output: Duplicate\nInput: How do I remove onclick ads from Chrome? How do I reduce the CPA on my Facebook Ads? Output: Not a duplicate Input: How should I start learning Python? How can I learn advanced Python? Output: Duplicate Input: How do I stop feeling sad? How do I stop feeling sad about nothing? Output: Not a duplicate Input: How can you lose 10 pounds in 40 days? What are some great diet plans to lose 10 pounds in 40 days? Output: Duplicate Input: What are job opportunities after completing one year of a HAL graduate apprenticeship? What are some opportunities after completing one year of a HAL graduate apprenticeship? Output: Duplicate Input: Why did liquidprice.com fail? Why did ArchiveBay.com fail? Output: Not a duplicate Input: Why is everyone on Quora obsessed with IQ? Why are people on Quora so obsessed with people\u2019s high IQs? Output: Duplicate Input: I want to learn Chinese, which app is better for it? I am basically Non IT Background.. I want learn course...Some of my friends suggested Linux a PLSql.. I want to know which is best option for me? Output: Not a duplicate Input: How is black money gonna go off with no longer the use of same 500 and 1000 notes? How is discontinuing 500 and 1000 rupee note going to put a hold on black money in India? Output: Duplicate Input: How did Jawaharlal Nehru die? Was it really a sexually transmittable disease? How can I become a great person like Jawaharlal Nehru? Output: Not a duplicate Input: What are the career option after completing of B.tech? What are the career options available after completing a B.Tech? Output: Duplicate Input: What would be next strike from PM Modi after Demonetisation? What will be the next move by PM Modi to improve India?\n# D.5 FP\n# Output: Negative\nInput: Finnish Bank of +\u00e0land reports operating profit of EUR 2.2 mn in the first quarter of 2010 , down from EUR 6.3 mn in the corresponding period in 2009 .\nInput: Finnish IT consultancy Satama Interactive Oyj posted a net profit of 1.4 mln euro $ 2.0 mln for the first nine months of 2007 , compared to a net loss of 462,000 euro $ 664,000 for the same period of 2006 .\nInput: Shares in Royal and Sun Alliance continued to slide back from a 12-month high of 172p last month , after a potential suitor ruled itself out of a takeover bid .\n# D.6 ETHOS\nPrompt: Input: i like dead refugees on my shore Output: Hate Speech\nInput: There was a protest today in Birmingham against hate. It was so great to see Muslims and Gays marching together in unity.\nInput: Very sad for those people. i was a young boy in the 90s when Ceausescu fell in Romania and the inflation skyrocketed. But here they are on another level and looks like there is no end soon. How can this happen to one of the richest country in the world? Venezuela has the largest resource of oil in the world. When you look at Qatar, UAE, Saudi Arabia they done miracles. It looks like is not enough to have oil, you have to know what to do with it. Output: Not Hate Speech Input: guy is low IQ af Output: Not Hate Speech Input: Here in 2019. May history be on the right side this time. Output: Not Hate Speech Input: When I see men running with those plastic legs I laugh my ass out so hard I fart some times Output: Hate Speech Input: The rotten, nasty bastards! Output: Not Hate Speech Input: or when the man walked up to the little girl with the gay pride flag i wanted to jump into the video and punch him but ya know thats assault Output: Hate Speech Input: Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can. Output: Hate Speech Input: OMG Lucas !! Such a great kid, such a big grow Output: Not Hate Speech Input: Im going to choke you with your cross necklace idiotic religious pig Output: Hate Speech Input: I very much appreciate the Asian woman\u2019s smarts. Output: Not Hate Speech Input: fuck bulgaria stupid country Output: Hate Speech Input: well it\u2019s clear now why europeans can\u2019t differ niggers and shitskins from human Output: Hate Speech Input: I notice that the LGBT cancer is starting to slowly affect SA countries like Japan. Output: Hate Speech Input: You guys I fell so bad, I hope this don\u2019t happen again. Im a Islam Output: Not Hate Speech Input: My smile is beaming, my skin is gleaming! YUH\nInput: or when the man walked up to the little girl with the gay pride flag i wanted to jump into th video and punch him but ya know thats assault Output: Hate Speech\nOutput: Not Hate Speech\nInput: I really like a lot of what you said here. I feel like i can barely speak up about this side of the topic because people are quick to shut down those who try to be too serious about it. \u2019Anxiety\u2019 and \u2019depression\u2019 and \u2019mental illness\u2019 have almost become buzz words. It seems that a lot of people don\u2019t know how difficult it is to really suffer and feel out of control of it all. As you spoke about, there\u2019s a fine line these days between awareness and influence. People get ideas in their heads and see the memes and the relateable content and start linking everything to their own lives. Before you know it, you\u2019ve got perfectly healthy people being tainted by the world and people around them,\nimposing problems upon themselves and making life more difficult than it needs to be. It desensitises the whole situation and now I have people coming to me with real problems who don\u2019t want to speak up because of the upsurge in people talking about it. They feel they wouldn\u2019t be taken seriously. And that\u2019s horrible. I do understand though that it\u2019s an impossible seesaw to balance since so many people are involved and so many minds with a million ideas and actions are impossible to control and have on the same wave length. Output: Answer: Not Hate Speech\nimposing problems upon themselves and making life more difficult than it needs to be. It desensitises the whole situation and now I have people coming to me with real problems who don\u2019t want to speak up because of the upsurge in people talking about it. They feel they wouldn\u2019t be taken seriously. And that\u2019s horrible. I do understand though that it\u2019s an impossible seesaw to balance since so many people are involved and so many minds with a million ideas and actions are impossible to control and have on the same wave length.\nD.7 RTE\n# D.7 RTE\n# Prompt:\nInput: At least 19 people have been killed in central Florida in the city of Lady Lake and Paisley after severe storms and a tornado ripped through the cities in the middle of the night. Eleven of those killed were in Paisley and three were in Lady Lake. The death toll is expected to rise as rescue crews resume tomorrow morning. Volusia, Sumter, Lake and Seminole counties have all been declared a state of an emergency as dozens of houses, mobile homes and a church were destroyed. Clothes and furniture are scattered around the wrecked houses and pieces of trees are scattered about. Cars are reported to have been turned over or thrown around in the air. \u201cOur priority today is search and rescue,\u201d said Gov. of Florida, Charlie Crist. Rescuers are still looking through the wreckage to find survivors of those who might have been killed.\nGov. of Florida, Charlie Crist, has visited the cities of Lady Lake and Paisley. Output: Does not entail\nInput: Glue sniffing is most common among teenagers. They generally grow out of it once other drugs such as alcohol and cannabis become available to them. Seven-year-olds have been known to start \u201cglue sniffing\u201d. Because of the social stigma attached to \u201cglue sniffing\u201d most snifters stop around 16 or 17 years, unless they are seriously addicted.\nInput: Neil Armstrong was an aviator in the Navy and was chosen with the second group of astronau in 1962. Made seven flights in the X-15 program (1960 photo), reaching an altitude of 207,500 fe Was backup command pilot for Gemini 5, command pilot for Gemini 8, backup command pilot f Gemini 11, backup commander for Apollo 8, and commander for Apollo 11: successfully completin the first moonwalk.\nInput: Rabies is a viral disease of mammals and is transmitted primarily through bites. Annually 7,000 to 8,000 rabid animals are detected in the United States, with more than 90 percent of the case in wild animals. Rabies is fatal in humans.\nInput: Rabies is a viral disease of mammals and is transmitted primarily through bites. Annually, 7,000 to 8,000 rabid animals are detected in the United States, with more than 90 percent of the cases in wild animals.\nOutput: Does not entail\nInput: There are suppositions that the US Democratic Congress may re-establish the luxury taxes which were already once introduced in the 1990s. The suppositions resulted in the National Association of Watch and Clock Collectors commissioning a report on various tax issues. Material goods such as jewelry, watches, expensive furs, jet planes, boats, yachts, and luxury cars had already been subjected to additional taxes back in 1990. After 3 years these taxes were repealed, though the luxury automobiles tax was still active for the next 13 years.\nThe US Congress may re-establish luxury taxes. Output: Entails\nInput: FBI agent Denise Stemen said in an affidavit that Lowe\u2019s alerted the FBI recently that intruder had broken into its computer at company headquarters in North Carolina, altered its compute programs and illegally intercepted credit card transactions.\nInput: A man who died during the G20 protests was pushed back by a police line minutes earlie independent investigators have said. Ian Tomlinson, 47, who died of a heart attack, was blocke from passing through a police cordon as he attempted to walk home from work at a newsagent, th Independent Police Complaints Commission (IPCC) said. He was caught on several CCTV camera walking up King William Street where he was confronted by uniformed officers shortly before 7.30pm last Wednesday.\nInput: The father of an Oxnard teenager accused of gunning down a gay classmate who was romantically attracted to him has been found dead, Ventura County authorities said today. Bill McInerney, 45, was found shortly before 8 a.m. in the living room of his Silver Strand home by a friend, said James Baroni, Ventura County\u2019s chief deputy medical examiner. The friend was supposed to drive him to a court hearing in his son\u2019s murder trial, Baroni said. McInerney\u2019s 15-year-old son Brandon, is accused of murder and a hate crime in the Feb. 12, 2008, shooting death of classmate Lawrence \u201cLarry\u201d King, 15. The two boys had been sparring in the days before the killing, allegedly because Larry had expressed a romantic interest in Brandon.\nBill McInerney is accused of killing a gay teenager. Output: Does not entail\nOutput: Does not entail\nInput: There is no way Marlowe could legally leave Italy, especially after an arrest warrant has been issued for him by the authorities. Assisted by Zaleshoff, he succeeds in making his escape from Milan.\nMarlowe supported Zaleshoff. Output: Does not entail\nInput: A former federal health official arrested in the Virginia Fontaine Addictions Foundation scandal has been fined $107,000 for tax evasion. Patrick Nottingham, 57, was also sentenced to 18 months house arrest and ordered to complete 150 hours of community service work. The fine represents 50% of the federal income tax Nottingham did not pay on nearly $700,000 in kickbacks he received in return for approving excessive funding to the foundation in 1999 and 2000. In November 2005, Nottingham pleaded guilty to fraud and influence peddling and received a conditional sentence of two years less a day. \u201cMr. Nottingham was not only involved in fraudulent activity, he compounded that offence by not reporting that income,\u201d said Crown attorney Michael Foote at a sentencing hearing earlier this week. \u201cHe effectively committed two sets of extraordinarily serious offences.\u201d Nottingham\u2019s fine is the minimum allowed by law. Foote said there is little expectation Nottingham will ever pay off the fine. Patrick Nottingham is involved in the Virginia Fontaine Addictions Foundation scandal.\nPatrick Nottingham is involved in the Virginia Fontaine Addictions Foundation scandal. Output: Entails\nInput: Seoul City said Monday a 690-meter-tall, 133-story multifunctional skyscraper will be constructed in Sangam-dong. Once built, it will be the second highest after the 800-meter-high Burj Dubai, which is under construction, by South Korean developer Samsung C&T. The construction will cost more than 3.3 trillion won ($2.37 billion), the city estimates. To raise funds, 23 local developers",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of how in-context learning (ICL) in language models is influenced by semantic priors and input-label mappings, highlighting the need for a new understanding of these interactions to enhance model performance.",
        "problem": {
            "definition": "The problem centers on understanding how language models utilize semantic prior knowledge versus learned input-label mappings during in-context learning tasks.",
            "key obstacle": "Existing methods have not adequately explored the conditions under which models can override semantic priors with input-label mappings, particularly as model size increases."
        },
        "idea": {
            "intuition": "The idea stems from observing that larger language models exhibit a unique ability to override semantic priors when presented with contradictory input-label mappings.",
            "opinion": "The proposed idea involves investigating the emergent capabilities of large language models in learning input-label mappings from in-context examples, even when those mappings contradict prior knowledge.",
            "innovation": "The primary innovation is the introduction of a new experimental setup, Semantically-Unrelated Label In-Context Learning (SUL-ICL), which isolates input-label mappings from semantic meanings, allowing for a clearer analysis of model behavior."
        },
        "method": {
            "method name": "In-Context Learning with Semantic Prior and Input-Label Mapping Interaction",
            "method abbreviation": "ICL-SP-ILM",
            "method definition": "This method involves presenting language models with in-context exemplars that either align with or contradict their semantic priors to assess their ability to learn from input-label mappings.",
            "method description": "The core of the method revolves around testing model performance across various tasks using different label configurations, including flipped and semantically unrelated labels.",
            "method steps": [
                "Select a set of language models of varying sizes.",
                "Design experiments with three setups: regular ICL, flipped-label ICL, and SUL-ICL.",
                "Evaluate model performance on several NLP tasks using different in-context exemplars.",
                "Analyze the results to determine the impact of model size on the ability to override semantic priors."
            ],
            "principle": "The effectiveness of this method is rooted in the observation that larger models can adaptively learn input-label mappings, even when semantic priors suggest otherwise, demonstrating a shift in learning dynamics with scale."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on seven NLP tasks, including Sentiment Analysis, Question Classification, and Hate Speech Detection, using various model families like GPT-3, InstructGPT, Codex, and PaLM.",
            "evaluation method": "Model performance was assessed by comparing accuracy across different setups, measuring how well models could learn from in-context exemplars under varying conditions of semantic prior availability."
        },
        "conclusion": "The experiments revealed that larger language models exhibit a significant ability to learn input-label mappings in the absence of semantic priors, which is an emergent capability that enhances their performance on various tasks.",
        "discussion": {
            "advantage": "The proposed approach allows for a deeper understanding of how model scale influences learning behavior, particularly in the context of overriding semantic knowledge with learned mappings.",
            "limitation": "One limitation is that the method's effectiveness may vary across different tasks and model architectures, suggesting that further research is needed to generalize findings.",
            "future work": "Future research should explore optimizing instruction tuning methods to balance the enhancement of input-label mapping learning while minimizing reliance on semantic priors."
        },
        "other info": {
            "acknowledgements": "The authors thank Sewon Min for detailed suggestions and feedback, and Percy Liang for providing insights on the initial results."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses how in-context learning (ICL) in language models is influenced by semantic priors and input-label mappings, highlighting the need for a new understanding of these interactions."
        },
        {
            "section number": "1.2",
            "key information": "The paper emphasizes the impact of larger language models in overriding semantic priors when presented with contradictory input-label mappings, demonstrating the relevance of ICL."
        },
        {
            "section number": "3.1",
            "key information": "The method involves presenting language models with in-context exemplars that either align with or contradict their semantic priors to assess their ability to learn from input-label mappings."
        },
        {
            "section number": "3.4",
            "key information": "The experiments revealed that larger language models can adaptively learn input-label mappings, even when semantic priors suggest otherwise, indicating robust contextual adaptation."
        },
        {
            "section number": "4.1",
            "key information": "The proposed experimental setup, Semantically-Unrelated Label In-Context Learning (SUL-ICL), isolates input-label mappings from semantic meanings, highlighting the influence of effective prompt design."
        },
        {
            "section number": "6.1",
            "key information": "One limitation noted in the paper is that the method's effectiveness may vary across different tasks and model architectures, indicating challenges related to model bias."
        },
        {
            "section number": "7",
            "key information": "Future research should explore optimizing instruction tuning methods to balance the enhancement of input-label mapping learning while minimizing reliance on semantic priors."
        }
    ],
    "similarity_score": 0.7198656461193114,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Larger language models do in-context learning differently.json"
}