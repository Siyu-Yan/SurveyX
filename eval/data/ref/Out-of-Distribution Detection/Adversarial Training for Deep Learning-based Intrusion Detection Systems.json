{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2104.09852",
    "title": "Adversarial Training for Deep Learning-based Intrusion Detection Systems",
    "abstract": "Nowadays, Deep Neural Networks (DNNs) report state-of-the-art results in many machine learning areas, including intrusion detection. Nevertheless, recent studies in computer vision have shown that DNNs can be vulnerable to adversarial attacks that are capable of deceiving them into misclassification by injecting specially crafted data. In security-critical areas, such attacks can cause serious damage; therefore, in this paper, we examine the effect of adversarial attacks on deep learning-based intrusion detection. In addition, we investigate the effectiveness of adversarial training as a defense against such attacks. Experimental results show that with sufficient distortion, adversarial examples are able to mislead the detector and that the use of adversarial training can improve the robustness of intrusion detection.",
    "bib_name": "debicha2021adversarialtrainingdeeplearningbased",
    "md_text": "# Adversarial Training for Deep Learning-based Intrusion Detection Systems\nIslam Debicha\n# Islam Debicha\nRoyal Military Academy and Universit\u00b4e Libre de Bruxelles Brussels, Belgium Email: debichasislam@gmail.com\n20 Apr 2021\n# Jean-Michel Dricot\nUniversit\u00b4e Libre de Bruxelles Brussels, Belgium Email: jdricot@ulb.ac.be\nUniversit\u00b4e Libre de Bruxelles Brussels, Belgium\n[cs.CR]\nAbstract\u2014Nowadays, Deep Neural Networks (DNNs) report stateof-the-art results in many machine learning areas, including intrusion detection. Nevertheless, recent studies in computer vision have shown that DNNs can be vulnerable to adversarial attacks that are capable of deceiving them into misclassification by injecting specially crafted data. In security-critical areas, such attacks can cause serious damage; therefore, in this paper, we examine the effect of adversarial attacks on deep learning-based intrusion detection. In addition, we investigate the effectiveness of adversarial training as a defense against such attacks. Experimental results show that with sufficient distortion, adversarial examples are able to mislead the detector and that the use of adversarial training can improve the robustness of intrusion detection. Keywords\u2013Intrusion detection; deep learning; Adversarial attacks; Adversarial training.\narXiv:2104.09852v1\n# I. INTRODUCTION\nWith the growth of the computer and telecommunications industry and the expansion of the Internet, there has been a significant escalation of cyberattacks targeting all types of networks, as attackers are increasingly motivated to develop new ways to penetrate systems, given the great reward. As a result, securing these networks has become a crucial area of interest. Intrusion Detection Systems (IDS), which are designed to detect and identify anomalies and attacks, are gaining popularity and are presented as one of the solutions against these cyber-attacks. There are mainly two types of intrusion detection systems: those based on signatures and those based on anomaly detection. The first one works more or less in the same way as most antivirus systems by maintaining a database with all known attack signatures. An exhaustive comparison of the incoming traffic with the signature database allows the system to determine if it represents an attack. These systems are remarkably effective at detecting known attacks and offer high accuracy but are obviously unable to detect zero-day\n# Thibault Debatty\nRoyal Military Academy Brussels, Belgium\nRoyal Military Academy Brussels, Belgium Email: thibault.debatty@rma.ac.be\n# Wim Mees\nRoyal Military Academy Brussels, Belgium\nEmail: wim.mees@rma.ac.be\nexploits. This is essentially what drives the use of anomalybased intrusion detection systems that work by modeling the normal behavior of traffic and network activities and then comparing new traffic to this baseline. Several research studies have examined the use of different Machine Learning (ML) techniques to improve the accuracy of anomaly-based IDS [1][2]. Nevertheless, the lack of transferability and the dependence of traditional machine learning on domain knowledge (feature engineering) have been among the main reasons for substituting them with DNNs which not only solved these problems but also yielded, in most cases, the highest accuracies, making them the state-of-the-art in the field of anomaly-based intrusion detection [3].\nexploits. This is essentially what drives the use of anomalybased intrusion detection systems that work by modeling the normal behavior of traffic and network activities and then comparing new traffic to this baseline. Several research studies have examined the use of different Machine Learning (ML) techniques to improve the accuracy of anomaly-based IDS [1][2]. Nevertheless, the lack of transferability and the dependence of traditional machine learning on domain knowledge (feature engineering) have been among the main reasons for substituting them with DNNs which not only solved these problems but also yielded, in most cases, the highest accuracies, making them the state-of-the-art in the field of anomaly-based intrusion detection [3]. Despite their popularity, DNNs have proven to be vulnerable to adversarial attacks in computer vision where, by introducing imperceptible changes in an image, an adversary can mislead the classifier. When applied to machine learningbased security products, these attacks can lead to a critical security breach. Although a considerable amount of studies has been conducted on adversarial attacks in computer vision, there are very few studies on this issue in intrusion detection. Therefore, the contribution of this paper is double: (1) we study the effect of adversarial attacks on deep learning-based intrusion detection systems. For that, three adversarial attacks are tested: Fast Gradient Sign Method (FGSM) [4], Basic Iterative Method (BIM) [5] and Projected Gradient Descent (PGD) [6] showing that adversarial attacks are able, given enough strength, to mislead the IDS significantly. In addition, (2) this is the first study, to the best of our knowledge, to examine the effectiveness of adversarial training as a defense against adversarial attacks for intrusion detection systems. In what follows, we briefly recall the concept of DNNs, present an overview of related work and explain the idea of adversarial examples in Section II. The experimental approach is explained in Section III. Results and discussions are presented in Section IV. Concluding remarks and suggestions for\nDespite their popularity, DNNs have proven to be vulnerable to adversarial attacks in computer vision where, by introducing imperceptible changes in an image, an adversary can mislead the classifier. When applied to machine learningbased security products, these attacks can lead to a critical security breach. Although a considerable amount of studies has been conducted on adversarial attacks in computer vision, there are very few studies on this issue in intrusion detection. Therefore, the contribution of this paper is double: (1) we study the effect of adversarial attacks on deep learning-based intrusion detection systems. For that, three adversarial attacks are tested: Fast Gradient Sign Method (FGSM) [4], Basic Iterative Method (BIM) [5] and Projected Gradient Descent (PGD) [6] showing that adversarial attacks are able, given enough strength, to mislead the IDS significantly. In addition, (2) this is the first study, to the best of our knowledge, to examine the effectiveness of adversarial training as a defense against adversarial attacks for intrusion detection systems. In what follows, we briefly recall the concept of DNNs, present an overview of related work and explain the idea of adversarial examples in Section II. The experimental approach is explained in Section III. Results and discussions are presented in Section IV. Concluding remarks and suggestions for\n# possible follow-up work are given in Section V.\npossible follow-up work are given in Section V.\nII. BACKGROUND\nA. Deep Neural Network (DNN) DNN refers to a machine learning algorithm made up of multiple interconnected layers where each layer is composed of several nodes - called neurons. Within each neuron, an activation function operates as a basic computing unit. The activation function input on a neuron is the parameter-weighted output of the immediately preceding layer, whilst each layer\u2019s output is at the same time the next layer\u2019s input. Frequently described as an end-to-end machine learning process, DNN is capable of learning complex patterns based on limited prior knowledge of input data representation. As a result, deep learning models are widely applied to address large-scale data problems that are frequently inadequately handled by traditional machine learning algorithms. DNN layers fall into three categories: the input layer, the output layer, and, in between, the hidden layer. For large-scale input data, it may be necessary to use several hidden layers so as to learn the subjacent correlation. DNN can be seen as a function f(\u00b7), f \u2208F: Rn \u2192Rm. let \u0398 be the DNN parameters. Training the model involves finding the optimal parameters \u0398 where the loss function J (e.g., cross-entropy) is minimal. For the classification task, the outputs of the last layer in DNN are called logits. The softmax function is added after the last layer in order to transform these logits into a probability distribution, i.e., 0 \u2264yi \u22641 and y1 + . + ym = 1 where yi is interpreted as the probability that input x has class i. The label with the highest probability C(x) = argmax yi is assigned as the class of the input x. Let Z(x) = z be the output of all layers excluding Softmax, thus the full DNN is F(x) = softmax(Z(x)) = y. At the neuron level, the input is first linearly transformed using weights \u02dc\u03b8 and baises \u02c6\u03b8 , and then subjected to a non-linear activation function \u03c3 (e.g., ReLU). The DNN model is a chain function :\n(1)\nWhere:\n(2)\n# B. Related work\nSeveral studies have shown the effectiveness of the DNN for intrusion detection systems in different types of networks. [7] has proposed an LSTM neural network for distributed detection of cyber-attacks in fog-to-things communications. [8] used the DNN to develop a framework for the identification of intrusions and attacks at the network and host level. [9] presented a lightweight framework using deep learning for encrypted traffic classification and intrusion detection. Nonetheless, little if any attention was paid to the effect of adversarial attacks against these frameworks. One of the first works on the vulnerability of DNN to adversarial examples was carried out by [10]. The boxconstrained Limited memory approximation of BroydenFletcher-Goldfarb-Shanno (LBFGS) optimization algorithm was used to generate imperceptible alterations in the handwritten images in order to deceive the DNN. Although several\nattacks and defenses were subsequently proposed [4][5][6], these attacks were designed for the computer vision field in which the vulnerability was first discovered. Lately, work on the effect of adversarial attacks against intrusion detection systems has been carried out. Wang [3] showed the effect of these attacks on intrusion detection systems using NSL-KDD dataset. [11] studied the impact of black boxes adversarial attacks on the performance of intrusion detection systems based on DNN. [12] investigated the robustness of Self-normalizing Neural Network (SNN) against adversarial attacks on IoT networks. According to our review of the literature, there is no work on the effectiveness of adversarial training against adversarial attacks for deep learning-based intrusion detection systems, therefore this work is presented to cover this aspect.\n# C. Adversarial examples\nDespite the fact that deep learning has made significant progress in a variety of areas, Szegedy el al.\u2019s intriguing research [5] reveals that DNNs may not be as smart as they seem. They found that inserting small but carefully crafted perturbations into original images can lead to misclassification with even higher confidence. These crafted perturbations are small enough to be considered insignificant and imperceptible changes to humans. Methods of creating adversarial examples can be categorized according to two criteria: the target class and knowledge about the model under attack.\nDespite the fact that deep learning has made significant progress in a variety of areas, Szegedy el al.\u2019s intriguing research [5] reveals that DNNs may not be as smart as they seem. They found that inserting small but carefully crafted perturbations into original images can lead to misclassification with even higher confidence. These crafted perturbations are small enough to be considered insignificant and imperceptible changes to humans. Methods of creating adversarial examples can be categorized according to two criteria: the target class and knowledge about the model under attack. Adversarial examples\u2019 target: given a target class T different from the initial class C\u2217(x) = I of an input x. An attacker seeks to find a slightly perturbed input x\u2032 very similar to x given a certain distance metric, yet the classifier assigns the class C(x\u2032) = T to it. Thus, the targeted adversarial attack leads the DNN to misclassify the input as the class T desired by the attacker. As opposed to the untargeted adversarial attack where the objective is to find an imperceptibly modified input x\u2032 so that C\u2217(x) \u0338= C(x\u2032) which is obviously less powerful than targeted attacks. Knowledge concerning the model under attack: When the attacker has knowledge of everything related to the trained neural network model, including its gradients, it is a \u201dwhite box\u201d type attack. unlike \u201dblack box\u201d type attacks where the attacker lacks knowledge of the model\u2019s gradients and has only access to the model\u2019s probability scores or, even harder, to the model\u2019s final decision. This is a common assumption for attacks on online ML services.\n# III. EXPERIMENTAL APPROACH\nIn this section, a state-of-the-art intrusion detection system based on deep learning is built to study the effectiveness of adversarial attacks. We focus on untargeted \u201dwhite box\u201d type evasion attacks, i.e., the attacker has prior knowledge of the internal architecture of the DNN used for detection and carries out his attacks during the prediction process in order to lead the system into misdetection. Subsequently, adversarial training [4] [6] is thoroughly assessed as a defense against adversarial attacks by mixing adversarial samples with clean training data during the training process to enhance the robustness of the DNN against these attacks.\n# A. NSL-KDD Dataset\nAs one of the most commonly utilized datasets for evaluating the performance of an intrusion detection system, NSLKDD dataset -which was released in 2009 [13]- is an enhancement of the KDD CUP\u201999 dataset that suffers from two major drawbacks: a huge amount of redundant records and the bias of classifiers towards frequent records. NSL-KDD addressed the two issues by removing redundant records and rebalancing the dataset classes, thereby enabling comparative analysis of different ML algorithms. This dataset covers several attacks organized into four classes according to their nature: denial of service (DoS) attacks, probe attacks (Probe), root-to-local (R2L) attacks, and user-to-root (U2R) attacks. The records in the NSLKDD dataset have 41 features in addition to a class label. These features are grouped into three categories: basic features, content features, and traffic features. For the experimental part, we use KDDTrain+, which contains 125973 records, as follows: 80% of the records are training data and 20% are test data. Table I provides a summary of the data.\n<div style=\"text-align: center;\">TABLE I. DIFFERENT CLASSES OF THE DATASET.</div>\nNormal\nDoS\nProbe\nR2L\nU2R\nTraining data\n53875\n36742\n9325\n796\n42\nTest data\n13468\n9185\n2331\n199\n10\n# B. Preprocessing\nThe preprocessing of the NSL-KDD dataset involves two steps: numericalization and standardization. Neural networks are unable to handle categorical values directly. Numericalization is the process of transforming these categorical values into numerical values. The features that contain categorical values in this dataset are \u201dprotocol type\u201d, \u201dservice\u201d and \u201dflag\u201d. Standardization is an important step to prevent the neural network from malfunctioning because of large differences between features\u2019 ranges. That is why we transform each feature into standard normal distribution. In this paper, we focus on binary classification; therefore we qualify all attack records as \u201danomaly\u201d and normal traffic as \u201dnormal\u201d. We use one-hot encoding to transform the class labels into numerical values.\n# C. Building deep learning-based IDS\nIn order to detect intrusions, a deep binary neural network with two hidden layers, each containing 512 hidden units, is implemented using TensorFlow [14]. Rectified Linear Unit (ReLU) is used as an activation function within each hidden unit so as to introduce non-linearity in these neurons\u2019 output. Following each hidden layer, a dropout layer with a dropout rate of 0.2 is employed to prevent Neural Networks from over-fitting. ADAM is set as an optimization algorithm and \u201dcategorical crossentropy\u201d as a loss function to be minimized. softmax layer is added at the end to convert the logits into a normalized probability distribution. the class with the highest probability is considered as the predicted class.\n# D. Generating adversarial samples\nWe use Adversarial Robustness Toolbox (ART) [15] to implement adversarial attacks as well as the adversarial training. ART is an open-source python library for ML security\ndeveloped by the International Business Machines corporation (IBM). The generation of adversarial samples can be explained in a simple way. One can consider it as the inverse process of gradient descent where, given a fixed input data x and its label y, the goal is to find the model parameters \u03b8 that minimize the loss function J. Now, to generate an adversarial sample x\u2032, we proceed inversely, given fixed model parameters \u03b8, we differentiate the loss function J with respect to the input data x in order to find a sample x\u2032 - close to x - that maximizes the loss function J. FGSM [4] uses a specific factor \u03f5 to control the magnitude of the introduced perturbation where \u2225x\u2032 \u2212x\u2225< \u03f5. The \u03f5 factor can be considered as the attack strength or the upper limit of the distortion amount. The adversarial sample x\u2032 is then generated as follows:\n(3)\nBIM [5] is another attack and is basically an iterative extension of the FGSM applying the attack repeatedly. Similar to BIM, another iterative version of the FGSM is PGD [6]. However, unlike BIM, the PGD is relaunched at each iteration of the attack from many points on the \u03f5-norm ball around the original input.\n# E. Adversarial training\nThe idea behind adversarial training is to inject adversarial examples with their correct labels into the training data so that the model learns how to handle them. To do this, we use the PGD attack to generate adversarial samples before mixing them with the training data set. Here, we want to study two parameters of this defense: first, the effect of attack strength \u03f5 used to generate adversarial samples for the training, let\u2019s call it \u03f5defense to avoid confusion with the strength of adversarial attack \u03f5attack in the attack phase. Second, the proportion of adversarial training samples compared to clean training samples in the training data.\n# IV. EXPERIMENTAL RESULTS\nIV. EXPERIMENTAL RESULTS\nIn this section, we first evaluate the effect of adversarial attacks on a deep learning-based intrusion detection system. then, in the second part, we examine the effectiveness of adversarial training as a means of making the system more robust against these attacks. we conclude this section by discussing and analyzing the results obtained.\n# A. Effect of adversarial attacks on deep learning-based ID\nAfter training our DNN model, we test its accuracy (the proportion of correct predictions among the total number of cases examined) on unmodified test data. The model gives an accuracy of 99.61%, we then proceed to generate adversarial test data using FGSM, BIM, and PGD respectively. For each attack, the experiment is repeated, intensifying the attack by increasing \u03f5 value each time. Figure 1 shows that all three attacks deteriorate significantly the performance of the intrusion detection system. The FGSM attack lowers the accuracy of the system from 99.61% to 14.13%, while the BIM and PGD attacks decrease it further to 8.85%.\nThis demonstrates that, with sufficient distortion, adversarial attacks are able to defeat intrusion detection systems based on DNNs and lead them into misdetection.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f594/f5941658-779b-4056-a2f6-9691300981fb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. Effect of adversarial attacks on deep learning-based intrusion detection system.</div>\n# B. Adversarial training effect\nB. Adversarial training effect As mentioned in Section III-E, we examine two parameters of adversarial training: 1) \u03f5defense which represents the attack force used to generate adversarial training samples that are mixed with clean training samples. 2) the percentage of adversarial training samples, compared to clean training samples, in the training data. Note that all the adversarial examples used are generated via the PGD attack. We begin by setting the percentage of adversarial training samples in the training data to 30%, giving a fixed value of \u03f5defense. After training the model with this mixed training data, we apply PGD attack by increasing the value of \u03f5attack each time. We repeat the experiment by increasing the value of \u03f5defense used for adversarial training as shown in Figure 2(a). The whole process is repeated by setting the percentage of adversarial training samples to 30%, 50%, 70%, and 90% respectively. Figure 2 illustrates that compared to using only clean training data, adversarial training improves the robustness of the intrusion detection system against adversarial attacks. Although with sufficient attack force, the accuracy of the detector decreases considerably. We also note that increasing strength of the adversarial examples \u03f5defense used for the training helps to improve the robustness of the detector to some extent, making it more difficult for the attacker to create adversarial samples with a small distortion that can mislead the intrusion detection system. The same cannot be said for the impact of the percentage of adversarial training examples on the robustness of the intrusion detection system because while for \u03f5defense = 0.7, higher percentages improved the robustness of the detector against adversarial attacks as shown in Figure 3, this improvement is not observed for the other values of \u03f5defense. Thus, it is safe to say that the percentage of adversarial training examples doesn\u2019t have a direct link to the robustness of the intrusion detection system using adversarial training. This could be explained by the fact that the added dropout layers are designed to reduce overfitting effect on DNN, so as long as the model is fed with enough adversarial samples in the training phase, its performance won\u2019t change much by adding data with similar information. Another important aspect is the effect of adversarial training on the performance of the intrusion detection system\nwhen tested on clean test data. While results of the previous experiments indicate that adversarial training increases the robustness of deep learning-based intrusion detection systems, Figure 4 shows that adversarial training slightly decreases the accuracy of the detector when tested on clean test data. This indicates that there is a trade-off between robustness and accuracy. The decrease in accuracy of the intrusion detection system on clean test data could be explained by the fact that as the model is trained with adversarial samples, its decision boundary would change in comparison to clean data training. From a practical point of view, given malicious network traffic, such as HTTP traffic that wants to connect to bad URLs, such as command and control servers, the attacker can use adversarial generation techniques to transform this malicious network traffic into normal traffic for the intrusion detection system while maintaining its maliciousness, for example by adding small amounts of specially crafted data to the network traffic as padding. This allows the attacker to mislead the intrusion detection system. Adversarial training, on the other hand, is a defensive technique. It seeks to make the attacker\u2019s task more difficult by making small distortions insufficient to bypass the intrusion detection system.\nIn conclusion, adversarial attacks are a real threat to intrusion detection systems based on deep learning. By generating samples using adversarial attacks, an attacker can lead the system to misdetection and, given sufficient attack strength, the performance of the intrusion detection system can deteriorate significantly. As a defense against such attacks, the adversarial training was examined in depth. The results show that this method can improve to some extent the robustness of deep learning-based intrusion detection systems. However, it comes with a trade-off of slightly decreasing detector accuracy on unattacked network traffic. An interesting future work would be to propose new defense mechanisms against adversarial attacks by exploring uncertainty handling techniques.\n# REFERENCES\n[1] J. Kevric, S. Jukic, and A. Subasi, \u201cAn effective combining classifier approach using tree algorithms for network intrusion detection,\u201d Neural Computing and Applications, vol. 28, no. 1, 2017, pp. 1051\u20131058. [2] I. Debicha, T. Debatty, W. Mees, and J.-M. Dricot, \u201cEfficient intrusion detection using evidence theory,\u201d in INTERNET 2020 : The Twelfth International Conference on Evolving Internet, 2020, pp. 28\u201332. [3] Z. Wang, \u201cDeep learning-based intrusion detection with adversaries,\u201d IEEE Access, vol. 6, 2018, pp. 38 367\u201338 384. [4] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572, 2014. [5] A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial examples in the physical world,\u201d arXiv preprint arXiv:1607.02533, 2016. [6] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d arXiv preprint arXiv:1706.06083, 2017. [7] A. Diro and N. Chilamkurti, \u201cLeveraging lstm networks for attack detection in fog-to-things communications,\u201d IEEE Communications Magazine, vol. 56, no. 9, 2018, pp. 124\u2013130. [8] R. Vinayakumar, M. Alazab, K. Soman, P. Poornachandran, A. AlNemrat, and S. Venkatraman, \u201cDeep learning approach for intelligent intrusion detection system,\u201d IEEE Access, vol. 7, 2019, pp. 41 525\u2013 41 550. [9] Y. Zeng, H. Gu, W. Wei, and Y. Guo, \u201cdeep \u2212full \u2212range: A deep learning based network encrypted traffic classification and intrusion detection framework,\u201d IEEE Access, vol. 7, 2019, pp. 45 182\u201345 190.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7cf/d7cf76d3-c44d-4c47-bd9f-6b2144c15f28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25cd/25cd59f0-d482-4bcd-a222-917bc93c3d56.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Percentage of adversarial training samples in the training data = 30%</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebed/ebed48af-0292-4424-ab96-47dfcbe36c09.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Percentage of adversarial training samples in the training data = 70% (d) Percentage of adversari</div>\n<div style=\"text-align: center;\">(c) Percentage of adversarial training samples in the training data = 70% (d) Percentage of adversarial training samples in the train Figure 2. Effect of adversarial training on the robustness of deep learning-based intrusion detection system</div>\n<div style=\"text-align: center;\">ples in the training data = 70% (d) Percentage of adversarial training samples in the training data = 90%  adversarial training on the robustness of deep learning-based intrusion detection system</div>\n<div style=\"text-align: center;\">Figure 2. Effect of adversarial training on the robustness of deep learning-based intrusion detection system</div>\n<div style=\"text-align: center;\">Figure 2. Effect of adversarial training on the robustness of deep learning-based intrusion detection system</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ac7a/ac7aa229-0eb2-4217-9e02-42c695d30577.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3. Effect of the percentage of adversarial training samples in the training data, \u03f5defense = 0.7 .</div>\n[10] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv preprint arXiv:1312.6199, 2013. [11] K. Yang, J. Liu, C. Zhang, and Y. Fang, \u201cAdversarial examples against the deep learning based network intrusion detection systems,\u201d in MILCOM 2018-2018 IEEE Military Communications Conference (MILCOM). IEEE, 2018, pp. 559\u2013564. [12] O. Ibitoye, O. Shafiq, and A. Matrawy, \u201cAnalyzing adversarial attacks against deep learning for intrusion detection in iot networks,\u201d in 2019 IEEE Global Communications Conference (GLOBECOM). IEEE,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/914a/914a8631-d3f1-41f7-977e-526638db54d4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Percentage of adversarial training samples in the training data = 50%</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2132/21324c40-00a6-441e-a343-9f3ef268e5bc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e59/2e593264-8859-486c-b805-2e3a5dcb5c65.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4. Effect of adversarial training on the performance of the intrusion detection system on clean test data.</div>\n2019, pp. 1\u20136. [13] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, \u201cA detailed analysis of the kdd cup 99 data set,\u201d in 2009 IEEE symposium on computational intelligence for security and defense applications. IEEE, 2009, pp. 1\u20136. [14] \u201cTensorflow,\u201d https://www.tensorflow.org/, retrieved: March, 2021. [15] M.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, M. Wistuba, V. Zantedeschi, N. Baracaldo, B. Chen, H. Ludwig et al., \u201cAdversarial robustness toolbox v1. 0.0,\u201d arXiv preprint arXiv:1807.01069, 2018.\n",
    "paper_type": "method",
    "attri": {
        "background": "With the growth of the computer and telecommunications industry and the expansion of the Internet, there has been a significant escalation of cyberattacks targeting all types of networks. Intrusion Detection Systems (IDS) are gaining popularity as a solution against these cyber-attacks. While traditional machine learning methods had limitations, Deep Neural Networks (DNNs) have become state-of-the-art in anomaly-based intrusion detection. However, DNNs are vulnerable to adversarial attacks, which can mislead them into misclassification, posing a critical security risk. This paper examines the effects of adversarial attacks on DNN-based IDS and investigates the effectiveness of adversarial training as a defense mechanism.",
        "problem": {
            "definition": "The problem addressed in this paper is the vulnerability of deep learning-based intrusion detection systems to adversarial attacks, which can degrade their performance and lead to misclassification of malicious traffic.",
            "key obstacle": "The main challenge is that existing methods do not adequately defend against adversarial attacks, which can significantly reduce the accuracy of intrusion detection systems."
        },
        "idea": {
            "intuition": "The idea stems from the observation that adversarial examples can be crafted to deceive DNNs, which inspired the exploration of adversarial training as a potential defense strategy.",
            "opinion": "The proposed idea is to utilize adversarial training to improve the robustness of deep learning-based intrusion detection systems against adversarial attacks.",
            "innovation": "The innovation of this work lies in being one of the first studies to systematically evaluate the impact of adversarial training specifically for intrusion detection systems."
        },
        "method": {
            "method name": "Adversarial Training",
            "method abbreviation": "AT",
            "method definition": "Adversarial training involves mixing adversarial samples with clean training data to enhance the robustness of the model against adversarial attacks.",
            "method description": "The core of the method is to train the DNN with both clean and adversarial examples, improving its ability to detect attacks.",
            "method steps": [
                "Generate adversarial samples using techniques like FGSM, BIM, and PGD.",
                "Combine these adversarial samples with clean training data.",
                "Train the DNN on this mixed dataset.",
                "Evaluate the performance of the trained model on both clean and adversarial test data."
            ],
            "principle": "This method is effective because it exposes the model to potential attacks during training, allowing it to learn to recognize and defend against such manipulations."
        },
        "experiments": {
            "evaluation setting": "The NSL-KDD dataset was used, which includes various types of network attacks. The dataset was split into training (80%) and testing (20%) data.",
            "evaluation method": "The performance of the intrusion detection system was measured by testing its accuracy on unmodified and adversarial test data, evaluating the impact of adversarial training on both types of data."
        },
        "conclusion": "The experiments demonstrated that adversarial attacks can significantly degrade the performance of DNN-based intrusion detection systems. However, adversarial training can improve robustness against these attacks, albeit at the cost of slightly reduced accuracy on clean data. This highlights a trade-off between robustness and accuracy.",
        "discussion": {
            "advantage": "The key advantage of the proposed approach is its ability to enhance the robustness of intrusion detection systems against adversarial attacks, making them more reliable in real-world applications.",
            "limitation": "A limitation of the method is that while it improves robustness, it may lead to a decrease in accuracy on clean test data, indicating a trade-off that must be managed.",
            "future work": "Future research could explore new defense mechanisms against adversarial attacks, particularly focusing on uncertainty handling techniques to further improve the resilience of intrusion detection systems."
        },
        "other info": [
            {
                "info1": "The paper was published on 20 Apr 2021."
            },
            {
                "info2": {
                    "info2.1": "Authors: Islam Debicha, Jean-Michel Dricot, Thibault Debatty, Wim Mees.",
                    "info2.2": "The paper discusses three adversarial attack methods: FGSM, BIM, and PGD."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Adversarial attacks pose a critical security risk to deep learning-based intrusion detection systems, highlighting the importance of OOD detection for developing reliable AI systems."
        },
        {
            "section number": "2.5",
            "key information": "Adversarial training is a method that mixes adversarial samples with clean training data to enhance the robustness of models against adversarial attacks, crucial for uncertainty estimation in machine learning."
        },
        {
            "section number": "3.4",
            "key information": "Deep Neural Networks (DNNs) are state-of-the-art in anomaly-based intrusion detection but are vulnerable to adversarial attacks, necessitating the exploration of deep learning techniques for OOD detection."
        },
        {
            "section number": "5.2",
            "key information": "Adversarial examples can mislead DNNs into misclassification, significantly affecting the robustness of machine learning models, particularly in the context of intrusion detection."
        },
        {
            "section number": "7.1",
            "key information": "Existing methods for defending against adversarial attacks in intrusion detection systems are inadequate, presenting a challenge in current OOD detection methods."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore new defense mechanisms against adversarial attacks, focusing on uncertainty handling techniques to improve the resilience of intrusion detection systems."
        }
    ],
    "similarity_score": 0.6624299688393354,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Adversarial Training for Deep Learning-based Intrusion Detection Systems.json"
}