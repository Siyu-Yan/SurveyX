{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.07588",
    "title": "AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning",
    "abstract": "In-context learning (ICL) advances Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters. However, in the area of multimodal Large Language Models (MLLMs), two problems hinder the application of multimodal ICL: (1) Most primary MLLMs are only trained on singleimage datasets, making them unable to read extra multimodal demonstrations. (2) With the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance. During preliminary explorations, we discovered that the inner LLM focuses more on the linguistic modality within multimodal demonstrations during generation. Therefore, we propose a general and light-weighted framework AIM to tackle the mentioned problems through Aggregating Image information of Multimodal demonstrations to the latent space of the corresponding textual labels. In image information aggregation, AIM independently generates fused virtual tokens to substitute each demonstration whose length is the same length as its texts. Apart from significantly shortening length, fused virtual tokens modify the original multi-image prompts approximately to ones containing a single query image, effectively upgrading MLLMs pretrained on single-image datasets to perform multimodal ICL. We build AIM upon QWen-VL and LLaVA-Next, and we comprehensively evaluate AIM on image caption, VQA, and hateful speech detection. Outstanding results reveal that AIM provides an efficient and effective solution in upgrading MLLMs for multimodal ICL.",
    "bib_name": "gao2024aimletmultimodallarge",
    "md_text": "# AIM: Let Any Multimodal Large Language Models Embrace Efficient In-Context Learning\nZiqiang Cao\nJun Gao\njunegao1106@gmail.com\n# Abstract\nIn-context learning (ICL) advances Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters. However, in the area of multimodal Large Language Models (MLLMs), two problems hinder the application of multimodal ICL: (1) Most primary MLLMs are only trained on singleimage datasets, making them unable to read extra multimodal demonstrations. (2) With the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance. During preliminary explorations, we discovered that the inner LLM focuses more on the linguistic modality within multimodal demonstrations during generation. Therefore, we propose a general and light-weighted framework AIM to tackle the mentioned problems through Aggregating Image information of Multimodal demonstrations to the latent space of the corresponding textual labels. In image information aggregation, AIM independently generates fused virtual tokens to substitute each demonstration whose length is the same length as its texts. Apart from significantly shortening length, fused virtual tokens modify the original multi-image prompts approximately to ones containing a single query image, effectively upgrading MLLMs pretrained on single-image datasets to perform multimodal ICL. We build AIM upon QWen-VL and LLaVA-Next, and we comprehensively evaluate AIM on image caption, VQA, and hateful speech detection. Outstanding results reveal that AIM provides an efficient and effective solution in upgrading MLLMs for multimodal ICL.\n# 1 Introduction\nIn-context learning (ICL) exhibits spectacular emergent ability in the NLP community (Brown\n# Wenjie Li\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8b08/8b081be5-8ac1-4e6c-8901-3f13c4e457cb.png\" style=\"width: 50%;\"></div>\nFigure 1: Memory cost comparison between AIM and LLaVA-Next on Flickr30k. The memory cost of LLaVA-Next occurs a surge, while it almost remains unchanged in AIM.\n<div style=\"text-align: center;\">Figure 1: Memory cost comparison between AIM and LLaVA-Next on Flickr30k. The memory cost of LLaVA-Next occurs a surge, while it almost remains unchanged in AIM.</div>\net al., 2020; Xie et al., 2021; Wang et al., 2023b), enabling scaled-up Large Language Models (LLMs) (Vaswani et al., 2017; Wang et al., 2023c; Yang et al., 2023; Wei et al., 2023; Wang et al., 2023a; Min et al., 2022) to attain desirable performance on training-agnostic data by providing with handful in-context demonstrations. Unfortunately, primary multimodal Large Language Models (MLLMs) such as LLaVA (Liu et al., 2024b, 2023), LLaMA-Adapter (Zhang et al., 2023b), and BLIP-2 (Li et al., 2023b), only support a single image as the vision input, impossible to learn from multimodal demonstrations composed of <image, instruction text, reference text> pairs. Additionally, most MLLMs utilize Perceiver (Alayrac et al., 2022; Awadalla et al., 2023; Liu et al., 2024b, 2023, 2024a; Li et al., 2023b; Dai et al., 2024) to generate visual tokens from image features encoded by an existing visual encoder, assisting the inner LLM understand visual inputs. However, multiple images in multimodal demonstrations inevitably produce thousands of visual tokens, resulting in extreme mem-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8300/83000b45-07bf-4ccd-8639-29e62f92d591.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Performance comparison between AIM and its underlying backbone in the 16-shot ICL setting.</div>\nory costs as depicted in Figure 1. Meanwhile, the prompt length surged by multimodal demonstrations might be one of the key factors constraining the performance of multimodal ICL (Alayrac et al., 2022; Awadalla et al., 2023; Bai et al., 2023; Zhao et al., 2023; Li et al., 2023a; Huang et al., 2024). Our experiments indicate a dramatic deterioration in the Perplexity (PPL) of answers generated by MLLMS with increasing demonstrations introduced (refer to Figure 5). During the early exploration, we surprisedly found that MLLMs attend more to the linguistic modality, namely the texts in demonstrations, than the visual modality for response generation, as shown in Figture 3. Motivated by this finding, we propose the framework AIM, with the aim to make any MLLM embrace efficient multimodal ICL. In contrast to mainstream MLLMs that always treat visual and textual tokens equally for both demonstrations and queries (Bai et al., 2023; Liu et al., 2023, 2024b; Zhao et al., 2023), AIM aggregates the image information of each demonstration into its linguistic modality and then drops their lengthy visual tokens. Thus, AIM approximately reduces image-text demonstrations into text-like demonstrations with the images and texts in queries unmodified. Specifically, AIM first applies the inner MLLM to forward each demonstration independently to obtain the hidden states of the image and its text. Then, AIM applies a linear layer to project the hidden states on top of texts to fused virtual tokens acceptable for LLMs while dropping hidden states on top of images. Therefore, the sizes of demonstrations are reduced to the dimensions of\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b0fe/b0fe0d65-e0fc-45af-9367-8bf1588ba3a5.png\" style=\"width: 50%;\"></div>\nFigure 3: The hot map of attention scores when QWenVL generates the first token on the hateful memes dataset. The brighter represents that responses to be generated have paid more attention to the current visual/textual tokens. Obviously, the generation relies more on the textual part of a multimodal demonstration\ntheir textual embeddings. Finally, the fused virtual tokens replace the original image-text pair, serving as a text-like demonstration with image information, fed into the inner LLM to guide response generation. In this case, the built-in MLLMs are only required to attend to a single query image because images from demonstrations are removed in the input end. Hence, AIM can perform ICL even if the backbones don\u2019t develop the understanding ability of interleaved multimodal inputs during pre-training. Additionally, as aggregating image information is independent, AIM asynchronously processes different demonstrations within a batch to configure a virtual demonstration sequence for few-shot settings through horizontal concatenation. The aggregated fused virtual tokens can be cached to formulate a demonstration bank (DB) for further reusing, avoiding repeated aggregation for the same demonstration. AIM keeps its backbone frozen, only equipping 17M trainable parameters originating from the projection layer. Considering ICL was proposed in a low-resource setting, previous studies training models on a mixture of downstream task datasets seem inappropriate that MLLMs perhaps fall into the short-cut answer, resulting in outstanding but not solid results on involved/related tasks. Hence, following the technical report of OpenFlamingo (Awadalla et al., 2023), we train the projection layer on the subset of MMC4 (Zhu et al., 2023), containing 223k images and 1M sequences from websites. We select LLaVA-Next and QWen-\nVL as the underlying MLLM in AIM to verify the generality, representing MLLMs reading a single image-text pair and images interleaved with texts. Furthermore, we comprehensively evaluate AIM on image caption (Plummer et al., 2015), visual question answering (VQA) (Gurari et al., 2018; Marino et al., 2019), and hateful detection (Kiela et al., 2020), none of the involved datasets occurring in the training data of AIM and mixture downstream pre-training dataset of QWen-VL and LLavA-Next. Outstanding results in Figure 2 reveal that AIM always achieves comparable or better performance than its underlying backbone with less than 10% tokens remaining on average (refer to Table 4). Generally, our main contributions are as follows:\n\u2022 Our proposed AIM exhibits efficiency in terms of trainable parameters, memory usage, and inference throughput.\n# 2 Related Work\n# 2.1 Multimodal Large Language Models\nRecently, the development of LLMs significantly advanced the iterations of MLLMs, and the inner LLMs play crucial roles. Researchers first trained the visual encoder to align to the frozen language models (Tsimpoukelli et al., 2021), performing vision-language tasks. Predominate MLLMs can be abstracted to Perceiver & LLM architecture, where the Perceiver is usually composed of a Vision Transformer (ViT) (Dosovitskiy et al., 2020) to extract image features and an adapter, concatenating visual and language tokens in the input end of the built-in LLM. Specifically, the perceiver in QWen-VL (Bai et al., 2023) and the Q-Former in BLIP-2 (Li et al., 2023b) apply learnable queries to extract visual information based on cross-attention, while the Connec-\ntor in LLaVA (Liu et al., 2023, 2024b,a) directly projects the visual features extracted from the pre-trained ViT-L/14 in CLIP (Radford et al., 2021). Considering the further alignment within LLMs, Flamingo (Alayrac et al., 2022) introduced the XATTN layer to align visual tokens originating from the Resampler and textual embeddings within the LLM. InternLM (Zhang et al., 2023a) propose Partial LoRA to align vision tokens to the LLM. However, the perceiver in MLLM will introduce hundreds or even thousands of visual tokens to the inner LLM in ICL, resulting in over-length multimodal prompts and thereby bringing enormous memory costs.\n# 2.2 In-Context Learning\nIn the field of NLP, LLMs including ChatGPT (Luo et al., 2023), GPT-4 (OpenAI, 2023), and LLaMA (Meta, 2023), exhibit general spectacular emergent abilities on downstream tasks that provide a novel paradigm for generative models known as \u201cPre-training, Prompting, and Prediction\u201d. Within this paradigm, ICL assumes a pivotal role, bolstering the generalization capability of LLMs (Wang et al., 2023c; Yang et al., 2023; Wei et al., 2022), without necessitating billions of parameters gradient updating. The success of ICL in NLP boosts studies focusing on transforming it into the multimodal setting (Tsimpoukelli et al., 2021; Zhao et al., 2023; Alayrac et al., 2022; Yang et al., 2024). Additionally, researchers extensively explore the influence of diverse demonstration configurations in captioning (Yang et al., 2024). As far as we know, recent studies focusing on multimodal ICL (Alayrac et al., 2022; Awadalla et al., 2023; Liu et al., 2023; Zhao et al., 2023; Li et al., 2023a) overlook deployment challenges to some extent. QWenVL (Bai et al., 2023) and MMICL (Zhao et al., 2023) treat visual and textual tokens equally during training, brought serious length challenges, and restricted model performance due to modeling enormous vision tokens. Flamingo (Alayrac et al., 2022) treated image information as informative noises adding to textual embeddings through extra introduced adapters within selectional inner LLM layers, resulting in additional module latency. However, visual tokens of different images still share the same input window, bringing extra memory costs. LLaVA-Next (Liu et al.,\n2024a) specialized in single-image inference, and it achieved outstanding performance on popular multimodal benchmarks and textual-only ICL, while its excellent performance failed to extrapolate to practical multimodal ICL settings, exhibiting poor ability of instruction following. Additionally, LLaVA-Next connected pre-trained ViT and LLM via an MLP that resulted in thousands of visual tokens for high-resolution pictures, causing more serious length disasters than perceivers based on cross-attention.\n# 2.3 Efficient In-Context Demonstration\nConsidering the huge inference costs brought by ICL, researchers recently focused more on formulating efficient in-context demonstrations (Wingate et al., 2022). Similar to prefix tunning (Li and Liang, 2021), Gist Tokens (Mu et al., 2023) were proposed to replace various task instructions. AutoCompressor (Chevalier et al., 2023) first randomized segmented the texts with thousands of words into model-accepted range and then recursively generated soft prompts for each segment. Similarly, ICAE (Ge et al., 2023) employed a LoRA-adopted Llama-7b (Touvron et al., 2023) to compress the processed demonstrations to compact virtual tokens. Gao (2024a,b) propose to compress over-limit prompts into virtual tokens via a frozen LLM and a linear layer. Correspondingly, researchers also endeavored to shorten prompts by extracting informative tokens from the original ones (Li, 2023; Jiang et al., 2023), namely token pruning (Kim et al., 2022) or token merging (Bolya et al., 2022). LLMLingua (Jiang et al., 2023) and Selective Context (Li, 2023) shared similarities but diverged on whether to eliminate tokens with high or low PPL. Unfortunately, these outstanding studies only focus on the textual modality, which did not suffer from the modal gap in MLLMs. To our best known, AIM is the first to explore the construction of efficient multimodal demonstrations.\n# 3 Methodology\nWe propose AIM as illustrated in Figure 4, which is a training- and inference-efficient framework that aggregates image information of multimodal demonstrations into their latent space of texts. Considering different details of popular MLLMs, we present an empirical comparison in Table 1. Specifically, The Flamingo is distin-\nMethods\nIn-context Learning Mode\nInner LLM\n# Trainable Para.\nFlamingo\nmultimodal\nChinchilla (7B)\n7B\nMMICL\nmultimodal\nFLANT5 (3B/11B)\n\u226b17M\nQWen-VL\nmultimodal\nQWen (7B)\n7B\nLLaVA-Next\nLanguage-only\nVicuna (7B)\n7B\nAIM\nmultimodal\nQWen/Vicuna (7B)\n17M\nTable 1: Quality comparison of recent MLLMs and AIM in ICL mode, LLM size, and trainable parameters.\nguished from others by its Gated XATTN layer inserted in the LLM blocks to fuse image information into embeddings, sacrificing inference efficiency to memory usage. LLaVA-Next directly projects visual features extracted from pre-trained ViT to thousands of visual tokens, resulting in higher memory cost increment than other methods based on Q-Former. Additionally, LLaVANext can read only a single image and thus it doesn\u2019t support multimodal ICL. Similar to Flamingo, AIM discards substantial visual tokens in multimodal demonstrations after aggregating demonstrated image information into its text, resembling a multimodal ICL prompt approximately containing a single query image. Therefore, AIM operates seamlessly with any MLLMs, regardless of whether they initially understand multimodal demonstrations well. AIM employs the 7B version of QWen-VL and LLaVANext as the built-in backbone, representing the two coarse-grained types of MLLMs divided by input form, to verify the effectiveness of fused virtual tokens.\n# 3.1 Preliminary\nA multimodal ICL prompt encompasses several demonstrations consisting of image-text pairs (< X1, Y1 >, < X2, Y2 >, ..., < Xn, Yn >) and a query denoted as < Xquery, ins. >, where Xi and Xquery representing the i-th demonstration image and the query image. Additionally, we use manually designed instructions ins. to wrap the bare label for each demonstration. Thus, the demonstrated texts in i\u2212th demonstration are composed of instruction ins. and its reference label Yi, e.g., [IMG] Describe the image in a sentence in English: [Caption].\n# 3.2 Image Information Aggregation\nTaking into account our discoveries, AIM configures efficient demonstrations by disrupting the original parity between visual and linguistic modalities. AIM signals the linguistic space to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c45/4c454e78-c9f1-4127-9148-41e65a560840.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The architecture of AIM. Fused tokens from different demonstrations are concatenated and fed into the inner LLM, discarding original visual tokens.</div>\ngather image information via the forward propagation of the inner LLM, as illustrated in the left part of Figure 4. Practically, we split the original concatenated n-shot demonstrations into n separate image-text pairs, decorating labels with manually designed instructions. Then, we feed images to the Perceiver, the Visual Prompt Generator (VPG), to obtain the visual tokens (Xv 1, Xv 2, ..., Xv n) normally. Consequently, the first N LLM layers forwards Xv i attached with the i-th textual embeddings Yi, obtaining last hidden states corresponding to Yi, while dropping the others1:\n(1)\nDue to the inner transformer layers, HY i is compelled to attend to the preceding visual tokens, making the latter textual tokens able to aggregate visual information. However, HY i is still in the output space that the LLM can\u2019t understand directly although it has fused with image information. We insert a learnable projection layer serving as the adapter to convert HY i into the LLMacceptable fused tokens, similar to the perceiver converting visual features from visual encoder to visual tokens:\n(2)\nwhere Wp is the parameters of the projection layer. Notably, aggregating image information of each image is independent of other demonstrations. Thus, AIM supports obtaining all \u02c6Yi in a batch asynchronously or repeating this process for each demonstration synchronously to trade off memory costs with time.\n1\u2295means token-level concatenation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b359/b35938b2-614a-49d4-a1c8-f49fe318f0c5.png\" style=\"width: 50%;\"></div>\nDataset\nTraining\n# Instances\nEval. Set\nEval.\nMetric\nFlickr30k\n\ufffd\n1000\nTest(Karpathy)\nOpen-Ended\nCIDEr\nOKVQA\n5046\nVal\nOpen-Ended\nVQA acc.\nVizwiz\n4319\nVal\nOpen-Ended\nVQA acc.\nHateful Memes\n815\nSeen Test\nCloese-Ended\nROC AUC\n<div style=\"text-align: center;\">Table 2: Details of involved evaluating datasets.</div>\n# 3.3 Response Generation in multimodal ICL\nAIM applies the entire frozen inner LLM to respond to current queries, with the guidance of our proposed fused tokens. For n-shot ICL, AIM obtains ( \u02c6Y1, \u02c6Y2, ..., \u02c6Yn) independent from each other, concatenating them to configure an efficient demonstration sequence D = \u02c6Y1 \u2295\u02c6Y2 \u2295... \u2295 \u02c6Yn. Finally, the demonstration sequence D together with query image Xquery and the instruction ins. are fed into the inner LLM, performing auto-regressive generation:\nyt = argmaxP(y|D; Xquery; ins.; y<t).\n# 3.4 Training\nThe trainable parameters in AIM are merely 17M originating from the projection layer Wp. We supervised-tune the projection layer under the language modeling objective of its built-in LLM. We collect 56k instances from the web multi-image dataset MMC4. Each instance includes several images [X1, X2, ..., Xk], and each image corresponds to a most similar text [Y1, Y2, ..., Yk] assigned by existing ViT-L/14 in CLIP, constructing an interleaved image-text training instance. During data preprocessing, we ensure each training instance has non-overlapping remaining texts and concatenate them, denoted as Y R = Yk+1 \u2295 Yk+2 \u2295.... AIM first independently aggregates Xi to its\n(4)\nNotably, the carefully designed training approach separates the information aggregation of different images, breaking the inner relevance among images crawled from the same web page, increasing learning difficulty, and scaling up model robustness. Specifically, this gets rid of the influence of other image-text pairs, allowing each image to focus on the given text only and better match the patterns of ICL intuitively. More importantly, assuming k image-text pairs of length l, breaking their inner relevance will reduce the memory complexity from O(k2l2) to O(kl2). It also brings AIM crucial merit that each aggregated result can be cached independently, formulating a demonstration bank (DB) for further reusing without aggregating image information into its latent space of demonstrated texts every time.\n# 4 Experiment\n# 4.1 Dataset\nWe briefly illustrate the involved dataset of AIM in Table 2. Involved evaluating datasets are filtered according to the mixture downstream training dataset of the underlying MLLMs to simulate the ICL scenario.\n# 4.1.1 Training Data\nmultimodal C4 MMC4 is a public open, billion-scale corpus of images interleaved with text. CLIP ViT-L/14 assigns each image with the most matched text crawled from the web. Due to AIM being trained parameter-efficiently, we merely utilize a subset of 56k instances to formulate our training set, which contains 223k images and 1M texts.\n# 4.1.2 Evaluating Data\nFlickr30k Flickr30k is a popular captioning dataset that contains 31,000 images collected from Flickr with 5 references annotated by humans.\nOKVQA OKVQA, Outside Knowledge Visual Question Answering, includes over 14k manually filtered questions that require external knowledge, with 5 ground truth answers for each question respectively.\nVizwiz Vizwiz (Gurari et al., 2018) dataset originates from a natural VQA setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers for each image. Especially, Vizwiz requires model responding \u201cunanswerable\u201d when the provided image is insufficient to answer the question.\nHateful Memes Hateful Memes is a binary classification dataset to detect hateful speech in multimodal memes. Their mentioned \u201cSeen/Unseen\u201d Test sets are distinguished by whether it is described in their paper or their challenge competition report.\n# 4.2 Evaluation Metrics\nWe widely evaluate AIM on a spectrum of vision-language benchmarks, including Flickr30k, OKVQA, Vizwiz, and Hateful Memes. Following previous studies, we use CIDEr for Flickr30k, VQA-ACC for OKVQA and Vizwiz, and ROC AUC scores for Hateful Memes. All the evaluation scripts are publicly available on the repository of OpenFlamingo.\n# 4.3 Setting\nDuring training, we set the maximum number of pictures to 5 per step for efficiency and filtered images if their similarities with all texts below 0.24 following OpenFlamingo. We fix the learning rate to 3e-5 and use Adam as the optimizer, and the effective batch size is 16 (4 GPUs data parallelism and 4 steps gradient accumulation). The number of epochs is set to 10 and we get a checkpoint per 3400 training steps. Additionally, we conduct all experiments on a single DGX node with 8*Nvidia H800 GPUs. LLaVA-Next supports processing any resolution image by splitting it into sub-images, bringing several times visual tokens. We ignore this character and require LLaVA to pad each image to 336*336 resolution since AIM introduces mass pictures as demonstrations2. We borrow some crafted prompts from previous studies. For captioning, we format demonstrations as \u201c[image] Describe the image in English in one sentence: [caption]\u201d; For VQA we format demonstrations as \u201c[image]\\n[question] Answer in a word: [answer]\u201d; For Hateful Memes, we prompt the model with \u201c[image] is an image with [text] written on it. Is it hateful? Answer:\n2Set image_aspect_ratio to pad.\nMethod\nLLM\nFlickr30k\nOKVQA\nVizwiz\nHateful Memes\nCIDEr \u2191\nVQA-ACC \u2191\nVQA-ACC \u2191\nROC-AUC \u2191\n#-shots\n#-shots\n#-shots\n#-shots\n0\n1\n2\n4\n8\n16\n0\n1\n2\n4\n8\n16\n0\n1\n2\n4\n8\n16\n0\n1\n2\n4\n8\n16\nFlamingo\u2020\u25b7\u25c1\nChinchilla (7B)\n61.5\n-\n-\n72.6\n-\n-\n44.7\n-\n-\n49.3\n-\n-\n28.8\n-\n-\n34.9\n-\n-\n57.0\n-\n-\n62.7\n-\n-\nOpen Flamingo\u2020\u25b7\u25c1\nMPT (7B)\n39.2\n-\n-\n52.2\n58.7\n60.6\n38.3\n-\n-\n42.0\n44.1\n45.1\n34.6\n-\n-\n41.0\n45.0\n46.2\n67.1\n-\n-\n70.1\n71.2\n73.2\n-Random\u2020\u25b7\u25c1\n59.5\n-\n-\n65.8\n62.9\n62.8\n37.8\n-\n-\n40.1\n41.1\n42.7\n27.5\n-\n-\n34.1\n38.5\n42.5\n51.6\n-\n-\n54.0\n54.7\n53.9\nQWen-VL \u22c6\nQWen (7B)\n73.4\n74.6\n75.3\n77.1\n75.1\n63.0\n46.3\n42.9\n51.2\n52.6\n53.2\n53.3\n27.6\n28.3\n29.2\n30.6\n30.1\n28.7\n56.5\n58.0\n57.3\n56.2\n57.1\n59.5\nQWen-VL\nQWen (7B)\n74.3\n75.6\n79.0\n80.4\n74.6\n66.1\n55.3\n45.5\n55.9\n56.4\n55.1\n53.7\n28.4\n29.5\n30.9\n31.3\n31.7\n29.8\n58.3\n57.8\n59.1\n59.0\n59.3\n58.2\n-w/o visual\n74.5\n75.1\n74.7\n74.3\n71.7\n53.2\n54.0\n54.7\n55.3\n55.1\n29.1\n30.3\n28.7\n30.5\n29.8\n58.0\n57.7\n58.2\n58.2\n57.1\nAIM\n67.6\n76.2\n78.1\n78.8\n82.3\n55.6\n55.4\n55.8\n56.1\n56.0\n34.2\n34.3\n35.1\n35.6\n36.1\n58.0\n60.0\n57.8\n58.9\n57.1\n-16\n55.1\n79.7\n81.2\n83.4\n84.1\n57.3\n57.3\n57.9\n58.2\n57.3\n30.9\n35.9\n37.6\n38.3\n39.4\n55.6\n56.4\n59.6\n62.9\n64.0\n-24\n63.9\n74.4\n73.3\n75.8\n78.1\n56.2\n56.6\n55.2\n54.3\n53.5\n31.8\n34.2\n34.3\n34.6\n34.6\n52.5\n57.1\n56.9\n57.4\n59.1\nLLaVA-Next \u22c6\nVicuna (7B)\n44.6\n38.5\n32.7\n23.5\n<1\n<1\n45.3\n30.1\n33.5\n29.8\n28.6\n<1\n18.7\n16.7\n17.8\n14.5\n15.9\n<1\n55.3\n54.6\n54.0\n52.8\n52.4\nNaN\nLLaVA-Next\nVicuna (7B)\n46.2\n<1\n<1\n<1\n<1\n<1\n49.7\n<1\n<1\n<1\n<1\n<1\n19.8\n16.4\n14.0\n11.2\n<1\n<1\n55.9\n53.6\n54.7\n55.6\nNaN\nNaN\n-w/o visual\n47.2\n47.7\n48.1\n47.6\n44.3\n51.2\n52.6\n52.9\n51.5\n52.8\n21.1\n20.6\n21.7\n20. 5\n19.9\n56.1\n56.5\n55.9\n55.6\n55.0\nAIM\n50.1\n54.5\n58.2\n57.5\n53.4\n54.0\n55.1\n55.3\n55.0\n54.6\n23.1\n23.8\n24.1\n21.5\n20.2\n55.6\n56.6\n58.6\n57.8\n55.6\n-16\n49.5\n51.7\n44.3\n30.8\n26.5\n51.8\n51.8\n51.7\n50.6\n48.7\n21.4\n20.9\n21.8\n17.4\n14.8\n55.5\n55.7\n56.3\n53.4\n53.0\n-24\n47.1\n47.5\n46.0\n39.2\n38.0\n53.3\n54.7\n54.3\n55.0\n51.4\n23.3\n23.8\n21.6\n20.0\n19.0\n55.7\n56.4\n55.3\n54.7\n53.3\nTable 3: Main results of AIM. w/o visual stands for providing textual label only. -16/24 represents the number of LLM layers applied to aggregate image information. \u2020 stands for the results from previous works and \u25b7\u25c1indicates extra providing 2 textual label in 0-shot. \u22c6represents further tune backbones with LoRA. The 0-shot results of AIM and its backbone are the same and merged because no demonstrations are provided for aggregation.\n[answer]\u201d. Notably, following the previous studies (Alayrac et al., 2022; Awadalla et al., 2023), we explicitly provide OCR text as inputs of AIM and baselines, and we don\u2019t extra prompt AIM can respond \u201cunanswerable\u201d in Vizwiz, reducing inappropriate induction.\n# 4.4 Baselines\nConsidering our aim to enable any MLLMs to embrace efficient ICL, the underlying backbones within AIM are convinced baselines to compare their efficiency and performance, namely, QWenVL and LLaVA-Next. We also cite the results of the comparative MLLMs from their published studies for reference:\n\u2022 Flamingo: Similar to AIM, Flamingo, with the inner LLM having 7B parameters, breaks the quality between visual and textual modality and achieves outstanding performance on large-shot settings.\n\u2022 OpenFlamingo: OpenFlamingo reproduces Flamingo in image-text tasks on the public dataset MMC4 which is also involved in the training set of AIM, with the inner LLM having 7B parameters.\nOther MLLMs focus on multimodal ICL such as Otter (Li et al., 2023a) and MMICL (Zhao et al., 2023) due to non-overlapped evaluating datasets (Otter) and different model sizes (MMICL).\n# 4.5 Result\nWe filter the benchmarks occurring in the training of our selected backbones to simulate the practical in-context learning situation. Interestingly,\nwhen provided QWen-VL with demonstrations including textual information merely (w/o visual in Table 3), it even outperforms the large shots situation provided both visual and textual. Additionally, QWen-VL produces significant performance degradation in all 4 benchmarks when provided with over 8 demonstrations. This further highlights that treating visual and textual tokens equally limits MLLMs from exhibiting outstanding ICL performance, despite QWen-VL having developed multi-image understanding ability during training. When concentrating on LLaVANext, especially in the close-ended evaluation, perplexities concerning golden labels become NotaNumber (NaN) in 8- and 16-shot settings, occurring overflow while calculating PPL. In other vision language tasks, LLaVA-Next fails to generate when provided over one demonstration and occurs <1 metric in evaluation since it didn\u2019t learn to understand interleaved image-text prompts during pre-training. AIM aggregates image information into its linguistic space before generation. The input format is close to prompts containing a single query image, significantly bridging the modal gap. In this case, MLLMs are only required to attend to the query image while fused tokens still guide generation, thus bringing more concise responses. Additionally, the valuable merit artfully unlocks the ICL ability of MLLM trained on the single imagetext pair. It is verified by the successful deployment on LLaVA-Next that the fused tokens combined with image and text information are harmless for the inner Vicuna. For vision language tasks involved in this spectrum, QWen-based AIM outperforms backbones\nprovided with concrete visual features that achieve +18 CIDEr gains in 16-shot in Flickr30k. In the Vizwiz dataset, over 33% answers are answerable in the statistic. AIM exhibits relatively lower performance compared with other multimodal ICL methods since we don\u2019t prompt AIM to output \u2018unanswerable\u2019, avoiding not solid short-cut answers. Both OpenFlamingo and AIM employ MMC4 as the multi-image training set, but AIM, even applying LLaVA-Next as the backbone, still achieves comparable or even outstanding performance via aggregating image information when provided with random demonstrations (refer to the Random row). (Open) Flamingo applies RICES (Retrievalbased In-Context Example Selection) (Yang et al., 2022) to select demonstrations in latent space and achieve better performance. The relevant results of AIM applying RICES are provided in Table 5 to discuss performance variance concerning random and well-retrieved demonstrations.\n# 5 Analysis\n# 5.1 Training Data Abalation\nTo avoid the model learning to generate shortcut answers because of in-domain or task-relevant training data, we train the linear layer on MMC4, which is a popular image-text-interleaved pretraining dataset used in OpenFlamingo. However, training AIM on MMC4 is still potential to help MLLMs better understand image-text-interleaved inputs, achieving performance gains. Due to AIM modifies the input form of MLLMs, we adopt LoRA to tune the built-in MLLMs, the QWen-VL and the LLaVA-Next, with comparable trainable parameters (17M) as AIM in Table 3 by setting the LoRA rank to 32 to simulate these gains(referring to \u22c6in Table 3). Further tuning on MMC4 improves LLaVA-Next in reading ICL prompts to some extent, but LLaVA-Next still underperforms providing pure text demonstrations (w/o visual). Due to QWen-VL having developed the multi-image understanding ability during pre-training, further training on MMC4 is not necessary, exhibiting even poorer performances since the web corpora is quite different from captioning, VQA, or image understanding.\n# 5.2 LLM Layer Count for Aggregation\nConsidering the first layers directly interact with pre-trained embeddings, we perform ablation experiments on the first half (16), and the first threequarters (-24) to explore the number of LLM layers required to aggregate image information in Table 3. It is interesting that QWen-VL prefers the first 16 layers, while LLaVA-Next is inclined to full layers being the aggregator. Therefore, the label words claim (Wang et al., 2023b) that shallow layers (first half) focus on information aggregation is not completely applicable for LLaVA-Next in multimodal settings. From a posterior view, LLaVA-based AIM obtains stable performance gains with the aggregating layers become deeper. We attribute this conflict to LLaVA being pre-trained on single images, requiring deeper LLM layers to fuse image information into corresponding label space thoroughly, thus reducing the understanding difficulty for the built-in LLM. Additionally, The Connecter in the LLaVA-Next project visual features from ViT to 576 visual tokens for a 336*336 image, while QWen-VL has 256 learnable queries. Therefore, LLaVA-based AIM requires deeper LLM layers to perform visual information gathering.\n# 5.3 Perplexity Tendency of ICL\nWe briefly illustrate the perplexity variation tendency concerning golen labels of Flickr30k in Figure 5 with the number of demonstrations changing from 0 to 16. Notably, the perplexity blast occurring in both QWen-VL and LLaVA-Next in the large shot setting indicates that the provided demonstration sequences significantly confused the underlying backbones, resulting in bad responses. While in the scope of AIM, the perplexity presents a decreasing tendency in general with some noise brought by randomly sampled demonstrations. Additionally, the most perplexity values are inferior to the 0-shot ones, indicating the provided demonstrations have a positive effect on helping MLLM generate current golden label responses.\n# 5.4 Inference Throughput of AIM\nAIM utilizes the inner LLM of existing MLLMs to complete image information aggregation operation. This character makes AIM not need to load the other \u201caggregator\u201d, alleviating the memory costs. However, image information aggrega-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9153/9153c505-5c6b-40d7-973f-0684b338e67a.png\" style=\"width: 50%;\"></div>\nFigure 5: The perplexity variation tendency corresponds to the number of demonstrations. 0-shot server as the baseline.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/57ae/57ae2102-25ab-4d95-b09b-dd67a4111ee1.png\" style=\"width: 50%;\"></div>\nFigure 6: The throughput (iter/s) variation tendency is evaluated on a single H800, with the number of demonstrations increasing from 0 to 16.\n<div style=\"text-align: center;\">Figure 6: The throughput (iter/s) variation tendency is evaluated on a single H800, with the number of demonstrations increasing from 0 to 16.</div>\ntion requires inevitable but minimal time costs due to the parallel computation of forward propagation. What\u2019s more, AIM drops all the visual tokens after aggregating them into the dense label space, which compensates for aggregation time costs to some extent by reducing the input length during generation3. We evaluate the throughput (iter/s) of AIM on Flick30k in Figure 11. Overall, in the few shot settings (less than 8), naive MLLMs are more efficient than AIM, but AIM has a lower inference latency increment. AIM becomes more efficient than the underlying backbone when provided with over 16 demonstrations.\n3For QWen-VL, AIM drops 256 visual tokens consistently, and AIM drops 576 visual tokens for a 336*336 resolution image in LLaVA-Next.\n3For QWen-VL, AIM drops 256 visual tokens consistently, and AIM drops 576 visual tokens for a 336*336 resolution image in LLaVA-Next.\nMethod\n# Visual Tokens\nAvg. Textual Tokens\nAvg. Ratio\nFlickr30k\nOKVQA\nVizwiz\nHateful Memes\nAIM-QWen\n256\n30.1\n16.0\n15.7\n29.3\n8%\nAIM-LLaVA\n576\n34.4\n18.3\n17.9\n33.6\n4%\nTable 4: Quantity statistics of visual and textual tokens in multimodal demonstrations.\n# 5.5 Memory Cost of AIM\nThe normal attention mechanism is known as O(W 2) space complexity concerning a sequence with W words. Therefore, the length challenge brought by in-context demonstrations stimulates memory explosion straightforwardly. AIM drops the visual tokens after image information aggregation and the remaining ratios of fused tokens R can be calculated according to the number of visual and textual tokens, denoted as |V | and |T|:\n(5)\nWe demonstrate the quantity statistics over four datasets in Table 4, indicating that LLaVA-based AIM merely retains about 4% origin tokens in each multimodal demonstration. Although LLaVA-Next integrates FlashAttention, dropping visual tokens still saves noticeable memory costs as illustrated in Figure 1. Notably, even if visionlanguage tasks have extremely long textual labels in assumption, AIM is capable of performing efficient ICL as normal with R close to 1 since visual tokens have been dropped and the textual tokens are required anyway.\n# 6 Conclusion\nIn this paper, our initial exploration delves into the attention distribution within the multimodal ICL, revealing that the MLLM exhibits a greater emphasis on the linguistic modality. Built upon this discovery, we propose a light multimodal framework AIM aiming to let any MLLMs embrace efficient ICL, which aggregates the image information of demonstrations into their dense latent space of demonstrated texts. Generally, AIM transforms the multimodal ICL demonstration sequence into a form resembling a single query image accompanied by textual tokens. Thereby, AIM successfully coordinates with any MLLMs regardless of their initial support for multimodal ICL. Except for the outstanding performance of AIM compared with MMLMs specifically trained on multimodal ICL, AIM is both training and inferencing efficient due to its frozen de facto backbone and dropping hundreds of visual tokens.\n# References\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966.\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. 2022. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. 2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\nJun Gao. 2024a. Selfcp: Compressing long prompt to 1/12 using the frozen large language model itself. arXiv preprint arXiv:2405.17052. Jun Gao. 2024b. Unifying demonstration selection and compression for in-context learning. arXiv preprint arXiv:2405.17062. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context compression in a large language model. arXiv preprint arXiv:2307.06945. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608\u20133617. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. 2024. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:2611\u2013 2624. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. 2022. Learned token pruning for transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 784\u2013794. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023a. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608\u20133617.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. 2024. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597. Xiang Lisa Li and Percy Liang. 2021. Prefixtuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. Yucheng Li. 2023. Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering. arXiv preprint arXiv:2304.12102. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024a. Llava-next: Improved reasoning, ocr, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024b. Visual instruction tuning. Advances in neural information processing systems, 36. Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for abstractive text summarization. arXiv preprint arXiv:2303.15621. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Okvqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204. AI Meta. 2023. Introducing llama: A foundational, 65-billion-parameter large language model. Meta AI. https://ai. facebook. com/blog/large-language-model-llama-metaai. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467.\n# OpenAI. 2023. Gpt-4 technical report.\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200\u2013212.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023b. Label words are anchors: An information flow perspective for understanding in-context learning. arXiv preprint arXiv:2305.14160.\nZengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023c. Is chatgpt a good sentiment analyzer? a preliminary study. arXiv preprint arXiv:2304.04339. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2023. Zero-shot information extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205. David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023. Exploring the limits of chatgpt for query or aspectbased text summarization. arXiv preprint arXiv:2302.08081. Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. 2024. Exploring diverse in-context configurations for image captioning. Advances in Neural Information Processing Systems, 36. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3081\u20133089. Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. 2023a. Internlm-xcomposer: A vision-language large model for advanced textimage comprehension and composition. arXiv preprint arXiv:2309.15112.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3081\u20133089.\nPan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. 2023a. Internlm-xcomposer: A vision-language large model for advanced textimage comprehension and composition. arXiv preprint arXiv:2309.15112.\nRenrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. 2023b. Llamaadapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199.\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. 2023. Mmicl: Empowering vision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915.\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2023. Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939.\n# A Limitation and Discussion\nAlthough AIM makes MLLMs embrace efficient ICL regardless of their backbone support reading multi-images initially, there are still limitations. The token remaining ratio in the original demonstration is determined according to the number of textual tokens, and therefore AIM will obtain less minimal efficiency gains if the labels are extremely over-length, despite the visual tokens being dropped and the textual tokens will exist anyway. Additionally, caching all demonstrations takes up a lot of storage space with the demonstrations increasing as well, and we are attempting to quantify the virtual demonstrations to alleviate this problem.\n# B Evaluation Using RICES\nWe mainly discuss using randomly sampled incontext demonstrations in the main text. We employ RICES to retrieve similar demonstrations by measuring the image cosine similarity according to CLIP ViT-L/14. The relevent results are presented in Table 5.\n# C Information Flow\nTo better explain how AIM performs image information aggregation and response generation, we illustrate the information flow of AIM in Figure 7. Specifically, the image and labels derivate visual\nMethod\nLLM\nFlickr30k\nOKVQA\nVizwiz\nHateful Memes\nCIDEr \u2191\nVQA-ACC \u2191\nVQA-ACC \u2191\nACC. \u2191\n#-shots\n#-shots\n#-shots\n#-shots\n1\n2\n4\n8\n16\n1\n2\n4\n8\n16\n1\n2\n4\n8\n16\n1\n2\n4\n8\n16\nQWen-VL\nQWen (7B)\n76.5\n79.3\n78.4\n72.9\n70.4\n47.3\n57.6\n56.2\n54.3\n53.5\n30.2\n31.4\n32.5\n32.2\n30.0\n61.7\n62.4\n64.0\n62.9\n59.2\nAIM\n69.8\n78.6\n79.5\n80.8\n83.3\n55.6\n55.4\n55.8\n56.1\n56.0\n36.3\n37.1\n37.9\n38.7\n39.4\n63.4\n65.2\n65.4\n66.1\n65.7\n-16\n60.3\n81.3\n80.9\n84.2\n85.6\n58.0\n59.6\n59.3\n60.1\n61.2\n32.1\n37.2\n38.5\n39.1\n40.2\n64.8\n66.7\n67.1\n66.9\n67.3\n-24\n64.2\n79.2\n80.7\n82.6\n84.5\n57.3\n58.1\n59.4\n60.8\n61.2\n33.2\n36.2\n36.8\n39.1\n38.9\n58.6\n59.6\n61.2\n63.6\n64.0\nLLaVA-Next\nVicuna (7B)\n<1\n<1\n<1\n<1\n<1\n<1\n<1\n<1\n<1\n<1\n16.9\n13.1\n<1\n<1\n<1\n53.8\n53.3\n54.4\nNaN\nNaN\nAIM\n53.9\n55.7\n60.3\n62.1\n64.6\n57.3\n59.2\n62.8\n61.9\n62.3\n25.8\n26.9\n28.3\n28.9\n31.2\n60.6\n61.4\n62.0\n63.4\n65.1\n-16\n52.1\n53.8\n46.5\n34.1\n28.6\n53.1\n52.4\n53.1\n54.7\n51.6\n23.2\n24.6\n22.8\n19.7\n18.4\n59.3\n60.1\n51.5\n52.1\n53.9\n-24\n53.8\n56.2\n54.3\n51.4\n47.1\n55.6\n56.7\n57.1\n59.2\n54.1\n24.6\n25.1\n26.3\n25.5\n23.3\n59.7\n61.2\n62.4\n58.7\n55.9\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ade/0adea194-ab10-4afa-a8fd-eac0e9d8420d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 5: Evlaution using RICES.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e10/2e102425-0e52-4f90-accf-42334e418018.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: The information flow of How AIM performs image information aggregation (left) and response generation (right).</div>\ntokens and textual tokens, respectively, where the textual ones are much shorter than the visual ones. Then, AIM aggregates the visual information into dense latent space of labels through forward propagation that gets the last hidden states on top of the labels and converts them into fused tokens. Finally, fused tokens, serving as in-context demonstrations, together with the current query, are fed into the inner MLLM for response generation.\n# D Perplexity and Throughput Analysises\nSince Hateful Memes utilizes close-ended evaluation reflecting PPL implicitly, we provide the perplexity tendency on Flickr30k, OKVQA, and VizWiz in Figure 10, with the backbones of QWen-VL and LLaVA-Next. Additionally, we demonstrate the throughput variation with respect to the number of demonstrations on Flickr30k, OKVQA, Vizwiz, and Hateful Memes in 11, with the backbones of QWen-VL and LLaVA-Next.\n# E Training Loss Curve\nWe utilize Huggingface Trainer to optimize AIM for about 12 GPU hours on a single H800 node, with the default optimizer and scheduler. We\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a0b/4a0ba0ad-9a4f-4ba3-a2a0-030d7710d6fe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Training loss curve of AIM, with QWen-VL being the backbone.</div>\ndemonstrate the training loss of AIM per 500 steps in Figure 8 and Figure 9.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e725/e725b2e0-d5b8-4eec-b8ba-227d6fd0ec14.png\" style=\"width: 50%;\"></div>\nFigure 9: Training loss curve of AIM, with LLaVANext being the backbone.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f09d/f09d3262-0804-47fb-9aae-8be6a77d3e02.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4dc0/4dc0c860-6763-4b2f-aaf0-9525ce142ab6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Throughput analysis on Flickr30k, OKVQA, Vizwiz and Hateful Memes, with the backbone of QWen VL and LLaVA-Next.</div>\n<div style=\"text-align: center;\">Figure 11: Throughput analysis on Flickr30k, OKVQA, Vizwiz and Hateful Memes, with the backbone of QWen</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) advances Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters. However, in the area of multimodal Large Language Models (MLLMs), two problems hinder the application of multimodal ICL: Most primary MLLMs are only trained on single-image datasets, making them unable to read extra multimodal demonstrations, and with the demonstrations increasing, thousands of visual tokens highly challenge hardware and degrade ICL performance.",
        "problem": {
            "definition": "The problem is the inability of current MLLMs to efficiently handle multimodal demonstrations due to their training on single-image datasets and the excessive number of visual tokens generated from multiple images, which leads to high memory costs and performance degradation.",
            "key obstacle": "The main difficulty is that existing methods treat visual and textual tokens equally, resulting in overwhelming memory costs and poor performance when handling multiple visual inputs."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that MLLMs focus more on the linguistic modality than the visual modality during generation, indicating that efficient processing of multimodal inputs is possible by prioritizing text.",
            "opinion": "The proposed idea entails a framework, AIM, which aggregates image information of multimodal demonstrations into the latent space of the corresponding textual labels, allowing MLLMs to perform in-context learning more efficiently.",
            "innovation": "The key innovation lies in the method of generating fused virtual tokens that replace lengthy visual tokens, effectively transforming multimodal prompts into a format resembling a single query image with associated text, thus enhancing performance while reducing memory costs."
        },
        "method": {
            "method name": "AIM",
            "method abbreviation": "AIM",
            "method definition": "AIM is a framework designed to facilitate efficient multimodal in-context learning by aggregating image information from multimodal demonstrations into their corresponding textual representations.",
            "method description": "AIM transforms multimodal ICL demonstrations into a format that resembles a single query image with associated text, allowing MLLMs to handle inputs more effectively.",
            "method steps": [
                "1. Process each multimodal demonstration independently to obtain hidden states for images and texts.",
                "2. Use a linear layer to project the hidden states of images onto their corresponding text to generate fused virtual tokens.",
                "3. Replace the original visual tokens with the fused virtual tokens, creating a demonstration bank for efficient retrieval.",
                "4. Feed the concatenated demonstration sequence and the query image into the inner LLM for response generation."
            ],
            "principle": "The effectiveness of AIM stems from its ability to reduce the input length by dropping visual tokens, allowing the inner LLM to focus solely on the query image while still utilizing the informative context provided by the fused tokens."
        },
        "experiments": {
            "evaluation setting": "AIM was evaluated on benchmarks such as image captioning (Flickr30k), visual question answering (OKVQA, Vizwiz), and hateful speech detection (Hateful Memes), using datasets that were not part of the training data.",
            "evaluation method": "The performance was assessed based on metrics like CIDEr for image captioning, VQA accuracy for visual question answering, and ROC AUC scores for hateful speech detection."
        },
        "conclusion": "AIM successfully demonstrates that it can efficiently upgrade MLLMs for multimodal ICL by significantly reducing the number of visual tokens while maintaining or improving performance compared to existing methods.",
        "discussion": {
            "advantage": "The main advantages of AIM include its efficiency in terms of memory usage, the ability to work with any MLLM regardless of prior training on multimodal inputs, and the reduction of input length, which leads to faster inference times.",
            "limitation": "A limitation of AIM is that its efficiency gains diminish when the textual labels are excessively long, as the remaining tokens are still based on the length of the labels.",
            "future work": "Future research could focus on optimizing the storage of cached demonstrations and further exploring the balance between visual and textual token processing to enhance performance."
        },
        "other info": {
            "info1": "AIM employs only 17M trainable parameters, making it parameter-efficient.",
            "info2": {
                "info2.1": "The framework allows asynchronous processing of demonstrations within a batch.",
                "info2.2": "AIM has been shown to achieve comparable or superior performance to its underlying MLLMs with significantly fewer tokens."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) advances Large Language Models (LLMs) exhibiting emergent ability on downstream tasks without updating billions of parameters."
        },
        {
            "section number": "1.3",
            "key information": "AIM is a framework designed to facilitate efficient multimodal in-context learning by aggregating image information from multimodal demonstrations into their corresponding textual representations."
        },
        {
            "section number": "3.1",
            "key information": "The main difficulty is that existing methods treat visual and textual tokens equally, resulting in overwhelming memory costs and poor performance when handling multiple visual inputs."
        },
        {
            "section number": "3.4",
            "key information": "The effectiveness of AIM stems from its ability to reduce the input length by dropping visual tokens, allowing the inner LLM to focus solely on the query image while still utilizing the informative context provided by the fused tokens."
        },
        {
            "section number": "4.1",
            "key information": "The proposed idea entails a framework, AIM, which aggregates image information of multimodal demonstrations into the latent space of the corresponding textual labels, allowing MLLMs to perform in-context learning more efficiently."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of AIM is that its efficiency gains diminish when the textual labels are excessively long, as the remaining tokens are still based on the length of the labels."
        },
        {
            "section number": "7",
            "key information": "AIM successfully demonstrates that it can efficiently upgrade MLLMs for multimodal ICL by significantly reducing the number of visual tokens while maintaining or improving performance compared to existing methods."
        }
    ],
    "similarity_score": 0.7187485943318376,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/AIM_ Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning.json"
}