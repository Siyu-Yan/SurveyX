{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2102.06062",
    "title": "Deep Learning with Label Differential Privacy",
    "abstract": "The Randomized Response (RR) algorithm [96] is a classical technique to improve robustness in survey aggregation, and has been widely adopted in applications with differential privacy guarantees. We propose a novel algorithm, Randomized Response with Prior (RRWithPrior), which can provide more accurate results while maintaining the same level of privacy guaranteed by RR. We then apply RRWithPrior to learn neural networks with label differential privacy (LabelDP), and show that when only the label needs to be protected, the model performance can be significantly improved over the previous state-of-the-art private baselines. Moreover, we study different ways to obtain priors, which when used with RRWithPrior can additionally improve the model performance, further reducing the accuracy gap between private and non-private models. We complement the empirical results with theoretical analysis showing that LabelDP is provably easier than protecting both the inputs and labels.",
    "bib_name": "ghazi2021deeplearninglabeldifferential",
    "md_text": "# Deep Learning with Label Differential Privacy\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c3f/3c3f7493-0076-4438-b0c7-95df3740e67e.png\" style=\"width: 50%;\"></div>\n# Abstract\nThe Randomized Response (RR) algorithm [96] is a classical technique to improve robustness in survey aggregation, and has been widely adopted in applications with differential privacy guarantees. We propose a novel algorithm, Randomized Response with Prior (RRWithPrior), which can provide more accurate results while maintaining the same level of privacy guaranteed by RR. We then apply RRWithPrior to learn neural networks with label differential privacy (LabelDP), and show that when only the label needs to be protected, the model performance can be significantly improved over the previous state-of-the-art private baselines. Moreover, we study different ways to obtain priors, which when used with RRWithPrior can additionally improve the model performance, further reducing the accuracy gap between private and non-private models. We complement the empirical results with theoretical analysis showing that LabelDP is provably easier than protecting both the inputs and labels.\n# 1 Introduction\nThe widespread adoption of machine learning in recent years has increased the concerns about the privacy of individuals whose data is used during the model training. Differential privacy (DP) [34, 33] has emerged as a popular privacy notion that has been the basis of several practical deployments in industry [35, 82, 43, 9, 29] and the U.S. Census [3]. A classical algorithm\u2014that predates DP and was initially designed to eliminate evasive answer biases in survey aggregation\u2014is Randomized Response (RR) [96]: when the input is an element from a finite alphabet, the output is equal to the input with a certain probability, and is a uniform random other element from the alphabet with the remaining probability. This simple algorithm is shown to satisfy the strong notion of local DP [38, 58], whereby the response of each user is protected, in contrast with the so-called central DP setting where a curator has access to the raw user data, and only the output of the curator is required to be DP. We note that schemes building on RR have been studied in several previous works on DP estimation (e.g., [30, 57]), and have been deployed in practice [82]. Meanwhile, the large error incurred by RR (e.g., [17]) has stimulated significant research aiming to improve its accuracy, mostly by relaxing to weaker privacy models (e.g., [36, 24, 4]). In this work, we use a different approach and seek to improve RR by leveraging available prior information. (A recent work of Liu et al. [63] also used priors to improve accuracy, but in the context of the DP multiplicative weights algorithm, which applies in the central DP setting.) The prior information can consist of domain-specific knowledge, (models trained on) publicly available data, or historical\nruns of a training algorithm. Our algorithm is presented in Section 3 (Algorithm 2). At a high level, given a prior distribution p on the alphabet, the algorithm uses p to prune the alphabet. If the prior is reliable and even if the alphabet is only minimally pruned, the probability that the output equals the input is larger than when RR is applied to the entire alphabet. On the other hand, if the prior is uniform over the entire alphabet, then our algorithm recovers the classical RR. To implement the above recipe, one needs to specify how to effect the pruning using p. It turns out that the magnitude of pruning can itself vary depending on p, but we can obtain a closed-form formula for determining this. Interestingly, by studying a suitable linear program, we show that the resulting RRWithPrior strategy is optimal in that among all \u03b5-DP algorithms, it maximizes the probability that the output equals the input when the latter is sampled from p (Theorem 3).\n# 1.1 Applications to Learning with Label Differential Privacy\nThere have been a great number of papers over the last decade that developed DP machine learning algorithms (e.g., [19, 102, 88, 83\u201385, 78]). In the case of deep learning, the seminal work of Abadi et al. [2] introduced a DP training framework (DP-SGD) that was integrated into TensorFlow [80] and PyTorch [92]. Despite numerous followup works, including, e.g., [75\u201377, 67, 98, 71, 20, 93], and extensive efforts, the accuracy of models trained with DP-SGD remains significantly lower than that of models trained without DP constraints. Notably, for the widely considered CIFAR-10 dataset, the highest reported accuracy for DP models is 69.3% [93], which strikingly relies on handcrafted visual features despite that in non-private scenarios learned features long been shown to be superior. Even using pre-training with external (CIFAR-100) data, the best reported DP accuracy, 73%2, is still far below the non-private baselines (> 95%). The performance gap becomes a roadblocker for many real-world applications to adopt DP. In this paper, we focus on a more restricted, but important, special case where the DP guarantee is only required to hold with respect to the labels, as described next. In the label differential privacy (LabelDP) setting, the labels are considered sensitive, and their privacy needs to be protected, while the input points are not sensitive. This notion has been studied in the PAC setting [18, 13] and for the particular case of sparse linear regression [94], and it captures several practical scenarios. Examples include: (i) computational advertising where the impressions are known to the Ad Tech3, and thus considered non-sensitive, while the conversions reveal user interest and are thus private (see, e.g., Nalpas and Dutton [70] and [8]), (ii) recommendation systems where the choices are known, e.g., to the streaming service provider, but the user ratings are considered sensitive, and (iii) user surveys and analytics where demographic information (e.g., age, gender) is non-sensitive but income is sensitive\u2014in fact, this was the motivating reason for Warner [96] to propose RR many decades ago! We present a novel multi-stage algorithm (LP-MST) for training deep neural networks with LabelDP that builds on top of RRWithPrior (see Section 3 and Algorithm 3), and we benchmark its empirical performance (Section 5) on multiple datasets, domains, and architectures, including the following. \u2022 On CIFAR-10, we show that it achieves 20% higher accuracy than DP-SGD4. \u2022 On the more challenging CIFAR-100, we present the first non-trivial DP learning results. \u2022 On MovieLens, which consists of user ratings of movies, we show improvements via LP-MST. In some applications, domain specific algorithms can be used to obtain priors directly without going through multi-stage training. For image classification problems, we demonstrate how priors computed from a (non-private) self-supervised learning [22, 23, 44, 53, 16] phase on the input images can be used to achieve higher accuracy with a LabelDP guarantee with extremely small privacy budgets (\u03b5 \u22640.1, see Section 5.2 for details). We note that due to the requirement of DP-SGD to compute and clip per-instance gradient, it remains technically challenging to scale to larger models or mini-batch sizes, despite numerous attempts to minigate this problem [42, 5, 26, 90]. On the other hand, our formulation allows us to use state-of2For DP parameters of \u03b5 = 8 and \u03b4 = 10\u22125, cited from Abadi et al. [2, Figure 6]. For a formal definition of DP, we refer the reader to Definition 2.1. 3Ad tech (abbreviating Advertising Technology) comprises the tools that help agencies and brands target, deliver, and analyze their digital advertising efforts; see, e.g., blog.hubspot.com/marketing/what-is-ad-tech. 4We remark that the notion of \u03b5-DP in [2, 76, 75] is not directly comparable to \u03b5-Label DP in our work in that they use the addition/removal notion whereas we use the substitution one. Please see the Supplementary Material for more discussion on this.\nthe-art deep learning architectures such as ResNet [51]. We also stress that our LP-MST algorithm goes beyond deep learning methods that are robust to label noise. (See [87] for a survey of the latter.) Our empirical results suggest that protecting the privacy of labels can be significantly easier than protecting the privacy of both inputs and labels. We find further evidence to this by showing that for the special case of stochastic convex optimization (SCO), the sample complexity of algorithms privatizing the labels is much smaller than that of algorithms privatizing both labels and inputs; specifically, we achieve dimension-independent bounds for LabelDP (Section 6). We also show that a good prior can ensure smaller population error for non-convex loss. (Details are in the Supplementary Material.)\n# 2 Preliminaries\nFor any positive integer K, let [K] := {1, . . . , K}. Randomized response (RR) [96] is the following: let \u03b5 \u22650 be a parameter and let y \u2208[K] be the true value known to RR\u03b5. When an observer queries the value of y, RR\u03b5 responds with a random draw \u02dcy from the following probability distribution:\nIn this paper, we focus on the application of learning with label differential privacy. We recall the definition of differential privacy (DP), which is applicable to any notion of neighboring datasets. For a textbook reference, we refer the reader to Dwork and Roth [32]. Definition 2.1 (Differential Privacy (DP) [33, 34]). Let \u03b5, \u03b4 \u2208R\u22650. A randomized algorithm A taking as input a dataset is said to be (\u03b5, \u03b4)-differentially private ((\u03b5, \u03b4)-DP) if for any two neighboring datasets D and D\u2032, and for any subset S of outputs of A, it is the case that Pr[A(D) \u2208S] \u2264 e\u03b5 \u00b7 Pr[A(D\u2032) \u2208S] + \u03b4. If \u03b4 = 0, then A is said to be \u03b5-differentially private (\u03b5-DP). When applied to machine learning methods in general and deep learning in particular, DP is usually enforced on the weights of the trained model [see, e.g., 19, 59, 2]. In this work, we focus on the notion of label differential privacy. Definition 2.2 (Label Differential Privacy). Let \u03b5, \u03b4 \u2208R\u22650. A randomized training algorithm A taking as input a dataset is said to be (\u03b5, \u03b4)-label differentially private ((\u03b5, \u03b4)-LabelDP) if for any two training datasets D and D\u2032 that differ in the label of a single example, and for any subset S of outputs of A, it is the case that Pr[A(D) \u2208S] \u2264e\u03b5 \u00b7 Pr[A(D\u2032) \u2208S] + \u03b4. If \u03b4 = 0, then A is said to be \u03b5-label differentially private (\u03b5-LabelDP).\n# 3 Randomized Response with Prior\nIn many real world applications, a prior distribution about the labels could be publicly obtained from domain knowledge and help the learning process. In particular, we consider a setting where for each (private) label y in the training set, there is an associated prior p = (p1, . . . , pK). The goal is to output a randomized label \u02dcy that maximizes the probability that the output is correct (or equivalently maximizes the signal-to-noise ratio), i.e., Pr[y = \u02dcy]. The privacy constraint here is that the algorithm should be \u03b5-DP with respect to y. (It need not be private with respect to the prior p.) We first describe our algorithm RRWithPrior by assuming access to such priors.\n# 3.1 Algorithm: RRWithPrior\nWe build our RRWithPrior algorithm with a subroutine called RRTop-k, as shown in Algorithm 1, which is a modification of randomized response where we only consider the set of k labels i with largest pi. Then, if the input label y belongs to this set, we use standard randomized response on this set. Otherwise, we output a label from this set uniformly at random. The main idea behind RRWithPrior is to dynamically estimate an optimal k\u2217based on the prior p, and run RRTop-k with k\u2217. Specifically, we choose k\u2217by maximizing Pr[RRTop-k(y) = y]. It is not\n(1)\nAlgorithm 1 RRTop-k\nInput: A label y \u2208[K]\nParameters: k \u2208[K], prior p = (p1, . . . , pK)\n1. Let Yk be the set of k labels with maximum prior probability (with ties broken arbritrarily).\n2. If y \u2208Yk, then output y with probability\ne\u03b5\ne\u03b5+k\u22121 and output y\u2032 \u2208Yk \\ {y} with probability\n1\ne\u03b5+k\u22121.\n3. If y \u0338\u2208Yk, output an element from Yk uniformly at random.\nhard to see that this expression is exactly equal to\ne\u03b5\ne\u03b5+k\u22121 \u00b7\n\ufffd\ufffd\n\u02dcy\u2208Yk p\u02dcy\n\ufffd\nif y \u223cp. RRWithPrior is\npresented in Algorithm 2.\nAlgorithm 2 RRWithPrior\nInput: A label y \u2208[K]\nParameters: prior p = (p1, . . . , pK)\n1. For k \u2208[K]:\n(a) Compute wk :=\ne\u03b5\ne\u03b5+k\u22121 \u00b7\n\ufffd\ufffd\n\u02dcy\u2208Yk p\u02dcy\n\ufffd\n, where Yk is the set of k labels with maximum\nprior probability (ties broken arbritrarily).\n2. Let k\u2217= arg maxk\u2208[K] wk.\n3. Return an output of RRTop-k (y) with k = k\u2217.\nIt is not hard to show that RRTop-k is \u03b5-DP. Lemma 1. RRTop-k is \u03b5-DP.\nThe privacy guarantee of RRWithPrior follows immediately from that of RRTop-k (Lemma 1) since our choice of k does not depend on the label y: Corollary 2. RRWithPrior is \u03b5-DP.\nFor learning with a LabelDP guarantee, we first use RRWithPrior to query a randomized label for each example of the training set, and then apply a general learning algorithm that is robust to random label noise to this dataset. Note that unlike DP-SGD [2] that makes new queries on the gradients in every training epoch, we query the randomized label once and reuse it in all the training epochs.\n# 3.2 Optimality of RRWithPrior\nIn this section we will prove the optimality of RRWithPrior. For this, we will need additional notation. For any algorithm R that takes as input a label y and outputs a randomized label \u02dcy, we let Objp(R) denote the probability that the output label is equal to the input label y when y is distributed as p; i.e., Objp(R) = Pry\u223cp[R(y) = y], where the distribution of y \u223cp is Pr[y = i] = pi for all i \u2208[K]. The main result of this section is that, among all \u03b5-DP algorithms, RRWithPrior maximizes Objp(R), as stated more formally next. Theorem 3. Let p be any probability distribution on [K] and R be any \u03b5-DP algorithm that randomizes the input label given the prior p . We have that\nIn this section we will prove the optimality of RRWithPrior. For this, we will need additional notation. For any algorithm R that takes as input a label y and outputs a randomized label \u02dcy, we let Objp(R) denote the probability that the output label is equal to the input label y when y is distributed as p; i.e., Objp(R) = Pry\u223cp[R(y) = y], where the distribution of y \u223cp is Pr[y = i] = pi for all i \u2208[K].\nThe main result of this section is that, among all \u03b5-DP algorithms, RRWithPrior maximizes Objp(R), as stated more formally next. Theorem 3. Let p be any probability distribution on [K] and R be any \u03b5-DP algorithm that randomizes the input label given the prior p . We have that\nBefore we proceed to the proof, we remark that our proof employs a linear program (LP) to characterize the optimal mechanisms; a generic form of such LPs has been used before in [48, 41]. However, these works focus on different problems (linear queries) and their results do not apply here.\nBefore we proceed to the proof, we remark that our proof employs a linear program (LP) to characterize the optimal mechanisms; a generic form of such LPs has been used before in [48, 41]. However, these works focus on different problems (linear queries) and their results do not apply here. Proof of Theorem 3. Consider any \u03b5-DP algorithm R, and let q\u02dcy|y denote Pr[R(y) = \u02dcy]. Observe that Objp(R) = \ufffd y\u2208[k] py \u00b7 qy|y.\nProof of Theorem 3. Consider any \u03b5-DP algorithm R, and let q\u02dcy|y denote Pr[R(y) = \u02dcy]. Observe that Objp(R) = \ufffd y\u2208[k] py \u00b7 qy|y.\n\ufffd \u02dcy\u2208[K] q\u02dcy|y = 1, \u2200y \u2208[K], and q\u02dcy|y \u22650, \u2200\u02dcy, y \u2208[K].\nFinally, the \u03b5-DP guarantee of R implies that\n  Combining the above, Objp(R) is upper-bounded by the optimum of the following linear program (LP), which we refer to as LP1:\nNotice that constraints (2) and (3) together imply that:\nIn other words, the optimum of LP1 is at most the optimum of the following LP that we call LP2:\nAn optimal solution to LP2 must be a vertex (aka extreme point) of the polytope defined by (4) and (5). Recall that an extreme point of a K-dimensional polytope must satisfy K independent constraints with equality. In our case, this means that one of the following occurs: \u2022 Inequality (5) is satisfied with equality for all y \u2208[K] resulting in the all-zero solution (whose objective is zero), or, \u2022 For some non-empty subset Y \u2286[K], inequality (4) is satisfied with equality for all y \u2208Y , and inequality (5) is satisfied with equality for all y \u2208[K] \\ Y . This results in\nAn optimal solution to LP2 must be a vertex (aka extreme point) of the polytope defined by (4) and (5). Recall that an extreme point of a K-dimensional polytope must satisfy K independent constraints with equality. In our case, this means that one of the following occurs:\n\u2022 Inequality (5) is satisfied with equality for all y \u2208[K] resulting in the all-zero solution (whose objective is zero), or, \u2022 For some non-empty subset Y \u2286[K], inequality (4) is satisfied with equality for all y \u2208Y , and inequality (5) is satisfied with equality for all y \u2208[K] \\ Y . This results in\nIn conclusion, we have that\nwhere the last two equalities follow from our definitions of Yk and wk. Notice that Objp(RRWithPrior) = maxk\u2208[K] wk. Thus, we get that Objp(RRWithPrior) \u2265Objp(R) as desired.\nwhere the last two equalities follow from our definitions of Yk and wk. Notice that Objp(RRWithPrior) = maxk\u2208[K] wk. Thus, we get that Objp(RRWithPrior) \u2265Objp(R) as desired.\n(2) (3)\n\u2200y \u2208[K].\n(5)\n# 4 Application of RRWithPrior: Multi-Stage Training\nOur RRWithPrior algorithm requires publicly available priors, which could usually be obtained from domain specific knowledge. In this section, we describe a training framework that bootstraps from a uniform prior, and progressively learns refined priors via multi-stage training. This general framework can be applied to arbitrary domains even when no public prior distributions are available. Specifically, we assume that we have a training algorithm A that outputs a probabilistic classifier which, on a given unlabeled sample x, can assign a probability py to each class y \u2208[K]. We partition our dataset into subsets S(1), . . . , S(T ), and we start with a trivial model M (0) that outputs equal probabilities for all classes. At each stage t \u2208[T], we use the most recent model M (t\u22121) to assign the probabilities (p1, . . . , pK) for each sample xi from S(t). Applying RRWithPrior with this prior on the true label yi, we get a randomized label \u02dcyi for xi. We then use all the samples with randomized labels obtained so far to train the model M (t). The full description of our LP-MST (Label Privacy Multi-Stage Training) method is presented in Algorithm 3. We remark here that the partition S(1), . . . , S(T ) can be arbitrarily chosen, as long as it does not depend on the labels y1, . . . , yn. We also stress that the training algorithm A need not be private. We use LP-1ST to denote our algorithm with one stage, LP-2ST to denote our algorithm with two stages, and so on. We also note that LP-1ST is equivalent to using vanilla RR. The tth stage of a multi-stage algorithm is denoted stage-t.\nAlgorithm 3 Multi-Stage Training (LP-MST)\nInput: Dataset S = {(x1, y1), . . . , (xn, yn)}\nParameters: Number T of stages, training algorithm A\n1. Partition S into S(1), . . . , S(T )\n2. Let M (0) be the trivial model that always assigns equal probability to each class.\n3. For t = 1 to T:\n(a) Let \u02dcS(t) = \u2205.\n(b) For each (xi, yi) \u2208S(t):\ni. Let p = (p1, . . . , pK) be the probabilities predicted by M (t) on xi.\nii. Let \u02dcyi = RRWithPriorp(yi).\niii. Add (xi, \u02dcyi) to \u02dcS(t).\n(c) Let M (t) be the model resulting from training on \u02dcS(1) \u222a\u00b7 \u00b7 \u00b7 \u222a\u02dcS(t) using A.\n4. Output M (T ).\n\ufffd\ufffd \ufffd Consider any two datasets D, D\u2032 that differ on a single user\u2019s label; suppose this user is j and that the user belongs to partition \u2113\u2208[T]. Then, the above expression for D and that for D\u2032 are the same in all but one term: Pr \ufffd \u02dcyj = zj \ufffd\ufffdM (\u2113\u22121) = m(\u2113\u22121)\ufffd , which is the probability that RRWithPriorm(\u2113\u22121)(xi) outputs zi. Since RRWithPrior is \u03b5-DP, we can conclude that the ratio between the two probabilities is at most e\u03b5 as desired.\nWe stress that this observation holds because each sensitive label yi is only used once in Line 3(b)ii of Algorithm 3, as the dataset S is partitioned at the beginning of the algorithm. As a result, since each\nTable 1: Test accuracy (%) on CIFAR-10. The baseline performances taken from previously published results correspond to (\u03b5, \u03b4)-DP with \u03b4 = 10\u22125. The star\u22c6indicates the use of CIFAR-100 pre-trained representations.\n<div style=\"text-align: center;\">Table 1: Test accuracy (%) on CIFAR-10. The baseline performances taken from previously published results correspond to (\u03b5, \u03b4)-DP with \u03b4 = 10\u22125. The star\u22c6indicates the use of CIFAR-100 pre-trained</div>\nAlgorithm\n\u03b5 = 1\n\u03b5 = 2\n\u03b5 = 3\n\u03b5 = 4\n\u03b5 = 6\n\u03b5 = 8\n\u03b5 = \u221e\nDP-SGD w/ pre-train\u22c6[2]\n67\n70\n73\n80\nDP-SGD [77]\n61.6(\u03b5=7.53)\n76.6\nTempered Sigmoid [77]\n66.2(\u03b5=7.53)\nYu et al. [98]\n44.3(\u03b5=6.78)\nNasr et al. [71]\n55\nChen and Lee [20]\n53\nScatterNet+CNN [93]\n69.3\nLP-1ST\n59.96\n82.38\n89.89\n92.58\n93.58\n94.70\n94.96\nLP-2ST\n63.67\n86.05\n92.19\n93.37\n94.26\n94.52\n-\nLP-1ST w/ pre-train\u22c6\n67.64\n83.99\n90.24\n92.83\n94.02\n94.96\n95.25\nLP-2ST w/ pre-train\u22c6\n70.16\n87.22\n92.12\n93.53\n94.41\n94.59\n-\n \u221e\nAlgorithm\n\u03b5 = 3\n\u03b5 = 4\n\u03b5 = 5\n\u03b5 = 6\n\u03b5 = 8\nLP-1ST\n20.96\n46.28\n61.38\n68.34\n73.59\nLP-2ST\n28.74\n50.15\n63.51\n70.58\n74.14\nstage is \u03b5-LabelDP, the entire algorithm is also \u03b5-LabelDP. This is known as (an adaptive version of a) parallel composition [68]. Finally, we point out that the running time of our RRWithPrior algorithm is quasi-linear in K (the time needed to sort the prior). This is essentially optimal within multistage training, since O(K) time will be required to write down the prior after each stage. Moreover, for reasonable values of K, the running time will be dominated by back-propagation for gradient estimation. Moreover, the focus of the current work is on small to modest label spaces (i.e., values of K).\n# 5 Empirical Evaluation\nWe evaluate RRWithPrior on standard benchmark datasets that have been widely used in previous works on private machine learning. Specifically, in the first part, we study our general multi-stage training algorithm that boostraps from a uniform prior. We evaluate it on image classification and collaborative filtering tasks. In the second part, we focus on image classification only and use domainspecific techniques to obtain priors for RRWithPrior. We use modern neural network architectures (e.g., ResNets [51]) and the mixup [101] regularization for learning with noisy labels. Please see the Supplementary Material for full details on the datasets and the experimental setup.\n# 5.1 Evaluation with Multi-Stage Training\nCIFAR-10 [60] is a 10-class image classification benchmark dataset. We evaluate our algorithm and compare it to previously reported DP baselines in Table 1. Due to scalability issues, previous DP algorithms could only use simplified architectures with non-private accuracy significantly below the state-of-the-art. Moreover, even when compared to those weaker non-private baselines, a large performance drop is observed in the private models. In contrast, we use ResNet18 with 95% nonprivate accuracy. Overall, our algorithms improve the previous state-of-the-art by a margin of 20% across all \u03b5\u2019s. Abadi et al. [2] treated CIFAR-100 as public data and use it to pre-train a representation to boost the performance of DP-SGD. We also observe performance improvements with CIFAR-100 pre-training (Table 1, bottom 2 rows). But even without pre-training, our results are significantly better than DP-SGD even with pre-training.\n<div style=\"text-align: center;\">Table 3: Experiments on MovieLens-1M. The numbers show the test RM</div>\nAlgorithm\n\u03b5 = 1\n\u03b5 = 2\n\u03b5 = 3\n\u03b5 = 4\n\u03b5 = 8\n\u03b5 = \u221e\nLP-1ST\n1.122\n0.981\n0.902\n0.877\n0.867\n0.868\nLP-2ST\n1.034\n0.928\n0.891\n0.874\n0.865\nGaussian DP [15]\n0.915 (\u03b5 \u226510)\nIn Table 2 we also show results on CIFAR-100, which is a more challenging variant with 10\u00d7 more classes. To the best of our knowledge, these are the first non-trivial reported results on CIFAR-100 for DP learning. For \u03b5 = 8, our algorithm is only 2% below the non-private baseline. In addition, we also evaluate on MovieLens-1M [49], which contains 1 million anonymous ratings of approximately 3, 900 movies, made by 6,040 MovieLens users. Following [15], we randomly split the data into 80% train and 20% test, and show the test Root Mean Square Error (RMSE) in Table 3. Results on MNIST [61], Fashion MNIST [97], and KMNIST [25], and comparison to more baselines can be found in the Supplementary Material. In all the datasets we evaluated, our algorithms not only significantly outperform the previous methods, but also greatly shrink the performance gap between private and non-private models. The latter is critical for applications of deep learning systems in real-world tasks with privacy concerns. Beyond Two Stages. In Figure 1(a), we report results on LP-MST with T > 2. For the cases we tried, we consistently observe 1\u20132% improvements on test accuracy when going from LP-2ST to LP-3ST. In our preliminary experiments, going beyond T > 4 stages leads to diminishing returns on some datasets.\nBeyond Two Stages. In Figure 1(a), we report results on LP-MST with T > 2. For the cases we tried, we consistently observe 1\u20132% improvements on test accuracy when going from LP-2ST to LP-3ST. In our preliminary experiments, going beyond T > 4 stages leads to diminishing returns on some datasets.\n# 5.2 Evaluation with Domain-Specific Priors\nThe multi-stage training framework evaluated in the previous section is a general domain-agnostic algorithm that bootstraps itself from uniform priors. In some cases, domain-specific priors can be obtained to further improve the learning performance. In this section, we focus on image classification applications, where new advances in self-supervised learning (SSL) [22, 23, 44, 53, 16] show that high-quality image representations could be learned on large image datasets without using the class labels. In the setting of LabelDP, the unlabeled images are considered public data, so we design an algorithm to use SSL to obtain priors, which is then fed to RRWithPrior for discriminative learning. Specifically, we partition the training examples into groups by clustering using their representations extracted from SSL models. We then query a histogram of labels for each group via discrete Laplace mechanism (aka Geometric Mechanism) [41]. If the groups are largely homogeneous, consisting of mostly examples from the same class, then we can make the histogram queries with minimum privacy budget. The queried histograms are used as label priors for all the points in the group. Figure 1(b) shows the results on two different SSL representations: BYOL [44], trained on unlabeled CIFAR-10 images and DINO [16], trained on ImageNet [27] images. Comparing to the baseline, the SSL-based priors significantly improves the model performance with small privacy budgets. Note that since the SSL priors are not true priors, with large privacy budget (\u03b5 = 8), it actually underperforms the uniform prior. But in most real world applications, small \u03b5\u2019s are generally more useful.\n# 6 Theoretical Analysis\nPrevious works have shown that LabelDP can be provably easier than DP in certain settings; specifically, in the PAC learning setting, Beimel et al. [13] proved that finite VC dimension implies learnability by LabelDP algorithms, whereas it is known that this is not sufficient for DP algorithms [7]. We extend the theoretical understanding of this phenomenon to the stochastic convex optimization (SCO) setting. Specifically, we show that, by applying RR on the labels and running SGD on top of the resulting noisy dataset with an appropriate debiasing of the noise, one can arrive at the following dimension-independent excess population loss.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f371/f3718217-3389-4d0d-9132-ece58c261df0.png\" style=\"width: 50%;\"></div>\nFigure 1: (a) Test accuracy (%) on various datasets with LP-MST for T > 2. The curve \u201cCIFAR-10 w/ pre-train\u201d is using CIFAR-100 as public data to pre-train the model. (b) RRWithPrior with priors obtained from histogram query based on clustering in various SSL representations. We also plot recent results (ClusterRR) from Esfandiari et al. [37], which is a clustering based LabelDP algorithm. Theorem 5 (Informal). For any \u03b5 \u2208(0, 1), there is an \u03b5-LabelDP algorithm for stochastic convex optimization with excess population loss \u02dcO \ufffd DL \u00b7 K \u03b5\u221an \ufffd where D denotes the diameter of the parameter space and L denotes the Lipschitz constant of the loss function. The above excess population loss can be compared to that of Bassily et al. [12], who gave an (\u03b5, \u03b4)DP algorithm with excess population loss OD,L \ufffd 1 \u221an + \u221ap \u03b5n \ufffd , where p denote the dimension of the parameter space; this bound is also known to be tight in the standard DP setting. The main advantage of our guarantee in Theorem 5 is that it is independent of the dimension p. Furthermore, we show that our bound is tight up to polylogarithmic factors and the dependency on the number of classes K. The above result provides theoretical evidence that running RR on the labels and then training on this noisy dataset can be effective. We can further extend this to the setting where, instead of running RR, we run RRTop-k before running the aforementioned (debiased) SGD, although\u2014perhaps as expected\u2014our bound on the population loss now depends on the quality of the priors. Corollary 6 (Informal). Suppose that we are given a prior px for every x and let Y x k denote the set of top-k labels with respect to px. Then, for any \u03b5 \u2208(0, 1), there is an \u03b5-LabelDP algorithm for stochastic convex optimization with excess population loss \u02dcO \ufffd DL \u00b7 \ufffd k \u03b5\u221an + Pr(x,y)\u223cD[y /\u2208Y x k ] \ufffd\ufffd where D, L are as defined in Theorem 5 and D is the data distribution.\nWhen our top-k set is perfect (i.e., y always belongs to Y x k ), the bound reduces to that of Theorem 5, but with the smaller k instead of K. Moreover, the second term is, in some sense, a penalty we pay in the excess population loss for the inaccuracy of the top-k prior. We defer the formal treatment and the proofs to the Supplementary Material, in which we also present additional generalization results for non-convex settings. Note that Corollary 6 is not in the exact setup we run in experiments, where we dynamically calculate an optimal k for each x given generic priors (via RRWithPrior), and for which the utility is much more complicated to analyze mathematically. Nonetheless, the above corollary corroborates the intuition that a good prior helps with training.\n# 7 Conclusions and Future Directions\nIn this work, we introduced a novel algorithm RRWithPrior (which can be used to improve on the traditional RR mechanism), and applied it to LabelDP problems. We showed that prior information can be incorporated to the randomized label querying framework while maintaining privacy constraints We demonstrated two frameworks to apply RRWithPrior: (i) a general multi-stage training algorithm LP-MST that bootstraps from uniform priors and (ii) an algorithm that build priors from clustering with SSL-based representations. The former is general purpose and can be applied to tasks even when no\ndomain-specific priors are available, while the latter uses a domain-specific algorithm to extract priors and performs well even with very small privacy budget. As summarized by the figure on the right, in both cases, by focusing on LabelDP, our RRWithPrior significantly improved the model performance of previous state-of-the-art DP models that aimed to protect both the inputs and outputs. We note that, following up on our work, additional results on deep learning with LabelDP were obtained [66, 100]. The narrowed performance gap between private and non-private models is vital for adding DP to real world deep learning models. We nevertheless stress that our algorithms only protect the labels but not the input points, which might not constitute a sufficient privacy protection in all settings.\nOur work opens up several interesting questions. Firstly, note that our multi-stage training procedure uses very different ingredients than those of Abadi et al. [2] (which employ DP-SGD, privacy amplification by subsampling, and Renyi accounting); can these tools be used to further improve LabelDP? Secondly, while our procedure can be implemented in the most stringent local DP setting5 [58], can it be improved in the weaker central (aka trusted curator) DP model, assuming the curator knows the prior? Thirdly, while our algorithm achieves pure DP (i.e., \u03b4 = 0), is higher accuracy possible for approximate DP (i.e., \u03b4 > 0)?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5856/585685ee-4261-425b-9d88-570addaca55b.png\" style=\"width: 50%;\"></div>\n# Acknowledgements\nThe authors would like to thank Sami Torbey for very helpful feedback on an early version of this work. At MIT, Noah Golowich was supported by a Fannie and John Hertz Foundation Fellowship and an NSF Graduate Fellowship.\n# References\n# Supplementary Material for \u201cDeep Learning with Label Differential Privacy\u201d\n# A Missing Proofs\n# A.1 Proof of Lemma 1\nProof of Lemma 1. Consider any inputs y, y\u2032 \u2208 [K] and any possible output \u02dcy \u2208 Yk. Pr[RRTop-k(y) = \u02dcy] is maximized when y = \u02dcy, whereas Pr[RRTop-k(y\u2032) = \u02dcy] is minimized when y\u2032 \u2208Yk \\ {\u02dcy}. This implies that\nThus, RRTop-k is \u03b5-DP as desired.\n# B Details of the Experimental Setup\nDatasets. We evaluate our algorithms on the following image classification datasets:\n\u2022 MNIST [61], 10 class classification of hand written digits, based on inputs of 28 \u00d7 28 gray scale images. The training set contains 60,000 examples and the test set contains 10,000. \u2022 Fashion MNIST [97], 10 class classification of Zalando\u2019s article images. The dataset size and input format are the same as MNIST. \u2022 KMNIST [25], 10 class classification of Hiragana characters. The dataset size and the input format are the same as MNIST. \u2022 CIFAR-10/CIFAR-100 [60] are 10 class and 100 class image classification datasets, respectively. Both datasets contains 32 \u00d7 32 color images, and both have a training set of size 50,000 and a test set of size 10,000. \u2022 MovieLens [49] contains a set of movie ratings from the MovieLens users. It was collected and maintained by a research group (GroupLens) at the University of Minnesota. There are 5 versions: \u201c25m\u201d, \u201clatest-small\u201d, \u201c100k\u201d, \u201c1m\u201d, \u201c20m\u201d. Following Bu et al. [15], we use the \u201c1m\u201d version, which the largest MovieLens dataset that contains demographic data. Specifically, it contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users, with some meta data such as gender and zip code.\nArchitectures. On CIFAR-10/CIFAR-100, we use ResNet [51], which is a Residual Network architecture widely used in the computer vision community. In particular, we use ResNet18 V2 [52]. Note the standard ResNet18 is originally designed for ImageNet scale (image size 224 \u00d7 224). When adapting to CIFAR (image size 32 \u00d7 32), we replace the initial block with 7 \u00d7 7 convolution and 3 \u00d7 3 max pooling with a single 3 \u00d7 3 convolution (with stride 1) layer. The upper layers are kept the same as the standard ImageNet ResNet18. On MNIST, Fashion MNIST, and KMNIST, we use a simplified Inception [91] model suitable for small image sizes, and defined as follows: Inception :: Conv(3\u00d73, 96) \u2192S1 \u2192S2 \u2192S3 \u2192GlobalMaxPool \u2192Linear. S1 :: Block(32, 32) \u2192Block(32, 48) \u2192Conv(3\u00d73, 160, Stride=2). S2 :: Block(112, 48) \u2192Block(96, 64) \u2192Block(80, 80) \u2192Block (48, 96) \u2192 Conv(3\u00d73, 240, Stride=2). S3 :: Block(176, 160) \u2192Block(176, 160). Block(C1, C2) :: Concat(Conv(1\u00d71, C1), Conv(3\u00d73,C2)). Conv :: Convolution \u2192BatchNormalization \u2192ReLU.\n\nmulti-class classification. During evaluation, we output the average rating according to the softmax probabilities output by the trained model.\nprobabilities output by the trained model. Training Procedures. On MNIST, Fashion MNIST, and KMNIST, we train the models with minibatch SGD with batch size 265 and momentum 0.9. We run the training for 40 epochs (for multi-stage training, each stage will run 40 epochs separately), and schedule the learning rate to linearly grow from 0 to 0.02 in the first 15% training iterations, and then linearly decay to 0 in the remaining iterations. On CIFAR-10, we use batch size 512 and momentum 0.9, and train for 200 epochs. The learning rate is scheduled according to the widely used piecewise constant with linear rampup scheme. Specifically, it grows from 0 to 0.4 in the first 15% training iterations, then it remains piecewise constant with a decay factor of 10 at the 30%, 60%, and 90% training iterations, respectively. The CIFAR-100 setup is similar to CIFAR-10 except that we use a batch size 256 and a peak learning rate 0.2. MovieLens experiments are trained similarly, but with batch size 128. On all datasets, we optimize the cross entropy loss with an \u21132 regularization (coefficient 10\u22124). All the networks are randomly initialized at the beginning of the training. For the experiment on CIFAR-10 where we explicitly study the effect of pre-training to compare with previous methods that use the same technique, we train a (non-private) ResNet18 on the full CIFAR-100 training set and initialize the CIFAR-10 model with the pre-trained weights. The classifier is still randomly initialized because there is no clear correspondence between the 100 classes of CIFAR-100 and the 10 classes of CIFAR-10. The remaining configuration remains the same as in the experiments without pre-training. In particular, we did not freeze the pre-trained weights. We apply standard data augmentations, including random crop, random left-right flip, and random cutout [28], to all the datasets during training. We implement our algorithms in TensorFlow [1], and train all the models on NVidia Tesla P100 GPUs. Learning with Noisy Labels. Standard training procedures tend to overfit to the label noise and generalize poorly on the test set when some of the training labels are randomly flipped. We apply mixup [101] regularization, which generates random convex combinations of both the inputs and the (one-hot encoded) labels during training. It is shown that mixup is resistant to random label noise. Note that our framework is generic and in principle any robust training technique could be used. We have chosen mixup for its simplicity, but there has been a rich body of recent work on deep learning methods with label noise, see, e.g., [55, 46, 99, 21, 104, 74, 69, 64, 105, 56, 50, 47, 65, 86, 79, 87] and the references therein. Potentially with more advanced robust training, even higher performance could be achieved. Multi-Stage Training. There are a few implementation enhancements that we find useful for multi-stage training. For concreteness, we discuss them for LP-2ST. First, we find it helps to initialize the stage-2 training with the models trained in stage-1. This is permitted as the stage-1 model is trained on labels that are queried privately. Moreover, we can reuse those labels queried in stage-1 and train stage-2 on a combined dataset. Although the subset of data from stage-1 is noisier, we find that it generally helps to have more data, especially when we reduce the noise of stage-1 data by using the learned prior model. Specifically, for each sample (x, \u02dcy) in the stage-1 data, where \u02dcy is the private label queried in stage-1, we make a prediction on x using the model trained in stage-1; if \u02dcy is not in the top k predicted classes, we will exclude it from the stage-2 training. Here k is simply set to the average k obtained when running RRWithPrior to query labels on the data held out for stage-2. Similar ideas apply to training with more stages. For example, in LP-3ST, stage-3 training could use the model trained in stage-2 as initialization, and use it to filter the queried labels in stage-1 and stage-2 that are outside the top k prediction, and then train on the combined data of all 3 stages. Priors from Self-supervised Learning. Recent advances in self-supervised learning (SSL) [22, 23, 44, 53, 16] show that representations learned from a large collection of unlabeled but diverse images could capture useful semantic information and can be finetuned with labels to achieve classification performance on par with the state-of-the-art fully supervised learned models. We apply SSL algorithms to extract priors for image classification problems, with the procedure described in Algorithm 4. Specifically, we choose two recent SSL algorithms: BYOL [44] and DINO [16]. For BYOL, we train the SSL model using the (unlabeled) CIFAR-10 images only, as a demonstration without using\nMulti-Stage Training. There are a few implementation enhancements that we find useful for multi-stage training. For concreteness, we discuss them for LP-2ST. First, we find it helps to initialize the stage-2 training with the models trained in stage-1. This is permitted as the stage-1 model is trained on labels that are queried privately. Moreover, we can reuse those labels queried in stage-1 and train stage-2 on a combined dataset. Although the subset of data from stage-1 is noisier, we find that it generally helps to have more data, especially when we reduce the noise of stage-1 data by using the learned prior model. Specifically, for each sample (x, \u02dcy) in the stage-1 data, where \u02dcy is the private label queried in stage-1, we make a prediction on x using the model trained in stage-1; if \u02dcy is not in the top k predicted classes, we will exclude it from the stage-2 training. Here k is simply set to the average k obtained when running RRWithPrior to query labels on the data held out for stage-2. Similar ideas apply to training with more stages. For example, in LP-3ST, stage-3 training could use the model trained in stage-2 as initialization, and use it to filter the queried labels in stage-1 and stage-2 that are outside the top k prediction, and then train on the combined data of all 3 stages. Priors from Self-supervised Learning. Recent advances in self-supervised learning (SSL) [22, 23, 44, 53, 16] show that representations learned from a large collection of unlabeled but diverse images could capture useful semantic information and can be finetuned with labels to achieve classification performance on par with the state-of-the-art fully supervised learned models. We apply SSL algorithms to extract priors for image classification problems, with the procedure described in Algorithm 4. Specifically, we choose two recent SSL algorithms: BYOL [44] and DINO [16]. For BYOL, we train the SSL model using the (unlabeled) CIFAR-10 images only, as a demonstration without using\nAlgorithm 4\nInput: Training set D = {(xi, yi)}n\ni=1, cluster count C, privacy budget for priors \u03b5p, trained SSL model fSSL\n1. Initialize P \u21901/K ones(n, K) as the uniform priors.\n2. Extract SSL features F = {fSSL(xi) : (xi, yi) \u2208D}.\n3. Run k-means algorithms to partition F into C groups.\n4. For each c = 1 to C:\n(a) Compute histogram of classes Hc \u2208NK\n\u22650 according to the labels of examples in the c-th group.\n(b) Get a private histogram query \u02dcHc \u2190Hc+ scipy.stats.dlaplace.rvs(\u03b5p/2, K), via the\ndiscrete Laplace mechanism.\n(c) Get a prior via normalization: pc = max( \u02dcHc, 0)/ \ufffdK\nk=1 max( \u02dcHc[k], 0).\n(d) For each example i in group c, assign P[i, :] \u2190pc.\n5. Output P.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/249d/249d9af0-7c08-444d-b135-568684185223.png\" style=\"width: 50%;\"></div>\nFigure 2: Accuracy evaluated on CIFAR-10 test set, of private histogram querying with kmeans clustering on self-supervised learning based features learned by (a) BYOL [44] on CIFAR-10 and (b) DINO [16] on ImageNet.\nexternal data. For DINO, we use the models pre-trained on (unlabeled) ImageNet [27] images. Since ImageNet is a much larger and more diverse dataset than CIFAR-10, the SSL representations are also more capable of capturing the semantic information. Note the ImageNet images are of higher resolution and resized to 224\u00d7224 during training. To extract features for 32\u00d732 CIFAR-10 images, we simply upscale the images to 224 \u00d7 224 before feeding into the trained neural network. We choose relatively large cluster sizes so that the private histogram query is more robust to the added discrete Laplace noise. In particular, we found C = 100 clusters for BYOL representations and C = 50 clusters for DINO representations achieve a good balance of robustness and accuracy. Since \u03b5p will be subtracted from the privacy budget for RRWithPrior, we simply choose the smallest \u03b5p without causing too much deterioration of the priors. In our experiments, we set \u03b5p = 0.05 for BYOL and \u03b5p = 0.025 for DINO. Note the model accuracy could potentially be further boosted by choosing C and \u03b5p adaptively according to the overall privacy budget. In the following, we provide a simple study to show how the interplay between \u03b5p and C affects the accuracy of the histogram queries. To compute an accuracy measure on the test set, we extract features using a SSL learned models on both training and test set. A k-means clustering algorithm is run on the joint set of training and test features. For each cluster, we apply the discrete Laplace mechanism to make a private histogram of class distributions from only the training examples in that cluster. The class with the maximum votes are then used as predicted labels for all the test examples in the cluster, and compared with the true test labels to calculate the accuracy. Figure 2 shows the accuracy with the two different SSL features under different privacy budgets (\u03b5) for making the histogram queries. As expected, the accuracy is higher with smaller clusters, but at the same time sensitive to noise introduced by the Geometric Mechanism when the privacy budget is small.\n<div style=\"text-align: center;\">Table 4: Test accuracy (%) on MNIST and Fashion MNIST. The baseline performances taken from previously published results correspond to (\u03b5, \u03b4)-DP with \u03b4 = 10\u22125.</div>\nAlgorithm\n\u03b5 = 1\n\u03b5 = 2\n\u03b5 = 3\n\u03b5 = 4\n\u03b5 = 8\n\u03b5 = \u221e\nMNIST\nDP-SGD [2]\n95\n97\n98.3\nPATE-G [75]\n98(\u03b5=2.04)\n98.1( \u03b5=8.03)\n99.2\nConfident-GNMax [76]\n98.5(\u03b5=1.97)\n99.2\nTempered Sigmoid [77]\n98.1(\u03b5=2.93)\nBu et al. [15]\n96.6(\u03b5=2.32)\n97.0(\u03b5 =5.07)\nChen and Lee [20]\n90.0(\u03b5 =2.5)\nNasr et al. [71]\n96.1(\u03b5 =3.2)\nYu et al. [98]\n93.2(\u03b5 =6.78)\nFeldman and Zrnic [39]\n96.56(\u03b5=1.2)\n97.71\nLP-1ST\n95.34\n98.16\n98.81\n99.08\n99.33\nLP-2ST\n95.82\n98.78\n99.14\n99.24\nFashion\nMNIST\nDP-SGD [77]\n81.9(\u03b5=2.7)\n89.4\nTempered Sigmoid [77]\n86.1(\u03b5=2.7)\nChen and Lee [20]\n82.3\nLP-1ST\n80.78\n90.18\n92.52\n93.50\n94.28\nLP-2ST\n83.26\n91.24\n93.18\n94.10\nAlgorithm\n\u03b5=1\n\u03b5=2\n\u03b5=3\n\u03b5=4\n\u03b5=\u221e\nLP-1ST\n76.56\n92.04\n95.86\n96.86\n98.33\nLP-2ST\n81.26\n93.72\n97.19\n97.83\n-\n# C Extra Results on Multi-Stage Training\nIn addition to the results presented in the main text, we include extra results of multi-stage training on MNIST [61], Fashion MNIST [97], and KMNIST [25]. Both MNIST and Fashion MNIST have been previously used to benchmark DP deep learning algorithms. We compare our algorithms with previously reported numbers in Table 4. Our algorithms outperform previous methods across all \u03b5\u2019s on both datasets. The gap is more pronounced on Fashion MNIST, which is slightly harder than MNIST. Furthermore, LP-2ST consistently improves over LP-1ST. Table 5 shows the model performances on KMNIST under different privacy losses. The results are qualitatively similar to the ones for MNIST and Fashion MNIST.\n# D Learning Dynamics of Multi-stage Training\nFig. 3 visualizes the learning curves of LP-1ST and LP-2ST on CIFAR-10 with \u03b5 = 2. Stage-1 of LP-2ST (using 65% training data) clearly underperforms LP-1ST with the full training set. But it is good enough to provide useful prior for stage-2. The RRWithPrior algorithm responds with an average k = 1.86 over the remaining 35% of the training set. As the dotted line shows, the top-2 accuracy of the model trained in stage-1 reaches 90% at the end of training, indicating that the true label on the test set is within the top-2 prediction with high probability. In stage-2, we continue with the model trained in stage-1, and train on the combined data of the two stages. This is possible because the labels queried in stage-1 are already private. As a result, LP-2ST achieves higher performance than LP-1ST.\n# E Analysis of Robustness to Hyperparameters\nFollowing previous work, [e.g., 77], we report the benchmark performance after hyperparameter tuning. In practice, to build a rigorous DP learning system, the hyperparameter tuning should be performed using private combinatorial optimization [45]. Since that is not the main focus of this paper, we skip this step for simplicity. Meanwhile, we do the following analysis of model performance\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f1f/2f1f14c9-25d4-4bb0-b3b2-71b3625ad445.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">25 50 75 100 125 150 175 200 training epoch Figure 3: The learning curves of LP-1ST vs LP-2ST on CIFAR-10 (\u03b5 = 2)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b33/2b33a643-6392-469b-8107-642a285970ed.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The final performance of LP-2ST on CIFAR-10 (\u03b5 = 2) (a) under different stage-1 / stage-2 data split and prior temperature; (b) under different mixup coefficients for stage-1 and stage-2.</div>\nunder variations of different hyperparameters, which shows that the algorithms are robust in a large range of hyperparameters, and also provides some intuition for choosing the right hyperparameters. Data Splits and Prior Temperature. The data split parameter decides the ratio of data in different stages of training. Allocating more data for stage-1 allows us to learn a better prior model for the LP-2ST algorithm. However, it will also decrease the number of training samples in stage-2, which reduces the utility of the learned prior model. In practice, ratios slightly higher than 50% for stage-1 strike the right balance for LP-2ST. We use a temperature parameter t to modify the learned prior. Specifically, let fk(x) be the logits prediction of the learned prior model for class k on input x. The temperature modifies the prior \u02c6pk(x) as:\n\ufffd As t \u21920, it sparsifies the prior by forcing it to be more confident on the top classes, and as t \u2192\u221e, the prior converges to a uniform distribution. In our experiments, we find it useful to sparsify the prior, and temperatures greater than 1 are generally not helpful. Fig. 4(a) shows the performance for different combinations of data split ratio and temperature. Accuracy of Stage-1. Ideally, one would want the k calculated in RRWithPrior to satisfy the condition that the ground-truth label is always in the top-k prior predictions. Because otherwise, the randomized response is guaranteed to be a wrong label. One way to achieve such a goal is to make\nAccuracy of Stage-1. Ideally, one would want the k calculated in RRWithPrior to satisfy the condition that the ground-truth label is always in the top-k prior predictions. Because otherwise, the randomized response is guaranteed to be a wrong label. One way to achieve such a goal is to make\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/61ca/61cac2a5-60c9-4c59-b5b7-11b51f741170.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><= 0.8 (0.8,0.84] (0.84,0.88] (0.88,0.9] (0.9,0.92] (0.92,0.94] (0.94,0.96] > 0.96 range of top-k test accuracy</div>\nFigure 5: The relation between top-k accuracy of stage-1 and the final accuracy of LP-2ST (CIFAR-10, \u03b5 = 2). The x-axis is the range of top-k accuracy of stage-1 models evaluated on the test set. For each range, the violin plot shows the distribution of the final test accuracy of LP-2ST where the RRWithPrior procedure calculated an average k (rounded to the nearest integer) for which the top-k accuracy of the stage-1 model falls in the given range.\nthe stage-1 model have high top-k accuracy. For example, we could allocate more data to improve the performance of stage-1 training, or tune the temperature to spread the prior to effectively increase the k calculated by RRWithPrior. In either case, a trade-off needs to be made. In Fig. 5, we visualize the relation between top-k test accuracy of stage-1 training and the final performance of LP-2ST. For each value range in the x-axis, we show the distribution of the final test accuracy where the average k (rounded to the nearest integer) calculated in RRWithPrior would make the top-k accuracy of the corresponding stage-1 training fall into this value range. The plot shows that the final performance drops when the top-k accuracy is too low or too high. In particular, achieving near perfect top-k accuracy in stage-1 is not desirable. Note this plot measures the top-k accuracy on the test set, so while it is useful to observe the existence of a trade-off, it does not provide a procedure to choose the corresponding hyperparameters. Mixup Regularization. Mixup [101] has a hyperparameter \u03b1 that controls the strength of regularization (larger \u03b1 corresponds to stronger regularization). We found that \u03b1 values between 4 and 8 are generally good in our experiments, and as shown in Fig. 4(b), stage-2 typically requires less regularization than stage-1. Intuitively, this is because the data in stage-2 is less noisier than stage-1.\nMixup Regularization. Mixup [101] has a hyperparameter \u03b1 that controls the strength of regularization (larger \u03b1 corresponds to stronger regularization). We found that \u03b1 values between 4 and 8 are generally good in our experiments, and as shown in Fig. 4(b), stage-2 typically requires less regularization than stage-1. Intuitively, this is because the data in stage-2 is less noisier than stage-1.\n# F Convex SCO with LabelDP\nIn this section, we give the proofs of the Theorem 5 and Corollary 6 for private stochastic convex optimization (SCO) and additionally prove some further, related results. We first formally introduce the setting of SCO. Suppose we are given some feature space X (e.g., the space of all images), and label space [K] = {1, 2, . . . , K}. Write Z = X \u00d7 [K]. Let W \u2282Rp be a convex parameter space. Let D be the (Euclidean) diameter of W, namely D := maxw,w\u2032\u2208W \u2225w \u2212w\u2032\u2225. Suppose we are given a loss function \u2113: W \u00d7 Z \u2192R, which specifies the loss \u2113(w, z) for a given parameter vector w \u2208W on the example z = (x, y). Given a sequence of samples (x1, y1), . . . , (xn, yn) drawn i.i.d. from a distribution P over Z, the goal is to find w minimizing the popoulation risk, namely L(w, P) := E(x,y)\u223cP [\u2113(w, (x, y))]. Write w\u22c6:= arg minw\u2208W L(w, P). In this section, we make the following assumptions on \u2113: Assumption 7 (Convexity). For each z \u2208Z, the function w \ufffd\u2192\u2113(w, z) is convex. Assumption 8 (Lipschitzness). For each z \u2208Z, the function w \ufffd\u2192\u2113(w, z) is L-Lipschitz (with respect to the Euclidean norm). Under Assumptions 7 and 8, Bassily et al. [12, Theorem 4.4] showed that there is an (\u03b5, \u03b4)-DP algorithm that given n i.i.d. samples from a distribution P and has access to a gradient oracle for \u2113,\nAs shown by Bassily et al. [12] (building off of previous work by Bassily et al. [10]), the rate (6) is tight up to logarithmic factors: in particular, there is a lower bound of \u2126 \ufffd\u221ap n\u03b5 \ufffd on the excess risk for any (\u03b5, \u03b4)-DP algorithm, meaning that dimension dependence is necessary for private SCO. Subsequent work [40] showed how to obtain the rate (6) in linear (in n) time. We additionally remark that there has much work (e.g., [19, 59, 10, 103, 95]) on the related problem of DP empirical risk minimization, for which rates similar to (6), except without the 1/\u221an term, are attainable.\n# F.1 Label-Private SGD\nIn this section we prove Theorem 5, showing that dimension-independent rates are possible in the setting of label DP privacy (in contrast to the standard setting of DP where privacy of the features must also be maintained). The algorithm that obtains the guarantee of Theorem 5 is LP-RR-SGD (Algorithm 5). Both LP-RR-SGD and the training procedure of Section 5 (which uses RRWithPrior) update the weight vectors using gradient vectors \u02c6gt, which are obtained by using randomized response on the labels yt for the training examples (xt, yt). LP-RR-SGD, however, ensures that \u02c6gt is an unbiased estimate of the true gradient, which facilitates the theoretical analysis, whereas this is not guaranteed the training procedure of Section 5.\nfor all \u02c6y \u2208[K]. (c) Let gt = \u2207w\u2113(wt, (xt, \u02dcyt)) and\n(d) Let wt+1 \u2190\u03a0W(wt \u2212\u03b7t \u00b7 \u02c6gt). 3. Output \u02c6w := wn+1.\nWe now restate Theorem 5 formally below: Theorem 9 (Formal version of Theorem 5). For any \u03b5 \u2208(0, 1), the algorithm LP-RR-SGD satisfies the requirement of \u03b5-LabelDP; moreover, if run with step size \u03b7t = D\u03b5 6KL \u221a t, its output \u02c6w satisfies\nTheorem 9 (Formal version of Theorem 5). For any \u03b5 \u2208(0, 1), the algorithm LP-RR-SGD satisfies the requirement of \u03b5-LabelDP; moreover, if run with step size \u03b7t = D\u03b5 6KL \u221a t, its output \u02c6w satisfies\nWe remark that even in the non-private setting, a lower bound of \u2126(DL/\u221an) is known on the excess risk for stochastic convex optimization [73, 6], meaning that Theorem 9 is tight up to a factor of O(K log n/\u03b5). In Section F.3, we improve the lower bound to \u02dc\u2126(DL/\u221a\u03b5n) for small \u03b5 \u22641 (where \u02dc\u2126hides a logarithmic factor in 1/\u03b5). Hence, our bound above is tight to within a factor of \u02dcO(K log n/\u221a\u03b5).\nof these points, respectively, then it is immediate from definition of Qt that for any subset S \u2282Rp, Pr[\u02c6gt\u2208S] Pr[\u02c6g\u2032 \u2208S] \u2264e\u03b5. That LP-RR-SGD is \u03b5-LabelDP follows immediately from the post-processing property\nof these points, respectively, then it is immediate from definition of Qt that for any subset S \u2282Rp, Pr[\u02c6gt\u2208S] Pr[\u02c6g\u2032 t\u2208S] \u2264e\u03b5. That LP-RR-SGD is \u03b5-LabelDP follows immediately from the post-processing property of DP. Next we establish the uility guarantee. Note that by definition of \u02c6gt, we have that\ni.e., \u02c6gt is an unbiased estimate of \u2207w\u2113(wt, (xt, yt)). Next, we bound the variance of the gradient error \u02c6gt \u2212\u2207w\u2113(wt, (xt, yt)), as follows:\nwhere we have used that \u2113is L-Lipschitz, \u03b5 \u22641, and that K \u22652. Using Shamir and Zhang [81, Theorem 2] with gradient moment G2 := 36K2L2 \u03b52 , we get that for step size choices \u03b7t := D G \u221a t, the output \u02c6w of LP-RR-SGD satisfies\nNow we prove Corollary 6; a formal version of the corollary is stated below. Corollary 10 (Formal version of Corollary 6). Suppose that we are given a prior px for every x and let Y x k denote the set of top-k labels with respect to px. Then, for any \u03b5 \u2208(0, 1), there is an \u03b5-LabelDP algorithm which outputs \u02c6w \u2208W satisfying\nProof. Suppose we are given access to samples (x, y) drawn from a distribution P on X \u00d7 [K]. For a pair (x, y) \u2208X \u00d7 [K], define a random pair \u03be((x, y)) \u2208X \u00d7 [K], by setting \u03be((x, y)) = (x, y) if y \u2208Y x k , and otherwise letting \u03be((x, y)) to be drawn uniformly over the set {(x, k\u2032) : k\u2032 \u2208Y x k }. Let P \u2032 be the distribution of \u03be((x, y)), where (x, y) \u223cP. For any w1, w2 \u2208W, it follows that\n|L \u2212L \u2212L \u2212L| = \ufffd\ufffd\ufffd\ufffd \ufffd Z [\u2113(w1, (x, y)) \u2212\u2113(w2, (x, y))]dP((x, y)) \u2212 \ufffd Z [\u2113(w1, (x, y)) \u2212\u2113(w2, (x, y))]dP \u2032((x, y)) \ufffd\ufffd\ufffd\ufffd \u2264 \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd {(x,y):y\u0338\u2208Y x k } ([\u2113(w1, (x, y)) \u2212\u2113(w2, (x, y))] \u2212[\u2113(w1, \u03be((x, y))) \u2212\u2113(w2, \u03be((x, y)))]) dP((x, y)) \ufffd\ufffd\ufffd\ufffd\ufffd \u22642DL \u00b7 Pr [y \u0338\u2208Y x  ], (9)\nwhere the last step uses that |\u2113(w1, (x, y))\u2212\u2113(w2, (x, y))| \u2264L\u2225w1\u2212w2\u2225\u2264LD for all w1, w2 \u2208W. Now we simply run the algorithm LP-RR-SGD, except that when we receive a point (x, y) \u223cP, we pass the example \u03be((x, y)) to LP-RR-SGD (instead of (x, y)), and we let the set of possible labels be Y x k (instead of [K]). Since each such example \u03be((x, y)) is only passed to LP-RR-SGD once, the resulting allgorithm is still \u03b5-LabelDP. Since the label of \u03be((x, y)) belongs to Y x k , which has size k for all x, Theorem 9 gives that the output \u02c6w of LP-RR-SGD satisfies E[L( \u02c6w, P \u2032)] \u2212minw L(w, P \u2032) \u2264\n\n(8)\nwhere (10) follows since L(w\u22c6 P \u2032, P \u2032) \u2264L(w\u22c6 P , P \u2032) by definition of w\u22c6 P \u2032. (8) is an immediate consequence.\n# F.2 A Better Bound for Approximate DP\nNext we introduce an algorithm, LP-Normal-SGD (Algorithm 6), which shows how to improve upon the excess risk bound of Theorem 9 by a factor of \u221a K, if we relax the privacy requirement to approximate LabelDP (i.e., (\u03b5, \u03b4)-LabelDP with \u03b4 > 0). LP-SGD performs a single pass of SGD over the input dataset, with the following modification: it adds a Gaussian noise vector to each gradient vector with nonzero variance only in the K-dimensional subspace Lt corresponding to the K possible labels for each point xt. This means that the norm of a typical noise vector scales only as \u221a K as opposed to the scaling \u221ap, which similar algorithms for the standard setting of DP (e.g., [10]) obtain.\nAlgorithm 6 LP-Normal-SGD\nInput: Distribution P over X \u00d7 [K], convex and L-Lipschitz loss function \u2113, privacy parameters \u03b5, \u03b4, convex\nparameter space W, variance factor \u03c3 > 0, step size sequence \u03b7t > 0.\n1. Choose an initial weight vector w1 \u2208W.\n2. For t = 1 to n:\n(a) Receive (xt, yt) \u223cP.\n(b) Let \u02dcbt \u223cN(0, \u03c32Ip).\n(c) Let Lt \u2190span{\u2207w\u2113(wt, (xt, k)) : k \u2208[K]} \u2282Rp.\n(d) Let bt \u2190\u03a0Lt(\u02dcbt) denote the Euclidean projection of \u02dcbt onto Lt.\n(e) Let wt+1 \u2190\u03a0W(wt \u2212\u03b7t \u00b7 (\u2207w\u2113(wt, (xt, yt)) + bt)).\n3. Output \u02c6w := wn+1.\nProposition 11. There is a constant C > 0 so that the following holds. For any \u03b5, \u03b4 \u2208(0, 1), \u03c3 = CL\u221a log 1/\u03b4 \u03b5 , \u03b7t = D \u221a (L2+K\u03c32)\u00b7t, the algorithm LP-SGD (Algorithm 6) is (\u03b5, \u03b4)-LabelDP and satisfies the following excess risk bound:\nProof of Proposition 11. We first argue that the privacy guarantee holds. Note that for any k, k\u2032 \u2208[n], for any x \u2208X, w \u2208W, we have \u2225\u2207w\u2113(w, (x, k)) \u2212\u2207w\u2113(w, (x, k\u2032))\u2225\u22642L. Therefore, for any , the mechanism\nis (\u03b5, \u03b4)-DP as long as \u03c3 \u2265 CL\u221a log 1/\u03b4 \u03b5 , for some constant C > 0 [32]. Since each (xt, yt) is used in only a single iteration of LP-Normal-SGD, it follows from the post-processing of DP that LP-Normal-SGDis (\u03b5, \u03b4)-LabelDP for this choice of \u03c3. Next we establish the utility guarantee. Since, for each t \u2208[n], Lt is a subspace of Rp of at most K dimensions, it holds that for each t, E[\u2225bt\u22252] \u2264K\u03c32. Thus E \ufffd \u2225\u2207w\u2113(wt, (xit, yit)) + bt\u22252\ufffd \u2264 L2 + K\u03c32. Using Shamir and Zhang [81, Theorem 2] with gradient moment G2 := L2 + K\u03c32, we get that for step size choices \u03b7t := D G \u221a t, it holds that\n(10)\n\nIn this section, we prove the following lower bound on excess risk, which is tight with respect to (11) in Proposition 11 up to a factor of \u02dcO( \ufffd K/\u03b5). Proposition 12. For any \u03b5 \u2208(0, 1], D, L > 0 and any sufficiently large n \u2208N and sufficiently small \u03b4 > 0 (both depending on \u03b5), the following holds: for any (\u03b5, \u03b4)-LabelDP algorithm A, there exists a loss function \u2113that is L-Lipschitz and convex, and a distribution P for which \ufffd \ufffd\nE \u02dcS\u223cP \u2297n, \u02c6 w\u223cA( \u02dcS)[L( \u02c6w, P)] \u2212L(w\u22c6, P) \u2265\u02dc\u2126 \ufffdDL \u221a\u03b5n \ufffd .\nWe remark that the lower bound of \u2126(DL/\u221an) is well known for non-private SCO. This lower bound applies to our setting as well and thus the lower bound in Proposition 12 can be viewed as an improvement of a factor for \u02dc\u2126(1/\u221a\u03b5) over the non-private lower bound. We prove Equation (11) by first proving an analogous bound in the empirical loss minimization (ERM) setting and then deriving SCO via a known reduction.\n# F.4 Lower Bound on Excess Risk for ERM\nRecall that in ERM setting, we are given a set S = {(x1, y1), . . . , (xn, yn)} \u2286Z of n labelled examples. The empirical risk of w is defined as L(w, S) := 1 n \ufffdn i=1 \u2113(w, (x, y)). Here we would like to devise an algorithm that minimizes the excess empirical risk, i.e., E[L( \u02c6w, S)] \u2212L(w\u22c6, S) where \u02c6w is the output of the algorithm and w\u22c6:= arg minw\u2208W L(w, S). We start by proving the following lower bound on excess risk for LabelDP ERM algorithms. Note that the lower bound does not yet grow as \u03b5 decreases; that version of the lower bound will be proved later in this section. Proposition 13. For any \u03b5, D, L, \u03b4 > 0, K \u22652 and n \u2208N such that \u03b5 \u2264O(1), \u03b4 \u22641 \u2212\u2126(1), the following holds: for any (\u03b5, \u03b4)-LabelDP algorithm A, there exists a loss function \u2113that is L-Lipschitz and convex, and a dataset \u02dcS of size n for which \ufffd \ufffd\nRecall that in ERM setting, we are given a set S = {(x1, y1), . . . , (xn, yn)} \u2286Z of n labelled examples. The empirical risk of w is defined as L(w, S) := 1 n \ufffdn i=1 \u2113(w, (x, y)). Here we would like to devise an algorithm that minimizes the excess empirical risk, i.e., E[L( \u02c6w, S)] \u2212L(w\u22c6, S) where \u02c6w is the output of the algorithm and w\u22c6:= arg minw\u2208W L(w, S).\n\u2208W L We start by proving the following lower bound on excess risk for LabelDP ERM algorithms. Note that the lower bound does not yet grow as \u03b5 decreases; that version of the lower bound will be proved later in this section. Proposition 13. For any \u03b5, D, L, \u03b4 > 0, K \u22652 and n \u2208N such that \u03b5 \u2264O(1), \u03b4 \u22641 \u2212\u2126(1), the following holds: for any (\u03b5, \u03b4)-LabelDP algorithm A, there exists a loss function \u2113that is L-Lipschitz and convex, and a dataset \u02dcS of size n for which \ufffd \ufffd\nProof. Let W := {w \u2208Rd : \u2225w\u2225\u2264D/2} and X := {x \u2208Rd : \u2225x\u2225\u22641}. We define the loss to be \uf8f1\n\uf8f3 Note that the diameter of W is D and \u2113(\u00b7, (x, y)) is convex and L-Lipschitz. Consider any (\u03b5, \u03b4)LabelDP algorithm A. Let ei \u2208Rn be the ith standard basis vector. Consider a dataset S = {(e1, y1), . . . , (en, yn)} where y1, . . . , yn \u2208{1, 2} are random labels which are 1 w.p. 0.5 and 2 otherwise. For notational convenience, we write \u02dcyi to denote 2yi \u22123 \u2208{\u22121, 1}. By the (\u03b5, \u03b4)LabelDP guarantee of A, we have\nPr S, \u02c6 w\u223cA(S)[\u02dcyi \u00b7 \u27e8\u02c6w, ei\u27e9> 0] = 1 2 Pr S, \u02c6 w\u223cA(S)[\u27e8\u02c6w, ei\u27e9< 0 | \u02dcyi = \u22121] + 1 2 Pr S, \u02c6 w\u223cA(S)[\u27e8\u02c6w, ei\u27e9> 0 | \u02dcyi = 1] \u22641 2 \u00b7 \ufffd e\u03b5 \u00b7 Pr S, \u02c6 w\u223cA(S)[\u27e8\u02c6w, ei\u27e9< 0 | \u02dcyi = 1] + \u03b4 \ufffd + 1 2 \ufffd e\u03b5 \u00b7 Pr S, \u02c6 w\u223cA(S)[\u27e8\u02c6w, ei\u27e9> 0 | \u02dcyi = \u22121] + \u03b4 \ufffd = e\u03b5 \u00b7 Pr S, \u02c6 w\u223cA(S)[\u02dcyi \u00b7 \u27e8\u02c6w, ei\u27e9< 0] + \u03b4.\n(11)\n(12)\nLetting I \u02c6 w,S := {i \u2208[n] : \u02dcyi \u00b7 \u27e8\u02c6w, ei\u27e9> 0} for any S, \u02c6w,\nConsider any S as generated above; it is obvious to see that w\u22c6= D 2 \u00b7 \ufffd 1 \u221an \ufffd i\u2208[n] \u02dcyiei \ufffd , which results in L(w\u22c6, S) = \u2212DL 2\u221an. On the other hand, for any \u02c6w,\nwhere the second inequality follows from Cauchy\u2013Schwarz inequality and the last inequality follows from our assumption that \u03b4 \u22641 \u2212\u2126(1) and \u03b5 \u2264O(1).\nTo make the lower bound above grows with 1/\u221a\u03b5 for \u03b5 \u22641, we will apply the technique used in [89]. Recall that a pair of datasets are said to be k-neighbor if they differ in at most k labels. The following is a well-known bound, so-called group privacy; see e.g. Steinke and Ullman [89, Fact 2.3]. (Typically this fact is stated for the standard DP but it applies to LabelDP in the same manner.) Fact 14. Let A be any (\u03b5, \u03b4)-LabelDP algorithm. Then, for any k-neighboring database S, S\u2032 and every subset T of the output, we have Pr[A(S) \u2286T] \u2264ek\u03b5 \u00b7 Pr[A(S\u2032) \u2286T] + ek\u03b5\u22121 e\u03b5\u22121 \u00b7 \u03b4. We can now prove the following lower bound that grows with 1/\u221a\u03b5 by simplying replicating each element 1/\u03b5 times. Lemma 15. For any \u03b5\u2032 \u2208(0, 1], D, L, \u03b4\u2032 > 0, K \u22652 and n \u2208N such that n \u22651/\u03b3, \u03b4\u2032 \u2264\u2126(\u03b5\u2032), the following holds: for any (\u03b5\u2032, \u03b4\u2032)-LabelDP algorithm A\u2032, there exists a loss function \u2113that is L-Lipschitz and convex, and a dataset \u02dcS\u2032 of size n for which \ufffdDL \ufffd\n(13)\n(14)\nProof. Suppose for the sake of contradiction there exists (\u03b5, \u03b4)-LabelDP algorithm A\u2032 such that E \u02c6 w\u223cA\u2032( \u02dcS\u2032)[L( \u02c6w, \u02dcS\u2032)] \u2212L(w\u22c6, \u02dcS\u2032) \u2264o \ufffd DL \u221a \u03b5\u2032n \ufffd . Let k = \u230a1/\u03b5\u230b. We construct an algorithm A as follows: on input \u02dcS, it replicates each element of \u02dcS k times to construct a dataset \u02dcS\u2032. It then returns A\u2032( \u02dcS\u2032). From the utility guarantee of A\u2032, we have E \u02c6 w\u223cA( \u02dcS)[L( \u02c6w, \u02dcS)] \u2212L(w\u22c6, \u02dcS) \u2264o \ufffd DL \u221an \ufffd . Furthermore, Fact 14 ensures that A is (\u03b5, \u03b4)-DP for \u03b5 = k\u03b5\u2032 \u22641 and \u03b4 = ek\u03b5\u2032\u22121 e\u03b5\u2032\u22121 \u03b4\u2032 \u2264O(\u03b4\u2032/\u03b5\u2032). When \u03b4\u2032 = C/\u03b5\u2032 for any sufficiently small C > 0, A violates Proposition 13, concluding our proof.\n# F.5 From ERM to SCO\nBassily et al. [12]6 gave a reduction from private SCO to private ERM. Although this bound is proved in the context of standard (both label and sample) DP, it is not hard to see that a similar bound holds for LabelDP with exactly the same proof. To summarize, their proof yields the following bound: Lemma 16. For any \u03b3, \u03b5 > 0 and \u03b4 \u2208(0, 1/2), suppose that there is an ( \u03b5 4 log(2/\u03b4), e\u2212\u03b5\u03b4 8 log(2/\u03b4))LabelDP algorithm that yields expected excess population risk of for SCO is at most \u03b3. Then, there exists an (\u03b5, \u03b4)-LabelDP algorithm for convex ERM (with the same parameters D, L, n) with excess empirical risk at most \u03b3. Plugging this into Lemma 15, we arrive at Proposition 12.\n# G Generalization Bounds for RR with Prior\nLet X, Z be similar to the previous section and Y = [K] be the class of labels. We consider a setting where there is a concept class F of functions f : X \u2192R. Given n samples drawn i.i.d. from some distribution P on Z, we would like to output a function f with a small population risk, which is defined as L(f; P) = E(x,y)\u223cP [\u2113(f(x), (x, y))]., where \u2113: R \u00d7 Z \u2192[0, 1] is a loss function. Throughout this section, we assume that \u2113is L-Lipschitz (Assumption 8). Priors and Randomized Response. Let k \u2264K be a positive integer. We work in the same setting as Corollary 10, i.e. we assume a prior px for every x and let Y x k denote the set of top-k labels with respect to px. We let \u02dcP be the distribution where we first draw (x, y) \u223cP and then output (x, \u02dcy) where \u02dcy \u223cRRTop-kpx(y) with DP parameter \u03b5. Debiased Loss Function. Let pk,\u03b5 denote 1 e\u03b5+k\u22121. We consider a debiased version of the loss \u2113; this was done before in [72] for the case of binary classification with noisy labels. In our setting, it generalizes to the following definition: \ufffd \ufffd\nFor a set S of n labeled examples (x1, y1), . . . , (xn, yn) \u2208Z, its empirical risk (w.r.t loss \u02dc\u2113) as \u02dcL(f; S) = 1 n \ufffdn i=1 \u02dc\u2113(f(xi), (xi, yi)). We consider simple \u03b5-LabelDP algorithm that randomly draws n i.i.d. samples S from P, apply (\u03b5-LabelDP) RRTop-k on each of the label to get a randomized dataset \u02dcS, and finally apply empirical risk minimization w.r.t. the debiased loss function \u02dc\u2113on \u02dcS. We remark that this algorithm is exactly the same as drawing n samples i.i.d. from \u02dcP and apply empirical risk minimization (again w.r.t. \u02dc\u2113). Our main result of this section is a generalization bound roughly saying that the empirical risk (w.r.t. \u02dc\u2113) is small iff the popultion risk (w.r.t. \u2113) is small. This is stated more formally below, where Rn,D(F) denote the Rademacher Complexity of F (defined below in Definition G.2). Theorem 17. Let PX be the marginal of P over X. Let \u02dcS be a set of n i.i.d. labeled samples drawn from \u02dcP. Then, with probability at least 1 \u2212\u03b2, the following holds for all f \u2208F: \ufffd\n(17)\n(18)\nVia standard techniques (see e.g. [72]), the above bound imply that the empirical risk minimizer incurs excess loss similar to the bound in Equation (18) (within a factor of 2). Recall that RRTop-k can of course be thought of RRWithPrior in the case when e.g. the prior px is uniform over the k labels in Y x k . Thus, Theorem 17 can be viewed as a generalization bound for RRWithPrior with these \u201cuniform top-k\u201d priors.\n# G.1 Additional Preliminaries\nTo prove Theorem 17, we need several additional observations and definitions. In addition to the previously defined L(f; P), \u02dcL(f; S), we analogously use \u02dcL(f; P), L(f; S) to denote the population risk w.r.t. \u02dc\u2113on distribution P and the empirical risk w.r.t. \u2113on the labeled sample set S respectively.\nProperties of the Debiased Loss Function. We will start by proving a few basic properties of th debiased loss functions. The first two lemmas are simple to check: Lemma 18. If y \u2208Y x k , it holds that E\u02dcy\u223cRRTop-kpx(y)[\u02dc\u2113(t, (x, \u02dcy))] = \u2113(t, (x, y)). Lemma 19. \u02dc\u2113is L \u00b7 1+k\u00b7pk,\u03b5 1\u2212k\u00b7pk,\u03b5 -Lipschitz (in t for every fixed x, y).\nFinally, we observe that the population risk w.r.t. \u02dc\u2113on distribution \u02dcP is close to that w.r.t. \u2113on P: Lemma 20. For any function f, we have |L(f; P) \u2212\u02dcL(f, \u02dcP)| \u2264 Pr (x,y)\u223cP[y /\u2208Y x k ]. (19\n|L(f; P) \u2212\u02dcL(f; \u02dcP)| = |E(x,y)\u223cP [\u2113(f, (x, y))] \u2212E(x,y)\u223cP,\u02dcy\u223cRRTop-kpx(y)[\u2113(f, (x, \u02dcy))]| \u2264E(x,y)\u223cP [|\u2113(f, (x, y)) \u2212E\u02dcy\u223cRRTop-kpx(y)[\u2113(f, (x, \u02dcy))]|].\nDue to Lemma 18, the inner term is zero whenever y \u2208Y x k ; furthermore, since the range of \u2113is in [0, 1], the last term is at most Pr(x,y)\u223cP [y /\u2208Y x k ] as desired.\nRademacher Complexity. Given a space V and a distribution D over V, we let S be a set of examples v1, . . . , vn drawn i.i.d. from D. We also let F be a class of functions f : V \u2192R. Definition G.1 (Empirical Rademacher Complexity). The empirical Rademacher complexity of F is defined as:\n\ufffd where \u03c31, . . . , \u03c3",
    "paper_type": "method",
    "attri": {
        "background": "The widespread adoption of machine learning has raised concerns about the privacy of individuals whose data is used during model training. Differential privacy (DP) has become a popular privacy notion, leading to the development of various algorithms. However, existing methods like Randomized Response (RR) incur significant errors, necessitating improvements to enhance accuracy while maintaining privacy.",
        "problem": {
            "definition": "The paper addresses the challenge of achieving high accuracy in machine learning models while ensuring label differential privacy (LabelDP), where only the labels are considered sensitive.",
            "key obstacle": "Current methods for achieving differential privacy, such as DP-SGD, result in significantly lower accuracy compared to non-private models, creating a barrier for real-world applications."
        },
        "idea": {
            "intuition": "The proposed method leverages prior information about labels to improve the accuracy of the learning process while still adhering to privacy constraints.",
            "opinion": "The authors introduce a novel algorithm, Randomized Response with Prior (RRWithPrior), which utilizes prior distributions to enhance the performance of models trained under LabelDP.",
            "innovation": "The key innovation lies in the integration of prior knowledge into the randomized response framework, allowing for more accurate label predictions compared to existing methods."
        },
        "method": {
            "method name": "Randomized Response with Prior",
            "method abbreviation": "RRWithPrior",
            "method definition": "RRWithPrior is a randomized algorithm that outputs labels with a higher probability of correctness by utilizing prior distributions about the labels.",
            "method description": "The method dynamically prunes the label set based on prior probabilities to maximize the likelihood of returning the correct label.",
            "method steps": [
                "1. Compute the set of k labels with the highest prior probabilities.",
                "2. Use standard randomized response on this set to generate a label.",
                "3. If the true label is not in the top-k set, output a label from this set uniformly at random."
            ],
            "principle": "The effectiveness of RRWithPrior stems from its ability to leverage prior knowledge, which reduces the noise introduced during the label randomization process, thereby improving accuracy."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on benchmark datasets including CIFAR-10 and CIFAR-100, comparing the performance of RRWithPrior against existing methods like DP-SGD.",
            "evaluation method": "Performance was measured using accuracy metrics, with comparisons drawn against previously published results to demonstrate improvements."
        },
        "conclusion": "The proposed RRWithPrior algorithm significantly enhances the accuracy of models trained under label differential privacy, bridging the performance gap between private and non-private models, and providing a viable solution for real-world applications.",
        "discussion": {
            "advantage": "RRWithPrior offers a substantial improvement in accuracy over traditional methods while maintaining the same level of privacy guarantees.",
            "limitation": "The method primarily focuses on label privacy and does not provide sufficient privacy protection for input data, which may be a limitation in certain applications.",
            "future work": "Future research could explore the integration of RRWithPrior in central DP settings and investigate the potential for higher accuracy under approximate DP conditions."
        },
        "other info": {
            "acknowledgements": "The authors thank Sami Torbey for feedback on the work. Noah Golowich was supported by a Fannie and John Hertz Foundation Fellowship and an NSF Graduate Fellowship."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the challenge of achieving high accuracy in machine learning models while ensuring label differential privacy (LabelDP), where only the labels are considered sensitive."
        },
        {
            "section number": "1.2",
            "key information": "The widespread adoption of machine learning has raised concerns about the privacy of individuals whose data is used during model training."
        },
        {
            "section number": "1.3",
            "key information": "Outline the main objectives of the survey paper, focusing on enhancing accuracy while maintaining privacy in machine learning models."
        },
        {
            "section number": "2.1",
            "key information": "Differential privacy (DP) has become a popular privacy notion, leading to the development of various algorithms."
        },
        {
            "section number": "2.2",
            "key information": "Current methods for achieving differential privacy, such as DP-SGD, result in significantly lower accuracy compared to non-private models, creating a barrier for real-world applications."
        },
        {
            "section number": "4.1",
            "key information": "The proposed method, Randomized Response with Prior (RRWithPrior), utilizes prior distributions to enhance the performance of models trained under LabelDP."
        },
        {
            "section number": "4.3",
            "key information": "RRWithPrior offers a substantial improvement in accuracy over traditional methods while maintaining the same level of privacy guarantees."
        },
        {
            "section number": "6.1",
            "key information": "The effectiveness of RRWithPrior stems from its ability to leverage prior knowledge, which reduces the noise introduced during the label randomization process, thereby improving accuracy."
        },
        {
            "section number": "8",
            "key information": "The proposed RRWithPrior algorithm significantly enhances the accuracy of models trained under label differential privacy, bridging the performance gap between private and non-private models."
        }
    ],
    "similarity_score": 0.5584410451007064,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Deep Learning with Label Differential Privacy.json"
}