{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.10189",
    "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
    "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL.",
    "bib_name": "ling2024uncertaintyquantificationincontextlearning",
    "md_text": "# Uncertainty Quantification for In-Context Learning of Large Language Models\nChen Ling1, Xujiang Zhao2, Xuchao Zhang3, Wei Cheng2, Yanchi Liu2, Yiyou Sun2, Mika Oishi4, Takao Osaki4, Katsushi Matsuda4, Jie Ji1, Guangji Bai1, Liang Zhao1, Haifeng Chen3\n1Emory University, 2NEC Labs America, 3Microsoft, 4NEC Corporation chen.ling@emory.edu, xuzhao@nec-labs.com, liang.zhao@emory.edu\n# Abstract\nIn-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL.\n 28 Mar 2024\n[cs.CL]\narXiv:2402.10189v2 \n# 1 Introduction\nLarge Language Models (LLMs) have revolutionized diverse domains by serving as general task solvers, which can be largely attributed to the emerging capability: in-context learning. By providing demonstrations of the task to LLMs as part of the prompt, LLMs can quickly grasp the intention and make corresponding responses to the particular task (Min et al., 2022). In this paradigm, LLMs can quickly adapt to solve new tasks at inference time (without any changes to their weights). Advanced LLMs, e.g., GPT-4 and LLaMA, have achieved state-of-the-art results on LAMBADA (commonsense sentence completion), TriviaQA\n(question answering) (Xie et al., 2021), and many tasks in other domains (Ling et al., 2023b,a). While in-context learning has achieved notable success, LLMs remain vulnerable to well-known reliability issues like hallucination (Rawte et al., 2023; Bai et al., 2024). Uncertainty quantification has emerged as a popular strategy to assess the reliability of LLM responses. In the past two years, numerous works (Xiao et al., 2022; Lin et al., 2023; Ling et al., 2023c; Amayuelas et al., 2023; Kuhn et al., 2023) have been proposed to quantify the uncertainty of LLM response. These approaches could return a confidence score or directly compute variance/entropy across multiple LLM responses; however, they often overlook the complex nature of LLMs and their reliance on provided demonstrations in in-context learning, so that existing methods may not provide insights into the underlying causes or the interactions among different factors contributing to uncertainty. A natural question therefore arises: when LLM uses in-context learning to predict a wrong answer with high uncertainty, can we indicate if it is caused by the demonstration examples or by the model itself? Given LLM\u2019s responses to a particular task, it\u2019s essential to decompose the uncertainty into its primary sources to address the question. Specifically, Aleatoric Uncertainty (AU) refers to variations in the data, often linked to the demonstration examples. As shown in Figure 1 (a), LLM\u2019s output can easily be disturbed by inappropriate demonstrations since the provided demonstrations do not cover all possible labels. The noise and potential ambiguity of these demonstrations could bring uncertainty, which, in turn, may hinder the accuracy of the response. In contrast, Epistemic Uncertainty (EU) stems from ambiguities related to the model parameters or different configurations. As depicted in Figure 1 (b), different decoding strategies (e.g., beam search and greedy decoding) and their hyperparameter settings can have different decoding re-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8689/86893fa7-1c9a-49e1-9c57-bfa75e6efaef.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d16f/d16f797f-52ef-4cfe-915b-d76118c07916.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Inappropriate or insufficient few-shot  demonstrations may cause uncertainty</div>\n<div style=\"text-align: center;\">Figure 1: Uncertainty in LLM\u2019s prediction can stem from two aspects: a) Demonstration Quality: LLMs are likely to make wrong predictions if the demonstrations are inappropriate; b) Model Configuration: different decoding strategies (e.g., beam_search and top_k sampling) and their parameter settings may return different predictions.</div>\nsults. Recognizing and quantifying the uncertainty from the model\u2019s perspective can also be critical in evaluating the generated responses, which allows us to understand the model\u2019s confidence level toward the task and make necessary adjustments (e.g., choosing a more powerful model or conducting an ensemble prediction). Despite the strides made by existing works in understanding the total uncertainty, the decomposition of uncertainty in the realm of in-context learning remains under-explored. In this work, we propose a novel framework for quantifying the uncertainty of in-context learning to aleatoric and epistemic components from the generated outputs. Our contributions are summarized as follows.\n\u2022 Problem. We formulate the problem of uncertainty decomposition that extracts epistemic and aleatoric uncertainties from the predictive distribution of LLMs with in-context learning.\n\u2022 Method. We propose quantifying both aleatoric and epistemic uncertainty from the mutual information perspective. A novel entropy-based estimation method is also designed to handle the free-form outputs of LLMs.\n\u2022 Experiment. Extensive experiments are conducted to evaluate different aspects of uncertainty, followed by specific applications and case studies to show how two types of uncertainty influence the model\u2019s performance.\n# 2 Uncertainty Decomposition of In-context Learning\nWe first formulate the process of in-context learning as Bayesian Neural Networks with latent variables. Based on the formulation, we propose to\n<div style=\"text-align: center;\">(b) Various decoding strategies and parameter  settings may cause uncertainty</div>\ndecompose the predictive uncertainty into its epistemic and aleatoric components from the mutual information perspective, followed by a novel way to estimate both uncertainties based on the entropy of the prediction\u2019s distribution.\n# 2.1 Background\nLLMs are typically trained using maximum likelihood estimation on a large corpus of text. The training goal is to maximize the likelihood of the observed data under the model: L(\u0398) = \ufffd i\u2264N p(\u03c9i|\u03c91, \u03c92, . . . , \u03c9i\u22121; \u0398), where each \u03c9i \u2208x is a token in a sentence x = [\u03c91, . . . , \u03c9N], and \u0398 denotes the set of parameters. Latent Concept. From the Bayesian point of view, LLM\u2019s in-context learning ability is obtained by mapping the training token sequence x to a latent concept z (Xie et al., 2021). The concept z is a latent variable sampled from a space of concepts Z, which defines a distribution over observed tokens \u03c9i from a training context x:\nThe concept can be interpreted as various document-level statistics, such as the general subject matter of the text, the structure/complexity of the text, the overall emotional tone of the text, etc. In-context Learning. Given a list of independent and identically distributed (i.i.d.) in-context demonstrations (contain both question and answer) [x1, . . . , xT\u22121] concatenated with a test question (without the task answer) xT as prompt. Each demonstration xi in the prompt is drawn as a sequence conditioned on the same concept z and describes the task to be learned. LLMs generate a\nThe concept can be interpreted as various document-level statistics, such as the general subject matter of the text, the structure/complexity of the text, the overall emotional tone of the text, etc.\n# response yT to the test question xT based on the aggregated prompt x1:T :\np(yT |x1:T ) = \ufffd z\u2208Z p(yT |x1:T , z)p(z|x1:T )dz.\nIn-context learning can be interpreted as locating a pre-existing concept z based on the provided demonstrations x1:T\u22121, which is then employed to tackle a new task xT . Including more high-quality demonstrations within the prompt can help refine the focus on the relevant concept, enabling its selection through the marginalization term p(z|x1:T ). Note that formulating in-context learning as Bayesian inference with latent variables is more of a hypothesis; however, demystifying the in-context learning from the view of Bayesian inference offers a probabilistic interpretation of how LLM learns and adapts to new data in context. In this work, we focus on quantifying the predictive uncertainty of LLMs in deterministic NLP tasks, such as text classification. Specifically, we address tasks where the training dataset D = {X, Y} consists of token sequences X = {x} and their corresponding target outputs Y = {y}. For LLMs, the generation process is defined by the function y = f(x, z; \u0398), where f : X \u00d7 Z \u2192Y is a deterministic function. The output y exhibits stochastic behavior, influenced by the latent concept z \u223cp(z|x1:T ) and the model parameters/configurations \u0398 (e.g., temperature, etc.).\n# 2.2 Predictive Uncertainty Formulation of In-context Learning\nWe formulate the predictive distribution of incontext learning for predicting yT given few-shot demonstrations x1:T\u22121 and a test case xT as:\n(1)\nwhere p(yT |\u0398, x1:T , z) is approximated by a Bayesian Neural Network-based likelihood function N(f(x1:T , z), \u03a3), and \u03a3 is the covariance matrix contains the variances and covariances associated with LLM parameters. q(\u0398) is the approximated posterior of the LLM\u2019s parameters \u0398. Eq. (1) approximates LLM outputs following a Gaussian distribution, which serves as an initial framework for generating predictions based on input data and accompanying demonstrations: p(yT |x1:T ), which entangles different types of uncertainties.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d20b/d20b0c5e-d876-484b-ba37-3a9f34e045d4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Uncertainty Quantification of In-context Learning Pipeline: we want to quantify the uncertainty that comes from 1) different in-context demonstrations x1:T ; and 2) different model configurations \u0398l.</div>\nWe first present the overall pipeline of our uncertainty quantification framework, followed by formulation on decomposing the total uncertainty based on mutual information (Sec. 2.3) and a novel way to estimate the uncertainty (Sec. 2.4). Note that LLMs can be categorized into white-box and black-box models (Ling et al., 2023b) based on their transparency. Quantifying mutual information involves accessing the probability of generated tokens, which is not applicable to black-box LLMs. In this study, we also provide a decomposition way from the variance perspective for black-box LLMs. Due to the space limit, the variance-based decomposition can be found in Appendix A.1.\nFramework Pipeline. In this work, we employ a Bayesian framework to quantify the predictive uncertainty from LLMs, and the overall pipeline is visualized in Figure 2. Specifically, the input x1:T is composed of a test query xT and a set of demonstrations x1:T\u22121 sampled from X. By sampling different model parameters/configurations \u0398l \u223cq(\u0398), LLM can return different outputs yl T \u2208[y1 T , \u00b7 \u00b7 \u00b7 , yL T ] based on the conditional probability p(yT |\u0398l, x1:T , z). The collection of outputs [y1 T , \u00b7 \u00b7 \u00b7 , yL T ] records the total uncertainty regarding \u0398l and demonstrations x1:T\u22121.\n# 2.3 Entropy-based Decomposition\nAs a widely adopted measure of uncertainty, entropy provides a quantifiable and interpretable metric to assess the degree of confidence in the model\u2019s predictions (Malinin and Gales, 2020). Since whitebox LLMs can return the probability of each token in the generated sequence, it naturally makes entropy-based uncertainty measures applicable uniformly across different types of white-box LLMs. Epistemic Uncertainty (EU). Let H(\u00b7) be the differential entropy of a probability distribution, the total uncertainty in Eq. (1) can be quantified as H (yT |x1:T ), which entangles both aleatoric (i.e.,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/91ec/91ec8dfc-7e66-46a8-a934-90036b131453.png\" style=\"width: 50%;\"></div>\nFigure 3: Framework of entropy-based uncertainty estimation, which consists of 1) generating M sequences based on a set of x1:T \u22121; 2) selecting token(s) that is relevant to the answer and extract the probabilities; 3) aggregating the token probabilities of M sequences into a distribution of predicted labels; 4) iterating the process L times corresponding to L different demonstration sets and form a probability matrix M, where the column denotes different demonstration sets and the row denotes labels of the dataset.\n<div style=\"text-align: center;\">Figure 3: Framework of entropy-based uncertainty estimation, which consists of 1) generating M sequences based on a set of x1:T \u22121; 2) selecting token(s) that is relevant to the answer and extract the probabilities; 3) aggregating the token probabilities of M sequences into a distribution of predicted labels; 4) iterating the process L times corresponding to L different demonstration sets and form a probability matrix M, where the column denotes different demonstration sets and the row denotes labels of the dataset.</div>\ndemonstration x1:T\u22121) and epistemic (i.e., model parameter \u0398) uncertainties. To estimate the EU, we condition Eq. (1) on a specific realization of the model parameter \u0398, yielding p(yT |x1:T , \u0398) = \ufffd p(yT |x1:T , z, \u0398)p(z|x1:T )dz with an associated entropy H(yT |x1:T , z, \u0398). The expected value of this entropy under different demonstration sets can be expressed as Ez [H(yT |x1:T , z, \u0398)], which serves as a metric to quantify the EU in Eq. (1). Aleatoric Uncertainty (AU). In terms of AU, the randomness comes from different sets of demonstration x1:T\u22121 and their corresponding latent concept z. To estimate AU, we can quantify the mutual information between yT and latent concept z, which can often be leveraged as an evaluation metric of AU (Wimmer et al., 2023). As we have already obtained the EU, AU can subsequently be calculated as the discrepancy between the total uncertainty and the epistemic uncertainty:\n(2)\n  The entropy H (yT |x1:T , \u0398) can be approximately calculated as \u2212\ufffd t \ufffd p(\u03c9yT t ) \u00b7 log p \ufffd \u03c9yT t \ufffd\ufffd , where p(\u03c9yT t ) represents the probability of each possible next token \u03c9yT t given the input prompt x1:T . Therefore, the AU in Eq. (2) can be approximated by sampling many z (by sampling different sets of demonstrations) to obtain different yT conditioning on one set of parameters \u0398. The group of\nyT can then be used to approximate the respective entropies for each group of demonstrations x1:T\u22121:\nwhere [y\u0398m,l T ] are obtained corresponding to different demonstrations [x1 1:T\u22121, . . . , xL 1:T\u22121], and [\u03981, . . . , \u0398M] are sampled from q(\u0398). However, in many cases, direct sampling from the posterior is hard since it requires a prohibitive number of samples to approximate it effectively. Beam search is then used as an efficient alternative to find highquality hypotheses. This approach can be viewed as a form of importance sampling, where hypotheses are drawn from high-probability regions of the space. Each hypothesis yT observed during the beam search process is associated with uncertainty, which is importance-weighted in proportion to p(yT |x1:T , z). Beam Search thus serves as a practical and efficient way to sample from the posterior by focusing on the most relevant parts of the hypothesis space. In addition, since calculating the entropy H (yT ) entails to obtain the joint probability of the generated tokens p(yT ) = (\u03c9yT 1 , . . . , \u03c9yT T ), entropy-based method may only be applicable to white-box LLMs.\n# 2.4 Entropy Approximation\nThe generation of LLMs is generally free-form, which makes the uncertainty estimation for incontext learning still different from well-studied classification models that have specific labels. Specifically, not only may the LLM not always be able to return an expected answer, but the generated sequence may also consist of placeholder tokens. Calculating the entropy of the whole generated sequence would involve redundant tokens. Therefore, in this work, we propose to approximate the entropy of the output H(yT ), and the process is summarized in Figure 3. Given the probability distributions of the generated tokens p(yT ) for one set of demonstrations, we only select token(s) \u03c9yT t that directly answer the provided question. Taking the text classification task as an example, LLM is asked to directly output a numerical value standing for a predefined category (e.g., 0: Sadness, 1: Joy, etc.). The probability of the token \u03c9yT t that represents the numerical value is then leveraged to denote the overall distribution of p(yT ). We aggregate the answer probabilities from all M decoded sequences and transform them as an answer distribution (as shown in the top right corner in Figure 3). After repeating the process L times, where L corresponds to L different sets of demonstrations, we have a matrix M recording the answer distributions of choosing different demonstrations and model configurations (as shown in the lower right corner in Figure 3). The EU and AU can then be approximated as:\n\ufffd \ufffd\ufffd \ufffd\ufffd \ufffd where \u03c3(\u00b7) normalizes the column M:,j into a probability distribution, and entropy H(\u00b7) can be calculated as \u2212\ufffdK k=1 (p(Mk,j) \u2217log(p(Mk,j))) if the number of labels is K. Note that we have instructed LLMs to not generate tokens with less semantic meaning, such as dashes, spaces, or non-related words. In practice, our adopted LLMs can follow the instruction to only return desired answers so that the whole sentence will be the answer tokens (no need to select tokens).\n# 3 Related Works\nUncertainty Quantification and Decomposition. Uncertainty quantification aims to measure the confidence of models\u2019 predictions, which has drawn\nattention from various domains (Zhao et al., 2020; Ling et al., 2022; Malo et al., 2014). Measuring uncertainty is essential in many real-world NLP applications where making a wrong prediction with high confidence can be disastrous (e.g., assessing the confidence in a translation or a generated piece of information). This is especially important in foundation models since we do not have enough resources to finetune the model (Abdar et al., 2021). To better understand the uncertainty, the primary focus is on understanding and categorizing the sources of uncertainty for interpreting the models\u2019 outputs more effectively. The output uncertainty can typically be categorized into Aleatoric Uncertainty that arises from the inherent noise in the data, and Epistemic Uncertainty that arises due to inappropriate model architecture or overfitted/underfitted parameters. Existing methods (Chowdhary and Dupuis, 2013; Depeweg et al., 2017; Malinin and Gales, 2020) have come up with various methods (e.g., Bayesian neural network, Deep Ensembles, and Monte Carlo Dropout) to decompose the uncertainty.\n# Uncertainty in Language Models. LLMs hav\n# Uncertainty in Language Model\nrevolutionized the learning and inference paradigm in many domains (Chen et al., 2024, 2021), but existing works using LLMs often neglect the importance of uncertainty in their responses. Earlier works (Xiao and Wang, 2019; Desai and Durrett, 2020; Jiang et al., 2021) on uncertainty in language models have focused on the calibration of classifiers (e.g., applying dropout to the model parameters or leveraging ensemble voting) to better assess the confidence of the generated output. When it comes to the era of LLMs, multiple works (Xiao and Wang, 2021; Xiao et al., 2022; Lin et al., 2022; Yu et al., 2022; Lin et al., 2023; Kuhn et al., 2023; Fadeeva et al., 2023) have been proposed to measure the uncertainty of LLM\u2019s prediction in multiple aspects (e.g., lexical uncertainty, text uncertainty, and semantic uncertainty) for multiple NLP tasks. Another line of works (Kadavath et al., 2022; Zhou et al., 2023; Amayuelas et al., 2023; Chen et al., 2024) instead tries to analyze how to extract knowledge from a language model correctly and self-evaluate the correctness with a confidence score. However, despite these commendable efforts, existing methods still lack an effective way to directly quantify and decompose the uncertainty inherent in the outputs of LLMs with in-context learning.\n# 4 Experiments\nWe evaluate the uncertainty decomposition procedure on realistic natural language understanding problems. By comparing state-of-the-art uncertainty quantification methods, we aim to examine what type of uncertainty is the most promising indicator of high confidence for LLMs. In addition, we also provide generalization analysis and two specific out-of-distribution detection applications. Due to the space limit, extra experiments and experiment settings are provided in the Appendix.\n# 4.1 Experiment Setup\nWe evaluate the decomposed uncertainties on opensource LLMs with different model sizes. We leverage LLAMA-2 (Touvron et al., 2023), which is the most widely applied open LLM, with its 7B, 13B, and 70B model sizes. The primary experiments are conducted with LLAMA-2 models. In order to further demonstrate the generalization ability of our method, we apply our uncertainty quantification method on OPT-13B (Zhang et al., 2022). Data. We consider different Natural Language Understanding tasks. 1) Sentiment Analysis: EMOTION (Saravia et al., 2018) contains 2, 000 test cases and six classes; Financial Phrasebank (Financial) (Malo et al., 2014) contains 850 financial news and three sentiment classes; Stanford Sentiment Treebank v2 (SST2) (Socher et al., 2013) consists of 872 sentences from movie reviews and two classes. 2) Linguistic Acceptability: The Corpus of Linguistic Acceptability (COLA) (Warstadt et al., 2019) is about English acceptability judgments, which has 1, 040 test cases and two classes. 3) Topic Classification: AG_News (Zhang et al., 2015) contains 1, 160 test cases and four classes. Demonstration & Model Configuration Sampling. We evaluate each method on the testing set of each dataset and choose two strategies to randomly sample in-context learning demonstrations. 1) Random: we randomly sample demonstrations (training instances with labels) from the training set regardless their labels. 2) Class: we randomly sample demonstrations but ensure there is at least one demonstration per label class. To generate various sequences based on one set of demonstrations, we adopt Beam Search with beam width = 10 to approximate the sampling process of \u0398 \u223cq(\u0398). Comparison Methods. Our study also evaluates the following baseline uncertainty estimation methods: 1) Likelihood-based Uncertainty (Likelihood)\n(Malinin and Gales, 2020) calculates the sum of log probabilities of all tokens generated from language models and normalizes it by the sequence length. 2) Entropy-based Uncertainty (Entropy) (Xiao and Wang, 2019) calculates the entropy of the probability distributions of the generated tokens. 3) Semantic Uncertainty (Semantic) (Kuhn et al., 2023) is the most advanced entropy-based uncertainty estimation method, which groups generated sequences into clusters according to their semantic embeddings. The average entropy across all groups is viewed as the uncertainty score. Evaluation Metrics. We show the prediction accuracy of each dataset. In addition, we leverage two standard metrics: the Area under Precision-Recall Curve (AUPR) and AUROC (ROC) to evaluate the uncertainty. AUPR calculates the area under the Precision-Recall curve. AP is high when both precision and recall are high, and low when either of them is low across a range of confidence thresholds. ROC represents the likelihood that a correct answer is selected. An ideal ROC rating is 1, whereas a random uncertainty estimate would yield ROC = 0.5.\n# 4.2 Quantitative Analysis\nWe compare the performance of different methods in assessing the misclassification samples based on their perspective uncertainty scores. We follow the procedure: 1) We use LLMs to classify all examples in the dataset with different beam search branches and demonstrations; 2) we use different uncertainty quantification methods to obtain a score associated with each test instance; 3) we assign each example a 0 if it was classified correctly or a 1 if it was misclassified; and 4) we calculate AUPR and AUROC based on the misclassification rate and uncertainty score. Ideally, misclassified samples should have higher uncertainty scores. The results are shown in Table 1. Note that our proposed method can decompose the uncertainty into epistemic uncertainty (EU) and aleatoric uncertainty (AU), we thus show the performance of EU and AU separately. As shown in the table, in most cases, our proposed methods (EU and AU) consistently show higher AUPR and ROC scores across all datasets, which indicates a better performance in assessing misclassification samples based on uncertainty scores. Moreover, we also draw some observations from the table. 1. Class Sampling Strategy Proves Superior: The class sampling strategy\nInference\nModel\nACC\nLikelihood\nEntropy\nSemantic\nOurs (EU)\nOurs (AU)\nAUPR\nROC\nAUPR\nROC\nAUPR\nROC\nAUPR\nROC\nAUPR\nROC\nEmotion\nLLAMA-7B-RANDOM\n0.407\n0.423\n0.426\n0.448\n0.501\n0.598\n0.607\n0.688\n0.667\n0.625\n0.579\nLLAMA-7B-CLASS\n0.411\n0.562\n0.423\n0.657\n0.538\n0.697\n0.653\n0.745\n0.696\n0.691\n0.601\nLLAMA-13B-RANDOM\n0.501\n0.597\n0.613\n0.584\n0.503\n0.612\n0.625\n0.645\n0.681\n0.559\n0.585\nLLAMA-13B-CLASS\n0.533\n0.641\n0.578\n0.593\n0.554\n0.652\n0.701\n0.622\n0.686\n0.526\n0.599\nLLAMA-70B-RANDOM\n0.584\n0.512\n0.462\n0.491\n0.452\n0.657\n0.696\n0.667\n0.713\n0.531\n0.663\nLLAMA-70B-CLASS\n0.592\n0.537\n0.484\n0.469\n0.442\n0.622\n0.689\n0.659\n0.721\n0.612\n0.693\nFinancial\nLLAMA-7B-RANDOM\n0.379\n0.821\n0.532\n0.728\n0.438\n0.715\n0.624\n0.731\n0.672\n0.669\n0.582\nLLAMA-7B-CLASS\n0.397\n0.593\n0.505\n0.548\n0.362\n0.732\n0.699\n0.803\n0.711\n0.753\n0.589\nLLAMA-13B-RANDOM\n0.476\n0.894\n0.571\n0.652\n0.463\n0.705\n0.545\n0.718\n0.512\n0.729\n0.573\nLLAMA-13B-CLASS\n0.477\n0.752\n0.594\n0.692\n0.531\n0.694\n0.543\n0.765\n0.610\n0.758\n0.592\nLLAMA-70B-RANDOM\n0.530\n0.816\n0.509\n0.754\n0.493\n0.679\n0.688\n0.779\n0.754\n0.734\n0.642\nLLAMA-70B-CLASS\n0.537\n0.668\n0.469\n0.623\n0.439\n0.774\n0.649\n0.893\n0.804\n0.739\n0.659\nSST-2\nLLAMA-7B-RANDOM\n0.856\n0.149\n0.636\n0.135\n0.587\n0.244\n0.593\n0.286\n0.683\n0.205\n0.702\nLLAMA-7B-CLASS\n0.897\n0.230\n0.666\n0.196\n0.579\n0.253\n0.577\n0.248\n0.701\n0.302\n0.673\nLLAMA-13B-RANDOM\n0.866\n0.268\n0.472\n0.204\n0.467\n0.355\n0.712\n0.314\n0.677\n0.326\n0.816\nLLAMA-13B-CLASS\n0.928\n0.178\n0.425\n0.113\n0.439\n0.343\n0.631\n0.397\n0.836\n0.367\n0.639\nLLAMA-70B-RANDOM\n0.932\n0.091\n0.597\n0.137\n0.475\n0.258\n0.565\n0.318\n0.764\n0.298\n0.571\nLLAMA-70B-CLASS\n0.938\n0.132\n0.552\n0.185\n0.531\n0.312\n0.679\n0.331\n0.851\n0.362\n0.697\nCOLA\nLLAMA-7B-RANDOM\n0.599\n0.388\n0.557\n0.329\n0.443\n0.358\n0.502\n0.416\n0.562\n0.377\n0.517\nLLAMA-7B-CLASS\n0.639\n0.392\n0.523\n0.381\n0.478\n0.425\n0.526\n0.473\n0.587\n0.401\n0.506\nLLAMA-13B-RANDOM\n0.652\n0.389\n0.498\n0.287\n0.512\n0.433\n0.562\n0.469\n0.572\n0.488\n0.565\nLLAMA-13B-CLASS\n0.649\n0.412\n0.418\n0.342\n0.517\n0.426\n0.548\n0.456\n0.568\n0.523\n0.641\nLLAMA-70B-RANDOM\n0.826\n0.481\n0.599\n0.312\n0.471\n0.372\n0.625\n0.317\n0.716\n0.329\n0.676\nLLAMA-70B-CLASS\n0.852\n0.357\n0.612\n0.397\n0.588\n0.397\n0.613\n0.389\n0.727\n0.425\n0.682\nAG_News\nLLAMA-7B-RANDOM\n0.646\n0.238\n0.472\n0.265\n0.463\n0.312\n0.612\n0.448\n0.634\n0.361\n0.537\nLLAMA-7B-CLASS\n0.679\n0.267\n0.505\n0.372\n0.523\n0.378\n0.562\n0.384\n0.627\n0.326\n0.538\nLLAMA-13B-RANDOM\n0.685\n0.365\n0.517\n0.364\n0.522\n0.374\n0.548\n0.395\n0.648\n0.378\n0.552\nLLAMA-13B-CLASS\n0.685\n0.378\n0.528\n0.359\n0.413\n0.411\n0.566\n0.429\n0.654\n0.401\n0.569\nLLAMA-70B-RANDOM\n0.792\n0.311\n0.478\n0.316\n0.498\n0.401\n0.552\n0.309\n0.635\n0.319\n0.543\nLLAMA-70B-CLASS\n0.838\n0.302\n0.511\n0.271\n0.528\n0.354\n0.532\n0.274\n0.662\n0.283\n0.571\nTable 1: The performance comparison on the misclassification rate based on the uncertainty score from each approach. For each dataset, correct predictions are labeled as 0 and incorrect ones are labeled as 1. We show the AUPR and ROC (the higher the better) based on the uncertainty score and misclassification rate with two types of demonstration selection strategy: RANDOM and CLASS as well as three LLAMA model sizes: 7B, 13B, and 70B.\ngenerally yields higher AUPR and ROC scores across datasets, which proves it is more effective than random demonstration sampling. Class sampling ensures that each class is represented in the sample and reduces sampling bias, which is crucial in scenarios where the dataset might be imbalanced or where certain classes are underrepresented. 2) Increasing Model Size Enhances Performance: Larger models (moving from 7B to 70B) tend to have better performance in terms of AUPR and ROC. Specifically, there\u2019s a general trend of increasing AUPR and ROC scores as model size increases from 7B to 13B to 70B for all comparison methods. Some datasets and metrics do not strictly follow this trend. For instance, in the EMO-\nTION dataset, the 70B model sometimes shows a slight decrease in performance compared to the 13B model. The inconsistencies in performance improvement with larger models, especially for EU, hint at the complexity of uncertainty assessment in different contexts and datasets. 3. Treating all tokens equally can be harmful in uncertainty quantification: both Likelihood and Entropy Uncertainty treat all tokens equally. However, some tokens carry greater relevance and representativeness than others, owing to the phenomenon of \u201clinguistic redundancy\u201d. However, most uncertainty estimation methods treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/82c3/82c3f9c6-24a1-4130-9d89-9b02a6e9eeac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/06e7/06e78d5f-d737-49a0-ad83-8f67c80ff818.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) PR by LLAMA-2-13B</div>\n<div style=\"text-align: center;\">(a) PR by OPT-13B</div>\n<div style=\"text-align: center;\">Figure 4: The performance of misclassification rate using two backbone LLMs: OPT-13B and LLAMA-2-13B on EMOTION dataset. (a) and (b) demonstrate the precision-recall curves (x-axis is the recall and y-axis is the precision) for OPT-13B and LLAMA-2-13B; (c) and (d) demonstrate the ROC curve (x-axis is the false positive rate and y-axis is the true positive rate) for OPT-13B and LLAMA-2-13B, respectively.</div>\n# 4.3 Generalization Capability\nIn this work, we also show how our method performs when applied to different LLMs. We compare the performance of misclassification rate when using OPT-13B and LLAMA-2-13B. We compute the precision-recall (PR) curve and ROC curve using two backbone LLMs on the EMOTION dataset, and the results are shown in Figure 4. As shown in Figure 4, our method exhibits consistent trends across different LLMs. The precisionrecall curves of both uncertainties (Figure 4 (a) and 4 (b)) between the two methods are almost identical, and the model\u2019s capability between two LLMs is also reflected in the PR curves of EU. Furthermore, by comparing Figure 4 (c) and 4 (d), the ROC curves of both LLMs show a similar pattern, with the AUC scores not deviating significantly. Specifically, both OPT-13B and LLAMA-2-13B exhibit the same Area Under ROC (AUROC) curve = 0.68 for AU. Since LLAMA-2-13B is a more powerful LLM than OPT-13B, our method can quantify that EU of LLAMA-2-13B (AUROC = 0.59) is better than OPT-13B (AUROC = 0.55). This finding further supports our method maintains its performance irrespective of the underlying model and its robust generalization capability.\n# 4.4 Misclassification Rate with Out of Domain Demonstration\nOut-of-domain in-context Demonstration refers to the test instance being coupled with less relevant or out-of-domain demonstrations, which the model may be misled and not handle the test instance reliably. In this work, we analyze the misclassification rate of out-of-domain Demonstration in the EMOTION dataset (six-class sentiment analysis task) by providing LLMs with relevant demonstrations (sampled from Finance Phrasebank which is a three-class sentiment analysis task) and complete out-of-domain demonstrations (sampled from\n<div style=\"text-align: center;\">(d) ROC by LLAMA-2-13B</div>\n<div style=\"text-align: center;\">(c) ROC by OPT-13B</div>\nCOLA which is a binary linguistic acceptability task). We conduct the task with two demonstration selection strategies, and the results are provided in Table 2.\nLLaMA-13B-Random\nLLaMA-13B-Class\nEU\nAU\nEU\nAU\nOriginal\nDemo\n0.681\n0.585\n0.686\n0.599\nRelevant\nDemo\n0.688\n(+1.0%)\n0.541\n(\u22127.5%)\n0.671\n(\u22122.2%)\n0.524\n(\u221212.5%)\nOOD\nDemo\n0.671\n(\u22121.4%)\n0.501\n(\u221213.3%)\n0.673\n(\u22121.8%)\n0.497\n(\u221217.0%)\nTable 2: Comparison of AUROC in misclassificatin rate on EMOTION dataset, where \u201cOriginal Demo\u201d indicates we sample demonstrations from its original training set, \u201cRelevant Demo\u201d indicates we sample demonstrations from Finance Phrasebank Dataset (a relevant sentiment analysis task, and \u201cOOD Demo\u201d indicates we sample demonstrations from an irrelevant dataset: COLA.\nAs shown in the table, changes in the performance of the EU are relatively minor under all conditions, suggesting that the model is more stable or less sensitive to the changes in demonstration data within this metric. In contrast, the AU shows more significant fluctuations, which implies that the AU is more sensitive to the quality and relevance of demonstration data. When relevant demonstrations from the Finance Phrasebank sentiment analysis dataset are used, there\u2019s a slight improvement or a minor decrease in EU, but a notable decrease in AU. This suggests that even relevant but not identical data can confuse the model, especially for the AU. With out-of-domain demonstrations from COLA, the model\u2019s performance drops more significantly, with the AU metric showing a dramatic decrease of up to 17.0%, which indicates that the model struggles significantly when the demonstrations are not relevant to the task it\u2019s being tested on.\nSemantic\nOurs (EU)\nOurs (AU)\nAUPR\nROC\nAUPR\nROC\nAUPR\nROC\nRelevant\nDemo\n0.702\n0.644\n0.742\n0.935\n0.657\n0.682\nOOD\nDemo\n0.698\n0.712\n0.784\n0.941\n0.773\n0.607\nTable 3: Out-of-domain demonstration detection con-\nTable 3: Out-of-domain demonstration detection conducted with LLAMA-2-13B on EMOTION Dataset.\n# 4.5 Out-of-domain Demonstration Detection\nOut-of-domain (OOD) demonstration refers to coupling a test instance with less relevant or OOD demonstrations, potentially leading the model to be misled and handle the test instance unreliably. In this study, we investigate whether uncertainty scores can effectively distinguish between in-domain and OOD demonstrations. In our labeling scheme, in-domain demonstrations are labeled as 0, while OOD demonstrations are labeled as 1. AUPR and ROC analyses are performed based on the labels and uncertainty scores, with results summarized in Table 3. Specifically, we conduct experiments on the EMOTION dataset, involving two scenarios: in-domain demonstrations (sampled from its training set) and relevant demonstrations (sampled from Finance Phrasebank, a threeclass sentiment analysis task). Additionally, we compare in-domain demonstrations with complete OOD demonstrations (sampled from COLA, a binary linguistic acceptability task). As shown in Table 3, compared to the state-ofthe-art Semantic Uncertainty and the AU, the EU demonstrates the best indicator to detect both less relevant and OOD demonstrations. Intuitively, the model\u2019s predictions would be impacted by irrelevant and OOD demonstrations and exhibit large variance. AU is less effective than EU in detecting OOD demonstrations since the demonstrations already have large inherent variability. Semantic Uncertainty instead cannot really distinguish what is the root cause of the predictive uncertainty.\n# 4.6 Semantic Out-of-distribution Detection\nSemantic out-of-distribution (SOOD) detection refers to distinguishing test samples with semantic shifts from the given demonstrations and the prompt. In this study, we mask out a few classes and ask LLMs to classify test samples into the rest of the classes. The method is expected to return a higher uncertainty score of SOOD test samples. Specifically, we mask two classes 1: sadness and 2: anger out of six classes from the EMOTION dataset\nSemantic\nOurs (EU)\nOurs (AU)\nAUPR\nROC\nAUPR\nROC\nAUPR\nROC\n7B\n0.477\n0.532\n0.548\n0.658\n0.461\n0.570\n13B\n0.417\n0.468\n0.525\n0.592\n0.414\n0.437\nTable 4: Semantic out-of-distribution detection using\nTable 4: Semantic out-of-distribution detection using LLAMA-2 7B and 13B on EMOTION Dataset. and ask LLM to categorize a given test sample only into the rest four classes. The SOOD samples are labeled as 1 and in-distribution samples are labeled as 0. Results of AUPR and ROC are recorded in Table 4 in terms of different model sizes. As shown in the table, EU still performs the best as a better indicator to recognize SOOD samples across different model sizes. SOOD samples are semantically different from the provided demonstrations, and the task description also masks out the correct class of these SOOD samples, leading to higher uncertainty in the model\u2019s predictions. Given the inappropriate task description and demonstrations, AU may not necessarily perform better in the presence of SOOD samples.\n# 5 Conclusion\nWe provide a novel approach to decompose the predictive uncertainty of LLMs into its aleatoric and epistemic perspectives from the Bayesian perspective. We also design novel approximation methods to quantify different uncertainties based on the decomposition. Extensive experiments are conducted to verify the effectiveness and better performance of the proposed method than others. We believe this research stands as a significant stride toward harnessing the full potential of LLMs while being acutely aware of their performance boundaries. For future works, we plan to extend our method to other forms of data (Chen et al., 2022) and tasks (Zhang et al., 2024) to quantify the uncertainty.\n# Limitations\nThe proposed work aims at quantifying predictive uncertainty and decomposing the value into its aleatoric and epistemic components. While we can achieve the best result compared to other methods, the proposed framework may only be applied in natural language understanding tasks (e.g., multiplechoice QA, text classification, linguistics acceptability, etc.). The proposed uncertainty estimation algorithm may have limited usage in quantifying uncertainties of generation tasks since we cannot tell which part of the generated sequence is semantically important.\nReferences Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. 2021. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion, 76:243\u2013 297. Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. 2023. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. arXiv preprint arXiv:2305.13712. Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et al. 2024. Beyond efficiency: A systematic survey of resource-efficient large language models. arXiv preprint arXiv:2401.00625. Pei Chen, Haibo Ding, Jun Araki, and Ruihong Huang. 2021. Explicitly capturing relations between entity mentions via graph neural networks for domainspecific named entity recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 735\u2013742. Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, and George Karypis. 2024. Hytrel: Hypergraphenhanced tabular data representation learning. Advances in Neural Information Processing Systems, 36. Pei Chen, Haotian Xu, Cheng Zhang, and Ruihong Huang. 2022. Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3329\u20133339. Kamaljit Chowdhary and Paul Dupuis. 2013. Distinguishing and integrating aleatoric and epistemic variation in uncertainty quantification. ESAIM: Mathematical Modelling and Numerical Analysis-Mod\u00e9lisation Math\u00e9matique et Analyse Num\u00e9rique, 47(3):635\u2013 662. Stefan Depeweg, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Finale Doshi-Velez, and Steffen Udluft. 2017. Uncertainty decomposition in bayesian neural networks with latent variables. arXiv preprint arXiv:1706.08495. Shrey Desai and Greg Durrett. 2020. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. 2021. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion, 76:243\u2013 297. Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. 2023. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. arXiv preprint arXiv:2305.13712. Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et al. 2024. Beyond efficiency: A systematic survey of resource-efficient large language models. arXiv preprint arXiv:2401.00625. Pei Chen, Haibo Ding, Jun Araki, and Ruihong Huang. 2021. Explicitly capturing relations between entity mentions via graph neural networks for domainspecific named entity recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 735\u2013742. Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, and George Karypis. 2024. Hytrel: Hypergraphenhanced tabular data representation learning. Advances in Neural Information Processing Systems, 36. Pei Chen, Haotian Xu, Cheng Zhang, and Ruihong Huang. 2022. Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3329\u20133339. Kamaljit Chowdhary and Paul Dupuis. 2013. Distinguishing and integrating aleatoric and epistemic variation in uncertainty quantification. ESAIM: Mathematical Modelling and Numerical Analysis-Mod\u00e9lisation Math\u00e9matique et Analyse Num\u00e9rique, 47(3):635\u2013 662. Stefan Depeweg, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Finale Doshi-Velez, and Steffen Udluft. 2017. Uncertainty decomposition in bayesian neural networks with latent variables. arXiv preprint arXiv:1706.08495. Shrey Desai and Greg Durrett. 2020. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander\nPanchenko, Maxim Panov, et al. 2023. Lmpolygraph: Uncertainty estimation for language models. arXiv preprint arXiv:2311.07383.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962\u2013977. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187. Zi Lin, Jeremiah Zhe Liu, and Jingbo Shang. 2022. Towards collaborative neural-symbolic graph semantic parsing via uncertainty. Findings of the Association for Computational Linguistics: ACL 2022. Chen Ling, Junji Jiang, Junxiang Wang, and Zhao Liang. 2022. Source localization of graph diffusion via variational autoencoders for graph inverse problems. In Proceedings of the 28th ACM SIGKDD, pages 1010\u2013 1020. Chen Ling, Xuchao Zhang, Xujiang Zhao, Yanchi Liu, Wei Cheng, Mika Oishi, Takao Osaki, Katsushi Matsuda, Haifeng Chen, and Liang Zhao. 2023a. Openended commonsense reasoning with unrestricted answer candidates. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8035\u20138047. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, et al. 2023b. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703. Chen Ling, Xujiang Zhao, Xuchao Zhang, Yanchi Liu, Wei Cheng, Haoyu Wang, Zhengzhang Chen, Takao Osaki, Katsushi Matsuda, Haifeng Chen, et al. 2023c. Improving open information extraction with large language models: A study on demonstration uncertainty. arXiv preprint arXiv:2309.03433. Andrey Malinin and Mark Gales. 2020. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962\u2013977.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.\nAndrey Malinin and Mark Gales. 2020. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650.\nP. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.\nVipula Rawte, Amit Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687\u20133697. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641. Lisa Wimmer, Yusuf Sale, Paul Hofman, Bernd Bischl, and Eyke H\u00fcllermeier. 2023. Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures? In Uncertainty in Artificial Intelligence, pages 2282\u20132292. Yijun Xiao and William Yang Wang. 2019. Quantifying uncertainties in natural language processing tasks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7322\u20137329. Yijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. arXiv preprint arXiv:2103.15025. Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2022. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. arXiv preprint arXiv:2210.04714.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641.\nYijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. arXiv preprint arXiv:2103.15025.\nYuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2022. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. arXiv preprint arXiv:2210.04714.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. Jialin Yu, Alexandra I Cristea, Anoushka Harit, Zhongtian Sun, Olanrewaju Tahir Aduragba, Lei Shi, and Noura Al Moubayed. 2022. Efficient uncertainty quantification for multilabel text classification. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In NIPS. Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, and Liang Zhao. 2024. Elad: Explanation-guided large language models active distillation. arXiv preprint arXiv:2402.13098. Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. 2020. Uncertainty aware semi-supervised learning on graph data. Advances in Neural Information Processing Systems, 33:12827\u201312836. Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439.\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439.\n# A Appendix\n# A.1 Variance-based Decomposition\nAlternatively, we can use the variance as a measure of uncertainty. Let \u03c32(\u00b7) compute the variance of a probability distribution, and the total uncertainty present in Eq. (1) is then \u03c32(yT |x1:T ). This quantity can then be decomposed using the law of total variance:\n(4)\n\ufffd \ufffd where E[yT |x1:T , \u0398] and \u03c32(yT |x1:T , \u0398) are mean and variance of yT given p (yT |x1:T , \u0398). \u03c32 q(\u0398) (E[yT |x1:T , \u0398]) represents the variance of E[yT |x1:T , \u0398] when \u0398 \u223cq(\u0398), which indicates the epistemic uncertainty since it ignores the contribution of z. In contrast, Eq(\u0398) \ufffd \u03c32(yT |x1:T , \u0398) \ufffd in Eq. (4) represents the aleatoric uncertainty since it denotes the average value of \u03c32(yT |x1:T , \u0398) with\n\u0398 \u223cp(\u0398) and ingores the contribution of \u0398 to yT . Note that variance-based uncertainty decomposition does not involve the probability of the generated tokens, which is applicable to black-box LLMs (e.g., GPT models).\nVariance Approximation. In practice, when we are dealing with black-box LLMs (e.g., ChatGPT), there are multiple hyperparameters (e.g., temperature and top_p) allowing to return different responses. Specifically, [y1 T , . . . , yL T ] can be obtained through querying the LLM with different demonstrations [x1 1:T\u22121, . . . , xL 1:T\u22121] L times. The different set of parameter configurations are denoted as [\u03981, . . . , \u0398M]. The E[yT |x1:T , \u0398] can then be calculated as the expected model output given the input data and the model parameters \u0398. Calculate the variance of this expectation with respect to a set of model configurations over all sets of demonstrations gives the epistemic uncertainty. The variance \u03c32(yT ) can also be obtained given a set of few-shot demonstrations over all model parameters. Finally, average this variance over the certain model configuration to obtain the aleatoric uncertainty.\n# A.2 Dataset Description\nSentiment Analysis. 1) EMOTION (Saravia et al., 2018) contains 2, 000 test cases, where LLMs are asked to classify a given sentence with six categories: sadness, joy, love, anger, fear, surprise. 2) Financial Phrasebank (Financial) (Malo et al., 2014) contains 850 test cases, where LLMs are asked to classify a given financial news with three categories: negative, neutral, positive. 3) Stanford Sentiment Treebank v2 (SST2) (Socher et al., 2013) consists of 872 sentences from movie reviews and human annotations of their sentiment, where the language model is asked to predict the sentiment from two classes: positive and negative. Linguistic Acceptability. 1) The Corpus of Linguistic Acceptability (COLA) (Warstadt et al., 2019) is about English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence, and there are 1, 040 test cases in total. Topic Classification. TC aims at categorizing the given sentence into predefined topics. We adopt AG_News (Zhang et al., 2015) is a dataset that collects more than 1 million news articles, where LLMs are asked to classify a given news into four\ncategories: World, Sports, Business, and Sci/Tech. There are 1, 160 test cases in total.\n# A.3 Experiment Setup\nWe conduct experiments primarily on LLAMA2-7B-CHAT-HF, LLAMA-2-13B-CHAT-HF, and LLAMA-2-70B-CHAT-HF, where the model weights are downloaded from the website1. Since we cannot actually \u201csample\u201d model weights as Bayesian Neural Networks, in order to let LLMs return different outputs, we leverage Beam Search since it considers multiple best options based on beam width using conditional probability, which is better than the sub-optimal Greedy search. The beam search is conducted with the beam size 10 and the max number of new tokens is set to be 16 uniformly across all datasets. We choose a different number of demonstrations (details are recorded in Table 5) to allow the LLM to achieve the best performance on each dataset, and we sample demonstrations four times uniformly across all datasets.\nRandom\nClass\nEmotion\n6\n1 per class\nFinancial\n6\n2 per class\nSST2\n4\n2 per class\nCOLA\n2\n1 per class\nAG_News\n4\n1 per class\nTable 5: Number of demonstrations selected in each dataset.\n# A.4 Prompt Template\nIn this work, we uniformly apply the following prompt template for all datasets. Take the EMOTION dataset as an example, we summarize the prompt in Table 6. Note that all datasets use the same template, small modifications are made on the actual label information and different demonstration numbers of different datasets.\n# A.5 Case Study\nTable 7 demonstrates the actual changes in AU and EU when presenting LLMs with different sizes and different demonstrations. Given the test query is: I had stated to her the reason I feel so fearful is that I feel unsafe with the ground truth label is (4: fear), which is a sentence with a negative\n1https://ai.meta.com/resources/models-andlibraries/llama-downloads/\nSystem Prompt\n### Below is an instruction that describes a task. Clearly follow the instruction and write a short\nresponse to answer it.\nTask Description\n### Instruction: Classify the sentiment in the following text based on the six categories:\n[0: Sadness; 1: Joy, 2: Love; 3: Anger; 4: Fear, 5: Surprise]. Provide the information in a\nstructured format WITHOUT additional comments, I just want the numerical label for each text.\nDemonstrations\n### Here are some examples:\nExample 1: Sentence: {i didnt feel humiliated} Category: {0: Sadness}\nExample 2: Sentence: {im grabbing a minute to post i feel greedy wrong} Category: {3: anger}\nExample 3: Sentence: {i have the feeling she was amused and delighted} Category: {1: joy}\nExample 4: Sentence: {i feel more superior dead chicken or grieving child} Category: {1: joy}\nExample 5: Sentence: {i get giddy over feeling elegant in a pencil skirt} Category: {1: joy}\n...\nTest Query\n### Test\nSentence: {} Category:\nTable 6: Prompt Template consists of four parts: 1) System Prompt aims at providing a basic hint of the task; 2) Task Description provides some details of the task, e.g., if it is a sentiment analysis task or how many labels are there; 3) Few-shot Demonstrations are leveraged to give LLMs some basic formats of how the returned responses can be constructed; and 4) Test Query is the final test query that we want LLMs to classify/categorize, and the LLM is only expected to return an exact answer to solve the given question.\nfeeling. For LLAMA-2-7B, by presenting LLMs with more diverse demonstrations (containing both positive and negative sentences), the results would be more diverse between different beam search returned sequences, leading to a relatively higher AU than EU. For LLAMA-2-70B with a larger parameter space and model capability, the EU and AU are significantly reduced, which indicates the model is more confident in the generated output and the variation of data may not influence much to the prediction.\nTesting Query:\nI had stated to her the reason I feel so fearful is because I feel unsafe (4: fear)\nExtracted\nPredictions\nEU\nAU\nLLaMA-2-7B\n1. i felt anger when at the end of a telephone call (3: anger)\n2. i feel a little mellow today (1: joy)\n3. i don t feel particularly agitated (4: fear)\n4. i hate it when i feel fearful for absolutely no reason (4: fear)\n5. im updating my blog because i feel shitty (0: sadness)\n0, 0, 0, 1, 3\n4, 3, 4, 4, 4\n0.171\n0.372\n1. i am feeling outraged it shows everywhere (4: fear)\n2. i do feel insecure sometimes but who doesnt (4: fear)\n3. i start to feel emotional (0: sadness)\n4. i feel so cold a href http irish (3: anger)\n5. i feel i have to agree with her even though i can imagine\nsome rather unpleasant possible cases (0: sadness)\n4, 4, 1, 3, 4\n4, 4, 4, 5, 4\n0.163\n0.189\nLLaMA-2-70B\n1. i felt anger when at the end of a telephone call (3: anger)\n2. i feel a little mellow today (1: joy)\n3. i don t feel particularly agitated (4: fear)\n4. i hate it when i feel fearful for absolutely no reason (4: fear)\n5. im updating my blog because i feel shitty (0: sadness)\n4, 3, 4, 3, 4\n4, 4, 2, 4, 4\n0.012\n0.079\n1. i am feeling outraged it shows everywhere (4: fear)\n2. i do feel insecure sometimes but who doesnt (4: fear)\n3. i start to feel emotional (0: sadness)\n4. i feel so cold a href http irish (3: anger)\n5. i feel i have to agree with her even though i can imagine\nsome rather unpleasant possible cases (0: sadness)\n4, 4, 4, 4, 4\n4, 4, 4, 4, 4\n0.004\n0.009\nTable 7: Case study on the actual EU and AU decomposed from the predictive uncertainty\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of uncertainty in the responses of Large Language Models (LLMs) during in-context learning, highlighting the need for a new method to quantify this uncertainty due to existing approaches overlooking the complex nature of LLMs and their reliance on provided demonstrations.",
        "problem": {
            "definition": "The problem is the difficulty in accurately quantifying uncertainty in LLM responses during in-context learning, which can arise from both the provided demonstrations and the model's configurations.",
            "key obstacle": "The core obstacle is the inability of existing methods to decompose uncertainty into its primary sources, specifically distinguishing between aleatoric and epistemic uncertainties."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to understand the sources of uncertainty in LLM predictions, particularly in the context of in-context learning where both demonstration quality and model configuration can affect outcomes.",
            "opinion": "The proposed idea involves a novel formulation and estimation method to quantify both aleatoric and epistemic uncertainties in LLM responses, providing insights into the prediction process.",
            "innovation": "The innovation lies in the decomposition of uncertainty into aleatoric and epistemic components, using a mutual information perspective, which significantly improves upon existing methods that do not account for these distinctions."
        },
        "method": {
            "method name": "Uncertainty Quantification for In-Context Learning",
            "method abbreviation": "UQ-ICL",
            "method definition": "This method quantifies both aleatoric and epistemic uncertainty from the predictive distribution of LLMs during in-context learning.",
            "method description": "The method provides a framework to decompose uncertainty into its sources, allowing for a better understanding of LLM predictions.",
            "method steps": [
                "Formulate the uncertainty decomposition problem.",
                "Estimate aleatoric and epistemic uncertainties using mutual information.",
                "Apply an entropy-based estimation method to quantify uncertainties."
            ],
            "principle": "The method is effective because it leverages the Bayesian perspective to understand how different factors contribute to the predictive uncertainty of LLMs."
        },
        "experiments": {
            "evaluation setting": "The experiments are conducted on open-source LLMs (LLAMA-2) with varying model sizes (7B, 13B, 70B) across different Natural Language Understanding tasks, including sentiment analysis and topic classification.",
            "evaluation method": "The evaluation involves comparing the performance of the proposed uncertainty quantification method against baseline methods, measuring metrics such as prediction accuracy, AUPR, and AUROC based on uncertainty scores."
        },
        "conclusion": "The proposed method effectively decomposes predictive uncertainty into aleatoric and epistemic components, showing superior performance compared to existing methods and enhancing our understanding of LLM limitations and reliability.",
        "discussion": {
            "advantage": "Key advantages include a more accurate assessment of uncertainty in LLM predictions, allowing for better decision-making in applications where reliability is critical.",
            "limitation": "The method may be limited in its applicability to natural language understanding tasks and may not perform well in generation tasks where semantic importance of tokens is unclear.",
            "future work": "Future research will focus on extending the method to other types of data and tasks, as well as improving uncertainty quantification for generation tasks."
        },
        "other info": {
            "code and data availability": "The code and data for this research are available at: https://github.com/lingchen0331/UQ_ICL.",
            "date of publication": "28 Mar 2024",
            "arxiv link": "arXiv:2402.10189v2"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of uncertainty in the responses of Large Language Models (LLMs) during in-context learning, highlighting the need for a new method to quantify this uncertainty."
        },
        {
            "section number": "1.2",
            "key information": "The significance of understanding uncertainty in LLMs is emphasized as it can impact the reliability of predictions in natural language processing tasks."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Uncertainty Quantification for In-Context Learning (UQ-ICL), allows LLMs to adapt to various contexts by decomposing uncertainty into aleatoric and epistemic components."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical framework presented in the paper focuses on the decomposition of uncertainty using a mutual information perspective."
        },
        {
            "section number": "4.1",
            "key information": "The design of uncertainty quantification significantly influences the outcomes of in-context learning by providing insights into the predictive process of LLMs."
        },
        {
            "section number": "6.1",
            "key information": "The paper discusses model bias and context sensitivity, particularly how existing methods fail to accurately quantify uncertainty, which can lead to biased predictions."
        },
        {
            "section number": "6.2",
            "key information": "Computational costs associated with applying the proposed UQ-ICL method are evaluated against baseline methods to assess its efficiency."
        },
        {
            "section number": "7",
            "key information": "The conclusion highlights the superior performance of the proposed method in decomposing predictive uncertainty compared to existing methods, enhancing understanding of LLM limitations."
        }
    ],
    "similarity_score": 0.7237620892779556,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Uncertainty Quantification for In-Context Learning of Large Language Models.json"
}