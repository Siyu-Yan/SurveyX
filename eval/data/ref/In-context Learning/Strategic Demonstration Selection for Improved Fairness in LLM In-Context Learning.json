{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.09757",
    "title": "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning",
    "abstract": "Recent studies highlight the effectiveness of using in-context learning (ICL) to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements in performance, the fairness implications of these methods are less understood. This study investigates how varying demonstrations within ICL prompts influence the fairness outcomes of LLMs. Our findings reveal that deliberately including minority group samples in prompts significantly boosts fairness without sacrificing predictive accuracy. Further experiments demonstrate that the proportion of minority to majority samples in demonstrations affects the trade-off between fairness and prediction accuracy. Based on these insights, we introduce a mitigation technique that employs clustering and evolutionary strategies to curate a diverse and representative sample set from the training data. This approach aims to enhance both predictive performance and fairness in ICL applications. Experimental results validate that our proposed method dramatically improves fairness across various metrics, showing its efficacy in real-world scenarios.",
    "bib_name": "hu2024strategicdemonstrationselectionimproved",
    "md_text": "# Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning\nJingyu Hu University of Bristol Bristol, UK ym21669@bristol.ac.uk Weiru Liu University of Bristol Bristol, UK weiru.liu@bristol.ac.uk Mengnan Du New Jersey Institute of Technology Newark, USA mengnan.du@njit.edu\nWeiru Liu University of Bristol Bristol, UK weiru.liu@bristol.ac.uk Mengnan Du New Jersey Institute of Technology Newark, USA mengnan.du@njit.edu\n# Abstract\nRecent studies highlight the effectiveness of using in-context learning (ICL) to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements in performance, the fairness implications of these methods are less understood. This study investigates how varying demonstrations within ICL prompts influence the fairness outcomes of LLMs. Our findings reveal that deliberately including minority group samples in prompts significantly boosts fairness without sacrificing predictive accuracy. Further experiments demonstrate that the proportion of minority to majority samples in demonstrations affects the trade-off between fairness and prediction accuracy. Based on these insights, we introduce a mitigation technique that employs clustering and evolutionary strategies to curate a diverse and representative sample set from the training data. This approach aims to enhance both predictive performance and fairness in ICL applications. Experimental results validate that our proposed method dramatically improves fairness across various metrics, showing its efficacy in real-world scenarios.\narXiv:2408.09757v1\n# 1 Introduction\nLarge Language Models (LLMs), such as GPT4 (OpenAI, 2023), Claude-3 (AnthropicAI, 2023), and LLaMA-2 (Touvron et al., 2023), have achieved state-of-the-art performance in many natural language processing tasks. These LLMs can adapt to different tasks by adding in-context prompts, without needing to retrain on the entire new dataset. This optimization technique for LLMs is called in-context learning (ICL), which leverages specific input prompts to guide LLMs to generate more accurate outputs. Recent research suggests that incorporating specially selected demonstrations into these prompts can significantly enhance LLM performance (Brown et al., 2020; Schick and Sch\u00fctze, 2020).\nDue to prompt length limitations, traditional LLMs have faced challenges in processing demonstrations from tabular datasets, which have a large number of features. However, with recent LLMs relaxing input length constraints, new avenues for applications in tabular datasets are opening up. (Hegselmann et al., 2023) has confirmed the predictive capabilities of LLMs on datasets from UCI repository. Considering the usages of tabular data in high-stakes domains (Grinsztajn et al., 2022), ensuring fairness alongside prediction performance is crucial for increasing trust in LLMs. (Liu et al., 2023) has highlighted biases in LLM predictions with tabular datasets, but there are limited further investigations on how the fairness of LLMs performance varies with different ICL demonstrations. To bridge this gap, we aim to answer the following research question: How do different demonstration strategies impact the fairness performance of LLMs on tabular classification tasks? And is there a demonstration strategy better than other strategies? To better understand the impact of incontext learning on fairness, our proposed demonstration strategy considers the distribution of both demographic groups and target labels. A dataset can be divided into subgroups by demographic features, labelling the smallest as the minority (underrepresented) and the larger one as the majority. The fairness investigation compares differences between majority and minority groups. Our investigation includes evaluating five advanced LLMs, i.e., Text-davinci-003, GPT-3.5-turbo, GPT-4-turbo 1, Claude-3 Haiku, and Claude-3 Sonnet2, across two fairness-focused tabular datasets: Credit and Adult. We found that prioritizing underrepresented samples and conscientiously including minority demographic groups and target labels during few-shot learning can significantly improve the fairness performance in LLMs output.\n1https://platform.openai.com/docs/models/ 2https://www.anthropic.com/news/claude-3-family\nDespite the experimental observations, we are still wondering: Why does prioritizing minority group demonstrations benefit the fairness performance of LLMs in tabular-based classification tasks? To further clarify this phenomenon, we perturb prediction labels and sensitive features in selected demonstrations and compare how the prediction outcomes of LLMs would be altered. Through these perturbation experiments, we found that increasing the proportion of underrepresented labels enhances fairness, but can lead to a decline in prediction performance, and vice versa. Up until now, the above findings and explanations have been based on random demonstration selection. We hypothesize that: We can deliberately select demonstrations to further improve fairness performance. Motivated by the fiLter-thEn-Search (LENS) (Li and Qiu, 2023) for textual classification, we adopt a similar process for extracting tabular demonstrations: first refine the training data set into a candidate pool and then sample and evaluate these candidates to identify the most supportive demonstrations. To this end, we introduced the Fairness via Clustering-Genetic (FCG) algorithm to effectively extract representative samples, to further enhance the fairness of LLM. Unlike LENS, which relies on progressive iterations on LLMs for candidate selection, our FCG method utilizes clustering. Clustering does not require access to LLMs and maintains diversity among the selected shots, effectively addressing concerns related to the time required for iterations and the cost of LLM access. Additionally, previous studies often assume the same selection probabilities for candidates across evaluation iterations, requiring enormous iterations to ensure that each sample is equally considered. Inspired by genetic evolution concepts, we adopt dynamic probabilities which give priority to representative samples with higher selection chances. Sample representativeness is measured by the LLM performance score, whose score is updated for each iteration. In this way, FCG can narrow the final sample set more efficiently, and drastically reduce the number of iterations needed. We implement experiments to evaluate the proposed FCG demonstration selection method. The results confirm that FCG algorithm improves LLMs performance in almost all strategies, with prioritizing the minority group still yielding the best results. To conclude, the main contributions in this paper are as follows:\n\u2022 We find that prioritizing underrepresented samples and conscientiously including minority demographic groups and target labels during fewshot learning can dramatically improve the fairness performance in LLM output (Section 3). \u2022 We explain why prioritizing minorities leads to a fairer solution, and find the trade-off between LLMs\u2019 performance and demographic labels: increasing the ratio of underrepresented labels enhances fairness, but can lead to a decline in prediction performance, and vice versa (Section 4). \u2022 We propose the FCG (Fairness via ClusteringGenetic) algorithm, an efficient approach to retrieve a diverse and representative set of demonstrations from training data. Across almost all strategies, FCG enhances fairness in LLMs under in-context learning (Section 5).\n\u2022 We find that prioritizing underrepresented samples and conscientiously including minority demographic groups and target labels during fewshot learning can dramatically improve the fairness performance in LLM output (Section 3). \u2022 We explain why prioritizing minorities leads to a fairer solution, and find the trade-off between LLMs\u2019 performance and demographic labels: increasing the ratio of underrepresented labels enhances fairness, but can lead to a decline in prediction performance, and vice versa (Section 4). \u2022 We propose the FCG (Fairness via ClusteringGenetic) algorithm, an efficient approach to retrieve a diverse and representative set of demonstrations from training data. Across almost all strategies, FCG enhances fairness in LLMs under in-context learning (Section 5).\n# 2 Experiment Setup\nOur primary goal is to investigate how different few-shot demonstration choices influence the fairness performance of LLMs under the in-context learning (ICL) setting. Detailed related work on this area is discussed in Appendix A. In this section, we introduce the overall experimental setups. Notations. Given a dataset D = (X, Y, Z)n i=1 where features X \u2208R, the binary classification labels Y \u2208Y := {0, 1}, and sensitive feature Z \u2208Z := {0, 1}. We set Z = 0 to represent the minority group and Z = 1 as the majority group. D is split into training dataset Dtr, validation dataset Ddev and testing dataset Dtest. For each data point d \u2208D := {x, y, z}, a classifier f predicts the label f(x) based on the input features x. Given a subset D\u2032 \u2208Dtr, the proportion of samples where Z = 0 within D\u2032 is denoted as rz. Specifically, rz = 1 means all samples in D\u2032 belong to a minority group, whereas rz = 0 implies that every sample in D\u2032 is from the majority group. Similarly, the proportion of samples for which Y = 0 within D\u2032 is represented by ry.\n(1)\nModels and Datasets. We use five LLMs as f: Text-davinci-003 (Davinci), GPT-3.5-turbo, GPT4-turbo, Claude-3 Haiku, and Claude-3 Sonnet. The temperature in the model parameter is set to zero to ensure consistent responses. We select two tabular-based fairness datasets: default of credit card clients dataset (Credit, (Yeh, 2016)) and adult\nincome (Adult, (Becker and Kohavi, 1996)). The Credit dataset covers information on credit card clients in Taiwan, including demographics, bills, payment history, etc. Its target is to predict whether there will be an overdue payment next month. The Adult dataset is to predict whether an individual\u2019s annual income exceeds 50K based on their individual features. Appendix B contains further descriptions of dataset structures. Evaluation Metrics. The predictive performance of LLMs on labels Y is evaluated by metrics Accuracy, Precision, Recall, and F-score 3. We introduce \u2206dp, Rdp, \u2206eo, and Reo to evaluate fairness 4. They refer to the differences and ratios of Demographic Parity (DP) (Dwork et al., 2012) and Equalized Odds (Eodds) (Hardt et al., 2016) between subgroups. The demographic parity of the two groups partitioned by Z is defined by Equation 2. DP difference \u2206dp represents the difference between two, and DP ratio Rdp is the ratio of the DP0 and DP1.\nThe True Positive Rate (TPR) and False Positive Rate (FPR) for both subgroups (Z = 0 and Z = 1) are defined as follows.\n(3)\nEodds difference \u2206eo is defined as the greater metrics of TPR and FPR differences (Equation 4) where \u2206TPR = (TPR1\u2212TPR0) and \u2206FPR = (FPR1 \u2212FPR0).\n(4)\nEodds ratio Reo is the smaller ratio of TPR and the ratio of FPR between two groups, as shown below. Here \u03f5 is used to avoid the setting where the denominator is zero, where we set \u03f5 = 1\u22126:\nThe four fairness metrics range from 0 to 1. Lower \u2206dp and \u2206eo show smaller performance differences between groups, which points to fairer\n3https://scikit-learn.org/stable/ 4https://fairlearn.org/main/user_ guide/fairness_in_machine_learning.html\npredictions. Higher Reo and Rdp reflect more consistent performance across subgroups, suggesting better fairness. Prompt Template. The output answer from the LLMs is based on the input prompt. As shown in Figure 1, the structure of a prompt can be divided into three parts: task description, in-context demonstrations, and questions. Part \u2776clarifies the task and defines the format of output prediction label options. Part \u2777contains a series of demonstrations as references. Part \u2778is the sample to be predicted. We consider both zero-shot learning and fewshot learning in our experiments. Zero-shot learning refers to LLMs with a prompt exclude demonstration references (without \u2777) and is set as the baseline. Few-shot learning, sometimes also called in-context learning (ICL), consists of all three parts as input prompts. We compare how different demonstrations in part \u2777influence the fairness of LLMs. The prompt example in Figure 1 simplifies the tabular dataset, the detailed template is provided in Appendix C.\n# 3 How Demonstrations Impact Fairness of LLMs for Tabular Inputs?\nIn this section, we aim to answer: how do few shot demonstrations influence the fairness performance of LLMs for processing tabular inputs under the in-context learning setting? To investigate this, we examine fairness performance variances across different demonstrations. We propose different combinations of prediction feature distribution rz and sensitive feature distribution ry, expecting to explore the potential correlation between these feature distributions and LLM fairness. In the experiment, different demonstrations are based on three distinct sampling strategies denoted as S1, S2, and S3, each with unique distribution combinations of rz and ry.\n\u2022 S1: Balanced Samples with Balanced Labels (rz = 0.5, ry = 0.5); \u2022 S2: Prioritize Minority Samples with Balanced Labels (rz = 1, ry = 0.5); \u2022 S3: Prioritize Minority Samples with Unbalanced Labels (rz = 1, ry \u0338= 0.5).\nFigure 2 displays the performance of different LLMs on the Credit dataset. ry is set to 1 in S3. The fairness performance improves when prioritizing samples from minority groups (rz = 1) compared to a balanced sample selection (rz = 0.5).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/97f8/97f8d910-a7c0-4649-93a9-565b87ca2875.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The Prompt Template and Content Example (*Perturbation is optional and is used to test the effectiveness of ICL. We discussed perturbations in Section 4)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c88/5c884b82-cdc1-4f45-b7f0-df7767eb8007.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Prediction and fairness performance comparison across different LLMs on Credit dataset. It shows improvements in fairness metrics when samples from minority groups are prioritized.</div>\nSimilar findings are found in the Adult dataset. Table 1 presents the performance of the GPT-3.5turbo with zero-shot and different few-shot strategies. To ensure the stability and reliability of the results, we use random seeds set={25, 35, 42, 45, 55} when selecting few-shot samples. The presented table summarizes average values and standard errors for the random seeds set. Overall, the results show that all few-shot strategies have generally improved fairness compared to zero-shot learning without lowering predictions. Also, prioritizing minorities (S2, S3) is an effective way to improve fairness. In contrast, balanced prompts (S1) show worse fairness performance. To further explain the observed pattern, we implement additional experiments and discussions on GPT-3.5\u2019s performance under the Adult dataset in the following sections. Complete results for other LLMs (e.g., Claude), using different seeds, are included in Appendix D.\n# 4 Why Prioritizing Minority Group Demonstrations Benefit Fairness?\nThe above analysis points out a strong correlation between prioritizing minority group demonstrations with improved fairness performance of LLMs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa25/aa25e0bd-62c7-4ee3-b502-a6384cb8b53e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The Workflow of Perturbations</div>\nHowever, it is not yet clear how and why this phenomenon occurs. Thereby our next step is to clarify which part of the demonstrations most influenced the performance of LLMs. Specifically, we perturb the prediction label Y and the sensitive feature Z in selected demonstrations and compare how the prediction outcomes of LLMs would be altered. The following experiment is performed on the Adult dataset with \u2018income\u2019 as feature Y and \u2018gender\u2019 as feature Z. We set the random seed to 55\n<div style=\"text-align: center;\">Table 1: Performance of GPT3.5-turbo with zero-shot and different few-shot strategies (S1, S2, S3) on Adult Income dataset. It demonstrates that strategic inclusion of demonstrations, particularly those from minority groups, can ignificantly enhance both predictive performance and fairness outcomes.</div>\nPrediction\nZero-shot\nrz = 0.5, ry = 0.5 (S1)\nrz = 1, ry = 0.5 (S2)\nrz = 1, ry = 1 (S3)\nAccuracy \u2191\n0.6855\n0.7312 \u00b1 0.0009\n0.7328 \u00b1 0.0028\n0.7230 \u00b1 0.0014\nPrecision \u2191\n0.8519\n0.7936 \u00b1 0.0012\n0.7841 \u00b1 0.0051\n0.7808 \u00b1 0.0038\nRecall \u2191\n0.4492\n0.6250 \u00b1 0.0012\n0.6461 \u00b1 0.0130\n0.6122 \u00b1 0.0036\nF-score \u2191\n0.5882\n0.6993 \u00b1 0.0011\n0.7060 \u00b1 0.0062\n0.6915 \u00b1 0.0017\nFairness\nZero-shot\nrz = 0.5, ry = 0.5 (S1)\nrz = 1, ry = 0.5 (S2)\nrz = 1, ry = 1 (S3)\nRdp \u2191\n0.4063\n0.6470 \u00b1 0.0019\n0.6769 \u00b1 0.0080\n0.6732 \u00b1 0.0095\nReo \u2191\n0.1111\n0.3682 \u00b1 0.0044\n0.4152 \u00b1 0.0125\n0.4722 \u00b1 0.0187\n\u2206dp \u2193\n0.2227\n0.1688 \u00b1 0.0009\n0.1578 \u00b1 0.0031\n0.1555 \u00b1 0.0046\n\u2206eo \u2193\n0.3203\n0.1875 \u00b1 0.0019\n0.1859 \u00b1 0.0058\n0.1906 \u00b1 0.0071\nto extract two groups with balanced labels DataF and DataM from Dtr as raw demonstrations. (1) DataF: balanced high-income and low-income females (ry = 0.5, rz = 1). (2) DataM: balanced high-income and low-income males (ry = 0.5, rz = 0). Figure 3 illustrates the perturbation workflow. We define four perturbations, each consisting of the feature to be perturbed and the new proportions after perturbation. For example, perturbing ry = 0.5 \u21921 means that the quantity of high-income and low-income samples are balanced in raw demonstrations (ry = 0.5), and the perturbed demonstrations will become all low-income samples (r\u2032 y = 1) by flipping high-income labels to low-income. The next part will discuss how perturbations at different proportions affect the overall prediction and fairness performance of LLM, along with a deeper performance comparison within subgroups.\n# 4.1 Perturbations Impact on Overall Fairness\nTable 2 compares the prediction and fairness performance with different perturbations on gender and income. Perturbing the income labels for DataF and DataM leads to a certain degree of decline in predictive performance (1% to 6%). Min et al., 2022 mentioned a similar phenomenon that replacing gold labels with random labels only presents the marginal impact on performance. Nevertheless, we also found that altering the ground truth labels (income) can greatly affect fairness performance, resulting in a drastic drop in all scenarios. Especially when replacing labels with high income, the Rdp in DataF decreased from 71.32% to 50.54%, and in DataM, it decreased from 50.94% to 43.06%. When we perturbed gender labels, results show\nthat fairness performance improves with a higher proportion of females. The fairness performance decreases when we perturb from female to male in DataF, as Rdp decreases from 71.32% to 59.15%. Similar patterns are observed in DataM, where fairness gradually increases by 8.1% when modifying from male to female. In most cases, the perturbation results align with the intuition that distorting real data can degrade its quality, thus potentially leading to negative impacts on LLMs performance. However, we also find that perturbing to a higher ratio of minority labels (r\u2032 z = 1) can positively enhance fairness, suggesting a strong connection between fairness performance and sensitive labels. To further validate this finding, Section 4.2 compares performance variations at the subgroup level.\n# 4.2 Perturbations Impact across Subgroups\nTable 3 displays the model performance of TPR and FPR on both minority (female) and majority (male) subgroups after income and gender perturbations. Similar to DP and EO, the metrics \u2206TPR and \u2206FPR assess performance disparities between female and male subgroups. Equal treatment is achieved when these differences approach zero, hence, lower values of \u2206TPR and \u2206FPR are preferable. We also consider absolute values of FPR and TPR within each subgroup to fully assess fairness changes in perturbations. In income perturbations, replacing the income labels results in a decrease in TPR and FPR for both female and male groups, with a more significant decline observed in the female group. This reduction is most notable when income labels are changed to high-income. In a few cases, the relative metrics \u2206TPR and \u2206FPR show improve-\n<div style=\"text-align: center;\">Table 2: Prediction and Fairness Performance on Income and Gender Perturbations</div>\nDifferent perturbations on income\nDifferent perturbations on gender\nrz = 1, ry = 0.5 (DataF)\nrz = 0, ry = 0.5 (DataM)\nrz = 1, ry = 0.5 (DataF)\nrz = 0, ry = 0.5 (DataM)\nPrediction\nRaw\nr\u2032\ny = 1\nr\u2032\ny = 0\nRaw\nr\u2032\ny = 1\nr\u2032\ny = 0\nRaw\nr\u2032\nz = 0.5\nr\u2032\nz = 0\nRaw\nr\u2032\nz = 0.5\nr\u2032\nz = 1\nAccuracy \u2191\n0.7480\n0.7383\n0.6992\n0.7148\n0.6914\n0.6543\n0.7480\n0.7422\n0.7617\n0.7148\n0.7168\n0.7090\nPrecision \u2191\n0.7873\n0.7933\n0.8643\n0.8438\n0.8451\n0.8835\n0.7873\n0.7925\n0.7965\n0.8438\n0.8364\n0.8204\nRecall \u2191\n0.6797\n0.6445\n0.4727\n0.5273\n0.4688\n0.3555\n0.6797\n0.6563\n0.7031\n0.5273\n0.5351\n0.5352\nF- score \u2191\n0.7296\n0.7112\n0.6111\n0.6490\n0.6030\n0.5070\n0.7296\n0.7179\n0.7469\n0.6490\n0.6556\n0.6478\nFairness\nRaw\nr\u2032\ny = 1\nr\u2032\ny = 0\nRaw\nr\u2032\ny = 1\nr\u2032\ny = 0\nRaw\nr\u2032\nz = 0.5\nr\u2032\nz = 0\nRaw\nr\u2032\nz = 0.5\nr\u2032\nz = 1\nRdp \u2191\n0.7132\n0.6508\n0.5054\n0.5094\n0.4639\n0.4306\n0.7132\n0.6308\n0.5915\n0.5094\n0.5421\n0.5905\nReo \u2191\n0.3824\n0.3438\n0.0556\n0.1364\n0.1579\n0.0000\n0.3824\n0.2571\n0.1500\n0.1364\n0.2273\n0.3043\n\u2206dp \u2193\n0.1445\n0.1719\n0.1797\n0.2031\n0.2031\n0.1602\n0.1445\n0.1875\n0.2266\n0.2031\n0.1914\n0.1680\n\u2206eo \u2193\n0.1641\n0.1797\n0.226\n0.2578\n0.2813\n0.2266\n0.1641\n0.2031\n0.2656\n0.2578\n0.2500\n0.2109\nment compared to the results without perturbations. However, the corresponding absolute metrics TPR and FPR do not show consistent trends and worsen instead. This inconsistency makes it difficult to validate the impact of income on fairness performance. Therefore, we conclude that the ground truth labels in the demonstrations are not the source of benefit for LLMs\u2019 fairness.\nIn gender perturbations, however, subgroup performance is greatly affected by these gender label changes. For absolute values, flipping female labels to male in DataF leads to a 4.69% increase in FPR and a 5.47% increase in TPR for the male group. Similarly, transforming male labels to female in DataM results in increases in both TPR and FPR for the female group. Similar trends are found in their relative values. Increasing the proportion of male labels leads to higher \u2206TPR and \u2206FPR, illustrating greater difference in subgroup treatment. Conversely, an increase in the ratio of female labels leads to reductions in both \u2206TPR and \u2206FPR, suggesting enhanced fairness.\nIn general, the above results show a trade-off between the LLMs performance and demographic labels. LLMs exhibit improved performance when the proportion of minority groups increases: they become fairer compared to the used of original data when perturbing demographic labels male to female. Therefore, we conclude that prioritizing demonstrations from minority groups can maximize these advantages and promote fairness in LLMs. In contrast, perturbing labels leads to demonstrations becoming less reliable, as they can lead models to learn noise and perform worse. The perturbation on prediction labels (income) conforms with this pattern.\n# 5 Mitigation Algorithm for Fair Demonstration Selection\nThe above results confirm that the application of diverse demonstrations, particularly those from the minority, can drastically influence the fairness of LLMs. Experiments on different sets of selected shots under the same strategy also reveal a similar trend, albeit with different absolute performance values. This leads to the question: how to extract representative demonstrations that yield better performance among these sets? Enumerating and evaluating the outcomes of LLMs across all possible sets is impractical due to the sheer number of combinations. Thus, in this section, we propose a fairness via clusteringgenetic (FCG) algorithm to efficiently select influential demonstrations, leading LLMs to a better performance without having to explore all possible combinations. The core idea of FCG includes three aspects: (1) Use clustering to shrink the selection sets while maintaining diverse samples. (2) Define a score that considers both prediction and fairness performance, applying a genetic approach to iteratively select and score these samples within the sets. (3) Rank samples from highest to lowest based on their scores to select the most influential ones.\n# 5.1 The Proposed FCG Algorihtm\nWe introduce details of the proposed FCG mitigation algorithm in this section. Clustering. Based on the value combinations of the sensitive feature Z and label Y, we divide the training data Dtr into four subgroups denoted as SG= {g1(Z = 1, Y = 0), g2(Z = 1, Y = 1),g3(Z = 0, Y = 0), g4(Z = 0, Y = 1)}. For each subgroup, we apply k-means clustering to extract a diverse and representative initial population.\n<div style=\"text-align: center;\">Table 3: TPR and FPR Assessment across Subgroups on Income and Gender Perturbations</div>\nDifferent perturbations on income\nDifferent perturbations on gender\nrz = 1, ry = 0.5 (DataF)\nrz = 0, ry = 0.5 (DataM)\nrz = 1, ry = 0.5 (DataF)\nrz = 0, ry = 0.5 (DataM)\nRaw\nr\u2032\ny = 1\nr\u2032\ny = 0\nRaw\nr\u2032\ny = 1\nr\u2032\ny = 0\nRaw\nr\u2032\nz = 0.5\nr\u2032\nz = 0\nRaw\nr\u2032\nz = 0.5\nr\u2032\nz = 1\nTPRM\n0.7422\n0.7344\n0.5859\n0.6563\n0.6094\n0.4688\n0.7422\n0.7422\n0.7969\n0.6563\n0.6641\n0.6406\nTPRF\n0.6172\n0.5547\n0.3594\n0.3984\n0.3281\n0.2422\n0.6172\n0.5703\n0.6094\n0.3984\n0.4141\n0.4297\n\u2206TPR \u2193\n0.1250\n0.1797\n0.2266\n0.2578\n0.2813\n0.2266\n0.1250\n0.1719\n0.1875\n0.2578\n0.2500\n0.2109\nFPRM\n0.2656\n0.2500\n0.1406\n0.1719\n0.1484\n0.0938\n0.2656\n0.2734\n0.3125\n0.1719\n0.1719\n0.1797\nFPRF\n0.1016\n0.0859\n0.0078\n0.0234\n0.0234\n0.0000\n0.1016\n0.0703\n0.0469\n0.0234\n0.0391\n0.0547\n\u2206FPR \u2193\n0.1641\n0.1641\n0.1328\n0.1484\n0.1250\n0.0938\n0.1641\n0.2031\n0.2656\n0.1484\n0.1328\n0.1250\nAlgorithm 1 FCG Algorithm\n1: procedure STEP1: DIVERSE CLUSTERING\n2:\nall_idx_set =[]\n3:\nfor each gi in SG do\n4:\nselected_idx_set = []\n5:\nX = gi.get_all_data()\n6:\ncentroids = Kmeans(clusters = n).fit(X)\n7:\nfor each center in centroids do\n8:\ndis_set = distance(X,center)\n9:\nclosest = argsort(dis_set)[:m]\n10:\nselected_idx_set.extend(closest)\n11:\nend for\n12:\nall_idx_set.append(selected_idx_set)\n13:\nend for\n14:\nreturn SG\u2019.set(all_idx_set)\n15: end procedure\n16: procedure STEP2:UPDATE EVOL SCORE\n17:\nch = [Mpred, Mfair]\n18:\nfor each g\u2032\ni in SG\u2019 do\n19:\nshots = g\u2032\ni.genetic_algorithm(k)\n20:\nevol_score = CAL_SCORE(shots,ch)\n21:\nshots.update(evol_score)\n22:\nrepeat iters times\n23:\nend for\n24: end procedure\n25: function CAL_SCORE(shots,ch)\n26:\nYbase= LLM.predict(Xdev)\n27:\nBasepred, Basefair = eval(Ybase,Ydev, ch)\n28:\nYICL = LLM.predict(shots, Xdev)\n29:\nICLpred, ICLfair = eval(YICL, Ydev, ch)\n30:\n\u2206pred = max((ICLpred \u2212Basepred), p)\n31:\n\u2206fair = max((ICLfair \u2212Basefair), p)\n32:\nreturn \u03b1 \u00b7 \u2206pred + (1 \u2212\u03b1) \u00b7 \u2206fair\n33: end function\nEach subgroup in SG is clustered into n clusters, with m neighbors selected around each centre of the cluster. The filtered new subgroups are denoted as SG\u2019={g\u2032 1, g\u2032 2, g\u2032 3, g\u2032 4}. Genetic Evolution for Score Updates. Next, for each subgroup within SG\u2019, we select Kdemonstrations for iters times using the roulette wheel genetic evolutionary approach and validate their ICL performance on Ddev. The evolutionary method means that data with a higher score is more likely to be chosen in each round. The score is first set as the default initialisation score p and then updated as the average of EvolScore computed\nduring the iterations when the sample is selected. EvolScores integrates the performance of both prediction Mpred and fairness Mfair metrics, with \u03b1 serving as the trade-off coefficient. The metrics provide options for selecting either Accuracy or F-score as Mpred, and either Rdp or Reo as Mfair. EvolScores in SG\u2032 will be updated and then used for subsequent selecting iterations. In the testing stage on Dtest, demonstrations in SG\u2032 are ranked by their average EvolScores, enabling different ICL strategies to select the topperforming demonstrations from their corresponding subgroups. The detailed steps of FCG pseudocode is given in Algorithm 1. Figure 4 gives an example of the whole process of representative sample selections with FCG on Adult dataset.\nTable 4 presents the experimental results evaluating the debiasing performance of the proposed FCG algorithm. The experiments are performed on the Adult dataset, setting the number of clusters to n = 8 and the number of neighbors to m = 5. We start with an initial score of p = 0.05 and perform 10 iterations to update EvolScore. F-score and Reo are selected for calculating Evolscore and the balancing parameter \u03b1 is set to 0.5. Results show that demonstrations selected by FCG perform well, and greatly outperforming random sampling. It is worth noting that using a balanced set from minority samples continues to yield the best performance, proving our finding that prioritizing minority samples (rz = 1) remains an effective strategy in ICL. Besides the minority group, the improvements in accuracy and fairness also happen in the majority group, which affirms the value of considering both factors in FCG selections. Ablation Study. We implement ablation experiments to verify the utility of the two-step extracting process in FCG mitigation. In the ablation study,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/686f/686f9d2f-116f-4721-8a22-c32d32503adf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9595/95950c4e-5700-4366-a2d6-a40ad3fc6abf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Step 3 Select Demonstrations with Different Strategy</div>\n<div style=\"text-align: center;\">Figure 4: The Workflow of Fairness via Clustering-Genetic (FCG) on the Adult Dataset (ry = 1, all high-income; ry = 0, all low-income; ry = 0.5, balanced samples of high-income and low-income; rz = 1, all females; rz = 0, all males; rz = 0.5, balanced samples of females and males.)</div>\nTable 4: The comparative analysis of the predictive and fairness performance achieved by various LLMs with demonstrations selected using the proposed FCG algorithm. The experiments are conducted on the Adult dataset. The table highlights that the FCG algorithm enhances fairness across almost all strategies.\nZero-shot\nK-Shot (K=8)\nK-Shot (K=4)\nZero-shot\nry = 0.5 (Balanced Labels)\nry = 1\nry = 0\nry = 0.5 (Balanced Labels)\nry = 1\nry = 0\nPrediction\nBaseline\nrz = 0.5\nrz = 0\nrz = 1\nrz = 1\nrz = 1\nrz = 0.5\nrz = 0\nrz = 1\nrz = 1\nrz = 1\nAccuracy \u2191\n0.6855\n0.7344\n0.7363\n0.7793\n0.7500\n0.7754\n0.7656\n0.7500\n0.7656\n0.7539\n0.7578\nPrecision \u2191\n0.8519\n0.7174\n0.6997\n0.7360\n0.7207\n0.7640\n0.7297\n0.7222\n0.7194\n0.7338\n0.7200\nRecall \u2191\n0.4492\n0.7734\n0.8281\n0.8711\n0.8164\n0.7969\n0.8438\n0.8125\n0.8711\n0.7969\n0.8438\nF-score \u2191\n0.5882\n0.7444\n0.7585\n0.7979\n0.7656\n0.7801\n0.7826\n0.7647\n0.7880\n0.7640\n0.7770\nFairness\nBaseline\nrz = 0.5\nrz = 0\nrz = 1\nrz = 1\nrz = 1\nrz = 0.5\nrz = 0\nrz = 1\nrz = 1\nrz = 1\nRdp \u2191\n0.4063\n0.7692\n0.7719\n0.8938\n0.7576\n0.7919\n0.7515\n0.8000\n0.8675\n0.7707\n0.8072\nReo \u2191\n0.1111\n0.6250\n0.5690\n0.7021\n0.5577\n0.5750\n0.5094\n0.6000\n0.7059\n0.5417\n0.6800\n\u2206dp \u2193\n0.2227\n0.1406\n0.1523\n0.0664\n0.1563\n0.1211\n0.1641\n0.1250\n0.0859\n0.1406\n0.1250\n\u2206eo \u2193\n0.3203\n0.1406\n0.1953\n0.1094\n0.1797\n0.1328\n0.2031\n0.1563\n0.1172\n0.1719\n0.1250\npart of the samples are selected using the same flow of choosing the top K samples based on their EvolScores, while the other part is selected randomly. The results in Table 5 suggest: (1) Even when EvolScores are ignored when selecting partial samples, the results still outperform the raw random selection method (Random (g1 + g2)), thus proving the effectiveness of the clustering selection in the first stage. (2) Moreover, both ablation test FCG (g1) and FCG (g2) performed worse compared to the results using complete FCG (g1 + g2), further confirming the need for the second stage of genetic selection based on EvolScore scoring.\n# 6 Conclusions\nIn this paper, we investigate how the choice of demonstrations impacts the fairness of LLMs on tabular data classification tasks when using incontext learning. Through experiments, we found that prioritizing underrepresented groups and including minority examples in the few-shot demonstrations can significantly enhance fairness performance, without sacrificing predictive accuracy. Further analysis revealed that increasing the propor-\nTable 5: The Ablation Study of FCG under Balanced Labels in Minority Group Strategy (S2) for Selecting K Demonstrations (K=8). S2 strategy is based on minority group (z = 1) with two possible labels y = {0, 1}, The corresponding subgroups are denoted as g1(z = 1, y = 0) and g2(z = 1, y = 1).\nShots from g1\nRandom\nTop K/2\nTop K/2\nRandom K/2\nShots from g2\nRandom\nTop K/2\nRandom K/2\nTop K/2\nPrediction\nRandom (g1 + g2)\nFCG (g1 + g2)\nFCG (g1)\nFCG (g2)\nAccuracy \u2191\n0.7480\n0.7793\n0.7500\n0.7559\nPrecision \u2191\n0.7592\n0.7360\n0.7013\n0.7079\nRecall \u2191\n0.7266\n0.8711\n0.8711\n0.8711\nFscore \u2191\n0.7425\n0.7979\n0.7770\n0.7811\nFairness\nRandom (g1 + g2)\nFCG (g1 + g2)\nFCG (g1)\nFCG (g2)\nRdp \u2191\n0.7254\n0.8938\n0.8276\n0.8208\nReo \u2191\n0.4390\n0.7021\n0.6964\n0.6140\n\u2206dp \u2193\n0.1523\n0.0664\n0.1172\n0.1211\n\u2206eo \u2193\n0.1797\n0.1094\n0.1328\n0.1719\ntion of underrepresented labels improves fairness metrics like demographic parity and equal odds. To efficiently retrieve effective demonstrations, we proposed the FCG algorithm that uses clustering and genetic evolution to select a diverse and representative set of examples from the training data. Across multiple strategies and datasets, experimental results indicate that FCG was able to improve fairness compared to random sampling.\n# 7 Limitations\nWhile our study presents significant advancements in understanding and improving fairness in LLMs using in-context learning (ICL), several limitations should be noted. Firstly, we equally weigh fairness and prediction performance in evaluating representative demonstrations using our Fairness via Clustering-Genetic (FCG) algorithm, which might not align with real-world applications that require a dynamic balance between these metrics. Additionally, our focus on binary classification with a single sensitive feature limits the broader applicability of our findings. In future, we plan to explore LLM\u2019s intersectional fairness and its performance in multi-classification tasks. Lastly, while we used pre-trained models without fine-tuning, investigating how fine-tuning on curated samples impacts fairness could provide deeper insights.\n# References\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. 2022. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507\u2013520. Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. 2023. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549\u20135581. PMLR. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023. Language is not all you need: Aligning perception with language models. Tenghao Huang, Faeze Brahman, Vered Shwartz, and Snigdha Chaturvedi. 2021. Uncovering implicit gender bias in narratives through commonsense inference. arXiv preprint arXiv:2109.06437. Michael P Kim, Amirata Ghorbani, and James Zou. 2019. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247\u2013254. Xiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6219\u20136235.\nercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai,\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804. Yanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. 2023. Investigating the fairness of large language models for predictions on tabular data. arXiv preprint arXiv:2310.14607. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456. Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. Crows-pairs: A challenge dataset for measuring social biases in masked language models. OpenAI. 2023. gpt-4-turbo, gpt-3.5-turbo, text-davinci003. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633. Timo Schick and Hinrich Sch\u00fctze. 2020. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676. Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing. arXiv preprint arXiv:2210.13693. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 819\u2013862, Dublin, Ireland. Association for Computational Linguistics. Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy Chakraborty. 2023. Multilingual llms are better cross-lingual in-context learners with alignment. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. arXiv preprint arXiv:2306.11698. Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang, Weifeng Chiu, Tao Wei, and Kui Ren. 2022. Fairnessaware adversarial perturbation towards bias mitigation for deployed deep models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10379\u201310388. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. What Makes In-Context Learning Work. Rethinking the role of demonstrations: What makes in-context learning work? I-Cheng Yeh. 2016. default of credit card clients. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C55S3H. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. Guanhua Zhang, Yihua Zhang, Yang Zhang, Wenqi Fan, Qing Li, Sijia Liu, and Shiyu Chang. 2022. Fairness reprogramming. Advances in Neural Information Processing Systems, 35:34347\u201334362. Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR.\n# A Related Work\n# A.1 Fairness in Large Language Model\nAddressing social biases is crucial for ensuring the trustworthiness of language models (Nangia et al., 2020; Nadeem et al., 2020). LLMs face similar fairness issues: many studies have confirmed that LLMs can capture social biases from unprocessed training data and transmit these biases to downstream tasks (Abid et al., 2021b; Brown et al., 2020; Wang et al., 2023). Abid et al. (2021a) addressed the issue of GPT-3\u2019s output displaying bias to Muslims. Huang et al. (2021) found that bias in LLMs\u2019 responses exists even without prompts explicitly asking about it. Liang et al. (2023) tested bias and stereotypes on LLMs using the BBQ dataset for question answering, finding that most models exhibit biases distinct from broader societal trends. Wang et al. (2023) assesses bias by prompting GPT models to express their views on stereotypes. Most bias studies have focused on representational harms caused by LLM generations, with only limited studies (Hegselmann et al., 2023) addressing fairness concerns classification problems with tabular datasets. Besides investigation on pre-trained LLMs, recent research also focuses on ensuring fairness in other trained machine learning models, such as perturbation-based (Zhang et al., 2022; Wang et al., 2022) and boosting-based (Kim et al., 2019) approaches.\n# A.2 In Context Learning\nIn-context learning (ICL), known as few-shot learning, allows LLMs to learn tasks with minimal examples as demonstrations (Dong et al., 2022; Zhao et al., 2021). Positive impacts of ICL on LLMs have been observed in different tasks such as text classification and answering (Gao et al., 2021; Liu et al., 2021), images generations (Bar et al., 2022), speech tasks (Zhang et al., 2023), and multi-modal scenarios (Huang et al., 2023; Wei et al., 2022). Meanwhile, researchers have found that the performance of ICL is highly sensitive to the demonstration prompt (Chen et al., 2023; Lu et al., 2021; Zhao et al., 2021; Shi et al., 2022). Investigations have explored factors that can influence ICL prediction performance, including demonstration retrievals (Tanwar et al., 2023; Sorensen et al., 2022), orderings (Lu et al., 2022), and input-label mapping (Yoo et al., 2022; Work). Demonstration Retrievals. A common demonstration retrievals approach in ICL involves ran-\ndomly selecting a subset of examples from the training set (Brown et al., 2020). Given the sensitivity of LLMs to the prompts, there has been investigation into selecting representative samples to enhance outcomes. Selecting the top-K training examples is one mitigation method and has been demonstrated in semantic parsing (Rubin et al., 2021) and semantic classification (Chang and Jia, 2023). LENS (Li and Qiu, 2023) proposed a two-step filter and search algorithm to identify informative support examples. Despite these advances, these retrieval techniques often focus solely on prediction performance and overlook the aspect of fairness. Additionally, most retrieval methods often require extensive experimental iterations, with significant time and resource investment.\n# B Dataset\nTable 6 describes the data structure in the Default Credit dataset. We calculate the mean values of PAY_AMT_i and BILL_AMT_i, and merge them into Avg_PAY_AMT and Avg_BILL_AMT separately. The raw Adult dataset shown in Table 7 contains 14 features, excluding education-num, fnlwgt, race, and native-country for this experiment. \u2018> 50K\u2019 and \u2018\u226450K\u2019 is mapped to \u2018greater than 50K\u2019 and \u2018less than or equal to 50K\u2019 respectively, for better alignment with the language model. In analysis, we call high income if the person\u2019s annual income is over 50K and low income if it is less than 50K. The size ratio of Dtrain: Ddev: Dtest is 9:1:10 in both two datasets. K demonstrations are extracted from Dtrain, 60 samples are extracted from Ddev, 512 samples for Dtest. We consider the balanced group and balanced labels scenario and extract samples with parameter random_seed=42.\n# C Prompt Architecture\nWe consider both zero-shot learning and few-shot learning (in-context learning). Zero-shot strategy combines task description and question as its prompt content without providing examples. Few-shot strategy includes three roles, and the incontext content is generated based on selected Kdemonstrations using different strategies (Table 10). The default value of K is set to 8, the case of K=4 is disscussed in Section 5. Table 8 and 9 provide templates for few-shot learning in the Adult and Credit datasets respectively.\nFeature\nLIMIT_BAL\nAmount of given credit\n Continuous; NT dollars\nSEX\nGender\n 2 categories, male / female\nEDUCATION\nHighest education\n 6 categories; graduate / high school /\nuniversity / etc\nMARRIAGE\nMarital status\n  6 categories of ; married / single / others\nAGE\n Age in years\n Continuous\nPAY_i\n10 categories of repayment status for\neach month;  ; pay duly / delay for one\nmonth / delay for two months / etc\nBILL_AMT_i\n Amount of bill statement for each month;\nContinuous; NT dollar\nPAY_AMT_i\n Amount of previous payment  for each\nmonth; Continuous; NT dollar\ndefault_payment_\nnext_month\nIf default payment\nnext month\nYes, overdue / no, on-time\ni\u2208{1,2,3,4,5,6} ,\nrepresents the\nmonth from April\n(6) to September\n(1) in 2005.\nDefault Credit Dataset - Description\n# D More Experimental Results\nTables below present additional detailed results not listed in the main text.\n# E Discussion between Our FCG and LENs Algorithm\nOur proposed FCG shares a similar architecture with LENs, another demonstration selection method. Here, we discuss these two methods and further explore the possibility of combining them. Given the time consumption of LENs, we simplified it by setting batch size to 8, and template index to {0,1}. The training dataset is randomly split (seed = 42) into groups of 500 samples for parallel computation, with other settings kept at their defaults. For LENs with FCG, we follow our FCG parameters setting: first extract 160 representative samples by clustering, then perform LENs to find the final candidates. Figure 5 presents the overall workflow of FCG, LENs, and their possible combination. Both FCG and LENs involve two steps: (1) selecting partial data and (2) searching for the optimal based on the filtered data. There are two key differences in implementations. Supervised & Unsupervised LENs algorithm uses LLMs as the classification assistance in both stages. This is a straightforward and effective way. However, since the processing time of language models is related to the amount of information in the input text, the selection time can become very\nlong when the input data is lengthy. This study focuses on tabular datasets, which have longer text when converted into prompts compared to commonly used NLP datasets. Therefore, we consider to optimize the method to reduce processing time and improve efficiency. Our FCG replaces LLMs with simpler unsupervised algorithms in the first stage. On the adult dataset, LENs can take over 50 hours to find supportive demonstrations (batchsize = 8), while FCG takes less than 3 hours (K = 4). The result in Table 17 validates the effectiveness: even if LLMs are not used initially, using LLMs to search on the validation set in the second stage can still find demonstrations that improve the model\u2019s prediction. Fairness Awareness Another difference is that LENs use accuracy as the sole evaluation metric when selecting demonstrations. Our FCG takes sensitive features into account and selects demonstrations at the subgroup level. Additionally, FCG considers both accuracy and fairness metrics as constraints when calculating performance scores. Table 17 confirms FCG with minority demonstrations prioritised strategy (rz = 1) shows fairer performance than baseline. Furthermore, we extend LENs with FCG (as shown on the right side of Figure 5) to make it fairness-aware. Table 17 proves the effectiveness of this combination and also shows the best performance achieved when using more minority demonstrations (rz = 1).\nFeature\nAdult Income Dataset - Description\nAge\n Age in years; Continuous\nWorkclass\n8 general types of employment; private / self-employed / government / etc\nEducation\n16 categories of highest level of education; college / bachelors / masters / etc.\nMarital-Status\n7 categories; married / divorced / separated / single / etc.\nOccupation\n15 categories; prof-specialty / craft-repair / Sales  / etc\nRelationship\n6 categories; not-in-family / husband / wife / etc\nSex\n2 categories; the biological sex; male / female\nCapital-Gain\nPerson's capital gains; Continuous\nCapital-Loss\nPerson's capital losses; Continuous\nHours-Per-Week\nHours worked per week; Continuous\nSalary\n2 categories of whether annual income exceeds $50K;  >50K / <=50K\n<div style=\"text-align: center;\">Adult Income Dataset - Description</div>\n<div style=\"text-align: center;\">Table 8: Few-shot Learning Templates for Adult Dataset</div>\nRoles\nPrompting Templates for Adult Income Dataset\nDescription\n<Task> Predict if income exceeds $50K per year. Answer with one of the following:                      \n[greater than 50K] | [less than or equal to 50K] </Task> \nIn-context \n</Example>Example 1:age is 40, and workclass is Private, and education is HS-grad, and marital-status is \nMarried-civ-spouse, and occupation is Sales, and relationship is Husband, and sex is Male, and capital-gain \nis 0, and capital-loss is 0, and hours-per-week is 60, and income is <=50K; </Example> \n<Example>Example 2 ......</Example> </Example>\nQuestion\n<Input> Age is 19, and workclass is Private, and education is Some-college, and marital-status is Never-\nmarried, and occupation is Other-service, and relationship is Own-child, and sex is Female, and capital-gain \nis 0, and capital-loss is 0, and hours-per-week is 15, please answer the income: </Input>\n<div style=\"text-align: center;\">Table 9: Few-shot Learning Templates for Credit Dataset</div>\nRoles\nPrompting Templates for Default Credit Dataset\nDescription\n<Task> Predict if the following data will default payment next month. Answer with one of the following:    \n[No] | [Yes] </Task>\nIn-context \n<Example> Example 1: Amount of given credit is 490000, and SEX is male, and EDUCATION is \ngraduate school, and MARRIAGE is married, and AGE is 45, and PAY_0 is pay duly,\u2026..., and default \npayment next month is No, on-time;</Example> <Example> Example 2: \u2026...</Example> \nQuestion\n<Input> Amount of given credit is 90000, and SEX is female, and EDUCATION is university, and \nMARRIAGE is married, and AGE is 49, and PAY_0 is delay for one month,\u2026..., and predict whether \ndefault payment: </Input> \nDefault Credit Dataset\nAdult Income Dataset\nStrategy\nAnnotation\nA balanced 1/4 ratio of female-overdue : female-\non-time : male-overdue : male-on-time\nA balanced 1/4 ratio of female-low-income : female-high-\nincome : male-low-income : male-high-income.\nS1\n\ud835\udc5f! = 0.5, \ud835\udc5f\" = 0.5\nPrioritize minority with balanced 1/2 ratio of \nfemale-overdue: female-on-time\nPrioritize minority with balanced 1/2 ratio of \nfemale-low-income : female-high-income\nS2\n\ud835\udc5f! = 1, \ud835\udc5f\" = 0.5\nPrioritize minority with imbalanced targets. \nAll female-on-time samples\nPrioritize minority with imbalanced targets. \nAll female-low-income samples\nS3\n\ud835\udc5f! = 1, \ud835\udc5f\" = 0\nPrioritize minority with imbalanced targets. \nAll female-overdue samples\nPrioritize minority with imbalanced targets. \nAll female-high-income samples\nS3\n\ud835\udc5f! = 1, \ud835\udc5f\" = 1\n<div style=\"text-align: center;\">Table 11: Different LLMs performance on Default Credit Dataset</div>\nText - Davinci - 3\nGPT 3.5 - turbo\nGPT 4 - turbo\nZero-shot\nS1\nS2\nS3\nZero-shot\nS1\nS2\nS3\nZero-shot\nS1\nS2\nS3\nAccuracy\n0.5449 \n0.6230 \n0.5996 \n0.6641 \n0.6250 \n0.6562 \n0.6484 \n0.6543 \n0.6602 \n0.6758 \n0.6719 \n0.6582 \nF-score\n0.4453 \n0.6030 \n0.5545 \n0.6641 \n0.5947 \n0.6453 \n0.6413 \n0.6543 \n0.6579 \n0.6758 \n0.6716 \n0.6578 \n\u0394dp\n0.0117 \n0.0586 \n0.0039 \n0.0391 \n0.0391 \n0.0313 \n0.0234 \n0.0430 \n0.2969 \n0.0547 \n0.0469 \n0.0195 \n\ud835\udc79\ud835\udc85\ud835\udc91\n0.8571 \n0.8077 \n0.9787 \n0.9254 \n0.8413 \n0.9080 \n0.9368 \n0.9160 \n0.4759 \n0.8955 \n0.9155 \n0.9590 \n\u0394eo\n0.0234 \n0.1016 \n0.0547 \n0.1016 \n0.0938 \n0.1016 \n0.1094 \n0.1328 \n0.3125 \n0.1250 \n0.0938 \n0.1094 \n\ud835\udc79\ud835\udc86\ud835\udc90\n0.8235 \n0.5000 \n0.5000 \n0.7400 \n0.7647 \n0.7917 \n0.7419 \n0.8132 \n0.2941 \n0.8298 \n0.8750 \n0.7955 \nClaude-3-haiku\nClaude-3-sonnet\nZero-shot\nrz = 0\nrz = 0.5\nrz = 1\nZero-shot\nrz = 0\nrz = 0.5\nrz = 1\nAccuracy\n0.7285\n0.7070\n0.7012\n0.7031\n0.6641\n0.7266\n0.7383\n0.7520\nPrecision\n0.7489\n0.8118\n0.7210\n0.7047\n0.6556\n0.7302\n0.7364\n0.7699\nRecall\n0.6875\n0.5391\n0.6563\n0.6992\n0.6914\n0.7188\n0.7422\n0.7188\nF-score\n0.7169\n0.6479\n0.6871\n0.7020\n0.6730\n0.7244\n0.7393\n0.7434\nZero-shot\nrz = 0\nrz = 0.5\nrz = 1\nZero-shot\nrz = 0\nrz = 0.5\nrz = 1\n\u2206dp\n0.2305\n0.1797\n0.1836\n0.1563\n0.0625\n0.1484\n0.1094\n0.1445\nRdp\n0.5986\n0.5741\n0.6643\n0.7279\n0.8881\n0.7379\n0.8042\n0.7319\n\u2206eo\n0.2344\n0.2188\n0.2031\n0.1641\n0.0703\n0.1563\n0.1094\n0.1719\nReo\n0.3409\n0.2800\n0.5116\n0.5625\n0.8235\n0.5455\n0.6585\n0.5714\n<div style=\"text-align: center;\">Table 13: Performance of GPT3.5-turbo on the Adult Dataset through Few-shot Strategies (S1) with 5 random seed</div>\nZero-shot\nS1 (\ud835\udc93\ud835\udc9b= \ud835\udfce. \ud835\udfd3, \ud835\udc93\ud835\udc9a= \ud835\udfce. \ud835\udfd3) Few-shot with Different Random Seed\nFew-shot Summary\nBaseline\nSeed 25\nSeed 35\nSeed 42\nSeed 45\nSeed 55\nAVG.\nSE\nHighest\nLowest\nAccuracy\n0.6855 \n0.7285 \n0.7363 \n0.7363 \n0.7266 \n0.7285 \n0.7313 \n0.0009 \n0.7363 \n0.7266 \nPrecision\n0.8519 \n0.7940 \n0.7980 \n0.8010 \n0.7871 \n0.7882 \n0.7937 \n0.0012 \n0.8010 \n0.7871 \nRecall\n0.4492 \n0.6172 \n0.6328 \n0.6289 \n0.6211 \n0.6250 \n0.6250 \n0.0012 \n0.6328 \n0.6172 \nF-score\n0.5882 \n0.6945 \n0.7059 \n0.7046 \n0.6943 \n0.6972 \n0.6993 \n0.0011 \n0.7059 \n0.6943 \nFairness\nBaseline\nSeed 25\nSeed 35\nSeed 42\nSeed 45\nSeed 55\nAVG.\nSE\nHighest\nLowest\n\u0394dp\n0.2227 \n0.1758 \n0.1680 \n0.1680 \n0.1641 \n0.1680 \n0.1688 \n0.0009 \n0.1758 \n0.1641 \n\ud835\udc79\ud835\udc85\ud835\udc91\n0.4063 \n0.6311 \n0.6504 \n0.6475 \n0.6557 \n0.6504 \n0.6470 \n0.0019 \n0.6557 \n0.6311 \n\u0394eo\n0.3203 \n0.2031 \n0.1875 \n0.1797 \n0.1797 \n0.1875 \n0.1875 \n0.0019 \n0.2031 \n0.1797 \n\ud835\udc79\ud835\udc86\ud835\udc90\n0.1111 \n0.3667 \n0.3667 \n0.3333 \n0.3871 \n0.3871 \n0.3682 \n0.0044 \n0.3871 \n0.3333 \n<div style=\"text-align: center;\">Table 14: Performance of GPT3.5-turbo on the Adult Dataset through Few-shot Strategies (S2) with 5 random seeds</div>\nZero-shot\nS2 (\ud835\udc93\ud835\udc9b= \ud835\udfcf, \ud835\udc93\ud835\udc9a= \ud835\udfce. \ud835\udfd3) Few-shot with Different Random Seed\nFew-shot Summary\nBaseline\nSeed 25\nSeed 35\nSeed 42\nSeed 45\nSeed 55\nAVG.\nSE\nHighest\nLowest\nAccuracy\n0.6855 \n0.7207 \n0.7207 \n0.7480 \n0.7266 \n0.7480 \n0.7328 \n0.0028 \n0.7480 \n0.7207 \nPrecision\n0.8519 \n0.7958 \n0.8192 \n0.7592 \n0.7589 \n0.7873 \n0.7841 \n0.0051 \n0.8192 \n0.7589 \nRecall\n0.4492 \n0.5938 \n0.5664 \n0.7266 \n0.6641 \n0.6797 \n0.6461 \n0.0130 \n0.7266 \n0.5664 \nF-score\n0.5882 \n0.6801 \n0.6697 \n0.7425 \n0.7083 \n0.7296 \n0.7060 \n0.0062 \n0.7425 \n0.6697 \nFairness\nBaseline\nSeed 25\nSeed 35\nSeed 42\nSeed 45\nSeed 55\nAVG.\nSE\nHighest\nLowest\n\u0394dp\n0.2227 \n0.1680 \n0.1445 \n0.1523 \n0.1797 \n0.1445 \n0.1578 \n0.0031 \n0.1797 \n0.1445 \n\ud835\udc79\ud835\udc85\ud835\udc91\n0.4063 \n0.6325 \n0.6542 \n0.7254 \n0.6593 \n0.7132 \n0.6769 \n0.0080 \n0.7254 \n0.6325 \n\u0394eo\n0.3203 \n0.2344 \n0.1641 \n0.1797 \n0.1875 \n0.1641 \n0.1859 \n0.0058 \n0.2344 \n0.1641 \n\ud835\udc79\ud835\udc86\ud835\udc90\n0.1111 \n0.5000 \n0.3333 \n0.4390 \n0.4211 \n0.3824 \n0.4152 \n0.0125 \n0.5000 \n0.3333 \nZero-shot\nS3 (\ud835\udc93\ud835\udc9b= \ud835\udfcf, \ud835\udc93\ud835\udc9a= \ud835\udfcf) Few-shot with Different Random Seed\nFew-shot Summary\nBaseline\nSeed 25\nSeed 35\nSeed 42\nSeed 45\nSeed 55\nAVG.\nSE\nHighest\nLowest\nAccuracy\n0.6855 \n0.7168 \n0.7168 \n0.7324 \n0.7285 \n0.7207 \n0.7230 \n0.0014 \n0.7324 \n0.7168 \nPrecision\n0.8519 \n0.7707 \n0.7681 \n0.7847 \n0.8128 \n0.7678 \n0.7808 \n0.0038 \n0.8128 \n0.7678 \nRecall\n0.4492 \n0.6172 \n0.6211 \n0.6406 \n0.5938 \n0.6328 \n0.6211 \n0.0036 \n0.6406 \n0.5938 \nF-score\n0.5882 \n0.6855 \n0.6868 \n0.7054 \n0.6862 \n0.6938 \n0.6915 \n0.0017 \n0.7054 \n0.6855 \nFairness\nBaseline\nSeed 25\nSeed 35\nSeed 42\nSeed 45\nSeed 55\nAVG.\nSE\nHighest\nLowest\n\u0394dp\n0.2227 \n0.1445 \n0.1445 \n0.1289 \n0.1758 \n0.1836 \n0.1555 \n0.0046 \n0.1836 \n0.1289 \n\ud835\udc79\ud835\udc85\ud835\udc91\n0.4063 \n0.6942 \n0.6967 \n0.7273 \n0.6121 \n0.6357 \n0.6732 \n0.0095 \n0.7273 \n0.6121 \n\u0394eo\n0.3203 \n0.1875 \n0.1563 \n0.1563 \n0.2188 \n0.2344 \n0.1906 \n0.0071 \n0.2344 \n0.1563 \n\ud835\udc79\ud835\udc86\ud835\udc90\n0.1111 \n0.5667 \n0.4118 \n0.5517 \n0.3462 \n0.4848 \n0.4722 \n0.0187 \n0.5667 \n0.3462 \n<div style=\"text-align: center;\">able 16: The Ablation Study with Balanced Labels in Minority Group (S2) under FCG Selections on GPT-3.5-turbo. he corresponding subgroups are denoted as g1(z = 1, y = 0) and g2(z = 1, y = 1).</div>\nK-shots\nK=8\nK=4\ng2\nTop K/2\nTop K/2\nRandom K/2\nTop K/2\nTop K/2\nRandom K/2\ng1\nTop K/2\nRandom K/2\nTop K/2\nTop K/2\nRandom K/2\nTop K/2\nFCG (g1 + g2)\nFCG(g2)\nFCG (g1)\nFCG (g1 + g2)\nFCG (g2)\nFCG (g1)\nAccuracy\n0.7793\n0.7500\n0.7559\n0.7656\n0.7441\n0.7539\nPrecision\n0.7360\n0.7013\n0.7079\n0.7194\n0.7036\n0.7031\nRecall\n0.8711\n0.8711\n0.8711\n0.8711\n0.8438\n0.8789\nFscore\n0.7979\n0.7770\n0.7811\n0.7880\n0.7673\n0.7813\nFCG(g1 + g2)\nFCG(g2)\nFCG(g1)\nFCG(g1 + g2)\nFCG(g2)\nFCG(g1)\n\u2206dp\n0.0664\n0.1172\n0.1211\n0.0859\n0.1133\n0.1016\nRdp\n0.8938\n0.8276\n0.8208\n0.8675\n0.8274\n0.8497\n\u2206eo\n0.1094\n0.1328\n0.1719\n0.1172\n0.1172\n0.1328\nReo\n0.7021\n0.6964\n0.6140\n0.7059\n0.7170\n0.6964\n<div style=\"text-align: center;\">Table 17: ICL Performance of LLaMa-3-8b on the Adult Dataset (ry = 0.5) using Different Demonstration  Methods (LENs, FCG, and Combined).</div>\nLLaMa-3\nLENs (K = 4)\nFCG (Ours, K = 4)\nLENs with FCG (K = 4)\nBaseline\nrz = 0\nrz = 0.5\nrz = 1\nrz = 0\nrz = 0.5\nrz = 1\nAccuracy\n0.6270\n0.6406\n0.6680\n0.7148\n0.5957\n0.6543\n0.6504\nPrecision\n0.7211\n0.8462\n0.7389\n0.7778\n0.7426\n0.7687\n0.6103\nRecall\n0.4141\n0.3438\n0.5195\n0.6016\n0.2930\n0.4414\n0.8320\nF \u2212score\n0.5261\n0.4889\n0.6101\n0.6784\n0.4202\n0.5608\n0.7041\nBaseline\nrz = 0\nrz = 0.5\nrz = 1\nrz = 0\nrz = 0.5\nrz = 1\n\u2206dp\n0.1523\n0.1250\n0.1094\n0.0938\n0.1602\n0.1445\n0.0039\nRdp\n0.5806\n0.5294\n0.7308\n0.7838\n0.4225\n0.5978\n0.9943\n\u2206eo\n0.2344\n0.1719\n0.1172\n0.0938\n0.2109\n0.1953\n0.0469\nReo\n0.5588\n0.2308\n0.5161\n0.5714\n0.3000\n0.4783\n0.9155\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ccb/0ccb25b6-655c-4982-92c5-b250f51a8fcb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Left. FCG; (b) Middle. LENs; (c) Right. LENs with FCG</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of fairness in large language models (LLMs) when using in-context learning (ICL) for tabular data processing. Previous methods have shown effectiveness in predictive performance but have not sufficiently addressed fairness implications. This study aims to explore how varying demonstrations within ICL prompts can influence fairness outcomes, highlighting the need for a new approach to enhance both predictive performance and fairness.",
        "problem": {
            "definition": "The problem this paper aims to solve is the lack of fairness in LLM outputs when processing tabular data through in-context learning, specifically how different demonstration strategies impact fairness performance.",
            "key obstacle": "The core obstacle is the existing bias in LLM predictions that arises from the selection of demonstrations, which can lead to unfair outcomes for minority groups."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is that including a higher proportion of minority group demonstrations in the training data can enhance fairness in LLM outputs.",
            "opinion": "The proposed idea involves strategically selecting demonstrations that prioritize underrepresented samples to improve fairness without sacrificing predictive accuracy.",
            "innovation": "The innovation lies in the introduction of the Fairness via Clustering-Genetic (FCG) algorithm, which utilizes clustering and genetic strategies to select a diverse and representative set of demonstrations, differing from traditional methods that often overlook fairness."
        },
        "method": {
            "method name": "Fairness via Clustering-Genetic (FCG)",
            "method abbreviation": "FCG",
            "method definition": "The FCG method is designed to extract representative samples from training data to improve fairness in LLM outputs by balancing the representation of minority and majority groups.",
            "method description": "FCG combines clustering techniques with genetic algorithms to iteratively select demonstrations that enhance fairness and predictive performance.",
            "method steps": [
                "Cluster the training data into subgroups based on sensitive features and labels.",
                "Select candidate demonstrations from each cluster.",
                "Apply a genetic algorithm to score and select the most effective demonstrations based on performance metrics."
            ],
            "principle": "The effectiveness of the FCG method is grounded in the principle that a diverse and representative set of demonstrations can mitigate bias and enhance fairness in LLM predictions."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using five LLMs on two fairness-focused tabular datasets: Credit and Adult. The evaluation metrics included accuracy, precision, recall, F-score, and various fairness metrics such as demographic parity and equalized odds.",
            "evaluation method": "The performance of the LLMs was assessed by comparing the output with different demonstration strategies, measuring how the inclusion of minority samples affected both predictive accuracy and fairness metrics."
        },
        "conclusion": "The study concludes that prioritizing underrepresented samples in demonstrations significantly improves fairness in LLM outputs, while the proposed FCG algorithm effectively enhances both predictive performance and fairness across various strategies and datasets.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved fairness outcomes without sacrificing predictive accuracy and the efficient selection of demonstrations through the FCG algorithm.",
            "limitation": "A limitation of the study is that it equally weighs fairness and predictive performance, which may not align with all real-world applications requiring a different balance.",
            "future work": "Future research directions include exploring intersectional fairness in LLMs and the effects of fine-tuning on curated samples to gain deeper insights into fairness improvements."
        },
        "other info": {
            "additional details": {
                "datasets": "The datasets used include the Credit dataset, which focuses on credit card clients, and the Adult dataset, which predicts income levels based on demographic features.",
                "models tested": "The models tested include Text-davinci-003, GPT-3.5-turbo, GPT-4-turbo, Claude-3 Haiku, and Claude-3 Sonnet."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "This paper addresses the issue of fairness in large language models (LLMs) when using in-context learning (ICL) for tabular data processing, highlighting the need for a new approach to enhance both predictive performance and fairness."
        },
        {
            "section number": "3.1",
            "key information": "The core obstacle is the existing bias in LLM predictions that arises from the selection of demonstrations, which can lead to unfair outcomes for minority groups."
        },
        {
            "section number": "3.3",
            "key information": "The proposed Fairness via Clustering-Genetic (FCG) algorithm utilizes clustering and genetic strategies to select a diverse and representative set of demonstrations, differing from traditional methods that often overlook fairness."
        },
        {
            "section number": "5.1",
            "key information": "The experiments were conducted using five LLMs on two fairness-focused tabular datasets: Credit and Adult, assessing how the inclusion of minority samples affected both predictive accuracy and fairness metrics."
        },
        {
            "section number": "6.1",
            "key information": "The study concludes that prioritizing underrepresented samples in demonstrations significantly improves fairness in LLM outputs."
        }
    ],
    "similarity_score": 0.7365513847611033,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning.json"
}