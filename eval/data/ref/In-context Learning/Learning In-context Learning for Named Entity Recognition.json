{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.11038",
    "title": "Learning In-context Learning for Named Entity Recognition",
    "abstract": "Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function \u03bbinstruction, demonstrations, text.M1, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (\u03bb.M)(instruction, demonstrations) \u2192F where F will be a new entity extractor, i.e., F: text \u2192entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor. Experimental results on 4 few-shot NER datasets show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts.",
    "bib_name": "chen2023learningincontextlearningnamed",
    "md_text": "4University of Chinese Academy of Sciences, Beijing, China {jiawei2020,yaojie,hongyu,boxi2020,xianpei,sunle}@iscas.ac.cn {loujie,jiawei07,daidai,wu_hua}@baidu.com\n# Abstract\nNamed entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function \u03bbinstruction, demonstrations, text.M1, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (\u03bb.M)(instruction, demonstrations) \u2192F where F will be a new entity extractor, i.e., F: text \u2192entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor. Experimental results on 4 few-shot NER datasets show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts.\n# 1 Introduction\nNamed entity recognition (NER) aims to detect and classify named entities in text, such as People, Disease, and Movie. Traditional NER methods (Lample et al., 2016; Li et al., 2020c; Yan et al., 2021) have achieved remarkable success \u2217This work was partially done when Jiawei Chen interned at Baidu. \u2020Corresponding authors. 1This paper represents functions using lambdacalculus (Barendregt, 1992), and each function is represented as \u03bbx,y,z.M, where x, y, z are variables and M is function definition/abstraction. The function can apply to arguments such as (\u03bbx,y,z.M)(x = A, y = B, z = C) (fully applied) or (\u03bbx,y,z.M)(x = A, y = B) (partially applied).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d9c/6d9c82cd-c4e4-4f0a-b97f-899edb6c697b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">In-context NER</div>\nFigure 1: Illustration of in-context NER, which uses instruction, demonstrations, and text as input to identify entities. The in-context learning model can be regarded as a meta-function that takes instruction and demonstrations as input and produces an entity extractor capable of identifying the desired entities (Aky\u00fcrek et al., 2022). when entity types are pre-defined and massive highquality annotations are provided. Unfortunately, real-world NER still suffers from the diversity of entity types (e.g., the extraction of Movie is very different to Disease), the emergence of new entity types (e.g., Virus of Cov-19 ), and the lack of high-quality annotations. To address these problems, recent studies often employ few-shot learning techniques, including fine-tuning-based and metric-based methods. Finetuning-based methods extract entities of new types by adjusting model weights using new instances (Ma et al., 2022a; Chen et al., 2022a; Das et al., 2022). The main drawbacks of these methods are that re-training is often expensive (especially for large-scale models) and new entity types cannot be addressed on-the-fly. Metric-based methods are free from updating parameters and identifying entities by learning to compare query instances with support instances (or prototypes) (Yang and Katiyar, 2020; Tong et al., 2021). These methods are limited to the matching architectures and are sensitive to domain shift since they do not fully explore the information of target domain (Ma et al., 2022c). In this paper, we propose an in-context learning-\nbased NER approach, which can effectively address the above problems by injecting in-context NER ability into PLMs and then recognizing entities of new types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function (Aky\u00fcrek et al., 2022) for NER \u03bbinstruction, demonstrations, text.M, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (\u03bb.M)(instructions, demonstrations) \u2192F where F will be a new entity extractor F: text \u2192entities. For example, in Figure 1, our method can construct entity extractors of new Disease and Virus types on-the-fly by applying PLMs using demonstrations such as \u201cText: Cancer is a leading cause of death worldwide. Entities: Cancer is disease\u201d. Furthermore, we propose a meta-function pre-training algorithm to inject the above in-context NER ability into PLMs. The algorithm pre-trains PLMs by comparing the implicitly (instruction, demonstration)constructed extractor with an explicitly fine-tuned surrogate golden extractor. The comparison ensures that the meta-function (\u03bb.M) will generate an entity extractor F from instructions and demonstrations as accurately as possible. The proposed method can seamlessly leverage the powerful language understanding and generation capabilities of large-scale PLMs (Brown et al., 2020), effectively address diverse and new entity types through in-context learning, and only requires a couple of demonstrations for each entity type. Compared to fine-tuning methods, our method does not require expensive retraining, and new entity types can be extracted on-the-fly, with no need for model weight adjusting. Compared with metricbased methods, our method can dynamically utilize the information entailed in instruction and demonstrations rather than be limited to the fixed metric space. To verify the effectiveness of our method, we further pre-train PLMs using a large-scale distantly annotated NER dataset from Wikipedia and Wikidata. Experimental results on 4 few-shot NER benchmarks show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts2. In general, this paper\u2019s main contributions are: \u2022 We propose an in-context NER method that\n\u2022 We propose an in-context NER method that can effectively extract entities of novel types\non-the-fly using only a few demonstrative in stances.\n We design a meta-function pre-training algorithm, which models PLMs as a meta-function and injects in-context NER ability into PLMs by comparing the (instruction, demonstration)constructed extractor with a surrogate golden extractor.\n\u2022 How to inject in-context ability into small models is an important research direction of NLP in the big model era. Our work can benefit new directions for future works.\n# 2 Related work\nFew-shot NER Few-shot learning is a promising technique for low-resource NER. Currently, there are two main categories of FS-NER methods: fine-tuning-based methods and metric-based methods. Fine-tuning-based FS-NER methods re-train NER models using new instances. Metric-based methods identify entities by pre-training to compare query instances with support instances (Snell et al., 2017; Fritzler et al., 2019; Yang and Katiyar, 2020; Tong et al., 2021; Wang et al., 2022; Ji et al., 2022) using given NER datasets. FS-NER is a challenging task, and several improvements have been proposed to enhance its performance. These include leveraging label information (Hou et al., 2020; Wang et al., 2021a; Lu et al., 2022b; Ma et al., 2022a; Chen et al., 2022a; Yang et al., 2022), designing new paradigms such as decomposition methods (Ji et al., 2022; Ma et al., 2022c; Yang et al., 2022), prompt-based methods (Cui et al., 2021; Liu et al., 2022; Ma et al., 2022b), and demonstration-based methods (Lee et al., 2022; Zhang et al., 2022a); , and proposing new learning strategies like meta-learning (Li et al., 2020a,b; de Lichy et al., 2021; Ma et al., 2022c), contrastive learning (Das et al., 2022), and self-training (Huang et al., 2021; Wang et al., 2021b). In this paper, we address FS-NER via in-context learning (Guti\u00e9rrez et al., 2022), which empowers PLMs with incontext learning ability and entities of new entity types can be extracted on-the-fly.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a954/a9547702-2c4a-4ae4-b061-6e0ccb49ef5a.png\" style=\"width: 50%;\"></div>\nFigure 2: The formats of input and output of in-context few-shot NER. The input is formed by instruction, demonstrations, and text.\naim to enhance in-context learning by selecting valuable demonstrations (Liu et al., 2021; Rubin et al., 2022), optimizing the order of demonstrations (Lu et al., 2022a), and calibrating output distributions (Zhao et al., 2021). Some studies try to replicate in-context learning in smaller models (Min et al., 2022a; Chen et al., 2022b). Additionally, some researchers attempt to replicate incontext learning using smaller models (Min et al., 2022b; Chan et al., 2022). Furthermore, there are efforts to understand the underlying mechanisms (Aky\u00fcrek et al., 2022) of in-context learning which suggest that it can be compared to a metafunction and facilitate implicit fine-tuning (Dai et al., 2022; von Oswald et al., 2022). This paper is inspired by previous studies and considers incontext named entity recognition (NER) as a metafunction. To enhance the ability of pre-trained language models (PLMs) to perform in-context NER, we propose an effective pre-training algorithm. Unlike MetaICL (Min et al., 2022a), which only transforms multi-task learning into the form of incontext learning for pre-training, our approach also includes meta-function pre-training (Section 4.3) based on the underlying mechanisms of in-context learning.\n# 3 In-context Named Entity Recognition\nThis section describes how to recognize entities through in-context NER. In in-context learning, the model will read the information of target entity types from both instruction and demonstrations, and then extract entities of target types within the text. In this way, new entity types can be extracted on-the-fly, without the need for model retraining.\nConcretely, this paper formulates in-context NER as a sequence-to-sequence generation process. The input X = [I; D; T] includes instruction I, demonstrations D, and text T while the output is a list of extracted entities Y = [e1, ..., en]. Figure 2 shows an example, where an in-context NER model will identify that the target entity types are Disease and Virus, distill the knowledge about these types from demonstrations(e.g., the context patterns of a disease), and finally recognize \"SARS-CoV-2\" as virus and \u201cCOVID-19\u201d as disease using the above knowledge. The details are described as follows. Instruction The instruction is a sequence of target entity types, guiding the model to extract what entity types (Min et al., 2022a). The instruction for target entity types {l1, . . . , ln} is I =\u201cTarget types: l1; . . . ; ln\u201d. For example, in Figure 2 the instruction is \u201cTarget types: disease; virus\u201d. Demonstrations Demonstrations provide the intra-class knowledge of target entity types (e.g., entity semantics and context patterns) and illustrate the form of outputs. As shown in Figure 2, the demonstrations contain the illustrative instances for different target types, and each instance is \u201cText: {text} Entities: {extractions}\u201d, where {extractions} are entities presented in the {text}. Extractions The output of the extraction process is a list of entities, denoted as Y = [e1, . . . , en] where ei is i-th extracted entities. Each extraction e is represented as \u201cENTITY is type\u201d. For instance, in Figure 2, the extraction \u201cCOVID-19 is disease.\u201d indicates that \u201cCOVID-19\u201d is an entity mention with the type \u201cDisease\u201d. This natural languagelike representation allows us to better utilize the text generation capabilities of pre-trained language models. During inference, we locate all mentions in the text and further output their locations. Architecture Given the above task formulation, we employ an encoder-decoder network like T5 (Raffel et al., 2020), where the encoder encodes <instruction, demonstrations, text> and the decoder generates all extractions as a tokenized text sequence Y = [y1, . . . , yn]. The success of in-context NER depends on two critical abilities: the in-context learning ability and the extraction ability. For in-context learning, the models should be able to implicitly construct accurate extractors of new entity types by following the instruction and capturing the knowledge in demon-\nExtractions The output of the extraction process is a list of entities, denoted as Y = [e1, . . . , en] where ei is i-th extracted entities. Each extraction e is represented as \u201cENTITY is type\u201d. For instance, in Figure 2, the extraction \u201cCOVID-19 is disease.\u201d indicates that \u201cCOVID-19\u201d is an entity mention with the type \u201cDisease\u201d. This natural languagelike representation allows us to better utilize the text generation capabilities of pre-trained language models. During inference, we locate all mentions in the text and further output their locations.\nArchitecture Given the above task formulation, we employ an encoder-decoder network like T5 (Raffel et al., 2020), where the encoder encodes <instruction, demonstrations, text> and the decoder generates all extractions as a tokenized text sequence Y = [y1, . . . , yn]. The success of in-context NER depends on two critical abilities: the in-context learning ability and the extraction ability. For in-context learning, the models should be able to implicitly construct accurate extractors of new entity types by following the instruction and capturing the knowledge in demon-\nstrations. In this way, we can see a PLM as a meta-function, i.e., a function of extractors whose input is (instruction, demonstrations) and whose output is an entity extractor. For extraction, the models should be able to locate specific spans and categorize them into target entity types. The following section demonstrates how to inject such an in-context learning ability into PLMs and construct an effective in-context NER model.\n# 4 Meta-Function Pre-training for In-Context NER\nIn this section, we will explain how to incorporate in-context named entity recognition (NER) capabilities into pre-trained language models (PLMs). Although large-scale PLMs like GPT-3 have demonstrated the ability to learn in-context, this capability is not always controllable or predictable. Additionally, unlike classification and question-answering tasks that align with the pre-training objective of language models (i.e., producing natural text output), NER requires more complex span extraction and type specification. As a result, Guti\u00e9rrez et al. (2022) show that LMs aren\u2019t well-suited for incontext NER tasks. In this paper, we propose metafunction pre-training, an algorithm that can inject in-context NER ability into PLMs in a controllable and predictable way. Specifically, we model PLMs as a metafunction (Aky\u00fcrek et al., 2022) for NER \u03bbinstruction, demonstrations, text.M, and a new entity extractor can be implicitly constructed by applying new instructions and demonstrations to PLMs, i.e., (\u03bb.M)(instructions, demonstractions) \u2192F where F will be a new entity extractor F:text \u2192entities. Based on the meta-function formulation, we further pre-train PLMs for in-context NER abilities by: \u2022 optimizing PLMs via a meta-function loss, so that the implicitly (instruction, demonstration)-constructed extractor F will be as close as an explicitly fine-tuned surrogate golden extractor; \u2022 optimizing PLMs via an extraction loss, so that the in-context NER can effectively locate and categorize entities in a text. The details are described in the following.\n# 4.1 Pre-training Settings\nPre-training Corpus Construction To continually pre-train PLMs for in-context NER, we first collect an in-context pre-training NER corpus\nDin-context = {x1, x2, ..., xn}, where each x is an n-context NER task represented as a tuple = (intruction, demonstrations, text, entities). Specifically, to sample in-context NER task x, we use traditional NER corpus DNER where each NER instance is a (text, entities) pair as follows: 1. In-context Task Sampling: To construct an in-context NER task x = (instruction, demonstrations, text, entities): (1) we first sample N target entity types from DNER to form instruction and sample K instances for each type to form demonstrations; (2) then we sample the text and the entities of x by either randomly sample an instance from N target entity types, or randomly sample from instances of other entity types, i.e., their extractions are NIL. We sample NIL instances because in real-world applications many instances will not contain target entities, and NIL instances are sampled with a predefined proportion \u03b3. 2. Type Anonymization: To ensure the models rely on in-context demonstrations for entity knowledge and avoid overfitting to entity type names, we anonymize entity types by randomly substituting them with a set of type indicators {<type1>, . . ., <type99>}, rather than directly using the original type names such as Disease and Virus. We found this anonymization strategy can significantly improve the in-context learning ability of PLMs. Specifically, we randomly substitute each entity type name with pre-defined 99 type indicators {<type1>, . . ., <type99>}, and the substitute probability for each name is 80%.\n# Pre-training Loss Based on the in-context pretraining corpus Din-context, we pre-train our incontext NER model by optimizing the loss:\nL = \u03b1Lmeta-function + Lextraction\n(1)\nwhere Lmeta-function is the meta-function loss which ensures PLMs can implicitly generate accurate entity extractors (Section 4.2), Lextraction is the extraction loss which ensures PLMs have good extraction ability (Section 4.3), \u03b1 is the coefficient of metafunction loss.\n# 4.2 Meta-function Pre-training\nAs mentioned above, a good in-context NER model should be able to implicitly construct an accurate entity extractor by partially applying PLMs with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/02d1/02d1bc79-85a5-49e4-a0f9-6cc8c3c46210.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Overview of our meta-function pre-training. Our goal is to ensure that the extractor F(instruction,demonstrations) closely resembles the golden extraction function. To obtain the golden extraction function, we use a surrogate strategy and the surrogate extraction function is the fine-tuned encoder using demonstrations.</div>\n# instruction I and demonstrations D:\n(2)\nFor example, given the instruction and demonstrations in Figure 2, we want PLMs to implicitly build an accurate extractor for Disease and Virus. Therefore if we know the golden extraction function F\u2217 for target entity types, we can optimize PLMs for in-context NER ability by minimizing the distance ||F\u2217\u2212F||. Unfortunately, the golden extraction function F\u2217is unknown. In this paper, we approximate F\u2217using a surrogate extractor which is the finetuned counterpart using demonstrations D. That is, for each in-context pre-training task x, we first recover all NER (text, entities) instances from x as x\u2032, then we fine-tune the model and use the fine-tuned encoder F\u2032 as the surrogate of F\u2217. The overall meta-function pre-training is shown in Figure 3. Formally, given instruction I, demonstration D, and text T, we first feed them into the encoder and obtain the feature of I and T,\nl1, ..., ln, d1, ..., dm, t1, ..., tk = Encoder(I; D; T) (3\n(3)\nThen we obtain the feature of the implicitly generated function F using the features of instruction I and text T, and ignore the features of D: F = [l1, ..., ln, t1, ..., tk]. In Figure 3, the feature F can be seen as the output of Disease and Virus extractor F. To obtain the feature of the fine-tuned counterpart, we perform a one-step gradient descent3 on\nthe encoder using the instances in the demonstration D and get the surrogate encoder, which can be seen as an approximation of golden F\u2217. Note that this fine-tuning operation is performed after the model has been copied, so there is no impact on the parameters of the original model. In the example in Figure 3, Encoder\u2032 is a Disease and Virus extractor. After performing one-step updating, we feed instruction and text [I; T] into the surrogate encoder to get their features:\n(4)\nwhere F\u2032 = {l\u2032 1, . . . , l\u2032 n, t\u2032 1, . . . , t\u2032 k} is features of instruction I and text T. In the example in Figure 3, the feature F\u2032 can be seen as the estimated output of golden extractor F\u2217for Virus and Disease entity types. Then, we pre-train our in-context NER model to be a good meta-function by making the output of F and F \u2217consistent, i.e., minimizing the distance between F and F\u2032. The meta-function loss is:\n(5)\nwhere d(\u00b7) is euclidean distance. Note that when calculating the gradient of Lmeta-function, F\u2032 is seen as constant. To this end, the meta-function gradient can be estimated as:\n(6)\nwhere \u03b8encoder is the parameters of the encoder and X = [I; D; T] is the input. The estimated gradient will be used to update the parameters of the encoder.\nIn this way, the in-context NER models will be trained to be a good meta-function (Aky\u00fcrek et al., 2022), which can also be seen as an ability for implicit fine-tuning (Dai et al., 2022; von Oswald et al., 2022).\n# 4.3 Extraction Function Pre-training\nBesides the in-context learning ability, we also pretrain PLMs to be good extractors via extraction loss. Given instruction I, demonstrations D, and text T, the sequence-to-sequence entity extractor directly models the generation probability token by token in an auto-regressive way. Formally, we optimize the model parameters \u03b8 by minimizing the negative likelihood of in-context instances:\n(7)\nAnd the extraction gradient is computed as:\n(8)\nTo learn the above extraction ability, we design two extraction pre-training tasks, including an entity extraction task and a pseudo extraction language modeling task: Entity Extraction Task. This task is used to train the ability to extract entities from text, we use both in-context NER settings whose input is (instruction, demonstrations, text) and traditional NER settings whose input is (instruction, text), and output is entities. Note that type anonymization is only conducted in in-context NER setting.\n# Pseudo Extraction Language Modeling Task\nBecause there is a mismatch between the entity extraction task and the original language modeling task, and the size of the NER corpus is usually far smaller than the text corpus for language modeling pre-training, we design a pseudo extraction LM task to bridge the above gap. Specifically, we randomly sample unlabeled sentences from the text corpus and automatically build pseudo extraction (instruction, demonstrations, text, pseudo entities) tasks. For instance, given a demonstration sentence such as \u201cI think this movie is cool and I really like it very much\u201d and a text \u201cI do not like it.\u201d: (1) To begin with, we choose some spans from demonstrations (such as \"this movie\" and \"like\") and designate them as pseudo entities4. We assign 4We introduce how to select spans in Appendix.\nrandom types to these entities from type indicators. For instance, we consider \"this movie\" as a pseudo entity of type <type2> and \"like\" as a pseudo entity of type <type14>. (2) The input of the pseudo extraction task is instruction=\"Target types:<type2>; <type14>\"; the demonstrations=\"Text: [MASK1] is cool and I really [MASK2] it [MASK3]. Entities: [MASK1] is <type2>. [MASK2] is <type14>\" where the entities (\u201cthis movie\u201d and \u201clike\u201d) and other random spans (\u201cvery much\u201d) in demonstrations are masked. The text=\u201cText: I do not like it.\u201d which is not masked. (3) The output of the pseudo extraction task is \u201clike is <type14>\u201d since the model will learn from demonstrations that <type14> corresponds to \"like\". (4) We also conduct traditional NER settings whose input is (instruction, text). The entities in the text will be masked as in demonstrations, e.g. \u201cTarget types: this movie; like Text: I [MASK1] not [MASK2] it.\u201d. The output will be \u201cEntities: [MASK2] is like.\u201d. We can see that the pseudo extraction LM task can benefit in-context NER in two ways. Firstly, it can significantly increase the size and diversity of in-context NER pre-training tasks from a largescale unlabeled corpus. Secondly, this task pretrains PLMs with a mixture of extraction target and span prediction task, therefore avoiding PLMs overfit to only extraction task. When pre-training, We transformed the NER and language model tasks into a uniform format and sampled input instances alternately.\n# 5 Experiments\nThis section evaluates our method by conducting experiments on few-shot NER settings.\n# 5.1 Experimental Settings\nPre-training settings. Following Chen et al. (2022a), we build a large-scale distant NER dataset by aligning Wikipedia and Wikidata. Specifically, our dataset was made from Wikipedia text with hyperlinks to Wikidata, where we labeled entity types using the linked Wikidata item\u2019s attributes. Entity types were gathered from Wikidata\u2019s SubclassOf and InstanceOf attributes for each span. We filtered ambiguous and low-frequency types (occurrences <100k) to obtain higher-quality demonstrations. Finally, we retained 2046 types and 55 million (text, entities) pairs and use a 40/15 million split for training/validation. We sample 5 million in-context tasks for training and 10k for valida-\nModels\n#Param\nCoNLL03\nWNUT17\nNCBI-disease\nSEC-filings\nAVE\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nPre-trained Language Models\nT5v1.1-large\n770M\n38.61\n44.90\n25.52\n26.32\n26.02\n37.63\n41.89\n53.44\n36.79\nGPT2-xl\n1.5B\n33.69\n39.55\n22.63\n24.86\n25.54\n33.25\n42.83\n57.05\n34.93\nT5-xl\n3B\n38.99\n45.74\n26.39\n26.31\n23.10\n36.78\n30.58\n42.22\n33.76\nGPT-J-6B\n6B\n46.14\n50.10\n31.41\n30.93\n35.82\n40.98\n40.12\n39.61\n39.39\nT5-xxl\n11B\n40.97\n46.14\n24.76\n25.27\n12.19\n26.34\n32.65\n42.44\n31.35\nOPT-13B\n13B\n46.65\n51.71\n27.74\n28.36\n23.73\n34.00\n41.60\n43.10\n37.11\nGPT-Neox-20B\n20B\n52.68\n58.12\n36.29\n35.68\n35.42\n42.85\n45.07\n45.17\n43.91\nOPT-30B\n30B\n42.86\n44.77\n25.85\n27.44\n22.31\n32.76\n40.83\n46.52\n35.42\nOPT-66B\n66B\n43.83\n53.89\n30.77\n32.00\n25.87\n34.58\n39.15\n47.01\n38.39\nPre-trained NER Models\nProtoNet\n345M\n30.04\n60.26\n9.74\n23.03\n24.73\n42.32\n16.79\n23.67\n28.82\nNNShot\n345M\n41.92\n58.39\n15.76\n21.78\n31.59\n33.14\n30.19\n37.86\n33.83\nStructShot\n345M\n42.34\n58.44\n15.78\n22.05\n19.87\n31.48\n30.40\n38.44\n32.35\nCONTAINER\n345M\n45.43\n61.69\n15.64\n20.37\n23.24\n27.02\n34.07\n40.44\n33.49\nMetaNER-base\n220M\n53.94\n62.59\n25.55\n30.41\n35.00\n37.24\n46.88\n51.39\n42.88\nMetaNER\n770M\n57.40\n63.45\n31.59\n36.52\n40.01\n44.92\n52.07\n54.87\n47.60\n<div style=\"text-align: center;\">Pre-trained NER Models</div>\nPre-trained NER Models\nProtoNet\n345M\n30.04\n60.26\n9.74\n23.03\n24.73\n42.32\n16.79\n23.67\n28.82\nNNShot\n345M\n41.92\n58.39\n15.76\n21.78\n31.59\n33.14\n30.19\n37.86\n33.83\nStructShot\n345M\n42.34\n58.44\n15.78\n22.05\n19.87\n31.48\n30.40\n38.44\n32.35\nCONTAINER\n345M\n45.43\n61.69\n15.64\n20.37\n23.24\n27.02\n34.07\n40.44\n33.49\nMetaNER-base\n220M\n53.94\n62.59\n25.55\n30.41\n35.00\n37.24\n46.88\n51.39\n42.88\nMetaNER\n770M\n57.40\n63.45\n31.59\n36.52\n40.01\n44.92\n52.07\n54.87\n47.60\nTable 1: Micro-F1 scores of 1-shot and 5-shot in-context NER on test set. For a fair comparison, the results of each model are based on a single frozen model without fine-tuning and the pre-trained NER models are pre-trained using the same dataset as MetaNER.\ntion, where each instance with type number N is 10 and instance number K is 10. We employ the T5-v1.1-large (Raffel et al., 2020) model as the initial model for MetaNER and further pre-train 500k steps with learning rate=5e-5 and warm-up steps=10k. In this paper, we refer to the pre-trained model as MetaNER. Few-shot settings. Our experiments follow the standard k-shot NER setting Huang et al. (2021): For each entity type, we sample k training instances as in-context demonstrations. We evaluate models by micro-F1 and report the average performance by repeating each experiment 10 times. We conducts experiments on 4 datasets across differnt domains: (1) CoNLL03 (Sang and Meulder, 2003) from news domain. (2) WNUT17 (Derczynski et al., 2017) from social media domain. (3) NCBI-disease (Do\u02d8gan et al., 2014) from biology domain. (4) SEC-filings (Alvarado et al., 2015) from finance domain. Baselines. For fair comparison, we use frozen models for all baselines in the in-context learning experiments, i.e., a pre-trained language/NER model is used for entity extraction without finetuning. In addition, we will discuss fine-tuning based methods in section 5.3.3. Two kinds of baselines are compared:\nBaselines. For fair comparison, we use frozen models for all baselines in the in-context learning experiments, i.e., a pre-trained language/NER model is used for entity extraction without finetuning. In addition, we will discuss fine-tuning based methods in section 5.3.3. Two kinds of baselines are compared:\n1) Pre-trained language models include models with different scales and architectures: (1) Encoderdecoder models \u2013 T5 models (Raffel et al., 2020), includes T5-v1.1-large (770M), T5-xl (3B) and T5xxl (11B). (2) Causal LM models \u2013 GPT and OPT models (Radford et al., 2019; Zhang et al., 2022b), includes GPT2-xl (1.5B), GPT-j-6B (Wang and Komatsuzaki, 2021), GPT-Neox-20B (Black et al., 2022), OPT-13B, OPT-30B and OPT-66B. Notice that, for PLMs, we use original type names rather than type indicators to capture the label semantics. For encoder-decoder models like T5, we formulate in-context NER as a span corruption task and the model will generate the extraction task. For example, for input \u201cTarget entity types: disease. Text: COVID-19 is spreading. Entities: COVID-19 is disease. Text: HIV is spread by three main routes. Entities: <extra_id_0>\u201d, the span corruption task requires the decoder to generate the extraction result \u201c<extra_id_0> HIV is disease.\u201d. 2) Pre-trained NER models are metric-based few-shot methods, includes prototype network (ProtoNet) (Snell et al., 2017), NNshot (Yang and Katiyar, 2020), StructShot (Yang and Katiyar, 2020) and CONTAINER (Das et al., 2022). We employed BERT-Large (Devlin et al., 2019) as the backbone and pre-trained them using the same dataset as MetaNER. For a fair comparison, we\nalso pre-train a 220M T5-v1.1-base (Raffel et al., 2020) model with our meta-function pre-training algorithm (MetaNER-base).\n# 5.2 Main Results\nThe experimental results are shown in Table 1. We can see that: 1) Few-shot NER is challenging even for large language models, while MetaNER can achieve good in-context NER performance. Compare with best-performed PLMs, MetaNER achieves 8.4% F1 improvements. Moreover, due to the gap between language model task and NER task, large language models achieve poor in-context learning performance on some datasets. 2) Our in-context NER method can achieve robust performance, even under a large sourcetarget domain gap. Compared with bestperformed metric-based NER models, MetaNERbase and MetaNER achieves 26.8% and 40.7% F1 improvement. Specifically, the performance improvement is more significant when source-target domain gap is larger, i.e., the NCBI-disease (biology domain) and SEC-filings (finance domain). 3) Meta-function pre-training can effectively inject in-context learning ability into both small and large PLMs. Both MetaNER-base and MetaNER achieve impressive performance in 1-shot and 5-shot settings, which verified that MetaNER can effectively inject in-context NER ability into small PLMs, although currently incontext learning has been seen an ability only emerged only on large language models such as GPT-3.\n# 5.3 Detailed Analysis 5.3.1 Ablation Studies\nCoNLL03\nNCBI-disease\nP\nR\nF1\nP\nR\nF1\nMetaNER\n73.59\n57.19\n64.34\n54.96\n36.85\n43.79\nw/o MF\n68.97\n57.62\n62.77\n38.27\n35.26\n36.28\nw/o LM\n70.86\n57.99\n63.77\n37.54\n34.82\n35.67\nw/o anonymization\n74.75\n52.86\n61.93\n47.47\n35.30\n40.48\nTable 2: Ablation studies on dev set. The results are based on 5-shot setting.\nTo analyze and understand the effect of type anonymization, meta-function pre-training, entity extraction pre-training, and pseudo extraction LM pre-training, we conduct the following ablation experiments: (1) MetaNER w/o MF: remove the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5f70/5f708953-8ca3-4bef-9367-5d74d4433403.png\" style=\"width: 50%;\"></div>\nFigure 4: The visualization of feature comparison between meta-function F and the surrogate extractor F\u2032. The x-axis represents the different datasets, and the yaxis represents the distances between the features from the original encoder and the features from the surrogate encoder.\nmeta-function pre-training; (2) MetaNER w/o LM: remove pseudo extraction LM pre-training; (3) MetaNER w/o anonymization: we use the original entity type names in both pre-training and incontext NER, without using type anonymization. The results are shown in Table 2, we can see that: 1) meta-function pre-training is critical for in-context learning ability. By removing the meta-function pre-training, the results drop significantly when the domain gaps are larger, i.e., NCBI-disease. At the same time, meta-function pre-training is helpful for the model to make more precise predictions. 2) The pseudo extraction LM task significantly benefits in-context NER. We found MetaNER w/o LM results in a performance drop than MetaNER. We believe this is because, although using an automatically constructed pseudo dataset, this task can significantly improve the size and the diversity of in-context NER tasks, meanwhile can retain a good language modeling ability. 3) Type name anonymization prevents incontext NER model from type name overfitting, and therefore enhances the in-context learning ability. The ablation of type name anonymization results a 5.7% performance drop in Table 2. We believe this is because type names will let models tend to memorize entity knowledge using type names, thus the model will not learn to capture entity knowledge from demonstrations on-the-fly.\n# 5.3.2 Effects of Meta-function Pre-training\nOne main idea of this paper is that in-context NER model can be viewed as a meta-function which can\nimplicitly build new entity extractors. To demonstrate whether meta-function pre-training can train a good meta-function, we sample 1000 instances from each dataset, and show the difference between the (instruction, demonstrations)-initialized entity extractor F and the surrogate entity extractor F\u2032, i.e., ||F\u2032 \u2212F|| in Section 4.2 in Figure 4. We can see that meta-function pre-training can equip PLMs with a good meta-function ability, i.e., the (instruction, demonstrations)-initialized entity extractor after pre-training is significantly close to its fine-tuned counterpart.\nCoNLL03\nWNUT17\n1shot\n5shot\n1shot\n5shot\nBERT-large (Devlin et al., 2019)\n14.66\n52.43\n8.95\n32.77\nT5-v11-large (Raffel et al., 2020)\n11.65\n42.13\n12.51\n39.54\nGPT-NEO-20B (Black et al., 2022)*\n52.68\n58.12\n36.29\n35.68\nUIE-large (Lu et al., 2022b)\n46.28\n67.62\n32.86\n42.67\nSDNet (Chen et al., 2022a)\n/\n71.40\n/\n44.10\nCONTAINER-FT (Das et al., 2022)\n48.56\n66.45\n19.46\n24.95\nMetaNER-ICL*\n57.40\n63.45\n31.59\n36.52\nMetaNER-FT\n61.51\n72.70\n39.68\n47.26\nTable 3: The experiments of fine-tuning based methods. * indicates in-context learning settings. CONTAINER is pre-trained using the same NER dataset as MetaNER. All the models are implemented by us except SDNet.\n# 5.3.3 In-context Learning vs Fine-tuning\nMetaNER can also be directly fine-tuned using traditional NER instances. We employed the identical fine-tuning approach as previous works (Huang et al., 2021; Lu et al., 2022b; Chen et al., 2022a). Following Lu et al. (2022b), we also implemented the Rejection Mechanism when fine-tuning the T5v11-large and MetaNER to achieve better few-shot performance. To compare in-context NER with fined-tuned NER, Table 3 reports the performance of the finetuned counterpart of MetaNER \u2013 MetaNER-FT(its training is similar to surrogate entity extractor but with multi-step gradient descent until coverage), together with several fine-tuned few-shot NER baselines. We can see that: 1) MetaNER is an effective architecture, which achieves good performance on both in-context learning and fine-tuning settings; 2) Currently, fine-tuning can achieve better performance than their in-context learning counterpart. We believe this is because fine-tuned models\u2019 parameters need to be specialized to specific entity types, meanwhile in-context learning needs to generalize to different types on-the-fly, i.e., generalization-specialization trade-off. We believe this also verified the reasonableness of using\na fine-tuned surrogate extractor to approximate the golden extractor.\n# 6 Conclusion\nIn this paper, we propose an in-context learningbased NER approach and model PLMs as a metafunction, which can inject in-context NER ability into PLMs and recognize entities of new types onthe-fly using only a few demonstrative instances. Experimental results show that our method is effective for in-context NER. For future work, we will extend our method to different NLP tasks like event extraction and relation extraction.\n# Limitations\nIn-context learning is an useful ability, this paper only focuses on in-context named entity recognition, leaves the learning of other NLP tasks\u2019 incontext learning abilities for future work. Currently, we learn in-context learning via metafunction pre-training, by comparing an in-context extraction function and a fined-tuned surrogate extraction function at the representation level of their encoders. There are two approximation here: one is fined-tuned surrogate extraction function for approximating golden extraction function, and the difference between representations for approximating the divergence between functions. We think the above two approximations can be further improved for better and faster in-context learning.\n# Acknowledgements\nWe sincerely thank the reviewers for their insightful comments and valuable suggestions. This research work is supported by the CAS Project for Young Scientists in Basic Research under Grant No.YSBR-040 and the National Natural Science Foundation of China under Grants no. 62122077, 62106251.\n# References\nNing Bian, Xianpei Han, Bo Chen, Hongyu Lin, Ben He, and Le Sun. 2021. Bridging the gap between language model and reading comprehension: Unsupervised mrc via self-supervision. arXiv preprint arXiv:2107.08582.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models.\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nStephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay McClelland, and Felix Hill. 2022. Data distributional properties drive emergent fewshot learning in transformers. arXiv preprint arXiv:2205.05055.\nJiawei Chen, Qing Liu, Hongyu Lin, Xianpei Han, and Le Sun. 2022a. Few-shot named entity recognition with self-describing networks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5711\u20135722, Dublin, Ireland. Association for Computational Linguistics.\nMingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. 2022b. Improving in-context few-shot learning via self-supervised training. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3558\u20133573, Seattle, United States. Association for Computational Linguistics.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, pages 140\u2013147. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\nRezarta Islamaj Do\u02d8gan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1\u201310.\niaxin Huang, Chunyuan Li, Krishan Subudhi, Damien Jose, Shobana Balakrishnan, Weizhu Chen, Baolin Peng, Jianfeng Gao, and Jiawei Han. 2021. Fewshot named entity recognition: An empirical baseline study. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10408\u201310423, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nBin Ji, Shasha Li, Shaoduo Gan, Jie Yu, Jun Ma, Huijun Liu, and Jing Yang. 2022. Few-shot named entity recognition with entity-level prototypical network enhanced by dispersedly distributed prototypes. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1842\u20131854, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022a. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR.\n# A Experiment Details\n# A.1 Datasets for the extraction language model task\nRather than randomly generating spans to form target labels in instruction, we use informative spans (Bian et al., 2021) as target labels. Unlike informative span selection at passage level for MRC (Bian et al., 2021), we select informative spans at a cross-document level. Specifically, we take 10 Wikipedia documents as a set and select informative spans according to the following rules: (1) spans that have appeared simultaneously in at least two and at most five documents. (2) spans that have appeared in only one document but have appeared in more than two. Rule (1) avoids some\nlow-information general spans, such as stop words, and rule (2) retains some important spans in each document. Note that we consider at most 4-gram as a span and select the target labels from the informative spans during pre-training.\n# A.2 Cost of pre-training\nWe used one A-100 80g GPU for pre-training the base/large model, which took approximately one to three days. The total FLOPs for the base model are 2.30e+18 and for the large model are 7.64e+18.\n",
    "paper_type": "method",
    "attri": {
        "background": "Named entity recognition (NER) in real-world applications faces challenges such as the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. Traditional methods have limitations, necessitating a breakthrough to effectively recognize novel entity types on-the-fly.",
        "problem": {
            "definition": "The paper addresses the issue of recognizing named entities of diverse and new types in text, particularly in scenarios with limited labeled data.",
            "key obstacle": "Existing methods, particularly fine-tuning and metric-based approaches, struggle with the high cost of retraining and the inability to adapt to new entity types dynamically."
        },
        "idea": {
            "intuition": "The idea is inspired by in-context learning, which allows models to adapt to new tasks using few examples without extensive retraining.",
            "opinion": "The proposed method injects in-context NER ability into pre-trained language models (PLMs) to recognize entities of novel types using only a few demonstrative instances.",
            "innovation": "The key innovation lies in modeling PLMs as a meta-function that can dynamically construct new entity extractors based on instructions and demonstrations, differentiating it from traditional fine-tuning methods."
        },
        "method": {
            "method name": "In-Context Learning-based NER",
            "method abbreviation": "MetaNER",
            "method definition": "MetaNER is a method that utilizes in-context learning to enable PLMs to recognize named entities of new types by leveraging few-shot examples.",
            "method description": "The method constructs entity extractors on-the-fly by applying new instructions and demonstrations to PLMs.",
            "method steps": [
                "Collect in-context NER tasks from traditional NER datasets.",
                "Sample instructions and demonstrations for target entity types.",
                "Pre-train PLMs using a meta-function pre-training algorithm.",
                "Utilize the trained model to extract entities from text based on new instructions and demonstrations."
            ],
            "principle": "The effectiveness of this method stems from its ability to leverage the powerful language understanding of PLMs while avoiding the need for retraining, allowing for rapid adaptation to new entity types."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on four few-shot NER datasets: CoNLL03, WNUT17, NCBI-disease, and SEC-filings, comparing the performance of MetaNER against various baseline models.",
            "evaluation method": "Performance was assessed using micro-F1 scores across different k-shot settings (1-shot and 5-shot), with results averaged over multiple runs."
        },
        "conclusion": "The proposed in-context learning-based NER approach effectively injects NER capabilities into PLMs, demonstrating significant improvements over traditional methods in recognizing entities of novel types with minimal examples.",
        "discussion": {
            "advantage": "MetaNER stands out due to its ability to recognize new entity types on-the-fly without the need for extensive retraining, significantly improving efficiency in NER tasks.",
            "limitation": "The method is currently focused on NER, leaving the potential for in-context learning in other NLP tasks unexplored, and relies on approximations that could be optimized further.",
            "future work": "Future research may explore extending the in-context learning capabilities to other NLP tasks, improving the approximations used in the current method for better performance."
        },
        "other info": {
            "acknowledgements": "The research was supported by the CAS Project for Young Scientists in Basic Research and the National Natural Science Foundation of China.",
            "dataset details": "The pre-training corpus was constructed from Wikipedia and Wikidata, resulting in 2046 entity types and 55 million (text, entities) pairs."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper introduces the concept of in-context learning in the context of named entity recognition (NER), emphasizing its significance in adapting to new entity types with few examples."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, MetaNER, utilizes pre-trained language models (PLMs) to facilitate in-context learning, allowing for the recognition of named entities of novel types."
        },
        {
            "section number": "3.1",
            "key information": "MetaNER demonstrates how PLMs adapt to various contexts by recognizing new entity types on-the-fly without extensive retraining."
        },
        {
            "section number": "3.4",
            "key information": "The method constructs entity extractors dynamically based on new instructions and demonstrations, showcasing contextual adaptation in in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "The design of the few-shot examples and instructions in MetaNER significantly influences the outcomes of in-context learning for NER tasks."
        },
        {
            "section number": "6.1",
            "key information": "The paper discusses the limitation of existing methods in adapting to new entity types and the potential model bias that may arise in NER scenarios."
        },
        {
            "section number": "7",
            "key information": "The conclusion highlights that the in-context learning-based NER approach shows significant improvements over traditional methods, with future research suggested to explore its application in other NLP tasks."
        }
    ],
    "similarity_score": 0.6967111422869574,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Learning In-context Learning for Named Entity Recognition.json"
}