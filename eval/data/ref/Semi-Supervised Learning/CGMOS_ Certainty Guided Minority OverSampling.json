{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1607.06525",
    "title": "CGMOS: Certainty Guided Minority OverSampling",
    "abstract": "Handling imbalanced datasets is a challenging problem that if not treated correctly results in reduced classification performance. Imbalanced datasets are commonly handled using minority oversampling, whereas the SMOTE algorithm is a successful oversampling algorithm with numerous extensions. SMOTE extensions do not have a theoretical guarantee during training to work better than SMOTE and in many instances their performance is data dependent. In this paper we propose a novel extension to the SMOTE algorithm with a theoretical guarantee for improved classification performance. The proposed approach considers the classification performance of both the majority and minority classes. In the proposed approach CGMOS (Certainty Guided Minority OverSampling) new data points are added by considering certainty changes in the dataset. The paper provides a proof that the proposed algorithm is guaranteed to work better than SMOTE for training data. Further experimental results on 30 real-world datasets show that CGMOS works better than existing algorithms when using 6 different classifiers.",
    "bib_name": "zhang2016cgmoscertaintyguidedminority",
    "md_text": "# CGMOS: Certainty Guided Minority OverSampling\n# Xi Zhang, Di Ma, Lin Gan, Shanshan Jiang, Gady Agam\nXi Zhang, Di Ma, Lin Gan, Shanshan Jiang, Gady Agam\nAbstract\u2014Handling imbalanced datasets is a challenging problem that if not treated correctly results in reduced classification performance. Imbalanced datasets are commonly handled using minority oversampling, whereas the SMOTE algorithm is a successful oversampling algorithm with numerous extensions. SMOTE extensions do not have a theoretical guarantee during training to work better than SMOTE and in many instances their performance is data dependent. In this paper we propose a novel extension to the SMOTE algorithm with a theoretical guarantee for improved classification performance. The proposed approach considers the classification performance of both the majority and minority classes. In the proposed approach CGMOS (Certainty Guided Minority OverSampling) new data points are added by considering certainty changes in the dataset. The paper provides a proof that the proposed algorithm is guaranteed to work better than SMOTE for training data. Further experimental results on 30 real-world datasets show that CGMOS works better than existing algorithms when using 6 different classifiers.\n# 1 INTRODUCTION\nIn many real world problems, the distribution of data between classes is imbalanced. Learning from imbalanced datasets is an important research problem with many applications. The fundamental issue in imbalanced learning is the ability of imbalanced data to significantly compromise the performance of standard learning algorithms [1]. Generally, there are three primary reasons that can cause this problem [2]. The first reason is that the lack of data in the minority class makes it difficult to detect regularities within the minority class. Thus, the learned decision boundaries are less likely to approximate the true decision boundaries. Second, many classification algorithms utilize a general bias for better generalization and to avoid overfitting during learning. However, such bias can adversely affect the ability to learn the minority class. Inductive bias also plays a key role with respect to the minority class. Most classification algorithms prefer more common classes in the presence of uncertainty (i.e., they are biased in favor of the class priors). Last but not least, noise exerts a greater impact on the minority class, because in this case it is more difficult for a classifier to distinguish noise from minority data. This is especially so in extreme cases where the number of noisy samples is greater than actual minority samples. The problem of overfitting rises again, when modifying the classifier to learn the minority data correctly.\nX. Zhang is currently a PhD student in Illinois Institue of Technology, email: vinxi.zhang@gmail.com. D. Ma, L. Gan and S. Jiang are students of Illinois Institute of Technology. G. Agam is a associate professor in Illinois Institute of Technology, email: agam@iit.edu.\nTo address these problems, numerous research efforts have been devoted to imbalanced learning in recent years. The majority of techniques that solve the imbalanced learning problem fall into two categories: cost-sensitive methods and sampling-based methods. In the next section, we review related work on samplingbased methods.1\n# 1.1 Related work\nA number of solutions to the class-imbalance problem were previously proposed both at the data and algorithmic levels [3]. There are mainly three groups of methods that can solve imbalanced learning problem [1] including sampling methods, cost sensitive methods, and kernel methods. Sampling-based methods are very effective and easy to use when solving imbalanced learning problems. In addition, sampling-based methods can be used together with methods in the other two groups to further improve performance. In such approaches a sampling technique is used to modify an imbalanced dataset to produce a balanced distribution. It has been shown that for most imbalanced datasets, sampling techniques do improve classification accuracy. The basic sampling methods include undersampling and oversampling. Undersampling reduces majority class samples while oversampling increases minority class samples. While several works achieving data balance through undersampling have been proposed in the past [4] [5], more research efforts have been devoted to oversampling due to the fact that oversampling does not discard information. The simplest form of oversampling is duplication of minority class samples. This approach decreases the overall level of class imbalance, but may lead to overfitting [6]. SMOTE [7] is a fundamental approach for oversampling using data synthesis. To balance the dataset, SMOTE randomly selects a seed sample and synthesizes a new sample by applying a linear interpolation between the seed sample and one of its neighbors. Large research efforts have been devoted to feature space data synthesis based on SMOTE. Several methods integrate data synthesis as a part of the learning procedure. For example, by introducing SMOTE in each iteration of boosting, SMOTEBoost [8] increases the number of minority class samples and focus on these cases in each boosting iteration. Using the same idea of boosting, DataBoost-IM [9] and RAMOBoost [10] discover samples difficult to classify during each iteration of boosting, which are used to guide the oversampling in both the majority and minority classes.\nIn another group of minority oversampling approaches, the data synthesis procedure is independent of the learning processes. Such methods give preferences to different regions of a dataset by assigning weights to samples in the dataset. These weights can then generate a probability distribution which is used for randomly drawing samples. In such approaches the data synthesis can be completed in one step. Methods in this group include BorderlineSMOTE [11], Adasyn [12], [13] and MWMOTE [14]. All of these methods synthesize more samples along decision boundaries. However, these methods do not have objective functions to systematically guide the process of oversampling and so do not have a systematic way to decide on where new data should be synthesized. Thus, such approaches cannot measure the impact of each synthetic sample. As a result, there are several potential problems. One is that the oversampling procedure may sacrifice the performance of the majority class in order to improve the performance of the minority class in the classification. Another is that synthetic minority samples themselves can be misclassified and affect the performance in the minority class.\n# 1.2 Novel Contribution\nThe proposed approach, CGMOS, is a member of the SMOTE family that can achieve data oversampling in a single step. To address some of the shortcomings in existing approaches, we propose a novel oversampling strategy by systematically considering the performance of both minority and majority classes. Based on a Bayesian classification framework, our proposed approach computes the influence of minority data addition on the certainty of the entire dataset. CGMOS thus can synthesize new samples that will improve the overall certainty of the entire dataset in classification. We prove that during training CGMOS is guaranteed to perform better than SMOTE when using Bayesian classification. To validate the proof, We further show experimentally that CGMOS outperforms known approaches when tested on real-world data set collections using different classifiers.\n# 2 PROBLEM FORMULATION\nIn this paper, we address the binary classification problem for imbalanced datasets. Let D = {(xj, yj)}n j=1 be a training dataset, where xj \u2208Rm are features and yj \u2208{lmjr, lmnr} are ground truth class labels. We begin by formally defining the certainty of imbalanced binary classification using a Bayesian framework, where a kernel density estimation (KDE) is used to estimate the samples\u2019 probability density function (PDF). We then show how CGMOS can synthesize more samples according to the certainty estimation.\n# 2.1 Definition of Certainty\nSuppose (xj, yj) is any tuple in the training dataset D, where xj is a feature vector and yj is the ground truth label of xj. A Bayesian classifier maps xj \u2192l, l \u2208{lmjr, lmnr} using following rule.\n# where the posterior probability P(l|xj) is computed using Bayes\u2019 rule:\nwhere the posterior probability P(l|xj) is computed using Bayes\u2019 rule:\nP(l|xj) = P(xj|l)P(l) P(xj) ; l \u2208{lmjr, lmnr}\nUncertainty is commonly used in machine learning algorithms. In this work, we use the posterior probability P(yj|xj) to define certainty. This is because in classification, the posterior probabilities P(yj|xj) reflect the certainty of assigning a sample to a correct label, where higher numbers indicate classification results with a stronger certainty.\nDefinition 1. (Certainty) Let (xj, yj) be any tuple in D, where xj is a feature vector and yj is the ground truth label of xj. The certainties for samples in the majority and minority class are respectively defined as:\nC(yj = lmjr|xj) = P(yj = lmjr|xj)\n(1)\nC(yj = lmnr|xj) = P(yj = lmnr|xj)\n(2)\nIt should be noted that in the case of binary classification the definition of certainty above is related up to some constants to the uncertainty defined in [15] based on margin confidence.\n# 2.2 PDF Estimation\nThere are two general ways to estimate a density function: parametric or non-parametric. In this work we use a non-parametric model so as to not depend on a specific distribution model. We use kernel density estimation (KDE) [16] [17] to estimate the likelihood P(xj|l), l \u2208{lmjr, lmnr}. Assuming that the data is independent and identically distributed (i.i.d) and drawn from some distribution with an unknown density P(xj|l), we have using KDE:\n(3)\n\ufffd where l \u2208{lmjr, lmnr}, I(\u00b7) is an indicator function, and K(\u00b7) is a kernel function which has zero mean and integrates to one. Given any sample xk, the bandwidth hk of the sample xk controls the effective range of the kernel and smoothness of the density function. Intuitively one wants to choose hk as small as the data allows to exhibit as many underlying structures of the data as possible. Small bandwidth, however, will result in a noisy estimate. In this work, for any sample xk, we calculate a bandwidth hk as a scaled average distance between xk and its q nearest neighbors:\n(4)\nwhere N(x) is the set of the q nearest neighbors of xk and \u03c3 > 0 is a scale factor applied to the distance. We will discuss selection of parameters \u03c3 and q in Section 4.\nTHIS PAPER HAS BEEN ACCEPTTED BY CIKM 2016.\n# 2.3 Oversampling Seed Selection\nIn most classification algorithms, samples close to decision boundaries have less certain classification results. In order to achieve better predictions for such samples, many existing approaches synthesize data directly along the boundaries. However, this is risky and the expected performance improvement is not guaranteed. There are two primary reasons. First, samples from both classes are mixed in regions near the boundaries. Synthetic samples if added to these regions are less predictable and hard to learn. Second, adding synthetic minority samples to these regions may adversely impact the majority class, which may in turn decrease the performance of the majority class in classification. Instead of unguided oversampling near the boundaries, our proposed approach targets adding samples by considering the certainties of both the minority and majority classes before and after adding the samples. The synthetic samples thus are added to locations that can improve the overall certainty of the original data and boost the performance of the classification. CGMOS uses a similar procedure as SMOTE when synthesizing a new sample. The sample is produced by interpolating between one seed sample and some of its neighbors. However, instead of randomly drawing a seed sample for interpolation, CGMOS assigns each sample (xi, yi) \u2208D a weight W(xi) which is used to determine the probabilities of xi being chosen for interpolation. A higher weight results in a higher probability of a point being selected. To compute W(xi), we suppose that a new sample will be added to the same location as xi. The weight W(xi) is computed as a relative certainty change2 comparing the certainty before and after the sample is added. With a new sample added at location xi, we update the certainty for all (xj, yj) \u2208D and denote it as C+i(yj|xj).\nDefinition 2. (Relative Certainty Change) The relative certainty change of label yj assigned to feature xj due to adding a minority example at location xi is defined by:\n(5)\nwhere C(yj|xj) is the certainty before addition. When computing W(xi), CGMOS considers the relative certainty changes of examples from both the majority and the minority classes. W(xi) is computed as the average value of relative certainty changes of all samples in the dataset.\n(6)\nGiven W(xi) for all xi \u2208D, it is easy to see W(xi) > 0. We compute a normalization factor z so that 1 z \ufffdn i=1 W(xi) = 1. Therefore, the oversampling procedure can randomly choose sample for interpolation according W(xi)/z. The interpolation phase of CGMOS is the same as SMOTE [7]. A demonstration of CGMOS is shown in Fig. 1. In this figure, samples in both the majority and minority classes are randomly drawn based on Gaussian distribution, where the means of the two datasets are on the same horizontal line, and the mean of the\n2. Measuring absolute certainty increments will not work, because measuring magnitude will give higher preference to parts which already have high certainty.\nmajority is to the right of the minority. The majority class contains 2000 samples and the minority class contains 400 samples. Color in part 1 of the figure indicates the certainty of each example with respect to its class, where red indicates high certainty. We highlight 3 regions (A, B, C) in the minority class. Samples in region A have relative high certainties, sample in region B has low certainties and region C is a boundary region in which samples have the lowest certainties. Part 2 of the figure shows the weight of each example as computed by our approach where red indicates high values. Region B has higher values and is where CGMOS will synthesize most of the samples. To show the certainty changes induced by adding samples at different locations of the dataset, in part 3 of the figure we add one minority sample and move its location with a fixed step size from left to right on a horizontal line passing through the two classes. We then compute the relative certainty changes for all samples in both classes. As can be observed, by measuring relative certainty changes, CGMOS will assign a higher weight to samples in region B. The figure also shows that by oversampling more in region B, the certainty of the entire dataset gets improved, because the relative certainty changes are positive.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d429/d429e584-9ef8-4d06-bae9-2ba267411ea8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8968/896849e6-1391-40b3-863e-c5978273cc44.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2c83/2c83a705-42ca-4f86-8c81-e6dff94f4298.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Demonstration of CGMOS. In first two figures, diamonds represent minority samples and circles represent majority samples. The positions of synthesized data points are labeled using a star symbol on a horizontal line passing through the center. The x and y axes represent features. In the bottom figure the x axis indicates a location where a sample was added (in correspondence with the first two figures) whereas the y-axis indicates the relative certainty change.</div>\nFig. 1. Demonstration of CGMOS. In first two figures, diamonds represent minority samples and circles represent majority samples. The positions of synthesized data points are labeled using a star symbol on a horizontal line passing through the center. The x and y axes represent features. In the bottom figure the x axis indicates a location where a sample was added (in correspondence with the first two figures) whereas the y-axis indicates the relative certainty change.\n# 3 THEORETICAL GUARANTEE OVER SMOTE\nSeveral existing approaches claim handling imbalanced learning better than SMOTE. Such claims are normally validated using empirical tests without a theoretical guarantee and in some instances may not extend to new datasets. In this section we provide a theoretical guarantee showing that CGMOS is expected to work better than SMOTE in training process. Let D = {(xj, yj)}n j=1 be a training dataset. Let W(D) = {W(xi)}n i=1 be the sample weights computed using Eqn. 6. Lemma 1. Given a set of weights {W(xi)}n i=1 as defined above and a normalization factor z given by z = \ufffdn i=1 W(xi), it must be that \ufffdn i=1 W(xi)2 \u2265z2 n .\nProof Let W be an n-dimensional vector whose elements are W(xi). Let I be an n-dimensional vector whose elements are all 1. Using the Cauchy-Schwarz inequality we have: |W \u00b7 I| \u2264 \u2225W\u2225\u00b7 \u2225I\u2225. Thus, | \ufffdn i=1 W(xi)| \u2264 \ufffd\ufffdn i=1 W(xi)2\u221an using the fact that \ufffdn i=1 W(xi) = z, we thus have \ufffdn i=1 W(xi)2 \u2265z2 n . \u25a0\n\ufffd  \ufffd Definition 3. (Addition Likelihood Ratio) Let \u03b8 denote the nonparametric likelihood estimate P(xj|l), l \u2208{lmjr, lmnr} before a new sample xi is added, and \u03b8\u2032 denote the non-parametric likelihood estimate after the new sample is added. The addition likelihood ratio r+i(yj|xj) of example xj by adding data to xi location is defined as the ratio between the likelihood estimate after the new addition and the likelihood estimate before the new addition:\n(7)\nLemma 2. The addition likelihood ratio r+i(yj|xj) is related to the relative certainty change ratio R+i(yj|xj) by:\n(8)\nProof According to the definition of the certainty, we have C+i(yj|xj) = P(yj|xj; \u03b8\u2032) and C(yj|xj; \u03b8) = P(yj|xj; \u03b8). Then P(yj|xj; \u03b8\u2032) = r+i(yj|xj)P(yj|xj; \u03b8) according to the definition of likelihood ratio. Given Eqn. 5, we have that R+i(yj|xj) = r+i(yj|xj)P (yj|xj;\u03b8)\u2212P (yj|xj;\u03b8) P (yj|xj;\u03b8) . By simplifying this equation, we thus have\nThe addition likelihood ratio defined in Eqn. 7 measures the gain in adding a new point, where higher gains are desired. Note that while the gain is normally close to 1 it may be bigger or smaller than 1.\nAccording to a survey of imbalanced learning [1], there are mainly three groups of methods addressing imbalanced learning: sampling methods, cost sensitive methods, and kernel methods. The proposed CGMOS belongs to the sampling group. Thus, we compare CGMOS to five other oversampling methods in this group: SMOTE [7], Borderline-SMOTE [11], ADASYN [12], MWMOTE [14] and RAMOBoost [10]. Since oversampling by duplication is broadly used in many applications as a baseline, we add it to our evaluation as well. To demonstrate the improvement of these oversampling strategies, we include in the comparison raw data with no oversampling. It should be noted that sampling methods are often combined with cost sensitive methods and kernel methods to further boost learning. [3] [8] [9].\nDefinition 4. (Average gain) The average gain when adding sample xi is defined by:\n(10)\nLemma 3. Given the average gain, it must be that:\n(11)\nProof Using the definition of W(xi) we have W(xi) = 1 n \ufffdn j=1 R+i(yj|xj). Using Lemma 2 we can replace r+i(yj|xj) \u22121 with R+i(yj|xj). Hence:\n(12)\nThe average gain is an indicator of the benefit of CGMOS. We show that the expected average gain is higher in proposed approach compared with SMOTE.\nTheorem 1. The expected average gain in CGMOS is higher or equal to that of SMOTE.\nProof For CGMOS the expected average gain is given by:\n(13)\nwhere z is the normalization factor as defined earlier. Using Lemma 3:\n(14)\n(15)\nUsing Lemma 3:\nUsing Lemma 1:\n(17)\n# 4 RESULTS AND DISCUSSION\n# 4.1 Datasets\n30 real-world datasets were randomly chosen from the UCI machine learning repository [18] for empirical testing of CGMOS. Most of the datasets were released within the past 10 years. As some of the datasets contain samples of more than two classes, we convert such datasets to a binary classification problem by keeping the class with the least data and merging all other classes. A summary of the test collections is provided in Table 1.\n# 4.2 Compared Approaches\n# 4.3 Base classifiers\nWe match the compared classifiers to classifiers used in other SMOTE extension evaluations. Six well-known classifiers are tested in experiments. The first is the Bayesian classifier based on kernel density estimation described in Section 2 (b-kde). The second is a K nearest neighbors classifier (knn). The third is a support vector machine classifier using RBF kernel (svm). The fourth one is a neural network (nn) with one hidden layer. We use in addition two ensemble methods: a random forest implementing the C4.5 decision tree [19] (rf) and Adaboost.M1 [20]. All hyperparameters of the classifiers tested were determined by cross validation to ensure the best performance of each method.\nTHIS PAPER HAS BEEN ACCEPTTED BY CIKM 2016.\n<div style=\"text-align: center;\">THIS PAPER HAS BEEN ACCEPTTED BY CIKM 2016.</div>\nName\nS #\nF #\nR\nYear\nName\nS #\nF #\nR\nYear\nBankMarket\n45211\n17\n0.13\n2012\nLibras\n360\n91\n0.07\n2009\nBloodService\n748\n5\n0.31\n2008\nMultipleFs\n2000\n649\n0.11\n1998\nBreastCancer\n400\n9\n0.53\n1988\nParkinson\n1040\n26\n0.02\n2014\nBreastTissue\n106\n10\n0.15\n2010\nPlanRelax\n182\n13\n0.4\n2012\nCarEvaluation\n1730\n6\n0.04\n1997\nQSAR\n1055\n41\n0.51\n2013\nCard\u2019graphy\n2126\n23\n0.09\n2010\nSPECT\n268\n22\n0.26\n2001\nCharacterTraj\n2860\n3\n0.04\n2008\nSPECTF\n134\n44\n0.26\n2001\nChess\n3198\n22\n0.91\n1989\nSeismicBumps\n2584\n19\n0.07\n2013\nClimateSim\n540\n18\n0.09\n2013\nStatlog\n2310\n19\n0.17\n1990\nContraceptive\n1474\n9\n0.29\n1997\nPlatesFaults\n1941\n27\n0.03\n2010\nFertility\n100\n10\n0.14\n2013\nTAEvaluation\n151\n5\n0.49\n1997\nHaberman\n306\n3\n0.36\n1999\nUKnowledge\n403\n5\n0.1\n2013\nILPD\n580\n10\n0.4\n2012\nVertebral\n310\n6\n0.48\n2011\nImgSeg\n2310\n19\n0.17\n1990\nCustomers\n440\n8\n0.48\n2014\nLeaf\n342\n16\n0.24\n2014\nYeast\n1484\n8\n0.04\n1996\nName\nS #\nF #\nR\nYear\nName\nS #\nF #\nR\nYear\nBankMarket\n45211\n17\n0.13\n2012\nLibras\n360\n91\n0.07\n2009\nBloodService\n748\n5\n0.31\n2008\nMultipleFs\n2000\n649\n0.11\n1998\nBreastCancer\n400\n9\n0.53\n1988\nParkinson\n1040\n26\n0.02\n2014\nBreastTissue\n106\n10\n0.15\n2010\nPlanRelax\n182\n13\n0.4\n2012\nCarEvaluation\n1730\n6\n0.04\n1997\nQSAR\n1055\n41\n0.51\n2013\nCard\u2019graphy\n2126\n23\n0.09\n2010\nSPECT\n268\n22\n0.26\n2001\nCharacterTraj\n2860\n3\n0.04\n2008\nSPECTF\n134\n44\n0.26\n2001\nChess\n3198\n22\n0.91\n1989\nSeismicBumps\n2584\n19\n0.07\n2013\nClimateSim\n540\n18\n0.09\n2013\nStatlog\n2310\n19\n0.17\n1990\nContraceptive\n1474\n9\n0.29\n1997\nPlatesFaults\n1941\n27\n0.03\n2010\nFertility\n100\n10\n0.14\n2013\nTAEvaluation\n151\n5\n0.49\n1997\nHaberman\n306\n3\n0.36\n1999\nUKnowledge\n403\n5\n0.1\n2013\nILPD\n580\n10\n0.4\n2012\nVertebral\n310\n6\n0.48\n2011\nImgSeg\n2310\n19\n0.17\n1990\nCustomers\n440\n8\n0.48\n2014\nLeaf\n342\n16\n0.24\n2014\nYeast\n1484\n8\n0.04\n1996\nTABLE 1 Summary of the datasets used in our experiments, where S#, F#, and R stand for the number of samples, the number of features, and imbalance ratio (defined as #minority/#majority).\nTABLE 1\nTABLE 1 Summary of the datasets used in our experiments, where S#, F#, and R stand for the number of samples, the number of features, and imbalance ratio (defined as #minority/#majority).\nSummary of the datasets used in our experiments, where S#, F#, and R stand for the number of samples, the number of features, and imbalance ratio (defined as #minority/#majority).\n# 4.4 Evaluation metric\nFinding an appropriate evaluation metric for different tasks is challenging, since different evaluation metrics are designed for different purposes. The datasets used in this paper cover from financial application to medical treatment. To achieve an general evaluation and avoid bias, we follow the method in [7] [11] [12] [14] [10] and use different metrics to evaluate the performance of the proposed CGMOS oversampling algorithm. Among these evaluation metrics, the most frequently adopted ones are Precision and Recall when the focus of evaluation is focus on one specific class such as problems in text classification, information extraction, natural language processing and bioinformatics. In these areas of application the number of examples belonging to one class is often substantially lower than the overall number of examples, which basically are imbalance learning problems. Precision and Recall are defined as:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f3d/6f3de8bf-79fd-4ae6-9d8a-bf1516e8f3e7.png\" style=\"width: 50%;\"></div>\nHowever, these two metrics share an inverse relationship between each other. A quick inspection on the Precision and Recall formulas readily yields that solely use each of these two metrics only provide a limit view of an algorithm under test. As Recall provides no insight to how many examples are incorrectly labeled as positive and Precision cannot assert how many positive examples are labeled incorrectly. Specifically, the F-score combines Precision and Recall as measure of the effectiveness of classification in terms of a ration of the weighted importance on either Recall or Precision, which is defined as:\nWe use \u03b2 = 1 to treat Precision and Recall equally in all evaluations. As a result, F-score provides more insight into the functionality of a classifier. As F-score measures the harmonic mean of Precision and Recall, we also compute Gscore which is the geometric mean of Precision and Recall and is able to evaluate the degree of\n# inductive bias in terms of a ratio of positive accuracy and negative accuracy [1].\ninductive bias in terms of a ratio of positive accuracy and negative accuracy [1].\nG-score = \u221a Precision \u00b7 Recall\nAs both F-score and G-score concentrate their measures on one class (positive examples) [21], to have a general way of comparing our test results, we altered the positive examples between the majority and minority classes when computing F-score and G-score. Thus we show F-score and G-score for the majority and the minority classes separately. Although, both F-score and G-score are great evaluation metrics, they are still less effective in some situations. So we also employ the ROC graphs [22] [23] [24] in the evaluation. ROC graph is a two-dimensional graph, while FP rate and TP rate are its X axis and Y axis respectively. An ROC graph basically manifest its usefulness by showing relative trade-off between benefit (true positive) and cost (false positive). One attractive property make ROC graph a good metric in imbalanced learning lies in the facts that ROC curve is insensitive to changes in class distribution. Because of this property, it is easier to see the performances of models trained by dataset oversampled by different algorithms. The goal in ROC space is to let curves be as close to upper-left-hand corner as possible, in which case the ratio between benefit and cost is maximized. To compare all test results in a more straightforward way, we also compute area under an ROC curve (AUC) which reduce the ROC performance to a single scalar value representing expected performance of the ROC curve.\n# 4.5 Results\nThis section presents the performance of CGMOS and all the other methods on 30 real-world datasets. The same experiment procedure as the one in the experiments of the artificial dataset was conducted. All results are averged from 10 rounds of 10-folds cross-validations. A summary of the experiment results is shown in Table 2 and ROC graphs are shown in Figure 2. Considering the classification results of the minority class, it can be observed that the proposed approach outperforms most of the compared methods under all classification algorithms in terms of F-score and G-score. For F-score and G-score of\nMinority\nMajority\nAUC\nPrecision\nRecall\nFscore\nGscore\nPrecision\nRecall\nFscore\nGscore\nb-kde\nOriginal\n0.797\n0.139\n0.033\n0.054\n0.068\n0.830\n0.995\n0.905\n0.909\nDup\n0.733\n0.385\n0.454\n0.417\n0.418\n0.869\n0.742\n0.801\n0.803\nSMOTE\n0.807\n0.488\n0.705\n0.577\n0.587\n0.833\n0.644\n0.726\n0.733\nB-SMOTE\n0.774\n0.258\n0.456\n0.330\n0.343\n0.846\n0.671\n0.748\n0.754\nMWMOTE\n0.794\n0.396\n0.754\n0.520\n0.547\n0.836\n0.557\n0.669\n0.682\nADASYN\n0.802\n0.395\n0.632\n0.487\n0.500\n0.817\n0.598\n0.691\n0.699\nRAMOboost\n0.748\n0.358\n0.343\n0.350\n0.350\n0.860\n0.822\n0.841\n0.841\nCGMOS\n0.842\n0.536\n0.517\n0.526\n0.526\n0.908\n0.815\n0.859\n0.860\nknn\nOriginal\n0.821\n0.701\n0.521\n0.598\n0.604\n0.902\n0.942\n0.922\n0.9217\nDup\n0.810\n0.519\n0.732\n0.607\n0.616\n0.921\n0.818\n0.867\n0.868\nSMOTE\n0.827\n0.506\n0.804\n0.621\n0.638\n0.925\n0.805\n0.861\n0.863\nB-SMOTE\n0.811\n0.494\n0.736\n0.591\n0.603\n0.927\n0.790\n0.853\n0.856\nMWMOTE\n0.832\n0.504\n0.792\n0.616\n0.632\n0.928\n0.794\n0.856\n0.858\nADASYN\n0.825\n0.495\n0.786\n0.607\n0.623\n0.929\n0.786\n0.851\n0.854\nRAMOboost\n0.827\n0.540\n0.684\n0.604\n0.608\n0.918\n0.847\n0.881\n0.881\nCGMOS\n0.840\n0.544\n0.766\n0.636\n0.646\n0.925\n0.842\n0.882\n0.883\nsvm\nOriginal\n0.792\n0.632\n0.587\n0.609\n0.609\n0.882\n0.935\n0.908\n0.908\nDup\n0.815\n0.543\n0.436\n0.484\n0.487\n0.981\n0.861\n0.917\n0.919\nSMOTE\n0.844\n0.579\n0.726\n0.644\n0.648\n0.879\n0.844\n0.861\n0.861\nB-SMOTE\n0.832\n0.475\n0.729\n0.575\n0.588\n0.893\n0.959\n0.924\n0.925\nMWMOTE\n0.830\n0.547\n0.647\n0.593\n0.595\n0.880\n0.884\n0.882\n0.882\nADASYN\n0.827\n0.536\n0.654\n0.589\n0.592\n0.880\n0.755\n0.813\n0.815\nRAMOboost\n0.842\n0.556\n0.673\n0.609\n0.611\n0.968\n0.852\n0.906\n0.908\nCGMOS\n0.864\n0.555\n0.788\n0.651\n0.661\n0.943\n0.830\n0.883\n0.885\nnn\nOriginal\n0.801\n0.632\n0.412\n0.499\n0.510\n0.892\n0.962\n0.925\n0.9258\nDup\n0.843\n0.543\n0.777\n0.639\n0.650\n0.926\n0.819\n0.869\n0.871\nSMOTE\n0.840\n0.555\n0.750\n0.638\n0.645\n0.921\n0.820\n0.868\n0.869\nB-SMOTE\n0.841\n0.475\n0.779\n0.590\n0.608\n0.924\n0.802\n0.859\n0.861\nMWMOTE\n0.841\n0.547\n0.778\n0.642\n0.652\n0.927\n0.812\n0.866\n0.867\nADASYN\n0.842\n0.536\n0.786\n0.637\n0.649\n0.929\n0.803\n0.861\n0.863\nRAMOboost\n0.841\n0.556\n0.743\n0.636\n0.643\n0.919\n0.830\n0.872\n0.873\nCGMOS\n0.865\n0.579\n0.750\n0.653\n0.659\n0.933\n0.845\n0.887\n0.888\nrf\nOriginal\n0.872\n0.699\n0.534\n0.606\n0.611\n0.909\n0.956\n0.932\n0.932\nDup\n0.873\n0.682\n0.641\n0.661\n0.661\n0.917\n0.924\n0.921\n0.921\nSMOTE\n0.875\n0.667\n0.655\n0.661\n0.661\n0.920\n0.917\n0.918\n0.918\nB-SMOTE\n0.867\n0.653\n0.637\n0.645\n0.645\n0.920\n0.906\n0.913\n0.913\nMWMOTE\n0.878\n0.658\n0.651\n0.655\n0.655\n0.920\n0.922\n0.921\n0.921\nADASYN\n0.876\n0.663\n0.669\n0.666\n0.666\n0.919\n0.915\n0.917\n0.917\nRAMOboost\n0.874\n0.686\n0.618\n0.650\n0.651\n0.915\n0.933\n0.924\n0.924\nCGMOS\n0.884\n0.685\n0.678\n0.681\n0.681\n0.923\n0.926\n0.924\n0.924\nAdaboost.M1\nOriginal\n0.868\n0.699\n0.572\n0.629\n0.632\n0.906\n0.944\n0.925\n0.9247\nDup\n0.865\n0.622\n0.708\n0.662\n0.664\n0.922\n0.873\n0.897\n0.897\nSMOTE\n0.867\n0.608\n0.714\n0.657\n0.659\n0.923\n0.880\n0.901\n0.901\nB-SMOTE\n0.864\n0.581\n0.724\n0.644\n0.648\n0.927\n0.861\n0.893\n0.893\nMWMOTE\n0.868\n0.600\n0.708\n0.650\n0.652\n0.922\n0.880\n0.901\n0.901\nADASYN\n0.867\n0.599\n0.726\n0.657\n0.660\n0.925\n0.873\n0.898\n0.899\nRAMOboost\n0.865\n0.631\n0.699\n0.663\n0.664\n0.922\n0.882\n0.901\n0.902\nCGMOS\n0.871\n0.619\n0.728\n0.670\n0.672\n0.925\n0.882\n0.903\n0.903\nTABLE 2 A summary of AUC, Precision, Recall, F-score and G-score of all competitors for the majority and minority classes produced by 6 classifiers o the artificial datasets.\n<div style=\"text-align: center;\">THIS PAPER HAS BEEN ACCEPTTED BY CIKM 2016.</div>\nCGMOS\nOriginal\nDup\nSMOTE\nB-SMOTE\nMWMOTE\nADASYN\nRAMOboost\nBankMarket\n0.728\n0.661\n0.708\n0.718\n0.710\n0.721\n0.710\n0.723\nBloodService\n0.733\n0.653\n0.648\n0.649\n0.651\n0.720\n0.714\n0.728\nBreastCancer\n0.992\n0.992\n0.993\n0.992\n0.989\n0.991\n0.991\n0.992\nBreastTissue\n0.984\n0.899\n0.946\n0.932\n0.917\n0.937\n0.908\n0.943\nCarEvaluation\n0.997\n0.995\n0.845\n0.997\n0.994\n0.996\n0.997\n0.995\nCard\u2019graphy\n0.977\n0.976\n0.939\n0.962\n0.956\n0.925\n0.957\n0.960\nCharacterTraj\n0.985\n0.962\n0.717\n0.985\n0.978\n0.981\n0.988\n0.909\nChess\n0.977\n0.974\n0.959\n0.973\n0.977\n0.974\n0.975\n0.959\nClimateSim\n0.908\n0.908\n0.861\n0.902\n0.863\n0.901\n0.901\n0.882\nContraceptive\n0.724\n0.705\n0.699\n0.712\n0.702\n0.705\n0.702\n0.705\nFertility\n0.673\n0.615\n0.594\n0.634\n0.592\n0.604\n0.639\n0.638\nHaberman\n0.651\n0.623\n0.577\n0.600\n0.593\n0.594\n0.587\n0.586\nILPD\n0.707\n0.687\n0.693\n0.715\n0.703\n0.702\n0.693\n0.703\nImgSeg\n0.999\n0.998\n0.999\n0.997\n0.998\n0.998\n0.997\n0.998\nLeaf\n0.908\n0.880\n0.782\n0.852\n0.775\n0.836\n0.839\n0.821\nLibras\n0.945\n0.922\n0.859\n0.929\n0.886\n0.936\n0.923\n0.883\nMultipleFs\n0.998\n0.998\n0.997\n0.998\n0.997\n0.997\n0.996\n0.997\nParkinson\n0.841\n0.676\n0.692\n0.834\n0.791\n0.837\n0.842\n0.760\nPlanRelax\n0.472\n0.457\n0.494\n0.469\n0.445\n0.467\n0.488\n0.464\nQSAR\n0.901\n0.886\n0.879\n0.895\n0.863\n0.886\n0.886\n0.882\nSPECT\n0.820\n0.772\n0.803\n0.808\n0.811\n0.752\n0.801\n0.799\nSPECTF\n0.819\n0.819\n0.800\n0.805\n0.816\n0.812\n0.825\n0.795\nSeismicBumps\n0.743\n0.735\n0.712\n0.727\n0.740\n0.732\n0.715\n0.691\nStatlog\n0.998\n0.992\n0.996\n0.998\n0.990\n0.996\n0.976\n0.996\nPlatesFaults\n0.956\n0.928\n0.844\n0.954\n0.920\n0.943\n0.956\n0.881\nTAEvaluation\n0.748\n0.682\n0.644\n0.703\n0.671\n0.707\n0.665\n0.657\nUserKnowledge\n0.958\n0.837\n0.919\n0.953\n0.947\n0.951\n0.950\n0.888\nVertebral\n0.890\n0.839\n0.869\n0.855\n0.829\n0.860\n0.794\n0.872\nCustomers\n0.952\n0.930\n0.943\n0.946\n0.884\n0.902\n0.946\n0.952\nYeast\n0.925\n0.792\n0.844\n0.907\n0.898\n0.900\n0.906\n0.851\nAverage\n0.864\n0.827\n0.808\n0.844\n0.830\n0.842\n0.842\n0.830\nTABLE 3 A summary of AUC of 8 oversampling algorithms over all 30 datasets used in our evaluation. The AUC is averaged over all 6 base classifiers used in the evaluation. It could be seen from above table that CGMOS achieves best AUC measures for 24 datasets out of 30. By average, the AUC of CGMOS is at least 2 percent higher than all other competitors.\nKnn\nRf\nB-kde\nNn\nSvm\nBoost\nOriginal\n5e-5\n1e-4\n0.004\n1e-4\n0.026\n0.04\nDup\n2e-6\n5e-5\n3e-6\n0.03\n0.049\n0.004\nSMOTE\n0.003\n2e-4\n6e-6\n0.018\n0.006\n0.046\nB-SMOTE\n4e-6\n7e-6\n2e-5\n5e-4\n0.047\n5e-4\nMWMOTE\n0.046\n4e-5\n1e-5\n0.003\n0.005\n0.007\nADASYN\n8e-6\n7e-5\n9e-5\n0.005\n1e-4\n0.003\nRAMOboost\n2e-6\n5e-5\n3e-6\n0.001\n0.045\n0.035\nA summary of p-values of statistical significant tests of classification results using CGMOS against each of all the other competitors.\nthe majority class, the proposed approach in most cases is only second to the original data without oversampling. This is because the original dataset is imbalanced and it favors the majority class more than the minority class during classification. Overall, CGMOS achieves the best AUC over all tests. This is because the proposed approach takes into account both of the majority and minority classes and increases the certainties of the two classes while oversampling. The same conclusion can be made from the ROC curves shown in Fig. 2. It could be seen from the ROC curves that the proposed approach has the highest values almost everywhere. The proposed approach achieves the best result when random forest is used as the classifier. For b-kde as the classifier, the proposed approach gets the largest improvement since the design of the proposed approach uses b-kde for certainty computations. To get a closer view of the performances of all compared methods on each dataset, we show the AUC results of CGMOS\nand all other compared methods for each dataset in Table 3. The table shows that by average the AUC of CGMOS is 2 percent higher than SMOTE whose AUC is 2nd highest. Previous studies show that it is not necessary for a learning procedure to obtain best classification results when a dataset is perfectly balanced [25] [26]. How much to oversample is usually empirically determined [3]. To evaluate this aspect we performed another experiment in which we synthesized increasing number of minority samples and investigated how different amounts of new samples impact classification results. Let \u03b4 denote the difference of data samples between the majority and the minority class. We performed multiple experiments where in each round we synthesized k\u03b4 new samples of the minority class where k gradually increased from 0.5 to 5. The classification results are shown in Figure 3. As can be observed in the results, CGMOS achieves the best results in all cases. Also, observe that when increasing the number of data samples added, the results of CGMOS are much more robust compared with other approaches. Note that the results of some methods such as Dup(b-kde), B-SMOTE(knn) and B-SMOTE(Adaboost.M1) are even lower than the results at the starting point where datasets are not oversampled. This highlights the advantage of CGMOS when handling oversampling on boundary samples.\n# 4.6 Statistical Significance Analysis\nWe evaluate the statistical significance of the classification results of all competitors. Statistical significance plays a critical role in determining whether a null hypothesis should be rejected\nTHIS PAPER HAS BEEN ACCEPTTED BY CIKM 2016.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03b2/03b2465f-4510-40b6-8d9b-28efe19dac74.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1420/14202551-7b3c-4bbb-bca9-633698cab48b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ccf1/ccf161eb-097e-4bff-a4d3-37d4ad056f1b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">b-kde</div>\n<div style=\"text-align: center;\">knn</div>\n<div style=\"text-align: center;\">nn</div>\n<div style=\"text-align: center;\">Adaboost.M1</div>\nFig. 2. ROC curves of classification results. From left to right, up to down, we show the results of 6 different classifiers: b-kde, knn, svm, nn, rf and Adaboost.M1. Curves in blue are the results of the proposed CGMOS.\n<div style=\"text-align: center;\">Fig. 2. ROC curves of classification results. From left to right, up to down, we show the results of 6 different classifiers: b-kde, knn, svm, nn, rf and Adaboost.M1. Curves in blue are the results of the proposed CGMOS.</div>\nor retained, where the term null hypothesis refers to a general statement that sample observations result purely from chance. For a null hypothesis to be rejected as false, the result has to be identified as being statistically significant. To determine whether to reject a null hypothesis, a p-value has to be calculated, which is the probability of observing an effect given that the null hypothesis is true [27]. The null hypothesis is rejected if p-value is less than the significance level. The significance level is the probability of rejecting the null hypothesis given that it is true. The lower the significance level the more confident we can be in replicating the results and usually the significance level is set at 5%. Then a sample observation is determined to be statistically significant if p-value is less than 5%, which is formally written as p < 0.05 [28]. We follow the same protocols used in [29] [10] [14] and choose to use Wilcoxon signed-ranks test in this paper. Wilcoxon signed-ranks test is a nonparametric statistical procedure for comparing two samples that are paired, or related [30]. Different from t-test [31] [32] [29] whose null hypothesis is that the mean difference between pairs is zero, the null hypothesis of Wilcoxon\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31e3/31e32232-94d3-43bd-89d2-3b3ecda90620.png\" style=\"width: 50%;\"></div>\nFig. 3. Comparison of results when increasing the number of data synthesized for the minority class. The curves measure the average AUC of the ROC curves. Curves in blue are the results of the proposed CGMOS.\n<div style=\"text-align: center;\">Fig. 3. Comparison of results when increasing the number of data synthesized for the minority class. The curves measure the average AUC of the ROC curves. Curves in blue are the results of the proposed CGMOS.</div>\nsigned-ranks test is that the median difference between pairs of observations is zero. The test results are shown in Table 4. It could be seen from the table that the p-value of all tests are smaller than 0.05 and pass the test.\n# 5 CONCLUSION\nIn this paper, we address the imbalanced binary classification problem by proposing a novel minority oversampling strategy. Different from existing approaches, CGMOS does not randomly synthesize new data along decision boundaries. Instead, CGMOS computes the Bayes classification certainties for both the majority and minority classes and then synthesize new samples based on improvement of the certainties for samples in both classes. We prove that CGMOS can achieve better classification results compared with SMOTE. In addition, experimental results show that CGMOS outperforms known oversampling techniques using various metrics.\nTHIS PAPER HAS BEEN ACCEPTTED BY CIKM 2016.\n# REFERENCES\n[1] H. He and E. A. Garcia, \u201cLearning from imbalanced data,\u201d Knowledge and Data Engineering, IEEE Transactions on, vol. 21, no. 9, pp. 1263\u2013 1284, 2009. [2] G. M. Weiss, \u201cMining with rarity: a unifying framework,\u201d ACM SIGKDD Explorations Newsletter, vol. 6, no. 1, pp. 7\u201319, 2004. [3] N. V. Chawla, N. Japkowicz, and A. Kotcz, \u201cEditorial: special issue on learning from imbalanced data sets,\u201d ACM Sigkdd Explorations Newsletter, vol. 6, no. 1, pp. 1\u20136, 2004. [4] X.-Y. Liu, J. Wu, and Z.-H. Zhou, \u201cExploratory undersampling for classimbalance learning,\u201d Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 39, no. 2, pp. 539\u2013550, 2009. [5] J. Zhang and I. Mani, \u201cKnn approach to unbalanced data distributions: A case study involving information extraction,\u201d in Int\u2019l Conf. Machine learning, workshop learning from imbalanced data sets, 2003. [6] C. Drummond and R. C. Holte, \u201cC4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling,\u201d in Workshop on Learning from Imbalanced Data Sets II, International Conference on Machine Learning, 2003, pp. 1\u20138. [7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, \u201cSmote: Synthetic minority over-sampling technique,\u201d Journal of Artificial Intelligence Research, vol. 16, pp. 321\u2013357, 2002. [8] N. V. Chawla, A. Lazarevic, L. O. Hall, and K. W. Bowyer, \u201cSmoteboost: Improving prediction of the minority class in boosting,\u201d Subseries of Lecture Notes in Computer Science, p. 107, 2003. [9] H. Guo and H. L. Viktor, \u201cLearning from imbalanced data sets with boosting and data generation: the databoost-im approach,\u201d ACM SIGKDD Explorations Newsletter, vol. 6, no. 1, pp. 30\u201339, 2004. [10] S. Chen, H. He, E. Garcia et al., \u201cRamoboost: Ranked minority oversampling in boosting,\u201d Neural Networks, IEEE Transactions on, vol. 21, no. 10, pp. 1624\u20131642, 2010. [11] H. Han, W.-Y. Wang, and B.-H. Mao, \u201cBorderline-smote: a new oversampling method in imbalanced data sets learning,\u201d in Advances in intelligent computing. Springer, 2005, pp. 878\u2013887. [12] H. He, Y. Bai, E. A. Garcia, and S. Li, \u201cAdasyn: Adaptive synthetic sampling approach for imbalanced learning,\u201d in Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 1322\u20131328. [13] S. Barua, M. M. Islam, and K. Murase, \u201cA novel synthetic minority oversampling technique for imbalanced data set learning,\u201d in Neural Information Processing. Springer, 2011, pp. 735\u2013744. [14] S. Barua, M. M. Islam, X. Yao, and K. Murase, \u201cMwmote\u2013majority weighted minority oversampling technique for imbalanced data set learning,\u201d Knowledge and Data Engineering, IEEE Transactions on, vol. 26, no. 2, pp. 405\u2013425, 2014. [15] M. Sharma and M. Bilgic, \u201cMost-surely vs. least-surely uncertain,\u201d in Data Mining (ICDM), 2013 IEEE 13th International Conference on, 2013, pp. 667\u2013676. [Online]. Available: http://ieeexplore.ieee.org/stamp/ stamp.jsp?arnumber=6729551 [16] A. Elgammal, R. Duraiswami, D. Harwood, and L. S. Davis, \u201cBackground and foreground modeling using nonparametric kernel density estimation for visual surveillance,\u201d Proceedings of the IEEE, vol. 90, no. 7, pp. 1151\u20131163, 2002. [17] X. Zhang, M. L. King, and R. J. Hyndman, \u201cA bayesian approach to bandwidth selection for multivariate kernel density estimation,\u201d Computational Statistics & Data Analysis, vol. 50, no. 11, pp. 3009\u20133031, 2006. [18] M. Lichman, \u201cUCI machine learning repository,\u201d 2013. [19] J. R. Quinlan, C4.5: Programs for Machine Learning. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1993. [20] Y. Freund and R. E. Schapire, \u201cExperiments with a new boosting algorithm,\u201d in Proceedings of the Thirteenth International Conference on Machine Learning (ICML 1996), L. Saitta, Ed. Morgan Kaufmann, 1996, pp. 148\u2013156. [Online]. Available: http: //www.biostat.wisc.edu/~kbroman/teaching/statgen/2004/refs/freund.pdf [21] M. Sokolova, N. Japkowicz, and S. Szpakowicz, \u201cBeyond accuracy, f-score and roc: a family of discriminant measures for performance evaluation,\u201d in AI 2006: Advances in Artificial Intelligence. Springer, 2006, pp. 1015\u20131021. [22] T. Fawcett, \u201cRoc graphs: Notes and practical considerations for researchers,\u201d Machine learning, pp. 1\u201338, 2004. [23] \u2014\u2014, \u201cAn introduction to roc analysis,\u201d Pattern recognition letters, vol. 27, no. 8, pp. 861\u2013874, 2006. [24] C. Mohri, \u201cConfidence intervals for the area under the roc curve,\u201d in Advances in neural information processing systems, 2005, p. 305. [25] G. E. Batista, R. C. Prati, and M. C. Monard, \u201cA study of the behavior of several methods for balancing machine learning training data,\u201d ACM Sigkdd Explorations Newsletter, vol. 6, no. 1, pp. 20\u201329, 2004.\n[26] G. M. Weiss and F. Provost, \u201cLearning when training data are costly: the effect of class distribution on tree induction,\u201d Journal of Artificial Intelligence Research, pp. 315\u2013354, 2003. [27] J. L. Devore, Probability and Statistics for Engineering and the Sciences. Duxbury Press, 2011. [28] S. McKillup, Statistics Explained: An Introductory Guide for Life Scientists. Cambridge University Press, 2006. [29] J. Dem\u0161ar, \u201cStatistical comparisons of classifiers over multiple data sets,\u201d The Journal of Machine Learning Research, vol. 7, pp. 1\u201330, 2006. [30] G. W. Corder and D. I. Foreman, Nonparametric Statistics for NonStatisticians: A Step-by-Step Approach. Wiley, 2009. [31] B. F. PhD, High-Yield(TM) Behavioral Science (High-Yield Series). LWW, 2008. [32] D. W. Zimmerman, \u201cTeacher\u2019s corner: A note on interpretation of the paired-samples t test,\u201d Journal of Educational and BEhavioral Statistics, vol. 22, no. 3, pp. 349\u2013360, 1997.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of handling imbalanced datasets, which can significantly compromise the performance of standard learning algorithms. Previous methods, particularly SMOTE and its extensions, lack a theoretical guarantee for improved performance, necessitating a new approach that can ensure better classification results.",
        "problem": {
            "definition": "The problem focuses on binary classification in imbalanced datasets, where the lack of data in the minority class hinders the detection of regularities, leading to poor decision boundaries.",
            "key obstacle": "The main challenge is that existing methods often favor the majority class, resulting in biased learning and difficulty in accurately classifying minority class instances, especially in the presence of noise."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by the need to systematically consider the performance of both minority and majority classes during the oversampling process.",
            "opinion": "The idea entails a novel oversampling strategy, CGMOS, which synthesizes new samples based on the changes in certainty for both classes, improving overall classification performance.",
            "innovation": "CGMOS differs from existing methods by providing a theoretical guarantee that it will perform better than SMOTE, focusing on enhancing the certainty of both classes rather than merely balancing the dataset."
        },
        "method": {
            "method name": "Certainty Guided Minority OverSampling",
            "method abbreviation": "CGMOS",
            "method definition": "CGMOS is defined as a systematic oversampling technique that considers the certainty changes in the dataset to synthesize new minority class samples.",
            "method description": "The core of CGMOS involves interpolating between existing minority samples and their neighbors, guided by the certainty of classification outcomes.",
            "method steps": [
                "Calculate the certainty for each sample using Bayesian classification.",
                "Determine the weights for each sample based on their relative certainty changes.",
                "Randomly select samples for interpolation based on computed weights.",
                "Synthesize new samples by interpolating between selected minority samples and their neighbors."
            ],
            "principle": "The method is effective because it directly addresses the uncertainty in classification, improving the predictive performance for both minority and majority classes."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using 30 real-world datasets, with various classifiers employed for comparison, including Bayesian classifiers, KNN, SVM, neural networks, random forest, and Adaboost.",
            "evaluation method": "Performance was assessed using metrics such as Precision, Recall, F-score, G-score, and AUC, with results averaged over multiple rounds of cross-validation."
        },
        "conclusion": "The experimental results indicate that CGMOS outperforms existing oversampling techniques, demonstrating its effectiveness in improving classification performance on imbalanced datasets.",
        "discussion": {
            "advantage": "The key advantages of CGMOS include its theoretical guarantee of improved performance over SMOTE and its ability to enhance the certainty of classification for both classes.",
            "limitation": "One limitation of the method may be its reliance on the accurate estimation of certainty, which could be affected by the quality of the underlying data.",
            "future work": "Future research could explore refining the method for multi-class imbalanced datasets and integrating CGMOS with other learning strategies to further enhance performance."
        },
        "other info": {
            "additional details": {
                "accepted conference": "CIKM 2016",
                "authors": [
                    {
                        "name": "Xi Zhang",
                        "affiliation": "Illinois Institute of Technology"
                    },
                    {
                        "name": "Di Ma",
                        "affiliation": "Illinois Institute of Technology"
                    },
                    {
                        "name": "Lin Gan",
                        "affiliation": "Illinois Institute of Technology"
                    },
                    {
                        "name": "Shanshan Jiang",
                        "affiliation": "Illinois Institute of Technology"
                    },
                    {
                        "name": "Gady Agam",
                        "affiliation": "Illinois Institute of Technology"
                    }
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational idea behind semi-supervised algorithms involves leveraging both labeled and unlabeled data to improve learning outcomes, particularly in scenarios with imbalanced datasets."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of semi-supervised learning is to address the challenges posed by imbalanced datasets, which can compromise the performance of standard learning algorithms."
        },
        {
            "section number": "1.3",
            "key information": "The main goal of the paper is to introduce a novel oversampling strategy, CGMOS, which systematically considers the performance of both minority and majority classes during the oversampling process."
        },
        {
            "section number": "2.1",
            "key information": "Key terms essential to understanding the paper include 'imbalanced datasets,' 'binary classification,' and 'oversampling techniques,' particularly the limitations of existing methods like SMOTE."
        },
        {
            "section number": "3.1",
            "key information": "CGMOS is defined as a systematic oversampling technique that synthesizes new minority class samples based on the certainty changes in the dataset, providing a theoretical guarantee of improved performance."
        },
        {
            "section number": "4.1",
            "key information": "Data labeling is crucial in this context as the performance of classifiers on imbalanced datasets heavily relies on the quality and quantity of labeled minority class samples."
        },
        {
            "section number": "5.1",
            "key information": "The fundamental difference highlighted in the paper is that existing methods often favor the majority class, leading to biased learning, which CGMOS aims to rectify by enhancing certainty for both classes."
        },
        {
            "section number": "6.1",
            "key information": "The paper discusses the application of CGMOS in various real-world datasets, demonstrating its effectiveness in improving classification performance in scenarios where traditional methods fall short."
        },
        {
            "section number": "7.1",
            "key information": "Challenges addressed in the paper include the reliance on accurate estimation of certainty, which can be affected by the quality of the underlying data, impacting the effectiveness of CGMOS."
        },
        {
            "section number": "8",
            "key information": "The conclusion emphasizes that CGMOS outperforms existing oversampling techniques, indicating its potential to enhance learning accuracy in imbalanced dataset scenarios."
        }
    ],
    "similarity_score": 0.5942447022545044,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/CGMOS_ Certainty Guided Minority OverSampling.json"
}