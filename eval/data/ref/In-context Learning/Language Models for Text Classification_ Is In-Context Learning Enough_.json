{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.17661",
    "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
    "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.",
    "bib_name": "edwards2024languagemodelstextclassification",
    "md_text": "# Language Models for Text Classification: Is In-Context Learning Enough?\nAleksandra Edwards and Jose Camacho-Collados edwardsai, camachocolladosj@cardiff.ac.uk\n# Abstract\nRecent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.\narXiv:2403.17661v2 \n# 1 Introduction\nA standard approach for supervised text classification is fine-tuning language models such as BERT using an additional classifier head (Radford et al., 2018; Dong et al., 2019; Devlin et al., 2018; Yin et al., 2019; Viswanathan et al., 2023; Mosbach et al., 2023). However, these approaches require large amounts of data to achieve state-of-the-art results (Edwards et al., 2022) which makes them unsuitable for classification tasks associated with class imbalances and data sparsity (Giridhara et al.,\n2019; Zhang and Wu, 2015; T\u00fcrker et al., 2019; Yin et al., 2019). These problems often occur in real world applications where annotation of data can be performed only by scarce domain experts such as medical and legal domains or applications with highly imbalanced classes such as crime data and fraud detection (Giridhara et al., 2019; Zhang and Wu, 2015; T\u00fcrker et al., 2019). Recent advances in Natural Language Processing (NLP) lead to the emerge of an alternative approach based on using autoregressive text generation models (Radford et al., 2019) that have zeroand few- shot capabilities and perform unseen tasks through the use of prompting (Schick and Sch\u00fctze, 2021a; Radford et al., 2019; Le Scao and Rush, 2021; Viswanathan et al., 2023; Plaza-del Arco et al., 2023). The ability of these models to understand natural language instructions let them generalise to different domains and tasks without the need of large training corpora (Plaza-del Arco et al., 2023). There have been even further improvements in the performance of these models in zero-shot settings by fine-tuning them on sets of instructions (task descriptions) (Raffel et al., 2020). The promising results of these models against various benchmark datasets (Wang et al., 2022b; Liu et al., 2023; Bang et al., 2023) led to increased research into developing methods, mainly based on prompt engineering techniques (Viswanathan et al., 2023; Le Scao and Rush, 2021) for improving their generalisation capabilities. Further, there has been an increased attention into evaluating the suitability of these models for more specialised domains such as the legal, medical, and financial domain (Sarkar et al., 2021; Chalkidis et al., 2020; Yin et al., 2019; Labrak et al., 2023). However, most of the proposed approaches are domain- and task-specific. There is lack of understanding of how these models perform in comparison to more established approaches for text classification. In general, analyses are performed for a small range\nof model types, domains, and tasks. Our work is the first attempt to systematically compare how text generation models using zeroshot and one-shot learning compare to more established but data-consuming approaches for classification based on fine-tuning language models. Our goal is to identify how well current large language models (LLMs) can adapt to different text classification tasks and domains given limited information, and outline the potential strengths and weaknesses of these models. For these purposes, we evaluate five heterogeneous models of different sizes, including traditional masked language models and more recent autoregressive LLMs. Our analyses span over 16 datasets from 7 domains representing binary, multiclass, and multilabel classification. Our main contributions are as follows. First, we explore an important but understudied problem of how suitable the newly developed text generation models such as LLaMA, Flan-T5, T5, and ChatGPT are for text classification in few-shot settings compared to lighter models that require training data such as RoBERTa or FastText. In addition to the performance, our analysis helps identify specific strengths and weaknesses of each type of model. Second, in contrast to the majority of existing research focusing on optimisation techniques for prompt creation, we analyse trends in the model\u2019s performance that are non-prompt sensitive as well as look at how the amount of specificity provided in the prompt regarding the task and the domain affect the performance of the models. Third, we evaluate generalisation abilities of models for 7 domains, including real-world specialised domains, such as legal, medical, and crime data. We also analyse how the models\u2019 behaviour changes for datasets used in the pre-training stage versus when testing on unseen datasets.\n# 2 Related Work\nWe first introduce the different types of methods and models used for text classification along with their strengths and weaknesses (see Section 2.1). Then, we discuss relevant work on comparing prompting and fine-tuning approaches for text classification as well as outline challenges and research gaps within existing work (see Section 2.2).\n# 2.1 Text Classification\nWe distinguish between three main approaches for text classification, linear methods (described in Section 2.1.1), fine-tuning language models (Section\n2.1.2) and prompting techniques combined with text generation models (Section 2.1.3).\n# 2.1.1 Linear Methods\nFastText (Joulin et al., 2017) is a linear text classification model which provides a strong baseline for many text classification tasks and gives performance comparable to state-of-the-art methods, including language models such as BERT (Zhou, 2020; Edwards et al., 2020). It integrates a linear model with a rank constraint which allows sharing parameters among classes and features. It also integrates word embeddings which are averaged into a text. These features help address many problems associated with other linear models such as outof-vocabulary words and fine-grained distinctions between classes.\n# 2.1.2 Fine-tuning Methods\nLanguage models like BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019), pre-trained using a masked language modeling (MLM) objective provide a state-of-the-art performance against most standard NLP benchmarks (Wang et al., 2019, 2018). These models can be easily adapted for text classification by using fine-tuning techniques which are based on adding a single classification layer onto the model. However, fine-tuning techniques require large amounts of data to be adapted to targeted tasks and domains which makes them impractical for low resource classification tasks (Strubell et al., 2019; Peng et al., 2021; Lu et al., 2021).\n# 2.1.3 Text Generation Models\nRecent advances in NLP have led to the development of bigger models composed of billion of parameters which have shown an improved performance especially in text generation and low resource settings (Zhang et al., 2019; Black et al., 2022; Labrak et al., 2023). These text generation models such as GPT and subsequent releases (Brown et al., 2020; Radford et al., 2018, 2019) as well as LLaMA (Touvron et al., 2023a,b) and T5 (Raffel et al., 2020) can understand natural language instructions (i.e., prompts) and thus can generalise to unseen tasks and domains without the need for large computational and data resources (Brown et al., 2020). Further progress has been made by fine-tuning these models on a set of natural language instructions, consisting of descriptions of the tasks and the expected output (Efrat and Levy, 2020; Mishra et al., 2022). This enables mod-\nels to generalise even better to tasks, domains, and languages (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2022; Efrat and Levy, 2020; Mishra et al., 2022). The ability of text generation models to make predictions with little or no training makes these models particularly suitable for tackling the problem of data scarcity for text classification (Wang et al., 2020; Gupta et al., 2020). Therefore, much of the approaches in zero- and few-shot learning are focused at optimising the performance of these models mainly through the use of prompting (Gera et al., 2022; Le Scao and Rush, 2021; Deng et al., 2022; Schick and Sch\u00fctze, 2021a; Radford et al., 2019; Le Scao and Rush, 2021; Viswanathan et al., 2023; Plaza-del Arco et al., 2023).\n# 2.2 Prompting versus Fine-Tuning\nPrompting in zero- and few- shot settings, also known as in-context learning (ICL), is the process of providing natural language instructions that describe a task as an input to a language model, including the expected output (Labrak et al., 2023). In few-shot prompting, the model is presented with some training examples along with the task instructions. In contrast to fine-tuning techniques, prompting does not involve changing the weights of the model which makes the approach less resource consuming. Additionally, previous research has suggested that prompting can lead to comparable or even better performance than standard fine-tuning techniques (Gao et al., 2021; Mosbach et al., 2023). A drawback of this approach is the models\u2019 sensitivity to the prompts where slight changes of the instruction can lead to big differences in the performance (Schick and Sch\u00fctze, 2021b; Le Scao and Rush, 2021; Sun et al., 2023). Thus, much of the work on text generation models is focused on prompt optimisation techniques based on automatic generation for prompts (Wang et al., 2022a; Shin et al., 2020), quantifying the benefits of prompting (Schick and Sch\u00fctze, 2021b; Le Scao and Rush, 2021), and improving the generalisation abilities of prompts (Zhang et al., 2022; Sch\u00f6nfeld et al., 2019; Song et al., 2021; Wang et al., 2022a; Oniani et al., 2023; Sun et al., 2023) There has been an increased research into evalu-\nThere has been an increased research into evaluating and improving the performance of text generation models for zero and few shot classification in more specialised domains such as the legal, medical, and financial domains (Ge et al., 2022; Sarkar et al., 2021; Chalkidis et al., 2020). Labrak et al.\n(2023) evaluate four state-of-the-art instructiontuned large language models (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on a set of 13 realworld clinical and biomedical natural NLP tasks, including text classification. The results show that instruction-tuned models tend to be outperformed by a specialised model trained for the medical field such as PubMedBERT (Gu et al., 2021). This rises questions into the suitability of text generation models and prompting techniques for more specialised domains which require domain experts for annotation. Another research by Mosbach et al. (2023) conducts a comparison between fine-tuning and prompting techniques for two text classification datasets showing that both approaches have similar performance, although with a large variation in results depending on properties such as model size and number of examples. These works show that adapting these models to tasks, especially text classification for more specialised domains, remains a challenge. The variance in performance between tasks and models depending on the prompt design makes the generalisation of text generation models a challenging problem. The small scale on which analyses are performed does not give enough knowledge on how well prompting techniques compare to the more established models for classification across different text classification types and more challenging unfamiliar domains. In this paper, we address these challenges by performing a large-scale comparison between different model types across a wider range of classification tasks and domains.\n# 3 Experimental Setting\n# 3.1 Datasets\nFor our experiments we selected a suite of datasets representing all three classification types, i.e., binary, multiclass, and multilabel. The datasets span across 7 domains and 13 classification tasks. Specifically, we selected the Twitter datasets from the SemEval 18 on emoji prediction (Barbieri et al., 2018), SemEval 18 on irony Detection (Van Hee et al., 2018), SemEval 19 on hate detection (Basile et al., 2019), SemEval 19 on offense detection (Zampieri et al., 2019), and SemEval 19 on sentiment analysis (Nakov et al., 2019). Further, we include datasets for topic categorisation such as BBC news1, AG News (Zhang et al., 2015), Reuters (Lewis et al., 2004), and 20\n1http://mlg.ucd.ie/datasets/bbc.html\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/16a6/16a64def-5aaa-4a79-9643-ac650745862d.png\" style=\"width: 50%;\"></div>\nDataset\nDomain\nTask\nType\nClass Type\nAvg tokens\nLabels\n# Train\n# Dev\n# Test\nSemEval 18 (Emoji)\nTwitter\nEmoji Prediction\nSentence\nmulticlass\n12\n20\n45,000\n5,000\n50,000\nSemEval 18 (Irony)\nTwitter\nIrony Detection\nSentence\nbinary\n13\n2\n2,862\n955\n784\nSemEval 19 (Hateval)\nTwitter\nHateval\nSentence\nbinary\n18\n2\n9,000\n1,000\n2,970\nSemEval 19 (OffensEval)\nTwitter\nOffensEval\nSentence\nbinary\n19\n2\n11,916\n1,324\n860\nSemEval 17 (Sentiment)\nTwitter\nSentiment Analysis\nSentence\nmulticlass\n20\n3\n45,389\n2,000\n11,906\nBBC news\nNews\nTopic categorisation\nDocument\nmulticlass\n220\n5\n1602\n178\n445\nReuters\nNews\nTopic categorisation\nDocument\nmulticlass\n83\n8\n6120\n680\n2659\nAG News\nNews\nTopic categorisation\nDocument\nmulticlass\n31\n4\n103,346\n11,482\n5,928\n20 Newsgroups\nNews\nTopic categorisation\nDocument\nmulticlass\n285\n6\n9,857\n1,095\n7,290\n20 Newsgroups\nNews\nTopic categorisation\nDocument\nmulticlass\n285\n20\n9,857\n1,095\n7,290\nIMDB reviews\nReviews\nPolarity Detection\nDocument\nbinary\n231\n2\n25200\n2,800\n25,601\nOhsumed\nMedical\nCardiovascular diesese det.\nDocument\nmulticlass\n104\n23\n9,390\n1,043\n12733\nToxic Comments\nWikipedia\nToxic prediction\nDocument\nmultilabel\n46\n7\n143,614\n15,957\n63,978\nPCL dataset\nNews\nPatronising language det.\nDocument\nmultilabel\n37\n7\n517\n57\n419\nEU legislation documents\nLegislation\nLegal legislation concept det.\nDocument\nmultilabel\n27\n10\n45,000\n6,000\n6,000\nHallmarks of cancer\nMedical\nHallmarks of cancer detection\nSentence\nmultilabel\n22\n10\n12,456\n1,384\n3624\nSafeguarding reports\nSafeguarding\nTheme detection\nSentence\nmultilabel\n18\n5\n5,719\n635\n3496\nSafeguarding reports\nSafeguarding\nTheme detection\nSentence\nmultilabel\n18\n10\n5,719\n635\n3496\nTable 1: Overview of the classification datasets used in our experiments.\nNewsgroups (Lang, 1995) , as well as IMDB reviews dataset for polarity detection (Maas et al., 2011), PCL dataset for patronising language detection (Perez Almendros et al., 2020), and Toxic comments (Hosseini et al., 2017). Additionally, we evaluate models for more specialised domains representing real world applications such as EU legislation documents (Chalkidis et al., 2019) for legal legislation concepts detection, Hallmarks of cancer (Baker et al., 2015) for detecting cancer hallmarks, Ohsumed (Joachims, 1998) for cardiovascular diseases detection, and Safeguarding reports (Edwards et al., 2022) for theme detection. Additionally, we perform prediction for the top classes as well as the sub-classes of the 20 Newsgroups and Safeguarding datasets. In this way, we can analyse how the models performance is affected by the number of classes. The main features and statistics of each dataset are summarized in Table 1. For the EU legislation documents we have performed experiments with the 10 most frequent labels, similarly to Chalkidis et al. (2019). For the Ohsumed dataset, we have selected the top 23 most frequent classes, similar to prior work (Pilehvar et al., 2017).\n# 3.2 Comparison Models\nWe compare three main types of models: generative language models, masked language models, and linear models, all described below. Generative Language Models. We include LLaMA 1 (Touvron et al., 2023a) and 2 (Touvron et al., 2023b) into the analysis as representatives of large auto-regressive generation models, both with 7 billion parameters. As a representative of smaller but instruction-tuned model, we use Flan-T5 (Chung et al., 2022). The model is fine-tuned using the Flan instruction tuning tasks\ncollection (Chung et al., 2022). We use the large Flan-T5 model with 780M parameters. We have also included T5 model (Raffel et al., 2020) into our analysis which we fine-tune, similarly to RoBERTa. In particular, we use T5 base model. We have downloaded the models from Hugging Face (Wolf et al., 2019). As a representative of the GPT family of autoregressive models (Brown et al., 2020), we use OpenAI GPT 3.5-Turbo for our analysis. We added this model for completeness. However, given budget constraints and its closed nature for which few conclusions can be drawn, we only provide results for a sample of all datasets. Masked Language Models. As a representative of masked language model, we use RoBERTa (Liu et al., 2019), pre-trained on English language. It is known to achieve state-of-the-art results for many text classification tasks. We perform experiments with RobERTa base (125 million parameters) and RoBERTa large (354 million parameters) models to allow analysis into the effect of model size over the classification performance. We have downloaded the models from Hugging Face (Wolf et al., 2019). Linear Models. Finally, we use FastText (Joulin et al., 2017) (see Section 2.1.1) as a representative of a linear text classification model. Despite its simplicity the model provides a strong baseline for many text classification tasks and it is known to give comparable results to state-of-the-art methods, including language models such as BERT for some classification problems (Zhou, 2020; Edwards et al., 2020).\n# 3.3 Prompting, Training and Evaluation\nAs mentioned in Section 1, our aim is to estimate how well the text generation models perform for text classification when compared to the more data consuming models such as RoBERTa and FastText.\nTherefore, we perform experiments for Flan-T5 and LLaMA in zero- and one- shot ICL settings. For zero shot, we provide information about the task to the model through prompting. For one shot, we randomly select a single training instance per label and we provide these examples along with the instruction to the model. To ensure robustness, the random selection of training samples is performed for three iterations and the results are averaged. For generating labels for the test sequences, we use default model settings. We judge the outputs as expected class labels or not by simply checking whether the output of the model matches one of the labels for the given classification task. We experiment with three different prompts which we describe further in Section 3.4. As for RoBERTa, we fine-tune it for the classification task on the training data of each dataset using a sequence classifier, a learning rate of 2e-5 and 4 epochs. In particular, we made use of RoBERTa\u2019s Hugging Face default transformers implementation for classifying sentences (Wolf et al., 2019). As for T5, we fine-tune it using conditional generation, 2 epochs, and learning rate of 5e-5. Finally, we use FastText classifier with 25 epochs and softmax as the loss function. Finally, we report results based on standard micro and macro averaged F1 (Yang, 1999).\n# 3.4 Prompt Design\nOur paper does not focus on identifying and describing most efficient prompt engineering practices (as majority of work described in Section 2) but instead we focus on highlighting promptindependent trends in the models performance in order to help outline advantages and disadvantages of out-of-the-box approaches for few shot text classification. We selected instructions that led to satisfactory results in previous research or have been used in the training set for the instructiontuned models Flan-T5 (Sun et al., 2023; Wei et al., 2021). These prompts vary in the detail they provide about the given task and domain. We want to analyse trends across models behaviour that are non-prompt sensitive as well as look at how the amount of specificity provided in the prompt affect the performance of the models. For these purposes, we use the following three prompts: (1) generic: a prompt which does not give information about the task or domain, used in (Sun et al., 2023); (2) task: describes the given task, i.e., classification; (3) domain: a prompt which gives more informa-\ntion about the domain, for instance, it specifies the type of test data, such as an article or tweet. We have created the domain-based prompts following examples provided in Wei et al. (2021). Table 2 presents examples of the prompts per classification type2.\nBinary\nMulticlass\nMultilabel\ngeneric\nChoose\nyour\nan-\nswer:\nAccording\nto the above para-\ngraph, the question\n\u2019Is the text ironic?\u2019:\nPick one category for\nthe\nfollowing\ntext.\nThe options are:\nPick one or more from\nthe categories for the fol-\nlowing text.The options\nare:\ntask\nClassify the input\ntext into one of\nthe following cate-\ngories:\nClassify the input text\ninto one of the follow-\ning categories:\nClassify the input text us-\ning one or more from the\nfollowing categories:\ndomain Is the Tweet classi-\nfied as irony or non-\nirony?\nSelect the topic that\nthe\ngiven\nnews\nis\nabout. The topics are\n-\nWhich of the given toxic\ntopics best describe the\ngiven comment? Choose\none or more from the fol-\nlowing topics:\nTable 2: Examples of prompts used for zero- and oneshot learning for Flan-T5 and LLaMA.\nTable 2: Examples of prompts used for zero- and oneshot learning for Flan-T5 and LLaMA.\n# 4 Results and Analysis\nThe aim of our analysis is (1) identify if and how the use of prompts affect the performance of text generation models (see Section 4.1); (2) compare performance of prompting and fine-tuning techniques in order to identify strengths and weaknesses of the different models \u2013 we focus on a comparison between the three types of classification, i.e., binary, multiclass, and multilabel (see Section 4.2); and (3) perform a fine-grained analysis comparing models\u2019 performance at the domain and dataset level (see Section 4.3). In addition to this general comparison, we analyse separately the performance of closed-source GPT3.5 and models for the \u2018IMDB reviews\u2019 and \u2018AG News\u2019 datasets as they are used in the fine-tuning of the Flan-T5 model.\n# 4.1 Model and Prompt Analysis\nA comparison between the two LLaMA models shows an advantage of LLaMA 2 over LLaMA 1 for both zero- and one-shot settings across all prompt types (see Figure 1 and Table 3). The two models have similar performance in the zero-shot setting in terms of F1 score. However, the number of wrong labels for LLaMA 1 is much larger with 0.470 wrong labels compared to the 0.100 wrong labels from LLaMA2. Results in Figure 1 also show a clear advantage of Flan-T5 over the other models for all three prompts in terms of micro- and macro- F1 for both zero- and one- shot settings.\n2A list of all prompts is given in the Appendix\nThe Flan-T5 model also leads to smaller number of wrong labels in zero-shot prompting. This suggests that smaller but instruction-tuned models can be more beneficial in zero- and few- shot classification in comparison to larger text generation models. Specifically, Flan-T5 has on average 0.110 improvement in micro- and macro-F1 for both zeroand one- shot settings over LLaMA 2. Further analysis into the prompts reveal that prompt choice does not lead to significant changes in the models behaviour where the deviation for the three prompts across all models is relatively small. For instance, for LLaMA 1 and LLaMA 2 is less than 0.02 difference in micro-F1 for both zero- and one- shot settings while for Flan-T5 it gradually decreases from 0.07 in zero-shot to 0.01 for one-shot. This suggests that smaller models such as Flan-T5 are more sensitive to the prompt in zero settings versus few shot learning. The benefits from oneshot prompting are evident across all three models where the F1 measure tends to increase and the number of wrong labels decreases. Flan-T5 improves its performance on a higher rate compared to to the other two models with around 0.047 increase in the micro-F1 score versus 0.027 increase for LLaMA 2. This illustrates the strong abilities of these models to learn tasks with minimal amount of training data.\n# 4.2 Prompting versus Fine-tuning\nResults in Figure 2 show the same trends for prompting methods where Flan-T5 outperforms LLaMA 1 and LLaMA 2 for all text classification types in terms of micro- and macro-F1. All three models improve their performance for one-shot prompting regarding the number of wrong labels. In one shot setting, Flan-T5 and LLaMA 2 tend to have close to 0 wrong labels with LLaMA 2 returning slightly lower number of irrelevant results, while Flan-T5 has a better F1 score (see Figures 2 and 33). The advantage of LLaMA 2 over LLaMA 1 is clearly shown for all classification tasks, especially binary and multilabel where LLaMA 2 has a smaller number of irrelevant results and higher F1 score (see Figure 2). Regarding fine-tuning approaches, results in Figure 2 show a clear dominance of RoBERTa-large in one-shot setting for all classification types. When fine-tuning is performed using the entire dataset, T5 outperforms the rest of the models for binary classification with micro-F1 = 0.672 versus RoBERTa-\n3Macro-F1 results are available in the Appendix.\nlarge with micro-F1 = 0.607. However, for multiclass and multilabeling tasks, the performance of T5 decreases and the model is outperformed by both RoBERTa-base and RoBERTa-large. For instance, for multiclass problems RoBERTa-large achieves micro-F1 of 0.726 versus micro-F1 for T5 = 0.700. For multilabeling problems the performance gap between the models increases and RoBERTa-large has a micro-F1 = 0.788 versus T5 with micro-F1 = 0.718. These results suggest that fine-tuned masked language models are more suitable for complex classification tasks such as multiclass and multilabeling problems when the number of labels is higher versus fine-tuning text-to-text models such as T5. A comparison between prompting and finetuning techniques for low resource settings suggests a better performance of prompting for binary and multiclass problems (see Figure 2) where FlanT5 and LLaMA 2 outperform fine-tuning models by a significant margin. For instance, Flan-T5 has micro-F1 = 0.553 versus micro-F1 for RoBERTalarge with micro-F1 = 0.485 for binary classification in one shot settings. The advantage of prompting in one shot settings becomes even more evident for multiclass problems where Flan-T5 achieves micro-F1 = 0.489 versus RoBERTa-large with micro-F1 = 0.162. However, for multilabeling problems, fine-tuning approaches outperform prompting methods with a difference in microF1 of 0.082 between the best fine-tuned model, RoBERTa-large, and the best prompting model, i.e. Flan-T5. It is worth noting that during one-shot training, all models have been provided the same training examples. However, further analyses are needed to identify most efficient ways for representing multi-labeling problems as part of prompting techniques. Despite the better overall performance of prompting techniques in zero- and one- shot settings, these approaches lead to unsatisfactory performance when compared to fine-tuned masked language models on a larger training set. Further, the difference in the performance between the two techniques grows larger for more complex text classification tasks such as multiclass and multilabeling problems. For instance, for binary classification, the difference in performance in terms of micro-F1 between best performing prompting and fine-tuning technique is 0.119 while for multiclass the difference in performance is 0.240. This shows that large autoregressive text generation models coupled with\nModel\nPrompt\nzero shot\none shot\nall\nmicro F1\nmacro F1\nmissing labs\nmicro F1\nmacro F1\nmissing labs\nmicro F1\nmacro F1\nFlan-T5\ngeneric\n.510\n.459\n.076\n.446\n.401\n.020\n\u2013\n\u2014\ntask\n.368\n.373\n.055\n.462\n.415\n.012\n\u2013\n\u2014\ndomain\n.369\n.302\n.092\n.480\n.432\n.072\n\u2013\n\u2014\nAVG\n.416\n.378\n.074\n.463\n.416\n.035\n\u2014\n\u2014\nLLaMA 1\ngeneric\n.309\n.213\n.484\n.274\n.274\n.043\n\u2013\n\u2014\ntask\n.319\n.230\n.471\n.339\n.303\n.414\n\u2013\n\u2014\ndomain\n.284\n.235\n.463\n.318\n.270\n.066\n\u2013\n\u2014\nAVG\n.304\n.279\n.469\n.311\n.267\n.038\n\u2013\n\u2014\nLLaMA 2\ngeneric\n.332\n.282\n.086\n.305\n.253\n.436\n\u2013\n\u2014\ntask\n.286\n.238\n.061\n.333\n.282\n.679\n\u2013\n\u2014\ndomain\n.309\n.269\n.153\n.360\n.322\n.007\n\u2013\n\u2014\nAVG\n.309\n.263\n.100\n.336\n.288\n.006\n\u2013\n\u2014\nT5\n\u2014\n\u2013\n\u2014\n\u2014\n.134\n.109\n.851\n.702\n.625\nRoBERTa (base)\n\u2014\n\u2013\n\u2014\n\u2014\n.273\n.207\n\u2014\n.707\n.625\nRoBERTa (large)\n\u2014\n\u2013\n\u2014\n\u2014\n.338\n.278\n\u2014\n.727\n.657\nfastText\n\u2014\n\u2014\n\u2014\n\u2014\n.254\n.164\n\u2014\n.505\n.419\nTable 3: Prompt Analysis where Micro-F1 and Macro-F1 results averaged across all datasets, comparing the performance of Flan-T5, LLaMA 1, and LLaMA 2 models for all three types of prompts, i.e., \u2018generic\u2019, \u2018task\u2019, and \u2018domain\u2019 as well as the average (\u2018AVG\u2019) between them. \u2018Missing labs\u2019 shows the fraction of results returned by the three models that are different from the classification labels. Results are displayed for zero-shot (\u2018zero\u2019) and one-shot setting (\u2018one\u2019).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/10bc/10bcac6d-d1e7-4d28-9997-ddeec50609f7.png\" style=\"width: 50%;\"></div>\nFigure 1: Micro-F1 (left) and Macro-F1 (middle) results averaged across all datasets, comparing the performance o Flan-T5, LLaMA 1, and LLaMA 2 models for all three types of prompts, i.e., \u2019generic\u2019, \u2019task\u2019, and \u2019domain\u2019 as wel as the average (\u2019AVG\u2019) between them. \u2019Missing label\u2019 (right) shows the fraction of results returned by the three models that are different from the classification labels. Results are displayed for zero-shot (\u2019zero\u2019) and one-sho setting (\u2019one\u2019).\n<div style=\"text-align: center;\">Figure 1: Micro-F1 (left) and Macro-F1 (middle) results averaged across all datasets, comparing the performance o Flan-T5, LLaMA 1, and LLaMA 2 models for all three types of prompts, i.e., \u2019generic\u2019, \u2019task\u2019, and \u2019domain\u2019 as wel as the average (\u2019AVG\u2019) between them. \u2019Missing label\u2019 (right) shows the fraction of results returned by the thre models that are different from the classification labels. Results are displayed for zero-shot (\u2019zero\u2019) and one-sho setting (\u2019one\u2019).</div>\nfew shot learning techniques still have room for improvement when it comes to text classification. Fine-tuned masked language models, despite being smaller, lead to better performance for text classification versus LLMs in ICL settings.\n# 4.3 Trends across datasets and models\nResults presented in Table 4 confirm findings from Section 4.2 showing a clear dominance of Flan-T5 over LLaMA for zero- and one-shot prompting for the majority of datasets. Exceptions are the \u2019irony\u2019, \u2019sentiment\u2019, and \u2019PCL\u2019 datasets where LLaMA performs better for either zero or one shot setting, or both. For some datasets such as \u2018hate\u2019, prompting models give better performance in zero- shot than one-shot setting. However, models still improve performance for these datasets in terms of number of wrong labels. Further, the choice of one shot training instances can influence the performance of models in few-shot learning. For the\npurposes of this analysis we have selected the one shot examples randomly. Analysing the impact of the training examples in few-shot learning can be a future research direction which we leave for future work. In contrast to the prompting approaches, results for the fine-tuned models do not show a clear dominance of either RoBERTa or T5. T5 shows a better performance for the majority of the binary classification tasks (those associated with Twitter datasets) as well as the datasets \u2018AG news\u2019, \u201820 News\u2019 (top 6 classes)\u2019, and the \u2018legal\u2019 domain. The two models attain a similar macro-F1 for the emoji prediction and safeguarding reports datasets. Impact of the number of labels. Analysis into the effect of the number of classification labels in the performance shows an interesting trend with the fine-tuned models (RoBERTa and T5) performing slightly better for classification tasks with 6 to 9 la-\nDataset\nModel\nzero shot\none shot\nall\nmicro F1\nmacro F1\nwrong labs\nmicro F1\nmacro F1\nwrong labs\nmicro F1\nmacro F1\nirony\nRoBERTa\n\u2013\n\u2013\n\u2013\n.459 (\u00b1.005)\n.459 (\u00b1.005)\n\u2013\n.508\n.508\nT5\n\u2014\n\u2014\n\u2014\n.455(\u00b1 .021)\n.455(\u00b1 .021)\n.589\n.688\n.688\nFlanT5\n.428\n.428\n.049\n.491(\u00b1 .034)\n.491(\u00b1.034)\n.009\n\u2013\n\u2013\nLLaMA\n.499\n.499\n.214\n.443 (\u00b1.003)\n.443 (\u00b1.003)\n.000\n\u2013\n\u2013\nGPT 3.5*\n.727\n.727\n.000\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\noffense\nRoBERTa\n\u2013\n\u2013\n\u2013\n.550 (\u00b1.143)\n.550 (\u00b1.143)\n\u2013\n.705\n.705\nT5\n\u2013\n\u2013\n\u2013\n.462 (\u00b1.001)\n.462 (\u00b1.001)\n.864\n.709\n.709\nFlanT5\n.429\n.429\n.269\n.558(\u00b1.019)\n.558(\u00b1.019)\n.003\n\u2013\n\u2013\nLLaMA\n.419\n.419\n.227\n.347 (\u00b1.026)\n.347 (\u00b1.026)\n.001\n\u2013\n\u2013\nGPT 3.5*\n.635\n.635\n.000\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nhate\nRoBERTa\n\u2013\n\u2013\n\u2013\n.445 (\u00b1.118)\n.445 (\u00b1.118)\n\u2013\n.607\n.607\nT5\n\u2014\n\u2014\n\u2013\n.386 (\u00b1.312)\n.386 (\u00b1.312)\n.732\n.619\n.619\nFlanT5\n.634\n.634\n.004\n.611 (\u00b1.006)\n.611 (\u00b1.006)\n.005\n\u2013\n-\nLLaMA\n.539\n.539\n.004\n.514 (\u00b1.111)\n.514 (\u00b1.111)\n.000\n\u2013\n\u2013\nemoji\nRoBERTa\n\u2014\n\u2014\n\u2013\n.047 (\u00b1.009)\n.005 (\u00b1.001)\n\u2013\n.366\n.317\nT5\n\u2014\n\u2014\n\u2014\n.000 (\u00b1.000)\n.000 (\u00b1.000)\n.100\n.259\n.317\nFlanT5\n.059\n.042\n.036\n.114(\u00b1.021)\n.082(\u00b1.007)\n.006\n\u2013\n\u2013\nLLaMA\n.060\n.041\n.091\n.033(\u00b1.036)\n.020(\u00b1.017)\n.001\n\u2013\n\u2013\nsentiment\nRoBERTa\n\u2014\n\u2014\n\u2013\n.449 (\u00b1.108)\n.271 (\u00b1.008)\n\u2013\n.714\n.714\nT5\n\u2014\n\u2013\n\u2013\n.312 (\u00b1.121)\n.272 (\u00b1.078)\n.563\n.708\n.709\nFlanT5\n.459\n.402\n.109\n.417 (\u00b1.004)\n.381 (\u00b1.007)\n.000\n\u2013\n\u2013\nLLaMA\n.369\n.334\n.027\n.482 (\u00b1.037)\n.402 (\u00b1.143)\n.000\n\u2013\n\u2013\nBBC\nRoBERTa\n\u2014\n\u2014\n\u2013\n.217 (\u00b1.027)\n.112 (\u00b1.029)\n\u2013\n.989\n.989\nT5\n\u2014\n\u2014\n\u2013\n.001 (\u00b1.001)\n.001 (\u00b1.001)\n.999\n.977\n.977\nFlanT5\n.922\n.867\n.096\n.939(\u00b1.008)\n.936(\u00b1.009)\n.038\n\u2013\n\u2013\nLLaMA\n.498\n.439\n.021\n.849 (\u00b1.098)\n.843 (\u00b1.081)\n.004\n\u2013\n\u2013\nGPT 3.5*\n.912\n.913\n.000\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nReuters\nRoBERTa\n\u2014\n\u2014\n\u2013\n.154 (\u00b1.111)\n.054 (\u00b1.021)\n\u2013\n.939\n.869\nT5\n\u2014\n\u2014\n\u2013\n.010 (\u00b1.034)\n.010 (\u00b1.067)\n.990\n.929\n.833\nFlanT5\n.321\n.334\n.334\n.467 (\u00b1.023)\n.504 (\u00b1.032)\n.017\n\u2013\n\u2013\nLLaMA\n.212\n.168\n.006\n.528 (\u00b1.076)\n.304 (\u00b1.145)\n.006\n\u2013\n\u2013\nGPT 3.5*\n.852\n.718\n.000\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n20 News(all)\nRoBERTa\n\u2014\n\u2014\n\u2013\n.190 (\u00b1.028)\n.101 (\u00b1.019)\n\u2013\n.859\n.853\nT5\n\u2014\n\u2014\n\u2013\n.000 (\u00b1.000)\n.000 (\u00b1.000)\n.999\n.861\n.854\nFlanT5\n.564\n.520\n.001\n.684 (\u00b1.008)\n.654 (\u00b1.007)\n.057\n\u2013\n\u2013\nLLaMA\n.324\n.272\n.094\n.368 (\u00b1.034)\n.300(\u00b1.079)\n.001\n\u2013\n\u2013\n20 News(subcl)\nRoBERTa\n\u2014\n\u2014\n\u2013\n.055 (\u00b1.013)\n.015 (\u00b1.003)\n\u2013\n.741\n.728\nT5\n\u2014\n\u2014\n\u2013\n.000 (\u00b1.000)\n.000 (\u00b1.000)\n.990\n.717\n.693\nFlanT5\n.510\n.507\n.000\n.512 (\u00b1.013)\n.501 (\u00b1.013)\n.011\n\u2013\n\u2013\nLLaMA\n.185\n.194\n.167\n.376 (\u00b1.015)\n.342 (\u00b1.014)\n.020\n\u2013\n\u2013\nOhsumed\nRoBERTa\n\u2014\n\u2014\n\u2013\n.025 (\u00b1.019)\n.002 (\u00b1.004)\n\u2013\n.476\n.415\nT5\n\u2014\n\u2014\n\u2013\n.002 (\u00b1.001)\n.002 (\u00b1.001)\n.958\n.452\n.362\nFlanT5\n.306\n.283\n.194\n.288 (\u00b1.003)\n.241 (\u00b1.001)\n.375\n\u2013\n\u2013\nLLaMA\n.151\n.099\n.154\n.180 (\u00b1.110)\n.162 (\u00b1.110)\n.036\n\u2013\n\u2013\nToxic\nRoBERTa\n\u2014\n\u2014\n\u2013\n.671 (\u00b1.005)\n.550 (\u00b1.003)\n\u2013\n.899\n.782\nT5\n\u2014\n\u2014\n\u2013\n.020 (\u00b1.001)\n.010 (\u00b1.011)\n.989\n.913\n.661\nFlanT5\n.629\n.380\n.140\n.710 (\u00b1.066)\n.262 (\u00b1.014)\n.003\n\u2013\n\u2013\nLLaMA\n.331\n.142\n.211\n.005 (\u00b1.079)\n.002 (\u00b1.077)\n.004\n\u2013\n\u2013\nLegal\nRoBERTa\n\u2014\n\u2014\n\u2013\n.429 (\u00b1.030)\n.285 (\u00b1.030)\n\u2013\n.965\n.601\nT5\n\u2014\n\u2014\n\u2013\n.500 (\u00b1.037)\n.125 (\u00b1.042)\n.970\n.982\n.612\nFlanT5\n.251\n.233\n.000\n.351 (\u00b1.047)\n.352 (\u00b1.028)\n.000\n\u2013\n\u2013\nLLaMA\n.224\n.167\n.069\n.269 (\u00b1.091)\n.232 (\u00b1.175)\n.005\n\u2013\n\u2013\nCancer\nRoBERTa\n\u2014\n\u2014\n\u2013\n.309 (\u00b1.003)\n.290 (\u00b1.002)\n\u2013\n.524\n.414\nT5\n\u2014\n\u2014\n\u2013\n.000 (\u00b1.000)\n.000 (\u00b1.000)\n.000\n.344\n.157\nFlanT5\n.296\n.286\n.246\n.361 (\u00b1.027)\n.319 (\u00b1.017)\n.000\n\u2013\n\u2013\nLLaMA\n.249\n.178\n.141\n.168 (\u00b1.131)\n.104 (\u00b1.098)\n.004\n\u2014\n\u2013\nPCL\nRoBERTa\n\u2014\n\u2013\n\u2013\n.555 (\u00b1.004)\n.518 (\u00b1.006)\n\u2013\n.719\n.592\nT5\n\u2014\n\u2014\n\u2013\n.001 (\u00b1.001)\n.001 (\u00b1.001)\n.999\n.654\n.525\nFlanT5\n.224\n.124\n.000\n.224 (\u00b1.008)\n.141 (\u00b1.112)\n.001\n\u2013\n\u2013\nLLaMA\n.392\n.303\n.050\n.287 (\u00b1.095)\n.159 (\u00b1.114)\n.000\n\u2013\n\u2013\nGPT 3.5*\n.207\n.117\n.000\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nSafeguard(all)\nRoBERTa\n\u2014\n\u2014\n\u2013\n.601 (\u00b1.011)\n.589 (\u00b1.011)\n\u2013\n.905\n.895\nT5\n\u2014\n\u2014\n\u2013\n.000 (\u00b1.000)\n.000 (\u00b1.000)\n.000\n.756\n.725\nFlanT5\n.347\n.326\n.000\n.392 (\u00b1.007)\n.360 (\u00b1.003)\n.000\n\u2013\n\u2013\nLLaMA\n.291\n.233\n.041\n.286 (\u00b1.007)\n.197 (\u00b1.003)\n.001\n\u2013\n\u2013\nGPT 3.5*\n.369\n.340\n.000\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nSafeguard(subcl)\nRoBERTa\n\u2014\n\u2014\n\u2013\n.247 (\u00b1.105)\n.201 (\u00b1.102)\n\u2013\n.718\n.515\nT5\n\u2014\n\u2014\n\u2013\n.010 (\u00b1.002)\n.020 (\u00b1.002)\n.969\n.657\n.516\nFlanT5\n.275\n.253\n.000\n.281 (\u00b1.012)\n.265 (\u00b1.009)\n.000\n\u2013\n\u2013\nLLaMA\n.195\n.176\n.051\n.237 (\u00b1.049)\n.231 (\u00b1.089)\n.006\n\u2013\n\u2013\nGPT 3.5*\n.359\n.366\n.012\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nable 4: Micro-F1 and Macro-F1 results per dataset for RoBERTa (large), fine-tuned T5, Flan-T5, LLaMA 2, an\nTable 4: Micro-F1 and Macro-F1 results per dataset for RoBERTa (large), fine-tuned T5, Flan-T5, LLaMA 2, an GPT 3.5-Turbo. The ratio of wrongly-formatted outputs is included in the wrong labels (labs) column.The resul for Flan-T5 and LLaMA 2 are based on averaged results across all prompts.\nDataset\nModel\nzero shot\none shot\nall\nmicro F1\nmacro F1\nwrong labs\nmicro F1\nmacro F1\nwrong labs\nmicro F1\nmacro F1\nIMDB\nRoBERTa\n\u2014\n\u2014\n\u2013\n.436 (\u00b1.311)\n.436 (\u00b10.311)\n\u2013\n.955\n.955\nT5\n\u2014\n\u2014\n\u2014-\n.751 (\u00b1.065)\n.751 (\u00b1.065)\n.711\n.952\n.952\nFlanT5\n.948\n.948\n.097\n.900 (\u00b1.007)\n.900 (\u00b1.007)\n.017\n\u2013\n\u2013\nLLaMA\n.628\n.628\n.219\n.803 (\u00b1.012)\n.803 (\u00b1.012)\n.005\n\u2013\n\u2013\nAG News\nRoBERTa\n\u2014\n\u2014\n\u2013\n.280 (\u00b1.022)\n.111 (\u00b1.024)\n\u2013\n.906\n.884\nT5\n\u2014\n\u2014\n\u2013\n.010 (\u00b1.003)\n.010 (\u00b1.003)\n.990\n.907\n.886\nFlanT5\n.819\n.789\n.000\n.813 (\u00b1.008)\n.782(\u00b1.009)\n.000\n\u2013\n\u2013\nLLaMA\n.479\n.463\n.011\n.787 (\u00b1.006)\n.753 (\u00b1.005)\n.003\n\u2013\n\u2013\nTable 5: Micro- and Macro-F1 results for \u2018AG News\u2019 and \u2018IMDB\u2019 datasets for RoBERTa-large, fine-tuned T5 model Flan-T5, LLaMA 2. The ratio of wrongly-formatted outputs is included in the wrong labels (labs) column. The results for Flan-T5 and LLaMA 2 are based on averaged results across all prompts.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1c9/d1c961e5-dd6b-4bfc-aad3-d54d2286284d.png\" style=\"width: 50%;\"></div>\nFigure 2: Comparison between prompting (left) and fine-tuning (right) approaches per text classification type where \u2019AVG\u2019 refers to averaged results across all prompt types per model. In \u2019Prompting\u2019, \u2019zero\u2019 and \u2019one\u2019 refer to zero- and one- shot prompt-based learning techniques, in \u2019Fine Tuning\u2019, \u2019one\u2019 refers to fine-tuning the models with one training instance per label and \u2019all\u2019 refers to fine-tuning using the entire dataset.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c7e/8c7e6390-b28d-47bc-8df9-766122af27f7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Wrong labels for prompting approaches per binary (left), multiclass (middle), and multilabel (right) classification where \u2019zero\u2019 refers to zero-shot learning and \u2019one\u2019 refers to one-shot learning.</div>\nbels than classification with less labels (see Figure 4). For RoBERTa this trend occurs for both microF1 and macro-F1 while for T5 it appears only for micro-F1. This can be attributed to the nature of the binary classification tasks (\u2019irony\u2019, \u2019offense\u2019, \u2019hate\u2019) which express human emotions and repre-\nsent the Twitter domain. This suggests that the models find it more challenging to categorise such texts versus more categorical-based datasets such as news and articles which are part of the datasets with 6 to 9 labels. In contrast, the performance of both prompting approaches decreases as the number of labels for the classification task increases.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5832/5832a3be-244a-4416-8fd2-0caf37a2667a.png\" style=\"width: 50%;\"></div>\nFigure 4: Averaged Micro-F1 and Macro-F1 results based on number of classification labels: \u2018RoBERTa (all)\u2019 and \u2018T5 (all)\u2019 refer to models fine-tuned on the entire training set, \u2018Flan-T5 (one)\u2019 and \u2018LLaMA (one)\u2019 refer to one-shot prompting.\n<div style=\"text-align: center;\">Figure 4: Averaged Micro-F1 and Macro-F1 results based on number of classification labels: \u2018RoBERTa (all)\u2019 and \u2018T5 (all)\u2019 refer to models fine-tuned on the entire training set, \u2018Flan-T5 (one)\u2019 and \u2018LLaMA (one)\u2019 refer to one-shot prompting.</div>\nDatasets used for pre-training. As mentioned earlier in the section, we analyse the performance of models for the \u2018IMDB reviews\u2019 and \u2018AG News\u2019 datasets separately as they are used in the finetuning of the Flan-T5 model. For these datasets (see Table 5) Flan-T5 performance significantly improves achieving micro- and macro-F1 results comparable to fine-tuning models on the entire dataset. For instance, for the IMDB dataset, the difference in macro-F1 between Flan-T5 and RoBERTa is 0.007 while for the AG news the difference in macro-F1 is 0.027. In contrast, the performance gap for the rest of the datasets between Flan-T5 and the best performing fine-tuning model is on average around 0.250 in micro-F1. This shows the significant impact that data contamination may have in the final results. However, a careful data contamination analysis becomes harder on large models for which training data is not available, and especially for closed models. GPT Analysis. Table 4 presents zero-shot prompting results for the GPT 3.5-Turbo model for the fol-\nlowing datasets: \u2018irony\u2019, \u2018offense\u2019, \u2018bbc\u2019, \u2018reuters\u2019, \u2018pcl\u2019, and \u2018safeguard\u2019. We have used the classbased prompt for prompting with GPT 3.5 because it has shown to lead to the higher overall performance for Flan-T5 and LLaMA. Results show a clear advantage of the GPT-based model over FlanT5 and LLaMA achieving on average 0.350 higher micro- and macro-F1 across the majority of the datasets, except for the \u2018PCL\u2019 dataset. Additionally, results achieved with zero-shot learning with GPT 3.5-Turbo outperform fine-tuned models on the entire dataset for the \u2018irony\u2019 dataset. However, for the rest of the datasets the model is still outperformed by fine-tuning approaches confirming the lack of generalisation abilities of few-shot learning techniques and text generation models for text classification.\n# 5 Conclusions\nThis paper presents a large-scale study on how prompt-based LLMs in zero- and one- shot settings compare to smaller but fine-tuned language models for text classification. The evaluation spans across 16 datasets covering binary, multiclass, and multilabel problems. In particular, we compared three different types of models, i.e., linear models such as FastText, masked language models (RoBERTa), and text generation models tested in ICL settings (T5, Flan-T5, and LLaMA, as well as GPT 3.5-Turbo). Analyses on prompting techniques showed a clear advantage of the Flan-T5 model over LLaMA 1 and LLaMA 2 regardless of the prompt used for both zero- and one-shot settings. This shows that smaller but instructiontuned models have better generalisation abilities for text classification than larger text generation models. Further, our analysis showed that results from zero- and few-shot learning LLMs are considerably lower in comparison to smaller models fine-tuned on the entire training set. This highlights the need for training data, even in the age of LLMs, and that fine-tuning smaller and more efficient language models can still outperform incontext learning methods of larger text generation models.\n# 6 Acknowledgements\nAleksandra Edwards and Jose Camacho-Collados are supported by a UKRI Future Leaders Fellowship. The safeguarding documents used for performing analysis in the paper have been collected in col-\nlaboration with the Wales Safeguarding Repository (WSR) project, funded by the National Independent Safeguarding Board (NISB), the Crime and Security Research Institute at Cardiff University (CSRI), and the School of Social Sciences at Cardiff University (SOCSI). We would like to thank the WSR team for their support.\n# 7 Limitations\nThe main limitation of this research is the lack of experiments on fine-tuning Flan-T5 and LLaMA models as well as the lack of further analysis with larger text generation models such as LLaMA with 13 and 17 billion parameters. Moreover, the paper presents a study for zero- and one-shot prompting. As future work, we plan to extend analysis to understand how the number of training instances affect the performance of in-context learning approaches. Further, considering the sensitivity of in-context learning approaches to the given instructions, it would be beneficial to perform further analysis on a larger more diverse set of prompts. Finally, the paper presents results for a single high resource language (English). Experiments for other languages (especially low-resource) could show a different tendency.\n# References\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan H\u00f6gberg, Ulla Stenius, and Anna Korhonen. 2015. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics, 32(3):432\u2013440.\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan H\u00f6gberg, Ulla Stenius, and Anna Korhonen. 2015. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics, 32(3):432\u2013440.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.\nFrancesco Barbieri, Jose Camacho-Collados, Francesco Ronzano, Luis Espinosa Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio Saggion. 2018. Semeval 2018 task 2: Multilingual emoji prediction. In Proceedings of the 12th international workshop on semantic evaluation, pages 24\u201333.\nValerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. 2019. Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter. In Proceedings of the 13th international workshop on semantic evaluation, pages 54\u201363.\nAnthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode# 5\u2013Workshop on Challenges & Perspectives in Creating Large Language Models, pages 95\u2013136. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on eu legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6314\u2013 6322. Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. An empirical study on largescale multi-label text classification including few and zero-shot labels. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7503\u20137515. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3369\u20133391. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems, 32. Aleksandra Edwards, Jose Camacho-Collados, H\u00e9l\u00e8ne De Ribaupierre, and Alun Preece. 2020. Go simple and pre-train on domain-specific corpora: On the role of training data for text classification. In Proceedings of the 28th international conference on computational linguistics, pages 5522\u20135529. Aleksandra Edwards, Asahi Ushio, Jose CamachoCollados, Helene Ribaupierre, and Alun Preece. 2022.\nIlias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on eu legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6314\u2013 6322.\nIlias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. An empirical study on largescale multi-label text classification including few and zero-shot labels. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7503\u20137515.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems, 32.\nAleksandra Edwards, Asahi Ushio, Jose CamachoCollados, Helene Ribaupierre, and Alun Preece. 2022.\nGuiding generative language models for data augmentation in few-shot text classification. In Proceedings of the Fourth Workshop on Data Science with Humanin-the-Loop (Language Advances), pages 51\u201363.\ntation in few-shot text classification. In Proceedings of the Fourth Workshop on Data Science with Humanin-the-Loop (Language Advances), pages 51\u201363. Avia Efrat and Omer Levy. 2020. The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816\u20133830. Yao Ge, Yuting Guo, Yuan-Chi Yang, Mohammed Ali Al-Garadi, and Abeed Sarker. 2022. Few-shot learning for medical text: A systematic review. arXiv preprint arXiv:2204.14081. Ariel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat Ein-Dor, and Noam Slonim. 2022. Zero-shot text classification with self-training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1107\u20131119, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Praveen Kumar Badimala Giridhara, Chinmaya Mishra, Reddy Kumar Modam Venkataramana, Syed Saqib Bukhari, and Andreas Dengel. 2019. A study of various text augmentation techniques for relation classification in free text. ICPRAM, 3:5. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1\u201323. Aakriti Gupta, Kapil Thadani, and Neil O\u2019Hare. 2020. Effective few-shot classification with transfer learning. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1061\u2013 1066. Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. Deceiving google\u2019s perspective api built for detecting toxic comments. arxiv 2017. arXiv preprint arXiv:1702.08138. Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In European conference on machine learning, pages 137\u2013142. Springer. Armand Joulin, \u00c9douard Grave, Piotr Bojanowski, and Tom\u00e1\u0161 Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427\u2013431.\nAvia Efrat and Omer Levy. 2020. The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816\u20133830.\nYao Ge, Yuting Guo, Yuan-Chi Yang, Mohammed Ali Al-Garadi, and Abeed Sarker. 2022. Few-shot learning for medical text: A systematic review. arXiv preprint arXiv:2204.14081.\nAriel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat Ein-Dor, and Noam Slonim. 2022. Zero-shot text classification with self-training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1107\u20131119, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nPraveen Kumar Badimala Giridhara, Chinmaya Mishra, Reddy Kumar Modam Venkataramana, Syed Saqib Bukhari, and Andreas Dengel. 2019. A study of various text augmentation techniques for relation classification in free text. ICPRAM, 3:5.\nYanis Labrak, Mickael Rouvier, and Richard Dufour. 2023. A zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks. arXiv preprint arXiv:2307.12114. Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the 12th International Conference on Machine Learning, pages 331\u2013339, Tahoe City, California. Teven Le Scao and Alexander M Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627\u20132636. David D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research, 5(Apr):361\u2013397. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Jinghui Lu, Maeve Henchion, Ivan Bacher, and Brian Mac Namee. 2021. A sentence-level hierarchical bert model for document classification with limited labelled data. In Discovery Science: 24th International Conference, DS 2021, Halifax, NS, Canada, October 11\u201313, 2021, Proceedings 24, pages 231\u2013 241. Springer. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142\u2013150. Association for Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487. Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938. Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin Stoyanov. 2019. Semeval2016 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.01973.\nMarius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938.\nPreslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin Stoyanov. 2019. Semeval2016 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.01973.\nDavid Oniani, Jordan Hilsman, Hang Dong, Fengyi Gao, Shiven Verma, and Yanshan Wang. 2023. Large language models vote: Prompting for rare disease identification. arXiv preprint arXiv:2308.12890.\nDavid Oniani, Jordan Hilsman, Hang Dong, Fengyi Gao, Shiven Verma, and Yanshan Wang. 2023. Large language models vote: Prompting for rare disease identification. arXiv preprint arXiv:2308.12890.\nGao, Shiven Verma, and Yanshan Wang. 2023. Large language models vote: Prompting for rare disease identification. arXiv preprint arXiv:2308.12890. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744. Bo Peng, Emmanuele Chersoni, Yu-Yin Hsu, and ChuRen Huang. 2021. Is domain adaptation worth your investment? comparing bert and finbert on financial tasks. In Proceedings of the Third Workshop on Economics and Natural Language Processing, pages 37\u201344. Carla Perez Almendros, Espinosa Anke, Luis, and Steven Schockaert. 2020. Don\u2019t patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5891\u20135902, Barcelona, Spain (Online). International Committee on Computational Linguistics. Mohammad Taher Pilehvar, Jose Camacho-Collados, Roberto Navigli, and Nigel Collier. 2017. Towards a seamless integration of word senses into downstream nlp applications. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1857\u20131869. Flor Miriam Plaza-del Arco, Debora Nozza, and Dirk Hovy. 2023. Leveraging label variation in large language models for zero-shot text classification. arXiv preprint arXiv:2307.12973. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. OpenAI. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. In ICLR 2022-Tenth International Conference on Learning Representations.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744.\nBo Peng, Emmanuele Chersoni, Yu-Yin Hsu, and ChuRen Huang. 2021. Is domain adaptation worth your investment? comparing bert and finbert on financial tasks. In Proceedings of the Third Workshop on Economics and Natural Language Processing, pages 37\u201344.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. In ICLR 2022-Tenth International Conference on Learning Representations.\nRajdeep Sarkar, Atul Kr. Ojha, Jay Megaro, John Mariano, Vall Herard, and John P. McCrae. 2021. Fewshot and zero-shot approaches to legal text classification: A case study in the financial sector. In Proceedings of the Natural Legal Language Processing Workshop 2021, pages 102\u2013106, Punta Cana, Dominican Republic. Association for Computational Linguistics. Timo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255\u2013269. Timo Schick and Hinrich Sch\u00fctze. 2021b. It\u2019s not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u20132352. Edgar Sch\u00f6nfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. 2019. Generalized zero-and few-shot learning via aligned variational autoencoders. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8239\u20138247. IEEE. Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980. Congzheng Song, Shanghang Zhang, Najmeh Sadoughi, Pengtao Xie, and Eric Xing. 2021. Generalized zeroshot text classification for icd coding. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 4018\u20134024. Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650. Jiuding Sun, Chantal Shaib, and Byron C Wallace. 2023. Evaluating the zero-shot robustness of instruction-tuned language models. arXiv preprint arXiv:2306.11270. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\nRima T\u00fcrker, Lei Zhang, Maria Koutraki, and Harald Sack. 2019. Knowledge-based short text categorization using entity and category embedding. In The Semantic Web: 16th International Conference, ESWC 2019, Portoro\u017e, Slovenia, June 2\u20136, 2019, Proceedings 16, pages 346\u2013362. Springer. Cynthia Van Hee, Els Lefever, and V\u00e9ronique Hoste. 2018. Semeval-2018 task 3: Irony detection in english tweets. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 39\u2013 50. Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, and Graham Neubig. 2023. Prompt2model: Generating deployable models from natural language instructions. arXiv preprint arXiv:2308.12261. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations. Han Wang, Canwen Xu, and Julian McAuley. 2022a. Automatic multi-label prompting: Simple and interpretable few-shot classification. arXiv preprint arXiv:2204.06305. Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing from a few examples: A survey on few-shot learning. ACM computing surveys (csur), 53(3):1\u201334. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022b. Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. 2019. Huggingface\u2019s transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Yiming Yang. 1999. An evaluation of statistical approaches to text categorization. Information retrieval, 1(1-2):69\u201390.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, pages 3914\u20133923. Association for Computational Linguistics.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, pages 3914\u20133923. Association for Computational Linguistics. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Semeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval). In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 75\u201386. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649\u2013657. Xinwei Zhang and Bin Wu. 2015. Short text classification based on feature extension using the n-gram model. In 2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), pages 710\u2013716. IEEE. Yiwen Zhang, Caixia Yuan, Xiaojie Wang, Ziwei Bai, and Yongbin Liu. 2022. Learn to adapt for generalized zero-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 517\u2013527, Dublin, Ireland. Association for Computational Linguistics.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Semeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval). In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 75\u201386.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649\u2013657.\nYifan Zhou. 2020. A review of text classification based on deep learning. In Association for Computing Machinery, ICGDA \u201920, page 132\u2013136, New York, NY, USA.\n# A Appendix\nIn Section A.1 we present a comparison between prompting and fine-tuning techniques based on Macro-F1. In Section A.2, we present the prompts we used for performing analysis with zero- and oneshot in-context learning with Flan-T5 and LLaMA 1 and LLaMA 2.\n# A.1 Prompting versus Fine-tuning: Macro Results\nFigure 5 shows the Macro-F1 results comparing prompting and fine-tuning techniques. Results show similar trends to those observed based on Micro-F1, presented in Section 4.2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d2c/4d2cb774-30a2-4cf6-a419-faa32f18c92e.png\" style=\"width: 50%;\"></div>\nFigure 5: Comparison between prompting (left) and fine-tuning (right) approaches per text classification type where \u2019AVG\u2019 refers to averaged results across all prompt types per model. In \u2019Prompting\u2019, \u2019zero\u2019 and \u2019one\u2019 refer to zero- and one- shot prompt-based learning techniques, in \u2019Fine Tuning\u2019, \u2019one\u2019 refers to fine-tuning the models with one training instance per label and \u2019all\u2019 refers to fine-tuning using the entire dataset.\n# A.2 Prompts\nIn Table 6 we have listed all \u2018domain\u2019 prompts we used per dataset. The \u2018task\u2019 and the \u2018generic\u2019 prompts are the same for all datasets and are presented in Table 2 in Section 3.4.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6481/64818172-0b58-490f-afc1-613319b11843.png\" style=\"width: 50%;\"></div>\nDataset\nDomain Prompt\nirony\nIs the Tweet classified as irony or non-irony?\noffense\nIs the Tweet classified as offensive or non-offensive?\nhate\nIs the Tweet classified as hate or non-hate?\nemoji\nWhich of the given emojis best describe the given\nTweet?The emojis are:\nsentiment\nIs the Tweet positive, negative, or neutral?\nBBC\nClassify the news into one of the following topics:\nReuters\nClassify the news into one of the following topics:\n20 News\nClassify the newsgroup into one of the following\ntopics:\nOhsumed\nSelect the medical conditions that this article is about.\nThe options are:\nToxic\nWhich of the given toxic topics best describe the\ngiven comment? Choose one or more from the fol-\nlowing topics:\nLegal\nWhich of the given legal topics best describe the\ngiven legislation document?Choose one or more from\nthe following topics:\nCancer\nWhich hallmarks of cancer are present in the text?\nChoose one or more from the following options\nPCL\nWhich of the given topics best describe the patronis-\ning comment. Choose one or more from the follow-\ning topics:\nSafeguard\nWhich of the given themes best describe the sen-\ntence?\nChoose one or more from the following\nthemes:\nIMDB\nIs the movie review positive or negative?\nAG News\nSelect the topic that the given article is about.The\ntopics are:\nTable 6: A list of all domain-based prompts used per dataset.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The paper addresses the challenges of supervised text classification, particularly the limitations of fine-tuning language models like BERT that require large amounts of annotated data. This issue is exacerbated in real-world applications, such as medical and legal domains, where data is scarce and often imbalanced.",
            "purpose of benchmark": "The benchmark aims to evaluate and compare the performance of various language models, particularly focusing on zero-shot and few-shot learning approaches against traditional fine-tuning methods in text classification tasks."
        },
        "problem": {
            "definition": "The benchmark is designed to address the effectiveness of different language models for text classification tasks, particularly in scenarios with limited training data.",
            "key obstacle": "Existing benchmarks often rely on large datasets for model training, which is impractical for low-resource classification tasks, leading to a lack of understanding of how well newer models can perform in these settings."
        },
        "idea": {
            "intuition": "The development of this benchmark was inspired by the need to understand how autoregressive text generation models can perform in text classification tasks without extensive training data.",
            "opinion": "The authors believe that this benchmark is crucial for advancing research in text classification, especially in specialized domains where traditional methods fall short.",
            "innovation": "This benchmark introduces a systematic comparison of various language models across multiple datasets and classification types, a feature that is often overlooked in existing studies.",
            "benchmark abbreviation": "NLP-TextClass"
        },
        "dataset": {
            "source": "The datasets were sourced from various established text classification tasks, including SemEval challenges and real-world applications across multiple domains.",
            "desc": "The benchmark evaluates 16 datasets that encompass binary, multiclass, and multilabel classification tasks across 7 different domains.",
            "content": "The dataset includes text data from social media, news articles, medical documents, and legal texts, making it diverse and relevant for various text classification challenges.",
            "size": "1,000,000",
            "domain": "Text Classification",
            "task format": "Text Classification"
        },
        "metrics": {
            "metric name": "Micro-F1, Macro-F1",
            "aspect": "Accuracy of classification across different models and tasks.",
            "principle": "The metrics were chosen to provide a comprehensive evaluation of model performance in classification tasks, focusing on both overall accuracy and the balance between precision and recall.",
            "procedure": "Models were evaluated based on their classification outputs compared to ground truth labels across various datasets, using standard micro and macro averaging methods."
        },
        "experiments": {
            "model": "The benchmark tested various models including Flan-T5, LLaMA, RoBERTa, and FastText.",
            "procedure": "Models were trained and evaluated using different prompting techniques in zero-shot and few-shot settings, with specific training parameters set for fine-tuning.",
            "result": "The results indicated that Flan-T5 consistently outperformed other models in zero- and few-shot settings, while fine-tuned RoBERTa models excelled in more complex classification tasks.",
            "variability": "Variability was accounted for by conducting multiple trials and averaging results across different random samples of training instances."
        },
        "conclusion": "The paper concludes that while prompt-based learning methods show promise, fine-tuned models still outperform them in text classification tasks, particularly in low-resource settings.",
        "discussion": {
            "advantage": "The benchmark highlights the strengths of smaller, instruction-tuned models in adapting to various text classification tasks with limited data.",
            "limitation": "A significant limitation is the lack of experiments on larger models and the need for further analysis on the impact of prompt design on model performance.",
            "future work": "Future research should explore the effects of different training instance selections on model performance and extend the analysis to other languages and more diverse datasets."
        },
        "other info": [
            {
                "info1": "The benchmark includes datasets from both academic challenges and real-world applications."
            },
            {
                "info2": {
                    "info2.1": "The authors acknowledge the support from UKRI Future Leaders Fellowship.",
                    "info2.2": "Collaboration with the Wales Safeguarding Repository for data collection."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "5.1",
            "key information": "The benchmark evaluates 16 datasets that encompass binary, multiclass, and multilabel classification tasks across 7 different domains, including social media, news articles, medical documents, and legal texts."
        },
        {
            "section number": "5.2",
            "key information": "The paper addresses the challenges of supervised text classification, particularly the limitations of fine-tuning language models like BERT that require large amounts of annotated data, especially in low-resource settings."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark tested various models including Flan-T5, LLaMA, RoBERTa, and FastText, evaluating their performance in zero-shot and few-shot learning scenarios."
        },
        {
            "section number": "4.1",
            "key information": "Models were trained and evaluated using different prompting techniques in zero-shot and few-shot settings, highlighting the importance of prompt design in influencing outcomes."
        },
        {
            "section number": "6.1",
            "key information": "A significant limitation is the lack of experiments on larger models and the need for further analysis on the impact of prompt design on model performance."
        }
    ],
    "similarity_score": 0.7532351561632482,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Language Models for Text Classification_ Is In-Context Learning Enough_.json"
}