{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1509.00137",
    "title": "Online Supervised Subspace Tracking",
    "abstract": "We present a framework for supervised subspace tracking, when there are two time series $x_t$ and $y_t$, one being the high-dimensional predictors and the other being the response variables and the subspace tracking needs to take into consideration of both sequences. It extends the classic online subspace tracking work which can be viewed as tracking of $x_t$ only. Our online sufficient dimensionality reduction (OSDR) is a meta-algorithm that can be applied to various cases including linear regression, logistic regression, multiple linear regression, multinomial logistic regression, support vector machine, the random dot product model and the multi-scale union-of-subspace model. OSDR reduces data-dimensionality on-the-fly with low-computational complexity and it can also handle missing data and dynamic data. OSDR uses an alternating minimization scheme and updates the subspace via gradient descent on the Grassmannian manifold. The subspace update can be performed efficiently utilizing the fact that the Grassmannian gradient with respect to the subspace in many settings is rank-one (or low-rank in certain cases). The optimization problem for OSDR is non-convex and hard to analyze in general; we provide convergence analysis of OSDR in a simple linear regression setting. The good performance of OSDR compared with the conventional unsupervised subspace tracking are demonstrated via numerical examples on simulated and real data.",
    "bib_name": "xie2015onlinesupervisedsubspacetracking",
    "md_text": "# Online Supervised Subspace Tracking\nYao Xie\u2217, Ruiyang Song, Hanjun Dai, Qingbin Li, Le Song\nAbstract\u2014We present a framework for supervised subspace tracking, when there are two time series xt and yt, one being the high-dimensional predictors and the other being the response variables and the subspace tracking needs to take into consideration of both sequences. It extends the classic online subspace tracking work which can be viewed as tracking of xt only. Our online sufficient dimensionality reduction (OSDR) is a meta-algorithm that can be applied to various cases including linear regression, logistic regression, multiple linear regression, multinomial logistic regression, support vector machine, the random dot product model and the multi-scale union-of-subspace model. OSDR reduces data-dimensionality on-the-fly with lowcomputational complexity and it can also handle missing data and dynamic data. OSDR uses an alternating minimization scheme and updates the subspace via gradient descent on the Grassmannian manifold. The subspace update can be performed efficiently utilizing the fact that the Grassmannian gradient with respect to the subspace in many settings is rank-one (or low-rank in certain cases). The optimization problem for OSDR is nonconvex and hard to analyze in general; we provide convergence analysis of OSDR in a simple linear regression setting. The good performance of OSDR compared with the conventional unsupervised subspace tracking are demonstrated via numerical examples on simulated and real data. Index Terms\u2014Subspace tracking, online learning, dimensionality reduction, missing data.\n# I. INTRODUCTION\nSubspace tracking plays an important role in various signal and data processing problems, including blind source separation [1], dictionary learning [2], [3], online principal component analysis (PCA) [4], [5], imputing missing data [4], denoising [6], and dimensionality reduction [7]. To motivate online supervised subspace tracking, we consider online dimensionality reduction. Applications such as the Kinect system generate data that are high-resolution 3D frames of dimension 1280 by 960 at a rate of 12 frames per second. At such a high rate, it is desirable to perform certain dimensionality reduction on-the-fly rather than storing the complete data. In the unsupervised setting, dimensionality reduction is achieved by PCA, which projects the data using dominant eigenspace of the data covariance matrix. However, in many signal and data processing problems, side information is available in the form of labels or tasks. For instance, the data generated by the Kinect system contains the gesture information (e.g. sitting, standing, etc.) [8], [9]. A supervised dimensionality reduction may take advantage of the side information in the choice of\nYao Xie\u2217(Email: yao.xie@isye.gatech.edu) is with the H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c02/9c02d5f9-013b-4e82-90cc-8918d925ac56.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b)</div>\n<div style=\"text-align: center;\">(a)</div>\nFig. 1: Example to illustrate the difference of unsupervised and supervised subspace tracking. (a): a case where dimensionality reduction along largest eigenvector is optimal and both supervised and unsupervised dimensionality reduction pick the dominant eigenvector; (b): a case where dimensionality reduction along the second largest eigenvector is optimal. Unsupervised dimensionality reduction erroneously picks the largest eigenvector since it only maximizes the variance of the predictor variable, but the supervised dimensionality reduction, by considering the response variable, correctly picks the second largest eigenvector.\nthe subspaces for dimensionality reduction. The supervised dimensionality reduction is a bit more involved as it has two objectives: making a choice of the subspace that represents the predictor vector and choosing parameters for the model that relates the predictor and response variables. Existing online subspace tracking research has largely focused on unsupervised learning, including the GROUSE algorithm (based on online gradient descent on the Grassmannian manifolds) [4], [10], [11], the PETRELS algorithm [12] and the MOUSSE algorithm [13]. Local convergence of GROUSE has been shown in [11] in terms of the expected principle angle between the true and the estimated subspaces. A preliminary exploration for supervised subspace tracking is reported in [14], which performs dimensionality reduction on the predictor vector without considering the response variable in the formulation. What can go wrong if we perform subspace tracking using only the predictor {xt} but ignoring the response variable {yt} (e.g., the approach in [14])? Fig. 1 and Fig. 2 demonstrate instances in classification, where unsupervised dimensionality reduction using a subspace may completely fail while the supervised dimension reduction does it right. In this paper, we present a general framework for supervised subspace tracking which we refer to as the online supervised dimensionality reduction (OSDR), which is a meta-algorithm that may be applied to various models. OSDR simultaneously learns the subspace and a predictive model through alternating minimization, and the formulation of OSDR takes into\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dccc/dccc76ae-7691-44c5-97d3-8ef38634fea3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9e1c/9e1c7ea8-f61a-4d5b-ae84-faf744e93b2d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b)</div>\nFig. 2: Simulated data correspond to the case in Fig.1(b): (a): threedimensional data cloud D = 3 with two classes; (b): scatter plot of the data projected by unsupervised subspace tracking into a twodimensional space d = 2; the two classes are completely overlapped in the projected space; (c): scatter plot of the data projected by supervised subspace tracking into a two-dimensional space d = 2; the two classes are well-separately. More details for this example can be found in Section V.\n<div style=\"text-align: center;\">Fig. 2: Simulated data correspond to the case in Fig.1(b): (a): threedimensional data cloud D = 3 with two classes; (b): scatter plot of the data projected by unsupervised subspace tracking into a twodimensional space d = 2; the two classes are completely overlapped in the projected space; (c): scatter plot of the data projected by supervised subspace tracking into a two-dimensional space d = 2; the two classes are well-separately. More details for this example can be found in Section V.</div>\nconsideration both the high-dimensional predictor sequence {xt} and the response sequence {yt}. We explore different forms of OSDR under various settings, with the loss function being induced by linear regression, logistic regression, multiple linear regression, multinomial logistic regression, support vector machine, random dot-product model, and union-ofsubspaces model, respectively. A common structure is that the Grassmannian gradient of the cost function with respect to the subspace U is typically rank-one or low-rank (e.g., rank-k for the k-classification problem, or the rank being dependent on the number of samples used for a mini-batch update). This structure enables us to develop a simple and efficient update for U along the geodesic. Due to the orthogonality requirement and bilinearity, the optimization problem involved in OSDR is non-convex. We provide convergence analysis for OSDR in a simple linear regression setting. Good performance of OSDR is demonstrated on simulated and real data. A notion in statistics related to our problem is sufficient dimensionality reduction, which combines the idea of dimensionality reduction with the concept of sufficiency. Given a response variable y, a D-dimensional predictor vector x, a dimensionality reduction statistic R(x) is sufficient if the distribution of y conditioned on R(x) is the same as that of y conditioned on x. In other words, in the case of sufficiency, no information about the regression is lost by reducing the dimension of x. Classic sufficient dimensionality reduction methods include the sliced inverse regression (SIR) [15],\nwhich uses the inverse regression curve, E[x|y] to perform a weighted principle component analysis; more recent works [16] use likelihood-based sufficient dimensionality reduction in estimating the central subspace. From this perspective, OSDR can be viewed as aiming at sufficiency for online subspace based dimensionality reduction. In the offline setting, a notable work is sufficient dimension reduction on manifold [17], which considers the problem of discovering a manifold that best preserves information relevant to a non-linear regression using a convex optimization formulation. The rest of the paper is organized as follows. Section II sets up the formalization and the meta-algorithm form for OSDR. Section III presents specific OSDR algorithms under various settings. Section IV includes theoretical analysis. Section V contains numerical examples based on simulated and real data. The notation in this paper is standard: R+ denotes the set of positive real numbers; \ufffdn\ufffd= {1, 2, . . . , n}; (x)+ = max{x, 0} for any scalar x; [x]j denotes the jth element of a vector x; I{\u03b5} is the indicator function for an event \u03b5; \u2225x\u22251 and \u2225x\u2225denote the \u21131 norm and \u21132 norm of a vector x, respectively; X\u22badenotes transpose of a vector or matrix X and \u2225X\u2225denotes the Frobenius norm of a matrix X; Id is the identity matrix of dimension d-by-d. Define the sign function sgn(x) is equal to 1 if x > 0 and is equal to 0 if x < 0.\nAssume two time-series (xt, yt), t = 1, 2, . . ., such that yt can be predicted from the high-dimensional vector xt \u2208RD. Here yt \u2208Y can be either binary, real, or vector-valued. To reduce the data dimensionality, we project xt by a subspace Ut \u2208RD\u00d7d to xt, with d \u226aD, to obtain a projected vector (or feature) U \u22ba t xt. Here d is the number of principle components we will use to explain the response series. Ideally, we would like to choose an Ut that maximizes the prediction power of U \u22ba t xt for yt, and Ut can be time-varying to be dataadaptive. With such a goal, we formulate the online supervised dimensionality reduction (OSDR), which simultaneously learns the subspace Ut and a function that relates xt and yt, by minimizing a cost function which measures the quality of the projection in terms of predictive power. The optimization problem of minimizing the loss function is inherently non-convex, due to the orthogonality requirement for Ut: U \u22ba t Ut = Id, as well as bilinear terms arising from coupling of Ut and the model parameters. Hence, we develop the algorithm based on an alternating minimization and stochastic gradient descent scheme. We consider two related formulations: the d-formulation, which assumes the response function only depends on the projected components and hence is a compact model that fits into the goal of dimensionality reduction. However, the d-formulation cannot deal with missing data. Then we also introduce the D-formulation, which handles missing data or can also be used for denoising. The D-formulation estimates the projection \u03b2 of the data using the subspace from the previous\nstep, and then uses \u03b2 to update the subspace; however, it requires us to store high-dimensional model parameters. The loss function for the d- and the D-formulations are different, but the Grassmannian gradient with respect to U is often rank-one (or low-rank). Such simple structure enables us to derive efficient algorithm to update U along the geodesic, as summarized in Algorithm 1. In the following derivations, we omit the time indices t for notational simplicity.\n# A. d-formulation\nThe d-formulation is motived by sliced inverse regression, which assumes that the response variable y depends only on the projected components. The loss function for the d-formulation takes the form of\nwhich measures the predictive quality of the projection for the response y: \u03c1\u03b8(U \u22bax, y) with some parameter \u03b8. To compute the gradient of \u03c1\u03b8 with respect to U on the Grassmannian manifold, we follow the program developed in [18]. First compute a partial derivative of \u03c1\u03b8 with respect to U. Let the partial derivative of the \u03c1\u03b8 function with respect to the first argument be denoted as\ng1 \u225c\u02d9\u03c1\u03b8(U \u22bax, y) \u2208Rd.\nBy the chain rule, we have the partial derivative with respective to U is given by\nUsing equation (2.70) in [18], we can calculate the gradient on the Grassmannian from this partial derivative\nIn many problems that we describe in Section III, the gradient g1 is one term or a sum a few terms. Hence, the gradient has the desired low-rank structure.\n# B. D-formulation\nThe D-formulation assumes that the loss function is defined in the ambient dimension space:\nThis setting is particularly useful for denoising and imputing the missing data, where we will assume the signal x lies near a low-dimensional subspace, and estimate a low-dimensional component U\u03b2 and use it to predict y. The loss function takes the form of \u03f1\u03d1(U\u03b2, y). Following a similar derivation as above, the gradient on the Grassmannian can be written as\n\u2207\u03f1\u03d1 = (I \u2212UU \u22ba)g2\u03b2\u22ba,\nwhere the partial derivative of \u03f1\u03d1 with respect to the first arguement is given by\nAgain, g2 is often only one term or a sum of a few terms, and hence \u2207\u03f1\u03d1 has the desired low-rank structure. To estimate \u03b2, for the denoising setting, we may use\n(1)\nWhen there is missing data, we are not able to observe the complete data vector xt, but are only able to observe a subset of entries \u2126t \u2282\ufffdD\ufffd. Using an approach similar to that in [4], \u03b2 is estimated as\n(2)\nwhere \u2206\u2126t is an n \u00d7 n diagonal matrix which has 1 in the jth diagonal entry if j \u2208\u2126t and has 0 otherwise. It can be shown \u03b2 = (U \u22ba \u2126U\u2126)\u22121U \u22ba \u2126x\u2126, where U\u2126= \u2206\u2126U and x\u2126= \u2206\u2126x.\nAlgorithm 1 OSDR: meta-algorithm\nRequire: a sequence of predictors and responses (xt, yt),\ninitial model parameter \u03b8 and subspace U \u2208RD\u00d7d.\n1: for t = 1, 2, . . . do\n2:\n{d-formulation}\n\u2206\u2190(I \u2212UU \u22ba)x \u02d9\u03c1\u03b8(U \u22bax, y) {\u03c1 : Rd \u00d7 Y \u2192R}\n3:\n{D-formulation}\n\u2206\u2190(I \u2212UU \u22ba) \u02d9\u03f1\u03d1(U\u03b2, y)\u03b2, {\u03f1 : RD \u00d7 Y \u2192R,\n\u03b2 estimated from x}\n4:\nfix \u03b8 or \u03d1, update U along Grassmannian gradient \u2206\nin geodesic\n5:\nfix U, update \u03b8 or \u03d1\n6: end for\n# III. OSDR FOR SPECIFIC MODELS\nIn the section, we illustrate various forms of loss functions and show that the Grassmannian gradient with respect to U typically takes the form of \u03b3rw\u22ba, for some scalar \u03b3 \u2208R, vectors r \u2208RD, and w \u2208Rd.\n# A. Linear regression\nFor linear regression, y \u2208R and the loss function will be the \u21132-norm of prediction error. In the d-formulation, \u03b8 = (a, b) with a \u2208Rd and b \u2208R, and the loss function is\nDefine\n(3)\nThe Grassmannian gradient of the loss function with respect to U is given by\n\u2207\u03c1\u03b8(U \u22bax, y) \u225c\u2212(y \u2212\u02c6y) (I \u2212UU \u22ba)x \ufffd \ufffd\ufffd \ufffd r a\u22ba \ufffd\ufffd\ufffd\ufffd w\u22ba\nUsing the rank-one structure of the gradient above, we perform geodesic gradient descent for U using a simple form. Write\n\ufffd where\nv2, . . . , vd are an orthonormal set orthogonal to r and z2, . . . , zd are an orthonormal set orthogonal to w. Subsequently, using the formula in [18] update of U is given by\n (4)\n\u2225\u2225 \u2225\u2225 \u2225\u2225 where \u03b7 > 0 is a step-size. Similarly, for a fixed U, we may find its gradient with respect to the regression coefficient vector and update via\n(5)\nwhere \u00b5 > 0 is step-size for the parameter update. In the D-formualtion, the model parameters are \u03d1 \u225c(c, e), with c \u2208RD and d \u2208R. Essentially, by replacing x by its estimate U\u03b2, we have the loss function\nIn the D-formualtion, the model parameters are \u03d1 \u225c(c, e), with c \u2208RD and d \u2208R. Essentially, by replacing x by its estimate U\u03b2, we have the loss function\nwhere \u03b2 is estimated using the subspace from the previous step using (1) or (2). Let \u02c6y = c\u22baU\u03b2, we can show\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd and hence the subspace can be updated similarly, and the model parameters are updated via \ufffd where the predicted response \u02c6y \u225ch(a\u22baU \u22ba\n(6) or\nRemark 1 (Difference from unsupervised tracking). Due to an alternative formulation, update in OSDR differs from that in the unsupervised version [4], [11] in that the update in OSDR depends on y \u2212\u02c6y. This is intuition since the amount of update we have on the subspace should be driven by the prediction accuracy for the response variable.\nRemark 2 (Mini-batch update). Instead of updating with every single new sample, we may also perform an update with a batch of samples. The Grassmannian gradient for this mini-batch update scheme can be derived as\n\ufffd In this case the gradient is no longer rank-one. We may use a rank-one approximation of this gradient, or use the exact rank-B update described in (10), which requires computing an SVD of this gradient.\nRemark 3 (Computational complexity). The computational complexity of OSDR is quite low and it is O(Dd). The most expensive step is to compute r = (I \u2212UU \u22ba)x or r = (I \u2212 UU \u22ba)a, in the d- and D-formulations, respectively. This term can be computed as, for instance x\u2212U(U \u22bax), to have a lower complexity O(Dd) (otherwise the complexity is O(D2d)).\n# B. Logistic regression\nFor logistic regression, y \u2208{0, 1}. Define the sigmoid function\nFor logistic regression, y \u2208{0, 1}. Define the sigmoid function 1\nThe loss function for logistic regression corresponds to the negative log-likelihood function assuming y is a Bernoulli random variable. For the d-formulation, the loss function is given by \u03c1\u03b8(U \u22bax, y) = y log h(a\u22baU \u22bax+b)+(1\u2212y) log(1\u2212h(a\u22baU \u22bax+b)) and the model parameter \u03b8 = (a, b) with a \u2208Rd and b \u2208R. For the D-formulation with a parameter estimate \u03b2, we have \u03f1\u03d1(U\u03b2, y) = y log h(c\u22baU\u03b2+e)+(1\u2212y) log(1\u2212h(c\u22baU\u03b2+e)) and the model parameter \u03d1 = (c, e) with c \u2208RD and e \u2208R. It can be shown that, in the logistic regression setting, the Grassmannian gradients with respect to U, for these two cases are given by\nThe loss function for logistic regression corresponds to the negative log-likelihood function assuming y is a Bernoulli random variable. For the d-formulation, the loss function is given by\n\u02c6y \u225ch(a\u22baU \u22bax + b),\n(7)\nwhere the predicted response\n(8)\nNote that the gradients for linear regression and logistic regression take a similar form, with the only difference being how the response is predicted: in linear regression it is defined linearly as in (3), and in logistic regression it is defined through the sigmoid function as in (8). Hence, OSDR for linear regression and logistic regression take similar forms and only differs by what response function is used, as shown in Algorithm 2.\n# C. Multiple linear regression\nWe may also extend OSDR to multiple linear regression, where y \u2208Rm for some integer m. The loss functions for the\nAlgorithm 2 OSDR for linear and logistic regressions\nInput: yt and xt (missing data, given xt observed on an index\nset \u2126t), t = 1, 2, . . .. Initial values for U, a and b (or c\nand d). Step-sizes \u03b7 and \u00b5. \u02dch can be linear or sigmoid\nfunction.\n1: for t = 1, 2, . . . do\n2:\n{d-formulation}\n\u02c6y \u2190\u02dch(a\u22baU \u22bax + b), r \u2190(I \u2212UU \u22ba)x,\n\u03c3 \u2190\u2212(y \u2212\u02c6y)\u2225r\u2225\u2225a\u2225\n3:\n{D-formulation}\n\u03b2 \u2190(U \u22ba\n\u2126U\u2126)\u22121U \u22ba\n\u2126x\u2126\n\u02c6y \u2190\u02dch(c\u22baU\u03b2) + d, r \u2190(I \u2212UU \u22ba)c,\n\u03c3 \u2190\u2212(y \u2212\u02c6y)\u2225r\u2225\u2225\u03b2\u2225\n4:\n{update subspace}\nU \u2190U + cos(\u03c3\u03b7)\u22121\n\u2225w\u22252\nUww\u22ba+ sin(\u03c3\u03b7) r\u22ba\n\u2225r\u2225\nw\u22ba\n\u2225w\u2225\n5:\n{update regression coefficients and residuals}\na \u2190a + \u00b5(y \u2212\u02c6y)U\u03b2, b \u2190b + \u00b5(y \u2212\u02c6y)\n6: end for\nd- and D-formulations are given by\nwith \u03b8 = A \u2208Rd\u00d7m, and \u03d1 = C \u2208RD\u00d7m. Here we assume the slope parameter is zero and this can be achieved by subtracting the means from the predictor vector and the response variable, respectively. It can be shown that\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd It can also be shown that the partial derivative of \u03c1\u03b8 with respect to A for a fixed U is given by \u2212U \u22bax(y \u2212\u02c6y)\u22ba, and the partial derivative of \u03f1\u03d1 with respect to C for a fixed U is given by \u2212U\u03b2(y \u2212\u02c6y)\u22ba. OSDR for multiple linear regression is given in Algorithm 3.\nMultinomial logistic regression means that y \u2208 {0, 1, . . . , K \u22121} for some integer K is a categorical random variable and it is useful for classification. In the following, we focus on the D-formulation; the d-formulation can be derived similarly. The loss function is the negative likelihood function\nAlgorithm 3 OSDR for multiple linear regression\nInput: yt and xt (missing data, given xt observed on an index\nset \u2126t), t = 1, 2, . . .. Initial values for U, A and b (or C\nand d). Step-sizes \u03b7 and \u00b5.\n1: for t = 1, 2, . . . do\n2:\n{d-formulation}\n\u02c6y \u2190A\u22baU \u22bax + b, r \u2190(I \u2212UU \u22ba)x,\nw \u2190(y \u2212\u02c6y)\u22baA\u22ba, \u03c3 \u2190\u2212\u2225r\u2225\u2225w\u2225\n3:\n{D-formulation}\n\u03b2 \u2190(U \u22ba\n\u2126U\u2126)\u22121U \u22ba\n\u2126x\u2126\n\u02c6y \u2190C\u22baU\u03b2 + d, r \u2190(I \u2212UU \u22ba)C(y \u2212\u02c6y),\nw = \u03b2, \u03c3 \u2190\u2212\u2225r\u2225\u2225w\u2225\n4:\n{update subspace}\nU \u2190U + cos(\u03c3\u03b7)\u22121\n\u2225w\u22252\nUww\u22ba+ sin(\u03c3\u03b7) r\u22ba\n\u2225r\u2225\nw\u22ba\n\u2225w\u2225\n5:\n{update regression coefficients and residuals}\nA \u2190A + \u00b5U \u22bax(y \u2212\u02c6y)\u22ba{d-formulation}\nC \u2190C + \u00b5U\u03b2(y \u2212\u02c6y)\u22ba{D-formulation}\n6: end for\ngiven by\n  In this case, the Grassmannian gradient will no longer be rank-one but rank-K, with\nand\n(9)\n  Note that \u03a3 consists of a sum of K terms and, hence, is usually rank-K. We no longer have the simple expression to calculate update of U along the geodesic and the precise update requires performing a (reduced) singular value decomposition of the gradient \u22ba\nwhere \u03a3 \u2208RK\u00d7K is a diagonal matrix with the diagonal entries being the singular values. Using Theorem 2.3 in [18], we update U as\n(10)\nwhere \u03b7 > 0 is the step-size. Alternatively, we may use the rank-one approximation to the Grassmannian gradient to derive, again, a simple update, which is given by Algorithm 4.\nAlgorithm 4 OSDR for K-multinomial logistic regression\nInput: yt and xt (missing data, given xt observed on an index\nset \u2126t), t = 1, 2, . . .. Initial values for U, ak. Step-sizes\n\u03b7 and \u00b5.\n1: for t = 1, 2, . . . do\n2:\n{D-formulation}\n\u03b2 \u2190(U \u22ba\n\u2126U\u2126)\u22121U \u22ba\n\u2126x\u2126\n3:\n{predict response}\n\u02c6yk \u221dea\u22ba\nkU\u03b2+bk, k = 1, . . . , K \u22121\n\u02c6yk \u2190\u02c6yk/(1 + \ufffdK\u22121\nl=1 \u02c6yl)\n\u02c6yK \u21901 \u2212\ufffdK\u22121\nl=1 \u02c6yl,\n4:\ncompute \u03a3 using (9)\n5:\nfind the dominant singular value \u03c3, corresponding left\nsingular vector \u03c1 and right singular vector r for (I \u2212\nUU \u22ba)\u03a3\n6:\n{update subspace}\np \u2190Ur\nU \u2190U + cos(\u03c3\u03b7)\u22121\n\u2225r\u2225\nUrr\u22ba+ sin(\u03c3\u03b7) \u03c1\n\u2225\u03c1\u2225\nr\u22ba\n\u2225r\u2225\n7:\n{Update regression coefficients}\nh = I{y = k} \u2212\n\ufffdK\u22121\nl=1\nI{y=l}ea\u22ba\nl U\u03b2+bl\n1+\ufffdK\u22121\nl=1\nea\u22ba\nkU\u03b2+bk\n\u2212I{y = K}\nea\u22ba\nkU\u03b2+bk\n1+\ufffdK\u22121\nl=1\nea\u22ba\nkU\u03b2+bk\nak \u2190ak + \u00b5hU\u03b2,\nk = 1, . . . , K \u22121\nbk \u2190bk + \u00b5h,\nk = 1, . . . , K \u22121\n8: end for\nE. Support vector machine (SVM) The loss function for SVM is the hinge loss. For the d- and D-formulations, the loss functions are\n\u03c1\u03b8(U \u22bax, y) = max{0, 1 \u2212ya\u22baU \u22bax},\nwhere \u03b8 = a \u2208Rd, and\n\u03f1\u03d1(U\u03b2, y) = max{0, 1 \u2212yc\u22baU\u03b2},\nwhere \u03b8 = c \u2208RD. Note that the loss function is not differentiable. We may use its sub-gradient to perform gradient descent, or find a smooth surrogate function to approximate the hinge loss. The Grassmannian sub-gradients for the two loss functions are\nand\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd These gradients are again rank-one and, hence, we may update U along geodesic efficiently.\n# F. Random dot product graph model\nThe random dot product graph model is useful for relational data which usually occurs in social network study [19]\u2013[21]. The model assumes that each node is associated with a feature \u03b2i, and an edge between two nodes are formed with a probability proportional to the inner product between their features \u03b2\u22ba j \u03b2j. Suppose at each time t, we observe a pair of nodes in the network with predictor vectors x1,t and x2,t as well as their relation indicator variable yt \u2208{0, 1} (i.e., an edge is formed or not). We assume a logistic regression model P(yt = 1) = h(at\u03b2\u22ba 1,t\u03b22,t + bt) for some feature vectors \u03b21,t and \u03b22,t that are projections of x1,t and x2,t. Here our goal is to choose a proper subspace that fits the data nicely. Note that given x1 and x2, the inner product can be estimated as x\u22ba 1UU \u22bax2, which involves a quadratic term in U (rather than linear in U as in other cases). To be able to obtain the rankone structure, we use a two-step strategy: first fix \u03b22 = U \u22bax2 and update U, and then fix \u03b21 = U \u22bax1 and update U. The log-likelihood function for fixed \u03b22 = U \u22bax2 is given by\nSimilar to logistic regression,\n\ufffd \ufffd\ufffd \ufffd which is rank-one and we may update the subspace similarly. In the second step, the log-likelihood function for fixed \u03b21 = U \u22bax1 is given by\nand the subspace U can be updated similarly. Finally, we fix U (and hence fix \u03b21 = U \u22bax1 and \u03b22 = U \u22bax2) and update the logistic regression parameters as\nanew = a + \u00b5(y \u2212h(a\u03b2\u22ba 1 \u03b22 + b))\u03b2\u22ba 1 \u03b22 bnew = b + \u00b5(y \u2212h(a\u03b2\u22ba 1 \u03b22 + b)).\nDescription of the complete algorithm is omitted here as it is similar to the case of logistic regression.\n# G. Union-of-subspaces model\nUnion-of-subspaces model [22] and multi-scale union-ofsubspace model [13], [23], [24] have been used to approximate manifold structure of the state. As illustrated in Fig. 3, the multi-scale union-of-subspace is a tree of subsets defined on low-dimensional affine spaces \ufffd (j,k)\u2208At Sj,k,t, with each of these subsets lying on a low-dimensional hyperplane with dimension d and is parameterized by\nSj,k,t = {\u03b2 \u2208Rd : v = Uj,k,t\u03b2 + cj,k,t, \u03b2\u22ba\u039b\u22121 j,k,tz \u22641, \u03b2 \u2208Rd},\nwhere j \u2208{1, . . . , Jt} denotes the scale or level of the subset in the tree, Jt is the tree depth at time t, and k \u2208{1, . . . , 2j} denotes the index of the subset for that level. The matrix Uj,k,t \u2208RD\u00d7d is the subspace basis, and cj,k,t \u2208RD is the offset of the subset from the origin. The diagonal matrix\nwith \u03bb(1) j,k,t \u2265. . . \u2265\u03bb(d) j,k,t \u22650, contains eigenvalues of the covariance matrix of the projected data onto each hyperplane. This parameter specifies the shape of the ellipsoid by capturing the spread of the data within the hyperplanes. We may couple the subspace tree with regression model, by attaching a set of regression coefficients {aj,k,t, bj,k,t} with each subset. Given x, we may find a subset in the union that has the smallest affinity, and use that subset to estimate \u03b2 by projection onto the associated subspace and use the associated (linear or logistic) regression coefficients to predict y. The affinity can be a distance to the subset similar to what has been used for discriminate analysis or in [13],\n# (j\u2217, k\u2217) = arg min j,k min w (x \u2212Uj,k,tw)\u22a4\u039bj,k,t(x \u2212Uj,k,tw),\n# or simply the distance to a subspace\n(j\u2217, k\u2217) = arg min j,k min w \u2225x \u2212Uj,k,tw\u2225.\n  Then we predict the local coefficient associated with that subset. OSDR can be derived for these models by combining a step of finding the subset with the smallest affinity with subsequent subspace and parameter update similar to the linear or logistic regression OSDR. We may also use this model together with the random dot product graph model for social networks, where two users may belong to two different subsets in the tree and their interaction is determined by the regression model associated with their common parent in the tree. This may capture the notion of community: each node represents one community and there is a logistic regression model for each community. The probability that two users interact is determined by the \u201csmallest\u201d community that they are both in. In this case, OSDR will be two-stage: classification based on affinity function followed by a two-step subspace update similar to the OSDR for the random dot product model. Section V-C presents one such example for illustration.\n# IV. THEORETICAL ANALYSIS\nThe general OSDR problem is hard to analyze due to its non-convexity and bilinearlity in U and the model parameters. In this section, we study the optimization problem for linear regression with the D-formulation to obtain some theoretical insights. In the linear regression case, the loss function is the \u21132 norm \u03f1\u03d1(U\u03b2, y) = (y \u2212c\u22baU\u03b2)2. When there is no missing data, the projection coefficient is given by \u03b2 = U \u22bax. In a simplified setting, assume the response variable is generated\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5548/55488113-a29b-426d-a2ae-cf6b05f1a1f8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f9e2/f9e2f3a1-165c-488d-bc75-c3a921c88ff7.png\" style=\"width: 50%;\"></div>\nFig. 3: Multi-scale union-of-subspaces model [24]. Data are bipartitioned iteratively to form nested subsets, and a low-dimensional affine space (with offset from the origin) is fitted for data in each subset. The bi-partitioning creates a binary tree with multi-scale representation of the data: nodes of the tree contain parameters for that subset, and leaves represent the union-of-subspaces used at time t. The lower panel illustrates the structure of these subspaces. The black dots represent historical data xt used to estimate the subspace. This model can be used in conjunction with, for example, a random dot product graph to model social networks: at each time t, two persons with features \u03b21 and \u03b22 from two subsets interact with probability proportional to the inner product \u03b2\u22ba 1\u03b22.\nwith the parameter c: y = c\u22bax. Then the loss function is given by \u03f1\u03d1(U\u03b2, y) = (c\u22ba(I \u2212UU \u22ba)x)2.\nwith the parameter c: y = c\u22bax. Then the loss function is given by \u22ba \u22ba\nA. Fixed-point with respect to U\n# A. Fixed-point with respect to U\nFirst, we show that for a fixed model parameter, the optimization problem with respect to the subspace U will converge to an orthonormal matrix even without the orthogonality constraint. We make a simplifying assumption that the true response is linear with parameter equal to the assumed parameter c: y = c\u22bax, then we have that one step in the alternating minimization can be written as\n(11)\nThis problem is non-convex due to the constraints as well as the quadratic term UU \u22bain the objective function. Without loss of generality, we may assume \u2225c\u22252 = 1. Construct a matrix C0 with c being its first column. Then we consider the following optimization problem related to (11) that will help us to establish properties later.\n(12)\nTheorem 1. Given a fixed orthogonal matrix C0, the stationary point U \u2217to the optimization problem (12) without the constraint:\n(13)\nare orthogonal matrices of size D \u00d7 d whose columns are d largest eigenvectors of the covariance matrix X of data x. Assume X has d distinct dominant eigenvalues.\n# We need the following lemma for the proof.\nLemma 1 ( [25]). Let C0 \u2208RD\u00d7D be a positive semi-definite matrix and U \u2208RD\u00d7d. For a function J : RD\u00d7d \ufffd\u2192R defined as J(U) = tr(C0) \u22122tr(U \u22baC0U) + tr(U \u22baC0U \u00b7 U \u22baU), the gradient of J is\n\u2207J(U) = \u22122[2C0 \u2212C0UU \u22ba\u2212UU \u22baC0]U.\n  U \u2217is a stationary point of J(U) if and only if U \u2217= UdQ, where Ud \u2208RD\u00d7d contains any d distinct eigenvectors of C and Q \u2208Rd\u00d7d is an arbitrary orthonormal matrix. Moreover, all stationary points of J(U) are saddle points except when Ud contains the d dominant eigenvectors of C0, in which case J(U) attains the global minimum at U \u2217. Proof of Theorem 1. Let X denote the covariance matrix of x: X = E[xx\u22ba], and let \u02dcC = C0C\u22ba 0 . We may write the objective function as L(U, C) = E[tr(C\u22ba  (I \u2212UU \u22ba)xx\u22ba(I \u2212UU \u22ba)C)]\nThen the partial derivative of L(U, C0) with respect to U is then given by\nIf we choose the columns of C0 properly that C0 = (c, c2, . . . , cD) is orthonormal, we have \u02dcC = C0C\u22ba 0 = ID and thus,\nWith the equation above together with Lemma 1, we have that the only stationary points of the optimization problem U \u2217are d distinct dominant eigenvectors of the matrix X (assuming X is full-rank).\n\n# B. Convergence\nIn the same setting, if we fix the model parameter, we may establish the following local convergence property with respect to the Grassmannian gradient of U. Suppose the case where x is exactly on the subspace U \u2217, and x = U \u2217s. We\nuse \u03c6i(Ut, U \u2217), the principal angles between Ut and the true subspace U \u2217, which is defined as\nas a metric. Further define\n\ufffd Note that when there is no missing data, p = U\u03b2, r = (I \u2212 UU \u22ba)a, and \u22ba\nTypically we can assume yt \u2212\u02c6yt \u0338= 0. Hence, we can choose a set of step-sizes \u00b5t > 0 properly such that\n(14)\nDefine \u03b8t such that\n\u2225\u2225 \u2225\u2225 If such \u00b5t > 0 exists, we may choose the constant ct = 1; otherwise we may choose ct accordingly to satisfy (14). When there is no missing data, it is easy to check that Lemma 3.1 in [11] still applies: if we choose the step size \u03b7t such that \u03b7t = \u03b8t/\u03c3t, then we have\n(15)\nwhere At = U \u22ba t U \u2217. Next, we establish conditions for the alternating minimization used in OSDR to converge. The following lemma from [26] comes handy. Lemma 2 (Theorem 4 in [26]). Let (M, d) be a compact metric space. Given two sets P, Q \u2282M, define the Hausdorff distance between them as \ufffd\n\ufffd Let {(Pn, Qn)}n\u22650, P, Q be compact subsets of the compact metric space (M, d) such that\nand let \u2113: M \u00d7 M \u2192R be a continuous function. If there exists a function \u03b4 : M \u00d7 M \u2192R, and the following two conditions hold: (a) for all n \u2265 1, P \u2208 Pn, \u02dcQ \u2208 Qn\u22121, \u02dcP = arg minP \u2208Pn \u2113(P, \u02dcQ)\n(b) for all n \u2265 1, P, \u02dcP \u2208 Pn, Q \u2208 Qn, \u02dcQ = arg minQ\u2208Qn \u2113( \u02dcP, Q), \u2113(P, \u02dcQ) \u2264\u2113(P, Q) + \u03b4(P, \u02dcP),\nthen the adaptive alternating minimization algorithm\nAnother technique we may explore to tackle the non-convex optimization problem in (13) is the efficient discretization. The idea is that instead of using alternating minimization, for the non-convex optimization problem involved, we can find a sufficiently fine yet efficient discretization (as function of the desired error guarantee) that allows us to replace a single non-convex optimization problem by a polynomial number of convex problems. This will not lead to practically efficient algorithms as everything beyond quadratic or cubic running time in the size is usually prohibitive. However, it will allow us to establish general, theoretical guarantees and guide the search for better practical algorithms. In particular, we can adopt approaches similar to the \u03b5-net approach in [27]. This provides a discrete set S of size |S| \u2264(1/\u03b5)d so that for all y \u2208Rn, there exists a point y0 \u2208S such that \u2225Ay\u2212y0\u2225\u221e< \u03b5. This approximation now allows us to handle bilinear terms of the form x\u22baAy, which are non-convex, by replacing them with the two terms x\u22bay0 and \u2225Ay \u2212y0\u2225\u221e< \u03b5 and iterating over all possible choices y0 \u2208S. With the help of the following lemma, we may establish our result.\nwith (P \u2217 0 , Q\u2217 0) as an initialization converges.\nLemma 2 can be applied to our setting in the following sense. Let q = (I \u2212UU \u22ba)x and P = cc\u22ba, we have that our optimization problem\n(16)\ncan be recast into\n(17)\nwhere\nQt = {z \u2208RD : \u2203V \u2208RD\u00d7d, V \u22baV = Id, z = (I\u2212V V \u22ba)xt} and Pt = {Z \u2208RD\u00d7D : \u2203v \u2208RD, yt = v\u22baxt, Z = vv\u22ba}. For simplicity, denote Pn, \u02dcPn and qn, \u02dcqn be arbitrary items from Pn and Qn, respectively, and\nQt = {z \u2208RD : \u2203V \u2208RD\u00d7d, V \u22baV = Id, z = (I\u2212V V \u22ba)xt}, and\nP  { \u2208R \u2203 \u2208R } For simplicity, denote Pn, \u02dcPn and qn, \u02dcqn be arbitrary items from Pn and Qn, respectively, and\nand\nSuppose we can choose the data such that limt\u2192\u221ext = \u00b5 and yt should also converge to some \u03bd, then there exists P, Q such that Pt dH \u2212\u2212\u2192P and Qt dH \u2212\u2212\u2192Q. In order to be able to choose \u03b4(\u00b7, \u00b7) such that\nwe will need to have\nthen we will need\n(18)\nIf the input sequence {xt} is properly selected such that for any Pn \u2208Pn and qn\u22121 \u2208Qn\u22121, the inequality (18) holds, then with Lemma 2, we have the adaptive alternating algorithm for the linear regression problem converges.\nLemma 3 (\u03b5-net for positive semidefinite matrix [27]). Let A = BB\u22ba, where A is an D \u00d7 D positive semidefinite matrix with entries in [\u22121, 1] and B is D \u00d7 d. Let \u2206= \u2206n = {x \u2208RD, \u2225x\u22251 = 1, x \u22650}. There is a finite set S \u2208Rd independent of A, B such that\nwith |S| = O((1/\u03f5)d). Moreover, S can be computed in time O((1/\u03b5)dpoly(D)).\nAssume \u2225x\u22251 = \u03b1 > 0. From lemma 3, we can compute such a set S in time O((\u03b1/\u03b5)dpoly(D)) such that \u2203\u02dcx \u2208S and \u2225U \u22bax\u2212\u02dcx\u2225\u221e\u2264\u03b5/d. Let S = {\u02dcx1, . . . .\u02dcx|S|} where |S| = O((\u03b1/\u03b5)d). We can then approximate a related problem to (12) by a family of |S| problems:\n(19)\nBy combining all sub problems in (19) together, we have the following equivalent problem\n(20)\nwhere\n\ufffd \ufffd\ufffd \ufffd and \u02dcXS = \ufffd \u02dcx1 . . . \u02dcx|S| \ufffd . Note that the set S depends on x, and by construction C0 is orthogonal, we approximate (20) by min U \u2225C\u22ba 0 (XS \u2212U \u02dcXS)\u22252 F = min U \u2225XS \u2212U \u02dcXS\u22252 F , (21) which is convex.\n(21)\nLemma 4. An \u03b5-net approximation to (13) can be computed in polynomial time O((\u03b1/\u03b5)3d \u00b7 poly(Dd).\nProof. The complexity of this epsilon-net method is the time for computing S plus the time for computing the optimization group 19 generated by epsilon-net. From lemma 3, computing S takes time O((\u03b1/\u03b5)dpoly(D)) and the size of the set is |S| = O((\u03b1/\u03b5)d). The dimension of the matrix in this problem is D \u00d7 |S|. With the result in [28], we may use an iterative iterative algorithm to solve the problem and the computational complexity in each iteration is O(D2|S| + |S|3). This leads to a polynomial time algorithm with complexity O(poly(Dd)). Hence, the total time needed for generating and solving the \u03b5-net approximation problems is\nO((\u03b1/\u03b5)dpoly(D) + O((\u03b1/\u03b5)3d) \u00b7 O(poly(Dd)) =O((\u03b1/\u03b5)3d \u00b7 poly(Dd).\n# V. NUMERICAL EXAMPLES\nWe demonstrate the performance of OSDR compared with the online dimensionality reduction (ODR) (learning a subspace online by minimizing \u2225x \u2212U\u03b2\u2225without using information of y) via numerical examples on simulated data and real data. We start with a simple numerical example, followed by a larger scale simulation, an example to illustrate the union-ofsubspaces and random dot product model, and finally real-data experiments.\nA. Simple subspace tracking\nStatic subspace, logistic regression. We first generate data by embedding a static low-dimensional space of intrinsic dimension d\u2217= 2 into a high dimensional space with dimension D, and generating a sequence of \u03b2t vector such that the entries of \u03b2t are i.i.d. N(0, 1) and lies within an ellipse:\n(22)\n  The predictor vector xt \u2208Rp is formed as xt = U\u03b2t + wt, where wt is a Gaussian noise with zero mean and variance equal to 10\u22123. The logistic regression coefficient vector is along the shorter axis of the ellipse. Among the 6000 samples generated this way, the first 3000 samples are for training, and the remaining 3000 samples are for testing. Given xt, ODR or OSDR predict the label \u02c6yt, then we reveal the true label to calculate error, and then use (xt, yt) to update. We use misclassification error Pe on the test data as our performance metric. In Fig. 4, OSDR outperforms ODR by an almost two order of magnitude smaller error.\nRotating subspaces, logistic regression. Next we consider tracking a time-varying subspace to demonstrate the capability\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfef/dfef5b28-d799-48af-861a-af1078be2afa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\nFig. 4: Tracking a static subspace with D = 100 and true intrinsic dimension d\u2217= 2. We plot the dimension of the subspace d versus misclassification errors for different aspect ratio of the ellipse in (22) (a) r1/r2 = 3; (b) r1/r2 = 5;\nof OSDR to handle dynamic data. Assume Ut = U0R(t) with the rotation matrix given by\n(23)\nwhere \u03b1t is the rotation angle, and U0 \u2208Rp\u00d72 is a random initial subspace. The vector \u03b2t is again generated with entries i.i.d. and lies within an ellipse described by (22). The predictors xt is generated as the last example. The rotation angle \u03b1t follows \ufffd\n(24)\n  where \u03c4 is the rotation speed (smaller \u03c4 corresponds to faster rotation). The logistic regression coefficient vector is along the shorter axis of the ellipse. Fig. 5 shows Pe for various ration speed, where, again OSDR significantly outperforms ODR.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2761/2761d74b-d2c8-45e9-ba72-32968751f087.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14cc/14cc6b9a-248b-4cbd-ab6e-8b8828e9d32f.png\" style=\"width: 50%;\"></div>\nFig. 5: Rotating subspace with aspect ratio of the ellipse r1/r2 = 10. Rotation ratio \u03c4 versus misclassification rate when (a) d = 30; (b) d = 10; (c) d = 5; (d) d = 2.\nStatic subspace, linear regression. The third simple example compares the performance of OSDR with ODR in the linear regression setting. The setup is similar to that of tracking a static subspace, except that the response variable yt is generated through a linear regression model yt = c\u22baU\u03b2 + b + \u03f5t, with D = 2, d = 1, c = [c1, c2]\u22a4and \u03f5t \u223cN(0, \u03b42) with \u03b42 = 10\u22123. We use the rooted mean squared error (RMSE) on the test data as our performance metric, which is the square root of the averaged square error on the predicted \u02c6y differs from true y. Fig. 6 shows the RMSE associated with OSDR is significantly lower than that of ODR.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5784/5784d60f-55c4-4fb0-a31a-59b0a84cd439.png\" style=\"width: 50%;\"></div>\nFig. 6: Static subspace, OSDR linear regression: RMSE versus log(c1/c2) versus log(RMSE) when (a) r1/r2 = 1; (b) r1/r2 = 2.\n<div style=\"text-align: center;\">Fig. 6: Static subspace, OSDR linear regression: RMSE versus log(c1/c2) versus log(RMSE) when (a) r1/r2 = 1; (b) r1/r2 = 2.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/52e9/52e9bfb4-3a06-4d34-a62e-d90829034707.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Type II Results</div>\n<div style=\"text-align: center;\">(a) Type I Results</div>\nFig. 7: Synthetic classification dataset. (a) test errors Pe under type I; (b) test errors Pe under type II. The ten (D, d) pairs are (2, 1); (3, 2); (10, 2); (10, 4); (10, 6); (10, 8); (50, 10); (50, 20); (50, 30); (50, 40), and have x-labels from 1 to 10 in the figure, correspondingly.\n<div style=\"text-align: center;\">Fig. 7: Synthetic classification dataset. (a) test errors Pe under type I; (b) test errors Pe under type II. The ten (D, d) pairs are (2, 1); (3, 2); (10, 2); (10, 4); (10, 6); (10, 8); (50, 10); (50, 20); (50, 30); (50, 40), and have x-labels from 1 to 10 in the figure, correspondingly.</div>\n# B. Synthetic classification dataset\nWe consider a larger scale example by generating a synthetic dataset for two-class classification. The dataset has N = 10000 samples {xn} \u2208RD, n = 1, . . . , N. This generating process guarantees that the eigenvalues of the covariance matrix for the dataset are picked from D, D \u22121, . . . , 1 (This is done by first generate a random subspace, then project the random generated data into this subspace, and also scale the dimensions to have the required eigenvalues). An example when D = 3 are shown in Fig. 2. The labels of the data are obtained as follows: Suppose the covariance matrix of data is X. We first find the eigenvalues [\u03bb1, \u03bb2, . . . , \u03bbD] and the corresponding eigenvectors [v1, v2, . . . , vD] where \u03bb1 \u2265\u03bb2 \u2265. . . \u2265\u03bbD; then select an eigenvector vp, p \u2208[1, . . . , D], and project the data onto this vector. After sorting the projected values, we label the first half\nas positive, and last half as negative. Consider two settings for selecting p: type-I: p = d + 1; type-II: p = d + (D \u2212d)/2. Clearly, type-II will be harder, since the corresponding variance of projected data will be smaller. We use half of the data for training, and another half for testing. The tuned learning rate \u00b5 \u2208[10\u22122, 10\u22123, 10\u22124] and \u03b7 \u2208[10\u22122, 10\u22123, 10\u22124] for both ODR and OSDR. The mean accuracy Pe are reported after 10 rounds of experiments for each setting. Besides different types of labelling directions, we also evaluated different combinations of (D, d) pairs, as shown in the Fig. 7. The results show that OSDR outperforms the SDR (baseline) significantly. This is expected, as the first d leading directions of principle components contain nothing about the label information; therefore, the unsupervised ODR will fail almost surely. The simple example shown in Fig. 2 proves that OSDR can identify the correct direction for projecting data.\n# C. Union-of-subspaces combined with random dot product model\nWe generate an example for interaction of two nodes with features \u03b21 and \u03b22 through a random dot product graph model defined over al union-of-subspaces, as illustrated in Fig. 3. There are three sets which correspond to the three leaf nodes in the tree. Each node in the tree is associated with a subset lies on a subspace and also a logistic regression coefficient vector. At each time, two predictor vectors x1,t and x2,t of 100 dimensions are observed (which may belong to the same subset or different subsets) and their interaction yt is generated through logistic regression model that depending on their inner product. In this example, we also assume there are missing data: only 40% of the samples are observed and the variance of the noise is 0.01. The subsets in the tree are also time varying. The subspace associated with the root node is a random orthogonal matrix that rotates: U1,t = exp(Rt) with R being a random per-symmetric matrix that determines the rotation. The children nodes of the root node are slight rotation of the subspace of their parent node: U2,1,t = exp(R)U1,1,t, U2,2,t = exp(\u2212R)U1,1,t, U3,1,t = exp(R/2)U2,1,t, U3,2,t = exp(\u2212R/2)U2,1,t. Results in Table I shows that OSDR outperforms the conventional online logistic regression (which does not perform dimension reduction and ignores the tree structure).\n<div style=\"text-align: center;\">TABLE I: Comparison of Pe for data generated from a union-ofsubspaces combined with random dot product model.</div>\nOnline\nHierarchical\nlogistic regression\nOSDR\n0.2133\n0.1440\nD. Real-data experiments USPS dataset. We test OSDR logistic regression on the wellknown USPS dataset of handwritten digits. The dataset contains a training set with 7291 samples, and a test set with 2007\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7280/72807053-9171-4ecc-9ca6-28e6ccae68ad.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8: USPS digits recognition. Dimension of the subspace d versus misclassification rate Pe for OSDR and ODR, respectively.</div>\nsamples. The digits are downscaled to 16 \u00d7 16 pixels. To demonstrate a online setting, we read one picture at a time with the task of classifying whether a digit is \u201c2\u201d or not. Again, we first use vectored image as xt, predict label \u02c6yt, and then reveal the true label followed by using (xt, yt) to update the logistic regression model. Fig. 8 demonstrates OSDR compared favorably with ODR for this task. The improvement is not significant; which may be due to the fact that in this case the dominant eigenvectors captures most useful features for classification already. \u201cBoy or girl\u201d classification. We next perform a \u201cboy or girl\u201d classification task on an image set we collected from college students enrolled in a class. This dataset contains 179 grey scaled images, where 116 of them are from boys. Each picture has the resolution of 65x65, thus the original space has a dimension D = 4225. We train both algorithms from raw images. For the parameter search, we tune the learning rate \u00b5 \u2208[10\u22122, 10\u22123, 10\u22124] and \u03b7 \u2208[10\u22122, 10\u22123, 10\u22124] for both algorithms. Experiments are carried out with different settings of d \u2208[10, 50, 100, 150]. The mean test error of 5-fold cross validation on this dataset are reported in Table II, for each configuration.\nTABLE II: Average test error after 5-fold cross validation on Boyvs-Girl dataset.\nMethod\nd = 10\nd = 50\nd = 100\nd = 150\nODR\n0.4800\n0.4629\n0.4517\n0.4229\nOSDR\n0.3314\n0.2343\n0.2171\n0.1714\nFor this dataset, the OSDR algorithm significantly outperforms ODR. To gain a better understanding for its good performance, we examine the subspace generated from the experiment. We first visualize the top 7 vectors in the basis of subspace U for OSDR and ODR, respectively. We reshape each vector into a image and, hence, this displays the socalled \u201ceigenface\u201d. Note that the eigenfaces generated by the unsupervised (corresponding to online PCA) and the supervised subspace learning are very different. The online PCA keeps some facial details, while the OSDR algorithm is getting some vectors that are hard to explain visually, but may actually captured details that are more important for telling apart boys and girls.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b8c2/b8c2efd0-5772-44cc-801b-f66ec3d13964.png\" style=\"width: 50%;\"></div>\nFig. 9: Visualization of top 7 basis in subspace U: \u201ceigenfaces\u201d. Images are processed via blurring and contrast enhancement for better visualization. The first row corresponds to the basis obtained from ODR (baseline) algorithm, while the second row consists of basis from the OSDR algorithm.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a3e1/a3e13f19-57cc-4039-9558-de2b2c1d5831.png\" style=\"width: 50%;\"></div>\nFig. 10: Mean reconstructed image for boys and girls. (a) and (b) are showing results obtained from ODR algorithm, while (c) and (d) are showing results created by OSDR. Images are processed via blurring and contrast enhancement for better visualization.\nWe further examine the average image of reconstructed faces x = U\u03b2 in Fig. 10. We compute the average reconstructed image of boys and girls separately, so as to evaluate the discriminative ability of both algorithms. We can see the unsupervised subspace tracking (online PCA) obtained many details of the facial attributes, and the reconstructions of boys and girls have little differences, which makes it hard to distinguish the two genders. So in this case, the unsupervised algorithm fails because of the lack of supervised information. In contrast, the supervised OSDR extracts two very different \u201caverage\u201d faces for the boy and the girl, respectively, although these average faces do not directly reflect any facial detail. Interestingly, from the contour we learned that, the most discriminative attribute learned by OSDR is the hair (see the dark part in Fig. 10 (d) around shoulder of the girls). This is a straightforward and efficient feature for distinguishing boys and girls. This example clearly demonstrate that OSDR focus on extracting the component that differentiate two classes, and hence, is quite suitable for dimensionality reduction in classification problems.\n[1] M. Zibulevsky and B. A. Pearlmutter, \u201cBlind source separation by sparse decomposition in a signal dictionary,\u201d Neural computation, vol. 13, no. 4, pp. 863\u2013882, 2001. [2] J. Marial, F. Bach, J. Ponce, and G. Sapiro, \u201cOnline dictionary learning for sparse coding,\u201d in Proc. 26th Int. Conf. Machine Learning (ICML), 2009. [3] D. A. Spielman, H. Wang, and J. Wright, \u201cExact recovery of sparselyused dictionaries,\u201d in Proc. 25th Annal Conf. Learning Theory, 2012.\n[4] L. Balzano, R. Nowak, and B. Recht, \u201cOnline identification and tracking of subspaces from highly incomplete information,\u201d in Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference On, 2010. [5] J. Feng, H. Xu, and S. Yan, \u201cOnline robust PCA via stochastic optimization,\u201d in Neural Information Processing Systems Foundation (NIPS), 2013. [6] B. Wang and Z. Tu, \u201cSparse subspace denoising for image manifolds,\u201d in 2013 IEEE Conf. Computer and Vision Pattern Recognition (CVPR), 2013. [7] D. Arpit, I. Nwogu, and V. Govindaraju, \u201cDimensionality reduction with subspace structure preservation,\u201d in Neural Information Processing Systems Foundation (NIPS), 2014. [8] K. K. Biswas and S. K. Basu, \u201cGesture recognition using microsoft kinect,\u201d in Automation, Robotics and Applications (ICARA), 2011 5th Int. Conf., 2011. [9] Y. Li, \u201cHand gesture recognition using kinect,\u201d in Software Engineering and Service Science (ICSESS), 2012 IEEE 3rd Int. Conf. on, 2012. [10] L. Balzano and S. J. Wright, \u201cOn grouse and incremental svd,\u201d in IEEE 5th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2013. [11] L. Balzano and S. J. Wright, \u201cLocal convergence of an algorithm for subspace identification from partial data.,\u201d Foundations of Computational Mathematics, pp. 1\u201336, Oct. 2014. [12] Y. Chi, Y. C. Eldar, and R. Calderbank., \u201cPetrels: Parallel subspace estimation and tracking using recursive least squares from partial observations,\u201d IEEE Trans. on Signal Processing, vol. 61, pp. 5947 \u2013 5959, 2013. [13] Y. Xie, J. Huang, and R. Willett, \u201cChange-point detection for highdimensional time series with missing data,\u201d IEEE Journal of Selected Topics in Signal Processing (J-STSP), vol. 7, pp. 12\u201327, Feb. 2013. [14] Y. Xie and R. Willett, \u201cOnline logistic regression on manifolds,\u201d in IEEE Int. Conf. Acoustics, Speeches and Sig. Proc. (ICASSP), 2012. [15] K.-C. Li, \u201cSliced inverse regression for dimension reduction,\u201d J. American Stat. Association, vol. 86, pp. 316\u2013327, 1991. [16] R. D. Cook and L. Forzani, \u201cLikelihood-based sufficient dimension reduction,\u201d J. American Stat. Association, pp. 197\u2013208, 2009. [17] F. S. J. Nilsson and M. I. Jordan, \u201cRegression on manifolds using kernel dimension reduction,\u201d Proc. 24th Int. Conf. Machine Learning (ICML)nd Int. Conf. Machine Learning (ICML), 2007. [18] A. Edelman, T. A. Arias, and S. T. Smith, \u201cThe geometry of algorithms with orthogonality constraints,\u201d SIAM journal on Matrix Analysis and Applications, vol. 20, no. 2, pp. 303\u2013353, 1998. [19] C. L. M. Nickel, Random dot product graph: a model for social networks. PhD thesis, University of Maryland, 2006. [20] S. J. Young and E. R. Scheinerman, \u201cRandom dot product graph models for social networks,\u201d Algorithms and Models for the Web-Graph: Lecture Notes in Computer Science Volume, vol. 4863, pp. 138\u2013149, 2007. [21] E. R. Scheinerman and K. Tucker, \u201cModeling graphs using dot product representations,\u201d Computational Statistics, vol. 25, pp. 1\u201316, 2010. [22] B. Eriksson, L. Balzano, and R. Nowak, \u201cHigh rank matrix completion,\u201d in Proc. of Intl. Conf. on Artificial Intell. and Stat, 2012. [23] Y. Xie, J. Huang, and R. Willett, \u201cMultiscale online tracking of manifolds,\u201d in IEEE Statistical Signal Processing Workshop (SSP), 2012. [24] W. Allard, G. Chen, and M. Maggioni, \u201cMulti-scale geometric methods for data sets II: Geometric multi-resolution analysis,\u201d App. and Comp. Harmonic Ana., vol. 32, pp. 435 \u2013 462, May 2011. [25] B. Yang, \u201cProjection approximation subspace tracking,\u201d Signal Processing, IEEE Transactions on, vol. 43, no. 1, pp. 95\u2013107, 1995. [26] U. Niesen, D. Shah, and G. W. Wornell, \u201cAdaptive alternating minimization algorithms,\u201d Information Theory, IEEE Transactions on, vol. 55, no. 3, pp. 1423\u20131429, 2009. [27] N. Alon, T. Lee, A. Shraibman, and S. Vempala, \u201cThe approximate rank of a matrix and its algorithmic applications,\u201d Proceedings of the forty-fifth annual ACM symposium on Theory of computing, 2013. [28] Y. Plan, Compressed sensing, sparse approximation, and low-rank matrix estimation. PhD thesis, California Institute of Technology, 2011.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of supervised subspace tracking in high-dimensional data, emphasizing the need for methods that utilize both predictor and response variables, as traditional methods often neglect the response variable.",
        "problem": {
            "definition": "The problem is defined as the need to track a subspace that effectively represents high-dimensional predictors while also considering their relationship with response variables.",
            "key obstacle": "The main difficulty lies in the traditional unsupervised approaches that fail to leverage available side information, leading to suboptimal subspace representations."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that incorporating response variables can significantly improve the accuracy of dimensionality reduction methods.",
            "opinion": "The proposed method, called Online Sufficient Dimensionality Reduction (OSDR), aims to simultaneously learn the subspace and a predictive model using an alternating minimization approach.",
            "innovation": "OSDR innovates by integrating supervised learning into subspace tracking, allowing for a more effective dimensionality reduction compared to existing unsupervised methods."
        },
        "method": {
            "method name": "Online Sufficient Dimensionality Reduction",
            "method abbreviation": "OSDR",
            "method definition": "OSDR is a meta-algorithm that reduces data dimensionality on-the-fly while considering both predictors and response variables.",
            "method description": "OSDR employs an alternating minimization scheme to update the subspace using gradient descent on the Grassmannian manifold.",
            "method steps": [
                "Initialize model parameters and subspace.",
                "For each time step, compute gradients based on the chosen formulation (d- or D-formulation).",
                "Update the subspace along the computed gradient direction.",
                "Update model parameters based on the prediction error."
            ],
            "principle": "The effectiveness of OSDR stems from its ability to adaptively learn the subspace that maximizes predictive power, leveraging the low-rank structure of the Grassmannian gradient."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on simulated and real datasets, comparing OSDR against conventional unsupervised methods like ODR across various regression settings.",
            "evaluation method": "Performance was assessed using metrics such as misclassification error and RMSE, highlighting the advantages of OSDR in handling dynamic data and missing values."
        },
        "conclusion": "The results demonstrate that OSDR significantly outperforms traditional unsupervised methods, providing a robust framework for supervised subspace tracking in high-dimensional settings.",
        "discussion": {
            "advantage": "Key advantages of OSDR include its ability to effectively utilize side information from response variables, leading to improved predictive performance and adaptability in dynamic environments.",
            "limitation": "The method may face challenges in extremely high-dimensional spaces where computational complexity could increase, and the non-convex nature of the optimization problem may complicate convergence guarantees.",
            "future work": "Future research could explore enhancements to the algorithm's efficiency, robustness against noise, and extensions to more complex models beyond linear and logistic regression."
        },
        "other info": {
            "info1": "The OSDR algorithm is applicable to various models including support vector machines and multinomial logistic regression.",
            "info2": {
                "info2.1": "The Grassmannian gradient often exhibits low-rank properties, facilitating efficient updates.",
                "info2.2": "OSDR can handle missing data effectively through its D-formulation."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of supervised subspace tracking in high-dimensional data, emphasizing the need for methods that utilize both predictor and response variables."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of methods like Online Sufficient Dimensionality Reduction (OSDR) is to improve the accuracy of dimensionality reduction methods by incorporating response variables."
        },
        {
            "section number": "1.3",
            "key information": "The main goal of the paper is to present OSDR, which aims to simultaneously learn the subspace and a predictive model using an alternating minimization approach."
        },
        {
            "section number": "2.1",
            "key information": "Key concepts include supervised learning, dimensionality reduction, and the relationship between predictors and response variables."
        },
        {
            "section number": "2.2",
            "key information": "The paper highlights the fundamental differences between supervised subspace tracking and traditional unsupervised methods, noting that unsupervised approaches often neglect response variables."
        },
        {
            "section number": "3.1",
            "key information": "OSDR is categorized as a supervised learning algorithm that integrates response variables into the dimensionality reduction process."
        },
        {
            "section number": "3.5",
            "key information": "OSDR represents a novel approach in the realm of supervised subspace tracking, offering improvements over existing unsupervised methods."
        },
        {
            "section number": "4.1",
            "key information": "The paper discusses the critical role of utilizing response variables in enhancing the predictive performance of dimensionality reduction methods."
        },
        {
            "section number": "7.1",
            "key information": "The paper identifies challenges related to computational complexity in extremely high-dimensional spaces as a limitation of the proposed method."
        },
        {
            "section number": "7.3",
            "key information": "OSDR can handle missing data effectively through its D-formulation, addressing challenges in data quality."
        },
        {
            "section number": "8",
            "key information": "The conclusion emphasizes that OSDR significantly outperforms traditional unsupervised methods, providing a robust framework for supervised subspace tracking."
        }
    ],
    "similarity_score": 0.6185424042768622,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Online Supervised Subspace Tracking.json"
}