{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.02671",
    "title": "Fair In-Context Learning via Latent Concept Variables",
    "abstract": "The emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different types of data facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate this inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce correlation between predictive outcomes and sensitive variables helping to promote fairness during latent concept learning. We utilize the learned concept and select demonstrations from a training dataset to obtain fair predictions during inference while maintaining model utility. The latent concept variable is learned using a smaller internal LLM and the selected demonstrations can be used for inference with larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods.",
    "bib_name": "bhaila2024fairincontextlearninglatent",
    "md_text": "# ir In-Context Learning via Latent Concept Va \n1 University of Arkansas, 2 Baylor University, 3 University of Texas at Dallas 1 {kbhaila, haovan, kedemacu, xintaowu}@uark.edu, 2 chen_zhao@baylor.edu, 3 feng.chen@utdallas.edu \n# Abstract \nThe emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different types of data facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate this inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce correlation between predictive outcomes and sensitive variables helping to promote fairness during latent concept learning. We utilize the learned concept and select demonstrations from a training dataset to obtain fair predictions during inference while maintaining model utility. The latent concept variable is learned using a smaller internal LLM and the selected demonstrations can be used for inference with larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods. Code and data are available at https: //github.com/karuna-bhaila/fairicl. \n\n# 1 Introduction \nLarge language models (LLMs) have demonstrated immense capabilities in performing a wide range of natural language processing (NLP) tasks. A factor contributing to widespread LLM usage is their incontext learning (ICL) (Brown et al., 2020) ability which allows adaptation to downstream tasks without costly training or fine-tuning. Provided with a small number of demonstration examples, ICL equips LLMs with the ability to infer task-specific context and perform inference with impressive utility. Recent research has also explored the applicability of LLMs on tabular data through serialization \nmethods that facilitate ICL by transforming the data into natural language formats (Hegselmann et al., 2023). With the increasing integration of LLM inference in domains such as healthcare (Wu et al., 2023), finance (Li et al., 2023a), and the legal sector (Sun, 2023) with various data formats, it has become crucial to scrutinize their use from a trustworthiness perspective. LLMs have been shown to exhibit discriminatory behavior in their outputs due to stereotypes and prejudices inherent in pre-training data (Abid et al., 2021; Basta et al., 2019). When used for decisive tasks, LLMs may mirror social inequalities and biases from the real world leading to harmful consequences. Furthermore, in the ICL setting with tabular data classification, recent research has empirically verified the presence of bias in LLM outputs. Liu et al. (2023) investigated unfairness in ICL with tabular data by flipping the labels of incontext demonstration examples and observed bias reduction but with significant trade-offs in model utility. Li et al. (2024) similarly implemented multiple heuristic methods for demonstration selection based on sensitive attribute and label distribution in the demonstrations. Hu et al. (2024) discovered that increasing the representation of minority groups and underrepresented labels in demonstrations helps to improve fairness at some cost to utility. They further developed a strategy that uses clustering to extract representative samples and selects demonstrations from the extracted samples based on their performance on a validation set. In this work, we similarly explore optimal demonstration selection for ICL to promote fairness in LLM predictions but we utilize the latent concept variable mechanism (Wang et al., 2024) to achieve fair in-context learning. Wang et al. (2024) formulate ICL via a Bayesian perspective and theorize that inference with a finite number of demonstrations selected using the latent concept variable approximates the optimal Bayes predictor. \nThe latent concept is learned from an observed set of task-specific training data with a small LLM and used to obtain demonstrations that can be generalized to larger LLMs for improving downstream task performance. Motivated by the influence of latent concepts on model performance, we formulate a fair demonstration selection approach for in-context learning, dubbed as FairICL. As the latent concepts are learned from an observed set of task-specific examples, we conjecture that the training data distribution may affect the quality of the learned latent concepts and ultimately the model predictions from both accuracy and fairness perspectives. Therefore, in FairICL, we incorporate an effective data augmentation technique that promotes decorrelation between the sensitive attributes and the outcome variables by randomizing the relationship between them. This augmentation allows us to obtain a fairer representation of the task-specific data used to learn the fair latent concept variable while preserving relevant information among nonsensitive attributes and the label. We then utilize the learned concepts to select demonstrations from the observed training examples such that the probability of observing the learned latent variable is maximized when conditioned on the corresponding example. The selected demonstrations are used to perform in-context learning with external LLMs larger than the one used for learning. We empirically validate our FairICL on real-world tabular datasets known to represent social biases and demonstrate that FairICL can effectively achieve fairness goals while maintaining predictive utility. We compare the performance of FairICL with multiple heuristic approaches and conduct a comprehensive analysis of the influence of different hyperparameters. Our empirical results show that FairICL can generalize demonstration selection to external LLMs and outperform baseline methods. \n# 2 Related Work \n# Fairness in LLMs \nAs LLM integration into decision-making systems continues to rise, it becomes essential for them to be evaluated from a fairness perspective. Multiple works have highlighted discriminatory behavior in LLM outputs originating from societal biases contained in pre-training data (Abid et al., 2021; Wang et al., 2023) or under-representation of minority population (Gallegos et al., 2023). For \ninstance, Huang et al. (2021) analyzed implicit gender-based stereotypes in LLM outputs via commonsense inference. Wang et al. (2023) evaluated the influence of normal and adversarial prompts on bias in GPT models. Abid et al. (2021) demonstrated unfairness in LLM outputs w.r.t religious groups. Following these works, the study of LLM fairness has also extended to tabular data inference with pre-trained language models (Li et al., 2023b; Liu et al., 2023; Chhikara et al., 2024; Hu et al., 2024; Atwood et al., 2024a). These works focus on LLM inference with in-context learning and formulate ways to select demonstration examples while prompting fairness or ensuring representation for minority groups. Li et al. (2023b) evaluated multiple heuristic methods of selecting demonstrations and a guardrail technique instructing LLM to be fair in its decision-making. (Liu et al., 2023) implemented label-flipping for demonstration examples and achieved fair predictions with significant utility loss. Chhikara et al. (2024) evaluated LLM\u2019s familiarity with commonly known fairness notions and utilized a similarity-based demonstration selection approach. Hu et al. (2024) aimed to increase minority group representation in demonstrations and selected demonstrations based on corresponding validation set performance. Atwood et al. (2024a) explored remediation techniques for fairness and compared prompt-based techniques with inprocessing and postprocessing methods. Similar to some earlier works, we aim to address bias in LLM predictions in tabular data by selecting demonstration examples that promote fairness. However, we utilize the latent concept variable model and present a framework to learn fair representations of the latent concept. The demonstrations selected by the latent concept are used for ICL to obtain fair outcomes while maintaining predictive utility. \n# 3 Preliminaries \n# 3.1 In-Context Learning \nThe in-context learning (Brown et al., 2020) ability of LLMs has prompted multiple research works that investigate how LLMs can learn from demonstration examples for certain tasks without being explicitly trained for those tasks. Let us denote a pre-trained LLM as M with parameters W. Let D = {(x i, y i)} n i =1 denote a tabular dataset observed for an arbitrary task where x i \u2208X  represents attributes of the i-th instance and y i \u2208Y its corresponding outcome. Assume a i \u2208A  de \nnotes its sensitive attribute. For in-context learning, the LLM is provided with k examples from D as demonstrations or context to guide the model in structuring its response for a test example x. Conditioned on a task description inst, a set of sampled demonstrations {(x 1, y 1), \u00b7 \u00b7 \u00b7, (x k, y k)}, and a test query x, the prediction output \u02c6 y from M can be formally formulated as \n(1) \n\ufffd \ufffd\ufffd \ufffd where g (x k, y k) denotes a prompt function (e.g., a template) that transforms the k-th demonstration example into natural language text. To simplify, we omit the task description and prompt function thereafter and represent the output probability as \n(2) \nICL performance has been found to be significantly influenced by demonstration examples and their ordering (Liu et al., 2021; Rubin et al., 2021; Su et al., 2022; Lu et al., 2021; Ma et al., 2024). Consequently, recent works explore effective demonstration selection based on similarity to query input (Liu et al., 2021; Rubin et al., 2021; Su et al., 2022), entropy of predicted labels (Lu et al., 2021), and low predictive bias (Ma et al., 2024). \n# 3.2 Latent Concept Learning \nAn essential research question in in-context learning is effective demonstration selection to enable optimal downstream performance. Towards this objective, Xie et al. (2021) put forth an interpretation of ICL based on latent concept variables concluding that pre-trained models learn latent concepts during next-token prediction training and infer a shared latent concept among demonstrations used for inference. They show that under the assumptions of the hidden Markovian data generation process, discrete latent concept tokens, and an approximately infinite number of demonstrations, in-context learning is an optimal predictor. Similarly, Wang et al. (2024) studied latent concepts in LLMs but under a more general assumption of continuous latent concepts and that the data generation process is governed by an underlying causal mechanism given as X \u2192 Y \u2190 \u03b8 or Y \u2192 X \u2190 \u03b8 where X represents the input, Y the output, and \u03b8 denotes the latent concept variable. Under these assumptions, in-context learning can become an \noptimal predictor with a finite number of demonstrations chosen using the latent concept variable \u03b8. To find the optimal value of \u03b8 when considering the X \u2192 Y \u2190 \u03b8 direction, Wang et al. (2024) formulated a latent concept variable framework for learning task-specific concept tokens that capture sufficient information for next-token prediction by minimizing a loss jointly conditioned on X and the learned \u02c6 \u03b8 as \nwhere \u02c6 \u03b8  represents the learned latent concept variable, x denotes an input token sequence and y the discrete target variable. In practice, \u02c6 \u03b8 is optimized by adding new tokens to M \u2019s vocabulary with corresponding embedding vectors which we refer to as W \u03b8. During training, W \u03b8 is updated using the loss defined above. The learned \u02c6 \u03b8 is then used to select k most suitable demonstrations based on the likelihood of observing the concept tokens when conditioned on the demonstration pairs formulated as P M (\u02c6 \u03b8 | (x i, y i), . . . , (x k, y k)). Assuming independence among the sampled demonstrations, the top-ranked examples are obtained based on latent concept likelihood for individual examples, \n(4) \nThe selected demonstrations are used to perform in-context learning and are further generalizable for inference with LLMs larger than the ones used to learn \u02c6 \u03b8. \n# 4 Fair Latent Concept Learning \nLLMs have been shown to replicate social bias and prejudice likely present in their pre-training corpus. Providing LLMs with biased examples as demonstrations during ICL may further corroborate the prediction bias, potentially leading to discriminatory outcomes in classification tasks. However, filtering pre-training data and re-training/fine-turing LLMs on unbiased data is often practically infeasible due to resource constraints. Moreover, removing discrimination from pre-training data may not entirely address the unfairness resulting from biased demonstrations during inference. Here, we focus on the demonstration selection process itself which can guide LLM predictions by providing task-specific contextual information. Researchers have empirically shown that varying demonstrations can affect the bias and fairness outcomes of \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/71a3/71a32e22-7873-44cb-958f-44cecd517839.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Overview of FairICL including steps from A to D, A: A hierarchical attribute sampling approach is proposed to craft synthetic samples and create augmented training data \u00af D; B: Samples in \u00af D are utilized to learn latent concept tokens with an internal LLM; C: A corresponding likelihood score is computed for each sample in D, and all samples are then ranked to choose k  demonstrations from topm candidates; D: Selected demonstrations and test input x are used to prompt an external LLM to get prediction \u02c6 y. </div>\nLLMs (Hu et al., 2024; Ma et al., 2024). Furthermore, the proportion of samples from minority and majority groups in demonstrations affects the tradeoff between fairness and performance metrics (Hu et al., 2024). \n# 4.1 Problem Setup \nIn the latent concept variable model, demonstrations are selected based on the likelihood of observing latent concept variable \u02c6 \u03b8 (Wang et al., 2024). Generally, concept variable captures format and task information and can help improve in-context learning performance. However, the quality of the learned latent concept variable highly depends on the observed data D. We hypothesize that using a biased dataset D to learn the latent concept can lead to selecting demonstrations that favor the majority group. For instance, consider a dataset containing a comparatively higher number of positive/advantaged class instances for the majority group reflecting real societal bias. The latent concept variable may associate the positive outcome with the majority class as this biased prediction can lead to better prediction accuracy owing to imbalanced label distributions. Consequently, demonstrations selected using the latent concept variable can reinforce the bias originating from the dataset. In the following, we propose FairICL, a fair latent concept learning framework with data augmentation to mitigate unfairness in ICL predictive outcomes arising from demonstration selection. An overview of the method is presented in Fig. 1. \n# 4.2 Constructing Augmented Training Data \nTo ensure fair predictive outcomes, we consider the correlation between the sensitive attribute a and \nthe outcome variable y in the dataset D used to learn the latent concept variable \u03b8. We conjecture that learning latent concept variables from an unbiased dataset can prevent \u02c6 \u03b8 from incorporating bias into the task-specific contextual information that improves ICL performance. To this end, we design and implement a data pre-processing strategy on D aimed at de-correlating the sensitive attribute and the label. Assuming we obtain a dataset \u02dc D that preserves task-relevant information from D and not the biased correlation between a and y, we then construct an augmented training dataset \u00af D from both D and \u02dc D to promote fairness while learning task-specific contextual information in a fair representation of latent concepts \u02c6 \u03b8 f. Note that our focus is on LLM classification with ICL on tabular data which is the most commonly used data representation in fairness literature. To de-correlate the outcome variable from the sensitive attribute, we introduce synthetically generated examples sampled from the distribution of D but with any influence of a on y removed. Naively, we can achieve this de-correlation in the generated examples by sampling each attribute in x and the label y independently. However, this could result in highly noisy data samples that obfuscate useful information about the relationship between nonsensitive attributes and the outcome thus negatively affecting the prediction accuracy. Here, we propose to generate samples based on an attribute hierarchybased sampling process that simultaneously promotes sensitive attribute and label de-correlation and preserves task-relevant information. For hierarchical attribute sampling, we define an ordering for all non-sensitive attributes. We construct a synthetic sample based on the attribute \nordering as follows: we first randomly sample a label from a uniform distribution and obtain a subset of D conditioned on the sampled label value. We then sample the first non-sensitive attribute in the ordered list uniformly from the values occurring in the subset. We further constrain the subset to include only the sampled value of the first non-sensitive attribute and sample the second nonsensitive attribute uniformly and so on. To populate the sensitive attribute value, we randomly sample it from a uniform distribution independent of the label and any non-sensitive attributes. Furthermore, if D contains any proxy-sensitive attributes that may allude to the sensitive attribute, we condition its sampling on the sensitive attribute attribute value to promote complete de-correlation. In this manner, we generate \u02dc D = {(x i, y i)} \u02dc n i =1 as an unbiased representation of D. We then construct our augmented training dataset which contains n + \u02dc n instances and each augmented instance contains q  demonstration examples from D and one query sample from either D or \u02dc D  to facilitate in-context learning. Formally each instance takes the form \u27e8 (x 1, y 1), \u00b7 \u00b7 \u00b7, (x q, y q), x, y \u27e9 which we denote as (x, y) thereafter. We also denote this formatted training dataset containing augmented samples as \u00af D = {\u00af x i, y i} n +\u02dc n i =1 and the process is shown in lines 1-5 of Algorithm 1. The following discusses how we learn the fair latent concept variable from \u00af D. \n# 4.3 Learning Fair Latent Concept Variable \nWe learn the latent concept variable by implementing prompt tuning to optimize a set of new token embeddings that is prepended to each training input token sequence (Wang et al., 2024). More importantly, we utilize the augmented dataset \u00af D to construct input sequences for learning \u03b8 f  to promote improvements in fairness and utility simultaneously. Directly optimizing \u03b8 f as a sequence of words is not efficient due to the discrete nature of text space. Typically, large language models (LLMs) process inputs as sequences of tokens, which are subsequently transformed into embeddings. Therefore, we optimize the fair latent concept in the LLM M \u2019s embedding space, where \u03b8 f is represented as a sequence of c learnable tokens, each associated with an embedding vector. We denote the subset of weights in W corresponding to \u03b8 f as W \u03b8 f. During training, we prepend \u02c6 \u03b8 f to the input sequences and learn W \u03b8 f by minimizing the \nAlgorithm 1 Fair Latent Concept Learning and Demonstration Selection Input: Training dataset D, generated dataset \u02dc D, test query x, LLM M, number of tokens c, learning rate \u03bb, training epochs T, number of demonstrations for training q, number of demonstration candidates M, number of demonstrations for inference k Output: k demonstrations for test query x /* Constructing Augmented Data */ 1: for (x i, y i) \u2208{D \u222a \u02dc D} do 2: Sample (x 1, y 1), \u00b7 \u00b7 \u00b7, (x q, y q) from D 3: \u00af x i = (x 1, y 1, \u00b7 \u00b7 \u00b7, x q, y q, x i) 4: Add (\u00af x i, y i) to \u00af D \n# 5: end for \n# 14: end for \ufffd \nnegative log-likelihood objective as follows \n(5) \nDuring gradient optimization with L, only parameters W \u03b8 f corresponding to \u02c6 \u03b8 f are updated, and all other parameters are frozen. The ultimate goal of learning a fair latent concept variable is to derive the optimal \ufffd W \u03b8 f using the task-specific data D to improve performance and the generated data \u02dc D to promote fairness simultaneously. The training \nprocedure is summarized in Algorithm 1 where the latent concept learning process is shown in lines 6-14. \n# 4.4 Demonstration Example Selection with \u03b8 f Likelihood \nThe learned fair latent concept \u02c6 \u03b8 f is then used to select top-ranking examples from D which will be provided as context to a larger external LLM during inference via ICL. This demonstration selection follows the rationale that training examples that maximize the likelihood of predicting the trained task-specific latent concept variable are optimal demonstrations for the corresponding task objective (Wang et al., 2024). For each training example (x i, y i) \u2208 D, the likelihood of \u02c6 \u03b8 f  is expressed using the probability distribution shown as \n(6) \nIn our implementation, we obtain this likelihood as the probability of observing the trained \u02c6 \u03b8 f when postfixed to a sample (x i, y i). In Algorithm 1, these steps are outlined in lines 15-17. Subsequently, training examples are sorted based on their computed likelihood values (line 18). We then select the top m examples that maximize the likelihood of \u02c6 \u03b8 f and form the demonstration candidate set (line 19). We subsample this candidate set to allow each test query to be paired with varying demonstrations during testing. Finally, we randomly select k demonstration examples from the candidate set for each test instance, combining these to construct the final prompt for inference with an external LLM. \n# 5 Experimental Evaluation \n# 5.1 Datasets \nWe evaluate the effectiveness of fair demonstration selection with FairICL using a benchmark fair machine learning dataset. The Adult Income dataset (Becker and Kohavi, 1996) contains 1994 U.S. census records and the task is to predict whether an individual has an annual income greater than 50,000 US dollars based on demographic and economic attributes that characterize each entry. Following previous work (Liu et al., 2023), we use a subset of 10 attributes from the dataset named in Fig. 2. We also subsample a training dataset of 30,000 records after preprocessing. We perform serialization on the tabular Adult dataset similar to Hegselmann et al. (2023); Carey et al. (2024), \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ee49/ee499310-e87c-4dec-8e0e-62d499dd6f92.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Hierarchical order of attributes in Adult dataset for augmented data generation </div>\n<div style=\"text-align: center;\">Table 1: Adult Dataset Statistics </div>\nLabel\nTrain (D)\nAugmented ( \u02dcD)\nTest (Dt)\nMale\nFemale\nMale\nFemale\nMale\nFemale\nPositive\n6363\n1105\n7574\n7468\n250\n250\nNegative\n13897\n8635\n7534\n7424\n250\n250\ni.e.: we convert each row in the dataset to a natural language format to facilitate LLM prompting. The specific serialization template and in-context learning format are shown in Fig. 3. As discussed in Section 4.2, we generate an augmented training dataset to enable fair latent concept learning. For the Adult dataset, we use sex as the sensitive attribute and distinguish relationship and marital status as the proxy-sensitive attributes as some instances of the relationship attribute contain gender-specific vocabulary and the attribute  marital status may depend on values of relationship. To generate augmented data samples, we specify a hierarchical order for the non-sensitive attributes and a separate order for the sensitive and proxysensitive attributes shown in Fig. 2. Using this attribute sampling technique, we generate \u02dc n = n number of unique augmented data samples and construct our training dataset \u00af D by combining D and \u02dc D. For the test dataset, we randomly sample 1000 instances with equal representation for majority and minority groups for each experimental run. Table 1 includes the training, augmented, and test dataset distribution in terms of the class label and the sensitive attribute. \n# 5.2 Fairness Metrics \nHere, we briefly describe two fairness notions used to determine LLM\u2019s bias w.r.t the majority and minority groups represented by sensitive attribute a when predicting a binary outcome variable y. \n### Instruction: Based on the profile description of an individual recorded in the 1994 U.S. census, answer the question about their income. ### Profile: This person is a 38 years old female. She has attended a professional school and works in the private sector. Her occupation is in professional specialty. She works 50 hours per week. She had a capital gain of 0 and a capital loss of 0 last year. She has never been married. She is not related to the other person in her household. ### Question: Does this person have an income of more than 50,000 U.S. dollars? ### Answer: Yes... ### Profile: This person is a 28 years old male. He has an academic associate\u2019s degree and works in local government. His occupation is in protective services. He works 40 hours per week. He had a capital gain of 0 and a capital loss of 0 last year. He is married to a civilian. He is the husband of the other person in his household. ### Question: Does this person have an income of more than 50,000 U.S. dollars? ### Answer: \nStatistical Parity Statistical parity (Dwork et al., 2012) requires the predictions to be independent of the sensitive attribute and can be evaluated as \n\u2206 SP = P (\u02c6 y | s = 0) \u2212 P (\u02c6 y | s = 1). \n(7) \nEqual Opportunity Equal opportunity (Hardt et al., 2016) requires that for members of majority and minority groups, the probability of being assigned a positive outcome is the same. We evaluate equal opportunity using group-based TPRs as \n(8) \n# 5.3 Baselines \nWe demonstrate the effectiveness of FairICL by comparing its performance against several baselines that implement different demonstration selection approaches briefly described as follows. \n\u2022 Random  refers to standard in-context learning where k training examples are randomly sampled as demonstrations for each test instance (Brown et al., 2020). \u2022 Balanced implements in-context learning with equal representation for each sensitive attribute and class label combination in the demonstrations (Li et al., 2023b). \u2022 Instruction is used to evaluate an LLM for fair and unbiased decisions based on manual prompting-based guidance with a balanced demonstration set (Li et al., 2023b; Atwood et al., 2024b). \u2022 Removal omits the sensitive attribute from the demonstrations of Balanced (Li et al., 2023b). \nAs we serialize tabular data, we further replace gendered pronouns with gender-neutral ones in the training data. \u2022 Counterfactual is another heuristic technique and constructs demonstrations using k/ 2  examples from the majority (minority) group and the remaining examples by flipping the sensitive attribute of the previously sampled examples (Li et al., 2023b). \u2022 LatentConcept  is the latent concept variablebased approach from Wang et al. (2024) where the latent concept variable is learned using the training dataset and then used to select topk demonstrations. \n# 5.4 Experimental Setup \nIn the FairICL framework we use LLaMA-27B (Touvron et al., 2023) as the internal LLM for fair latent concept learning and LLaMA-213B (Touvron et al., 2023) as the external LLMs for inference. We fix the learning rate at 0.0001 for all experiments and optimize the concept token embeddings over 5 epochs. For main experiments, we fix the number of added tokens c at 10, and the number of demonstrations during training q at 2. We randomly sample k = 4 demonstrations from a top-ranked candidate set of M  = 100 training examples for each test query. We report performance as an average of 5 runs with standard deviations for different test splits. We also evaluate the learned fair latent concepts with LLaMA-2-7B as the external LLM. To analyze the effectiveness of latent concept learning with an augmented dataset, we conduct an ablation study where the augmented samples are created via complete random sampling \n<div style=\"text-align: center;\">Table 2: Performance and fairness metrics of FairICL compared with baselines on LLaMA-2-7B and LLaMA-2-13B as external LLMs and LLaMA-2-7B as the internal LLM for latent concept learning; bold denotes best performance among fairness-aware methods and underline denotes best performance among all models </div>\nExternal LLM\nMethod\nAcc(%)\u2191\nF1(%)\u2191\n\u2206SP\u2193\n\u2206EO\u2193\nLLaMA-2-7B\nRandom (Brown et al., 2020)\n69.920.87\n62.801.25\n0.080.02\n0.080.04\nLatentConcept (Wang et al., 2024)\n70.041.69\n64.792.42\n0.170.02\n0.170.04\nBalanced (Li et al., 2023b)\n63.306.33\n44.0917.37\n0.040.03\n0.040.03\nCounterfactual (Li et al., 2023b)\n59.581.02\n34.442.87\n0.080.01\n0.130.02\nRemoval (Li et al., 2023b)\n64.606.25\n47.9217.34\n0.090.05\n0.110.06\nInstruction (Li et al., 2023b)\n63.866.35\n46.6414.86\n0.070.06\n0.090.05\nFairICL\n68.480.89\n64.421.01\n0.020.03\n0.010.04\nLLaMA-2-13B\nRandom (Brown et al., 2020)\n76.001.19\n75.751.44\n0.140.04\n0.110.08\nLatentConcept (Wang et al., 2024)\n77.480.70\n77.220.74\n0.160.02\n0.120.01\nBalanced (Li et al., 2023b)\n74.581.38\n71.922.19\n0.130.05\n0.100.07\nCounterfactual (Li et al., 2023b)\n68.182.05\n57.394.55\n0.130.06\n0.170.08\nRemoval (Li et al., 2023b)\n75.720.98\n76.482.16\n0.140.03\n0.090.02\nInstruction (Li et al., 2023b)\n76.201.09\n77.211.61\n0.200.07\n0.150.06\nFairICL\n75.721.60\n77.611.35\n0.080.02\n0.030.03\nas opposed to hierarchy-based sampling. We also evaluate the learned fair latent concepts directly by pretending them to test queries during inference. We investigate the impact of FairICL hyperparameters on fair latent concept learning via its overall performance. To this end, we report results when varying q as {0, 2, 4} and evaluate the effect of \u02dc n, i.e., the size of generated dataset \u02dc D, on FairICL\u2019s effectiveness. Finally, we vary k among {2, 4, 6, 8} to analyze the influence of demonstration size on inference results. All experiments are conducted on NVIDIA A100 GPUs with 40GB RAM. \n# 5.5 Results \nModel performance and comparison with baselines We report the results from inference with LLaMA-2-7B and LLaMA-2-13B in Table 2 for the Adult income dataset. Firstly, we observe the baseline performances denoted by Random where k demonstrations are randomly selected from the training set D. LLaMA-2-13B has increased utility compared to LLaMA-2-7B undoubtedly due to the model\u2019s complexity. However, the fairness metrics \u2206 SP and \u2206 EO are larger indicating a significant presence of bias in the outputs generated by LLaMA-2-13B. With the LatentConcept method which optimizes demonstration selection for utility, model performance is improved but the bias is further amplified for both 7B and 13B models. These results motivate our study of bias in LLMs specifically for tabular classification and methods \nthat can promote fairness in a resource-efficient manner. In Table 2, we observe that FairICL can noticeably improve statistical parity and equal opportunity measure for LLaMA-2-7B compared to the Random and LatentConcept methods while achieving comparable performance. Similarly, FairICL significantly reduces unfairness for LLaMA-2-13B with minimal loss of utility. Note that, the latent concept variable is learned using the smaller LLaMA-2-7b as the internal model and the selected demonstrations are utilized to construct inference prompts for LLaMA-2-13b. This shows that FairICL can generalize the fair demonstration selection process to larger LLMs thus making the method resource-efficient. Since the external LLMs are used only for few-shot inference, FairICL also enables generalization to black-box LLMs. We also evaluate the effectiveness of FairICL compared to multiple fair demonstration selection baselines. As discussed in Section 5.3, these methods have been formulated to address the LLM fairness issue via heuristic approaches. For LLaMA-27B, the fair baselines reduce unfairness compared to the LatentConcept method but incur a significant loss in performance. Compared to Random, only the Balanced approach shows a notable reduction in SP and EO metrics. FairICL, however, achieves the best fairness results without negatively affecting utility. For LLaMA-2-13B, the baselines mostly maintain performance but do not achieve \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7df2/7df2ffbd-2946-4211-b030-207fccebc1c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: FairICL performance with LLaMA-2-13B over training epochs </div>\nfair outcomes. In contrast, FairICL shows a large decline in fairness metrics with similar or even improved accuracy and f1-scores compared to nonfair baselines. These results demonstrate that the de-correlation of the sensitive attribute and the outcome in FairICL helps learn fair latent concepts, resulting in demonstration selection that promotes fair predictions. \nFairICL performance over training epochs We analyze the performance of FairICL as latent concept learning progresses over the training epochs and present the results in Fig. 4 for LLaMA-2-13B. To obtain these results, we fix the parameters q at 2, c at 10, and k  at 4. We observe that the accuracy and F1 experience a small decline after the first epoch but remain fairly stable thereafter. The SP and EO metrics on the other hand have a decreasing trend as the latent concepts are further optimized. This indicates that FairICL effectively allows the concept tokens to learn fairness-promoting context from the augmented examples and utilitypreserving information from the original training samples. This ultimately leads to a demonstration selection process that improves both fairness and performance in LLMs. \nAblation Study In this section, we investigate the role of data augmentation and latent concept learning in FairICL through an ablation study. We implement two variations of FairICL, FairICL-LC and FairICL-R. FairICL-LC directly evaluates the fair latent concepts as we prepend the learned concept variable to the prompt for test instances along with randomly sampled k  demonstrations. FairICLR adopts a completely random sampling mechanism for all attributes to create the augmented dataset and follows an inference procedure similar to FairICL. In other words, the generated dataset \nMethod\nAcc(%)\u2191\nF1(%)\u2191\n\u2206SP\u2193\n\u2206EO\u2193\nFairICL\n68.480.89\n64.421.01\n0.020.03\n0.010.04\nFairICL-LC\n75.961.20\n70.701.67\n0.060.01\n0.080.02\nFairICL-R\n58.580.62\n31.720.94\n0.010.01\n0.000.03\nTable 3: Ablation results on LLaMA-2-7B \ndoes not preserve the useful correlation between the non-sensitive attributes and outcomes like in FairICL. We report ablation results in Table 3 for LLaMA-2-7B since FairICL-LC can be evaluated only for the internal LLM whose vocabulary contains additional tokens corresponding to the latent concept variable. FairICL-LC achieves the best accuracy and F1 score indicating that the latent concept learns information relevant to the task. Also, the low fairness metrics imply that training with the augmented dataset prompts the latent concept to favor fair predictions. FairICL-R achieves almost ideal fairness metrics but does not maintain model accuracy as the randomly generated dataset removes even the useful correlation between nonsensitive attributes and labels. FairICL, however, preserves relevant information in \u02dc D prompting fair latent concept learning thus achieving fair and accurate predictive results. \n# Number of Demonstrations We investigate the \n# Number of Demonstrations \ninfluence of the number of demonstrations during latent concept learning, denoted by q, and during inference, denoted by k, on the overall performance of FairICL. Firts, we vary q  among {0, 2, 4} and include the results in Fig. 5 for LLaMA-2-13B while keeping the other parameters fixed at c = 10 and k = 4. From Fig. 5, we observe that the accuracy and F1-scores remain fairly unchanged across different values of q. However, the SP and EO values are noticeably higher at q  = 4 with the best metrics observed at q = 2. Since the q demonstrations \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/568a/568a4067-0c76-4203-ba11-7991dfdbac82.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Performance metrics </div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dc69/dc697066-abe4-4a60-aa38-343f3f399ffe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: FairICL performance with LLaMA-2-13B for varying number of demonstrations (k) during inference </div>\nfor training are obtained from the original dataset containing biased examples, training prompts constructed with more biased samples negatively affect the fairness during inference. On the other hand, having fewer demonstrations does not affect model utility as the augmented samples preserve useful correlations from the original dataset. We then vary k  among {2, 4, 6, 8} for LLaMA2-13B with fixed q = 2 and c = 10, and include the results in Fig. 6. Here, the fairness metrics demonstrate a sharper decline as the number of demonstrations during inference increases. We also observe a slight decrease in the utility metrics as the number of demonstrations during inference increases most likely due to the trade-off between utility and fairness. Since the k demonstrations are obtained from the top 100 training examples ranked by the fair latent concept variable, having a larger k allows the inference prompt to strongly guide the LLM towards fairer predictions. \n# Size of Augmented data In this section, we con \n# Size of Augmented data \nSize of Augmented data In this section, we conduct a sensitivity analysis of \u02dc n, the size of \u02dc D  relative to n, the size of D, to evaluate the influence of the augmented dataset on FairICL\u2019s performance. \nWe vary \u02dc n as {0, 25, 50, 100}% of its original size of 30,000 generated samples. We fix the other parameters q at 2, c at 10, and k at 10 to perform latent concept learning and obtain results for LLaMA-213B shown in Fig. 7. Note that the \u02dc n = 0% setting corresponds to the LatentConcept baseline method in Table 2. From the results, we notice that the accuracy and F1-scores are generally unchanged the more augmented examples are included in the training prompt. This indicates that the data augmentation process in FairICL does not negatively affect an LLM\u2019s predictive performance. We further observe significant drops in the fairness metrics as the size of \u02dc D used for latent concept learning is increased. This demonstrates the positive impact of the data augmentation strategy in FairICL. \n# 6 Conclusion \nWe have investigated the issue of fairness in large language models during in-context learning for classification on tabular datasets. We focused on a latent concept learning framework that optimizes the demonstration selection process for improved model utility via latent concept variables learned \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c1f/7c1fc648-3c73-4238-adeb-3fabf4775a77.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c16/4c16dc63-6b27-47c7-ba60-5a8baf91c548.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Performance metrics </div>\nFigure 7: Performance and fairness metrics of FairICL with LLaMA-2-13B for different sizes of \u02dc D for latent concept learning \nfrom a training dataset. We hypothesized that learning the latent concepts from a biased dataset can cause the selection of biased demonstrations resulting in unfair predictions during in-context learning and empirically verified this phenomenon. To tackle this issue, we presented a fairness-aware latent concept learning framework, FairICL, that incorporates data augmentation to enable learning concept tokens that promote fairness while preserving task-relevant contextual information. Our experimental analysis showed that FairICL can effectively mitigate fairness without causing significant tradeoffs in model utility. Furthermore, FairICL can improve fairness and utility metrics compared to multiple heuristic fairness-aware demonstration selection approaches. We also conducted extensive experiments to evaluate the influence of different FairICL hyperparameters on the learned latent concepts and consequently in-context learning performance with respect to utility and fairness. \n# Acknowledgements \nThis work was supported in part by NSF grants 1946391, 2119691, and 2147375. \n# References \nAbubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models associate muslims with violence. Nature Machine Intelligence, 3(6):461\u2013463. James Atwood, Preethi Lahoti, Ananth Balashankar, Flavien Prost, and Ahmad Beirami. 2024a.  Inducing group fairness in llm-based decisions. CoRR, abs/2406.16738. James Atwood, Preethi Lahoti, Ananth Balashankar, Flavien Prost, and Ahmad Beirami. 2024b.  Inducing group fairness in llm-based decisions. ArXiv, abs/2406.16738. \n<div style=\"text-align: center;\">(b) Fairness metrics </div>\nChristine Basta, Marta R Costa-Juss\u00e0, and Noe Casas. 2019. Evaluating the underlying gender bias in contextualized word embeddings. arXiv preprint arXiv:1904.08783. Barry Becker and Ronny Kohavi. 1996. Adult. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5XW20. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Alycia N Carey, Karuna Bhaila, Kennedy Edemacu, and Xintao Wu. 2024. Dp-tabicl: In-context learning with differentially private tabular data. arXiv preprint arXiv:2403.05681. Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, and Abhijnan Chakraborty. 2024. Few-shot fairness: Unveiling llm\u2019s potential for fairness-aware classification. arXiv preprint arXiv:2402.18502. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. 2012. Fairness through awareness. In Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, January 8-10, 2012, pages 214\u2013226. ACM. Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. 2023. Bias and fairness in large language models: A survey. CoRR, abs/2309.00770. Moritz Hardt, Eric Price, and Nati Srebro. 2016.  Equality of opportunity in supervised learning. In  Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3315\u20133323. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. \n2023. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549\u20135581. PMLR. \ndata with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549\u20135581. PMLR. Jingyu Hu, Weiru Liu, and Mengnan Du. 2024.  Strategic demonstration selection for improved fairness in LLM in-context learning. CoRR, abs/2408.09757. Tenghao Huang, Faeze Brahman, Vered Shwartz, and Snigdha Chaturvedi. 2021.  Uncovering implicit gender bias in narratives through commonsense inference. In  Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 3866\u20133873. Association for Computational Linguistics. Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023a. Large language models in finance: A survey. In Proceedings of the Fourth ACM International Conference on AI in Finance, pages 374\u2013382. Yunqi Li, Lanjing Zhang, and Yongfeng Zhang. 2023b. Fairness of chatgpt. arXiv preprint arXiv:2305.18569. Yunqi Li, Lanjing Zhang, and Yongfeng Zhang. 2024. Probing into the fairness of large language models: A case study of chatgpt. In 58th Annual Conference on Information Sciences and Systems, CISS 2024, Princeton, NJ, USA, March 13-15, 2024, pages 1\u20136. IEEE. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt3? arXiv preprint arXiv:2101.06804. Yanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. 2023. Confronting llms with traditional ml: Rethinking the fairness of large language models in tabular classifications. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786. Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, and Bingzhe Wu. 2024. Fairnessguided few-shot prompting for large language models.  Advances in Neural Information Processing Systems, 36. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975. \nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975. \nZhongxiang Sun. 2023. A short survey of viewing large language models in legal aspect. arXiv preprint arXiv:2303.09136. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023.  Decodingtrust: A comprehensive assessment of trustworthiness in GPT models. In  Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2024. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36. Chengyan Wu, Zehong Lin, Wenlong Fang, and Yuyan Huang. 2023. A medical diagnostic assistant based on llm. In China Health Information Processing Conference, pages 135\u2013147. Springer. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080. \n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of fairness in large language models (LLMs) during in-context learning (ICL) for classification on tabular datasets. Previous methods have shown that LLMs can inherit social bias from their pre-training data, leading to discriminatory outputs. This necessitates a new method to promote fairness while maintaining predictive utility.",
        "problem": {
            "definition": "The problem is the presence of bias in LLM outputs during in-context learning with tabular data, which can result in unfair predictions that reflect societal inequalities.",
            "key obstacle": "The core obstacle is that existing methods for demonstration selection often reinforce biases present in the training data, leading to biased predictions."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that the latent concept variable can influence model performance and fairness outcomes in LLMs.",
            "opinion": "The proposed idea, FairICL, utilizes latent concept variables to select demonstrations that promote fairness in predictions while maintaining model utility.",
            "innovation": "The primary innovation of FairICL lies in its incorporation of data augmentation strategies that reduce correlation between sensitive attributes and outcomes, thereby promoting fairness in LLM outputs."
        },
        "method": {
            "method name": "FairICL",
            "method abbreviation": "FICL",
            "method definition": "FairICL is a framework that leverages latent concept variables and data augmentation to improve fairness in LLM predictions during in-context learning.",
            "method description": "The core of FairICL is to learn fair latent concepts from augmented training data that decorrelates sensitive attributes from outcomes.",
            "method steps": [
                "Construct an augmented training dataset that reduces correlation between sensitive attributes and outcomes.",
                "Learn the fair latent concept variable using a smaller internal LLM.",
                "Select demonstrations based on the learned latent concept variable to guide predictions in a larger external LLM."
            ],
            "principle": "FairICL is effective because it reduces the influence of biased training data on the demonstration selection process, leading to fairer predictions."
        },
        "experiments": {
            "evaluation setting": "The evaluation uses the Adult Income dataset, with a focus on predicting income based on demographic attributes. The dataset includes a training set of 30,000 records and a test set with 1,000 instances, ensuring equal representation of majority and minority groups.",
            "evaluation method": "The performance of FairICL is assessed by comparing its results against multiple baseline methods using metrics such as accuracy, F1 score, statistical parity, and equal opportunity."
        },
        "conclusion": "FairICL effectively mitigates bias in LLM predictions while maintaining predictive utility. The empirical results demonstrate that FairICL can generalize the fair demonstration selection process to larger LLMs, outperforming various heuristic approaches in terms of fairness and utility.",
        "discussion": {
            "advantage": "The key advantage of FairICL is its ability to improve fairness metrics significantly without incurring substantial losses in model performance.",
            "limitation": "A limitation of the method is that it relies on the quality of the data augmentation process; poor augmentation may lead to suboptimal fairness outcomes.",
            "future work": "Future work could explore further refinements in data augmentation techniques and investigate the applicability of FairICL in other domains beyond tabular data."
        },
        "other info": {
            "acknowledgements": "This work was supported in part by NSF grants 1946391, 2119691, and 2147375.",
            "code_url": "https://github.com/karuna-bhaila/fairicl"
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The problem is the presence of bias in LLM outputs during in-context learning with tabular data, which can result in unfair predictions that reflect societal inequalities."
        },
        {
            "section number": "1.3",
            "key information": "This paper addresses the issue of fairness in large language models (LLMs) during in-context learning for classification on tabular datasets."
        },
        {
            "section number": "3.1",
            "key information": "The core of FairICL is to learn fair latent concepts from augmented training data that decorrelates sensitive attributes from outcomes."
        },
        {
            "section number": "4.1",
            "key information": "FairICL utilizes latent concept variables to select demonstrations that promote fairness in predictions while maintaining model utility."
        },
        {
            "section number": "6.1",
            "key information": "The key advantage of FairICL is its ability to improve fairness metrics significantly without incurring substantial losses in model performance."
        },
        {
            "section number": "6.2",
            "key information": "A limitation of the method is that it relies on the quality of the data augmentation process; poor augmentation may lead to suboptimal fairness outcomes."
        }
    ],
    "similarity_score": 0.7092377314130517,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Fair In-Context Learning via Latent Concept Variables.json"
}