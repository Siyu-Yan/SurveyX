{
    "from": "google",
    "scholar_id": "kCttIVky67gJ",
    "detail_id": null,
    "title": "Llmrec: Benchmarking large language models on recommendation task",
    "abstract": " Abstract\n\nRecently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs\u2019 instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainabilitybased tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.\n\n# Introduction\n\nDeep learning based recommender systems (RSs) have gained significant attention due to their ability to capture complex patterns and relationships embedded in user behavior data. These RSs leverage deep neural networks to first learn informative representations of users and items from user-item interaction data and then make accurate recommendations using these representations. They have shown impressive performance in various",
    "bib_name": "liu2023llmrec",
    "md_text": "# LLMRec: Benchmarking Large Language Models on Recommendation T\n\n# nling Liu 1, Chao Liu 2, Peilin Zhou 3, Qichen Ye 4, Dading Chong 4, Kang Zhou 1, Yueqi Xie 5 Yuwei Cao 8, Shoujin Wang 6, Chenyu You 7, Philip S. Yu 8\n\n1 Alibaba Group, 2 Ant Group, 3 The Hong Kong University of Science and Technology (Guangzh 4 School of Electronic and Computer Engineering, Peking University 5 The Hong Kong University of Science and Technology 6 Data Science Institute,University of Technology Sydney 7 Department of Electrical Engineering, Yale University 8 Department of Computer Science, University of Illinois Chicago\n\n7 Department of Electrical Engineering, Yale University 8 Department of Computer Science, University of Illinois Chicago william.liuj@gmail.com, chaoliu pku@pku.edu.cn, zhoupalin@gmail.com, yeeeqichen@pku.edu.cn 1601213984@pku.edu.cn, kangbeyond89@163.com, yxieay@connect.ust.hk, ycao43@uic.edu shoujin.wang@mq.edu.au, chenyu.you@yale.edu, psyu@uic.edu\n\n# Abstract\n\nRecently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs\u2019 instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainabilitybased tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.\n\n# Introduction\n\nDeep learning based recommender systems (RSs) have gained significant attention due to their ability to capture complex patterns and relationships embedded in user behavior data. These RSs leverage deep neural networks to first learn informative representations of users and items from user-item interaction data and then make accurate recommendations using these representations. They have shown impressive performance in various web applications, including e-commerce (Sun et al. 2022; Liu 2022; Tsagkias et al. 2021; Xie, Zhou, and Kim 2022), video platforms (Wei et al.\n\nCopyright \u00a9 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\n2019; Zhao et al. 2019; Papadamou et al. 2022), news websites (Wu et al. 2022, 2020, 2019), and music streaming services (Kowald, Schedl, and Lex 2020; Singh et al. 2022). However, most existing deep learning based recommendation methods are task and domain specific. As a result, they require a large amount of task-specific and in-domain data for training numerous recommendation models that cater to various tasks and application scenarios. Such specialization results in a lack of efficient and effective generalization ability, inhibiting these models from effectively utilizing a vast amount of general knowledge. At the same time, Large Language Models (LLMs), such as ChatGPT, have showcased remarkable adaptability, significantly enhancing the performance of downstream Natural Language Processing (NLP) tasks, as a general model. Even under zero-shot settings, where no additional training data is provided for downstream tasks, LLMs still achieve significant performance across numerous NLP tasks, progressively narrowing the gap with human. For instance, (Dai et al. 2023) employs ChatGPT to enhance text data augmentation by rephrasing sentences. Their findings reveal the potential of ChatGPT in effectively diversifying and expanding the training data. (Jiao et al. 2023) finds ChatGPT surpasses the previously state-of-the-art zero-shot model by a considerable margin in the sentiment analysis task. (Bang et al. 2023) finds ChatGPT outperforms the previous stateof-the-art zero-shot model by a large margin in the sentiment analysis task. To further explore the capabilities of LLMs, existing studies have assessed LLMs (mainly focus on ChatGPT) from various aspects of NLP tasks, including reasoning (Liu et al. 2023a), fairness (Li and Zhang 2023), and ethical issues (Zhuo et al. 2023). However, these studies are still confined to the domain of natural language processing. The performance of LLMs under other data modalities, such as user-item interaction data, has not been thoroughly investigated (Liu et al. 2023b). Besides, whether LLMs can perform well on classical recommendation tasks remains an open question. Motivated by the aforementioned facts, in this paper, we explore the potential of creating an LLM-based recommender system to solve recommendation problems in\n\na unified manner. This is especially pertinent given that various tasks within the realm of recommendation inherently involve natural language components, such as explainable recommendations, interest summarization, item descriptions, and more. Existing recommendation models that utilize language models (LMs) typically confine themselves to smaller-sized LMs and involve a finetuning process. For instance, (Zhang et al. 2021) propose to leverage BERT (Devlin et al. 2019) and GPT2 (Radford et al. 2018) as recommender systems and utilize prompts to transform the session-based recommendation task into a multi-token cloze task. Furthermore, P5 (Geng et al. 2022) utilizes a mixture of prompts from various tasks to assist the T5 (Raffel et al. 2020a) in supporting a broad spectrum of recommendation tasks. However, these methods are not directly transferable to LLMs due to the high cost of finetuning. Additionally, they do not investigate the effectiveness of LLMs for recommendation. To fill this research gap, we design an LLM-based recommender system, namely LLMRec, to benchmark the performance of various LLMs (e.g., ChatGPT and ChatGLM (Du et al. 2022)) on Amazon Beauty dataset. In LLMRec, each LLM is employed as a general-purpose recommendation model to handle five classical recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. By doing so, we want to investigate whether LLM\u2019s extensive linguistic and world knowledge acquired from large-scale corpora can be effectively transferred to recommendation scenarios and boost recommmendation performance. In order to equip LLMs with the capability to tackle recommendation tasks, one of the most crucial steps involves constructing high-quality prompts. We devise prompts that can fully utilize the inherent knowledge of the large language model, while simultaneously accommodating various recommendation tasks. Prompt construction plays a pivotal role not only in directly inferring from the large language model but also in fine-tuning PLMs for recommendation tasks. The main findings of this benchmark are as follows:\n\u2022 Off-the-shelf LLMs, such as ChatGPT, exhibit limited proficiency in accuracy-based tasks like sequential and direct recommendation, possibly due to the lack of extensive exposure to all potential candidate items without specific fine-tuning using recommendation data.\n\u2022 Supervised finetuning (SFT) can significantly improves LLMs\u2019 instruction compliance ability in recommendation tasks. For instance, ChatGLM-6B with P-tuning is superior to ChatGPT in rating prediction, explanation generation, and review summarization.\n\u2022 For explainable recommendation tasks such as explanation generation and review summarization, objective metrics like BLEU and ROUGE fail to accurately measure the true capability of LLM-based recommender systems. While LLMs receive lower scores on these objective metrics, their generated contents are superior to state-of-the-art methods via qualitative evaluation.\n\nLanguage Models (LMs) are a foundational component of natural language processing (NLP) and have been the subject of extensive research over several decades. Large Language Models (LLMs) represent a specific category of LMs that harness vast amounts of data and computational resources, enabling them to achieve cutting-edge performance across a wide spectrum of NLP tasks. For instance, LLMs excel in machine translation(Chen et al. 2018; Aharoni, Johnson, and Firat 2019; Zeng et al. 2022), summarization(See, Liu, and Manning 2017; Liu 2019), and dialogue generation(Li et al. 2016; Dhingra et al. 2016). In the early stage, traditional LMs leverage simple neural architectures including recurrent neural networks (RNNs) and long shortterm memory (LSTM) networks to capture dependencies in language(Bengio et al. 2003; Hochreiter and Schmidhuber 1997). However, traditional neural language models still struggled with capturing the rich semantic and contextual relationships present in natural language. The introduction of the Transformer architecture by (Vaswani et al. 2017) was a major breakthrough in this area. This architecture has been used as the backbone of many successful LLMs, including BERT(Devlin et al. 2019), GPT-2(Radford et al. 2019), ChatGPT(OpenAI 2023), ChatGLM (Du et al. 2021), LLaMA(Zhang et al. 2023), Alpaca(Taori et al. 2023), etc.\n\n# Large Language Model for Recommendation\n\nThe impressive accomplishments of LLMs have inspired increased attention towards their application in recommender systems, leading to notable advancements in this field. For instance, LMRecSys (Zhang et al. 2021) tackles zero-shot and data efficiency issues by transforming recommendation tasks into multi-token cloze tasks. P5 (Geng et al. 2022) is the first attempt to integrate different recommendation tasks within a shared conditional language generation framework (i.e., T5 (Raffel et al. 2020b)), while M6-Rec (Cui et al. 2022) focuses on building a foundation model to support a wide range of recommendation tasks, including retrieval, ranking, explanation generation, etc. Additionally, Chat-REC (Gao et al. 2023) utilizes ChatGPT to enhance existing recommender models through conversational recommendations, adding interactivity and explainability. More recently, (Liu et al. 2023b) conducts a preliminary study that evaluates the potential of ChatGPT across various recommendation tasks. Our approach differs from the above methods in that we treat LLMs as a self-contained recommender, and based on this, we have designed an LLM-based recommender system called LLMRec. We perform a comprehensive evaluation and comparison of the performance of LLMs on various recommendation tasks. Our objective is to provide researchers with valuable insights to further explore the capabilities of LLMs in recommendation systems.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0857/085770e8-00c0-403c-a42f-431b623465b0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The overall architecture of LLMRec.\n</div>\n# LLMRec\n\n# Task Descriptions\n\nTo conduct a comprehensive test on the recommendation ability of LLMs, we carried out experiments on five tasks: rating prediction, direct recommendation, sequential recommendation, explanation generation, and review summarization. Rating prediction aims to predict the ratings that a user would give to a particular item. Sequential recommendation aims to predict a user\u2019s next item or action based on their past sequential behavior. Direct Recommendation is a type of recommendation system that relies on explicit feedback from users in the form of ratings or reviews. Explanation generation refers to providing users or system designers with explanations to clarify why such items are recommended. Review summarization is primarily used to extract and summarize the content of a series of user reviews, enabling other users to quickly understand the strengths and weaknesses of a product or service. The five tasks mentioned above can be categorized into two groups based on their characteristics: accuracy-based and explainability-based.\n\n# Overall Architecture\n\nIn this paper, we have designed an LLM-based recommender system called LLMRec in order to benchmark the performance of various LLM models on the aforementioned five tasks. The workflow of the proposed recommendation system is illustrated in Fig.1, which consists of three steps. Firstly, we generate task-specific prompts using task description, behavior injection, and format indicator modules. The task description module is utilized to adapt recommendation tasks to natural language processing tasks. The behavior injection module is designed to assess the impact\n\nof Chain-of-Thought (CoT) prompting, which incorporates user-item interaction to aid LLMs in capturing user preferences and needs more effectively. The format indicator serves to constrain the output format, making the recommendation results more comprehensible and assessable. Secondly, we leverage LLM\u2019s strong understanding and generation capabilities to directly input the prompt into LLM and obtain corresponding recommendation results. Our LLMRec also supports fine-tuning open-source LLMs using generated prompts to enhance their ability in the recommendation domain. Finally, although LLMs incorporate randomness into the response generation process to ensure diverse results, this randomness can pose challenges in evaluating recommended items. To address this issue, we have developed an output refinement module that checks the format of the recommender\u2019s output, corrects any non-compliant results, or feeds them back into LLMs for re-recommendation.\n\n# Off-the-shelf LLM as Recommender\n\nTo evaluate the recommendation capabilities of off-the-shelf LLMs in five recommendation tasks, we conducted a benchmarking study on ChatGPT, ChatGLM, LLaMA, and Alpaca using LLMRec. We followed the prompt construction method used in (Liu et al. 2023b) and readers can check the details from their paper.\n\n# Fine-tuned LLM as Recommender\n\nIn order to fully assess the recommendation capabilities of LLMs, we conducted task-specific finetuning on various open-source LLMs, including ChatGLM, LLaMA, and Alpaca, using LLMRec as a foundation. We utilized LLMRec\u2019s prompt construction module to produce training data for finetuning. Specifically, as shown in Table 2, we de\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4264/42644613-dbe6-42f7-8f58-bd9430dcd93a.png\" style=\"width: 50%;\"></div>\nsigned several templates for 5 recommendation tasks according to their characteristics. Due to limited computational resources, we conducted task-specific finetuning using corresponding prompts for each task, instead of jointly training prompts for all tasks as in P5, which aligns more with multi-task finetuning. In the inference process, sequential and direct recommendation tasks typically necessitate an item list as the target output. In that case, for sequential recommendation, beam search algorithms are employed to generate a list of potential next items, which is then evaluated under the all-item setting. In direct recommendation, recommended items are predicted from a candidate set. In this context, beam search is also utilized to decode a list of potential target items with the highest scores, after which evaluations are conducted.\n\n# Benchmark\n\nFollowing P5 (Geng et al. 2022), we gather a range of approaches that are representative of various tasks. For rating prediction, we employ MF (Koren, Bell, and Volinsky 2009) and MLP (Cheng et al. 2016) as our baselines. For direct recommendation, we use BPR-MF (Rendle et al. 2012), BPRMLP (Cheng et al. 2016) and SimpleX (Mao et al. 2021) as baselines. For sequential recommendation, we adopt Caser (Tang and Wang 2018), HGN (Ma, Kang, and Liu 2019), GRU4Rec (Hidasi et al. 2015), BERT4Rec (Sun et al. 2019), FDSA (Zhang et al. 2019), SASRec (Kang and McAuley 2018) and S 3-Rec (Zhou et al. 2020) as baselines for comparison. For explanation generation, we utilize Attn2Seq (Dong et al. 2017), NRT (Li et al. 2017) and PETER (Li, Zhang, and Chen 2021) as baselines. For review summarization, we adopt pretrained T0 (Sanh et al. 2021) and GPT-2 (Radford et al. 2019) as baselines.\n\n# Experimental Setup\n\nDatasets and Metrics. Our study involves conducting both quantitative and qualitative evaluations on the realworld Amazon recommendation dataset. This dataset comprises customer review text and associated metadata for products across 29 categories. This paper focuses on evaluating the Beauty category. For rating prediction, we employ Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). For sequential recommendation and direct recommendation, we adopt topk Hit Ratio (HR@ k), topk  Normalized Discounted Cumulative Gain (NDCG@ k) which are widely used in related works (Geng et al. 2022; Zhou et al. 2020). For explanation generation and review summarization, n-gram Bilingual Evaluation Understudy (BLEUn) and n-gram RecallRoiented Understudy for Gising Evaluation (ROUGEn) are used for evaluation.\n\nImplementation Details. For off-the-shelf settings, we utilize the gpt-3.5-turbo version of ChatGPT. The rest of the experimental settings remain consistent with (Liu et al. 2023b). For the settings of supervised finetuning, we apply P-tuning V2 (Liu et al. 2022) for ChatGLM-6B and LoRa (Hu et al. 2021) for LLaMa-7B as well as Alpaca for efficiently model training. Specifically, we set the prefix sequence length to 128 for P-tuning V2. For LoRa, we set the rank and alpha to 8 and 16, respectively. We train all compared LLMs with batch size of 8 for 10 Epochs across all five tasks. The learning rate was set to 2e-2 for ChatGLM6B and 3e-4 for LLaMa and Alpaca. In order to trade off the model performance and time consumed, we select one type of prompt template from P5 (Geng et al. 2022)\u2019s training data for each task, and evaluate them using the same prompt template. When evaluating models on sequential recommendation and direct recommendation, we perform beam search with beam size 20 to calculate the HR@ k and NDCG@ k\n\nTable 1: Performance comparison on explanation generation (%).\n\n<div style=\"text-align: center;\">Table 1: Performance comparison on explanation generation (%).\n</div>\nMethods\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nAttn2Seq\n0.7889\n12.6590\n1.6820\n9.7481\nNRT\n0.8295\n12.7815\n1.8543\n9.9477\nPETER\n1.1541\n14.8497\n2.1413\n11.4143\nP5-B\n0.9742\n16.4530\n1.8858\n11.8765\nPETER+\n3.2606\n25.5541\n5.9668\n19.7168\nChatGPT\n0.1266\n6.4060\n0.3347\n5.1458\nChatGLM w/o SFT\n0.1318\n7.8846\n0.4242\n4.2388\nLLaMA w/o SFT\n0.0564\n1.0847\n0.1075\n0.6900\nAlpaca w/o SFT\n0.2002\n5.5061\n0.4328\n3.3958\nTable 2: Performance comparison on review summarization (%).\n\nMethods\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nT0\n1.2871\n1.2750\n0.3904\n0.9592\nGPT-2\n0.5879\n3.3844\n0.6756\n1.3956\nP5-B\n2.1225\n8.4205\n1.6676\n7.5476\nChatGPT\n0.3121\n4.7177\n0.6924\n4.2557\nChatGLM w/o SFT\n0.2472\n4.3544\n0.6306\n3.2348\nLLaMA w/o SFT\n0.4306\n3.9954\n0.8280\n1.9456\nAlpaca w/o SFT\n0.2020\n2.9902\n0.4285\n2.2188\nscores.\n\n# Off-the-shelf Results\n\nIn this study, we conducted an evaluation of commonly used off-the-shelf models including ChatGPT, ChatGLM, LLaMA, and Alpaca as Recommender System in LLMRec for both accuracy-based and explainability-based tasks. Our findings suggest that LLMs generally underperformed in accuracy-based tasks. Specifically, in rating prediction task, ChatGPT demonstrated RMSE and MAE scores of 1.4913 and 1.2836, respectively, as depicted in Table 3. For sequential and direct recommendation tasks, off-the-shelf ChatGPT showed a significant gap when compared to trained models such as P5 due to the lack of learning on product information and user interests, as illustrated in Table 4 and Table 5. Furthermore, ChatGLM, LLaMA, and Alpaca were unable to produce standard results following the prompt requirement, hence, were not directly applicable to accuracy-based tasks.\nIn the context of explainability-based tasks, it is evident that off-the-shelf models are progressively closing the gap with trained models and, in some cases, superseding base\n\n<div style=\"text-align: center;\">Table 3: Performance comparison on rating prediction.\n</div>\nMethods\nRMSE\nMAE\nMF\n1.1973\n0.9461\nMLP\n1.3078\n0.9597\nChatGPT\n1.4913\n1.2836\nChatGLM w/o SFT\nN/A\nN/A\nLLaMA w/o SFT\nN/A\nN/A\nAlpaca w/o SFT\nN/A\nN/A\nTable 4: Performance comparison on sequential recommendation.\n\n<div style=\"text-align: center;\">Table 4: Performance comparison on sequential recommendation.\n</div>\nMethods\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nCaser\n0.0205\n0.0131\n0.0347\n0.0176\nHGN\n0.0325\n0.0206\n0.0512\n0.0266\nGRU4Rec\n0.0164\n0.0099\n0.0283\n0.0137\nBERT4Rec\n0.0203\n0.0124\n0.0347\n0.0170\nFDSA\n0.0267\n0.0163\n0.0407\n0.0208\nSASRec\n0.0387\n0.0249\n0.0605\n0.0318\nS3-Rec\n0.0387\n0.0244\n0.0647\n0.0327\nP5-B\n0.0493\n0.0367\n0.0645\n0.0416\nChatGPT\n0.0012\n0.0020\n0.0008\n0.0011\nChatGLM w/o SFT\nN/A\nN/A\nN/A\nN/A\nLLaMA w/o SFT\nN/A\nN/A\nN/A\nN/A\nAlpaca w/o SFT\nN/A\nN/A\nN/A\nN/A\nTable 5: Performance comparison on direct recommendation.\n\nMethods\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nBPR-MF\n0.1426\n0.0857\n0.2573\n0.1224\nBPR-MLP\n0.1392\n0.0848\n0.2542\n0.1215\nSimpleX\n0.2247\n0.1441\n0.3090\n0.1711\nP5-B\n0.1564\n0.1096\n0.2300\n0.1332\nChatGPT\n0.0217\n0.0111\n0.0652\n0.0252\nChatGLM w/o SFT\nN/A\nN/A\nN/A\nN/A\nLLaMA w/o SFT\nN/A\nN/A\nN/A\nN/A\nAlpaca w/o SFT\nN/A\nN/A\nN/A\nN/A\nline algorithms based on certain metrics, as clearly demonstrated in Table 1 and Table 2. For instance, in review summarization task, ChatGPT achieves 4.7177, 0.6924, and 4.2557 on ROUGE1, ROUGE2, and ROUGEL metrics, respectively, exceeding the performance of T0 and GPT-2 trained on Beauty dataset. This suggests that it is possible to gain an insightful understanding of users\u2019 interest with current items, even without acquiring their historical interaction information. We also present samples generated by different models in supplementary materials, showing that ChatGPT\u2019s explanations are clearer and more rational, while P5 tends to produce generic-style explanations and performs the worst. The same conclusion can be drawn for review summarization task, where ChatGPT is able to comprehend the reviews and generate precise summaries, rather than simply extracting a few keywords.\n\n# Fine-tuned Results\n\nFrom the section of off-the-shelf results, we could see that directly applying LLMs (e.g., ChatGPT) in recommendation tasks results in inferior performance compared to baseline methods in certain recommendation tasks. In order to further explore the potential of utilizing LLMs in recommendation tasks, we conduct Supervised finetuning (SFT) on LLMs and test their performances on five recommendation tasks. Specifically, we construct prompts to test the original LLM\u2019s performance and use the training and testing datasets proposed by P5 (Geng et al. 2022) for SFT setting. Table 6 and Table 7 show the results of different LLMs under different settings in all five different recommendation\n\nReview\nI truly love this soap. I have\nvery sensitive skin and this is\none\nof\nthe\nfew\nsoaps\nthat\ndoesn't dry out or break out my\nskin.\nWould\nrecommend\nto\nothers. It smells soft as well\nThese brushes are okay. I don't\nthink they're anything special,\nbut for the price their quality is\nokay. I don't know that I would\nbuy them again though.\nGreat\nto\nuse\nafter\nthe\nmicrodermabrasion\nroller\nneedle\nprocess. Skin absorbs it quickly. Face\nlooks\nmore\nrejuvenated\nin\nthe\nmorning.\nGround \nTruth\nSoap\nIt's okay\nWorks Well\nP5\nGreat soap\nOkay\nGreat\nChatGPT\nGentle,\neffective\nsoap\nrecommended.\nAverage brushes for price.\nQuickly absorbed rejuvenating serum.\nChatGLM\nLove this soap, sensitive skin\nno problem\nThese brushes are okay, but\nnot special.\nRenowned for its skin rejuvenation\nbenefits, this product is a must-have\nfor those with acne-prone skin.\nLLaMA\nI truly love this soap. I have\nvery sensitive skin and this is\none\nof\nthe\nfew\nsoaps\nthat\ndoesn\u2018t dry out or break out my\nskin.\nWould\nrecommend\nto\nothers. It smells soft as well.\nThese brushes are okay. I don't\nthink they're anything special,\nbut for the price their quality is\nokay. I don't know that I would\nbuy them again though.\nGreat\nto\nuse\nafter\nthe\nmicrodermabrasion\nroller\nneedle\nprocess. Skin absorbs it quickly. Face\nlooks\nmore\nrejuvenated\nin\nthe\nmorning.\nAlpaca\nLove it!\nOkay quality for the price.\nFace looks more rejuvenated in the\nmorning.\nChatGLM \nw/ SFT\nSmells like silk\nNot worth the price\nGreat for microdermabrasion\nFigure 3: Example summarization results of different LLMs.\n\n<div style=\"text-align: center;\">Figure 3: Example summarization results of different LLMs\n</div>\ntasks, namely rating prediction, sequential recommendation, direct recommendation, explanation generation and review summarization. From the results we could have following observations:\n\nSFT help LLMs align better with recommendation instructions  For those tasks that require formatted output (e.g., rating prediction), we found that LLMs such as ChatGLM exhibits poor capability in instruction compliance compared to ChatGPT, which results in extremely unstructured outputs, leading to inferior performance or even incapability. Hence it\u2019s infesible to directly adopt these kinds of LLMs for solving tasks that need formatted outputs. After apply Supervised Fine Tuning, these models\u2019 outputs could be restricted to the desired format, and achieve competitive performances compared to the baseline methods. Besides, we demonstrated that LLMs are naturaly suitable for those tasks that expect natural language outputs (e.g.,  explanation generation). Similar to the observations on ChatGPT, LLMs such as ChatGLM could generate fluent and reasonable results using prompts, but achieves poor scores on automatic metrics. When apply further Supervised Fine Tuning on these LLMs, we found that the LLMs output could be further aligned with the recommendation tasks\u2019 requirements\n\nand the scores of the corresponding automatic metrics are significantly improved, e.g.,  the ROUGEL score in explanation generation task improves from 4.2388 to 9.2806.\n\nComparision with P5 Although SFT makes LLMs such as ChatGLM capable to handle recommendation tasks like rating prediction or sequential recommendation, their performances are still far from stastified when compared with previous methods like P5 (Geng et al. 2022). For example, P5-B get 0.0645 on HR@10 score but ChatGLM-6B only achieves 0.0455 after SFT. We attribute this results to the following reasons: First, the compared LLM methods have fewer amount of fine-tuning parameters. Take ChatGLM-6B as an example, we use P-tuning V2 (Liu et al. 2022) method to finetune the model, leading to 13% of total trainable parameters compared to P5-B, which greatly limits the model\u2019s potential to bridge the gap between pretrained NLP tasks and finetuned recommendation tasks. Second, the amount of training data. Due to the limited calculating resouce, we could only finetune all compared LLM methods using one type of prompt for 10 Epochs for each kind of recommendation task, while P5 adopt a much larger training dataset that upsamples and merges all five task together using multiple kinds of prompt, resulting in about 50 times more train\n\nmance comparison on rating prediction, sequential recommendation and direct recommenda\n\nMethods\nRating\nSequential\nDirect\nRMSE\nMAE\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nHR@5\nNDCG@5\nHR@10\nNDCG@10\nChatGLM-6B w/ SFT\n1.2912\n0.8945\n0.0403\n0.0349\n0.0455\n0.0366\n0.0157\n0.0105\n0.0205\n0.0121\nLLaMa-7B w/ SFT\n1.4890\n1.0330\n0.0277\n0.0280\n0.0280\n0.0277\n0.0166\n0.0205\n0.0153\n0.0165\nAlpaca w/ SFT\n1.3734\n0.9048\n0.0325\n0.0330\n0.0325\n0.0327\n0.0165\n0.0260\n0.0156\n0.0186\nMethods\nExplanation\nReview\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nBLUE4\nROUGE1\nROUGE2\nROUGEL\nChatGLM-6B w/ SFT\n0.6766\n12.5927\n1.5564\n9.2806\n2.0254\n8.1003\n2.1351\n7.2126\nLLaMa-7B w/ SFT\n0.3006\n11.3522\n1.0774\n5.1382\n0.3464\n5.7327\n1.1382\n2.7915\nAlpaca w/ SFT\n0.3034\n11.7312\n1.1018\n5.3354\n0.2752\n5.1295\n0.8269\n2.4536\ning data. Third, the diversity in training data. As aforementioned, there are different types of task samples using multiple kinds of prompt in P5\u2019s training data, which greatly improves the diversity in training data and further strengthen the generization ability of the corresponding models. The multi-task training strategy in P5\u2019s setting could also improves the model\u2019s performance to some extent, which is also demonstrated in (Ruder 2017). Comparison with ChatGPT The results demonstrate that ChatGPT excels at explanation generation, and review summarization, compared with previous state-of-theart methods. According to results above, ChatGPT only outperforms ChatGLM-6B in the direct recommendation task, but falls behind in the rating prediction, explanation generation, and review summarization tasks. In addition, ChatGLM-6B\u2019s automatic metrics are all higher than those of ChatGPT. Therefore, LLM within SFT, such as ChatGLM-6B, is injected with more user behavior data, product information data, and user profile data, allowing it to deeply characterize users, strictly follow prompt instructions, personalize recommendations, and suggest more suitable items, ultimately resulting in optimal recommendation outcomes.\n\n# Qualitative Evaluation\n\nIn order to analyze the performance of LLMs in explainability-based tasks, we present the results of several models on review summarization tasks and explanation generation. Due to limited space, we only report the summarization results and readers can check explanation generation results in Appendix.3. As shown in Fig.3, while P5\u2019s summarization outcome has extracted some keywords, it has also disregarded crucial information that exists throughout the review. In contrast, large language models such as ChatGPT, ChatGLM, and Alpaca can generate more effective and meaningful summaries by comprehensively understanding and summarizing the reviews. However, LLaMA only duplicates the review and does not fulfill the task of summarization.\n\n# Limitations\n\nWe acknowledge several limitations in this study. Firstly, we have only evaluated specific Large Language Models (LLMs) such as ChatGPT, ChatGLM, LLaMA, and Alpaca. The findings may not be applicable to other models or future versions of these models. To address this, we plan to evaluate the recommendation performance of a broader range of LLMs in the future. Additionally, due to the expensive computational resources required for assessing LLM performance, we conducted experiments solely on the widely used Amazon Beauty dataset. However, due to different characteristics among datasets, we cannot guarantee that our findings will hold on other datasets. To mitigate this issue, we intend to conduct evaluations on more diverse datasets in the future. Furthermore, the study highlights the disagreement between objective and subjective evaluation results. In the future, we will explore objective metrics that align more closely with subjective evaluations.\n\n# Conclusion\n\nIn this work, we present LLMRec, a general-purpose recommender system powered by LLMs. Using this system, we benchmark the performance of various LLMs in both offthe-shelf and supervised fine-tuning settings across five classical recommendation tasks. The benchmark results demonstrate that existing LLMs perform well in rating prediction tasks but show poor performance in sequential and direct recommendation tasks. This highlights the need for further exploration and enhancement in these areas. Notably, for explanable recommendation tasks, ChatGPT surpasses state-of-the-art methods in qualitative evaluations, indicating its potential in generating explanations and summaries for recommendation results. Our study contributes valuable insights into the strengths and limitations of LLMs in recommender systems. We hope our study can facilitate and encourage the development of more effective approaches for integrating LLMs into recommendation scenarios, bridging the semantic gap between language and user interests.\n\n# References\n\nAharoni, R.; Johnson, M.; and Firat, O. 2019. Massively multilingual neural machine translation. arXiv preprint arXiv:1903.00089.\nBang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie, B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.\nBengio, Y.; Ducharme, R.; Vincent, P.; Jauvin, C.; and Shawe-Taylor, J. 2003. Journal of Machine Learning Research 3 (2003) 1137\u20131155 Submitted 4/02; Published 2/03 A Neural Probabilistic Language Model. JMLR.org, (6).\nChen, M. X.; Firat, O.; Bapna, A.; Johnson, M.; Macherey, W.; Foster, G.; Jones, L.; Parmar, N.; Schuster, M.; Chen, Z.; et al. 2018. The best of both worlds: Combining recent advances in neural machine translation. arXiv preprint arXiv:1804.09849.\nCheng, H.-T.; Koc, L.; Harmsen, J.; Shaked, T.; Chandra, T.; Aradhye, H.; Anderson, G.; Corrado, G.; Chai, W.; Ispir, M.; et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, 7\u201310.\nCui, Z.; Ma, J.; Zhou, C.; Zhou, J.; and Yang, H. 2022. M6-Rec: Generative Pretrained Language Models are OpenEnded Recommender Systems. CoRR, abs/2205.08084.\nDai, H.; Liu, Z.; Liao, W.; Huang, X.; Wu, Z.; Zhao, L.; Liu, W.; Liu, N.; Li, S.; Zhu, D.; et al. 2023. Chataug: Leveraging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (1), 4171\u20134186. Association for Computational Linguistics.\nDhingra, B.; Li, L.; Li, X.; Gao, J.; and Li, D. 2016. Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access.\nDong, L.; Huang, S.; Wei, F.; Lapata, M.; Zhou, M.; and Xu, K. 2017. Learning to generate product reviews from attributes. In  Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, 623\u2013632.\nDu, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2021. GLM: General Language Model Pretraining with Autoregressive Blank Infilling.\nDu, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 320\u2013335.\nGao, Y.; Sheng, T.; Xiang, Y.; Xiong, Y.; Wang, H.; and Zhang, J. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. arXiv preprint arXiv:2303.14524.\n\nGeng, S.; Liu, S.; Fu, Z.; Ge, Y.; and Zhang, Y. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In  Proceedings of the 16th ACM Conference on Recommender Systems, 299\u2013315. Hidasi, B.; Karatzoglou, A.; Baltrunas, L.; and Tikk, D. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939. Hochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term Memory. Neural Computation, 9(8): 1735\u20131780. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Jiao, W.; Wang, W.; Huang, J.-t.; Wang, X.; and Tu, Z. 2023. Is ChatGPT a good translator? A preliminary study. arXiv preprint arXiv:2301.08745. Kang, W.-C.; and McAuley, J. 2018. Self-attentive sequential recommendation. In  2018 IEEE international conference on data mining (ICDM), 197\u2013206. IEEE. Koren, Y.; Bell, R.; and Volinsky, C. 2009. Matrix factorization techniques for recommender systems. Computer, 42(8): 30\u201337. Kowald, D.; Schedl, M.; and Lex, E. 2020. The unfairness of popularity bias in music recommendation: A reproducibility study. In Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14\u201317, 2020, Proceedings, Part II 42, 35\u201342. Springer. Li, J.; Galley, M.; Brockett, C.; Spithourakis, G. P.; Gao, J.; and Dolan, B. 2016. A Persona-Based Neural Conversation Model. arXiv e-prints. Li, L.; Zhang, Y.; and Chen, L. 2021. Personalized transformer for explainable recommendation. arXiv preprint arXiv:2105.11601. Li, P.; Wang, Z.; Ren, Z.; Bing, L.; and Lam, W. 2017. Neural rating regression with abstractive tips generation for recommendation. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, 345\u2013354. Li, Y.; and Zhang, Y. 2023. Fairness of ChatGPT. arXiv preprint arXiv:2305.18569. Liu, G. 2022. An ecommerce recommendation algorithm based on link prediction. Alexandria Engineering Journal, 61(1): 905\u2013910. Liu, H.; Ning, R.; Teng, Z.; Liu, J.; Zhou, Q.; and Zhang, Y. 2023a. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439. Liu, J.; Liu, C.; Zhou, P.; Lv, R.; Zhou, K.; and Zhang, Y. 2023b. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv:2304.10149. Liu, X.; Ji, K.; Fu, Y.; Tam, W. L.; Du, Z.; Yang, Z.; and Tang, J. 2022. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. arXiv:2110.07602. Liu, Y. 2019. Fine-tune BERT for Extractive Summarization.\n\nGeng, S.; Liu, S.; Fu, Z.; Ge, Y.; and Zhang, Y. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In  Proceedings of the 16th ACM Conference on Recommender Systems, 299\u2013315. Hidasi, B.; Karatzoglou, A.; Baltrunas, L.; and Tikk, D. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939. Hochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term Memory. Neural Computation, 9(8): 1735\u20131780. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Jiao, W.; Wang, W.; Huang, J.-t.; Wang, X.; and Tu, Z. 2023. Is ChatGPT a good translator? A preliminary study. arXiv preprint arXiv:2301.08745. Kang, W.-C.; and McAuley, J. 2018. Self-attentive sequential recommendation. In  2018 IEEE international conference on data mining (ICDM), 197\u2013206. IEEE. Koren, Y.; Bell, R.; and Volinsky, C. 2009. Matrix factorization techniques for recommender systems. Computer, 42(8): 30\u201337. Kowald, D.; Schedl, M.; and Lex, E. 2020. The unfairness of popularity bias in music recommendation: A reproducibility study. In Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14\u201317, 2020, Proceedings, Part II 42, 35\u201342. Springer. Li, J.; Galley, M.; Brockett, C.; Spithourakis, G. P.; Gao, J.; and Dolan, B. 2016. A Persona-Based Neural Conversation Model. arXiv e-prints. Li, L.; Zhang, Y.; and Chen, L. 2021. Personalized transformer for explainable recommendation. arXiv preprint arXiv:2105.11601. Li, P.; Wang, Z.; Ren, Z.; Bing, L.; and Lam, W. 2017. Neural rating regression with abstractive tips generation for recommendation. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, 345\u2013354. Li, Y.; and Zhang, Y. 2023. Fairness of ChatGPT. arXiv preprint arXiv:2305.18569. Liu, G. 2022. An ecommerce recommendation algorithm based on link prediction. Alexandria Engineering Journal, 61(1): 905\u2013910. Liu, H.; Ning, R.; Teng, Z.; Liu, J.; Zhou, Q.; and Zhang, Y. 2023a. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439. Liu, J.; Liu, C.; Zhou, P.; Lv, R.; Zhou, K.; and Zhang, Y. 2023b. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv:2304.10149. Liu, X.; Ji, K.; Fu, Y.; Tam, W. L.; Du, Z.; Yang, Z.; and Tang, J. 2022. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. arXiv:2110.07602. Liu, Y. 2019. Fine-tune BERT for Extractive Summarization.\n\nMa, C.; Kang, P.; and Liu, X. 2019. Hierarchical gating networks for sequential recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 825\u2013833. Mao, K.; Zhu, J.; Wang, J.; Dai, Q.; Dong, Z.; Xiao, X.; and He, X. 2021. SimpleX: A simple and strong baseline for collaborative filtering. In  Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 1243\u20131252. OpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774. Papadamou, K.; Zannettou, S.; Blackburn, J.; De Cristofaro, E.; Stringhini, G.; and Sirivianos, M. 2022. \u201cIt is just a flu\u201d: Assessing the Effect of Watch History on YouTube\u2019s Pseudoscientific Video Recommendations. In Proceedings of the international AAAI conference on web and social media, volume 16, 723\u2013734. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. 2018. Improving language understanding by generative pre-training. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020a. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1): 5485\u20135551. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020b. Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer. J. Mach. Learn. Res., 21: 140:1\u2013140:67. Rendle, S.; Freudenthaler, C.; Gantner, Z.; and SchmidtThieme, L. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618. Ruder, S. 2017. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098. Sanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.; Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.; et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207. See, A.; Liu, P. J.; and Manning, C. D. 2017. Get To The Point: Summarization with Pointer-Generator Networks. Singh, J.; Sajid, M.; Yadav, C. S.; Singh, S. S.; and Saini, M. 2022. A Novel Deep Neural-based Music Recommendation Method considering User and Song Data. In 2022 6th International Conference on Trends in Electronics and Informatics (ICOEI), 1\u20137. IEEE. Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In  Proceedings of the 28th ACM international conference on information and knowledge management, 1441\u20131450. Sun, Z.; Yang, J.; Feng, K.; Fang, H.; Qu, X.; and Ong, Y. S. 2022. Revisiting Bundle Recommendation: Datasets,\n\nMa, C.; Kang, P.; and Liu, X. 2019. Hierarchical gating networks for sequential recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 825\u2013833. Mao, K.; Zhu, J.; Wang, J.; Dai, Q.; Dong, Z.; Xiao, X.; and He, X. 2021. SimpleX: A simple and strong baseline for collaborative filtering. In  Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 1243\u20131252. OpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774. Papadamou, K.; Zannettou, S.; Blackburn, J.; De Cristofaro, E.; Stringhini, G.; and Sirivianos, M. 2022. \u201cIt is just a flu\u201d: Assessing the Effect of Watch History on YouTube\u2019s Pseudoscientific Video Recommendations. In Proceedings of the international AAAI conference on web and social media, volume 16, 723\u2013734. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. 2018. Improving language understanding by generative pre-training. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020a. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1): 5485\u20135551. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020b. Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer. J. Mach. Learn. Res., 21: 140:1\u2013140:67. Rendle, S.; Freudenthaler, C.; Gantner, Z.; and SchmidtThieme, L. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618. Ruder, S. 2017. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098. Sanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.; Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.; et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207. See, A.; Liu, P. J.; and Manning, C. D. 2017. Get To The Point: Summarization with Pointer-Generator Networks. Singh, J.; Sajid, M.; Yadav, C. S.; Singh, S. S.; and Saini, M. 2022. A Novel Deep Neural-based Music Recommendation Method considering User and Song Data. In 2022 6th International Conference on Trends in Electronics and Informatics (ICOEI), 1\u20137. IEEE. Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In  Proceedings of the 28th ACM international conference on information and knowledge management, 1441\u20131450. Sun, Z.; Yang, J.; Feng, K.; Fang, H.; Qu, X.; and Ong, Y. S. 2022. Revisiting Bundle Recommendation: Datasets,\n\nTasks, Challenges and Opportunities for Intent-aware Product Bundling. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2900\u20132911. Tang, J.; and Wang, K. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the eleventh ACM international conference on web search and data mining, 565\u2013573. Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https: //github.com/tatsu-lab/stanford alpaca. Tsagkias, M.; King, T. H.; Kallumadi, S.; Murdock, V.; and de Rijke, M. 2021. Challenges and research opportunities in ecommerce search and recommendations. In ACM Sigir Forum, volume 54, 1\u201323. ACM New York, NY, USA. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention Is All You Need. arXiv. Wei, Y.; Wang, X.; Nie, L.; He, X.; Hong, R.; and Chua, T.-S. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia, 1437\u20131445. Wu, C.; Wu, F.; An, M.; Huang, J.; Huang, Y.; and Xie, X. 2019. NPA: neural news recommendation with personalized attention. In  Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2576\u20132584. Wu, C.; Wu, F.; Qi, T.; Liu, Q.; Tian, X.; Li, J.; He, W.; Huang, Y.; and Xie, X. 2022. Feedrec: News feed recommendation with various user feedbacks. In Proceedings of the ACM Web Conference 2022, 2088\u20132097. Wu, F.; Qiao, Y.; Chen, J.-H.; Wu, C.; Qi, T.; Lian, J.; Liu, D.; Xie, X.; Gao, J.; Wu, W.; et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3597\u20133606. Xie, Y.; Zhou, P.; and Kim, S. 2022. Decoupled side information fusion for sequential recommendation. In  Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1611\u2013 1621. Zeng, Q.; Garay, L.; Zhou, P.; Chong, D.; Hua, Y.; Wu, J.; Pan, Y.; Zhou, H.; and Yang, J. 2022. GreenPLM: Crosslingual pre-trained language models conversion with (almost) no cost. arXiv preprint arXiv:2211.06993. Zhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li, H.; Gao, P.; and Qiao, Y. 2023. Llama-adapter: Efficient finetuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199. Zhang, T.; Zhao, P.; Liu, Y.; Sheng, V. S.; Xu, J.; Wang, D.; Liu, G.; Zhou, X.; et al. 2019. Feature-level Deeper SelfAttention Network for Sequential Recommendation. In  IJCAI, 4320\u20134326.\n\nZhang, Y.; DING, H.; Shui, Z.; Ma, Y.; Zou, J.; Deoras, A.; and Wang, H. 2021. Language Models as Recommender Systems: Evaluations and Limitations. In  I (Still) Can\u2019t Believe It\u2019s Not Better! NeurIPS 2021 Workshop. Zhao, Z.; Hong, L.; Wei, L.; Chen, J.; Nath, A.; Andrews, S.; Kumthekar, A.; Sathiamoorthy, M.; Yi, X.; and Chi, E. 2019. Recommending what video to watch next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender Systems, 43\u201351. Zhou, K.; Wang, H.; Zhao, W. X.; Zhu, Y.; Wang, S.; Zhang, F.; Wang, Z.; and Wen, J.-R. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In  Proceedings of the 29th ACM international conference on information & knowledge management, 1893\u20131902. Zhuo, T. Y.; Huang, Y.; Chen, C.; and Xing, Z. 2023. Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867.\n\n# Appendix.1 Dataset Statistics and Splits\n\nBasic Statistics.  We conducted numerical and human evaluations on the real-world Amazon recommendation dataset, specifically focusing on the Beauty category. This dataset exhibits the following basic statistics: there are 22,363 users, 12,101 items, 198,502 reviews, and 198,502 user-item actions. On average, each user performs 8.9 actions, while each item receives an average of 16.4 actions. The sparsity of this dataset is 99.93%. Dataset Splits. Following P5 (Geng et al. 2022), we adopt different data partitioning strategies for different tasks. Specifically, for rating, explanation, and review tasks, we divide the Amazon Beauty dataset into training, validation, and testing sets using a ratio of 8:1:1. We ensure that each user and item has at least one instance included in the training set. To obtain ground-truth explanations, Sentires toolkit 1 is first utilized to extract item feature words from the reviews and then sentences that comment on one or multiple item features are regarded as explanations of user preferences. For the sequential recommendation task, we employ a leave-one-out strategy to split the dataset: for each interaction sequence, the last item is treated as the test data, the item before the last one as the validation data, and the remaining data for training. In the direct recommendation task, the training set is consistent with the training split of the sequential recommendation task to avoid data leakage issues during pretraining.\n\n# Appendix.2 Implementation Details\n\nThe configuration and hyper-parameters of the LLMs used in our work have been presented in the Sec 4.2 of our manuscript. To ensure a fair comparison with P5, we maintain consistency with their dataset splits for each task. Consequently, in the main table of the manuscript, we directly inherit the performance results of the baseline and P5 reported in (Geng et al. 2022). As for the evaluation of the finetuned LLM models, we run experiments multiple rounds to report the mean and standard deviation to ensure the reliability of the experimental results. For reproducibility, we provide the following information on the hardware used for training and testing the LLMs:\n\u2022 CPU: AMD EPYC 7713 \u2022 GPU: NVIDIA A100 SXM4 80GB \u2022 RAM: 1024GB DDR4 ECC\nAll the experiments were conducted in the following environment:\n\n\u2022 Libraries and Frameworks: Transformers 4.30.0, PEFT 0.4.0.dev0, PyTorch 2.0.1\n\nPlease ensure that the exact versions of these libraries and frameworks are installed to guarantee reproducibility. The full list of dependencies can be found in the requirements.txt file located in our code appendix.\n\n1 https://github.com/evison/Sentires\n\n# Appendix.3 In-depth Analysis\n\nCase study of N/A results In Table 3-5 of the manuscript, we denote the performance of off-the-shelf LLM models including ChatGLM, LLAMA, and Alpaca, as \u201dN/A\u201d in rating prediction, sequential recommendation, and direct recommendation tasks. The main reason for this is that, as shown in Supplement Fig 4, the output of LLAMA and Alpaca cannot be parsed into scores or item title using pre-defined rules. While ChatGLM is capable of generating some seemingly reasonable results in sequential recommendation tasks, the generated titles are mostly either present in the prompts or fake ones that do not exist in the dataset, leading to very low accuracy metrics. Therefore, we also mark it as N/A. To summarize, these \u201dN/A\u201d results indicate that LLM models with smaller parameter sizes are unable to complete accuracy-driven recommendation tasks without fine-tuning on recommendation datasets.\n\nCase study of explanation and summarization results In order to analyze the performance of LLMs in explainability-based tasks, we present the results of several models on explanation generation and review summarization tasks. P5 and general LLMs have distinct design goals and diverse applications as language models. P5 aims to generate explanatory language that resembles known texts and therefore places emphasis on learning text structure and grammar rules during training, resulting in more standardized generated output, as illustrated in Supplement Fig.5. In contrast, ChatGPT prioritizes language interaction and diversity. Its principal application is to simulate human conversation, and thus it considers multiple factors, such as context, emotion, and logic, to better express human thinking and language habits. This design approach leads to more diverse and creative text generated by ChatGPT. ChatGLM is also capable of achieving commendable results. However, LLaMA is prone to more noticeable issues with generating duplicates, while some explanations generated by Alpaca may be incomplete. With the application of Parameter Efficient Fine Tuning, it can be observed that ChatGLM is capable of achieving outcomes that are more closely aligned with the ground truth. Similar to the explanation generation task, while P5\u2019s summarization outcome has extracted some keywords, it has also disregarded crucial information that exists throughout the review. In contrast, language models such as ChatGPT, ChatGLM, and Alpaca can generate more effective and meaningful summaries by comprehensively understanding and summarizing the reviews, as illustrated in Manuscript Fig.3. However, LLaMA only duplicates the review and does not fulfill the task of summarization.\n\nTask\nRating Prediction \nSequential Recommendation \nDirect Recommendation \nReview \nHow\nwill\nuser\nrate\nthis \nproduct_title: \u201cSHANY Nail Art\nSet (24 Famous Colors Nail Art\nPolish, Nail Art Decoration)\u201d, \nand product_category: Beauty? (\n1 being lowest and 5 being\nhighest ) Attention! Just give me\nback the exact number a result, \nand you don\u2018t need a lot of text.\nRequirements: you must choose\n10 items for recommendation\nand sort them in order of priority,\nfrom highest to lowest. Output\nformat: a python list. Do not\nexplain the reason or include any\nother\nwords.\n<prompt_begin> \nThe user has interacted with the\nfollowing items in chronological\norder: ['Clairol Shimmer Lights\nShampoo Blonde &amp; Silver\n31.5oz',\n'Neutrogena\nTriple\nMoisture\nDaily\nDeep\nConditioner, 8.5 Ounce', 'Cricket\nTechnique Barrel Hair Brush,\nXX-large\nRound,\n2\nInch',\n\"O'Keeffe's\nWorking\nHands\nCream,\n3.4\noz.\"].\nPlease\nrecommend the next item that\nthe\nuser\nmight\ninteract\nwith.<prompt_end>\nRequirements: you must choose 10\nitems for recommendation and sort\nthem in order of priority, from\nhighest to lowest. Output format: a\npython list. Do not explain the\nreason or include any other words.\n<prompt_begin>The\nuser\nhas\ninteracted with the following items\n(in\nno\nparticular\norder):\n['23&quot;clip in Hot Synthetic \u2026\nFish\nTail\nClip'].\nFrom\nthe\ncandidates listed below, choose the\ntop 10 items to recommend to the\nuser and rank them in order of\npriority from highest to lowest.\nCandidates:\n['elf\nStudio\nHigh\nDefinition Loose Face Powder\u2019,\u2026\n'Garnier Skincare Moisture Rescue\nRefreshing Gel-Cream for\n1.35\nOunce']. <prompt_end>\nGround \nTruth\n5\nMoroccanOil Hydrating Styling\nCream, 10.2-Ounce Bottle\nelf Studio High Definition Loose\nFace Powder\nGPT-3.5\n5\nTangle\nTeezer\nThe\nUltimate\nDetangler Hair Brush\nelf Studio High Definition Loose\nFace Powder\nChatGLM\nI'm sorry, but I am not able to\npredict the star rating that Terri\nwill give to item_3815. It is not\npossible for me to determine the\nstar\nrating\nbased\non\nthe\ninformation provided.\nHere is a list of 10 recommended\nitems\nbased\non\nthe\nuser\u2018s\ninteractions:  \u2019Clairol Shimmer\nLights Shampoo Blonde & Silver\n31.5oz\u2018,  \n\u2019Neutrogena\nTriple\nMoisture\n \nDaily\n \nDeep \nConditioner, 8.5 Ounce\nThe items are ranked in the order\nthey\nwere\nrecommended,\nfrom\nhighest to lowest priority, as they\nare all hair-related products. The\nfirst three items are highly prioritize\nbecause they are all designed to\nhelp\nwith\nhair\ngrowth\nand/or\nmaintenance.\nLLaMA\nThis is great because it's great\nfor small children to make their\nown designs with and it's only\n$3.00! It's great for me, because\nI'm a girl and I like to make my\nown designs. I think it would be\nbetter if it came with a sponge\napplicator, so it's easier to apply\nthe polish. I would buy this\nagain.\u201d\nWhich\nitem\nshould\nthe\nuser\ninteract with next?\nto have a lower number than that of\nthe highest recommended item.\nWe use .values() to retrieve the lists\nin the dictionary.\nAlpaca\n\"20)))\"\nYou\nsaid\n\"Cricket\nTechnique\nBarrel H\nThis is the fourth Winter Classic in\nthe NHL and the first one to feature\nthe Toronto Maple Leafs since the\nleague\nannounced\nit\nwould\nbe\nplaying\nagainst\nthe\nChicago\nBlackhawks at Citi Field on\nFigure 4: Case study of N/A results.\n\nItem Title\nSHANY Nail Art Set (24\nFamouse\nColors\nNail\nArt\nPolish, Nail Art Decoration)\nSHANY\nNail\nArt\nSet\n(24\nFamouse Colors Nail Art Polish,\nNail Art Decoration)\nSHANY Nail Art Set (24 Famouse\nColors\nNail\nArt\nPolish,\nNail\nArt\nDecoration)\nReview\nAbsolutely great product.\nI\nbought this for my fourteen\nyear old niece for Christmas\nand of course I had to try it\nout, then I tried another one,\nand another one and another\none.\nSo much fun!\nI even\ncontemplated keeping a few\nfor myself!\nLove the colors. Didn't get any\ndoubles. 1 bottle was not fully\nclosed and the bottle chipped on\nthe neck of the bottle. But being\nwhere the break was I just closed\nit and it is still usable. I wouldn't\nrecommend this for painting your\nfull nail (It is for art), but I would\nfor stamping and nail art. Small\nbrushes great for that. Not all\nwork for stamping though, like\nthe metallic ones.\nWow, this is the best deal I've seen on\nnail polish in a long time. You get so\nmany\nvibrant\nbeautiful\ncolors\nto\nchoose from. These are nail art brushes\nfor fine detail. I love that you can get a\nwhole kit for this price!\nGround \nTruth\nAbsolutely great product\nI wouldn't recommend this for\npainting your full nail (It is for art)\nthis is the best deal I've seen on nail\npolish in a long time\nP5\ngreat colors and great price\nfor the price\ngreat price and great price and\ngreat price\ngreat price and great quality and great\nprice\nChatGPT\nLove this nail art set - perfect\ncolors and variety!\nSHANY's Nail Art Set is a must-\nhave for creative nails.\nSHANY's Nail Art Set is a must-have\nfor stunning manicures.\nChatGLM\nThe SHANY Nail Art Set is a\ncollection\nof\n24\nfamous\ncolors for nail art polish and\nnail art decorations.\nThis SHANY Nail Art Set is a\ngreat option for those looking for\na variety of nail art colors and\ndecorations.\nIt seems like user_1 has purchased a\nvariety\nof\nmakeup\nproducts\nfrom\nSHANY,\nincluding\nprofessional\nmakeup brush sets, eyeshadow palettes,\nnail polish sets, and more. Some of\nthese products have a limited edition or\nunique design, while others have a\nmore standard design. The user has\nalso purchased a UV Gel Light Na\nLLaMA\n1. SHANY Nail Art Set (24\nFamouse\nColors\nNail\nArt\nPolish, Nail Art Decoration)\n2. SHANY Nail Art Set (24\nFamouse\nColors\nNail\nArt\nPolish, Nail Art Decoration)\n3. SHANY Nail Art Set\n1. SHANY Nail Art Set (24\nFamouse Colors Nail Art Polish,\nNail Art Decoration) 2. SHANY\nNail Art Set (24 Famouse Colors\nNail\nArt\nPolish,\nNail\nArt\nDecoration) 3. SHANY Nail Art\nSet\n1.\nSHANY\nProfessional\n13-Piece\nCosmetic Brush Set with Pouch, Set of\n12 Brushes and 1 Pouch, Red\\n2.\nSHANY Makeup Artists Must Have\nPro Eyeshadow Palette, 96 Color\nAlpaca\nSHANY Nail Art Set (24\nFamouse\nColors\nNail\nArt\nPolish, Nail Art Decoration)\nis a great choice for anyone\nlooking to add a touch of\nglamour\nto\ntheir\nnails.\nIt\ncomes\nwith\n24\ndifferent\ncolors\nof\nnail\nart\npolish,\nallowing you to\nThe SHANY Nail Art Set (24\nFamouse Colors Nail Art Polish,\nNail Art Decoration) is a must-\nhave for any nail art lover. It\ncomes with 24 different colors of\nnail\nart\npolish\nand\nnail\nart\ndecorations. It is\nSHANY Nail Art Set (24 Famouse\nColors\nNail\nArt\nPolish,\nNail\nArt\nDecoration) is a nail art set with 24\ncolors of nail art polish and nail art\ndecoration. It is perfect for creating a\nvariety of nail art designs.\nChatGLM \nw/ SFT\nI love the different colors\nI have long nails\nThe colors are beautiful and the price\nwas right\nFigure 5: Example explanation results of different LLMs.\n\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The rapid development of Large Language Models (LLMs) has significantly advanced Natural Language Processing (NLP) tasks, yet their application in recommendation systems has not been thoroughly explored. Existing deep learning-based recommendation methods are often task and domain-specific, requiring extensive task-specific data, which limits their generalization ability. This benchmark aims to address the gap in understanding how LLMs can be utilized in recommendation scenarios.",
            "purpose of benchmark": "The benchmark is intended to evaluate the performance of various LLMs on multiple recommendation tasks, providing insights for researchers to explore the potential of LLMs in enhancing recommendation systems."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of effectively utilizing LLMs for classical recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization.",
            "key obstacle": "Existing benchmarks often lack the ability to assess LLMs on recommendation tasks due to their focus on NLP tasks, leading to a gap in understanding LLMs' capabilities in recommendation contexts."
        },
        "idea": {
            "intuition": "The development of LLMRec is inspired by the observation that LLMs have demonstrated strong performance in various NLP tasks, prompting an exploration of their applicability in recommendation systems.",
            "opinion": "The authors believe that LLMRec can significantly improve the understanding of LLMs' capabilities in the recommendation domain, potentially inspiring further research in this area.",
            "innovation": "LLMRec differs from previous benchmarks by treating LLMs as self-contained recommenders and evaluating their performance across multiple recommendation tasks using a unified approach.",
            "benchmark abbreviation": "LLMRec"
        },
        "dataset": {
            "source": "The dataset was sourced from the Amazon Beauty category, comprising customer review text and associated metadata.",
            "desc": "The dataset includes 22,363 users, 12,101 items, and 198,502 reviews, specifically focusing on user-item interactions in the beauty domain.",
            "content": "The dataset contains textual data from user reviews, which relate to the recommendation tasks by providing insights into user preferences and product characteristics.",
            "size": "198,502",
            "domain": "Beauty",
            "task format": "Rating Prediction"
        },
        "metrics": {
            "metric name": "RMSE, MAE",
            "aspect": "Accuracy",
            "principle": "Metrics were selected based on their relevance to the tasks being benchmarked, ensuring they effectively measure the performance of LLMs in providing accurate recommendations.",
            "procedure": "Model performance is evaluated using the chosen metrics through quantitative assessments on the recommendation tasks, comparing LLM outputs to ground truth values."
        },
        "experiments": {
            "model": "The models tested include ChatGPT, ChatGLM, LLaMA, and Alpaca, both in off-the-shelf and fine-tuned settings.",
            "procedure": "Models were trained and evaluated using a combination of prompt engineering and task-specific finetuning to assess their performance across the recommendation tasks.",
            "result": "Results indicated that off-the-shelf LLMs performed poorly in accuracy-based tasks but showed promise in explainability-based tasks, with fine-tuning improving performance significantly.",
            "variability": "Variability was accounted for by conducting multiple trials and utilizing different subsets of the dataset to ensure robust evaluation."
        },
        "conclusion": "The benchmark results demonstrate that while LLMs struggle with accuracy-based recommendation tasks, they excel in generating explanations and summaries, highlighting their potential in enhancing recommendation systems.",
        "discussion": {
            "advantage": "LLMRec provides a comprehensive framework for evaluating LLMs in recommendation tasks, facilitating further research into their capabilities and applications.",
            "limitation": "The current study is limited to specific LLMs and the Amazon Beauty dataset, which may not generalize to other models or datasets.",
            "future work": "Future research should explore a broader range of LLMs and datasets to validate findings and develop more effective benchmarks."
        },
        "other info": [
            {
                "info1": "The benchmark results and codes are available at https://github.com/williamliujl/LLMRec."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "2.3",
            "key information": "The benchmark aims to evaluate the performance of various LLMs on multiple recommendation tasks, providing insights for researchers to explore the potential of LLMs in enhancing recommendation systems."
        },
        {
            "section number": "4.1",
            "key information": "The rapid development of Large Language Models (LLMs) has significantly advanced Natural Language Processing (NLP) tasks, yet their application in recommendation systems has not been thoroughly explored."
        },
        {
            "section number": "4.2",
            "key information": "LLMRec treats LLMs as self-contained recommenders and evaluates their performance across multiple recommendation tasks using a unified approach."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore a broader range of LLMs and datasets to validate findings and develop more effective benchmarks."
        },
        {
            "section number": "11",
            "key information": "The benchmark results demonstrate that while LLMs struggle with accuracy-based recommendation tasks, they excel in generating explanations and summaries, highlighting their potential in enhancing recommendation systems."
        }
    ],
    "similarity_score": 0.7896867529014061,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0857/085770e8-00c0-403c-a42f-431b623465b0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4264/42644613-dbe6-42f7-8f58-bd9430dcd93a.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Llmrec_ Benchmarking large language models on recommendation task.json"
}