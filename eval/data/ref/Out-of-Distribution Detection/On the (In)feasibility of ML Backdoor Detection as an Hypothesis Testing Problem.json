{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.16926",
    "title": "On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem",
    "abstract": "We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. Thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. However, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. Furthermore, we connect our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem.",
    "bib_name": "pichler2024infeasibilitymlbackdoordetection",
    "md_text": "# On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem\n# Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg\nFebruary 28, 2024\nWe introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. Thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. However, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. Furthermore, we connect our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem.\narXiv:2402.16926v1\n# 1. INTRODUCTION\nThe adoption of modern Machine Learning (ML) methods in a range of real-world tasks including navigation Chen et al. (2022); Wang et al. (2022), medical diagnosis Varoquaux and Cheplygina (2022); Tchango et al. (2022), and system control Zhang et al. (2023) has grown dramatically. However, safe and trustworthy ML systems remain elusive Ilyas et al. (2019); Wu et al. (2022), for reasons including poor interpretability Burkart and Huber (2021); Roscher et al. (2020), test time adversarial inputs Goodfellow et al. (2015) and, relevant to this paper, training time poisoning and backdooring attacks Gu et al. (2017). As the scale, complexity and training data requirements of modern deep neural network architectures has grown, few can afford to train models from scratch. Many users therefore download and fine-tune pre-trained models, or deploy them as is. Consequently, purposefully implanted backdoors in pre-trained ML models pose a key security risk for future ML deployments. In the classic backdoor threat model, a malicious actor trains a backdoored ML model by altering its training data. During inference, for certain, backdoored inputs, modified in an attacker chosen way, the model then provides erroneous predictions. For example, Gu et al. (2017) demonstrate a backdoor in a traffic sign detector that misclassifies stop signs as speed limit signs when stop signs are modified with stickers or sticky notes. Here the sticker or sticky note serve as a trigger, misleading the model into making incorrect decisions. While there are many ways such a backdoor could be embedded into a model, prior work shows that altering even a small fraction of training data yields models with stealthy and effective backdoors Qi et al. (2023).\nIn light of these attacks, substantial efforts have been devoted to backdoor defenses with the goal of identifying such backdoor attacks. To detect a backdoor, the model user (i.e., the defender) has access to a, typically small, validation dataset of clean inputs. In the Model Backdoor Detection (MBD) problem (Chen et al., 2019, Sec. 4.2), Wang et al. (2019); Shen et al. (2021), the defender wishes to detect if the model itself contains a backdoor. In the Sample Backdoor Detection (SBD) problem Liu et al. (2023); Ma et al. (2022), the defender wants to detect if a specific test input is backdoored or not, assuming that models deployed in the field might be backdoored. We note that our backdooring threat model is part of the larger body of work on data poisoning threats. The latter encompasses all scenarios where a model trained on\u2013partially\u2013poisoned data is negatively impacted in some way, possibly, but necessarily, by implanting a backdoor. With the growing use of large pretrained foundation models, the backdooring threat (where the defender receives a model, not training data) is of increasing relevance. Unfortunately, despite several years of research, the field is still plagued by a cat-and-mouse game between attackers and defenders. Certifiable defenses against backdooring attacks have remained elusive, and despite some recent progress in this direction for data poisoning attacks, those results don\u2019t translate to our setting as discussed in Section 3. In fact, despite the large body of work in the area, backdoor detection has not been formally studied from first principles. Here, we undertake the first formal exposition of the ML backdooring problem for both the MBD and SBD settings. Although it has been observed that backdoor detection can work in specific scenarios, we are interested in the feasibility of detecting arbitrary, unknown backdoors. We thus ask the following questions. First, what are fundamental bounds lower bounds on backdoor detection\u2014in fact, is backdoor detection even feasible, and if so, under what assumptions? Second, how is backdoor detection related to other statistical problems in ML? Recent work has leveraged Out-Of-Distribution (OOD) detection methods for backdoor detection; can this relationship be formalized? Third, how are MBD and SBD related? Both are separately addressed in literature, but their relationship has not been explicated. And finally, what are the implications for future progress in building practical backdoor defenses? We provide answers to all four questions using theoretical results and a \u201ctoy\u201d example. Contributions. In this paper, we present the first precise statistical formulation of the MBD and SBD problems (Section 2.1). This formulation enables several new insights on backdoor detection. 1. Relationship to well-known statistical problems: Our formulation unifies MBD, SBD and even OOD detection within a common framework and we reduce these problems to standard statistical hypothesis testing problems. 2. Infeasibility: Leveraging these reductions, we conclude that under realistic assumptions, universal (adversary-unaware) backdoor detection is not possible for an infinite alphabet of the training data. 3. Bound for finite alphabet size: For a finite data alphabet, we provide a bound on the achievable error probability given a fixed training set size. These bounds are evaluated for commonly used datasets in ML, showing that universal backdoor detection is only achievable for very small alphabets. 4. Connections to Probably Asymptotically Correct (PAC) learning theory of OOD detection: We show that detecting a backdoor in training data is equivalent to a binary NeymanPearson hypothesis test if OOD detection is PAC learnable as defined in Fang et al. (2022).\nonly evaluate on backdoored models, and fail to report false positive rates in the likely case that models are actually clean.\n# HEORETICAL FORMULATION AND RESU\nWe focus on MBD and SBD as introduced above, in the case, where the attacker has limited control over the training data and is able to backdoor a certain portion of the dataset. The training itself is performed using a standard method, e.g., Stochastic Gradient Descent (SGD). For an extensive overview of other empirical backdoor problems, the reader is referred to, e.g., Wu et al. (2022).\n# 2.1. Formulating Model Backdoor Detection (MBD)\nOverview. After N samples of training data are collected, the backdoor attacker has the option of backdooring a portion of the training data, by replacing each clean sample with a backdoored sample. This backdooring may alter, e.g., images as well as their labels. Subsequently, an Artificial Neural Network (ANN) is trained on the resulting training set. Given the resulting trained network (i.e., the network parameters), the task of the backdoor detector is to determine whether the training data had been backdoored. The detector may obtain M additional clean samples, e.g., by independently collecting additional data. We assume that the backdoor attacker has no access to these samples. Table 1 provides an overview of the notation used.\nSymbol\nDescription\nX \u223cP\nX distributed according to P\nX\nAlphabet of X\n|X| \u2208N \u222a{\u221e}\nAlphabet size\nP(X)\nSet of all probability distributions on X\nX(0) \u223cP0\nClean sample/distribution\nX(1) \u223cPb\nBackdoored sample/distribution\n\u03b3 \u2208[0, 1]\nBackdoor probability\nP1 = \u03b3Pb + (1 \u2212\u03b3)P0\nMixture of P0 and Pb\nD = (X1, . . . XN) \u223cP N\nDataset of N i.i.d. samples\nD(0) = (X(0)\n1 , . . . X(0)\nN ) \u223cP N\n0\nClean dataset of N i.i.d. samples\nD(1) = (X(1)\n1 , . . . X(1)\nN ) \u223cP N\n1\nBackdoored dataset of N i.i.d. samples\nD\u2032 = (X\u2032\n1, . . . X\u2032\nM) \u223cP M\n0\nDataset of M additional clean samples\n\u03b8 = A(D)\nParameters resulting from training on D\nP \u2286P(X)2\nSet of allowed distributions (P0, Pb) \u2208P\nJ \u223cB( 1\n2)\nBernoulli random variable indicating a backdoor\nQ = (A(D(J)), D\u2032)\nParameters \u03b8 and samples D\u2032 given to the detector\ng(Q) \u2208{0, 1}\nBackdoor detector\nR(g; P0, Pb) = Pr{g(Q) \u0338= J}\nRisk of detector g at (P0, Pb)\n\u03b1 \u2208[0, 1\n2]\nError probability\ng0 = g, g1, g2, g3\nType 0,1,2,3 detectors\nQ0 = Q, Q1, Q2, Q3\nInput for Type 0,1,2,3 detectors\nTV(P, Q)\nTotal variational distance between two distributions\n\u03b2 \u2208[0, 1)\nDistance constraint TV(P0, Pb) \u22651 \u2212\u03b2\nTable 1: Overview of Notation. This excludes Section 2.3, where some definitions are generalize\nDataset and training. Consider a, possibly stochastic, training algorithm A (e.g., SGD), that trains a model on training data1 D = (X1, X2, . . . , XN), consisting of N i.i.d. random variables, distributed like X \u223cP, as input and produces a parameter vector \u03b8 = A(D) as output. Clean data. Let P0 \u2208P(X) be the probability distribution on X of clean samples and let D(0) = (X(0) 1 , X(0) 2 , . . . , X(0) N ) be a clean dataset, consisting of N i.i.d. random variables, drawn from P0. Backdoor. To backdoor a model, an adversary may replace some training samples with backdoored samples, drawn from a different distribution Pb \u2208P(X). This distribution may result from applying a backdoor function to the clean samples. Note, that as the training sample X includes the data and the label, both may be altered by the adversary. Backdoored training data. Assuming that a fraction \u03b3 \u2208(0, 1] of the training data is backdoored, the backdoored training dataset D(1) = (X(1) 1 , X(1) 2 , . . . , X(1) N ) is independently drawn according to P1 = \u03b3Pb + (1 \u2212\u03b3)P0, i.e., according to Pb with probability \u03b3 and from P0 with probability 1 \u2212\u03b3. Additional clean data. Furthermore, let D\u2032 = (X\u2032 1, X\u2032 2, . . . , X\u2032 M) \u223cP M 0 be M i.i.d. additional clean samples distributed according to P0. These samples correspond to clean validation data or may have been collected by the backdoor detector prior to making a decision. Model Backdoor Detection. The backdoor detector is a function g, that takes \u03b8 = A(D(j)) and additional data D\u2032 as its input and outputs 0 for \u201cbackdoor\u201d and 1 for \u201cno backdoor\u201d. For MBD, we require the detector to determine j with high probability. For ease of notation, we use a Bernoulli-1 2 random variable J \u223cB( 1 2) and define the input for the detector as Q = (A(D(J)), D\u2032), such that the error probability Pr{g(Q) \u0338= J} of the detector is well-defined. Possible data distributions. Finally, the last observation needed to obtain a well-defined backdoor detection problem is, that we need to avoid the possibility of P0 = Pb. In the case where the clean and the backdoor distributions are identical, clearly, detection is impossible. We opt for the general approach of defining a suitable set P \u2286P(X)2 that contains all possible clean and backdoor distribution pairs (P0, Pb) \u2208P. These discussions then naturally lead to the following central definition.\nDefinition 1. The MBD problem for a training algorithm A is determined by the following quantities: \u03b3 \u2208(0, 1], N \u2208N, M \u2208N, and P \u2286P(X)2. Fixing these quantities, we define the risk of a backdoor detector g associated with (P0, Pb) as\nWe say that a backdoor detector is \u03b1-error for some \u03b1 \u2208[0, 1 2] if, for every pair (P0, Pb) \u2208P the risk is bounded by\nRemark 1. Instead of bounding the risk as in (3), it may seem more natural to require Pr{g(Q) \u0338= j|J = j} \u2264\u03b1 for j = 0, 1. But note that Pr{g(Q) \u0338= j|J = j} \u22642\u03b1 for j = 0, 1 immediately follows from (3).\nThe training sample X may be a vector that includes data and label\n(1)\n(2)\n(3)\n# 2.2. (In)feasibility of Model Backdoor Detection\n will be useful to consider easier problems than \u03b1-error detection (Definition 1) and establish ductions. To this end, we consider four different Types of detectors, starting with Type 0 that orresponds to MBD. The other three detectors also seek to infer J, but are given access to rogressively more information via oracles; as such, any subsequent detectors can improve on revious ones. The four detectors are: Type 0: The default detector, g0(Q0), as used in Definition 1 with with Q0 = Q = (A(D(J)), D This detector corresponds to our MBD problem. Type 1: Detector g1(Q1) with Q1 = (D(J), D\u2032), i.e., with access to the training dataset D(J) instead of just the trained model, and M independent clean validation samples D\u2032. Type 2: Detector g2(Q2) with Q2 = (D(J), P0), i.e., with access to the clean data distribution instead of clean validation samples. This is an OOD detection problem: Is D(J) OOD with respect to P0? Type 3: Detector g3(Q3) with Q3 = (D(J), P0, Pb); i.e., the previous detector also now gets the backdoor distribution Pb, and must decide which distribution the training data came from. This is classical binary Neyman-Pearson hypothesis testing problem between P N 0 and P N 1 .\nIt will be useful to consider easier problems than \u03b1-error detection (Definition 1) and establish reductions. To this end, we consider four different Types of detectors, starting with Type 0 that corresponds to MBD. The other three detectors also seek to infer J, but are given access to progressively more information via oracles; as such, any subsequent detectors can improve on previous ones. The four detectors are:\nType 1: Detector g1(Q1) with Q1 = (D(J), D\u2032), i.e., with access to the training dataset D(J instead of just the trained model, and M independent clean validation samples D\u2032.\nType 2: Detector g2(Q2) with Q2 = (D(J), P0), i.e., with access to the clean data distribution instead of clean validation samples. This is an OOD detection problem: Is D(J) OOD with respect to P0?\nType 3: Detector g3(Q3) with Q3 = (D(J), P0, Pb); i.e., the previous detector also now get the backdoor distribution Pb, and must decide which distribution the training data came from. This is classical binary Neyman-Pearson hypothesis testing problem between P N 0 and P N 1 .\nWe assume that detectors of Types 2 and 3 have access to P0 (and Pb for a Type 3 detector) in terms of evaluation of the distribution, and also have the ability to sample from the distribution. We thus consider Types 2 and 3 as randomized detectors to account for sampling. The definitions of risk and \u03b1-error detection of g2, g3 apply mutatis mutandis as in Definition 1, where the probability in (1) is also taken over the randomness of g. Remark 2 (Ordering of detector Types). Types 0 to 3 are listed in order of decreasing difficulty as, e.g., more information is provided to a Type 3 detector than to a Type 2 detector. Thus, an \u03b1error detector g immediately provides an \u03b1-error Type 1 detector g1, which in turn immediately provides an \u03b1-error Type 2 detector g2, which yields an \u03b1-error detector g3 of Type 3. Thus, we can define a total ordering on the different Types of detectors, using A \u227aB to signify that A can be derived from B: Q0 \u227aQ1 \u227aQ2 \u227aQ3. The formal argument, showing this claim can be found in Lemma 5 in Appendix C. In Section 2.2.1 we will show that for a reasonable P, \u03b1-error Type 2 detection is impossible with \u03b1 < 1 2. The reduction argument in Remark 2 thus ensures that \u03b1-error detection with \u03b1 < 1 2 is also impossible for Type 0 and Type 1 detectors. We can resolve the situation for a Type 3 detector using the Neyman-Pearson lemma. Lemma 1. Given a Type 3 backdoor detector g3(D, P0, Pb), for any pair (P0, Pb) \u2208P(X)2 we have\n  where the first equality in (4) can be achieved by the Neyman-Pearson detector. Thus, an \u03b1-error detector of Type 3 can only exist if \u03b1 \u22651 2 \u2212\u03b3N 2 TV(P0, Pb) for all (P0, Pb) \u2208P.\n  where the first equality in (4) can be achieved by the Neyman-Pearson detector. Thus, an \u03b1-error detector of Type 3 can only exist if \u03b1 \u22651 2 \u2212\u03b3N 2 TV(P0, Pb) for all (P0, Pb) \u2208P. See proof on page 17. Before we can analyze detectors of Types 1 and 2, we need to specify the set of allowable distributions P. We do this, using Lemma 1.\nSee proof on page 17. Before we can analyze detectors of Types 1 and 2, we need to specify the set of allowable distributions P. We do this, using Lemma 1.\n(4) (5)\nFirst, we show that merely excluding the identity P0 \u0338= Pb, i.e., P = {(P0, Pb) \u2208P(X)2 : P0 \u0338= Pb} is not sufficient. Example 1. Let g3(D, P0, P1) be an \u03b1-error Type 3 detector and assume that X is infinite, i.e., |X| = \u221e. Let P be given as above, ensuring only that P0 \u0338= Pb. For any \u03b5 > 0, we can then choose2 (P0, Pb) \u2208P with 0 < TV(P0, Pb) \u2264 2 \u03b3N \u03b5. By Lemma 1, we have \u03b1 \u2265 1 2 \u2212\u03b3N 2 TV(P0, Pb) \u22651 2 \u2212\u03b5. As \u03b5 > 0 was arbitrary, we have \u03b1 = 1 2. Lemma 1 and Example 1 show that even for a Type 3 detector, we need TV(P0, Pb) > 1\u22122\u03b1 \u03b3N for all (P0, Pb) \u2208P, in order for \u03b1-error detection to be achievable. In the following we will assume that P is the set of probability distributions P0, Pb with TV(P0, Pb) \u22651 \u2212\u03b2, for some fixed \u03b2 \u2208[0, 1). This strong requirement is motivated by the fact that in this case, 1\u2212\u03b3+\u03b3\u03b2 2 -error Type 3 detection is achievable with only one sample. Remark 3. Thorough reasoning and examples, illustrating why total variation distance is the preferred distance measure for distribution hypothesis testing can be found in (Canonne, 2022, Section 1.2).\n# 2.2.1. Impossibility\nIn the following we prove an impossibility result, which implies that for an infinite alphabet X, the error probability (as given in Definition 1) of any detector (of Type 0, Type 1 or Type 2) is 1 2, the error probability of a random guess. Additionally, for finite X, we provide a lower bound on the size of the training set N, as a function of \u03b1. Theorem 1. Fix N \u2208N, \u03b1 \u2208(0, 1 2], \u03b2 \u2208[0, 1], and P = {(P0, Pb) : TV(P0, Pb) \u22651 \u2212\u03b2}. Let g2(D, P0) be an \u03b1-error Type 2 detector. For |X| = \u221e, we then have necessarily \u03b1 = 1 2, while for |X| < \u221e, we have\nIn the following we prove an impossibility result, which implies that for an infinite alphabet X, the error probability (as given in Definition 1) of any detector (of Type 0, Type 1 or Type 2) is 1 2, the error probability of a random guess. Additionally, for finite X, we provide a lower bound on the size of the training set N, as a function of \u03b1.\nTheorem 1. Fix N \u2208N, \u03b1 \u2208(0, 1 2], \u03b2 \u2208[0, 1], and P = {(P0, Pb) : TV(P0, Pb) \u22651 \u2212\u03b2}. Le g2(D, P0) be an \u03b1-error Type 2 detector. For |X| = \u221e, we then have necessarily \u03b1 = 1 2, while for |X| < \u221e, we have\nSee proof on page 17. It is important to notice that the bound (6) relates the number of training samples N with the alphabet size |X| and the risk \u03b1, while the number M of clean samples available to the defender does not appear. By the reduction argument in Lemma 1 the impossibility result in Theorem 1 also holds for detector Types 0 and 1 for all possible values M \u2208N. For a fixed dataset alphabet size |X| and allowed error probability \u03b1, the bound (6) gives the minimum size of the training set N for the error level \u03b1 to be achievable. Note the following special cases in terms of \u03b1, \u03b2: \u2022 For \u03b1 = 1 2, the bound (6) is always satisfied as the RHS is 0, showing that 1 2-error detection is always achievable. This coincides with the error probability of a random guess. \u2022 The bound (6) is monotonically decreasing in \u03b1 and for \u03b1 \u21920, it approaches \u03b2|X|. \u2022 In case \u03b2 = 0, the bound (6) is always satisfied as the RHS is zero for \u03b1 \u2208(0, 1 2] in this case. This shows that \u03b1-error detection is always possible if P and P have disjoint support,\n\u2022 In case \u03b2 = 0, the bound (6) is always satisfied as the RHS is zero for \u03b1 \u2208(0, 1 2] in this case. This shows that \u03b1-error detection is always possible if P0 and Pb have disjoint support, i.e., TV(P0, Pb) = 1.\n2Without loss of generality, we can assume X = N. Then, this can, e.g., be achieved by P0 = U({0, 1, 2, . . . , \u230a\u03b3N 2\u03b5 \u230b}) and Pb = U({1, 2, . . . , \u230a\u03b3N 2\u03b5 \u230b}). We use U(\u00b7) to denote a uniform distribution on a finite set.\n(6)\nFor an infinite alphabet X, (6) needs to be satisfied for arbitrarily large values of |X|. For finite training set size N, this is only possible if \u03b1 = 1 2 as then, log 1 2\u03b1 = 0. Thus, in this case, for any Type 2 detector, there is a particular clean distribution and backdoor strategy, such that this detector performs no better than random guessing. For fixed \u03b1 and \u03b2, we can use (6) to determine the minimum size of the training set N for popular datasets, for \u03b1 error probability to be achievable by a Type 2 detector. To this end, we use the width W, height H, number of channels C and color depth D of an image dataset to compute |X| = DWHC. For categorical datasets, we may multiply the number of categories for all the properties recorded in the dataset to obtain |X|. The resulting value for the bound in (6) is given in Table 2 for several popular datasets. As can be seen by these numbers, this universal backdoor detection is infeasible for all, but the smallest tabular datasets. Code for computing the values in Table 2 can be found at https://github.com/g-pichler/in feasibility of ml backdoor detection. Note also, that the impossibility of Type 2 backdoor detection automatically precludes the existence of Type 1 or Type 0 error detectors with equal performance by the reduction argument in Remark 2.\n# Illustrative Example\nWe noted previously the following consequence of Theorem 1, in case of an infinite alphabet: For any Type 2 backdoor detector, there exists an attacker, such that the detector is no better than a random guess. Here, we will showcase this on a toy example of a binary classification task, for a specific data distribution P0, and any backdoor detector from a family of Type 2 detectors, parameterized by v \u2208RK. For any parameter v, we show how to construct a backdoor attack that is both effective in changing the decision regions of a classifier trained on backdoored data, and undetectable by the backdoor detector. Data Distribution. We have data and label pairs X = (Y, Z) , where Y \u2208{\u22121, 1} is a binary label and Z = Y 1 + \u03c3W with \u03c3 > 0 and W is multivariate normal with dimension K. For K = 2 dimensions, the optimal classifier for this problem decides \u02c6y = 1 if z1 + z2 \u22650 and otherwise \u02c6y = \u22121, leading to the decision boundary z2 = \u2212z1. Backdoor Detector. The Type 2 detector g2(D, P0) is parameterized by the unit vector v \u2208RK with \u2225v\u2225= 1. Using the fact that f(X) := v \u00b7 (Y Z) = v \u00b7 1 + \u03c3W with a standard normal variable W, we see that applying the function f( \u00b7 ) to D yields N i.d.d. Gaussian random variables with mean \u00b5 := 1\u00b7v and variance \u03c32. The detector then performs a statistical goodness-of-fit test on this dataset. We utilize the Kolmogorov-Smirnov test for this purpose. Backdoor. Knowing v, the attacker transforms an input sample x = (z, y) into a backdoored sample b(x) := (z + y\u2206, \u2212y) with the opposite label, and shifted by y\u2206, where \u2206= 2 \u221a K\u2212\u00b52 (1 \u2212 \u00b5v)\u22122\u00b5v. This transformation ensures that the statistics of f(X) do not change when applying the backdoor. After the attacker replaces clean samples with backdoored samples at a rate of \u03b3 \u2208(0, 1], the Kolmogorov-Smirnov test is performed. Figure 1 showcases this strategy in K = 2 dimensions with N = 150, \u03b3 = 0.5, \u03c3 = 0.5, and v = (0.981, 0.196), resulting in \u00b5 = 1.177. The KolmogorovSmirnov test obtained a p-value of pval = 0.2381, thus not detecting the backdoor. The resulting histograms of f(D) for clean and backdoored data are shown in Fig. 2. The code for this example can be found at https://github.com/g-pichler/in feasibility of ml backdoor detection.\n# 2.2.2. Achievability\nIn this section we are going to show that \u03b1-error Type 2 detection is always achievable if the size of the alphabet |X| is small enough:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5413/5413da01-185b-4c05-a10d-2de27e42f2ae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: MBD example with N = 150 samples. The backdoor detector uses projection onto v to take a decision. The vector \u2206is the additive backdoor trigger used by the attacker The decision boundary changes when applying the backdoor.</div>\nheorem 2. Considering the backdoor detection setup of Definition 1 with P = {(P0, Pb) : V(P0, Pb) \u22651 \u2212\u03b2} and a finite alphabet |X| < \u221e. If\n\u2022 The bound on the RHS of (7) increases monotonically from 0 to \u221efor increasing |X|. Thus, there is some fixed alphabet size, below which, \u03b1-error detection is guaranteed to be possible.\n(7)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3579/35798e38-d5e0-4f4f-b610-49cf11e6af31.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Histogram of the detector decision statistics clean and backdoored samples de in Fig. 1.</div>\n<div style=\"text-align: center;\">or decision statistics clean and backdoored samples depicte</div>\nDataset\n|X|\nN\nLisa Traffic Sign\n256307200\n\u226510369904\nImageNet\n256150528\n\u226510181252\nCIFAR10\n2563072\n\u2265103697\nMNIST\n256784\n\u226510942\nB/W MNIST\n2784\n\u226510116\nAdult\n\u22651021.86\n\u2265109\nHeart Disease\n\u22651013.51\n\u2265105\nIris\n\u2265106.35\n\u2265101\n# .3. Connections to PAC-Learnability of OOD Detection\nNote that a Type 1 detector essentially needs to solve an OOD detection problem: The detector g1 needs to determine if the N samples D were drawn from the same distribution as D\u2032. The goal of this section is to prove Theorem 3. This theorem has an interesting implication in case the OOD detection problem is PAC-learnable: If an \u03b1-error Type 3 backdoor detector g3 exists, then (\u03b1 + \u03f5)-error detection is also possible for a Type 1 detector for any \u03f5 > 0. Thus, essentially Types 1 to 3 all become equivalent if OOD detection is PAC-learnable. Note that Type 3 detection is characterized by Lemma 1. The PAC-learnability of the detector in Type 1 was analyzed in Fang et al. (2022). We fist restate a special case of the definition of (weak) PAC-learnability as given in (Fang et al., 2022, Def. 1).\n# Definition 2. For distributions P0, Pb on X, the OOD-risk of a function f : X \u2192{0, 1}, w.r.t the Hamming distance, is defined as\nDefinition 2. For distributions P0, Pb on X, the OOD-risk of a function f : X \u2192{ the Hamming distance, is defined as\n\u00afR(f, P0, Pb) := Pr{f(X(J)) \u0338= J} = 1 2 Pr{f(X(0)) = 1} + 1 2 Pr{f(X(1)) = 0}.\nGiven a space of probability function P, OOD-detection is PAC-learnable on P if there exists an algorithm G : \ufffd\u221e m=1 X m \u2192{0, 1}X and a monotonically decreasing sequence \u03f5(m) such that\nwhere the expectation is taken w.r.t. D\u2032 and the infimum is over {0, 1}X , i.e., all functions f : X \u2192{0, 1}. Remark 4. Definition 2 is a special case of (Fang et al., 2022, Def. 1) in several ways4: \u2022 The hypothesis space is the complete function space H = {0, 1}X , of functions f : X \u2192 {0, 1}. \u2022 The loss function, as used in (Fang et al., 2022, Eq. (1)) is the Hamming distance, i.e., \u2113(y, y\u2032) = 1 if and only if y \u0338= y\u2032. \u2022 We are purely concerned with one-class novelty detection, i.e., K = 1 in (Fang et al., 2022, Sec. 2). Therefore we do not take YO and YI into account, as YI \u22611 and YO \u22612. \u2022 Note that (P0, Pb) \u2208P play the role of (DXO, DXI) and the complete domain space is then given by DXY = {DXY : DXY = 1 2P0 + 1 2Pb, (P0, Pb) \u2208P}. Note, that strong PAC-learnability (Fang et al., 2022, Def. 2) implies weak learnability. To connect PAC-learnability of OOD detection to the learning of backdoor detectors, we consider PAC-learnability on the N-dimensional product space, i.e., on X N with distributions P N 0 , P N b . We can now connect PAC-learnability to the existence of \u03b1-error detectors of Types 1 and 3.\nNote, that strong PAC-learnability (Fang et al., 2022, Def. 2) implies weak learnability. To connect PAC-learnability of OOD detection to the learning of backdoor detectors, we consider PAC-learnability on the N-dimensional product space, i.e., on X N with distributions P N 0 , P N b . We can now connect PAC-learnability to the existence of \u03b1-error detectors of Types 1 and 3.\nTheorem 3. Consider the backdoor detection setup of Definition 1, with fixed \u03b3 \u2208(0, 1], N \u2208N and some set of possible distributions P. Let P\u2032 be the set of N-fold products of (P0, P1), i.e., P\u2032 = {(P N 0 , (\u03b3Pb + (1 \u2212\u03b3)P0)N) : (P0, Pb) \u2208P}. Then, OOD-detection is PAC-learnable on P\u2032 if and only if the following holds for any \u03f5 > 0 and any Type 3 detector g3(D, P0, Pb): We can find M \u2208N and a Type 1 detector g1(D, D\u2032), which satisfies R(g1, P0, Pb) \u2264R(g3, P0, Pb) + \u03f5 for every (P0, Pb) \u2208P.\n# See proof on page 20.\nCorollary 1. If OOD-detection is PAC-learnable on P\u2032, we have the following: If \u03b1-error backdoor detection is possible in the easier case of Type 3 detection, which is completely characterized by Lemma 1, then (\u03b1 + \u03f5)-error detection is also possible for a Type 1 detector for any \u03f5 > 0. Consequently, up to topological closure, the same error probability is achievable for all detector Types 1 to 3, if OOD-detection is PAC-learnable on P\u2032.\n# 2.3. Generalizing to Sample Backdoor Detection\nWe can generalize Definition 1 to SBD by providing a detector g\u2032(Q\u2032) with input Q\u2032 = (Q, X(I)) = (A(D(J)), D\u2032, X(I)), where a random variable I on {0, 1} determines if a sample X(I) was drawn as X(0) \u223cP0 (I = 0) or as5 X(1) \u223cPb (I = 1).\n3Note that D\u2032 contains M samples. 4We use the notation of (Fang et al., 2022, Sec. 2) for the following symbols: H, XO, XI, YO, YI, DXO, DXI , DXY , DXY , \u2113(\u00b7, \u00b7), K. 5Note, that X(1) is distributed according to Pb and not according to P1 = (1 \u2212\u03b3)P0 + \u03b3Pb.\n(8)\nWe define a general target function t(j, i) \u2208{0, 1} and require that a backdoor detector satisfies g\u2032(Q\u2032) = t(J, I) with high probability. In this case, it is beneficial to allow for an arbitrary probability distribution PJI of (J, I) on {0, 1}2. This naturally leads to the following alternative definition of \u03b1-error detection, generalizing Definition 1. Definition 3. A backdoor detection problem for a training algorithm A is determined by the following quantities: \u03b3 \u2208(0, 1], N \u2208N, M \u2208N, P \u2286P(X)2, PJI \u2208P({0, 1}2), and t: {0, 1}2 \u2192 {0, 1}. Fixing these quantities, we define the risk of a backdoor detector g\u2032 associated with (P0, Pb) as R(g\u2032; P0, Pb) := Pr{g\u2032(Q\u2032) \u0338= t(J, I)}, where the probability is w.r.t. Q\u2032 = (A(D(J)), D\u2032, X(I)) and (J, I). We say that a backdoor detector is \u03b1-error for some \u03b1 \u2208[0, 1 2] if, for every pair (P0, Pb) \u2208P, the risk is bounded by R(g\u2032; P0, Pb) \u2264\u03b1. Then, even OOD can be modeled using our setup. Figure 3 presents an overview of the target function t(j, i) for MBD, SBD and OOD.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e3a/3e3a8720-6f81-4734-9ca9-2ed860551fb1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) tSBD(j, i) = i.</div>\n<div style=\"text-align: center;\">Figure 3: Target function t(j, i) for different backdoor detection flavors. j \u2208{0, 1} signals if t training dataset is backdoored (j = 1) or not (j = 0), while i \u2208{0, 1} indicates if t test sample is backdoored.</div>\nNote that several cells in the diagrams in Fig. 3 are grayed out. This reflects the fact that for certain flavors of backdoor detection, specific combinations of (j, i) are not relevant. For MBD for instance, we are not interested in whether the target sample X(I) contains a backdoor and we can thus assume I = 0 in this case, effectively reducing this case to the problem introduced in Section 2.1 with M + 1 samples being drawn from P0, i.e., (D\u2032, X(0)), available to the detector. Conversely, the case of a clean model, i.e., j = 0 and a sample with a backdoor, i.e., i = 1 is not realistic for SBD and we set PJI(0, 1) = 0 in this case. By setting J = 0 (i.e., model is trained on clean data and PJI(1, 0) = PJI(1, 1) = 0) and using tOOD(j, i) = tSBD(i, 0) = i, we obtain an OOD detection problem, where the detector has access to a model A(D(0)) trained on clean data and additional clean data D\u2032. The detector then needs to determine whether X(I) is in or out of distribution. To show how our result from Sections 2.2.1 and 2.2.2 carry over to other variants of backdoor detection, we will directly use Theorem 1 to derive a similar result for SBD. In analogy to the different Types of MBD detectors introduced in Section 2, we have a Type 2 detector g\u2032 2(Q\u2032 2) with Q\u2032 2 = (D(J), P0, X(I)) for SBD. For such a detector we can leverage a reduction argument to obtain the following.\nCorollary 2. Let g\u2032 2(D(J), P0, X(I)) be a Type 2 detector for an SBD problem, where we hav r = min{PJI(0, 0), PJI(1, 1)} > 0 and P = {(P0, Pb) : TV(P0, Pb) \u22651 \u2212\u03b2}. Then, if g\u2032 2 \nN \u2265log \u03b1 r 2 + \ufffd (log \u03b1 r )2 4 + (\u03b2|X| \u22121) log r \u03b1.\nSee proof on page 21.\n# 3. RELATED WORKS\nBackdoor attacks. Early backdoor methods rely on triggers that are visible to the human eye, and generally consist of a local patch on the samples Gu et al. (2017); Shafahi et al. (2018); Nguyen and Tran (2020). Other attacks add a layer of stealthiness by using invisible triggers, which are commonly covering the whole sample and are not detectable by the human eye Chen et al. (2017); Zeng et al. (2021); Li et al. (2021a). Additive attacks Gu et al. (2017); Chen et al. (2017); Shafahi et al. (2018) fuse the triggers to the clean samples as additive noise. Conversely, non-additive attacks Zeng et al. (2021); Li et al. (2021a); Nguyen and Tran (2021) modify the samples by changing attributes such as the color of the pixels or applying spatial transformations. Additionally, some attacks add the same trigger to all samples and are therefore sample-agnostic Gu et al. (2017); Chen et al. (2017), while others are sample-specific Nguyen and Tran (2020, 2021). Finally, recent attacks have been proposed to reduce the issue of the linear separability between clean and backdoored samples Qi et al. (2023) which arises in many of the previously mentioned works. Backdoor defenses. This work is concerned with \u201cpost-training\u201d defenses, i.e. those methods that aim to remove or mitigate the backdoor effect from a backdoored model, as opposed to techniques that deal with the problem before Udeshi et al. (2022); Gao et al. (2023) or during training Huang et al. (2022). The solutions that mostly align with the proposed frameworks are those that are designed to detect whether the model is backdoored Liu et al. (2017); Wang et al. (2019), or those that can detect backdoored samples Li et al. (2021b); Huang et al. (2022); Liu et al. (2019). While some detector only work when the trigger is assumed to be sample-agnostic Chou et al. (2020); Gao et al. (2019); Tao et al. (2022), others are reported to be effective on sample-specific triggers Zeng et al. (2021); Liu et al. (2023). Moreover, recent detectors propose to replace the need for a set of clean samples with the generation of perturbed samples which may help to create a representation of the backdoored samples Liu et al. (2023); Pang et al. (2023). In providing a theoretical analysis from first principles, and in suggesting a strong connection between OOD detection and backdoor detection, our work is complementary to Ma et al. (2022). Finally, is important to notice that most literature on backdoor detection focuses on the SBD problem and on the mitigation of backdoor attacks, when a dataset is known to be backdoored Udeshi et al. (2022); Huang et al. (2022); Tran et al. (2018); Wu and Wang (2021). Certifiable Defenses. For data poisoning attacks, where the attacker\u2019s goal is only to diminish the accuracy of the trained classifier, certifiable defenses do exist Steinhardt et al. (2017); Koh et al. (2022). Also for backdoor attacks, smoothing strategies Weber et al. (2023); Wang et al. (2020) were proposed, which allow for certified robustness against backdoor attacks. However, the threat model is severely restricted to very small (in size and amplitude) triggers, which can be successfully obscured by adding smoothing noise. The impossibility result in Section 2.2.1 showcases, why certifiable defenses cannot be mounted against a capable attacker in general.\n# 4. CONCLUSIONS\nWe provided a formal statistical definition of backdoor detection and investigated the feasibility of backdoor detection. As the backdoor attack is usually not known to the defender, in our analysis we focused on universal (adversary-unaware) backdoor detection. This implies that such backdoor defense schemes must be robust against targeted attacks, which are crafted to fool the specific defense strategy, excluding any \u201csecurity-by-obscurity\u201d schemes, where defense only holds as long as it is not public knowledge. We concluded that under very general assumptions, universal (adversary-unaware) backdoor detection is not possible. Thus, backdoor detectors need to be adversary-aware to perform well at their task. Ultimately, this work makes the claim that designing universal (adversary-unaware) backdoor detection methods is an exercise in futility. As did Shokri and Tan (2020), we make the case that backdoor detectors need to be adversary-aware or make specific assumptions on the data distribution and/or backdoor strategy employed. Unfortunately this is not the case for much published work, which implies that the proposed methods must fail on many untested instances of backdoor detection. Furthermore, we note that when designing a backdoor detection algorithm, the advantage should be given to the attacker, which is able to adapt to a defense strategy, but not the other way around.\n# Acknowledgments\nThis work is supported in part by grants from the National Science Foundation (NSF) and th ARO 77191, in part by the Army Research Office under grant #W911NF-21-1-0155, in par by Intel, and in part by the NYUAD Center for Artificial Intelligence and Robotics, funded by Tamkeen under the NYUAD Research Institute Award CG010.\n# References\nBurkart, N. and Huber, M. F. (2021). A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245\u2013317. Canonne, C. L. (2022). Topics and techniques in distribution testing: A biased but representative sample. Foundations and Trends in Communications and Information Theory, 19(6):1032\u2013 1198. Chen, H., Fu, C., Zhao, J., and Koushanfar, F. (2019). Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks. In IJCAI, volume 2, page 8. Chen, P., Ji, D., Lin, K., Zeng, R., Li, T. H., Tan, M., and Gan, C. (2022). Weakly-supervised multi-granularity map learning for vision-and-language navigation. In NeurIPS. Chen, X., Liu, C., Li, B., Lu, K., and Song, D. (2017). Targeted backdoor attacks on deep learning systems using data poisoning. CoRR, abs/1712.05526. Chou, E., Tramer, F., and Pellegrino, G. (2020). Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354, Los Alamitos, CA, USA. IEEE Computer Society. Fang, Z., Li, Y., Lu, J., Dong, J., Han, B., and Liu, F. (2022). Is out-of-distribution detection learnable?\nBurkart, N. and Huber, M. F. (2021). A survey on the explainability of supervised machin learning. Journal of Artificial Intelligence Research, 70:245\u2013317.\nBurkart, N. and Huber, M. F. (2021). A survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245\u2013317. Canonne, C. L. (2022). Topics and techniques in distribution testing: A biased but representative sample. Foundations and Trends in Communications and Information Theory, 19(6):1032\u2013 1198. Chen, H., Fu, C., Zhao, J., and Koushanfar, F. (2019). Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks. In IJCAI, volume 2, page 8. Chen, P., Ji, D., Lin, K., Zeng, R., Li, T. H., Tan, M., and Gan, C. (2022). Weakly-supervised multi-granularity map learning for vision-and-language navigation. In NeurIPS. Chen, X., Liu, C., Li, B., Lu, K., and Song, D. (2017). Targeted backdoor attacks on deep learning systems using data poisoning. CoRR, abs/1712.05526. Chou, E., Tramer, F., and Pellegrino, G. (2020). Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354, Los Alamitos, CA, USA. IEEE Computer Society. Fang, Z., Li, Y., Lu, J., Dong, J., Han, B., and Liu, F. (2022). Is out-of-distribution detection learnable?\nGao, K., Bai, Y., Gu, J., Yang, Y., and Xia, S. (2023). Backdoor defense via adaptively splitting poisoned dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 4005\u20134014. IEEE. Gao, Y., Xu, C., Wang, D., Chen, S., Ranasinghe, D. C., and Nepal, S. (2019). STRIP: a defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pages 113\u2013125. Goodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples. In Bengio, Y. and LeCun, Y., editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Gu, T., Dolan-Gavitt, B., and Garg, S. (2017). Badnets: Identifying vulnerabilities in the machine learning model supply chain. Huang, K., Li, Y., Wu, B., Qin, Z., and Ren, K. (2022). Backdoor defense via decoupling the training process. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. (2019). Adversarial examples are not bugs, they are features. Advances in neural information processing systems, 32. Koh, P. W., Steinhardt, J., and Liang, P. (2022). Stronger data poisoning attacks break data sanitization defenses. Machine Learning, pages 1\u201347. Li, Y., Li, Y., Wu, B., Li, L., He, R., and Lyu, S. (2021a). Invisible backdoor attack with samplespecific triggers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16463\u201316472. Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., and Ma, X. (2021b). Anti-backdoor learning: Training clean models on poisoned data. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 14900\u201314912. Liu, X., Li, M., Wang, H., Hu, S., Ye, D., Jin, H., Wu, L., and Xiao, C. (2023). Detecting backdoors during the inference stage based on corruption robustness consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16363\u201316372. Liu, Y., Lee, W.-C., Tao, G., Ma, S., Aafer, Y., and Zhang, X. (2019). Abs: Scanning neural networks for back-doors by artificial brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS \u201919, pages 1265\u20131282, New York, NY, USA. Association for Computing Machinery. Liu, Y., Xie, Y., and Srivastava, A. (2017). Neural trojans. In 2017 IEEE International Conference on Computer Design (ICCD), pages 45\u201348. IEEE. Ma, W., Wang, D., Sun, R., Xue, M., Wen, S., and Xiang, Y. (2022). The \u201dBeatrix\u201d resurrections: Robust backdoor detection via gram matrices. Nguyen, T. A. and Tran, A. T. (2020). Input-aware dynamic backdoor attack. In NeurIPS.\nNguyen, T. A. and Tran, A. T. (2021). Wanet - imperceptible warping-based backdoor attack. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Pang, L., Sun, T., Ling, H., and Chen, C. (2023). Backdoor cleansing with unlabeled data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 12218\u201312227. IEEE. Qi, X., Xie, T., Li, Y., Mahloujifar, S., and Mittal, P. (2023). Revisiting the assumption of latent separability for backdoor defenses. In 11th International Conference on Learning Representations, ICLR. OpenReview.net. Roscher, R., Bohn, B., Duarte, M. F., and Garcke, J. (2020). Explainable machine learning for scientific insights and discoveries. IEEE Access, 8:42200\u201342216. Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and Goldstein, T. (2018). Poison frogs! targeted clean-label poisoning attacks on neural networks. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 6106\u20136116. Shen, G., Liu, Y., Tao, G., An, S., Xu, Q., Cheng, S., Ma, S., and Zhang, X. (2021). Backdoor scanning for deep neural networks through k-arm optimization. In International Conference on Machine Learning, pages 9525\u20139536. PMLR. Shokri, R. and Tan, T. J. L. (2020). Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P), pages 175\u2013183. IEEE. Steinhardt, J., Koh, P. W. W., and Liang, P. S. (2017). Certified defenses for data poisoning attacks. Advances in neural information processing systems, 30. Tao, G., Shen, G., Liu, Y., An, S., Xu, Q., Ma, S., Li, P., and Zhang, X. (2022). Better trigger inversion optimization in backdoor scanning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13358\u201313368. Tchango, A. F., Goel, R., Wen, Z., Martel, J., and Ghosn, J. (2022). DDXPlus: A new dataset for automatic medical diagnosis. In NeurIPS. Tran, B., Li, J., and Madry, A. (2018). Spectral signatures in backdoor attacks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc. Udeshi, S., Peng, S., Woo, G., Loh, L., Rawshan, L., and Chattopadhyay, S. (2022). Model agnostic defence against backdoor attacks in machine learning. IEEE Transactions on Reliability, 71(2):880\u2013895. Varoquaux, G. and Cheplygina, V. (2022). Machine learning for medical imaging: methodological failures and recommendations for the future. npj Digit. Medicine, 5. Villani, C. (2021). Topics in optimal transportation, volume 58. American Mathematical Soc. Wang, B., Cao, X., jia, J., and Gong, N. Z. (2020). On certifying robustness against backdoor attacks via randomized smoothing.\nWang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., and Zhao, B. Y. (2019). Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723. Wang, H., Liang, W., Gool, L. V., and Wang, W. (2022). Towards versatile embodied navigation. In NeurIPS. Weber, M., Xu, X., Karla\u02c7s, B., Zhang, C., and Li, B. (2023). Rab: Provable robustness against backdoor attacks. In 2023 IEEE Symposium on Security and Privacy (SP), pages 1311\u20131328. IEEE. Wu, B., Chen, H., Zhang, M., Zhu, Z., Wei, S., Yuan, D., and Shen, C. (2022). Backdoorbench: A comprehensive benchmark of backdoor learning. In 36th Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Wu, D. and Wang, Y. (2021). Adversarial neuron pruning purifies backdoored deep models. Advances in Neural Information Processing Systems, 34:16913\u201316925. Zeng, Y., Park, W., Mao, Z. M., and Jia, R. (2021). Rethinking the backdoor attacks\u2019 triggers: A frequency perspective. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 16473\u201316481. Zhang, T., Ren, T., Xiao, C., Xiao, W., Gonzalez, J. E., Schuurmans, D., and Dai, B. (2023). Energy-based predictive representations for partially observed reinforcement learning. In Evans, R. J. and Shpitser, I., editors, Uncertainty in Artificial Intelligence, UAI 2023, July 31 - 4 August 2023, Pittsburgh, PA, USA, volume 216 of Proceedings of Machine Learning Research, pages 2477\u20132487. PMLR.\nZeng, Y., Park, W., Mao, Z. M., and Jia, R. (2021). Rethinking the backdoor attacks\u2019 trig gers: A frequency perspective. In Proceedings of the IEEE/CVF International Conference o Computer Vision (ICCV), pages 16473\u201316481.\n# A. Proofs\nLemma 1. Given a Type 3 backdoor detector g3(D, P0, Pb), for any pair (P0, Pb) \u2208P(X)2 we have\nLemma 1. Given a Type 3 backdoor detector g3(D, P0, Pb), for any pair (P0, Pb) \u2208P(X)2 we have\nwhere the first equality in (4) can be achieved by the Neyman-Pearson detector. Thus, an \u03b1-error detector of Type 3 can only exist if \u03b1 \u22651 2 \u2212\u03b3N 2 TV(P0, Pb) for all (P0, Pb) \u2208P. Proof of Lemma 1. Fix (P0, Pb) and let Q = {x \u2208X N : g3(x, P0, Pb) = 1} to obtain\ndetector of Type 3 can only exist if \u03b1 \u22651 2 \u2212\u03b3N 2 TV(P0, Pb) for all (P0, Pb) \u2208P. Proof of Lemma 1. Fix (P0, Pb) and let Q = {x \u2208X N : g3(x, P0, Pb) = 1} to obtain\ndetector of Type 3 can only exist if \u03b1 \u2265 2 \u2212 2 TV(P0, Pb) for all (P0, Pb) \u2208P. Proof of Lemma 1. Fix (P0, Pb) and let Q = {x \u2208X N : g3(x, P0, Pb) = 1} to obtain\nwhere (13) is a consequence of (Villani, 2021, Exercise 1.17). Also using (Villani, 2021, Exercise 1.17), we see that equality in (13) is achieved for the Neyman-Pearson detector\nThe last two steps (14) and (15) follow from Lemma 2.\nTheorem 1. Fix N \u2208N, \u03b1 \u2208(0, 1 2], \u03b2 \u2208[0, 1], and P = {(P0, Pb) : TV(P0, Pb) \u22651 \u2212\u03b2}. Let g2(D, P0) be an \u03b1-error Type 2 detector. For |X| = \u221e, we then have necessarily \u03b1 = 1 2, while for |X| < \u221e, we have\nProof of Theorem 1. For brevity we assume P0 to be given and drop it as an argument for g2(D, P0) = g2(D). Assume that g2 is an \u03b1-error detector. Without loss of generality, we will assume |X| = K \u2208N and set X = {1, . . . , K}. The case |X| = \u221ewill follow by letting K \u2192\u221e. Choose P0 = U(X), the uniform distribution on X = {1, . . . , K}. For an arbitrary, vector y = (y1, y2, . . . , yM) \u2208X M, let Qy be the discrete uniform distribution on the elements of y. Note that this is only the uniform distribution on the set {ym : m = 1, . . . , M} if all components of y are different. Clearly, we have TV(P0, Qy) \u22651 \u2212M K . Thus, by choosing M \u2264\u03b2K it is ensured that TV(P0, Qy) \u22651 \u2212\u03b2.\n(4) (5)\n(5)\n(10) (11) (12) (13) (14) (15)\n(10)\n(15)\n(16)\n\n(6)\nLet Y = (Y1, Y2, . . . YM) be a random vector with M elements, each drawn i.i.d. according to Ym \u223cP0. We now draw another random vector Z with N elements Z = {Zn}n=1,2,...,N according to Zn = (1 \u2212Gn)X(0) n + GnYVn, where Vn \u223cU({1, 2, . . . , M}) and Gn \u223cB(\u03b3) are all independently drawn for n = 1, 2, . . . , N. Thus, Vn is uniformly drawn from {1, 2, . . . , M} and Gn satisfies Pr{Gn = 1} = \u03b3 and Pr{Gn = 0} = 1 \u2212\u03b3. We note the following two facts about this construction: 1. The marginal distribution of every Zn \u2208Z is P0, but the selection is non-i.i.d. as Zn and Zn\u2032 depend on each other through Y. However, when conditioning on the fact that all components of V = (V1, V2, . . . , VN) are pairwise distinct, then the random variables YVn and YVn\u2032 are independent for n \u0338= n\u2032 and thus Z is a vector of i.i.d. variables distributed according to P0. 2. When conditioning on Y = y, we have a different situation, where Zn \u223c(1 \u2212\u03b3)P0 + \u03b3Qy are i.i.d., and by choosing M \u2264\u03b2K, we have (P0, Qy) \u2208P. Let |V| = |{V1, V2, . . . , VN}| = N be the event that V contains pairwise distinct elements, i.e., no repetitions occur. Using the first fact above, we calculate\nwhere we used the union bound as well as the inequality log(1 + x) \u2265 x 1+x. Using the second fact from above, we condition on Y = y and then have Z i.i.d. according to P1 = (1 \u2212\u03b3)P0 + \u03b3Pb for a valid backdoor distribution Pb = Qy. We then write\n(17)\n(18)\n(19)\n(20)\n(21) (22) (23) (24) (25) (26)\n(21)\n(23)\nand thus\nThis already resolves the case |X| = \u221eas we can then let K \u2192\u221eand M = \u230a\u03b2K\u230b\u2192\u221e showing that \u03b1 = 1 2 for |X| = \u221e. On the other hand, for |X| < \u221e, we choose K = |X|, M = \u230a\u03b2K\u230band obtain (6) by\nTheorem 2. Considering the backdoor detection setup of Definition 1 with P = {(P0, Pb) : TV(P0, Pb) \u22651 \u2212\u03b2} and a finite alphabet |X| < \u221e. If\nIn the proof of this theorem, the auxiliary Lemmas 2 and 3 are used, which are provided in Appendix B. Proof of Theorem 2. In the following we will show that the detector\nis \u03b1-error if (7) is satisfied. Here, the distribution SN is the so-called type of D, i.e.,\n(27)\n(28)\n(29)\n(30)\n(31)\n(7)\n(36)\n(37)\nwhere for any x \u2208X, 1x(Xn) is the indicator function that takes value 1 if Xn = x and 0 otherwise. In Lemma 3 it is shown that the type SN is close to the true distribution P with high probability. We can now analyze the error probability of the detector (36) for P = P1, i.e.,\nwhere we used Lemma 3 in (42) and the fact that TV(P0, P1) = \u03b3 TV(P0, Pb) \u2265(1 \u2212\u03b2)\u03b3 by Lemma 2 in (39) and (41). Similarly, we obtain that the error probability for j = 0 is upper bounded by the same expression\napplying Lemma 3 in (44).\nThus, we have shown that g, as defined in (36), is \u03b1-error, provided that (7) holds.\nTheorem 3. Consider the backdoor detection setup of Definition 1, with fixed \u03b3 \u2208(0, 1], N \u2208N and some set of possible distributions P. Let P\u2032 be the set of N-fold products of (P0, P1), i.e., P\u2032 = {(P N 0 , (\u03b3Pb + (1 \u2212\u03b3)P0)N) : (P0, Pb) \u2208P}. Then, OOD-detection is PAC-learnable on P\u2032 if and only if the following holds for any \u03f5 > 0 and any Type 3 detector g3(D, P0, Pb): We can find M \u2208N and a Type 1 detector g1(D, D\u2032), which satisfies R(g1, P0, Pb) \u2264R(g3, P0, Pb) + \u03f5 for every (P0, Pb) \u2208P. In the proof of this theorem, the auxiliary Lemma 4 is used, which is provided in Appendix B Proof of Theorem 3. Assume first that OOD-detection is PAC-learnable on P\u2032, fix \u03f5 > 0 and let g3 be any Type 3 detector. By Lemma 4, we know that there is a Type 1 detector gM 1 with some M such that \u03f5(M) \u2264\u03f5, satisfying (82). Noting that 1 2 \u22121 2 TV(P N 0 , P N 1 ) \u2264R(g3, P0, Pb) by Lemma 1 completes this part of the proof.\nTheorem 3. Consider the backdoor detection setup of Definition 1, with fixed \u03b3 \u2208(0, 1], N \u2208N and some set of possible distributions P. Let P\u2032 be the set of N-fold products of (P0, P1), i.e., P\u2032 = {(P N 0 , (\u03b3Pb + (1 \u2212\u03b3)P0)N) : (P0, Pb) \u2208P}. Then, OOD-detection is PAC-learnable on P if and only if the following holds for any \u03f5 > 0 and any Type 3 detector g3(D, P0, Pb): We can find M \u2208N and a Type 1 detector g1(D, D\u2032), which satisfies R(g1, P0, Pb) \u2264R(g3, P0, Pb) + \u03f5 for every (P0, Pb) \u2208P.\nProof of Theorem 3. Assume first that OOD-detection is PAC-learnable on P\u2032, fix \u03f5 > 0 and let g3 be any Type 3 detector. By Lemma 4, we know that there is a Type 1 detector gM 1 with some M such that \u03f5(M) \u2264\u03f5, satisfying (82). Noting that 1 2 \u22121 2 TV(P N 0 , P N 1 ) \u2264R(g3, P0, Pb) by Lemma 1 completes this part of the proof. On the other hand, let g3 be the Type 3 Neyman-Pearson detector that satisfies R(g3, P0, Pb) = 1 2 \u22121 2 TV(P N 0 , P N 1 ), which exists by Lemma 1. By our assumptions, for any k \u2208N, we set \u03f5 = 1 k and find a Type 1 detector \u02c6gk 1, operating on D with size M = m(k) satisfying\nWe can find a monotonically increasing sequence kM for M = 1, 2, . . . with limM\u2192\u221ekM = \u221e, that satisfies m(kM) \u2264M. Using the sequence of Type 1 detectors6 gM 1 (D, D\u2032) = \u02c6gkM 1 (D, [D\u2032]m(kM 1\n(38) (39) (40) (41) (42)\n(38)\n(39)\n(41)\n(42)\n(43)\n(44)\n\n(45)\nThis completes the proof as limm\u2192\u221e\u03f5(m) = limm\u2192\u221e 1 km = 0 and thus, PAC learnability is guaranteed by Lemma 4. Corollary 2. Let g\u2032 2(D(J), P0, X(I)) be a Type 2 detector for an SBD problem, where we have r = min{PJI(0, 0), PJI(1, 1)} > 0 and P = {(P0, Pb) : TV(P0, Pb) \u22651 \u2212\u03b2}. Then, if g\u2032 2 is \u03b1-error, we have \u03b1 \u2265r if |X| = \u221e, and for |X| < \u221e, we obtain\nProof of Corollary 2. Assuming that this detector is \u03b1-error implies \u03b1 \u2265R(g\u2032 2, P0, Pb) \u2265PJI(0, 0) Pr{g\u2032 2(Q\u2032 2) \u0338= 0|J = I = 0}\n\ufffd \ufffd Now consider the MBD problem with \u03b3 = 1 and the training set size N\u2032 = N + 1. We can define a Type 2 detector7 g2(D, P0) = g\u2032 2(D, P0, XN\u2032) with risk R(g2, P0, Pb) = 1 2 Pr{g\u2032 2(Q\u2032 2) \u0338= 0|J = I = 0} + 1 2 Pr{g\u2032 2(Q\u2032 2) \u0338= 1|J = I = 1} (50) \u22641 2r\u03b1. (51)\n\ufffd \ufffd Now consider the MBD problem with \u03b3 = 1 and the training set size N\u2032 = N + 1. We can fine a Type 2 detector7 g2(D, P0) = g\u2032 2(D, P0, XN\u2032) with risk\nFrom Theorem 1, we now know that 1 2r\u03b1 \u22651 2 if X = N and obtain (9) for |X| < \u221e.\n# B. Auxiliary Results\nThis appendix contains auxiliary results, which are utilized in the proofs provided in Appendix A. Lemma 2 (Properties of Total Variation). The total variation between two probability distributions P0, P1 \u2208P(X), is given by TV(P0, P1) = \u2225P0 \u2212P1\u2225TV := sup A |P0(A) \u2212P1(A)|, (52)\nThis appendix contains auxiliary results, which are utilized in the proofs provided in Appendix A Lemma 2 (Properties of Total Variation). The total variation between two probability distributions P0, P1 \u2208P(X), is given by\nwhere the supremum is over all measurable sets A \u2286X. We then have \u2225P0 \u2212P1\u2225TV = 2 inf X0,X1:PX0=P0,PX1=P1 Pr{X0 \u0338= X1}\nwhere the infimum is over all random variables X0, X1 on X, such that the marginal distributions satisfy PX0 = P0, PX1 = P1. For P \u2032 0, P \u2032 1 \u2208P(Y), we have \u2225P0 \u2212P1\u2225TV \u2264\u2225P0 \u00d7 P \u2032 0 \u2212P1 \u00d7 P \u2032 1\u2225TV \u2264\u2225P0 \u2212P1\u2225TV + \u2225P \u2032 0 \u2212P \u2032 1\u2225TV. (54) and thus \u2225P0 \u2212P1\u2225TV \u2264\u2225P N 0 \u2212P N 1 \u2225TV \u2264N\u2225P0 \u2212P1\u2225TV. Furthermore, for \u03b3 \u2208[0, 1], \u2225P0 \u2212(1 \u2212\u03b3)P0 \u2212\u03b3P1\u2225TV = \u03b3\u2225P0 \u2212P1\u2225TV (55)\n(46) (47)\n(47)\n(9)\n(48) (49)\n(50) (51)\n(52)\n(53)\n(54)\n(55)\nTo show the second inequality in (54), we use (53) and for an arbitrary \u03b5 > 0, choose (X0, X1) \u22a5(Y0, Y1) such that PX0 = P0, PX1 = P1, PY0 = P \u2032 0, PY1 = P \u2032 1, and\n\u2225P0 \u2212P1\u2225TV + \u03b5 \u22652 Pr{X0 \u0338= X1}, \u2225P \u2032 0 \u2212P \u2032 1\u2225TV + \u03b5 \u22652 Pr{Y0 \u0338= Y1}.\nClearly PX0,Y0 = P0 \u00d7 P \u2032 0 as well as PX1,Y1 = P1 \u00d7 P \u2032 1 and thus by (53),\nAs \u03b5 > 0 was arbitrary, this proves (54). To show (55), we use (52) and have\n\u2225P0 \u2212(1 \u2212\u03b3)P0 \u2212\u03b3P1\u2225TV = sup A |P0(A) \u2212(1 \u2212\u03b3)P0(A) \u2212\u03b3P1\nLemma 3. Let SN be the type of X = (X1, X2, . . . , XN), distributed according to P N. For any t \u2208[0, 1], we then have the bound\nProof. By using the Hoeffding\u2019s inequality we can bound the probability of the deviation of SN from its expected value. In particular, we have that\nPr {|SN(x) \u2212P(x)| \u2265t} = Pr {|SN(x) \u2212E[SN(x)]| \u2265t} \ufffd \ufffd\n\ufffd where we note that E[SN(x)] = 1 N \ufffdN n=1 E[1x[Xn]] = P(x).\n\ufffd where we note that E[SN(x)] = 1 N \ufffdN n=1 E[1x[Xn]] = P(x).\n(56) (57) (58)\n(59) (60)\n(61) (62) (63)\n(64) (65) (66)\n(67)\n(68) (69)\n(71)\nThe next and final step is to extend the bound to the whole alphabet X. In order to do so we define the event Ax = {|SN(x) \u2212P(x)| \u2265t}. We want to bound the probability of the even\nThe next and final step is to extend the bound to the whole alpha we define the event Ax = {|SN(x) \u2212P(x)| \u2265t}. We want to bound th\n the union bound we obtain\nBy applying the union bound we obtain\n\ufffd \ufffd Let us consider the event A = {\u2203x \u2208X : |SN(x) \u2212P(x)| \u2265t}: this is the error event, i.e., the divergence between the observed samples frequency and its expected value diverges more than a given value t > 0 for at least one x \u2208X. The complement of this event is the event that the divergence is less than t for all x \u2208X, i.e., the event that the observed frequency is close to the expected value for all x \u2208X. This can be written as\nNow, Ac implies that\nwhere t\u2032 = 1 2t|X|. Thus, Pr Ac \u2264Pr{TV(SN, P) < t\u2032} and therefore Pr{TV(SN, P) \u2265t\u2032} \u2264Pr A \u22642|X| exp \ufffd \u22122Nt2\ufffd\nLemma 4. Given P and N \u2208N and letting \u03b3 \u2208(0, 1], OOD-detection is PAC-learnable on P\u2032 = {(P N 0 , P N 1 ) : (P0, Pb) \u2208P} with P1 = (1 \u2212\u03b3)P0 + \u03b3Pb if and only if the following holds: For the MBD problem, there exists a sequence of Type 1 backdoor detectors gM 1 (D, D\u2032) for M = 1, 2, . . . and a decreasing sequence \u03f5(m) with limm\u2192\u221e\u03f5(m) = 0 such that for any M \u2208N and any pair (P0, Pb) \u2208P, we have\nProof. Assume that OOD-detection is PAC-learnable on P\u2032. By definition we can find a function G : \ufffd\u221e m=1 X Nm \u2192{0, 1}X N and a monotonically decreasing sequence \u03f5\u2032(m) that tends to zer and satisfies for every (P0, Pb) \u2208P, m \u2208N, that E[ \u00afR(G(D\u2032), P N 0 , P N 1 )] \u2212inf f \u00afR(f, P N 0 , P N 1 ) \u2264\u03f5\u2032(m), (83\n(72)\n(73) (74) (75) (76)\n(73)\n(74)\n(76)\n(77)\n(78)\n(80)\n(81)\n(82)\n(83)\nwhere we set the size of D\u2032 to be M = mN and the infimum is over all functions f : X N \u2192{0, 1}. For any M \u2208N, we define8 gM 1 (D, D\u2032) := G([D\u2032]mN 1 )(D) as well as \u03f5(M) = \u03f5\u2032(m), where m is the largest integer such that mN \u2264M. Notice that R(gM 1 , P0, Pb) = E[ \u00afR(G([D\u2032]mN 1 ), P N 0 , P N 1 )] and that inff \u00afR(f, P N 0 , P N 1 ) = 1 2 \u22121 2 TV(P N 0 , P N 1 ) by Lemma 1. We thus obtain from (83), that for any M \u2208N,\nNoting that \u03f5(M) approaches zero completes this part of the proof. On the other hand, assume that gM 1 (D, D\u2032) and \u03f5(M) satisfy the requirement (82). For any m \u2208N, we then set M = mN and define G(D\u2032)(D) := gM 1 (D, D\u2032) as well as \u03f5\u2032(m) = \u03f5(M). We can now rewrite (82) using E[ \u00afR(G(D\u2032), P N 0 , P N 1 )] = R(gM 1 , P0, Pb) and Lemma 1 to obtain\nThus, we have shown that the algorithm G and the sequence \u03f5\u2032 satisfy Definition 2 and OODdetection is PAC-learnable on P\u2032.\n# C. Additional Results\nLemma 5. Let gl be a detector as listed in Section 2 with input Ql for l \u2208{0, 1, 2}, where we set g0 = g and Q0 = Q. If gl is \u03b1-error in the sense of Definition 1, then for m \u2208{1, 2, 3} and m > l we can find a backdoor detector gm with input Qm that is also \u03b1-error. Proof. It is sufficient to show the lemma for m = l + 1. The claim then follows by applying the result repeatedly. In the case l = 2 (and m = 3) we obtain g3 with R(g3, P0, Pb) = R(g2, P0, Pb) by g3(D, P0, Pb) = g2(D, P0). For l = 1, we can define the randomized detector g2(D, x, P0) to first draw M i.i.d. samples D\u2032 \u223cP M 0 and then yield g2(D, P0) = g1(D, D\u2032). Finally, for l = 0 we obtain g1 with equal risk by defining g1(D, D\u2032) = g(A(D), D\u2032).\n(84) (85)\n(86)\n(86) (87)\n(87)\n\n\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of backdoor detection in machine learning systems, which poses significant security risks due to the increasing use of pre-trained models that may have been compromised during training. The authors highlight the challenges in ensuring safe and trustworthy ML systems amidst various adversarial threats, particularly focusing on backdoor attacks where malicious modifications to training data lead to erroneous model predictions.",
        "problem": {
            "definition": "The problem of backdoor detection involves identifying whether a machine learning model has been compromised through backdoor attacks, where specific inputs trigger erroneous outputs.",
            "key obstacle": "The main challenge is the lack of universally effective detection methods that can identify backdoors without prior knowledge of the attacker's strategy, particularly in scenarios with infinite alphabet sizes."
        },
        "idea": {
            "intuition": "The idea stems from the observation that existing backdoor detection methods often fail due to the adversarial nature of attacks, prompting the need for a formal statistical framework for analysis.",
            "opinion": "The authors propose that backdoor detection needs to be adversary-aware to be effective, challenging the notion that universal detection methods can succeed.",
            "innovation": "The primary innovation is the formal statistical definition of the Model Backdoor Detection (MBD) and Sample Backdoor Detection (SBD) problems, which connects these issues to standard statistical hypothesis testing."
        },
        "Theory": {
            "perspective": "The theoretical framework presented connects backdoor detection to established statistical theories, particularly focusing on the impossibility of universal detection in certain scenarios.",
            "opinion": "The authors argue that without specific assumptions about the adversary or the data distribution, effective backdoor detection is not feasible.",
            "proof": "The paper provides a no-free-lunch theorem demonstrating that universal backdoor detection is impossible for infinite alphabet sizes and establishes bounds for finite sizes."
        },
        "experiments": {
            "evaluation setting": "The evaluation involves various datasets, including commonly used ones in machine learning, to assess the effectiveness of backdoor detectors against known backdoor attacks.",
            "evaluation method": "The evaluation method includes theoretical analysis and proofs demonstrating the impossibility of certain detection scenarios, supplemented by illustrative examples."
        },
        "conclusion": "The paper concludes that universal backdoor detection methods are fundamentally flawed and emphasizes the necessity for adversary-aware detection strategies to enhance security in machine learning systems.",
        "discussion": {
            "advantage": "The formal statistical formulation provides a clearer understanding of the limitations and challenges in backdoor detection, guiding future research directions.",
            "limitation": "The main limitation is the focus on theoretical results, which may not directly translate to practical detection methods in real-world scenarios.",
            "future work": "Future work should explore the development of adversary-aware detection techniques and investigate specific scenarios where backdoor detection can be feasible."
        },
        "other info": [
            {
                "info1": "The authors connect their findings to the Probably Approximately Correct (PAC) learnability framework, suggesting implications for out-of-distribution detection."
            },
            {
                "info2": {
                    "info2.1": "The paper includes a detailed analysis of the statistical properties relevant to backdoor detection, enhancing the theoretical foundation of the research.",
                    "info2.2": "The authors provide a comprehensive review of related works on backdoor attacks and defenses, situating their contributions within the broader context of machine learning security."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of backdoor detection in machine learning systems, which poses significant security risks due to the increasing use of pre-trained models that may have been compromised during training."
        },
        {
            "section number": "1.2",
            "key information": "The authors highlight the challenges in ensuring safe and trustworthy ML systems amidst various adversarial threats, particularly focusing on backdoor attacks where malicious modifications to training data lead to erroneous model predictions."
        },
        {
            "section number": "2.1",
            "key information": "The problem of backdoor detection involves identifying whether a machine learning model has been compromised through backdoor attacks, where specific inputs trigger erroneous outputs."
        },
        {
            "section number": "2.5",
            "key information": "The authors connect their findings to the Probably Approximately Correct (PAC) learnability framework, suggesting implications for out-of-distribution detection."
        },
        {
            "section number": "3.5",
            "key information": "The primary innovation is the formal statistical definition of the Model Backdoor Detection (MBD) and Sample Backdoor Detection (SBD) problems, which connects these issues to standard statistical hypothesis testing."
        },
        {
            "section number": "7.1",
            "key information": "The main challenge is the lack of universally effective detection methods that can identify backdoors without prior knowledge of the attacker's strategy, particularly in scenarios with infinite alphabet sizes."
        },
        {
            "section number": "7.2",
            "key information": "Future work should explore the development of adversary-aware detection techniques and investigate specific scenarios where backdoor detection can be feasible."
        }
    ],
    "similarity_score": 0.6407735566239788,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem.json"
}