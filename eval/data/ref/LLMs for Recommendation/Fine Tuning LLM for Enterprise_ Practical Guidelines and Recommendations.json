{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.10779",
    "title": "Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations",
    "abstract": "There is a compelling necessity from enterprises for fine tuning LLMs (Large Language Models) to get them trained on proprietary domain knowledge. The challenge is to imbibe the LLMs with domain specific knowledge using the most optimial resource and cost and in the best possible time. Many enterprises rely on RAG (Retrieval Augmented Generation) which does not need LLMs to be fine-tuned but they are limited by the quality of vector databases and their retrieval capabilities rather than the intrinsic capabilities of the LLMs themselves. In our current work we focus on fine tuning LLaMA, an open source LLM using proprietary documents and code from an enterprise repository and use the fine tuned models to evaluate the quality of responses. As part of this work, we aim to guide beginners on how to start with fine tuning an LLM for documentation and code by making educated guesses on size of GPU required and options that are available for formatting the data. We also propose pre processing recipes for both documentation and code to prepare dataset in different formats. The proposed methods of data preparation for document datasets are forming paragraph chunks, forming question and answer pairs and forming keyword and paragraph chunk pairs. For code dataset we propose forming summary and function pairs. Further, we qualitatively evaluate the results of the models for domain specific queries. Finally, we also propose practical guidelines and recommendations for fine tuning LLMs.",
    "bib_name": "j2024finetuningllmenterprise",
    "md_text": "FINE TUNING LLMS FOR ENTERPRISE: PRACTICAL GUIDELINES AND RECOMMENDATIONS\nMathav Raj J\nHCLTech\nBengaluru\nmathavraj.j@hcl.com\nKushala VM\nHCLTech\nBengaluru\nkushala.vm@hcl.com\nHarikrishna Warrier\nHCLTech\nBengaluru\nharikrishna.w@hcl.com\nYogesh Gupta\nHCLTech\nBengaluru\nyogeshg@hcl.com\ncs.SE]  23 Mar 2024\n23 Mar 202\n# ABSTRACT\nThere is a compelling necessity from enterprises for fine tuning LLMs (Large Language Models) to get them trained on proprietary domain knowledge. The challenge is to imbibe the LLMs with domain specific knowledge using the most optimial resource and cost and in the best possible time. Many enterprises rely on RAG (Retrieval Augmented Generation) which does not need LLMs to be fine-tuned but they are limited by the quality of vector databases and their retrieval capabilities rather than the intrinsic capabilities of the LLMs themselves. In our current work we focus on fine tuning LLaMA, an open source LLM using proprietary documents and code from an enterprise repository and use the fine tuned models to evaluate the quality of responses. As part of this work, we aim to guide beginners on how to start with fine tuning an LLM for documentation and code by making educated guesses on size of GPU required and options that are available for formatting the data. We also propose pre processing recipes for both documentation and code to prepare dataset in different formats. The proposed methods of data preparation for document datasets are forming paragraph chunks, forming question and answer pairs and forming keyword and paragraph chunk pairs. For code dataset we propose forming summary and function pairs. Further, we qualitatively evaluate the results of the models for domain specific queries. Finally, we also propose practical guidelines and recommendations for fine tuning LLMs.\narXiv:2404.10779v1\narXiv:2404.1\n# 1 Introduction\nThe advent of LLMs has revolutionised natural language processing. Applications are varying from language translation [1], content creation [2] and to emotional support chatbots [3]. LLMs like LLaMA have been trained on trillions of tokens[4] from various resources. To adapt a general purpose LLM for one of these specific tasks, it has to be trained on task oriented dataset. This additional training allows the model to fine tune its parameters to the task or domain we are interested in. Models like FinGPT for finance domain [5], PMC-LLaMA for medical domain [6] are fine tuned on particular domain datasets to achieve improved accuracy on domain related questions. Domain specific LLMs can be helpful in scenarios such as support ticket resolution, querying document base or code repository to adapt into new system etc. Though there is an option to use OpenAI models to solve most of the use-cases, there is a high demand for domain specific LLMs due to data privacy and pricing concerns. The stake holder\u2019s dataset can stay on premise as the LLMs are also present on premise. Fine-tuned LLMs provide quality and custom feel to the stake holder and also has low latency in displaying the results. This paper aims to enable a beginner in preparing the data for fine tuning, estimating the compute capability and memory needed, choosing the right dataset format and the optimal configurations for fine tuning. The paper is arranged as follows: \u2022 Research Background: A short survey on related research work on fine tuning vs RAG, fine tuning guidelines, efficient techniques for fine tuning and preparation of datasets for fine tuning \u2022 Fine tuning configurations: Before starting the fine tuning process it is necessary to understand what configurations can be adjusted to run on available resources.\n\u2022 Proposed Dataset formats for Text and Code: The overall workflow of the proprietary data fine tuning is detailed in this section. The different proposed formats of text and code data has been explained in detail\n\u2022 Proposed Dataset formats for Text and Code: The overall workflow of the proprietary data fine tuning is detailed in this section. The different proposed formats of text and code data has been explained in detail \u2022 Experiments: A proprietary document and code repository is used to showcase the fine tuning work flow. Empirical studies have been conducted to understand the effects of quantization on time and memory, to understand the selection of appropriate rank in LORA (Low Rank Adapater) fine tuning, understand the memory requirements for full model fine tuning and to understand the effects of fine tuned model in a Retrieval Augmented Generation (RAG) pipeline \u2022 Guidelines: In the final section, practical guidelines for fine tuning has been given. Some tips to choosing right parameters for fine tuning efficient techniques like LORA is also listed.\n\u2022 Experiments: A proprietary document and code repository is used to showcase the fine tuning work flow. Empirical studies have been conducted to understand the effects of quantization on time and memory, to understand the selection of appropriate rank in LORA (Low Rank Adapater) fine tuning, understand the memory requirements for full model fine tuning and to understand the effects of fine tuned model in a Retrieval Augmented Generation (RAG) pipeline\n\u2022 Experiments: A proprietary document and code repository is used to showcase the fine tuning work flow. Empirical studies have been conducted to understand the effects of quantization on time and memory, to understand the selection of appropriate rank in LORA (Low Rank Adapater) fine tuning, understand the memory requirements for full model fine tuning and to understand the effects of fine tuned model in a Retrieval Augmented Generation (RAG) pipeline \u2022 Guidelines: In the final section, practical guidelines for fine tuning has been given. Some tips to choosing right parameters for fine tuning efficient techniques like LORA is also listed.\n\u2022 Guidelines: In the final section, practical guidelines for fine tuning has been given. Some tips to choosing rig parameters for fine tuning efficient techniques like LORA is also listed.\n# 2 Research Background\nThe authors in [7] give an overall view of the recent trends in LLMs. LLMs can be trained in different phases. The first phase being the pretraining with defined objectives such as causal language modelling, masked language modelling, span denoising objective etc. Then comes the transfer learning phase which is further classified as feature based transfer and finetuning approach. Transfer learning is required when the dataset is inadequate for a complete training from scratch. Therefore pretrained weights are used as starting point. In feature based transfer learning features obtained from the pretrained model for the given domain dataset are used by another smaller model to train. Meanwhile finetuning on a dataset is to nudge the pretrained weights on a particular task oriented dataset. Depending on the how many layers are fine tuned and how prompts are handled during finetuning, it is further classified as adapter tuning, gradual unfreezing, prompt tuning etc. An alternate approach to fine tuning is to chunk the documents, convert to embeddings and store it in a vector database for retrieval using similarity search with the query and use the pretrained LLM to come up with a consolidated answer from the retrieved documents. [8]. This is the production ready approach as it is fast and gives more or less exact results. However the RAG approach can be crippled by a not so good retrieval mechanism. Though there are simple alleviations like the claim in [9] that information retrieval in RAG has improved by purposeful addition of noisy documents, the quality of answers are limited by the similarity search which has nothing to do with the LLM capability itself. A study by [10] shows the comparison of between finetuning and RAG. The experiments in the paper reveal that finetuning on a domain data extracted from agriculture journals have given more succinct and accurate responses than a RAG pipeline. That being said, the authors also have ackowledged the high initial cost required to fine tune a model. Low Rank Adaptation (LORA) finetuning of LLMs [11] has opened up a whole new possibility of finetuning limited number of essential parameters usually of the order of few thousands to a millions instead of the entire parameters which is in the order of billions. The work on quantizing LLMs has opened up avenues for resource deficient systems to train at low memory cost [12]. Papers on quantized LLMs in combination with parameter efficient techniques like LORA have further enabled obtaining satisfactory results with low resources[13]. Code generation with LLM is the most attractive task engineers are looking at. Even though LLM are already trained with lots of data which makes them to generate code depending on the input, the challenge is generating code about a specific enterprise domain code. LLM fine-tuning for a specific task makes the model utilize its capacity to fullest by making it adapt to a domain by making the model familiar to jargons, domain terminology by understanding the context of the code, class, functions, exceptions, libraries etc., Fine-tuning also helps in adapting the model to address the task specific problems. Fine-tuning large language models for code-related tasks presents a myriad set of challenges that must be carefully addressed to ensure optimal performance and reliability. The challenges encompass aspects such as data quality and quantity, domain-specific understanding, tokenization and vocabulary, contextual understanding, code generation quality, code understanding vs. generation, model size and computational resources, overfitting and generalization, evaluation metrics, and ethical and security concerns.\n# 3 Fine Tuning LLMs on Available Resources 3.1 Quantization\n# 3 Fine Tuning LLMs on Available Resources\n# 3.1 Quantization\nBy default, most open source LLM weights are released in full 32 bit floating point precision. Even for fine tuning a model of relatively smaller size say 7 billion parameters, nearly 28 GB space is required. With higher precision weights, compute units have to spend higher energy in memory movement operations during fine tuning [14]. Quantization is the process of constraining an input from continuous set of values to a discrete set. Quantizing the model weights to a lower precision and fine tuning greatly reduces the size without hampering the quality [12]. Data is stored in different numeric types namely: FP32, FP16, int8, FP8, BF16. Integers can be represented in unsigned or signed format or 2\u2019s complement form. To represent a fractional decimal we could go for a fixed point representation.\nTo further extend the range, systems use the IEEE 754 floating point representation which has a much larger range since the numbers are expressed in exponents of 2. The 32 bits of floating point representation has three parts namely 1 sign bit, 8 bits for exponent (both positive and negative) and 23 bits for mantissa or significant figures. The width of the exponent bits determines the range of numbers and the width of the mantissa bits determines the precision of numbers. Based on the widths there are different forms. Different forms may be needed based on the availability of memory resources. FP16 reduces both the range and precision by using 5 bits for exponent and 10 bits for mantissa. Brain float 16[15] or BF16 maintains the same range as FP32 but reduction of precision to 7 bits. Floating point 16 or FP16 enables training of larger models or training with larger mini-batches. With neural networks, quantization can be done with the weights during storage and activation during the computation. Quantization (QAT) and post training quantization (PTQ). Integer quantization favours memory reduction and thereby energy and cost reduction during inference [14]. In a QAT scheme proposed by Jacob et al. [16], the quantization errors are included in the computation graph during fine tuning so that the model\u2019s inference performance can be as if it were never quantized. This allows for deployment of models in edge hardware that support only integer arithmetic. Tim Dettmers et al. propose a new type of integer quantization scheme called LLM int8 [12]. This has been implemented in the python library \u2019bitsandbytes\u2019. To explain in a more detailed manner, quantization is a two step process that involves i) Finding the normalization constant and scale the vector into target range ii) Rounding off to the nearest value in target range. During matrix multiplication of tensors, quantization of weights with outliers will lead to huge quantization loss. To mitigate this, bitsandbytes employs a combination of vector wise quantization and mixed precision decomposition to achieve a performance similar to that without quantization. Though LLM int8 quantization does not degrade performance, the inference time gets increased due to the overhead of quantization [17]. However the memory gets reduced drastically by 71%, which is major cost saving when choosing cloud premise GPUs.\n# 3.2 Gradient Accumulation\nOther than quantization, techniques like gradient accumulation help in reducing the memory requirement during fine tuning. The process of accumulating gradients in the context of backpropagation involves a strategic approach to parameter updates within a neural network during the training phase. Unlike the conventional method where parameters are updated after processing each mini-batch, the accumulation of gradients entails deferring the parameter updates until all the instances in a mini-batch have been processed. In the standard back propagation algorithm, the gradients computed for each instance in a mini-batch are typically used to immediately update the model parameters. However, in the case of accumulated gradients, these individual gradients are not immediately applied to the parameters. Instead, they are summed or averaged over the entire mini-batch. As each instance in the mini-batch undergoes forward and backward passes, the gradients with respect to the model parameters are computed but not immediately applied. These gradients are stored, and the accumulation occurs over the entire mini-batch. Only when all instances in the mini-batch have been processed, the accumulated gradients are employed to update the model parameters. This aggregated update is akin to the effect of utilizing a higher batch size for training the neural network.\nOther than quantization, techniques like gradient accumulation help in reducing the memory requirement during fine tuning. The process of accumulating gradients in the context of backpropagation involves a strategic approach to parameter updates within a neural network during the training phase. Unlike the conventional method where parameters are updated after processing each mini-batch, the accumulation of gradients entails deferring the parameter updates until all the instances in a mini-batch have been processed.\nIn the standard back propagation algorithm, the gradients computed for each instance in a mini-batch are typically used to immediately update the model parameters. However, in the case of accumulated gradients, these individual gradients are not immediately applied to the parameters. Instead, they are summed or averaged over the entire mini-batch. As each instance in the mini-batch undergoes forward and backward passes, the gradients with respect to the model parameters are computed but not immediately applied. These gradients are stored, and the accumulation occurs over the entire mini-batch. Only when all instances in the mini-batch have been processed, the accumulated gradients are employed to update the model parameters. This aggregated update is akin to the effect of utilizing a higher batch size for training the neural network.\n# 3.3 PEFT (Parameter Efficient Fine Tuning)\nLarge language models get more efficient with transfer learning through fine tuning. But on the other hand, fine tuning becomes challenging with respect to the infrastructure needed, time required and overall memory needs. To overcome these challenges, parameter efficient fine tuning comes into picture. Parameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained model\u2019s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task. There are many methods in PEFT training: Adapter, LoRA, QLoRA, Prefix tuning, Prompt tuning, P-tuning, and IA3. In this paper, we will be delving on LoRA and QLoRA methodology of training. LoRA, [11] a method for fine-tuning large language models, operates by integrating small trainable submodules alongside feed-forward layers in the pre-trained transformer architecture. These LoRA modules employ rank decomposition to significantly reduce the number of trainable parameters while maintaining or even enhancing model performance across various tasks. Specifically, LoRA inserts two feed-forward layers adjacent to each feed-forward layer in the transformer model, where the first layer projects the input into a lower-dimensional space and the second layer restores it to the original dimensionality. This incremental change, represented as delta h, is added to the original hidden representation, resulting in an updated representation h\u2019. Through this approach, task-specific parameters are minimized, facilitating efficient task-switching and reducing hardware requirements without introducing additional inference latency.\nQLoRA, or quantized LORA, is an optimized version of LoRA, where the precision of the weight parameters are reduced to 4 bit precision. Since QLoRA shrinks the model size due to the reduced precision, it is helpful in scenarios where there is limited memory to fine tune.\n# 4 Fine Tuning Workflow\nThe workflow for fine tuning an LLM, be it for text or code, can be represented as in Figure 1 below.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e985/e9858dec-c5fb-4487-acda-db5d18e9b88b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Fine tuning Workflow (with LLaMA model as an example)</div>\nUnstructured text or code is fed to the first stage of fine tuning, the pre-processing recipe. Based on the recipe, it chunks the data into logical parts and then it is tokenized (and padded where needed) so that it accomodates the supported sequence lenght of the model. Further, the LoRA / QLoRA configurations are applied and fine tuning is done till the loss is minimized. This is the standard process of fine tuning. Thus, we can see that the key step in fine tuning is the data pre-processing, and the quality of fine tuning is primarily dependent on that. So, we will describe that in more detail below.\n# 4.1 Text Data Pre-Processing\nThis step plays a vital role in fine tuning process. From scratch, a Large language model is trained to do next token prediction. The datasets are massive text corpus like Common crawl, The Pile and code repositories from Github. Further the model can be fine tuned for specific tasks using specialised datasets. One such series of datasets are called Instruction datasets like Dolly, Orca, Alpaca and Vicuna. All these datasets enable instruction fine tuning. Instruction fine tuning bridges the gap between next token prediction and user\u2019s requirement of following instruction prompts[18]. An instruction dataset consists of instruction prompt, response paired with or without context for additional information. The challenge is how to create an instruction dataset from a general document. As part of our experiments, we developed fine tuning datasets pre-processing recipes of four different formats namely a) raw data b) keywords as instruction c) headings as instruction and d) queries as instruction. For a decoder model like LLaMA 2, the raw format is essentially continuing the initial training objective of unsupervised next token prediction with raw text. In the raw data method, we are passing the document as raw chunks with no template. In the keywords method, we are passing the chunks in a template with prompt being keywords extracted from the chunk. For this, we used Rapid Automatic Keyword Extraction (RAKE) algorithm (the corresponding python library being rake-nltk[19]). Another method is to use the document headings with different headings levels as prompt. As an example, in cases of Microsoft Word docx documents, the document parser from llm search[20] is used. The last methods that we used is a query based approach. This is particularly usefuly when a user is interested in querying for information about the document from the LLM, hence it is ideal if the prompt is also having queries. The\nsame model is used to generate possible queries that can be asked about a chunk and then the dataset is prepared. A single chunk can have multiple queries to promote diversity in the dataset. When data set is not in a paired format, the challenge is how to structure the data and let the model understand the given text. As a baseline, we split the text into chunks of fixed length and trained the model with those chunks. The objective of fine tuning the model is next token prediction. As a first step the model encodes the splitted chunks into embeddings. LLaMA tokenizer is a Byte Pair Encoding tokenizer model based on sentencepiece. Once the words are tokenized, embedded and padded to appropriate fixed sequence length, the data is ready to be fed to model for fine tuning. In Fine tuning a paired dataset format, the input ids will be the input+output+pad tokens and labels will be the ignore tokens+output+pad tokens. Here we have essentially masked the length of input tokens and it will not be considered while calculating the loss. The input ids and labels for the transformer are the same except that in case of input ids the padding token is <unk> and for labels it is -100. The drawback in this approach is that the structure in the document such as headings and topical information might not be present within a chunk. The overlap option in Langchain splitters helps with this issue to an extent. However some topical information are too long to be captured by just overlapping.\nsame model is used to generate possible queries that can be asked about a chunk and then the dataset is prepared. A single chunk can have multiple queries to promote diversity in the dataset.\n# 4.2 Code Data Pre-processing\nLLM models are trained such that higher the context, detailing and prompting higher the results. When LLM are trained for code generation task it is very important to make the model understand the domain with quality along with quantity information. Researchers have been exploring the best possible way to reduce the amount of effort required during the preparation of the data. In our experiment we have considered three different ways of preparing the training data. They are a) Summary method b) Metadata method and c) Tokenization method The first method involves splitting the code at a class level or functional level code. The functional level code is considered as a source for preparing the data. The entire code repository is split into function level code. The functional level code is fed into the instruct model to generate the summaries by prompting the model. This type of data becomes our generated dataset which has function level code associated with their summaries. The second method involves extracting information from the coding practices embedded in the code. It is said that synthetic, structured, high quality, text book like data makes LLM learn faster and produce good results[21]. In this approach, the comments and docstrings in the code are extracted along with detailed comments and is used along with the raw information gathered from the code as pre-processing data. The third method involves tokenizing the whole code base irrespective of the file type into the supported sequence length. This method doesn\u2019t involve gathering any other data. The LLM model with this tokenized data is trained for the purpose of next token prediction usecase.\nLLM models are trained such that higher the context, detailing and prompting higher the results. When LLM are trained for code generation task it is very important to make the model understand the domain with quality along with quantity information. Researchers have been exploring the best possible way to reduce the amount of effort required during the preparation of the data.\nIn our experiment we have considered three different ways of preparing the training data. They are a) Summary method b) Metadata method and c) Tokenization method\nThe first method involves splitting the code at a class level or functional level code. The functional level code is considered as a source for preparing the data. The entire code repository is split into function level code. The functional level code is fed into the instruct model to generate the summaries by prompting the model. This type of data becomes our generated dataset which has function level code associated with their summaries.\nThe third method involves tokenizing the whole code base irrespective of the file type into the supported sequenc length. This method doesn\u2019t involve gathering any other data. The LLM model with this tokenized data is trained fo the purpose of next token prediction usecase.\n# 4.3 Compute Estimation\nAfter deciding on the input, the expected output and objective at hand, the next step is to decide on the hardware. To get the memory that will be occupied by a model in FP32 precision a rough estimate is to multiply the model parameter size by 4, since a single parameter occupies 4 bytes. For FP16, the multiplication factor is 2, and for 8 bit quantized model it is 1 and hence for 4 bit it is 0.5. This estimate alone will not help us in finding the right hardware for fine tuning as a higher percentage of memory is required to store the gradients and activations during the fine tuning process. This additional storage also has to be accounted for.\n# 5 Experiments\nLLaMA 2, the open source model used for experimentation in this paper has a pre training data cut off time on September 2022. Additional data till July 2023 has also been added as fresh data during fine tuning before its release [4]. Hence to test the approaches of data preparation recipes, proprietary documents and code repositories were used from our inhouse machine learning platform. A100 80 GB Nvidia GPU from Google Cloud Platform is used as the hardware accelerator for the fine tuning process.\n# 5.1 Text\nTo fine tune with text data, documentation resources from our machine learning platform were used. The user guide of this platform which has 66 pages and 5 MB of data was used as the raw source of data. This was converted to 60 KB csv file with 33 rows. Since the document is not available to public and was also created after the fine tuning cut off\ntime of the original LLaMA 2 model, it makes a good dataset to study. The user guide is in PDF format and the content is well structured with index, headings and step by step instructions.\ntime of the original LLaMA 2 model, it makes a good dataset to study. The user guide is in PDF format and the content is well structured with index, headings and step by step instructions.\nBefore the start of fine tuning process, the influence of quantization on inference is shown in table 1. Quantizing a model saves on GPU memory and allows fine tuning with higher batch sizes there by reducing time and money spent on the training job. However as shown in figure 2, quantized model on average takes a higher time for inference compared to a non quantized model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f9df/f9df6d45-2ad0-4dcc-9c97-4f0695ac18b5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Inference time of fine tuned Llama 2 7B Chat model</div>\nQuantization\nGPU memory\nWhat is AION?\nWithout quantization\n28 GB\nAION (Artificial Intelligence ON) is a cloud-based platform that enables\ndevelopers to build, train and deploy machine learning models. It pro-\nvides an end-to-end solution for data scientists and engineers to create,\ntest, refine, and deploy ML models in production environments.\n8 bit quantization\n8 GB\nAION (Artificial Intelligence ON) is a cloud-based platform that enables\ndevelopers to build, train and deploy machine learning models. It pro-\nvides an end-to-end solution for data scientists and engineers to create,\ntest, refine, and deploy predictive modeling solutions in the form of APIs\nor containerized microservices.\nIn table 2, the maximum possible configurations of PEFT methods on a A100 80 GB GPU machine are listed.  example, 70 B parameter flavour of LLaMA 2 model can be fine tuned only with QLoRA in 80 GB machine. LORA not possible due to memory constraint.\n<div style=\"text-align: center;\">Table 2: Maximum possible PEFT configurations of Llama 2 models on A100 80 GB</div>\nModel\n& size\nDataset size\nEpochs\nPEFT\nmethod\nCPU\nmemory\nGPU\nmemory\nEstimated\ntime\nLlama 2 Chat 7B\n60 KB\n3\nLORA\n6 GB\n18 GB\n15 mins\nLlama 2 Chat 13B\n60 KB\n3\nLORA\n6 GB\n26 GB\n25 mins\nLlama 2 Chat 70B\n60 KB\n3\nQLORA\n7 GB\n65 GB\n40 mins\nNow with the understanding of PEFT configurations possible for different size models, next there is a need to tune the PEFT hyper parameters. In [11], alpha is suggested to be fixed and rank is fine tuned. Typically lower ranks are preferred with a number of target modules. However owing to a smaller dataset size, target modules were kept as q_proj and v_proj only. A high alpha is used so that model could learn more from new gradient updates pertaining to the new information from documentation. The quality of the responses are captured with a manual assessment. As evident from table 3, the appropriate rank and alpha that gives decent results vary for the different parameter sized models.\n<div style=\"text-align: center;\">Table 3: LoRA rank tuning</div>\nModel and size\nLoRA parameters\nPrompt Manual assessments\nRank\nAlpha\nQn1\nQn2\n7B Chat\n1\n512\nGood\nGood\n2\n512\nAverage\nPoor\n4\n512\nPoor\nPoor\n8\n512\nPoor\nPoor\n16\n512\nPoor\nPoor\n13B Chat\n1\n512\nPoor\nGood\n2\n512\nPoor\nAverage\n4\n512\nPoor\nGood\n8\n512\nGood\nGood\n16\n512\nAverage\nAverage\nWith data extracted from the pdf, the final dataset is prepared in four ways and it is clear that each variation of the data preparation recipe produces different small changes in the response. \u2019Raw\u2019 and \u2019Keyword\u2019 recipe give short and succinct responses. \u2019Heading\u2019 and \u2019Summary\u2019 methods give a somewhat detailed response. PEFT methods are recommended for smaller dataset. However if there is a huge collection of dataset with new information, full fine tuning of all the parameters of a model is possible. The A100 80 GB GPU is capable of supporting full model fine tuning of 7B and 13B models. In table 5 the loading parameters and batch size details are given in detail. Higher batch sizes gave cuda out of memory error. The configurations in each row of each of the model size are named as config A, B and C respectively. In 3, a graph is plotted between number of data rows and the training time.\nDataset type What is AION? Default AION (Artificial Intelligence ON) is a cloud-based platform that enables developers to build, train and deploy machine learning models. It provides an end-to-end solution for data scientists and engineers to create, test, refine, and deploy predictive modeling solutions in the form of APIs or containerized microservices. ML AION (AI-ON) is an open source platform for building, deploying and managing machine learning models at scale. It provides features such as data version control, reproducibility, collaboration support, model monitoring, and deployment to cloud or on premise infrastructure. Meta Data AION (Artificial Intelligence ON) is an open-source, cloud-native platform for building and deploying modern machine learning models at scale. It provides a unified environment to develop, train, test, and deploy ML models across different data sources and platforms. The main goal of AION is to make it easy for developers to build, manage, and monitor their ML workflows by automating most of the underlying tasks such as data ingestion, feature engineering, model training, Gen AI AION (Artificial Intelligence ON): It refers to the integration of machine learning, data analytics, robotics, and artificial intelligence to automate business processes, improve productivity, and enable more informed decisions.AION stands for Artificial Intelligence ON which means all the features such as ML, Deep Learning, etc are enabled by default without any need to disable them. This makes it easy to use and deploy models in production environments seamlessly.\nWith increase in data rows, the time increases linearly. A different configuration with higher batch size and gradient accumulation steps will decrease the fine tuning time slightly. A huge time will be saved if full fine tuning is done for half the total capacity of maximum sequence length of LLaMA 2 models.\n<div style=\"text-align: center;\">Table 5: Full fine tuning of Llama2 Chat on A100 80 GB</div>\nLlama2 Chat\nModel\nsize\nData size\nSeq\nlen\nModel\nprecision\nBatch\nsize\nGradient\naccumulation\nsteps\nResulting\nsteps\nCPU\nmemory\nGPU\nmemory\nEstimated\ntime\n(mins)\n7B\n60 KB\n4096\nFP32\n1\n2\n254\n54 GB\n80 GB\n57\n7B\n60 KB\n4096\nFP16\n2\n4\n63\n54 GB\n80 GB\n33\n7B\n60 KB\n2048\nFP16\n4\n8\n16\n54 GB\n80 GB\n10\n13B\n60 KB\n4096\nFP32\n1\n2\n254\n102 GB\n80 GB\n110\n13B\n60 KB\n4096\nFP16\n2\n4\n127\n102 GB\n80 GB\n75\n13B\n60 KB\n2048\nFP16\n4\n8\n16\n102 GB\n80 GB\n25\nRAG- (Retrieval Augmented Generation) is a popular way of using embeddings and a vector similarity search to find only relevant context from a pool of documentation with new information or any content longer than maximum capacity of the LLM. The LLM interprets the context and gives a polished response. This entire process can be summed as embedding conversion followed by similiarity search usually through a vector DB which in turn is followed by LLM handling the context. It is evident from this process that the contribution of LLM will be good only if the contexts obtained are good. A comparison is made in table 6 between RAG with a pretrained model and RAG with the fine tuned model. It is seen that RAG responses are improved with specific answers and even following the style of the document. Since the document being dealt here is a user guide there are steps mentioned in every process. Only the fine tuned model was able to follow this pattern of giving responses in step by step format. Another observation is regarding the last question which is a multi part question. The last question was not answered by the base LLaMA 2 model. Our fine tuned model was not only able to follow the style but also give the precise answer to the second part of the question. Hence fine tuning could be one solution to hallucinations in RAG.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7256/7256cc39-fec3-49fe-b14d-b8a06e57e87b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Fine tuning time of Llama 2 Chat models on different fine tuning configurations</div>\nQuestions\nRAG\nFinetuned model and RAG\nWhat is AION?\nBased on the provided context, I can\nconfirm that AION is an artificial in-\ntelligence (AI) life cycle management\nplatform designed to help organizations\napply machine learning to real-world\nproblems. AION provides a comprehen-\nsive suite of tools and services for man-\naging the entire AI lifecycle, including\ndata ingestion, exploratory data analysis,\ndata processing, feature selection, model\ntraining and hyperparameter tuning, and\ninference services. The platform offers\na less-code/no-code approach, allowing\nbusiness users to generate ML models\nwithout coding. Additionally, AION\nincludes explainability capabilities to\nprovide higher accuracy predictions with\nsupported explanations and model confi-\ndence.\nBased on the provided context, I can ex-\nplain what AION is.\nAION stands for Artificial Intelligence ON,\nwhich is an AI life cycle management plat-\nform used for solving real-world problems\nusing machine learning. AION provides a\nless-code/no-code approach to users, allow-\ning them to train and deploy machine learn-\ning models without any coding knowledge.\nIt includes various sub-processes such as\nAutoML, MLOPs, MLaC, and Explainabil-\nity. These sub-processes are automated,\nand AION generates the best deployment\nmodel after analyzing the input data and\nproblem statement.\nWrite in detail about\nMLLite feature in\nAION. List the input\nmodel formats that\ncan be converted to\nONNX.\nBased on the provided context, here is\nthe information regarding the MLLite\nfeature in AION: MLLite is a feature in\nAION that allows users to convert their\ntrained models into formats compatible\nwith various deep learning frameworks,\nincluding ONNX and TFLite. The main\npurpose of MLLite is to enable efficient\ninference execution on different devices\nwithout requiring significant modifica-\ntions to the original model architecture.\nRegarding the input model formats that\ncan be converted to ONNX, according\nto the context, the following formats\nare supported:Keras, Sklearn, Tensor-\nFlow, TFLite, Light GBM, XGBoost,\nLibSVM,CoreML In summary, MLLite\nis a useful feature in AION that enables\nusers to convert their trained models into\noptimized formats for efficient inference\nexecution on various devices, while also\nproviding support for multiple input\nmodel formats for conversion to ONNX.\nSure! Here\u2019s what I could gather from\nthe given text regarding the MLLite fea-\nture in AION:MLLite feature in AION\nallows users to convert their trained\nmodels into different formats such as\nONNX, TFLite, and Sagemaker. The\nmain advantage of this feature is that\nit enables users to consume these opti-\nmized models for predictions without\nhaving to retrain them.Input Model For-\nmats that can be converted to ONNX in-\nclude:Keras,Sklearn,TensorFlow,Light\nGBM,XGBoost,LibSVM, CoreML\nIs there Incremental\nlearning in AION? If\nso what are the steps to\ncreate an incremental\nmodel?\nYes, AION supports incremental learn-\ning for regression and classification\nproblems. To create an incremental\nmodel in AION, follow these steps: 1.\nSelect the \"Online Learning\" (Beta) or\n\"Distributed Learning\" (Beta) checkbox\nin the Incremental Learning section of\nthe configuration page. 2. Choose the\nproblem type (Regression or Classifica-\ntion) that supports incremental learning.\n3. Proceed to the next step by clicking\nthe \"Next\" button. In summary, to cre-\nate an incremental model in AION, you\nneed to enable online learning in the\nconfiguration page and choose the appro-\npriate problem type.\nYes, there is incremental learning in AION.\nTo create an incremental model follow the\nbelow-mentioned steps:\nStep 2: Click online learning tab.\nStep 3: Select the Online Learning (Beta)\nor Distributed learning (Beta) checkbox for\nIncremental Learning.\nStep 4: Click next icon to proceed to ad-\nvanced configuration tab.\nWrite in detail about MLLite feature in AION. List the input model formats that can be converted to ONNX. Based on the provided context, here is the information regarding the MLLite feature in AION: MLLite is a feature in AION that allows users to convert their trained models into formats compatible with various deep learning frameworks, including ONNX and TFLite. The main purpose of MLLite is to enable efficient inference execution on different devices without requiring significant modifications to the original model architecture. Regarding the input model formats that can be converted to ONNX, according to the context, the following formats are supported:Keras, Sklearn, TensorFlow, TFLite, Light GBM, XGBoost, LibSVM,CoreML In summary, MLLite is a useful feature in AION that enables users to convert their trained models into optimized formats for efficient inference execution on various devices, while also providing support for multiple input model formats for conversion to ONNX. Sure! H the give ture in A allows  models ONNX main ad it enabl mized m having  mats th clude:K GBM,X Is there Incremental learning in AION? If so what are the steps to create an incremental model? Yes, AION supports incremental learning for regression and classification problems. To create an incremental model in AION, follow these steps: 1. Select the \"Online Learning\" (Beta) or \"Distributed Learning\" (Beta) checkbox in the Incremental Learning section of the configuration page. 2. Choose the problem type (Regression or Classification) that supports incremental learning. 3. Proceed to the next step by clicking the \"Next\" button. In summary, to create an incremental model in AION, you need to enable online learning in the configuration page and choose the appropriate problem type. Yes, the To crea below-m Step 2:  Step 3:  or Distr Increme Step 4:  vanced \nWrite in detail about\nMLLite feature in\nAION. List the input\nmodel formats that\ncan be converted to\nONNX.\nBased on the provided context, here is\nthe information regarding the MLLite\nfeature in AION: MLLite is a feature in\nAION that allows users to convert their\ntrained models into formats compatible\nwith various deep learning frameworks,\nincluding ONNX and TFLite. The main\npurpose of MLLite is to enable efficient\ninference execution on different devices\nwithout requiring significant modifica-\ntions to the original model architecture.\nRegarding the input model formats that\ncan be converted to ONNX, according\nto the context, the following formats\nare supported:Keras, Sklearn, Tensor-\nFlow, TFLite, Light GBM, XGBoost,\nLibSVM,CoreML In summary, MLLite\nis a useful feature in AION that enables\nusers to convert their trained models into\noptimized formats for efficient inference\nexecution on various devices, while also\nproviding support for multiple input\nmodel formats for conversion to ONNX.\nIs there Incremental\nlearning in AION? If\nso what are the steps to\ncreate an incremental\nmodel?\nYes, AION supports incremental learn-\ning for regression and classification\nproblems. To create an incremental\nmodel in AION, follow these steps: 1.\nSelect the \"Online Learning\" (Beta) or\n\"Distributed Learning\" (Beta) checkbox\nin the Incremental Learning section of\nthe configuration page. 2. Choose the\nproblem type (Regression or Classifica-\ntion) that supports incremental learning.\n3. Proceed to the next step by clicking\nthe \"Next\" button. In summary, to cre-\nate an incremental model in AION, you\nneed to enable online learning in the\nconfiguration page and choose the appro-\npriate problem type.\nSure! Here\u2019s what I could gather from the given text regarding the MLLite feature in AION:MLLite feature in AION allows users to convert their trained models into different formats such as ONNX, TFLite, and Sagemaker. The main advantage of this feature is that it enables users to consume these optimized models for predictions without having to retrain them.Input Model Formats that can be converted to ONNX include:Keras,Sklearn,TensorFlow,Light GBM,XGBoost,LibSVM, CoreML\nHow can we do text\nlabelling with AION?\nGive the API URL for\nmodel monitoring?\nBased on the provided context, the API\nURL for model monitoring during text\nlabeling with AION is not explicitly\nmentioned. Therefore, I cannot provide\nan answer to this question.\nBased on the provided context,\nI can answer the question as follows:\nTo perform text labelling with AION, fol-\nlow these steps:\nStep 1: Go to\nUtilities > Text Data Labelling.\nStep 2: Upload the text data in\nCSV format.\nStep 3: Select relevant features and click\nNext.\nStep 4: Generate predictions by clicking\nPredictions tab.\nThe predicted labels are displayed under\nPredicted Labels section.\nFor example, if the input text is \"This is a\nsample text\", the predicted label would be\n\"positive\".\nAPI URL for Model Monitoring:\nTo monitor the performance of the\nmodel, users can use the API endpoint\n/monitoring/model-performance. This\nAPI returns the summary statistics of the\nmodel\u2019s performance along with the confu-\nsion matrix.\nExample:\ncurl -X POST\nhttp://localhost:8080/monitoring/model-\nperformance -H \u2019Content-Type:\napplication/json\u2019 -d \u2019\"data\":[\"This is a\nsample text\"],\"label\":[\"positive\"]\u2019\nHow can we do text labelling with AION? Give the API URL for model monitoring?\nBased on the provided context, the API URL for model monitoring during text labeling with AION is not explicitly mentioned. Therefore, I cannot provide an answer to this question.\n<div style=\"text-align: center;\">Finally the objective of the fine tuning process is to see if the LLM has learnt the new knowledge and as shown in table 7, it indeed has learnt.</div>\nFinally the objective of the fine tuning process is to see if the LLM has learnt the new knowledge an 7, it indeed has learnt.\n<div style=\"text-align: center;\">Table 7: Model learning new information after fine tuning</div>\nTable 7: Model learning new information after fine tuning\nFine tuning\nModel\nWhat is AION?\nBefore\nLlama 2 chat\nAion (AION) is a blockchain-based platform that enables the creation and ex-\nchange of digital assets, such as NFTs. It was founded in 2018 by a team led by\nMatthew Roszak, who has extensive experience in the cryptocurrency industry.\nAfter\nLlama 2 chat\nAION (Artificial Intelligence ON) is a cloud-based platform that enables develop-\ners to build, train and deploy machine learning models. It provides an end-to-end\nsolution for data scientists and engineers to create, test, refine, and deploy ML\nmodels in production environments.\n# 5.2 Code\nFor the code dataset, JAVA files from a sustenance engineering solutions platform was used. This being a closed source code repository, the codes are new unlearned information for Llama models making it a good dataset to experiment with. The total size of the code data was 16MB with a total function count of 13807. The dataset was condensed and packed according to token limit of LLaMA model and resulted in a csv file of 10 MB size with 60 rows. Table 8 exhibits the experiment on the hyper parameters conducted to investigate the performance of the model. Few results are discussed by prompting the trained model and comparing the results from the code repository. It was seen from the results from the experiments the Code LLaMA models gave an exceptionally good results with LoRA rank 16 and alpha 32. Higher the rank and alpha made the models to hallucinate and randomly generate the code.\n<div style=\"text-align: center;\">Table 8: PEFT methods on Code Llama on A100 80 GB</div>\nModel\n& size\nData size\nEpochs\nPEFT\nmethod\nCPU\nmemory\nGPU\nmemory\nEstimated\ntime\nCode Llama 7B\n10 MB\n3\nLORA\n6 GB\n18 GB\n23 mins\nCode Llama 13B\n10 MB\n3\nLORA\n6 GB\n26 GB\n25 mins\nCode Llama 7B\n10 MB\n3\nQLORA\n7 GB\n15 GB\n10 mins\n<div style=\"text-align: center;\">Table 9: Full fine tuning of Code Llama on A100 80 GB</div>\nModel\n& size\nSeq\nlen\nModel\nprecision\nBatch\nsize\nGradient\naccumulation\nsteps\nCPU\nmemory\nGPU\nmemory\nEstimated\ntime\nCode Llama 7B\n4096\nFP16\n2\n2\n54 GB\n57 GB\n10 mins\nCode Llama 7B\n4096\nFP16\n2\n4\n54 GB\n46 GB\n38 mins\nCode Llama 7B\n2048\nFP16\n4\n8\n54 GB\n59 GB\n10 mins\nCode Llama 13B\n4096\nFP32\n2\n2\n102 GB\n53 GB\n9 mins\nCode Llama 13B\n4096\nFP16\n2\n4\n102 GB\n53 GB\n18 mins\nCode Llama 13B\n2048\nFP16\n4\n8\n102 GB\n62 GB\n33 mins\n<div style=\"text-align: center;\">Table 10: Code Llama LORA</div>\nTable 10: Code Llama LORA\nModel and size\nLoRA parameters\nPrompt Manual assessments\nRank\nAlpha\nCode Llama 7B\n2\n4\nPoor\n4\n8\nPoor\n8\n16\nAverage\n16\n32\nGood\nPrompt\nGround truth\nPredicted response\nA connection object is\ncreated to connect to\na database for a given\nproject name\nConnection\nis\nestab-\nlished to the database\nusing the URL, user-\nname and password\nIf an exception occurs\nthe error message is\nprinted The connection\nobject is returned to the\ncalling function\nConnection\ncon = n u l l ;\nH t t p S e r v l e t R e q u e s t\nh t t p R e q u e s t\n= n u l l ;\nt r y {\nSystem . out . p r i n t l n ( \" \" get New\nConnection\nprojectName : \" \"+\nprojectName )\n;\nClass .\nforName ( driverName )\n.\nnewInstance ( )\n;\ncon = DriverManager .\ngetConnection ( u r l +\nprojectName ,\ndbUser ,\ndbPass ) ;\n}\ncatch\n( SQLException\ns q l e ) {\nSystem . out . p r i n t l n ( \" \" in\nConnectionPool\nof\ngetNewConnection ( ) 11111 \" \"+\ns q l e . getStackTrace ( ) ) ;\n}\ncatch\n( Exception\ne ) {\nSystem . out . p r i n t l n ( \" \" in\nConnectionPool\nof\ngetNewConnection ( ) 222 \" \" ) ;\ne . p r i n t S t a c k T r a c e ( ) ;\n}\nreturn\ncon ;\np u b l i c\nConnection\ngetConnection (\nS t r i n g\nprojectName ) {\nConnection\ncon = n u l l ;\nH t t p S e r v l e t R e q u e s t\nh t t p R e q u e s t =\nn u l l ;\nt r y {\nClass . forName ( driverName ) .\nnewInstance ( ) ;\ncon = DriverManager .\ngetConnection ( u r l +\nprojectName ,\ndbUser ,\ndbPass ) ;\n}\ncatch\n( SQLException\ns q l e ) {\nSystem . out . p r i n t l n ( \" In\nConnectionPool\nof\ngetConnection ( ) \"+ s q l e .\ngetMessage ( ) ) ;\n}}\ncatch\n( Exception\ne ) {\nSystem . out . p r i n t l n ( \" In\nConnectionPool\nof\ngetConnection ( ) \"+e .\ngetMessage ( ) ) ;\n}\nreturn\ncon ;\n<div style=\"text-align: center;\">Predicted response</div>\nPrompt\nGround truth\nPredicted response\nConnect\nto\nthe\ndatabase.\nPrepare a query with a\ngiven test case ID to the\ntable testcases table.\nSet the parameter to the\ngiven value.\nExecute the query and\nretrieve the results.\nGet the desired data\nfrom the results.\nRelease resources used\nby the query and results.\nReturn the desired data.\np u b l i c\nList <\nDefectTestCaseMappingDTO >\ng e t F i e l d D e f e c t s ( S t r i n g\nt e s t c a s e I d ) {\ni n t\ntcIdPk = DeltaUploadDAO .\ng e t I n s t a n c e ( ) .\ngetTestCaseId ( t e s t c a s e I d ) ;\nList <DefectTestCaseMappingDTO >\nf i e l d D e f e c t s L i s t\n= new\nArrayList <\nDefectTestCaseMappingDTO\n>() ;\nConnection\nconnection =\nConnectionPool . g e t I n s t a n c e\n( ) . getConnection ( ) ;\ni f ( n u l l\n!=\nconnection ) {\nStatement\nstmt = n u l l ;\nR e s u l t S e t\nr s = n u l l ;\nt r y\n{\ni f ( n u l l\n!=\nconnection ) {\nS t r i n g\ns q l = \" \"SELECT *\nFROM\nDEFECT_TESTCASE_MAPPING\nWHERE STATUS = \u2019R\u2019\nAND TESTCASE_ID_FK = \"\n\"+ tcIdPk +\" \" \" \" ;\nstmt = connection .\nc r e a t e S t a t e m e n t ( ) ;\nr s = stmt . executeQuery ( s q l\n) ;\nwhile ( r s . next ( ) ) {\nDefectTestCaseMappingDTO\ndefectDTO= new\nDefectTestCaseMappingDTO\n( ) ;\ndefectDTO . s e t T e s t C a s e I d (\ngetTcById ( r s . g e t I n t ( \" \"\nTESTCASE_ID_FK\" \" ) ) ) ;\ndefectDTO . s e t S t a t u s ( r s .\ng e t S t r i n g ( \" \"STATUS\" \" ) ) ;\ndefectDTO . setAuto ( r s . g e t S t r i n g\n( \" \"AUTO\" \" ) ) ;\ndefectDTO . setReviewed ( r s .\ng e t S t r i n g ( \" \"REVIEWED\" \" ) ) ;\nf i e l d D e f e c t s L i s t . add ( defectDTO\n) ;}}\n}\n}\nreturn\ncon ;\np u b l i c\nDataSet\ngetTestCaseByTestCaseId ( i n t\nt e s t C a s e I d ) {\nSystem . out . p r i n t l n ( \u2019\ngetTestCaseByTestCaseId \u2019 ) ;\nConnection\ncon = n u l l ;\nStatement\nstmt = n u l l ;\nR e s u l t S e t\nr s = n u l l ;\nDataSet\nds = new DataSet ( ) ;\nt r y {\ncon = ConnectionProvider .\ngetConnection ( ) ;\ni f ( n u l l\n!= con ) {\nstmt = con .\nc r e a t e S t a t m e n t ( ) ;\nS t r i n g\ns q l = \"SELECT *\nFROM TESTCASES where\nTESTCASE_ID_FK=\"+\nt e s t C a s e I d +\" \" ;\nwhile ( r s . next ( ) ) {\nTestCaseDAO tcDao =\nnew TestCaseDAO\n( ) ;\ntcDao .\nsetTestCaseName (\nr s . g e t S t r i n g ( \"\nTESTCASE_NAME\" ) )\n;\ntcDao .\nsetTestCaseDesc (\nr s . g e t S t r i n g ( \"\nTESTCASE_DESC\" ) )\n;\ntcDao . s e t T e s t C a s e I d (\nr s . g e t I n t ( \"\nTESTCASE_ID_FK\" )\n) ;\nds . add ( tcDao ) ;\n}\nreturn\nds ;\n}\ncatch\n( Exception\ne ) {\n}}}\nGround truth p u b l i c List < DefectTestCaseMappingDTO > g e t F i e l d D e f e c t s ( S t r i n g t e s t c a s e I d ) { i n t tcIdPk = DeltaUploadDAO . g e t I n s t a n c e ( ) . getTestCaseId ( t e s t c a s e I d ) ; List <DefectTestCaseMappingDTO > f i e l d D e f e c t s L i s t = new ArrayList < DefectTestCaseMappingDTO >() ; Connection connection = ConnectionPool . g e t I n s t a n c e ( ) . getConnection ( ) ; i f ( n u l l != connection ) { Statement stmt = n u l l ; R e s u l t S e t r s = n u l l ; t r y { i f ( n u l l != connection ) { S t r i n g s q l = \" \"SELECT * FROM DEFECT_TESTCASE_MAPPING WHERE STATUS = \u2019R\u2019 AND TESTCASE_ID_FK = \" \"+ tcIdPk +\" \" \" \" ; stmt = connection . c r e a t e S t a t e m e n t ( ) ; r s = stmt . executeQuery ( s q l ) ; while ( r s . next ( ) ) { DefectTestCaseMappingDTO defectDTO= new DefectTestCaseMappingDTO ( ) ; defectDTO . s e t T e s t C a s e I d ( getTcById ( r s . g e t I n t ( \" \" TESTCASE_ID_FK\" \" ) ) ) ; defectDTO . s e t S t a t u s ( r s . g e t S t r i n g ( \" \"STATUS\" \" ) ) ; defectDTO . setAuto ( r s . g e t S t r i n g ( \" \"AUTO\" \" ) ) ; defectDTO . setReviewed ( r s . g e t S t r i n g ( \" \"REVIEWED\" \" ) ) ; f i e l d D e f e c t s L i s t . add ( defectDTO ) ;}} } } return con ;\n# 6 Guidelines and Recommendations\nollowing guidelines and recommendations have been summarized below, based on the various experiements that ave done on text and code fine tuning: \u2022 Empirically loading the model in half precision is sufficient to go ahead with fine tuning and it also saves GPU memory to accommodate more batches if needed to save on finetuning time \u2022 Unless there is an abundance of data, parameter efficient finetuning is preferable than full finetuning. This also helps in creating easily moveable low sized adapters tuned for different tasks or domains \u2022 Choose a model quantization level based on section 4.3. For example, consider Llama 7B model in a 16 GB colab environment; in this scenario, 8 bit quantized LORA fine tuning is possible but not full model fine tuning. \u2022 For full fine tuning, typically multiple GPUs are required. In case of a constraint of having only one GPU available and a large CPU memory, it is recommended to use paged adam optimizer \u2022 For small datasets, it is ideal to use LORA fine tuning. Rank and Alpha has to be fine tuned \u2022 From the empirical experiments on text and code data, to make a language model assimilate new information, lower rank and higher alpha is recommended \u2022 For large documents with text content of the order of few hundred MBs, it is recommended to utilise the full sequence length capability of the model in every row of data \u2022 Fine tuning time largely depends on the number of rows in dataset. If the text content is chunked to full context length without padding, number of data rows can be greatly reduced \u2022 Gradient accumulation steps is the number of steps after which the optimizer is stepped. Until then gradients are accumulated over the batches. This is good in distributed system but in single GPU it is slow \u2022 A higher batch size will lead to faster convergence and might give better performance at inference. Batch size is recommended to be kept at a lower value suitable for the model and not to the limiting value of GPU memory. \u2022 Higher the gradient accumulation steps more the memory will be saved but at the cost of longer fine tuning time.\n# 7 Further work\nIn this paper we show that LLMs are able to learn new information from limited data with right LORA configurations However the results have traces of hallucinations. To mitigate hallucinations, within the current setting different prompt templates have to be experimented. It also boils down to the way dataset is prepared. Chunking techniques like semantic chunking provide a way to create chunks that stand on their own as separate information entities. This could be explored further as a dataset preparation recipe to reduce hallucinations.\n# 8 Conclusion\nThe paper discussed on the topic of fine tuning open source Large Language Models with proprietary documents and code repositories. In the Dataset preparation sections detailed steps on creating the dataset from raw documents and code bases is given. It is followed by experiments with different methods of preparation and manual evaluation of the model responses with different LORA configurations. Finally some pointers observed during the fine tuning process are given as guidelines.\n# References\n[1] Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in machine translation: Boosting translation performance of large language models, 2024. [2] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, 2024.\n[1] Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in machine translation: Boosting translation performance of large language models, 2024.\n[1] Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in machine translation: Boosting translation performance of large language models, 2024. [2] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, 2024.\n] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, 2024.\n[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. [5] Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha. Fingpt: Democratizing internet-scale data for financial large language models, 2023. [6] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Towards building open-source language models for medicine, 2023. [7] Rajvardhan Patil and Venkat Gudivada. A review of current trends, techniques, and challenges in large language models (llms). Applied Sciences, 14(5), 2024. [8] Cheonsu Jeong. A study on the implementation of generative ai services using an enterprise data-based llm application architecture. Advances in Artificial Intelligence and Machine Learning, 03(04):1588\u20131618, 2023. [9] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems, 2024. [10] Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estev\u00e3o Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. Rag vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture, 2024. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022. [13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 10088\u201310115. Curran Associates, Inc., 2023. [14] Mark Horowitz. 1.1 computing\u2019s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10\u201314, Feb 2014. [15] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of bfloat16 for deep learning training, 2019. [16] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference, 2017. [17] Memory Decreases! But Latency Increases...., howpublished = https://github.com/timdettmers/ bitsandbytes/issues/6 . [18] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction tuning for large language models: A survey, 2023. [19] Rapid Automatic Keyword Extraction algorithm domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text. https://pypi.org/project/rake-nltk/.\n[20] Querying local documents, powered by LLM. https://github.com/snexus/llm-search/blob/main/src/ llmsearch/parsers/doc.py. [21] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of fine-tuning large language models (LLMs) for enterprises, emphasizing the need for training LLMs on proprietary domain knowledge to enhance their performance. Previous methods, such as Retrieval Augmented Generation (RAG), have limitations in quality due to reliance on vector databases, necessitating a breakthrough in fine-tuning techniques.",
        "problem": {
            "definition": "The problem is the challenge of efficiently fine-tuning LLMs on proprietary documents and code, ensuring optimal resource use and cost-effectiveness while maintaining high performance.",
            "key obstacle": "The main difficulty lies in the high initial costs and resource requirements for fine-tuning LLMs compared to simpler methods like RAG, which can be limited by the quality of the retrieval mechanism."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that existing LLMs can be adapted to specific tasks through fine-tuning, allowing them to better understand domain-specific knowledge and improve response quality.",
            "opinion": "The proposed idea involves fine-tuning the LLaMA model using proprietary enterprise documents and code, focusing on effective data preparation and model configuration to enhance performance.",
            "innovation": "The innovation lies in the implementation of efficient data preparation recipes and parameter-efficient fine-tuning methods like LoRA and QLoRA, which significantly reduce the computational burden while improving model performance."
        },
        "method": {
            "method name": "Fine Tuning LLaMA",
            "method abbreviation": "FTL",
            "method definition": "FTL is a process for adapting the LLaMA model to specific enterprise needs by training it on proprietary datasets, which involves optimizing configurations and data formats.",
            "method description": "The method involves preprocessing proprietary documents and code, followed by fine-tuning the LLaMA model using parameter-efficient techniques.",
            "method steps": [
                "Data collection from proprietary documents and code repositories.",
                "Preprocessing the data into suitable formats (raw data, keywords, headings, queries).",
                "Applying LoRA or QLoRA configurations for fine-tuning.",
                "Training the model until loss is minimized."
            ],
            "principle": "The method is effective because it leverages existing pre-trained models while adapting them to specific tasks using minimal resources, thus enhancing their performance without the need for extensive computational power."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized a proprietary document and code repository, employing an A100 80 GB Nvidia GPU for fine-tuning and comparing the results against baseline methods.",
            "evaluation method": "Performance was assessed through empirical studies that measured the effects of quantization, memory requirements, and the quality of responses to domain-specific queries."
        },
        "conclusion": "The experiments demonstrated that fine-tuning LLMs with proprietary datasets significantly improves their ability to generate accurate and contextually relevant responses, highlighting the effectiveness of the proposed methods.",
        "discussion": {
            "advantage": "The key advantages include reduced memory requirements through quantization and parameter-efficient fine-tuning, allowing enterprises to leverage LLMs without extensive computational resources.",
            "limitation": "The limitations include potential hallucinations in generated responses and the need for high-quality data preparation to achieve optimal results.",
            "future work": "Future research directions include exploring different data chunking techniques and prompt templates to further reduce hallucinations and improve fine-tuning outcomes."
        },
        "other info": [
            {
                "info1": "The paper provides practical guidelines for fine-tuning LLMs, including recommendations for parameter selection and data preprocessing techniques."
            },
            {
                "info2": {
                    "info2.1": "The experiments demonstrated varying performance based on data preparation methods."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The proposed idea involves fine-tuning the LLaMA model using proprietary enterprise documents and code, focusing on effective data preparation and model configuration to enhance performance."
        },
        {
            "section number": "4.2",
            "key information": "FTL is a process for adapting the LLaMA model to specific enterprise needs by training it on proprietary datasets, which involves optimizing configurations and data formats."
        },
        {
            "section number": "4.3",
            "key information": "The key advantages include reduced memory requirements through quantization and parameter-efficient fine-tuning, allowing enterprises to leverage LLMs without extensive computational resources."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions include exploring different data chunking techniques and prompt templates to further reduce hallucinations and improve fine-tuning outcomes."
        },
        {
            "section number": "1.2",
            "key information": "This paper addresses the issue of fine-tuning large language models (LLMs) for enterprises, emphasizing the need for training LLMs on proprietary domain knowledge to enhance their performance."
        }
    ],
    "similarity_score": 0.7431624848222513,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Fine Tuning LLM for Enterprise_ Practical Guidelines and Recommendations.json"
}