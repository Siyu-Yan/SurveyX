{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.07556",
    "title": "Using Natural Language Explanations to Improve Robustness of In-context Learning",
    "abstract": "Recent studies demonstrated that large language models (LLMs) can excel in many tasks via in-context learning (ICL). However, recent works show that ICL-prompted models tend to produce inaccurate results when presented with adversarial inputs. In this work, we investigate whether augmenting ICL with natural language explanations (NLEs) improves the robustness of LLMs on adversarial datasets covering natural language inference and paraphrasing identification. We prompt LLMs with a small set of human-generated NLEs to produce further NLEs, yielding more accurate results than both a zero-shot-ICL setting and using only human-generated NLEs. Our results on five popular LLMs (GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) show that our approach yields over 6% improvement over baseline approaches for eight adversarial datasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore, previous studies have demonstrated that prompt selection strategies significantly enhance ICL on in-distribution test sets. However, our findings reveal that these strategies do not match the efficacy of our approach for robustness evaluations, resulting in an accuracy drop of 8% compared to the proposed approach.1",
    "bib_name": "he2024usingnaturallanguageexplanations",
    "md_text": "# Using Natural Language Explanations to Improve Robustness of In-context Learning\nXuanli He\u2663 Yuxiang Wu\u2665 Oana-Maria Camburu\u2663 Pasquale Minervini\u2660 Pontus Stenetorp\u2663 \u2663University College London \u2665Weco AI \u2660University of Edinburgh z.xuanli.he@gmail.com yuxiang@weco.ai p.minervini@ed.ac.uk {o.camburu, p.stenetorp}@ucl.ac.uk\nAbstract\n# Abstract\nRecent studies demonstrated that large language models (LLMs) can excel in many tasks via in-context learning (ICL). However, recent works show that ICL-prompted models tend to produce inaccurate results when presented with adversarial inputs. In this work, we investigate whether augmenting ICL with natural language explanations (NLEs) improves the robustness of LLMs on adversarial datasets covering natural language inference and paraphrasing identification. We prompt LLMs with a small set of human-generated NLEs to produce further NLEs, yielding more accurate results than both a zero-shot-ICL setting and using only human-generated NLEs. Our results on five popular LLMs (GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) show that our approach yields over 6% improvement over baseline approaches for eight adversarial datasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore, previous studies have demonstrated that prompt selection strategies significantly enhance ICL on in-distribution test sets. However, our findings reveal that these strategies do not match the efficacy of our approach for robustness evaluations, resulting in an accuracy drop of 8% compared to the proposed approach.1\narXiv:2311.07556v2 \n# 1 Introduction\nThe landscape of AI has recently undergone a significant transformation with the advent of large language models (LLMs). These models can produce accurate predictions on unseen data after observing a small number of demonstrations. Remarkably, they can achieve this based on examples provided directly in their inputs, without explicit retraining or fine-tuning \u2013 this learning paradigm is referred to as in-context learning (ICL, Brown et al., 2020;\n1Code and datasets are accessible at: https://github. com/xlhex/acl2024_xicl\nRae et al., 2021). However, ICL struggles to execute complex tasks, such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021). To improve the effectiveness of ICL in solving tasks requiring complex reasoning, Wei et al. (2022b) drew inspiration from natural language explanations (NLEs) to introduce a method denoted as the Chain-of-Thought (CoT) prompting. CoT prompting involves prompting a model with a sequence of intermediate steps or reasoning processes to guide it towards generating more accurate answers.2 In this work, we denote ICL equipped with NLEs as X-ICL. Despite its simplicity, X-ICL has advanced the performance of ICL across a broad range of complex reasoning tasks (Wei et al., 2022b; Wang et al., 2023b). Similarly to supervised learning, ICL tends to be vulnerable to adversarial examples (Wang et al., 2023a). Previous research shows that improving the robustness of fine-tuned models against such adversarial datasets is possible by fine-tuning with task-relevant NLEs (Chen et al., 2022; Ludan et al., 2023). Inspired by this, we hypothesize that incorporating NLEs into ICL could also improve the robustness of LLMs against adversarial examples. To this end, we evaluate the robustness of X-ICL on eight adversarial datasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Moreover, the effectiveness of X-ICL so far relies on the availability of human-written NLEs (Wei et al., 2022b), which usually require domainspecific knowledge, making them hard to collect. However, the advent of LLMs uncovered a range of possibilities where LLMs can assist human annotators (Bang et al., 2023; Guo et al., 2023). Motivated by this development, we investigate using three LLMs, namely GPT3.5-turbo, Llama2, and Vicuna,\n2CoTs and NLEs are similar concepts, as they both describe the reasoning process behind a decision in natural language; as NLEs were introduced before CoTs (Camburu et al., 2018; Hendricks et al., 2018), we use the former term.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d79/0d79e201-ae6d-4c42-a204-1328b57353fd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Human evaluation on 100 NLEs generated by GPT3.5-turbo (labeled as ChatGPT NLEs) and 100 NLEs generated by human annotators (labeled as Human NLEs). The satisfaction scores span from 1 (extremely dissatisfied) to 5 (extremely satisfied).</div>\nto generate NLEs for ICL. We then use human annotators to assess the quality of 200 human-written and LLM-generated NLEs. As shown in Figure 1, most annotators (3 out of 4) prefer NLEs produced by ChatGPT (GPT3.5-turbo) over those crafted by humans.3 This observation further motivates us to evaluate models prompted with LLM-generated NLEs. We then evaluate the improvement in the robustness of X-ICL in three settings \u2013 in two of the settings, an LLM is prompted with LLM-generated NLEs (generated in zero-shot-ICL and few-shotICL settings, and in the last setting, the LLM is prompted with human-generated NLEs. In the evaluation, we consider five popular LLMs (i.e., Mistral (Jiang et al., 2023), Zephyr (Tunstall et al., 2023), Vicuna (Chiang et al., 2023), Llama2 (Touvron et al., 2023) and GPT3.5-turbo) on eight adversarial datasets. Our experimental results suggest that X-ICL produces more accurate results than ICL and, moreover, that NLEs generated by ChatGPT in a few-shot-ICL setting (by prompting ChatGPT with human-generated NLEs) significantly improve over the ICL baseline (+6%) for the majority of the considered datasets and LLMs. Thus, our findings suggest that an integrated approach, combining human inputs with LLMs, can provide a more effective solution than utilizing either human annotators or LLMs in isolation. Finally, we show that while prompt selection strategies (i.e., retrieving relevant training examples) can significantly improve the accuracy of ICL on in-distribution test sets (Gupta et al., 2023; Levy et al., 2023; Ye et al., 2023), they are less effective on adversarial datasets when com-\n3More details are available in Appendix D.1.\npared to X-ICL methods, with our approach (fewshot-ICL) outperforming them by more than 8% in accuracy.\n# 2 Related Work\nLearning with Explanations. There has been a surge of work on explaining predictions of neural NLP systems, from highlighting decision words (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2017; Serrano and Smith, 2019) to generating NLEs (Camburu et al., 2018; Narang et al., 2020; Wiegreffe and Marasovic, 2021). Our work concentrates on the latter category, namely, the selfgeneration of NLEs for justifying model predictions. Rajani et al. (2019) propose a two-stage training process to improve the prediction performance for commonsense reasoning tasks. In their work, the first stage revolves around generating NLEs, which are then used to inform the label prediction training process in the second stage. Alternatively, one can leverage a multi-task framework to generate NLEs and labels simultaneously (Hase et al., 2020). Li et al. (2022) propose advancing the reasoning abilities of smaller LMs by leveraging NLEs generated by GPT-3 (Brown et al., 2020). NLEs have also vastly been employed beyond NLP, such as in computer vision (Hendricks et al., 2018; Zellers et al., 2019; Majumder et al., 2022), in the medical domain (Kayser et al., 2022), and for selfdriving cars (Kim et al., 2018), with some works showing improved task performance when training with NLEs (Kayser et al., 2021). However, these studies primarily concentrate on supervised fine-tuning approaches, which is different from the focus of this work, i.e., ICL.\nPrompting with NLEs. Despite its remarkable performance on several downstream tasks (Brown et al., 2020), ICL can still produce inaccurate results in tasks requiring reasoning abilities, such as arithmetic, logical, and commonsense reasoning tasks (Rae et al., 2021; Srivastava et al., 2022). To improve the reasoning abilities of LLMs, Wei et al. (2022b) introduced CoT prompting. This technique prompts an LM to generate a sequence of concise sentences that imitate the reasoning process an individual might undergo to solve a task before providing the ultimate answer, essentially to provide an NLE/CoT before generating the final answer. Furthermore, Wang et al. (2023b) propose to improve CoT prompting by combining multiple diverse reasoning paths generated by LLMs, en-\nhancing the accuracy of a greedy CoT prompting approach. However, these aforementioned methods need human-written NLEs as CoT in the prompts. Instead, our LLM-based zero-shot-ICL regime harnesses the power of an LLM to synthesize NLEs without human-written NLEs.\nLearning Robust Models. Several works show that NLP models are prone to performance degradation when presented with adversarial examples, a consequence of inherent artifacts or biases within the annotation of the training dataset (Naik et al., 2018; McCoy et al., 2019; Nie et al., 2020; Liu et al., 2020b). Various strategies have been proposed to mitigate biases within NLP models, e.g., initially training a weak model to recognize superficial features, subsequently enforcing a target model to learn more robust and generalizable characteristics (He et al., 2019; Clark et al., 2019; Karimi Mahabadi et al., 2020; Yaghoobzadeh et al., 2021; Korakakis and Vlachos, 2023). Additionally, data augmentation presents another viable option (Minervini and Riedel, 2018; Wu et al., 2021, 2022). Moreover, studies have shown that supervised finetuning of models using rationales or human-written NLEs can significantly enhance the models\u2019 resilience against adversarial datasets (Chen et al., 2022; Stacey et al., 2022; Kavumba et al., 2023; Ludan et al., 2023). Unlike them, our research examines the robustness of X-ICL across eight adversarial datasets, highlighting a novel finding: NLEs generated by LLMs surpass those produced by human annotators in enhancing model robustness. In addition, unlike human-written NLEs, those produced by LLMs exhibit greater scalability and adaptability across diverse tasks.\n# 3 Methodology\nThis section first outlines the workflow of X-ICL. Then, the focus shifts to detailing how an LLM can generate an NLE for a labeled instance.\n# 3.1 ICL with NLEs (X-ICL)\nLLMs can provide significantly more accurate predictions across various reasoning tasks when supplied with human-written NLEs (Wei et al., 2022b,a). In X-ICL, given an instance, the task is to generate the most likely prediction and NLE for that instance. More formally, in X-ICL, given an unlabeled instance x\u2032 \u2208X and a set of training examples (xi, ri, yi), where xi \u2208X is an instance,\nyi \u2208Y is its label, and ri \u2208E is the corresponding explanation, the task is to identify the most likely label and explanation for x\u2032:\narg max (r\u2032,y\u2032)\u2208E\u00d7Y P\u03b8 \ufffd (r\u2032, y\u2032) | (xi, ri, yi)k i=1, (x\u2032) \ufffd ,\nwhere \u03b8 denotes the model parameters, and X, Y, and E are the sets of all possible instances, labels, and explanations, respectively. The objective is to generate the most likely combination of label y\u2032 and explanation r\u2032 from an LLM, after prompting it with the demonstration examples, including labeled instances and NLEs (xi, ri, yi)k i=1, as well as the unlabeled instance x\u2032.\n# 3.2 Generating NLEs with LLMs\nIn existing X-ICL works, human-written NLEs r were used for the instances within the demonstration set. Instead, in this work, we opt for the NLEs synthesized via LLMs. This preference is driven by noting that NLEs produced by LLMs tend to receive higher approval ratings from human evaluators, as indicated in Figure 1. We argue that this preference will boost the performance of X-ICL. The methods utilized for the generation of NLEs are outlined below.\nwhere s denotes a meta-prompt representing the task. More details on the meta-prompt and demonstration sets are available in Appendix B.\n# Zero-shot prompting for NLEs\ntend our approach to situations where humanwritten NLEs are absent, which is generally more prevalent across most datasets. In this context, LLMs are prompted to generate an NLE for a labeled instance devoid of any pre-existing examples with NLEs. The objective bears a resemblance to Equation (1), albeit without the inclusion of the demonstration set (xj, yj, rj)m j=1. Notably, the NLEs generated by the aforementioned approaches can be seamlessly integrated\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cebe/cebe6b02-9f38-421d-8b63-1a8c6da3b4b2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Illustrction of using LLM-generated NLEs for ICL: (1) prompt an LLM in a few-shot or zero-shot man to generate NLEs for new instances; (2) prompt LLMs using ICL with the NLEs generated in step 1.</div>\ninto the existing X-ICL framework as delineated in Section 3.1. We primarily focus on using GPT3.5 (more specifically, GPT3.5-turbo-0613 \u2013 we will refer to this model as ChatGPT) to synthesize NLEs. Given that LLMs, such as ChatGPT, may have been trained on datasets incorporating NLEs, it challenges the assumption of genuine zero- or few-shot learning scenarios. To clarify terminology and avoid confusion, we redefine \u2018zero-shot learning\u2019 as the absence of demonstration sets, and \u2018few-shot ICL\u2019 as learning that utilizes a demonstration set. Thus, we denote the aforementioned two approaches as zs-X-ICL (ChatGPT) and fsX-ICL (ChatGPT), respectively. In addition, we explore the application of two other widely used open-source LLMs for generating NLEs. Detailed results of these experiments are provided in Appendix C.\n# 4 Experiments\nWe conduct a series of experiments to assess the performance of our proposed X-ICL framework.\n# 4.1 Experimental Setup\nTasks and datasets We consider the Natural Language Inference (NLI) and paraphrasing identification tasks as our testbed. To ascertain the robustness of LLMs when employing the proposed approach, we evaluate it across eight adversarial datasets. For the NLI task, we include HANS, ISCS, ST, PICD, PISP, NaN, and ANLI. The first five datasets (HANS, ISCS, ST, PICD, PISP) are from Liu et al. (2020b), while NaN and ANLI are sourced from Truong et al. (2022) and Nie et al. (2020), respectively. Regarding the paraphrasing identification task, we use the PAWS-QQP (or PAWS) dataset (Zhang et al., 2019). Additionally, the SNLI dataset (Bowman et al., 2015) and QQP (Wang et al., 2018), which are non-adversarial, are employed for a comparative purpose. The details of these datasets are provided in Appendix A.\nLanguage models and prompts The evaluation of our approach is undertaken across five prominent LLMs: (1) Mistral, (2) Zephyr, (3) Vicuna, (4) Llama2, and (5) GPT3.5-turbo (version 0613).\n<div style=\"text-align: center;\">Natural Language Inference</div>\nModels\nMethods\nNatural Language Inference\nParaphrasing\nAvg.\nSNLI\nHANS\nISCS\nNaN\nST\nPICD\nPISP\nANLI\nQQP\nPAWS\nMistral 7B\nICL\n59.8\n54.0\n51.9\n55.0\n44.4\n58.2\n23.0\n39.8\n69.9\n68.3\n50.3\n\u00b13.4\n\u00b12.2\n\u00b11.4\n\u00b11.3\n\u00b11.7\n\u00b12.6\n\u00b12.6\n\u00b14.6\n\u00b11.7\n\u00b12.7\nX-ICL (Human)\n60.0\n56.0\n54.7\u25bd\n58.6\u25bd\n51.7\u25bc\n56.9\n35.8\u25bc\n43.9\u25bc\n69.9\n66.4\n53.5\n\u00b12.0\n\u00b12.9\n\u00b12.5\n\u00b12.9\n\u00b14.0\n\u00b13.3\n\u00b16.7\n\u00b11.7\n\u00b10.8\n\u00b11.5\nzs-X-ICL (ChatGPT)\n56.7\n51.8\n47.7\n55.9\n44.9\n56.7\n25.1\n28.8\n67.3\n64.7\n46.4\n\u00b16.3\n\u00b15.1\n\u00b13.5\n\u00b15.0\n\u00b14.8\n\u00b16.6\n\u00b18.9\n\u00b14.4\n\u00b12.3\n\u00b13.1\nfs-X-ICL (ChatGPT)\n61.8\n58.2\u25bc\n57.2\u25bc\n62.4\u25bc\n55.2\u25bc\n59.2\n47.6\u25bc\n46.9\u25bc\n70.3\n72.5\u25bd\n57.1\n\u00b13.1\n\u00b12.5\n\u00b12.2\n\u00b12.6\n\u00b11.5\n\u00b12.7\n\u00b11.8\n\u00b12.3\n\u00b11.1\n\u00b11.3\nZephyr 7B\nICL\n67.1\n71.0\n63.4\n65.7\n60.5\n64.8\n48.4\n47.1\n76.9\n57.7\n59.8\n\u00b13.4\n\u00b11.8\n\u00b11.2\n\u00b11.8\n\u00b11.0\n\u00b11.5\n\u00b11.4\n\u00b11.6\n\u00b10.4\n\u00b11.1\nX-ICL (Human)\n72.4\u25bc\n64.3\n58.3\n62.0\n57.0\n60.6\n52.0\n49.4\n75.8\n61.4\u25bd\n59.3\n\u00b14.3\n\u00b16.7\n\u00b15.5\n\u00b15.3\n\u00b16.3\n\u00b19.7\n\u00b16.7\n\u00b13.0\n\u00b11.7\n\u00b12.3\nzs-X-ICL (ChatGPT)\n67.2\n72.7\n60.4\n64.0\n61.4\n64.1\n50.8\n40.9\n74.7\n59.1\n58.1\n\u00b13.9\n\u00b12.6\n\u00b15.3\n\u00b15.2\n\u00b15.7\n\u00b15.4\n\u00b15.2\n\u00b13.8\n\u00b11.8\n\u00b12.4\nfs-X-ICL (ChatGPT)\n74.2\u25bc\n77.4\u25bc\n67.0\n67.7\n69.3\u25bc\n70.0\u25bc\n65.6\u25bc\n52.1\u25bd\n77.3\n61.5\u25bd\n65.5\n\u00b13.6\n\u00b12.2\n\u00b11.6\n\u00b12.3\n\u00b11.5\n\u00b12.1\n\u00b12.5\n\u00b12.8\n\u00b10.9\n\u00b11.0\nVicuna 30B\nICL\n65.2\n69.4\n62.7\n61.4\n58.7\n67.1\n50.9\n50.0\n81.8\n69.7\n61.4\n\u00b12.7\n\u00b11.2\n\u00b10.9\n\u00b13.5\n\u00b10.8\n\u00b11.6\n\u00b11.3\n\u00b12.6\n\u00b10.5\n\u00b12.6\nX-ICL (Human)\n67.8\n62.9\n60.9\n64.2\n57.3\n63.7\n55.0\n48.2\n77.4\n63.4\n59.8\n\u00b13.2\n\u00b13.7\n\u00b12.2\n\u00b11.2\n\u00b12.0\n\u00b17.2\n\u00b15.8\n\u00b14.7\n\u00b12.8\n\u00b13.5\nzs-X-ICL (ChatGPT)\n64.2\n61.4\n64.9\n60.2\n61.7\n57.9\n51.8\n49.7\n72.1\n61.8\n58.8\n\u00b15.9\n\u00b17.7\n\u00b12.3\n\u00b14.0\n\u00b13.1\n\u00b18.7\n\u00b18.7\n\u00b13.6\n\u00b13.2\n\u00b14.9\nfs-X-ICL (ChatGPT)\n65.0\n74.5\u25bd\n65.5\u25bd\n66.3\u25bd\n64.8\u25bc\n61.6\n65.9\u25bc\n57.5\u25bc\n78.6\n70.0\n65.4\n\u00b13.1\n\u00b14.4\n\u00b11.6\n\u00b11.1\n\u00b11.8\n\u00b18.9\n\u00b14.7\n\u00b11.3\n\u00b11.7\n\u00b13.3\nLlama2 70B\nICL\n69.3\n65.7\n63.1\n61.5\n58.8\n67.6\n48.5\n54.2\n80.8\n44.5\n60.3\n\u00b11.2\n\u00b13.4\n\u00b11.6\n\u00b12.3\n\u00b14.4\n\u00b13.0\n\u00b17.3\n\u00b12.9\n\u00b10.6\n\u00b12.9\nX-ICL (Human)\n73.0\u25bc\n65.2\n59.6\n62.4\n55.7\n64.3\n50.4\n49.0\n74.5\n42.6\n57.7\n\u00b13.1\n\u00b14.6\n\u00b14.4\n\u00b13.3\n\u00b13.9\n\u00b12.3\n\u00b15.1\n\u00b12.6\n\u00b13.0\n\u00b13.3\nzs-X-ICL (ChatGPT)\n55.4\n64.0\n37.4\n58.1\n47.7\n53.5\n44.2\n35.8\n69.1\n37.8\n48.1\n\u00b15.5\n\u00b16.3\n\u00b16.0\n\u00b15.4\n\u00b15.4\n\u00b18.5\n\u00b18.7\n\u00b10.8\n\u00b14.1\n\u00b14.8\nfs-X-ICL (ChatGPT)\n74.2\u25bc\n73.3\u25bc\n57.7\n65.9\u25bd\n63.1\u25bd\n70.6\u25bd\n55.8\u25bc\n59.2\u25bc\n77.6\n46.5\u25bd\n63.6\n\u00b12.5\n\u00b18.5\n\u00b11.2\n\u00b13.2\n\u00b13.7\n\u00b16.5\n\u00b15.9\n\u00b11.6\n\u00b10.6\n\u00b11.9\nGPT3.5-turbo\nICL\n71.9\n72.4\n64.4\n70.0\n62.1\n64.0\n51.2\n56.1\n81.5\n42.9\n62.4\n\u00b11.4\n\u00b10.6\n\u00b10.9\n\u00b10.8\n\u00b11.6\n\u00b13.1\n\u00b10.4\n\u00b12.0\n\u00b10.3\n\u00b12.8\nX-ICL (Human)\n78.0\u25bc\n71.0\n69.0\u25bd\n70.5\n65.7\u25bd\n72.7\u25bc\n59.3\u25bd\n59.8\u25bd\n76.0\n53.4\u25bc\n66.2\n\u00b11.7\n\u00b11.7\n\u00b11.2\n\u00b12.2\n\u00b11.0\n\u00b11.3\n\u00b11.9\n\u00b12.3\n\u00b13.9\n\u00b15.3\nzs-X-ICL (ChatGPT)\n71.9\n71.6\n68.4\u25bd\n70.2\n67.6\u25bd\n67.7\u25bd\n61.7\u25bc\n60.4\u25bc\n80.4\n51.2\u25bc\n66.0\n\u00b12.7\n\u00b10.8\n\u00b10.3\n\u00b10.0\n\u00b11.3\n\u00b14.1\n\u00b11.9\n\u00b12.0\n\u00b10.8\n\u00b13.1\nfs-X-ICL (ChatGPT)\n75.5\u25bd\n76.0\u25bc\n74.9\u25bc\n73.1\u25bc\n73.3\u25bc\n76.9\u25bc\n75.5\u25bc\n59.6\u25bd\n79.0\n54.0\u25bc\n69.7\n\u00b12.8\n\u00b12.0\n\u00b10.1\n\u00b11.4\n\u00b10.4\n\u00b10.4\n\u00b13.0\n\u00b11.8\n\u00b11.7\n\u00b12.6\nTable 1: Accuracy of multiple LLMs using (1) standard ICL without NLEs, (2) X-ICL with human-written NLEs: X-ICL (Human), (3) X-ICL with ChatGPT-generated NLEs in a zero-shot scenario: zs-X-ICL (ChatGPT), (4) X-ICL with ChatGPT-generated NLEs in a few-shot scenario: fs-X-ICL (ChatGPT). The best performance for each task within a model is shown in bold. Significance testing was assessed via an unequal variances t-test in comparison with ICL: \u25bc(resp. \u25bd) represents a p-value lower than 10-3 (resp. 10-1). The results of ANLI are the average of ANLI R1, R2, and R3.\nSpecifically, the Mistral and Zephyr models have 7B parameters each. For Vicuna and Llama2, we use the 30B and 70B versions, respectively.\nWe perform all X-ICL experiments in an 8-shot setting, wherein each experiment is conducted four times independently, thereby drawing 32 unique instances from the training-associated datasets as follows. Specifically, for NLI datasets (except ANLI, which includes its own training set and NLEs), we adhere to the established methodology of using the e-SNLI dataset as the demonstration set, as suggested by Liu et al. (2020b). The e-SNLI dataset is a modified version of SNLI, where each instance is annotated with NLEs written by humans. In the case of the QQP and PAWS datasets, the QQP\ndataset is utilized as the demonstration set. As no NLEs are available, we contribute the corresponding NLEs (refer to Appendix E). Regarding the generation of NLEs via few-shot learning described in section 3.2, the methodology involves selecting a random instance from each label category within the training dataset to form the demonstration set. Consequently, the demonstration set comprises three instances for the e-SNLI dataset and two for the QQP dataset. Baselines In addition to the proposed method, our study investigates two baselines for comparative analysis. The first baseline uses standard ICL without NLEs. The second employs human-written NLEs within the X-ICL process, referred to as X-\ndataset is utilized as the demonstration set. As no NLEs are available, we contribute the corresponding NLEs (refer to Appendix E). Regarding the generation of NLEs via few-shot learning described in section 3.2, the methodology involves selecting a random instance from each label category within the training dataset to form the demonstration set. Consequently, the demonstration set comprises three instances for the e-SNLI dataset and two for the QQP dataset.\n# 4.2 Main Results\nThis section examines ICL and X-ICL across the studied datasets using Mistral, Zephyr, Vicuna, Llama2, and GPT3.5-turbo. The results are summarized in Table 1. The results demonstrate a consistent outcome across both scenarios: with and without the application of X-ICL. As the capabilities of the models increase, there is a noticeable improvement in average accuracy. This progression is evident when comparing the least potent model, exemplified by Mistral, to the most advanced one, represented by GPT3.5-turbo. Table 1 demonstrates that X-ICL (Human) yields better predictive accuracy than ICL across all five LLMs assessed using the SNLI dataset, with enhancements of up to 6.1%. This performance elevation is, however, limited to the Mistral and GPT3.5-turbo models when subjected to all adversarial NLI test sets. The advantage of X-ICL (Human) relative to ICL diminishes when applied to the QQP and PAWS datasets. For fs-X-ICL (ChatGPT), both Mistral and Zephyr demonstrate a significant performance advantage in all evaluated tasks, outperforming ICL and X-ICL (Human) by at least 5.7% and 3.6%, respectively. Despite the notable improvement on ICL when employing GPT3.5-turbo in comparison to other LLMs, fs-X-ICL (ChatGPT) offers substantially additional gains, with an increase in absolute accuracy between 11%-24% on tasks such as ISCS, ST, PICD, PISP, and PAWS. This suggests that X-ICL enhances LLM effectiveness on in-distribution test sets and increases their robustness against adversarial test sets. Remarkably, despite the predominant preference of human evaluators for NLEs generated by GPT3.5 over those written by humans, zs-X-ICL (ChatGPT) consistently produces less accurate results than X-ICL (Human) across all models under study. The exception to this trend is GPT3.5-turbo, where a tie is observed. Furthermore, it appears counter-intuitive that zs-X-ICL (ChatGPT) is outperformed by ICL for 4 out of the 5 LLMs analyzed, especially on Llama2. We conduct a systematic analysis in section 4.4 to understand this apparent discrepancy between human preferences and LLM performance. In light of the encompassment of diverse robustness scenarios by the seven adversarial NLI\nModels\nMethods\nSNLI\nAdvNLI\n\u2206\nZephyr\nICL\n67.1\n57.2\n9.9\nfs-X-ICL (ChatGPT)\n74.2\n63.7\n10.5\nCOSINE\n77.0\n55.6\n21.4\nBM25\n70.1\n53.7\n16.4\nSET-BSR\n79.9\n59.7\n20.2\nGPT3.5-turbo\nICL\n71.9\n61.4\n10.5\nfs-X-ICL (ChatGPT)\n75.5\n69.8\n5.6\nCOSINE\n75.0\n58.1\n16.9\nBM25\n71.4\n56.0\n15.4\nSET-BSR\n77.4\n59.5\n17.9\n<div style=\"text-align: center;\">Models Methods</div>\nTable 2: Performance of ICL, fs-X-ICL (ChatGPT) and three data selection approaches on SNLI and AdvNLI (i.e., seven adversarial test sets). \u2206indicates the difference between SNLI and adversarial NLI test sets. We report the average performance over all adversarial test sets.\n# datasets, our primary focus henceforth will be the examination of these NLI datasets.\ndatasets, our primary focus henceforth will be the examination of these NLI datasets.\n# 4.3 Impacts of NLEs\nOur research has demonstrated that using NLEs generated by GPT3.5 can substantially enhance the performance of X-ICL. To provide a more comprehensive understanding of the NLEs\u2019 influence, we conducted two investigations, presented below.\nData selection vs. X-ICL. The effectiveness of ICL in LLMs is closely linked to the quality of demonstrations provided, as these demonstrations are critical for the model\u2019s ability to understand and address the test instances (Zhao et al., 2021; Liu et al., 2022; Lu et al., 2022). Consequently, considerable research has focused on developing data selection techniques to optimize the curation of ICL demonstrations from relevant candidate data pools, aiming to enhance their alignment with the test instances (Gupta et al., 2023; Levy et al., 2023; Ye et al., 2023). While these approaches have proven to be highly effective on in-distribution test sets, their performance on adversarial test sets remains uncertain, as these sets have the potential to misguide the selection algorithms. In this context, we compare the performance of fs-X-ICL (ChatGPT) to three prevalent data selection techniques: COSINE, BM25, and SETBSR. COSINE incorporates sentence embeddings (Reimers and Gurevych, 2019) to identify the most relevant demonstrations for each test instance, while BM25 employs the BM25 algorithm (Sparck Jones et al., 2000) for retrieving candidate demonstrations. SET-BSR utilizes BERTScore (Zhang\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3137/31379691-260f-464e-8c70-cf5860aa974a.png\" style=\"width: 50%;\"></div>\nFigure 3: ICL performance of GPT3.5-turbo using (1) standard ICL without NLEs, (2) X-ICL with GPT3.5generated NLEs in a few-shot scenario: fs-X-ICL (ChatGPT), (3) X-ICL with GPT3.5-generated NLEs, where the NLEs of the prompt are swapped and do not match the instances: fs-X-ICL (ChatGPTswap), and (4) X-ICL with random human NLEs: X-ICL (Humanrand).\net al., 2020), integrated with set theory, to ensure comprehensive information coverage and diversity within the selected instances (Gupta et al., 2023). Note that these data selection techniques are designed to sift through the entirety of the training data to choose demonstrations, a computationally demanding and computationally expensive process for generating NLEs for the full dataset. Therefore, our analysis is confined to applying ICL to these methods. To facilitate a generic comparison with the in-distribution set, we consider the average performance across all adversarial NLI test sets. According to Table 2, as expected, the data selection approaches markedly enhance ICL performance on the SNLI dataset for all studied LLMs, with notable improvements observed in SET-BSR, achieving gains of up to 17.8% over standard ICL. However, this pronounced advantage diminishes considerably on adversarial test sets, particularly for COSINE and BM25 models, which are outperformed by ICL across all tested LLMs. This discrepancy results in a marked disparity between the in-distribution and adversarial test sets, contrary to what is observed in fs-X-ICL (ChatGPT). These results imply that current data selection approaches may be prone to overfitting on in-distribution tests, potentially leading to significant challenges in processing OOD and adversarial datasets due to their limited generalizability.\nDo proper NLEs really help? The prevailing assumption argues that the benefits of the X-ICL pri-\nPremise: None of them supported her.\nHypothesis: One of them supported her.\nNLE [X-ICL (Human) ]: If none of them sup-\nported her, then one of them did not support\nher.\nNLE [fs-X-ICL (ChatGPT) ]: The hypothesis\ncontradicts the given premise, which states that\nnone of them supported her.\nPremise: Not all people have had the opportu-\nnities you have had.\nHypothesis: Some people have not had the\nopportunities you have had.\nNLE [X-ICL (Human) ]: If not all people have\nhad the opportunities you have had, then some\npeople have not had the opportunities you have\nhad.\nNLE [fs-X-ICL (ChatGPT) ]: The hypothesis\nis a direct result of the premise, and the label\nassigned is entailment.\nTable 3: Two test examples from the NAN dataset and the corresponding NLEs generated by X-ICL (Human) and fs-X-ICL (ChatGPT) using Zephyr.\nmarily originate from the NLEs provided. To conclusively attribute these gains to the NLEs rather than any potential influence of additional sentences, we investigate two experimental setups. In the first setup, we randomly swap the NLEs within the prompt, leading to a mismatched NLE for each instance. This variant is henceforth referred to as fs-X-ICL (ChatGPTswap). Regarding the second variant, for each instance in the demonstration set, we randomly select an unrelated human NLE from the corresponding training set, referred to as X-ICL (Humanrand). As depicted in Figure 3, despite identical content being provided to GPT3.5-turbo, a misalignment between the NLE and the instance results in a marked reduction in the performance of fs-XICL (ChatGPTswap) when compared to fs-X-ICL (ChatGPT). This decline is discernible across various datasets, including NaN, PICD, and ANLI (R1/R2).4 It is also shown that an irrelevant and arbitrary NLE triggers a performance reduction within the X-ICL framework. Furthermore, the efficiency of both fs-X-ICL (ChatGPTswap) and XICL (Humanrand) substantially lags behind that of ICL. Therefore, it can be inferred that the efficacy of the fs-X-ICL (ChatGPT) hinges on providing an accurate and relevant NLE.\n4Similar patterns have been detected in other datasets\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eaa2/eaa29824-923c-4129-88a7-db06be253888.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: ROUGE-L between the NAN test set and the corresponding generated NLEs. Top: ROUGE-L betw test premise and NLE. Bottom: ROUGE-L between test hypothesis and NLE.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5547/5547a81a-965a-4b6a-b80e-99cca4184578.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Average length (#words) of NLEs generated by fs-X-ICL (ChatGPT) and zs-X-ICL (ChatGPT).</div>\n# 4.4 Further Analysis\nWhy is fs-X-ICL (ChatGPT) producing the most accurate results? Our study demonstrates that fsX-ICL (ChatGPT) surpasses both X-ICL (Human) and zs-X-ICL (ChatGPT) in accuracy. However, the reasons behind this superior performance are not yet understood. Therefore, this section focuses on systematically analyzing the efficacy of fs-XICL (ChatGPT). We first dissect the effectiveness of fs-X-ICL (ChatGPT) over X-ICL (Human). As shown in Table 3, NLEs from X-ICL (Human) are mere verbatim copies of inputs rather than insightful explanations. To substantiate this, we calculate the ROUGE-L scores between the NAN test set and the corresponding NLEs from X-ICL (Human) and fs-X-ICL (ChatGPT) as a means of similarity measurement. As depicted in Figure 4, NLEs from X-ICL (Human) often replicate the given premise and hypothesis, resulting in high ROUGE-L scores. Instead, fs-X-ICL (ChatGPT) can produce meaningful NLEs, demonstrating lower similarity to the test instances. After analyzing the NLEs from zs-X-ICL (Chat-\nMethods\nMistral\nZephyr\nVicuna\nX-ICL (Human)\n53.5\n59.3\n59.8\nzs-X-ICL (ChatGPT)\n46.4\n58.1\n58.8\nzs-X-ICL (ChatGPTs)\n56.2\n62.3\n63.4\nfs-X-ICL (ChatGPT)\n57.1\n65.5\n62.1\nTable 4: Average accuracy of X-ICL (Human), zs-XICL (ChatGPT), zs-X-ICL (ChatGPTs) and fs-X-ICL (ChatGPT) among all test sets.\nGPT), we attribute the inefficiency to verbose NLEs. Specifically, Figure 5 shows that zs-XICL (ChatGPT) produces longer NLEs than fs-XICL (ChatGPT). As a result, we observe inconsistency within the NLEs, leading to incorrect predictions. As a remedy, we prompt ChatGPT to generate shorter NLEs in the zero-shot setting, denoted as zs-X-ICL (ChatGPTs). Compared to zs-XICL (ChatGPT), the NLEs generated by zs-X-ICL (ChatGPTs) are reduced to an average of 27 tokens. Consequently, with the help of the concise NLEs, we can improve the accuracy significantly and even surpass the X-ICL (Human) as shown in Table 4.\n# 5 Summary and Outlook\nWe introduced a simple yet effective method called fs-X-ICL (ChatGPT), leveraging human-written NLEs to generate synthetic NLEs by prompting ChatGPT. fs-X-ICL (ChatGPT) significantly boosts accuracy across various adversarial datasets and five LLMs, compared to standard in-context learning and X-ICL using human-written NLEs. Additionally, our analysis revealed that data selection methodologies may exhibit overfitting within the in-distribution dataset, thus potentially failing to extend to unseen or adversarial datasets. In contrast, our approach employing NLEs has shown consistent performance in both in-distribution and\nadversarial contexts. Our work paves the way for more robust performance and enhanced explainability capabilities of LLMs.\n# Limitations\nOne limitation of X-ICL might be the observed lack of fidelity in the NLEs generated by LLMs, despite their capability to provide accurate answers. These NLEs may sometimes include unfaithful or hallucinated information, which if relied upon by users for model trust, can lead to severe implications. Testing and enhancing the faithfulness of NLEs is a challenging open question (Atanasova et al., 2023). In this work, we show that X-ICL improves robustness, but we do not advocate using the generated NLEs as faithful explanations without further testing. Second, our approach exhibited promising results when tested against adversarial datasets in two notable NLP tasks: natural language inference and paraphrasing identification. However, further research is required to examine the performance of LLMs and their generalizability across diverse NLP tasks in the context of adversarial examples.\n# Acknowledgements\nXuanli He was supported by an industry grant from Cisco. Oana-Maria Camburu was supported by a Leverhulme Early Career Fellowship. Pasquale Minervini was partially funded by the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement no. 875160, ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence) EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and a donation from Accenture LLP; and is grateful to NVIDIA for the GPU donations. This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh.\n# References\nDavid Alvarez-Melis and Tommi Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence-to-sequence models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 412\u2013421, Copenhagen, Denmark. Association for Computational Linguistics.\nPepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. 2023. Faithfulness Tests for Natural Language Explanations. In ACL.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. CoRR, abs/2302.04023. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP, pages 632\u2013642. The Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations. Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization improve robustness? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3792\u20133805. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2019. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4069\u20134082, Hong Kong, China. Association for Computational Linguistics. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. CoRR, abs/2301.07597.\ncomparison corpus, evaluation, and detection. CoRR, abs/2301.07597. Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2023. Coverage-based example selection for incontext learning. arXiv preprint arXiv:2305.14907. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107\u2013112, New Orleans, Louisiana. Association for Computational Linguistics. Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. 2020. Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4351\u20134367, Online. Association for Computational Linguistics. He He, Sheng Zha, and Haohan Wang. 2019. Unlearn dataset bias in natural language inference by fitting the residual. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132\u2013142, Hong Kong, China. Association for Computational Linguistics. Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. 2018. Grounding visual explanations. In Proceedings of the European Conference on Computer Vision (ECCV). Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. 2020. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8706\u20138716, Online. Association for Computational Linguistics. Pride Kavumba, Ana Brassard, Benjamin Heinzerling, and Kentaro Inui. 2023. Prompting for explanations improves adversarial NLI. is this true? Yes it is true because it weakens superficial cues. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2165\u20132180, Dubrovnik, Croatia. Association for Computational Linguistics. Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. 2021. e-ViL: A dataset and benchmark for natural language explanations in vision-language tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1244\u20131254.\nShivanshu Gupta, Sameer Singh, and Matt Gardner. 2023. Coverage-based example selection for incontext learning. arXiv preprint arXiv:2305.14907.\nMaxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. 2021. e-ViL: A dataset and benchmark for natural language explanations in vision-language tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1244\u20131254.\nMaxime Kayser, Cornelius Emde, Oana-Maria Camburu, Guy Parsons, Bartlomiej Papiez, and Thomas Lukasiewicz. 2022. Explaining chest x-ray pathologies in natural language. In Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2022, pages 701\u2013713, Cham. Springer Nature Switzerland.\nJinkyu Kim, Anna Rohrbach, Trevor Darrell, John F. Canny, and Zeynep Akata. 2018. Textual explanations for self-driving vehicles. CoRR, abs/1807.11546.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\nJosh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. 2023. Explanation-based finetuning makes models more robust to spurious cues. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4420\u20134441, Toronto, Canada. Association for Computational Linguistics.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932\u20134942, Florence, Italy. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"why should i trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, page 1135\u20131144, New York, NY, USA. Association for Computing Machinery. Oscar Sainz, Jon Ander Campos, Iker Garc\u00eda-Ferrero, Julen Etxaniz, and Eneko Agirre. 2023. Did chatgpt cheat on your test? William A. Scott. 1962. Cognitive complexity and cognitive flexibility. Sociometry, 25(4):405\u2013414. Sofia Serrano and Noah A. Smith. 2019. Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931\u20132951, Florence, Italy. Association for Computational Linguistics. K. Sparck Jones, S. Walker, and S.E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments: Part 1. Information Processing and Management, 36(6):779\u2013808. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615. Joe Stacey, Yonatan Belinkov, and Marek Rei. 2022. Supervising model attention with human explanations for robust natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11349\u201311357. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Thinh Hung Truong, Yulia Otmakhova, Timothy Baldwin, Trevor Cohn, Jey Han Lau, and Karin Verspoor.\n2022. Not another negation benchmark: The NaNNLI test suite for sub-clausal negation. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 883\u2013894, Online only. Association for Computational Linguistics.\n2022. Not another negation benchmark: The NaNNLI test suite for sub-clausal negation. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 883\u2013894, Online only. Association for Computational Linguistics. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics. Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. 2023a. Adversarial demonstration attacks on large language models. arXiv preprint arXiv:2305.14950. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS. Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to explain: A review of datasets for explainable natural language processing. 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944.\nSarah Wiegreffe and Ana Marasovic. 2021. Teach me to explain: A review of datasets for explainable natural language processing. 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks.\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans,\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans,\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. 2021. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6707\u20136723, Online. Association for Computational Linguistics. Yuxiang Wu, Matt Gardner, Pontus Stenetorp, and Pradeep Dasigi. 2022. Generating data to mitigate spurious correlations in natural language inference datasets. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2660\u20132676, Dublin, Ireland. Association for Computational Linguistics. Yadollah Yaghoobzadeh, Soroush Mehri, Remi Tachet des Combes, T. J. Hazen, and Alessandro Sordoni. 2021. Increasing robustness to spurious correlations using forgettable examples. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3319\u20133332, Online. Association for Computational Linguistics. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Tianyi Zhang, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298\u20131308, Minneapolis, Minnesota. Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\n# A Details of Datasets\nThe details of all studied datasets are delineated as follows\n SNLI Dataset: The SNLI dataset, a benchmark in natural language inference, encompasses approximately 570,000 human-annotated sentence pairs, each pair formed by a premise and a hypothesis. These sentences originate from an existing corpus of image captions, thus offering a broad spectrum of common subjects and linguistic structures (Bowman et al., 2015).\n HANS Dataset: McCoy et al. (2019) developed a dataset with the express purpose of scrutinizing the performance of models when confronted with sentences characterized by several types of distracting signals. These signals encompass the presence of lexical overlap, sub-sequences, and constituent heuristics between the corresponding hypotheses and premises.\n# \u2022 Datasets Sensitive to Compositionality (ISCS):\nAs proposed by Nie et al. (2019), a softmax regression model was employed to utilize lexical features present in the premise and hypothesis sentences, thereby generating instances of misclassification. Here, the Lexically Misleading Score (LMS) denotes the predicted probability of the misclassified label. Adapting the approach of Liu et al. (2020b), we concentrated on the subsets possessing LMS values exceeding 0.7.\n\u2022 Not another Negation (NaN) NLI Dataset: NaN dataset is developed to probe the capabilities of NLP models in comprehending sub-clausal negation (Truong et al., 2022).\n Stress Test Datasets (ST): Our analysis also incorporates various stress tests described by Naik et al. (2018) such as \u201cword overlap\u201d (ST-WO), \u201cnegation\u201d (ST-NE), \u201clength mismatch\u201d (ST-LM), and \u201cspelling errors\u201d (ST-SE). Specifically, STWO aims to identify lexical overlap heuristics between the premise and hypothesis, ST-NE seeks to detect intense negative lexical cues in partialinput sentences, ST-LM aspires to create misleading predictions by artificially lengthening the premise using nonsensical phrases, and ST-SE employs spelling errors as a means to deceive the model.\n\u2022 Datasets Detected by Classifier (PICD): In the approach proposed by Gururangan et al. (2018),\nfastText was applied to hypothesis-only inputs. Subsequent instances from the SNLI test sets (Bowman et al., 2015) that could not be accurately classified were designated as \u2018hard\u2019 instances.\n# \u2022 Surface Pattern Datasets (PISP): Liu et al\n(2020a) identified surface patterns that exhibit strong correlation with specific labels, thereby proposing adversarial test sets counteracting the implications of surface patterns. As suggested by Liu et al. (2020b), we employed their \u2018hard\u2019 instances extracted from the MultiNLI mismatched development set (Williams et al., 2018) as adversarial datasets.\n# \u2022 Adversarial NLI (ANLI): ANLI dataset \n Adversarial NLI (ANLI): ANLI dataset (Nie et al., 2020) is a challenging resource created for training and testing models on NLI, featuring adversarial examples intentionally curated to obfuscate or mislead benchmark models, thereby increasing its challenge factor. This dataset is constructed in multiple rounds, with each subsequent round featuring human-created examples specifically designed to outsmart models trained on the previous rounds. In total, the dataset comprises three distinct rounds, specifically ANLI R1, ANLI R2, and ANLI R3, highlighting the layered complexity of this resource.\n\u2022 Quora Question Pairs (QQP)\ndataset (Wang et al., 2018) comprises pairs of questions sourced from the Quora community question-answering platform. The primary objective is to ascertain whether each question pair exhibits semantic equivalence.\n# \u2022 Paraphrase Adversaries from Word Scr\nbling (PAWS): The PAWS-QQP dataset (Zhang et al., 2019), derived from the QQP datasets, targets the intricate task of paraphrasing identification, emphasizing the differentiation of sentences that, despite high lexical similarity, convey distinct meanings. It incorporates adversarial examples generated via word scrambling, presenting a stringent assessment for NLP models.\n# B Meta-prompts for Generating Synthetic NLEs\nTable 5 and 6 present the meta-prompts and demonstration instances employed for producing NLEs utilizing ChatGPT in zero- and few-shot scenarios.\n<div style=\"text-align: center;\">Meta-prompt for zero-shot generation</div>\nMeta-prompt for zero-shot generation\nAssume that you\u2019re an expert working on natu-\nral language inference tasks. Given a premise,\na hypothesis, and the corresponding label.\nPlease write a concise and precise reason to\nexplain why the label is assigned to the exam-\nple:\n<div style=\"text-align: center;\">Meta-prompt and demonstration instances for few-shot generation</div>\nfor few-shot generation\nAssume that you\u2019re an expert working on natu-\nral language inference tasks. Given a premise,\na hypothesis, and the corresponding label.\nPlease write a concise and precise reason to\nexplain why the label is assigned to the exam-\nple by following the provided examples:\nPremise: A boy peers out of an open window.\nHypothesis: The boy looks out the window.\nLabel: entailment\nNLE: The boy peers out of a window, so the\nboy looks out the window.\n=====\nPremise: A kid doing a trick on a skateboard.\nHypothesis: The kid eating lunch inside the\ncafeteria.\nLabel: contradiction\nNLE: The kid cannot be doing a trick and\neating lunch at the same time\n=====\nPremise: A man jumps off of his skateboard\non the top of a cement ramp.\nHypothesis: a man jumps off a skateboard at\nthe top of a ramp.\nLabel: neutral\nNLE: A man can jump off a skateboard with-\nout being at the top of a ramp.\nTable 5: Meta-prompts used to generate NLEs via ChatGPT in zero- and few-shot scenarios for natural language inference tasks.\n# C Supplementary Studies\nUsing NLEs Generated by Vicuna and Llama2. Our research demonstrates that the integration of NLEs generated by ChatGPT significantly enhances the performance of X-ICL for five advanced LLMs. To assess the efficacy of these ChatGPTgenerated NLEs, we explore the generation of synthetic NLEs using Vicuna and Llama2, ranked as\n<div style=\"text-align: center;\">Meta-prompt for zero-shot generation</div>\nMeta-prompt for zero-shot generation\nAssume that you\u2019re an expert working on para-\nphrasing identification tasks. Given two sen-\ntences and the corresponding label. Please\nwrite a concise and precise reason to explain\nwhy the label is assigned to the example:\nMeta-prompt and demonstration instances\nfor few-shot generation\nAssume that you\u2019re an expert working on para-\nphrasing identification tasks. Given two sen-\ntences and the corresponding label. Please\nwrite a concise and precise reason to explain\nwhy the label is assigned to the example by\nfollowing the provided examples:\nQ1: Does life get harder as you get older?\nQ2: Does life really get harder as you get\nolder?\nLabel: duplicate\nNLE: Both questions ask whether life does\nget harder as you get older.\n=====\nQ1: What is the National nanotechnology ini-\ntiative?\nQ2: What is the lead time for SSN4EGS411\nboard?\nLabel: not duplicate\nNLE: completely different questions\nMeta-prompt for zero-shot generation\nAssume that you\u2019re an expert working on para-\nphrasing identification tasks. Given two sen-\ntences and the corresponding label. Please\nwrite a concise and precise reason to explain\nwhy the label is assigned to the example:\n<div style=\"text-align: center;\">Meta-prompt and demonstration instances for few-shot generation</div>\nTable 6: Meta-prompts used to generate NLEs via ChatGPT in zero- and few-shot scenarios for paraphrasing identification tasks.\nthe third and second-best models respectively. Likewise, these NLEs are generated in a few-shot setting, referred to herein as Vicunafew and Llama2few, respectively. To ensure a fair comparison, we employ Vicuna as the underlying model to evaluate fs-X-ICL(Vicuna), fs-X-ICL (Llama2), and fs-XICL (ChatGPT) on all studied datasets. Our results, detailed in Table 7, highlight that X-ICL generally gains greater benefit from LLMgenerated NLEs as opposed to those produced by humans. Meanwhile, fs-X-ICL (ChatGPT) consistently outperforms fs-X-ICL(Vicuna) and fs-X-ICL (Llama2) considerably, except for ANLI R1 and R2. These findings suggest that to harness the potential of AI-generated NLEs fully, the employment of a powerful LLM is integral.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/497e/497ec0f0-beba-4365-8699-489cb7d0cde3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: ICL performance of Llama2 (7B, 13B, 70B) using (1) standard ICL without NLEs, (2) X-ICL with human-written NLEs: X-ICL (Human), (3) X-ICL with ChatGPT-generated NLEs in a zero-shot scenario: zs-X-ICL (ChatGPT), (4) X-ICL with ChatGPT-generated NLEs in a few-shot scenario:fs-X-ICL (ChatGPT). ANLI is the average of R1, R2 and R3.</div>\nTasks\nNLEs\nfs-Vicuna\nfs-Llama2\nfs-ChatGPT\nSNLI\n62.9 ( -5.0)\n64.1 ( -3.7)\n65.0 ( -2.9)\nHANS\n55.5 ( -7.4)\n67.4 ( +4.5)\n74.5 (+11.6)\nISCS\n65.1 ( +4.2)\n63.6 ( +2.7)\n65.5 ( +4.6)\nNaN\n62.6 ( -1.6)\n65.1 ( +0.9)\n66.3 ( +2.1)\nST\n59.5 ( +2.2)\n61.9 ( +4.6)\n64.8 ( +7.5)\nPICD\n60.2 ( -3.5)\n60.8 ( -2.9)\n61.6 ( -2.1)\nPISP\n66.0 (+11.0)\n66.1 (+11.1)\n66.0 (+11.0)\nANLI (R1)\n66.1 ( +9.1)\n65.8 ( +8.8)\n64.9 ( +7.9)\nANLI (R2)\n55.4 ( +6.5)\n55.9 ( +7.0)\n55.5 ( +6.6)\nANLI (R3)\n49.6 (+10.8)\n50.7 (+11.9)\n52.0 (+13.2)\nAverage\n60.3 ( +3.8)\n62.1 ( +5.6)\n63.5 ( +6.9)\nTable 7: ICL performance of Vicuna using (1) standard ICL without NLEs, (2) X-ICL with Vicuna-generated NLEs in a few-shot scenario: fs-Vicuna, (3) X-ICL with Llama2-generated NLEs in a few-shot scenario: fs-Llama2, (4) X-ICL with ChatGPT-generated NLEs in a few-shot scenario: fs-ChatGPT. Numbers in the parentheses represent differences compared to X-ICL (Human).\nDoes model size matter? We have shown the efficacy of X-ICL across a range of LLMs of varying sizes. However, the variability in data and training processes among these models renders the applicability of our approach to smaller-scale models inconclusive, especially since the smaller models often exhibit less benefit from NLEs compared to larger models within the same family (Wei et al., 2022a). Therefore, we have evaluated our approach using three distinct sizes of Llama2 models: 7B, 13B, and 70B parameters. Referring to Figure 6, one can find the perfor-\nmance of both ICL and X-ICL generally improves in correspondence with the escalation of model size, except for zs-X-ICL (ChatGPT). Moreover, the gap in performance between ICL and fs-X-ICL (ChatGPT) widens, indicating that models with greater capabilities derive increased benefits from NLEs. This observation aligns with the results reported by Wei et al. (2022a).\nDistribution Shift Prompting. Previous works indicate that X-ICL can potentially encourage LLMs to engage in deliberate thinking, a predominant factor responsible for substantial performance improvements over the standard ICL in complex reasoning tasks (Wei et al., 2022b). In addition, our findings have demonstrated a dramatic enhancement in the robustness of LLMs due to X-ICL, which contributes to significant improvements in ICL when applied to various adversarial datasets. Moreover, a previous study established that upon understanding the concept underlying particular tasks, humans can address similar tasks despite a distribution shift (Scott, 1962). To explore the robustness of ICL and X-ICL against distribution shifts, we employ the e-SNLI dataset as the demonstration set for ANLI (R1/R2), while utilizing the ANLI training set for testing NaN and PICD. Due to its outstanding performance, we use GPT3.5turbo as the backbone model. As suggested in Table 8, for NaN and PICD, using e-SNLI as the prompt proves to be more effective than ANLI for both ICL and fs-X-ICL (ChatGPT). This improvement can be attributed to the\nNaN\nPICD\nANLI (R1)\nANLI (R2)\ne-SNLI\nANLI\n|\u2206|\ne-SNLI\nANLI\n|\u2206|\ne-SNLI\nANLI\n|\u2206|\ne-SNLI\nANLI\n|\u2206|\nICL\n70.0\n69.4\n0.6\n64.0\n64.1\n0.1\n52.6\n62.4\n9.7\n43.9\n51.7\n7.8\nfs-X-ICL (ChatGPT)\n73.1\n71.8\n1.2\n76.9\n76.1\n0.8\n65.0\n68.5\n3.5\n53.2\n54.4\n1.2\nTable 8: Performance of ICL and fs-X-ICL (ChatGPT) employing e-SNLI and ANLI as prompts for testing NaN, PICD, and ANLI (R1/R2). |\u2206| signifies the absolute difference in the performance outcomes when utilizing e-SNLI in contrast to ANLI. The backbone model is GPT3.5-turbo.\ndistribution shift. Likewise, the distribution shift results in a noticeable distinction between e-SNLI and ANLI for ICL on ANLI (R1/R2). Nonetheless, incorporating NLEs enables fs-X-ICL (ChatGPT) to substantially reduce this gap, from 9.7 to 3.5 for ANLI (R1), and from 7.8 to 1.2 for ANLI (R2). This finding indicates that X-ICL may improve the robustness of LLMs in the face of distribution shifts.\nAnalysis on memorization LLMs such as ChatGPT have occasionally replicated instances from renowned benchmark datasets, including MNLI and BoolQ (Sainz et al., 2023). This unintentional \u2018contamination\u2019 might contribute to misconceptions regarding the superior performance of LLMs on these widespread benchmarks due to data memorization. Following Carlini et al. (2023), we merge the premise and hypothesis of each test instance into a single sentence, using the first part as the prefix. If an LLM could perfectly replicate the second part, we labeled the instance as \u2018extractable\u2019. Evaluating all studied models, we observe that the proportion of extractable instances is under 0.001% across all datasets and backbone models, indicating that the superior performance of LLMs might not be ascribed to memorization.\n# D Qualitative Analysis on NLEs\n# D.1 Qualitative Analysis on NLEs for Demonstration Set\nWe first conducted a qualitative analysis of NLEs generated by ChatGPT under zero- and few-shot scenarios, using the demonstration set as a basis. Note that each instance in the demonstration set has three distinct NLEs: (1) the zero-shot NLE from ChatGPT, (2) the few-shot NLE from ChatGPT, and (3) the human-written NLE. From these three NLEs per instance, one was randomly selected, and both the instance and the chosen NLE were incorporated into the evaluation set.\nSubsequently, this evaluation set was rated independently by four authors on a 5-point Likert scale to assess the quality of the NLEs. The scale ranges were 1 (extremely dissatisfied), 2 (dissatisfied), 3 (neutral), 4 (satisfied), and 5 (extremely satisfied). Finally, we calculated the average scores for both ChatGPT-generated and human-written NLEs for each evaluator.\n# D.2 Qualitative Analysis on NLEs for Inference Set\nWe also conducted a qualitative analysis of NLEs generated by fs-X-ICL (ChatGPT), utilizing GPT3.5-turbo as the foundational model. A total of 280 randomly sampled, correctly predicted examples from fs-X-ICL (ChatGPT) were distributed evenly among seven evaluators. These evaluators were tasked to assess the quality of the NLE for each assigned instance, based on the premisehypothesis pair and its corresponding correctly predicted label. The evaluators were required to rate the quality of the NLE using the aforementioned 5-point Likert scale. In case of dissatisfaction, they were asked to identify the reason from a list of predefined factors, including:\n# \u2022 template: The NLE simply restates the input and employs it as a justification.\n# \u2022 insufficient justification: The NLE requires more support for the prediction.\n\u2022 too verbose: The NLE is overly detailed and includes unnecessary information.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b136/b1368804-caf4-4573-ad36-32bea21c53d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Human evaluation on ChatGPT-generated NLEs for the correct predictions from fs-X-ICL (ChatGPT). Top: distribution of satisfaction scores. Bottom: distribution of reasons for dissatisfaction.</div>\nAccording to Figure 7, 46.6% and 39.3% of NLEs are marked as \u2018extremely satisfied\u2019 and \u2018satisfied\u2019 respectively, constituting 85.9% of the total 280 NLE samples. This suggests a high-quality output from GPT3.5-turbo in general. As for the lower-quality NLEs, the primary reasons for dissatisfaction include \u2018template\u2019, \u2018insufficient justification\u2019, and \u2018too verbose\u2019. Interestingly, this suggests that, despite the expressed dissatisfaction, evaluators generally did not find incorrect justifications in most instances.\n# E Human-written NLEs for QQP\nGiven the absence of NLEs for the QQP dataset, we have conducted a randomized sampling of 32 instances from the QQP training set. For each selected instance, we crafted a corresponding NLE. The details of these QQP instances and their respective NLEs are presented in Table 11, 12, 13, and 14.\nPremise: The organizing committee said 65 countries have entered the Lillehammer Olympic Games, matching the number of nations at the 1992 Winter Games in Albertville, France. Hypothesis: The Lillehammer Olympic games take place 12 years after the Albertville Olympic games. NLE: The premise does not provide any information about the time difference between the Lillehammer Olympic Games and the Albertville Olympic Games. Therefore, there is no evidence to support or contradict the hypothesis, resulting in a neutral label. Predicted Label: neutral Correct Label: neutral\nPremise: A woman was not injured when she was carjacked in west Charlotte Tuesday morning. According to Charlotte-Mecklenburg police, two men reportedly pulled a gun on a woman who was stopped at a stop sign on State Street at Whitehaven Avenue around 3:30 a.m. Police say the men then stole the woman\u2019s Honda Fit. Officers have not found the stolen vehicle. No other details were released. Copyright 2018 WBTV. All rights reserved. Hypothesis: A woman stole a Honda Fit in west Charlotte at 3:30 AM but was not injured. NLE: The premise states that a woman was carjacked in west Charlotte at 3:30 AM and that two men stole her Honda Fit. The hypothesis suggests that the woman stole a Honda Fit, which contradicts the information provided in the premise. Therefore, the hypothesis is false and leads to a contradiction. Predicted Label: contradiction Correct Label: contradiction\nPremise: New question. Last question. For you, Governor. And this flows somewhat out of the Boston debate. You, your running mate, your campaign officials have charged that Vice President Gore exaggerates, embellishes and stretches the facts, etcetera. Are you \u2013 do you believe these are serious issues? This is a serious issue that the voters should use in deciding which one of you two men to vote for on November 7? Hypothesis: the Vice President exaggerates the truth. NLE: The premise states that the question is about whether the Governor believes that the Vice President exaggerates, embellishes, and stretches the facts. Therefore, the hypothesis is a restatement of the question and can be confirmed as true based on the information provided in the premise. Predicted Label: entailment Correct Label: entailment\nPremise: Budapest consists of two parts, Buda and Pest, which are situated on opposite sides of the river and connected by a series of bridges. Hypothesis: Buda and Pest function as separate cities. NLE: The premise states that Budapest consists of two parts, Buda and Pest, which are connected by a series of bridges. However, it does not suggest that Buda and Pest function as separate cities. Therefore, the hypothesis cannot be confirmed or refuted based on the given information, resulting in a neutral label. Predicted Label: neutral Correct Label: neutral\nTable 9: Test instances and the corresponding generated NLEs from GPT-3.5-turbo for the correct predictions.\nPremise: Mulberry () is a 1986 South Korean drama film directed by Lee Doo-yong. Based on the eponymous classic story by Na Do-hyang, the film became known for its erotic subject matter, made possible by the government\u2019s gradual relaxation of censorship and control over the film industry during the 1980s. It was part of the wave of \"Folk erotic\" films that had become popular in South Korea at this time. Hypothesis: Mulberry () is a 1986 South Korean drama film directed by Lee Doo-dong. NLE: The hypothesis is identical to the premise, except for a minor spelling error in the director\u2019s name. Therefore, the hypothesis is true based on the provided premise. Predicted Label: entailment Correct Label: contradiction\nPremise: In the Company of Men is a 1997 Canadian/American black comedy written and directed by Neil LaBute and starring Aaron Eckhart, Matt Malloy, and Stacy Edwards. The film, which was adapted from a play written by LaBute, and served as his feature film debut, won him the Independent Spirit Award for Best First Screenplay. Hypothesis: Neil LaBute worked with Aaron Eckhart for the comedy that was his first screenplay. NLE: The premise states that Neil LaBute wrote and directed In the Company of Men, which was his feature film debut and won him the Independent Spirit Award for Best First Screenplay. It also mentions that the film starred Aaron Eckhart, but it does not provide any information about whether or not LaBute worked with Eckhart specifically for this film. Therefore, the hypothesis cannot be either true or false based on the provided premise. Predicted Label: neutral Correct Label: entailment\nPremise: Barry Cunningham (born 19 December 1952) is a British publisher, who worked for various publishers including Penguin Books and Bloomsbury before setting up Chicken House publishing in 2000. He is most well known for signing J.K Rowling, and publishing \"Harry Potter and the Philosopher\u2019s Stone\" in 1997. Hypothesis: Barry Cunningham is most proud of signing J.K Rowling, and publishing \"Harry Potter and the Philosopher\u2019s Stone\" in 1997. NLE: The premise states that Neil LaBute wrote and directed In the Company of Men, which was his feature film debut and won him the Independent Spirit Award for Best First Screenplay. It also mentions that the film starred Aaron Eckhart, but it does not provide any information about whether or not LaBute worked with Eckhart specifically for this film. Therefore, the hypothesis cannot be either true or false based on the provided premise. Predicted Label: entailment Correct Label: neutral\nPremise: Barry Cunningham (born 19 December 1952) is a British publisher, who worked for various publishers including Penguin Books and Bloomsbury before setting up Chicken House publishing in 2000. He is most well known for signing J.K Rowling, and publishing \"Harry Potter and the Philosopher\u2019s Stone\" in 1997. Hypothesis: Barry Cunningham is most proud of signing J.K Rowling, and publishing \"Harry Potter and the Philosopher\u2019s Stone\" in 1997. NLE: The premise states that Neil LaBute wrote and directed In the Company of Men, which was his feature film debut and won him the Independent Spirit Award for Best First Screenplay. It also mentions that the film starred Aaron Eckhart, but it does not provide any information about whether or not LaBute worked with Eckhart specifically for this film. Therefore, the hypothesis cannot be either true or false based on the provided premise. Predicted Label: entailment Correct Label: neutral\nPremise: Gyula Trebitsch (3 November 1914 - 12 December 2005) was a German film producer born in Budapest, Hungary. He was nominated in 1956 for the Academy Award for Best Foreign Language Film along with Walter Koppel for",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving the robustness of large language models (LLMs) in in-context learning (ICL) against adversarial examples by integrating natural language explanations (NLEs). Previous methods have struggled with adversarial inputs, necessitating a new approach to enhance model reliability.",
        "problem": {
            "definition": "The problem defined in this paper is the vulnerability of ICL in LLMs to adversarial examples, which leads to inaccurate predictions in critical reasoning tasks.",
            "key obstacle": "The main difficulty lies in the existing methods' inability to effectively handle adversarial inputs, which compromises the accuracy of the predictions made by these models."
        },
        "idea": {
            "intuition": "The intuition behind this work is that incorporating NLEs into ICL can bolster the model's robustness against adversarial examples, drawing inspiration from previous studies that showed the benefits of explanations in enhancing model performance.",
            "opinion": "The proposed idea is to utilize a method called X-ICL, which integrates NLEs generated by LLMs to improve the performance of ICL in adversarial settings.",
            "innovation": "The innovation of this approach lies in its ability to generate NLEs through LLMs, eliminating the need for human-written explanations and thereby enhancing scalability and adaptability across various tasks."
        },
        "method": {
            "method name": "X-ICL",
            "method abbreviation": "X-ICL",
            "method definition": "X-ICL is an approach that combines in-context learning with natural language explanations generated by LLMs to improve the accuracy and robustness of predictions.",
            "method description": "The core of X-ICL involves prompting LLMs with NLEs to guide their reasoning processes during prediction tasks.",
            "method steps": [
                "Prompt the LLM with a set of training examples that include instances and their corresponding NLEs.",
                "Generate predictions and explanations for new instances based on the provided examples.",
                "Evaluate the accuracy of the generated predictions against adversarial datasets."
            ],
            "principle": "The effectiveness of X-ICL is based on the premise that NLEs help clarify the reasoning process for the model, enabling it to better navigate complex and adversarial inputs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using five prominent LLMs (GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) across eight adversarial datasets, including HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS.",
            "evaluation method": "The performance of X-ICL was assessed by comparing its accuracy against standard ICL and X-ICL with human-generated NLEs, using metrics such as accuracy percentages and statistical significance tests."
        },
        "conclusion": "The results indicate that X-ICL significantly enhances the robustness and accuracy of LLMs in adversarial settings, outperforming both standard ICL and previous methods that relied on human-written NLEs.",
        "discussion": {
            "advantage": "The key advantages of X-ICL include its ability to improve model accuracy on adversarial datasets and its scalability through the use of LLM-generated NLEs.",
            "limitation": "A limitation noted is the potential lack of fidelity in the NLEs generated by LLMs, which may sometimes include inaccuracies or hallucinated information.",
            "future work": "Future research should focus on enhancing the faithfulness of NLEs produced by LLMs and exploring the generalizability of X-ICL across a wider range of NLP tasks."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge funding support from various institutions, including Cisco and the European Union's Horizon 2020 research and innovation programme.",
            "dataset_info": {
                "datasets used": [
                    "HANS",
                    "ISCS",
                    "NaN",
                    "ST",
                    "PICD",
                    "PISP",
                    "ANLI",
                    "PAWS",
                    "SNLI",
                    "QQP"
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The vulnerability of in-context learning (ICL) in large language models (LLMs) to adversarial examples leads to inaccurate predictions in critical reasoning tasks."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, X-ICL, integrates natural language explanations (NLEs) to improve the robustness of LLMs in ICL against adversarial examples."
        },
        {
            "section number": "3.4",
            "key information": "X-ICL involves prompting LLMs with NLEs to guide their reasoning processes during prediction tasks, enhancing their ability to navigate complex and adversarial inputs."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts in X-ICL includes training examples with corresponding NLEs, which significantly influences the outcomes of in-context learning."
        },
        {
            "section number": "6.1",
            "key information": "A limitation noted in the paper is the potential lack of fidelity in the NLEs generated by LLMs, which may sometimes include inaccuracies or hallucinated information."
        },
        {
            "section number": "6.4",
            "key information": "Future research should focus on enhancing the faithfulness of NLEs produced by LLMs and exploring the generalizability of X-ICL across a wider range of NLP tasks."
        }
    ],
    "similarity_score": 0.7338800099799945,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Using Natural Language Explanations to Improve Robustness of In-context Learning.json"
}