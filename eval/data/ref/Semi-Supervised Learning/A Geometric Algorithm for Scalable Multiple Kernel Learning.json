{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1206.5580",
    "title": "A Geometric Algorithm for Scalable Multiple Kernel Learning",
    "abstract": "We present a geometric formulation of the Multiple Kernel Learning (MKL) problem. To do so, we reinterpret the problem of learning kernel weights as searching for a kernel that maximizes the minimum (kernel) distance between two convex polytopes. This interpretation combined with novel structural insights from our geometric formulation allows us to reduce the MKL problem to a simple optimization routine that yields provable convergence as well as quality guarantees. As a result our method scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with a uniform unweighted combination of kernels.",
    "bib_name": "moeller2014geometricalgorithmscalablemultiple",
    "md_text": "# A Geometric Algorithm for Scalable Multiple Kernel Learning\nJohn Moeller moeller@cs.utah.edu Parasaran Raman praman@yahoo-inc.com Avishek Saha avishek2@yahoo-inc.com Suresh Venkatasubramanian suresh@cs.utah.edu\n.LG]  15 Mar 2014\n# November 9, 2018\nAbstract\nWe present a geometric formulation of the Multiple Kernel Learning (MKL) problem. To do so, we reinterpret the problem of learning kernel weights as searching for a kernel that maximizes the minimum (kernel) distance between two convex polytopes. This interpretation combined with novel structural insights from our geometric formulation allows us to reduce the MKL problem to a simple optimization routine that yields provable convergence as well as quality guarantees. As a result our method scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with a uniform unweighted combination of kernels.\n# 1 Introduction\narXiv:1206.5580v2\nMultiple kernel learning is a principled alternative to choosing kernels (or kernel weights) and has been successfully applied to a wide variety of learning tasks and domains [18, 4, 2, 36, 10, 35, 22, 26]. Pioneering work by Lanckriet et al. [18] jointly optimizes the Support Vector Machine (SVM) task and the choice of kernels by exploiting convex optimization at the heart of both problems. Although theoretically elegant, this approach requires repeated invocations of semidefinite solvers. Other existing methods [26, 18, 25, 32, 33], albeit accurate, are slow and have large memory footprints. In this paper, we present an alternate geometric perspective on the MKL problem. The starting point for our approach is to view the MKL problem as an optimization of kernel distances over convex polytopes. The ensuing formulation is a Quadratically Constrainted Quadratic Program (QCQP) which we solve using a novel variant of the Matrix Multiplicative Weight Update (MMWU) method of Arora and Kale [3]; a primal-dual combinatorial algorithm for solving Semidefinite Programs (SDP) and QCQPs. While the MMWU approach in its generic form does not yield an efficient solution for our problem, we show that a careful geometric reexamination of the primal-dual algorithm reveals a simple alternating optimization with extremely light-weight update steps. This algorithm can be described as simply as: \u201cfind a few violating support vectors with respect to the current kernel estimate, and reweight the kernels based on these support vectors\u201d. Our approach (a) does not require commercial cone or SDP solvers, (b) does not make explicit calls to SVM libraries (unlike alternating optimization based methods), (c) provably converges in a fixed number of iterations, and (d) has an extremely light memory footprint. Moreover, our focus is on optimizing MKL on a single machine. Existing techniques [26] that use careful engineering to parallelize MKL optimizations in\norder to scale can be viewed as complementary to our work. Indeed, our future work is focused on adding parallel components to our already fast optimization method. A detailed evaluation on eleven datasets shows that our proposed algorithm (a) is fast, even as the data size increases beyond a few thousand points, (b) compares favorably with LibLinear [11] after Nystr\u00a8om kernel approximations are applied as feature transformations, and (c) compares favorably with the uniform heuristic that merely averages all kernels without searching for an optimal combination. As has been noted [7], the uniform heuristic is a strong baseline for the evaluation of MKL methods. We use LibLinear with Nystr\u00a8om approximations (LIBLINEAR+) as an additional scalable baseline, and we are able to beat both these baselines when both m and n are significantly large.\n# 2 Related Work\nIn practice, since the space of all kernels can be unwieldy, many methods operate by fixing a base set of kernels and determining an optimal (conic) combination. An early approach (UNIFORM) eliminated the search and simply used an equal-weight sum of kernel functions [22]. In their seminal work, Lanckriet et al. [18] proposed to simultaneously train an SVM as well as learn a convex combination of kernel functions. The key contribution was to frame the learning problem as an optimization over positive semidefinite kernel matrices which in turn reduces to a QCQP. . Soon after, Bach et al. [4] proposed a block-norm regularization method based on second order cone programming (SOCP). For efficiency, researchers started using alternating optimization methods that alternate between updating the classifier parameters and the kernel weights. Sonnenburg et al. [26] modeled the MKL objective as a cutting plane problem and solved for kernel weights using Semi-Infinite Linear Programming (SILP) techniques. Rakotomamonjy et al. [25] used sub-gradient descent based methods to solve the MKL problem. An improved level set based method that combines cutting plane models with projection to level sets was proposed by Xu et al. [32]. Xu et al. [33] also derived a variant of the equivalence between group LASSO and the MKL formulation that leads to closed-form updates for kernel weights. However, as pointed out in [7], most of these methods do not compare favorably (both in accuracy as well as speed) even with the simple uniform heuristic. Other works in MKL literature study the use of different kernel families, such as Gaussian families [19], hyperkernels [20] and non-linear families [29, 8]. Regularization based on the \u21132-norm [16] and \u2113p-norm [15, 30] have also been introduced. In addition, stochastic gradient descent based online algorithms for MKL have been studied in [21]. Another work by Jain et al. [13] discusses a scalable MKL algorithm for dynamic kernels. We briefly discuss and compare with this work when presenting empirical results (Section 5). In two-stage kernel learning, instead of combining the optimization of kernel weights as well as that of the best hypothesis in a single cost function, the goal is to learn the kernel weights in the first stage and then use it to learn the best classifier in the second stage. Recent two-stage approaches seem to do well in terms of accuracy \u2013 such as Cortes et al. [9], who optimize the kernel weights in the first stage and learn a standard SVM in the second stage, and Kumar et al. [17], who train on meta-examples derived from kernel combinations on the ground examples. In Cortes et al. [9], the authors observe that their algorithm reduces to solving a meta-SVM which can be solved using standard off-the-shelf SVM tools such as LibSVM. However, despite being highly efficient on few examples, LibSVM is very inefficient on more than a few thousand examples due to quadratic scaling [6]. As for Kumar et al. [17], the construction of meta-examples scales quadratically in the number of samples and so their algorithm may not scale well past the small datasets evaluated in their work. Interestingly, our proposed MWUMKL can easily be run as a single-kernel algorithm. We can then\napply our scalability to the two-stage algorithm of [9], allowing it not to be limited by the same constraints as LibSVM (which scales quadratically or worse in the number of examples [6]).\n# 3 Background\nNotation. We will denote vectors by boldface lower case letters like z, and matrices by bold up letters M.\n0 zero vector or matrix 1 all-ones vector or matrix M \u2ab00 M is positive semidefinite A\u2022B Tr(AB) = \u2211i, j Ai jBi j diag(a) The diagonal matrix A such that Aii = ai\nModeling the geometry of SVM. Suppose that X \u2208Rn\u00d7d is a collection of n training samples in a ddimensional vector space (the rows x1,x2,...,xn are the points). Also, y = (y1,y2,...,yn) \u2208{\u22121,+1}n are the binary class labels for the data points in X. Let X+ \u2282X denote the rows corresponding to the positive entries of y, and likewise X\u2212\u2282X for the negative entries.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e965/e9659bf6-d11a-4822-ba5f-0362eb75b7fe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Illustration of primal-dual relationship for classification.</div>\nFrom standard duality, the maximum margin SVM problem is equivalent to finding the shortest distance between the convex hulls of X+ and X\u2212. This shortest distance between the hulls will exist between two points on the respective hulls (see Figure 1). Since these points are in the hulls, they can be expressed as some convex combination of the rows of X+ and X\u2212, respectively. That is, if p+ is the closest point on the positive hull, then p+ can be expressed as \u03b1\u22a4 +X+, where \u03b1\u22a4 +1 = 1 and \u03b1 j \u22650, with a similar construction for p\u2212and \u03b1\u2212. This in turn can be written as an optimization\n(primal) margin\n(3.1)\nCollecting all the \u03b1 terms together by defining \u03b1j \u225c\u03b1y j,j, and expanding the distance term \u2225p+ \u2212p\u2212\u22252,  is straightforward to show that Problem (3.1) is equivalent to\nwhere \u03b1\u22a4YXX\u22a4Y\u03b1 is merely a compact way of writing \u2211j,k\u2208X \u03b1j\u03b1kyjyk \ufffd xj,xk \ufffd . Problem (3.2) is of course the familiar dual SVM problem. The equivalence of (3.1) and (3.2) is well known, so we decline to prove it here; see Bennett and Bredensteiner [5] for a proof of this equivalence.\nKernelizing the dual. The geometric interpretation of the dual does not change when the examples are transformed by a reproducing kernel Hilbert space (RKHS). The Euclidean norm of the base vector space in \u2225p+ \u2212p\u2212\u22252 is merely substituted with the RKHS norm:\n\u2225p+ \u2212p\u2212\u22252 \u03ba = \u03ba(p+,p+)+\u03ba(p\u2212,p\u2212)\u22122\u03ba(p+,p\u2212),\nwhere the kernel function \u03ba stands in for the inner product. This is dubbed the kernel distance [24] or the maximum mean discrepancy [12]. The dual formulation then changes slightly, with the covariance term XX\u22a4being replaced by the kernel matrix K. For brevity, we will define G \u225cYKY.\nMultiple kernel learning. Multiple kernel learning is simply the SVM problem with the additional complication that the kernel function is unknown, but is expressed as some function of other known kernel functions. Following standard practice [18] we assume that the kernel function is a convex combination of other kernel functions; i.e., that there is some set of coefficients \u00b5i > 0, that \u2211\u00b5i = 1, and that \u03ba = \u2211\u00b5i\u03bai (which implies that the Gram matrix version is K = \u2211\u00b5iKi). We regularize by setting tr(K) = 1 [18]. The dual problem then takes the following form [18]:\nWhen juxtaposed with (3.1) and (3.2), this can be interpreted as searching for the kernel that maximizes the shortest (kernel) distance between polytopes.\n# 4 Our Algorithm\nThe MKL formulation of (3.3) can be transformed (as we shall see later) into a quadratically-constrained quadratic problem that can be solved by a number of different solvers [18, 1, 27]. However, this approach requires a memory footprint of \u0398(mn2) to store all kernel matrices. Another approach would be to exploit the min-max structure of (3.3) via an alternating optimization: note that the problem of finding the shortest distance between polytopes for a fixed kernel is merely the standard SVM problem. There are two problems with this approach: (a) standard SVM algorithms do not scale well with m,n, and (b) it is not obvious how to adjust kernel weights in each iteration.\n(3.2)\n(3.3)\nOverview. Our solution exploits the fact that a QCQP is a special case of a general SDP. We do this in order to apply the combinatorial primal-dual matrix multiplicative weight update (MMWU) algorithm of Arora and Kale [3]. While the generic MMWU has expensive steps (a linear program and matrix exponentiation), we show how to exploit the structure of the MKL QCQP to yield a very simple alternating approach. In the \u201cforward\u201d step, rather than solving an SVM, we merely find two support vector that are \u201cmost violating\u201d normal to the current candidate hyperplane (in the lifted feature space). In the \u201cbackward\u201d step, we reweight the kernels involved using a matrix exponentiation that we reduce to a closed form computation without requiring expensive matrix decompositions. Our speedup comes from the facts that (a) the updates to support vectors are sparse (at most two in each step) and (b) that the backward step can be computed very efficiently. This allows us to reduce our memory footprint to O(mn).\nQCQPs and SDPs. We start by using an observation due to Lanckriet et al. [18] to convert (3.3)1 into th following QCQP:\nwhere Gi = YKiY, r \u2208Rm, and ri = tr(Ki). Next, we rewrite (4.1) in canonical SDP form in order to apply the MMWU framework:\nwhere Gi = YKiY, r \u2208Rm, and ri = tr(Ki). Next, we rewrite (4.1) in canonical SDP form in order to apply the MMWU framework: \u03c9\u2217= max \u03b1s 2\u03b1\u22a41\u2212s\nwhere A\u22a4 i Ai = 1 ri Gi for all i \u2208[0..m].\nThe MMWU framework. We give a brief overview of the MMWU framework of Arora and Kale [3] (for more details, the reader is directed to Satyen Kale\u2019s thesis [14]). The approach starts with a \u201cguess\u201d \u03c9 for the optimal value \u03c9\u2217of the SDP (and uses a binary search to find this guess interleaved with runs of the algorithm). Assuming that this guess at the optimal value is correct, the algorithm then attempts to find either a feasible primal (P) or dual assignment such that this guess is achieved.\nAlgorithm 1 MMWU template [3]\nInput: \u03b5, primal P(1), rounds T, guess \u03c9\nfor t = 1...T do\nforward: Compute update to \u03b1(t) based on constraints, P(t) and \u03b1(t)\nbackward: Compute M(t) from constraints and \u03b1(t).\nW(t+1) \u2190e\u2212\u03b5 \u2211t\nt=1 M(t)\nP(t+1) \u2190\nW(t+1)\nTr(W(t+1))\nend for\nOutput: P(T)\n1We note that (4.1) is the hard-margin version of the MKL problem. The standard soft-margin variants can also be placed in this general framework [18]. For the 1-norm soft margin, we add the constraint that all terms of \u03b1 are upper bounded by the margin constant C. For the 2-norm soft margin, another term 1 C \u03b1\u22a4\u03b1 appears in the objective, or we can simply add a constant multiple of I to each Gi.\n1We note that (4.1) is the hard-margin version of the MKL problem. The standard soft-margin variants can also be placed in this general framework [18]. For the 1-norm soft margin, we add the constraint that all terms of \u03b1 are upper bounded by the margin constant C. For the 2-norm soft margin, another term 1 C \u03b1\u22a4\u03b1 appears in the objective, or we can simply add a constant multiple of I to each Gi.\n(4.1)\n(4.2)\nThe process starts with some assignment to P(1) (typically the identity matrix I). If this assignment is both primal feasible and at most \u03c9, the process ends. Else, there must be some assignment to \u03b1 (the dual) that \u201cwitnesses\u201d this lack of feasibility or optimality, and it can be found by solving a linear program using the current primal/dual assignments and constraints (i.e., is positive, has dual value at least \u03c9, and satisfies constraints (4.1)). The primal constraints and \u03b1 are then used to guide the search for a new primal assignment P(t+1). They are combined to form the matrix Qi(\u03b1(t)) (see (4.1)), and then adjusted to form an \u201cevent matrix\u201d M(t) (see Paragraph \u201cthe backward step\u201d for details)2. Exponentiating the sum of all the observed M(t) so far, the algorithm exponentially re-weights primal constraints that are more important, and the process repeats. By minimizing the loss, the assignments to P(t) and \u03b1(t) are guaranteed to result in an SDP value that approximates \u03c9\u2217within a factor of (1+\u03b5).\n# 4.1 Our algorithm\nWe now adapt the above framework to solve the MKL SDP given by (4.2). As we will explain below, we can assign \u03c9\u2217a priori in most cases and we can solve our problem with only one round of feasibility search. We denote the dual update in iteration t by \u03b1(t), the ith event matrix in iteration t by M(t) i and the ith primal variable (matrix) in iteration t by P(t) i . P(t) i is closely related to the desired primal kernel coefficients \u00b5i. We denote \u03b1 = \u2211i \u03b1(i) as the accumulated dual assignment thus far and Mi = \u2211t M(t) i as the accumulated ith event matrix.\n# 4.1.1 The backward step\nIt will be convenient to explain the backward step first. Given \u03b1(t) and Qi(\u03b1(t)), we define M(t) i \u225c1 2\u03c1 (Qi(\u03b1(t))+ \u03c1In+1) where \u03c1 is a rate parameter to be set later. Note that M(t) i (and M(t)) is \u201calmost-diagonal\u201d, taking the form \ufffdaIn u u\u22a4 a \ufffd . Such matrices can be exponentiated in closed form.\nLemma 4.1. The exponential of a matrix in the form \ufffdaIn u u\u22a4 a \ufffd , where a \u22650 and \u02c6u = u/\u2225u\u2225, is\nLemma 4.1. The exponential of a matrix in the form \ufffdaIn u u\u22a4 a \ufffd , where a \u22650 and \u02c6u = u/\u2225u\u2225, is\nProof. We symbolically exponentiate an n+1\u00d7n+1 matrix of the form\nSince this matrix is real and symmetric, its eigenvalues \u03bbi are positive and its unit eigenvectors vi form an orthonormal basis. The method that we use to symbolically exponentiate it is to express it in the form\n2M(t) generalizes the loss incurred by experts in traditional MWU \u2013 by deriving M(t) from the S of the SDP takes the role of the loss.\n2M(t) generalizes the loss incurred by experts in traditional MWU \u2013 by deriving M(t) from the SDP constraints, the duality gap of the SDP takes the role of the loss.\nAs a matter of notation, let \u02c6u be the unit vector such that \u2225u\u2225\u02c6u = u.\n(\u03bb \u2212a)n\u22121(\u03bb 2 \u22122a\u03bb +a2 \u2212\u2225u\u22252) = (\u03bb \u2212a)n\u22121(\u03bb \u2212a+\u2225u\u2225)(\u03bb \u2212a\u2212\u2225u\u2225).\nThis yields n\u22121 eigenvalues equal to a, and the other two equal to a+\u2225u\u2225and a\u2212\u2225u\u2225. We label them \u03bb1 and \u03bb2, respectively, and the rest are equal to a.\nEigenvectors. First we show that M has two eigenvectors of the form (u,\u00b1\u2225u\u2225)\u22a4:\no these are eigenvectors with eigenvalues a\u00b1\u2225u\u2225. We will call the corresponding eigenvectors v1 and v2. Since M is symmetric, all of its eigenvectors are orthogonal. The remaining eigenvectors are of the form\nso these are eigenvectors with eigenvalues a\u00b1\u2225u\u2225. We will call the corresponding eigenvectors v1 and v2. Since M is symmetric, all of its eigenvectors are orthogonal. The remaining eigenvectors are of the form (w,0)\u22a4, where w\u22a4u = 0: \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\nClearly the corresponding eigenvalue for any such eigenvector is a, so there are n \u22121 of them. The corresponding parts of these eigenvectors are labeled wi, where 3 \u2264i \u2264n + 1, and we assume they are unit vectors.\nThe Exponential. For unit eigenvectors \u02c6vi, since\nand the eigenvalue a is of multiplicity n\u22121, we have\neM = e\u03bb1 2\u2225u\u22252 \ufffduu\u22a4 \u2225u\u2225u \u2225u\u2225u\u22a4 \u2225u\u22252 \ufffd + e\u03bb2 2\u2225u\u22252 \ufffd uu\u22a4 \u2212\u2225u\u2225u \u2212\u2225u\u2225u\u22a4 \u2225u\u22252 \ufffd +ea n \u2211 i=3 \ufffdwiw\u22a4 i 0 0\u22a4 0 \ufffd \ufffd \ufffd\nThe last term in the equality is due to the fact that \u02c6u and the \u02c6wi form an orthonormal basis for Rn,  \u02c6u\u02c6u\u22a4+\u2211\u02c6wi \u02c6w\u22a4 i = In.\nThe last term in the equality is due to the fact that \u02c6u and the \u02c6wi form an orthonormal basis for Rn, so \u02c6u\u02c6u\u22a4+\u2211\u02c6wi \u02c6w\u22a4 i = In. Lemma 4.1 implies that we can exponentiate the event matrix M(t) (see Algorithm 1) quickly, as promised. In particular, we set P(t+1) i = cexp(\u2212\u03b5 \u2211t M(t+1) i ) where c normalizes the matrix to have uni trace.\n(4.3)\nPractical considerations. In Lemma 4.1, large inputs to the functions exp, cosh, and sinh will cause them to rapidly overflow even at double-precision range. Fortunately there are two steps we can take. First, cosh(x) and sinh(x) converge exponentially to exp(x)/2, so above a high enough value, we can simply approximate sinh(x) and cosh(x) with exp(x)/2. Because exp can overflow just as much as sinh or cosh, this doesn\u2019t solve the problem completely. However, since P is always normalized so that tr(P) = 1, we can multiply the elements of P by any factor we choose and the factor will be normalized out in the end. So above a certain value, we can use exp alone and throw a \u201cquashing\u201d factor (e\u2212\u03c6\u2212q) into the equations before computing the result, and it will be normalized out later in the computation (this also means that we can ignore the ea factor). For our purposes, setting q = 20 suffices. This trades overflow for underflow, but underflow can be interpreted merely as one kernel disappearing from significance. Note that the structure of P(t) also allows us to avoid storing it explicitly, since (aI)\u2022(b\u02c6u\u02c6u\u22a4) = ab. We need only store the coefficients of the blocks of the P(t) i .\nThe exponentiation algorithm. From Mt i in Algorithm 1 and (4.2), we have Mt i = 1 2\u03c1 (Qi(\u03b1(t) \u03c1In+1), where \u03c1 is a program parameter which is explained in 4.4. Our Qi(\u03b1) = \ufffd In Ai\u03b1 (Ai\u03b1)\u22a4 1 \ufffd is of the form \ufffdaIn ui u\u22a4 i a \ufffd , where a = 1 \u2200i and ui = Ai\u03b1. So we hav\nu\u22a4 i ui = (Ai\u03b1)\u22a4Ai\u03b1 = \u03b1\u22a4A\u22a4 i Ai\u03b1 = \u03b1\u22a41 ri Gi\u03b1\nwhere the last equality follows from A\u22a4 i Ai = 1 ri Gi (cf. (4.2)). As we shall show in Algorithm 4, at each iteration the matrix to be exponentiated is a sum of matrices of the form 1 2\u03c1 (Qi(\u2211\u03c4 t=1 \u03b1(t)) + \u03c1tIn+1), so Lemma 4.1 can be applied at every iteration. We provide in detail the algorithm we use to exponentiate the matrix M. This subroutine is called from Algorithm 4 in Section 4.\n# 4.1.2 The forward step\nIn the forward step, we wish to check if our primal solution P is feasible and optimal, and if not find update to \u03b1(t). In order to do so, we apply the MMWU template. The goal now is to find \u03b1(t) such that\n\u2211 i Qi(\u03b1(t))\u2022Pi \u22650, \u03b1(t) \u22650, (\u03b1(t))\u22a4y = 0, and (\u03b1(t))\u22a41 = 1.\nThe existence of such a \u03b1(t) will prove that the current guess P(t) is either primal infeasible or suboptimal (see Arora and Kale [3] for details). We now exploit the structure of P(t) given by Lemma 4.1. In particular, let p11 i = p22 i = ea cosh\u2225ui\u2225/trP and p12 i = \u2212ea sinh\u2225ui\u2225/trP. So\nQi(\u03b1(t))\u2022Pi = \ufffd 0 Ai\u03b1(t) (Ai\u03b1(t))\u22a4 0 \ufffd \u2022Pi +In+1 \u2022Pi = 2p12 i \u02c6u\u22a4 i Ai\u03b1(t) +tr(Pi)\n\u2211i Qi(\u03b1(t))\u2022Pi \u22650 then reduces to:\n(\u03b1(t))\u22a4 m \u2211 i=0 (2p12 i Ai \u02c6ui) \u2265\u2212tr(P).\n(4.4)\n(4.5)\nAlgorithm 2 EXPONENTIATE-M\nInput: y, \u03b1, {Gi}, \u03b5\u2032, \u03c1\nfor i \u2208[1..m] do\n\u2225ui\u2225\u2190\n\ufffd\n\u03b1\u22a4Gi\u03b1\ngi \u2190\n1\n\u2225ui\u2225Gi\u03b1\n\u2225ui\u2225\u2190\u03b5\u2032\n2\u03c1 \u2225ui\u2225\nend for\nq \u2190maxi \u2225ui\u2225\nif q < 20 then\nfor i \u2208[1..m] do\np11\ni\n\u2190cosh(\u2225ui\u2225)\np12\ni\n\u2190\u2212sinh(\u2225ui\u2225)\nend for\neM \u21901\nelse\nfor i \u2208[1..m] do\np11\ni\n\u2190e\u2225ui\u2225\u2212q\np12\ni\n\u2190\u2212p11\ni\nend for\neM \u2190e\u2212q\nend if\nS \u2190m(n\u22121)eM +2\u2211i p11\ni\nfor i \u2208[1..m] do\np12\ni\n\u2190p12\ni /S\nend for\ng \u2190\u2211i 2p12\ni gi\nReturn p12, g\n# The right hand side is the negative trace of P (which is normalized to 1), so this becomes\n(\u03b1(t))\u22a4\u2211 i 2p12 i gi \u2265\u22121,\nwhere gi = ( 1 ri Gi\u03b1)/( 1 ri \u03b1\u22a4Gi\u03b1)1/2. If we let g = \u2211i 2p12 i gi (which can be calculated at the end of the backward step), then we have simply g\u22a4\u03b1 \u2265\u22121 which is a simple collection of linear constraints that can\nwhere gi = ( 1 ri Gi\u03b1)/( 1 ri \u03b1\u22a4Gi\u03b1)1/2. If we let g = \u2211i 2p12 i gi (which can be calculated at the end of the backward step), then we have simply g\u22a4\u03b1 \u2265\u22121 which is a simple collection of linear constraints that can always be satisfied3. Geometrically, g gives us a way to examine the training points that are farthest away from the margin. The higher a value gj is, the more it violates the current decision boundary. In order to find a \u03b1 that satisfies (4.6), we simply choose the highest elements of g that correspond to both positive and negative labels, then set each corresponding entry in \u03b1 to 1/2. Algorithm 3 describes the pseudo-code for this process.\nalways be satisfied3. Geometrically, g gives us a way to examine the training points that are farthest away from the margin. The higher a value gj is, the more it violates the current decision boundary. In order to find a \u03b1 that satisfies (4.6), we simply choose the highest elements of g that correspond to both positive and negative labels, then set each corresponding entry in \u03b1 to 1/2. Algorithm 3 describes the pseudo-code for this process.\nAlgorithm 3 FIND-\u03b1\nInput: y, g\nP \u2190{i | yi = 1}, N \u2190{i | yi = \u22121}\niP \u2190argmaxi\u2208P gi, iN \u2190argmaxi\u2208N gi\n\u03b1 \u21900\n\u03b1iP \u21901/2, \u03b1iN \u21901/2\nreturn \u03b1\nOutput: \u03b1 s.t. \u03b1 \u22650, \u03b1\u22a41 = 1, \u03b1\u22a4y = 0\n3\n<div style=\"text-align: center;\">Algorithm 3 FIND-\u03b1</div>\n3The current margin borders a convex combination of points from each side. If we could not find a point such that the inequality is satisfied, then no point from the convex combination can be found on or past the margin, which is impossible.\n(4.6)\nPractical Considerations. We highlight two important practical consequences of our formulation. First, the procedure produces a very sparse update to \u03b1: in each iteration, only two coordinates of \u03b1 are updated. This makes each iteration very efficient, taking only linear time. Second, by expressing ui in terms of gi we never need to explicitly compute Ai (as ui = Ai\u03b1), which in turn means that we do not need to compute the (expensive) square root of Gi explicitly. Another beneficial feature of the dual-finding procedure for MKL is that terms involving the primal variables P are either normalized (when we set the trace of P to 1) or eliminated (due to the fact that we have a compact closed-form expression for P), which means that we never have to explicitly maintain P, save for a small number (4m) of variables.\n# 4.2 Avoiding binary search for \u03c9\nThe objective function in (4.2) is linear, so we can scale s and \u03b1 and use the fact that s = \u03b1\u22a41 = \u03c9 transform the problem4:\n# The objective function in (4.2) is linear, so we can scale s and \u03b1 and use the fact that s = \u03b1\u22a41 = \u03c9 to transform the problem4:\n1/\u03c9 \u22651 ri \u03b1\u22a4Gi\u03b1, \u03b1\u22a4y = 0, \u03b1\u22a41 = 1, \u03b1 \u22650,\nwhere \u03b1 = \u03c9\u03b1. The first constraint can be transformed back into an optimization; that is, min\u03c9 max\u03b1,i 1 ri \u03b1\u22a4Gi\u03b1 subject to the remaining linear constraints. Because \u03c9 does not figure into the maximization, we can compute \u03c9 simply by maximizing 1 ri \u03b1\u22a4Gi\u03b1. Practically, this means that we simply add the constraint \u03b1\u22a41 = 1, and the \u201cguess\u201d for \u03c9 is set to 1. We then know the objective, and only one iteration is needed, so the binary search is eliminated.\n# 4.3 Extracting the solution from the MMWU\n# 4.3 Extracting the solution from the MMWU\nNow recall (from section 3) that \u03b1\u22a4G\u03b1 = \u2211m i=1 \u00b5i \u00b7 \u03b1\u22a4Gi\u03b1, and we also use the fact that \u03b1\u22a4G\u03b1 = \u03b1\u22a41 = \u03c9 = 1. Combining the above two we have:\nMatching (4.7) with (4.8) suggests that 2p12 i ri \ufffd ri \u03b1\u22a4Gi\u03b1 \ufffd1/2 is the appropriate choice for \u00b5i.\nMatching (4.7) with (4.8) suggests that 2p12 i ri \ufffd ri \u03b1\u22a4Gi\u03b1 \ufffd1/2 is the appropriate choice for \u00b5i.\n\ufffd \ufffd 4This fact follows from the KKT conditions for the original problem. The support constraints of the SVM problem can be written as G\u03b1 + by \u22651. If we multiply both sides of this inequality by \u03b1\u22a4then it becomes an equality (by complementary slackness): \u03b1\u22a4G\u03b1 = \u03b1\u22a41. s is a substitution for \u03b1\u22a4G\u03b1 in the MKL problem [18] so s = \u03b1\u22a41 = \u03c9 as well.\n\ufffd \ufffd 4This fact follows from the KKT conditions for the original problem. The support constraints of the SVM problem can be written as G\u03b1 + by \u22651. If we multiply both sides of this inequality by \u03b1\u22a4then it becomes an equality (by complementary slackness): \u03b1\u22a4G\u03b1 = \u03b1\u22a41. s is a substitution for \u03b1\u22a4G\u03b1 in the MKL problem [18] so s = \u03b1\u22a41 = \u03c9 as well.\n(4.7)\n(4.8)\n# 4.4 Putting it all together\nAlgorithm 4 summarizes the discussion in this section. The parameter \u03b5 is the error in approximating the objective function, but its connection to classification accuracy is loose. We set the actual value of \u03b5 via cross-validation (see Section 5). The parameter \u03c1 is the width of the SDP, a parameter that indicates how much the solution can vary at each step. \u03c1 is equal to the maximum absolute value of the eigenvalues of Qi(\u03b1(t)), for any i [3].\nLemma 4.2. \u03c1 is bounded by 3/2.\n# Lemma 4.2. \u03c1 is bounded by 3/2.\nProof. \u03c1 is defined as the maximum of \u2225Q(\u03b1(t))\u2225for all t. Here \u2225\u00b7 \u2225denotes the largest eigenvalue in absolute value [3]. Because s = \u03c9 = 1 (see Section 4), the eigenvalues of Qi(\u03b1(t)) are 1 (with multiplicity n\u22121), and 1\u00b1\u2225Ai\u03b1(t)\u2225. The greater of these in absolute value is clearly 1+\u2225Ai\u03b1(t)\u2225. \u2225Ai\u03b1(t)\u2225is equal to \ufffd \ufffd\n\u03b1(t) always has two nonzero elements, and they are equal to 1/2. They also correspond to values of y with opposite signs, so if j and k are the coordinates in question, (\u03b1(t))TGi\u03b1(t) \u2264(1/4)(Gi( j j) + Gi(kk)), because Gi(jk) and Gi(k j) are both negative. Because of the factor of 1/ri, and because ri is the trace of Gi, \u2225Ai\u03b1(t)\u2225\u22641/2. This is true for any of the i, so the maximum eigenvalue of Q(\u03b1(t)) in absolute value is bounded by 1+1/2 = 3/2.\nRunning time. Every iteration of Algorithm 4 will require a call to FIND-\u03b1, a call to EXPONENTIATE-M and an update to Gi\u03b1 and \u03b1\u22a4Gi\u03b1. FIND-\u03b1 requires a linear search for two maxima in g, so the first is O(n). The latter are each O(mn), which dominate FIND-\u03b1. Algorithm 4 requires a total of T iterations at most, where T = 8\u03c12 \u03b52 ln(n). Since we only require one run of the main algorithm, the running time is bounded by O \ufffd mnln(n) 1 \u03b52 \ufffd .\n\ufffd\n\ufffd\nAlgorithm 4 MWUMKL\nInput: g(1) = 0;\n\u03c1, the width;\n\u03b5, the desired approximation error\nSet \u03b5\u2032 = \u2212ln(1\u2212\u03b5\n2\u03c1 )\nSet T = 8\u03c12\n\u03b52 ln(n)\nrepeat {T times}\nGet \u03b1(t) from Algorithm 3\nif Algorithm 3 failed then\nReturn\nend if\nUpdate \u03b1 = \u03b1 +\u03b1(t)\nSet M(t)\ni\n= 1\n2\u03c1\n\ufffd\nQi(\u03b1(t))+\u03c1In+1\n\ufffd\nSet W(t)\ni\n= e\u2212\u03b5\u2032 \u2211T\nt=1 M(t)\ni\nSet P(t+1)\ni\n= W(t)\ni /tr(W(t)\ni )\nCompute g(t+1) from P(t+1), {Gi}, and \u03b1\nuntil t = T\nReturn 1\nT \u03b1, P(T+1)\n# 5 Experiments\nIn this section we compare the empirical performance of MWUMKL with other multiple kernel learning algorithms. Our results have two components: (a) qualitative results that compares test accuracies on small scale datasets, and (b) scalability results that compares training time on larger datasets. We compare MWUMKL with the following baselines: (a) UNIFORM (uniformly weighted combination of kernels), and (b) LibLinear [11] with Nystr\u00a8om kernel approximations for each kernel (hereafter referred to as LIBLINEAR+). We evaluate these MKL methods on binary datasets from UCI data repository. They include: (a) small datasets Iono, Breast Cancer, Pima, Sonar, Heart, Vote, WDBC, WPBC, (b) medium dataset Mushroom, and (c) comparatively larger datasets Adult, CodRna, and Web. Classification accuracy and kernel scalability results are presented on small and medium datasets (with many kernels). Scalability results (with 12 kernels due to memory constraints) are provided for large datasets. Finally, we show results for lots of kernels on small data subsets. Uniform kernel weights. UNIFORM is simply LibSVM [6] run with a kernel weighted equally amongst all of the input kernels (where the kernel weights are normalized by the trace of their respective Gram matrices first). The performance of UNIFORM is on par or better than LIBLINEAR+ on many datasets (see Figure 2) and the time is similar to MWUMKL. However UNIFORM does not scale well due to the poor scaling of LibSVM beyond a few thousand samples (see Figure 3), because of the need to hold the entire Gram matrix in memory 5. We employ Scikit-learn [23] because it offers efficient access to LibSVM.\nUniform kernel weights. UNIFORM is simply LibSVM [6] run with a kernel weighted equally amongs all of the input kernels (where the kernel weights are normalized by the trace of their respective Gram matrices first). The performance of UNIFORM is on par or better than LIBLINEAR+ on many datasets (see Figure 2) and the time is similar to MWUMKL. However UNIFORM does not scale well due to the poor scaling of LibSVM beyond a few thousand samples (see Figure 3), because of the need to hold the entire Gram matrix in memory 5. We employ Scikit-learn [23] because it offers efficient access to LibSVM.\n# LibLinear [11] with Nystr\u00a8om kernel approximations [31, 34] (LIBLINEAR+). One important ob\nLibLinear [11] with Nystr\u00a8om kernel approximations [31, 34] (LIBLINEAR+). One important observation about multiple kernel learning is that UNIFORM performs as well or better than many MKL algorithms with better efficiency. Along this same line of thought, we should consider comparison against methods that are as simple as possible. One of the very simplest algorithms to consider is to use a linear classifier (in this case, LibLinear [11]), and transform the features of the data with a kernel approximation. For our purposes, we use Nystr\u00a8om approximations as described by Williams and Seeger [31] and discussed further by Yang et al. [34]. Because LibLinear is a primal method, we don\u2019t need to scale each kernel \u2013 each kernel manifests as a set of features, which the algorithm weights by definition. For the Nystr\u00a8om feature transformations, one only needs to specify the kernel function and the number of sample points desired from the data set. We usually use 150 points, unless memory constraints force us to use fewer. Theoretically, if s is the number of sample points, n the number of data points, and m the number of kernels, then we would need space to store O(snm) double-precision floats. With regard to time, the training task is very rapid \u2013 the transformation is the bottleneck (requiring O(s2mn) time to transform every point with every kernel approximation). We employ Scikit-learn [23] for implementations of both the linear classifier and the kernel approximation because (a) this package offloads linear support-vector classification to the natively-coded LibLinear implementation, (b) it offers a fast kernel transformation using the NumPy package, and (c) Scikit-learn makes it very easy and efficient to chain these two implementations together. In practice this method is very good and very fast for low numbers of kernels (see Figures 2, 4a, and 4b). For high numbers of kernels, this scaling breaks down due to time and memory constraints (see Figure 5).\n5This is true even when LibSVM is told to use one kernel, which it can compute on the fly \u2013 the scaling of LibSVM is O(n2) O(n3) [6], poor compared to MWUMKL and LIBLINEAR+ with increasing sample size.\nLegacy MKL implementations. In all cases, we omit the results for older MKL algorithm implementations such as (a) SILP [26], (b) SDPMKL [18], (c) SIMPLEMKL [25], (d) LEVELMKL [32], and (e) GROUPMKL [33] which take significantly longer to complete, have no significant gain in accuracy, and do not scale to any datasets larger than a few thousand samples. For example, on Sonar (one of the smallest sets in our pool), each iteration of SILP takes about 4500 seconds on average whereas UNIFORM requires 0.03 seconds on average.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f6c0/f6c090a4-e76b-4b87-854e-adb86d2da69d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 1: Datasets used in experiments.</div>\nExperimental parameters. Similar to Rakotomamonjy et al. [25] and Xu et al. [33], we test our algorithms on a base kernel family of 3 polynomial kernels (of degree 1 to 3) and 9 Gaussian kernels. Contrary to [25, 33], however, we test with Gaussian kernels that have a tighter range of bandwidths ({20,21/2,...,24}, instead of {2\u22123,2\u22122,...,25}). The reason for this last choice is that our method actively seeks solutions for each of the kernels, and kernels that encourage overfitting the training set (such as low-bandwidth Gaussian kernels) pull MWUMKL away from a robust solution. For small datasets, kernels are constructed using each single feature and are repeated 30 times with different train/test partitions. For medium and large datasets, due to memory constraints on LIBLINEAR+, we test only on 12 kernels constructed using all features, and repeat only 5 times. All kernels are normalized to trace 1. Results from small datasets are presented with a 95% confidence interval that the median lies in the range. Results from medium-large datasets present the median, with the min and max values as a range around the median. In each iteration, 80% of the examples are randomly selected as the training data and the remaining 20% are used as test data. Feature values of all datasets have been scaled to [0,1]. SVM regularization parameter C is chosen by cross-validation. For example, in Figure 2 results are presented for the best value of C for each dataset and algorithm. For MWUMKL, we choose \u03b5 by cross-validation. Most datasets get \u03b5 = 0.2, but the exceptions are Web (\u03b5 = 0.07), CodRna (\u03b5 = 0.07), and Adult (\u03b5 = 0.05). Contrary to existing works we do not compare the number of SVM calls (as MWUMKL does not explicitly use an underlying SVM) and the number of kernels selected. Experiments were performed on a machine with an Intel R\u20ddCoreTM 2 Quad CPU (2.40 GHz) and 2GB RAM. All methods have an outer test harness written in Python. MWUMKL also uses a test harness in Python with an inner core written in C++.\n<div style=\"text-align: center;\">method G MWUMKL LibLinear+ Uniform</div>\n<div style=\"text-align: center;\">method G MWUMKL LibLinear+ Uniform</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/485b/485b421a-85c3-45c9-b869-1558ee655de4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Median misclassification rate for small datasets.</div>\nAccuracy. On small datasets our goal is to show that MWUMKL compares favorably with LIBLINEAR+ and UNIFORM in terms of test accuracies. In Figure 2 we present the median misclassification rate for each small dataset over 30 random training/test partitions. In each case, we train the classifier with 12 kernels for each feature in the dataset, and each kernel only operates on one feature. We are able either to beat the other methods or remain competitive with them.\nData Scalability. Both MWUMKL and LIBLINEAR+ are much faster as compared with UNIFORM. At this point, Adult, CodRna, and Web are large enough datasets that UNIFORM fails to complete because of memory constraints. This can be seen in Figure 3, where we plot training time versus the proportion of the training data used \u2013 the training time taken by UNIFORM rises sharply and we are unable to train on this dataset past 11907 points. Hence, for the remaining experiments on large datasets, we compare MWUMKL with LIBLINEAR+. In Figures 4a and 4b, we choose a random partition of train and test, and then train with increasing proportions of the training partition (but always test with the whole test partition). With more data, our algorithm settles in to be competitive with LIBLINEAR+. Kernel Scalability. We aim to demonstrate not only that MWUMKL performs well with the number of examples, but also that it performs well against the number of kernels. In fact, for an MKL algorithm to be truly scalable it should do well against both examples and kernels. For kernel scalability, we present the training times for the best parameters of several of the datasets, divided by the number of kernels used, versus the size of the dataset (see Figure 5). We divide time by number of kernels because time scales very close to linearly with the number of kernels for all methods. Also presented are log-log models fit to the data, and the median of each experiment is plotted as a point. We report the time for the same experiments that produced the results in Figure 2, and also train on increasing proportions of Mushroom (1625, 3250, 4875, and 6500 examples) with 1344 per-feature kernels.\nData Scalability. Both MWUMKL and LIBLINEAR+ are much faster as compared with UNIFORM. At this point, Adult, CodRna, and Web are large enough datasets that UNIFORM fails to complete because of memory constraints. This can be seen in Figure 3, where we plot training time versus the proportion of the training data used \u2013 the training time taken by UNIFORM rises sharply and we are unable to train on this dataset past 11907 points. Hence, for the remaining experiments on large datasets, we compare MWUMKL with LIBLINEAR+. In Figures 4a and 4b, we choose a random partition of train and test, and then train with increasing proportions of the training partition (but always test with the whole test partition). With more data, our algorithm settles in to be competitive with LIBLINEAR+. Kernel Scalability. We aim to demonstrate not only that MWUMKL performs well with the number of examples, but also that it performs well against the number of kernels. In fact, for an MKL algorithm to be\nData Scalability. Both MWUMKL and LIBLINEAR+ are much faster as compared with UNIFORM. At this point, Adult, CodRna, and Web are large enough datasets that UNIFORM fails to complete because of memory constraints. This can be seen in Figure 3, where we plot training time versus the proportion of the training data used \u2013 the training time taken by UNIFORM rises sharply and we are unable to train on this dataset past 11907 points. Hence, for the remaining experiments on large datasets, we compare MWUMKL with LIBLINEAR+. In Figures 4a and 4b, we choose a random partition of train and test, and then train with increasing proportions of the training partition (but always test with the whole test partition). With more data, our algorithm settles in to be competitive with LIBLINEAR+.\nKernel Scalability. We aim to demonstrate not only that MWUMKL performs well with the number of examples, but also that it performs well against the number of kernels. In fact, for an MKL algorithm to be truly scalable it should do well against both examples and kernels. For kernel scalability, we present the training times for the best parameters of several of the datasets, divided by the number of kernels used, versus the size of the dataset (see Figure 5). We divide time by number of kernels because time scales very close to linearly with the number of kernels for all methods. Also presented are log-log models fit to the data, and the median of each experiment is plotted as a point. We report the time for the same experiments that produced the results in Figure 2, and also train on increasing proportions of Mushroom (1625, 3250, 4875, and 6500 examples) with 1344 per-feature kernels. With these selections, we are testing mn in the neighborhood of 8.7 million elements.\nmethod G MWUMKL LibLinear+ Uniform\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0214/0214d42e-b9a0-4d75-b45a-a5298b79c3fc.png\" style=\"width: 50%;\"></div>\nAs expected, UNIFORM scales quadratically or more with the number of examples, performing very well at the lower range. The number of examples from Mushroom is not so high that LibSVM runs out of memory, but we do see the algorithm\u2019s typical scaling. LIBLINEAR+ shows slightly superlinear scaling, with a high multiplier due to the matrix computations required for the feature transformations. As we run the algorithm on Mushroom, the number of samples taken for the kernel approximations is reduced so that the features can fit in machine memory. Even so, this reduction doesn\u2019t offer any help to the scaling and at 6500 examples with 1344 kernels, training time is several hours. Even though we reduced the number of samples for LIBLINEAR+, MWUMKL outperforms both UNIFORM and LIBLINEAR+ when both examples and kernels are greater than about 103.\nwell at the lower range. The number of examples from Mushroom is not so high that LibSVM runs out of memory, but we do see the algorithm\u2019s typical scaling. LIBLINEAR+ shows slightly superlinear scaling, with a high multiplier due to the matrix computations required for the feature transformations. As we run the algorithm on Mushroom, the number of samples taken for the kernel approximations is reduced so that the features can fit in machine memory. Even so, this reduction doesn\u2019t offer any help to the scaling and at 6500 examples with 1344 kernels, training time is several hours. Even though we reduced the number of samples for LIBLINEAR+, MWUMKL outperforms both UNIFORM and LIBLINEAR+ when both examples and kernels are greater than about 103. Dynamic Kernels. We also present results for a few datasets with lots of kernels. By computing columns of the kernel matrices on demand, we can run with a memory footprint of O(mn), improving scalability without affecting solution quality (a technique also used in SMOMKL [30]). Table 2 shows that we can indeed scale well beyond tens of thousands of points, as well as many kernels.\nDynamic Kernels. We also present results for a few datasets with lots of kernels. By computing columns of the kernel matrices on demand, we can run with a memory footprint of O(mn), improving scalability without affecting solution quality (a technique also used in SMOMKL [30]). Table 2 shows that we can indeed scale well beyond tens of thousands of points, as well as many kernels.\nDataset\n#Points\n#Kernels\nTime\nAdult\n39073\n3\n13 minutes\nCodRna\n47628\n3\n147 seconds\nSonar 1M\n208\n1000000\n3.65 hours\n<div style=\"text-align: center;\">Table 2: MWUMKL with on-the-fly kernel computations.</div>\nTable 2: MWUMKL with on-the-fly kernel computations.\nWe choose the above datasets to compare against another work on scalable MKL [13]. Jain et al. [13]\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ea9c/ea9c8203-8b61-4867-8b00-fccb04c823fc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">method G MWUMKL LibLinear+</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f36/0f36ab4f-eaf6-4b1b-baf2-e04db7f82571.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Adult (n = 48842, d = 123) and Web (n = 64700, d = 300) with m = 12 kernels</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/993e/993eef67-3d63-4477-9020-314258af6777.png\" style=\"width: 50%;\"></div>\nindicate the ability to deal with millions of kernels, but in effect the technique also has a memory footprint of \u2126(mn) (the footprint of MWUMKL is \u0398(mn), in contrast). This limits any such approach to either many kernels or many points, but not both. Since the work in Jain et al. [13] does not provide accuracy numbers, a direct head-to-head comparison is difficult to make, but we can make a subjective comparison. The above table shows times for MWUMKL with accuracy similar to or better than what LIBLINEAR+ can achieve on the same datasets. The time numbers we achieve are similar in order of magnitude when scaled to the number of kernels demonstrated in Jain et al. [13].\n# 6 Conclusions and Future Work\nWe have presented a simple, fast and easy to implement algorithm for multiple kernel learning. Our proposed algorithm develops a geometric reinterpretation of kernel learning and leverages fast MMWU-based routines to yield an efficient learning algorithm. Detailed empirical results on data scalability, kernel scalability and with dynamic kernels demonstrate that we are significantly faster than existing legacy MKL implementations and outpeform LIBLINEAR+ as well as UNIFORM. Our current results are for a single machine. As mentioned earlier, one of our future goals is to add parallellization techniques to improve the scalability of MWUMKL over data sets that are large and use a large number of kernels. The MWUMKL algorithm lends itself easily to the bulk synchronous parallel (BSP) framework [28], as most of the work is done in the loop that updates G\u03b1 (see the last line of the loop in Algorithm 4). This task can be \u201csharded\u201d for either kernels or data points, and scalability of O(mn) would not suffer under BSP. Since there are many BSP frameworks and tools in use today, this is a natural\n# 7 Acknowledgments\nThis research was partially supported by the NSF under grant CCF-0953066. The authors would also like to thank Satyen Kale and S\u00b4ebastien Bubeck for their valuable feedback.\n[1] E. D. Andersen and K. D. Andersen. The MOSEK interior point optimization for linear programming: an implementation of the homogeneous algorithm, pages 197\u2013232. Kluwer Academic Publishers, 1999. [2] Andreas Argyriou, Raphael Hauser, Charles A. Micchelli, and Massimiliano Pontil. A DC-programming algorithm for kernel selection. In ICML, Pennsylvania, USA, 2006. [3] Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite programs. In STOC, pages 227\u2013236, New York, NY, USA, 2007. ACM. [4] Francis R. Bach, Gert R. G. Lanckriet, and Michael I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In ICML, pages 6\u2013, New York, NY, USA, 2004. ACM. [5] Kristin P. Bennett and Erin J. Bredensteiner. Duality and geometry in SVM classifiers. In ICML, pages 57\u201364, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. [6] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM TIST, 2(3): 27:1\u201327:27, May 2011. [7] Corinna Cortes. Invited talk: Can learning kernels help performance? In ICML, Montreal, Canada, 2009. [8] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning non-linear combinations of kernels. In NIPS, pages 396\u2013404, Vancouver, Canada, 2009. [9] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Two-stage learning kernel algorithms. In ICML, pages 239\u2013246, Haifa, Israel, 2010. 10] Nello Cristianini, John Shawe-Taylor, Andr\u00b4e Elisseeff, and Jaz S. Kandola. On kernel-target alignment. In Innovations in Machine Learning, pages 205\u2013256. Springer, 2006. 11] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library for large linear classification. JMLR, 9:1871\u20131874, 2008. 12] Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Sch\u00a8olkopf, and Alexander J Smola. A kernel method for the two-sample problem. In NIPS, pages 513\u2013. MIT, 2007. 13] Ashesh Jain, S.V.N. Vishwanathan, and Manik Varma. SPF-GMKL: Generalized multiple kernel learning with a million kernels. In KDD, pages 750\u2013758, New York, NY, USA, 2012. ACM. 14] Satyen Kale. Efficient algorithms using the multiplicative weights update method. PhD thesis, Princeton University, 2007. 15] Marius Kloft, Ulf Brefeld, S\u00a8oren Sonnenburg, Pavel Laskov, Klaus-Robert M\u00a8uller, and Alexander Zien. Efficient and accurate Lp-norm multiple kernel learning. In NIPS, pages 997\u20131005, Vancouver, Canada, 2009. 16] Marius Kloft, Ulf Brefeld, S\u00a8oren Sonnenburg, and Alexander Zien. lp-norm multiple kernel learning. JMLR, 12: 953\u2013997, 2011. 17] Abhishek Kumar, Alexandru Niculescu-Mizil, Koray Kavukcuoglu, and Hal III Daume. A binary classification framework for two stage multiple kernel learning. In ICML, pages 1295\u20131302, 2012. 18] Gert R. G. Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I. Jordan. Learning the kernel matrix with semidefinite programming. JMLR, 5:27\u201372, December 2004. 19] Charles A. Micchelli and Massimiliano Pontil. Learning the kernel function via regularization. JMLR, 6:1099\u2013 1125, December 2005.\n[20] Cheng Soon Ong, Alexander J. Smola, and Robert C. Williamson. Learning the kernel with hyperkernels. JMLR, 6:1043\u20131071, 2005. [21] Francesco Orabona and Luo Jie. Ultra-fast optimization algorithm for sparse multi kernel learning. In ICML, pages 249\u2013256, Bellevue, USA, 2011. [22] Paul Pavlidis, Jason Weston, Jinsong Cai, and William Noble Grundy. Gene functional classification from heterogeneous data. In Proc. Intl. Conf. on Computational Biology, RECOMB \u201901, pages 249\u2013255, New York, NY, USA, 2001. ACM. [23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12:2825\u20132830, 2011. [24] Jeff M. Phillips and Suresh Venkatasubramanian. A gentle introduction to the kernel distance. CoRR, abs/1103.1625, 2011. [25] Alain Rakotomamonjy, Francis Bach, St\u00b4ephane Canu, and Yves Grandvalet. More efficiency in multiple kernel learning. In ICML, pages 775\u2013782, New York, NY, USA, 2007. ACM. [26] S\u00a8oren Sonnenburg, Gunnar R\u00a8atsch, Christin Sch\u00a8afer, and Bernhard Sch\u00a8olkopf. Large scale multiple kernel learning. JMLR, 7:1531\u20131565, December 2006. [27] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software, 11\u201312:625\u2013653, 1999. [28] Leslie G. Valiant. A bridging model for parallel computation. Commun. ACM, 33(8):103\u2013111, August 1990. [29] Manik Varma and Bodla Rakesh Babu. More generality in efficient multiple kernel learning. In ICML, pages 1065\u20131072, New York, NY, USA, 2009. ACM. [30] S. V. N. Vishwanathan, Zhaonan Sun, Nawanol Ampornpunt, and Manik Varma. Multiple kernel learning and the SMO algorithm. In NIPS, volume 22, pages 2\u2013, Vancouver, Canada, 2010. [31] Christopher Williams and Matthias Seeger. Using the Nystr\u00a8om method to speed up kernel machines. In NIPS, pages 682\u2013688, 2001. [32] Zenglin Xu, Rong Jin, Irwin King, and Michael R. Lyu. An extended level method for efficient multiple kernel learning. In NIPS, pages 1825\u20131832, Vancouver, Canada, 2008. [33] Zenglin Xu, Rong Jin, Haiqin Yang, Irwin King, and Michael R. Lyu. Simple and efficient multiple kernel learning by group lasso. In ICML, pages 1175\u20131182, Haifa, Israel, 2010. [34] Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystr\u00a8om method vs random fourier features: A theoretical and empirical comparison. In NIPS, pages 485\u2013493, 2012. [35] Jieping Ye, Jianhui Chen, and Shuiwang Ji. Discriminant kernel and regularization parameter learning via semidefinite programming. In ICML, pages 1095\u20131102, New York, NY, USA, 2007. ACM. [36] Alexander Zien and Cheng Soon Ong. Multiclass multiple kernel learning. In ICML, pages 1191\u20131198, New York, NY, USA, 2007. ACM.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of Multiple Kernel Learning (MKL), which has been successfully applied across various learning tasks. Prior methods, while theoretically sound, often require extensive computational resources and do not scale well with larger datasets. This paper proposes a geometric formulation of the MKL problem that allows for efficient scaling and optimization.",
        "problem": {
            "definition": "The problem defined in this paper revolves around learning kernel weights in a way that maximizes the minimum distance between two convex polytopes, which is crucial for effective classification using support vector machines.",
            "key obstacle": "The main challenge lies in the computational inefficiency and memory requirements of existing MKL methods, which struggle to handle large datasets and complex kernel combinations."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by the geometric interpretation of the MKL problem, focusing on optimizing kernel distances rather than using traditional methods that rely heavily on semidefinite programming.",
            "opinion": "The paper presents a novel approach that reduces the MKL problem to a simpler optimization routine, making it more efficient and scalable.",
            "innovation": "The key innovation is the use of a geometric perspective to derive a Quadratically Constrained Quadratic Program (QCQP) that can be solved using an efficient algorithm, thus avoiding the need for expensive SDP solvers."
        },
        "method": {
            "method name": "Matrix Weight Update for Multiple Kernel Learning (MWUMKL)",
            "method abbreviation": "MWUMKL",
            "method definition": "MWUMKL is defined as an optimization algorithm that seeks to find the optimal kernel weights by maximizing the minimum distance between the convex hulls of positive and negative samples.",
            "method description": "The method utilizes a geometric interpretation of the MKL problem to achieve efficient optimization.",
            "method steps": [
                "Define the kernel distance as a function of existing kernel matrices.",
                "Transform the MKL problem into a QCQP.",
                "Use a modified Matrix Multiplicative Weight Update (MMWU) algorithm to iteratively update kernel weights based on support vectors."
            ],
            "principle": "The effectiveness of MWUMKL stems from its ability to leverage the geometric properties of the kernel space, leading to a faster convergence and reduced memory usage compared to traditional MKL methods."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on eleven datasets, including small, medium, and large datasets from the UCI repository, comparing MWUMKL against baseline methods like UNIFORM and LibLinear with Nystr\u00f6m approximations.",
            "evaluation method": "The performance was assessed based on classification accuracy and training time, with a focus on scalability in terms of both dataset size and the number of kernels used."
        },
        "conclusion": "The experimental results demonstrate that MWUMKL significantly outperforms existing MKL algorithms in both speed and accuracy, making it a viable solution for large-scale machine learning tasks.",
        "discussion": {
            "advantage": "MWUMKL's primary advantages include faster computation, reduced memory footprint, and the ability to handle larger datasets more effectively than traditional MKL methods.",
            "limitation": "One limitation of the proposed method is that it is currently designed for single-machine implementations, which may restrict its application to extremely large datasets.",
            "future work": "Future directions include enhancing the scalability of MWUMKL through parallelization techniques and exploring its application in distributed computing environments."
        },
        "other info": {
            "acknowledgments": "This research was partially supported by the NSF under grant CCF-0953066. The authors express gratitude to Satyen Kale and S\u00e9bastien Bubeck for their valuable feedback."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational idea behind semi-supervised algorithms is to leverage both labeled and unlabeled data to improve learning efficiency, which relates to the geometric formulation of Multiple Kernel Learning (MKL) proposed in this paper."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of semi-supervised learning is to overcome the limitations of traditional methods that require extensive computational resources, as highlighted by the challenges faced in existing MKL methods."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Matrix Weight Update for Multiple Kernel Learning (MWUMKL), is an optimization algorithm that maximizes the minimum distance between convex hulls of positive and negative samples, categorizing it under generative and probabilistic models."
        },
        {
            "section number": "4.1",
            "key information": "The importance of data labeling is emphasized in the context of MKL, where learning effective kernel weights is crucial for classification, which relies on the quality of labeled data."
        },
        {
            "section number": "5.1",
            "key information": "The fundamental differences between semi-supervised and unsupervised learning can be illustrated through the challenges faced by existing MKL methods, which struggle with large datasets, highlighting the need for semi-supervised approaches."
        },
        {
            "section number": "7.1",
            "key information": "Scalability and computational complexity are significant challenges in semi-supervised learning, as discussed in the paper, where existing MKL methods do not scale well with larger datasets."
        },
        {
            "section number": "8",
            "key information": "The conclusion of the paper reinforces the importance of efficient and scalable solutions in semi-supervised learning, as demonstrated by the performance of MWUMKL in comparison to existing MKL algorithms."
        }
    ],
    "similarity_score": 0.6040721058056655,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/A Geometric Algorithm for Scalable Multiple Kernel Learning.json"
}