{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.17040",
    "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks",
    "abstract": "We study the phenomenon of in-context learning (ICL) exhibited by large language models, where they can adapt to a new learning task, given a handful of labeled examples, without any explicit parameter optimization. Our goal is to explain how a pre-trained transformer model is able to perform ICL under reasonable assumptions on the pre-training process and the downstream tasks. We posit a mechanism whereby a transformer can achieve the following: (a) receive an i.i.d. sequence of examples which have been converted into a prompt using potentiallyambiguous delimiters, (b) correctly segment the prompt into examples and labels, (c) infer from the data a sparse linear regressor hypothesis, and finally (d) apply this hypothesis on the given test example and return a predicted label. We establish that this entire procedure is implementable using the transformer mechanism, and we give sample complexity guarantees for this learning framework. Our empirical findings validate the challenge of segmentation, and we show a correspondence between our posited mechanisms and observed attention maps for step (c).",
    "bib_name": "abernethy2023mechanismsampleefficientincontextlearning",
    "md_text": "# A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks\nJacob Abernethy\nGoogle Research\n& Georgia Institute of Technology\nabernethyj@google.com\nAlekh Agarwal\nGoogle Research\nalekhagarwal@google.com\nTeodor V. Marinov\nGoogle Research\ntvmarinov@google.com\nManfred K. Warmuth\nGoogle Research\nmanfred@google.com\n# Abstract\nWe study the phenomenon of in-context learning (ICL) exhibited by large language models, where they can adapt to a new learning task, given a handful of labeled examples, without any explicit parameter optimization. Our goal is to explain how a pre-trained transformer model is able to perform ICL under reasonable assumptions on the pre-training process and the downstream tasks. We posit a mechanism whereby a transformer can achieve the following: (a) receive an i.i.d. sequence of examples which have been converted into a prompt using potentiallyambiguous delimiters, (b) correctly segment the prompt into examples and labels, (c) infer from the data a sparse linear regressor hypothesis, and finally (d) apply this hypothesis on the given test example and return a predicted label. We establish that this entire procedure is implementable using the transformer mechanism, and we give sample complexity guarantees for this learning framework. Our empirical findings validate the challenge of segmentation, and we show a correspondence between our posited mechanisms and observed attention maps for step (c).\n# 1 Introduction\nIn-context learning has emerged as a powerful and novel paradigm where, starting with Brown et al. [3], it has been observed that a pre-trained language model can \u201clearn\u201d simply through prompting with a handful of desired input-output pairs from a new task. Strikingly, the model is able to perform well on future input queries from the same task by simply conditioning on this prompt, without updating any model parameters, and using a surprisingly small number of examples in the prompt to learn a target task. While model fine-tuning for few shot learning can be explained in terms of the vast literature on transfer learning and domain adaptation, ICL eludes an easy explanation for its sample-efficiency and versatility. In this paper, we study the question: What are plausible mechanisms to explain ICL for some representative tasks and what is their sample complexity? Before discussing potential answers, we note why the ICL capability is surprising, and merits a careful study. Typical few-shot learning settings consist of a family of related tasks among which transfer is expected. On the other hand, the pre-training task of predicting the next token for language models appears largely disconnected from the variety of downstream tasks ranging from composing verses to answering math and analogy questions or writing code, that they are later prompted for. More importantly, a typical prompt consists of a sequence of unrelated inputs for a problem, followed by the desired outputs. This should constitute a very unlikely sequence for the model, since its training data seldom contains input, output pairs for a single concept occurring together. Due to this\nabruptness of example boundaries, recognizing such a prompt as a sequence of independent examples to learn across is an impressive feat in of itself. Once the model segments this prompt, it still needs to learn a consistent hypothesis across examples in the prompt to map inputs to desired outputs, and then apply this learned hypothesis to fresh query inputs. Given that this capability has been primarily observed in transformer based models, we investigate if there are aspects of self-attention which are particularly well-suited to addressing the aforementioned challenges? In this paper, we study all the questions mentioned above. A formal investigation of ICL was pioneered in Garg et al. [10], who trained transformer models [17] from scratch that can learn linear regression via ICL. Within this model, gradient descent or closed form ridge regression approaches to map the prompt to a hypothesis were put forth in von Oswald et al. [18] and Aky\u00fcrek et al. [1]. These works show that the posited mechanisms can be implemented using a transformer, given an appropriate formatting and tokenization of the inputs. Dai et al. [5] study the relationship between linear attention and gradient descent, and Li et al. [12] study transformers as producing general purpose learning algorithms. Of these, only Li et al. [12] studies sample complexity aspects, though the stability parameter in their bounds is not explicitly quantified. From a statistical perspective, Xie et al. [21] and Zhang et al. [22] cast ICL as posterior inference, with the former studying a mixture of HMM models and the latter analyzing more general exchangeable sequences. Wies et al. [20] give PAC guarantees for ICL, when pre-trained on a mixture of downstream tasks. These works do not, however, provide mechanisms to implement the desired learning procedures using a transformer. Olsson et al. [13] give some evidence that ICL might arise from a mechanism called induction heads, but do not discuss the sample complexity aspects or describe how induction heads might be leveraged to address a variety of learning tasks. We defer a more detailed discussion relative to these works, as well as connections with the broader literature on uses of the ICL capability to Appendix A. Our Contributions. Our work studies the ICL process in an end-to-end manner. Unlike most prior works on ICL, which either require pre-training task to be identical to the downstream task [10, 18, 1], or comprised of some mixture of downstream tasks [21, 20], we abstract the details of this procedure by representing it as a fixed and given prior distribution over sequences. Our results include: \u2022 Prompt segmentation: We propose a segmentation mechanism for the prompt, which maximizes the likelihood of a proposed segmentation under the prior learned during pre-training. The mechanism crucially leverages aspect of the attention architecture to learn the segmentation with few examples. The sample complexity scales logarithmically with the number of candidate delimiters and inversely in a gap parameter between the prior likelihoods of correctly and incorrectly segmented sequences. \u2022 Inferring consistent hypothesis: We then take the segmentation of the prompt and illustrate how to infer a consistent hypothesis which explains all the (input, output) pairs in the prompt using a transformer model. For this part, we specialize to a family of sparse retrieval tasks, where the output is simply a token of the input, or the sum of a subset of input tokens. This family is a useful abstraction of token extraction and manipulation tasks in practical ICL settings. We show how attention can naturally leverage correlations to identify a consistent hypothesis on such tasks. The proposed mechanism finds an \u03f5 accurate hypothesis from a class F using O \ufffd1 \u03f5 ln |F| \ufffd examples. \u2022 Inference with the learned hypothesis: We also show how the attention mechanism is well-suited to carry this hypothesis learned from the prompt and apply it to subsequent query inputs. \u2022 Empirical validation: Finally, we validate some of our theoretical findings through empirical validation, showing the dependence of ICL on easily identifiable delimiters. For hypothesis learning, we show that transformer models can be indeed trained to solve the sparse retrieval tasks studied here, and that the attention outputs correspond to the key steps identified in our theoretical mechanisms.\n# 2 Problem Setting and Notation\nA language model LM is an oracle that takes as input elements of a language V\u2217, sequences of tokens from a vocabulary V, with V := |V|. A typical language model is autoregressive: it aims to predict the next sequence of tokens from an prefix. To complete the phrase \u201cI came, I saw\u201d, we construct prompt = [<begin>,I,<space>,came,<comma>,<space>,I,<space>,saw], and input prompt \u2192 LM \u2192 output, and we expect that output = [<comma>,<space>,I,<space>,conquered,<end> ].\nWe describe the design of a language model using the architecture known as the decoder-only transformer [17]. In short, transformers are models that process arbitrary-length token sequences by passing them through a sequence of layers in order to obtain a distribution over the next token. For the following definition, we will need some special operators, which we describe here. The operation softmax(M) returns a matrix the same shape as M whose i, j entry is exp(Mi,j) \ufffd j\u2032 exp(Mi,j\u2032). The concat(M1, M2) operation stacks the matrices vertically. The operation mask(M) takes a square matrix M and returns M \u2032 such that M \u2032 i,j = Mi,j for i \u2264j and M \u2032 i,j = \u2212\u221eotherwise. (The \u2212\u221eis converted to a 0 after the softmax operation.) The GeLu operation is the Gaussian Error Linear Unit. Definition 1. Let d, datt, \u03ba be arbitrary positive integers. A transformer layer is a function T\u03a0 parameterized by matrices \u03a0 := {Qk, Kk, Vk \u2208Rdatt\u00d7d for k \u2208[\u03ba], WO \u2208Rd\u00d7\u03ba\u00b7datt}, that maps, for any length N sequence, Rd\u00d7N \u2192Rd\u00d7N using the following procedure: Input: X \u2208Rd\u00d7N, Set: Ak \u2190softmax \u25e6mask(d\u22121/2 att X\u22a4Q\u22a4 k KkX) \u2200k \u2208[\u03ba] Set: X\u2032 \u2190WO concat(V1XA1, . . . , V\u03baXA\u03ba), Output: X + GeLu(X\u2032) \u2208Rd\u00d7N.\nWe omit the layer normalization present in implementations [17] for ease of presentation. A convenient aspect of transformer layers is their composability. Assume we have L transformer layers, where the \u2113-th layer is parameterized by \u03a0\u2113:= {W \u2113 O \u2208Rd\u00d7\u03ba\u00b7datt; Q\u2113 k, K\u2113 k, V \u2113 k \u2208Rdatt\u00d7d, k \u2208[\u03ba]}. The remaining piece we need for the full transformer model is the token embedding layer, which is parameterized by a matrix WE \u2208Rd\u00d7|V|. If we take a prefix x \u2208V\u2217with N tokens, and write it using one-hot encoding as a matrix Z \u2208{0, 1}|V|\u00d7N, then WEZ is referred to as the \u201cembedded\u201d tokens. Once these embedded tokens are passed through one or more transformer layers to obtain Z\u2032, we can convert back to vocab space by W \u22a4 E Z\u2032. Here Zi,j represents the model\u2019s estimated probability that the j + 1th token will be token i given the first j tokens in the sequence. We need these embeddings to be reasonably distinct. Definition 2. A (decoder-only) L-layer transformer is a parameterized function that maps R|V|\u00d7N \u2192 R|V|\u00d7N for any sequence length N where the input X is a one-hot encoding of a token sequence x in V\u2217, and the output is a column-stochastic matrix Z. The parameters are given by the matrix WE and the sequence \u03a01, . . . , \u03a0L. The full map is defined as the composition, X \ufffd\u2192Z = softmax(W \u22a4  \u00b7 T \u25e6\u00b7 \u00b7 \u00b7 \u25e6T(W \u00b7 X)).\nThe one-hot encoding can be replaced with other (possibly learned) encodings when V is large or infinite. Of much interest in this work is to understand what operations can be implemented using a transformer. To establish our results, we often show that certain operations Rd\u00d7N \u03d5\u2192Rd\u00d7N on embedded token sequences can be implemented using a transformer layer parameterized by \u03a0. When there is a \u03a0 such that T\u03a0 \u2261\u03d5 for all N, then \u03d5 can be implemented as a transformer layer.\n# 2.2 In-Context Learning\nLet us now imagine that we hope to solve the following learning problem. We are given an input space X and output space Y. We assume that X, Y \u2282V\u2217for simplicity \u2013i.e., we are able to express inputs/outputs in the given language. Assume we have a set F of functions f : X \u2192Y. A task in this setting is a pair f, D, with f \u2208F a function and D \u2208\u2206(X) a distribution on inputs x \u2208X. A sample Sn from this task is a collection of n labelled examples {(x1, y1), . . . , (xn, yn)} \u2282X \u00d7 Y where the xi\u2019s are samples i.i..d. from D and yi = f(xi) for every i \u2208[n]. When viewed as a typical supervised learning setting, we would design a learning algorithm A that is able to estimate \u02c6f \u2208F from a sample Sn, {(x1, y1), . . . , (xn, yn)} \u2192 A \u2192 \u02c6fn. The goal of A is to minimize expected loss Ex\u223cD[loss(f(x), \u02c6fn(x))] with respect to a typical sample x \u223cD and (unknown) function f. The in-context learning framework poses the idea that perhaps for a large family of tasks we do not need to design such an algorithm A and instead we can leverage a pre-trained language model in order to solve a large family of learning tasks. That is, for a sample above and a test point x \u223cD, we have the following setup Encoding({(x1, y1), . . . , (xn, yn)}, x) \u2192 LM \u2192 output,\nSince there is no canonical procedure to encode a set of example-label pairs guaranteed to be understood by LM, the choice of Encoding influences its output behavior. Empirical works use typical delimiters\u2014special tokens that are typically used to give structure to documents by segmenting text into lists, relations, etc.\u2014for this task. As part of our language definition, we assume that there is a set of special tokens Vdelims \u2282V including, e.g. punctuations (<comma>, <colon>, <semicolon>), or spacing characters (<space>, <newline>, <tabspace>). We assume that the user has selected one delimiter that separates the n examples, which we will call <esep>, and another that distinguishes between xi and yi, which we will call <lsep>. The only requirement is that <esep> and <lsep> are distinct elements of Vdelims, and that <esep> and <lsep> do not occur in any x, y examples generated in the task. With this in mind, we define Encoding({(x1, y1), . . . , (xn, yn)}, x) as\n# 3 An Overview of Results\nWe now survey the core results of the paper on segmenting the input sequence through delimiter identification, and the subsequent hypothesis learning.\n# 3.1 Segmenting an input sequence\nSuppose we have an underlying distribution p0(\u00b7) on V\u2217, which measures the typical likelihoo of sequences observed \u201cin the wild\u201d, and this distribution is encoded in a transformer through th pre-training process. The goal of the segmentation mechanism is to identify a pair of separator <lsep>, <esep> \u2208Vdelims \u00d7 Vdelims, such that the input z can be reasonably decomposed as:\nz = <begin>x1<lsep>y1<esep> . . . <esep>xk<lsep>yk<esep>x\u2217<lsep>.\nTo formalize a reasonable decomposition of z obtained using delimiters <lsep>, <esep>, we de its likelihood by leveraging the base model p0 and then follow a maximum likelihood segmentati\n\ufffd <lsep>, \ufffd <esep> = argmax <lsep>\u2208Vdelims <esep>\u2208Vdelims p0(x\u2217) k \ufffd i=1 p0(<begin>xi<end>)p0(<begin>yi<end>), (2)\n(2)\nHaving generated a segmentation, the next step in ICL is to take the inputs (xi, yi)n i=1 identified above and generate a hypothesis \ufffdf such that \ufffdf(x) \u2248f \u22c6(x), where yi = f \u22c6(xi). To formalize the hypothesis learning setup, we focus on a specific family of learning problems that we define next. Definition 3 (Tokenized sparse regression). Fix an input space X, an output space Y, a basis map \u03c8(x) : Rdim(X) \u2192Rm, and distribution D over X. Given s \u2264m, an s-sparse tokenized regression problem is defined by weights \u03b21, . . . , \u03b2m \u2208{0, 1}m such that |{j : |\u03b2j| = 1}| = s and y = \ufffdm j=1 \u03b2j\u03c8(x)j. In words, a tokenized sparse regression problem maps from inputs to outputs by taking sparse linear combinations of the inputs under some fixed basis transformation. We call the task tokenized due to the way in which the transformer processes the input x during ICL, accepting it one coordinate at a time as we will see momentarily. This is to distinguish from the regression tasks studied in prior works [10, 1, 18], which consider each vector x to be a token. In particular, we study problems where the basis \u03c8 is fixed across contexts and only the coefficients \u03b2i vary across tasks. Since \u03c8 is fixed, it can be assumed to be known from pre-training and we focus on the case of X = Rm and \u03c8(x)i = xi, that is the basis is just the standard basis and the task is sparse linear regression. We focus on these tasks because extracting and manipulating a few input tokens seems emblematic of many of the string processing tasks where ICL is often used in practice. We analyze the following estimator for all i \u2208[n]:\n \u2208 \ufffd  \u2208 | \u2212| \u2264 \ufffd The optimization problem (3) can be implemented with a transformer, as stated next. Theorem 3 (Transformers find a consistent hypothesis). There exists a transformer with O(m) layers and 1 head per layer which computes an fi according to (3) after reading example xi.\n\ufffd \ufffd The optimization problem (3) can be implemented with a transformer, as stated next. Theorem 3 (Transformers find a consistent hypothesis). There exists a transformer with O(m) layers and 1 head per layer which computes an fi according to (3) after reading example xi. The estimator above is natural for the task, as it finds a solution with a zero loss on the training samples. Implementing this estimator is particularly natural with a transformer using properties of the attention mechanism, that is well suited to extracting coordinates of x which are highly correlated with the label y. In fact, the actual mechanism computes a weighting over {1, 2, . . . , m} as candidate solutions from each example, and then returns fi as the coordinate with the largest cumulative weight (across examples) after each example i. This provides further robustness in case of label noise. For this procedure, we provide the following sample complexity guarantee. Theorem 4 (Sample complexity of hypothesis learning, informal). For any \u03f5 > 0, suppose the initial token embeddings are such that tokens z\u03b1, z\u03b3 with |z\u03b1 \u2212z\u03b3| \u2265\u03f5 have nearly orthogonal embeddings. Let fn be any hypothesis returned by Equation 3 after seeing n examples from the s-sparse token regression task. Then for n = \u2126(s log(m/\u03f5)/\u03f5) we have E[|fn(x) \u2212f \u22c6(x)|] \u22642\u03f5. The condition on the token embeddings is natural, since aliased tokens with different values can be problematic for learning. Our sample complexity matches the typical guarantees in sparse regression, which scale with the s log m \u03f5 . Note that the estimator (3) learns fn from scratch, in that there is no use of the pre-training to bias the estimator towards certain functions. While this is also done in several prior works on ICL [10, 1, 18], in practice ICL is often used in settings where the correct y has a high probability under the base distribution p0, given x. Improving the estimator (3) to prioritize among the consistent tokens j using the prior probabilities p0(xi,m+1|xi,1, . . . , xi,m) is an easy modification to our construction and allows us to further benefit from an alignment between p0 and D.\n# 4 Segmenting an ICL Instance\nIn this section, we provide more details about the segmentation mechanism and the underlying assumptions. Given a sequence of N tokens z = z1, . . . , zN, and given a pair \u03c3 = (<lsep>, <esep>) we first segment the sequence into as many chunks as possible, separated by <esep> tokens:\n(3)\nDouble, double toil and trouble: / Macbeth; To die, - To sleep, - To sleep! / Hamlet; This above all: to thine own self be true / Hamlet; A deed without a name / Figure 1: An example of an ICL task: quotes from Shakespeare followed by the name of the play. Correct delimiters are (<lsep>, <esep>) = (/, ;), yet the presence of other potential delimiters creates ambiguity. minimum probability. If no <lsep> is found, then we set ui = xi. With this segmentation in mind, we can now define our probability model p\u03c3 as follows,\n \ufffd To understand the meaning of this definition better, it is helpful to look at an example. In Figure 1, we show a concatenation of quotes from Shakespeare and the corresponding play names. Using candidate delimiter pairs \u03c3 = (/, ;) and \u03c3\u2032 = (:, -), we get two different likelihood models: p\u03c3(z) = p0(<begin>Double,...trouble:<end>)p0(<begin>Macbeth<end>) \u00b7 p0(<begin>To die, <end>)... p\u03c3\u2032(z) = p0(<begin>Double,...trouble<end>)p0(<begin>/ Macbeth;...To die,<end>) \u00b7 p0(<begin>To sleep,<end>)p0(<begin><end>)...\n \ufffd To understand the meaning of this definition better, it is helpful to look at an example. In Figure 1, we show a concatenation of quotes from Shakespeare and the corresponding play names. Using candidate delimiter pairs \u03c3 = (/, ;) and \u03c3\u2032 = (:, -), we get two different likelihood models:\n\u00b7 p\u03c3\u2032(z) = p0(<begin>Double,...trouble<end>)p0(<begin>/ Macbeth;...To die,<end>) \u00b7 p0(<begin>To sleep,<end>)p0(<begin><end>)...\nIt is also important to note that, even though the model above provides a nicely-factored estimate of the sequence likelihood, we are still able to compute the probability of other segments. For any 1 \u2264i < j \u2264N, p\u03c3(zi \u00b7 \u00b7 \u00b7 zj|z1 \u00b7 \u00b7 \u00b7 zi\u22121) can be evaluated from the model above, even if [i : j] may cross delimiter boundaries. The objective (2) maximizes p\u03c3(z) over \u03c3 to select the model.\nNote that in both cases above, the segmented chunks, ui, are always determined by \u03c3\u22c6even though their likelihood is evaluated on the \u201cfalse\u201d \u03c3\u2032. What does this assumption mean? It may appear to be highly technical, but it encodes something that we would very naturally expect: incorrectly segmented data should look very weird (unlikely) relative to correctly interpreted data. For instance, revisiting the example from Figure 1, consider the second chunk according to the true segmentation\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b17f/b17fc92e-6bb7-4e14-9396-61ce21c6a503.png\" style=\"width: 50%;\"></div>\nFigure 2: A transformer for 1-sparse tokenized regression with n = 2 examples and m = 3 tokens per example. The curved lines show attentions, with heights proportional to the attention. The blue and red attention lines show the attentions of y1 and y2 over the previous tokens. The green attention lines show the attentions of x2,2 and x2,3 over the previous tokens. In this case, f \u22c6= 2. After the first example, there is ambiguity between f1 \u2208{2, 3}, hence the output f1(x2) mixes theseand is not correct. After the second example, the answer is uniquely determined, for inference on third example and beyond. In the first layer, each yi attends to tokens xi,j from example i to find all consistent hypotheses in example i. By attending across previous yt\u2019s, each yi aggregates these hypotheses over all preceding inputs t \u2264i. Example i + 1 then attends to yi to predict using the aggregated hypothesis in the final two layers.\nwith \u03c3\u22c6= (/, ;), ui =\u201cTo die, - To sleep, - To sleep! / Hamlet;\u201d. When correctly segmented we obtain the pair xi = To die, - To sleep, - To sleep! and yi = Hamlet, and the likelihood p\u03c3\u22c6(ui) = p0(xi)p0(yi). On the other hand, when we use the incorrect delimiters \u03c3\u2032 = (:, -) we get a much less natural segmentation, with model estimate p\u03c3\u2032(ui | prefixi\u22121) =p0(To die,<end> | <begin>/ Macbeth;) \u00b7 p0(<begin>To sleep,<end>) \u00b7 p0(<begin><end>)p0(<begin>To sleep! / Hamlet). Assumption 1 says that the model estimate for these chunks according to p\u03c3\u22c6should be much higher, on average, than that for p\u03c3\u2032. This is indeed the only needed assumption to obtain the following. Theorem 5. Let \u03bd > 0 be such that p\u03c3\u2032(ui|prefixi\u22121) \u2265\u03bd where ui is the ith chunk under \u03c3\u22c6, for all ui and prefixi\u22121 almost surely. Under Assumption 1, the maximum likelihood segmentation algorithm (2) outputs the correct delimiters \u03c3\u22c6w.p. at least 1 \u2212\u03b4, as long as n \u2265 16(log 1 \u03bd ) 2 log |Vdelims| \u03b4 c2 .\n# Learning a Consistent Hypothesis for Tokenized Sparse R\nWe now formalize the results for extracting a consistent hypothesis for the tokenized sparse regression tasks described in Definition 3. For intuition, we begin with s = 1.\nThe 1-sparse tokenized regression task: Recall that when s = 1, we have yi = xi,f \u22c6for each example i \u2208[n], and f \u22c6\u2208[m] is the coordinate of xi being copied to y. The objective (3) similarly simplifies to finding an index fi \u2208[m] such that |xt,j \u2212yt| \u2264\u03f5 for all examples t \u2264i. We now describe the key elements of a transformer that implements such a procedure; details in Appendix D. The construction is depicted in Figure 2. Let z1 i be the initial token embedding, i.e. the ith column of WEX in Definition 2. Assumption 2. Let z1 \u03b1 be the input embedding of a token z\u03b1. For any z\u03b1, z\u03b3, such that |z\u03b1 \u2212z\u03b3| \u2265\u03f5, we have |\u27e8z1 \u03b1, z1 \u03b3\u27e9| < 1 2\u03c4 , for some \u03c4 \u22651. If |z\u03b1 \u2212z\u03b3| \u2264\u03f5, then \u27e8z1 \u03b1, z1 \u03b3\u27e9\u22650 and \u27e8z1 \u03b1, z1 \u03b1\u27e9= 1. One embedding which satisfies the assumption sends all z\u03b1, z\u03b3 s.t. |z\u03b1 \u2212z\u03b3| \u2265\u03f5 to orthogonal vectors in R\u2308(1/\u03f5)\u2309. In Appendix D we discuss alternatives to cut the dimension from O( 1 \u03f5 ) to \u02dcO(\u03c4 2). Given such an embedding, we can now use inner products between tokens to detect similarity, which is the key first step in our construction. This is also very natural to implement using attention. We define the first attention layer so that each xi,j only attends to itself and yi only attends to xi,1, . . . , xi,m using the position embeddings. The attention weight between yi and xi,j is defined using the inner product of their first layer embeddings. By Assumption 2, this inner product is large for j = f \u22c6and small whenever |xi,j \u2212yi| > \u03f5. In Appendix D, we define the query and key matrices to induce such an attention map. Under Assumption 2, this attention map identifies a good hypothesis for example i, as formalized below. Lemma 1. Given an example i, let Ji = {j : |xi,j \u2212yi| \u2264\u03f5}, and let f \u22c6\u2208[m] be such that yi = xi,f \u22c6. Under Assumption 2, for any m \u22652, the output of the first layer at f \u22c6is larger than the output at any j \u2208[m] \\ Ji by at least e/(4(m + 1)). This key step in our construction gives us some consistent hypothesis from Ji, for each example i individually. The second layer now finds a hypothesis consistent with all examples (xt, yt), t \u2264i,\nand saves this hypothesis in token y3 i , which is the input to the third layer. The remaining two layers implement appropriate copy and extraction mechanisms for the hypothesis fi identified at yi to be applied to the next input xi+1 by first extracting the value at xi+1,fi and then outputting this value at the token xi+1,m as the ICL prediction at the (i + 1)th example. We illustrate the key points of the construction in Figure 2. For more details, we refer the reader to Appendix D. Prior work has focused on tasks where xi is not decomposed into one token per coordinate but rather given to the transformer as a vector in Rm directly. In Appendix F we demonstrate how this setting can be reduced to the tokenized setting that we study here. Iterative deflation for s-sparse token regression: For the more general case, it is tempting to directly apply the 1-sparse construction once more and hope that all the tokens in f \u22c6will be identified simultaneously, since they all should have a high inner product with yi under Assumption 2. However, suppose that f \u22c6= (1, 2) and let us say that the hypothesis (1, 3) is also consistent. Then an approach to learn 2 coordinates independently using the approach from the previous section can result in the estimate (2, 3) which might not be consistent. To avoid this, we identify coordinates one at a time, and deflate the target successively to remove the identified coordinates from further consideration. The deflation procedure works because of the following crucial lemma. Lemma 2. Under Assumption 2 with \u03c4 \u22652s, for any C \u2286f \u22c6and, j \u2208f \u22c6\\ C, we have \u27e8x1 i,j, y1 i \u2212 \ufffd j\u2032\u2208C x1 i,j\u2032\u27e9\u22653 4, while if |xi,j \u2212xi,j\u2032| \u2265\u03f5 for all j\u2032 \u2208f \u22c6then we have \u27e8x1 i,j, y1 i \u2212\ufffd j\u2032\u2208C x1 i,j\u2032\u27e9\u22641 4. This lemma says that the deflated target favors unidentified coordinates in f \u22c6. Next, we show that the deflation procedure, to remove all previously identified tokens from each yi, can be implemented using attention. We then stack O(s) layers of the 1-sparse task, with a deflation layer after extracting each coordinate to identify a complete consistent hypothesis that optimizes the objective 3. Putting everything together, we have the following formal version of the earlier informal Theorem 4 Theorem 6. For any \u03f5 > 0, let fn be some optimum of 3 after seeing n examples from the ssparse token regression task. Then there exists an embedding of xi,j, yi, \u2200i \u2208[n], j \u2208[m] in RO(s/\u03f5) satisfying Assumption 2, such that for any n = \u2126(s log(m/\u03f5)/\u03f5), w.p. 1\u2212\u03b4, E[|fn(x)\u2212f \u22c6(x)|] \u22642\u03f5.\n# 6 Empirical Results\nIn this section, we report some empirical findings on the 1-sparse tokenized regression task. For other experiments detailing the sensitivity of ICL to delimiters, we refer the readers to Appendix G.2. We follow the experimental setup from Garg et al. [10] and Aky\u00fcrek et al. [1], building on the implementation of Aky\u00fcrek et al. [1]. We use transformers with 8 layers, 1 head per layer and embedding size 128. We experiment on the 1-sparse tokenized regression task with (xi, yi)i\u2208[n] generated in the following way. First a random hypothesis f \u22c6is drawn uniformly at random from [m], where m = 5. Next, xi \u2208Rm is sampled i.i.d from a fixed distribution which is either a standard Gaussian (xi \u223cN(0, I5\u00d75)), or uniform over {+1, \u22121}5 (xi \u223cUnif({+1, \u22121}5)). yi is always set to f \u22c6(xi). We refer to the first setting as the Gaussian setting and the second as the Rademacher setting. We train three different transformers, one for each of the two settings, and one where the samples come from a uniform mixture over both Gaussian and Rademacher settings. After pre-training we carry out the ICL experiments by generating 64 example sequences of length 5, either all from the Gaussian setting or the Rademacher setting. A randomly drawn f \u22c6is sampled and fixed and shared across these 64 sequences, to allow averaging of results across multiple sequences. In Figure 3 we show the averaged loss of a model pre-trained on the uniform mixture of the settings, attention weights at the final layer (8) and attention weights at layer 6 for both settings. Appendix G.1 shows results for models trained using Gaussian or Rademacher examples only. For the loss plots, y-axis is the loss of the ICL inference at each example and x-axis is the number of in-context examples observed. All examples are indexed from 0. We see that the model reaches a loss of 0 in the Gaussian setting from a single sample, which is information theoretically optimal. In the Rademacher setting there are often multiple coordinates consistent with f \u22c6on the first 3 or 4 examples, which\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a648/a64872e8-d22e-4589-896a-d5513e9bac2c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Attention at layer 8 (Gaussian)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5108/5108799a-fdc6-4b1c-a921-7ddd8027b429.png\" style=\"width: 50%;\"></div>\nFigure 3: Loss and attention plots for 1-sparse tokenized regression for Gaussian (top) and Rademacher (bottom) inputs. Loss drops to zero as soon as f \u22c6is determined, and attentions follow the construction of Section 5 Indices 4, 10, 16, . . . are tokens where the label is predicted. In panels (b) and (e), these indices attend to the index of f \u22c6in xi to predict yi correctly. The target indices line (blue) in panel (b) perfectly overlaps with the attention spikes at tokens xi,0. In panel (d), the attention spikes largely overlap with target indices, but there is some noise (see text). In panels (c) and (f), these indices attend to all previous labels (indices 5, 11, 17, . . .) to aggregate a consistent hypothesis across previous examples.\nIn Figures 3 (b) and (e) we plot the attention of the tokens at which the model outputs its predicted label, corresponding to the last x token. For example i (starting at i = 0), this token is at index 6 \u2217(i + 1) \u22122 in our sequence, and we plot the attention weights from this token over all previous tokens in the sequence. We also show (in blue) the index corresponding to y, identified by f \u22c6. For the Gaussian setting (b), we see that the attention weights, starting at example i = 1, are peaked at f \u22c6(so these lines perfectly overlap with the target indices line in the plot). For instance, yi = xi,0 here and token 10 attends to token 6 accordingly to correctly predict y1. In the Rademacher setting (e) (where yi = xi,2), we see that there is more variance, due to the fact that there are often multiple consistent hypothesis for the first few examples, however as the transformer processes more examples from the sequence the attention becomes more peaked at f \u22c6. This behavior is consistent with our theoretical construction. We note that our theoretical construction from Section 5 implies that attention weights in the last layer should be split uniformly over all j which are consistent with f \u22c6up to example i. In Appendix G we also empirically demonstrate that this is the case by looking at the attention on a single randomly sampled sequence. Finally in Figure 3 (c) and (f) we plot the attention of the same tokens but at layer 6 of the transformer, that is the third layer counting from the final layer. We see that attentions are peaked at the tokens holding the labels for f \u22c6, that is xi,5. This is analogous to the step where yi attends to all previous ys in our construction to aggregate across examples, and we expect its role is the same here. In summary, we find that the attention maps in these experiments bear striking correspondence to our theory. We refer the reader to Appendix D for more results that validate this correspondence.\n# 7 Conclusion\nIn this paper, we take a fresh look at the in-context learning capability of transformers. We provide mechanisms that can implement sequence segmentation and hypothesis learning for a family of ICL tasks, and provide statistical guarantees showing that a fairly small number of examples indeed suffice\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/801b/801bc43f-ca56-4953-9e57-bdf9b874a7cf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Attention at layer 6 (Gaussian)</div>\n<div style=\"text-align: center;\">(f) Attention at 6 (Rademacher)</div>\nto convey a target concept using ICL. More broadly, the ability of ICL to demonstrate information theoretically optimal learning in the types of tasks used both here and in prior works [10, 18, 1] is quite impressive. It would be interesting to understand if there are learning tasks where this optimality fails to hold in ICL, and determine the necessary scaling of model size as a function of problem size needed to achieve sample-optimal learning, when possible.\n# References\n# A Related Work\nThere is a rich literature on fine-tuning a pre-trained model through various forms of meta-learning or bi-level optimization [15, 2, 7], which typically update all or some of the model\u2019s parameters to adapt it to a target task, with theoretical underpinnings either through direct analysis [8] or through the broader literature on transfer learning (see e.g. [14] and rerefences therein). Starting with the work of Brown et al. [3], and with careful prompt design, this has become a predominant mechanism for adaptation in large language models [23, 9, 19], including to complex scenarios such as learning reinforcement learning algorithms [11]. A consistently striking feature of these works is that it takes just a handful of examples from the target task to teach a new concept to the language model, once the inputs have been carefully phrased and formatted. This has naturally motivated a number of studies on the mechanisms underlying ICL, as well as its sample complexity. A formal treatment of this topic is pioneered in Garg et al. [10], who study ICL for linear regression problems, by pre-training a transformer architecture specifically for this task. That is, the model is trained on prompts consisting of (x, y) pairs from a linear regression instance, with the regression weights randomly chosen for each prompt. They showed that ICL can learn this task using a prompt of O(d) examples when x \u2208Rd. Subsequent work of von Oswald et al. [18] proposes an explanation for ICL in this task by showing that self-attention can simulate gradient descent steps for linear regression, so that the model can effectively learn an optimizer during pre-training. Aky\u00fcrek et al. [1] further extend this by suggesting that given enough parameters, the model can compute a full linear regression solution. Both works present some empirical evidence as well suggesting that these operations correspond to the final outputs and some intermediate statistics of the transformer, when trained for this task. Dai et al. [5] study the relationship between linear attention and gradient descent, and Li et al. [12] study transformers as producing general purpose learning algorithms. Xie et al. [21] and Zhang et al. [22] cast ICL as posterior inference, with the former studying a mixture of HMM models and the latter analyzing more general exchangeable sequences. Wies et al. [20] give PAC guarantees for the sample complexity of ICL, when pre-trained on a mixture of downstream tasks. Olsson et al. [13] and Elhage et al. [6] view ICL as an algorithm which copies concepts previously seen in a context example and then does inference by recalling these concepts when a new prompt matching previous examples occurs. Elhage et al. [6] explain this behavior formally for transformers with a single attention head and two layers and Olsson et al. [13] conduct an empirical study on a wider variety of tasks for larger transformers. Despite this growing literature, many aspects of the ICL capability remain unexplained so far. First, only Li et al. [12], Wies et al. [20] and Zhang et al. [22] provide any kind of sample complexity guarantees. Of these, the pre-training distribution in Wies et al. [20] is too specific as the downstream task mixture, while Li et al. [12] depend on an measure of algorithmic stability that is hard to quantify apriori. Secondly, all the works with the exception of Xie et al. [21] require that the prompt has already been properly parsed into input and output examples, so as to facilitate the explanation of learning in terms of familiar algorithms, and the explanation of Xie et al. [21] relies on a particular mixture of HMMs model. Further, we note that none of these works take into consideration the specifics of the transformer architecture and how self-attention can implement the proposed learning mechanisms. While we do not study the properties of the pre-training process and data distribution in the ICL capability, these factors have been found to be crucial in empirical investigations [4, 16], and expanding the theoretical model to address such phenomena is an important direction for future research.\n# B Notation\nWe denote the input sequence as z \u2208VN consisting of N tokens. Each sequence is assumed to contain up to n i.i.d. samples corresponding to a task, where sample i consists of an example xi \u2208Vm\u2032 and label yi \u2208V, where m\u2032 \u2264m is the token length of example xi. For majority of the learning tasks that we analyze, the label yi consists of only 1 token, and xi consists of up to m tokens. We use xi,j \u2208V for j \u2208[m] to denote the tokens of xi. The ICL instance is encoded as we have already described, where each xi and yi is separated by <lsep> \u2208Vdelims, and <esep> \u2208Vdelims is found between between each successive pair (xi, yi), where \u03c3 = (<lsep>, <esep>) are segmentation tokens. We will often refer to the \u201cground truth\u201d segmentation token pair as \u03c3\u22c6, where \u03c3 \u2208Vdelims \u00d7 Vdelims\nis an arbitrary pair of delimiters. We also recall the notation p\u03c3\u22c6(z) = \u03a0n i=1p0(xi)p0(yi) for some underlying distribution p0 on V\u2217, where the segmentation is determined by \u03c3\u2217, and we remind the reader that we can analogously define p\u03c3(z) for any other delimiter, which segments z into another sequence of (x, y) pairs. When it comes to constructing transformer architectures, we will generally follow the same notation laid out in Section 2.1, but with a few new symbols and additional conventions. When we write \u27e8u, v\u27e9M for matrix M and vectors u, v we mean the inner product \u27e8uM, v\u27e9= \u27e8u, Mv\u27e9. For characters z, x, y, o, v, K, Q, V , which describe the algorithmic objects of the transformer architecture, a numeric superscript, as the 2 in x2 i,j, shall be used to identify the layer index and should not be interpreted as an exponent. We described the attention mechanism in Definition 1, where the operation performed at layer \u2113\u2208[L] and head k \u2208[\u03ba] is parameterized by the query, key, and value matrices Q\u2113 k, K\u2113 k and V \u2113 k , respectively. The initial embeddings of the token sequence z is the matrix (equiv., list of vectors) [z1 1, . . . , z1 N] \u2208Rd\u00d7N, which is the matrix WEX where X \u2208{0, 1}|V|\u00d7N is the one-hot encoding of the tokens z. In general, when a token variable is superscripted with 1, we mean the embedded token, i.e. after multiplying by WE. So x1 i,j is the column of WE corresponding to the token index of xi,j. The attention operation computes the matrix A\u2113 k \u2208[0, 1]N\u00d7N. Normally we index this matrix with token indices i, j \u2208[N], but occasionally we will find it convenient to interpret A\u2113 k is a function on pairs of embedded tokens z\u2113 i, z\u2113 j, meaning that overload notation by setting A\u2113 k(z\u2113 i, z\u2113 j) := A\u2113 k(i, j). This is well defined as our token embeddings contain a positional encoding, and we note that this allows us to avoid determining the exact index of the embedded token xi,j, which can be cumbersome to describe. Thus we have\n\ufffd \ufffd \ufffd Finally, v\u2113 k,i and o\u2113 k,i refer to the \u201cvalue vector\u201d computed at position i for head k and layer \u2113, and the corresponding output vector:\nFor each i we vertically concatenate all of the outputs (o\u2113 k,i)k\u2208\u03ba to obtain a tall vector o\u2113 i \u2208Rdatt\u03ba, and the final output z\u2113+1 = [z\u2113+1 1 , . . . , z\u2113+1 N ] \u2208Rd\u00d7N is defined for all i \u2208[N] as\n our constructions in the remainder of this appendix, we make a number of simplifications for nvenience. Let us state these here, and argue why these are acceptable without loss of generality. 1. We often assume that \u03ba = 1 and we drop the reference to head k. Similarly, when \u2113is omitted it should be clear from context. 2. We often implicitly assume that either there is no \u201cskip connection,\u201d thus z\u2113+1 i = GeLU(W \u2113 Oo\u2113 i), and often go further and assume z\u2113+1 i = o\u2113 i, avoiding the GeLU transformation. While we did not use this feature in Definitions 1 and 2, in most transformer architectures there is an additional skip connection that makes this possible. 3. We occasionally refer to the dimension of the embedding d as being different between input and output, i.e. din and dout. This is for notational ease, and we may do this by padding earlier or later embedding dimensions with 0\u2019s.\n# C Proofs of the segmentation results\nWe first give a proof of Theorem 5, and then give the details of the transformer construction from Theorem 1.\n(5)\nProof. Let the distribution U (\u03c3\u2032,i) \u03c3\u22c6 := p\u03c3\u2032(ui|prefixi\u22121) where ui is the ith chunk as parsed by the correct segmentation \u03c3\u22c6. Let \u03c3 be the correct choice of delimiters (we drop the \u22c6superscript for ease of reading). Our goal will be to show that, if we construct the ICL sequence z by sampling i.i.d. x1, . . . , xn \u223cD, computing y1, . . . , yn by applying yi = f(xi), and assembling these example/label pairs into a sequence with \u03c3, then the model estimate p\u03c3(z) is very likely to be much larger than p\u03c3\u2032(z) for every alternative \u03c3\u2032 \u0338= \u03c3. What we analyze is the log ratio\n\ufffd \ufffd which we aim to show is very likely to be positive. We do this by converting the above to a martingale sequence. Let \u00b5i := Exi\u223cD \ufffd log U (\u03c3,i) \u03c3 U (\u03c3\u2032,i) \u03c3 | prefixi\u22121 \ufffd , and observe that \u00b5i > c according to Assumption 1. Now we have that the sequence \u03bej := \ufffdj i=1 \ufffd log U (\u03c3,i) \u03c3 U (\u03c3\u2032,i) \u03c3 \u2212\u00b5i \ufffd is a martingale. We note that, since we have a lower bound \u03f5 on the model probabilities, we have that log U (\u03c3,i) \u03c3 U (\u03c3\u2032,i) \u03c3 falls within the range [log(\u03bd), log(1/\u03bd)]. We can then apply Azuma\u2019s Inequality to see that \ufffd \ufffd \ufffd \ufffd\nSetting n as in the Theorem statement ensures that the right hand side above is smaller than \u03b4 |Vdelims|2 . Now if we take a union bound over all possible choices of \u03c3\u2032 \u0338= \u03c3 ensures that P(\u2203\u03c3\u2032 \u0338= \u03c3 : p\u03c3\u2032(z) > p\u03c3(z)) < \u03b4\n# C.1 Segmenting example and label delimiters\nWe show how to identify example and label delimiters using one head per combination of an example delimiter \u03b4e and label delimiter \u03b4l in Vdelims \u00d7 Vdelims. To simplify notation and avoid subscripts, we first focus on one such head for a fixed delimiter pair (\u03b4e, \u03b4l). We assume that the input to the transformer consists of the vector z1 i = \uf8eb \uf8ec \uf8ed \u02dcz1 i i 1 \u22121(zi = \u03b4e) 1 \u22121(zi = \u03b4l) \uf8f6 \uf8f7 \uf8f8, where \u02dcz1 i \u2208Rd is the (pre-trained) encoding of zi which we augment for convenience. First transformer layer: The goal of the first layer is to take the input at index i and map it to an output o1 i = \uf8eb \uf8ec \uf8ed z1 i i me(i) 1 \u22121(zi = \u03b4l) \uf8f6 \uf8f7 \uf8f8, where me(i) is the largest index j < i such that zj = \u03b4e. Let Q1 and K1 matrices in this head be such Q1(K1)\u22a4is 0 on the first d coordinates and 0 on the last coordinate. The remaining 2 \u00d7 2 coordinates are specified as sending \u27e8\u03b8, \u03b8\u2032\u27e9Q1(K1)\u22a4(d+1:d+2,d+1:d+2) = \u03b3(\u03b8\u2032(1)\u03b8(2) \u2212\u03b8(1)\u03b8\u2032(2)) for any \u03b8, \u03b8\u2032 \u2208R2 for \u03b3 \u2192\u221e. That is the attention weights act as a selector of the me(i) as \u27e8z1 i , z1 j \u27e9Q1(K1)\u22a4= \u03b3(j(1 \u22121(zi = \u03b4e)) \u2212i(1 \u22121(zj = \u03b4e))). Consequently the soft attention weights act like hard attention and we get that A1(zi, zj) = 1(j = me(i)). We further define the value tokens to be v1 j = j, so that o1 i = me(i). Using the skip connection, we further augment the input to the second layer as z2 i = \uf8eb \uf8ec \uf8ed z1 i i me(i) 1 \u22121(zi = \u03b4l) \uf8f6 \uf8f7 \uf8f8.\n\u27e8  \u27e9 \u2212 \u2212 \u2212 Consequently the soft attention weights act like hard attention and we get that A1(zi, zj) = 1(j = me(i)). We further define the value tokens to be v1 j = j, so that o1 i = me(i). Using the skip connection, we further augment the input to the second layer as z2 i = \uf8eb \uf8ec \uf8ed z1 i i me(i) 1 \u22121(zi = \u03b4l) \uf8f6 \uf8f7 \uf8f8.\n\nSecond transformer layer: The second transformer layer is constructed in a similar way as the first layer, however, it\u2019s goal is to extract ml(i). We are going to assume that each example token can also be treated as a label token (e.g. by adding the indicator of example delimiter to the indicator of label delimiter in the previous layer), so that in the case that there are no label tokens between two example tokens we have me(i) = ml(i). Further, for any xi it also holds that me(i) = ml(i) and for any yi it will hold that ml(i) > me(i). This will allow us to distinguish between xi tokens and yi tokens, which is important for computing the necessary probabilities for the proof of Theorem 5. We also extend the input z3 i to contain the following\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/24fd/24fdab29-ac48-4865-a91d-e2fd423536d8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">This is easily accomplished by the MLP at in the second layer, following the attention.</div>\nThird attention layer: In the third attention layer we are going to check for consistency of the positions on all separator tokens. Suppose that we have tokens z3 i and z3 i\u2032 s.t. i \u2264i\u2032. Then the following must be satisfied:\nThird attention layer: In the third attention layer we are going to check for consistency of the positions on all separator tokens. Suppose that we have tokens z3 i and z3 i\u2032 s.t. i \u2264i\u2032. Then the following must be satisfied:\n\u21d2 that is there can not be more than one label token between two example tokens. To check this consistency we use an attention head for ml(i) \u2264ml(i\u2032). The attention between zi, z\u2032 i is computed as \ufffd \ufffd\nfor \u03b3 \u2192\u221e, where n is the max sequence length. Note that by construction it holds that me(i\u2032) \u2265 me(i) so that this inner product is only \u221eif the following hold together ml(i\u2032) > ml(i), ml(i) > me(i) and me(i) = me(i\u2032). me(i) = me(i\u2032) implies that i and i\u2032 are part of the same example, ml(i) > me(i) implies that i is part of the answer sequence for that example and ml(i\u2032) > ml(i) implies that there is <lsep> between token zi\u2032 and token zi. The value vectors v3 i \u2208Rd+9 are set as v3 i = ied+9. The resulting output of the attention layer is now o3 i\u2032 = i\u2032ed+9 iff the condition in Equation 6 are met, otherwise o3 i\u2032 = ied+9 for some i where the condition is violated. Next, we describe how the MLP acts on o3 i\u2032 + z3 i\u2032 (obtained using skip connection) as follows\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55d8/55d87fa7-ea06-4dd3-bd62-4a5b2143a931.png\" style=\"width: 50%;\"></div>\nFourth transformer layer: The input to the third layer effectively gives a mapping from the current token to the proposed start of an example and most recent label under the delimiters being considered. Since our objective (2) evaluates candidate delimiters in terms of the probabilities of the segmented sequences under a pre-trained distribution p0, we need to map the tokens z4 i to the appropriate conditional probabilities related to the objective. In fact, we assume that the pre-training process provides us with a transformer which can do this mapping, as we state formally below.\n(6)\nAssumption 3 (Conditional probability transformer). There exists a transformer which takes as input a token sequence \u03b6i, . . . , \u03b6T , with each token \u03b6i = \ufffd z1 i , i, j, \u03b4err \ufffd for z1 \u2208Rd, i, j \u2208 [T], \u03b4err, \u03b4delim \u2208{0, 1}. It produces, ln pi, for each \u03b6i s.t. \u03b4err = 0, where \uf8f1\n\uf8f3 Further if \u03b4err = 1 it returns ln \u03bd.\nWe note that since the basic language models are trained to do next word prediction through log likelihood maximization, this is a very reasonable abstraction to assume from pre-training. As a result, we assume that the fourth layer produces z5 i = ln pi. Assumption 3 allows us to compute conditional probabilities of sequences according to their segmentation in the following way. Consider the sequence <begin>z1, z2, <lsep>, z3, z4<esep>, z5, <lsep><end>.\nWe note that since the basic language models are trained to do next word prediction through l likelihood maximization, this is a very reasonable abstraction to assume from pre-training. As result, we assume that the fourth layer produces z5 i = ln pi.\n\uf8ed \uf8f8 \uf8ed \uf8f8 \uf8ed \uf8f8 The transformer, respectively, computes\np1 = p0(z1), p2 = p0(z2|z1), p3 = 1, p4 = p0(z3), p5 = 1, p6 = p0(z5), p7 = 1.\nFifth transformer layer: Note that so far we have acquired the conditional probabilities of individual tokens, when conditioned on a prefix. Further, these conditional probabilities have the following properties. If i is a delimiter then pi = 1. If i is such that i \u22121 is a delimiter then pi = p0(zi), that is the marginal of zi is returned and finally pi = \u03bd for some small \u03bd > 0 if there is some inconsistency in the token segmentation. Note that this ensures that inconsistent token segmentations will have small probability. The fifth transformer layer just assigns uniform attention with the 0 matrix for QKT and use vj = \ufffd ln pj 1 \ufffd . This results in an output \ufffdi j=1(ln pj)/i at token i and z6 i = \ufffd\ufffdi j=1(ln pj)/i i \ufffd . Finally, we use two MLP layers to send z6 i \u2192z6 i (1) \u00d7 z6 i (2) = \ufffdi j=1(ln pj). Lemma 3. For any token pair \u03c3 the output at token k of the transformer at attention head associated with \u03c3 satisfies the factorization in Equation 4. The above lemma is a direct consequence of the construction of the transformer. We have already seen how the fourth layer acts on the sequence <begin>z1, z2, <lsep>, z3, z4<esep>, z5, <lsep><end>. The final layer of the transformer will now output for token 7 the sum 7 \ufffd 7\n\ufffd ln pi = ln \u03a07 i=1pi = ln (p0(z1) \u00d7 p0(z2|z1) \u00d7 p0(z3) \u00d7 p0(z4|z3) \u00d7 p0(z5))\nFinal layer to select across delimiters: Finally, the network implements the objective (2) for a particular delimiter in one head. By using the MLP to implement maximization across heads from the concatenated output values, we can identify the optimal delimiter.\n# D Learning the the 1-sparse tokenized regression task\nHere we give the construction of a transformer mechanism and sample complexity for the 1-sparse tokenized regression task, to build intuition for general the s-sparse case. We start with the mechanism before giving the sample complexity result.\nRecall that the 1-sparse tokenized regression task is defined by a vector in x \u2208Rm and the hypothesis class F consists of all basis vectors {ei}m i=1 in Rm. Each instance of the task is defined by fixing a vector ef \u2208{ei}m i=1. The labels for the task are yi = \u27e8xi, e\u22c6 f\u27e9for some unknown f \u22c6\u2208F. The i-th element of the sequence given to the transformer for the task is (xi,1, . . . , xi,m, yi), where xi,j denotes the j-th coordinate of the vector xi. We now describe how ICL can learn the above task, by using 1 head per layer and 4 attention layers. We begin by stating a useful lemma which will allow us to set attention weights between any two tokens zi, zj to 0 by only using positional embedding dependent transformations. Lemma 4. For any given token embeddings {\u00afzi}i\u2208[mn] \u2208Rd, there exist embeddings {z\u2113 i}i\u2208[mn] \u2208 Rd+mn which only depend on the positions i \u2208[mn] such that for any subsets S, S\u2032 \u2286[mn] (not necessarily disjoint) we have \u27e8z\u2113 i, z\u2113 j\u27e9Q\u2113 k(K\u2113 k)\u22a4= c1 + c2 \u27e8\u00afzi, \u00afzj\u27e9\u2208R \ufffd{\u221e, \u2212\u221e} for all i \u2208S, j \u2208S\u2032 and \ufffd z\u2113 j, z\u2113 j \ufffd = c2 \u27e8\u00afzi, \u00afzj\u27e9otherwise.\nProof. We embed z\u2113 i into Rd+mn in the following way\n\uf8f3 We now take Q\u2113 k \u2208R(d+mn)\u00d7(d+1) to contain the c2-scaled identity mapping in the first d columns. The d+1-st column is set to 0 in the first d rows and as Q\u2113 k(t, d+1) = 1(t\u2212d \u2208S) . Similarly we set the first d columns of K\u2113 k \u2208R(d+mn)\u00d7(d+1) to the identity and Kk(t, d+1) = sgn(c1)1(t\u2212d \u2208S\u2032) and 0 otherwise.\nRecall that Assumption 2 gives approximate orthogonality of the embeddings in the first layer of the transformer. We note that the assumption is not hard to satisfy, e.g., one can use the \"bucket\" construction described right after the assumption in Section 5. Other possible embeddings include a Random Fourier Feature approximation to a Gaussian kernel with appropriate bandwidth. In fact, since the lemma only requires only approximate orthogonality, we can further assign each bucket in the bucket embedding to a random Gaussian vector in O(\u03c4 2) dimensions and obtain the same result with dimension \u02dcO(\u03c4 2) with high probability. We now give the construction of a transformer for this learning task, assuming the availability of such an embedding satisfying Assumption 2 in Rd\u03f5 for some d\u03f5. Construction of the first transformer layer. We now specify the first transformer layer, by specifying the value vectors. For the query and key matrices we use Lemma 4 to argue that there exists a setting such that for any embedding in dimension din = d\u03f5 + mn satisfying Assumption 2 we have an embedding for which the attention weights can be computed as\nConstruction of the first transformer layer. We now specify the first transformer layer, by specifying the value vectors. For the query and key matrices we use Lemma 4 to argue that there exists a setting such that for any embedding in dimension din = d\u03f5 + mn satisfying Assumption 2 we have an embedding for which the attention weights can be computed as\nThat is the attention weights only depend on inner products between tokens within an example, and each xi,j token only attends to itself, while the yi attends to all the tokens in xi with different weights. Next, the value transformation is chosen as the map which sends any x1 i,j \u2208Rdin to vi,j \u2208Rdin+m, where the first din coordinates of vi,j are equal to x1 i,j and the remaining m coordinates are equal to the basis vector ej \u2208Rm. We will shortly see how ICL learns a hypothesis consistent with all examples but the main idea is that the consistent hypothesis will have the highest weight within the last m coordinates of the vector \ufffdn i=1 \ufffdm j=1 A1(yi, xi,j)v1 i,j, that is the sum of the value vectors associated with each of the answer tokens yi, i \u2208[n], after the attention layer has been applied.\n(7)\n(8)\nWe now argue that output from the yi tokens identifies all positions j such that |xi,j \u2212yi| \u2264\u03f5. Lemma 5 (Restatement of Lemma 1). Given an example i, let Ji = {j : |xi,j \u2212yi| \u2264\u03f5}, and let f \u22c6\u2208[m] be such that yi = xi,f \u22c6. Under Assumption 2, for any m \u22652, the output of the first layer satisfies that for any i \u2208[n]:\nProof. By Equation 9, we know that under Assumption 2, for any sample i and index j /\u2208Ji, A1(y1 i , x1 i,j) \u2264e1/(2\u03c4)/Zi \u2264e1/2/Zi, where Zi is denominator in Equation 8. On the other hand, A1(yi, yi) = A1(yi, xi,f \u22c6) = e/Z, under Assumption 2. Hence, we see that\nwhere the second inequality uses m \u22652. Since Zi \u2264e(m + 1) under our definition of the attention weights, this yields the lemma.\nIn words, the xi,j tokens only attend to themselves again, while yi attends uniformly to all the previous labels, including itself. This construction implies\nWe assume that the following MLP acts as the identity mapping on the first d coordinates of the value vectors and then sends the remaining m coordinates to the basis vector corresponding to the index with highest value. That is, MLP2o2 i,m+1 = (v efi), with v \u2208Rdin equal to the first din coordinates of o2 i,m+1, and fi = argmaxj\u2208[m] o2 i,m+1(j). Ties are broken arbitrarily but consistently. In particular, together with the construction of the attention weights at the previous layer we have\nx3 i,j = \ufffd x1 i,j ej \ufffd , j \u2208[m], y3 1,i(d + 1 : d + m) = efi \u2208Rm, fi = argmax j\u2208[m] 1 i i \ufffd t=1 o1 t,m+1(j).\nIn the next section, we show that this aggregation across examples followed by the maximization selects some coordinate such that |xi,j \u2212yi| \u2264\u03f5 for all examples i in the context, and that any such hypothesis has a small prediction error on future examples in this task. That is, the first two layers identify an approximately correct hypothesis for the task.\n(9)\n(11)\n(12)\nInference with learned hypothesis. Finally we explain how to apply the returned hypothesis fi to the next example. This will also describe how to do inference with the hypothesis fn on the n + 1-st example. The attention pattern required here is a bit different in that each xi,j only attends to the previous label yi and itself. Q3 1(K3 1)\u22a4only acts on coordinates [d + 1 : d + m] as the identity and sends everything else to 0. In particular, the unnormalized attention weights are\nThis results in attention values\n\ufffd The remaining attention outputs are not used and hence not specified here. The value vectors in R2 are set as\n\ufffd The remaining attention outputs are not used and hence not specified here. The value vectors in are set as \ufffd \ufffd \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd Notice that this requires access to the raw input token, which can be done by either providing a skip connection from the inputs, or by carrying the input token as part of the embedding through all the layers at the cost of one extra embedding dimension. As a result, for the index fi selected at the end of example i, we have that\n# D.2 Sample complexity\nLet the target hypothesis be f \u22c6, that is we assume, yi = f \u22c6(xi), \u2200i \u2208[n]. We are going to analyze the error of the hypothesis returned by ICL after m examples. From Lemma 1, we know that the true hypothesis f \u22c6has a large value in the output of the first layer, in the coordinate d + f \u22c6, at each example i. Suppose our construction identifies the hypothesis to make the prediction with, after seeing i examples. Then if fi makes an incorrect prediction (that is |yi\u2032 \u2212fi(xi\u2032)| \u2265\u03f5 for some i\u2032 \u2264i) on even one of these i examples, the output in coordinate d + fi is guaranteed to be smaller than in d + f \u22c6by Lemma 1. Consequently, the hypothesis fn returned after n examples is guaranteed to have an error at most \u03f5 on each of the n examples in context. We now show that this implies a risk bound on the hypothesis fn. Lemma 6. Let pn = P(|fn(x) \u2212f \u22c6(x)| \u2264\u03f5) be the probability of the returned hypothesis deviating from f \u22c6by more than \u03f5 on any example. Then with probability 1\u2212\u03b4 it holds that pn \u22651\u221220 log(m/\u03b4) 3n . Proof. Fix any hypothesis f. Let Xi denote the Bernoulli random variable indicating the event that |f(xi) \u2212f \u22c6(xi)| \u2264\u03f5 and pf = P(|f(x) \u2212f \u22c6(x)| \u2264\u03f5). We compute the probability that this hypothesis is potentially returned by the transformer which is equivalent to the event that \ufffdn i=1 Xi \u2265n. P( n \ufffd i=1 Xi \u2265n) = P \ufffd n \ufffd i=1 Xi \u2265npf + 2 \ufffd npf(1 \u2212pf) log(1/\u03b4) + 4 3 log(1/\u03b4) + n(1 \u2212pf) \u22122 \ufffd npf(1 \u2212pf) log(1/\u03b4) \u22124 3 log(1/\u03b4) \ufffd \u2264\u03b4,\nLet the target hypothesis be f \u22c6, that is we assume, yi = f \u22c6(xi), \u2200i \u2208[n]. We are going to analyze the error of the hypothesis returned by ICL after m examples. From Lemma 1, we know that the true hypothesis f \u22c6has a large value in the output of the first layer, in the coordinate d + f \u22c6, at each example i. Suppose our construction identifies the hypothesis to make the prediction with, after seeing i examples. Then if fi makes an incorrect prediction (that is |yi\u2032 \u2212fi(xi\u2032)| \u2265\u03f5 for some i\u2032 \u2264i) on even one of these i examples, the output in coordinate d + fi is guaranteed to be smaller than in d + f \u22c6by Lemma 1. Consequently, the hypothesis fn returned after n examples is guaranteed to have an error at most \u03f5 on each of the n examples in context. We now show that this implies a risk bound on the hypothesis fn. Lemma 6. Let pn = P(|fn(x) \u2212f \u22c6(x)| \u2264\u03f5) be the probability of the returned hypothesis deviating from f \u22c6by more than \u03f5 on any example. Then with probability 1\u2212\u03b4 it holds that pn \u22651\u221220 log(m/\u03b4) 3n . Proof. Fix any hypothesis f. Let Xi denote the Bernoulli random variable indicating the event that |f(xi) \u2212f \u22c6(xi)| \u2264\u03f5 and pf = P(|f(x) \u2212f \u22c6(x)| \u2264\u03f5). We compute the probability that this hypothesis is potentially returned by the transformer which is equivalent to the event that\n(13)\nFinally, we note that pf \u22641 \u221220 log(1/\u03b4) 3n implies n(1\u2212pf ) 2 \u221210 3n log(1/\u03b4) \u22650. Taking a union bound over all possible f and applying with f = fn, so that pfn = pn completes the proof.\nProof. We use the embedding into \u23081/\u03f5\u2309buckets, as mentioned in the previous section together with the construction of the transformer to satisfy the conditions of Lemma 6. Conditioning on the good event, A, in Lemma 6 implies that P(|fn(x) \u2212f \u22c6(x)| > \u03f5|A) \u2264\u03f5 and so under A, we have E |fn(x) \u2212f \u22c6(x)| \u2264\u03f5pn + 1 \u2212pn \u22642\u03f5,\nwhere the second inequality follows from our condition on n.\n# E s-sparse Tokenized Regression\nIn this section we study the general s-sparse case defined in Definition 3. Recall that the hypothesis class now consists of f = (j1, . . . , js) \u2208[m]s, that is each hypothesis selects s out of the m coordinates of x. We begin by making the following simple observation under Assumption 2: if j \u2208f \u22c6then for any i it holds that \u27e8x1 i,j, y1 i \u27e9\u22653/4, while if j is not part of a consistent policy then we have \u27e8x1 i,j, y1 i \u27e9\u22641/4. Lemma 7. Under Assumption 2 with \u03c4 \u22652s we have that for any C \u2286f \u22c6and, j \u2208f \u22c6\\ C, we have \u27e8x1 i,j, y1 i \u2212\ufffd j\u2032\u2208C x1 i,j\u2032\u27e9\u2265 3 4, while if |xi,j \u2212xi,j\u2032| \u2265\u03f5 for all j\u2032 \u2208f \u22c6then we have \u27e8x1 i,j, y1 i \u2212\ufffd j\u2032\u2208C x1 i,j\u2032\u27e9\u22641 4.\n\ufffd Proof. Since y1 i = \ufffd j\u2208f \u22c6x1 i,j, for any j \u2208f \u22c6\\ C, we have\nwhere the first inequality follows from Assumption 2, since any token which does not have an inner product of 1 with xi,j has the inner product at least \u22121/(2\u03c4). The last equality follows from the precondition \u03c4 \u22652s in the lemma. On the other hand, for any token j which is not \u03f5-close to any token in f \u22c6, the inner product is at most s/(2\u03c4) by a similar argument, which completes the proof. We proceed to give a construction which will use O(m) layers with one head per layer. The idea behind the construction is to learn each coordinate of a single consistent hypothesis in F. We note that it is not possible to directly take the approach in the index token task to learn each coordinate in f \u22c6independently now, unless there is a unique consistent hypothesis with high probability. As described in Section 5, we follow an iterative deflation approach to avoid this issue. Suppose that at layer \u2113we have learned a set of coordinates of a consistent hypothesis. Denote the subset of the learned coordinates which are equal to 1 as C\u2113 i . The embedding for y\u2113 i \u2208Rd+m then consists of y1 i in the first d coordinates, and the following holds for the remaining m coordinates. If coordinate j \u2208C\u2113 i , then y\u2113 i(j) = 0, otherwise y\u2113 i(j) = \u2212\u221e. The embedding of x\u2113 i,j \u2208Rd+m is as follows. The first d coordinates are again equal to x1 i,j, the remaining m coordinates equal the coordinates of ej. The value vectors are set to v\u2113 i,j = \ufffd \u2212x1 i,j \u22121 \ufffd for j \u2264m and v\u2113 i,m+1 = \ufffd y1 i 1 \ufffd . The\n\nquery and key matrices are set to act as the 0 matrix on the first d coordinates and as the identity on the remaining m coordinates, except for the token associated with yi, so that \u27e8y\u2113 i, y\u2113 i\u27e9Q\u2113(K\u2113)\u22a4= 0 We have the following. Lemma 8. There exists a setting for the query, key and value matrices at layer \u2113so that given the embeddings y\u2113 i and x\u2113 i,j, i \u2208[n], j \u2208[m] it holds that \ufffd \ufffd\nProof. To show the claim of the lemma we only need to compute the attention weights from the \u2113-th attention layer. First, using Lemma 4 we can set A\u2113(x\u2113 i,j, x\u2113 i,j\u2032) = \u22121(j = j\u2032), which, together with the value vector choice, shows that o\u2113 i,j = \u2212x1 i,j. If j \u0338\u2208C\u2113 i then the construction implies\n\u27e8 \u27e9 \u2212\u221e ition embedding of yi we use Lemma 4 to set\n\u27e8 \u27e9 \u2212\u221e Further, using the position embedding of yi we use Lemma 4 to set\nFinally, we want to ensure uniform weights for all consistent examples in C\u2113 i and so we enforce \u27e8y1 i , x1 i,j\u27e9Q\u2113(K\u2113)\u22a4= 0 as described in the construction. Thus for any j \u2208C\u2113 i we have\n\ufffd   For j \u0338\u2208C\u2113 i we have \u27e8y\u2113 i, x\u2113 i,j\u27e9= \u2212\u221eand this implies A\u2113(y\u2113 i, x\u2113 i,j) = 0, which completes the cl of the lemma.\nLemma 8 shows that we can \"deflate\" y1 i by subtracting all consistent coordinates which have been identified so far. Next, we are going to use the construction for the 1-sparse task on i-th example y\u2113+1 i = y1 i \u2212\ufffd j\u2208C\u2113 i x1 i,j and x\u2113+1 i,j = x1 i,j. We make a slight modification to the outputs of the \u2113-th attention layer by setting \ufffd \ufffd\n\ufffd \ufffd This can be achieved using the skip-connection and appropriate padding of oi,m+1. However, to simplify the argument we avoid describing this operation. We assume that the MLP layer after the \u2113-th attention layer acts on o\u2113 i,j in the following way, it sends o\u2113 i,j \u2192 1 o\u2113 i,j(d+1)o\u2113 i,j. Further, it acts on the coordinates corresponding to y\u2113 i(d + 1 : m) by sending \u2212\u221eto 0 and 0 to \u2212\u221e. This can be done by first adding 1 to all coordinates, then using a relu to clip all remaining \u2212\u221eto 0, and finally multiply the remaining positive coordinates by \u2212\u221eagain. This operation is needed to take the complement of C\u2113 i so that all consistent coordinates which have already been added to C\u2113 i can be removed from consideration. We note that both these operations actually need a 2-layer MLP, however, for simplicity we assume that these are implementable by the MLP layer following the attention layer. We now describe the inputs x\u2113+1 i,j and y\u2113+1 i to the \u2113+ 1-st transformer layer:\ny\u2113+1 i (d + 1 : m)(j) = \u2212\u221e1(j \u2208C\u2113 i ).\n(14)\nThe above implies for all j \u2208C\u2113 i we have \u27e8x\u2113+1 i,j , y\u2113+1 i \u27e9= \u2212\u221eand otherwise \u27e8x\u2113+1 i,j , y\u2113+1 i \u27e9= \u27e8x1 i,j, y1 i \u2212\ufffd j\u2208C\u2113 i x1 i,j\u27e9. Finding a consistent coordinate is now equivalent to recovering a consistent hypothesis for the 1-sparse task, which we know how to do using exactly two attention layers as described previously. Lemma 9. Applying the first two layers of the 1-sparse task from Section D.1 to x\u2113+1 i,j , j \u2208[m], y\u2113+1 i as defined in Equation 14 yields: \ufffd \ufffd\nThis condition is directly implied by Lemma 7. Using Lemma 1 we have that for every j selected by some consistent hypothesis A\u2113+1(y\u2113+1 t , x\u2113+1 t,j ) will exceed A\u2113+1(y\u2113+1 t , x\u2113+1 t,j\u2032 ), where j\u2032 is not selected by any consistent hypothesis. This implies that the maximum coordinate among [d + 1, m] of o\u2113+3 i,m+1 will be included in a consistent hypothesis for all t \u2264i examples. This implies that applying the second MLP layer from the 1-sparse task will write a consistent coordinate in y\u2113+4 i (d + 1 : m). Further, Lemma 9 implies that this consistent coordinate will not be part of the already fixed coordinates in C\u2113 i . Let this new consistent coordinate be j\u2113. We would like to add j\u2113to C\u2113 i . This is done as follows. First we assume that the MLP sets y\u2113+4 i (d+1 : m) = \u2212ej\u2113. Next, we assume access to a skip connection from layer \u2113+1 so that we can add y\u2113+4 i (d+1 : m)+y\u2113+1 i (d+1 : m). To transform y\u2113+4 i (d+1 : m)+y\u2113+1 i (d+1 : m) to a similar construction used with y\u2113 i we first add 1/2 to every coordinate of y\u2113+4 i (d + 1 : m) + y\u2113+1 i (d + 1 : m). Next, we use another relu activation on each coordinate. The resulting vector already satisfies that every coordinate j \u2208C\u2113+4 i is equal to 0, and every coordinate outside of the set is 1 2. It remains to multiply the resulting vector by \u2212\u221eand add y1 i to the first d coordinates using a skip connection. All of the above can be done using one additional attention layer, together with an MLP. Since skip connections in the original transformer architecture are only in between consecutive attention layers, we can implement the above by extending the embedding of each y\u2113+1 i , . . . , y\u2113+4 i to have an additional d + m coordinates in which to store y1 i together with the representation of C\u2113 i . Applying the learned hypothesis. The above construction implies that after L = O(s) layers the resulting yL i (d + 1 : m) will contain exactly a set C\u2113 i of cardinality s which contains only consistent coordinates. Further, using the deflation construction, we can show the following. Lemma 10. After L = O(s) layers it holds that |CL i | = s and further, there exists a bijection bi from f \u22c6to CL i such that for any j \u2208f \u22c6, |xt,j \u2212xt,bi(j)| \u2264\u03f5, t \u2264i. The output yL i \u2208Rd+m is such that yL i (CL i ) = 0 and yL i ([m] \\ CL i ) = \u2212\u221e. Proof. For the first part of the lemma we begin by showing that for any j\u2032 \u2208CL i , there exists a j \u2208f \u22c6 such that |xi,j \u2212xi,j\u2032| < \u03f5. Suppose that this does not hold true, i.e., there is some j\u2032 such that for all j \u2208f \u22c6, for which |xt,j \u2212xt,j\u2032| \u2265\u03f5 for some t \u2264i. Lemma 7 implies that \u27e8x1 t,j\u2032, y1 t \u27e9\u22641 4. On the other hand if j\u2032 \u2208CL i then the construction implies that at some layer \u2113\u2032 \u2264L it must have been the case that \u27e8x1 t,j\u2032, y1 t \u2212\ufffd j\u2208C\u2113\u2032 t x1 t,j\u27e9\u22653/4 for all t, otherwise j\u2032 can not be added to C\u2113\u2032 i as it is not consistent with f \u22c6on some round t and so it would not be part of CL i as C\u2113\u2032 i \u2286CL i . This is now a contradiction as it implies\nwhere the second inequality follows from Assumption 2 as \u27e8x1 i,j\u2032, x1 i,j\u27e9> \u22121 2s. This shows that we can never add a coordinate which is not similar to some coordinate in f \u22c6across all the examples till i. We show that the map is injective as follows. Let j\u21130 some coordinate for which we have already established the mapping j\u21130 \u2192j \u2208f \u22c6at layer \u21130. Consider another candidate j\u21131, for \u21131 > \u21130 such that |xt,j\u21131 \u2212xt,j| \u2264\u03f5, that is j\u21131 can potentially be mapped to j as well on round t. We consider two cases, first for j\u2032 \u2208f \u22c6s.t. j\u2032 \u0338= j we have |xt,j\u21131 \u2212xt,j\u2032| > \u03f5 or j\u2032 \u2208C\u21131\u22121 t already. In this case we show that xt,j\u21131 is nearly orthogonal to y1 t \u2212\ufffd j\u2208C\u21131\u22121 t x1 t,j so that xt,j\u21131 can not be added at any layer after xt,j\u2032 has been added:\nwhere the last inequality follows as before together with the assumption |xt,j\u21131 \u2212xt,j\u2032| > \u03f5 outside of C\u21131\u22121 t . Next, if there exists some j\u2032 \u2208f \u22c6, j\u2032 \u0338\u2208C\u21131\u22121 t such that |xt,j\u21131 \u2212xt,j\u2032| < \u03f5 we can map j\u21131 \u2192j\u2032 and add j\u2032 to C\u21131 t as long as the consistency property holds for all t\u2032 \u2264t. Otherwise, there exists a round t where |xt\u2032,j\u21131 \u2212xt\u2032,j\u2032| > \u03f5, \u2200j\u2032 \u2208C\u21131\u22121 t and the argument above can be repeated. Further, we note that the construction can add at least every j \u2208f \u22c6to CL i as the following is always satisfied:\n\ufffd unless S contains some coordinate j\u2032 such that |xi,j \u2212xi,j\u2032| \u2264\u03f5 for all i. That is, every j \u2208f \u22c6 is mapped to at least one coordinate in CL i . Taken together, each j \u2208CL i is mapped to exactly one element of f \u22c6and each element of f \u22c6is mapped to some element of CL i . This establishes the claim for the bijection. The second claim of the lemma follows just from the construction of the transformer.\nTo use the returned y\u2113 i guaranteed by Lemma 10 for inference we first modify it in the following way. We add the vector consisting of all 1s and then apply a relu on each coordinate. The resulting vector now contains a consistent hypothesis in the y\u2113 i(d + 1 : m). To apply the hypothesis we simply use the construction of the final three layers from the index token task.\n# E.1 Proof of Theorem 6\nWe treat f \u22c6and fn as two subsets of [m] with cardinality s. Lemma 10 implies that for every example i \u2208[n], there is a bijection bn between fn and f \u22c6which maps any j \u2208f \u22c6to a j\u2032 \u2208fn such that |xi,j \u2212xi,j\u2032| \u2264\u03f5, i \u2208[n]. The same argument as in Lemma 6 shows the following. Lemma 11. For any x \u2208Rm, j \u2208f \u22c6let pn,j = P(|xj \u2212xbn(j\u2032)| \u2264\u03f5). Then with probability 1 \u2212\u03b4 it holds that pn \u22651 \u221220s log(m/\u03b4) 3n . Using the above lemma we can show the equivalent to the sample complexity bound for the index token task.\nProof of Theorem 6. The same argument as in Theorem 7 can be used to show that for the bijectio guaranteed by Lemma 10 and the setting of n we have E[|xj \u2212xbn(xj)|] \u22642\u03f5, \u2200j \u2208f \u22c6. This implie the result of the theorem as \ufffd \ufffd\nE[|fn(x) \u2212f \u22c6(x)|] = E[| \ufffd j\u2208f \u22c6 xbn(j) \u2212xj|] \u2264 \ufffd j\u2208f \u22c6 E[|xbn(j) \u2212xj|] \u22642s\u03f5.\nRedefining \u03f5 \u2192\u03f5/s completes the proof.\n# F Vector 1-sparse regression task\nWe now quickly discuss how to solve the vector version of the 1-sparse regression task, where the transformer\u2019s input is a sequence of examples (xi, yi)i\u2208[n], however, now xi \u2208Rm is a single\n\ntoken, rather than being split into m tokens. The idea is to learn each bit of a consistent hypothesis sequentially using a total of O(log(m)) attention layers. To do so we focus on recovering learning a consistent hypothesis for example i as done in the first attention layer in the 1-sparse token task. The remainder of the construction follows the ideas from the 1-sparse token task. First attention layer. Unlike in the 1-sparse tokenized regression task, we can not represent a single hypothesis by the respective token (even though it does still correspond to a coordinate in x). Instead we assume that the value vector v1 i,1, for xi, in the first layer, contains 0 in its first m coordinates and the following vector \u03b21 i,1 \u2208Rm in the next m coordinates\nThe value vector v1 i,2 for yi is constructed similarly, with the first m coordinates equal to 0 again and the second m coordinates equaling \u03b21 i,2 \u2208Rm which is the complement of \u03b21 i,1 in {0, 1}m. The embeddings in the first layer are as follows. x1 i \u2208R(d+1)m contains the embedding of x1 i,j from Assumption 2 in coordinates x1 i,1(d(j \u22121) + 1 : dj). The remaining m coordinated are all set to 1. y1 i \u2208R(d+1)m is constructed similarly, where the first dm coordinates contain the embedding of yi from Assumption 2, repeated d times. The last m coordinates equal the last m coordinates of v1 i,2, that is y1 i (dm + 1 : (d + 1)m) = v1 i,2(m + 1 : 2m). The query and key matrices Q1, K1 now implement the following linear operation:\nThis is implemented in the following way, the query matrix Q is a diagonal matrix with Q(d(j\u22121)+1 : dj) = \u03b2",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) exhibited by large language models, where they can adapt to a new learning task given a handful of labeled examples without explicit parameter optimization. ICL is surprising because it allows models to learn from prompts consisting of unrelated inputs and outputs, which are not typically present in training data.",
        "problem": {
            "definition": "The problem is to understand the mechanisms that enable large language models to perform in-context learning effectively, particularly in segmenting prompts and inferring consistent hypotheses from them.",
            "key obstacle": "The main challenge is the segmentation of prompts into independent examples and labels, as the examples are often presented in a format that does not align with the model's training data."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that large language models can learn from a few examples through prompting, despite the apparent disconnect between pre-training tasks and downstream tasks.",
            "opinion": "The proposed mechanism leverages the transformer architecture to segment prompts and infer hypotheses, enhancing the model's ability to learn from context.",
            "innovation": "The main innovation is the introduction of a segmentation mechanism that maximizes the likelihood of a proposed segmentation, utilizing the attention architecture to learn from fewer examples."
        },
        "Theory": {
            "perspective": "The theoretical perspective is that transformers can implement specific learning mechanisms that allow for effective segmentation and hypothesis inference in ICL tasks.",
            "opinion": "The paper posits that the self-attention mechanism in transformers is particularly well-suited to addressing the challenges of ICL, such as segmentation and hypothesis learning.",
            "proof": "The paper provides proofs demonstrating that the proposed mechanisms can be implemented using the transformer architecture, along with sample complexity guarantees for the learning framework."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using a transformer model with 8 layers, 1 head per layer, and an embedding size of 128. Two datasets were used: one sampled from a standard Gaussian distribution and another from a uniform distribution over {+1, -1}.",
            "evaluation method": "The evaluation involved generating sequences of examples and measuring the model's performance in predicting outputs based on the provided context."
        },
        "conclusion": "The paper concludes that transformers can implement effective mechanisms for ICL, achieving sample-efficient learning with a small number of examples. The empirical results validate the theoretical findings, showing that attention maps correspond to the key steps in the proposed mechanisms.",
        "discussion": {
            "advantage": "The advantages include the ability of transformers to learn from minimal examples, the robustness of the proposed segmentation mechanism, and the theoretical guarantees provided for sample efficiency.",
            "limitation": "A limitation is that the method relies on the proper selection of delimiters, which may not always be achievable in practice.",
            "future work": "Future work could explore improving the robustness of the segmentation mechanism and investigating the applicability of these methods to a broader range of tasks."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) allows large language models to adapt to new tasks with a few labeled examples without explicit parameter optimization."
        },
        {
            "section number": "1.3",
            "key information": "The self-attention mechanism in transformers is particularly well-suited to addressing challenges in ICL, such as segmentation and hypothesis learning."
        },
        {
            "section number": "2",
            "key information": "The problem is to understand the mechanisms enabling large language models to perform in-context learning effectively, particularly in segmenting prompts and inferring consistent hypotheses."
        },
        {
            "section number": "3.2",
            "key information": "Theoretical perspective suggests that transformers can implement specific learning mechanisms for effective segmentation and hypothesis inference in ICL tasks."
        },
        {
            "section number": "3.3",
            "key information": "The main innovation is the introduction of a segmentation mechanism that maximizes the likelihood of a proposed segmentation, utilizing the attention architecture to learn from fewer examples."
        },
        {
            "section number": "6.1",
            "key information": "A limitation is that the method relies on the proper selection of delimiters, which may not always be achievable in practice."
        },
        {
            "section number": "7",
            "key information": "The empirical results validate the theoretical findings, showing that attention maps correspond to the key steps in the proposed mechanisms."
        }
    ],
    "similarity_score": 0.7058752892639902,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks.json"
}