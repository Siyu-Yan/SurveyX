{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.11751",
    "title": "Asymptotic theory of in-context learning by linear attention",
    "abstract": " Abstract\nAbstract\nTransformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers\u2019 success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model\u2019s behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.\n# 1 Introduction\nSince their introduction by Vaswani et al. in 2017 [1], Transformers have become a cornerstone of modern artificial intelligence (AI). Originally designed for sequence modeling tasks, such as language modeling and machine translation, Transformers achieve state-of-the art performance across many domains, even those that are not inherently sequential [2]. Most strikingly, they underpin the breakthroughs achieved by large language models such as BERT [3], LLaMA [4], and the GPT series [5\u20138].\n\u2217To whom correspondence should be addressed. E-mail: (Y.M.L.) yuelu@sea",
    "bib_name": "lu2024asymptotictheoryincontextlearning",
    "md_text": "# Asymptotic theory of in-context learning by linear attention\nYue M. Lu\u2217a, Mary I. Leteya, Jacob A. Zavatone-Veth\u2020a,b,c, Anindita Maiti\u2020d, and Cengiz Pehlevan*a,c,e\naThe John A. Paulson School of Engineering and Applied Sciences, Harvard University bDepartment of Physics, Harvard University cCenter for Brain Science, Harvard University dPerimeter Institute for Theoretical Physics The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University\n# May 19, 2024\n# Abstract\nAbstract\nTransformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers\u2019 success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model\u2019s behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures.\n# 1 Introduction\nSince their introduction by Vaswani et al. in 2017 [1], Transformers have become a cornerstone of modern artificial intelligence (AI). Originally designed for sequence modeling tasks, such as language modeling and machine translation, Transformers achieve state-of-the art performance across many domains, even those that are not inherently sequential [2]. Most strikingly, they underpin the breakthroughs achieved by large language models such as BERT [3], LLaMA [4], and the GPT series [5\u20138].\n\u2217To whom correspondence should be addressed. E-mail: (Y.M.L.) yuelu@seas.harvard.edu and (C.P.) cpehle van@seas.harvard.edu \u2020J.A.Z-V.(Author Three) contributed equally to this work with A.M. (Author Four)\nThe technological advancements enabled by Transformers have inspired a substantial body of research aimed at understanding their working principles. One key observation is that language models gain new behaviors and skills as their number of parameters and the size of their training datasets grow [7, 9\u201311]. A particularly important emergent skill is in-context learning (ICL), which describes the model\u2019s ability to learn and execute tasks based on the context provided within the input itself, without the need for explicit prior training on those specific tasks. To give an example from natural language processing, a pretrained large language model might be able to successfully translate English to Italian after being prompted with a few example translations, even if it has not been specifically pretrained on that translation task [7]. ICL enables language models to perform new, specialized tasks without retraining, which is arguably a key reason for their general-purpose abilities. Despite many recent studies on understanding ICL, important questions about how and when ICL emerges in large language models are still mostly open. Large language models are trained (or pretrained) with a next token prediction objective. How do the different algorithmic and hyperparameter choices that go into the pretraining procedure affect ICL performance? What algorithms do Transformers implement for ICL? How many pretraining examples are required for ICL to emerge? How many examples should be provided within the input for the model to be able to solve an in-context task? How diverse should the tasks in the training dataset be for in-context learning of truly new tasks not observed in the training dataset? In this paper, we address these questions by investigating the ICL capabilities of a linear attention module for linear regression tasks. This model setting allows us to derive an asymptotically precise theory of the learning curve. In the remainder of this section, we first provide a comprehensive overview of related works on ICL. Following this, we summarize our main contributions.\n# 1.1 Related Works\nICL in Transformer architectures. The striking ICL abilities of Transformers were thrust to the fore by Brown et al. [7]\u2019s work on GPT-3. Focusing on natural language processing (NLP) tasks, they showed that ICL performance dramatically improves with an increase in the number of model parameters, with an increase in the number of examples in the model\u2019s context, and with the addition of a natural language task description. In subsequent work, Wei et al. [11] proposed that the emergence of ICL with increasing scale is an abrupt, unpredictable transition. This perspective has substantially influenced proposed accounts for the emergence of ICL [12]. However, Schaeffer et al. [13] have disputed the idea that the emergence of ICL is unpredictable; they suggest that appropriately-chosen measures of otherwise hidden progress [14] reveal that ICL gradually develops with scale.\nEmpirical studies of synthetic ICL tasks. Though ICL in NLP is both impressive and useful, these natural data do not allow precise experimental control. Towards a fine-grained understanding of the conditions required for ICL, many recent works have explored ICL of parametricallycontrollable synthetic tasks, notably linear regression and classification. These works have identified various features of pretraining data distributions that contribute to the emergence of ICL [15\u201319]. Closely related to our work is a study of ICL of linear regression by Ravent\u00f3s et al. [18]. Their work identified a task diversity threshold for the emergence of ICL, below which a pretrained Transformer behaves as a Bayesian estimator with prior determined by the limited set of pretraining tasks. Above this threshold, the model\u2019s performance matches that of within-context ridge regression, corresponding to a Gaussian prior over all tasks, including those not seen during pretraining. This work underscores the roles of task diversity, regularization, model capacity, and data structure\nin the emergence of ICL; a motivating objective of our work is to provide a theoretical account  their results.\nTheoretical studies of ICL. Many theoretical studies of ICL have centered on the idea that Transformers learn a particular algorithm during pretraining, which is then flexibly deployed to solve in-context tasks. In broad strokes, papers from this program of research often consider a particular algorithm for solving an in-context task, prove that Transformers can approximately implement this algorithm, and then empirically compare the ICL performance of a pre-trained Transformer to the performance of that algorithm [20\u201327]. A clear consensus on which algorithm underlies ICL of linear regression in full transformers has yet to emerge [20\u201327]. Within this line of research, closest to our work are a series of papers that consider ICL of linear regression by simplified Transformers using linear, rather than softmax, attention modules [23, 25\u201330]. Zhang et al. [27] study these models in the limit of infinite pretraining dataset size (i.e., the population risk limit), and show that their performance on in-context linear regression nearly matches that of the Bayes-optimal estimator for the ICL task. However, they found that linear Transformers are not robust to shifts in the within-context covariate distribution. Zhang et al. [26] then showed that any optimizer of the within-context risk for a linear Transformer solves the ICL task with an approximation to one step of gradient descent from a learnable initialization, and that the resulting estimator can saturate the Bayes error for tasks with a Gaussian prior and non-zero mean. As we will discuss in Section 2, our reduction of the linear attention module is inspired in part by these works. In very recent work, Duraisamy [30] has studied the fine-sample risk of in-context linear regression with a single step of gradient descent, without directly analyzing Transformers. Ahn et al. [23] and Wu et al. [28] investigated how linear Transformers adapt to limited pretraining data and context length, again showing that in certain cases nearly-optimal error is achievable. Like these studies, our work considers linear attention, but our analysis, with its asymptotically sharp predictions of the ICL performance, allows us to pinpoint when and how the transition from memorization to ICL of linear regression occurs. We thus in closing highlight work by Reddy [19] on in-context classification, who analyzed the transition to ICL using a phenomenological model.\n# 1.2 Summary of contributions\nWe now summarize the primary contributions of our paper, relative to the prior art reviewed above. Following the recent literature, we focus on a simplified model of a Transformer that captures its key architectural motif: the linear self-attention module [23, 25\u201329]. Linear attention includes the quadratic interaction between inputs that lies at the heart of softmax attention, but does not include the normalization steps or fully-connected layers. This simplification makes the model more amenable for theoretical analysis. Our main result is a sharp asymptotic analysis of ICL of linear regression by linear attention, resulting in a more precisely predictive theory than previous population risk analyses or finite-sample bounds [26, 27]. The main contributions of our paper are structured as follows:\n1. We begin in \u00a72 by developing a simplified parameterization of linear self-attention that allow pretraining on the ICL linear regression task to be performed using ridge regression.\n2. Within this simplified model, it is easy to identify the non-trivial scaling limit in which performance should be analyzed (\u00a73): as the token dimension tends to infinity, the number of pretraining examples should scale quadratically (with the token dimension), while the context length and pretraining task diversity should scale linearly. In this joint limit, we can compute sharp asymptotics for ICL performance using random matrix theory.\n2. Within this simplified model, it is easy to identify the non-trivial scaling limit in which performance should be analyzed (\u00a73): as the token dimension tends to infinity, the number of pretraining examples should scale quadratically (with the token dimension), while the context length and pretraining task diversity should scale linearly. In this joint limit, we can compute sharp asymptotics for ICL performance using random matrix theory.\n3. Our theoretical results reveal several interesting phenomena (\u00a73). First, we observe doubledescent in the model\u2019s ICL generalization performance as a function of pretraining dataset size, reflecting our assumption that it is pretrained to interpolation. Second, we uncover a transition to in-context learning as the pretraining task diversity increases. Concretely, there is a threshold task diversity above which linear attention saturates the Bayes error for the ICL regression task. Below that threshold, the model tends to memorize the limited set of pretraining tasks, and its excess risk is substantial. This transition recapitulates the empirical findings of Ravent\u00f3s et al. [18] in full Transformer models. 4. In \u00a74, we show through numerical experiments that the insights from our theory derived on a simplified model transfer to full Transformer models with softmax self-attention. In particular, the scaling of pretraining sample complexity and task diversity with token dimension required for successful ICL is consistent.\n3. Our theoretical results reveal several interesting phenomena (\u00a73). First, we observe doubledescent in the model\u2019s ICL generalization performance as a function of pretraining dataset size, reflecting our assumption that it is pretrained to interpolation. Second, we uncover a transition to in-context learning as the pretraining task diversity increases. Concretely, there is a threshold task diversity above which linear attention saturates the Bayes error for the ICL regression task. Below that threshold, the model tends to memorize the limited set of pretraining tasks, and its excess risk is substantial. This transition recapitulates the empirical findings of Ravent\u00f3s et al. [18] in full Transformer models.\n4. In \u00a74, we show through numerical experiments that the insights from our theory derived on a simplified model transfer to full Transformer models with softmax self-attention. In particular, the scaling of pretraining sample complexity and task diversity with token dimension required for successful ICL is consistent.\nMore broadly, the study of solvable models is crucial for enhancing our grasp of how machin learning algorithms learn and generalize across various tasks. Understanding the mechanistic un derpinnings of ICL of well-controlled synthetic tasks is an important prerequisite to understanding how it emerges from pretraining on natural data [19].\n# 2 Problem formulation\nWe start by describing the setting of our study.\n# 2.1 ICL of linear regression\nIn an ICL task, the model takes as input a sequence of tokens {x1, y1, x2, y2, . . . , x\u2113, y\u2113, x\u2113+1}, and outputs a prediction of y\u2113+1. We will often refer to an input sequence as a \u201ccontext.\u201d The pairs {xi, yi}\u2113+1 i=1 are i.i.d. samples from a context-dependent joint distribution P(x, y). Hence, the model needs to gather information about P(x, y) from the first \u2113examples and use this information to predict y\u2113+1 from x\u2113+1. We will refer to \u2113as the \u201ccontext length\u201d. In this work, we focus on an approximately linear mapping between xi \u2208Rd and yi \u2208R:\nyi = \u27e8xi, w\u27e9+ \u03f5i,\nwhere \u03f5i is a Gaussian noise with mean zero and variance \u03c1, and w \u2208Rd is referred to as a task vector. We note that the task vector w is fixed within a context, but can change between different contexts. The model has to learn w from the \u2113pairs presented within the context, and use it to predict y\u2113+1 from x\u2113+1.\n# 2.2 Linear self-attention\nThe model that we will analytically study is the linear self-attention block [31]. Linear self-attention takes as input an embedding matrix Z, whose columns hold the sequence tokens. The mapping o sequences to matrices is not unique. Here, following the convention in [27, 28, 31], we will embed the input sequence {x1, y1, x2, y2, . . . , x\u2113, y\u2113, x\u2113+1} as:\n(1)\n(2)\nwhere 0 in the lower-right corner is a token that prompts the missing value y\u2113+1 to be predicted. For appropriately sized key, query, and value matrices K, Q, V , the output of a linear-attention block [31\u201333] is given by\nThe output A is a matrix while our goal is to predict a scalar, y\u2113+1. Following the choice of positional encoding in eq. (2), we will take Ad+1,\u2113+1, the element of A corresponding to the 0 prompt, as the prediction for y\u2113+1:\n# 2.3 Pretraining data\nThe model is pretrained on n sample sequences, where the \u00b5th sample is a collection of \u2113+  vector-scalar pairs {x\u00b5 i \u2208Rd, y\u00b5 i \u2208R}\u2113+1 i=1 related by the approximate linear mapping in eq. (1) y\u00b5 i = \u27e8x\u00b5 i , w\u00b5\u27e9+ \u03f5\u00b5 i . Here, w\u00b5 denotes the task vector associated with the \u00b5th sample. We make the following statistical assumptions:\n1. x\u00b5 i are d-dimensional random vectors, sampled i.i.d. over both i and \u00b5 from a Gaussian distri bution N(0, Id/d).\n. For 1 \u2264\u00b5 \u2264n, the task vector w\u00b5 associated with the \u00b5th sample context is uniformly sampled from a finite set with k elements, denoted by {w1, . . . , wk}. The elements of this set are independently drawn once at the beginning of training from\nwi \u223ci.i.d. Unif(Sd\u22121( \u221a d)),\nwhere Unif(Sd\u22121( \u221a d)) denotes the uniform distribution on the sphere Sd\u22121( \u221a d) of radius \u221a d. The variable k controls the task diversity in the pretraining data set. Importantly, k can be less than n, in which case the same task vector may be repeated multiple times.\n3. The noise terms \u03f5\u00b5 i are i.i.d. over both i and \u00b5, and drawn from a normal distribution N(0, \u03c1). We denote a sample from this distribution by (Z, y\u2113+1) \u223cPtrain.\n# 2.4 Parameter reduction\nBefore specifying a training procedure, it is insightful to examine the prediction mechanism of the linear attention module for the ICL task. This turns out to be a fruitful exercise, shedding light on critical questions: Can linear self-attention learn linear regression in-context? If so, what information do model parameters learn from data in solving this ICL problem? By closely examining these aspects, we can also formulate a simplified problem that lends itself to analytical study. We start by rewriting the output of the linear attention module, eq. (3), in an alternative form. Following Zhang et al. [27], we define\n(3)\n(4)\n(5)\nwhere V11 \u2208Rd\u00d7d, v12, v21 \u2208Rd\u00d71, v22 \u2208R, M11 \u2208Rd\u00d7d, m12, m21 \u2208Rd\u00d71, and m22 \u2208R. T expression we desire is\nwhere \u27e8\u00b7, \u00b7\u27e9stands for the inner product. This expression reveals several interesting points. First, not all parameters in (5) contribute to the output: We can ignore all the parameters except the last row of V and the first d columns of M. Second, the first term\n\ufffd offers a hint about how the linear attention module might be solving the task. The sum 1 \u2113 \ufffd i\u2264\u2113yixi is a noisy estimate of E[xx\u22a4]w for that context. Hence, if the parameters of the model are such that v22M\u22a4 11 is approximately E[xx\u22a4]\u22121, this term alone makes a good prediction for the output. Third, the third term does not depend on outputs y, and thus does not directly contribute to the ICL task that relies on the relationship between x and y. Fourth, the last term only considers a one dimensional projection of x onto v21. Because the task vectors w and x are isotropic in the statistical models that we consider, there are no special directions in the problem. Consequently, we expect the optimal v21 to be approximately zero by symmetry considerations. Motivated by these observations, and for analytical tractability, we study the linear attention module with the constraint v21 = 0. In this case, collecting the remaining parameters in a matrix\n\ufffd we can rewrite the predicted label as\nThe 1/d scaling of M11 in \u0393 is chosen so that the columns of HZ scale similarly; it does not affect\nThe 1/d scaling of M11 in \u0393 is chosen so that the columns of HZ scale similarly; it does not affect the final predictor \u02c6y. We note that Zhang et al. [27] provide an analysis of population risk (whereas we focus on empirical risk) for a related reduced model in which they set v21 = 0 and m21 = 0. Consequently, the predictors they study differ from ours (8) by an additive term. They justify this choice through an optimization argument: if these parameters are initialized to zero, they remain zero under gradient descent optimization of the population risk, given certain conditions. In the remainder of this paper, we will examine the ICL performance of the reduced model given in (7) and (8), except when making comparisons to a full, nonlinear Transformer architecture. Henceforth, unless explicitly stated otherwise, we will refer to this reduced model as the linear attention module.\n# 2.5 Model pretraining\nThe parameters of the linear attention module are learned from n samples of input sequences,\nThe parameters of the linear attention module are learned from n samples of input sequences, {x\u00b5 1, y\u00b5 1 , . . . , x\u00b5 \u2113+1, y\u00b5 \u2113+1}, \u00b5 = 1, . . . , n.\nThe parameters of the linear attention module are learned from n samples of input sequences, {x\u00b5 , y\u00b5  , . . . , x\u00b5 , y\u00b5 }, \u00b5 = 1, . . . , n.\n(6)\n(7)\n(8)\nwhere \u03bb > 0 is a regularization parameter, and HZ\u00b5 refers to the input matrix (7) populated with the \u00b5th sample sequence. The factor n/d in front of \u03bb makes sure that, when we take the d \u2192\u221eor n \u2192\u221elimits later, there is still a meaningful ridge regularization. The solution to the optimization problem in (9) can be expressed explicitly as\n\uf8f0 \uf8fb where vec(\u00b7) denotes the vectorization operation. Throughout this paper, we adopt the row-major convention. Thus, for a d1 \u00d7 d2 matrix A, vec(A) is a vector in Rd1d2, formed by stacking the rows of A together.\n# 2.6 Evaluation\nFor a given set of parameters \u0393, the model\u2019s generalization error is defined as\n# For a given set of parameters \u0393, the model\u2019s generalization error is defined as\ne(\u0393) = EPtest \ufffd\ufffd y\u2113+1 \u2212\u27e8\u0393, HZ\u27e9 \ufffd2\ufffd ,\n\ufffd\ufffd \ufffd\ufffd where (Z, y\u2113+1) \u223cPtest is a new sample drawn from the distribution of the test data set. W consider two different test data distributions Ptest:\n1. ICL task: xi and \u03f5i are i.i.d. Gaussians as in the pretraining case. However, for each 1 \u2264 \u00b5 \u2264n, the task vector w\u00b5 associated with the \u00b5th input sequence is drawn independently from Unif(Sd\u22121( \u221a d)). We will denote the test error under this setting by eICL(\u0393).\n1. ICL task: xi and \u03f5i are i.i.d. Gaussians as in the pretraining case. However, for each 1 \u2264 \u00b5 \u2264n, the task vector w\u00b5 associated with the \u00b5th input sequence is drawn independently from Unif(Sd\u22121( \u221a d)). We will denote the test error under this setting by eICL(\u0393). 2. In-distribution generalization (IDG) task: The test data are generated in exactly the same manner as the training data, i.e., Ptest = Ptrain, hence the term in-distribution generalization. In particular, the set of unique task vectors {w1, . . . , wk} is identical to that used in the pretraining data. We will denote the test error under this setting by eIDG(\u0393).\n2. In-distribution generalization (IDG) task: The test data are generated in exactly the same manner as the training data, i.e., Ptest = Ptrain, hence the term in-distribution generalization. In particular, the set of unique task vectors {w1, . . . , wk} is identical to that used in the pretraining data. We will denote the test error under this setting by eIDG(\u0393).\nThe ICL task evaluates the true in-context learning performance of the linear attention module. The task vectors in the test set differ from those seen in training, requiring the model to infer them from context. The IDG task assesses the model\u2019s performance on task vectors encountered during pretraining. High performance on the IDG task but low performance on the ICL task indicates that the model memorizes the training task vectors. Conversely, high performance on the ICL task indicates that the model can learn task vectors from the provided context. To understand the performance of our model on both ICL and IDG tasks, we will need to evaluate these expressions for the pretrained attention matrix \u0393\u2217. An asymptotically precise prediction of eICL(\u0393\u2217) and eIDG(\u0393\u2217) will be a main result of this work.\n# 2.7 Bayes optimal estimators\nFollowing Ravent\u00f3s et al. [18], it is useful to compare the predictions made by the trained linear attention to optimal estimators that use only the current context information. These estimators do\n(9)\nnot rely on data outside of the given context for their predictions. Under the mean square loss, the optimal Bayesian estimator \u02c6yBayes = EPtest[y\u2113+1|x1, y1, x2, y2, . . . , x\u2113, y\u2113, x\u2113+1] in our setting has the form\nnot rely on data outside of the given context for their predictions. Under the mean square loss, the optimal Bayesian estimator \u02c6yBayes = EPtest[y\u2113+1|x1, y1, x2, y2, . . . , x\u2113, y\u2113, x\u2113+1] in our setting has the form\nwhere wBayes is the Bayes estimator of the task vector w. For the ICL task, the Bayes-optimal ridge regression estimator is given b\n\uf8ed \uf8f8 \uf8ed \uf8f8 where the ridge is set to the noise variance \u03c1. We will refer to it as the ridge estimator. For the IDG task, the Bayes-optimal estimator is given by\n\ufffd Here, we assume that the training task vectors {w1, . . . , wk} are known to the estimator. Following [18], we will refer to this estimator as the discrete minimum mean squared error (dMMSE) estimator The test performance of these estimators are calculated by\nwhere Ptest can be the ICL or IDG task, and wBayes can be the ridge or the dMMSE estimator. To avoid possible confusion, we emphasize that we will sometimes plot the performance of an estimator on a task for which it is not optimal. For example, we will test the dMMSE estimator, which is Bayes-optimal for the pretraining distribution, on the ICL task, where it is not optimal. This will be done for benchmarking purposes.\n# 3 Theoretical results\nTo answer the questions raised in the introduction, we provide a precise asymptotic analysis of the learning curves of the linear attention module for ICL of linear regression. We then verify through simulations that the primary insights gained from our theoretical analysis extend to more realistic nonlinear Transformers.\n# 3.1 Joint asymptotic limit\nWe have now defined both the structure of the training data as well as the parameters to be optimized. For our theoretical analysis, we consider a joint asymptotic limit in which the input dimension d, the pretraining dataset size n, the context length \u2113, and number of task vectors in the training set k, go to infinity together such that\n(10)\n(11)\nIdentification of these scalings constitutes one of the main results of our paper. As we will see, the linear attention module exhibits rich learning phenomena in this limit. The intuition for these scaling parameters can be seen as follows. Standard results in linear regression [34\u201336] show that to estimate a d-dimensional task vector w from the \u2113samples within a context, one needs at least \u2113= \u0398(d). The number of unique task vectors that must be seen to estimate the true d-dimensional Gaussian task distribution should also scale with d, i.e. k = \u0398(d). Finally, we see from (6) that the number of linear attention parameters to be learned is \u0398(d2). This suggests that the number of individual contexts the model sees during pretraining should scale similarly, i.e., n = \u0398(d2).\n# 3.2 Learning curves for ICL of linear regression by a linear attention module\nOur theoretical analysis, explained in detail in the Supplementary Information, leads to asympto ically precise expressions for the generalization error under the two test distributions under stud Specifically, our theory predicts that, as d, n, \u2113, k \u2192\u221ein the joint limit given in (11),\nand\neIDG(\u0393\u2217) \u2212\u2192eIDG(\u03c4, \u03b1, \u03ba, \u03c1, \u03bb) almost surely,\nwhere eICL(\u03c4, \u03b1, \u03ba, \u03c1, \u03bb) and eIDG(\u03c4, \u03b1, \u03ba, \u03c1, \u03bb) are two deterministic functions of the parameters \u03c4, \u03b1, \u03ba, \u03c1 and \u03bb. The exact expressions of these two functions can be found in SI.5.2 and SI.5.3, respectively. For simplicity, we only present in what follows the ridgeless limit (i.e., \u03bb \u21920+) of the asymptotic generalization errors.\nesult 1 (ICL generalization error in the ridgeless limit). Let\nq\u2217:= 1 + \u03c1 \u03b1 , m\u2217:= M\u03ba \ufffd q\u2217\ufffd , and \u00b5\u2217:= q\u2217M\u03ba/\u03c4(q\u2217),\n\ufffd \ufffd where M\u03ba(\u00b7), defined in (B.3), is a function related to the Stieltjes transform of the MarchenkoPastur law. Then\n\ufffd \ufffd where M\u03ba(\u00b7), defined in (B.3), is a function related to the Stieltjes transform of the MarchenkoPastur law. Then eICL  := lim  eICL(\u03c4, \u03b1, \u03ba, \u03c1, \u03bb)\nResult 2 (IDG generalization error in the ridgeless limit). Let q\u2217, m\u2217, and \u00b5\u2217be the scalars defined in (12). We have\n\uf8f3 where \u03be\u2217= (1\u2212\u03c4)q\u2217 \u03c4\u00b5\u2217 and p\u2217= \ufffd 1 \u2212\u03ba \ufffd\u03ba\u03be\u2217 1\u2212\u03c4 + 1 \ufffd\u22122\ufffd\u22121.\n(12)\n\u03c4 < 1 \u03c4 > 1 ,\nWe derive these results using techniques from random matrix theory. The full setup and technical details are presented in the Supplementary Information in SI.1 through SI.5. A key technical component of our analysis involves characterizing the spectral properties of the sample covariance matrix of n = \u0398(d2) i.i.d. random vectors in dimension \u0398(d2). Each of these vectors is constructed as the vectorized version of the matrix in (7). Related but simpler versions of this type of random matrices involving the tensor product of i.i.d. random vectors have been studied in recent work [37]. Some of our derivations are based on non-rigorous yet technically plausible heuristics. We support these predictions with numerical simulations and discuss in the Supplementary Information the steps required to achieve a fully rigorous proof. Before discussing the implications of our theoretical results, we first note that if we take the limit of \u03ba \u2192\u221eand \u03b1 \u2192\u221ein Result 1 (in either order), the ICL generalization error reduces to the generalization error of simple ridgeless interpolation with isotropic Gaussian covariates in d2 dimensions [36, 38]:\n\uf8f4 \uf8f3 This limiting result makes sense, given that in this limit the ICL generalization problem reduces to the generalization error of ridge regression in d2 dimensions with covariates formed as the tensor product of i.i.d. Gaussian vectors, which by universality results in [37] should in turn be asymptotically equal to that for isotropic Gaussian covariates [36].\n# 3.3 Sample-wise double-descent\nHow large should n, the pretraining dataset size, be for the linear attention to succesfully learn the ICL and IDG tasks? In Figure 1, we plot our theoretical predictions for the ICL and IDG generalization error as a function of \u03c4 = n/d2 and verify them with numerical simulations. Our results demonstrate that the quadratic scaling of sample size with input dimensions is indeed the appropriate regime for nontrivial learning phenomena to occur. As apparent in Figure 1, we find that the generalization error for both ICL and IDG tasks are not monotonic in the number of samples. In the ridgeless limit, both ICL and IDG errors diverge at \u03c4 = 1, with the leading order behavior in the \u03c4 \u21911 (respectively \u03c4 \u21931) limit given by c1 1\u2212\u03c4 (respectively c2 \u03c4\u22121), where c1 (respectively c2) is a \u03c4-independent constant. This leads to a \u201cdoubledescent\u201d behavior [36, 39] in the number of samples. As in other models exhibiting double-descent [36, 38, 39], the location of the divergence is at the interpolation threshold: the number of parameters of the model (elements of \u0393) is, to leading order in d, equal to d2, which matches the number of pretraining samples at \u03c4 = 1. Further, we can investigate the effect of ridge regularisation on the steepness of the double descent, as illustrated in Figure 1c for the ICL task. As we would expect from other models exhibiting double-descent [36, 38, 39], increasing the regularization strength suppresses the peak in error around the interpolation threshold.\n# 3.4 The ICL error can have non-monotonic dependence on context length\nHow large should the context length be? In Figure 2, we plot our theoretical results verified with experiments. We observe that we have correctly identified the regime where ICL appears: context length scales linearly with input dimensions. An interesting observation is that the ICL error does not always monotonically decrease with context length. There are parameter configurations with \u03ba < 1 (blue curve in Figure 2a) for which the ICL error is minimal at some finite \u03b1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c036/c036fd45-515f-4653-8e3f-f33ec323eb35.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Ridgeless ICL error against \u03c4</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1860/1860dc0e-5ef1-4105-978f-5363dc62684b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Finite ridge IDG error against \u03c4</div>\nFigure 1: ICL performance as a function of \u03c4: theory (solid lines) vs simulations (dots). Plots of (a), (c) ICL error eICL(\u0393\u2217) and (b) IDG error eIDG(\u0393\u2217) vs \u03c4 at optimal \u0393\u2217. Parameters: d = 80, \u03ba = 0.5, and \u03c1 = 0.01. Averages and standard deviations are computed over 10 runs.\n# 3.5 Memorization to ICL transition with increasing pretraining task diversity\nRecall that the parameter \u03ba = k/d controls the diversity of the training task vectors. How large should it be for ICL to emerge? Our theory corroborates a phenomenon that was empirically observed in a recent study [18]. Figure 3 shows a transition in the nature of the predictions that the linear attention module makes. For low \u03ba, the model\u2019s performance is close to that of the dMMSE estimator. This indicates that the model inherently assumes the task vector is one of the k vectors encountered in its pretraining dataset, effectively memorizing these task vectors. As \u03ba increases beyond 1, the model\u2019s performance approaches that of the ridge estimator. In this regime, the model generalizes to task vectors beyond its pretraining dataset, behaving as if it has learned the true prior on the task vectors despite having only seen a finite subset in the pretraining dataset.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b988/b9883517-1518-44c1-b764-0504f30dabbc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Ridgeless IDG error against \u03c4</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa54/aa54917d-1831-47e1-9311-1db93bc23cb8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a97/0a97abc8-8fa4-4df9-8b17-2724e7cfea2a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) ICL error against \u03b1</div>\nFigure 2: ICL performance as a function of \u03b1: theory (solid lines) vs simulations (dots). Plots of (a) ICL error eICL g (\u0393\u2217) and (b) IDG error eIDG g (\u0393\u2217) vs \u03b1 at optimal \u0393\u2217. We highlight that, while the IDG error is monotonic in \u03b1, the ICL error for \u03ba = 0.5 and \u03ba = 0.75 is non-monotonic. Parameters: d = 70, \u03c4 = 20, \u03c1 = 0.01, ridgeless. Averages and standard deviations are computed over 10 runs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6935/6935e21c-2ef5-46b0-86e3-e915f14c4580.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparison of linear Transformer generalization error with the dMMSE estimator given by eq. (10): theory (solid lines) vs simulations (dots, triangles). Each value plotted is the excess value of generalisation error over the noise level \u03c1. Parameters: \u03c4 = 0.2\u03b1, \u03c1 = 0.01, ridgeless. Averages and standard deviations for linear model are computed over 10 runs.</div>\nTo further understand the role of \u03ba in the solution learned by the linear attention mechanism, consider the regime where \u03c4, \u03b1 \u2192\u221ewith \u03c4/\u03b1 = c\u2217kept fixed. Under this setting, we have\n<div style=\"text-align: center;\">(b) IDG error against \u03b1</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae1f/ae1fb487-e9c3-42ba-a4b5-b8391f8642c4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3ab8/3ab8a7e1-f9bd-4b81-8e3d-2c8d67a4a5bc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Nonlinear model exhibits double descent of test ICL error in scaling parameter \u03c4. (b) Interpolation threshold follows predicted n \u221dd scaling.</div>\nFigure 4: Experimental verification of both scaling definitions and double descent behaviour in n. Figure 4a: Increasing n will increase error until an interpolation threshold is reached. Figure 4b which occurs for n proportional to d2, as predicted by the linear theory. Best fit lines (\u2217) correspond to fitting log(n) = a log(d) + b giving a2 = 1.82, b2 = 3.55 for 2-layer model and a3 = 2.22, b3 = 2.81 for 3-layer model. Interpolation threshold was computed empirically by searching for location in \u03c4 of sharp increase in value and variance of training error at a fixed number of gradient steps. Parameters: d = 10, \u03b1 = 5, \u03ba = \u221e, \u03c1 = 0.252. For fig. 4a: 2-layer architecture; variance shown comes from model trained over different samples of pretraining data; lines show averages over 10 runs and shaded region shows standard deviation.\nbranch approaches \u03c1, the error of the Bayes-optimal ridge estimator in this limit. The smooth memorization-ICL transition observed in Figure 3 for the finite \u03b1, \u03c4 case stems from this phase transition.\n# Experiments with full, nonlinear Transfor\nAs our theoretical results are derived in a simplified setting, we aim to test if these insights are applicable to a full, nonlinear Transformer model. Specifically, we will investigate: (1) whether we have identified the correct scaling regime for non-trivial learning in an ICL task; (2) if the full Transformer exhibits a sample-wise double descent, and whether the location of the peak error scales quadratically with input dimensions as predicted by our theory; and (3) if the transition from memorization to generalization occurs, with the transition point being around \u03ba = 1. Our experiments1 are done with a standard Transformer architecture consisting of blocks with: (1) a single-head softmax self-attention with K, Q, V \u2208Rhd\u00d7\u2113(d+1) matrices2, followed by (2) a two-layer dense MLP with GELU activation and hidden layer of size 10d [1]. Residual connections are used between the input vector, the pre-MLP output, and the MLP output. Each sample takes the form given by eq. (2). We use either two or three Transformer blocks before returning the final\n1Code to reproduce all experiments is available at https://github.com/Pehlevan-Group/icl-asymptotic. 2Provided that the hidden layer dimension hd is greater than d, it does not affect the expressivity of the attention mechanism. In our experiments, we use h = 10.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2dbb/2dbbf0ed-5140-4df9-aa05-50fff82c8099.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b8c/0b8cd893-9b59-4fb6-b433-678ea09ad433.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Experiment in 2-layer nonlinear network demonstrates a sharp transition between dMMSE estimator and ridge estimator, familiar from the linear theory. Parameters: d = 20, \u03c4 = 50, \u03b1 = 5, \u03c1 = 0.01. Variance shown comes from model trained over different samples of pretraining data; lines show averages over 10 runs and shaded region shows standard deviation.</div>\nlogit corresponding to the (d + 1, \u2113+ 1)th element in the embedding. The loss function is the mean squared error (MSE) between the predicted label (the output of the model for a given sample Z) and the true \u02c6y\u2113+1 value. We train the model in an offline setting with n samples Z1, \u00b7 \u00b7 \u00b7 , Zn, using the Adam optimizer [40] with a learning rate 10\u22124 until the training error converges, typically requiring 300000-500000 gradient steps. The structure of the pretraining and test distributions exactly follows the setup described in Section 2. In Figure 4a, we plot the generalization error and observe the double descent behavior of a full, nonlinear Transformer for the ICL task as the number of pretraining samples varies (plotted as a function of \u03c4). We find that the peak of this curve occurs at the interpolation threshold, identified by tracking when the training loss is non-zero (see figure caption). Our theory predicts that the number of samples n at the peak of the curve, as well as the interpolation threshold, should scale with d2. This scaling is indeed observed in Figure 4b for the full, nonlinear Transformers. These observations suggest that the nonlinear Transformer operates within the scaling regime we have identified. Further, as observed before in Ravent\u00f3s et al. [18], we recover the memorization-to-ICL transition as a function of pretraining task diversity, shown in Figure 3. We note that this transition happens near \u03ba = 1 in the nonlinear model, consistent with our theoretical predictions on the linear model.\n# 5 Discussion\nIn this paper, we computed sharp asymptotics for the in-context learning (ICL) performance in a simplified model of ICL for linear regression using linear attention. This exactly solvable model demonstrates a transition from memorizing pretraining task vectors to generalizing beyond them as the diversity of pretraining tasks increases, echoing empirical findings in full Transformers [18]. Additionally, we observe a sample-wise double descent as the amount of pretraining data increases. Our numerical experiments show that full, nonlinear Transformers exhibit similar behavior in the scaling regime relevant to our solvable model. Our work represents a first step towards a detailed\ntheoretical understanding of the conditions required for ICL to emerge [19]. Our paper falls within a broader program of research that seeks sharp asymptotic characterizations of the performance of machine learning algorithms. This program has a long history in statistical physics [38, 41, 42], and has in recent years attracted substantial attention in machine learning [36, 38, 43\u201349]. For simplicity, we have assumed that the covariates in the in-context regression problem are drawn from an isotropic Gaussian. However, our technical approach could be extended to anisotropic covariates, and, perhaps more interestingly, to featurized linear attention models in which the inputs are passed through some feature map before linear attention is applied [32, 33]. This extension would be possible thanks to an appropriate form of Gaussian universality: for certain classes of regression problems, the asymptotic error coincides with that of a model where the true features are replaced with Gaussian features of matched mean and covariance [36, 37, 43\u2013 48, 50]. This would allow for a theoretical characterization of ICL for realistic data structure in a close approximation of full softmax attention, yielding more precise predictions of how performance scales in real Transformers. In our analysis, we have assumed that the model is trained to interpolation on a fixed dataset. This allows us to cast our simplified form of linear attention pretraining as a ridge regression problem, which in turn enables our random matrix analysis. In contrast, Transformer-based large language models are usually trained in a nearly-online setting, where each gradient update is estimated using fresh examples with no repeating data [51]. Some of our findings, such as double-descent in the learning curve as a function of the number of pretraining examples, are unlikely to generalize to the fully-online setting. It will be interesting to probe these potential differences in future work. Finally, our results have some bearing on the broad question of what architectural features are required for ICL [7, 11, 19]. Our work shows that a full Transformer\u2014or indeed even full linear attention\u2014is not required for ICL of linear regression. However, our simplified model retains the structured quadratic interaction between inputs that is at the heart of the attention mechanism. It is this quadratic interaction that allows the model to solve the ICL regression task, which it does essentially by reversing the data correlation. One would therefore hypothesize that our model is minimal in the sense that further simplifications within this model class would impair its ability to solve this ICL task. In the specific context of regression with isotropic data, a simple point of comparison would be to fix \u0393 = Id, which gives a pretraining-free model that should perform well when the context length is very long. However, this further-reduced model would perform poorly if the covariates of the in-context task are anisotropic. More generally, it would be interesting to investigate when models lacking this precisely-engineered quadratic interaction can learn linear regression in-context, and if they are less sample-efficient than the attention-based models considered here.\n# Acknowledgements\nYML was supported by NSF Award CCF-1910410, and by the Harvard FAS Dean\u2019s Fund for Promising Scholarship. JAZV and CP were supported by NSF Award DMS-2134157 and NSF CAREER Award IIS-2239780. CP is further supported by a Sloan Research Fellowship. AM acknowledges support from Perimeter Institute, which is supported in part by the Government of Canada through the Department of Innovation, Science and Economic Development and by the Province of Ontario through the Ministry of Colleges and Universities. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. This research was supported in part by grants NSF PHY-1748958 and PHY-2309135 to the Kavli Institute for Theoretical Physics (KITP),\nthrough the authors\u2019 participation in the Fall 2023 program \u201cDeep Learning from the Perspectiv of Physics and Neuroscience.\u201d\n# References\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv, 2021. [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [4] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [5] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018. [6] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [8] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [9] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1747\u20131764, 2022. 10] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 11] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=yzkSU5zdwD.\n1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=yzkSU5zdwD.\n[12] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/ in-context-learning-and-induction-heads/index.html. [13] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=ITw9edRDlD. [14] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 21750\u201321764. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 884baf65392170763b27c914087bde01-Paper-Conference.pdf. [15] Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers, 2022. [16] Aaditya K. Singh, Stephanie C. Y. Chan, Ted Moskovitz, Erin Grant, Andrew M. Saxe, and Felix Hill. The transient nature of emergent in-context learning in transformers, 2023. [17] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1560\u20131588. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_ files/paper/2023/file/0561738a239a995c8cd2ef0e50cfa4fd-Paper-Conference.pdf. [18] Allan Ravent\u00f3s, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 14228\u201314246. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 2e10b2c2e1aa4f8083c37dfe269873f8-Paper-Conference.pdf. [19] Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=aN4Jf6Cx69. [20] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 57125\u201357211. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ b2e63e36c57e153b9015fece2352a9f9-Paper-Conference.pdf. [21] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023.\n[22] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=0g0X4H8yN4I. [23] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning, 2023. [24] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models, 2023. [25] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 35151\u201335174. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ von-oswald23a.html. [26] Ruiqi Zhang, Jingfeng Wu, and Peter L. Bartlett. In-context learning of a linear transformer block: Benefits of the mlp component and one-step gd initialization, 2024. [27] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1\u201355, 2024. URL http://jmlr.org/ papers/v25/23-1042.html. [28] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=vSh5ePa0ph. [29] Pritam Chandra, Tanmay Kumar Sinha, Kabir Ahuja, Ankit Garg, and Navin Goyal. Towards analyzing self-attention via linear neural network, 2024. URL https://openreview. net/forum?id=4fVuBf5HE9. [30] Karthik Duraisamy. Finite sample analysis and bounds of generalization error of gradient descent in in-context linear regression. arXiv preprint arXiv:2405.02462, 2024. [31] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020. [32] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 3531\u20133539, 2021. [33] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020. [34] Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for some sets of random matrices. Matematicheskii Sbornik, 114(4):507\u2013536, 1967.\n[35] Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices, volume 20. Springer, 2010. [36] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949 \u2013 986, 2022. doi: 10.1214/21-AOS2133. URL https://doi.org/10.1214/21-AOS2133. [37] Sofiia Dubova, Yue M. Lu, Benjamin McKenna, and Horng-Tzer Yau. Universality for the global spectrum of random inner-product kernel matrices in the polynomial regime. arXiv, 2023. [38] Alexander B. Atanasov, Jacob A. Zavatone-Veth, and Cengiz Pehlevan. Scaling and renormalization in high-dimensional regression. arXiv, 2024. [39] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849\u201315854, 2019. [40] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015. [41] Timothy L. H. Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a rule. Rev. Mod. Phys., 65:499\u2013556, Apr 1993. doi: 10.1103/RevModPhys.65.499. URL https://link.aps.org/doi/10.1103/RevModPhys.65.499. [42] Andreas Engel and Christian van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001. doi: https://doi.org/10.1017/CBO9781139164542. [43] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M\u00e9zard, and Lenka Zdeborov\u00e1. Generalisation error in learning with random features and the hidden manifold model. In International Conference on Machine Learning, pages 3452\u20133462. PMLR, 2020. [44] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborov\u00e1. Learning curves of generic features maps for realistic datasets with a teacher-student model. Advances in Neural Information Processing Systems, 34:18137\u201318151, 2021. [45] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667\u2013766, 2022. [46] Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features. IEEE Transactions on Information Theory, 69(3):1932\u20131964, 2022. [47] Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression beyond the linear scaling regime. arXiv:2403.08160, 2024. [48] Oussama Dhifallah and Yue M Lu. A precise performance analysis of learning with random features. arXiv preprint arXiv:2008.11904, 2020. [49] Hugo Cui, Freya Behrens, Florent Krzakala, and Lenka Zdeborov\u00e1. A phase transition between positional and semantic learning in a solvable model of dot-product attention. arXiv, 2024.\n[50] Andrea Montanari and Basil N. Saeed. Universality of empirical risk minimization. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4310\u20134312. PMLR, 02\u201305 Jul 2022. URL https://proceedings.mlr.press/v178/montanari22a.html. [51] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=j5BuTrEj35. [52] L\u00e1szl\u00f3 Erd\u0151s, Antti Knowles, Horng-Tzer Yau, and Jun Yin. The local semicircle law for a general class of random matrices. Electronic Journal of Probability, 18(none):1 \u2013 58, 2013. doi: 10.1214/EJP.v18-2473. URL https://doi.org/10.1214/EJP.v18-2473. [53] L\u00e1szl\u00f3 Erd\u0151s and Horng-Tzer Yau. A dynamical approach to random matrix theory, volume 28. American Mathematical Soc., 2017. [54] Alston S. Householder. Unitary triangularization of a nonsymmetric matrix. J. ACM, 5(4): 339\u2013342, oct 1958. ISSN 0004-5411. doi: 10.1145/320941.320947. URL https://doi.org/10. 1145/320941.320947. [55] Yue M. Lu. Householder dice: A matrix-free algorithm for simulating dynamics on Gaussian and random orthogonal ensembles. IEEE Transactions on Information Theory, 67(12):8264\u2013 8272, 2021. doi: 10.1109/TIT.2021.3114351. [56] Lloyd N. Trefethen and David Bau, III. Numerical Linear Algebra. Society for Industrial and Applied Mathematics, Philadelphia, PA, 1997. doi: 10.1137/1.9780898719574. URL https: //epubs.siam.org/doi/abs/10.1137/1.9780898719574.\n# Supplementary Information\nSome of the derivations in this document are based on non-rigorous yet technically sound heuristic arguments from random matrix theory. We support these predictions with numerical simulations and discuss the steps required to achieve a fully rigorous proof. All rigorously proven results will be clearly stated as lemmas, propositions, and the like.\n# SI.1 Notation\nSets, vectors and matrices: For each n \u2208N, [n] := {1, 2, . . . , n}. The sphere in Rd with radius \u221a d is expressed as Sd\u22121( \u221a d). For a vector v \u2208Rd, its \u21132 norm is denoted by\u2225v\u2225. For a matrix A \u2208Rd\u00d7d, \u2225A\u2225op and \u2225A\u2225F denote the operator (spectral) norm and the Frobenius norm of A, respectively. Additionally, \u2225A\u2225\u221e:= maxi,j\u2208[n] \ufffd\ufffdA(i, j) \ufffd\ufffddenotes the entry-wise \u2113\u221enorm. We use e1 to denote the first natural basis vector (1, 0, . . . , 0), and I is an identity matrix. Their dimensions can be inferred from the context. The trace of A is written as tr(A). Our derivations will frequently use the vectorization operation, denoted by vec(\u00b7). It maps a d1\u00d7d2 matrix A \u2208Rd1\u00d7d2 to a vector vA = vec(A) in Rd1d2. Note that we shall adopt the row-major convention, and thus the rows of A are stacked together to form vA. We also recall the standard identity:\nvec(E1E2E3) = (E1 \u2297E\u22a4 3 ) vec(E2),\n  where \u2297denotes the matrix Kronecker product, and E1, E2, E3 are matrices whose dimensions are compatible for the multiplication operation. For any square matrix A \u2208R(L+1)\u00d7(L+1), we introduce\n  where \u2297denotes the matrix Kronecker product, and E1, E2, E3 are matrices whose dimensions  compatible for the multiplication operation. For any square matrix A \u2208R(L+1)\u00d7(L+1), we introdu the notation\n  to denote the principal minor of M after removing its first row and column. Stochastic order notation: In our analysis, we use a concept of high-probability bounds known as stochastic domination. This notion, first introduced in [52, 53], provides a convenient way to account for low-probability exceptional events where some bounds may not hold. Consider two families of nonnegative random variables:\nX(d)(u) : d \u2208N, u \u2208U (d)\ufffd , Y = \ufffd Y (d)(u) : d \u2208N, u \u2208U (d)\ufffd ,\n\ufffd \ufffd \ufffd \ufffd where U (d) is a possibly d-dependent parameter set. We say that X is stochastically dominated b Y , uniformly in u, if for every (small) \u03b5 > 0 and (large) D > 0 we have\nsup u\u2208U(d) P[X(d)(u) > d\u03b5Y (d)(u)] \u2264d\u2212D\nsup  P[X(d)(u) > d\u03b5Y (d)(u)] \u2264d\u2212D\nfor sufficiently large d \u2265d0(\u03b5, D). If X is stochastically dominated by Y , uniformly in u, we use the notation X \u227aY . Moreover, if for some family X we have |X| \u227aY , we also write X = O\u227a(Y ). We also use the notation X \u2243Y to indicate that two families of random variables X, Y are asymptotically equivalent. Precisely, X \u2243Y , if there exists \u03b5 > 0 such that for every D > 0 we\nfor all sufficiently large d > d0(\u03b5, D).\n(SI.1.1)\n(SI.1.2)\n(SI.1.3)\n# Moment Calculations and Generalization E\nFor a given set of parameters \u0393, its generalization error is defined as\n\ufffd\ufffd \ufffd\ufffd where (Z, y\u2113+1) \u223cPtest is a new sample drawn from the distribution of the test data set. Recall that Z is the input embedding matrix defined in (2) in the main text, and y\u2113+1 denotes the missing value to be predicted. The goal of this section is to derive an expression for the generalization error e(\u0393). Note that the test distribution Ptest crucially depends on the probability distribution of the task vector w used in the linear model in (1). For the ICL task, we have w \u223cUnif(Sd\u22121( \u221a d)), the uniform distribution on the sphere Sd\u22121( \u221a d). For the IDG task, w is sampled uniformly from the set {w1, w2, . . . , wk}, where these k vectors are the same as those used in the training data [see (4)]. In what follows, we slightly abuse the notation by writing w \u223cPtest to indicate that w is sampled from the task vector distribution associated with Ptest. Let w be the task vector used in the input matrix Z. Throughout the paper, we use Ew [\u00b7] to denote the conditional expectation with respect to the randomness in the data vectors {xi}i\u2208[\u2113+1] and the noise {\u03f5i}i\u2208[\u2113+1], with the task vector w kept fixed. We have the following expressions for the first two conditional moments of (HZ, y\u2113+1). \u221a\nMoreover,\nand\nProof. Using the equivalent representations in (A.1) and (A.2), it is straightforward to verify the stimates of the first (conditional) moments in (SI.2.2). To show (SI.2.3), we note that\nwhere\nUsing the representation in (A.2), we have\n\ufffd \ufffd omputing the expectations Ew [y\u2113+1za] and Ew \ufffd z\u22a4 b \ufffd then gives us (SI.2.3). Next, we show (SI.2.4 nce za and zb are independent, E \ufffd vec(HZ) vec(HZ)\u22a4\ufffd = (d/\u2113)2 E \ufffd zaz\u22a4 a \ufffd \u2297E \ufffd zbz\u22a4 b \ufffd .\n\ufffd \ufffd Computing the expectations Ew [y\u2113+1za] and Ew \ufffd z\u22a4 b \ufffd then gives us (SI.2.3). Next, we show (SI.2 Since za and zb are independent,\n(SI.2.2)\n(SI.2.3)\n(SI.2.4)\n\ufffd \ufffd To obtain the second expectation on the right-hand side of the above expression, we can first verif\nTo o that\nMoreover,\nand\n\ufffd Combining (SI.2.6), (SI.2.7), and (SI.2.8), we have\nSubstituting (SI.2.5) and (SI.2.9) into (SI.2.4), we reach the formula in (SI.2.4). Proposition 1 (Generalization error). For a given weight matrix \u0393, the generalization error of th linear transformer is\nwhere\nbtest := Ew\u223cPtest [w] and Rtest := Ew\u223cPtest \ufffd ww\u22a4\ufffd .\n\ufffd \ufffd Remark 1. We use w \u223cPtest to indicate that w is sampled from the task vector distribution associated with Ptest. Recall our discussions in Section 2.6. For the ICL task, w \u223cUnif(Sd\u22121( \u221a d)). It is straightforward to check that, in this case, (ICL) : b = 0 and R = I. (SI.2.11)\n\ufffd \ufffd Remark 1. We use w \u223cPtest to indicate that w is sampled from the task vector distribution associated with Ptest. Recall our discussions in Section 2.6. For the ICL task, w \u223cUnif(Sd\u22121( \u221a d)). It is straightforward to check that, in this case,\nFor the IDG task, we have\nwhere {wi}i\u2208[k] is the set of fixed task vectors used in the training data\n {wi}i\u2208[k] is the set of fixed task vectors used in the training d\n(SI.2.5)\n(SI.2.6)\n(SI.2.7)\n(SI.2.8)\n(SI.2.9)\n(SI.2.10)\n(SI.2.11)\n(SI.2.12)\nProof. Recall the definition of the generalization error in (SI.2.1). We start by writing e(\u0393) = vec(\u0393)\u22a4E \ufffd vec(HZ) vec(HZ)\u22a4\ufffd vec(\u0393) \u22122 vec(\u0393)\u22a4vec(E [yN+1HZ]) + E \ufffd y2 \u2113+1 \ufffd , where HZ is a matrix in the form of (7) and HZ is independent of \u0393. Since y\u2113+1 = x\u22a4 \u2113+1w + \u03f5, with \u03f5 \u223cN(0, \u03c1) denoting the noise, it is straightforward to check that \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd where HZ is a matrix in the form of (7) and HZ is independent of \u0393. Since y\u2113+1 = x\u22a4 \u2113+1w + \u03f5, with \u03f5 \u223cN(0, \u03c1) denoting the noise, it is straightforward to check that\n\ufffd \ufffd Using the moment estimate (SI.2.4) in Lemma 1 and the identity (SI.1.1), we have \ufffd \ufffd\nMoreover, by (SI.2.3),\nwhere\nwhere C\u03b1,\u03c1 is some constant that only depends on \u03b1 and \u03c1. Proof. Let \ufffd\nProof. Let\nIt is straightforward to check that\nhe bound in (SI.2.16) follows from the estimate that \u2225\u2206\u2225op \u2264C\u03b1,\u03c1 max \ufffd \u2225Rtest\u2225op ,\u2225btest\u2225, 1 \ufffd /d.\nThe bound in (SI.2.16) follows from the estimate that \u2225\u2206\u2225op \u2264C\u03b1,\u03c1 max \ufffd \u2225Rtest\u2225op ,\u2225btest\u2225, 1 \ufffd /d.\n(SI.2.16)\nRemark 2. Consider the optimal weight matrix \u0393\u2217obtained by solving the ridge regression problem in (9). Since \u0393\u2217is the optimal solution of (9), we must have\n\ufffd\ufffd here the right-hand side is the value of the objective function of (9) when we choose \u0393 to be the\n\ufffd\ufffd where the right-hand side is the value of the objective function of (9) when we choose \u0393 to be th all-zero matrix. It follows that \ufffd\n\ufffd By the law of large numbers, \ufffd \u00b5\u2208[n] y2 \u00b5 n \u21921+\u03c1 as n \u2192\u221e. Thus, \u2225\u0393\u2217\u22252 F /d is asymptotically bounded by the constant (1 + \u03c1)/\u03bb. Furthermore, it is easy to check that \u2225Rtest\u2225op = O(1) and \u2225btest\u2225= O(1) for both ICL [see (SI.2.11)] and IDG [see (SI.2.12)]. It then follows from Corollary 1 that the generalization error associated with the optimal parameers \u0393\u2217is asymptotically determined by the first three terms on the right-hand side of (SI.2.13).\n# SI.3 Analysis of Ridge Regression: Extended Resolvent Matrices\nWe see from Corollary 1 and Remark 2 that the two key quantities in determining the generalization\nWe see from Corollary 1 and Remark 2 that the two key quantities in determ error e(\u0393\u2217) are\nwhere Atest and Btest are the matrices defined in (SI.2.14) and (SI.2.15), respectively. In this section, we show that the two quantities in (SI.3.1) can be obtained by studying a parameterized family of extended resolvent matrices. To start, we observe that the ridge regression problem in (9) admits the following closed-form solution: \ufffd \ufffd\nwhere G is a resolvent matrix defined as\n\ufffd\ufffd \ufffd For our later analysis of the generalization error, we need to consider a more general, \u201cparameterized version of G, defined as\n\ufffd\ufffd \ufffd where \u2126\u2208R(d2+d)\u00d7(d2+d) is a symmetric positive-semidefinite matrix and \u03c0 is a nonnegative scalar. The original resolvent G in (SI.3.3) is a special case, corresponding to \u03c0 = 0. The objects in (SI.3.2) and (SI.3.4) are the submatrices of an extended resolvent matrix, which we construct as follows. For each \u00b5 \u2208[n], let\nbe an (d2 + d + 1)-dimensional vector. Let\n25\n(SI.3.1)\n(SI.3.2)\n(SI.3.3)\n(SI.3.4)\n(SI.3.5)\n(SI.3.6)\n\ufffd y block-matrix inversion, it is straightforward to check that\nwhere\n\ufffd\ufffd is a vector in Rd(d+1), and c(\u03c0) is a scalar such that\nBy comparing (SI.3.9) with (SI.3.2), we see that\nMoreover, as shown in the following lemma, the two key quantities in (SI.3.1) can also be obtained from the extended resolvent Ge(\u03c0). Lemma 2. For any matrix A \u2208Rd\u00d7(d+1),\n  where e1 denotes the first natural basis vector in Rd2+d+1. Moreover, for any symmetric and positive semidefinite matrix B \u2208R(d+1)\u00d7(d+1), if we set\nin (SI.3.6), then\n\ufffd\ufffd\ufffd Proof. The identity (SI.3.12) follows immediately from the block form of Ge(\u03c0) in (SI.3.8) and the observation in (SI.3.11). To show (SI.3.14), we take the derivative of 1/c(\u03c0) with respect to \u03c0. From (SI.3.10), and using the identity d d\u03c0G(\u03c0) = \u2212G(\u03c0)\u2126G(\u03c0),\n\ufffd\ufffd\ufffd Proof. The identity (SI.3.12) follows immediately from the block form of Ge(\u03c0) in (SI.3.8) and the observation in (SI.3.11). To show (SI.3.14), we take the derivative of 1/c(\u03c0) with respect to \u03c0. From (SI.3.10), and using the identity\nwe have\nd d\u03c0 \ufffd1 c(\u03c0) \ufffd = 1 d3 \ufffd \u00b5,\u03bd\u2208[n] y\u00b5y\u03bd vec(H\u00b5)\u22a4G(\u03c0)\u2126G(\u03c0) vec(H\u03bd) = q\u22a4(\u03c0)\u2126q(\u03c0).\n26\n(SI.3.7)\n(SI.3.8)\n(SI.3.9)\n(SI.3.10)\n(SI.3.11)\n(SI.3.12)\n(SI.3.13)\n(SI.3.14)\n\ufffd \ufffd Applying the identity in (SI.1.1) to the right-hand side of the above equation, we reach (SI.3.14). Remark 3. To lighten the notation, we will often write Ge(\u03c0) [resp. G(\u03c0)] as Ge [resp. G], leaving their dependence on the parameter \u03c0 implicit. Remark 4. In light of (SI.3.13) and (SI.3.14), we will always choose\nwhere Btest is the matrix defined in (SI.2.15).\n# SI.4 An Asymptotic Equivalent of the Extended Resolvent Matrix\nIn this section, we derive an asymptotic equivalent of the extended resolvent Ge defined in (SI.3.7). From this equivalent version, we can then obtain the asymptotic limits of the right-hand sides of (SI.3.12) and (SI.3.14). Our analysis relies on non-rigorous but technically sound heuristic arguments from random matrix theory. Therefore, we refer to our theoretical predictions as results rather than propositions. Recall that there are k unique task vectors {wi}i\u2208[k] in the training set. Let\ndenote the empirical mean and correlation matrix of these k regression vectors, respectively. Define \ufffd \ufffd\nd correlation matrix of these k regression vectors, respectively\nand\nDefinition 1. Consider the extended resolvent Ge(\u03c0) in (SI.3.7), with \u2126e chosen in the forms of (SI.3.6) and (SI.3.15). Let \ufffdGe be another matrix of the same size as Ge(\u03c0). We say that \ufffdGe and Ge(\u03c0) are asymptotically equivalent, if the following conditions hold.\n(2) Let Atr = \ufffd Rtr (1 + \u03c1)btr \ufffd . For any deterministic, unit-norm vector v \u2208Rd2+d+1,\n(SI.3.15)\n(SI.4.1)\n(SI.4.2)\n(SI.4.3)\n(SI.4.4)\n(SI.4.5)\n\ufffd \ufffd \ufffd where \ufffd Ge(\u03c0) \ufffd \\0 and \ufffd Ge(\u03c0) \ufffd \\0 denote the principal minors of Ge(\u03c0) and Ge(\u03c0), respectively.\n\ufffd \ufffd \ufffd \ufffd Result 3. Let \u03c7\u03c0 denote the unique positive solution to the equation\nwhere Btest is the positive-semidefinite matrix in (SI.2.15), with btest, Rtest chosen accroding to (SI.2.11) or (SI.2.12). The extended resolvent Ge(\u03c0) in (SI.3.7) is asymptotically equivalent to\nwhere Btest is the positive-semidefinite matrix in (SI.2.15), with btest, Rtest chosen accroding  (SI.2.11) or (SI.2.12). The extended resolvent Ge(\u03c0) in (SI.3.7) is asymptotically equivalent to \uf8eb \uf8ee \uf8f9 \uf8f6 \u2212\nin the sense of Definition 1. In the above expression, \u2126e is the matrix in (SI.3.6) with \u2126= Id\u2297Btest In what follows, we present the steps in reaching the asymptotic equivalent Ge(\u03c0) given in (SI.4.8). To start, let G[\u00b5] e to denote a \u201cleave-one-out\u201d version of Ge, defined as\nBy (SI.3.7), we have\n\ufffd\ufffd Applying the Woodbury matrix identity then gives us\nTo proceed, we study the quadratic form z\u22a4 \u00b5 G[\u00b5] e z\u00b5. Let w\u00b5 denotes the task vector associated with z\u00b5. Conditioned on w\u00b5 and G\u00b5 e , the quadratic form z\u22a4 \u00b5 G[\u00b5] e z\u00b5 concentrates around its conditional expectation with respect to the remaining randomness in z\u00b5. Specifically,\nwhere\nand\n28\n(SI.4.6)\n(SI.4.7)\n\uf8f7 \uf8f8 (SI.4.8)\n(SI.4.10)\n(SI.4.11)\nSubstituting z\u22a4 \u00b5 G[\u00b5] e z\u00b5 in (SI.4.9) by \u03c7\u00b5(w\u00b5), we get\nwhere\nis a matrix that captures the approximation error of the above substitution. Next, we replace z\u00b5z\u22a4 \u00b5 on the left-hand side of (SI.4.12) by its conditional expectation Ew\u00b5 \ufffd z\u00b5z\u22a4 \u00b5 \ufffd , conditioned on the task vector w\u00b5. This allows us to rewrite (SI.4.12) as\n\ufffd \u00b5\u2208[n] 1 1 + \u03c7\u00b5(w\u00b5)G[\u00b5] e Ew\u00b5 \ufffd z\u00b5z\u22a4 \u00b5 \ufffd + Ge(\u03c0\u2126e + \u03c4\u03bbI) = I + \u22061 + \u22062,\nwhere\ncaptures the corresponding approximation error. Recall the definition of z\u00b5 in (SI.3.5). Using the moment estimates in Lemma 1, we have\n\uf8f0 where E(w\u00b5) is the matrix defined in (SI.4.11) and\nReplacing the conditional expectation Ew\u00b5 \ufffd z\u00b5z\u22a4 \u00b5 \ufffd in (SI.4.13) by the main (i.e. the first) term on the right-hand side of (SI.4.14), we can transform (SI.4.13) to\nwhere we recall \u03c4 = n/d2, and we use \u22063 to capture the approximation error associated with E\u00b5. Next, we replace the \u201cleave-one-out\u201d terms G\u00b5 e and \u03c7\u00b5(w\u00b5) in (SI.4.15) by their \u201cfull\u201d versions Specifically, we replace G\u00b5 e by Ge, and \u03c7\u00b5(w\u00b5) by\n\u03c7(w\u00b5) := 1 d2 tr \ufffd [Ge]\\0 \u00b7 \ufffd I \u2297E(w\u00b5) \ufffd\ufffd .\n(SI.4.13)\n(SI.4.14)\n(SI.4.15)\n(SI.4.16)\nIt is important to note the difference between (SI.4.10) and (SI.4.16): the former uses G\u00b5 e and the latter Ge. After these replacements and using \u22064 to capture the approximation errors, we have\nRecall that there are k unique task vectors {wi}1\u2264i\u2264k in the training set consisting of n input samples. Each sample is associated with one of these task vectors, sampled uniformly from the set {wi}1\u2264i\u2264k. In our analysis, we shall assume that k divides n and that each unique task vector is associated with exactly n/k input samples. (We note that this assumption merely serves to simplify the notation. The asymptotic characterization of the random matrix Ge remains the same even without this assumption.) Observe that there are only k unique terms in the sum on the left-hand side of (SI.4.17). Thus,\nSo far, we have been treating the k task vectors {wi}i\u2208[k] as fixed vectors, only using the randomness in the input samples that are associated with the data vectors \ufffd x\u00b5 i \ufffd . To further simplify our asymptotic characterization, we take advantage of the fact that {wi}i\u2208[k] are independently sampled from Unif(Sd\u22121( \u221a d)). To that end, we can first show that \u03c7(wi) in (SI.4.16) concentrates around its expectation. Specifically,\nBy symmetry, we must have\n\ufffd \ufffd \ufffd \ufffd for any 1 \u2264i < j \u2264k. It follows that \ufffd\ufffd\u03c7(wi) \u2212\u03c7(wj) \ufffd\ufffd= O\u227a(d\u22121/2), and thus, by a union bound, max i\u2208[k] \ufffd\ufffd\u03c7(wk1) \u2212\ufffd\u03c7ave \ufffd\ufffd= O\u227a(d\u22121/2), (SI.4.19\nwhere\n\ufffd Upon substituting (SI.4.16) into (SI.4.20), it is straightforward to verify the following characterization of \ufffd\u03c7:\n\ufffd Upon substituting (SI.4.16) into (SI.4.20), it is straightforward to verify the following charact tion of \ufffd\u03c7ave: \ufffd \ufffd\n\ufffd Upon substituting (SI.4.16) into (SI.4.20), it is straightforward to verify the following characteriza-\n \ufffd \ufffd \ufffd \ufffd The estimate in (SI.4.19) prompts us to replace the terms \u03c7(wi) in the right-hand side of (SI.4.18) by the common value \ufffd\u03c7ave. As before, we introduce a matrix \u22065 to capture the approximation error\n(SI.4.17)\n(SI.4.18)\n(SI.4.19)\n(SI.4.20)\n(SI.4.21)\nassociated with this step. Using the newly introduced notation Etr, btr and Rtr in (SI.4.3) an (SI.4.1), we can then simplify (SI.4.18) as\nDefine\nThen\n   \ufffd \ufffd\ufffd \ufffd Remark 5. We claim that \ufffdGe is asymptotically equivalent to Ge, in the sense of Definition 1. Given (SI.",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in Transformers, a critical capability that enables these models to learn from examples presented in the input without explicit prior training. Despite its importance, the necessary conditions for effective ICL, including sample complexity, task diversity, and context length, remain largely unresolved.",
        "problem": {
            "definition": "The paper investigates the conditions under which ICL emerges in linear regression tasks using linear attention, specifically focusing on the scaling of pretraining examples, context length, and task diversity.",
            "key obstacle": "A significant challenge is understanding how the model transitions from memorization of training tasks to genuine ICL and generalization, particularly under varying levels of task diversity."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that increasing the number of parameters and training examples in Transformers enhances their learning capabilities.",
            "opinion": "The authors propose a simplified model of linear attention to derive precise theoretical insights into ICL performance.",
            "innovation": "The primary improvement over previous methods lies in the sharp asymptotic analysis of the learning curve for ICL, revealing a double-descent phenomenon and a phase transition based on task diversity."
        },
        "Theory": {
            "perspective": "The theoretical framework focuses on a simplified model of a Transformer, particularly the linear self-attention mechanism, to analyze ICL performance.",
            "opinion": "The authors assume that Transformers learn algorithms during pretraining, which they then apply flexibly to in-context tasks.",
            "proof": "The paper provides rigorous asymptotic results for the generalization error of ICL and IDG tasks, derived using random matrix theory."
        },
        "experiments": {
            "evaluation setting": "The experiments involve both linear attention and full nonlinear Transformer architectures, assessing their performance on ICL tasks with varying pretraining sample sizes and task diversities.",
            "evaluation method": "The model's generalization error is evaluated through numerical simulations, comparing theoretical predictions with empirical results."
        },
        "conclusion": "The findings indicate that the model exhibits a transition from memorization to ICL as task diversity increases, and that both ICL and IDG errors display a double-descent behavior as the amount of pretraining data varies.",
        "discussion": {
            "advantage": "This work provides a rigorous theoretical understanding of ICL in linear regression tasks, highlighting the conditions necessary for its emergence.",
            "limitation": "The simplified model may not fully capture the complexities of real-world Transformers, particularly in online learning settings.",
            "future work": "Future research could explore the implications of these findings in more complex models and investigate the effects of different data structures on ICL."
        },
        "other info": [
            {
                "info1": "The study emphasizes the role of task diversity in facilitating ICL, suggesting a threshold above which the model can generalize effectively."
            },
            {
                "info2": {
                    "info2.1": "The results are supported by numerical simulations that verify the theoretical predictions.",
                    "info2.2": "The paper contributes to a broader understanding of machine learning algorithms and their performance scaling."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning (ICL) in Transformers, a critical capability that enables these models to learn from examples presented in the input without explicit prior training."
        },
        {
            "section number": "1.2",
            "key information": "Despite its importance, the necessary conditions for effective ICL, including sample complexity, task diversity, and context length, remain largely unresolved."
        },
        {
            "section number": "1.3",
            "key information": "The authors propose a simplified model of linear attention to derive precise theoretical insights into ICL performance."
        },
        {
            "section number": "2",
            "key information": "The paper investigates the conditions under which ICL emerges in linear regression tasks using linear attention, specifically focusing on the scaling of pretraining examples, context length, and task diversity."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical framework focuses on a simplified model of a Transformer, particularly the linear self-attention mechanism, to analyze ICL performance."
        },
        {
            "section number": "3.3",
            "key information": "The primary improvement over previous methods lies in the sharp asymptotic analysis of the learning curve for ICL, revealing a double-descent phenomenon and a phase transition based on task diversity."
        },
        {
            "section number": "6.1",
            "key information": "This work provides a rigorous theoretical understanding of ICL in linear regression tasks, highlighting the conditions necessary for its emergence."
        },
        {
            "section number": "6.4",
            "key information": "The study emphasizes the role of task diversity in facilitating ICL, suggesting a threshold above which the model can generalize effectively."
        }
    ],
    "similarity_score": 0.7145449212580295,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Asymptotic theory of in-context learning by linear attention.json"
}