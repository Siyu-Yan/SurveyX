{
    "from": "google",
    "scholar_id": "kXNA3qRuxVAJ",
    "detail_id": null,
    "title": "Delving into Out-of-Distribution",
    "abstract": " Abstract\n\nRecognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (e.g., either vision or language), leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts. We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM. Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1% (AUROC). Code is available at https://github.com/ deeplearning-wisc/MCM.\n\n# 1 Introduction\n\nOut-of-distribution (OOD) detection is critical for deploying machine learning models in the wild, where samples from novel classes can naturally emerge and should be flagged for caution. Despite increasing attention, the vast majority of OOD detection methods are driven by single-modal learning [26, 29, 34, 68, 89, 93, 95, 98]. For example, labels are typically encoded as one-hot vectors in image classification, leaving the semantic information encapsulated in texts largely unexploited. OOD detection relying on pure visual information can inherit the limitations, e.g., when an OOD input may be visually similar to in-distribution (ID) data yet semantically different from any ID class.\nIn this paper, we delve into a new landscape for OOD detection, departing from the classic singlemodal toward a multi-modal regime. While the motivation is appealing, a core challenge remains: how to effectively utilize",
    "bib_name": "Delvingint2",
    "md_text": "# Delving into Out-of-Distribution Detection with\nVision-Language Representations\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/235b/235b801e-a991-4f4d-b1ae-ce1c846e065c.png\" style=\"width: 50%;\"></div>\nYifei Ming 1 Ziyang Cai 1 Jiuxiang Gu 2 Yiyou Sun 1 Wei Li 3 Yixuan Li\n1 Department of Computer Sciences, University of Wisconsin-Madison\n2 Adobe 3 Google Research\n{alvinming,ziyangc,sunyiyou,sharonli}@cs.wisc.edu\njigu@adobe.com mweili@google.com\n\n# Abstract\n\nRecognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (e.g., either vision or language), leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts. We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM. Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1% (AUROC). Code is available at https://github.com/ deeplearning-wisc/MCM.\n\n# 1 Introduction\n\nOut-of-distribution (OOD) detection is critical for deploying machine learning models in the wild, where samples from novel classes can naturally emerge and should be flagged for caution. Despite increasing attention, the vast majority of OOD detection methods are driven by single-modal learning [26, 29, 34, 68, 89, 93, 95, 98]. For example, labels are typically encoded as one-hot vectors in image classification, leaving the semantic information encapsulated in texts largely unexploited. OOD detection relying on pure visual information can inherit the limitations, e.g., when an OOD input may be visually similar to in-distribution (ID) data yet semantically different from any ID class.\nIn this paper, we delve into a new landscape for OOD detection, departing from the classic singlemodal toward a multi-modal regime. While the motivation is appealing, a core challenge remains: how to effectively utilize joint vision-language features for OOD detection? In the visual domain, existing methods typically require good feature representations [66, 72], and a distance metric under which OOD data points are relatively far away from the in-distribution (ID) data [42, 71]. These approaches, however, do not directly translate into the multi-modal regime. On the representation learning side, recent vision-language pre-training schemes such as CLIP [59] and ALIGN [33] have emerged as promising alternatives for visual representation learning. The main idea is to align an image with its corresponding textual description in the feature space. While the resulting representations are powerful, OOD detection based on such aligned multi-modal features is still in its infancy.\nWe bridge the gap by exploring a distance-based OOD detection approach, leveraging the joint vision-language representations. Our method capitalizes on the compatibility between visual features and textual features. By defining the textual features as the \u201cconcept prototypes\u201d for each ID class,\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f14/7f14674f-2674-4e6d-9fe8-618f029e8562.png\" style=\"width: 50%;\"></div>\nFigure 1: Overview of the proposed zero-shot OOD detection framework. The ID classification task is defined by a set of class labels Y in. The goal of OOD detection is to detect samples that do not belong to Y in. We view the textual embeddings of ID classes (wrapped by text templates) as concept prototypes. The OOD uncertainty of an input image can be characterized by the distance from visual features to the closest ID prototype. By properly scaling the distance, the MCM score achieves strong ID-OOD separability. See Section 3 for details.\n\nwe characterize OOD uncertainty by the distance from the visual feature to the closest ID prototype. That is, images closer to one of the textual embeddings of ID classes are more likely to be ID and vice versa. By a proper scaling of the distance, our proposed Maximum Concept Matching (MCM) score achieves strong ID-OOD separability (see Figure 1). MCM stands in contrast with the previous distance-based approaches, such as Mahalanobis [42], which defines class prototypes based on pure visual embeddings. Indeed, we show later in Section 5 that MCM (with multi-modal vision-language features) is far more competitive than Mahalanobis (with single-modal visual features). Moreover, while prior works of CLIP-based OOD detection [16, 19] rely on a set of candidate OOD labels, MCM is OOD-agnostic and alleviates the need for any prior information about test inputs.\nOur work also advances the field by showcasing the promise of zero-shot OOD detection, which offers strong performance and generality without training on the ID samples. In particular, classic OOD detection methods often require training from scratch [9, 27] or fine-tuning [19, 32] on a given ID dataset. In this setting, a classifier and its companion OOD detector are good at only one task. Every new task (ID dataset) requires additional training and brings additional computation and storage costs. In contrast, we show for the first time that: (1) MCM achieves superior performance across a wide variety of real-world tasks\u2014with just one single pre-trained model. This is encouraging given that there is no training or any OOD information involved. (2) On the challenging ImageNet-1k benchmark, MCM\u2019s zero-shot OOD detection performance favorably matches and even outperforms strong task-specific baselines fine-tuned on BiT [32] and ViT models [19]. (3) MCM remains robust against hard OOD inputs, including both semantically hard OODs [85] and spurious OODs [50].\nWe summarize our main contributions as follows:\n1.  We propose MCM, a simple yet effective OOD detection method based on aligned vision\nlanguage features. MCM offers several compelling advantages over other OOD detection methods: generalizable (one model supports many tasks), OOD-agnostic (no information required from OOD data), training-free (no downstream fine-tuning required), and scalable to large real-world tasks.\n2. We conduct extensive experiments and show that MCM achieves superior performance on\na wide range of real-world tasks. On ImageNet-1k, MCM achieves an average AUROC of 91.49%, outperforming methods that require training. Moreover, we show that MCM remains competitive under challenging hard OOD evaluation tasks.\n3. We provide in-depth empirical and theoretical analysis, providing insights to understand the\neffectiveness of MCM. We hope that this work will serve as a springboard for future works on OOD detection with multi-modal features.\n\nwe characterize OOD uncertainty by the distance from the visual feature to the closest ID prototype. That is, images closer to one of the textual embeddings of ID classes are more likely to be ID and vice versa. By a proper scaling of the distance, our proposed Maximum Concept Matching (MCM) score achieves strong ID-OOD separability (see Figure 1). MCM stands in contrast with the previous distance-based approaches, such as Mahalanobis [42], which defines class prototypes based on pure visual embeddings. Indeed, we show later in Section 5 that MCM (with multi-modal vision-language features) is far more competitive than Mahalanobis (with single-modal visual features). Moreover, while prior works of CLIP-based OOD detection [16, 19] rely on a set of candidate OOD labels, MCM is OOD-agnostic and alleviates the need for any prior information about test inputs.\nOur work also advances the field by showcasing the promise of zero-shot OOD detection, which offers strong performance and generality without training on the ID samples. In particular, classic OOD detection methods often require training from scratch [9, 27] or fine-tuning [19, 32] on a given ID dataset. In this setting, a classifier and its companion OOD detector are good at only one task. Every new task (ID dataset) requires additional training and brings additional computation and storage costs. In contrast, we show for the first time that: (1) MCM achieves superior performance across a wide variety of real-world tasks\u2014with just one single pre-trained model. This is encouraging given that there is no training or any OOD information involved. (2) On the challenging ImageNet-1k benchmark, MCM\u2019s zero-shot OOD detection performance favorably matches and even outperforms strong task-specific baselines fine-tuned on BiT [32] and ViT models [19]. (3) MCM remains robust against hard OOD inputs, including both semantically hard OODs [85] and spurious OODs [50].\nWe summarize our main contributions as follows:\n\n1.  We propose MCM, a simple yet effective OOD detection method based on aligned vision\nlanguage features. MCM offers several compelling advantages over other OOD detection methods: generalizable (one model supports many tasks), OOD-agnostic (no information required from OOD data), training-free (no downstream fine-tuning required), and scalable to large real-world tasks.\n2. We conduct extensive experiments and show that MCM achieves superior performance on\na wide range of real-world tasks. On ImageNet-1k, MCM achieves an average AUROC of 91.49%, outperforming methods that require training. Moreover, we show that MCM remains competitive under challenging hard OOD evaluation tasks.\n3. We provide in-depth empirical and theoretical analysis, providing insights to understand the\neffectiveness of MCM. We hope that this work will serve as a springboard for future works on OOD detection with multi-modal features.\n\n# 2 Preliminaries\n\nContrastive vision-language pre-training. Compared to visual representation learning models\nsuch as ViT [13], vision-language representation learning demonstrates superior performance on image classification tasks. For instance, CLIP [59] adopts a self-supervised contrastive objective (i.e., InfoNCE loss [75]) to align an image with its corresponding textual description in the feature space. Specifically, CLIP adopts a simple dual-stream architecture with one text encoder T: t! R d (e.g., Transformer [77]) and one image encoder I: x! R d (e.g., ViT [13]). After pre-training on a dataset of 400 million text-image pairs, the joint vision-language embeddings of CLIP well associate objects in different modalities. Despite the promise, existing CLIP-like models perform zero-shot classification in a closed-world setting. That is, it will match an input into a fixed set of categories, even if it is irrelevant (e.g., a tree being predicted as a bird in Figure 1). This motivates our work to leverage the multi-modal representation for OOD detection, which is largely unexplored.\nZero-shot OOD detection. Given a pre-trained model, a classification task of interest is defined\nby a set of class labels/names Y in, which we refer to as the known (ID) classes. Here ID classes are defined w.r.t. the classification task of interest, instead of the classes used in pre-training. Accordingly, OOD is defined w.r.t. the ID classes, not the data distribution during pre-training. The goal of OOD detection is to (1) detect samples that do not belong to any of the known classes; (2) otherwise, assign test samples to one of the known classes. Therefore, the OOD detector can be viewed as a\n\u201csafeguard\u201d for the classification model. Formally, we denote the OOD detector as a binary function:\nG (x; Y in, T, I) : X ! {in, out}, where x 2 X denotes a test image. Our method is based on only the names of the given classes in Y in, and a pre-trained model. Different from standard supervised learning, there is no training on the ID samples involved, hence zero-shot.\n\n# 3 OOD Detection via Concept Matching\n\nWe illustrate our approach in Figure 1, which derives the OOD detector G (\u00b7) based on concept matching. For a given task with label set Y in = {y 1, y 2, ..., y K}, we can construct a collection of concept vectors T (t i), i 2 {1, 2, ..., K}, where t i is the text prompt \u201cthis is a photo of a h y i i\u201d for a label y i. The concept vectors are represented by the embeddings of the text prompts.\nFor any test input image x 0, we can calculate the label-wise matching score based on the cosine similarity between the image feature I (x 0) and the concept vector T (t i): s i (x 0) = I (x 0) \u00b7T (t i)\nkI (x 0) k\u00b7kT (t i) k.\nFormally, we define the maximum concept matching (MCM) score as:\n\nP\ne \u2327 is the temperature. For ID data, it will be matched to one of the concept vectors (textual ypes) with a high score; and vice versa. Formally, our OOD detection function can be formulated\n\nwhere by convention 1 represents the positive class (ID) and 0 indicates OOD. \u03bb is chosen so that a high fraction of ID data (e.g., 95%) is above the threshold. For samples that are classified as ID, one can obtain the class prediction based on the closest concept: \u02c6 y = arg max i 2 [K] s i.\n\nwhere by convention 1 represents the positive class (ID) and 0 indicates OOD. \u03bb is chosen so that a high fraction of ID data (e.g., 95%) is above the threshold. For samples that are classified as ID, one can obtain the class prediction based on the closest concept: \u02c6 y = arg max i 2 [K] s i.\nRemark: (1) Our work differs from (and is complementary to) CLIP by focusing on OOD detection rather than (closed-world) zero-shot classification. We show new theoretical insights that softmax scaling plays a unique role in zero-shot OOD detection\u2014improving the separability between ID and OOD data. This role has not been studied rigorously for zero-shot OOD detection. Readers familiar with CLIP may notice that MCM can be used for zero-shot classification in the closed world. This also makes MCM practically convenient for dual goals: detect OOD samples and assign ID data to one of the known classes. (2) Our method in principle is not limited to CLIP; it can be generally applicable for contrastive vision-language pre-training models that promote multi-modal feature alignment.\n\nRemark: (1) Our work differs from (and is complementary to) CLIP by focusing on OOD detection rather than (closed-world) zero-shot classification. We show new theoretical insights that softmax scaling plays a unique role in zero-shot OOD detection\u2014improving the separability between ID and OOD data. This role has not been studied rigorously for zero-shot OOD detection. Readers familiar with CLIP may notice that MCM can be used for zero-shot classification in the closed world. This also makes MCM practically convenient for dual goals: detect OOD samples and assign ID data to one of the known classes. (2) Our method in principle is not limited to CLIP; it can be generally applicable for contrastive vision-language pre-training models that promote multi-modal feature alignment.\n\n(1)\n\nNew insights on softmax scaling for zero-shot OOD detection. We provide theoretical justifi\ncations that softmax scaling improves the separability between ID and OOD data for CLIP-based OOD detection, which is contrary to models trained with cross-entropy (CE) loss. In particular, CLIP-like models are trained with a multi-modal contrastive loss, which maximizes the cosine similarity between an image and its textual description in the feature space. The resulting cosine similarity scores display strong uniformity 1 across labels, as evidenced in Figure 2 (right). Compared to OOD inputs, the gap between the maximum cosine similarity and the average is larger for ID inputs. However, the gap can be small when the number of ID classes increases where ID samples occur with lower highest cosine similarity. As a result, the highest cosine similarity for ID samples and OOD samples can be highly close (c.f. Figure 2 (left)).\n\nMotivated by these observations, MCM employs softmax as a post hoc mechanism to magnify the difference. This is fundamentally different from the softmax score derived from a model trained with cross-entropy loss, which inherently maximizes the posterior p (y | x) for the ground-truth label, and minimizes the probability for other labels. Unlike CLIP\nlike models, logit scores displaying uniformity would be heavily penalized by the CE loss. As a result, the logit score corresponding to the ground-truth label can already be significantly higher than other labels. Applying softmax on the logit scores can exacerbate overconfident predictions, and reduce the separability\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e849/e849eeb1-16dc-4539-9352-9b2f6ae6b693.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Left: Maximum cosine similarity for ID and OOD inputs. There exists overlapping regions (shown in yellow); Right: Cosine similarities between OOD inputs and ID concept vectors. For OOD inputs, the cosine similarities display uniformity.\n</div>\nbetween ID and OOD data [46]. Indeed, for a model trained with cross-entropy loss, a logit-based score such as Energy [48] is shown to be much more effective than the softmax score.\nInterestingly, for CLIP-like models, the trend is the opposite\u2014applying softmax helps sharpen the uniform-like inner product scores, and increases the separability between ID and OOD data. To help readers better understand the insights, we first formalize our observations that OOD inputs trigger similar cosine similarities across ID concepts (Figure 2, right) as the following assumption: Assumption 3.1. Let z:= {y 2 Y in}. Q x denotes the out-of-distribution P x | z =0 (marginal\ndistribution of x conditioned on z = 0). Assume 9 \u03b4 > 0 such that\n\n@\nA\nwhere \u02c6 y:= argmax i 2 [K] s i (x) and \u02c6 y 2:= argmax i 6 =\u02c6 y,i 2 [K] s i (x) denote the indices of the largest and second largest cosine similarities for an OOD input x.\nNow we provide formal guarantees that using softmax can provably reduce the false positive rate (FPR) compared to that without softmax. Theorem 3.1. Given a task with ID label set Y in = {y 1, y 2, ..., y K} and a pre-trained CLIP-like\n\u03bb (K \u2212 1) (\u03bb wo + \u03b4 \u2212 s \u02c6 y 2)\nmodel (T, I). If Q x satisfies Assumption 3.1, then there exists a constant T =\nK\u03bb \u2212 1\nsuch that for any temperature \u2327> T, we have\n\nFPR(\u2327, \u03bb) \uf8ff FPR wo (\u03bb wo),\n\nwhere FPR(\u2327, \u03bb) is the false positive rate based on softmax scaling with temperature \u2327 and detection threshold \u03bb; FPR wo (\u03bb wo) is the false positive rate without softmax scaling based on threshold \u03bb wo\n\n1 This can be explained both theoretically [84] and empirically [81]. It has been shown that self-supervised\ncontrastive learning with a smaller temperature (e.g., initialized as 0.07 for CLIP) promotes uniform distribution for L 2-normalized features. Moreover, as CLIP features lie on a high-dimensional space (512 for CLIP-B/16 and 768 for CLIP-L/14), uniformly distributed points in a high-dimensional sphere tend to be equidistant to each other [79]. Therefore, for OOD inputs, we observe approximately uniform cosine similarity with concept vectors.\n\n1 This can be explained both theoretically [84] and empirically [81]. It has been shown that self-supervised\ncontrastive learning with a smaller temperature (e.g., initialized as 0.07 for CLIP) promotes uniform distribution for L 2-normalized features. Moreover, as CLIP features lie on a high-dimensional space (512 for CLIP-B/16 and 768 for CLIP-L/14), uniformly distributed points in a high-dimensional sphere tend to be equidistant to each other [79]. Therefore, for OOD inputs, we observe approximately uniform cosine similarity with concept vectors.\n\nThis suggests that applying softmax scaling with a moderate temperature results in superior OOD detection performance compared to that without softmax scaling. The proof is in Appendix A. Later in Section 5, we empirically verify on a real-world ImageNet dataset that our bound can indeed be satisfied in CLIP where the thresholds are chosen at 95% true positive rate.\n\nadvantages of our zero-shot OOD detection approach, owing to the strong pre-trained CLIP model:\n\u2022 Generalizable to many tasks: Traditional OOD detection methods are based on a task-specific\nmodel. As a result, the OOD detector is not suitable for a realistic online scenario where the task changes from one to another. In contrast, we will show in Section 4 that MCM can perform a wide variety of OOD detection tasks, with just one single model. For a new task, only the names of the task\u2019s visual concepts Y in are required.\n\u2022 OOD-agnostic: Our method does not rely on any OOD information, and thus suits many real\nworld scenarios where one cannot anticipate what the unknowns would be ahead of time. This also mitigates the shortcoming of a recent approach [19], which assumes that a set of unseen labels are given as some weak information about OOD data.\n\u2022 Training-free: MCM enables OOD detection in a zero-shot fashion. This stands in contrast to the\nvast majority of OOD detection literature, which often requires training from scratch or fine-tuning to achieve competitive performance.\n\u2022 Scalable: The contrastive vision-language pre-training paradigm makes MCM scalable to a large\nnumber of class labels and realistic high-resolution images.\nWe now proceed to the experimental results, demonstrating these advantages on real-world tasks.\n\n# 4 A Comprehensive Analysis of MCM\n\n# 4.1 Datasets and Implementation Details\n\nDatasets. Most previous works on OOD detection only focus on small-scale datasets with blurry\nimages such as CIFAR [40] and TinyImageNet [41]. With pre-trained models such as CLIP, OOD detection can be extended to more realistic and complex datasets. In this work, we scale up evaluations in terms of (1) image resolution, (2) dataset variety, and (3) number of classes. We consider the following ID datasets: C UB-200 [80], S TANFORD-C ARS [39], F OOD-101 [6], O XFORD-P ET [57] and variants of I MAGE N ET [11]. For OOD test datasets, we use the same ones in [32], including subsets of iNaturalist [76], S UN [86], P LACES [96], and T EXTURE [10]. For each OOD dataset, the categories are not overlapping with the ID dataset. We also use subsets of ImageNet-1k for fine-grained analysis. For example, we construct ImageNet-10 that mimics the class distribution of CIFAR-10 but with high-resolution images. For hard OOD evaluation, we curate ImageNet-20, which consists of 20 classes semantically similar to ImageNet-10 (e.g., dog (ID) vs. wolf (OOD)).\n\nModel. In our experiments, we adopt CLIP [59] as the target pre-trained model, which is one of the\nmost popular and publicly available vision-language models. Note that our method is not limited to CLIP; it can generally be applicable for contrastive vision-language pre-training models that promote multi-modal feature alignment. Specifically, we mainly use CLIP-B/16, which consists of a ViT-B/16 Transformer as the image encoder and a masked self-attention Transformer [77] as the text encoder. To indicate the input patch size in ViT models, we append \u201c/x\u201d to model names. We prepend -B, -L to indicate Base and Large versions of the corresponding architecture. For instance, ViT-B/16 implies the Base variant with an input patch resolution of 16 \u21e5 16. We also use CLIP-L/14 which is based on ViT-L/14 as a representative of large models. Unless specified otherwise, the temperature \u2327 is 1 for all experiments. Details of the datasets, experimental setup, and hyperparameters are provided in Appendix B.\n\nMetrics. For evaluation, we use the following metrics: (1) the false positive rate (FPR 95) of OOD\nsamples when the true positive rate of in-distribution samples is at 95%, (2) the area under the receiver operating characteristic curve (AUROC), and (3) ID classification accuracy (ID ACC).\n\n<div style=\"text-align: center;\">D detection with MCM score based on CLIP-B/16 with various ID\n</div>\n<div style=\"text-align: center;\">1: Zero-shot OOD detection with MCM score based on CLIP-B\n</div>\nID Dataset\nOOD Dataset\nAverage\niNaturalist\nSUN\nPlaces\nTexture\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nCUB-200 [80]\n9.83\n98.24\n4.93\n99.10\n6.65\n98.57\n6.97\n98.75\n7.09\n98.66\nStanford-Cars [39]\n0.05\n99.77\n0.02\n99.95\n0.24\n99.89\n0.02\n99.96\n0.08\n99.89\nFood-101 [6]\n0.64\n99.78\n0.90\n99.75\n1.86\n99.58\n4.04\n98.62\n1.86\n99.43\nOxford-Pet [57]\n2.85\n99.38\n1.06\n99.73\n2.11\n99.56\n0.80\n99.81\n1.70\n99.62\nImageNet-10\n0.12\n99.80\n0.29\n99.79\n0.88\n99.62\n0.04\n99.90\n0.33\n99.78\nImageNet-20\n1.02\n99.66\n2.55\n99.50\n4.40\n99.11\n2.43\n99.03\n2.60\n99.32\nImageNet-100\n18.13\n96.77\n36.45\n94.54\n34.52\n94.36\n41.22\n92.25\n32.58\n94.48\nTable 2:\nMethod\nOOD Dataset\nAverage\niNaturalist\nSUN\nPlaces\nTexture\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nRequires training (or w. fine-tuning)\nMOS [32] (BiT)\n9.28\n98.15\n40.63\n92.01\n49.54\n89.06\n60.43\n81.23\n39.97\n90.11\nFort et al. [19] (ViT-B)\n15.07\n96.64\n54.12\n86.37\n57.99\n85.24\n53.32\n84.77\n45.12\n88.25\nFort et al. [19] (ViT-L)\n15.74\n96.51\n52.34\n87.32\n55.14\n86.48\n51.38\n85.54\n43.65\n88.96\nEnergy [48] (CLIP-B)\n21.59\n95.99\n34.28\n93.15\n36.64\n91.82\n51.18\n88.09\n35.92\n92.26\nEnergy [48] (CLIP-L)\n10.62\n97.52\n30.46\n93.83\n32.25\n93.01\n44.35\n89.64\n29.42\n93.50\nMSP [25] (CLIP-B)\n40.89\n88.63\n65.81\n81.24\n67.90\n80.14\n64.96\n78.16\n59.89\n82.04\nMSP [25] (CLIP-L)\n34.54\n92.62\n61.18\n83.68\n59.86\n84.10\n59.27\n82.31\n53.71\n85.68\nZero-shot (no training required)\nMCM (CLIP-B)\n30.91\n94.61\n37.59\n92.57\n44.69\n89.77\n57.77\n86.11\n42.74\n90.77\nMCM (CLIP-L)\n28.38\n94.95\n29.00\n94.14\n35.42\n92.00\n59.88\n84.88\n38.17\n91.49\n# 4.2 Main Results\n\nMCM supports a diverse collection of tasks while being zero-shot. We first show that zero-shot\nOOD detection with MCM is effective across a wide variety of tasks\u2014with just one single pre-trained model. To showcase the versatility of MCM, we consider the seven ID datasets here. To the best of our knowledge, this is among the first attempts to showcase the efficacy under an expansive and diverse collection of ID datasets. The zero-shot OOD detection performance is summarized in Table 1. A salient observation is that MCM can achieve superior detection performance on many tasks. For example, using S TANFORD-C ARS as ID, MCM yields an average FPR95 of 0.08 %. Considering that there are no training samples or OOD information involved, these results are very encouraging.\nIt can be also seen from Table 1 that MCM is promising, especially when the number of samples per ID class is limited in the training set. For example, there are only around 40 samples per class for Stanford-Cars, 100 for Oxford-Pet, and 30 for CUB-200. The sample scarcity makes OOD detection methods that rely on fine-tuning difficult. For example, after fine-tuning on Food-101, while the ID accuracy is increased from 86. 3% to 92. 5%\", OOD detection based on MSP is on par with MCM (99. 5% vs. 99. 4% in AUROC).\n\nMCM scales effectively to large datasets. To examine the scalability of MCM, we compare i\nwith recent competitive OOD detection methods [19, 32] on the ImageNet-1k dataset (ID) in Table 2 We observe the following trends:\n\n\u2022 Larger models lead to superior performance. Compared with CLIP-B, MCM based on CLIP-L\nreduces FPR95 by 4. 57%. Zero-shot ID classification accuracy is also improved by 6. 27% with the larger model, reaching 73. 28% (see Appendix D). This suggests that larger models are endowed with a better representation quality, which benefits both ID classification and OOD detection with MCM. Our finding echos with the recent observations [78] that higher ID classification accuracy is correlated with stronger OOD detection performance.\n\u2022 MOS [32] recently demonstrated competitive performance on ImageNet-1k, which requires model\nfine-tuning based on BiT [38]. In contrast, we show that MCM (CLIP-L) outperforms MOS by 1. 38% in AUROC while being zero-shot (training-free).\n\u2022 MCM shares a softmax scaling function with the classic (visual) confidence-based score MSP [25].\nTo implement MSP, we adopt the commonly used linear probe approach by fine-tuning a linear layer on frozen visual features of CLIP. After fine-tuning, ID accuracy significantly improves, reaching 84. 12% (CLIP-L). Interestingly, the OOD detection performance of MSP is worse than\n\n<div style=\"text-align: center;\">Table 3: Performance comparison on hard OOD detection tasks. MCM is competitive on all three hard OOD asks without training involved. MSP (based on fine-tuned CLIP) does not further improve performance.\n</div>\nMethod\nID\nImageNet-10\nImageNet-20\nWaterbirds\nOOD\nImageNet-20\nImageNet-10\nSpurious OOD\nFPR95 / AUROC\nFPR95 / AUROC\nFPR95 / AUROC\nMSP [25] (fine-tuning)\n9.38 / 98.31\n12.51 / 97.70\n39.57 / 90.99\nMahalanobis [42] (visual only)\n78.32 / 85.60\n43.03 / 89.94\n2.21 / 99.55\nMCM (zero-shot)\n5.00 / 98.71\n12.91 / 98.09\n5.87 / 98.36\nMCM by 15. 54% in FPR95. Under the same model fine-tuned with linear probing, we observe that the Energy score outperforms MSP, corroborating findings in [48]. We investigate more in Section 5.\n\u2022 Recently, Fort et al. [19] explore small-scale OOD detection by fine-tuning the full ViT model.\nWhen extended to large-scale tasks, we find that MCM still yields superior performance under the same image encoder configuration (ViT-B or ViT-L). This further highlights the advantage of utilizing vision-language joint embeddings for large-scale visual OOD detection.\n\nMCM benefits hard OOD detection. Going beyond, we investigate whether MCM is still effective\nfor hard OOD inputs. We consider the following two categories of hard OOD:\n\nSemantically hard OOD: OOD samples that are semantically similar to ID samples are particularly\nchallenging for OOD detection algorithms [85]. To evaluate hard OOD detection tasks in realistic settings, here we consider ImageNet-10 (ID) vs. ImageNet-20 (OOD) and vice versa. The pair consists of high-resolution images with semantically similar categories such as dog versus wolf. As shown in Table 3, MCM outperforms Mahalanobis [42] by 73.32% in FPR95 for ImageNet-10 (ID) vs. ImageNet-20 (OOD) and 30.12% vice versa.\nSpurious OOD: Modern neural networks can exploit spurious correlations for predictions [3].\nFor example, in the Waterbirds dataset [64], there exist spurious correlations between the habitat (e.g., water) and bird types. A recent work [50] proposes a new type of hard OOD named spurious OOD and shows that most OOD detection methods perform much worse for spurious OOD inputs compared to non-spurious inputs. The spurious OOD inputs are created to share the same background (i.e., water) as ID data but have different object labels (e.g., a boat rather than a bird). See Appendix C for illustrations. The results are shown in Table 3. It has been shown that CLIP representations are robust to distributional shifts [59]. Therefore, while prior works [50] show that spurious OOD inputs are challenging for methods based on ResNet [23], MCM and Mahalanobis scores based on pre-trained CLIP perform much better. On the other hand, fine-tuning exposes the model to the training set containing spurious correlations. As a result, MSP performs much worse than MCM (39. 57% vs. 5. 87% in FPR95).\n\nMCM outperforms CLIP-based baselines. Two recent\ny 2Y C \u02c6 p (y | x) for OOD detection. Here\nworks also use CLIP embeddings for OOD detection [16, 19]. However, fundamental limitations exist for both works. Fort et al. [19] assume that a candidate OOD label set Y C is known, and used P\nthe predictive probability \u02c6 p (y | x) is obtained by normalizing the inner products over |Y in | + |Y C | classes. While applying softmax converts any vector to probabilities, as we show in Section 3, the converted probabilities do not necessarily correspond to P (OOD | x). Moreover, obtaining such an OOD label set is typically not feasible, which fundamentally limits its applicability. A recent work [16] realizes this idea by training an extra text decoder on top of CLIP\u2019s image encoder to generate candidate labels. However, [16] cannot guarantee the generated labels are non-overlapping with the ID labels.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b368/b368d102-1f2e-4b06-8f8a-f7b9d3c0f5be.png\" style=\"width: 50%;\"></div>\nFigure 3: Comparison with a candidate label-based score ZO-CLIP on ImageNet-20, based on our implementation of [16]. Implementation details are deferred to Appendix E.1.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b29/1b299f6c-3d72-407e-bf5a-623747ea3095.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The influence of softmax scaling and temperature. We use ImgeNet-100 (ID) vs. iNaturali OOD). Softmax scaling with a moderate temperature significantly improves FPR95.\n</div>\nWe enhance the baseline with a stronger decoder and a filter module (see Appendix E.1). As shown in Figure 3, MCM outperforms the enhanced baseline on all OOD datasets. Moreover, MCM is much simpler to use\u2014alleviating the need for an OOD label set or training an additional caption generator. In contrast, the caption generator\u2019s performance largely affects OOD detection. Poor caption quality degenerates the OOD detection performance of candidate label-based methods. Moreover, obtaining a reliable caption generator for any input image can significantly increase the computational overhead.\n\n# 5 Discussion: A Closer Look at MCM\n\nEmpirical verification on the role of softmax. In Section 3, we prove that softmax scaling on\ncosine similarity scores with a moderate \u2327 improves the ID-OOD separability. Here we empirically verify our theoretical results. As shown in Figure 4, compared to directly using the maximum cosine similarity without softmax (leftmost figure), softmax scaling with a temperature \u2327 = 1 significantly improves the performance by 22. 6% in FPR95, and further increasing \u2327 (e.g., \u2327 = 10) leads to similar performance. The results are based on ImageNet-100 (ID) versus iNaturalist (OOD).\nNow, we verify if our theoretical bound (c.f. Theorem 3.1) is satisfied empirically as well in Figure 4. From the leftmost figure, we can estimate \u03bb wo \u21e1 0. 26, \u03b4 \u21e1 0. 03, and s \u02c6 y 2 \u21e1 0. 23. By checking the third figure (\u2327 = 1 is the temperature value we use for most experiments), we approximate \u03bb \u21e1 0. 011.\n\u03bb (K \u2212 1) (\u03bb wo + \u03b4 \u2212 s \u02c6 y 2)\nAs K = 100, we plug in the values and obtain the lower bound T =\nK\u03bb \u2212 1 \u21e1 0. 65.\nSince \u2327 = 1> 0. 65, by Theorem 3.1, applying softmax scaling with \u2327 = 1 is provably superior to without softmax scaling for OOD detection.\n\nAre vision-language features better than visual feature alone? MCM can be interpreted as a distance-based\napproach\u2014images that are closer to one of the K class prototypes are more likely to be ID and vice versa. Here the class prototypes are defined based on a textual encoder. Alternatively, one can define the class prototypes based on visual features. For example, Mahalanobis [42] defines a class prototype as the average of visual embeddings for images belonging to the same class. This raises the question whether MCM (with multi-modal vision-language features) is better than Mahalanobis (with single-modal visual feature). For a fair comparison, we use the same ViT image encoder from CLIP-B. Both MCM and Mahalanobis extract visual features from the penultimate layer. On ImageNet-1k, Mahalanobis displays a limited perfor\n\nmance, with 73. 14% AUROC averaged across four OOD test datasets (90.77% for MCM), as shown in Figure 5. From a practical perspective, Mahalanobis requires computing the inverse covariance matrix, which can be both computationally expensive and inaccurate when the number of samples is scarce and the number of ID classes grows. In contrast, MCM is easier to use and more robust.\n\nMCM without softmax scaling. In Section 3, we provide theoretical justifications for the necessity\nof softmax scaling for CLIP-like models. To further verify our observations empirically, we show OOD detection performance based on the maximum cosine similarity score S wo\nMCM (x 0; Y in, T, I) =\nmax i 2 [K] s i (x 0). The results are shown in Table 4. For easy tasks such as Food-101 [39], Stanford\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7bb/a7bb9ef5-da63-41f0-84be-67fef20f754e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Comparison with Mahalanobis (Maha) score on ImageNet-1k.\n</div>\n<div style=\"text-align: center;\">Table 4: Zero-shot OOD detection of S wo\nMCM based on CLIP-B/1\n</div>\nID Dataset\nOOD Dataset\nAverage\niNaturalist\nSUN\nPlaces\nTexture\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nStanford-Cars [39]\n0.00\n100\n0.02\n99.99\n0.26\n99.94\n0.00\n100\n0.07\n99.98\nFood-101 [6]\n0.56\n99.86\n0.09\n99.95\n0.49\n99.88\n8.33\n97.44\n2.37\n99.28\nOxford-Pet [57]\n0.02\n99.98\n0.05\n99.97\n0.20\n99.94\n0.27\n99.91\n0.14\n99.95\nImageNet-10\n2.40\n99.42\n1.79\n99.55\n2.83\n99.32\n1.86\n99.56\n2.22\n99.46\nImageNet-20\n14.96\n97.87\n13.10\n97.97\n14.21\n97.67\n13.46\n97.32\n13.93\n97.71\nImageNet-1k\n61.66\n89.31\n64.39\n87.43\n63.67\n85.95\n86.61\n71.68\n69.08\n83.59\nCars [39], and Oxford-Pet [57] as ID, the performance of maximum cosine similarity score is similar to MCM (see Table 1 and Table 2). However, for more challenging tasks such as ImageNet-20 and ImageNet-1k, MCM significantly outperforms that without softmax scaling. For example, the average FPR95 is improved by 11.33% on ImageNet-20 and 26.34% on ImageNet-1k, which highlights the necessity of a proper scaling function for CLIP-based OOD detection.\n\nCars [39], and Oxford-Pet [57] as ID, the performance of maximum cosine similarity score is similar to MCM (see Table 1 and Table 2). However, for more challenging tasks such as ImageNet-20 and ImageNet-1k, MCM significantly outperforms that without softmax scaling. For example, the average FPR95 is improved by 11.33% on ImageNet-20 and 26.34% on ImageNet-1k, which highlights the necessity of a proper scaling function for CLIP-based OOD detection.\nMCM for ResNet-based CLIP models. Our main results are based on the CLIP model with\nViT image encoder. We additionally investigate the effectiveness of MCM on ResNet-based CLIP. Specifically, we use RN50x4 (178.3M), which shares a similar number of parameters as CLIP-B/16\n\nMCM for ResNet-based CLIP models. Our main results are based on the CLIP model with\nViT image encoder. We additionally investigate the effectiveness of MCM on ResNet-based CLIP. Specifically, we use RN50x4 (178.3M), which shares a similar number of parameters as CLIP-B/16 (149.6M). The results are shown in Table 5. We can see that MCM still shows promising results with ResNet-based CLIP models, and the performance is comparable between RN50x4 and CLIP-B/16 (89. 97 vs. 90. 77 in AUROC).\n\n<div style=\"text-align: center;\">Table 5: Comparison with ResNet-based CLIP models on ImageNet-1k (ID).\n</div>\nModel\nOOD Dataset\nAverage\niNaturalist\nSUN\nPlaces\nTexture\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nFPR95#\nAUROC\"\nRN50x4\n44.51\n91.51\n35.11\n92.84\n43.74\n89.60\n57.73\n85.93\n45.27\n89.97\nCLIP-B/16\n30.91\n94.61\n37.59\n92.57\n44.69\n89.77\n57.77\n86.11\n42.74\n90.77\nEffect of prompt ensembling. We examine MCM\u2019s per\nA photo of a <label>. A blurry photo of a <label>. A photo of many <label>. A photo of the large <label>. A photo of the small <label>.\nTable 6: The five prompt templates.\nformance with prompt ensembling. For example, Radford et al. [59] create 80 possible prompts according to the image modalities and nuances in ImageNet. We experiment with the two prompt sets, one of size 80 as in [59], and our own set of 5 prompts. Ensembles are obtained by averaging the textual features. As expected, using ensembles increases the ID classification accuracy on ImageNet-1k (2% with CLIP-B and 3% with CLIP-L). For OOD detection, the average FPR95 is reduced from 38.17% with the default prompt to 35.23% # with an ensemble of five prompts shown in Table 6. In addition, the detection performance with 5 prompts is slightly better than with 80 prompts. Note that prompt ensembling does not increase the inference-time cost, as the textual embeddings (across many prompts) can be pre-calculated and averaged into a single embedding.\n\nTable 6: The five prompt templates.\nof 5 prompts. Ensembles are obtained by averaging the textual features. As expected, using ensembles increases the ID classification accuracy on ImageNet-1k (2% with CLIP-B and 3% with CLIP-L). For OOD detection, the average FPR95 is reduced from 38.17% with the default prompt to 35.23% # with an ensemble of five prompts shown in Table 6. In addition, the detection performance with 5 prompts is slightly better than with 80 prompts. Note that prompt ensembling does not increase the inference-time cost, as the textual embeddings (across many prompts) can be pre-calculated and averaged into a single embedding.\n\n3% with CLIP-L). For OOD detection, the average FPR95 is reduced from 38.17% with the default prompt to 35.23% # with an ensemble of five prompts shown in Table 6. In addition, the detection performance with 5 prompts is slightly better than with 80 prompts. Note that prompt ensembling does not increase the inference-time cost, as the textual embeddings (across many prompts) can be pre-calculated and averaged into a single embedding.\n\nOOD detection in computer vision. For open-world multi-class classification, the goal of OOD detection is to derive a binary ID-OOD classifier along with a multi-class classification model for visual inputs. A plethora of methods has been proposed for deep neural networks [91], including generative model-based methods [7, 20, 36, 53, 54, 56, 61, 67, 88], and discriminative-model based methods. For the latter category, an OOD score can be derived based on the softmax output [4, 12, 24, 25, 29, 32, 46, 90], energy-based score [15, 48, 49, 69, 70, 82], gradient information [31], or the feature embeddings [14, 42, 65, 66, 71, 72, 85] of a model. Morteza et al. [52], Fang et al. [17], and Bitterwolf et al. [5] provided theoretical analysis for OOD detection. Recent works [63, 83] also explored OOD detection for long-tailed distributions. Works insofar have mostly focused on OOD detection for a task-specific model using only visual information. In contrast, we explore a novel\n\nA photo of a <label>.\nA blurry photo of a <label>.\nA photo of many <label>.\nA photo of the large <label>.\nA photo of the small <label>.\nTable 6: The five prompt templates.\n\nwide variety of tasks.\nOOD detection in natural language processing. Distribution shifts can occur due to the change of topics and domains, unexpected user utterances, etc. Challenging benchmarks [37] and characterization of distributional shifts [1] have been proposed in recent years. Compared to early language models such as ConvNets and LSTM [28], pre-trained language models are more robust to distribution shifts and more effective at identifying OOD instances [26, 58, 89]. Various algorithmic solutions are proposed to handle OOD detection, including outlier exposure [30], model ensembling [44], data augmentation [8, 93, 95], contrastive learning [34, 98], and an auxiliary module that incorporates domain labels [68]. Tan et al. [73] also explore zero-shot OOD detection for text classification tasks. However, prior works focus on pure natural language processing (NLP) settings, while we explore utilizing textual embeddings for zero-shot visual OOD detection.\nVision-language models. Utilizing large-scale pre-trained vision-language models for multimodal\ndownstream tasks has become an emerging paradigm with remarkable performance [22, 74]. In general, two types of architectures exist: single-stream models like VisualBERT [43] and ViLT [35] feed the concatenated text and visual features into a single transformer-based encoder; dual-stream models such as CLIP [59], ALIGN [33], and FILIP [92] use separate encoders for text and image and optimize with contrastive objectives to align semantically similar features in different modalities. In particular, CLIP enjoys popularity due to its simplicity and strong performance. CLIP-like models inspire numerous follow-up works [45, 94, 97], which aim to improve data efficiency and better adaptation to downstream tasks. This paper adopts CLIP as the target pre-trained model, but our approach can be generally applicable to contrastive models that promote vision-language alignment.\nMulti-modal OOD detection. Exploring textual information for visual OOD detection is a new\narea with limited existing works. Fort et al. [19] propose to feed the potential OOD labels to the textual encoder of CLIP [59]. Recently, Esmaeilpour et al. [16] propose to train a label generator based on the visual encoder of CLIP and use the generated labels for OOD detection. While both works rely on a set of candidate OOD labels, MCM is OOD-agnostic and alleviates the need for prior information on OOD. Moreover, prior works [16, 59] only focus on small-scale inputs. We largely expand the scope to a wide range of large-scale realistic datasets, and show new theoretical insights.\n\n# 7 Conclusion\n\nIn this work, we delve into a new landscape for OOD detection, departing from the classic singlemodal toward a multi-modal regime. By viewing the textual features as the \u201cconcept prototypes\u201d, we explore a new OOD detection approach MCM, based on the joint vision-language representations. Unlike the majority of OOD detection methods, MCM offers several compelling advantages: trainingfree, generalizable to many tasks, scalable to hundreds of classes, and does not require any prior information on OOD inputs. Moreover, we provide theoretical guarantees on how softmax scaling provably improves zero-shot OOD detection. We investigate the effectiveness of MCM on a wide range of large-scale realistic tasks, including several types of hard OOD datasets. Lastly, we demonstrate the advantage of vision-language features over pure visual features for OOD detection. We hope our work will inspire future research toward multi-modal OOD detection.\n\n# Acknowledgement\n\nThe authors wish to thank Junjie Hu, Ying Fan, Ruisu Zhang, Andrew Geng, and Soumya Suvra Ghosal for the helpful discussions. The work is supported by a Google-Initiated Research Grant, and gift funding from Adobe Research.\n\n# References\n\n[1] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect\nthem. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund,\nJosh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing\n\n[1] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect\nthem. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund,\nJosh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing\n\nthe limits of object recognition models. In Conference on Neural Information Processing Systems (NeurIPS), 2019.\n[3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In The European\nConference on Computer Vision (ECCV), 2018.\n[4] Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In The IEEE / CVF\nComputer Vision and Pattern Recognition Conference (CVPR), 2016.\n[5] Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, and Matthias Hein. Breaking\ndown out-of-distribution detection: Many methods based on ood training data estimate a combination of the same core quantities. In International Conference on Machine Learning, pages 2041\u20132074. PMLR, 2022.\n[6] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative\ncomponents with random forests. In The European Conference on Computer Vision (ECCV), 2014.\n[7] Mu Cai and Yixuan Li. Out-of-distribution detection via frequency-regularized generative\nmodels. In Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision, 2023.\n[8] Derek Chen and Zhou Yu. Gold: improving out-of-scope detection in dialogues using data\naugmentation. arXiv preprint arXiv:2109.03079, 2021.\n[9]  Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of\ndistribution detection using outlier mining. In The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), 2021.\n[10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2014.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2009.\n[12] Terrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection\nin neural networks. arXiv preprint arXiv:1802.04865, 2018.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.\n[14] Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for\ndetecting out-of-distribution objects. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\n[15] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\u2019t know\nby virtual outlier synthesis. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.\n[16] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot open set detection by\nextending clip. In The AAAI Conference on Artificial Intelligence (AAAI), 2022.\n[17] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution\ndetection learnable? In Advances in Neural Information Processing System (NeurIPS), 2022.\n[18] Christiane Fellbaum. Wordnet. In Theory and Applications of Ontology: Computer Applications,\npages 231\u2013243. Springer, 2010.\n[19] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution\ndetection. In Conference on Neural Information Processing Systems (NeurIPS), 2021.\n[20] ZongYuan Ge, Sergey Demyanov, Zetao Chen, and Rahil Garnavi. Generative openmax for\nmulti-class open set classification. arXiv preprint arXiv:1707.07418, 2017.\n[21] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann,\nand Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations\n(ICLR), 2019.\n\n[22] Jiuxiang Gu, Jason Kuen, Shafiq Joty, Jianfei Cai, Vlad Morariu, Handong Zhao, and Tong Sun.\nSelf-supervised relationship probing. In Conference on Neural Information Processing Systems\n(NeurIPS), 2020.\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2016.\n[24] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield\nhigh-confidence predictions far away from the training data and how to mitigate the problem. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 41\u201350, 2019.\n[25] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\nexamples in neural networks. In International Conference on Learning Representations (ICLR), 2017.\n[26] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn\nSong. Pretrained transformers improve out-of-distribution robustness. In Association for Computational Linguistics (ACL), 2020.\n[27] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier\nexposure. In International Conference on Learning Representations (ICLR), 2018.\n[28] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[29] Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of\ndistribution image without learning from out-of-distribution data. In The IEEE / CVF Computer\nVision and Pattern Recognition Conference (CVPR), 2020.\n[30] Yibo Hu and Latifur Khan. Uncertainty-aware reliable text classification. In  SIGKDD Confer\nence on Knowledge Discovery and Data Mining (KDD), 2021.\n[31] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting\ndistributional shifts in the wild. In Conference on Neural Information Processing Systems\n(NeurIPS), 2021.\n[32] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic\nspace. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2021.\n[33] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021.\n[34] Di Jin, Shuyang Gao, Seokhwan Kim, Yang Liu, and Dilek Hakkani-Tur. Towards textual\nout-of-domain detection without in-domain labels. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022.\n[35] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without\nconvolution or region supervision. In International Conference on Machine Learning (ICML), 2021.\n[36] Polina Kirichenko, Pavel Izmailov, and Andrew G Wilson. Why normalizing flows fail to detect\nout-of-distribution data. Conference on Neural Information Processing Systems (NeurIPS), 2020.\n[37] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay\nBalsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021.\n[38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\nGelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In The European Conference on Computer Vision (ECCV), 2020.\n[39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for\nfine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013.\n[40] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n\n[41] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. [42] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting\nout-of-distribution samples and adversarial attacks. In Conference on Neural Information Processing Systems (NeurIPS), 2018.\n[43] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\n[44] Xiaoya Li, Jiwei Li, Xiaofei Sun, Chun Fan, Tianwei Zhang, Fei Wu, Yuxian Meng, and Jun\nZhang. kfolden: k-fold ensemble for out-of-distribution detection. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n[45] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu,\nand Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In International Conference on Learning Representations (ICLR), 2022.\n[46] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of\ndistribution image detection in neural networks. In International Conference on Learning Representations (ICLR), 2018.\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In The European Conference on Computer Vision (ECCV), 2014.\n[48] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution\ndetection. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n[49] Yifei Ming, Ying Fan, and Yixuan Li. Poem: Out-of-distribution detection with posterior\nsampling. In International Conference on Machine Learning (ICML), 2022.\n[50]  Yifei Ming, Hang Yin, and Yixuan Li. On the impact of spurious correlation for out-of\ndistribution detection. The AAAI Conference on Artificial Intelligence (AAAI), 2022.\n[51] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning.\narXiv preprint arXiv:2111.09734, 2021.\n[52] Peyman Morteza and Yixuan Li. Provable guarantees for understanding out-of-distribution\ndetection. The AAAI Conference on Artificial Intelligence (AAAI), 2022.\n[53] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.\nDo deep generative models know what they don\u2019t know? In International Conference on\nLearning Representations (ICLR), 2019.\n[54] Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set\nlearning with counterfactual images. In The European Conference on Computer Vision (ECCV), 2018.\n[55] Edwin G. Ng, Bo Pang, Piyush Sharma, and Radu Soricut. Understanding guided image\ncaptioning performance across domains. arXiv preprint arXiv:2012.02339, 2020.\n[56] Poojan Oza and Vishal M Patel. C2ae: Class conditioned auto-encoder for open-set recognition.\nIn The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2019.\n[57] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In\nThe IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2012.\n[58] Alexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya.\nRevisiting mahalanobis distance for transformer-based out-of-domain detection. In The AAAI Conference on Artificial Intelligence (AAAI), 2021.\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning\n(ICML), 2021.\n[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[61] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon,\nand Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In Conference on Neural Information Processing Systems (NeurIPS), 2019.\n\n[62] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining\nthe predictions of any classifier. In SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2016.\n[63] Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil Mustafa, Nick\nPawlowski, Jan Freyberg, Yuan Liu, Zach Beaver, et al. Does your dermatology classifier know what it doesn\u2019t know? detecting the long-tail of unseen conditions. Medical Image Analysis, 75:102274, 2022.\n[64] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally\nrobust neural networks for group shifts: On the importance of regularization for worst-case generalization. In International Conference on Learning Representations (ICLR), 2019.\n[65] Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with\nGram matrices. In International Conference on Machine Learning (ICML), 2020.\n[66] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised\noutlier detection. In International Conference on Learning Representations (ICLR), 2021.\n[67] Joan Serr\u00e0, David \u00c1lvarez, Vicen\u00e7 G\u00f3mez, Olga Slizovskaia, Jos\u00e9 F. N\u00fa\u00f1ez, and Jordi Luque.\nInput complexity and out-of-distribution detection with likelihood-based generative models. In International Conference on Learning Representations (ICLR), 2020.\n[68] Yilin Shen, Yen-Chang Hsu, Avik Ray, and Hongxia Jin. Enhancing the generalization for\nintent classification and out-of-domain detection in slu. arXiv preprint arXiv:2106.14464, 2021.\n[69] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified\nactivations. In Conference on Neural Information Processing Systems (NeurIPS), 2021.\n[70] Yiyou Sun and Yixuan Li. Dice: Leveraging sparsification for out-of-distribution detection. In\nProceedings of European Conference on Computer Vision (ECCV), 2022.\n[71] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep\nnearest neighbors. In International Conference on Machine Learning (ICML), 2022.\n[72] Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via\ncontrastive learning on distributionally shifted instances. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n[73] Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang, and Mo Yu.\nOut-of-domain detection for low-resource text classification tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\n[74] Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumder, Soujanya Poria,\nRoger Zimmermann, and Amir Zadeh. Multimodal research in vision and language: A review of current and emerging trends. Information Fusion, 77:149\u2013171, 2022.\n[75] Aaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv e-prints, pages arXiv\u20131807, 2018.\n[76] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig\nAdam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2018.\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Conference on Neural Information Processing Systems (NeurIPS), 2017.\n[78] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good\nclosed-set classifier is all you need. In International Conference on Learning Representations\n(ICLR), 2022.\n[79] Roman Vershynin. High-dimensional probability: An introduction with applications in data\nscience, volume 47. Cambridge university press, 2018.\n[80] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011\ndataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\n[81] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In The IEEE /\nCVF Computer Vision and Pattern Recognition Conference (CVPR), 2021.\n\n[82] Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification\nnetworks know what they don\u2019t know? Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[83] Haotao Wang, Aston Zhang, Yi Zhu, Shuai Zheng, Mu Li, Alex J Smola, and Zhangyang Wang.\nPartial and asymmetric contrastive learning for out-of-distribution detection in long-tailed recognition. In International Conference on Machine Learning (ICML), 2022.\n[84] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through\nalignment and uniformity on the hypersphere. In International Conference on Machine Learning\n(ICML), 2020.\n[85]  Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Led\nsam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.\n[86] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2010.\n[87] Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal:\nThe role of image backgrounds in object recognition. In International Conference on Learning Representations (ICLR), 2021.\n[88] Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection\nscore for variational auto-encoder. In Conference on Neural Information Processing Systems\n(NeurIPS), volume 33, 2020.\n[89]  Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, and Caiming Xiong. Unsupervised out\nof-domain detection via pre-trained transformers. In Association for Computational Linguistics\n(ACL), 2021.\n[90] Jingkang Yang, Haoqi Wang, Litong Feng, Xiaopeng Yan, Huabin Zheng, Wayne Zhang, and\nZiwei Liu. Semantically coherent out-of-distribution detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021.\n[91] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution\ndetection: A survey. arXiv preprint arXiv:2110.11334, 2021.\n[92] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang,\nZhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. International Conference on Learning Representations (ICLR), 2021.\n[93] Li-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, Xiao-Ming Wu, and Albert Lam. Out-of-scope\nintent detection with self-supervision and discriminative training. Association for Computational Linguistics (ACL), 2021.\n[94] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and\nHongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021.\n[95] Yinhe Zheng, Guanyi Chen, and Minlie Huang. Out-of-domain detection for natural language\nunderstanding in dialog systems. TASLP, 2020.\n[96] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A\n10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.\n[97] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning\nfor vision-language models. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2022.\n[98] Wenxuan Zhou, Fangyu Liu, and Muhao Chen. Contrastive out-of-distribution detection for\npretrained transformers. Conference on Empirical Methods in Natural Language Processing\n(EMNLP), 2021.\n[99] Zhuotun Zhu, Lingxi Xie, and Alan Yuille. Object recognition with and without objects. In\nInternational Joint Conferences on Artificial Intelligence (IJCAI), 2017.\n\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [N/A]\n(c) Did you discuss any potential negative societal impacts of your work? [N/A]\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 3\nand Section A\n(b) Did you include complete proofs of all theoretical results? [Yes] See Section A\n3. If you ran experiments...\n(a)  Did you include the code, data, and instructions needed to reproduce the main experi\nmental results (either in the supplemental material or as a URL)? [Yes] See Section B\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Section 4 and Section B\n(c)  Did you report error bars (e.g., with respect to the random seed after running ex\nperiments multiple times)? [N/A] There is no training involved in MCM. For fair comparison and reproducibility, we use the publicly available checkpoints of CLIP from OpenAI.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See Section B\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] See Section B\n(b) Did you mention the license of the assets? [N/A]\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nSee Section B\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re\nusing/curating? [N/A]\n(e) Did you discuss whether the data you are using/curating contains personally identifiable\ninformation or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of out-of-distribution (OOD) detection, which is critical for deploying machine learning models in real-world scenarios. Most existing methods rely on single-modal learning, which limits their effectiveness in situations where OOD samples may be visually similar to in-distribution data but semantically different. The authors propose a shift from single-modal to multi-modal OOD detection using vision-language representations, highlighting the need for a new approach that leverages the rich information available in multi-modal data.",
        "problem": {
            "definition": "The problem focuses on the challenge of detecting OOD samples in machine learning models, particularly when these samples are visually similar to in-distribution samples but belong to different classes.",
            "key obstacle": "The main difficulty lies in the reliance of existing methods on single-modal representations, which can lead to misclassification when OOD inputs resemble in-distribution data visually but differ semantically."
        },
        "idea": {
            "intuition": "The idea is inspired by the success of vision-language pre-training techniques, which demonstrate that aligning visual features with textual concepts can enhance the detection of OOD samples.",
            "opinion": "The proposed method, Maximum Concept Matching (MCM), involves using aligned vision-language features to create a zero-shot OOD detection framework that evaluates the distance of visual features from concept prototypes defined by textual embeddings.",
            "innovation": "MCM differentiates itself from existing methods by utilizing multi-modal features for OOD detection, thus providing a more robust approach that does not require prior knowledge of OOD classes."
        },
        "method": {
            "method name": "Maximum Concept Matching",
            "method abbreviation": "MCM",
            "method definition": "MCM is defined as a zero-shot OOD detection method that aligns visual features with textual concept prototypes to assess the likelihood of an input being OOD based on their distance in feature space.",
            "method description": "MCM characterizes OOD uncertainty by measuring the distance from visual features to the closest ID class prototype in a joint vision-language representation space.",
            "method steps": [
                "Define concept vectors for each ID class using textual embeddings.",
                "Calculate the cosine similarity between the visual feature of a test image and each concept vector.",
                "Determine the maximum concept matching score based on these similarities.",
                "Classify the input as in-distribution or OOD based on a threshold applied to the MCM score."
            ],
            "principle": "The method is effective because it leverages the alignment between visual and textual features, allowing for better separation between in-distribution and OOD samples, which is enhanced by the use of softmax scaling."
        },
        "experiments": {
            "evaluation setting": "The experimental setup includes a variety of datasets, such as CUB-200, Stanford-Cars, Food-101, and ImageNet-1k, with OOD test datasets like iNaturalist and SUN. Baseline methods used for comparison include traditional single-modal OOD detection approaches.",
            "evaluation method": "The performance of MCM is assessed using metrics such as the false positive rate at 95% true positive rate (FPR95) and area under the receiver operating characteristic curve (AUROC)."
        },
        "conclusion": "The experiments demonstrate that MCM achieves superior performance in detecting OOD samples across a wide range of tasks without requiring training on ID samples. The method shows robustness against hard OOD inputs and highlights the advantages of utilizing multi-modal features.",
        "discussion": {
            "advantage": "MCM offers several key advantages: it is generalizable to many tasks, OOD-agnostic (no prior information needed), training-free, and scalable to large datasets.",
            "limitation": "While MCM performs well in various scenarios, its effectiveness may be limited in cases where the alignment between visual and textual features is weak or when OOD samples are highly deceptive.",
            "future work": "Future research could explore improving the robustness of MCM against particularly challenging OOD samples and extending its applicability to other domains beyond vision."
        },
        "other info": {
            "code_url": "https://github.com/deeplearning-wisc/MCM",
            "acknowledgments": "The authors acknowledge the support from Google and Adobe Research, as well as contributions from colleagues in discussions related to this work."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Out-of-distribution (OOD) detection is critical for deploying machine learning models in real-world scenarios."
        },
        {
            "section number": "1.2",
            "key information": "The paper discusses the shift from single-modal to multi-modal OOD detection, highlighting the need for new approaches that leverage multi-modal data."
        },
        {
            "section number": "2.1",
            "key information": "The problem focuses on the challenge of detecting OOD samples that are visually similar to in-distribution samples but belong to different classes."
        },
        {
            "section number": "2.2",
            "key information": "The proposed method, Maximum Concept Matching (MCM), aligns visual features with textual concept prototypes to enhance OOD detection."
        },
        {
            "section number": "3.4",
            "key information": "MCM characterizes OOD uncertainty by measuring the distance from visual features to the closest in-distribution class prototype in a joint vision-language representation space."
        },
        {
            "section number": "4.1",
            "key information": "The paper evaluates the performance of MCM across various datasets, demonstrating its robustness against hard OOD inputs."
        },
        {
            "section number": "7.1",
            "key information": "MCM's effectiveness may be limited when the alignment between visual and textual features is weak or when OOD samples are highly deceptive."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore improving the robustness of MCM against particularly challenging OOD samples."
        }
    ],
    "similarity_score": 0.7221775947370067,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/235b/235b801e-a991-4f4d-b1ae-ce1c846e065c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f14/7f14674f-2674-4e6d-9fe8-618f029e8562.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e849/e849eeb1-16dc-4539-9352-9b2f6ae6b693.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b368/b368d102-1f2e-4b06-8f8a-f7b9d3c0f5be.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b29/1b299f6c-3d72-407e-bf5a-623747ea3095.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7bb/a7bb9ef5-da63-41f0-84be-67fef20f754e.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Delving into Out-of-Distribution.json"
}