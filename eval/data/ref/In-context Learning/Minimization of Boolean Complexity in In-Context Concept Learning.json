{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2412.02823",
    "title": "Minimization of Boolean Complexity in In-Context Concept Learning",
    "abstract": " Abstract\n\nWhat factors contribute to the relative success and corresponding difficulties of in-context learning for Large Language Models (LLMs)? Drawing on insights from the literature on human concept learning, we test LLMs on carefully designed concept learning tasks, and show that task performance highly correlates with the Boolean complexity of the concept. This suggests that in-context learning exhibits a learning bias for simplicity in a way similar to humans.\n\n# 1 Introduction\n\nThe human conceptual apparatus represents one of the most remarkable aspects of our species\u2019 intelligence [Murphy, 2002]. In order to understand the ways in which artificial intelligences do and do not resemble our own, understanding their conceptual structure is an important first step.\nOne prominent tradition argues that concepts are representations in a language of thought (LoT) [Fodor, 1975, Goodman et al., 2015, Quilty-Dunn et al., 2022]. The recent Bayesian revolution in cognitive science has argued that concept learning exhibits a very strong bias for simplicity: human learners infer the simplest expression in an LoT that is consistent with the data that they have seen [Feldman, 2000, Chater and Vit\u00e1nyi, 2003, Goodman et al., 2008, Piantadosi et al., 2016].\nIn this paper, we study in-context concept learning with large language models (LLMs), allowing us to address the following questions: when presented with labeled examples of an unknown concept, can an LLM infer the underlying concept? If so, what inductive biases does this in-context concept learning exhibit; in particular, does it exhibit a simplicity bias akin to the simplicity bias displayed by humans?\nConsider the prompt in (1). In the first two lines, we see labeled examples of a new numerical concept, bnik. The final line asks a model to label a new example. Repeating this for a range of example sets and concepts, we can measure whether models have greater success with simpler concepts.\n\n(1)\n\nPreprint. Under review.\n",
    "bib_name": "wang2024minimizationbooleancomplexityincontext",
    "md_text": "# Minimization of Boolean Complexity in In-Context Concept Learning\n\n# Abstract\n\nWhat factors contribute to the relative success and corresponding difficulties of in-context learning for Large Language Models (LLMs)? Drawing on insights from the literature on human concept learning, we test LLMs on carefully designed concept learning tasks, and show that task performance highly correlates with the Boolean complexity of the concept. This suggests that in-context learning exhibits a learning bias for simplicity in a way similar to humans.\n\n# 1 Introduction\n\nThe human conceptual apparatus represents one of the most remarkable aspects of our species\u2019 intelligence [Murphy, 2002]. In order to understand the ways in which artificial intelligences do and do not resemble our own, understanding their conceptual structure is an important first step.\nOne prominent tradition argues that concepts are representations in a language of thought (LoT) [Fodor, 1975, Goodman et al., 2015, Quilty-Dunn et al., 2022]. The recent Bayesian revolution in cognitive science has argued that concept learning exhibits a very strong bias for simplicity: human learners infer the simplest expression in an LoT that is consistent with the data that they have seen [Feldman, 2000, Chater and Vit\u00e1nyi, 2003, Goodman et al., 2008, Piantadosi et al., 2016].\nIn this paper, we study in-context concept learning with large language models (LLMs), allowing us to address the following questions: when presented with labeled examples of an unknown concept, can an LLM infer the underlying concept? If so, what inductive biases does this in-context concept learning exhibit; in particular, does it exhibit a simplicity bias akin to the simplicity bias displayed by humans?\nConsider the prompt in (1). In the first two lines, we see labeled examples of a new numerical concept, bnik. The final line asks a model to label a new example. Repeating this for a range of example sets and concepts, we can measure whether models have greater success with simpler concepts.\n\n(1)\n\nPreprint. Under review.\n\nepresentations in a hypothesized LoT are easier to learn in-context. This shows that LLMs are capable of learning non-trivial mathematical concepts and exhibit learning biases that are similar to hose used in human concept learning.\n\n# 2 Related Work\n\nFeldman [2000] showed that ease of human concept learning is highly negatively correlated with Boolean logical complexity: concepts with longer minimal logical formulas are harder for people to learn. A large body of subsequent work has extended the range and scope of this view using Bayesian inference in various LoTs [Goodman et al., 2008, Piantadosi et al., 2016]. Carcassi and Szymanik [2022] show that neural networks trained from scratch to learn Boolean concepts exhibit a similar bias for simplicity. A wide range of work has recently analyzed when, how, and why in-context learning (ICL) in LLMs works [Min et al., 2022, Aky\u00fcrek et al., 2022, Aky\u00fcrek et al., 2024, i.a.]. To the best of our knowledge, ours is the first to explicitly study concept learning and measure a learning bias for logical simplicity in ICL.\n\n# 3 Methodology\n\n# 3.1 Concept generation\n\nOur data generation methodology is inspired by van de Pol et al. [2022] and Z. Wang and SteinertThrelkeld [2023], where we define the complexity of a concept using its minimal description length\u2014 the length of the shortest expression that can capture the concept (defined more precisely below). We define a concept as a semantic object generated by a logical grammar, whose basic structure is shown in Table 1. The full grammar used during generation is given in Appendix A.4, which imposes some additional constraints to prevent certain types of unwanted recursive generation. 1 Code for generating the concepts, as well as prompting models and analyzing data, may be found at https://github.com/lerow/llm-concept-learning-complexity.\n\nOperator\nType\nGloss\n=\nNUM \u00d7 NUM \u2192BOOL\nNumerical equality\n\u0338=\nNUM \u00d7 NUM \u2192BOOL\nNumerical inequality\n>\nNUM \u00d7 NUM \u2192BOOL\nNumerical more than\n<\nNUM \u00d7 NUM \u2192BOOL\nNumerical less than\n\u00d7\nNUM \u00d7 NUM \u2192NUM\nNumerical multiplication\n\u2227\nBOOL \u00d7 BOOL \u2192BOOL\nAnd\n\u2228\nBOOL \u00d7 BOOL \u2192BOOL\nOr\nTable 1: Operators in the logical grammar.\n\u00d7 \u2192\nTable 1: Operators in the logical grammar.\n\nThe complexity of a concept is determined by the number of operators in its minimal description. For example, the concept \ufffd between 5 and 10 \ufffd has a complexity of 3 (and therefore will be in class 3) because there are three operators (>, \u2227, and>) in its minimal description: (x > 5) \u2227 (10> x). One example of a concept with a complexity of 1 is \ufffd less than 5 \ufffd, with minimal description: (x <5). Concept complexity class n will thus contain all unique concepts with minimal description lengths of n that can be generated by the logical grammar. The complexity classes, along with some representative concepts for each class, are included in Appendix A.8.\nUnder the procedure described so far, it is possible to generate two concepts that have similar or identical meanings. Here, concepts classify pairs of numbers (in (1), the number of apples and the number that Alice has) into true and false. We identify a concept\u2019s meaning as its extension, i.e. the set of the set of such pairs that it maps to true.\nThis fact creates several issues for interpreting the results:\n\n1 See Appendix A.4 for an example.\n\nour hypotheses are about a concept\u2019s minimal description length; thus, only the simplest description for a concept should be considered.\n2. If two concept descriptions in the same complexity class yield the same meaning (e.g., x <5 \u2228 x > 17 and x > 17 \u2228 x <5), then they are effectively the same concept. Thus, generating both concepts is effectively the same as sampling one concept twice, which is undesirable because it might make that concept overrepresented, biasing the results.\n3. If two concept descriptions yield similar but non-identical meanings, there are potential challenges in interpreting a model\u2019s performance. Consider the concepts with meanings (x > 5) and (x > 5) \u2227 (x \u0338 = 7). When n = 100, the second meaning only differs from the first at exactly one place (when x = 7). Thus, if we intend to test learning of the second, more complex concept, a model would get almost the same accuracy if it had learned the incorrect simpler concept as it would get if it had learned the correct concept, since both concepts almost always yield identical predictions. This is a problem since it means we cannot tell if the model has learned the intended concept (as opposed to a similar but unintended concept).\n\nTo address these issues, we perform deduplication in which a generated concept is discarded if its meaning is the same as, or similar to, a previously-generated concept. Since concepts are generated in order of increasing complexity, this procedure ensures that we keep only the minimal description for a given meaning. For deduplication across complexity classes, we consider two concepts similar when their Levenshtein distance 2 is less than 3, i.e. they differ in truth value on at most 3 inputs, in which case we discard the one with the longer description. Deduplication is also performed within complexity classes: when multiple concepts in the same class have a Levenshtein distance of 0 with each other, we discard all but one of them.\n\n# 3.2 Data generation\n\nEach prompt that we give to LLMs, such as the prompt shown in (1) above, is made of sever examples. Each example is generated from the following template, where the slots that vary betwee examples are underlined:\n\nAs shown in Algorithm 1 in Appendix A.3, we iterate through all meaningful numerical ranges for both the number of total objects (which we restrict to be between 5 and 100 inclusive) and the number of objects the person has, and generate an example for each combination. If we want to generate a prompt with m positive examples and n negative examples, we would sample without replacement m and n examples from the sets of positive examples and negative examples respectively, and use an unseen example as a question at the end. The sets of examples used in this paper are balanced \u2013 in every prompt, the model sees the same number (10) of positive and negative examples 3; and for each accuracy data point, the model is tested on the same number (500) of prompts with true labels and false labels. In this work, we use the template \u201cLet us define a new word, bnik.\u201d, followed by labeled examples and a question at the end, for all prompts in the dataset.\n\n# 3.3 Models\n\nWe ran experiments on two LLM families: Qwen2 [Yang et al., 2024] from Alibaba research, and Gemma 2 [Riviere et al., 2024] from Google DeepMind. During testing, the instruction-tuned versions of the models and default Huggingface chat templates were used. Qwen2-72b was the bestperforming open model on Hugging Face Open LLM Leaderboard 4 as of June 2024. The Gemma 2 models achieved state-of-the-art results for their size, while reaching competitive performance on many benchmarks when compared to models with 2 \u00d7 more parameters.\n\n2 See Appendix A.5 on how the distance is calculated. 3 Because of this, we exclude concepts that would have fewer than 10 positive or negative examples. 4 https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n\nFor each complexity class, we randomly sample 18 concepts from all possible concepts in that class. Each data point on the plot in Figure 1 represents the model\u2019s accuracy on a specific concept. The trend line shows the line of best fit to the average accuracy of each complexity class.\nFor each model family, we run experiments for models in two sizes \u2013 the largest model in that family, and a model that has approximately 10 billion parameters. As shown in the figure, the average accuracy for all LLMs decreases as concept complexity increases. The drop in average accuracy is most evident in Gemma 2-9B: from 83% in class 1 to 66% in class 5. For the largest model we tested, Qwen2-72B, there is a 16% decrease (90% \u2192 74%) in average accuracy as complexity increases from 1 to 5. See Appendix A.2 for a plot of only average accuracy values for each model.\nIn Table 2, we see there is a strong negative correlation between concept complexity and average model accuracy, indicated by the Pearson correlation coefficients (PCC). All results are statistically significant except for Qwen2-72B, which is nearly significant at a p = 0. 05 threshold.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e735/e7359d88-87bd-43b3-b2ee-51e97c48a47e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The influence of concept complexity on LLM accuracy at concept learning (see text for how complexity is operationalized). On average, LLM accuracy drops as complexity increases.\n</div>\nModel Name\nPCC\np-value\nGemma-2-9B-it\n-0.961\n0.009\nGemma-2-27B-it\n-0.898\n0.038\nQwen2-7B-Instruct\n-0.884\n0.046\nQwen2-72B-Instruct\n-0.854\n0.065\n2: Pearson correlation between complexity and average acc\nTable 2: Pearson correlation between complexity and average accuracy\n\n# 5 Conclusion\n\nThis work has shown that LLM in-context concept learning exhibits a simplicity bias of a similar sort as the simplicity bias that has been observed in human concept learning. Much work remains for\n\nthe future: (i) more detailed comparisons with human concept learning data (e.g., of exact learning curves), (ii) extending this research to conceptual domains beyond the numerical, and (iii) more detailed analysis of factors explaining the ease of in-context concept learning beyond LoT complexity. We hope that this work will spur a new line of research exploring how perspectives from the concept learning literature can shed light on the nature of in-context learning in LLMs.\n\n# References\n\nEkin Aky\u00fcrek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors,  Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 787\u2013812. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/ v235/akyurek24a.html.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models. 2022. URL https:// openreview.net/forum?id=0g0X4H8yN4I.\nFausto Carcassi and Jakub Szymanik. Neural networks track the logical complexity of boolean concepts. Open Mind : Discoveries in Cognitive Science, 6:132 \u2013 146, 2022. URL https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00059/ 112493/Neural-Networks-Track-the-Logical-Complexity-of.\nNick Chater and Paul Vit\u00e1nyi. Simplicity: A unifying principle in cognitive science? Trends in Cognitive Sciences, 7(1):19\u201322, 2003.\nJacob Feldman. Minimization of Boolean Complexity in Human Concept Learning. Nature, 407: 630\u2013633, 2000. doi: 10.1038/35036586.\nJerry A. Fodor. The Language of Thought. Harvard University Press, 1975. ISBN 978-0-674-51030-2.\nNoah D Goodman, Joshua B Tenenbaum, Jacob Feldman, and Thomas L Griffiths. A rational analysis of rule-based concept learning. Cognitive Science, 32(1):108\u2013154, 2008. doi: 10.1080/ 03640210701802071.\nNoah D. Goodman, Joshua B. Tenenbaum, and Tobias Gerstenberg. Concepts in a Probabilistic Language of Thought. In Eric Margolis and Stephen Laurence, editors,  The Conceptual Mind, pages 623\u2013654. The MIT Press, 2015. ISBN 978-0-262-32686-5. doi: 10.7551/ mitpress/9383.003.0035. URL https://direct.mit.edu/books/book/3430/chapter/ 115711/Concepts-in-a-Probabilistic-Language-of-Thought.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.emnlp-main.759. URL https://aclanthology.org/2022.emnlp-main.759.\nGregory L. Murphy. The big book of concepts. 2002. URL https://mitpress.mit.edu/ 9780262632997/the-big-book-of-concepts/.\nSteven T Piantadosi, Joshua B Tenenbaum, and Noah D Goodman. The logical primitives of thought: Empirical foundations for compositional cognitive models. Psychological Review, 123(4):392\u2013424, 2016. doi: 10.1037/a0039980.\nJake Quilty-Dunn, Nicolas Porot, and Eric Mandelbaum. The Best Game in Town: The ReEmergence of the Language of Thought Hypothesis Across the Cognitive Sciences. pages 1\u201355, 2022. ISSN 0140-525X, 1469-1825. doi: 10.1017/S0140525X22002849. URL https: //www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/ best-game-in-town-the-reemergence-of-the-language-of-thought-hypothesis-ac 76F46784C6C07FF52FF45B934D6D3542.\n\nemma Team Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u2019eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u2019e, Johan Ferret, Peter Liu, Pouya Dehghani Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Sta\u00b4nczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Boxi Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Christoper A. Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozi\u2019nska, D. Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci\u2019nska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost R. van Amersfoort, Josh Gordon, Josh Lipschultz, Joshua Newlan, Junsong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, L. Sifre, L. Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Gorner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, S. Mc Carthy, Sarah Perrin, S\u2019ebastien Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tom\u00e1s Kocisk\u00fd, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Brian Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeffrey Dean, Demis Hassabis, Koray Kavukcuoglu, Cl\u2019ement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a practical size. ArXiv, abs/2408.00118, 2024. URL https://arxiv.org/pdf/2408.00118.\n\nIris van de Pol, P. Lodder, Leendert van Maanen, Shane Steinert-Threlkeld, and Jakub Szymanik. Quantifiers satisfying semantic universals have shorter minimal description length. Cognition, 232, 2022. URL https://www.sciencedirect.com/science/article/pii/ S001002772200138X.\n\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Min Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, and Zhi-Wei Fan. Qwen2 technical report. ArXiv, abs/2407.10671, 2024. URL https://arxiv.org/abs/2407.10671.\n\nLeroy Z. Wang and Shane Steinert-Threlkeld. GQG: Generalized quantifier generalization - a dataset for evaluating quantifier semantics understanding in language models. In Dieuwke Hupkes, Verna Dankers, Khuyagbaatar Batsuren, Koustuv Sinha, Amirhossein Kazemnejad, Christos Christodoulopoulos, Ryan Cotterell, and Elia Bruni, editors, Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pages 185\u2013192, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.genbench-1.15. URL https://aclanthology.org/2023.genbench-1.15.\n\nLeroy Z. Wang and Shane Steinert-Threlkeld. GQG: Generalized quantifier generalization - a dataset for evaluating quantifier semantics understanding in language models. In Dieuwke Hupkes, Verna Dankers, Khuyagbaatar Batsuren, Koustuv Sinha, Amirhossein Kazemnejad, Christos Christodoulopoulos, Ryan Cotterell, and Elia Bruni, editors, Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pages 185\u2013192, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.genbench-1.15. URL https://aclanthology.org/2023.genbench-1.15.\n\n# A Appendix / supplemental material\n\nOnly a finite set of fractions are used in the grammar to improve efficiency. As a consequence, it may be the case that some concepts that require a certain minimal number of operators under our framing could be expressed using fewer operators if more fractions were allowed.. To address this issue, we plan to use a richer set of fractions in future work.\nOnly a limited set of LLMs are tested. It is possible that newer models / models with different architectures do not exhibit the phenomena discussed in this text.\nWe use only one prompt template for all experiments, which may introduce implicit biases in the data and affect the experiment results.\n\nA.2 Complexity vs. average accuracy plot\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/195c/195c8b35-99b2-44aa-851c-1ede2485af4a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Complexity vs. average accuracy.\n</div>\nAlgorithm 1 Data Generation\nInputs: set of subject nouns S, set of predicate verbs P, set of objects O,\nfunction GENERATE_EXAMPLE\ninitialize positive_examples = [], negative_examples = []\nfor total in [5, 100] do\nfor num in [0, total] do\nuniformly randomly sample s, p, o from S, P, O\nexample = GENERATE_EXAMPLE (total, num, s, p, o)\nappend example with true labels to positive_examples\nappend example with false labels to negative_examples\nend for\nend for\nreturn positive_examples, negative_examples\n# A.4 Grammar for concept generation\n\nSimpleInt -> [0, 100]\n\nConstraints on the grammar For the sake of efficiency, the fine-grained type system in the grammar prevents the generation of fractions that are not in the predefined list of fractions. Multiplication can only take place between \"SimpleFloat\" and \"Var1\", so it is not possible to obtain new fractions by multiplying existing ones.\n\nThe deduplication methodology used here is based on vectors representing concept meanings. Suppose the total number of objects is n = 100. We can then imagine the semantics of a concept being represented by a 101-dimensional vector, where each dimension is the truth value of f (n = 100, x) = {\u27e8 n, x \u27e9: (x > 3)} for x = [0, 100].\nFor instance,\n\nThe deduplication methodology used here is based on vectors representing concept meanings. Suppose the total number of objects is n = 100. We can then imagine the semantics of a concept being represented by a 101-dimensional vector, where each dimension is the truth value of f (n = 100, x) = {\u27e8 n, x \u27e9: (x > 3)} for x = [0, 100].\n\nWe compute such a semantics vector for n = {25, 50, 100} for all concepts in each class, and use the edit distance 5 to remove similar concepts. Two concepts are considered similar if they belong to different classes and have an edit distance <3 between their vectors.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e91e/e91e5d72-0b0e-4653-b019-de50a9938fe1.png\" style=\"width: 50%;\"></div>\nAlgorithm 2 Concept deduplication: when two concepts are similar, only remove the one in the more\ncomplex class\nInputs: current concept class this_class, set of all previous concept classes prev_classes\nfunction EDIT_DIST\ndeduped_concepts = this_class\nfor concept in this_class do\nfor prev_concept in prev_classes do\nif EDIT_DIST (concept, prev_concept) < 3 then\nremove concept from deduped_concepts\nend if\nend for\nend for\nreturn deduped_concepts\nPrompt\nLabel\nLet us define a new word, bnik.\nThere are 17 plants. Alice has 13 of the plants.\nDoes Alice have bnik of the plants? No.\nThere are 99 trees. Bob has 7 of the trees.\nDoes Bob have bnik of the trees? Yes.\nThere are 40 tables. Alice owns 36 of the tables.\nDoes Alice own bnik of the tables? No.\nThere are 72 chairs. Bob owns 9 of the chairs.\nDoes Bob own bnik of the chairs? Yes.\nThere are 82 chairs. Bob has 47 of the chairs.\nDoes Bob have bnik of the chairs? No.\nThere are 100 plants. Alice owns 70 of the plants.\nDoes Alice own bnik of the plants? No.\nThere are 56 chairs. Alice owns 3 of the chairs.\nDoes Alice own bnik of the chairs? Yes.\nThere are 56 birds. Bob owns 37 of the birds.\nDoes Bob own bnik of the birds? No.\nThere are 84 tables. Alice owns 12 of the tables.\nDoes Alice own bnik of the tables? Yes.\nThere are 69 bikes. Alice owns 32 of the bikes.\nDoes Alice own bnik of the bikes? Yes.\nThere are 99 apples. Alice has 3 of the apples.\nDoes Alice have bnik of the apples?\nYes\nTable 3: Example of prompts in the dataset. The underlying concept in this example is \u201cless than\nhalf\u201d.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d67/3d673040-1aab-4622-9e11-6663e4c548ec.png\" style=\"width: 50%;\"></div>\n# Lexical items used in prompts\n\n\u2022 nonce word: bnik \u2022 subjects: Alice, Bob \u2022 predicates: has, owns \u2022 objects: \"tables\", \"chairs\", \"apples\", \"bikes\", \"trees\", \"fish\", \"birds\", \"plants\"\n\n# A.7 Compute resources\n\n\n.omblexity\n\nComplexity 3: conjunction / disjunction of primitive operators (x >c1) or (x< c2) (x >c1) and (x< c2)\n\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning for Large Language Models (LLMs) and its correlation with Boolean complexity, drawing parallels with human concept learning.",
        "problem": {
            "definition": "The problem is to determine whether LLMs can infer underlying concepts from labeled examples and to identify the inductive biases in this process.",
            "key obstacle": "The main challenge is understanding the learning biases of LLMs and how they compare to human concept learning, particularly regarding complexity."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that human concept learning exhibits a bias for simplicity, leading to the hypothesis that LLMs may exhibit similar biases.",
            "opinion": "The paper posits that LLMs can learn non-trivial mathematical concepts and that their learning process may mirror human cognitive processes.",
            "innovation": "The primary improvement is the explicit measurement of learning bias for logical simplicity in in-context learning, which has not been thoroughly studied in LLMs before."
        },
        "Theory": {
            "perspective": "The theoretical framework is based on the premise that concepts can be represented in a language of thought (LoT) and that simplicity is a key factor in concept learning.",
            "opinion": "It is assumed that both humans and LLMs prefer simpler conceptual representations when learning.",
            "proof": "The correlation between Boolean complexity and model accuracy was statistically analyzed, demonstrating a significant negative correlation."
        },
        "experiments": {
            "evaluation setting": "The experiments involved two LLM families, Qwen2 and Gemma 2, with varying model sizes. The dataset included prompts with balanced positive and negative examples.",
            "evaluation method": "Models were tested on their accuracy in inferring concepts from prompts, and the average accuracy was plotted against concept complexity."
        },
        "conclusion": "The findings suggest that LLMs exhibit a simplicity bias in concept learning akin to that observed in humans, with implications for future research in this area.",
        "discussion": {
            "advantage": "The paper provides a novel perspective on LLMs' learning processes by linking them to established theories of human concept learning.",
            "limitation": "The study is limited to specific LLMs and a single prompt template, which may not capture the full range of LLM capabilities.",
            "future work": "Future research could explore a wider variety of models, additional prompt templates, and different conceptual domains to validate and extend these findings."
        },
        "other info": [
            {
                "info1": "The complexity of concepts is defined by their minimal description length."
            },
            {
                "info2": {
                    "info2.1": "Deduplication methods were employed to ensure unique concept representations.",
                    "info2.2": "Statistical significance was assessed using Pearson correlation coefficients."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of in-context learning for Large Language Models (LLMs) and its correlation with Boolean complexity, drawing parallels with human concept learning."
        },
        {
            "section number": "1.2",
            "key information": "The significance of in-context learning is highlighted through the exploration of learning biases of LLMs and their comparison to human concept learning."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical framework is based on the premise that concepts can be represented in a language of thought (LoT) and that simplicity is a key factor in concept learning."
        },
        {
            "section number": "3.3",
            "key information": "The primary improvement discussed is the explicit measurement of learning bias for logical simplicity in in-context learning, which has not been thoroughly studied in LLMs before."
        },
        {
            "section number": "6.1",
            "key information": "The study identifies the challenge of understanding the learning biases of LLMs and how they compare to human concept learning, particularly regarding complexity."
        },
        {
            "section number": "7",
            "key information": "The findings suggest that LLMs exhibit a simplicity bias in concept learning akin to that observed in humans, with implications for future research in this area."
        }
    ],
    "similarity_score": 0.7047535535693006,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Minimization of Boolean Complexity in In-Context Concept Learning.json"
}