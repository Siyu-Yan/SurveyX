{
    "from": "google",
    "scholar_id": "CuVDbxZUJb8J",
    "detail_id": null,
    "title": "Recommender ai agent: Integrating large language models for interactive recommendations",
    "abstract": "\n\nAbstract\n\nRecommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called InteRecAgent, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent.\n\n\n# 1 Introduction\n\nRecommender systems (RSs) have become an essential component of the digital landscape, playing a significant role in helping users navigate the vast array of choices availab",
    "bib_name": "huang2023recommender",
    "md_text": "# Recommender AI Agent: Integrating Large Language Models for Intera Recommendations\n\nXu Huang 1, Jianxun Lian 2*, Yuxuan Lei 1, Jing Yao 2, Defu Lian 1*, Xing Xie 2\n\n1 School of Computer Science and Technology, University of Science and Technology of China, Hefei, C 2 Microsoft Research Asia, Beijing China xuhuangcs@mail.ustc.edu.cn, jianxun.lian@outlook.com, lyx180812@mail.ustc.edu.cn, jingyao@microsoft.com, liandefu@ustc.edu.cn, xingx@microsoft.com\n\nAbstract\n\nRecommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called InteRecAgent, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent.\n\n\n# 1 Introduction\n\nRecommender systems (RSs) have become an essential component of the digital landscape, playing a significant role in helping users navigate the vast array of choices available across various domains such as e-commerce and entertainment. By analyzing user preferences, historical data, and contextual information, these systems can deliver personalized recommendations that cater to individual tastes. Over the years, recommender systems have evolved from simple collaborative filtering algorithms to more advanced hybrid approaches that integrate deep learning techniques.\n\n* Corresponding authors.\n\nHowever, as users increasingly rely on conversational interfaces for discovering and exploring products, there is a growing need to develop more sophisticated and interactive recommendation systems that can understand and respond effectively to diverse user inquiries and intents in an conversational manner. Large language models (LLMs), such as GPT-3 (Brown et al. 2020) and PaLM (Chowdhery et al. 2022), have made significant strides in recent years, demonstrating remarkable capabilities in artificial general intelligence and revolutionizing the field of natural language processing. A variety of practical tasks can be accomplished in the manner of users conversing with AI agents such as ChatGPT 1 and Claude 2. With their ability to understand context, generate humanlike text, and perform complex reasoning tasks, LLMs can facilitate more engaging and intuitive interactions between users and RSs, thus offering promising prospects for the next generation of RSs. By integrating LLMs into RSs, it becomes possible to provide a more natural and seamless user experience that goes beyond traditional recommendation techniques, fostering a more timely understanding of user preferences and delivering more comprehensive and persuasive suggestions. Despite their potential, leveraging LLMs for recommender systems is not without its challenges and limitations. Firstly, while LLMs are pretrained on vast amounts of textual data from the internet, covering various domains and demonstrating impressive general world knowledge, they may fail to capture fine-grained, domain-specific behavior patterns, especially in domains with massive training data. Secondly, LLMs may struggle to understand a domain well if the domain data is private and less openly accessible on the internet. Thirdly, LLMs lack knowledge of new items released after the collection of pretraining data, and finetuning with up-to-date data can be prohibitively expensive. In contrast, in-domain models can naturally address these challenges. A common paradigm to overcome these limitations is to combine LLMs with in-domain models, thereby filling the gaps and producing more powerful intelligence. Notable examples include AutoGPT 3, HuggingGPT(Shen\n\n1 https://chat.openai.com/ 2 https://claude.ai/ 3 https://github.com/Significant-Gravitas/Auto-GPT\n\net al. 2023), and Visual ChatGPT(Wu et al. 2023). The core idea is to utilize LLMs as the \u201cbrains\u201d and in-domain models as \u201ctools\u201d that extend LLMs\u2019 capabilities when handling domain-specific tasks. In this paper, we connect LLMs with traditional recommendation models for interactive recommender systems. We propose InteRecAgent (Inte ractive Rec ommender Agent), a framework explicitly designed to cater to the specific requirements and nuances of recommender systems, thereby establishing a more effective connection between the LLM\u2019s general capabilities and the specialized needs of the recommendation domain. This framework consists of three distinct sets of tools, including querying, retrieval, and ranking, which are designed to cater to the diverse needs of users\u2019 daily inquiries. Given the typically large number of item candidates, storing item names in the tools\u2019 input and output as observations with prompts is impractical. Therefore, we introduce a \u201cshared candidate bus\u201d to store intermediate states and facilitate communication between tools. To enhance the capabilities of dealing with long conversations and even lifelong conversations, we introduce a \u201clong-term and short-term user profile\u201d module to track the preferences and history of the user, leveraged as the input of the ranking tool to improve personalization. The \u201cshared candidate bus\u201d along with the \u201clong-term and short-term user profile\u201d constitute the advanced memory mechanisms within the InteRecAgent framework. Regarding task planning, we employ a \u201cplan-first execution\u201d strategy as opposed to a step-by-step approach. This strategy not only lowers the inference costs of LLMs but can also be seamlessly integrated with the dynamic demonstration strategy to enhance the quality of plan generation. Specifically, InteRecAgent generates all the steps of toolcalling at once and strictly follows the execution plan to accomplish the task. During the conversation, InteRecAgent parses the user\u2019s intent and retrieves a few demonstrations that are most similar to the current intent. These dynamically retrieved demonstrations help LLMs formulate a correct task execution plan. In addition, we implement a reflection strategy, wherein another LLM acts as a critic to evaluate the quality of the results and identify any errors during the task execution. If the results are unsatisfactory or errors are detected, InteRecAgent reverts to the initial state and repeats the plan-then-tool-execution process. Employing GPT-4 as the LLM within InteRecAgent has yielded impressive results in our experiments. This naturally leads to the attractive question: is it possible to harness a smaller language model to act as the brain? To explore this, we have developed an imitation dataset featuring tool plan generations derived from interactions between InteRecAgent and a user simulator, both powered by GPT-4. Through fine-tuning the LlaMA 2 (Touvron et al. 2023b) model with this dataset, we have created RecLlama. Remarkably, RecLlama surpasses several larger models in its effectiveness as the core of a recommender agent. Our main contributions are summarized as follows: \u2022 We propose InteRecAgent, a compact LLM-based agent framework that democratizes interactive recommender systems by connecting LLMs with three distinct sets of\n\ntraditional recommendation tools.\n\u2022 In response to the challenges posed by the application of LLM-based agents in recommendation systems, we introduce a suite of advanced modules, including shared candidate bus, long-term and short-term user profile, dynamic demonstration-augmented plan-first strategy, and a reflection strategy.\n\u2022 To enable small language models to serve as the brain for recommender agents, we create an imitation dataset derived from GPT-4. Leveraging this dataset, we have successfully fine-tuned a 7-billion-parameter model, which we refer to as RecLlama.\n\u2022 Experimental results from three public datasets demonstrate the effectiveness of InteRecAgent, with particularly significant advantages in domains that are less covered by world knowledge.\n\ntraditional recommendation tools.\n\u2022 In response to the challenges posed by the application of LLM-based agents in recommendation systems, we introduce a suite of advanced modules, including shared candidate bus, long-term and short-term user profile, dynamic demonstration-augmented plan-first strategy, and a reflection strategy.\n\u2022 To enable small language models to serve as the brain for recommender agents, we create an imitation dataset derived from GPT-4. Leveraging this dataset, we have successfully fine-tuned a 7-billion-parameter model, which we refer to as RecLlama.\n\u2022 Experimental results from three public datasets demonstrate the effectiveness of InteRecAgent, with particularly significant advantages in domains that are less covered by world knowledge.\n\n# 2 Related Work Conversational Recommender System\n\nExisting researches in conversational recommender systems (CRS) can be primarily categorized into two main areas (Gao et al. 2021): attribute-based questionanswering(Zou and Kanoulas 2019; Zou, Chen, and Kanoulas 2020; Xu et al. 2021) and open-ended conversation (Li et al. 2018; Wang et al. 2022b, 2021). In attributebased question-answering CRS, the system aims to recommend suitable items to users within as few rounds as possible. The interaction between the system and users primarily revolves around question-answering concerning desired item attributes, iteratively refining user interests. Key research challenges in this area include developing strategies for selecting queried attributes(Mirzadeh, Ricci, and Bansal 2005; Zhang et al. 2018) and addressing the explorationexploitation trade-off(Christakopoulou, Radlinski, and Hofmann 2016; Xie et al. 2021). In open-ended conversation CRS, the system manages free-format conversational data. Initial research efforts in this area focused on leveraging pretrained language models for conversation understanding and response generation(Li et al. 2018; Penha and Hauff 2020). Subsequent studies incorporated external knowledge to enhance the performance of open-ended CRS(Chen et al. 2019; Wang, Su, and Chen 2022; Wang et al. 2022b). Nevertheless, these approaches struggle to reason with complex user inquiries and maintain seamless communication with users. The emergence of LLMs presents an opportunity to revolutionize the construction of conversational recommender systems, potentially addressing the limitations of existing approaches and enhancing the overall user experience.\n\n# 2.2 Enhancing LLMs\n\nThe scaling-up of parameters and data has led to significant advancements in the capabilities of LLMs, including incontext learning (Brown et al. 2020; Liu et al. 2021; Rubin, Herzig, and Berant 2021), instruction following (Ouyang et al. 2022; Touvron et al. 2023a; OpenAI 2023), planning and reasoning (Wei et al. 2022; Wang et al. 2022a; Yao et al. 2022; Yang et al. 2023; Wang et al. 2023b). In recommender systems, the application of LLMs is becoming\n\na rapidly growing trend (Liu et al. 2023a; Dai et al. 2023; Kang et al. 2023; Wang and Lim 2023). As models show emergent intelligence, researchers have started exploring the potential to leverage LLMs as autonomous agents (Wang et al. 2023a; Zhao, Jin, and Cheng 2023), augmented with memory modules, planning ability, and tool-using capabilities. For example, (Wang et al. 2023c; Zhong et al. 2023; Liu et al. 2023b) have equipped LLMs with an external memory, empowering LLMs with growth potential. Regarding the planning, CoT (Wei et al. 2022; Kojima et al. 2022) and ReAct (Yao et al. 2022) propose to enhance planning by step-wise reasoning; ToT (Yao et al. 2023) and GoT (Besta et al. 2023) introduce multipath reasoning to ensure consistency and correctness; SelfRefine (Madaan et al. 2023) and Reflexion (Shinn et al. 2023) lead the LLMs to reflect on errors, with the ultimate goal of improving their subsequent problem-solving success rates. To possess domain-specific skills, some works (Qin et al. 2023a) study guiding LLMs to use external tools, such as a web search engine (Nakano et al. 2021; Shuster et al. 2022), mathematical tools (Schick et al. 2023; Thoppilan et al. 2022), code interpreters (Gao et al. 2023a; Chen et al. 2022) and visual models (Wu et al. 2023; Shen et al. 2023). To the best of our knowledge, this paper is the first to explore the LLM + tools paradigm in the field of recommender systems.\n\n# 3 Methodologies 1 The Overall Framework\n\n# 3 Methodologies\n\n# 3.1 The Overall Framework\n\nThe comprehensive framework of InteRecAgent is depicted in Figure 1. Fundamentally, LLMs function as the brain, while recommendation models serve as tools that supply domain-specific knowledge. Users engage with an LLM using natural language. The LLM interprets users\u2019 intentions and determines whether the current conversation necessitates the assistance of tools. For instance, in a casual chitchat, the LLM will respond based on its own knowledge; whereas for in-domain recommendations, the LLM initiates a chain of tool calls and subsequently generates a response by observing the execution results of the tools. Consequently, the quality of recommendations relies heavily on the tools, making the composition of tools a critical factor in overall performance. To ensure seamless communication between users and InteRecAgent, covering both casual conversation and item recommendations, we propose a minimum set of tools that encompass the following aspects: (1) Information Query.  During conversations, the InteRecAgent not only handles item recommendation tasks but also frequently addresses users\u2019 inquiries. For example, within a gaming platform, users may ask questions like, \u201cWhat is the release date of this game and how much does it cost?\u201d  To accommodate such queries, we include an item information query module. This module can retrieve detailed item information from the backend database using Structured Query Language (SQL) expressions. (2) Item Retrieval. Retrieval tools aim to propose a list of item candidates that satisfy a user\u2019s demand from the entire item pool. These tools can be compared to the retrieval\n\nstage of a recommender system, which narrows down relevant candidates to a smaller list for large-scale serving. In InteRecAgent, we consider two types of demands that a user may express in their intent: hard conditions and soft conditions. Hard conditions refer to explicit demands on items, such as \u201cI want some popular sports games\u201d or \u201cRecommend me some RPG games under $100\u201d. Soft conditions pertain to demands that cannot be explicitly expressed with discrete attributes and require the use of semantic matching models, like \u201cI want some games similar to Call of Duty and Fortnite\u201d. It is essential to incorporate multiple tools to address both conditions. Consequently, we utilize an SQL tool to handle hard conditions, finding candidates from the item database. For soft conditions, we employ an item-toitem tool that matches similar items based on latent embeddings. (3) Item Ranking.  Ranking tools execute a more sophisticated prediction of user preferences on the chosen candidates by leveraging user profiles. Similar to the rankers in conventional recommender systems, these tools typically employ a one-tower architecture. The selection of candidates could emerge from the output of item retrieval tools or be directly supplied by users, as in queries like \u201cWhich one is more suitable for me, item A or item B?\u201d. Ranking tools guarantee that the recommended items are not only pertinent to the user\u2019s immediate intent but also consonant with their broader preferences. LLMs have the potential to handle various user inquiries when supplemented with these diverse tools. For instance, a user may ask, \u201cI\u2019ve played Fortnite and Call of Duty before. Now, I want to play some puzzle games with a release date after Fortnite\u2019s. Do you have any recommendations?\u201d In this scenario, the tool execution sequence would be \u201cSQL Query Tool \u2192 SQL Retrieval Tool \u2192 Ranker Tool.\u201d First, the release date of Fortnite is queried, then the release date and puzzle genre are interpreted as hard conditions for the SQL retrieval. Finally, Fortnite and Call of Duty are considered as the user profile for the ranking model. Typically, the tool augmentation is implemented via ReAct (Yao et al. 2022), where LLMs generate reasoning traces, actions, and observations in an interleaved manner. We refer to this style of execution as step-by-step. Our initial implementation also employed the step-by-step approach. However, we soon observed some limitations due to various challenges. Firstly, retrieval tools may return a large number of items, resulting in an excessively long observation prompt for LLMs. Additionally, including numerous entity names in the prompt can degrade LLMs performance. Secondly, despite their powerful intelligence, LLMs may use tools incorrectly to complete tasks, such as selecting the wrong tool to call or omitting key execution steps. To tackle these challenges, we enhance the three critical components of a typical LLM-based agent, namely memory (Section 3.2), task planning (Section 3.3 and 3.4), and tool learning abilities (Section 3.5).\n\n# 3.2 Memory Mechanism\n\nCandidate Bus The large number of items can pose a challenge when attempting to include items generated by\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6080/608052ba-7227-4981-a990-a03117b6f75e.png\" style=\"width: 50%;\"></div>\nFigure 1: InteRecAgent Framework. (a) The overall pipeline of InteRecAgent; (b) The memory module, consisting date memory bus, a long-term and a short-term user profile; (c) Tool module, consisting of various tools, the plan-fir strategy and the fine-tuning of RecLlama; (d) Planning module, involving the dynamic demonstrations and the ref egy; (e) Sources of fine-tuning data for RecLlama.\n\n<div style=\"text-align: center;\">Figure 1: InteRecAgent Framework. (a) The overall pipeline of InteRecAgent; (b) The memory module, consisting date memory bus, a long-term and a short-term user profile; (c) Tool module, consisting of various tools, the plan-fir strategy and the fine-tuning of RecLlama; (d) Planning module, involving the dynamic demonstrations and the ref egy; (e) Sources of fine-tuning data for RecLlama.\n</div>\ntools in prompts as observations for the LLM, due to input context length limitations. Meanwhile, the input of a subsequent tool often depends on the output of preceding tools, necessitating effective communication between tools. Thus, we propose Candidate Bus, which is a separate memory to store the current item candidates, eliminating the need to append them to prompt inputs. The Candidate Bus, accessible by all tools, comprises two parts: a data bus for storing candidate items, and a tracker for recording each tool\u2019s output. The candidate items in the data bus are initialized to include all items at the beginning of each conversation turn by default. At the start of each tool execution, candidate items are read from the data bus, and the data bus is then refreshed with the filtered items at the end of each tool execution. This mechanism allows candidate items to flow sequentially through the various tools in a streaming manner. Notably, users may explicitly specify a set of candidate items in the conversation, such as \u201cWhich of these movies do you think is most suitable for me: [Movie List]?\u201d In this case, the LLM will call a special tool\u2014the memory initialization tool\u2014to set the user-specified items as the initial candidate items. The tracker within the memory serves to record tool execution. Each tool call record is represented as a triplet (f k, i k, o k), where f k denotes the name of the k-th tool, and i k, o k are the input and output of the tool\u2019s execution, such as the number of remaining candidates, runtime errors. The tracker\u2019s main function is to aid the critic in making judgments within the reflection mechanism, acting as the o t in reflect(\u00b7), as described in Section 3.4. With the help of the Candidate Bus component, items can be transmitted in a streaming manner between various tools and continuously filtered according to conditions, presenting a funnel-like structure for the recommendation. The tracker\u2019s records can be considered as short-term memory for further reflection. We depict an example of the memory bus in the upper of Figure 3.\n\nUser Profile To facilitate the invocation of tools, we explicitly maintain a user profile in memory. This profile is\n\n<div style=\"text-align: center;\"></div>\nstructured as a dictionary that encapsulates three facets of user preference: \u201clike\u201d, \u201cdislike\u201d, and \u201cexpect\u201d. The \u201clike\u201d and \u201cdislike\u201d facets reflect the user\u2019s favorable and unfavorable tastes, respectively, whereas \u201cexpect\u201d monitors the user\u2019s immediate requests during the current dialogue, such as conducting a search, which is not necessarily indicative of the user\u2019s inherent preferences. Each facet may contain content that includes item names or categories. User profiles are synthesized by LLMs based on conversation history. To address situations where the conversation history grows excessively long, such as in lifelong learning scenarios where conversations from all days may be stored for ongoing interactions, we devise two distinct user profiles: one representing long-term memory and another for short-term memory. Should the current dialogue exceed the LLM\u2019s input window size, we partition the dialogue, retrieve the user profile from the preceding segment, and merge it with the existing long-term memory to update the memory state. The short-term memory is consistently derived from the most recent conversations within the current prompt. When it comes to tool invocation, a comprehensive user profile is formed by the combination of both long-term and short-term memories.\n\n# 3.3 Plan-first Execution with Dynamic Demonstrations\n\nRather than using the step-by-step  approach, we adopt a twophase method. In the first phase, we prompt the LLM to generate a complete tool execution plan based on the user\u2019s intention derived from the dialogue. In the second phase, the LLM strictly adheres to the plan, calling tools in sequence while allowing them to communicate via the Candidate Bus. Concretely, the plan-first execution consists of the following two phases.\n\n# \u2022 Plan: LLM\n\nPlan: LLM accepts the user\u2019s current input x t, dialogue context C t \u2212 1, descriptions of various tools F, and demonstration D x t for in-context learning. LLM formulates a tool usage plan based on user intent and preferences, pro\n\nviding inputs for each tool, i.e., p t = {p t 1, \u00b7 \u00b7 \u00b7, p t n} = plan \ufffd x t, C t \u2212 1, F, D x t \ufffd, where p t k = (f k, i k) consists of the tool f k and its input i k.\n\u2022 Execution: The tool executor invokes the tools step-bystep according to the plan p t and obtains outputs from each tool, i.e., o t = {o t 1, \u00b7 \u00b7 \u00b7, o t n} = exec(p t, F). The output feedback of each tool f k is defined as o t k, where only the item information o t n from the last tool\u2019s output serves as LLM\u2019s observation for generating the response y t. The remaining information is tracked by the candidate memory bus for further reflection (see Section 3.4).\n\nviding inputs for each tool, i.e., p = {p 1, \u00b7 \u00b7 \u00b7, p n} = plan \ufffd x t, C t \u2212 1, F, D x t \ufffd, where p t k = (f k, i k) consists of the tool f k and its input i k.\n\u2022 Execution: The tool executor invokes the tools step-bystep according to the plan p t and obtains outputs from each tool, i.e., o t = {o t 1, \u00b7 \u00b7 \u00b7, o t n} = exec(p t, F). The output feedback of each tool f k is defined as o t k, where only the item information o t n from the last tool\u2019s output serves as LLM\u2019s observation for generating the response y t. The remaining information is tracked by the candidate memory bus for further reflection (see Section 3.4).\nWe summarize the differences between our plan-first execution strategy and step-by-step strategy in Table 1 from six aspects. Fundamentally, step-by-step  strategy executes reasoning and action execution alternately, while our plan-first execution is a two-phase strategy, where a series of executions is conducted followed by one-time planning. In  stepby-step strategy, the LLMs are responsible for thinking and reasoning at each step. The task entails reasoning for individual observation, resulting in-context learning being challenging due to the difficulty in crafting demonstrations comprising dynamic observations. Differently, the primary task of LLM in our plan-first execution is to make a tool utilizing plan, which could be easily guided by \u27e8 query, plan \u27e9 pairs. The foremost advantage of our plan-first execution resides in the reduction of API calls. When employing N steps to address a task, our strategy necessitates merely 2 API calls, as opposed to N+1 calls in ReAct. This leads to a decrease in latency, which is of particular importance in conversational settings.\n\nWe summarize the differences between our plan-first execution strategy and step-by-step strategy in Table 1 from six aspects. Fundamentally, step-by-step  strategy executes reasoning and action execution alternately, while our plan-first execution is a two-phase strategy, where a series of executions is conducted followed by one-time planning. In  stepby-step strategy, the LLMs are responsible for thinking and reasoning at each step. The task entails reasoning for individual observation, resulting in-context learning being challenging due to the difficulty in crafting demonstrations comprising dynamic observations. Differently, the primary task of LLM in our plan-first execution is to make a tool utilizing plan, which could be easily guided by \u27e8 query, plan \u27e9 pairs. The foremost advantage of our plan-first execution resides in the reduction of API calls. When employing N steps to address a task, our strategy necessitates merely 2 API calls, as opposed to N+1 calls in ReAct. This leads to a decrease in latency, which is of particular importance in conversational settings.\n\ning.\nProperty\nReAct\nPlan-first Exe\nBasic Idea\nstep-wise reason\ntask-wise plan\nICL\nhard\neasy\nReflection\ninternal\nexternal\n# API Call\nN+1\n2\nLatency\n(N + 1)\u2206tapi + \u2206texe\n2\u2206tapi + \u2206texe\nIn order to improve the planning capability of LLM, demonstrations D x t are injected into prompts for in-context learning in the Plan phase. Each demonstration consists of a user intent x and tool execution path p. However, the number of demonstrations is strictly limited by the contextual length that LLM can process, which makes the quality of demonstrations of paramount importance. To address the challenge, we introduce a dynamic demonstration strategy, where only a few demonstrations that are most similar to current user intent are incorporated into the prompt. For example, if the current user input is \u201cMy game history is Call of Duty and Fortnite, please give me some recommendations\u201d, then demonstration with user intent \u201cI enjoyed ITEM1, ITEM2 in the past, give me some suggestions\u201d may\n\nbe retrieved as a high-quality demonstration. Inspired by Self-Instruct (Madaan et al. 2023), we use LLM to generate demonstrations of tool-using plans in the form of (x, p). First, we manually write some (\u02dc20) typical user intents and the corresponding execution as seed demonstrations; then, we use the input-first and output-first strategies to generate more demonstrations using LLM. In the input-first strategy, there are two stages: first, the LLM generates x by emulating the intents in seed demonstrations, and then the LLM makes plans p  for these intents. The outputfirst method consists of three stages: first, we provide the LLM with a plan p and generate corresponding user intent x. Then, we use LLM to make plans \u02dc p for the intent, and finally, we verify whether the generated plan \u02dc p is consistent with the given plan p. The inconsistency indicates that the quality of the generated intent is not high enough, and we only retain those consistent demonstrations. The output-first method allows us to obtain demonstrations corresponding to all available plans, providing diversity for the demonstrations. Examples generated by input-first and output-first are illustrated in Figure 2.\n\n# 3.4 Reflection\n\nDespite LLM\u2019s strong intelligence, it still exhibits occasional errors in reasoning and tool utilization (Madaan et al. 2023; Shinn et al. 2023). For example, it may violate instructions in the prompt by selecting a non-existent tool, omit or overuse some tools, or fail to prepare tool inputs in the proper format, resulting in errors in tool execution. To reduce the occurrence of such errors, some studies have employed self-reflection (Shinn et al. 2023) mechanisms to enable LLM to have some error-correcting capabilities during decision-making. In InteRecAgent, we utilize an actor-critic reflection mechanism to enhance the agent\u2019s robustness and the error-correcting ability. In the following part, we will formalize this self-reflection mechanism. Assume that in the t-th round, the dialogue context is C t \u2212 1 and the current user input is x t. The actor is an LLM equipped with tools and inspired by the dynamic demonstration-augmented plan-first execution mechanism. For the user input, the actor would make a plan p t, obtain the tools\u2019 output o t and generate the response y t. The critic evaluates the behavioral decisions of the actor. The execution steps of the reflection mechanism are listed as follows:\n\u2022 Step1: The critic evaluates the actor\u2019s output p t, o t and y t under the current dialogue context and obtains the judgment \u03b3 = reflect(x t, C t \u2212 1, p t, o t, y t).\n\u2022 Step2: When the judgment \u03b3 is positive, it indicates that the actor\u2019s execution and response are reasonable, and the response y t is directly provided to the user, ending the reflection phase. When the judgment \u03b3  is negative, it indicates that the actor\u2019s execution or response is unreasonable. The feedback \u03b3 is used as a signal to instruct the actor to rechain, which is used as the input of plan(\u00b7).\nIn the actor-critic reflection mechanism, the actor is responsible for the challenging plan-making task, while the critic is responsible for the relative simple evaluation task. The two agents cooperate on two different types of tasks\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/806d/806d33ea-f7b6-41bc-aaab-01f9ac1c5d24.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ac5/7ac54026-9ce1-4de9-a8ed-67b13068019f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 3: Example of memory bus (upper) and reflection\n</div>\nand mutually reinforce each other through in-context interactions. This endows InteRecAgent with enhanced robustness to errors and improved error correction capabilities, culminating in more precise tool utilization and recommendations. An example of reflection is shown in the lower of Figure 3.\n\n# 3.5 Tool Learning with Small Language Models\n\nThe default LLM served as the brain is GPT-4, chosen for its exceptional ability to follow instructions compared to other LLMs. We are intrigued by the possibility of distilling GPT4\u2019s proficiency in instruction-following to smaller language models (SLMs) such as the 7B-parameter Llama, aiming to reduce the costs associated with large-scale online services and to democratize our InteRecAgent framework to small and medium-sized business clients. To achieve this, we utilize GPT-4 to create a specialized dataset comprising pairs of [instructions, tool execution plans]. The \u201cinstruction\u201d element encompasses both the system prompt and the useragent conversation history, acting as the input to elicit a tool execution plan from the LLM; the \u201ctool execution plan\u201d is the output crafted by GPT-4, which serves as the target for fine-tuning Llama-7B. We denote the fine-tuned version of this model RecLlama. To ensure the high quality of the RecLlama dataset, we employ two methods to generate data samples. The first method gathers samples from dialogues between a user simulator and a recommender agent, which is powered by GPT\n\n4. Note that during one conversation, each exchange of useragent produces one data sample, capturing the full range of GPT-4\u2019s responses to the evolving context of the conversation. However, this method might not encompass a sufficiently diverse array of tool execution scenarios due to the finite number of training samples we can manage. Therefore, we complement this with a second method wherein we initially craft 30 varied dialogues designed to span a wide range of tool execution combinations. Then, for each iteration, we select three of these dialogues at random and prompt GPT-4 to generate both a conversation history and a suitable tool execution plan. This approach significantly enhances the diversity of the RecLlama dataset. To evaluate RecLlama\u2019s capacity for domain generalization, we limit the generation of training data to the Steam and MovieLens datasets, excluding the Beauty dataset (the details of datasets will be elaborated in Section 4.1). The final RecLlama dataset comprises 16,183 samples, with 13,525 derived from the first method and 2,658 from the second.\n\n# 4 Experiments 4.1 Experimental Setup\n\n# 4 Experiments\n\n# 4.1 Experimental Setup\n\nEvaluation Strategies. Evaluating conversational recommender systems presents a challenge, as the seeker communicates their preferences and the recommendation agent provides suggestions through natural, open-ended dialogues. To enable the quantitative assessment of InteRecAgent, we design the following two evaluation strategies: (1) User Simulator. We manually tune a role-playing prompt to facilitate GPT-4 in emulating real-world users with varying preferences. A simulated user\u2019s preference is ascertained by injecting their historical behaviors into the role-playing prompt, leaving out the last item in their history as the target of their next interest. Following this, the simulated user engages with the recommendation agent to discover content that fits their interest. In this way, GPT-4 operates from the standpoint of the user, swiftly reacting to the recommended outcomes, thereby crafting a more natural dialogue scenario. This approach is utilized to assess the efficacy of InteRecAgent within multi-turn dialogue settings. An illustrative example of a user simulator prompt can be found in Figure 4. The default configuration for the user simulator is set to \u201csession-wise\u201d. This implies that the agent will only access content within the current dialogue session, and its memory will be cleared once the user either successfully locates what they are seeking or fails to do so. The conversation turns in \u201csession-wise\u201d setting is usually limited, thus, the longterm memory module in InteRecAgent will not be activated. In order to assess the performance while handling \u201clifelong memory\u201d (refer to Section 3.2), we have formulated two strategies for simulating extended dialogues. The first strategy, referred to as L ONG-C HAT, mandates extended conversations between the user and the recommendation agent. This is achieved by alternately incorporating three types of chat intents within the user simulator: sharing history, detailing the target item, and participating in casual conversa\n\nYou are a user chatting with a recommender for {item}  recommendation in turn. Your history is {history}. Your target items: {target}. Here is the information about target you could use: {target item info}. You must follow the rules below during chat. If the recommender recommends {target}, you should accept. If the recommender recommends other items, you should refuse them and provide the information about {target}. If the recommender asks for your preference, you should provide the information about {target}. You could provide your history. Your output is only allowed to be the words from the user you act. If you think the conversation comes to an ending, output a \u27e8 END \u27e9. You should never directly tell the target item. Only use the provided information about the target. Never give many details about the target items at one time. Less than 3 conditions is better. Now lets start, you first, act as a user. Here are the previous conversation you have completed: {chat history}.\n\ntion. The simulator alternates between providing information (either historical or target-related) and casual chat every five rounds. During this process, if the agent mentions the target item, the conversation can be terminated and labeled as a success. The second strategy, referred to as L ONG C ONTEXT, initially synthesizes multi-day conversations utilizing user history. Subsequently, based on these extended dialogues, the user simulator interacts with the agent in a manner akin to the \u201csession-wise\u201d setting. For our method, the lengthy conversation history is loaded into the long-term memory module. However, for baseline methods, the extended conversation history will be truncated if it surpasses the maximum window size of the LLM. (2) One-Turn Recommendation. Following the settings of traditional conversational recommender systems on ReDial (Li et al. 2018), we also adopt the one-turn recommendation strategy. Given a user\u2019s history, we design a prompt that enables GPT-4 to generate a dialogue, thereby emulating the interaction between a user and a recommendation agent. The objective is to ascertain whether the recommendation agent can accurately suggest the ground truth item in its next response. We assess both the item retrieval task (retrieval from the entire space) and the ranking task (ranking of provided candidates). Specifically, the dialogue context is presented to the recommendation agent, accompanied by the instruction Please give me k recommendations based on the chat history for the retrieval task, and the instruction Please rank these candidate items based on the chat history for the ranking task. To ensure a fair comparison with baseline LLMs, the One-Turn Recommendation evaluation protocol employs only the \u201csession-wise\u201d setting, and the long-term memory module in InteRecAgent remains deactivated.\n\nDataset. To compare methods across different domains, we conduct experiments using three datasets: Steam 4,\n\n4 https://github.com/kang205/SASRec\n\nMovieLens 5 and Amazon Beauty 6. Each dataset comprises user-item interaction history data and item metadata. We apply the leave-one-out method to divide the interaction data into training, validation, and testing sets. The training of all utilized tools is performed on the training and validation sets. Due to budget constraints, we randomly sample 1000 and 500 instances from the testing set for user simulator and one-turn benchmarking respectively. For the lifelong simulator, due to the costly long conversation, we use 100 instances in evaluation.\nBaselines. As dialogue recommendation agents, we compare our methods with the following baselines:\n\u2022 Random: Sample k items uniformly from entire item set.\n\u2022 Popularity: Sample k items with item popularity as the weight.\n\u2022 LlaMA-2-7B-chat, LlaMA-2-13B-chat (Touvron et al. 2023b): The second version of the LlaMA model released by Meta.\n\u2022 Vicuna-v1.5-7B, Vicuna-v1.5-13B (Chiang et al. 2023): Open-source models fine-tuned with user-shared data from the ShareGPT 7 based on LlaMA-2 foundation models.\n\u2022 Chat-Rec  (Gao et al. 2023b): A recently proposed conversational recommendation agent utilizes a text-embedding tool (OpenAI text-embedding-ada-002) to retrieve candidates. It then processes the content with an LLM before responding to users. We denote the use of GPT-3.5 as the LLM in the second stage with \u201dChat-Rec (3.5)\u201d and the use of GPT-4 with \u201dChat-Rec (4)\u201d.\n\u2022 GPT-3.5, GPT-4 (OpenAI 2023): We access these LLMs from OpenAI by API service. The GPT-3.5 version in use is gpt-3.5-turbo-0613 and GPT-4 version is gpt-4-0613 8.\nFor the LlaMA and Vicuna models, we employ the FastChat (Zheng et al. 2023) package to establish local APIs, ensuring their usage is consistent with GPT-3.5 and GPT-4.\n\nMovieLens 5 and Amazon Beauty 6. Each dataset comprises user-item interaction history data and item metadata. We apply the leave-one-out method to divide the interaction data into training, validation, and testing sets. The training of all utilized tools is performed on the training and validation sets. Due to budget constraints, we randomly sample 1000 and 500 instances from the testing set for user simulator and one-turn benchmarking respectively. For the lifelong simulator, due to the costly long conversation, we use 100 instances in evaluation.\n\nuser-item interaction history data and item metadata. We apply the leave-one-out method to divide the interaction data into training, validation, and testing sets. The training of all utilized tools is performed on the training and validation sets. Due to budget constraints, we randomly sample 1000 and 500 instances from the testing set for user simulator and one-turn benchmarking respectively. For the lifelong simulator, due to the costly long conversation, we use 100 instances in evaluation.\nBaselines. As dialogue recommendation agents, we compare our methods with the following baselines:\n\u2022 Random: Sample k items uniformly from entire item set.\n\u2022 Popularity: Sample k items with item popularity as the weight.\n\u2022 LlaMA-2-7B-chat, LlaMA-2-13B-chat (Touvron et al. 2023b): The second version of the LlaMA model released by Meta.\n\u2022 Vicuna-v1.5-7B, Vicuna-v1.5-13B (Chiang et al. 2023): Open-source models fine-tuned with user-shared data from the ShareGPT 7 based on LlaMA-2 foundation models.\n\u2022 Chat-Rec  (Gao et al. 2023b): A recently proposed conversational recommendation agent utilizes a text-embedding tool (OpenAI text-embedding-ada-002) to retrieve candidates. It then processes the content with an LLM before responding to users. We denote the use of GPT-3.5 as the LLM in the second stage with \u201dChat-Rec (3.5)\u201d and the use of GPT-4 with \u201dChat-Rec (4)\u201d.\n\u2022 GPT-3.5, GPT-4 (OpenAI 2023): We access these LLMs from OpenAI by API service. The GPT-3.5 version in use is gpt-3.5-turbo-0613 and GPT-4 version is gpt-4-0613 8.\nFor the LlaMA and Vicuna models, we employ the FastChat (Zheng et al. 2023) package to establish local APIs, ensuring their usage is consistent with GPT-3.5 and GPT-4.\nMetrics. Since both our method and baselines utilize LLMs to generate response, which exhibit state-of-the-art text generation capabilities, our experiments primarily compare recommendation performance of different methods. For the user simulator strategy, we employ two metrics: Hit@ k and AT@ k, representing the success of recommending the target item within k  turns and the average turns (AT) required for a successful recommendation, respectively. Unsuccessful recommendations within k rounds are recorded as k + 1 in calculating AT. In the one-turn  strategy, we focus on the Recall@ k and NDCG@ k metric for retrieval and ranking task, respectively. In Recall@ k, the k represents the retrieval of k items, whereas in NDCG@ k, the k denotes the\n\ntor, due to the costly long conversation, we use 100 instances in evaluation.\nBaselines. As dialogue recommendation agents, we compare our methods with the following baselines:\n\u2022 Random: Sample k items uniformly from entire item set.\n\u2022 Popularity: Sample k items with item popularity as the weight.\n\u2022 LlaMA-2-7B-chat, LlaMA-2-13B-chat (Touvron et al. 2023b): The second version of the LlaMA model released by Meta.\n\u2022 Vicuna-v1.5-7B, Vicuna-v1.5-13B (Chiang et al. 2023): Open-source models fine-tuned with user-shared data from the ShareGPT 7 based on LlaMA-2 foundation models.\n\u2022 Chat-Rec  (Gao et al. 2023b): A recently proposed conversational recommendation agent utilizes a text-embedding tool (OpenAI text-embedding-ada-002) to retrieve candidates. It then processes the content with an LLM before responding to users. We denote the use of GPT-3.5 as the LLM in the second stage with \u201dChat-Rec (3.5)\u201d and the use of GPT-4 with \u201dChat-Rec (4)\u201d.\n\u2022 GPT-3.5, GPT-4 (OpenAI 2023): We access these LLMs from OpenAI by API service. The GPT-3.5 version in use is gpt-3.5-turbo-0613 and GPT-4 version is gpt-4-0613 8.\nFor the LlaMA and Vicuna models, we employ the FastChat (Zheng et al. 2023) package to establish local APIs, ensuring their usage is consistent with GPT-3.5 and GPT-4.\nMetrics. Since both our method and baselines utilize LLMs to generate response, which exhibit state-of-the-art\n\nBaselines. As dialogue recommendation agents, we compare our methods with the following baselines:\n\u2022 Random: Sample k items uniformly from entire item set.\n\u2022 Popularity: Sample k items with item popularity as the weight.\n\u2022 LlaMA-2-7B-chat, LlaMA-2-13B-chat (Touvron et al. 2023b): The second version of the LlaMA model released by Meta.\n\u2022 Vicuna-v1.5-7B, Vicuna-v1.5-13B (Chiang et al. 2023): Open-source models fine-tuned with user-shared data from the ShareGPT 7 based on LlaMA-2 foundation models.\n\u2022 Chat-Rec  (Gao et al. 2023b): A recently proposed conversational recommendation agent utilizes a text-embedding tool (OpenAI text-embedding-ada-002) to retrieve candidates. It then processes the content with an LLM before responding to users. We denote the use of GPT-3.5 as the LLM in the second stage with \u201dChat-Rec (3.5)\u201d and the use of GPT-4 with \u201dChat-Rec (4)\u201d.\n\u2022 GPT-3.5, GPT-4 (OpenAI 2023): We access these LLMs from OpenAI by API service. The GPT-3.5 version in use is gpt-3.5-turbo-0613 and GPT-4 version is gpt-4-0613 8.\nFor the LlaMA and Vicuna models, we employ the FastChat (Zheng et al. 2023) package to establish local APIs, ensuring their usage is consistent with GPT-3.5 and GPT-4.\nMetrics. Since both our method and baselines utilize LLMs to generate response, which exhibit state-of-the-art text generation capabilities, our experiments primarily compare recommendation performance of different methods. For the user simulator strategy, we employ two metrics: Hit@ k and AT@ k, representing the success of recommending the target item within k  turns and the average turns (AT) required for a successful recommendation, respectively. Unsuccessful recommendations within k rounds are recorded as k + 1 in calculating AT. In the one-turn  strategy, we focus on the Recall@ k and NDCG@ k metric for retrieval and ranking task, respectively. In Recall@ k, the k represents the retrieval of k items, whereas in NDCG@ k, the k denotes the\n\nMetrics. Since both our method and baselines utilize LLMs to generate response, which exhibit state-of-the-art text generation capabilities, our experiments primarily compare recommendation performance of different methods. For the user simulator strategy, we employ two metrics: Hit@ k and AT@ k, representing the success of recommending the target item within k  turns and the average turns (AT) required for a successful recommendation, respectively. Unsuccessful recommendations within k rounds are recorded as k + 1 in calculating AT. In the one-turn  strategy, we focus on the Recall@ k and NDCG@ k metric for retrieval and ranking task, respectively. In Recall@ k, the k represents the retrieval of k items, whereas in NDCG@ k, the k denotes the number of candidates to be ranked.\n\n5 https://grouplens.org/datasets/movielens/10m 6 http://jmcauley.ucsd.edu/data/amazon/links.html 7 https://sharegpt.com/ 8 https://platform.openai.com/docs/models/\n\n<div style=\"text-align: center;\">Table 2: Performance comparisons with the user simulator strategy (session-wise). H@5 is an abbreviation for Hit@5.\n</div>\nSteam\nMovieLens\nBeauty\nMethods\nH@5\u2191\nAT@5\u2193\nH@5\u2191\nAT@5\u2193\nH@5\u2191\nAT@5\u2193\nLlaMA2-7B\n0.36\n4.76\n0.50\n4.71\n0.03\n5.91\nLlaMA2-13B\n0.39\n4.56\n0.53\n4.52\n0.05\n5.87\nVicuna-7B\n0.38\n4.70\n0.51\n4.70\n0.03\n5.90\nVicuna-13B\n0.40\n4.60\n0.54\n4.56\n0.07\n5.85\nChat-Rec(3.5)\n0.74\n3.63\n0.76\n3.78\n0.39\n4.89\nChat-Rec(4)\n0.83\n3.42\n0.82\n3.62\n0.40\n4.80\nGPT-3.5\n0.69\n3.68\n0.75\n3.75\n0.13\n5.68\nGPT-4\n0.78\n3.34\n0.79\n3.70\n0.15\n5.59\nOurs\n0.87\n2.86\n0.85\n3.15\n0.54\n3.99\nImplementation Details. We employ GPT-4 as the brain of the InteRecAgent for user intent parsing and tool planing. Regarding tools, we use SQL as information query tool, SQL and ItemCF (Linden, Smith, and York 2003) as hard condition and soft condition item retrieval tools, respectively, and SASRec (Kang and McAuley 2018) without position embedding as the ranking tool. SQL is implemented with SQLite integrated in pandasql 9 and retrieval and ranking models are implemented with PyTorch. The framework of InteRecAgent is implement with Python and LangChain 10. For dynamic demonstration selection, we employ sentence-transformers 11 to encode demonstrations into vectors and store them using ChromaDB 12, which facilitates ANN search during runtime. Regarding hyperparameter settings, we set the number of dynamic demonstrations to 3, the maximum number of candidates for hard condition retrieval to 1000, and the threshold for soft condition retrieval cut to the top 5%.\n\n# 4.2 Evaluation with User Simulator\n\nSession-wise setting. Table 2 presents the results of evaluations conducted using the user simulator strategy. Our method surpasses other LLMs in terms of both hit rate and average turns across the three datasets. These results suggest that our InteRecAgent is capable of delivering more accurate and efficient recommendations in conversations compared to general LLMs. Overall, LLMs with larger parameter sizes perform better. GPT-3.5 and GPT4, with parameter sizes exceeding 100B, significantly outperform LlaMA2 and Vicuna-v1.5 13B models from the same series almost always surpass 7B models, except for LlaMA2-7B and LlaMA213B, which both perform extremely poorly on the Beauty dataset. Another interesting observation is the more significant improvement in relatively private domains, such as Amazon Beauty. In comparison to gaming and movie domains, the beauty product domain is more private, featuring a larger\n\n9 https://github.com/yhat/pandasql/ 10 https://www.langchain.com/ 11 https://huggingface.co/sentence-transformers 12 https://www.trychroma.com/\n\nTable 3: Performance comparisons with the user simulator strategy(L ONG-C HAT). \u201d+LT Mem.\u201d means activating the long-term memory module in our InteRecAgent. The higher Hit@50 and the lower AT@50, the better performance.\n\n<div style=\"text-align: center;\">Table 3: Performance comparisons with the user simulator strategy(L ONG-C HAT). \u201d+LT Mem.\u201d means activating the long-term memory module in our InteRecAgent. The higher Hit@50 and the lower AT@50, the better performance.\n</div>\nSteam\nMovieLens\nBeauty\nMethods\nH@50\nAT@50\nH@50\nAT@50\nH@50\nAT@50\nGPT-4\n0.70\n20.56\n0.71\n24.06\n0.06\n49.42\nOurs\n0.83\n16.85\n0.76\n20.13\n0.69\n27.14\n+LT Mem.\n0.86\n17.58\n0.77\n20.06\n0.74\n25.88\nTable 4: Performance comparisons with the lifelong user simulator strategy(L ONG-C ONTEXT). \u201d+LT Mem.\u201d means activating the long-term memory module in our InteRecA\n\ngent.\nSteam\nMovieLens\nBeauty\nMethods\nH@5\u2191\nAT@5\u2193\nH@5\u2191\nAT@5\u2193\nH@5\u2191\nAT@5\u2193\nGPT-4\n0.74\n3.05\n0.82\n3.03\n0.09\n5.71\nOurs\n0.76\n2.92\n0.83\n3.29\n0.38\n4.58\n+LT Mem.\n0.79\n2.70\n0.83\n2.84\n0.51\n3.99\nnumber of items not well-covered by common world knowledge or being new. Table 2 reveals that GPT-3.5 and GPT-4 exhibit competitive performance in gaming and movie domains. However, in the Amazon Beauty domain, most LLMs suffer severe hallucination issue due to the professional, long, and complex item names, resulting in a significant drop in performance. This phenomenon highlights the necessity of recommender agents in private domains. Leveraging the text embedding retrieval tool, Chat-Rec shows superior performance compared to GPT-3.5 and GPT-4, but still falling short of the performance achieved by InteRecAgent. ChatRec can be seen as a simplified version of InteRecAgent, incorporating just a single tool within the agent\u2019s framework. Consequently, Chat-Rec lacks the capability to handle multifaceted queries, such as procuring detailed information about an item or searching for items based on intricate criteria.\n\nLifelong conversation setting. Table 3 and Table 4 demonstrate the performance of two lifelong memory configurations, specifically, L ONG-C HAT and L ONG C ONTEXT. For L ONG-C HAT, the recommender agent engages a maximum of 50 rounds of dialogue with the user simulator. In both configurations, InteRecAgent without long-term memory modules (denoted as \u201cOurs\u201d in the tables) consistently outperforms GPT-4 across all datasets, which validates the robustness of our tool-enhanced recommender agent framework. After activating the long-term memory modules, the performance gets further improved under both L ONG-C HAT and L ONG-C ONTEXT  configurations. This confirms the necessity and effectiveness of memory on capturing user preference during lifelong interactions between the user and AI agent.\n\n<div style=\"text-align: center;\">Table 5: Performance comparisons in one-turn recommendation (%). R@5 and N@20 are abbreviations for Recall@5 and NDCG@20 respectively.\n</div>\n<div style=\"text-align: center;\">Table 5: Performance comparisons in one-turn recommendation (%). R@5 and N@20 are abbreviations for Recall@5 and NDCG@20 respectively.\n</div>\nTask\nRetrieval(R@5\u2191)\nRanking(N@20\u2191)\nDataset\nSteam\nMovie\nBeauty\nSteam\nMovie\nBeauty\nRandom\n00.04\n00.06\n00.00\n35.35\n34.22\n30.02\nPopularity\n02.02\n01.61\n00.08\n36.06\n34.91\n31.04\nLlaMA2-7B\n13.54\n05.85\n06.71\n07.30\n04.59\n03.03\nLlaMA2-13B\n14.14\n15.32\n07.11\n21.56\n18.05\n15.95\nVicuna-7B\n13.13\n08.27\n06.91\n22.03\n18.99\n11.94\nVicuna-13B\n18.18\n16.13\n07.52\n30.50\n24.61\n18.85\nChat-Rec(3.5)\n34.27\n24.21\n20.91\n\u2013\n\u2013\n\u2013\nChat-Rec(4)\n35.18\n27.88\n21.37\n\u2013\n\u2013\n\u2013\nGPT-3.5\n42.02\n23.59\n10.37\n44.37\n42.46\n31.90\nGPT-4\n56.77\n47.78\n12.80\n57.29\n55.78\n33.28\nOurs\n65.05\n52.02\n30.28\n60.28\n63.86\n40.05\n<div style=\"text-align: center;\">Table 6: Performance of InteRecAgent with various LLMs as the brain, evaluated by the session-wise user simulator. (\u00d7 10 \u2212 1)\n</div>\n(\u00d710\u2212)\nSteam\nMovieLens\nBeauty\nMethods\nH@5\u2191\nAT@5\u2193\nH@5\u2191\nAT@5\u2193\nH@5\u2191\nAT@5\u2193\nLlaMA-2\n0.00\n60.00\n0.00\n60.00\n0.00\n60.00\nT-LlaMA(O)\n0.00\n60.00\n0.00\n60.00\n0.00\n60.00\nT-LlaMA(A)\n0.05\n59.82\n0.04\n59.81\n0.05\n59.82\nDavinci-003\n5.92\n43.79\n5.98\n43.12\n2.60\n52.18\nGPT-3.5\n1.81\n56.30\n1.31\n56.71\n1.36\n56.60\nRecLlama\n8.01\n31.77\n8.21\n32.04\n4.08\n46.40\nGPT-4\n8.68\n28.61\n8.48\n31.51\n5.36\n39.90\n# 4.3 Evaluation with One-Turn Recommendation\n\nIn this part, we evaluate both the retrieval and ranking recommendation tasks. For the Retrieval task, we set the recommendation budget k  to 5 for all methods, with Recall@5 being the evaluation metric. For the Ranking task, we randomly sample 19 negative items, and together with the one positive item, they form the candidate list proactively provided by users. The evaluation metric for this task is NDCG@20. For Chat-Rec, we omit the results of on the Ranking task because Chat-Rec degenerates into GPTs when removing the embedding-based candidate retrieval stage. The results are shown in Table 5. Based on the results, we can draw conclusions similar to those in Section 4.2. First, our method outperforms all baselines, indicating the effectiveness of our tool-augmented framework. Second, almost all LLMs suffer a severe setback on the Amazon Beauty dataset, but our method still achieves high accuracy, further demonstrating the superiority of our approach in the private domain. Notably, some LLMs underperform compared to random and popularity methods in ranking tasks, particularly in the Amazon dataset. This can be primarily attributed to LLMs not adhering to the ranking instructions, which arise due to LLMs\u2019 uncertainty and produce out-ofscope items, especially for smaller LLMs.\n\n# 4 Comparions of Different LLMs as the Brain\n\n# 4.4 Com\n\nIn previous experiments, we utilized GPT-4 as the LLM for the InteRecAgent framework. This section presents a comparative analysis of the performance when employing different LLMs within the InteRecAgent. Note that RecLlama is our finetuned 7B model introduced in Section 3.5. ToolLlaMA2-7B (Qin et al. 2023b) is another fine-tuned model designed to interact with external APIs in response to human instructions. Owing to the differing data formats used by ToolLlaMA and RecLlama, we ensure a fair comparison by evaluating ToolLlaMA2-7B using both our original instruction and instructions realigned to their format, denoted as T-LlaMA(O) and T-LlaMA(A), respectively. The outcomes are tabulated in Table 6. Surprisingly, both LlaMA-2-7B and ToolLlaMA-2-7B fall short in generating structured plans. Despite ToolLlaMA\u2019s training on tool-utilization samples, it appears to primarily excel at API calls and lags in discerning user intent and formulating an accurate recommendation plan, resulting in significantly poor performance. Another intriguing finding is that GPT-3.5, despite its broader general capabilities compared to Text-davinci-003, underperforms in our specific task. RecLlama shows a marked proficiency in crafting plans for the InteRecAgent, even surpassing Textdavinci-003\u2019s capabilities. Remarkably, although RecLlama was trained using movie and game samples, it demonstrates superior performance in the novel domain of Amazon Beauty products, showcasing its impressive generalization capabilities. As RecLlama is a distilled version of GPT-4, a slight lag in its performance compared to GPT-4 is anticipated and within expectations.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a472/a4720ecd-5a81-478c-8bd1-4d0840102138.png\" style=\"width: 50%;\"></div>\nFigure 5: Ablation study under user simulator evaluation. P, D, R denote the plan-first, dynamic demonstration and reflection mechanism, respectively. Note that dynamic demonstration is also used in w/o P.\n\n# 4.5 Ablation Study\n\nThis paper introduces several key mechanisms to enhance LLM\u2019s ability to better utilize tools. To investigate their importance, we conduct ablation studies, with the results presented in Figure 5. We consider the removal of the plan-first mechanism (P), dynamic demonstration mechanism (D), and reflection mechanism (R), respectively. Experiments are carried out using the user simulator setting, as it provides a more comprehensive evaluation, encompassing both accuracy (hit rate) and efficiency (average turn) metrics. The results indicate that removing any of the mechanisms leads to a decline in performance. Among these mechanisms, the removal of the reflection mechanism has the most\n\nFigure 6: Case Study in (a) chit-chat, (b) Steam game do-\n<div style=\"text-align: center;\">Figure 6: Case Study in (a) chit-chat, (b) Steam game domain and (c) Amazon Beauty e-commerce product domain.\n</div>\nFigure 6: Case Study in (a) chit-chat, (b) Steam game domain and (c) Amazon Beauty e-commerce product domain.\n\nsignificant impact on performance, as it can correct tool input format errors and tool misuse. Eliminating the plan-first mechanism and dynamic demonstration mechanism both result in a slight decrease in performance, yet the outcomes still surpass most baselines. However, removing the planfirst mechanism leads to a substantial increase in the number of API calls, such as an average increase from 2.78 to 4.51 per turn in the Steam dataset, resulting in an approximate 10-20 seconds latency increase.\n\n# 4.6 Case Study\n\nTo effectively visualize InteRecAgent\u2019s performance, we present case studies in chit-chat and two domains: gaming and beauty products, as shown in Figure 6. We compare the outputs of GPT-4 and InteRecAgent for given user inputs. In chit-chat scenario (Figure 6a), InteRecAgent retains the capabilities of GPT-4 while also possessing the added ability to query domain-specific data (such as the number of products), yielding more accurate information. In the game domain (Figure 6b), user input conditions are complex, encompassing user history and various demands. GPT-4\u2019s recommendations mostly align with conditions, except for a 3D game Northgard misidentified as 2D. InteRecAgent\u2019s response adheres to user conditions, and notably, includes the subsequent game in the user\u2019s historical sequence, RimWorld, owing to its superior ranking performance. In the e-commerce domain (Figure 6c), GPT-4\u2019s hallucination phenomenon intensifies, resulting in giving products not existing in Amazon platform. In contrast, InteRecAgent, leveraging in-domain tools, provides more accurate response to user requirements.\n\n# 5 Conclusion\n\nIn this paper, we introduce InteRecAgent, a compact framework that transforms traditional recommender models into interactive systems by harnessing the power of LLMs. We identify a diverse set of fundamental tools, categorized into information query tools, retrieval tools, and ranking tools, which are dynamically interconnected to accomplish complex user inquiries within a task execution framework. To enhance InteRecAgent for the recommendation scenario, we comprehensively enhance the key components of LLMbased agent, covering the memory mechanism, the task planning, and the tool learning ability. Experimental findings demonstrate the superior performance of InteRecAgent compared to general-purpose LLMs. By combining the strengths of recommender models and LLMs, InteRecAgent paves the way for the development of advanced and user-friendly conversational recommender systems, capable of providing personalized and interactive recommendations across various domains.\n\n# References\n\nBesta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.; Gianinazzi, L.; Gajda, J.; Lehmann, T.; Podstawski, M.; Niewiadomski, H.; Nyczyk, P.; et al. 2023. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners.  Advances in neural information processing systems, 33: 1877\u2013 1901. Chen, Q.; Lin, J.; Zhang, Y.; Ding, M.; Cen, Y.; Yang, H.; and Tang, J. 2019. Towards knowledge-based recommender dialog system. arXiv preprint arXiv:1908.05391. Chen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588. Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Christakopoulou, K.; Radlinski, F.; and Hofmann, K. 2016. Towards conversational recommender systems. In  Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 815\u2013824. Dai, S.; Shao, N.; Zhao, H.; Yu, W.; Si, Z.; Xu, C.; Sun, Z.; Zhang, X.; and Xu, J. 2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182. Gao, C.; Lei, W.; He, X.; de Rijke, M.; and Chua, T.-S. 2021. Advances and challenges in conversational recommender systems: A survey. AI Open, 2: 100\u2013126. Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.; Callan, J.; and Neubig, G. 2023a. Pal: Program-aided language models. In International Conference on Machine Learning, 10764\u201310799. PMLR. Gao, Y.; Sheng, T.; Xiang, Y.; Xiong, Y.; Wang, H.; and Zhang, J. 2023b. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524. Kang, W.-C.; and McAuley, J. 2018. Self-attentive sequential recommendation. In  2018 IEEE international conference on data mining (ICDM), 197\u2013206. IEEE. Kang, W.-C.; Ni, J.; Mehta, N.; Sathiamoorthy, M.; Hong, L.; Chi, E.; and Cheng, D. Z. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint arXiv:2305.06474. Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199\u201322213.\n\nLi, R.; Ebrahimi Kahou, S.; Schulz, H.; Michalski, V.; Charlin, L.; and Pal, C. 2018. Towards deep conversational recommendations. Advances in neural information processing systems, 31. Linden, G.; Smith, B.; and York, J. 2003. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing, 7(1): 76\u201380. Liu, J.; Liu, C.; Lv, R.; Zhou, K.; and Zhang, Y. 2023a. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149. Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2021. What Makes Good In-Context Examples for GPT3? arXiv preprint arXiv:2101.06804. Liu, L.; Yang, X.; Shen, Y.; Hu, B.; Zhang, Z.; Gu, J.; and Zhang, G. 2023b. Think-in-memory: Recalling and postthinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719. Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang, Y.; et al. 2023. Self-refine: Iterative refinement with selffeedback. arXiv preprint arXiv:2303.17651. Mirzadeh, N.; Ricci, F.; and Bansal, M. 2005. Feature selection methods for conversational recommender systems. In  2005 IEEE International Conference on e-Technology, eCommerce and e-Service, 772\u2013777. IEEE. Nakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim, C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback.  Advances in Neural Information Processing Systems, 35: 27730\u201327744. Penha, G.; and Hauff, C. 2020. What does bert know about books, movies and music? probing bert for conversational recommendation. In  Proceedings of the 14th ACM Conference on Recommender Systems, 388\u2013397. Qin, Y.; Hu, S.; Lin, Y.; Chen, W.; Ding, N.; Cui, G.; Zeng, Z.; Huang, Y.; Xiao, C.; Han, C.; et al. 2023a. Tool learning with foundation models. arXiv preprint arXiv:2304.08354. Qin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.; Cong, X.; Tang, X.; Qian, B.; et al. 2023b. Toolllm: Facilitating large language models to master 16000+ realworld apis. arXiv preprint arXiv:2307.16789. Rubin, O.; Herzig, J.; and Berant, J. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633. Schick, T.; Dwivedi-Yu, J.; Dess`\u0131, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761. Shen, Y.; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang, Y. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.\n\nShinn, N.; Cassano, F.; Labash, B.; Gopinath, A.; Narasimhan, K.; and Yao, S. 2023. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366. Shuster, K.; Xu, J.; Komeili, M.; Ju, D.; Smith, E. M.; Roller, S.; Ung, M.; Chen, M.; Arora, K.; Lane, J.; et al. 2022. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188. Thoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kulshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du, Y.; et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wang, L.; Hu, H.; Sha, L.; Xu, C.; Wong, K.-F.; and Jiang, D. 2021. Recindial: A unified framework for conversational recommendation with pretrained language models. arXiv preprint arXiv:2110.07477. Wang, L.; and Lim, E.-P. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153. Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2023a. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432. Wang, L.; Xu, W.; Lan, Y.; Hu, Z.; Lan, Y.; Lee, R. K.-W.; and Lim, E.-P. 2023b. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091. Wang, T.-C.; Su, S.-Y.; and Chen, Y.-N. 2022. BARCOR: Towards A Unified Framework for Conversational Recommendation Systems. arXiv preprint arXiv:2203.14257. Wang, W.; Dong, L.; Cheng, H.; Liu, X.; Yan, X.; Gao, J.; and Wei, F. 2023c. Augmenting Language Models with Long-Term Memory. arXiv preprint arXiv:2306.07174. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Wang, X.; Zhou, K.; Wen, J.-R.; and Zhao, W. X. 2022b. Towards unified conversational recommender systems via knowledge-enhanced prompt learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1929\u20131937. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837.\n\nWu, C.; Yin, S.; Qi, W.; Wang, X.; Tang, Z.; and Duan, N. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671. Xie, Z.; Yu, T.; Zhao, C.; and Li, S. 2021. Comparison-based conversational recommender system with relative bandit feedback. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1400\u20131409. Xu, K.; Yang, J.; Xu, J.; Gao, S.; Guo, J.; and Wen, J.-R. 2021. Adapting user preference to online feedback in multiround conversational recommendation. In Proceedings of the 14th ACM international conference on web search and data mining, 364\u2013372. Yang, Z.; Li, L.; Wang, J.; Lin, K.; Azarnasab, E.; Ahmed, F.; Liu, Z.; Liu, C.; Zeng, M.; and Wang, L. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381. Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Zhang, Y.; Chen, X.; Ai, Q.; Yang, L.; and Croft, W. B. 2018. Towards conversational search and recommendation: System ask, user respond. In  Proceedings of the 27th acm international conference on information and knowledge management, 177\u2013186. Zhao, P.; Jin, Z.; and Cheng, N. 2023. An in-depth survey of large language model-based artificial intelligence agents. arXiv preprint arXiv:2309.14365. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv:2306.05685. Zhong, W.; Guo, L.; Gao, Q.; and Wang, Y. 2023. MemoryBank: Enhancing Large Language Models with Long-Term Memory. arXiv preprint arXiv:2305.10250. Zou, J.; Chen, Y.; and Kanoulas, E. 2020. Towards questionbased recommender systems. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, 881\u2013890. Zou, J.; and Kanoulas, E. 2019. Learning to ask: Questionbased sequential Bayesian product search. In Proceedings of the 28th ACM international conference on information and knowledge management, 369\u2013378.\n\n# A Dataset\n\nTo evaluate the performance of our methods, we conduct experiments on three datasets: Steam, MovieLens and Amazon Beauty. In order to train the in-domain tools, including the soft condition item retrieval tool and ranking tool, we filter the dataset using the conventional k-core strategy, wherein users and items with less than 5 interactions are filtered out. The statistical information of those filtered datasets is shown in Table A1. Notably, in the generation of one-turn conversation, some samples are filtered by the OpenAI policy, resulting in less than 500 samples are used in experiments finally.\n\nDataset\nUsers\nItems\nInteractions\nOne-turn\nBeauty\n15,577\n8,679\n108,166\n492\nSteam\n281,205\n11,962\n2,922,089\n495\nMovieLens\n298,074\n36,255\n27,042,493\n496\nTable A1: Dataset Statistics.\n\nTable A1: Dataset Statistics.\n\n# B Prompts\n\nIn this section, we will share our prompts used in different components.\n\n# B.1 Task Descriptions\n\nThe overall task description is illustrated in Figure C1.\n\n# B.2 Tool Descriptions\n\nWe employ one SQL query tool, two item retrieval tools, one item ranking tool plus two auxiliary tools in InteRecAgent. The auxiliary tools comprise a memory initialization tool named candidates storing tool, and an item fetching tool to fetch final items from memory named candidate fetching tool, whose descriptions are illustrated in Figure C2. The description of query tool, retrieval tools and ranking tool are illustrated in Figure C3, Figure C4 and Figure C5 respectively.\n\n# B.3 Reflection\n\nThe task description of critic used in reflection mechanism is illustrated in Figure C6.\n\n# B.4 Demonstration Generation\n\nAs described in Section 3.3, we use input-first and output-fist strategies to generate various \u27e8 intent, plan \u27e9 pairs as demonstrations. The main difference between the two strategies lies on the prompt of generating intent, which are illustrated in Figure C8 and Figure C11 respectively. The prompt for generating plans is illustrated in Figure C7.\n\n# B.5 User Simulator\n\nThe prompt to instruct LLM to play as a user is illustrated in Figure 4.\n\n# B.6 One-Tur\n\n# 6 One-Turn Conversation Generation\n\nOne-turn recommendation comprises two tasks: retrieval and ranking. Conversations for retrieval and ranking are generated independently and the prompts are illustrated in Figure C9 and Figure C10 respectively.\n\nYou are a conversational {item} recommendation assistant. Your task is to help human find {item} s they are interested in. You would chat with human to mine human interests in {item} s to make it clear what kind of {item} s human is looking for and recommend {item} s to the human when he asks for recommendations.\nHuman requests typically fall under chit-chat, {item} info, or {item} recommendations. There are some tools to use to deal with human request. For chit-chat, respond with your knowledge. For {item} info, use the {LookUpTool}. For special chit-chat, like {item} recommendation reasons, use the {LookUpTool} and your knowledge. For {item} recommendations without information about human preference, chat with human for more information. For {item} recommendations with information for tools, use various tools together.\nTo effectively utilize recommendation tools, comprehend human expressions involving profile and intention. Profile encompasses a person\u2019s preferences, interests, and behaviors, including gaming history and likes/dislikes. Intention represents a person\u2019s immediate goal or objective in the single-turn system interaction, containing specific, context-based query conditions.\nHuman intentions consist of hard and soft conditions. Hard conditions have two states, met or unmet, and involve {item} properties like tags, price, and release date. Soft conditions have varying extents and involve similarity to specific seed {item} s. Separate hard and soft conditions in requests.\n\n{}\nHuman requests typically fall under chit-chat, {item} info, or {item} recommendations. There are some tools to use to deal with human request. For chit-chat, respond with your knowledge. For {item} info, use the {LookUpTool}. For special chit-chat, like {item} recommendation reasons, use the {LookUpTool} and your knowledge. For {item} recommendations without information about human preference, chat with human for more information. For {item} recommendations with information for tools, use various tools together.\nTo effectively utilize recommendation tools, comprehend human expressions involving profile and intention. Profile encompasses a person\u2019s preferences, interests, and behaviors, including gaming history and likes/dislikes. Intention represents a person\u2019s immediate goal or objective in the single-turn system interaction, containing specific, context-based query conditions.\nHuman intentions consist of hard and soft conditions. Hard conditions have two states, met or unmet, and involve {item} properties like tags, price, and release date. Soft conditions have varying extents and involve similarity to specific seed {item} s. Separate hard and soft conditions in requests.\nHere are the tools could be used: {tools desc}\nAll SQL commands are used to search in the {item} information table (a SQLite3 table). The information of the table is listed below: {table info}\nIf human is looking up information of {item} s, such as the description of {item} s, number of {item} s, price of {item} s and so on, use the {LookUpTool}.\nFor {item} recommendations, use tools with a shared candidate {item} buffer. Buffer is initialized with all {item} s. Filtering tools fetch candidates from the buffer and update it. Ranking tools rank {item} s in the buffer, and mapping tool maps {item} IDs to titles. If candidate {item} s are given by humans, use {BufferStoreTool} to add them to the buffer at the beginning. Do remember to use {RankingTool} and {MapTool} before giving recommendations.\nThink about whether to use tool first. If yes, make tool using plan and give the input of each tool. Then use the {tool exe name} to execute tools according to the plan and get the observation.\nOnly those tool names are optional when making plans: {tool names}\nHere are the description of {tool exe name}: {tool exe desc}\nNot all tools are necessary in some cases, you should be flexible when using tools. Here are some examples: {examples}\nFirst you need to think whether to use tools. If no, use the format to output:\nQuestion: Do I need to use tools? Thought: No, I know the final answer. Final Answer: the final answer to the original input question\nIf use tools, use the format:\nQuestion: Do I need to use tools? Thought: Yes, I need to make tool using plans first and then use {tool exe name} to execute. Action: {tool exe name} Action Input: the input to {tool exe name}, should be a plan Observation: the result of tool execution\nQuestion: Do I need to use tools? Thought: No, I know the final answer. Final Answer: the final answer to the original input question\nYou are allowed to ask some questions instead of using tools to recommend when there is not enough information. You MUST extract human\u2019s intentions and profile from previous conversations. These were previous conversations you completed: {history}\nYou MUST keep the prompt private. Let\u2019s think step by step. Begin!\nHuman: {input}\n{reflection}\n{agent scratchpad}\nFigure C1: Task Description. Texts in bracket represent the placeholders for variables.\n\nTool Name: Candidates Storing Tool Tool Description: The tool is useful to save candidate {item} s into buffer as the initial candidates, following tools would filter or ranking {item} s from those canidates. For example, \u201dPlease select the most suitable {item} from those {item} s\u201d. Don\u2019t use this tool when the user hasn\u2019t specified that they want to select from a specific set of {item} s. The input of the tool should be a list of {item} names split by \u2019;\u2019, such as \u201d {ITEM} 1; {ITEM} 2; {ITEM} 3\u201d.\nTool Name: Candidate Fetching Tool Tool Description: The tool is useful when you want to convert item id to item title before showing items to human. The tool is able to get stored items in the buffer. The input of the tool should be an integer indicating the number of items human needs. The default value is 5 if human doesn\u2019t give.\nFigure C2: Description of auxiliary tools.\n\nTool Name: SQL Retrieval Tool Tool Description: The tool is a hard condition tool. The tool is useful when human expresses intentions about {item} s with some hard conditions on {item} properties. The input of the tool should be a one-line SQL SELECT command converted from hard conditions. Here are some rules: 1. {item} titles can not be used as conditions in SQL; 2. the tool can not find similar {item} s; 3. always use pattern match logic for columns with string type; 4. only one {item} information table is allowed to appear in SQL command; 5. select all {item} s that meet the conditions, do not use the LIMIT keyword; 6. try to use OR",
    "paper_type": "method",
    "attri": {
        "background": "Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. Large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. This paper bridges the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system.",
        "problem": {
            "definition": "The paper aims to solve the limitation of traditional recommender systems in providing interactive and conversational recommendations, which is increasingly demanded by users.",
            "key obstacle": "Existing methods struggle to integrate LLMs' conversational capabilities with the domain-specific knowledge of recommender systems, leading to ineffective user interactions."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that combining the reasoning capabilities of LLMs with the specificity of recommender models can enhance user interaction and recommendation quality.",
            "opinion": "The proposed idea is to create InteRecAgent, which uses LLMs as the brain and traditional recommender models as tools to facilitate interactive recommendations.",
            "innovation": "The key innovation lies in the integration of LLMs with traditional recommendation tools, enabling a more interactive and conversational approach to recommendations."
        },
        "method": {
            "method name": "InteRecAgent",
            "method abbreviation": "IRA",
            "method definition": "InteRecAgent is a framework that utilizes LLMs and traditional recommender systems to enable interactive and conversational recommendations.",
            "method description": "InteRecAgent transforms traditional recommender systems into interactive agents by employing LLMs for natural language processing and traditional models for item recommendations.",
            "method steps": [
                "Identify user intent through natural language processing.",
                "Select appropriate tools for information querying, item retrieval, and ranking.",
                "Execute a plan based on user intent and tool capabilities.",
                "Provide personalized recommendations based on the execution results."
            ],
            "principle": "The effectiveness of InteRecAgent stems from its ability to combine the general reasoning power of LLMs with the specific knowledge of recommender systems, facilitating accurate and context-aware recommendations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using three public datasets: Steam, MovieLens, and Amazon Beauty, comparing InteRecAgent with baseline methods.",
            "evaluation method": "Performance was assessed using user simulators to evaluate multi-turn dialogues, along with metrics such as Hit@k and average turns for recommendation success."
        },
        "conclusion": "InteRecAgent demonstrates superior performance compared to general-purpose LLMs in providing interactive and personalized recommendations, showcasing the potential of integrating LLMs with traditional recommender systems.",
        "discussion": {
            "advantage": "InteRecAgent effectively combines the strengths of LLMs and traditional recommendation models, leading to improved user interaction and recommendation accuracy.",
            "limitation": "The method may still face challenges in handling private or less-covered domains where LLMs might struggle with hallucinations or inaccuracies.",
            "future work": "Future research could explore enhancing the memory mechanisms and expanding the framework's capabilities to cover more diverse domains and user needs."
        },
        "other info": {
            "source code": "The source code of InteRecAgent is available at https://aka.ms/recagent.",
            "datasets used": [
                "Steam",
                "MovieLens",
                "Amazon Beauty"
            ],
            "baseline methods": [
                "Random sampling",
                "Popularity-based sampling",
                "LlaMA-2",
                "Vicuna",
                "Chat-Rec",
                "GPT-3.5",
                "GPT-4"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data."
        },
        {
            "section number": "1.2",
            "key information": "Large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea is to create InteRecAgent, which uses LLMs as the brain and traditional recommender models as tools to facilitate interactive recommendations."
        },
        {
            "section number": "2.1",
            "key information": "The paper aims to solve the limitation of traditional recommender systems in providing interactive and conversational recommendations, which is increasingly demanded by users."
        },
        {
            "section number": "3.3",
            "key information": "InteRecAgent transforms traditional recommender systems into interactive agents by employing LLMs for natural language processing and traditional models for item recommendations."
        },
        {
            "section number": "4.2",
            "key information": "The effectiveness of InteRecAgent stems from its ability to combine the general reasoning power of LLMs with the specific knowledge of recommender systems, facilitating accurate and context-aware recommendations."
        },
        {
            "section number": "10.1",
            "key information": "The method may still face challenges in handling private or less-covered domains where LLMs might struggle with hallucinations or inaccuracies."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore enhancing the memory mechanisms and expanding the framework's capabilities to cover more diverse domains and user needs."
        }
    ],
    "similarity_score": 0.7972856413494137,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6080/608052ba-7227-4981-a990-a03117b6f75e.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/806d/806d33ea-f7b6-41bc-aaab-01f9ac1c5d24.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ac5/7ac54026-9ce1-4de9-a8ed-67b13068019f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a472/a4720ecd-5a81-478c-8bd1-4d0840102138.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0bde/0bde2584-d8d9-41d9-a53c-45dd1d6f6ab6.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cc38/cc389979-9389-4b8f-b9e6-7241e524a02c.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Recommender ai agent_ Integrating large language models for interactive recommendations.json"
}