{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2310.08049",
    "title": "Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability",
    "abstract": " ABSTRACT\nABSTRACT\nWhat is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying the number of in-context examples and task difficulty. We also measure each architecture\u2019s predisposition towards in-context learning when presented with the option to memorize rather than leverage in-context examples. Finally, and somewhat surprisingly, we find that several attention alternatives are sometimes competitive with or better in-context learners than transformers. However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.\narXiv:2310.08049v3\nIn-context learning (ICL) refers to the ability to learn new tasks at inference time, using only inputoutput pair exemplars as guidance. Radford et al. (2019) demonstrate early signs of this ability in GPT-2, a causal transformer (Vaswani et al., 2017). ICL was further popularized by GPT-3 (Brown et al., 2020), a large language model with the same architectural foundation but augmented with greater capacity and trained on large-scale data. By simply adjusting a natural language prompt, it was shown that GPT-3 could adapt to new tasks, such as translation and qu",
    "bib_name": "lee2024attentionrequirediclexploring",
    "md_text": "# IS ATTENTION REQUIRED FOR ICL? EXPLORING THE RELATIONSHIP BETWEEN MODEL ARCHITECTURE AND IN-CONTEXT LEARNING ABILITY\nIS ATTENTION REQUIRED FOR ICL? EXPLORING THE RELATIONSHIP BETWEEN MODEL ARCHITECTURE AND IN-CONTEXT LEARNING ABILITY\nIvan Lee, Nan Jiang, Taylor Berg-Kirkpatrick University of California, San Diego {iylee,n3jiang,tberg}@ucsd.edu\nIvan Lee, Nan Jiang, Taylor Berg-Kirkpatrick University of California, San Diego {iylee,n3jiang,tberg}@ucsd.edu\n# ABSTRACT\nABSTRACT\nWhat is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying the number of in-context examples and task difficulty. We also measure each architecture\u2019s predisposition towards in-context learning when presented with the option to memorize rather than leverage in-context examples. Finally, and somewhat surprisingly, we find that several attention alternatives are sometimes competitive with or better in-context learners than transformers. However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.\narXiv:2310.08049v3\nIn-context learning (ICL) refers to the ability to learn new tasks at inference time, using only inputoutput pair exemplars as guidance. Radford et al. (2019) demonstrate early signs of this ability in GPT-2, a causal transformer (Vaswani et al., 2017). ICL was further popularized by GPT-3 (Brown et al., 2020), a large language model with the same architectural foundation but augmented with greater capacity and trained on large-scale data. By simply adjusting a natural language prompt, it was shown that GPT-3 could adapt to new tasks, such as translation and question answering, without updating any of its parameters. These findings spurred significant interest in the research community to investigate this curious behavior (Zhao et al., 2021; Min et al., 2022; Liu et al., 2022). Yet, a prevailing uncertainty remains: are large language models genuinely learning from their prompts or simply being conditioned to surface relevant aspects of their training data? To address this, a new line of research emerged that examines ICL in controlled, synthetic environments where task resolution fundamentally depends on prompt utilization (Xie et al., 2021; von Oswald et al., 2022; Garg et al., 2023; Aky\u00a8urek et al., 2023). However, most of these studies anchor their investigations on the assumption that models utilize an internal attention mechanism (as is the case for transformers). Whether attention mechanisms are necessary for in-context learning to emerge remains an open question. Notable exceptions to this assumption include Xie et al. (2021) and Chan et al. (2022) who consider recurrent neural networks alongside transformers. The former finds RNNs and LSTMs fail to learn image classification in the ICL setting. In contrast, the latter demonstrate that LSTMs possess ICL abilities in a synthetic language modeling task, where hidden Markov models generate the data.\n<div style=\"text-align: center;\">Table 1: Examples of our synthetic in-context learning tasks.</div>\nTask\nPrompt\nTarget\nAssociative Recall\na, 1, b, 3, c, 2, b\n3\nLinear Regression\nx1, y1, x2, y2, x3, y3, x4\ny4\n\u2203w such that \u2200i, yi = xi \u00b7 w\nMulticlass Classification x1, b, x2, a, x3, a, x4\nb\nx1, x4 \u223cN(yb, Id)\nx2, x3 \u223cN(ya, Id)\nImage Classification\nl4 E9 E9 l4 l4 E9 l\n4\nbursty training prompt\nL5 G8 E9 B6 O3 l4 e\n2\nnon-bursty training prompt\nf1 c0 c0 f1 f1 c0 c\n0\nevaluation prompt\nLanguage Modeling\nColorless green ideas sleep\nfuriously\nremains uncertain. The community\u2019s focus on attention is understandable given the success of transformers. However, the architecture comes with a number of limitations, such as quadratic time and memory complexity. These limitations spurred research into alternative architectures such as efficient self-attention models (Tay et al., 2022a) and state space models (Gu et al., 2021). If these alternatives are to replace transformers as the dominant model architecture, it is natural to wonder if they are capable of ICL. Moreover, some are designed to handle prompts of arbitrary length, potentially introducing a novel ICL form, constrained only by dataset size rather than inherent architectural limitations. Furthermore, classic architectures such as recurrent neural networks and convolutional neural networks were once the backbone of machine learning research before the introduction of transformers and ICL as a concept. Do these classic architectures inherently lack ICL capabilities, or were they simply constrained by the compute and data available during their heyday. In this study, we set out to address the aforementioned questions. Specifically, we aim to answer the following research questions: Which architectures are capable of ICL, and which exhibit superior ICL performance? Our primary focus lies on the former question. While the latter is more challenging to assess, our experiments provide insights into which families of architectures tend to perform well, even if they do not offer definitive answers. To advance our objectives, we evaluate a diverse range of model architectures that span several design paradigms. This includes both the classical methods previously mentioned and modern approaches such as the transformer and those inspired by state space models. Our assessment covers the ICL capabilities of each architecture over a wide array of synthetic tasks, spanning different modalities and including both classification and regression, as depicted in Table 1. Our specific contributions are as follows:\n\u2022 LARGE-SCALE EMPIRICAL STUDY: We conduct the first large-scale empirical study comparing ICL performance across diverse model architectures, shedding light on their relative strengths and weaknesses. Code is available at https://github.com/ivnle/ synth-icl. \u2022 UNIVERSALITY OF ICL: We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented, lending support to the position that ICL is not exclusive to attention-based models.\n\u2022 EMPIRICAL SUCCESS OF ATTENTION ALTERNATIVES: Our findings demonstrate that some attention alternatives not only compete with but, in certain cases, surpass transformers at in-context learning. This suggests that efficiency gains in these architectures do not necessarily come at the expense of performance.\n# 2 SYNTHETIC IN-CONTEXT LEARNING TASKS\nStudying in-context learning in large language models presents inherent challenges. One fundamental question is whether these models are truly learning new predictors during the forward-pass, or whether in-context examples simply focus the model on specific aspects of the knowledge already acquired during gradient-based pretraining. While from a Bayesian perspective this dichotomy represents endpoints of a spectrum (Xie et al., 2021), it nonetheless clouds interpretation of ICL experimental results. To address this concern, a new line of research has emerged that examines ICL in controlled, synthetic environments where task resolution depends fundamentally on prompt utilization (von Oswald et al., 2022; Garg et al., 2023; Aky\u00a8urek et al., 2023). In these settings, models must rely on their prompts to solve tasks, eliminating the possibility of memorization: Models are trained from scratch to take a labeled dataset as input and then predict the result of learning from this data directly in the forward-pass of the resulting model. Thus, each train and test example is a unique learning problem but of a consistent type (e.g. linear regression). In addition to offering a clearer perspective on in-context learning, synthetic tasks have low computational requirements. These decreased barriers allow for more equitable comparisons across model architectures. Utilizing publicly available pretrained models may introduce confounding variables, stemming from disparities in model capacity, training durations, and data quality. By training models from scratch on synthetic tasks, we are given greater control over these factors. Furthermore, a suite of such tasks is a valuable tool for the research community, enabling rapid benchmarking of emerging architectures without the intensive computational overhead typically associated with large language models. For these reasons, we curate a suite of synthetic in-context learning tasks and summarize them in Table 1. The majority of our tasks take the form\n\ufffd \ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd where the goal is to learn function f by observing a prompt, a sequence of input-output pairs (xi, f(xi)), which ends with a query. The model\u2019s objective is to produce an appropriate completion based on the given prompt. We train model M\u03b8 parameterized by \u03b8 to minimize the expected loss over all prompts min E [\u2113(M(P), f(x))] , (1)\nwhere \u2113(\u00b7, \u00b7) is the appropriate loss function for a given task.\n\u00b7 \u00b7 Associative recall (Ba et al., 2016; Fu et al., 2023) is the task of learning key-value mappings from a prompt and can be viewed as the simplest form of in-context learning. Let V be a discrete vocabulary of size k. We consider the class of functions\nwhere f is a bijective mapping. These mappings are created by randomly pairing elements of V without replacement, ensuring each element maps to a unique counterpart. We uniformly sample f from F and x1, ..., xn from V to construct the prompt as P = (x1, f(x1), x2, f(x2), ...xn). Elements of P are mapped to vectors with a simple lookup table, as is standard in language modeling. Linear regression (Garg et al., 2023) is the task of learning a linear function from a prompt. We consider the class of functions\nWe sample x1, . . . , xn and w from the isotropic Gaussian distribution N(0, Id). We then compute each yi = w\u22a4xi and construct the prompt as P = (x1, y1, x2, y2, . . . , xn). Since yi is a scalar, we represent it as a d-dimensional vector, with its first index set to yi and remaining indices set to zero. Multiclass Classification is a clustering task in which the items to be clustered are sampled from k distinct Gaussians. For this task, we use the procedure \u00b5 \u223cU(\u22121, 1)d, for i = 1, . . . , k\n(1)\n# yj \u223cU({1, . . . , k}), for j = 1, . . . , n\nxj \u223cN(\u00b5yj, Id), for j = 1, . . . , n\nto construct the prompt as P = (x1, y1, x2, y2, . . . , xn). Since yj \u2208{1, . . . , k}, we map each cluster label to a d-dimensional vector with a simple lookup table. We set d to 16 in all experiments. To facilitate a clearer understanding, we defer detailed discussions of Image Classification and Language Modeling to Sections 5 and 6, respectively.\n# 3 MODEL ARCHITECTURES\nRecurrent We consider three common variations of recurrent neural networks: Elman (Rumelhart et al., 1986, RNN), long short-term memory (Hochreiter & Schmidhuber, 1997, LSTM), and gated recurrent unit (Cho et al., 2014, GRU). Recurrent neural networks are characterized by their lengthinvariant inference cost and theoretically infinite context size, though empirical findings suggest an upper limit on this context size (Khandelwal et al., 2018). Furthermore, since the introduction of transformers, this class of architecture has seen diminished focus within the community, particularly in the ICL setting. We believe revisiting approaches that have fallen out of favor helps counterbalance the community\u2019s potential over-reliance on a select few contemporary methodologies. Convolutional Representing the class of convolutional neural networks (CNN), we focus on the architectures proposed by Wu et al. (2019): lightweight convolutions (LIGHTCONV) and dynamic convolutions (DYNAMICCONV). These architectures, derived as special cases of depthwise convolutions (SIfre & Mallat, 2014), have demonstrated competitive performance with transformers in specific contexts (Tay et al., 2022b). LIGHTCONV is simply a depthwise CNN with weights normalized across the temporal dimension via a softmax. This design means that, unlike in self-attention, its context window is fixed and the importance placed on context elements does not change across time. To remedy this shortcoming, DYNAMICCONV predicts a different convolution kernel at every time-step. However, the kernel is a function of the current time-step only as opposed to the entire context as in self-attention. Similar to the recurrent class, CNNs exhibit length-invariant inference costs. However, they trade infinite context size for training parallelism. Structured State Space Sequence Models (SSMs) We also examine a category of recently proposed architectures inspired by state space models (Kalman, 1960). These architectures attempt to merge the efficient inference capabilities of RNNs with the parallel training attributes of transformers and CNNs. S4 (Gu et al., 2021) set a new state-of-the-art on long-range sequence modeling, but falls short in language modeling compared to transformers. Subsequently, H3 (Fu et al., 2023), HYENA (Poli et al., 2023), and Mamba (Gu & Dao, 2023) were proposed, each progressively improving upon this language modeling gap. We also include architectures inspired by linear attention (Katharopoulos et al., 2020; Zhai et al., 2021). Specifically, we examine RETNET (Sun et al., 2023) and RWKV (Peng et al., 2023). While not necessarily inspired by state space models, these architectures also strive for efficient inference, parallelizable training, and can be viewed as variants of SSMs. Transformers Finally, we consider two popular autoregressive transformer designs: GPT2 (Radford et al., 2019) and LLAMA2 (Touvron et al., 2023). Their primary differences lie in choice of positional embeddings and activation functions. GPT2 utilizes learned absolute positional embeddings and ReLU activation while LLAMA2 incorporates rotary positional embedding (Su et al., 2022) and SWIGLU activation (Shazeer, 2020). Rotary embeddings endow transformers with both absolute and relative positional information through rotations in complex space. We also perform an ablation study across positional embeddings (or lack thereof) and show our results in Appendix E. Note that we train all models from scratch, adopting only the architectural design choices made by the named models\u2019 authors. In the following sections, we delve into our experimental methods and findings. Section 4 presents our results for linear regression, associative recall, and multiclass classification. We discuss image classification outcomes in Section 5, and conclude with our language modeling results in Section 6.\n# 4 LEARNING TO LEARN (IN-CONTEXT)\nIn our initial experiments, we evaluate the capacity of various architectures to in-context learn associative recall, multiclass classification, and linear regression. Results are shown in Figure 1 and experimental details are shown in Appendix A.1. Besides confirming the existence of ICL ability, we are particularly interested in measuring statistical efficiency\u2014which models make better use of a fixed amount of data (in-context examples)\u2014and in determining if our trained models demonstrate consistency, i.e., whether their performance converges in probability to some ceiling.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9de2/9de20f3f-650a-4363-974e-f5875eb51b23.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Multiclass classification</div>\nFigure 1: Evaluating various architectures on associative recall, linear regression, and multiclass classification. We plot test accuracy and mean squared error as a function of the number of in-context examples. A query index of 25 = 32 implies 31 in-context examples, which is also the highest number of in-context examples seen during training (vertical dotted line). Task difficulty increases from left to right. Each line represents the single run that achieved the best validation accuracy or mean squared error at query index 25. See Tables 9, 7, 11 for a tabular view of the same data. See Figure 5 for average performance across training runs. See Appendix B.1 for linear regression experiments with Gaussian noise where we observe trends are largely unchanged relative to the non-noisy setting. Classical baselines (black) are shown for linear regression (ridge regression) and multiclass classification (logistic regression). Why is consistency of interest? First, a proficient learner, irrespective of the ICL setting, is expected to improve its performance given more i.i.d. training data. Consequently, a rise in in-context examples should lead to regular performance improvements. However, it is unclear if this is true in the in-context setting, a query we offer clarity on shortly. Second, the emergence of length-invariant inference architectures, rivaling transformers in task performance, paves the way for ICL with a substantially larger number of in-context examples than what is typically used today. One can imagine\na new paradigm to replace finetuning: adapting pretrained language models to new tasks by utilizing a precomputed (previous) hidden state without parameter updates.\na precomputed (previous) hidden state without parameter updates. All architectures can in-context learn. We first turn our attention to the left most plots in Figure 1, and specifically the region left of the dashed vertical line. Clearly, all architectures successfully in-context learn the three tasks. This provides an existence proof that ICL is not a unique property of transformers. Differences among the architectures becomes more evident as we increase difficulty and take into account their ability to extrapolate to large data sizes than seen in training (right of the dotted vertical line). Which architectures are consistent? Initially, all architectures appear consistent when considering only prompt lengths encountered during training. However, this perception changes when we introduce prompt lengths well beyond those seen during training. Specifically, the performance degradation is most pronounced in the four state space model inspired architectures and the two transformers. Note that this behavior is expected for GPT2 which uses learned positional embeddings, but not for LLAMA2 which uses rotary embeddings. Interestingly, other architectures with recurrent formulations (such as the RNNs, RETNET, and RWKV) do not exhibit such drastic declines. This also holds true for the CNNs, which are inherently limited to finite context lengths. This behavior in CNNs makes intuitive sense, as long range information that may \u201cconfuse\u201d this architecture class are discarded over time. It is possible that, similar to RNNs (Khandelwal et al., 2018), RETNET and RWKV exhibit stronger preference to nearby context relative to the state space model inspired architectures (originally motivated by long sequence modeling) and transformers (which have random access to their entire context). This preference may explain why these architectures are more robust to unseen prompt lengths. Variations in statistical efficiency. The following summary assumes the most difficult setting for all tasks. For associative recall, the top performers were the transformers, H3, HYENA, MAMBA, RETNET, and RWKV when given 31 in-context examples (the longest prompt length seen during training). When extrapolating to longer prompt lengths, HYENA, MAMBA, and RWKV achieved near perfect accuracy, but performance degraded as the number of in-context examples grew. Our ablation over positional embeddings in Table 15 reveal that transformers without positional embeddings and transformers with sinusoidal embeddings are the best at associative recall regardless of prompt length. For linear regression, the transformers, MAMBA, and RETNET achieve near perfect MSE when given 31 in-context examples. Interestingly, these four architectures match the performance of ridge regression. Beyond 31 examples, however, performance quickly deteriorates, with RETNET showing the most robustness to this deterioration. Surprisingly, GRU and LSTM demonstrated competitive performance when extrapolating to unseen prompt lengths. We saw improved extrapolation ability in transformers without positional embeddings (Table 16), but its performance still degraded as the number of examples increased. For multiclass classification, the transformers, all the state space model inspired architectures (except for S4), RETNET and RWKV achieved the best accuracy, surpassing logistic regression. In particular, MAMBA scored the highest accuracy when given 255 in-context examples. We also note that LSTM was competitive with the other architectures but did not achieve a top score. Hyperparameter sensitivity. We now consider average performance for each architecture (Figure 5). Earlier, we found that some RNNs, despite not achieving the best scores, were competitive with modern architectures. However, these performances were difficult to replicate and were isolated to a few lucky combinations of hyperparameters. For associative recall, the transformers, HYENA, MAMBA, and RETNET were consistently strong performers. In particular, MAMBA achieved an average accuracy of 0.96 when given 63 examples. For linear regression, LLAMA2 was the clear leader for prompt lengths seen during training, followed by RETNET. For multiclass classification, LLAMA2, MAMBA, and RWKV were the top performers, followed by H3 and HYENA. Both RWKV and MAMBA improved in performance as prompt lengths increased beyond those seen during training. Interestingly, multiclass classification was the sole task where GPT2 did not perform well on average.\n# 5 THE INFLUENCE OF TRAINING DATA DISTRIBUTIONAL PROPERTIES\nWe now study how the distributional properties of training data can influence ICL. We follow the image classification experiments of Chan et al. (2022) who show ICL emerges when training data\nexhibits particular properties such as burstiness and having large numbers of rarely occurring classes. To manage the number of experiments in this study, we focus exclusively on burstiness, a feature of natural data not found in typical supervised datasets. For example, natural language is temporally \u2018bursty\u201d. That is, a given entity (e.g., word, person) may appear in clusters rather than uniformly across time (Altmann et al., 2009). We train models on a mixture of bursty and non-bursty prompts. See Table 1 and Figure 7 for examples. In bursty prompts, the query class appears 3 times. To prevent the model from simply outputting the most common class in the prompt, a second class also appears 3 times. Bursty prompts can be solved by either leveraging query-label pairs across different training prompts (i.e. memorization) or referring to the in-context examples within prompts (i.e., ICL). For non-bursty prompts, the image-label pairs are drawn randomly and uniformly. This implies there is no incentive for a model to utilize the in-context examples. Note that models now have two options to learn how to classify images: memorization or ICL. This stands in contrast to our experiments in Section 4 where ICL was the only option to solve a task. We want to understand if certain architectures are predisposed towards adopting one of these modes. We evaluate models with standard few-shot sequences containing images from two holdout classes and randomly assign one class to label 0 and the other to label 1. To solve this evaluation task, the model must utilize ICL. Images are sourced from Omniglot (Lake et al., 2019), a dataset of handwritten characters with 1623 classes. We follow Chan et al. (2022) and embed images using a randomly initialized ResNet (He et al., 2015) that trains alongside the evaluated model. Their corresponding labels are mapped to vectors with a simple lookup table. We perform the same sweep outlined in Section 4 resulting in 1512 training runs. We show our results in Figure 2 with supplementary results in Appendix C. We note that all training runs achieved near perfect training accuracy, confirming that models have indeed learned at least one of the two methods of image classification.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c65/8c650ba0-ab00-4fba-87f5-d1d10fe93efe.png\" style=\"width: 50%;\"></div>\ntabular view of the same data. See Figure 8 for training runs that achieved max validation accuracy. Can ICL emerge given purely non-bursty examples? As shown in the first column of Figure 2, no architectures demonstrate ICL ability when all prompts are non-bursty. This is not surprising given that i.i.d in-context examples rarely provide useful information for classifying the query image. Are some architectures predisposed towards ICL? After increasing P(bursty) to 0.5, we find that LLAMA2 and HYENA demonstrate a strong preference towards ICL. It is surprising that GPT2 did not share this predisposition as it is similar in design to LLAMA2. We hypothesize that the rotary positional embeddings employed by LLAMA2 provide a stronger inductive bias towards ICL than the absolute learned positional embeddings used by GPT2. Further increasing P(bursty) to 0.9 reveals that ICL ability emerges consistently in GPT2, MAMBA, H3, and RWKV. Are some architectures predisposed towards memorization? Setting P(bursty) to 1 reveals that a subset of architectures strongly prefer memorization over ICL. In particular, RETNET, S4, the two CNNs and all three RNNs strongly favor memorization. This is not to say that these architectures are incapable of solving this task which we address shortly. We were particularly surprised at the resistance of RETNET to develop ICL ability given that it was one of the top performers in Section 4. ICL emerged in only 2 of 108 training runs for RETNET, and notably, this development occurred after 30K training steps, a window similar to that of the three RNNs. In contrast, the other highperforming architectures from Section 4 developed ICL capabilities in fewer than 10K steps.\nDoes ICL emerge in all architectures? While average accuracy across training runs is depicted in Figure 2, we also present the training runs that achieved the best validation accuracy in Figure 8. In these analyses, we observe that ICL emerges in all evaluated architectures, except for LIGHTCONV. We hypothesize that the absence of a time-step dependent kernel, a feature present in DYNAMICCONV, might be responsible for this outcome. Interestingly, ICL emerges in all three RNNs when P(bursty) is set to 0.9 and 1.0, a finding that contradicts those reported by Chan et al. (2022). Moreover, GRU exhibits the ability to perform ICL even with P(bursty) set as low as 0.5. Given that the RNNs fail at this task on average, we credit this finding to luck with our hyperparameter sweep.\n# TOWARDS IN-CONTEXT LEARNING IN THE REAL WORLD\nUp until now, our experiments have fallen under the few-shot learning concept of ICL where models are prompted with several in-context examples in a next-token-prediction format. We now consider an alternative perspective on ICL, represented in Kaplan et al. (2020) and Olsson et al. (2022). This approach focuses on observing loss at different token indices to measure improvements in language modeling performance as context length grows. Indeed, this is simply what language models are designed to do. However, as the their ability to predict later tokens based on earlier ones improves, they can be utilized in increasingly interesting ways, such as instruction following. We report both in-context learning score and validation loss in Figure 3. Olsson et al. (2022) define in-context learning score as \u201cthe loss of the 500th token in the context minus the average loss of the 50th token in the context, averaged over dataset examples.\u201d One can view ICL score as a simple heuristic to measure the statistical efficiency of a given model. Note that this task is distinct from the large language model setting of in-context learning, where models are trained on language modeling and undergo evaluation with few-shot prompts. We assess models on the same task they were trained on: next-token prediction. See Appendix A.2 for experiment details.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec40/ec40441d-b966-4173-9a1f-3ecc235c5da7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Evaluating architectures on language modeling. Left: Validation loss during training. Middle: ICL score as training progresses. Right: Validation loss as a function of context length.</div>\nMost architectures exhibit an abrupt improvement in ICL score. This same phenomenon was noted by Olsson et al. (2022) in transformers. They discover that induction heads, which they hypothesize as the key mechanism behind ICL, form during the same window where ICL score abruptly improves. Since most architectures considered do not incorporate the concept of an attention head, an intriguing question emerges: What mechanism, analogous to induction heads in transformers, exists in these alternative architectures that facilitate a similar role in ICL? Does ICL score correlate with our previous experiments? In Section 4, our top performers included the two transformers, RWKV, RETNET, H3, HYENA, and MAMBA. Section 5 shares this list (except for RETNET). Consistently, these architectures also achieved the highest ICL scores, led by the transformers and MAMBA. We noted that DYNAMICCONV and LSTM, despite sharing similar validation loss, exhibited a significant gap in ICL score. We find that, when considering their best training runs, LSTM consistently outperformed DYNAMICCONV in all prior tasks and demonstrated superior extrapolation abilities. We observe the same relationship between GRU and LIGHTCONV. While ICL score does appear to correlate with performance in the previous sections, it should not be considered in isolation. For example, S4 and H3 share almost identical ICL scores. However, S4 did not perform as well in our prior tasks as H3 and achieved a lower validation loss on language modeling. Lastly, it is worth mentioning that RNN, despite its poor ICL score, outperformed the two CNNs in image classification when looking at their best training runs (see\nTable 13). This suggests that RNN might be more effective at ICL than the CNNs in scenarios with shorter prompt lengths, as our image classification experiments used prompt lengths of 17 versus 512 in language modeling. We also observe that ICL ability in Section 5 appears to emerge during the same window where ICL score dramatically improves, lending credibility to Olsson et al. (2022)\u2019s use of the metric.\n# 6.1 A SIMPLE FEW-SHOT NATURAL LANGUAGE TASK\nAn interesting property of the dataset we use for language model training (Appendix A.2) is that we can produce relatively small models that still result in fluent language generation. To take advantage of this property, we evaluate architectures on a final ICL task that more resembles those used with large language models: in-context examples are composed using only natural language. Specifically, we compose 200 sentence pairs of the following form: \u201cLilly scrapped her knee. Lily is sad.\u201d Given a target number of in-context examples, for each of the 200 pairs, we randomly sample from the remaining 199 pairs without replacement to assemble 200 prompts. We ensure the two classes (happy and sad) are balanced. For example: \u201cLilly scrapped her knee. Lily is sad. Lilly played with her friends. Lilly is happy. Lilly ate ice cream. Lilly is \u201d. This procedure is repeated 10 times yielding 2000 prompts for each target number of in-context examples.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9ca8/9ca87a39-5aaf-4ba8-a9e3-9afa3dffffa9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Evaluating various architectures on a simple natural language ICL task. We report accuracy as a function of the number of in-context examples. We use the open sourced weights for Llama2-7B and do not fine-tune. All other models are trained from scratch and are approximately 33M parameters (excluding embedding layers). Right: Flipped label setting, i.e., \u201chappy\u201d is replaced with \u201csad\u201d and vice versa. See Figure 9 for normalized accuracy.</div>\nWe also repeat the experiment but flip the classes, i.e., all instances of \u201csad\u201d are replaced with \u201chappy\u201d and vice versa, testing if the model can override semantic priors (Wei et al., 2023). We show our results in Figure 4. Note that we include Llama2-7B as a reference point. We use the open sourced weights for this model as is and do not further train it on TinyStories. Accuracy improves with more examples, but quickly plateaus in the unflipped setting. This pattern held true for all architectures, with the exception of HYENA which showed an initial peak in accuracy, followed by a decline. This decay was also noted in Section 4, when HYENA encountered prompt lengths unseen during training. However, the prompt lengths in the current context fall well within the sequence lengths encountered during their language model training. Given how quickly accuracy plateaus for all architectures, we believe that any gains are due to reallocating probability mass from non-target tokens to both target tokens, rather than truly learning in-context. Most architectures fail in the flipped setting. A notable exception was HYENA, which demonstrated steady improvement up to 5 examples per class before plateauing. This suggests that HYENA, among the architectures we considered, might possess a stronger capability to override its semantic priors. However, we are unable to reconcile this with the observed performance decay in the unflipped setting.\nREFERENCES\n# REFERENCES\nR Ekin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, 2023. Eduardo G. Altmann, Janet B. Pierrehumbert, and Adilson E. Motter. Beyond word frequency: Bursts, lulls, and scaling in the temporal distributions of words. PLoS ONE, 4(11):e7678, November 2009. ISSN 1932-6203. doi: 10.1371/journal.pone.0007678. URL http://dx.doi.org/ 10.1371/journal.pone.0007678. Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent incontext learning in transformers, 4 2022. URL http://arxiv.org/abs/2205.05055v6. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation, 2014. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00b4e. Hungry hungry hippos: Towards language modeling with state space models, 2023. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes, 2023. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Albert Gu, Karan Goel, and Christopher R\u00b4e. Efficiently modeling long sequences with structured state spaces, 10 2021. URL http://arxiv.org/abs/2111.00396v3. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735\u2013 1780, 1997. URL https://api.semanticscholar.org/CorpusID:1915014. Kalman. A new approach to linear filtering and prediction problems. 1960. URL https://api. semanticscholar.org/CorpusID:1242324. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc\u00b8ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural language models use context. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 284\u2013294, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1027. URL https: //aclanthology.org/P18-1027.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural language models use context. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 284\u2013294, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1027. URL https: //aclanthology.org/P18-1027.\nBrenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The omniglot challenge: a 3-year progress report, 2019. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022. deelio-1.10. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 9 2022. URL http://arxiv.org/ abs/2209.11895v1. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 5 2023. URL http://arxiv.org/ abs/2305.13048v1. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00b4e. Hyena hierarchy: Towards larger convolutional language models, 2 2023. URL http://arxiv.org/abs/2302.10866v3. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://www.semanticscholar.org/paper/ Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/ 9405cc0d6169988371b2755e573cc28650d14dfe. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by backpropagating errors. Nature, 323:533\u2013536, 1986. URL https://api.semanticscholar. org/CorpusID:205001834. Noam Shazeer. Glu variants improve transformer, 2020. Laurent SIfre and St\u00b4ephane Mallat. Rigid-motion scattering for texture classification, 2014. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2022. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 7 2023. URL http://arxiv.org/abs/2307.08621v1. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022a. ISSN 0360-0300. doi: 10.1145/3530811. URL https: //doi.org/10.1145/3530811. Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are pre-trained convolutions better than pre-trained transformers?, 2022b.\nBrenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. The omniglot challenge: a 3-year progress report, 2019. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100\u2013114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022. deelio-1.10. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 9 2022. URL http://arxiv.org/ abs/2209.11895v1. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 5 2023. URL http://arxiv.org/ abs/2305.13048v1. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00b4e. Hyena hierarchy: Towards larger convolutional language models, 2 2023. URL http://arxiv.org/abs/2302.10866v3. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://www.semanticscholar.org/paper/ Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/ 9405cc0d6169988371b2755e573cc28650d14dfe. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by backpropagating errors. Nature, 323:533\u2013536, 1986. URL https://api.semanticscholar. org/CorpusID:205001834. Noam Shazeer. Glu variants improve transformer, 2020. Laurent SIfre and St\u00b4ephane Mallat. Rigid-motion scattering for texture classification, 2014. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2022. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 7 2023. URL http://arxiv.org/abs/2307.08621v1. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022a. ISSN 0360-0300. doi: 10.1145/3530811. URL https: //doi.org/10.1145/3530811. Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are pre-trained convolutions better than pre-trained transformers?, 2022b.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u02dcao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 12 2022. URL http://arxiv.org/abs/2212.07677v2. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions, 2019. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference, 2021. URL https://arxiv.org/abs/2111. 02080. Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer, 2021. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12697\u201312706. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/zhao21c.html.\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer, 2021.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12697\u201312706. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/zhao21c.html.\n# A EXPERIMENTAL DETAILS\nA.1 EXPERIMENTAL DETAILS FOR LINEAR REGRESSION, MULTICLASS CLASSIFICATION, AND ASSOCIATIVE RECALL\nWe train each model with prompts containing 32 in-context examples. Training loss is computed for each of the examples and averaged, i.e., models are effectively trained on prompts of varying lengths. We evaluate the trained models on prompts comprising 1024 in-context examples, assessing their ability to extrapolate to unseen prompt lengths. We train each architecture for 100,000 iterations with a batch size of 128. Embedding size is fixed to 64 but we sweep over 3 learning rates, 3 layer depths, 3 seeds, 3 difficulties and 3 tasks, for a total of 243 training runs per architecture (Table 2). Some architectures contain far less parameters per layer than others. For example the largest model trained was RETNET with 530K parameters while the largest GRU was only 200K parameters. To account for this discrepancy, we conduct 81 extra training runs for each of the smaller architectures by adjusting their embedding size and layer depth such that their parameter count is approximately 500K (Table 3).\nWe trained each architecture on 5.12 billion tokens of TinyStories (Eldan & Li, 2023), a synthetic dataset of short stories which contain only words that 3 to 4-year-olds typically understand. The stories are generated by GPT-3.5 and GPT-4 and summary statistics are presented in Table 6. All models were approximately 33 million parameters (excluding embedding layers). Unless otherwise specified in Table 5, we set embedding size to 512 and layers to 8. Additional settings and hyperparameters are shown in Table 4.\nB SUPPLEMENTARY DATA FOR SECTION 4: ASSOCIATIVE RECALL, LINEAR REGRESSION, MULTICLASS CLASSIFICATION\nWe show line plots of average performance on associative recall, linear regression, and multiclass classification across all training runs in Figure 5. Tabular views for linear regression are shown in Tables 9, 10, associative recall in Tables 7, 8, and multiclass classification in Tables 11, 12.\n# B.1 NOISY LINEAR REGRESSION\nWe repeat the linear regression experiments from Section 4 but add progressively more Gaussian noise (\u00b5 = 0, \u03c3 \u2208{0, 0.1, 0.5, 1}) to the outputs of the in-context input-output pairs. As expected, performance degrades with increasing noise. However, the relative performance differences among the architectures remain largely unchanged. Results are shown in Figure 6.\nWe show examples of the sequences used for training and evaluation in Figure 7. The single training run with achieved the best validation accuracy is shown in Figure 8 as as line plot. Tabular views of the experiments in this section are shown in Table 13 and 14.\nNormalized accuracies for the simple in-context learning experiment are shown in Figure 9.\n# E TRANSFORMER POSITIONAL EMBEDDING ABALATIONS\nGiven the poor extrapolation abilities observed in transformers, we decided to test the effects of various positional embeddings, namely: sinusoidal (Vaswani et al., 2017), learned absolute (Radford et al., 2019), rotary (Su et al., 2022), and ALiBi (Press et al., 2022). We also tested the effects of removing positional embeddings entirely. To ensure that each transformer variant is identical in design (except for positional embedding), we use the x-transformers library. Associative recall is shown in Table 15. We observe that performance for prompt lengths seen during training are nearly identical across positional embeddings. However, when considering the best run per model, only sinusoidal and no positional embeddings extrapolate well, reaching and maintaining near perfect accuracy across prompt lengths when |V | = 40. On average (across training runs), sinusoidal and no embeddings still extrapolate better than other options but do not always reach and maintain perfect accuracy. Linear regression is shown in Table 16. Again, performance for prompt lengths seen during training are nearly identical across embedding options. While no training run demonstrated consistency, removing positional embeddings extrapolated better than all other options. Multiclass classification is shown in Table 17. Performance, again was nearly identifical for prompt lenghts seen during training. Differences in extrapolation ability were less pronounced for this task but removing positional embeddings was still the top performer on average. Language modeling is shown in Table 10. While ALiBi did not extrapolate well in the previous experiments, we found that it resulted in the best validation loss for language modeling, followed\nby rotary embeddings. Removing positional embeddings resulted in the worst language modelin validation loss.\n# F PERMUTATION INVARIANCE EXPERIMENTS\nPositional embeddings: whether to use learned absolute positional embeddings or no positional embeddings at all.\nAttention mask: encoder-only vs decoder-only transformer. Note that in both scenarios, the query can attend to all in-context examples. In the encoder-only transformer, each example can attend to all other examples since it does not employ a causal mask. Examples in the decoder-only transformer can only attend to examples to its left. The remaining settings are identical to Section 4 with the following changes: Our hyperparameter sweep covers 2 learning rates, 2 seeds, and 2 layer depths. We train for 50K steps and only take the loss (and evaluate) at the token index 32 (i.e., models are trained to make a single prediction given 31 example pairs and the query). We conducted 768 training runs in total.\nAttention mask: encoder-only vs decoder-only transformer. Note that in both scenarios, the query can attend to all in-context examples. In the encoder-only transformer, each example can attend to all other examples since it does not employ a causal mask. Examples in the decoder-only transformer can only attend to examples to its left.\nToken representation scheme sensitivity: Associative recall and multiclass classification are not sensitive to tokenization schemes. However, we observe that concatenating embeddings in linear regression and image classification resulted in noticeably improved performance. We suspect that it is easier for attention heads to discern in-context inputs from outputs if they initially reside in their own subspace. Removing positional embeddings did not impact performance. This makes intuitive sense as in-context examples in this setting are permutation invariant. For most tasks, encoder-only and decoder-only transformers perform on par. The exception was linear regression where the encoder-only outperformed the decoder-only in the more difficult settings (d=20, 30). For image classification, we observed that ICL emerged in both transformers in very similar windows and followed a similar decay scheduled (as discussed in Section 5). Table 2: Hyperparameters for linear regression, multiclass classification, associative recall, and image classification experiments.\nOptimizer\nAdamW\n\u03b21, \u03b22\n0.90, 0.95\nLearning rate\n{1e-3, 3e-4, 1e-4}\nWarmup schedule\nlinear\nLearning rate schedule\ncosine decay\nTraining iterations\n100,000\nBatch size\n128\nLayers\n{4, 8, 12}\nEmbedding size\n64\nSeed\n{8, 16, 32}\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2606/26064e50-34e9-465c-ae45-0fd49f5bab6f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d98/0d989ba4-c9d6-4659-98ae-e75f468c3ffa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d10/4d107420-f75c-415c-a92d-6034d6850800.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) Multiclass classification</div>\nFigure 5: Evaluating various architectures on in-context learning associative recall, linear regression, and multiclass classification. We plot average test accuracy and mean squared error as a function of the number of in-context examples. A query index of 25 = 32 implies 31 in-context examples, which is also the highest number of in-context examples seen during training (vertical dotted line). Task difficulty increases from left to right. Each line represents an average over all training runs for a given combination of task, difficulty, and architecture. Classical baselines (black) are shown for linear regression (ridge regression) and multiclass classification (logistic regression). See Tables 10, 8, 12 for a tabular view of the same data. See Figure 1 for the training runs that achieved the best performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b141/b1416abb-a71f-4c1d-9eeb-16242cc6108b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) training run with best mean squared error at query index 25</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6761/6761b15d-7d09-44dd-ae2c-a40dada0334b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) average across training runs</div>\nFigure 6: Linear regression with Gaussian noise. We plot mean squared error as a function of number of in-context examples. Ridge regression is shown in black.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/302d/302dbef1-837c-4fbe-ac87-97b9d87d0bc2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Image classification experimental design as outlined in Section 5. Figure taken from Chan et al. (2022) and included here for the reader\u2019s convenience. (a) \u201ctransformer\u201d can be replaced with any of our architectures, e.g., RWKV. (d) This subplot can be safely ignored because we do not evaluate in-weights learning.</div>\nTable 3: Embedding sizes and layers for normalizing parameters to approximately 500K in linear regression, multiclass classification, associative recall, and image classification experiments.\nLayers\nEmbedding Size\nS4\n5\n96\nDYNAMICCONV\n5\n96\nLSTM\n4\n128\nLIGHTCONV\n5\n96\nGRU\n5\n128\nRNN\n5\n224\n<div style=\"text-align: center;\">Table 4: Hyperparameters for language modeling experiments.</div>\nOptimizer\nAdamW\n\u03b21, \u03b22\n0.90, 0.95\nLearning rate\n3e-4\nWarmup schedule\nlinear\nLearning rate schedule\ncosine decay\nTraining iterations\n200,000\nBatch size\n50\nSequence length\n512\nLayers\n8\nEmbedding size\n512\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cc4c/cc4c6fc0-c072-4a69-916e-c710f72c4937.png\" style=\"width: 50%;\"></div>\n\nFigure 8: Measuring the effects training data distributional properties on in-context learning. We plot test accuracy as a function of training steps. P(bursty) indicates the proportion of training samples that were bursty. The remaining samples are non-bursty (i.i.d in-context examples). Each line represents the single run that achieved the best validation accuracy. See Table 13 for a tabular view of the same data. See Figure 2 for average test accuracy (across runs).\nTable 5: Embedding sizes and layers for normalizing parameters to approximately 33M in language modeling experiments.\nLayers\nEmbedding Size\nGPT2\n8\n576\nRWKV\n6\n640\nHYENA\n8\n576\nH3\n7\n1024\nS4\n6\n768\nDYNAMICCONV\n7\n640\nLSTM\n5\n896\nLIGHTCONV\n5\n768\nGRU\n5\n1024\nRNN\n5\n1792\nTraining\nValidation\nTotal stories\n2,119,719\n21,990\nTotal tokens\n512,274,933\n5,151,931\nUnique tokens\n15,200\n8,235\nAverage tokens\n241\n234\nMedian tokens\n208\n205\nStandard deviation\n116\n109\nShortest story\n0\n17\nLongest story\n1,431\n1,183\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1690/16903cb5-f5c4-40b5-ab26-910cb248cebb.png\" style=\"width: 50%;\"></div>\nFigure 9: Evaluating various architectures on a simple natural language ICL task. We report accuracy as a function of the number of in-context examples. Accuracy is normalized with respect to accuracy when given 0 examples. We use the open sourced weights for Llama2-7B and do not finetune. All other models are trained from scratch and are no larger than 33M parameters (excluding embedding layers). Right: Flipped label setting, i.e., \u201chappy\u201d is replaced with \u201csad\u201d and vice versa. See Figure 4 for unnormalized accuracy.\n<div style=\"text-align: center;\">Figure 9: Evaluating various architectures on a simple natural language ICL task. We report accuracy as a function of the number of in-context examples. Accuracy is normalized with respect to accuracy when given 0 examples. We use the open sourced weights for Llama2-7B and do not finetune. All other models are trained from scratch and are no larger than 33M parameters (excluding embedding layers). Right: Flipped label setting, i.e., \u201chappy\u201d is replaced with \u201csad\u201d and vice versa. See Figure 4 for unnormalized accuracy.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/720a/720a3a7c-f281-4a95-9d3a-498085f25a64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Language modeling experiments repeated across various transformer positional embedding options.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c02/3c0239b8-a5a7-4c81-a3f9-5ee9ad426383.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/013d/013d8906-9275-4790-9273-b9fbdd88c51c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ad0f/ad0fbd46-7114-4ed5-a808-45ad5f9f45f1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 7: Associative recall best accuracy. See Figure 1 for line plots of the same data\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a9a0/a9a0d74f-7e3c-4873-8c39-c13a5614e555.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3478/3478cf75-9a76-46d7-8c05-dd3c5e7a902a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e16/4e1698a8-4058-4460-bea5-89002a2924eb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 8: Associative recall average accuracy. See Figure 5 for line plots of the same data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dfca/dfcab77a-dc3f-4b04-8905-1dceb042f7db.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 9: Linear regression best mean squared error. See Figure 1 for line plots of the same data\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8013/8013969f-42e5-4987-bb7a-2745d772daa0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ffd0/ffd00dbe-ef0f-468a-bb66-359ce0326fc2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/175b/175bdf9b-6c95-47bc-876e-90d653f2661b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 10: Linear regression average mean squared error. See Figure 5 for line plots of the same data.\ne 10: Linear regression average mean squared error. See Figure 5 for line p\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b89e/b89e7c67-a9ef-40cb-be6e-dd7d80d6824d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0126/0126c7b3-6a29-4c81-9f9d-507d29fa220b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe76/fe76f7c4-2e96-48f9-a592-52d317ac7f59.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 11: Multiclass classification best accuracy. See Figure 1 for line plots of the same data\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ca4/6ca4138c-07d8-405e-9ba3-dfa094453eab.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4974/497449de-223e-4923-84aa-ad24996e30a9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d229/d2295510-d6d6-46ba-b6a6-37ef12cee0c2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 12: Multiclass classification average accuracy. See Figure 5 for line plots of the same data.\nle 12: Multiclass classification average accuracy. See Figure 5 for line pl\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7f1/b7f1d87d-0b72-42c3-921d-0c0c866d6347.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f80/2f80db41-1126-40a9-8258-68d5dc729345.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/24ae/24ae627a-5a31-4e76-92db-9a492e1d5341.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c54/7c5455f7-a488-4863-a15c-9f49c74bca31.png\" style=\"width: 50%;\"></div>\n\nTable 13: Image classification max accuracy. See Figure 8 for line plots of the same data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6748/6748e19c-bdbe-4bdf-8c1b-ce2ea6798676.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c16c/c16cdd02-6b62-4a06-bbba-fb4e7ac54f1e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/337e/337e0d2d-1d5b-4c09-917d-3ea838f1d1f4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 14: Image classification average accuracy. See Figure 2 for line plots of the same data.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2218/2218e9e3-b879-43ce-bcf5-f8b8d2026ac2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf26/cf2627ab-d876-48ef-8e9e-68841477d8cc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b406/b406c02d-ac12-4938-9222-61bf1bb6b8da.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3791/379155d1-8596-42b3-aed5-ef1c8b6e0b79.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c7ae/c7ae5bb1-f5f1-404e-9d94-1c010448b095.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nTable 15: Associative recall experiments repeated across various transformer positional embedding options.\nTable 15: Associative recall experiments repeated across various transformer positional embedding options.\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca48/ca484ad0-59cc-4ef4-8338-83a53d40a554.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9c0c/9c0c06ed-b245-48af-9f5d-8b6a75593735.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f6b/7f6bbbbe-5100-4155-9338-c69d9930f979.png\" style=\"width: 50%;\"></div>\nTable 16: Linear regression experiments repeated across various transformer positional embedding options.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ffe/6ffe6f0e-243c-4e20-a515-798314d26ce0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f11c/f11c6f97-0d98-4b25-9bfa-ec2e4b443fb3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9447/94472252-7f0f-4020-988b-72c5e281100a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/168a/168a4246-b965-4b15-bf04-d1f364ae6e11.png\" style=\"width: 50%;\"></div>\nTable 17: Multiclass classification experiments repeated across various transformer positional embedding options.\n(a) Linear regression (best run)\n(b) Linear regression (average)\n(c) Associative recall (best run)\n(d) Associative recall (average)\n(e) Multiclass classification (best run)\n(f) Multiclass classification (average)\n(g) Image classification (best run)\n(h) Image classification (average)\n\n<div style=\"text-align: center;\">(a) Linear regression (best run)</div>\n\n<div style=\"text-align: center;\">(c) Associative recall (best run)</div>\n\n<div style=\"text-align: center;\">(e) Multiclass classification (best run)</div>\n<div style=\"text-align: center;\">(g) Image classification (best run)</div>\nFigure 11: Permutation invariance experiments.\n\n<div style=\"text-align: center;\">(b) Linear regression (average)</div>\n\n<div style=\"text-align: center;\">(d) Associative recall (average)</div>\n<div style=\"text-align: center;\">(f) Multiclass classification (average)</div>\n<div style=\"text-align: center;\">(h) Image classification (average)</div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The study addresses the relationship between model architecture and in-context learning (ICL), a behavior observed in large language models like GPT-2 and GPT-3. Previous research has predominantly focused on attention mechanisms, particularly in transformers, raising questions about the necessity of attention for ICL. This benchmark aims to explore a wider range of architectures to understand their capabilities in ICL, filling a gap in existing research.",
            "purpose of benchmark": "The benchmark is intended to evaluate and compare the in-context learning performance across various model architectures, including transformers and attention alternatives, to determine which architectures are capable of ICL and which perform better."
        },
        "problem": {
            "definition": "The benchmark is designed to address the problem of understanding which model architectures can effectively perform in-context learning across a variety of synthetic tasks, simulating real-world scenarios where models learn from examples provided at inference time.",
            "key obstacle": "Existing benchmarks primarily focus on attention-based models, leading to a lack of understanding of the capabilities of alternative architectures, such as recurrent and convolutional neural networks, in performing ICL."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the observation that while transformers have gained prominence due to their performance, other architectures may also exhibit strong ICL capabilities, which have not been adequately explored.",
            "opinion": "The authors believe that understanding ICL across different architectures is crucial for advancing the field and could lead to more efficient model designs that do not rely solely on attention mechanisms.",
            "innovation": "This benchmark introduces a diverse set of model architectures and a suite of synthetic tasks to comprehensively evaluate ICL, representing a significant departure from previous studies that focused mainly on transformer-based models.",
            "benchmark abbreviation": "ICL-Benchmark"
        },
        "dataset": {
            "source": "The dataset consists of synthetic tasks created to evaluate in-context learning capabilities, designed to focus on prompt utilization without the possibility of memorization.",
            "desc": "The dataset includes a variety of synthetic tasks such as associative recall, linear regression, and multiclass classification, allowing for a controlled assessment of model performance.",
            "content": "The dataset includes tasks that require models to learn from input-output pairs, such as key-value mappings for associative recall and regression tasks, with the data structured to facilitate ICL.",
            "size": "5,120,000,000",
            "domain": "Synthetic Learning Tasks",
            "task format": "In-Context Learning"
        },
        "metrics": {
            "metric name": "Accuracy, MSE",
            "aspect": "Model performance in terms of prediction accuracy and mean squared error.",
            "principle": "The metrics were chosen to provide a clear indication of how well models can generalize from in-context examples to unseen tasks, reflecting both accuracy and the quality of predictions.",
            "procedure": "Models are evaluated based on their performance on synthetic tasks using the chosen metrics, comparing results across different architectures and task difficulties."
        },
        "experiments": {
            "model": "The tested models include a variety of architectures such as transformers (GPT-2, LLAMA2), recurrent neural networks (RNN, LSTM, GRU), and convolutional networks (LIGHTCONV, DYNAMICCONV), as well as state space models (S4, H3, HYENA, MAMBA).",
            "procedure": "Models were trained from scratch on synthetic tasks, with hyperparameters adjusted to ensure comparability across architectures. Each model's performance was assessed on a suite of tasks with varying levels of difficulty.",
            "result": "Results indicated that while all architectures can perform ICL, their performance varies significantly across tasks and conditions, with some attention alternatives performing competitively with transformers.",
            "variability": "Variability was accounted for through multiple training runs for each architecture, allowing for statistical analysis of performance across different tasks and configurations."
        },
        "conclusion": "The experiments reveal that in-context learning is not exclusive to attention-based models, with several alternative architectures demonstrating competitive performance. The findings suggest that model architecture plays a critical role in ICL capabilities, highlighting the need for further exploration in this area.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive framework for evaluating ICL across diverse architectures, promoting a broader understanding of model capabilities beyond transformers.",
            "limitation": "The synthetic nature of the tasks may not fully capture the complexities of real-world applications, potentially limiting the generalizability of the findings.",
            "future work": "Future research could explore the integration of real-world tasks with synthetic benchmarks to enhance the understanding of ICL and model performance in practical scenarios."
        },
        "other info": {
            "info1": "The benchmark includes code and datasets available at https://github.com/ivnle/synth-icl.",
            "info2": {
                "info2.1": "The study emphasizes the importance of statistical efficiency in model training.",
                "info2.2": "Further investigations into the role of prompt design and data distributional properties on ICL are suggested."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The study addresses the relationship between model architecture and in-context learning (ICL), a behavior observed in large language models like GPT-2 and GPT-3."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark is intended to evaluate and compare the in-context learning performance across various model architectures, including transformers and attention alternatives."
        },
        {
            "section number": "3.1",
            "key information": "Results indicated that while all architectures can perform ICL, their performance varies significantly across tasks and conditions, with some attention alternatives performing competitively with transformers."
        },
        {
            "section number": "3.2",
            "key information": "The benchmark introduces a diverse set of model architectures and a suite of synthetic tasks to comprehensively evaluate ICL."
        },
        {
            "section number": "4.2",
            "key information": "The study emphasizes the importance of statistical efficiency in model training and suggests further investigations into the role of prompt design and data distributional properties on ICL."
        },
        {
            "section number": "6.4",
            "key information": "The synthetic nature of the tasks may not fully capture the complexities of real-world applications, potentially limiting the generalizability of the findings."
        }
    ],
    "similarity_score": 0.7139126613288201,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Is attention required for ICL_ Exploring the Relationship Between Model Architecture and In-Context Learning Ability.json"
}