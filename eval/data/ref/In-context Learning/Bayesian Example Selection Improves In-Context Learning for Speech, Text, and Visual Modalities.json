{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2404.14716",
    "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities",
    "abstract": "Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the incontext examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes\u2019 theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), incontext examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.",
    "bib_name": "wang2024bayesianexampleselectionimproves",
    "md_text": "# Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities\nSiyin Wang1, Chao-Han Huck Yang2, Ji Wu1, Chao Zhang1 1Tsinghua University, 2NVIDIA Research\n# Abstract\nLarge language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the incontext examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes\u2019 theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), incontext examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.\n16 Jun 2024\narXiv:2404.14716v2 \n# 1 Introduction\nLarge language models (LLMs) (Touvron et al., 2023b; OpenAI, 2023a) have achieved great success on many text-based natural language processing (NLP) tasks. By connecting with extra visual and audio encoders (Sun et al., 2023b; Radford et al., 2023), the resulting multimodal LLMs can also achieve remarkable performance on imagetext and audio-text tasks (Li et al., 2023; OpenAI, 2023b; Tang et al., 2023). With the ability of incontext learning (ICL) (Brown et al., 2020), LLMs can adapt to new tasks easily and efficiently in a training-free manner, to generate output following the prompting paradigm based on a few input-label pairs pre-pended to the test input. The existence of ICL ability has also been verified on image-text and audio-text tasks (Tsimpoukelli et al., 2021; Wang et al., 2023c; Hsu et al., 2023; Pan et al., 2023).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/73bd/73bd2689-6894-46bf-9d32-82218bbbed8e.png\" style=\"width: 50%;\"></div>\nFigure 1: A brief illustration of the proposed Bayesian in-context example selection includes: (i) first randomly selecting k examples; (ii) examining the examples in the datastore through \u201cinverse inference,\u201d where the test input-label pair serves as the in-context example; and (iii) selecting samples with correct label predictions as good examples (colored in blue), considered to have high mutual information interaction with the test input. Although ICL requires no gradient descent and thus does not suffer from the instability caused by stochastic optimisation compared to other testtime adaptation approaches, care still needs to be taken when selecting the in-context examples since they often lead to distinct ICL performance variations (Zhao et al., 2021; Min et al., 2022; Lu et al., 2022b). Prior work on in-context example selection trains an example retrieval module (Rubin et al., 2022; Zhang et al., 2022; Lu et al., 2022a; Wang et al., 2023b), selects close examples in embedding space (Liu et al., 2022; An et al., 2023; Qin et al., 2023), or leverages the feedback of LLMs to score the examples (Su et al., 2022; Nguyen and Wong, 2023; Iter et al., 2023; Mavromatis et al., 2023). While boosting ICL performance, most methods treat in-context examples and test input separately, overlooking their mutual interactions. This paper proposes ByCS (Bayesian in-Context example Selection), a novel in-context example selection approach focusing on mutual information interactions based on the Bayesian formula. Refer to the inference of test input conditioned on in-context examples as ICL inference, and the inference of in-context example\u2019s input based on the test input-label pair as the inverse inference.\nBy introducing inverse inference via Bayes\u2019 theorem, ByCS leverages the inverse inference result to evaluate the quality of each in-context example. Assuming the contextual information interaction is mutual, an accurate inverse inference is likely to result in an accurate inference. Examples with accurate inverse inference results are selected as optimal examples. Extensive experiments across audio, image, and text modalities are conducted to verify the effectiveness and robustness of ByCS, such as ASR, visual question answering (VQA), as well as NLP tasks (including topic classification, sentiment analysis, and text-to-SQL etc). Our main contributions are summarised as follows:\n ByCS, a novel in-context example selection method inspired by Bayes\u2019 theorem, is proposed. To improve the efficiency, the use of a smaller model for fast inverse inference implementation and a ranking-based pre-selection to reduce the number of in-context examples are also proposed in this paper.\n The method is verified using both \u201cdecoderonly ICL\" on NLP tasks and \u201cencoderdecoder\u201d ICL on ASR and VQA. To the best of our knowledge, this is the first work of an in-context example selection method verified across text, audio, and visual modalities as shown in Figure 2.\n# 2 Related Work\nMultimodal ICL. Inspired by the decoder-only ICL in text-based NLP, efforts have been made to extend such a few-shot learning ability to other modalities, in particular image and audio. Frozen (Tsimpoukelli et al., 2021) is the first attempt to exploit ICL ability in the vision-language model (VLM). By using a vision encoder to map the input image to textual tokens in the input embedding space of a frozen text language model, Frozen can handle interleaved image and text input and achieve image-text ICL. Other work manages to improve VLM\u2019s ICL ability by using adapter blocks (Eichenberg et al., 2022), adding blockwise modality fusion structures (Alayrac et al., 2022) and scaling up the model size (Sun et al., 2023a). In audio modality, Borsos et al. (2023) proposed AudioLM, a language model based on quantised audio tokens for audio generation tasks, which exhibits ICL ability for audio continuation. Similarly,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa62/aa62b8f4-9737-4705-9877-2774ab771d88.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c)  VQA  ICL</div>\nFigure 2: Multimodal ICL. Although ICL on different modalities shares the same formula expression, the actual inputs and inference model architectures differ. For ASR ICL on Whisper, the speech is fed into the encoder while the text example is labelled into the decoder, which is aware of speech input through cross-attention with the encoder. For VQA ICL, images are first encoded to the same embedding space of LM\u2019s input, then interleaved images and texts are fed into decoder LM.\nWang et al. (2023a) proposed VALL-E, a controllable text-to-speech synthesis system with ICL ability based on audio and text prompts. Wang et al. (2023c) presented the first ICL work for ASR based on paired speech-text examples, which adapted the Whisper (Radford et al., 2023) model to receive considerable word error rate (WER) reductions on unseen Chinese dialects. Further explorations enabled the recent speech-language models to perform ICL on more speech input tasks through warmup training (Hsu et al., 2023) or speech instruction-tuning (Pan et al., 2023). In-Context Example Selection Methods. Rubin et al. (2022) proposed a scoring LM to retrieve incontext examples using contrastive learning, which can also be trained with reinforced learning algorithms, such as Q-learning (Zhang et al., 2022) and policy gradient (Lu et al., 2022a). Alternatively, examples that are semantically similar to the test input can be selected. Liu et al. (2022) proposed to select the k nearest neighbours (kNN) in the embedding space of the examples. When combining with chain-of-thought (Wei et al., 2022), Qin et al. (2023) proposed to select examples in the embedding space of the reasoning path. LLM feedback is often used in in-context example selection. Iter et al. (2023) selected in-context examples with cross-entropy differences of the fine-tuned model\n<div style=\"text-align: center;\">\ud835\udc36\"!\"#$! = arg max \ud835\udc43(\ud835\udc36!\"#$!|\ud835\udc7f, \ud835\udc80/, \ud835\udc36%&'()) Inverse inference \u2461</div>\nFigure 3: The detailed pipeline of our ByCS method includes: First, conduct the first-round inference to estimate the label of the test input. Then, perform inverse inference on each example in the datastore, where the test input and the estimated label serve as in-context examples. A detailed illustration of inverse inference can be found in Figure 5 in the Appendix. Finally, rank in-context examples by the text similarity between the inverse inference result and the true context label. Examples with high similarity scores are selected due to their high mutual information interaction.\nbased on the assumption that ICL may act as implicit gradient descent (Dai et al., 2022). Nguyen and Wong (2023) identified highly impactful examples according to the proposed influence score. Although ByCS also uses LLM feedback when evaluating the quality of in-context examples through inverse inference, it leverages the text-similarity of the inverse inference results and the corresponding ground-truth labels, in no need of complete output probability distributions which are often not available for commercial LLMs. Wang et al. (2023d) selected optimal in-context examples in the Bayesian framework by viewing LLMs as latent variable models and ICL as latent concept learning. In comparison, ByCS directly extends the ICL inference probability using Bayes\u2019 theorem. Xu and Zhang (2024) selected examples with high discrepancy between the labels and LLM\u2019s outputs when performing question answering. ByCS also selected examples from candidates in a datastore based on LLM\u2019s outputs but computes the mutual information interactions between the in-context examples and test input.\n# 3 Methodology\nAs shown in Figure 3, given a test input X and paired in-context examples (Cinput, Clabel), LLMs predict the most possible answer \u02c6Y by maximising the inference probability P(Y|Cinput, Clabel, X):\n\u02c6Y = arg max P(Y|Cinput, Clabel, X),\n(1)\nwhere Cinput and Clabel are the inputs and labels of different data types in different tasks. Regarding text-based NLP tasks, Cinput and Clabel are referred to as text questions and corresponding answers. Regarding ASR, Cinput and Clabel are speech audio\n# Select examples with max(\ud835\udc78) \u2462\n\ud835\udc44= \ud835\udc46\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc66(\ud835\udc36!\"#$!, \ud835\udc36\"!\"#$!)\nand corresponding text transcriptions. Regarding VQA, Cinput are images and text questions based on the images and Clabel are the text answers. The inference probability can be extended using Bayes\u2019 theorem:\n (2)\nThe likelihood P(Clabel|X, Y, Cinput) is termed as inverse inference probability, since it can be interpreted as the probability of the context label Clabel when the test input-label pair (X, Y) is inversely treated as the in-context example. ByCS is focused on the inverse inference probability and assumes the influence of the prior P(Y|X, Cinput) is subordinate for simplification. In practice, since the ground-truth label Yref of the test input X is not available, the correct likelihood P(Clabel|X, Yref, Cinput) is approximated by P(Clabel|X, \u02c6Y, Cinput), where \u02c6Y is produced by the first-round inference. Specifically,\n First, the first-round inference is performed to produce a hypothesized label \u02c6Y based on the test input X, which can be achieved using decoding rule without any in-context examples by \u02c6Y = arg max P(Y|X). Better performance can be achieved when using the hypothesized label obtained by in-context examples by \u02c6Y = arg max P(Y| \u02dcCinput, \u02dcClabel, X) based on Eqn. (1), where ( \u02dcCinput, \u02dcClabel) is a pair of first-round in-context example selected either randomly or using other example selection methods.\n\u2022 Next, for the datastore with all candidate incontext examples, generate the inverse infer-\nence result in \u02c6Clabel for every candidate example based on the approximated inverse inference probability P(Clabel|X, \u02c6Y, Cinput) by \u02c6Clabel = arg max P(Clabel|X, \u02c6Y, Cinput).\n Last, compute Q = Similarity(Clabel, \u02c6Clabel) as the text similarity between Clabel and \u02c6Clabel, and use Q as the metric for the evaluation of the quality of inverse inference. Since more accurate inverse inference probability often results in higher text similarity, ByCS selects the in-context examples with higher Q. Note that Q is adopted since it does not require to assessment of the model\u2019s output probability distribution of the LLM, which is often unavailable for commercial LLMs.\n# To reduce the computation cost of inverse inference, two methods are used when the number of examples in the datastore is large:\n\u2022 Conduct inverse inference using a model in the same model family as our inference model but has a smaller model size.\nN)\n Apply ByCS to a small number (e.g. N) of pre-selected candidate examples. In preselection, all examples in the datastore are first ranked, and only the top N best examples are reserved as the pre-selected candidates. The pre-selection is performed using fast rankingbased algorithms like kNN.\n# 4 Experimental Setup\n# 4 Experimental Setup 4.1 Models\n# 4.1 Models\n# 4.1 Models\nExperimental results are performed on audio, text, and image modalities. For audio-text and imagetext tasks, ASR and VQA are used to evaluate the ICL ability of encoder-decoder structured models. For text-only NLP tasks, topic classification, sentiment analysis, and text-to-SQL are used to evaluate the ICL performance with decoder-only models. Regarding the NLP tasks, experiments are conducted using GPT-3.5-Turbo and GPT-4 (OpenAI, 2023a). For the ASR task, the open-sourced Whisper model (Radford et al., 2023) is used, which is a series of speech models released by OpenAI. The Whisper model family uses vanilla encoderdecoder Transformer (Vaswani et al., 2017) architecture ranging from 39 million (M) parameters (tiny) to 1.55 billion (B) parameters (large). Specifically, the Whisper small (244M) and Whisper largev2/-v3 (1.55B) models are used. For the VQA task,\nexperiments are performed on Emu2 (Sun et al., 2023a) and GPT-4V (OpenAI, 2023b). Emu2 is a 37B text-image model (VLM) which leverages pretrained EVA-02-CLIP-E-plus (Sun et al., 2023b) and LLAMA-33B (Touvron et al., 2023a), which has ICL ability when taking interleaved inputs of images and texts. For experiments on Emu2, the outputs are generated using a greedy decoding setting for fast evaluation. GPT-4V is a GPT4 variant that can directly perceive image inputs, showing state-of-the-art image understanding performance.\n# 4.2 Datasets\nSeven datasets covering NLP, ASR and VQA are used in this paper. For text-only ICL, four datasets are used in four different task categories: the TREC dataset for topic classification (Voorhees and Tice, 2000), the SST2 dataset for sentiment analysis (Socher et al., 2013), the Spider dataset for text-to-SQL (Yu et al., 2018), and the CHiME4 (Vincent et al., 2017) split of the HyPoradise dataset (Chen et al., 2023) for generative language model re-scoring to correct pre-generated ASR transcriptions. For audio-text ICL, Two datasets are used for ASR tasks, namely RASC863 (ChineseLDC.org, 2004) and CORAAL (Gunter et al., 2021). RASC863 is a commonly used Chinese dialect ASR dataset and its dialectal words split of Chongqing and Guangzhou dialects are used. CORAAL is an English corpus with speech recordings from regional African Americans. For imagetext ICL, VQA experiments are conducted on OKVQA (Marino et al., 2019), a dataset that requires methods to draw upon external knowledge to answer the visual questions.\n# 4.3 Baselines\nOn all three modalities, random selection and improved KATE (Liu et al., 2022) are used as baseline approaches. For random selection, in-context examples are uniformly selected from the example datastore three times and the average results are reported. For KATE (Liu et al., 2022), k neighbours that are nearest to the test input in the embedding space in terms of Euclidean distance are selected. For ASR ICL, the encoder of Whisper large-v2 acts as the embedding retrieval module on the Chinese dataset, while on the English dataset, we use the encoder of Whisper large-v3. In text-ICL, OpenAI text-embedding-ada-002 is used as the embedding retrieval model. For VQA ICL, KATE is only based on the embedding space of the query\nCorpus & In-context example number k\nSetting\nRASC863 Chongqing\nRASC863 Guangzhou\nCORAAL <15s\nk = 1 k = 2 k = 3 k = 4 k = 1 k = 2 k = 3 k = 4\nk = 1\nrandom\n67.1\n56.1\n52.7\n51.0\n61.7\n38.3\n31.2\n28.8\n13.2\nKATE+\n67.1\n54.7\n51.3\n49.7\n61.3\n36.1\n26.9\n24.8\n12.6\nByCS\n62.4\n53.4\n50.6\n48.6\n49.5\n31.9\n27.1\n26.6\n12.4\noracle ByCS\n62.4\n52.4\n49.5\n47.2\n49.4\n30.7\n25.8\n24.7\n12.4\n<div style=\"text-align: center;\">(a) Results with Whisper-large-v2</div>\nCorpus & In-context example number k\nSetting\nRASC863 Chongqing\nRASC863 Guangzhou\nCORAAL <15s\nk = 1 k = 2 k = 3 k = 4 k = 1 k = 2 k = 3 k = 4\nk = 1\nrandom\n68.9\n60.3\n57.0\n55.7\n67.1\n42.8\n38.3\n35.2\n12.4\nKATE+\n68.1\n58.2\n54.8\n54.1\n67.7\n41.3\n34.3\n31.6\n12.1\nByCS\n63.5\n56.3\n53.5\n51.8\n50.7\n36.7\n33.0\n31.5\n12.0\noracle ByCS\n63.4\n55.2\n53.0\n50.7\n51.3\n35.6\n31.9\n30.7\n11.9\n(b) Results with Whisper-large-v3\nTable 1: %WERs on RASC863 dialectal word dataset and CORAAL with different in-context example selection methods. For RASC863, the example datastore is the RASC863 dialectal word dataset of the corresponding dialect For CORAAL, the size of the example datastore for ByCS is narrowed down to 10 using kNN algorithm. For the \u201coracle ByCS\u201d setting, the ground-truth label Yref is used in the inverse reference.\nimage and EVA02-CLIP-bigE-14-plus (Sun et al., 2023b) serves as the embedding retrieval module. We use the term \u201cKATE+\u201d to refer to the baseline in our paper, putting stress on the fact that it is actually an improved KATE version enhanced using stronger embedding retrieval models, which results in better performance. For text ICL, bm25 (Robertson et al., 1995) and LLM-R (Wang et al., 2023b) are also compared as baselines. bm25 is a ranking metric originally designed for search engines to estimate the relevance of documents to a given query based on word-overlapping similarity. LLM-R provides a recent and preferment dense retriever distilled using a reward model trained based on LLM feedback.\n# 5 Results\n# 5.1 ASR ICL\nResults in WER are reported for ASR tasks in Table 1, and here in Chinese WER is calculated based on Chinese characters, which is also termed as character error rate. The ByCS method outperforms the KATE+ baseline in most cases, showing the robustness and effectiveness of our method. When the number of in-context examples k is small, ByCS surpasses KATE+ baseline in a large margin, with a 10.25%\nrelative WER reduction on average when k = 1. Such performance advantage of ByCS reduces when the number of in-context examples increases, which may be attributed to the fact that ByCS performs the inverse inference of each in-context example individually by applying an independence assumption that ignores the contextual interactions between different in-context examples. The use of Yref in \u201coracle ByCS\u201d further boosts the performance gain, indicating the upper bound of our method with the same number of k.\n# 5.2 Ablation study on ASR ICL\n# 5.2 Ablation study on ASR ICL 5.2.1 Inverse decoding option\n# 5.2.1 Inverse decoding option\nThe influence of different decoding options of inverse inference is studied on the RASC863 dialectal word dataset. The results are shown in Table 2. For the setting notation, \u201cnoprompt\u201d denotes decoding in the default decoding option, and \u201cprompt\u201d means to decode with a specially designed prompt \u201c\u8bc6\u522b\u65b9\u8a00\u201d (meaning to \u201crecognize dialect speech\u201d). \u201cLID\u201d denotes decoding with the correct language identity of Chinese (\u201czh\u201d). The results show that among the three inverse decoding options, \u201cnoprompt\u201d obtains the best performance, \u201cprompt\u201d becomes the second, and \u201cLID\u201d the worst. The WERs of inverse inference are re-\nported in Table 3. The WERs under the \u201cnoprompt\u201d setting are more than 100% due to the high insertion error rate. The repeated outputs are not removed when calculating the WERs of inverse inference and when calculating the text similarity, making a more obvious distinction between the examples with high mutual information interaction and those with low. Although it may be a little counter-intuitive that low inverse inference accuracy results in high ByCS selection performance, it is reasonable since inverse inference in ByCS helps to separate good in-context examples from the rest, which can be better achieved by using worse decoding options during inverse inference. This is because our decoding options can often make the model make more mistakes for worse in-context examples.\nSetting\nCorpus\nText\nInverse\nRASC863\nChongqing\nRASC863\nGuangzhou\nsimilarity\ndecoding\nmeasurement\noption\nJaccard\ncoefficient\nnoprompt\n62.4\n49.5\nprompt\n62.9\n50.7\nLID\n64.1\n52.3\nBERT\nwordvecs\nnoprompt\n62.4\n51.5\nprompt\n63.5\n56.8\nLID\n64.5\n57.7\nTable 2: %WERs of Whisper large-v2 on RASC863 dialectal word dataset using ByCS method with different inverse decoding options and text similarity measurements. The number of in-context examples is k = 1.\nInverse\ndecoding\noption\nCorpus\nRASC863\nChongqing\nRASC863\nGuangzhou\nnoprompt\n91.5\n125.2\nprompt\n70.2\n70.1\nLID\n54.6\n61.7\nTable 3: Inverse inference %WERs of Whisper largev2 on RASC863 dialectal word dataset with different inverse decoding options.\n# 5.2.2 Text similarity measurement\nThe results of ByCS with different text similarity measurements are also reported in Table 2. For the setting notation, the \u201cJaccard coefficient\u201d is a com-\nSetting\nIn-context example number k\nk = 1 k = 2 k = 3\nk = 4\nKATE+\n67.1\n54.7\n51.3\n49.7\nByCSlargev2\n62.4\n53.4\n50.6\n48.6\nByCSsmall\n64.2\n53.3\n50.5\n48.7\n<div style=\"text-align: center;\">In-context example number k</div>\n<div style=\"text-align: center;\">(a) Results with Whisper large-v2</div>\nSetting\nIn-context example number k\nk = 1 k = 2 k = 3\nk = 4\nKATE+\n68.1\n58.2\n54.8\n54.1\nByCSlargev3\n63.5\n56.3\n53.5\n51.8\nByCSsmall\n64.4\n56.5\n54.1\n51.7\n(b) Results with Whisper large-v3\n<div style=\"text-align: center;\">(b) Results with Whisper large-v3</div>\nTable 4: %WERs on RASC863 Chongqing dialectal word dataset with ByCS with different inverse inference models. ByCSlargev3 and ByCSsmall use Whisper-largev3 and Whisper-small as the inverse inference model separately.\nmonly used statistic to gauge similarity, defined as the intersection over the union of two sentences. \u201cBERT wordvecs\u201d is to measure similarity based on the Euclidean distance in the embedding space of BERT encoded word vectors. The embedding retrieval module is bert-base-chinese 1. ByCS with the Jaccard coefficient as text similarity have lower WERs, which may be because the training data of the BERT model doesn\u2019t include sufficient dialectal Chinese words and expressions. It also indicates that ByCS can work well with even a simple rule-based text similarity measurement, further verifying its high robustness. The Jaccard coefficient is used as the text similarity measurement in later experiments unless explicitly specified, due to the performance and simplicity.\n# 5.2.3 Inverse inference model\nThe inverse inference with different models is also investigated, with the results displayed in Table 4. A smaller model is used for inverse inference to speed up ByCS, since it is expensive to perform inverse inference using the inference model for every candidate example in datastore. Replacing Whisper-large-v2/v3 with Whisper-small will speed up six times2. For the notation, the subscript denotes the inverse inference model. For example, ByCSsmall is the ByCS method with Whisper small\n1https://huggingface.co/ bert-base-chinese 2https://github.com/openai/whisper\n<div style=\"text-align: center;\">Corpus & In-context example number k Acc. \u2191) SST2(%Acc. \u2191) Spider(%Acc. \u2191) HyPoradise CHiME-4 (%WER \u2193)</div>\nSetting\nTREC(%Acc. \u2191)\nSST2(%Acc. \u2191) Spider(%Acc. \u2191) HyPoradise CHiME-4 (%WER \u2193)\nk = 1 k = 2 k = 4 k = 1\nk = 2\nk = 1\nk = 1\nk = 2\nk = 5\ndefault\n63.0\n92.92\n67.41\n8.0\nrandom\n63.5\n72.7\n75.3\n94.96\n94.80\n67.02\n7.5\n7.5\n7.3\nKATE+\n78.8\n86.4\n91.0\n95.05\n94.69\n69.44\n7.7\n7.1\n6.8\nbm25\n74.6\n89.4\n89.8\n95.27\n95.40\n67.41\n7.4\n7.5\n8.1\nLLM-R\n78.0\n88.8\n90.4\n95.05\n94.02\n67.82\n7.4\n6.9\n7.0\nByCS\n81.2\n88.0\n90.6\n95.16\n95.04\n69.63\n7.1\n6.8\n6.4\n<div style=\"text-align: center;\">(a) Results using GPT-3.5-Turbo</div>\nCorpus & In-context example number k\nSetting\nTREC(%Acc. \u2191)\nSST2(%Acc. \u2191) Spider(%Acc. \u2191) HyPoradise CHiME-4 (%WER \u2193)\nk = 1 k = 2 k = 4 k = 1\nk = 2\nk = 1\nk = 1\nk = 2\nk = 5\ndefault\n75.2\n95.01\n69.63\n11.6\nrandom\n81.3\n82.5\n84.6\n96.38\n96.11\n70.66\n6.9\n6.8\n6.5\nKATE+\n88.2\n91.6\n93.4\n96.43\n95.85\n71.95\n7.0\n6.3\n5.8\nbm25\n81.8\n87.4\n91.4\n96.19\n96.09\n71.47\n6.8\n6.6\n6.3\nLLM-R\n88.2\n91.0\n93.6\n95.74\n95.06\n72.63\n6.8\n6.3\n5.9\nByCS\n88.6\n92.4\n93.6\n96.55\n96.31\n72.82\n6.7\n6.3\n5.9\nTable 5: Results of four text ICL tasks on two GPT-family models with different in-context example selection methods. The evaluation metrics are denoted in the brackets. The example datastore is narrowed down to a small size using kNN for ByCS. In the \u2018default\u2019 setting, the answers are generated directly with the questions without ICL.\n# as an inverse inference model.\nByCSsmall has similar results to ByCSlargev2 and ByCSlargev3, verifying the effectiveness of using a smaller model from the same family for inverse inference. This is intuitive since Whisper-small is trained using the same data and settings compared to the inference model Whisper-large-v2 and Whisper-large-v3, which therefore processes information similarly and can serve as a good alternative when evaluating the quality of the in-context examples. The smaller size of Whisper-small makes ByCS a more practical method in cost-sensitive scenarios. A detailed analysis of time cost is in Appendix B.\n# 5.3 Text ICL\nText-only ICL results are shown in Table 5. As shown, ByCS outperforms all baselines on most dataset settings, showing not only the effectiveness but also the robustness of ByCS. In particular, ByCS outperforms the best baseline on the generative ASR rescoring dataset HyPoradise with a considerable 4.7% relative WER reduction with GPT3.5-Turbo. On TREC and SST2 datasets, ByCS\ndoes not always outperform the baselines. This indicates that ByCS is more suitable for open-ended long-answer datasets due to the calculation of text similarity in ByCS, in which answers are much more diverse and examples with rich information interactions can be better separated. In contrast, in multi-choice classification datasets, only a few short answers are often available, containing little contextual information. As the example shown in Figure 4, the distribution of the text similarity for ranking the examples is often sharp, merging the optimal and the suboptimal examples. Furthermore, considering the hypothesized labels of the test inputs for inverse inference, the hypothesized answers in open-ended datasets (in the form of long sentences) are often more similar to their corresponding references compared to those in the multi-choice classification datasets (in the form of a word or phrase or just an index of choice). It is observed that different in-context example selection methods perform differently with different models, even though on the same dataset. The bm25 method outperforms the KATE+ method with GPT-3.5-Turbo on the SST2 dataset, but not\nIt is observed that different in-context example selection methods perform differently with different models, even though on the same dataset. The bm25 method outperforms the KATE+ method with GPT-3.5-Turbo on the SST2 dataset, but not\nwith GPT4. Compared to KATE+ and bm25 that is model-free in the actual selection step, the performance advantage of ByCS is more consistent since it takes into account the influence of the model. The outputs of the inverse inference model are used, which can serve as a good approximation to the inference model as verified in Section 5.2.3. Note that for ByCS on GPT-4, although the inverse inference procedure is conducted on GPT-3.5Turbo, the performances of ByCS are still superior. This further verifies that smaller models from the same model family can serve as a good low-cost approximation of the inverse inference model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe40/fe409613-e586-41f9-991f-73e1aa3e4262.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2268/22689f73-88df-41f5-bc9a-897f1f597bef.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Distribution on HyPoradise</div>\nFigure 4: The distribution of text similarity scores on different datasets. The text similarity score is the Jaccard coefficient. The entropy of distribution is calculated and placed on the upper left. The distribution on the multichoice classification dataset SST2 (blue) is much sharper than that of the open-ended dataset HyPoradise (red).\n# 5.4 VQA ICL\nByCS is tested on VQA ICL and the results are reported in Table 6. ByCS outperforms the KATE+ baseline on the VQA ICL task, demonstrating strong performances across modalities. The performance improvement from ByCS is not as obvious as in audio and text tasks, since the answers of VQA are usually short (usually a word or phrase), lacking sufficient contextual information. ByCS on\nIn-context\nexample\nnumber k\nExample selection method\nKATE+\nByCS\nk = 2\n40.47\n40.12\nk = 4\n45.11\n45.14\n<div style=\"text-align: center;\">(a) Results with Emu-2</div>\n(a) Results with Emu-2\nIn-context\nexample\nnumber k\nExample selection method\nKATE+\nByCS\nk = 2\n52.54\n52.86\nk = 4\n54.00\n54.39\n<div style=\"text-align: center;\">(b) Results with GPT-4V</div>\nTable 6: Results of VQA ICL with different in-context example selection methods and numbers of examples on OKVQA dataset.\nthe VQA dataset suffers from the problem of having sharp text similarity score distributions, similar to the multichoice classification dataset. For ByCS with GPT-4V, inverse inference results on Emu-2 are used to pre-select the candidate examples, and ByCS still outperforms the KATE+ baseline. The performance may be further improved if GPT-4V is also used for inverse inference. This demonstrates that ICL may perform similarly cross models not only on speech and text, but also on images.\n# 6 Conclusion\nThis paper proposes ByCS, a novel in-context example selection method based on Bayes\u2019 theorem, which assumes that contextual information interaction is mutual between the test input and in-context examples and selects high-quality examples based on the inverse inference results. Experiments are performed across three modalities: speech, text, and images, using six different tasks and seven datasets. Results demonstrated the robustness and effectiveness of ByCS. It is also validated that the inverse inference results can be approximated using a smaller model from the same model family, which considerably reduces the computational cost. Moreover, relying on text similarity to rank in-context examples, ByCS is more suitable for open-ended long-answer datasets which contain sufficient contextual information. Future work is to extend the inverse inference to sequences with multiple incontext examples to model the interactions among the in-context examples.\nThere are three limitations to this work. First, ByCS follows the simple assumption that the influence of each in-context example is independent and treats each in-context example individually, which neglects the contextual interactions between in-context examples. The approximation may not be adapted to the scenario in which the number of in-context examples is high. Second, ByCS requires sufficient contextual diversity to select optimal examples, which depends on text similarity to evaluate inverse inference results. ByCS may suffer a performance penalty when applied to a short-answer dataset. The third limitation is the extra time cost introduced by inverse inference, making ByCS less suitable for cost-sensitive scenarios. Future work includes enhancing ByCS in more scenarios.\n# Ethics Statement\nThe work doesn\u2019t give rise to any ethical risks and issues. All the models and data used in this paper are publicly accessible and used under licenses.\n# References\nom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proc. NeurIPS.\nChen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, and Ensiong Chng. 2023. Hyporadise: An open baseline for generative speech recognition with large language models. In Proc. NeurIPS. ChineseLDC.org. 2004. Introduction to RASC863. http://www.chineseldc.org/doc/ CLDC-SPC-2004-005/intro.htm. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. In Proc. ACL 2023 findings. Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. 2022. Magma\u2013multimodal augmentation of generative models through adapter-based finetuning. In Proc. EMNLP 2022 findings. Kaylynn Gunter, Charlotte Vaughn, and Tyler Kendall. 2021. Contextualizing/s/retraction: Sibilant variation and change in washington dc african american language. Language Variation and Change, 33(3):331\u2013 357. Ming-Hao Hsu, Kai-Wei Chang, Shang-Wen Li, and Hung-yi Lee. 2023. An exploration of in-context learning for speech language model. arXiv preprint arXiv:2310.12477. Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu, and Chenguang Zhu. 2023. In-context demonstration selection with cross entropy difference. arXiv preprint arXiv:2305.14726. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In Proc. ICML. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for gpt-3? In Proc. DeeLIO. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022a. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In Proc. ICLR. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proc. ACL (Volume 1: Long Papers). Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proc. CVPR.\nOpenAI. 2023b. Gpt-4v(ision) system card.\nJing Pan, Jian Wu, Yashesh Gaur, Sunit Sivasankaran, Zhuo Chen, Shujie Liu, and Jinyu Li. 2023. Cosmic: Data efficient instruction-tuning for speech in-context learning. arXiv preprint arXiv:2311.02248. Chengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. 2023. In-context learning with iterative demonstration selection. arXiv preprint arXiv:2310.09881. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In Proc. ICML. Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. Nist Special Publication Sp, 109:109. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proc. NAACL. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. EMNLP, pages 1631\u20131642. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. In Proc. ICLR. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. 2023a. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023b. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389.\nChangli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2023. Salmonn: Towards generic hearing abilities for large language models. arxiv preprint arXiv:2310.13289. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and finetuned chat models. arxiv preprint arxiv:2307.09288. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. In Proc. NeurIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS. E. Vincent, S. Watanabe, A. Nugraha, J. Barker, and R. Marxer. 2017. An analysis of environment, microphone and data simulation mismatches in robust speech recognition. Computer Speech and Language, 46:535\u2013557. Ellen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In Proc. SIGIR. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023a. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111. Liang Wang, Nan Yang, and Furu Wei. 2023b. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164.\nEllen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In Proc. SIGIR.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023a. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111. Liang Wang, Nan Yang, and Furu Wei. 2023b. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023a. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111.\nLiang Wang, Nan Yang, and Furu Wei. 2023b. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv:2307.07164.\nSiyin Wang, Chao-Han Huck Yang, Ji Wu, and Chao Zhang. 2023c. Can whisper perform speech-based incontext learning. arxiv preprint arXiv:2309.07081. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023d. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Prov. NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proc. NeurIPS. Shangqing Xu and Chao Zhang. 2024. Misconfidencebased demonstration selection for llm in-context learning. arXiv preprint arXiv:2401.06301. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proc. EMNLP. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proc. EMNLP. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proc. ICML.\n# A Experimental Details\n# A.1 Datasets, baselines and prompt templates\nThe dataset details are listed in Table 9. For Spider, the evaluation metric is execution accuracy. For CORAAL, we use the processing script from the FairSpeech project3. For convenience, we only use speech less than 15 seconds because Whisper can accept input audio up to 30 seconds. For the ASR dataset, there is no train/test split, the dataset except the test input serves as the in-context example datastore. For bm25 implementation, we use the okapi variant in rank_bm254 library. The inverse inference example is presented in Figure 5 and prompt templates are shown in Table 13.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4358/4358b943-b0a4-4ec9-bcf6-ff02188119f8.png\" style=\"width: 50%;\"></div>\nFigure 5: We provide an additional \u201cinverse inference\u201d illustration of the proposed Bayesian example selection method for in-context learning in a text format, similar to Min et al. (2022).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/90b3/90b3430d-1842-4210-8c29-c18f0822ea14.png\" style=\"width: 50%;\"></div>\nse inference result and the true label.  (\ud835\udc36'+,', =\ud835\udc36'+,') Figure 6: An illustration of the calculation of text similarity between inverse inference results and their true labels in Mandarin accent recognition, where the red inverse inference tokens indicate misrecognition.\nmeasurement score 3https://github.com/ stanford-policylab/asr-disparities 4https://github.com/dorianbrown/rank_ bm25\n# A.2 First-round inference of ByCS\nWe experimented with ByCS on different firstround inference settings to examine the influence of first-round inference, and the results are reported in Table 7. The first-round inference produces the hypothesized label of test input. With better firstround inference hypotheses, the approximated inverse inference probability will be more close to the oracle one. Figure 6 provides an example of text similarity calculation. The first-round accuracy for the \u2018default\u2019, \u2018random\u2019 and \u2018KATE+\u2019 settings is 63.0, 75.8 and 91.0, respectively. The first-round inference with ICL improves the accuracy of the hypothesized label, thus boosting the performance of ByCS. In practice, we use ICL with random example selection as the first-round inference setting for ASR ICL and best ICL baseline as the first-round inference setting for text and VQA ICL.\nFirst-round\ninference\nIn-context example number k\nk = 1\nk = 2\nk = 4\ndefault\n75.6\n83.8\n88.4\nrandom k = 4\n79.8\n87.0\n91.6\nKATE+ k = 4\n81.2\n88.0\n90.6\n<div style=\"text-align: center;\">(a) Results with GPT-3.5-Turbo</div>\nFirst-round\ninference\nIn-context example number k\nk = 1\nk = 2\nk = 4\ndefault\n87.2\n91.8\n93.0\nrandom k = 4\n86.6\n92.4\n93.0\nKATE+ k = 4\n88.6\n92.4\n93.6\n(b) Results with GPT-4\nTable 7: Results on TREC of ByCS with different firstround inference settings.\n# A.3 Pre-selection of ByCS\nSince the datastore size is usually large, we use a simple ranking algorithm to compress in-context example datastore and then use ByCS inverse inference to select good examples. We usually choose kNN as the ranking algorithm and twice the maximum number of in-context examples as reduced size after pre-selection. For RASC863, we simply use the speech from the same speaker as in-context examples, so the number of reduced size is approximate. We experimented on the TREC dataset to analyze whether reduced size matters, the results are reported in 8. The results imply that reduced\nsize has nearly negligible impact on the performance of ByCS method. Thus twice the number of in-context examples is a balanced choice for example diversity and conducting speed. The details of pre-selection are shown in Table 9.\nReduced\nsize\nIn-context example number k\nk = 1\nk = 2\nk = 4\n4\n81.6\n87.6\n90.6\n8\n81.2\n88.0\n90.6\n16\n81.0\n88.0\n90.4\n<div style=\"text-align: center;\">(a) results on GPT-3.5-Turbo</div>\nReduced\nsize\nIn-context example number k\nk = 1\nk = 2\nk = 4\n4\n88.0\n92.6\n93.2\n8\n88.6\n92.4\n93.6\n16\n88.4\n92.8\n93.2\n(b) results on GPT-4\nTable 8: Results on TREC of ByCS with different reduced sizes after pre-selection.\n# B Analysis of time cost\n# B.1 Computational complexity\nAlthough ByCS may be time-consuming, the existing improvement methods have reduced the complexity from O(N) to O(1), where N is the size of the example datastore. The original version of ByCS will conduct inverse inference on every candidate in the whole dataset, which results in complexity in O(N). Using a smaller model for fast inverse inference decreases the number of computations by a constant factor. For instance, Whisper small is 6 times faster than Whisper large, and using Whisper small for inverse inference reduces the inverse inference cost by \u223c6 times. Furthermore, by using a ranking-based pre-selection, we can reduce the size of the example datastore to a fixed number, reducing the computational complexity of inverse inference further down to O(1). In our experiments, we found empirically that a number around 10 is a good choice in balancing the example diversity and conduction speed, as shown in Appendix A.3.\n# B.2 Attempt to further speed up\nSince inverse inference spends most of its time in ByCS, we try to conduct inverse inference on\nexamples in the datastore before the test input arrives. For each example in the datastore, suitable in-context examples are selected for it using ByCS. In practice, the in-context examples of the test input are those of the nearest neighbour. By this means, the time cost of ByCS is comparable with kNNbased methods. The results of this new sped-up version of ByCS, which is denoted as ByCSfast are shown in Table 14. As expected, ByCSfast always performs worse than ByCS. Furthermore, ByCSfast is more dependent on the contextual diversity. On the open-ended long-answer speech datasets, ByCSfast can outperform the best baseline. While on short-answer text datasets, the performance of ByCSfast suffers a significant deterioration. It emphasizes the importance of inverse inference directly on test input, not on a similar substitution.\nModality\nTask category\nDataset\nTrain size\nTest size\nPre-selection Reduced\nsize\nText\nTopic classification\nTREC\n5452\n500\nkNN\n8\nSentiment analysis\nSST2\n67349\n872\nkNN\n4\nText to SQL\nSpider\n7000\n1034\nkNN\n3\nASR LM rescoring\nHyPoradise CHiME-4\n9728\n1320\nkNN\n10\nAudio\nAutomatic speech recognition\nRASC863 Guangzhou\n1889\n1990(1.41h) same speaker\n\u223c10\nRASC863 Chongqing\n2993\n2994(3.26h) same speaker\n\u223c15\nCORAAL <15s\n2761\n2762(6.77h)\nkNN\n10\nImage\nVision question answering\nOKVQA\n9009\n5046\nkNN\n8\nTable 9: Datasets used in this work\nDataset\nTemplate example\nTREC\nQuestion: What is the temperature at the centre of the earth?\nAvailable Type: description, entity, expression, human, number, location.\nType: number.\nSST2\nReview: \u201cThe Time Machine\u201d is a movie that has no interest in itself.\nAvailable sentiment: positive, negative.\nSentiment: negative.\nSpider\nGiven the database schema, you need to translate the question into the SQL query.\nDatabase schema:\nTable name: Movie\nCreation SQL: CREATE TABLE Movie(\nmID int primary key,\ntitle text,\nyear int,\ndirector text\n)\nTable name: Reviewer\nCreation SQL: CREATE TABLE Reviewer(\nrID int primary key,\nname text\n)\nTable name: Rating\nCreation SQL: CREATE TABLE Rating(\nrID int,\nmID int,\nstars int,\nratingDate date,\nFOREIGN KEY (mID) references Movie(mID),\nFOREIGN KEY (rID) references Reviewer(rID)\n)\nQuestion: Find the names of all reviewers who have contributed three or more ratings.\nSQL query: SELECT T2.name FROM Rating AS T1 JOIN Reviewer AS T2 ON T1.rID = T2.rID GROUP BY T1.rID HAVING COUNT(*) >= 3.\nHyPoradise\nCHiME-4\nYou need to do language model rescoring in ASR. Given the 5-best hypotheses, you need to report the true transcription from the 5-best hypotheses.\nThe 5-best hypothesis is:\ninterest rates rose on torture and treasury bills sold by the government yesterday at its regular weekly auction.\ninterest rates rose on short-term treasury bills sold by the government yesterday at its regular weekly auction.\ninterest rates rose at a torture and treasury bill sold by the government yesterday at its regular weekly auction.\ninterest rates rose on a torture and treasury bill sold by the government yesterday at its regular weekly auction.\ninterest rates rose on torturing treasury bills sold by the government yesterday at its regular weekly auction.\nThe true transcription from the 5-best hypotheses is:\ninterest rates rose on short-term treasury bills sold by the government yesterday at its regular weekly auction.\nOKVQA\nAnswer in one word or phrase.\nWhat softwood is used to close the top of the container in his hand?\ncork.\n<div style=\"text-align: center;\">Table 10: Prompt template examples used in this work</div>\nTable 10: Prompt template examples used in this work\nTable 10: Prompt template examples used in this work\nIn-context\nexample\nnumber k\nInverse\ninference\nmodel\nText similarity measurement & inverse decoding option\nJaccard coefficient\nBERT wordvecs\nnoprompt prompt\nLID\nnoprompt prompt\nLID\nk = 1\nByCSlargev2\n62.4\n62.9\n64.1\n62.4\n63.5\n64.5\nByCSsmall\n64.2\n64.0\n65.4\n65.0\n65.4\n66.3\nk = 2\nByCSlargev2\n53.4\n53.3\n53.7\n53.6\n54.1\n54.1\nByCSsmall\n53.3\n53.7\n54.0\n54.1\n54.9\n54.8\nk = 3\nByCSlargev2\n50.6\n51.0\n50.9\n50.2\n51.6\n50.6\nByCSsmall\n50.5\n50.5\n51.1\n51.3\n50.9\n51.3\nk = 4\nByCSlargev2\n48.6\n48.7\n48.7\n49.1\n48.9\n49.1\nByCSsmall\n48.7\n48.7\n48.6\n49.6\n49.1\n49.9\n<div style=\"text-align: center;\">(a) Results with Whisper large-v2</div>\nIn-context\nexample\nnumber k\nInverse\ninference\nmodel\nText similarity measurement & inverse decoding option\nJaccard coefficient\nBERT wordvecs\nnoprompt prompt\nLID\nnoprompt prompt\nLID\nk = 1\nByCSlargev3\n63.5\n64.1\n65.6\n64.5\n65.3\n65.8\nByCSsmall\n64.4\n64.7\n64.8\n65.5\n65.0\n65.6\nk = 2\nByCSlargev3\n56.3\n56.3\n57.0\n57.7\n57.0\n57.8\nByCSsmall\n56.5\n57.0\n57.0\n57.3\n57.2\n57.5\nk = 3\nByCSlargev3\n53.5\n54.1\n53.7\n55.2\n55.6\n54.9\nByCSsmall\n54.1\n54.6\n54.4\n55.5\n55.3\n55.4\nk = 4\nByCSlargev3\n51.8\n52.3\n52.1\n53.1\n53.4\n53.3\nByCSsmall\n51.7\n52.2\n51.9\n53.6\n53.4\n53.5\n(b) Results with Whisper large-v3\nTable 11: Full results on RASC863 Chongqing dialectal word dataset of ByCS with different inverse decoding ptions, text similarity measurements and inverse inference models. The subscript denotes the inverse inference\nTable 11: Full results on RASC863 Chongqing dialectal word dataset of ByCS with different inverse decoding options, text similarity measurements and inverse inference models. The subscript denotes the inverse inference model.\nIn-context\nexample\nnumber k\nInverse\ninference\nmodel\nText similarity measurement & inverse decoding option\nJaccard coefficient\nBERT wordvecs\nnoprompt prompt\nLID\nnoprompt prompt\nLID\nk = 1\nByCSlargev2\n49.5\n50.7\n52.3\n51.5\n56.8\n57.7\nByCSsmall\n52.9\n55.1\n58.7\n56.8\n57.1\n58.8\nk = 2\nByCSlargev2\n31.9\n33.6\n34.3\n32.9\n34.3\n35.0\nByCSsmall\n34.5\n34.1\n35.6\n35.1\n35.9\n37.0\nk = 3\nByCSlargev2\n27.1\n28.4\n27.7\n27.1\n27.4\n27.5\nByCSsmall\n28.3\n27.8\n27.6\n27.9\n28.6\n28.3\nk = 4\nByCSlargev2\n26.6\n25.5\n24.8\n25.4\n26.5\n25.5\nByCSsmall\n25.9\n25.7\n25.5\n25.3\n26.3\n26.2\n<div style=\"text-align: center;\">Text similarity measurement & inverse decoding option Jaccard coefficient BERT wordvecs</div>\n<div style=\"text-align: center;\">(a) Results with Whisper large-v2</div>\nIn-context\nexample\nnumber k\nInverse\ninference\nmodel\nText similarity measurement & inverse decoding option\nJaccard coefficient\nBERT wordvecs\nnoprompt prompt\nLID\nnoprompt prompt\nLID\nk = 1\nByCSlargev3\n50.7\n51.8\n55.4\n56.6\n57.1\n59.1\nByCSsmall\n55.3\n55.4\n61.7\n61.8\n58.7\n60.7\nk = 2\nByCSlargev3\n36.7\n38.1\n38.9\n38.2\n37.8\n38.9\nByCSsmall\n37.3\n37.3\n40.0\n39.0\n38.0\n39.6\nk = 3\nByCSlargev3\n33.0\n33.4\n34.0\n33.6\n33.4\n33.3\nByCSsmall\n33.3\n33.3\n34.6\n34.8\n33.3\n34.3\nk = 4\nByCSlargev3\n31.5\n31.3\n31.4\n31.7\n31.7\n31.4\nByCSsmall\n31.0\n31.5\n31.9\n31.5\n31.0\n31.0\n(b) Results with Whisper large-v3\nTable 12: Full results on RASC863 Guangzhou dialectal word dataset of ByCS with different inverse decoding options, text similarity measurements and inverse inference models. The subscript denotes the inverse inference\nTable 12: Full results on RASC863 Guangzhou dialectal word dataset of ByCS with different inverse decoding options, text similarity measurements and inverse inference models. The subscript denotes the inverse inference model.\nTest input\nKATE+\nByCS\nsometime they do not act like they hear nothing\nbut know nothing about tarboro\nwhen you say you from tarboro\nthey will talk about where is tarboro at\n(CORAAL)\nExample:\nin the era and th the way\nin there them floors along that time\nthey cut timber certain time of the year\nResult:\nsometimes it do not work out there\nbut no nothing about tarver\nwhen you say you from tarver\nthey will talk about where tarver is\nExample:\nso they put her and him together\nand i was praying to the lord\nthat he did not try to jump out of there\ncause i was so scared me and my husband\nResult:\nsometimes they do not want to let their hear nothing\nbut know nothing about tarver\nwhen you say you from tarver\nthey will talk about where tarver is\nWhat person \u2019s head is on a dime?\nhuman.\n(TREC)\nExample:\nWhat is money made of?\nentity.\nResult:\nentity.\nExample:\nWho is the head of the World Bank?\nhuman.\nResult:\nhuman.\nTable 13: In-context examples selected by kNN and ByCS and corresponding results.\nCorpus & In-context example number k\nSetting\nRASC863 Chongqing\nRASC863 Guangzhou\nCORAAL <15s\nk = 1 k = 2 k = 3 k = 4 k = 1 k = 2 k = 3 k = 4\nk = 1\nbest baseline\n67.1\n54.7\n51.3\n49.7\n61.3\n36.1\n26.9\n24.8\n12.6\nByCSfast\n63.1\n52.5\n50.2\n48.3\n55.8\n35.6\n29.2\n27.1\n12.5\nByCS\n62.4\n53.4\n50.6\n48.6\n49.5\n31.9\n27.1\n26.6\n12.4\n<div style=\"text-align: center;\">(a) Results with Whisper-large-v2</div>\nCorpus & In-context example number k\nSetting\nRASC863 Chongqing\nRASC863 Guangzhou\nCORAAL <15s\nk = 1 k = 2 k = 3 k = 4 k = 1 k = 2 k = 3 k = 4\nk = 1\nbest baseline\n68.1\n58.2\n54.8\n54.1\n67.1\n41.3\n34.3\n31.6\n12.1\nByCSfast\n66.7\n57.5\n54.5\n52.6\n60.5\n40.3\n34.1\n32.3\n12.2\nByCS\n63.5\n56.3\n53.5\n51.8\n50.7\n36.7\n33.0\n31.5\n12.0\n<div style=\"text-align: center;\">(b) Results with Whisper-large-v3</div>\nCorpus & In-context example number k\nSetting\nTREC(%Acc. \u2191)\nSST2(%Acc. \u2191)\nk = 1 k = 2 k = 4 k = 1\nk = 2\nbest baseline\n78.8\n89.4\n91.0\n95.27\n95.40\nByCSfast\n77.0\n83.8\n86.4\n94.15\n94.61\nByCS\n81.2\n88.0\n90.6\n95.16\n95.04\n(c) Results using GPT-3.5-Turbo\nCorpus & In-context example number k\nSetting\nTREC(%Acc. \u2191)\nSST2(%Acc. \u2191)\nk = 1 k = 2 k = 4 k = 1\nk = 2\nbest baseline\n88.2\n91.6\n93.6\n96.43\n96.11\nByCSfast\n85.4\n89.2\n92.6\n95.07\n95.18\nByCS\n88.6\n92.4\n93.6\n96.55\n96.31\n and text tasks. Results of best baseline and ByCS are also shown f\nTable 14: Results of ByCSfast on speech and text tasks. Results of best baseline and ByCS are also shown for comparison.\nTable 14: Results of ByCSfast on speech and text tasks. Results of best baseline and By comparison.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) for large language models (LLMs), which heavily relies on the quality of in-context examples. Previous methods for selecting these examples have not effectively considered the mutual interactions between in-context examples and test inputs, leading to suboptimal performance. The proposed method, Bayesian in-Context example Selection (ByCS), aims to improve the selection process by leveraging Bayes' theorem for better example quality assessment.",
        "problem": {
            "definition": "The problem is the ineffective selection of in-context examples for ICL, which can significantly impact the performance of LLMs on various tasks.",
            "key obstacle": "The main challenge is that existing methods often treat in-context examples and test inputs separately, failing to account for their mutual interactions, which can lead to performance variations."
        },
        "idea": {
            "intuition": "The idea behind ByCS stems from the observation that the quality of in-context examples can be evaluated more accurately by considering the inverse inference results based on Bayes' theorem.",
            "opinion": "ByCS proposes a novel approach to select in-context examples by focusing on their inverse inference results, which are believed to correlate with the examples' effectiveness in ICL.",
            "innovation": "The key innovation of ByCS lies in its focus on mutual information interactions between the in-context examples and test inputs, contrasting with previous methods that did not leverage this relationship."
        },
        "method": {
            "method name": "Bayesian in-Context example Selection",
            "method abbreviation": "ByCS",
            "method definition": "ByCS is a method that selects in-context examples based on their inverse inference results, utilizing Bayes' theorem to evaluate their quality in relation to a given test input.",
            "method description": "ByCS selects optimal in-context examples by performing inverse inference on each example and ranking them based on their similarity to the predicted labels.",
            "method steps": [
                "Perform initial inference to generate a hypothesized label for the test input.",
                "Conduct inverse inference for each candidate example in the datastore using the hypothesized label.",
                "Rank the examples based on the text similarity between the inverse inference results and the true context labels.",
                "Select examples with the highest similarity scores as optimal in-context examples."
            ],
            "principle": "The effectiveness of ByCS is based on the assumption that accurate inverse inference results will lead to higher quality in-context examples, thus improving the overall performance of ICL."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted across multiple modalities (speech, text, and images) using various tasks, including ASR, VQA, and NLP tasks. Datasets such as TREC, SST2, and OKVQA were utilized for evaluation.",
            "evaluation method": "The performance of ByCS was assessed by comparing it against baseline methods through metrics like word error rate (WER) and accuracy across different tasks and datasets."
        },
        "conclusion": "The ByCS method demonstrates significant improvements in the selection of in-context examples for ICL, showing robustness and effectiveness across various modalities and tasks. The results indicate that leveraging inverse inference can enhance the quality of selected examples, ultimately benefiting LLM performance.",
        "discussion": {
            "advantage": "ByCS stands out due to its ability to consider mutual interactions between in-context examples and test inputs, leading to more effective example selection compared to traditional methods.",
            "limitation": "The method assumes independence among in-context examples, which may not hold true in scenarios with a high number of examples, potentially limiting its effectiveness.",
            "future work": "Future research should explore enhancing ByCS to better account for contextual interactions among multiple in-context examples, as well as its application to various datasets and tasks."
        },
        "other info": {
            "ethics statement": "The work does not present any ethical risks as all models and datasets used are publicly accessible and utilized under appropriate licenses."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) for large language models (LLMs) relies heavily on the quality of in-context examples."
        },
        {
            "section number": "1.2",
            "key information": "The ineffective selection of in-context examples can significantly impact the performance of LLMs on various tasks."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method, Bayesian in-Context example Selection (ByCS), improves the selection process by leveraging Bayes' theorem for better example quality assessment."
        },
        {
            "section number": "3.4",
            "key information": "ByCS selects optimal in-context examples by performing inverse inference on each example and ranking them based on their similarity to the predicted labels."
        },
        {
            "section number": "6.1",
            "key information": "ByCS assumes independence among in-context examples, which may not hold true in scenarios with a high number of examples, potentially limiting its effectiveness."
        },
        {
            "section number": "7",
            "key information": "The ByCS method demonstrates significant improvements in the selection of in-context examples for ICL, showing robustness and effectiveness across various modalities and tasks."
        }
    ],
    "similarity_score": 0.7159426554176328,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities.json"
}