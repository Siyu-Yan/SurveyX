{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.06054",
    "title": "Breaking through the learning plateaus of in-context learning in Transformer",
    "abstract": "In-context learning, i.e., learning from context examples, is an impressive ability of Transformer. Training Transformers to possess this in-context learning skill is computationally intensive due to the occurrence of learning plateaus, which are periods within the training process where there is minimal or no enhancement in the model\u2019s incontext learning capability. To study the mechanism behind the learning plateaus, we conceptually separate a component within the model\u2019s internal representation that is exclusively affected by the model\u2019s weights. We call this the \u201cweights component\u201d, and the remainder is identified as the \u201ccontext component\u201d. By conducting meticulous and controlled experiments on synthetic tasks, we note that the persistence of learning plateaus correlates with compromised functionality of the weights component. Recognizing the impaired performance of the weights component as a fundamental behavior that drives learning plateaus, we have developed three strategies to expedite the learning of Transformers. The effectiveness of these strategies is further confirmed in natural language processing tasks. In conclusion, our research demonstrates the feasibility of cultivating a powerful in-context learning ability within AI systems in an eco-friendly manner.",
    "bib_name": "fu2024breakinglearningplateausincontext",
    "md_text": "# Breaking through the Learning Plateaus of In-context Learning in Transformer\nJingwen Fu 1 Tao Yang 1 Yuwang Wang 2 Yan Lu 3 Nanning Zheng 1\n# Abstract\nIn-context learning, i.e., learning from context examples, is an impressive ability of Transformer. Training Transformers to possess this in-context learning skill is computationally intensive due to the occurrence of learning plateaus, which are periods within the training process where there is minimal or no enhancement in the model\u2019s incontext learning capability. To study the mechanism behind the learning plateaus, we conceptually separate a component within the model\u2019s internal representation that is exclusively affected by the model\u2019s weights. We call this the \u201cweights component\u201d, and the remainder is identified as the \u201ccontext component\u201d. By conducting meticulous and controlled experiments on synthetic tasks, we note that the persistence of learning plateaus correlates with compromised functionality of the weights component. Recognizing the impaired performance of the weights component as a fundamental behavior that drives learning plateaus, we have developed three strategies to expedite the learning of Transformers. The effectiveness of these strategies is further confirmed in natural language processing tasks. In conclusion, our research demonstrates the feasibility of cultivating a powerful in-context learning ability within AI systems in an eco-friendly manner.\narXiv:2309.06054v3\n# 1. Introduction\nThis paper is centered on the in-context learning ability of Transformer, which stands as one of the most significant abilities for current applications of Transformer models (Brown et al., 2020; Dong et al., 2022; Shin et al., 2022;\n1National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi\u2019an Jiaotong University 2Tsinghua University 3Microsoft Research Asia. Correspondence to: Nanning Zheng <nnzheng@mail.xjtu.edu.cn>.\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\nMin et al., 2021). Fig. 1A gives examples of the in-context learning tasks. The in-context learning ability has a confusing property that it is emergent when increasing training FLOPs (Wei et al., 2022). By exploring the learning dynamic of in-context learning, previous works (Edelman et al., 2022; Michaud et al., 2023; Kirsch et al., 2022; Singh et al., 2023; Reddy, 2023) discover that the Transformer learns the in-context learning ability abruptly. We summarize the phenomenon during the learning process as plateaus and transition pattern, illustrated in Fig. 1B. This pattern indicates that there is negligible or no enhancement in the in-context learning ability during the initial learning period, which we call the learning plateaus, and these plateaus are then followed by rapid and substantial gains, which are referred to as the transition process. Often, the strategy to shorten learning plateaus involves expanding the model\u2019s scale, which consequently demands greater computational resources and energy consumption. In this paper, our objective is to investigate the possibility of overcoming the learning plateaus without scaling the model\u2019s size. Addressing this challenge could lead to a novel approach to creating environmentally sustainable intelligent systems. Since learning plateaus are not typically observed in conventional supervised learning, concentrating on the difference may be crucial for unraveling the mechanisms behind the learning plateaus.\n# Weights and context components\nWeights and context components A principal distinction between in-context learning and traditional supervised learning lies in the fact that in-context learning outcomes are shaped by both the model\u2019s parameters and the specific context examples provided. In traditional supervised learning, for a given input sample xp with its corresponding label yp, the goal is to find a parameterized function fw with weights w, such that the prediction fw(xp) is equal to the label yp. In this scenario, the weights w hold all the information needed to perform the task at hand. Conversely, within the in-context learning paradigm, there is an additional source of information, which is the context examples sc. Hence, the prediction model is represented as fw,sc(xp), indicating that both the weights and the context examples have the potential to affect the prediction outcome. To examine how the weights and in-context examples impact the prediction, we assume that the function fw,sc(x) can be conceptual decomposite into fw,sc(xp) =\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f7ca/f7cafdb1-234e-44a8-a137-724ed5c56b02.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. A: Examples of the in-context learning tasks. Examples of (1) comes from Alayrac et al. (2022), Examples of (2),(3),(4) come from Brown et al. (2020). B: Illustration of learning plateaus and transition pattern. We evaluate the in-context learning ability of Pythia 13B model (Biderman et al., 2023) trained on pile dataset (Gao et al., 2020) using WordSelection task (Detail in Appendix B) during the training process.</div>\ngcomb(gweights(xp), gcontext(sc)). This decomposition allows us to separate the component gweights(\u00b7), which is part of the Transformer solely dependent on the weights, from the component gcontext(\u00b7), which is influenced by both the weights and the context examples. We refer to gweights(\u00b7) as the weights component and gcontext(\u00b7) as the context component. Owing to the design of the Transformer\u2019s architecture, there is an interaction between gweights(\u00b7) and gcontext(\u00b7). Importantly, conceptual decomposition implies that the decomposition isn\u2019t physical; rather, it\u2019s solely for analytical purposes.\n# Break through the learning plateaus.\nBreak through the learning plateaus. Drawing from our observations, we consider the dysfunction of the weights component during learning to be the primary cause of the extended learning plateaus. Based on this, we suggest three methods for enhancing the weights component and all these methods can mitigate the learning plateaus. The effectiveness of these methods is also verified in NLP tasks.\nContributions Our main contributions can be summarized as follows: a) We give formulations of weights and context components with a new synthetic task that enables the study of the mechanism behind learning plateaus. b) We study the learning process of synthetic tasks with different complexity. The experiments reveal the relation between the weights component and the learning plateaus. c) To further verify the causal relation between the learning plateaus and the weights component, we propose different methods to improve the weights component and we observe the mitigat-\n\ning of the learning plateaus. The discoveries are verified in the NLP task.\n# 2. Related works\nIn this section, we explore the works most closely related to our study. Additional related literature can be found in Appendix C. This appendix encompasses a) an examination of the weights and context components in relation to the previously proposed division of in-weights and in-context learning, b) a review of evidence from prior studies that supports the importance of both context and weights components in practical applications, and c) a compilation of related works aimed in understanding Transformers.\n# Understanding of the mechanism of in-context lea\nUnderstanding of the mechanism of in-context learning Considerable research has been devoted to this vital topic, with most prior studies focusing on understanding the mechanism of in-context learning from the perspective of algorithm implementation. For example, a number of recent papers (von Oswald et al., 2022; Dai et al., 2023; Aky\u00a8urek et al., 2022) have described in-context learning as\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c9e/7c9e78d0-383f-445b-9eaa-3e891c6f9f76.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 2. Synthetic task. In the task, Transformer is required to predict the label of xp given context examples sc. The images from the D Shapes dataset are synthesized based on six factors. The output factor is determined by the context. In this case, we provide two quences of factors: \u201dobject color\u201d and \u201dobject shape,\u201d respectively.</div>\nakin to performing gradient descent. Additional research (Li et al., 2023; Bai et al., 2023) has interpreted in-context learning in terms of algorithm implementation and choice. Our study, however, takes a novel approach by examining the in-context learning mechanism through the lens of the weights and context components, offering a distinct perspective from earlier works.\n# 3. Experimental Design\nThis paper employs a synthetic task to investigate the fundamental mechanisms behind learning plateaus. The use of a synthetic task is due to the fact that the intricacies of real tasks pose challenges in monitoring and comprehending the precise factors that influence the emergence of learning plateaus.\n# 3.1. Dataset Construction\nWe propose a task using the Shapes3D (Kim & Mnih, 2018) dataset for a more controllable study. The experimental setting is shown in Fig. 2. Specifically, given a sequence of image and label pairs as context, the task involves predicting the label of the prompt image. Each image contains six different factors: object color, object shape, object scale, background color, floor color, and pose. We denote the factor as e and the factor value of factor e as v(e). For each sequence, we randomly choose a factor to generate the labels of the images, referring to this factor as the hidden factor eh for this sequence. For the two context sequences in Fig. 2, the hidden factor of Seq #1 is object color, and the correct label for the prompt image is 1 (object color is green). In Seq #2, for the same prompt image, the correct label is 3 (the object shape is a cube). We give a formal formulation of the data generation process below. Notations We denote xp as the prompt example with ground truth label yp. The context examples are sc =\n{(x1, y1), \u00b7 \u00b7 \u00b7 , (xl, yl)}. The prediction of the model is denoted as fw,sc(xp). We denote the factor values of x as vx and the corresponding factor value for factor e as v(e) x . vp is short for vxp. The hidden factor is denoted as eh. We denote the mapping function as m, which maps the factor value to the corresponding label, i.e. yp = m(v(eh) p ). We denote the probability as P. Definition 3.1. The data is generated according to the equation that \ufffd\n\ufffd where P(m), P(eh) are manually setted distributions and P(x, y, sc|m, eh) is a fixed distribution. P(m), P(eh) are uniform distributions over all possible values by default. In this paper, we rely on changing P(m) to obtain the tasks with different complexity.\nTo successfully tackle the in-context learning task, the network is required to discern the values of the six factors present in the prompt image, which relate to the weights component, as well as accurately determine the appropriate hidden factor for output based on the given contexts, which pertains to the context component. We break down the distribution P(yp|xp, sc) as follows. Proposition 3.2. The probability of P(yp|xp, sc) can be decomposite as:\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd where P(vp|xp) is weights related information, and P(eh|sc, m)P(m|sc) is context related information. P(yp|vp, m, eh) is related for the properties of task, and we have P(yp|vp, m, eh) = 1 if m(veh p ) = yp else P(yp|vp, m, eh) = 0.\nIdeally, if we can approximate the distribution P(vp|xp) and the distribution P(eh|sc, m)P(m|sc), we can obtain the distribution P(yp|xp, sc). Recall that we denote gweights(xp) as weights component and gcontext(sc) as context component. Based on the decomposited results of P(yp|xp, sc), we define our expectation for the components to be good: Definition 3.3. If f(\u00b7) has a good weights component in its representation, for any x, we can infer P(vx|x) from gweights(x), and if it has good context component, we can infer P(eh|sc, m) from gcontext(sc).\n# 3.3. Evaluation framework\nIn practice, since we cannot find the specific form of gweights and gcontext, such that fw,sc(xp) = gcomb(gweights(xp), gcontext(sc)). We leverage the probe method to measure the goodness of the components in the inner representation of the Transformer as defined in Definition 3.3. We choose the layers in the Transformer that can produce the best probe results. We give the details of the probing framework in the Appendix A.3.\nProbing methods and metrics We use three metrics here. weights comp. score: accuracy of the probe model to predict vx given x, where \u201ccomp.\u201d is short for component. context comp. score: the accuracy of the probe model to predict eh given sc, xp. Accuracy: We evaluate the prediction accuracy of the prompt example as the metric to evaluate the in-context learning performance. We give 39 context examples when evaluate context comp. score and accuracy.\nAll the results given in this paper are evaluated in the test set by default, as the performance of the test set is more aligned with the model\u2019s performance on real situations.\n# 3.4. Model, training and dataset detail\nTo simulate the auto-regression framework, we calculate the loss for the sequence s = {(x1, y1), . . . , (xL, yL)} as:\n(3)\nwhere s(j) \u225c{(x1, y1), \u00b7 \u00b7 \u00b7 , (xj, yj)}, l denotes the loss function. x will be tokenized by VAE (Kingma & Welling, 2013) before being passed to Transformer. Note that the weights of VAE are pretrained and fixed during the whole experiments. The training loss in the dataset S, which contains n sequence, is calculated as the average of loss over all training sequences, i.e.,\n\ufffd We leave the details of the model, dataset, training design and configuration in the Appendix A.\n# 4. Learning plateaus and weights compone\nRecall that we define learning plateaus as periods during the learning process in which the model experiences minimal or no improvement in performance on test data. Conversely, the transition process is characterized by a time span in which the model\u2019s performance rapidly enhances. Typically, a learning plateau precedes a transition process. When a learning process includes several instances of learning plateaus and transition processes, the terms \u201clearning plateaus\u201d and \u201ctransition process\u201d refer by default to the first occurrence of each.\n# 4.1. Controlling of complexity\nWe aim to understand when the learning plateaus will happen by controlling the complexity of the tasks. Therefore, we will present the method for controlling the complexity in this part. Recall from Definition 3.1 that the data is generated following the formula P(x, y, sc) = \ufffd m,eh P(m)P(eh)P(x, y, sc|m, eh). By altering the probabilities P(m), we can manipulate the resulting dataset. We first give two baseline configurations:\n\u2022 Dfix: There exsits a m0 such that P(m0) = 1 and for all m \u0338= m0, we have P(m) = 0.\n\u2022 Drnd: P(m) is a uniform distribution over all possible values.\nClearly, introducing greater randomness in the selection of m increases the complexity of the problem. We quantify this complexity using entropy, defined as H \u225c \ufffd m \u2212P(m) log P(m). It is noted that H(Dfix) = 0. Since our primary objective is to adjust the complexity of the incontext learning task rather than to examine the effects of various distributions of P(m), we simplify P(m) to a uniform distribution. The entropy is then managed by varying the size of the support set, that is, the number of possible mapping functions.\nWhen considering two distinct data configurations D1 and D2, the notation D1 \u21d2D2 denotes the performance evaluation of a model on data setting D2 after it has been trained on data setting D1. Unless specified otherwise, we assume that the model is both trained and tested on the same data configuration.\nLearning Plateuas and Transition Point Th\nLearning Plateuas and Transition Point sue at hand involves learning plateaus and transition patterns. To begin with, we investigate the capability of the synthetic task to mimic the specific pattern depicted in Fig. 1B. Our scrutiny is directed toward the learning trajectory of the synthetic task. As demonstrated in Fig. 3A, it becomes evi-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d68/3d68389c-16f5-4675-ba3a-26812eba0b9e.png\" style=\"width: 50%;\"></div>\nFigure 3. Learning plateus. A. We reproduce the learning plateaus and transition pattern in our synthetic task, similar to Fig 1B. B. The length of learning plateaus increase with the complexity of the task measured by entropy of P(m).\ndent that the synthetic task is successful in replicating the plateaus and transition pattern observed in the actual task.\nThe length of learning plateaus increases when increases the complexity of task We delved into the relationship between task complexity and the duration of learning plateaus. We employed the entropy of the mapping function m\u2019s distribution as an indicator of task complexity. To pinpoint the transition process, we identified the first epoch at which the model achieved a test accuracy greater than 0.17. This threshold was selected because the model\u2019s accuracy remains below 0.17 before reaching the transition process and rises above this thereafter. As anticipated, more complex tasks necessitate longer learning plateaus (Fig. 3B). However, the relationship between the length of the plateaus and the entropy is not linear. With every unit increase in entropy, the extension of the learning plateau is marginal when the entropy is either low or relatively high; conversely, the growth in plateau length is more pronounced at intermediate levels of entropy.\n# 4.3. Dysfunction of weights component\nIn the preceding section, we analyzed the plateaus-andtransition pattern in in-context learning concerning task complexity. In this section, our objective is to delve deeper into the role that internal mechanisms\u2014specifically, the quality of the weights and context components\u2014have in influencing the plateaus-and-transition pattern.\n# Confusing pattern of weights component\ncuted the task under two conditions: Dfix \u21d2Dfix and Drnd \u21d2Drnd, with the outcomes presented in Fig. 4A. As expected, in the simpler scenario of Dfix \u21d2Dfix, both the context and weights components exhibit improvement throughout the learning process, leading to enhanced incontext learning performance. However, in the more challenging setting of Drnd \u21d2Drnd, the weights component deteriorates over time, with its score remaining below the initial value for the entire duration of the learning process.\nThis differs from the context component, which improves with the rise in in-context learning performance. We refer to the situation where the weights component score falls below the starting value as a dysfunction of the weights component. This outcome is intriguing because training has no effect in improving the weights component. To gain better insight into this phenomenon, we carried out additional experiments across varying levels of task complexity. The model was trained for 50 epochs on these tasks, and we monitored the weights component score post-training. We found that the weights component score gradually declines as the entropy increases, eventually stabilizing at around 0.8, as shown in Fig. 4B.\n# Weight component degradation is linked to duration\n# Weight component degradation is linked to duration learning plateaus. Our primary concern is the duration\nof learning plateaus, and we seek to comprehend its connection to the in-weights component. To investigate this, we graphed the relationship between learning plateau length and weights component score after 50 epochs, as shown in Fig. 4C. Our analysis reveals an approximately linear correlation between the weights component score and the learning plateau duration. Short learning plateaus occur when there is a significant improvement in the in-weights component score. Conversely, long learning plateaus arise when the weights component is dysfunctioning or on the edge of dysfunction, that is when the weights component score is at or below the initial value.\n# Why the weights component is related to learning\nplateaus. We are attempting to comprehend this phenomenon, yet theoretically analyzing the learning process of a multilayer transformer on intricate data proves to be a formidable challenge. Recent theoretical studies (Tian et al., 2023; Deora et al., 2023; Huang et al., 2023) focus on the learning dynamics of Transformers with one or two layers using simple datasets. Given these limitations, we propose a more attainable, albeit weaker, construction analysis in Appendix E. Our approach is grounded in the notion that if a model with a good weights component can enhance its in-context learning capabilities with just a few additional parameters, then the weights component must be pivotal for achieving in-context learning prowess. To test this idea, we hypothesize that the in-weights component has been perfectly learned within a specific layer of the Transformer. Our findings reveal that by adding at most three extra Transformer layers specifically tailored for processing contextual information, the model demonstrates significant in-context learning performance. These outcomes suggest that in-context learning abilities are more readily achieved when the weights component is well-optimized.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79e4/79e4ace3-beb7-406c-8191-3d5f21c41332.png\" style=\"width: 50%;\"></div>\nFigure 4. Weights component and learning plateaus. A. The weights component score is increasing under Dfix \u21d2Dfix, while the weights component score is descreasing under the Drnd \u21d2Drnd setting. Note that the \u201cWeights\u201d and \u201cContext\u201d in the figure are short for weights comp. score and context comp. score respectively. B. The weights component score after 50 epoch training decreases when increasing the complexity of the task. The dashed green line indicates the weights component score at the initialization point. C. The weights component score at 50 epoch negative correlates with the length of learning plateaus. The dashed green line indicates the weights component score at the initialization point.\n# 5. Breaking Through Learning Plateaus\nThe previous section examined the mechanisms behind learning plateaus and identified their connection to the weights component. In this section, our goal is to investigate whether we can shorten the learning plateaus or improve the performance increasing for each transition process without scaling models. We have chosen the Drnd configure as the configuration of the test set by default, as it has demonstrated pronounced learning plateau behavior, and this particular scenario is known to be more effective in evaluating in-context learning capabilities, as discussed by Wei et al. (2023); Min et al. (2022).\n# 5.1. Weights warm-up method\nPrevious experiments demonstrate that the model better learns the weight component more effectively in the Dfix setting. A straightforward intuition is whether we can use Dfix to improve the weights component in Drnd test set. We give a further analysis of the relation between Drnd and Dfix: Increasing weights component with Dfix: Recall that we have P(yp|xp, sc) \u223cP(vp|xp)P(eh|sc, m)P(m|sc). Under Dfix setting, we only has one mapping function m0. Consequently, our model can readily learn that P(m0|sc) = 1. This simplification allows the model to concentrate on mastering P(vx|x). Therefore, it is anticipated that the model will develop a more refined weights component under this setting. Knowledge transfering between Dfix and Drnd settings: The knowledge of P(vx|x) is shared between these two settings. The reason is that P(vx|x) is unrelated to the mapping function m and the context sc. Based on this analysis, we propose the following data configuration to improve the weights component on Dfix before training on Drnd. This setting is denoted as Dfix\u2192rnd, which\nmeans that we initially train the model on Dfix for a specific epoch (weights warm-up), and then, we train the model on Drnd. We have the following discovery based on the experiments on Dfix\u2192rnd \u21d2Drnd.\nWeights warm-up helps to mitigate the learning plateaus. From Fig. 5A Top, we observe a notable enhancement in the weights component during the initial \u201cwarm-up\u201d phase. Subsequent to this phase, there is a swift improvement in the accuracy of the in-context learning. Nonetheless, a subsequent decline in the weights component score post-warm-up is apparent. This further verify that the Transformer can not learn a good weights component under Drnd settings. Further investigation is conducted by training the Transformer with various \u201cwarm-up\u201d durations under the Dfix setting. As shown in Figure5A Bottom, training the Transformer for a brief number of epochs effectively eliminates the initial learning plateaus. Furthermore, this \u201cwarm-up\u201d phase contributes to a reduction in the duration of the subsequent learning plateaus. The application of the \u201cwarm-up\u201d technique yields significantly better results compared to those without it, particularly after a training duration of 50 epochs.\n# 5.2. mixed training method\nIn this section, we try to improve the weights component during the whole training process by designing the Dfix\u2227rnd \u21d2Drnd setting. In Dfix\u2227rnd setting, half training data comes from Dfix setting, and half training data comes from Drnd setting. For a fair comparison, the total number of training data is the same as the training set that uses Drnd or Dfix setting only. The mixed training method significantly boosts the learning process. As anticipated, both the weights component and the task accuracy exhibit improvements throughout the\nIn this section, we try to improve the weights component during the whole training process by designing the Dfix\u2227rnd \u21d2Drnd setting. In Dfix\u2227rnd setting, half training data comes from Dfix setting, and half training data comes from Drnd setting. For a fair comparison, the total number of training data is the same as the training set that uses Drnd or Dfix setting only.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7ddc/7ddc2dac-3231-4773-8140-4a624098b989.png\" style=\"width: 50%;\"></div>\nFigure 5. Three methods are proposed to assist in overcoming learning plateaus. A: Effective of the warm-up method. Top: Employing Dfix as a warm-up for the Transformer significantly mitigates learning plateaus. The dashed line indicates the transition point from Dfix to Drnd. Bottom: We execute the transition from Dfix\u2192rnd \u21d2Drnd at various switching points. The curve labeled \u201d2\u201d signifies the switch from Dfix to Drnd at epoch 2. The curve labeled \u201d0\u201d serves as the baseline, that is, Drnd \u21d2Drnd. The dashed lines highlight the respective switching points. B: Combining Dfix and Drnd. Top: Mixed training substantially improves the weights component score during the learning process and eliminates learning plateaus. Bottom: Boosting the weights component can promote the development of in-context learning capabilities in smaller models. The dashed line depicts the task configuration Dfix\u2227rnd \u21d2Drnd, while the solid line represents the Drnd \u21d2Drnd setting. C: Extra Loss. Top: Incorporating a weights loss can significantly enhance learning, whereas adding context loss does not have a noticeable impact. The baseline is Drnd \u21d2Drnd. Bottom: With the weights loss, the Transformer can attain a commendable weights component score after 50 epochs of training. The green dashed line indicates the weights comp. score at the initialization point.\nlearning process, as depicted in Fig.5B Top. And the results do not show a pronounced learning plateau. This suggests that concurrently enhancing the weights component is beneficial. In Fig.5B Bottom, we compare models of various sizes trained on the Drnd \u21d2Drnd setting (represented by the solid line) against those trained on the Drnd\u2227fix \u21d2Drnd setting (depicted with a dashed line). It is observed that a model of size 6.69M trained on Dfix\u2227rnd, after 20 epochs of training, can attain an accuracy comparable to a model of size 13.88M trained on Drnd that has undergone 50 epochs of training. These findings suggest that overcoming learning plateaus can lead to a reduction in the computational resources required for training.\n# 5.3. Extra loss method\nThe previous methods require another data setting Dfix to improve the weights component. Here, we consider another alternate method by providing an extra supervision signal to improve the weights component when we cannot find the Dfix setting. We consider two extra loss settings. The f \u2032 w\u2032,sc(x) is denote as the subnetwork of fw,sc(x) without the output classifier. We denote the clse as the classifier for the factor value of factor e. The weights loss is defined as\nThe context loss is defined as\nwhere clseh is the classifier to predict hidden factor, and e(s) h is the hidden factor for sequence s. Then, the original loss function of Equation 3 is modified into L(\u03b8, s) + \u03bbLc(\u03b8, s) or L(\u03b8, s) + \u03bbLw(\u03b8, s). The \u03bb is chosen as 0.1 in our experiments.\nAdd weights loss speedup the learning process while adding context loss fails. Fig. 5C Top reveals that incorporating an additional weights loss significantly aids the Transformer model in overcoming learning plateaus. In contrast, adding an extra context loss yields only a marginal benefit. These outcomes further substantiate our hypothesis that enhancing the weights component is crucial for breaking through learning plateaus, rather than concentrating on the context component. As shown in Fig. 5C Bottom, we note a marked improvement in the weights component when an extra weights loss.\n# 5.4. Further Exploration: Simple Functions Tasks\nIn this section, we give further exploration of the relation between the weights component and the learning plateaus on the simple function tasks (Garg et al., 2022) and further understand the role of weights component in learning plateaus.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf0b/cf0bdd60-a881-4753-a0fb-0305ad85efb6.png\" style=\"width: 50%;\"></div>\nFigure 6. Experiments results on simple function tasks after 50 epochs. A: the dysfunction of the weights component happens when d > 30. B: the effect of Extra Loss technique is significant only when the dysfunction of weights component happens.\nTwo Roles of Weights Component Initially, I\u2019d like to emphasize that the weights component serves two primary functions: 1) It takes the examples xi into the internal working of the Transformer. 2) It aims to find a better representation for xi. Considering this, the dysfunction of the weights component can stem from these two sources correspondingly. In the previous analysis of the Shape3D task, the weights component draws from two sources. Since simple function tasks lack representation learning, the dysfunction of the weights component might primarily be attributed to the first reason.\nDate generation process In this task, we generate a dataset with n sequences. The generation process for each sequence is:\n\u2022 Initially, we sample a vector w from the Gaussian distribution N(0, Id) in Rd. Each component of w is drawn independently.\n\u2022 Subsequently, we sample {x1, x2, . . . . . . , xL}, where each xi \u2208Rd is drawn from a Gaussian distribution N(0, Id).\n\u2022 The labels are then determined using the formula yi = sign(wT xi). Finally, we obtain a sequence s = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xL, yL)}.\nExploring Framework The training methodology aligns with that described in the paper (Garg et al., 2022). We\u2019ll modify the dimension of xi, i.e. the value of d to modulate the difficulty of the task. The evaluation of the weights component follows the same process outlined in Appendix A.3, but with the utilization of the metric MSEp = Exi\u2225\u02dcxi \u2212xi\u2225to assess its performance, where \u02dcx is the prediction of the probe model. We use the Transformer with 6 layers and we probe at the layer 3. It\u2019s worth noting that unlike the weight component score, where higher is better, lower values of MSEp indicate better weights component.\nResults The MSEp and test accuracy are given in Figure 6. We find that: 1) Dysfunction in the weights component is evident in the SimpleFunction dataset when d > 30. The Transformer exhibits poor performance in these situations. 2) the effect of the Extra Loss technique is significant only when the dysfunction of weights component happens. The effect is incremental when the Transformer doesn\u2019t experience a significant weights dysfunction. This further verifies the causal relation between the weights component and learning plateaus.\n# 5.5. Beyond Synthetic Task\nTo ensure that the proposed methods are broadly applicable, this section examines their adaptability to natural language processing (NLP) tasks.\nSST-ICL task The task employs the SST dataset as outlined by Socher et al. (2013). An illustration of the task is given in Fig. 7A. Comprehensive details regarding the dataset structure, training methods, and model configurations are available in Appendix B. We assess the results for both the Dfix\u2227rnd \u21d2Drnd. It is noted that improving the weights component through training on Dfix similarly aids in-context learning within this domain. Nevertheless, it should be acknowledged that variations exist between the outcomes of the SST-ICL task and those of the synthetic task; specifically, Dfix settings worse the performance when trained on small epochs, which is evident in the Figure 7 that the Dfix\u2227rnd performs worse than Drnd when training epoch is less than 30. The key reason to this divergence is that the weights component is more difficult to improve compared that in the synthetic one. As a result, more epochs of training are needed to improve the weights component so as to make it come into effect. This explanation is evident by the result in Figure 7 that the model trained on Dfix\u2227fix setting significantly outperforms that trained on Drnd settings after the 35 epochs. WordSelection task We have devised an additional task named the WordSelection Task, which requires selecting a single word from a group of four options. For example, given the input \u201chello information learning art \u2192learning\u201d, the task for the model is to identify \u201clearning\u201d as the correct choice from the provided set. The model must infer the correct answer by considering context examples. We offer five such in-context examples, all selecting the words at the same position as the prompt example. Creating a specific Dfix setting to improve the weights component poses a challenge in this task. Consequently, we opt for the implementation of the extra weights loss technique. We define the factor as the position and the factor values of a factor is the corresponding word in this position. For example, with x being \u201chello information learning art\u201d, we define v(e1) x as \u201chello\u201d, v(e2) x as \u201cinformation\u201d, v(e3) x as \u201clearning\u201d and\nSST-ICL task The task employs the SST dataset as outlined by Socher et al. (2013). An illustration of the task is given in Fig. 7A. Comprehensive details regarding the dataset structure, training methods, and model configurations are available in Appendix B. We assess the results for both the Dfix\u2227rnd \u21d2Drnd. It is noted that improving the weights component through training on Dfix similarly aids in-context learning within this domain. Nevertheless, it should be acknowledged that variations exist between the outcomes of the SST-ICL task and those of the synthetic task; specifically, Dfix settings worse the performance when trained on small epochs, which is evident in the Figure 7 that the Dfix\u2227rnd performs worse than Drnd when training epoch is less than 30. The key reason to this divergence is that the weights component is more difficult to improve compared that in the synthetic one. As a result, more epochs of training are needed to improve the weights component so as to make it come into effect. This explanation is evident by the result in Figure 7 that the model trained on Dfix\u2227fix setting significantly outperforms that trained on Drnd settings after the 35 epochs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c96/8c96cea0-0aa5-4d80-b547-b8fdb7e834ed.png\" style=\"width: 50%;\"></div>\nFigure 7. Experiments on natural language tasks. A: Example of dataset SST-ICL task. The example label y is obtained from v by ma function m, i.e. y = m(v). B: Results on SST-ICL task. We explore the Dfix\u2227rnd \u21d2Drnd and Drnd \u21d2Drnd settings. The dashed lin denotes the time when we transit from Dfix to Drnd. C: Results on WordSelection Task. Adding extra weights loss has a significant effec in shortening the learning plateaus.\nv(e4) x as \u201cart\u201d. The extra weights loss is then applied in a manner consistent with what is described in Eq. (5). Our experimental findings align with those from the synthetic task, indicating that the advantage of integrating an extra weights loss is not limited to synthetic environments. The results highlight that adding extra weights loss can be a beneficial strategy across different task types.\n# 6. Discussion\nQ1: What are the reasons and intuitions behind the dysfunction of weights? The observed dysfunction in the Transformer\u2019s performance may stem from the confluence of two crucial factors: 1) The problem-solving process relies on the information derived from both context examples and query examples. 2) The contextual information is hard to obtain. In situation 1, where both contextual and weight information are critical, the Transformer is compelled to harness both effectively. Yet, under condition 2, where the contextual information proves difficult to learn, the Transformer finds itself unable to fully extract this vital knowledge. Consequently, it lacks the incentive to refine the weight component at this stage, as doing so would not yield appreciable improvements in overall performance. This phenomenon aligns with our observation in Figure 4C that the weight dysfunction occurs predominantly when the in-context tasks exhibit a high degree of complexity, as measured by the high entropy of the distribution of m. As a result, we witness the manifestation of weight dysfunction, characterized by the stagnation or regression of the weight component during the learning process, despite the Transformer\u2019s ongoing attempts to adapt and solve the given problem.\n# Q2: Whether the decomposition of weights and context components is general? The conceptual decomposition is general based on the following reasons:\n# Q2: Whether the decomposition of weights and context components is general? The conceptual decomposition\n1) Firstly, the decomposition of the weights component and the context component is conceptual instead of physical and only for analysis purposes. The decomposition stems from the understanding that the in-context learning task\nnecessitates information from both its context samples and the query example.\nthe query example. 2) Secondly, our evaluation of the weights component and context component is not based on the physical decomposition of these components. We employ complex probe methods (see Section A.3) to analyze these two components because we have only conceptually decomposed them.\n3) We tested our method across various scenarios, including the Shape3D task, SST-ICL task, Word Selection Task, and SimpleFunction task, which consistently validated the efficacy of our approach.\n# 7. Conclusion\nThis paper establishes a connection between the weights component and learning plateaus. Building on this connection, the paper proposes three strategies to overcome these plateaus. These strategies have proven to be effective in both synthetic and natural language tasks.\n# 8. Limitation\nIn this paper, we mainly focus on understanding of the learning plateaus of in-context learning with Transformer, where the Transformer requires the information from the context examples and the query example to make a prediction. Therefore, our method cannot explain the learning plateaus phenomenon outside this score. There are some works (Nanda et al., 2023; Power et al., 2022) that discuss the learning plateaus phenomenon in supervised learning. Our work fails to explain these phenomena.\n# Acknowledgements\nJingwen Fu, Tao Yang, and Nanning Zheng are supported by the National Natural Science Foundation of China (Grant No. 62088102).\n# Impact Statement\nThe goal of this paper is to understand the mechanism of learning plateaus in Transformers and find a method to avoid learning plateaus. Our work doesn\u2019t have a direct influence on society. However, future works based on our work may influence society but it is unpredictable currently.\n# References\nAky\u00a8urek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.\nAlain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\nBai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397\u20132430. PMLR, 2023.\nBietti, A., Cabannes, V., Bouchacourt, D., Jegou, H., and Bottou, L. Birth of a transformer: A memory viewpoint. arXiv preprint arXiv:2306.00802, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020.\nBrunner, G., Liu, Y., Pascual, D., Richter, O., Ciaramita, M., and Wattenhofer, R. On identifiability in transformers. arXiv preprint arXiv:1908.04211, 2019.\nChan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022a.\nChan, S. C., Dasgupta, I., Kim, J., Kumaran, D., Lampinen, A. K., and Hill, F. Transformers generalize differently from information stored in context vs in weights. arXiv preprint arXiv:2210.05675, 2022b. Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. Deora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C. On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680, 2023. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Edelman, B. L., Goel, S., Kakade, S., and Zhang, C. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pp. 5793\u20135831. PMLR, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023. Huang, Y., Cheng, Y., and Liang, Y. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023. Kim, H. and Mnih, A. Disentangling by factorising. In International Conference on Machine Learning, pp. 2649\u2013 2658. PMLR, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nDeora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C. On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680, 2023.\nDong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nEdelman, B. L., Goel, S., Kakade, S., and Zhang, C. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pp. 5793\u20135831. PMLR, 2022.\nKirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nLi, K., Hopkins, A. K., Bau, D., Vi\u00b4egas, F., Pfister, H., and Wattenberg, M. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022. Li, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. Transformers as algorithms: Generalization and stability in incontext learning. International Conference on Machine Learning, 2023. Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. Lu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H. T., and Gurevych, I. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809, 2023. Michaud, E. J., Liu, Z., Girit, U., and Tegmark, M. The quantization model of neural scaling. arXiv preprint arXiv:2303.13506, 2023. Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022. Press, O., Smith, N. A., and Levy, O. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. Reddy, G. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. arXiv preprint arXiv:2312.03002, 2023. Richter, O. and Wattenhofer, R. Normalized attention without probability cage. arXiv preprint arXiv:2005.09561, 2020.\nSchouten, S., Bloem, P., and Vossen, P. Probing the representations of named entities in transformer-based language models. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 384\u2013393, 2022. Shin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B., Cho, K., Lee, G., Park, W., Ha, J.-W., et al. On the effect of pretraining corpora on in-context learning by a largescale language model. arXiv preprint arXiv:2204.13509, 2022. Singh, A. K., Chan, S. C., Moskovitz, T., Grant, E., Saxe, A. M., and Hill, F. The transient nature of emergent in-context learning in transformers. arXiv preprint arXiv:2311.08360, 2023. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013. Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Voita, E. and Titov, I. Information-theoretic probing with minimum description length. arXiv preprint arXiv:2003.12298, 2020. Voita, E., Sennrich, R., and Titov, I. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. arXiv preprint arXiv:1909.01380, 2019. von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. Wiegreffe, S. and Pinter, Y. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019.\n# A. Detail of experiments on sythetic tasks\nA.1. 3DShape\n3dshapes1 (Kim & Mnih, 2018) is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation. All possible combinations of these latents are present exactly once, generating N = 480000 total images. Latent factor values including: 1) floor hue: 10 values linearly spaced in [0, 1], 2) wall hue: 10 values linearly spaced in [0, 1], 3) object hue: 10 values linearly spaced in [0, 1], 4) scale: 8 values linearly spaced in [0, 1], 5) shape: 4 values in [0, 1, 2, 3], and 6) orientation: 15 values linearly spaced in [-30, 30].\nShape3D dataset We employ the Adam optimizer (Kingma & Ba, 2014) and mini-batch training to optimize the loss function L(\u03b8, S). Here, we use cross-entropy as the loss function. We utilize a batch size of 128 and set the learning rate to 0.0001. For training purposes, we use 105 sequences and, for evaluation, 4 \u00d7 104 sequences. There is no overlap between images in the training sequences and those in the evaluation sequences.\n# A.2. Architecture Detail\nWe employ a pre-trained Variational Autoencoder (VAE) to transform images into tokens. The encoder of the VAE comprises seven convolutional layers with ReLU activations, followed by three linear layers two ReLU activations. Conversely, the VAE\u2019s decoder is structured with three linear layers and two ReLU activations, which precede a sequence of seven convolutional layers also with ReLU activations. The resulting latent representation serves as the token representation of the input image. The label embedding is learned during the in-context training process. Our study\u2019s primary goal is to explore the characteristics of in-context learning. To this end, we utilize a causal Transformer architecture that restricts each token to only interact with preceding tokens. Specifically, the default configuration of the Transformer, denoted as f, includes 12 layers, 4 attention heads, and an embedding dimension of 128. As depicted in the lower section of Figure 5B, we experiment with varying model sizes. Detailed configurations for these model sizes can be found in Table 1.\n<div style=\"text-align: center;\">Table 1. Model configure with different size in Fig 5B Bottom</div>\nTable 1. Model configure with different size in Fig 5B Bottom\nNumber of Layers\n12\n6\n6\n3\nAttention head\n4\n4\n2\n2\nEmbedding size\n128\n64\n32\n16\nModel Size\n34.53\n13.88\n6.69\n3.24\n# A.3. More detail about probe framework\nA.3. More detail about probe framework\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4750/4750c656-1dcc-4e34-933d-b3669c0793ac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8. Probe method. AB: illustration of the probe method. C: The weights and in-context score when we probe at different layers. W choose the Dfix\u2227rnd \u21d2Drnd settings. The dashlines marked the chosen layers in the experiments.</div>\nhidded in the representation, we use the probe method (Alain & Bengio, 2016). The probe classifier has a single linear layer, with softmax and cross-entropy calculating the loss. Because we don\u2019t know the specific form of gweights and gcontext, we choose the representation within the Transformer that given the high score for weights and context components for approximating. We use the linear probe because using more complex probe model doesn\u2019t have significant improve in accuracy and the change of the accuracy during the training process is more important than the absolutely value. The probe model is trained utill totally converge. The probe is trained for 2 epoch for context component and 1 epoch for weights component. The in-weight probe predicts values of six factors of all images, while the in-context probe identifies the hidden factor for each sequence. The details are as follows. context comp. score Context comp. score measures whether the Transformer can capture the information from all the context examples, i.e., whether the inner representation within Transformer can capture the distribution P(yp|xp, sc). Given the test set S\u2032, the context comp. score is calculated as 1 |S\u2032| \ufffd s\u2208S\u2032 1\u02c6eh,s=eh,s, where 1expr is indicator function, s is the sequence in the dataset S, eh,s is the hidden factor for the sequence s, and \u02c6eh,s is the prediction of probe classifier. We use | \u00b7 | to denote the corresponding size of a set. weights comp. score The weights comp. score measures whether the Transformer can learn the information from individual examples, i.e. whether the inner representation of Transformer can capture the distribution P(vx|x). Because the distribution P(vx|x) is unrelated to the context examples sc, we remove the influence of context examples by removing the context examples when evaluate the weights component score. The weights comp score is calculated as 1 |S\u2032| \ufffd s\u2208S\u2032 1 |s||E| \ufffd (x,y)\u2208s \ufffd e\u2208E 1\u02c6v(e) x =v(e) x , where ve x is factor value of factor e and sample x , \u02c6v(e) k is the prediction of probe classifier, s = {(x1, y1), . . . , (xL, yL)} is the sequence in the dataset S\u2032 and E is the set of all factors. Accuracy We measure the accuracy of the prediction of in-context learning task given a fix number of context example as the measure for the in-context performance. In this paper, we choose the number of context examples as 39.\n# A.4. Dataset split\nIn-context training We first split all the the images in Shape3D into two part: the training image set (80 %) and the test image set (20 %). Then, we organize all the training images into Sfix, Srnd, Sfix\u2227rnd, corresponding to Dfix, Drnd, Dfix\u2227rnd settings. Sfix, Srnd, Sfix\u2227rnd Test image set are also organized into S\u2032 fix, S\u2032 rnd, S\u2032 fix\u2227rnd. Each of Sfix, Srnd, and Sfix\u2227rnd contains 105 sequences. Each of S\u2032 fix, S\u2032 rnd, S\u2032 fix\u2227rnd contains 4 \u00d7 104 sequences. Probe model training If we want to probe a model fw,sc(\u00b7) on setting Drnd (test setting), we will first train the probe model on Srnd with fw,sc(\u00b7) and we evaluate the probe model on S\u2032 rnd with fw,sc(\u00b7). The same for Dfix and Dfix\u2227rnd settings.\n# B. Detail of experiments on langugae task.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/24a1/24a1a47e-f37a-4ff5-94ac-99587c3ac65d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9. Example of the SST-ICL task and WordSelection task.</div>\n# B.1. Experiments on Pythia\nWe first indroduce the experiment design on the Pythia 13B model, because setup of this experiment is totally different from the other experiments on Section 5.5. We leverage the opensouce of the Pythia 13B checkpoints. The Pythia is trained using autoregression framework on the Pile dataset. We evaluate the performance of the Pythia model for each 10k steps. We construct 400 sequence of 4 wordselection task. All the words are randomly sampled from 2000 words. 4 context examples are given for each prompt. The prediction of the Pythia model is processed to ensure that it has same form as the given label.\n# B.2. Model Struture\nFor both tasks, we use the GPT2 model in this setting. The model is consist of 6 layers, 4 attention heads with 368 embedding size.\n# B.3. Dataset Detail\nSST-ICL dataset The dataset is contructed based on SST (Socher et al., 2013) datasets. We remove the long review in the datasets and transform the original labels into \u201cNegative\u201d, \u201cPositive\u201d and \u201cNeutral\u201d. Then, we organize the reviews follow the same way as that in Subsection A.4. We produce 104 sequence for training and 4 \u00d7 103 for testing. Each sequence contains 5 reviews. We illustrate the example of the dataset in Fig. 7 AB. WordSelection Dataset We choose 2000 words for the experiments. The 2000 words is organzed into 105 training sequence and 5 \u00d7 104 training sequence. Each sequence contain six examples and five examples served as in-context examples and the rest served as the prompt example.\n# B.4. Training Detail\nFor SST-ICL task and WordSelection task, the models are both trained using AdamW optimizer with learning rate 2e \u22125 We choose the batch size as 64.\n# C. Other Related works\n# C.1. In-weights and in-context learning\nPrevious studies (Chan et al., 2022a;b) examined the relationship between in-weights learning and in-context learning. The division of in-weights learning and the in-context learning process is conceptually similar to our distinction between the influence of the weights and context component. In in-context learning, the Transformer relies on a combination of both its internal weights and the context provided to address a given task. In contrast, for in-weights learning, the model depends exclusively on its weights. Nevertheless, there are notable differences between these two notions: 1) The internal weights and context components are concerned with capturing information within the Transformer, whereas the focus of in-weights and in-context learning is on how the Transformer tackles a task. In-weights learning is a rename for regular supervised learning. 2) Both weights and context components coexist and play roles within the paradigm of in-context learning. While in-weights learning is distinguished from in-context learning.\n# regarding weights and context component are both important for in-\nIntuition 1: Influence of words replacing A key difference between the weights and context components lies in the susceptibility of the weights component to word substitution. The weights component can be easily disrupted if a word is replaced with a token that was not present during the training phase, as the weights lack information about this new token. On the other hand, if the context examples are rich in information, the meaning of this new token can still be deduced. This mirrors the human ability to infer the meaning of an unknown word based on its context. If word substitution leads to a decline in performance, it suggests that the Transformer\u2019s prediction relies heavily on the weights component. Intuition 2:Influence of number of in-context examples The efficiency of the context component is expected to rise with the inclusion of more context-specific examples, a characteristic not shared by the weights components, which remain unaffected by the addition of in-context examples. Therefore, if performance improves with the integration of more context-specific examples, it would suggest that the Transformer\u2019s prediction is heavily influenced by the context component.\nIntuition 3: Zero-shot performance The zero-shot performance can directly indicate the effectiveness of the weights component. This is because no in-context examples are provided in this scenario, reducing the problem to a traditional supervised one Based on the intuitions above, we collect the related experiments in practice paper.\n1. Min et al. (2022) discovered that (1) performance can be improved by increasing the number of in-context examples. (2) Changing the labels of in-context examples does not influence the predicted label. The first discovery indicates that the prediction relies on the context components. The second discovery suggests that the Transformer uses the weights component for label prediction, given that there is no observed change when the labels of in-context examples are altered. 2. Brown et al. (2020) found that larger models are increasingly effective at utilizing in-context information. This suggests that in real-world scenarios, the efficiency of the context component improves with the enlargement of the model\u2019s size. Brown et al. (2020) also found that enhancing the model size can boost its zero-shot capabilities. These findings suggest that scaling the model can enhance both the weights and context components, and the model employs these two components to address the problem. 3.(Wei et al., 2023) carried out research on a two-class classification issue. They conducted experiments in which they altered a certain percentage of labels in the context examples to ascertain if the model\u2019s prediction would also change. If a change was observed, it would imply that the prediction relies on the context components. If no change was noticed, the prediction would be considered to depend on the weights component. Their results were intermediate, suggesting that both weights and context components contribute. Additionally, they found that enhancing the model size increases the impact of in-context examples.\n# C.3. Related works for understanding Transformer\nAnalysis of Transformer The analysis of Transformers can be broken down into two main components: examining the expressibility of Transformers and comprehending the mechanisms of learned Transformers. To analyze the expressibility of Transformers, a common approach is to determine if they can solve specific problems by constructing appropriate weights. Giannou et al. (2023) demonstrates that Transformers can function as Turing machines, while Liu et al. (2022) shows that they can learn shortcuts to solve automata problems. In addition to expressibility, researchers have also investigated the mechanisms behind learned Transformers. Bietti et al. (2023) examines Transformers from a memory standpoint, and Tian et al. (2023) focuses on single-layer Transformers. While the analysis of Transformers is crucial to our work, our ultimate goal differs; we aim to bridge the gap between representation learning and in-context learning. Exploration of representation within Transformer. Owing to the widespread use of Transformers, numerous studies (Li et al., 2022; Voita & Titov, 2020) seek to investigate their internal representations as a means of comprehending their functionality. The most prevalent approach involves utilizing probe models and tasks to discern the information stored within these representations (Voita & Titov, 2020; Schouten et al., 2022). Taking a different perspective, Voita et al. (2019) explores the flow of information across Transformer layers and how this process is influenced by the selection of learning objectives. Our work shares similarities with these studies in that we employ the probe method to examine representations. However, our focus differs in that we do not concentrate on the semantic meaning within the representation. Instead, we investigate how the weights and in-context information impact representation.\nAnalysis of Transformer The analysis of Transformers can be broken down into two main components: examining the expressibility of Transformers and comprehending the mechanisms of learned Transformers. To analyze the expressibility of Transformers, a common approach is to determine if they can solve specific problems by constructing appropriate weights. Giannou et al. (2023) demonstrates that Transformers can function as Turing machines, while Liu et al. (2022) shows that they can learn shortcuts to solve automata problems. In addition to expressibility, researchers have also investigated the mechanisms behind learned Transformers. Bietti et al. (2023) examines Transformers from a memory standpoint, and Tian et al. (2023) focuses on single-layer Transformers. While the analysis of Transformers is crucial to our work, our ultimate goal differs; we aim to bridge the gap between representation learning and in-context learning.\n# D. Proof of Proposition 3.2\nProposition D.1. Given yp, probability of P(yp|xp, sc) can be decomposite as:\nP(yp|xp, sc) = \ufffd vp,m,eh P(yp|vp, m, eh)P(vp|xp)P(eh|sc, m)P(m|sc).\nProof.\n(7)\nwhere the first equation is due to the law of total probability, the third equation is leverages the formul P(yp|xp, sc, vp, m, eh) = P(yp|vp, m, eh).\n# E. Contruction analysis of why the weights component is related to the learning plateaus\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1969/196908f3-3b00-4005-9b7d-2e8d245c5912.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">gure 10. The constructed Transformer can match the performancce of trained Transformer (Dfix\u2227rnd setting) in experiment part</div>\nIntuition In this section, our objective is to comprehend the connection between the weights component of the model and the occurrence of learning plateaus. Owing to the challenges involved in dissecting the training dynamics of the Transformer, we turn to constructive analysis as a methodology. By examining a scenario where a Transformer possesses an effective weights component at a specific layer, and only a small number of additional layers are required to attain proficient in-context learning performance, we can indirectly infer insights about their relationship. Notation The position embedding is denoted as pi = (0, \u00b7 \u00b7 \u00b7 0, 1, 0, \u00b7 \u00b7 \u00b7 ), where we only have value 1 at i-th position and 0 others. The weights for the attention operation of l-th layer and c-th head in Transformer is denoted as W(l,c) Q , W(l,c) K and W(l,c) V . The weights of the forward layer in the Transformer are denoted as W(l) 1 , W(l) 2 . We use E to denote all possible values of the factor e. we denote yi as the one hot version of y. The vector with all zero values is denoted as 0 \u225c(0, \u00b7 \u00b7 \u00b7 , 0). We consider the naive Transformer (Vaswani et al., 2017). The hidden representation of token i in Transformer is denoted as hi \u2208Rd. The hidden representation of l-th layer is denoted as H(l) = [h(l) 1 , \u00b7 \u00b7 \u00b7 , h(l) 2L]T \u2208R2L\u00d7d. Given a input token x, we denote h(l) x as its corresponding representation at layer l. The factor value of this token is denoted as vx. The factor value of the corresponding factor e is denoted as v(e) x . The set of all possible values of factor e is denoted as Ve. The size of the set is denoted as |Ve|. In our analysis, we explore a modified, more relaxed variant of the Transformer model. The rationale behind this relaxation is underpinned by evidence suggesting that 1) employing the ReLU activation function in the feed-forward layers can achieve results comparable to the original model (Press et al., 2019), and 2) the softmax operation may not be essential for the functioning of the Transformer (Wiegreffe & Pinter, 2019; Brunner et al., 2019; Richter & Wattenhofer, 2020). The relaxed Transformer is defined as follows: Definition E.1. (Transformer) One layer of the Transformer contains one attention layer and one MLP layer. The calculation of the Attention Layer is\nAnd the calculation of MLP layer is\nH(l+1) = Attn(l)(H(l)) + Relu(Attn(l)(H(l))W(l) 1 )W( 2\nHere we consider relaxed case where \u03c3 = Id.\nRemember that a good weights component implies that we have the ability to deduce P(vx|x) based on this component. To streamline the construction, we introduce a stronger assumption: the concept of a perfect weights component. Unlike the\n\n(9)\n(10)\ndefinition of a good weights component, which necessitates that the Transformer encapsulates information about P(vx|x), a perfect weights component also demands that this information should be readily accessible. If the representations of the images corresponding to different factor values are situated in distinct orthogonal bases, then the factor values\u2019 information can be easily decoded. Drawing on this insight, we propose the following definition. Definition E.2. (Perfect weights component) If a Transformer has perfect weights component in layer l, then for all factor e, any i, j, exists We \u2208Rd\u00d7|Ve| such that f (e) x1 \u00b7 f (e) x2 = 1 only when v(e) x1 = v(e) x2 , else we have f (e) x1 \u00b7 f (e) x2 = 0, where f (e) x = Weh(l) x . Under the assumption of a perfect weights component, we can enhance the Transformer by adding at most three extra layers that are specifically designed to learn the context component. The detailed results of this construction are as follows: Proposition E.3. We consider the data with ne factors and each factor has nv values in Drnd setting. For causal Transformer with the number of heads larger or equal the number of factors with the hidden size O(nenv + L), if the Transformer can learn a perfect weights component in layer k, then it can learn a representation given i in-context examples with context comp. score srsi = (1 \u2212srsi\u22121)si + srsi\u22121 and srs0 = s0 at layer k + 2, where si = 1 \u2212\ufffdi j=0 \ufffdi j \ufffd\ufffd|E| k=2 \ufffd|E| k \ufffdk\u22121 k \ufffd (nv\u22121)i\u2212j niv \ufffdk \ufffd 1 \u2212(nv\u22121)i\u2212j niv \ufffd|E|\u2212k , and we can obtain the accuracy as clsi = (nv\u22121)(ni\u22121 v \u2212(nv\u22121)i\u22121) niv srsi + 1 nv at k + 3 layers. The constucted Transformer achieve significant performance. To illustrate that the construction is meaningful, we compare the performance of the constructed model with the previously trained model. We choose the performance of the model trained on Dfix\u2227rnd settings as it is the best-performed model on Drnd setting. We find that the constructed model achieved a comparable performance as that of the trained model. These results indicate that our construction is meaningful.\nThe constucted Transformer achieve significant performance. To illustrate that the construction is meaningful, we compare the performance of the constructed model with the previously trained model. We choose the performance of the model trained on Dfix\u2227rnd settings as it is the best-performed model on Drnd setting. We find that the constructed mode achieved a comparable performance as that of the trained model. These results indicate that our construction is meaningfu\n# E.1. Proof Sketch\n# a) Contruction of the Transformer.\nWe divided the contruction into two steps. The first step is to estimate the factor in the sequence and the second step is to estimate the label based on the discovered hidden factor. Given the sequence s = {(x1, y1), . . . , (xL, yL)}, we short fxj as\nestimate the label based on the discovered hidden factor. Given the sequence s = {(x1, y1), . . . , (xL, yL)}, we short fxj as fj. 1. Estimate the hidden factor. According to the perfect weights component assumption, for any j, we can project the j-th token feature into the space f (e) j . Assuming the j-th token is not the prompt token, then we have its label information yj. Then, for i-th token, where i < j, obviously i is also not the prompt token. As a result, we also have the information about yi. If a factor e is the hidden factor, then we would expect yi \u00b7 yj = m(v(eh) i ) \u00b7 m(v(eh) j ) = v(eh) i \u00b7 v(eh) j = f (eh) i \u00b7 f (eh) j , where v, y, m is the corresponding one-hot version of v, y, m. Therefore, if we can find e such that f (e) i \u00b7 f (e) j can have a same value as yi \u00b7 yj, for all i, then e can be predicted as hidden factor. Based on this intuition, in the construction, we focus on finding a way to compare the value between f (e) i \u00b7 f (e) j and yi \u00b7 yj. All these operations are done in the first layer. 2. Block unrelated information. After finding the hidden factor, the next step is to block the information that is unrelated to the hidden factor. Blocking unrelated information can remove the influence of it and simplify the following steps. We add a large negative value to the positions of the representation that is unrelated to the hidden factor. Then, through the Relu operation, all these negative values will be removed. These operations are placed in Layer 2. 3. Predict y. The final step is to predict the yp for the prompt sample xp. The challenge here is that we don\u2019t know the mapping function m that bridges the factor value and the corresponding label. Consider the relation that for any i, j satisfying i < j, we have v(eh) i \u00b7v(eh) j = f (eh) i \u00b7f (eh) j = yi \u00b7yj. Therefore, if we can find a i, satisfying that f (eh) i \u00b7f (eh) j = 1, then we have yi = yj. Then, we can copy yi to the representation of j-th token and use the final linear layer to output yi as the prediction for yj.\n# b) Analyzing the performance.\nGiven the perfect weights component assumption, there are two source that cause the error prediction. We give the separate analysis below. 1. Fail to find the correct hidden factor. Finding the correct hidden factor is essential to make the correct prediction. The correct hidden factor cannot be inferred if there are more than two factors such that the values of f (e) i \u00b7 f (e) j and yi \u00b7 yj can\nbe matched according to the construction of first layer.\n2. Make error prediction and give the correct factor. Given the hidden factor, it is also possible that we cannot make the correct prediction if we fail to associate the correct label the factor value. This will happen if all the context examples don\u2019t contain the same factor value of the hidden factor as the prompt image.\n# E.2. Proof of useful lemma\nIn this section, we proof some useful lemma for the proof. The lemma E.4 indicates that the attention head can copy part of the representation from its previous token into current token. The lemma E.5 indicates that there exists a construction to make the MLP to only operate on the part of its input.\nemma E.4. One attention head can implement copy and past behavior.\nProof. According to the definition of pi, we have pi \u00b7 pj = 0 if i \u0338= j, otherwise, we have pi \u00b7 pj = 1. We denote\n\uf8f0 \uf8fb Then we have piM = pi\u22121. For j > i, we denote the value of 2j-th token as h2j = (0, 0, 0, 0, 0, 0, 0, pj) and 2i-th token as h2i = (h\u2032 i, 0, 0, 0, 0, 0, 0, pi). If we want to copy the value of 2i-th token to the value of 2j-th token, we can set the query matrix as WQ = (0, 0, 0, 0, 0, 0, 0, Mj\u2212i), the key matrix as WK = (0, 0, 0, 0, 0, 0, 0, I) and value matrix as WV = (W\u2032 V , 0, 0, 0, 0, 0, 0, 0). Then we have\nTherefore, the 2j-th token can only attend to the token with the position embedding pi. If h2i\u22121 = (0, 0, 0, 0, 0, 0, 0, pi), we have the value of hj after attention as hattn j = ((h\u2032 i)TWV , 0, 0, 0, 0, 0, 0, pj). By setting WV as different value, we can copy different part information of i-th to j-th token. Then the lemma is held.\nTherefore, the 2j-th token can only attend to the token with the position embedding pi. If h2i\u22121 = (0, 0, 0, 0, 0, 0, 0, pi), we have the value of hj after attention as hattn j = ((h\u2032 i)TWV , 0, 0, 0, 0, 0, 0, pj). By setting WV as different value, we can copy different part information of i-th to j-th token. Then the lemma is held. Lemma E.5. For the input h = (h1, h2, h3), where hi \u2208Rdi and d1+d2+d3 = d, for all MLPs(h) = W\u2032 2 Relu(W\u2032 1h2) : Rd2 \u2192Rd2, there exists MLP(h) = W2 Relu(W1h) : Rd \u2192Rd, such that MLP(h) = (h1, MLPs(h2), h3). Proof. Obviously, for any W\u2032 1, there exists W1, such that h(a) \u225chW1 = (h1, \u2212h1, (W\u2032 1h2), hT 3 , \u2212h3). Obviously, for any W\u2032 2, There exists W2, such that h(b) = W2 Relu(h(a)) = ((Relu(h1) + Relu(\u2212h1)), (W\u2032 2 Relu(W\u2032 1)), (Relu(h3) + Relu(\u2212h3))) = (h1, MLPs(h2), h3)\nTherefore, the 2j-th token can only attend to the token with the position embedding pi. If h2i\u22121 = (0, 0, 0, 0, 0, 0, 0, pi), we have the value of hj after attention as hattn j = ((h\u2032 i)TWV , 0, 0, 0, 0, 0, 0, pj). By setting WV as different value, we can copy different part information of i-th to j-th token. Then the lemma is held. Lemma E.5. For the input h = (h1, h2, h3), where hi \u2208Rdi and d1+d2+d3 = d, for all MLPs(h) = W\u2032 2 Relu(W\u2032 1h2) : Rd2 \u2192Rd2, there exists MLP(h) = W2 Relu(W1h) : Rd \u2192Rd, such that MLP(h) = (h1, MLPs(h2), h3).\nWithout loss of generality, we assume the representation of the Transformer in layer k is in a form that h(k) 2i\u22121 = (fi, 0, 0, 0, 0, 0, 0, pi)T and h(k) 2i = (0, yi, 0, 0, 0, 0, 0, pi)T (Remind that one sample will take two token, one for x and one for y). Because the representation usually lies in low-dimension space, a simple linear layer can transfer the representation in our defined sparse form. Moreover, it is natural to assume that the position information is stored in the representation since it is given in the input and is essential for attention. The consider the operations of Transformer in different layers. 1) ** Layer 1 **\nBecause we assume that h(k) 2i\u22121 is a perfect token representation, then there exists We, such that h(k) 2i\u22121We = f (e) k , wher f (e) i satisfies that \u2200e, i, we have f (e) j \u00b7 f (e) i = 1 only when v(e) i = v(e) j else f (e) j \u00b7 f (e) i = 0. Step 1, we use each attention head to obtain the matching information of each factor.\n(11)\nWe first consider the query token at the position 2i \u22121 And we assign W(l,k) Q = W(l,k) K = Wek and W(l,k) V (0, 0, 0, 0, 0, 0, 0, I)T so that (h(l) i )TW(l,k) V = pi. Then, we have\nwhere v(e) i is the one-hot vector of v(e) i . We denote base = (20, 21, \u00b7 \u00b7 \u00b7 , 2L)T and u2i\u22121 = ({base \u00b7 be1, \u00b7 \u00b7 \u00b7 , base \u00b7 bene }. Obvious, there is W(l,k) O such that \ufffdne k=1 bekW(l,k) O = (0, 0, u2i\u22121, 0, 0, 0, 0, 0). Then, we consider the token at position 2i as query token. We assign W(l,ne+1) Q = W(l,ne+1) K = (0, I, 0, 0, 0, 0, 0, 0)T and W(l,ne+1) V = (0, 0, 0, 0, 0, 0, 0, I)T. We have\nObvious, there is WO such that byW(l,ne+1) O = (0, 0, 0, u2i, 0, 0, 0, 0, 0), where u2i = {base \u00b7by, \u00b7 \u00b7 \u00b7 , base \u00b7by}. The ne + 1 head doesn\u2019t affect the token 2i \u22121, because value of the h2i\u22121 is 0. As a result, we have h2i\u22121 = h2i\u22121 + \ufffdne k=1 bekW(l,k) O = (fi, 0, u2i\u22121, 0, 0, 0, 0, pi) after the operation. Similarily, because the first ne head dosen\u2019t affect the value of h2i, we have h2i = (0, yi, 0, u2i, 0, 0, 0, pi) after the operation. Note that base \u00b7 bek has the property that base \u00b7 bek = base \u00b7 bek\u2032 if and only if bek = be\u2032 k. This result indicates that all the context examples that have same factor value of factor ek with the sample i also has the same factor value of factor ek\u2032 as sample i. Therefore, we denote u as the matching information. If base \u00b7 bek = base \u00b7 by, we can infer that the factor value of ek has a similar pattern with the label. Therefore, ek is regard as the possible hidden factor.\nFor embedding of h2i, using the copy past of Lemma E.4, we can obtain h2i = (0, yi, 0, u2i, u2i\u22121, 0, 0, pi). (By settin the copy position as pi and therefore the operation will only influence y token.) According to Lemma E.5, there exist W(l) 1 , W(l) 2 , such that we have h2i = (0, yi, 0, u2i, u2i\u22121, m2i, 0, pi), where m2i = Relu(u2i \u2212u2i\u22121) + Relu(u2i\u22121 \u2212 u2i). Recall that h2i\u22121 = (fi, 0, u2i\u22121, 0, 0, 0, 0, pi). because all the corresponding terms of h2i\u22121 are 0, this operatio won\u2019t impact the value of it. This copy past operation can be put into a same layer as the pervious operations is because i this operation we mainly copy the infomation from 2i \u22121 token to 2i-token. Because 2i \u22121 precede 2i, the operations o 2i \u22121 is finished before the copy past operation happens.\nFor embedding of h2i, using the copy past of Lemma E.4, we can obtain h2i = (0, yi, 0, u2i, u2i\u22121, 0, 0, pi). (By setting the copy position as pi and therefore the operation will only influence y token.) According to Lemma E.5, there exists W(l) 1 , W(l) 2 , such that we have h2i = (0, yi, 0, u2i, u2i\u22121, m2i, 0, pi), where m2i = Rel",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of learning plateaus in in-context learning for Transformer models, a critical ability for their applications. Previous methods have struggled to mitigate these plateaus, which require significant computational resources and energy consumption. The authors propose a new approach to overcome these learning plateaus without increasing model size, thus promoting environmentally sustainable AI systems.",
        "problem": {
            "definition": "The core problem is the occurrence of learning plateaus during the training of Transformers, where there is minimal or no improvement in in-context learning capability over time.",
            "key obstacle": "The main challenge is the dysfunction of the weights component within the model, which leads to prolonged learning plateaus and hinders effective learning."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that the weights component's performance degrades during training, particularly in complex tasks, leading to learning plateaus.",
            "opinion": "The proposed idea involves enhancing the weights component to expedite learning and reduce the duration of learning plateaus.",
            "innovation": "The innovation lies in the introduction of three specific methods to improve the weights component's functionality, which differs from existing approaches that typically focus on scaling model size."
        },
        "method": {
            "method name": "Weights Component Enhancement",
            "method abbreviation": "WCE",
            "method definition": "A method aimed at improving the weights component of Transformers to alleviate learning plateaus in in-context learning.",
            "method description": "This method focuses on enhancing the weights component's performance during training to facilitate better learning dynamics.",
            "method steps": [
                "Use a weights warm-up method to train on a simpler task before the main task.",
                "Implement a mixed training method that combines data from both simple and complex tasks.",
                "Introduce an extra loss specifically targeting the weights component."
            ],
            "principle": "The effectiveness of this method is based on the premise that a well-optimized weights component is crucial for overcoming learning plateaus and enhancing in-context learning capabilities."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using synthetic tasks and natural language processing tasks, including the Shapes3D dataset and the SST-ICL task, comparing performance metrics across different training configurations.",
            "evaluation method": "Performance was assessed using accuracy metrics on test sets, with specific attention to the weights and context components through probing methods."
        },
        "conclusion": "The research demonstrates that enhancing the weights component significantly reduces learning plateaus and improves in-context learning performance, confirming the proposed methods' effectiveness across multiple task settings.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include reduced learning plateaus and improved efficiency in training smaller models, thereby lowering computational resource requirements.",
            "limitation": "The study primarily focuses on learning plateaus within in-context learning frameworks and does not address similar phenomena in traditional supervised learning contexts.",
            "future work": "Future research could explore further enhancements to the weights component and investigate the applicability of these methods in different learning paradigms."
        },
        "other info": {
            "acknowledgements": "The authors acknowledge support from the National Natural Science Foundation of China (Grant No. 62088102).",
            "impact statement": "While the immediate societal impact is limited, the findings may influence future developments in sustainable AI systems."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of learning plateaus in in-context learning for Transformer models, emphasizing the importance of overcoming these plateaus for effective learning."
        },
        {
            "section number": "1.2",
            "key information": "The significance of addressing learning plateaus is highlighted, as they require significant computational resources and energy consumption, impacting the efficiency of in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge identified is the dysfunction of the weights component within the model, leading to prolonged learning plateaus and hindering effective learning."
        },
        {
            "section number": "3.2",
            "key information": "The proposed method, Weights Component Enhancement (WCE), aims to improve the weights component of Transformers to alleviate learning plateaus in in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "The proposed approach includes a weights warm-up method, mixed training with simple and complex tasks, and an extra loss targeting the weights component, influencing the design of prompts and training strategies."
        },
        {
            "section number": "6.1",
            "key information": "The study identifies learning plateaus as a significant challenge in in-context learning, emphasizing the need for methods to enhance model performance and reduce these plateaus."
        },
        {
            "section number": "6.4",
            "key information": "The research discusses the scalability of the proposed methods, which aim to improve the weights component without increasing model size, thus promoting efficiency in various applications."
        }
    ],
    "similarity_score": 0.6946129812919722,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Breaking through the learning plateaus of in-context learning in Transformer.json"
}