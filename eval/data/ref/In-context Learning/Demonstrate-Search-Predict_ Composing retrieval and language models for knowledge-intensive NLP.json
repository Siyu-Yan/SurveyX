{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.14024",
    "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
    "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple \u201cretrievethen-read\u201d pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose DEMONSTRATE\u2013 SEARCH\u2013PREDICT (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art incontext learning results and delivering 37\u2013120%, 8\u201339%, and 80\u2013290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-thenread pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https: //github.com/stanfordnlp/dsp.",
    "bib_name": "khattab2023demonstratesearchpredictcomposingretrievallanguage",
    "md_text": "# DEMONSTRATE\u2013SEARCH\u2013PREDICT: Composing retrieval and language models for knowledge-intensive NLP\nOmar Khattab 1 Keshav Santhanam 1 Xiang Lisa Li 1 David Hall 1 Percy Liang 1 Christopher Potts 1 Matei Zaharia 1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8d8d/8d8de4ac-6488-4489-91c8-6c3a6421e6f8.png\" style=\"width: 50%;\"></div>\n# Abstract\nRetrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple \u201cretrievethen-read\u201d pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose DEMONSTRATE\u2013 SEARCH\u2013PREDICT (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art incontext learning results and delivering 37\u2013120%, 8\u201339%, and 80\u2013290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-thenread pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https: //github.com/stanfordnlp/dsp.\narXiv:2212.14024v2\n# 1. Introduction\nIn-context learning adapts a frozen language model (LM) to tasks by conditioning the LM on a textual prompt including task instructions and a few demonstrating examples (McCann et al., 2018; Radford et al., 2019; Brown et al., 2020). For knowledge-intensive tasks such as question answering, fact checking, and information-seeking dialogue, retrieval models (RM) are increasingly used to augment prompts\nFigure 1. A comparison between three systems based on GPT3.5 (text-davinci-002). On its own, the LM often makes false assertions. An increasingly popular retrieve-then-read pipeline fails when simple search can\u2019t find an answer. In contrast, a taskaware DSP program successfully decomposes the problem and produces a correct response. Texts edited for presentation. with relevant information from a large corpus (Lazaridou et al., 2022; Press et al., 2022; Khot et al., 2022).\nRecent work has shown such retrieval-augmented in-context learning to be effective in simple \u201cretrieve-then-read\u201d pipelines: a query is fed to the RM and the retrieved passages become part of a prompt that provides context for the LM to use in its response. In this work, we argue that the fact that both LMs and RMs consume (and generate or retrieve) natural language texts creates an opportunity for much more sophisticated interactions between them. Fully realizing this would be transformative: frozen LMs and RMs could serve as infrastructure across tasks, enabling ML- and domain-experts alike to rapidly build grounded AI systems at a high level of abstraction and with lower deployment overheads and annotation costs.\nFigure 1 begins to illustrate the power of retrievalaugmented in-context learning, but also the limitations of \u201cretrieve-then-read\u201d (Lazaridou et al., 2022; Izacard et al., 2022). Our query is \u201cHow many storeys are in the castle David Gregory inherited?\u201d When prompted to answer this, GPT-3.5 (text-davinci-002; Ouyang et al. 2022) makes up a fictitious castle with incorrect attributes, highlighting the common observation that knowledge stored in LM parameters is often unreliable (Shuster et al., 2021; Ishii et al., 2022). Introducing an RM component helps, as the LM can ground its responses in retrieved passages, but a rigid\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6516/6516e841-d42b-4acd-8f88-229daf2b1b80.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision. Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.</div>\nFigure 1 shows the path that a DSP program might take to arrive at an answer, and Figure 2 illustrates how a deliberate program achieves this. Instead of asking the LM to answer this complex question, the program\u2019s SEARCH stage uses the LM to generate a query \u201cWhich castle did David Gregory inherit?\u201d The RM retrieves a passage saying Gregory inherited the Kinnairdy Castle. After a second search \u201chop\u201d finds the castle\u2019s number of storeys, the PREDICT stage queries the LM with these passages to answer the original question. Although this program implements behaviors such as query generation, it requires no hand-labeled examples of these intermediate transformations (i.e., of the queries and passages of both retrieval hops). Instead, the DEMONSTRATE\nstage uses labeled question\u2013answer pairs to implement a form of weak supervision that programmatically annotates the transformations invoked within SEARCH and PREDICT. We evaluate several DSP programs on answering questions in open-domain, multi-hop, and conversational settings. In them, we implement novel and reusable transformations such as bootstrapping annotations for all of our pipelines with weak supervision (\u00a72.3), reliably rewriting questions to resolve conversational dependencies and iteratively decompose complex queries with summarization of intermediate hops (\u00a72.4), and generating grounded responses from multiple passages with self-consistency (\u00a72.5). We report preliminary results on Open-SQuAD, HotPotQA, and QReCC using the frozen LM GPT-3.5 and RM ColBERTv2 (Khattab & Zaharia, 2020; Santhanam et al., 2022b) with no fine-tuning. Our DSP programs deliver 37\u2013120%, 8\u201339%, and 80\u2013290% relative gains against corresponding vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline (Press et al., 2022), respectively. Future versions of this report will include additional test tasks and LM choices.\nIn summary, this work makes the following contributions. First, we argue that simple task-agnostic pipelines for incontext learning should give way to deliberate, task-aware strategies. Second, we show that this shift need not be a burden: with DSP, such strategies can be easily expressed as short programs using composable operators. Third, this composability spawns powerful capacities, like automatically annotating demonstrations for complex pipelines from end-task labels. Fourth, for three knowledge-intensive tasks, we implement rich programs that establish state-of-the-art results for in-context learning.\n# 2. DEMONSTRATE\u2013SEARCH\u2013PREDICT\nWe now introduce the DSP framework and show its expressive power by suggesting a number of strategies in which the LM and RM can come together to tackle complex problems effectively. We show in \u00a73 that such strategies outperform existing in-context learning methods. We begin by discussing the LM and RM foundation modules on which DSP is built (\u00a72.1) and then the datatypes and control flow within DSP (\u00a72.2). Subsequently, we discuss each of the three inference stages: DEMONSTRATE (\u00a72.3), SEARCH (\u00a72.4), and PREDICT (\u00a72.5).\n# 2.1. Pretrained Modules: LM and RM\nA DSP program defines the communication between the language model LM and the retrieval model RM.\nLanguage Model We invoke a frozen language model LM to conditionally generate (or score) text. For each invocation, the program prepares a prompt that adapts the LM to a specific function (e.g., answering questions or generating queries). A prompt often includes instructions, a few demonstrations of the desired behavior, and an input query to be answered. As in Figure 2, the LM generates not only: (i) the final answer to the input question (in the PREDICT stage), but also (ii) intermediate \u201chop\u201d queries to find useful information for the input question (SEARCH) as well as (iii) exemplar queries that illustrate how to produce queries for questions in the training set (DEMONSTRATE). This systematic use of the LM is a hallmark of DSP programs. Retrieval Model DSP programs also invoke a frozen retrieval model RM to retrieve the top-k most \u201crelevant\u201d text sequences for a given query. The RM can index a massive set of pre-defined passages for scalable search, and those passages can be updated without changing the retrieval parameters. The RM accepts free-form textual inputs and specializes in estimating the relevance (or similarity) of a text sequence to a query. As in Figure 2, the RM is responsible for retrieving (i) passages for each query generated by the LM (during the SEARCH stage), but also (ii) passages that are used within demonstrations (DEMONSTRATE). In the latter case, the RM\u2019s contributions are less about providing directly relevant information to the input question and more about helping the LM adapt to the domain and task. Though not utilized in this example, the RM is also used in DSP for functions like retrieving \u201cnearest-neighbor\u201d demonstrations from task training data (DEMONSTRATE) and selecting well-grounded generated sequences from the LM (PREDICT).\nLanguage Model We invoke a frozen language model LM to conditionally generate (or score) text. For each invocation, the program prepares a prompt that adapts the LM to a specific function (e.g., answering questions or generating queries). A prompt often includes instructions, a few demonstrations of the desired behavior, and an input query to be answered.\nAs in Figure 2, the LM generates not only: (i) the final answer to the input question (in the PREDICT stage), but also (ii) intermediate \u201chop\u201d queries to find useful information for the input question (SEARCH) as well as (iii) exemplar queries that illustrate how to produce queries for questions in the training set (DEMONSTRATE). This systematic use of the LM is a hallmark of DSP programs.\n2.2. Datatypes and Control Flow\n# 2.2. Datatypes and Control Flow\nWe have implemented the DSP framework in Python. The present section introduces the core data types and composable functions provided by the framework. We use illustrative code snippets to ground the examples, and to convey the power that comes from being able to express complex interactions between the LM and RM in simple programs. The Example Datatype To conduct a task, a DSP program manipulates one or more instances of the Example datatype. An Example behaves like a Python dictionary with multiple fields. The program is typically provided with a few training examples. The code snippet below illustrates this for multi-hop question answering.\nThis snippet contains two labeled examples, each with a multi-hop question (e.g., \u201cIn which city did Akeem Ellis play in 2017?\u201d) and its short answer (\u201cEllesmere Port\u201d). Arbitrary keys and values are allowed within an Example, though typical values are strings or lists of strings. In this task, we are unlikely to find an individual passage that provides the answer to any question. For example, the first training example can probably be resolved only by first answering the question of who discovered Palomar (\u201cEdwin Hubble\u201d) and then addressing the question of Hubble\u2019s birth date using different evidence passages. We typically assume that the human-labeled training data do not include labels for intermediate transformations (e.g., queries for individual hops) that would be useful for following these steps, and so it is the job of the DSP program to discover these strategies via in-context learning. A DSP Program The following code snippet is a complete program for resolving multi-hop questions like those in Figure 1, with help from train examples like those above.\nThe program takes the input (here, a question) and outputs the system output (its short answer). It starts by creating an Example for the input question and assigning the train field to the training set from the previous snippet. Programs\ninvoke and compose DSP primitives (i.e., built-in functions) to build the DEMONSTRATE, SEARCH, and PREDICT transformations that define the program.\ninvoke and compose DSP primitives (i.e., built-in functions) to build the DEMONSTRATE, SEARCH, and PREDICT transformations that define the program. Transformations A transformation is a function that takes an Example as input and returns an Example, populating new fields (or modifying existing fields) in it. This program invokes three developer-defined transformations, namely, multihop_demonstrate, multihop_search, and multihop_predict. Transformations may themselves invoke other transformations, and they act analogously to layers in standard deep neural network (DNN) programming frameworks such as PyTorch, except that they pass text data instead of tensors between each other and do not involve backpropagation. We categorize transformations according to their behavior (or purpose) under one of the DEMONSTRATE, SEARCH, and PREDICT stages. That said, DSP does not impose this categorization and allows us to define functions that may blend these stages. We will discuss each of the three stages next.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/28f5/28f5396e-d49d-4133-b7f5-a069726f79f5.png\" style=\"width: 50%;\"></div>\n# 2.3. DEMONSTRATE\nIt is known that including examples of the desired behavior from the LM in its prompt typically leads to better performance (Brown et al., 2020). In DSP, a demonstration is a training example that has been prepared to illustrate specific desired behaviors from the LM. A DEMONSTRATE transformation takes as input x of type Example and prepares a list of demonstrations in x.demos, typically by selecting a subset of the training examples in x.train and bootstrapping new fields in them.\ning set typically consist of the input text and the target output of the task. The DEMONSTRATE stage can augment a training example by programmatically bootstrapping annotations for intermediate transformations. In our running \u201cmulti-hop\u201d example, the demonstrations illustrate three LM-based transformations: (i) how to break down the input question in order to gather information for answering it (i.e., first-hop retrieval), (ii) how to use information gathered in an earlier \u201chop\u201d to ask follow-up questions, and (iii) how to use the information gathered to answer complex questions.\nAkin to a specialized map, the annotate primitive accepts a user-defined transformation fn and applies it over a list\nof training examples. Whenever fn returns an example (rather than None), annotate caches the intermediate predictions (i.e., the generated queries and retrieved passages). These predictions serve as successful demonstrations for the pipeline transformations. In simple uses, fn may attempt to answer the example \u201czero-shot\u201d one or more times. This is typically done by invoking the SEARCH and PREDICT stages of the program. When an answer is produced, if fn assesses it as correct, it returns a populated example in which the intermediate predictions are present. Case Study The snippet below defines the function multihop_demonstrate, called in Line 3 of multihop_program, and illustrates the usage of annotate.\nIn Line 10, multihop_demonstrate invokes annotate, which bootstraps missing fields in training examples by caching annotations from attempt_example. The transformation attempt_example takes a training example d and attempts to answer it in a zero-shot fashion: it creates a copy of d with no demonstrations (Line 4; i.e., zero-shot) and invokes the multi-hop search and predict pipeline (Lines 5 and 6). Each transformation returns an updated version of d with additional fields populated. If the pipeline answers correctly (Line 7), the updated d is returned. Figure 2 illustrates this behavior. DEMONSTRATE transforms a training question\u2013answer pair to a fully-populated demonstration, including fields such as hop1 and hop2 (i.e., queries for multi-hop search) as well as psg1 and psg2. When the LM is later invoked to conduct a transformation, say, generating a \u201csecond-hop\u201d query during SEARCH, the psg1 field serves as context and the hop2 field serves as a label for this particular training example. Discussion This simple case study illustrates the power of composition in the DSP abstraction. Because the pipeline is a well-defined program in which transformations communicate by passing text attached to Examples, a simple map-and-filter strategy can leverage the LM and RM to bootstrap annotations for a full pipeline from end-task labels. This is an extensible strategy, but even in its simplest form it generalizes the approaches explored recently by Zelikman et al. (2022), Wei et al. (2022), Zhang et al. (2022), and Huang et al. (2022) in which an LM self-generates chain-of-thought rationales for an individual prompt.\nBy bootstrapping pipelines, DEMONSTRATE makes it easy to explore complex strategies in SEARCH and PREDICT without writing examples for every transformation. This includes strategies that are challenging to explore without custom annotations in traditional retrieval-augmented NLP. For instance, Khattab et al. (2021a) introduces a pipeline for multi-hop reasoning that is trained with weak supervision, extending work by Lee et al. (2019) and Khattab et al. (2021b). In it, the target 3 or 4 passages that need to retrieved must be labeled but the system discovers the best order of \u201chops\u201d automatically.\nIn contrast, DSP allows us to build complex pipelines without labels for intermediate steps, because we can compose programs out of small transformations. If LM and RM can accurately process such transformations \u201czero-shot\u201d (i.e., without demonstrations) on at least one or two examples, these examples can be discovered with end-task labels and used as demonstrations.\nTo draw on our earlier analogy with DNN frameworks like PyTorch, DEMONSTRATE aims to replace the function of backpropagation in extensible ways by simulating the behavior of the program (corresponding to a \u201cforward\u201d pass) and programmatically learning from errors. In doing this with frozen models and with only end-task labels, DEMONSTRATE introduces a high degree of modularity. In particular, without hand-labeling intermediate transformations, developers may swap the training domain, update the training examples, or modify the program\u2019s strategy, and use annotate to automatically populate all of the intermediate fields for demonstrations.\nSelecting Demonstrations It is not always possible to fit all of the training examples in the context window of the LM. DSP provides three primitives for selecting a subset of training examples, namely, sample, knn, and crossval.\nAs a baseline choice, k demonstrations can be randomly sampled from train using the sample primitive, an approach used by Brown et al. (2020) and much subsequent work. We can also leverage the RM\u2019s representations and select from the training set the k nearest neighbors to the input text, a strategy explored by Liu et al. (2021). Another strategy is to apply cross-validation to select among a number of sampled sets of demonstrations (Perez et al., 2021). For example, given |train| = 100 training examples, crossval\nwould select n subsets of k = 5 examples each, and return the set with which a transformation evaluate performs best on the remaining 95 examples.\nCompositions & Extensions By manipulating demonstrations and higher-order transformations, these simple selection and bootstrapping primitives can be combined to conduct larger novel strategies. If the training set is very large (e.g., |train| = 100, 000), we can conduct knn to find the nearest k = 16 examples and only annotate these, arriving at a system that learns incrementally in real-time. If the training set is moderately large (e.g., |train| = 1000), we can conduct crossval and cache the performance of all prompts it evaluates on each training example. At test time, we can use knn to find k = 50 similar examples to the test input and select the prompt that performs best on these k examples, producing an adaptive system that is informed by the quality of its pipeline on different types of examples.\n# 2.4. SEARCH\nThe SEARCH stage gathers passages to support transformations conducted by the LM. We assume a large knowledge corpus\u2014e.g., a snippet of Web, Wikipedia, or arXiv\u2014that is divided into text passages. Providing passages to the LM facilitates factual responses, enables updating the knowledge store without retraining, and presents a transparency contract: when in doubt, users can check whether the system has faithfully used a reliable source in making a prediction. In the simplest scenarios, SEARCH can directly query the RM, requesting the top-k passages (from a pre-defined index) that match an input question. This baseline instantiation of SEARCH simulates retrieval in most open-domain question answering systems, which implement a \u201cretrievethen-read\u201d pipeline, like Lee et al. (2019), Khattab et al. (2021b), Lazaridou et al. (2022), and many others.\n from dsp import retrieve  def simple_search(x): passages = retrieve(query=x.question , k=2) return passages\nSEARCH Strategies In many scenarios, the complexity of the task demands more sophisticated SEARCH strategies that empower the RM to find relevant passages. Our running example (Figure 2) is one such scenario, in which we suspect examples are likely to require multi-hop reasoning in particular. Other settings, for instance, pose conversational challenges, in which the information need expressed by a user can only be resolved by taking into account previous turns in the conversation, or demand more extensive planning (Zhong et al., 2022). In the retrieval-augmented NLP literature, multi-hop search (Xiong et al., 2020; Khattab et al., 2021a) and con-\nversational search (Del Tredici et al., 2021; Raposo et al., 2022) pipelines have received much attention. These systems are typically fine-tuned with many hand-labeled query \u201crewrites\u201d (Anantha et al., 2020), \u201cdecompositions\u201d (Geva et al., 2021; Min et al., 2019), or target hops (Yang et al., 2018; Jiang et al., 2020). Supported with automatic annotations from DEMONSTRATE, the SEARCH stage allows us to simulate many such strategies and many others in terms of passing queries, passages, and demonstrations between the RM and LM. More importantly, SEARCH facilitates our vision of advanced strategies in which the LM and RM cooperate to incrementally plan a research path for which the RM gathers information and the LM identifies next steps. Case Study Let us build on our running multi-hop example as a case study. We can define multihop_search_v2 (Line 4 in our core program), a slightly more advanced version of the SEARCH transformation from Figure 2. This transformation simulates the iterative retrieval component of fine-tuned retrieval-augmented systems like IRRR (Qi et al., 2020), which reads a retrieved passage in every hop and generates a search query (or a termination condition to stop hopping), and Baleen (Khattab et al., 2021a), which summarizes the information from many passages in each hop for inclusion in subsequent hops.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7598/7598c9ef-60ef-44b3-97b5-196052a460e8.png\" style=\"width: 50%;\"></div>\nIn multihop_search_v2, Line 7 calls the generate primitive, which invokes the LM to produce a query for each retrieval hop. The LM is conditioned on a prompt that is prepared using the hop_template template. (We discuss prompt templates and the generate primitive in \u00a72.5.) Here, this template may be designed to generate a prompt that has the following format (e.g., for the second hop).\n1 My task is to write a simple query that gathers\ninformation for answering a complex question. I\nwrite N/A if the context contains all\ninformation required.\n2\n3 {Task demonstrations from x.demos , if any}\n4\n5 Context: {x.context}\n6 Question: {x.question}\n7 Summary: Let 's summarize the above context.\n__{summary}__\n8 Search Query: __{query}__\nAs shown, the LM is instructed to read the context retrieved in earlier hops and a complex question. It is then prompted to write: (i) a summary of the supplied context and (ii) a search query that gathers information for answering that question. The generated text will be extracted and assigned to the summary and query variables in (multihop_search_v2; Line 7). On Line 10, we terminate the hops if the query is \u201cN/A\u201d. Otherwise, Line 12 retrieves k = 5 passages using the query and Line 13 assigns the context for the subsequent hop (or for PREDICT), setting that to include the summary of all previous hops as well as the passages retrieved in the final hop so far.\nAs shown, the LM is instructed to read the context retrieved in earlier hops and a complex question. It is then prompted to write: (i) a summary of the supplied context and (ii) a search query that gathers information for answering that question. The generated text will be extracted and assigned to the summary and query variables in (multihop_search_v2; Line 7). On Line 10, we terminate the hops if the query is \u201cN/A\u201d. Otherwise, Line 12 retrieves k = 5 passages using the query and Line 13 assigns the context for the subsequent hop (or for PREDICT), setting that to include the summary of all previous hops as well as the passages retrieved in the final hop so far. Comparison with self-ask It may be instructive to contrast this multi-hop DSP program with the recent \u201cselfask\u201d (Press et al., 2022) prompting technique, which we compare against in \u00a73. Self-ask can be thought of as a simple instantiation of DSP\u2019s SEARCH stage. In it, the LM asks one or more \u201cfollow-up questions\u201d, which are intercepted and sent to a search engine. The search engine\u2019s answers are concatenated into the prompt and are used to answer the question. This is essentially a simplified simulation of IRRR (Qi et al., 2020). As a general framework, DSP can express ideas like self-ask and many other, more sophisticated pipelines as we discuss in the present section. More importantly, DSP offers a number of intrinsic advantages that lead to large empirical gains: 80%\u2013290% over self-ask. For instance, DSP programs are deeply modular, which among other things means that DSP programs will annotate and construct their own demonstrations. Thus, they can be developed without labeling any of the intermediate transformations (e.g., the queries generated). In addition, the LM prompts constructed by DSP get automatically updated to align with the training data and retrieval corpus provided. In contrast, approaches like self-ask rely on a hand-written prompt with hard-coded examples. Moreover, DSP assigns the control flow to an explicit program and facilitates design patterns that invoke the LM (or RM) to conduct small transformations. This allows us to build steps that are dedicated to generating one or more retrieval queries, summarizing multiple passages per hop, and answering questions. These steps are individually simpler than the self-ask prompt, yet our multi-hop DSP program deliberately composes them to build richer pipelines that are thus more reliable. In contrast, self-ask delegates the control flow to the LM completions, maintaining state within the prompt itself and intercepting follow-up questions to conduct search. We find that this paradigm leads to a \u201cselfdistraction\u201d problem (\u00a73.5) that DSP programs avoid. Fusing Retrieval Results For improved recall and robustness, we can also fuse the retrieval across multiple generated queries. Fusion has a long history in information\nFusing Retrieval Results For improved recall and robustness, we can also fuse the retrieval across multiple generated queries. Fusion has a long history in information\nretrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kurland & Culpepper, 2018) and sequentially processing multiple queries was explored recently by Gao et al. (2022) for retroactively attributing text generated by LMs to citations. Inspired by these, we include a fused_retrieval primitive to DSP to offer a versatile mechanism for interacting with frozen retrievers. It accepts an optional fusion function that maps multiple retrieval lists into one. By default, DSP uses a variant of CombSUM (Fox & Shaw, 1994), assigning each passage the sum of its probabilities across retrieval lists.\n1 Template\n# template: an object that can produce\nprompts and parse completions\n2\n3 generate(template: Template)\n4\n-> fn(example: Example)\n5\n-> Completions\n# object with keys to access\nextracted preds and scores\n6\n7 rank(query: str , passages: List[str])\n8\n-> List[float]\n# object with keys to access\npassage texts and scores\nc = generate(hop_template , n=10)(x)\npassages = fused_retrieval(c.queries , k=5)\nsummary = c.summaries [0] # highest -scoring summary\nCompositions & Extensions To illustrate a simple composition, we can equip a chatbot with the capacity for conversational multi-hop search by combining a query rewriting step, which produces a query that encompasses all of the relevant conversational context, with the multi-hop transformation, as follows.\nSimilar approaches can be used for correcting spelling mistakes or implementing pseudo-relevance feedback (Cao et al., 2008; Wang et al., 2022a), in which retrieved passages are used to inform a better search query, though this has not been attempted with pretrained LMs to our knowledge.\n# 2.5. PREDICT\nThe PREDICT stage generates the system output using demonstrations (e.g., in x.demos) and passages (e.g., in x.context). PREDICT tackles the challenges of reliably solving the downstream task, which integrates much of the work on in-context learning in general. Within DSP, it also has the more specialized function of systematically aggregating information across a large number of demonstrations, passages, and candidate predictions.\nGenerating Candidates Generally, PREDICT has to produce one or more candidate predictions for the end-task. To this end, the basic primitive in PREDICT is generate, which accepts a Template and (via currying) an Example and queries the LM to produce one or more completions, as explored earlier in \u00a72.4. A corresponding primitive that uses the RM in this stage is rank, which accepts a query and one or more passages and returns their relevance scores.\nA Template is an object that can produce prompts, that is, map an Example to a string, and extract fields out of completions. For instance, we can map an example x that has a question and retrieved passages to the following prompt:\n1 My task is to answer questions using Web documents.\n2\n3 {Task demonstrations from x.demos , if any}\n4\n5 Context: {x.passage}\n6 Question: {x.question}\n7 Rationale: Let 's think step by step. __{rationale}__\n8 Answer: __{answer}__\nAs this illustrates, the LM will be asked to generate a chainof-thought rationale (CoT; Wei et al. 2022; Kojima et al. 2022) and an answer, and the generated text will be extracted back into the rationale and answer keys of each completion. Each invocation to the LM can sample multiple candidate predictions. Selecting a \u201cbest\u201d prediction is the subject of much work on decoding (Wiher et al., 2022; Li et al., 2022), but a frozen and general-purpose LM may not support custom modifications to decoding. Within these constraints, we present several high-level strategies for selecting predictions and aggregating information in DSP via the LM and RM. Selecting Predictions Among multiple candidates, we can simply extract the most popular prediction. When a CoT is used to arrive at the answer, this is the self-consistency method of Wang et al. (2022c), which seeks to identify predictions at which multiple distinct rationales arrive.\nDSP generalizes this in two ways. First, we can sample multiple \u201cpipelines of transformations\u201d (PoT) within the program, rather than locally with \u201cchains of thought\u201d (CoT) in one transformation. These chains may even invoke different paths in the program, as illustrated below.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/54e0/54e03226-e86c-4338-88c6-d56fcb91ab11.png\" style=\"width: 50%;\"></div>\nIn the snippet above, Line 10 invokes the primitive branch which samples n different PoTs with a high temperature (e.g., t = 0.7) and accumulates their intermediate and final predictions. In this example, our pipeline invokes multihop_search_v2 (\u00a72.4), which applies a variable number of retrieval hops depending on the questions generated, before doing PREDICT. That is, PoT_program potentially invokes multiple distinct paths in the program (i.e., with different multi-hop queries and number of hops in each) across branches. It then selects the majority answer overall. DSP generalizes self-consistency in a second way. When sampling our CoTs or PoTs provides multiple candidates, we can select the top-k (e.g., top-4) predictions and then compare them directly. For instance, we may prompt the LM to compare these choices as MCQ candidates, a transformation for which DEMONSTRATE can automatically prepare exemplars. This effectively simulates the LM recursion of Levine et al. (2022), though unlike their approach it does not require a large training set or updating any (prompttuning) weights. One such implementation is illustrated in openqa_predict below.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5133/5133c3af-6079-436d-a0d1-01b8b9ef34e1.png\" style=\"width: 50%;\"></div>\nAs an alternative comparison approach, we can invoke the RM via rank to find the prediction that is most grounded in a retrieved contexts (i.e., most similar to the concatenation of the retrieved passages) or, given an RM that can score completions (Krishna et al., 2022), simply the prediction that has the highest score given the prompt. Aggregating Information When only a few demonstrations or passages are selected, we can simply concatenate them all into the prompt. For instance, GPT-3.5 text-davinci-002 has a context window of 4097 tokens, which we find to be reasonably large for accommodating several (e.g., 3\u20135) demonstrations, which individually include their own passages and rationales.\nTo deal with a larger number of demonstrations or passages, we can branch in parallel to process individual subsets of the passages or demonstrations and then aggregate the individual answers using one of the scoring methods presented earlier. Indeed, Lewis et al. (2020) and Lazaridou et al. (2022) have explored marginalization as a way to combine scores across passages and Le et al. (2022) ensemble prompts across demonstrations, which can be expressed in this way.\nTo deal with a larger number of demonstrations or passages, we can branch in parallel to process individual subsets of the passages or demonstrations and then aggregate the individual answers using one of the scoring methods presented earlier. Indeed, Lewis et al. (2020) and Lazaridou et al. (2022) have explored marginalization as a way to combine scores across passages and Le et al. (2022) ensemble prompts across demonstrations, which can be expressed in this way. An alternative aggregation strategy is to accumulate information across passages sequentially, rather than independently. This is effectively how our multi-hop approach works (\u00a72.4). Such a strategy has also been employed recently by Gao et al. (2022) for retroactively attributing text generated by LMs to citations. They generate many queries but instead of fusion (\u00a72.4), they run their pipeline on each query and use its outputs to alter the input to subsequent queries.1\n# 3. Evaluation\nWe now consider how to implement DSP programs for three diverse knowledge-intensive NLP tasks: open-domain question answering (QA), multi-hop QA, and conversational QA. All of these tasks are \u201copen-domain\u201d, in the sense that systems are given a short question or participate in a multi-turn conversation without being granted access to context that answers these questions.\nWe build and evaluate intuitive compositions of the functions explored in \u00a72 for each task. We show that, despite low development effort, the resulting DSP programs exhibit strong quality and deliver considerable empirical gains over vanilla in-context learning and a standard retrieve-then-read pipeline with in-context learning.\n# 3.1. Evaluation Methodology\nIn this report, we consider one development dataset for each of the tasks we consider, namely, the open-domain version of SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the multi-hop HotPotQA (Yang et al., 2018) dataset in the opendomain \u201cfullwiki\u201d setting, and the conversational question answering QReCC (Anantha et al., 2020; Vakulenko et al., 2022) dataset, which we used for developing the DSP abstractions. We report the validation set accuracy on all three datasets and discuss them in detail \u00a73.5.\nUnless otherwise stated, systems are given access to 16shot training examples, that is, each DSP program can use (up to) 16 questions\u2014or conversations, where applicable\u2014 randomly sampled from the respective training set. We\nsubsample the validation and test sets to 1000 questions (or 400 conversations, where applicable) and report average quality across five seeds where each seed fixes a single kshot training set of examples. To control the language model API spending budget, each seed processes one fifth of the evaluation examples (e.g., 200 questions per seed, for a total of 1000 unique questions). We also dedicate held-out test datasets (e.g., OpenNaturalQuestions; Kwiatkowski et al. 2019) and test tasks (e.g., claim verification, as in FEVER; Thorne et al. 2018) that we only use for evaluating pre-defined DSP programs rather than development. We will include these results in a future version of this report.\n# 3.2. Pretrained Modules\nRM We use ColBERTv2 (Santhanam et al., 2022b), a state-of-the-art retriever based on late interaction (Khattab & Zaharia, 2020). We choose ColBERTv2 for its highly effective zero-shot search quality and efficient search (Santhanam et al., 2022a). However, our DSP programs are agnostic to how the retriever represents examples or scores passages, so essentially any retriever can be used. In addition, by making retrieval a first-class construct, DSP allows us to change or update the search index over time. We simulate this in our experiments by aligning each of our datasets with the nearest Wikipedia corpus among the Dec 2016 Wikipedia dump from Chen et al. 2017, the Nov 2017 Wikipedia \u201cabstracts\u201d dump from Yang et al. 2018, and the Dec 2018 Wikipedia dump from Karpukhin et al. 2020.\nLM We use the GPT-3.5 (text-davinci-002; Brown et al. 2020; Ouyang et al. 2022) language model. Unless otherwise stated, we use greedy decoding when generating n = 1 prediction. We sample with temperature t = 0.7 when n > 1, like related work (Wang et al., 2022c).\n# 3.3. Baselines\nVanilla LM The vanilla LM baselines represent the fewshot in-context learning paradigm used by Brown et al. (2020). The open-domain QA and multi-hop QA baselines randomly sample 16 demonstrations (i.e., all of the examples available to each program in our evaluation) from the training set and do not augment these demonstrations with evidence. Similarly, the conversational QA baseline samples four conversations. The vanilla baselines do not search for passages relevant to the input query.\nRetrieve-then-Read The \u201cretrieve-then-read\u201d baselines use the RM to support each example with a potentially relevant passage before submitting the prompt to the LM. This emulates the pipelines used by state-of-the-art open-domain question answering systems (Khattab et al., 2021b; Izacard & Grave, 2020; Hofst\u00e4tter et al., 2022). In conversational QA, we concatenate the first turn and the final question, an approach that we found to perform much better than simply using the final turn. For multi-hop QA, we retrieve and concatenate two passages per question.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa93/aa936ddb-4d52-4bd4-bcc5-40d79c7ff22b.png\" style=\"width: 50%;\"></div>\nSelf-ask We also compare against self-ask (Press et al., 2022), a contemporaneous pipeline that can be thought of as a specific instantiation of DSP\u2019s SEARCH stage followed by a simple PREDICT step. For direct comparison with our methods, we modify the self-ask control flow to query the same ColBERTv2 index used in our DSP experiments instead of Google Search. We evaluate two configurations of self-ask. The first uses the original self-ask prompt template, which contains four hand-written demonstrations. In the second configuration, we modify the prompt template to apply a number of changes that we find are empirically useful for HotPotQA.2\n# 3.4. Proposed DSP Programs\nWe build on transformations presented in \u00a72. Our programs for all three tasks have the following structure, illustrated for open-domain QA.\n1 def openqa_program(question: str) -> str:\n2\nx = Example(question=question , train=train)\n3\nx = openqa_demonstrate(x)\n4\nx = openqa_search(x)\n5\nx = openqa_predict(x)\n6\nreturn x.answer\n2In particular: (i) use ColBERTv2-style passages in the handcrafted demonstrations of self-ask (i.e., instead of the original Google-style snippets), (ii) concatenate 16-shot training examples from the task (i.e., question\u2013answer pairs) as a prefix of the prompt, (iii) ask the model to generate a short intermediate answer per retrieval step, and (iv) explicitly ask the model to generate a followup \u201csearch query\u201d at each step. We found the final item to be important because self-ask\u2019s default prompt often produces followup questions that are not self-contained (e.g., \u201cwhat is the name of the national park?\u201d, which is not an informative search query). We also fix the casing in the prompt to be consistent.\nTable 1. Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as recent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based rows use ColBERTv2. The results marked with \u00b6 are collected from related work as of mid-December 2022, and attributed to their individual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons since they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points.\nOpen-SQuAD\nHotPotQA\nQReCC\nEM\nF1\nEM\nF1\nF1\nnF1\nVanilla LM\n16.2\n25.6\n28.3\n36.4\n29.8\n18.4\nNo-retrieval LM SoTA\n20.2\u00b6\n\u2013\n33.8\u00b6\n44.6\u00b6\n\u2013\n\u2013\nRetrieve-then-Read\n33.8\n46.1\n36.9\n46.1\n31.6\n22.2\nSelf-ask w/ ColBERTv2 Search\n9.3\n17.2\n25.2\n33.2\n\u2013\n\u2013\n+ Refined Prompt\n9.0\n15.7\n28.6\n37.3\n\u2013\n\u2013\nRetrieval-augmented LM SoTA\n34.0\u00b6\n\u2013\n35.1\u00b6\n\u2013\n\u2013\n\u2013\nTask-aware DSP Program\n36.6\n49.0\n51.4\n62.9\n35.0\n25.3\nconvqa_program, accepts turns (i.e., a list of strings, representing the conversational history) instead of a single question. Unless otherwise stated, our programs default to greedy decoding during the DEMONSTRATE stage. For SEARCH, our open-domain QA program uses the question directly for retrieving k = 7 passages and concatenates these passages into our QA prompt with CoT. For PREDICT, it generates n = 20 reasoning chains and uses self-consistency (SC; Wang et al. 2022c) to select its final prediction. For DEMONSTRATE, our open-domain QA program uses the following approach, slightly simplified for presentation. In it, the parameter k = 3 passed to annotate requests annotating only three demonstrations, which will then be used in the prompts.\n1 def openqa_demonstrate(x: Example) -> Example:\n2\ndemos = sample(x.train , k=16)\n3\n4\ndef openqa_attempt(d: Example) -> Example:\n5\nd.demos = all_but(demos , d)\n# all (raw)\nexamples different from d\n6\n7\nd = openqa_search(d, k=2)\n8\nif not passage_match(d): return None\n# skip\nexamples where search fails\n9\n10\nd = openqa_predict(d, sc=False)\n11\nif not answer_match(d): return None\n# skip\nexamples where predict fails\n12\n13\nreturn d\n14\n15\nx.demos = annotate(demos , openqa_attempt , k=3)\n16\nreturn x\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e97/5e97a153-1300-4ad0-a823-3875701546b7.png\" style=\"width: 50%;\"></div>\nOur multi-hop program adopts a very similar approach for DEMONSTRATE and PREDICT. For SEARCH, it uses the approach described in \u00a72.4, with the following adjustments. It uses result fusion across n = 10 queries per hop and, among the n predictions, uses the summary corresponding to the largest average log-probability. It uses a fixed number of hops for HotPotQA, i.e., two hops. In each prompt (i.e.,\neach hop and QA), it concatenates the summaries of all previous hops (i.e., hop 1 onwards) and a total of k = 5 passages divided between the hops (i.e., five passages from the first hop or two passages from the first and three from the second). For conversational QA, we use a simple PREDICT which generates a response with greedy decoding, conditioned on all of the previous turns of the conversation and five retrieved passages. For SEARCH, our conversational QA pipeline generates n = 10 re-written queries (and also uses the simple query as the retrieve-and-read baseline; \u00a73.3) and fuses them as in \u00a72.4. We implement DEMONSTRATE similar to openqa_demonstrate, but sample only four examples (i.e., four conversational turns; instead of 16 questions as in open-domain QA) for demonstrating the task for the higherorder transformation convqa_attempt, which is passed to annotate (not shown for brevity).\n# 3.5. Development Datasets & Results\nOpen-SQuAD We conduct the open-domain version of SQuAD over the Wikipedia 2016 corpus from Chen et al. (2017), as processed by Khattab et al. (2021b). We use the\nsame train/validation/test splits as in Karpukhin et al. (2020) and Khattab et al. (2021b). Table 1 reports the answer EM and F1. The task-aware DSP program achieves 36.6% EM, outperforming the vanilla LM baseline by 126% EM relative gains. This indicates the importance of grounding the LM\u2019s predictions in retrieval, and it shows that state-of-the-art retrievers like ColBERTv2 have the capacity to do so off-the-shelf. The proposed DSP program also achieves relative gains of 8% in EM and 6% in F1 over the retrieve-then-read pipeline, highlighting that nontrivial gains are possible by aggregating information across several retrieved passages as we do with self-consistency. These in-context learning results are competitive with a number of popular fine-tuned systems. For instance, on the Open-SQuAD test set, DPR achieves 29.8% EM, well below our 16-shot DSP program. On the Open-SQuAD dev set, the powerful Fusion-in-Decoder (Izacard & Grave, 2020) \u201cbase\u201d approach achieves approximately 36% (i.e., very similar quality to our system) when invoked with five retrieved passages. Nonetheless, with the default setting of reading 100 passages, their system reaches 48% EM in this evaluation. This may indicate that similar gains are possible for our DSP program if the PREDICT stage is made to aggregate information across many more passages.\nHotPotQA We use the open-domain \u201cfullwiki\u201d setting of HotPotQA using its official Wikipedia 2017 \u201cabstracts\u201d corpus. The HotPotQA test set is hidden, so we reserve the official validation set for our testing. We sub-divide\nthe training set into 90%/10% train/validation splits. In the training (and thus validation) split, we keep only examples marked as \u201chard\u201d in the original dataset, which matches the designation of the official validation and test sets. We report the final answer EM and F1 in Table 1. On HotPotQA, the task-aware DSP program outperforms the baselines and existing work by very wide margins, exceeding the vanilla LM, the retrieve-then-read baseline, and the self-ask pipeline by 82%, 39%, and 80%, respectively, in EM. This highlights the effectiveness of building up more sophisticated programs that coordinate the LM and RM for the SEARCH step. These results may be pegged against the evaluation on HotPotQA in a number of concurrent papers. We first compare with non-retrieval approaches, though our comparisons must be tentative due to variation in evaluation methodologies. Si et al. (2022) achieve 25.2% EM with CoT prompting. With a \u201crecite-and-answer\u201d technique for PaLM-62B (Chowdhery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang et al. (2022b) achieve 33.8% EM and 44.6 F1 when applying a self-consistency prompt for PaLM-540B. Next, we compare with a contemporaneous retrieval-based approach: Yao et al. (2022) achieve 35.1% EM using a system capable of searching using a Wikipedia API. All of these approaches trail our task-aware DSP program, which achieves 51.4% EM, by large margins.\nQReCC We use QReCC (Anantha et al., 2020) in an opendomain setting over Wikipedia 2018. QReCC does not have an official development set, so we sub-divide the training set into 90%/10% train/validation splits. For the first question in every conversation, we use the rewritten question as the original question often assumes access to a groundtruth document. We also filter low-quality examples from QReCC.3 We conduct the QReCC conversations in an auto-regressive manner. At turn t > 1 of a particular conversation, the system sees its own responses (i.e., not the ground truth responses) to previous turns of the conversation. We report the novel-F1 metric (nF1; Paranjape et al. 2022), which computes the F1 overlap between the system response and the ground truth while discounting common stopwords and terms present in the question (or earlier questions). The results are shown in Table 1, and follow the same general pattern as SQuAD and HotPotQA.\nQReCC We use QReCC (Anantha et al., 2020) in an opendomain setting over Wikipedia 2018. QReCC does not have an official development set, so we sub-divide the training set into 90%/10% train/validation splits. For the first question in every conversation, we use the rewritten question as the original question often assumes access to a groundtruth document. We also filter low-quality examples from QReCC.3\n3We remove conversations that have one or more empty groundtruth answers and conversations that have only one or two questions. We also find many conversations that include \u201cwhat other interesting facts are in this article?\u201d, which conflict with the opendomain formulation and have no well-defined answer. Hence, we remove any conversation that includes the keywords \u201cother interesting\u201d or \u201celse\u201d, which we found to be markers of low quality.\n# 4. Conclusion\nFor a long time, the dominant paradigm for building models in AI has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. However, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems. The promise of in-context learning is that we can build complex systems from pretrained components using only natural language as the medium for giving systems instructions and, as we argue for, allowing components to communicate with each other. In this new paradigm, the building blocks are pretrained models and the core operations are natural language instructions and operations on natural language texts. If we can realize this potential, then we can broaden participation in AI system development, rapidly prototype systems for new domains, and maximize the value of specialized pretrained components.\nIn the current paper, we introduced the DEMONSTRATE\u2013 SEARCH\u2013PREDICT (DSP) framework for retrieval augmented in-context learning. DSP consists of a number of simple, composable functions for implementing in-context learning systems as deliberate programs\u2014instead of endtask prompts\u2014for solving knowledge intensive tasks. We implemented DSP as a Python library and used it to write programs for Open-SQuAD, HotPotQA, and QReCC. These programs deliver substantial gains over previous in-context learning approaches. However, beyond any particular performance number, we argue that the central contribution of DSP is in helping to reveal a very large space of conceptual possibilities for in-context learning in general.\n# Acknowledgements\nWe thank Ashwin Paranjape, Amir Ziai, and Rick Battle for valuable discussions and feedback. This work was partially supported by IBM as a founding member of the Stanford Institute for Human-Centered Artificial Intelligence (HAI). This research was supported in part by affiliate members and other supporters of the Stanford DAWN project\u2014Ant Financial, Facebook, Google, and VMware\u2014as well as Cisco, SAP, and the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We thank Giuseppe Attanasio for his public LATEX GitHub-style Python code formatting gist.4 We also thank Riley Goodside for his public tips on formatting LM\nprompts (at @goodside on Twitter).\n# References\nAnantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S. Open-domain question answering goes conversational via question rewriting. arXiv preprint arXiv:2010.04898, 2020.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020.\nCao, G., Nie, J.-Y., Gao, J., and Robertson, S. Selecting good expansion terms for pseudo-relevance feedback. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp. 243\u2013250, 2008.\nChen, D., Fisch, A., Weston, J., and Bordes, A. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870\u20131879, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://aclanthology.org/P17-1171.\nDel Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de Gispert, A. Question rewriting for open-domain conversational qa: Best practices and limitations. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 2974\u20132978, 2021.\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.\nGao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan, Y., Zhao, V. Y., Lao, N., Lee, H., Juan, D.-C., et al. Attributed text generation via post-hoc research and revision. arXiv preprint arXiv:2210.08726, 2022.\nGeva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J. Did aristotle use a laptop? a question answering\nLe, N. T., Bai, F., and Ritter, A. Few-shot anaphora resolution in scientific protocols via mixtures of in-context experts. arXiv preprint arXiv:2210.03690, 2022.\nGeneration for Knowledge-Intensive NLP Tasks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract. html.\nLi, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T., Zettlemoyer, L., and Lewis, M. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022.\nLiu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\nPerez, E., Kiela, D., and Cho, K. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054\u201311070, 2021.\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\nQi, P., Lee, H., Sido, O., Manning, C. D., et al. Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text. arXiv preprint arXiv:2010.12527, 2020. URL https://arxiv.org/ abs/2010.12527.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nSun, Z., Wang, X., Tay, Y., Yang, Y., and Zhou, D. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.\nThorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 809\u2013819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://aclanthology.org/N18-1074.\nVakulenko, S., Kiesel, J., and Fr\u00f6be, M. SCAI-QReCC shared task on conversational question answering. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 4913\u20134922, Marseille, France,\nJune 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.525. Wang, X., Macdonald, C., Tonellotto, N., and Ounis, I. Colbert-prf: Semantic pseudo-relevance feedback for dense passage and document retrieval. ACM Transactions on the Web, 2022a. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022c. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. Wiher, G., Meister, C., and Cotterell, R. On decoding strategies for neural text generators. arXiv preprint arXiv:2203.15721, 2022. Xiong, W., Li, X. L., Iyer, S., Du, J., Lewis, P., Wang, W. Y., Mehdad, Y., Yih, W.-t., Riedel, S., Kiela, D., et al. Answering complex open-domain questions with multihop dense retrieval. arXiv preprint arXiv:2009.12756, 2020. URL https://arxiv.org/abs/2009.12756. Xue, X. and Croft, W. B. Modeling reformulation using query distributions. ACM Transactions on Information Systems (TOIS), 31(2):1\u201334, 2013. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Zelikman, E., Wu, Y., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022. Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. Zhong, V., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa: A benchmark for robust, multi-evidence, multi-answer question answering. arXiv preprint arXiv:2210.14353, 2022.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of existing retrieval-augmented in-context learning methods, particularly the simple 'retrieve-then-read' pipelines that fail to fully leverage the capabilities of frozen language models (LM) and retrieval models (RM). The proposed DEMONSTRATE\u2013SEARCH\u2013PREDICT (DSP) framework aims to enhance interactions between LMs and RMs, allowing for more sophisticated problem-solving approaches in knowledge-intensive tasks.",
        "problem": {
            "definition": "The problem is the inefficiency of current retrieval-augmented methods for knowledge-intensive tasks, where existing systems often generate unreliable responses due to their simplistic architectures.",
            "key obstacle": "The main challenge is that traditional methods do not adequately utilize the potential of both LMs and RMs, leading to limitations in their ability to process and generate accurate responses to complex queries."
        },
        "idea": {
            "intuition": "The idea behind DSP is inspired by the observation that both LMs and RMs can effectively process natural language, suggesting a need for a more integrated and compositional approach to problem-solving.",
            "opinion": "The proposed DSP framework allows for a structured interaction between LM and RM, facilitating high-level programmatic solutions that can handle complex queries through iterative transformations.",
            "innovation": "The key innovation of DSP lies in its ability to systematically break down tasks into smaller, manageable transformations that can be handled more reliably by LMs and RMs, thus improving the accuracy and efficiency of knowledge-intensive NLP tasks."
        },
        "method": {
            "method name": "DEMONSTRATE\u2013SEARCH\u2013PREDICT",
            "method abbreviation": "DSP",
            "method definition": "DSP is a framework that combines retrieval and language models through a structured pipeline, enabling the generation of context-aware responses to complex queries.",
            "method description": "DSP orchestrates a series of transformations between LMs and RMs to enhance the retrieval and generation of information for knowledge-intensive tasks.",
            "method steps": [
                "DEMONSTRATE: Prepare demonstrations from training examples to illustrate desired behaviors.",
                "SEARCH: Generate queries to retrieve relevant passages based on the input question.",
                "PREDICT: Use the retrieved passages and demonstrations to generate the final answer."
            ],
            "principle": "The effectiveness of DSP stems from its compositional nature, allowing for the integration of various transformations that leverage the strengths of both LMs and RMs, thus leading to more accurate and reliable outputs."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on datasets including Open-SQuAD, HotPotQA, and QReCC, comparing DSP against baseline methods such as vanilla LMs and retrieve-then-read pipelines.",
            "evaluation method": "Performance was assessed based on accuracy metrics (Exact Match and F1 scores) across various tasks, demonstrating substantial improvements with DSP over traditional methods."
        },
        "conclusion": "The DSP framework significantly enhances the performance of retrieval-augmented in-context learning systems, achieving state-of-the-art results in multiple knowledge-intensive NLP tasks and illustrating the potential for more sophisticated AI system development.",
        "discussion": {
            "advantage": "The primary advantage of DSP is its ability to systematically decompose complex tasks into manageable transformations, allowing for improved accuracy and efficiency in generating responses.",
            "limitation": "One limitation of the DSP approach may be its reliance on the quality of the training examples and the underlying models, which could affect performance if the data is not representative.",
            "future work": "Future research could explore further enhancements to the DSP framework, such as incorporating additional model architectures or expanding its applicability to other NLP tasks beyond those evaluated."
        },
        "other info": {
            "release_link": "https://github.com/stanfordnlp/dsp",
            "acknowledgments": "The authors thank contributors for discussions and support, including IBM and the Stanford Institute for Human-Centered Artificial Intelligence."
        }
    },
    "mount_outline": [
        {
            "section number": "3.1",
            "key information": "The DSP framework systematically decomposes complex tasks into manageable transformations, allowing for improved accuracy and efficiency in generating responses."
        },
        {
            "section number": "3.2",
            "key information": "The DSP framework facilitates high-level programmatic solutions that can handle complex queries through iterative transformations."
        },
        {
            "section number": "4.1",
            "key information": "The effectiveness of DSP stems from its compositional nature, allowing for the integration of various transformations that leverage the strengths of both LMs and RMs."
        },
        {
            "section number": "5.2",
            "key information": "Experiments were conducted on datasets including Open-SQuAD, HotPotQA, and QReCC, demonstrating substantial improvements with DSP over traditional methods in question answering."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the DSP approach may be its reliance on the quality of the training examples and the underlying models, which could affect performance if the data is not representative."
        }
    ],
    "similarity_score": 0.6920074240214463,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Demonstrate-Search-Predict_ Composing retrieval and language models for knowledge-intensive NLP.json"
}