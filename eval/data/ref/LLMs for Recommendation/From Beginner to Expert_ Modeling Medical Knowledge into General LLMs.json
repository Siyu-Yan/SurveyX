{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.01040",
    "title": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs",
    "abstract": "Recently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (>100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smallerscale model sizes (<100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, i.e., general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-trained general LLM in medical domain, especially for a specific medical task. (2) We collect and construct largescale medical datasets for each stage of the optimization process. These datasets encompass various data types and tasks, such as question-answering, medical reasoning, multi-choice questions, and medical conversations. (3) Specifically for multi-choice questions in the medical domain, we propose a novel Verificationof-Choice approach for prompting engineering, which significantly enhances the reasoning ability of LLMs. Remarkably, by combining the above approaches, our AntGLM-Med-10B model can outperform the most of LLMs on PubMedQA, including both general and medical LLMs, even when these LLMs have larger model size.",
    "bib_name": "li2024beginnerexpertmodelingmedical",
    "md_text": "# FROM BEGINNER TO EXPERT: MODELING MEDICAL KNOWLEDGE INTO GENERAL LLMS\nQiang Li:, Xiaoyan Yang:, Haowen Wang, Qin Wang, Junjie Wang, Yang Zhang, Mingyuan Chu, Sen Hu, Yicheng Chen, Yue Shen, Cong Fan, Wangshu Zhang, Teng Xu, Jinjie Gu, Jing Zheng, Guannan Zhang Ant Group {mangxiao.lq,joyce.yxy,zhanying}@antgroup.com\n# Qiang Li:, Xiaoyan Yang:, Haowen Wang, Qin Wang, Junjie Wang, Yang Zhang, Mingyuan Chu, Sen Hu, Yicheng Chen, Yue Shen, Cong Fan, Wangshu Zhang, Teng Xu, Jinjie Gu, Jing Zheng, Guannan Zhang Ant Group {mangxiao.lq,joyce.yxy,zhanying}@antgroup.com\nLei Liu;,\u25b3 Ant Group, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ) {liulei1497}@gmail.com\nLei Liu;,\u25b3 Ant Group, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ) {liulei1497}@gmail.com\nAnt Group, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ liulei1497}@gmail.com\n7 Jan 2024\n# ABSTRACT\nRecently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (>100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smallerscale model sizes (<100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, i.e., general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-trained general LLM in medical domain, especially for a specific medical task. (2) We collect and construct largescale medical datasets for each stage of the optimization process. These datasets encompass various data types and tasks, such as question-answering, medical reasoning, multi-choice questions, and medical conversations. (3) Specifically for multi-choice questions in the medical domain, we propose a novel Verificationof-Choice approach for prompting engineering, which significantly enhances the reasoning ability of LLMs. Remarkably, by combining the above approaches, our AntGLM-Med-10B model can outperform the most of LLMs on PubMedQA, including both general and medical LLMs, even when these LLMs have larger model size.\narXiv:2312.01040v3\n# INTRODUCTION\nThe ability of large language models (LLMs) is truly remarkable to understand and generate text in various fields like natural language, computer code, and protein sequences. These LLMs leverage the transformer architecture (Vaswani et al., 2017), which is specifically designed for sequence modeling and trained through self-supervision (Kenton & Toutanova, 2019). By increasing the model size, dataset size, and training computation, the performance on various benchmarks are consistently improved (Liang et al., 2022). These empirical findings are in line with a theoretical analysis (Kaplan et al., 2020), which highlights the significance of scale in ensuring the reliability of inferences made by LLMs. It is a long-standing research topic for AI in medicine to develop LLMs for solving medical problems, where accurate assessment of medical knowledge and reasoning capabilities is crucial for\n:These authors contributed equally to this work. ;Corresponding author. \u25b3Work was done during Lei Liu\u2019s research internship in Ant Group.\ninformed decision-making and favorable patient outcomes. Currently, LLMs for applications in medicine usually fail to fully utilize medical domain data, due to lacking general and specific clinical knowledge (Yim et al., 2020). As indicated by (Singhal et al., 2022), there is a discordance between what AI models can do and what may be expected of them in real-world clinical workflows (Lakkaraju et al., 2022; Schaekermann et al., 2020). There are two kinds of exploration paths to investigate the adaptation of LLMs in the medical domain. One approach is to conduct a thorough assessment of general language models like GPT3.5 (Brown et al., 2020), GPT-4 (OpenAI, 2023), and ChatGPT (OpenAI, 2022), without any specific fine-tuning for medical clinical issues. These models are designed for general purposes and are not specialized for medical domain. In (Nori et al., 2023), researchers evaluated performance of GPT-4 model with its predecessors in the GPT family on medical problems. Another way is to fine-tune a specialized LLM model through training or engineered to solve medical clinical tasks. For example, Singhal et al. (2023) developed a new medical LLM called Med-PaLM 2 and targeted medical domain-specific fine-tuning, which is based on a new base model (PaLM 2 Anil et al. (2023)). To evaluate how effectively LLMs encodes clinical medical knowledge, previous works (Jin et al., 2019; Singhal et al., 2023) generally considered the medical question answering task, which requires deep understanding on medical context, accurately recalling relevant medical knowledge, and reasoning with expert-level experiences. Nevertheless, existing medical LLMs are mainly based on scaling law (Chung et al., 2022) to train a larger model over massive data, which indeed lacks a fundamental optimization paradigm to adapt a pre-trained general language model towards a medical-specific expert. We conjecture that it is mainly due to the intrinsic LLM training paradigm, i.e., large-scale pre-training followed by specific fine-tuning. Concretely, as a medical beginner, a pre-trained general LLM needs to learn basic medical knowledge as background, which requires a continual pre-training over the large-scale medical data. Then, considering diverse task types on medical domain (e.g., QA, Multi-choice Question, and Reasoning), a relatively large-scale instruction fine-tuning process should be applied to encode task-related knowledge into LLMs. Finally, given a medical problem with a specific task type, a careful fine-tuning step can help to quickly and accurately adjust a LLM as a medical expert. In this study, we present a comprehensive methodology for fine-tuning a pre-trained general LLM model to transform it from a medical beginner to a medical expert. As shown in Figure 1, this process involves a 3-stage optimization procedure, namely continual pre-training for medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. To support each stage of fine-tuning, we curate and construct diverse large-scale medical datasets that encompass various data types and cover different tasks. These tasks include question-answering (QA), medical reasoning, multi-choice question, and medical conversations. Additionally, for the multi-choice question task within the medical domain, we introduce a novel Verification-of-Choice approach for prompting engineering. This approach significantly enhances the reasoning ability of LLMs, offering a valuable contribution to the field. By incorporating the afore-mentioned components, the obtained AntGLM-Med-10B can achieve an impressive accuracy on the PubMedQA. Notably, AntGLM-Med-10B can outperform many larger LLMs (>40B), demonstrating its effectiveness and potential in the medical domain.\n# The organization of this paper is as follows:\n(1) In Section 2, we provide a overview for the recent progress of LLMs and discuss some applications of LLMs in the medical domain. (2) In Section 3, we illustrate the framework preliminaries in detail, including the 3-stage optimization (continual pre-training, instruction fine-tuning, and specific-task adaptation), dataset collection and construction, and the utilized techniques. (3) In Section 4, we provide comprehensive experiments to indicate the effectiveness of 3-stage optimization, as well as the well analysis for each tuning strategies. The performance on PubMedQA is significantly improved.\n(1) In Section 2, we provide a overview for the recent progress of LLMs and discuss some applications of LLMs in the medical domain. (2) In Section 3, we illustrate the framework preliminaries in detail, including the 3-stage optimization (continual pre-training, instruction fine-tuning, and specific-task adaptation), dataset collection and construction, and the utilized techniques. (3) In Section 4, we provide comprehensive experiments to indicate the effectiveness of 3-stage optimization, as well as the well analysis for each tuning strategies. The performance on PubMedQA is significantly improved.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e234/e23484a3-e51c-4e15-9071-d89207c054da.png\" style=\"width: 50%;\"></div>\nStep-\u2160II Specific Medical Task Adaptation\nFigure 1: The 3-stage optimization procedure of AntGLM-Med-10B. We collect different data types (e.g., medical articles, books, and examinations) and medical tasks (question answering, reasoning, and multi-choice questions). In the adaptation stage, we mainly consider the optimizations for multi-choice questions, resulting in competitive performance on the PubMedQA.\nLarge Language Models. LLMs have achieved remarkable performance in various natural language processing (NLP) tasks (Brown et al., 2020; Wei et al., 2022; Liu et al., 2023), in recent years. These developments benefits from scaling up the training of both model size (typically for transformer-based models) and data scale, where scaling law Chung et al. (2022) indicates the relationship between model performance and model scale and dataset size. For example, when trained on extensive text corpora like Wikipedia and BooksCorpus, LLMs could show promising performance across various NLP tasks, including tasks requiring specialized domain knowledge and reasoning. In detail, GPT-3 (Brown et al., 2020) stands out as the pioneering language model with impressive 100 billion parameters, which showcases remarkable few-shot learning capabilities and introduces the concept of in-context learning. Following its success, a plethora of other LLMs have been proposed, such as Megatron-LM (Korthikanti et al., 2023), OPT (Zhang et al., 2022), Chinchilla (Hoffmann et al., 2022), Galactica (Taylor et al., 2022), LLaMA (Touvron et al.), PaLM (Chowdhery et al., 2022), and PaLM-2 (Singhal et al., 2023). These LLM models have further enhanced language understanding, generation, instruction following, reasoning abilities, and even possess a deep understanding of common sense knowledge (Mao et al., 2023). As a result, they have become indispensable base models across various domains, including Finance (Yang et al., 2023), Education (Milano et al., 2023), and Healthcare Arora & Arora (2023). Medical LLMs. These advanced improvements for LLMs have also demonstrated the effectiveness on medical domain, such as HuatuoGPT (Zhang et al., 2023), Med-PaLM 2 (Singhal et al., 2023), and Visual Med-Alpaca (Gao et al., 2023). In particular, HuatuoGPT (Zhang et al., 2023) presented to actively ask questions for the patients rather than only make a respond. Visual Med-Alpaca (Gao et al., 2023) integrated visual experts with LLMs for multi-modal biomedical tasks, which can perform better over various tasks. Although these approaches utilized scientific and biomedical corpora for both discriminative and generative language modeling, they are typically small in model size compared with LLMs GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).\n<div style=\"text-align: center;\">Table 1: The collected datasets for different optimization stages.</div>\nOptimization\nDataset\nDescription\nGeneral Medical\nKnowledge Injection\nMedical Books\nGeneral medical knowledge in medical and science textbooks\nKnowledge Graphs\nHighly structured medical knowledge in open-source knowledge graphs\nQuestion-Answer Pairs\nReal-world medical consultation information in textual form\nExam Questions\nQuestion, answer, and explanation for testing medical knowledge points\nArticles\nProfessional medical and science articles written by different doctors\nMedical Domain\nInstruction Tuning\nPromptCBLUE\nA instruction-tuning dataset for multi-task and few-shot learning in Chinese\nChinese Examination\nA dataset collected from the Chinese physician examination data\nWuma QA\nA large-scale QA database covering encyclopedia, hospital, and doctor\nHuatuo Wiki\nA subset (the data source is Chinese Wikipedia) of Huatuo-26M\nMultiple-choice Question\nMultiple-choice questions from different datasets\nSpecific Medical Task Adaptation\nMultiple-choice Question\nMultiple-choice questions from different datasets\n# 3 FROM BEGINNER TO EXPERT\n# 3.1 FRAMEWORK FORMULATION\nIn this work, our objective is to teach a medical beginner (i.e., a pre-trained general LLM) and let it become a medical expert (i.e., a fine-tuned medical LLM), which corresponds to a full procedure for adapting a pre-trained foundational large language model in the medical domain. The optimization process can be divided into three key steps: General Medical Knowledge Injection, Medical Domain Instruction Tuning, and Specific Medical Task Adaptation. As shown in Figure 1, General Medical Knowledge Injection aims to encode the fundamental medical knowledge into the pre-trained language model. Medical Domain Instruction Tuning can enrich the language model with diverse medical task types. Specific Medical Task Adaptation can tailor the model to align with a specific clinical task.\n# 3.2 BASE LLM: ANTGLM\nThe base LLM in this work is AntGLM, a general large-scale model developed by Ant Group. We conduct further pre-training and fine-tuning for its adaptations on the medical domain.\nArchitecture. AntGLM is based on the GLM architecture (Du et al., 2022), which combines the ideas of auto-encoding and auto-regression to enhance the learning process. AntGLM is featured by 48 transformer layers, a hidden size of 4096, and 64 attention heads, resulting in 10B parameters. AntGLM incorporates two-dimensional positional encoding and enables the pre-training task of predicting the order of blank regions, which can significantly improve the performance of blank filling during pre-training in a flexible manner. Overall, AntGLM performs well on various natural language processing tasks.\n# 3.3 DATASETS FOR GENERAL MEDICAL KNOWLEDGE INJECTION\nAs introduced below, several authoritative Chinese and English datasets are collected for the stage of General Medical Knowledge Injection, which mainly include PromptCBLUE (Zhu et al., 2023), MedPaLM2 (Singhal et al., 2023) dataset, and 3 sets of Chinese examination datasets.\nMedPaLM2 (Singhal et al., 2023) dataset, and 3 sets of Chinese examination datasets. Medical Books. In particular, we collect some medical books as training corpora, which include medical textbooks and popular science books. The high-quality and lengthy text in the book data makes LLMs easier to learn semantic correlations among contexts. We manually remove the contents in non-text format from the books, such as tables and appendices, which may influence the model training. After data pre-processing, we used approximately 150,000 medical-related books during pre-training. Knowledge Graphs. We use an open-source medical knowledge graph to generate pre-training data, i.e., Open Medical and Healthcare Alliance (Omaha). The data in the graph is highly structured and typically stored in the form of triplets. Since it is difficult for pre-training models to directly learn\nMedical Books. In particular, we collect some medical books as training corpora, which include medical textbooks and popular science books. The high-quality and lengthy text in the book data makes LLMs easier to learn semantic correlations among contexts. We manually remove the contents in non-text format from the books, such as tables and appendices, which may influence the model training. After data pre-processing, we used approximately 150,000 medical-related books during pre-training.\nKnowledge Graphs. We use an open-source medical knowledge graph to generate pre-training data, i.e., Open Medical and Healthcare Alliance (Omaha). The data in the graph is highly structured and typically stored in the form of triplets. Since it is difficult for pre-training models to directly learn\n<div style=\"text-align: center;\">Table 4: Chinese Examination Datasets.</div>\nChinese Examination\nSize\nClinical practitioner\n600\nVeterinary general practice\n400\nLicensed pharmacist of Western medicine\n480\nfrom triplet information, we sample sub-graphs of different scales and rewrite them into natu language texts.\nQuestion-Answer Pairs. We collect publicly available real-world medical question-answer pairs to enrich the training, which is easily accessible and rich medical consultation information in textual form. Therefore, we directly concatenate the question and answer data and use it as training corpora. Exam Questions. Exam questions usually consist of the question, answer, and explanation. These exam questions are designed to assess medical knowledge and require logical reasoning based on the provided knowledge. Therefore, its data quality is very high, making it an excellent source for pre-training corpora.\n<div style=\"text-align: center;\">Table 2: The data statistics involved in general medical knowledge injection.</div>\nData Type\nToken Size\nMedical Books\n7.18B\nKnowledge Graphs\n0.15B\nQA Pairs\n0.24B\nExam Questions\n0.11B\nArticles\n7.71B\nHowever, due to specific formatting requirements such as multiple-choice and fill-in-the-blank questions, pre-training models cannot directly consume this data. We also rewrite the exam question data and convert it into medical knowledge points.\nArticles. Article data consists of cutting-edge medical papers and popular science articles, which are used to enrich the latest medical knowledge to the dataset. We use PubMed Canese & Weis (2013) as the source of the paper data, which is a search engine for medical papers and includes over 25 million articles. PubMed contains both full-text and abstract data. Due to the presence of special text, such as tables in the full-text, which is difficult to process and may interfere with the model\u2019s performance, we only use PubMed abstracts as our training data, excluding the abstracts of the expert-level labeled PQA-L (see in Table 5) in PubMedQA dataset for avoiding annotation leakage. For medical popular science articles, we purchase professional medical articles written by different doctors from websites as training corpora. PromptCBLUE. PromptCBLUE (Zhu et al., 2023) is a dataset for Chinese medical evaluation jointly constructed by Alibaba Cloud Tianchi, East China Normal University, and other well-known enterprises and academic research institutions, with rankings being set.\n3.3.1 DATASET FOR MEDICAL DOMAIN INSTRUCTION TUNING\nChinese Examination Datasets. We collect three examination datasets from the Chinese physician examination data, including clinical practitioner, veterinary general practice, and licensed pharmacist of Western medicine. The overall dataset contains about 150k data. Table 4 illustrates the distribution of the test set, involving more than 1000 records. Wuma QA. We additionally collect 17,600 medical question-answer pairs from the realistic environment. Huatuo Wiki. Huatuo-26M (Li et al., 2023) is a largescale Chinese medical QA dataset. Huatuo Wiki is a subset of Huatuo-26M (Li et al., 2023). The data dical encyclopedia.\n<div style=\"text-align: center;\">Table 3: The data statistics involved in medical domain instruction tuning.</div>\nData Type\nSize\nPromptCBLUE\n68,500\nChinese Examination\n151,568\nWuma QA\n17,604\nHuatuo Wiki\n200,000\nMultiple-choice Question\n194,455\nof Huatuo Wiki is from the online medical encyclopedia.\nsource of Huatuo Wiki is from the online medical encyclopedia.\nMedical Multiple-choice Question Datasets. Following (Singhal et al., 2023), as shown in Table 1, we utilize the MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019) and MMLU clinical topics (Hendrycks et al., 2020) datasets.\n3.3.2 DATASET FOR SPECIFIC MEDICAL TASK ADAPTATION\n<div style=\"text-align: center;\">Table 5: The detailed information of the PubMedQA dataset.</div>\nStatistic\nPQA-L\nPQA-U\nPQA-A\nNumber of QA pairs\n1.0k\n61.2k\n211.3k\nProp. of yes (%)\n55.2\n-\n92.8\nProp. of no (%)\n33.8\n-\n7.2\nProp. of maybe (%)\n11.0\n-\n0.0\nAvg. question length\n14.4\n15.0\n16.3\nAvg. context length\n238.9\n237.3\n238.0\nAvg. long answer length\n43.2\n45.9\n41.0\n# 3.4 MEDICAL KNOWLEDGE MODELING\nTo take full advantage of medical data, we provide a detailed description for the training tricks for each step of 3-stage optimization. The technique tricks for different steps are shown in Figure 2.\n3.4.1 CONTINUAL PRE-TRAINING FOR MEDICAL KNOWLEDGE INJECTION\nFollowing Du et al. (2022), continual pre-training for AntGLM-Med-10B is to optimize an autoregressive blank-filling objective. Concretely, given an input text x \u201c[x1, x2, ..., xn], multiple spans ts1, ..., snu are sampled from it as PART B. Then PART B is replaced by the special [MASK] token, resulting in a corrupted sequence xcorrupt as PART A. Tokens in Part A can attach each other in a pair-wise way, while tokens in Part B can only attach Part A and the preceding parts in Part B. GLM randomly shuffles the spans to improve the model\u2019s semantic understanding for capturing the relationships among different spans. Two-dimensional positional encoding is utilized to represent the positional relationships within each span and among different spans. For autoregressive generation, each span is padded with special tokens [START] and [END] for input and output, respectively. This approach allows to automatically train both a bidirectional encoder and a unidirectional decoder within a unified architecture. By controlling the span length and granularity, the model can effectively handle both natural language understanding and generation tasks. Due to the limitations of the scale and capability of the general LLM models, we further pre-train the model over the medical data, significantly improving the performance in the medical field. Pre-Training Implementations. The training settings are basically consistent with GLM, using Adam as the optimizer with the following hyper-parameters: \u03b21 = 0.9, \u03b22 = 0.95. We employ a cosine learning rate schedule, where the final learning rate is equal to 10% of the maximum learning rate. Additionally, we apply a weight decay of 0.1 and gradient clipping of 1.0. The batch size is set to 256. Since we continue pretraining based on a general base, the learning rate is set to 7e-6. When training a model with a parameter count of 10B, our code achieves a processing speed of approximately 2000 tokens/gpu/sec on 32 A100 GPUs with 80GB RAM.\n# 3.4.2 FINE-TUNING FOR MEDICAL DOMAIN INSTRUCTION\nInstruction Fine-tuning. We applied instruction fine-tuning for the base LLM following the protocol used by Chung et al. (2022). For this process, we use a cosine learning rate schedule with an initial learning rate of 6e\u00b46, a weight decay of 0.1 and gradient clipping of 1.0. We utilize 40 A100 GPUs and allocate a batch size of 9 on each device, resulting in an overall batch size of 360. LoRA Fine-tuning. LoRA, introduced by (Hu et al., 2022), can decrease the the number of trainable parameters by optimizing pairs of rank-decomposition matrices for pre-trained LLMs, while the\nPubMedQA. As shown in Table 5, the PubMedQA dataset (Jin et al., 2019) consists of 1,000 question answer pairs annotated by the medical experts and more than 60k unlabeled questions. Given a question with a PubMed abstract as the context, the task is to generate a true choice from the multiple-choice answers (i.e., yes/no/maybe). Besides, PubMedQA requires answer inference from the supporting PubMed abstract context, which is a closed domain question answering task.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c7db/c7db588a-c23c-437a-9f9b-2d86eea9751b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: The detailed techniques for different optimization stages.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/31e6/31e6607d-498c-4a08-a6d5-8c2761b0d4b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/74e6/74e65fe7-d13a-468c-a6f6-7ecc5a3bda7d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99ed/99edcd05-f684-4ee6-b4e8-bece0ba0cbc7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3a71/3a71d87d-2b47-4755-a8d1-8e5335866849.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: A comparison example for Chain-of-Thought and Verification-of-Choi</div>\noriginal pre-trained model weights are frozen. The approach greatly reduces the storage requirement for LLMs, especially when adapted to some specific tasks. Thus, LoRA enables efficient taskswitching without introducing inference latency. For the performance, LoRA can also outperform several other adaptation methods including adapter, prefix-tuning, and fine-tuning. We use the same SFT strategy as described by (Hu et al., 2022) for the PubMedQA benchmark.\n# FINE-TUNING FOR SPECIFIC MEDICAL TASK ADAPTATION\n3.4.3 FINE-TUNING FOR SPECIFIC MEDICAL TASK ADAPTATION\nIn this section, we provide a detailed description for the prompting strategies used for specific medical task adaptation.\nIn this section, we provide a detailed description for the prompting strategies used for specific med ical task adaptation.\nChain-of-Thought Chain-of-Thought (CoT) is firstly introduced by Wei et al. Wei et al. (2022), which augments few-shot examples as a enhanced prompt with a step-by-step explanation. The approach empowers an LLM to condition on multi-step outputs towards the final answer. Medical questions usually require a complex multi-step reasoning process, which is well fit for CoT prompting. According to CoT, we exploited a self-generated explanation of each choice for the given medical questions, which provides the reason why an LLM gives this answer. Chain-of-Verification. Chain-of-Verification (COVE) Dhuliawala et al. (2023) is proposed to alleviate the hallucination issue, enabling LLMs deliberate on their responses for correcting the mistakes. Given the initial response as a draft, an LLM is required to plan verification questions for fact-checking the draft. Then the LLM should answers those questions independently, guaranteeing each response is not biased by others. Based on the above verification, the LLM can generate the final verified response with a higher confidence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1557/1557882b-526d-4e24-932a-ea3ba5a6d277.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/50ff/50ff6152-6474-472b-a564-bfc721b36255.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Overview of C-Poly framework.</div>\nVerification-of-Choice. Building on COT Wei et al. (2022) and COVE Dhuliawala et al. (2023), we presented a simple prompting strategy named as Verification-of-Choice (VoC) for multi-choice medical questions. VoC involves conditioning an LLM on its own generations for each choice before selecting a choice as the final answer. The overall process of VoC contains three steps: (1) Plan Multi-Choice Verifications: Given a query, assume that each choice is the true answer, let an LLM self-generate the corresponding explanation for each choice. Each explanation can be taken as a CoT for the choice. (2) Execute Multi-Choice Verifications: Taking multi-choice verifications as a context, LLM could make a comparison among them and self-analyze if there are any inconsistencies. (3) Generate Final Response: Given the inconsistencies (if any) among multi-choice verification, LLM could generate a final response. Unlike CoT and COVE, VoC may be used to investigate the inconsistency between the explanations of multiple choices and questions, thus it is helpful to produce more accurate responses. For example, the explanation of a choice is different with the conditions in the question. In this work, we apply VoC only for multiple-choice question task. Post-hoc Processing. PPL ranking is adopted as the indicator of uncertainty. For multi-choice QA datasets, the specific solution is to concatenate the original text with each option. Then a LLM is required to calculate the corresponding perplexity (Perplexity score) and select the option with the smallest PPL as the final predicted option. As for other datasets, generation and post-processing are utilized to produce the final response. LLM-annotated PQA-U. As shown in Table 5, the PQA-U subset of the PubMedQA dataset is not well utilized due to without accurate annotations. It is noticed that long answers of the PQAU subset are provided, which implicitly involve the correct answer for each question. To fully utilize the PQA-U subset, given a question in PQA-U, LLMs are required to generate responses for questions with the corresponding long answer, where VoC strategy is used to improve the correctness of answers. Then the generate answers are taken as pseudo annotations of PQA-U, which join the stage of specific medical task adaptation. To evaluate the annotation accuracy for PQA-U, we fine-tuned two models and reported their performance on the test set of PubMedQA. Concretely, one model is fine-tuned on the PQA-L and PQA-U annotated using long answers. Another model is fine-tuned on the PQA-L and PQA-U annotated using both long answers and VoC. The results are reported in Section. CPoly Fine-tuning. C-Poly, a fine-tuning method by Anonymous (2023), uses multi-task learning and adapters to differentiate shared and customized skills. It allows multi-class task samples to learn from each other in multi-adapter PEFT. The unified MTL framework C-Poly (Figure 4) enhances sample efficiency across tasks. For T tasks with data xt, the PLE-like structure has adapter modules \u03a6 with |\u03a6A| ` T \u02c6 |\u03a6t B| \u201c A ` T \u02c6 B adapters, assuming B \u201c 1. The C-Poly output is the sum of shared and task-specific adapters (Equation 1), with wi as learnable weights.\nCPoly Fine-tuning. C-Poly, a fine-tuning method by Anonymous (2023), uses multi-task learning and adapters to differentiate shared and customized skills. It allows multi-class task samples to learn from each other in multi-adapter PEFT. The unified MTL framework C-Poly (Figure 4) enhances sample efficiency across tasks. For T tasks with data xt, the PLE-like structure has adapter modules \u03a6 with |\u03a6A| ` T \u02c6 |\u03a6t B| \u201c A ` T \u02c6 B adapters, assuming B \u201c 1. The C-Poly output is the sum of shared and task-specific adapters (Equation 1), with wi as learnable weights.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cfd7/cfd7672c-2bda-4096-9a47-eba9e2f7ced6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/feb3/feb39197-0883-41d2-8a7c-c036f5c10c52.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/37d5/37d5c794-d890-48d9-8ae2-16460bf334c1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: The PubMedQA learderboard.</div>\nTable 6: The leaderboard for the PubMedQA dataset. Our AntGLM-Med-10B could obtain competitive performance with a relatively small model size.\nModel\nSize\nAccuracy\nMed-PaLM 2 (Singhal et al., 2023)\nNA\n81.8\nPalmyra-Med (Kamble & Alshikh, 2023)\n40B\n81.1\nAntGLM-Med\n10B\n80.6\nGPT-4-base (Nori et al., 2023)\nNA\n80.4\nGPT-3.5 + Z-Code++ (He et al., 2023)\n175B\n79.6\nFlan-PaLM-3-shot (Singhal et al., 2022)\n540B\n79.0\nCodex-5-shot (Li\u00e9vin et al., 2023)\n175B\n78.2\nHuman Performance (Jin et al., 2019)\nNA\n78.0\nGalactica (Taylor et al., 2022)\n120B\n77.6\nGatorTronGPT (Peng et al., 2023)\n20B\n77.6\nThe allocation matrix W P RT \u02c6pA`T q differentiates shared (WA) and customized (WB) skills. Different learning methods optimize skill acquisition, using low-rank approximations for efficiency. Shared skills use a Gumbel-sigmoid approach for differentiable sampling. Specialized skills are learned by differentiating shared and exclusive modules. By combining the above-mentioned three optimization stages together, we fine-tine AntGLM-10B on the collected large-scale, high-quality, medical-domain corpus, resulting in our final model AntGLM-Med-10B.\n# 4 EXPERIMENTS\n# 4.1 EXPERIMENTAL SETUP\nIn our experiments, we mainly discuss the LLM\u2019s performance after the final stage (Specific Medical Task Adaptation). The details are described as follows. In the case of vanilla LoRA, we set the rank of the low-rank approximation, r \u201c 8. We utilize 4 parallel LoRA for task-common skills and 1 LoRA for task-custom skill in CPoly, the rank is set to r \u201c 4. This decision is made to ensure a comparable number of training parameters across all methods. We train our model for 10 epoch with a batch size of 12 on given datasets during training. The AdamW optimizer (Loshchilov & Hutter, 2017) is used with a learning rate of 5e\u00b45. We also employ the linear decay strategy (Loshchilov & Hutter, 2016) as the learning rate scheduler with a weight decay of 0.01 and a warming up ratio of 0.06.\n# 4.2 MODEL EVALUATION\nTo evaluate our AntGLM-Med-10B model, we consider multi-choice question as the adaptation task in the third optimization stage). The PubMedQA dataset is utilized as the evaluation dataset, which requires medical research comprehension skills to make reasoning over PubMed abstract context. We use 500 test samples for evaluation.\n# 4.3 EXPERIMENTAL RESULTS\n# 4.3.1 MAIN RESULTS\nResults for PubMedQA. On PubMedQA, AntGLM-Med-10B obtained a score of 80.6%. This is below the state-of-the-art performance (81.8 from Med-PaLM 2 (Singhal et al., 2023)) and second place (81.1 from Palmyra-Med (Kamble & Alshikh, 2023)). The main reason is due to the relatively smaller model size, e.g., AntGLM-Med-10B vs. Palmyra-Med-40B. Although that, AntGLMMed-10B could exhibit improved performance compared to some other larger LLMs (as shown in Figure 5). Besides, AntGLM-Med-10B surpasses the models with similar model sizes (e.g., GatorTronGPT (Peng et al., 2023)).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae28/ae28d655-cc79-4caf-9534-d6f1c280eeac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Accuracy results on PubMedQA at different optimization stages. Stage-1 is the accuracy based on general medical knowledge injection. Stage-2 is for medical domain instruction based on Stage-1. Stage-3 is for specific medical task adaptation based on Stage-2.</div>\nTable 7: The accuracy results for models fine-tuned on PQA-L and PQA-U. PQA-U are annotated using different strategies. Long answer and VoC can provided the best accuracy results for LoRA and full paramete fine-tuning adaptation strategies, indicating that VoC can further improve the annotation accuracy for PQA-U.\nFine-tuning\nAnnotation Strategy\nPubMedQA\nLoRA\nLong Answer\n86.6\nLong Answer + VoC\n87.6\nFull Parameter Tuning\nLong Answer\n87.8\nLong Answer + VoC\n88.2\nled performance for specific task adaptation using different datasets and tuning strategies.\n<div style=\"text-align: center;\">Table 8: The detailed performance for specific task adaptation using different datasets and tuning strategies.</div>\nFine-tuning\nDataset\nSize\nPubMedQA\nCpoly\nPubMedQA\n500\n80.6\nMedMCQA\n500\nMedQA\n500\nPubMedQA\n500\n80.4\nMedMCQA\n500\nPubMedQA\n500\n80.2\nMedQA\n500\nMedMCQA\n500\n-\nMedQA\n500\nLoRA\nPubMedQA\n500\n78.8\nMedMCQA\n500\n78.6\nMedQA\n500\n78.0\nResults for Different Stages. As shown in Figure 6, we observed continuous accuracy improvements on the PubMedQA dataset throughout the entire optimization procedure. It is observed tha the base model only obtained an initial accuracy of 57.2% on PubMedQA, and ultimately achieved a 80.6% accuracy by the end of the optimization procedure, which indicate the effectiveness of the 3-stage optimization.\nEffectiveness of VOC. To evaluate the annotation accuracy for PQA-U, we fine-tuned two LLM models and reported their performance on the test set of PubMedQA. Concretely, one model is finetuned on the PQA-L and PQA-U, where PQA-U is annotated only using long answers. Another model is fine-tuned on the PQA-L and PQA-U, where PQA-U is annotated using both long answers and VoC. The results are reported in Table 7. Using both long answer and VoC to annotate PQA-U, the model can exhibit higher accuracy, which indicate that VoC can help to improve the annotation accuracy for PQA-U. Effectiveness of Cpoly. We evaluated the effectiveness of CPoly on multiple datasets and conducted an ablation experiment in Table 8. We randomly selected 500 samples from the PubMedQA, MedMCQA, and MedQA datasets respectively, and combined them using different strategies. We then validated the training results of a single, two, and three dataset samples, respectively. Since the router vectors of multiple adapters trained in CPoly cannot index and effectively predict untrained unknown tasks, we only reported the dataset\u2019s performance with the corresponding training set when using CPoly. The training results of CPoly on multiple datasets achieved significantly more improvement than those of LoRA on a single dataset. As the number of datasets involved in the CPoly training process increases, the best performance on PubMedQA increases. As the number of datasets trained with multitasking increased, the performance is significantly improved compared with training on a single dataset.\n# 5 CONCLUSION\nIn this work, we explore how to adapt a pre-trained general LLM in medical domain, from a medical beginner to a medical expert. Starting from a pre-trained general LLM model (AntGLM-10B), we leverage a 3-stage optimization procedure to fine-tune it, i.e., continual pre-training for medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Different large-scale medical datasets are collected, covering various data types and different tasks, such as question-answering (QA), medical reasoning, multi-choice question, and medical conversations. Besides, for multi-choice question in medical domain, we design a novel Verification-of-Choice approach for prompting engineering, which can significantly enhance the reasoning ability of LLMs. By combining the above points together, our AntGLM-Med-10B model can exhibit competitive performance compared with both general LLMs and other LLMs pre-trained on medical knowledge on the PubMedQA. It is noticed that AntGLM-Med-10B can outperform the most of larger LLMs (>40B).\n# REFERENCES\nREFERENCES Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Anonymous. Customizable combination of parameter-efficient modules for multi-task learning. In Submitted to The Twelfth International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=G1Hlubz1fR. under review. Anmol Arora and Ananya Arora. The promise of large language models in health care. The Lancet, 401(10377):641, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Kathi Canese and Sarah Weis. Pubmed: the bibliographic database. The NCBI handbook, 2(1), 2013. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: general language model pretraining with autoregressive blank infilling. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 320\u2013335. Association for Computational Linguistics, 2022. doi: 10.18653/ v1/2022.acl-long.26. URL https://doi.org/10.18653/v1/2022.acl-long.26. Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, et al. Ophglm: Training an ophthalmology large languageand-vision assistant based on instructions and dialogue. arXiv preprint arXiv:2306.12174, 2023. Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, and Xuedong Huang. Z-code++: A pre-trained language model optimized for abstractive summarization, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have a large-scale open domain question answering dataset from medical exams? Applied Sciences, 11(14):6421, 2021.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Anonymous. Customizable combination of parameter-efficient modules for multi-task learning. In Submitted to The Twelfth International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=G1Hlubz1fR. under review. Anmol Arora and Ananya Arora. The promise of large language models in health care. The Lancet, 401(10377):641, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Kathi Canese and Sarah Weis. Pubmed: the bibliographic database. The NCBI handbook, 2(1), 2013. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: general language model pretraining with autoregressive blank infilling. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 320\u2013335. Association for Computational Linguistics, 2022. doi: 10.18653/ v1/2022.acl-long.26. URL https://doi.org/10.18653/v1/2022.acl-long.26. Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, et al. Ophglm: Training an ophthalmology large languageand-vision assistant based on instructions and dialogue. arXiv preprint arXiv:2306.12174, 2023. Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, and Xuedong Huang. Z-code++: A pre-trained language model optimized for abstractive summarization, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have a large-scale open domain question answering dataset from medical exams? Applied Sciences, 11(14):6421, 2021.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2567\u20132577, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259. Kiran Kamble and Waseem Alshikh. Palmyra-med: Instruction-based fine-tuning of llms enhancing medical domain performance, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171\u2013 4186, 2019. Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023. Himabindu Lakkaraju, Dylan Slack, Yuxin Chen, Chenhao Tan, and Sameer Singh. Rethinking explainability as a dialogue: A practitioner\u2019s perspective. arXiv preprint arXiv:2202.01875, 2022. Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. Huatuo-26m, a large-scale chinese medical qa dataset, 2023. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023. URL https://arxiv.org/abs/2311.08719. Valentin Li\u00e9vin, Christoffer Egeberg Hother, and Ole Winther. Can large language models reason about medical questions?, 2023. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, and Erik Cambria. Gpteval: A survey on assessments of chatgpt and gpt-4. arXiv preprint arXiv:2308.12488, 2023. Silvia Milano, Joshua A McGrane, and Sabina Leonelli. Large language models challenge the future of higher education. Nature Machine Intelligence, 5(4):333\u2013334, 2023. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems, 2023. OpenAI. Chatgpt. 2022. URL https://chat.openai.com/chat. OpenAI. Gpt-4 technical report, 2023. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, pp. 248\u2013260. PMLR, 2022.\nCheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa, Cheryl Martin, Mona G Flores, Ying Zhang, Tanja Magoc, Gloria Lipori, Duane A Mitchell, Naykky S Ospina, Mustafa M Ahmed, William R Hogan, Elizabeth A Shenkman, Yi Guo, Jiang Bian, and Yonghui Wu. A study of generative large language model for medical research and healthcare, 2023. Mike Schaekermann, Carrie J Cai, Abigail E Huang, and Rory Sayres. Expert discussions improve comprehension of difficult cases in medical image assessment. In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 1\u201313, 2020. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge, 2022. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: open and efficient foundation language models, 2023. URL https://arxiv. org/abs/2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023. Jason Yim, Reena Chopra, Terry Spitz, Jim Winkens, Annette Obika, Christopher Kelly, Harry Askham, Marko Lukic, Josef Huemer, Katrin Fasler, et al. Predicting conversion to wet agerelated macular degeneration using deep learning. Nature Medicine, 26(6):892\u2013899, 2020. Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint arXiv:2305.15075, 2023. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Wei Zhu, Xiaoling Wang, Huanran Zheng, Mosha Chen, and Buzhou Tang. Promptcblue: A chinese prompt tuning benchmark for the medical domain. arXiv preprint arXiv:2310.14151, 2023.\n# Table 9: A prompt example for generating LLM-annotated PQA-U on PubMedQA dataset.\nOBJECTIVE. We compare 30-day and 180-day postadmission hospital mortality rates for all Medicare patients and those in three categories of cardiac care: coronary artery bypass graft surgery, acute myocardial infarction, and congestive heart failure. DATA SOURCES/COLLECTION. Health Care Financing Administration (HCFA) hospital mortality data for FY 1989. STUDY DESIGN. Using hospital level public use files of actual and predicted mortality at 30 and 180 days, we constructed residual mortality measures for each hospital. We ranked hospitals and used receiver operating characteristic (ROC) curves to compare 0-30, 31-180, and 0-180-day postadmission mortality. PRINCIPAL FINDINGS. For the admissions we studied, we found a broad range of hospital performance when we ranked hospitals using the 30-day data; some hospitals had much lower than predicted 30-day mortality rates, while others had much higher than predicted mortality rates. Data from the time period 31-180 days postadmission yield results that corroborate the 0-30 day postadmission data. Moreover, we found evidence that hospital performance on one condition is related to performance on the other conditions, but that the correlation is much weaker in the 31-180-day interval than in the 0-30-day period. Using ROC curves, we found that the 30-day data discriminated the top and bottom fifths of the 180-day data extremely well, especially for AMI outcomes. QUESTION: Measuring hospital mortality rates: are 30-day enough? A) yes B) no C) maybe\nContext\nOBJECTIVE. We compare 30-day and 180-day postadmission hos-\npital mortality rates for all Medicare patients and those in three cat-\negories of cardiac care: coronary artery bypass graft surgery, acute\nmyocardial infarction, and congestive heart failure.\nDATA SOURCES/COLLECTION. Health Care Financing Admin-\nistration (HCFA) hospital mortality data for FY 1989.\nSTUDY DESIGN. Using hospital level public use files of actual and\npredicted mortality at 30 and 180 days, we constructed residual mor-\ntality measures for each hospital. We ranked hospitals and used re-\nceiver operating characteristic (ROC) curves to compare 0-30, 31-180,\nand 0-180-day postadmission mortality.\nPRINCIPAL FINDINGS. For the admissions we studied, we found\na broad range of hospital performance when we ranked hospitals us-\ning the 30-day data; some hospitals had much lower than predicted\n30-day mortality rates, while others had much higher than predicted\nmortality rates. Data from the time period 31-180 days postadmission\nyield results that corroborate the 0-30 day postadmission data. More-\nover, we found evidence that hospital performance on one condition is\nrelated to performance on the other conditions, but that the correlation\nis much weaker in the 31-180-day interval than in the 0-30-day period.\nUsing ROC curves, we found that the 30-day data discriminated the\ntop and bottom fifths of the 180-day data extremely well, especially\nfor AMI outcomes.\nQUESTION: Measuring hospital mortality rates: are 30-day enough?\nA) yes B) no C) maybe\nVerification-of-Choice\nStep-1: Plan Multi-Choice Verification\nThink about why the answer is A) yes.\nA) Yes. The use of data on cumulative hospital mortality from 180\ndays postadmission does not provide a different perspective compared\nto using data from 30 days postadmission. This suggests that measur-\ning mortality rates at 30 days is sufficient for studying the conditions\nunder investigation.\nThink about why the answer is B) no.\nB) No. Using 30-day data may not be enough to accurately measure\nhospital mortality rates. Some conditions may have longer-term ef-\nfects on patients, and considering data from 180 days postadmission\ncould provide a more comprehensive understanding of the true mor-\ntality rates for these conditions.\nThink about why the answer is C) maybe.\nC) Maybe. The adequacy of using 30-day data to measure hospital\nmortality rates depends on the specific conditions being studied. Cer-\ntain conditions may have longer-term effects or complications that ex-\ntend beyond 30 days, warranting the inclusion of data from a longer\nduration for a more comprehensive understanding of mortality rates\nStep-2: Execute Multi-Choice Verification\nPlease judge the yes/no/maybe thinking process according to its logi-\ncal completeness and context.\nThe answer is A) Yes. The thinking process is logically complete and\ntakes into account the context provided. It states that using data on\ncumulative hospital mortality from 180 days postadmission does not\nprovide a different perspective compared to using data from 30 days\npostadmission. Therefore, it suggests that measuring mortality rates\nat 30 days is sufficient for studying the conditions under investigation.\nStep-3: Generate Final Response\nAnswer: A\nVerification-of-Choice\nStep-1: Plan Multi-Choice Verification\nThink about why the answer is A) yes.\nA) Yes. The use of data on cumulative hospital mortality from 180\ndays postadmission does not provide a different perspective compared\nto using data from 30 days postadmission. This suggests that measur-\ning mortality rates at 30 days is sufficient for studying the conditions\nunder investigation.\nThink about why the answer is B) no.\nB) No. Using 30-day data may not be enough to accurately measure\nhospital mortality rates. Some conditions may have longer-term ef-\nfects on patients, and considering data from 180 days postadmission\ncould provide a more comprehensive understanding of the true mor-\ntality rates for these conditions.\nThink about why the answer is C) maybe.\nC) Maybe. The adequacy of using 30-day data to measure hospital\nmortality rates depends on the specific conditions being studied. Cer-\ntain conditions may have longer-term effects or complications that ex-\ntend beyond 30 days, warranting the inclusion of data from a longer\nduration for a more comprehensive understanding of mortality rates\nStep-2: Execute Multi-Choice Verification\nPlease judge the yes/no/maybe thinking process according to its logi-\ncal completeness and context.\nThe answer is A) Yes. The thinking process is logically complete and\ntakes into account the context provided. It states that using data on\ncumulative hospital mortality from 180 days postadmission does not\nprovide a different perspective compared to using data from 30 days\npostadmission. Therefore, it suggests that measuring mortality rates\nat 30 days is sufficient for studying the conditions under investigation.\nStep-3: Generate Final Response\nAnswer: A\nStep-3: Generate Final Response\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of adapting large language models (LLMs) for medical applications, particularly in reasoning over medical knowledge and answering medical questions like a physician. Previous methods relied on increasing model size to improve performance, but there remains room for improvement in smaller models. This work introduces a new approach to fine-tune a pre-trained general LLM model, AntGLM-10B, into a medical expert model, AntGLM-Med-10B, through a structured optimization process.",
        "problem": {
            "definition": "The problem is the inadequacy of existing LLMs in effectively utilizing medical domain data to provide accurate medical reasoning and responses.",
            "key obstacle": "Existing methods primarily depend on scaling model size to achieve better performance, which does not fundamentally address the optimization needed for adapting general LLMs to medical-specific tasks."
        },
        "idea": {
            "intuition": "The idea stems from the observation that a structured approach to fine-tuning can enhance a general LLM's capabilities in the medical domain without necessitating a larger model.",
            "opinion": "The proposed idea involves a three-stage optimization process that transitions a pre-trained general LLM to a specialized medical expert model.",
            "innovation": "The key innovation lies in the introduction of a Verification-of-Choice approach for multi-choice medical questions, which significantly enhances the reasoning ability of LLMs compared to existing methods."
        },
        "method": {
            "method name": "AntGLM-Med-10B",
            "method abbreviation": "AGM-10B",
            "method definition": "AntGLM-Med-10B is a fine-tuned version of AntGLM-10B specifically optimized for medical question answering through a three-stage process.",
            "method description": "The method involves continual pre-training for medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation.",
            "method steps": [
                "General medical knowledge injection",
                "Medical domain instruction tuning",
                "Specific medical task adaptation"
            ],
            "principle": "The method is effective due to its structured approach that combines extensive pre-training on medical data with targeted fine-tuning for diverse medical tasks."
        },
        "experiments": {
            "evaluation setting": "The evaluation was conducted using the PubMedQA dataset, comparing the performance of AntGLM-Med-10B against other LLMs, including those with larger model sizes.",
            "evaluation method": "Performance was assessed based on accuracy in answering medical questions, utilizing various tuning strategies and datasets during training."
        },
        "conclusion": "The experiments demonstrate that AntGLM-Med-10B achieves competitive performance on the PubMedQA dataset, outperforming many larger models and illustrating the effectiveness of the proposed three-stage optimization approach.",
        "discussion": {
            "advantage": "The main advantages include enhanced reasoning capabilities in medical contexts and effective utilization of existing medical knowledge without the need for larger models.",
            "limitation": "A limitation of the method is that it may not fully capture all nuances of medical knowledge due to the reliance on pre-existing datasets and the model's size.",
            "future work": "Future research could focus on expanding the datasets used for training and exploring additional fine-tuning techniques to further improve performance."
        },
        "other info": {
            "additional details": {
                "model architecture": "AntGLM is based on the GLM architecture with 10 billion parameters, featuring 48 transformer layers.",
                "data sources": [
                    "Medical textbooks",
                    "Knowledge graphs",
                    "Real-world medical question-answer pairs",
                    "Exam questions",
                    "Professional medical articles"
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "AntGLM-Med-10B is a fine-tuned version of AntGLM-10B specifically optimized for medical question answering through a three-stage process."
        },
        {
            "section number": "4.2",
            "key information": "The method involves continual pre-training for medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation."
        },
        {
            "section number": "5.1",
            "key information": "The problem is the inadequacy of existing LLMs in effectively utilizing medical domain data to provide accurate medical reasoning and responses."
        },
        {
            "section number": "10.1",
            "key information": "A limitation of the method is that it may not fully capture all nuances of medical knowledge due to the reliance on pre-existing datasets and the model's size."
        },
        {
            "section number": "10.2",
            "key information": "Future research could focus on expanding the datasets used for training and exploring additional fine-tuning techniques to further improve performance."
        }
    ],
    "similarity_score": 0.730048478834469,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/From Beginner to Expert_ Modeling Medical Knowledge into General LLMs.json"
}