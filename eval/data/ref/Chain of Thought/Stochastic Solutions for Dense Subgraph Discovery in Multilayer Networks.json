{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2211.03306",
    "title": "Stochastic Solutions for Dense Subgraph Discovery in Multilayer Networks",
    "abstract": "Network analysis has played a key role in knowledge discovery and data mining. In many real-world applications in recent years, we are interested in mining multilayer networks, where we have a number of edge sets called layers, which encode different types of connections and/or time-dependent connections over the same set of vertices. Among many network analysis techniques, dense subgraph discovery, aiming to find a dense component in a network, is an essential primitive with a variety of applications in diverse domains. In this paper, we introduce a novel optimization model for dense subgraph discovery in multilayer networks. Our model aims to find a stochastic solution, i.e., a probability distribution over the family of vertex subsets, rather than a single vertex subset, whereas it can also be used for obtaining a single vertex subset. For our model, we design an LP-based polynomial-time exact algorithm. Moreover, to handle large-scale networks, we also devise a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speedup. Computational experiments demonstrate the validity of our model and the effectiveness of our algorithms.",
    "bib_name": "kawase2022stochasticsolutionsdensesubgraph",
    "md_text": "# Stochastic Solutions for Dense Subgraph Discovery in Multilayer Networks\nAtsushi Miyauchi The University of Tokyo Tokyo, Japan miyauchi@mist.i.u-tokyo.ac.jp Hanna Sumita Tokyo Institute of Technology Tokyo, Japan sumita@c.titech.ac.jp\nAtsushi Miyauchi The University of Tokyo Tokyo, Japan miyauchi@mist.i.u-tokyo.ac.jp\nYasushi Kawase The University of Tokyo Tokyo, Japan kawase@mist.i.u-tokyo.ac.jp\n# ABSTRACT\nNetwork analysis has played a key role in knowledge discovery and data mining. In many real-world applications in recent years, we are interested in mining multilayer networks, where we have a number of edge sets called layers, which encode different types of connections and/or time-dependent connections over the same set of vertices. Among many network analysis techniques, dense subgraph discovery, aiming to find a dense component in a network, is an essential primitive with a variety of applications in diverse domains. In this paper, we introduce a novel optimization model for dense subgraph discovery in multilayer networks. Our model aims to find a stochastic solution, i.e., a probability distribution over the family of vertex subsets, rather than a single vertex subset, whereas it can also be used for obtaining a single vertex subset. For our model, we design an LP-based polynomial-time exact algorithm. Moreover, to handle large-scale networks, we also devise a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speedup. Computational experiments demonstrate the validity of our model and the effectiveness of our algorithms.\nCCS CONCEPTS \u2022 Theory of computation \u2192Graph algorithms analysis; Mathematical optimization.\n# \u2022 Theory of computation \u2192Graph algorithms analysis; Mathematical optimization.\n# KEYWORDS\nnetwork analysis, multilayer networks, dense subgraph discovery, stochastic solutions\n# 1 INTRODUCTION\nNetwork analysis has played a key role in knowledge discovery and data mining. In many real-world applications in recent years, we are interested in mining multilayer networks rather than ordinary (i.e., single-layer) networks, where we have a number of edge sets called layers, which encode different types of connections and/or time-dependent connections over the same set of vertices [7, 15, 30]. For example, in the Twitter network, there are various layers representing different types of connections, e.g., follower\u2013followee relations, retweets, and mentions, among users. Moreover, each of those connections is time-dependent and therefore leads to multiple layers by itself. As another example, consider brain networks arising in neuroscience, where vertices correspond to small regions of a brain [19]. In this network, we can obtain at least two layers representing the structural connectivity and the functional connectivity (e.g., co-activation) among the small pieces of a brain.\nHanna Sumita Tokyo Institute of Technology Tokyo, Japan sumita@c.titech.ac.jp\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0520/0520fa1b-ae66-4ab0-902d-25d82370e680.png\" style=\"width: 50%;\"></div>\nFigure 1: An optimal solution to our model with the regret metric for the real-world multilayer network called WILDBIRDS: selecting the black vertices with probability 0.29 and the union of the gray and black vertices with probability 0.71. There are six layers, and the color of each edge represents the layer containing the edge.\nAmong many network analysis techniques, dense subgraph discovery, aiming to find a dense component in a network, is an essential primitive with a variety of applications in diverse domains [22, 32]. Examples include detecting communities and spam link farms in Web graphs [17, 21], experts extraction in crowdsourcing systems [28], real-time story identification in microblogging streams [1], and extracting molecular complexes in protein\u2013protein interaction networks [3]. Recently, dense subgraph discovery has been extended from single-layer networks to multilayer networks. Jethava and Beerenwinkel [27] introduced the densest common subgraph problem, where given a multilayer network, we are asked to find a vertex subset that maximizes the minimum degree density (defined later) over the layers. They mentioned some concrete applications of this problem in biological networks. Moreover, we can see that the densest common subgraph problem may appear in the context of robust optimization. Indeed, multilayer networks can be seen as a model of single-layer networks with uncertain edges with a number of scenarios, where each layer corresponds to one scenario. If we can find a subgraph that is reasonably dense for all layers, the solution is more robust than that obtained by single-layer network analysis, which may be arbitrarily bad for some layers.\n# 1.1 Our contribution\nIn this paper, we introduce a novel optimization model for dense subgraph discovery in multilayer networks. Given an edge-weighted multilayer network, our model aims to find a vertex subset that is dense for the layer selected by an adversary. Specifically, we employ a stochastic solution, i.e., a probability distribution over the family of vertex subsets, while existing work focuses only on a\ndeterministic solution, i.e., a single vertex subset. More precisely, we stochastically choose a vertex subset according to a stochastic solution and then, knowing only the stochastic solution but not a realization, the adversary selects the worst layer for us. Note that the worst layer with respect to each realization of the stochastic solution may be different from that with respect to the stochastic solution. It is worth mentioning that our model can be seen as a zero-sum two-player Stackelberg game [2], where the leader is our algorithm and the follower is the adversary. We measure the density of a vertex subset using the quality function called the degree density (or simply density) similarly to Jethava and Beerenwinkel [27]. The degree density of a vertex subset (in a single-layer network) is defined as half the average degree of the subgraph induced by the subset. This quality function is often used in the literature of dense subgraph discovery; in fact, this is the objective function of the well-known densest subgraph problem (see Section 2). We evaluate the performance of a stochastic solution (or an algorithm) using the following three metrics for the layer selected by the adversary: (i) the density, i.e., the expected degree density, (ii) the robust ratio, i.e., the ratio of the expected density to the optimal density, and (iii) the regret, i.e., the difference between the optimal density and the expected density. Therefore, we address three optimization problems according to the metrics. Our main algorithmic contribution is to design a polynomialtime exact algorithm for our optimization model, which is applicable to all of the above three metrics. Specifically, the algorithm first solves an LP, which is a generalization of the LP used for the densest common subgraph problem [27] and then computes an optimal probability distribution based on the LP\u2019s optimal solution. We observe that the output of our algorithm has a useful structure; the family of the vertex subsets with positive probabilities has a hierarchical structure. This leads to several practical benefits, e.g., the largest size subset contains all the other subsets and the optimal solution obtained by our algorithm has support size at most the number of vertices (although a solution of our optimization model may have support size exponential in the number of vertices). Moreover, we can demonstrate that the support size of the solution obtained is upper bounded by the number of layers. It is desirable to have a small support size for the understandability of stochastic solutions and for purposes of simple verification and validation. For practical use of our proposed algorithm, we wish to speed up our algorithm so that it is applicable to larger-scale multilayer networks. The bottleneck is the computation cost of the LP, which has a lot of variables and constraints. To this end, we devise a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speed-up. Specifically, the algorithm first computes an approximate solution by solving a much smaller LP than the aforementioned LP and then removes vertices from the original network using the information of the approximate solution obtained. Note that our preprocessing algorithm is a generalization of that proposed by Balalau et al. [4] for the densest subgraph problem. However, to verify that the preprocessing algorithm does not harm any optimal solution, we need a more sophisticated analysis, which is totally different from theirs.\nFinally, we conduct thorough computational experiments using synthetic graphs and real-world networks to verify the validity of our model and to evaluate the performance of our algorithms. Figure 1 illustrates an optimal solution to our model with the regret metric for the real-world multilayer network called WILDBIRDS (see Section 7 for its detailed characteristics). The optimal solution obtained is reasonably dense for all layers, as desired. It should be remarked that stochastic solutions can also be used for obtaining a single vertex subset. For example, the following three rules are reasonable to select a vertex subset from a stochastic solution: (i) stochastically select a subset following the probability distribution, (ii) select a subset with the highest probability, and (iii) select a subset with non-zero probability that optimizes some metric at hand (e.g., the minimum density, minimum robust ratio, and maximum regret, over layers). In our experiments, we compare the vertex subsets detected using the above rules with those obtained by existing algorithms for the densest common subgraph problem, in terms of various evaluation metrics.\n# 2 RELATED WORK\nThe densest subgraph problem is one of the most popular optimization models for dense subgraph discovery. Let \ud835\udc3a= (\ud835\udc49, \ud835\udc38) be an undirected graph and \ud835\udc64: \ud835\udc38\u2192R++ a positive edge weight. For a vertex subset \ud835\udc46\u2286\ud835\udc49, the subgraph induced by \ud835\udc46is denoted by \ud835\udc3a[\ud835\udc46] \ufffd(\ud835\udc46, \ud835\udc38[\ud835\udc46]), where \ud835\udc38[\ud835\udc46] \ufffd \ufffd {\ud835\udc62, \ud835\udc63} \u2208\ud835\udc38| \ud835\udc62, \ud835\udc63\u2208\ud835\udc46 \ufffd. In addition, we denote by \ud835\udc64(\ud835\udc46) the total weight of the edges in \ud835\udc46, i.e., \ud835\udc64(\ud835\udc46) = \ufffd \ud835\udc52\u2208\ud835\udc38[\ud835\udc46] \ud835\udc64(\ud835\udc52). For a nonempty \ud835\udc46\u2286\ud835\udc49, the degree density (or simply called density) of \ud835\udc46is defined as \ud835\udc64(\ud835\udc46)/|\ud835\udc46| (where we define the density of the empty set (i.e., 0/0) to be 0). In the densest subgraph problem, given a graph \ud835\udc3a= (\ud835\udc49, \ud835\udc38) with an edge weight \ud835\udc64, we are asked to find \ud835\udc46\u2286\ud835\udc49that maximizes the density \ud835\udc64(\ud835\udc46)/|\ud835\udc46|. It is well known that the densest subgraph problem can be solved exactly in polynomial time using a maximum-flow-based algorithm [23] or an LP-based algorithm [9]. Moreover, it was shown that a simple greedy algorithm called the greedy peeling admits 2-approximation in \ud835\udc42(\ud835\udc5a+ \ud835\udc5blog\ud835\udc5b) time [9, 31]. Recently, Boob et al. [8] designed an iterative greedy peeling algorithm, and demonstrated empirically that the output tends to be nearly optimal. Later, Chekuri, Quanrud, and Torres [11] proved the convergence to optimality of this algorithm (in a more general context). Balalau et al. [4] introduced a simple preprocessing algorithm to improve the scalability of (exact) algorithms for the densest subgraph problem. Their preprocessing algorithm first computes an approximate solution \ud835\udc46\u2286\ud835\udc49(using the greedy peeling), and then iteratively removes a vertex with the (weighted) degree less than the objective value of the approximate solution obtained. The validity of this preprocessing algorithm is guaranteed by the fact that any vertex with the (weighted) degree less than the optimal value is not contained in any optimal solution [4]. For the densest common subgraph problem, Jethava and Beerenwinkel [27] devised an LP-based polynomial-time heuristic and a 2\ud835\udc58-approximation algorithm based on the greedy peeling, where \ud835\udc58is the number of layers. Later, Charikar, Naamad, and Wu [10] designed two polynomial-time algorithms with approximation ratios \ud835\udc42( \u221a\ufe01 |\ud835\udc49| log\ud835\udc58) and \ud835\udc42(|\ud835\udc49|2/3) (irrespective of \ud835\udc58), respectively. Moreover, they showed some strong inapproximability results for\nthe problem, based on some reasonable computational complexity assumptions. Thus, it is very unlikely that a well-approximate solution can be found in polynomial time. In contrast to this, as mentioned above, we can compute an optimal stochastic solution in terms of the aforementioned three metrics in polynomial time. Recently, Galimberti, Bonchi, and Gullo [20] introduced a generalization of the densest common subgraph problem, which they refer to as the multilayer densest subgraph problem. This problem exploits a trade-off between the minimum density value over layers and the number of layers considered. They proposed an approximation algorithm using a core decomposition technique for multilayer networks. Very recently, Hashemi, Behrouz, and Lakshmanan [24] designed a sophisticated core decomposition algorithm, which they call the FirmCore decomposition algorithm. Their algorithm finds the set of (\ud835\udc58, \ud835\udf06)-FirmCores for all possible \ud835\udc58and \ud835\udf06in polynomial time, where (\ud835\udc58, \ud835\udf06)-FirmCore is a maximal subgraph in which every vertex has degree no less than \ud835\udc58in the subgraph for at least \ud835\udf06layers. They demonstrated that the decomposition unfolds a better solution to the multilayer densest subgraph problem than the algorithm by Galimberti, Bonchi, and Gullo [20] for many instances. Semertzidis et al. [38] introduced another generalization of the densest common subgraph problem, called the Best Friends Forever (BFF) problem, in the context of evolving graphs with a number of snapshots. The BFF problem is a series of optimization problems that maximize an aggregate density over snapshots, where the aggregate density is set to be the average/minimum value of the average/minimum degree of vertices over layers. They investigated the computational complexity of the problems and designed some approximation or heuristic algorithms. Recalling that multilayer networks can also be seen as a model of networks with uncertainty, we can find some other optimization models related. Zou [44] studied the densest subgraph problem in uncertain graphs. An uncertain graph is a pair of \ud835\udc3a= (\ud835\udc49, \ud835\udc38) and \ud835\udc5d: \ud835\udc38\u2192[0, 1], where \ud835\udc52\u2208\ud835\udc38is present with probability \ud835\udc5d(\ud835\udc52) whereas \ud835\udc52\u2208\ud835\udc38is absent with probability 1 \u2212\ud835\udc5d(\ud835\udc52). In the problem, given an uncertain graph \ud835\udc3a= (\ud835\udc49, \ud835\udc38) with \ud835\udc5d, we seek \ud835\udc46\u2286\ud835\udc49that maximizes the expected density. Zou [44] showed that this problem can be reduced to the original densest subgraph problem, and designed a polynomial-time exact algorithm based on the reduction. Recently, Tsourakakis et al. [41] introduced a more general optimization problem called the risk-averse dense subgraph discovery. As another example, Miyauchi and Takeda [35] introduced an optimization problem called the robust densest subgraph problem. In this problem, given an undirected graph \ud835\udc3a= (\ud835\udc49, \ud835\udc38) and an edge-weight space \ud835\udc3c= \u00d7\ud835\udc52\u2208\ud835\udc38[\ud835\udc59\ud835\udc52,\ud835\udc5f\ud835\udc52] \u2286\u00d7\ud835\udc52\u2208\ud835\udc38[0, \u221e), we are asked to find \ud835\udc46\u2286\ud835\udc49that maximizes min\ud835\udc64\u2208\ud835\udc3c \ud835\udc64(\ud835\udc46)/|\ud835\udc46| \ud835\udc64(\ud835\udc46\u2217\ud835\udc64)/|\ud835\udc46\u2217\ud835\udc64| , where \ud835\udc46\u2217\ud835\udc64is an optimal solution to the densest subgraph problem for \ud835\udc3awith \ud835\udc64. The intuition of this problem is the same as that of ours; this problem also seeks \ud835\udc46\u2286\ud835\udc49that is reasonably dense for any \ud835\udc64\u2208\ud835\udc3c. However, they considered only deterministic solutions and gave a strong hardness result. As well as dense subgraph discovery, many important primitives for single-layer network analysis have recently been extended to multilayer networks. Examples include community detection [6, 13, 25, 39], link prediction [13, 26], analyzing spreading processes [14, 37], and identifying central vertices [5, 16].\n# 3 MODEL\nIn this section, we formally define our optimization model. Let \ud835\udc3a= (\ud835\udc49, (\ud835\udc38\ud835\udc56)\ud835\udc56\u2208[\ud835\udc58]) be a multilayer network consisting of \ud835\udc58layers, and let \ud835\udc64\ud835\udc56: \ud835\udc38\ud835\udc56\u2192R++ be a positive edge weight for layer \ud835\udc56. We denote by \ud835\udc38the union of all edge sets, i.e., \ud835\udc38\ufffd\ufffd \ud835\udc56\u2208[\ud835\udc58] \ud835\udc38\ud835\udc56. Let \ud835\udc46\u2217 \ud835\udc56be a densest subgraph for layer \ud835\udc56, i.e., \ud835\udc46\u2217 \ud835\udc56\u2208arg max\ud835\udc46\u2286\ud835\udc49\ud835\udc64\ud835\udc56(\ud835\udc46)/|\ud835\udc46|. Our task is to find a vertex subset that is dense for the layer selected adversarially. As mentioned in the introduction, we consider a stochastic solution to compete with the adversary. Let \u0394(2\ud835\udc49) be the set of probability distributions over 2\ud835\udc49. For each \ud835\udc5d\u2208\u0394(2\ud835\udc49), we denote by \ud835\udc5d\ud835\udc46the probability of choosing \ud835\udc46\u2286\ud835\udc49. We aim to compute \ud835\udc5d\u2208\u0394(2\ud835\udc49) that maximizes some metric (when the adversary selects the worst layer to \ud835\udc5d). We employ the following three metrics: Density. The first metric is the degree density itself. Specifically, when we select a vertex subset according to a probability distribution \ud835\udc5d\u2208\u0394(2\ud835\udc49) and the adversary selects a layer \ud835\udc56\u2208[\ud835\udc58], our metric is defined as follows: \ufffd \ufffd \ufffd \ufffd\nAs the adversary selects the worst layer, we aim to find \ud835\udc5d\u2208\u0394(2\ud835\udc49) that maximizes the minimum of the density (1) among \ud835\udc56\u2208[\ud835\udc58]. Our optimization model with this metric can be seen as a stochastic version of the densest common subgraph problem introduced by Jethava and Beerenwinkel [27]. Robust ratio. The robust ratio is a metric based on the ratio of the expected degree to the optimal degree. For a nonempty \ud835\udc46\u2286\ud835\udc49and \ud835\udc56\u2208[\ud835\udc58], let us consider a normalized density defined as \ud835\udc64\ud835\udc56(\ud835\udc46)/|\ud835\udc46| \ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56)/|\ud835\udc46\u2217 \ud835\udc56| . When we select a vertex subset according to a probability distribution \ud835\udc5d\u2208\u0394(2\ud835\udc49) and the adversary selects a layer \ud835\udc56\u2208[\ud835\udc58], the metric is defined as follows: \ufffd \ufffd \ufffd \ufffd\n(2)\nIn other words, the robust ratio is equivalent to the density for the multilayer network with weights \ud835\udc64\u2032 1, . . . ,\ud835\udc64\u2032 \ud835\udc58given by \ud835\udc64\u2032 \ud835\udc56(\ud835\udc52) \ufffd \ud835\udc64\ud835\udc56(\ud835\udc52) \ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56)/|\ud835\udc46\u2217 \ud835\udc56| for each \ud835\udc56\u2208[\ud835\udc58] and \ud835\udc52\u2208\ud835\udc38\ud835\udc56. As the adversary selects the worst layer, we aim to find \ud835\udc5d\u2208\u0394(2\ud835\udc49) that maximizes the minimum of (2) among \ud835\udc56\u2208[\ud835\udc58]. Note that the optimal robust ratio is contained in the interval [1/\ud835\udc58, 1] because \ud835\udc5d\u2208\u0394(2\ud835\udc49) such that \ud835\udc5d\ud835\udc46\u2217 \ud835\udc56= 1/\ud835\udc58for each \ud835\udc56\u2208[\ud835\udc58] has the objective value of 1/\ud835\udc58. Regret. The regret is a metric based on the difference between the optimal density and the expected density. For \ud835\udc46\u2286\ud835\udc49and \ud835\udc56\u2208[\ud835\udc58], the regret is defined as \ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56)/|\ud835\udc46\u2217 \ud835\udc56| \u2212\ud835\udc64\ud835\udc56(\ud835\udc46)/|\ud835\udc46|. When we select a vertex subset according to a probability distribution \ud835\udc5d\u2208\u0394(2\ud835\udc49) and the adversary selects a layer \ud835\udc56\u2208[\ud835\udc58], the metric is defined as follows: \ufffd \ufffd \ufffd \ufffd\n(3)\nAs the adversary selects the worst layer, we aim to find \ud835\udc5d\u2208\u0394(2\ud835\udc49) that minimizes the maximum of (3) among \ud835\udc56\u2208[\ud835\udc58].\nHere we explain how to select an appropriate metric. The density and regret metrics are useful when we are concerned with multilayer networks with homogeneous layers such as time-dependent follower-followee relations in the Twitter network. Although the density metric can be the first choice, the regret metric is more suitable for robust analysis. For example, consider the case where there are a number of layers consistent with each other together with some noisy (e.g., random) layers. The density metric would suffer from the effect of the noisy layers, but the regret metric would avoid it and find dense subgraphs in the other meaningful layers. On the other hand, the robust ratio metric is useful when we analyze multilayer networks with heterogeneous layers such as brain networks with structural and functional connectivity layers. From its definition, the robust ratio metric would find subgraphs that are reasonably dense for all layers. The density and regret metrics focus only on the layers with small optimal densities and the layers with large optimal densities, respectively. Unified concept: (\ud835\udf36, \ud835\udf37)-density. Here we introduce a general metric, enabling us to deal with the above three metrics in a unified manner. An important fact is that the robust ratio and regret metrics can be obtained by affine transformations of the density. Specifically, when we select a subgraph according to a probability distribution \ud835\udc5d\u2208\u0394(2\ud835\udc49) and the adversary selects a layer \ud835\udc56\u2208[\ud835\udc58], we define the (\ud835\udf36, \ud835\udf37)-density using two vectors \ud835\udf36\u2208R\ud835\udc58+ and \ud835\udf37\u2208R\ud835\udc58as follows: \ufffd \ufffd \ufffd \ufffd\nNote that the above three metrics, the density, robust ratio, and regret, are equivalent to the (1, 0)-density, ((|\ud835\udc46\u2217 \ud835\udc56|/\ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56))\ud835\udc56\u2208[\ud835\udc58], 0)density, and (1, (\u2212\ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56)/|\ud835\udc46\u2217 \ud835\udc56|)\ud835\udc56\u2208[\ud835\udc58])-density1, respectively. Note that\ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56)/|\ud835\udc46\u2217 \ud835\udc56| is polynomially computable for each \ud835\udc56\u2208[\ud835\udc58] [9, 23]. We refer to (\ud835\udf36, \ud835\udf37)-DENSITY as the problem of finding \ud835\udc5d\u2208\u0394(2\ud835\udc49) that maximizes the minimum of the (\ud835\udf36, \ud835\udf37)-density among \ud835\udc56\u2208[\ud835\udc58]. Therefore, in the following, we aim to design an algorithm for (\ud835\udf36, \ud835\udf37)-DENSITY.\n# 4 ALGORITHM\nIn this section, we provide an LP-based polynomial-time exact algorithm for (\ud835\udf36, \ud835\udf37)-DENSITY. Let ((\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, (\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49,\ud835\udc61) be continuous variables. We consider the following LP:\n(4)\n(\u2200\ud835\udc52= {\ud835\udc62, \ud835\udc63} \u2208\ud835\udc38), (4)\n(\u2200\ud835\udc52\u2208\ud835\udc38, \u2200\ud835\udc63\u2208\ud835\udc49).\nWhen (\ud835\udf36, \ud835\udf37) = (1, 0), this formulation coincides with the LP introduced by Jethava and Beerenwinkel [27] for the densest common subgraph problem. However, they did not point out the connection between a solution of the LP and a distribution in \u0394(2\ud835\udc49). Our algorithm first computes an optimal solution to LP (4), denoted by (( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61). Then we set \ud835\udc5f0,\ud835\udc5f1, . . . ,\ud835\udc5f\u2113to be the\n1The actual regret value is the negation of (1, (\u2212\ud835\udc64\ud835\udc56(\ud835\udc46\u2217 \ud835\udc56)/|\ud835\udc46\u2217 \ud835\udc56|)\ud835\udc56\u2208[\ud835\udc58])-density.\n<div style=\"text-align: center;\">Algorithm 1: LP-based algorithm</div>\nAlgorithm 1: LP-based algorithm\n1 Solve LP (4) to obtain an optimal solution\n(( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61);\n2 Let \ud835\udc5f0,\ud835\udc5f1, . . . ,\ud835\udc5f\u2113be reals such that\n{\ud835\udc5f0,\ud835\udc5f1, . . . ,\ud835\udc5f\u2113} = { \u02c6\ud835\udc66\ud835\udc63| \ud835\udc63\u2208\ud835\udc49} \u222a{0} and\n\ud835\udc5f0 (= 0) < \ud835\udc5f1 < \ud835\udc5f2 < \u00b7 \u00b7 \u00b7 < \ud835\udc5f\u2113;\n3 Let \ud835\udc46\ud835\udc57= {\ud835\udc63\u2208\ud835\udc49| \u02c6\ud835\udc66\ud835\udc63\u2265\ud835\udc5f\ud835\udc57} (\ud835\udc57= 1, . . . , \u2113);\n4 return \u02c6\ud835\udc5d\u2208\u0394(2\ud835\udc49) with \u02c6\ud835\udc5d\ud835\udc46\ud835\udc57= (\ud835\udc5f\ud835\udc57\u2212\ud835\udc5f\ud835\udc57\u22121) \u00b7 |\ud835\udc46\ud835\udc57| (\ud835\udc57= 1, . . . , \u2113)\n(and \u02c6\ud835\udc5d\ud835\udc46= 0 for the other \ud835\udc46\u2019s).\nreals such that {\ud835\udc5f0,\ud835\udc5f1, . . . ,\ud835\udc5f\u2113} = { \u02c6\ud835\udc66\ud835\udc63| \ud835\udc63\u2208\ud835\udc49} \u222a{0} and \ud835\udc5f0 (= 0) < \ud835\udc5f1 < \ud835\udc5f2 < \u00b7 \u00b7 \u00b7 < \ud835\udc5f\u2113. In addition, let \ud835\udc46\ud835\udc57= {\ud835\udc63\u2208\ud835\udc49| \u02c6\ud835\udc66\ud835\udc63\u2265\ud835\udc5f\ud835\udc57} (\ud835\udc57= 1, . . . , \u2113). The output of our algorithm is the probability distribution \u02c6\ud835\udc5d\u2208\u0394(2\ud835\udc49) defined as\n(5)\n \ufffd 1 = \u2211\ufe01 \ud835\udc63\u2208\ud835\udc49 \u02c6\ud835\udc66\ud835\udc63= \u2211\ufe01 \ud835\udc57\u2208[\u2113] \u2211\ufe01 \ud835\udc63: \u02c6\ud835\udc66\ud835\udc63=\ud835\udc5f\ud835\udc57 \ud835\udc5f\ud835\udc57= \u2211\ufe01 \ud835\udc57\u2208[\u2113] \ud835\udc5f\ud835\udc57(|\ud835\udc46\ud835\udc57| \u2212|\ud835\udc46\ud835\udc57+1|) = \u2211\ufe01 \ud835\udc57\u2208[\u2113] \u02c6\ud835\udc5d\ud835\udc46\ud835\udc57,\nwhere \ud835\udc46\u2113+1 = \u2205for ease of notation. Our algorithm is formally described in Algorithm 1. Clearly, the algorithm runs in polynomial time.\n# 5 ANALYSIS\nIn this section, we first demonstrate that the output \u02c6\ud835\udc5dof Algorithm 1 is optimal to (\ud835\udf36, \ud835\udf37)-DENSITY. Then we analyze the structure of the output \u02c6\ud835\udc5dwith a special attention to its support size.\n# 5.1 Optimality of the output of Algorithm 1 We first prove that the (\ud835\udf36, \ud835\udf37)-density of \u02c6\ud835\udc5dis equal to the optimal value of LP (4):\n5.1 Optimality of the output of Algorithm 1 We first prove that the (\ud835\udf36, \ud835\udf37)-density of \u02c6\ud835\udc5dis equal to the optimal value of LP (4):\nLemma 5.1. It holds that\nProof. To prove the lemma, we show that for any \ud835\udc56\u2208[\ud835\udc58],\nBy the definition of \ud835\udc46\ud835\udc57(\ud835\udc57= 1, . . . , \u2113), for each \ud835\udc52= {\ud835\udc62, \ud835\udc63} \u2208\ud835\udc38\ud835\udc56, we observe that \ud835\udc52\u2208\ud835\udc38\ud835\udc56[\ud835\udc46\ud835\udc57] \u21d0\u21d2\ud835\udc62, \ud835\udc63\u2208\ud835\udc46\ud835\udc57 \u21d0\u21d2\u02c6\ud835\udc66\ud835\udc62\u2265\ud835\udc5f\ud835\udc57and \u02c6\ud835\udc66\ud835\udc63\u2265 \ud835\udc5f\ud835\udc57 \u21d0\u21d2 \u02c6\ud835\udc65\ud835\udc52\u2265\ud835\udc5f\ud835\udc57, where the last equivalence follows from \u02c6\ud835\udc65\ud835\udc52= min{ \u02c6\ud835\udc66\ud835\udc62, \u02c6\ud835\udc66\ud835\udc63}.\nRecall that \u02c6\ud835\udc5dis defined as (5). Then we have \ufffd \ufffd \u2211\ufe01\n\u2208 where the second last equality follows from the above equivalence and the last equality follows from the fact that for each \ud835\udc52\u2208\ud835\udc38,  \ufffd\n \ufffd Next, we prove that the optimal value of LP (4) gives an upper bound on the optimal value of (\ud835\udf36, \ud835\udf37)-DENSITY:\nProof. Let us take an arbitrary \u02dc\ud835\udc5d\u2208\u0394(2\ud835\udc49). We consider a solution (( \u02dc\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02dc\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02dc\ud835\udc61) of LP (4) such that \u2211\ufe01 \u2211\ufe01 \ufffd \u2211\ufe01 \ufffd\n (6)\nThe solution (( \u02dc\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02dc\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02dc\ud835\udc61) is feasible for LP (4); in fact, the only concern is the third constraint but we see that \u2211\ufe01 \u2211\ufe01 \u2211\ufe01 \u2211\ufe01 \ufffd \ufffd\n\u2208 Hence, the optimal value of LP (4) is at least \u02dc\ud835\udc61. Moreover, we have \ufffd \u2211\ufe01 \ufffd\nRecalling that \u02dc\ud835\udc5dis taken arbitrarily from \u0394(2\ud835\udc49), we see that the optimal value of LP (4) is at least that of (\ud835\udf36, \ud835\udf37)-DENSITY. \u25a1 Combining Lemmas 5.1 and 5.2, we have the desired result: Theorem 5.3. Algorithm 1 outputs an optimal solution to (\ud835\udf36, \ud835\udf37)DENSITY.\n# 5.2 Hierarchical structure and support size of the output of Algorithm 1\nHere we observe some useful properties of the output of Algorithm 1. By the design of Algorithm 1, we see that the support of the output \u02c6\ud835\udc5dof the algorithm (i.e., an optimal solution to (\ud835\udf36, \ud835\udf37)DENSITY) has a hierarchical structure. We denote the support of \ud835\udc5d\u2208\u0394(2\ud835\udc49) by supp(\ud835\udc5d) = {\ud835\udc46\u2286\ud835\udc49| \ud835\udc5d\ud835\udc46> 0}.\nProof. Let (( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61) be a basic solution. Without loss of generality, we may assume that\n{} (\u2200 {} \u2208) Recall that \u2113denotes the number of different positive values in { \u02c6\ud835\udc66\ud835\udc63| \u02c6\ud835\udc66\ud835\udc63> 0, \ud835\udc63\u2208\ud835\udc49}. Let \ud835\udc490 = {\ud835\udc63\u2208\ud835\udc49| \u02c6\ud835\udc66\ud835\udc63= 0}. We divide \ud835\udc49\\ \ud835\udc490 into \u2113subsets of vertices sharing the same value of \u02c6\ud835\udc66\ud835\udc63, denoted by \ud835\udc491, . . . ,\ud835\udc49\u2113. Let us focus on the constraints in LP (4) that are satisfied with equality. For each \ud835\udc57= 0, 1, . . . , \u2113, let \ud835\udc39\ud835\udc57\u2286\ud835\udc38[\ud835\udc49\ud835\udc57] be a spanning forest in \ud835\udc38[\ud835\udc49\ud835\udc57]. Let \ud835\udf0cbe the number of connected components in \ud835\udc38[\ud835\udc491] \u222a\u00b7 \u00b7 \u00b7 \u222a\ud835\udc38[\ud835\udc49\u2113], and let \ud835\udf01be that of \ud835\udc38[\ud835\udc490]. We arbitrarily take \ud835\udf01 vertices, denoted by\ud835\udc621, . . . ,\ud835\udc62\ud835\udf01, one from each connected component in \ud835\udc38[\ud835\udc490]. We focus on the following constraints (satisfied with equality): \ufffd\n(\u2200\u2208) where \ud835\udc3e\u2032 = {\ud835\udc56\u2208[\ud835\udc58] | \ud835\udc61= \ud835\udefc\ud835\udc56\u00b7 \ufffd \ud835\udc52\u2208\ud835\udc38\ud835\udc56\ud835\udc64\ud835\udc56(\ud835\udc52)\ud835\udc65\ud835\udc52+ \ud835\udefd\ud835\udc56}. We prove that the coefficient matrix of those constraints is not full-rank. For ease of discussion, we remove constraints that are represented by a linear combination of others. Specifically, it is enough to focus on the following constraints:  \ufffd\n(7) (8) (9) (10) (11) (12)\nThere are two types of missing constraints. First, let \ud835\udc52= {\ud835\udc62, \ud835\udc63} \u2208 \ud835\udc38[\ud835\udc49\ud835\udc57] \\ \ud835\udc39\ud835\udc57(\ud835\udc57\u2208[\u2113]) such that \ud835\udc65\ud835\udc52= \ud835\udc66\ud835\udc63appears in (9). There exists a cycle\ud835\udc36in \ud835\udc39\ud835\udc57\u222a{\ud835\udc52}. By a telescoping sum of the constraints (8) along \ud835\udc36, i.e., \ud835\udc66\ud835\udc63= \ud835\udc65\ud835\udc52\u2032, \ud835\udc65\ud835\udc52\u2032 = \ud835\udc66\ud835\udc63\u2032, ..., \ud835\udc65\ud835\udc52\u2032\u2032 = \ud835\udc66\ud835\udc62, we obtain \ud835\udc66\ud835\udc62= \ud835\udc66\ud835\udc63. Then, constraint \ud835\udc65\ud835\udc52= \ud835\udc66\ud835\udc62is obtained by summing \ud835\udc65\ud835\udc52= \ud835\udc66\ud835\udc63for \ud835\udc66\ud835\udc62= \ud835\udc66\ud835\udc63. Next, let \ud835\udc63\u2208\ud835\udc490 belong to a connected component containing \ud835\udc62\ud835\udc57 (\ud835\udc57\u2208[\ud835\udf01]). By summing (8) along a path from \ud835\udc63to \ud835\udc62\ud835\udc57, i.e., \ud835\udc66\ud835\udc63= \ud835\udc65\ud835\udc52\u2032, \ud835\udc65\ud835\udc52\u2032 = \ud835\udc66\ud835\udc63\u2032, ..., \ud835\udc65\ud835\udc52\u2032\u2032 = \ud835\udc66\ud835\udc62\ud835\udc57, we obtain\ud835\udc66\ud835\udc63= \ud835\udc66\ud835\udc62\ud835\udc57. Then, constraint\ud835\udc66\ud835\udc63= 0 is obtained by summing \ud835\udc66\ud835\udc63= \ud835\udc66\ud835\udc62\ud835\udc57and \ud835\udc66\ud835\udc62\ud835\udc57= 0 from (12). The rank of the coefficient matrix is equal to the number of constraints (7)\u2013(12). For (7), we have at most \ud835\udc58constraints; the\nnumber of constraints (8) is 2(|\ud835\udc49| \u2212\ud835\udf0c\u2212\ud835\udf01); that for (9) and (10) is |\ud835\udc38| \u2212|\ud835\udc49| + \ud835\udf0c+ \ud835\udf01; that for (11) and (12) is 1 + \ud835\udf01. Therefore, we have at most |\ud835\udc49| + |\ud835\udc38| \u2212\ud835\udf0c+ 1 + \ud835\udc58constraints. We have |\ud835\udc49| + |\ud835\udc38| + 1 variables in LP (4). If \ud835\udf0c> \ud835\udc58, then we have at most |\ud835\udc49| + |\ud835\udc38| constraints, and hence the coefficient matrix of (7)\u2013(12) cannot have rank |\ud835\udc49| + |\ud835\udc38| + 1. As the solution is basic, \ud835\udf0c\u2264\ud835\udc58must hold. Recall that each \ud835\udc49\ud835\udc57(\ud835\udc57= 1, . . . , \u2113) has at least one connected component. Therefore, we have at most \ud835\udc58different positive values of \ud835\udc66\ud835\udc63\u2019s, which implies that the output \u02c6\ud835\udc5dhas support size at most \ud835\udc58. \u25a1\n# 6 PREPROCESSING\nIn this section, we present a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speed-up. Specifically, the algorithm first computes an approximate solution by solving an LP, which is much smaller than LP (4) in practice, and then removes vertices from the original network using the information of the approximate solution obtained. We assume that \ud835\udc46\u2217 \ud835\udc56for all \ud835\udc56\u2208[\ud835\udc58] are known in advance because they can be computed efficiently using Charikar\u2019s LP-based algorithm [9] together with the preprocessing algorithm introduced by Balalau et al. [4]. To describe our algorithm, we introduce some notations. For \ud835\udc46\u2286\ud835\udc49, \ud835\udc63\u2208\ud835\udc46, and \ud835\udc56\u2208[\ud835\udc58], let \ud835\udc51\ud835\udc56(\ud835\udc46, \ud835\udc63) denote the weighted degree of \ud835\udc63in the subgraph induced by \ud835\udc46in layer \ud835\udc56, i.e., \ud835\udc51\ud835\udc56(\ud835\udc46, \ud835\udc63) \ufffd\ufffd \ud835\udc52\u2208\ud835\udc38\ud835\udc56[\ud835\udc46]: \ud835\udc63\u2208\ud835\udc52\ud835\udc64\ud835\udc56(\ud835\udc52). When \ud835\udc46= \ud835\udc49, we simply write \ud835\udc51\ud835\udc56(\ud835\udc63). We first describe a fast algorithm for finding an approximate solution for (\ud835\udf36, \ud835\udf37)-DENSITY. Specifically, we compute a probability distribution\ud835\udc5e\u2208\u0394(2\ud835\udc49) that maximizes the (\ud835\udf36, \ud835\udf37)-density under the constraint that \ud835\udc5e\ud835\udc46= 0 for all \ud835\udc46\u22082\ud835\udc49\\ {\ud835\udc46\u2217 1, . . . ,\ud835\udc46\u2217 \ud835\udc58}. The distribution can be found by solving the following LP:\n(13)\n(\u2200\ud835\udc57\u2208[\ud835\udc58]).\nNote that this LP has \ud835\udc58+ 1 variables and 2\ud835\udc58+ 1 constraints. As \ud835\udc58 is usually much smaller than |\ud835\udc49| and |\ud835\udc38|, this LP is much smaller than LP (4) in practice. Next we describe an algorithm for removing vertices using the information of the above approximate solution \ud835\udc5e. Let \u2113\u2217be the (\ud835\udf36, \ud835\udf37)-density of \ud835\udc5e, i.e., the optimal value of LP (13). Note that this is a lower bound on the optimal value of (\ud835\udf36, \ud835\udf37)-DENSITY. Our algorithm iteratively removes any vertex \ud835\udc63\u2217that satisfies max\ud835\udc56\u2208[\ud835\udc58] [\ud835\udefc\ud835\udc56\u00b7 \ud835\udc51\ud835\udc56(\ud835\udc49\u2032, \ud835\udc63\u2217) + \ud835\udefd\ud835\udc56] < \u2113\u2217, where \ud835\udc49\u2032 is a remaining vertex set (initially \ud835\udc49\u2032 = \ud835\udc49), as long as there exists such a vertex. For reference, we describe the procedure in Algorithm 2. This algorithm can be implemented to run in \ud835\udc42(\ud835\udc58|\ud835\udc38| + |\ud835\udc49| log |\ud835\udc49|) time. From now on, we demonstrate that the algorithm does not remove any vertex that is contained in a subset in supp(\ud835\udc5d), where \ud835\udc5d is an arbitrary optimal solution to (\ud835\udf36, \ud835\udf37)-DENSITY. The following is a key lemma in our analysis.\nAlgorithm 2: Remove useless vertices\nInput\n: (\ud835\udc49, (\ud835\udc38\ud835\udc56)\ud835\udc56\u2208[\ud835\udc58]) with \ud835\udc641, . . . ,\ud835\udc64\ud835\udc58, and \u2113\u2217\u2208R\nOutput: (\ud835\udc49\u2032, (\ud835\udc38\ud835\udc56[\ud835\udc49\u2032])\ud835\udc56\u2208[\ud835\udc58])\n1 \ud835\udc49\u2032 \u2190\ud835\udc49;\n2 while True do\n3\nLet \ud835\udc63\u2217\u2208arg min\ud835\udc63\u2208\ud835\udc49\u2032 max\ud835\udc56\u2208[\ud835\udc58] [\ud835\udefc\ud835\udc56\u00b7 \ud835\udc51\ud835\udc56(\ud835\udc49\u2032, \ud835\udc63) + \ud835\udefd\ud835\udc56];\n4\nif max\ud835\udc56\u2208[\ud835\udc58] [\ud835\udefc\ud835\udc56\u00b7 \ud835\udc51\ud835\udc56(\ud835\udc49\u2032, \ud835\udc63\u2217) + \ud835\udefd\ud835\udc56] \u2265\u2113\u2217then\n5\nreturn (\ud835\udc49\u2032, (\ud835\udc38\ud835\udc56[\ud835\udc49\u2032])\ud835\udc56\u2208[\ud835\udc58]).\n6\nelse \ud835\udc49\u2032 \u2190\ud835\udc49\u2032 \\ {\ud835\udc63\u2217};\nLemma 6.1. Let \u2113\u2217be a lower bound on the optimal value of (\ud835\udf36, \ud835\udf37)DENSITY. If max\ud835\udc56\u2208[\ud835\udc58] [\ud835\udefc\ud835\udc56\u00b7 \ud835\udc51\ud835\udc56(\ud835\udc63\u2217) + \ud835\udefd\ud835\udc56] < \u2113\u2217, then \ud835\udc66\ud835\udc63\u2217= 0 for any optimal solution to LP (4). Proof. We prove the lemma by contradiction. We denote by (( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61) an optimal solution to LP (4) and let \ud835\udc63\u2217\u2208\ud835\udc49 be a vertex that satisfies \ud835\udefc\ud835\udc56\u00b7\ud835\udc51\ud835\udc56(\ud835\udc63\u2217) + \ud835\udefd\ud835\udc56< \u2113\u2217for all \ud835\udc56\u2208[\ud835\udc58]. Suppose for contradiction that \u02c6\ud835\udc66\ud835\udc63\u2217> 0. We construct a solution ((\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, (\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49,\ud835\udc61) of LP (4) as follows: \ufffd \ufffd\nIt is easy to see that ((\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, (\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49,\ud835\udc61) is a feasible solution of LP (4). Moreover, we have \ufffd \u2211\ufe01 \ufffd\n \u2212  \u2212 where the first inequality follows from \u02c6\ud835\udc65\ud835\udc52\u2264\u02c6\ud835\udc66\ud835\udc63\u2217for each \ud835\udc52\u220b\ud835\udc63\u2217, the second inequality follows from the assumptions \ud835\udefc\ud835\udc56\u00b7\ud835\udc51\ud835\udc56(\ud835\udc63\u2217) +\ud835\udefd\ud835\udc56< \u2113\u2217 and \u02c6\ud835\udc66\ud835\udc63\u2217> 0, and the third inequality follows from Lemma 5.2. This contradicts the optimality of (( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61). \u25a1\n\u25a1\nTheorem 6.2. Let \u2113\u2217be a lower bound on the optimal value of (\ud835\udf36, \ud835\udf37)-DENSITY. Then, any vertex \ud835\udc63\u2217that satisfies max\ud835\udc56\u2208[\ud835\udc58] [\ud835\udefc\ud835\udc56\u00b7 \ud835\udc51\ud835\udc56(\ud835\udc63\u2217) + \ud835\udefd\ud835\udc56] < \u2113\u2217is not contained in any subset in the support of any optimal solution to (\ud835\udf36, \ud835\udf37)-DENSITY.\nProof. Let \ud835\udc63\u2217be any vertex with max\ud835\udc56\u2208[\ud835\udc58] [\ud835\udefc\ud835\udc56\u00b7 \ud835\udc51\ud835\udc56(\ud835\udc63\u2217) + \ud835\udefd\ud835\udc56] < \u2113\u2217. Let \u02c6\ud835\udc5dbe any optimal solution to (\ud835\udf36, \ud835\udf37)-DENSITY. Construct (( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61) from \u02c6\ud835\udc5das in (6). From Lemma 5.1 and the proof of Lemma 5.2, we see that (( \u02c6\ud835\udc65\ud835\udc52)\ud835\udc52\u2208\ud835\udc38, ( \u02c6\ud835\udc66\ud835\udc63)\ud835\udc63\u2208\ud835\udc49, \u02c6\ud835\udc61) is an optimal solution to LP (4). Thus, by Lemma 6.1, we have \u02c6\ud835\udc66\ud835\udc63\u2217= 0. By the construction of \u02c6\ud835\udc66\ud835\udc63\u2217in (6), \u02c6\ud835\udc5d\ud835\udc46= 0 for all \ud835\udc46\u2286\ud835\udc49containing \ud835\udc63\u2217. \u25a1\nThis theorem indicates that Algorithm 2 does not remove any vertex that is contained in a subset in the support of any optimal solution to (\ud835\udf36, \ud835\udf37)-DENSITY.\n# 7 EXPERIMENTAL EVALUATION\nIn this section, we conduct computational experiments using synthetic graphs and real-world networks to verify the validity of our proposed model and to evaluate the performance of our proposed algorithms. All experiments were conducted on a machine equipped with Intel Xeon W 10-core processor and 64GB RAM. Algorithms were implemented in Python using Gurobi Optimizer 9.0.2.\n# 7.1 Validity of our model\nHere we aim to verify the validity of our model using synthetic graphs. To this end, we use a randomly generated multilayer network with a planted clique, and examine whether an optimal solution detects vertex subsets close to the clique. We first explain our random procedure for generating multilayer networks. We produce an (unweighted) random power-law graph as a layer using the Chung\u2013Lu model [12], where we first specify an expected degree \ud835\udc51\ud835\udc63for each \ud835\udc63\u2208\ud835\udc49according to the power-law distribution with exponent \ud835\udefd, and then connect each pair of vertices {\ud835\udc62, \ud835\udc63} with probability \ud835\udc51\ud835\udc62\u00b7\ud835\udc51\ud835\udc63 \ufffd \ud835\udc5f\u2208\ud835\udc49\ud835\udc51\ud835\udc5f. Note that the graph becomes sparser as the exponent \ud835\udefdincreases. In this multilayer network, we randomly select a vertex subset \ud835\udc49\ud835\udc50with some size, and plant a clique on \ud835\udc49\ud835\udc50(in some specified layers). To evaluate the performance of optimal solution \ud835\udc5d\u2208\u0394(2\ud835\udc49) to (\ud835\udf36, \ud835\udf37)-DENSITY in the above multilayer network, we introduce the following measure, which we refer to as the (expected) F measure:\n  \u223c \ufffd |\ud835\udc49\ud835\udc50| \ufffd  \ufffd \u2286 |\ud835\udc49\ud835\udc50|  if \ud835\udc5dtends to be close to \ud835\udc49\ud835\udc50. We first investigate the case where a clique is planted in all layers. We generate \ud835\udc58(= 1, 2, 3, 4, 5) power-law graphs (i.e., layers) with \ud835\udefd= 2.3 on \ud835\udc49with |\ud835\udc49| = 1,000. Then we randomly select a subset \ud835\udc49\ud835\udc50\u2286\ud835\udc49consisting of 10 vertices, and plant a clique on \ud835\udc49\ud835\udc50in all layers. The performance of optimal solutions to (\ud835\udf36, \ud835\udf37)-DENSITY is shown in Figure 2(a). As can be seen, for any metric, the F measure is reasonably large for \ud835\udc58\u22652, meaning that our algorithm tends to detect \ud835\udc49\ud835\udc50using the information of multiple layers. Next we investigate the case where a clique is planted in only one layer. We generate \ud835\udc58(= 1, 2, 3, 4, 5) power-law graphs with \ud835\udefd= 3.0 on \ud835\udc49with |\ud835\udc49| = 1,000. Then we randomly select \ud835\udc49\ud835\udc50\u2286 \ud835\udc49consisting of 20 vertices, but plant a clique on \ud835\udc49\ud835\udc50only in one randomly selected layer. The performance of optimal solutions are described in Figure 2(b). As can be seen, the F measure becomes\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3b54/3b54731a-c7a3-4fad-a2a4-15fd0ee6cc1e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/83d3/83d3e19b-1ae2-4d03-aee7-b70c855b357e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d02e/d02e043a-dd20-471c-acb2-66858bdf653b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Clique in all layers.</div>\n<div style=\"text-align: center;\">(b) Clique in only one layer.</div>\nFigure 2: Performance of optimal solutions. Each point corresponds to the average value over 100 network realizations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c012/c012d67d-371e-49c5-b3e9-f7c3d4c34f50.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e27/7e27b357-e233-4948-b10a-49b9f15e6c1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dad9/dad9593a-daaa-4390-b05c-74cda1f22db1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Clique in all layers.</div>\n<div style=\"text-align: center;\">(b) Clique in only one layer.</div>\nFigure 3: Performance of single vertex subsets obtained by optimal solutions. The same averaging procedure is applied.\n# Figure 3: Performance of single vertex subsets obtained by optimal solutions. The same averaging procedure is applied.\nsmaller as the number of layers increases. Among the three metrics, the regret performs particularly well because it concentrates on the layer containing the clique from its definition; therefore, the regret metric seems most suitable for robust analysis with noisy layers. The robust ratio performs second best; it also cares about the layer containing the clique. Finally we conclude this subsection by evaluating vertex subsets that are obtained from optimal solutions \ud835\udc5d\u2208\u0394(2\ud835\udc49). To verify the validity of our model, we select a vertex subset attaining the highest probability in \ud835\udc5d\u2208\u0394(2\ud835\udc49). As our algorithm does not know about\ud835\udc49\ud835\udc50, it cannot select a vertex subset using the F measure. For reference, we also run two baseline algorithms, DCS-LP and DCS-Greedy, designed by Jethava and Beerenwinkel [27]. Note that DCS-LP can be seen as the algorithm that selects a vertex subset with the largest minimum density value over layers from the support of \ud835\udc5d\u2208\u0394(2\ud835\udc49), where \ud835\udc5d\u2208\u0394(2\ud835\udc49) is an optimal solution to our algorithm with the density metric. The results are depicted in Figure 3, where we employed the same experimental settings as above and used the usual (deterministic) F measure for evaluation. As can be seen, the trend of our algorithm is similar to that observed in the above experiments; all metrics almost detect \ud835\udc49\ud835\udc50for the all-layers setting, but only the regret metric is successful for the only-one-layer setting. As for the baseline methods, DCS-LP is comparable to our algorithm with the density metric, while DCS-Greedy performs quite poorly.\n# 7.2 Performance of our algorithm\nHere we examine the performance of our proposed algorithm using publicly-available real-world multilayer networks. Table 1 shows the results together with the summary of the characteristics of the datasets. Even for large networks, our algorithm obtains an optimal\nTable 1: Performance of our algorithm for real-world datasets. OPT and |supp| denote the optimal value of (\ud835\udf36, \ud835\udf37)-DENSITY and the support size of the optimal solution, respectively. Note that the objective values for the regret are negated. LB indicates the optimal value of LP (13). |\ud835\udc49\u2032| and |\ud835\udc38[\ud835\udc49\u2032]|, respectively, denote the number of vertices and edges after running Algorithm 2. Preprocess time represents the running time of Algorithm 2. Total time represents the running time of our proposed algorithm, i.e., Algorithm 1 together with Algorithm 2. For reference, the running time of Algorithm 1 without Algorithm 2 is also presented in the next column. The last three columns report the performance of single vertex subsets obtained by optimal solutions. For each instance and metric, the best value among the algorithms is in bold.\nDataset\n|\ud835\udc49|\n|\ud835\udc38|\n\ud835\udc58\nMetric\nOPT |supp|\nLB\n|\ud835\udc49\u2032| |\ud835\udc38[\ud835\udc49\u2032]|\nPreprocess\ntime (s)\nTotal\ntime (s)\nw/o preprocess\ntime (s)\nDensity\nRobust\nratio\nRegret\nDensity\n1.1950\n3\n1.1313\n123\n3,126\n0.04\n0.66\n0.73\n1.1875\n0.6579\n-0.6282\nWILDBIRDS\n202\n4,574\n6 Robust ratio\n0.7707\n4\n0.7364\n121\n2,901\n0.05\n0.77\n0.79\n1.0058 0.6970\n-0.5065\n[18]\u2217\nRegret\n-0.4122\n2\n-0.4473\n124\n2,976\n0.04\n0.71\n0.77\n0.9129\n0.6518\n-0.5821\nDensity\n4.7023\n2\n4.4257\n730\n6,403\n1208.79\n1304.28\n1834.88\n4.6316\n0.6399\n-3.1581\nAS-733\n7,716\n24,179 733 Robust ratio\n0.7721\n4\n0.7295\n278\n3,452\n1165.87\n1211.82\n1906.21\n4.5789\n0.7057\n-1.9621\n[33]\u2020\nRegret\n-6.9861\n4\n-1.8023\n228\n3,020\n1289.15\n1329.56\n1950.33\n3.6154 0.7317\n-1.6316\nDensity\n12.0263\n1\n12.0263\n118\n1,815\n0.29\n0.92\n8.31 12.0263\n0.9666\n-0.4382\nOregon1\n11,492\n26,461\n9 Robust ratio\n0.9808\n3\n0.9738\n110\n1,702\n0.48\n1.02\n7.55\n11.9016 0.9728\n-0.3578\n[33]\u2020\nRegret\n-0.2544\n3\n-0.3441\n110\n1,702\n0.50\n0.95\n9.72\n11.8889 0.9728\n-0.3578\nDensity\n22.2503\n2\n21.8228\n1,130\n14,166\n1.90\n4.31\n93.30 18.4000\n0.1183\n-247.6000\nMoscowAthletics2013\n88,804 186,846\n3 Robust ratio\n0.4709\n3\n0.3566\n105\n787\n2.02\n2.12\n112.50\n13.2000 0.4197 -181.1667\n[36]\u2021\nRegret\n-125.4477\n2 -130.1752\n88,804 186,846\n1.40\n336.56\n334.43\n0.4286\n0.0186\n-200.8333\nDensity\n3.5649\n2\n3.5077\n17,551 184,659\n1.98\n332.02\n5466.73\n3.5625\n0.1768\n-17.0770\nNYClimateMarch2014 102,439 329,474\n3 Robust ratio\n0.6661\n3\n0.5503\n3,901\n78,126\n2.36\n147.40\n6058.12\n2.3839 0.6652\n-6.8047\n[36]\u2021\nRegret\n-2.3052\n3\n-3.5835 102,439 329,473\n1.58 18575.97\n18909.39\n1.2785\n0.3576\n-2.3221\nDensity\n60.7462\n2\n60.7462\n375\n4,536\n11.07\n11.72\n4268.72 52.7143\n0.0840\n-837.8095\nCannes2013\n438,537 848,017\n3 Robust ratio\n0.3633\n3\n0.3633\n246\n1,617\n11.53\n11.72\n3981.18\n42.6000 0.2441\n-365.0667\n[36]\u2021\nRegret\n-132.8628\n2 -132.8628 438,537 848,017\n7.31\n4021.19\n4022.75\n0.6667\n0.0073 -155.5000\nDensity\n1.1891\n10\n1.1236 191,074 559,628\n18.52\n7826.41\n13787.37\n0.4146\n0.0387\n-31.2295\nDBLP\n513,627 888,353\n10 Robust ratio\n0.1178\n10\n0.1125 317,231 687,335\n17.01 15323.66\n22828.17\n0.6067 0.0607\n-20.2759\n[20]\u00a7\nRegret\n-11.6944\n3\n-11.6963 513,627 888,353\n14.78 37266.95\n37085.30\n0.1045\n0.0114\n-13.4481\n\u2217http://networkrepository.com\n\u2020 http://snap.stanford.edu\n\u2021 https://comunelab.fbk.eu/data.php\n\u00a7 https://goo.gl/8741Gs\nsolution in reasonable time. The preprocessing algorithm often reduces the size of the networks significantly using a reasonably large lower bound computed by LP (13) and results in a substantial speed-up. As an extreme example, for Cannes2013 with the density and robust ratio metrics, the lower bound attains the optimal value, and the number of vertices is reduced by more than 99.9%, which makes the computation more than 300 times faster. Consistent with our theoretical analysis, |supp| is at most \ud835\udc58. For AS-733, \ud835\udc58is quite large but |supp| is still small. Finally we evaluate the single vertex subsets obtained from optimal solutions. For \ud835\udc5d\u2208\u0394(2\ud835\udc49), we select a vertex subset from supp(\ud835\udc5d) that optimizes the metric employed in the algorithm. Note that in this setting, the output of DCS-LP coincides with that obtained by our algorithm with the density metric. As DCS-Greedy performed quite poorly, it is omitted. The results are shown in the last three columns of Table 1. Although there are a few exceptions, the algorithm with a metric performs best in terms of the metric employed. A critical fact is that depending only on the density metric, we may fail to obtain meaningful structure from networks. For example, the algorithm with the robust ratio admits a particularly large robust ratio value of 0.6652 for NYClimateMarch2014, meaning that the vertex subset obtained achieves an approximation ratio of 0.6652 for all layers. Moreover, the algorithm with the regret metric admits a particularly small regret value of 155.5000 for Cannes2013. For those instances, the algorithm with the density metric (i.e., DCS-LP) performs poorly in terms of those metrics,\nrespectively. From the above, it seems quite important to select an appropriate metric depending on the practical purpose at hand.\n# 8 CONCLUSION\nIn this paper, we have introduced a novel optimization model and algorithms for dense subgraph discovery in multilayer networks. There are several possible directions for future research. One direction is to improve the scalability of our algorithm, particularly for the regret metric, for which our preprocessing algorithm does not necessarily perform well. Another direction is to apply our model and algorithms to some real-world applications of multilayernetwork analysis. Investigating multilayer-network counterparts of some existing generalizations of the densest subgraph problem (see e.g., [29, 34, 40, 43]) is also interesting future work.\n# ACKNOWLEDGMENTS\nThis work was partially supported by JST PRESTO Grant Number JPMJPR2122 and JSPS KAKENHI Grant Numbers JP17K12646, JP19K20218, JP20K19739, JP21K17708, and JP21H03397.\n# REFERENCES\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of dense subgraph discovery in multilayer networks, highlighting the limitations of previous deterministic methods that focus on single vertex subsets and the need for a stochastic approach to enhance robustness and adaptability in various applications.",
        "problem": {
            "definition": "The problem is to find a vertex subset in a multilayer network that is dense for the layer selected by an adversary, which is challenging due to the stochastic nature of the connections across multiple layers.",
            "key obstacle": "The main difficulty lies in the adversarial selection of layers, which can lead to suboptimal solutions when using deterministic methods that only consider a single vertex subset."
        },
        "idea": {
            "intuition": "The intuition behind the proposed method is to utilize a probability distribution over vertex subsets, allowing for a more flexible and robust solution that can adapt to the worst-case scenarios presented by the adversary.",
            "opinion": "The proposed idea involves generating a stochastic solution that provides not just a single vertex subset but a distribution, which can be utilized to derive various subsets based on different selection criteria.",
            "innovation": "The key innovation is the introduction of a polynomial-time exact algorithm that computes optimal stochastic solutions, contrasting with existing methods that focus solely on deterministic solutions."
        },
        "method": {
            "method name": "Stochastic Dense Subgraph Discovery",
            "method abbreviation": "SDSD",
            "method definition": "The method aims to compute a probability distribution over vertex subsets that maximizes the expected density against the worst-case layer chosen by an adversary.",
            "method description": "It employs linear programming to derive an optimal probability distribution that can be used to select vertex subsets efficiently.",
            "method steps": [
                "Define the multilayer network and its edge weights.",
                "Formulate the optimization problem as a linear program.",
                "Solve the linear program to obtain optimal solutions.",
                "Generate the probability distribution from the LP solution.",
                "Select vertex subsets based on the probability distribution."
            ],
            "principle": "The effectiveness of this method is grounded in the ability to evaluate multiple layers simultaneously, allowing for a more robust solution that mitigates the impact of adversarial layer selection."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using synthetic graphs and real-world multilayer networks, with a focus on various metrics such as density, robust ratio, and regret to assess the performance of the proposed algorithms.",
            "evaluation method": "The performance was measured by comparing the output vertex subsets against known optimal solutions and baseline methods using metrics that evaluate the quality of the detected dense subgraphs."
        },
        "conclusion": "The proposed model and algorithms for dense subgraph discovery in multilayer networks demonstrated significant improvements in robustness and adaptability, with promising results from computational experiments validating their effectiveness.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to provide a stochastic solution that is more robust against adversarial conditions, allowing for better performance in practical applications.",
            "limitation": "A limitation of the method is that the preprocessing algorithm may not always enhance performance for all metrics, particularly the regret metric, which can affect scalability.",
            "future work": "Future research directions include enhancing the scalability of the algorithms, applying the methods to real-world multilayer network analyses, and exploring generalizations of the densest subgraph problem."
        },
        "other info": {
            "acknowledgments": "This work was partially supported by JST PRESTO and JSPS KAKENHI Grants.",
            "keywords": [
                "network analysis",
                "multilayer networks",
                "dense subgraph discovery",
                "stochastic solutions"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem is to find a vertex subset in a multilayer network that is dense for the layer selected by an adversary, which is challenging due to the stochastic nature of the connections across multiple layers."
        },
        {
            "section number": "2.3",
            "key information": "The key innovation is the introduction of a polynomial-time exact algorithm that computes optimal stochastic solutions, contrasting with existing methods that focus solely on deterministic solutions."
        },
        {
            "section number": "3.3",
            "key information": "The effectiveness of the Stochastic Dense Subgraph Discovery method is grounded in the ability to evaluate multiple layers simultaneously, allowing for a more robust solution that mitigates the impact of adversarial layer selection."
        },
        {
            "section number": "5.1",
            "key information": "The proposed idea involves generating a stochastic solution that provides not just a single vertex subset but a distribution, which can be utilized to derive various subsets based on different selection criteria."
        },
        {
            "section number": "7.2",
            "key information": "The main advantage of the proposed approach is its ability to provide a stochastic solution that is more robust against adversarial conditions, allowing for better performance in practical applications."
        },
        {
            "section number": "8",
            "key information": "The proposed model and algorithms for dense subgraph discovery in multilayer networks demonstrated significant improvements in robustness and adaptability, with promising results from computational experiments validating their effectiveness."
        }
    ],
    "similarity_score": 0.5214456680916035,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/Stochastic Solutions for Dense Subgraph Discovery in Multilayer Networks.json"
}