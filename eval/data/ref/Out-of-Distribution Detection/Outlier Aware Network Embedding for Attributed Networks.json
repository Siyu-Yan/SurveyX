{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1811.07609",
    "title": "Outlier Aware Network Embedding for Attributed Networks",
    "abstract": "Attributed network embedding has received much interest from the research community as most of the networks come with some content in each node, which is also known as node attributes. Existing attributed network approaches work well when the network is consistent in structure and attributes, and nodes behave as expected. But real world networks often have anomalous nodes. Typically these outliers, being relatively unexplainable, affect the embeddings of other nodes in the network. Thus all the downstream network mining tasks fail miserably in the presence of such outliers. Hence an integrated approach to detect anomalies and reduce their overall effect on the network embedding is required. Towards this end, we propose an unsupervised outlier aware network embedding algorithm (ONE) for attributed networks, which minimizes the effect of the outlier nodes, and hence generates robust network embeddings. We align and jointly optimize the loss functions coming from structure and attributes of the network. To the best of our knowledge, this is the first generic network embedding approach which incorporates the effect of outliers for an attributed network without any supervision. We experimented on publicly available real networks and manually planted different types of outliers to check the performance of the proposed algorithm. Results demonstrate the superiority of our approach to detect the network outliers compared to the state-of-the-art approaches. We also consider different downstream machine learning applications on networks to show the efficiency of ONE as a generic network embedding technique. The source code is made available at https://github.com/ sambaranban/ONE.",
    "bib_name": "bandyopadhyay2018outlierawarenetworkembedding",
    "md_text": "# Outlier Aware Network Embedding for Attributed Networks\nambaran Bandyopadhyay \u2217 IBM Research, India sambaran.ban89@gmail.com Lokesh N Indian Institute of Science, Bangalore nlokeshcool@gmail.com M. N. Murty Indian Institute of Science, Bangalore mnm@iisc.ac.in\n# Sambaran Bandyopadhyay \u2217 IBM Research, India\nmbaran Bandyopadhya IBM Research, India sambaran.ban89@gmail.com\n19 Nov 2018\n# Abstract\nAttributed network embedding has received much interest from the research community as most of the networks come with some content in each node, which is also known as node attributes. Existing attributed network approaches work well when the network is consistent in structure and attributes, and nodes behave as expected. But real world networks often have anomalous nodes. Typically these outliers, being relatively unexplainable, affect the embeddings of other nodes in the network. Thus all the downstream network mining tasks fail miserably in the presence of such outliers. Hence an integrated approach to detect anomalies and reduce their overall effect on the network embedding is required. Towards this end, we propose an unsupervised outlier aware network embedding algorithm (ONE) for attributed networks, which minimizes the effect of the outlier nodes, and hence generates robust network embeddings. We align and jointly optimize the loss functions coming from structure and attributes of the network. To the best of our knowledge, this is the first generic network embedding approach which incorporates the effect of outliers for an attributed network without any supervision. We experimented on publicly available real networks and manually planted different types of outliers to check the performance of the proposed algorithm. Results demonstrate the superiority of our approach to detect the network outliers compared to the state-of-the-art approaches. We also consider different downstream machine learning applications on networks to show the efficiency of ONE as a generic network embedding technique. The source code is made available at https://github.com/ sambaranban/ONE.\n19 Nov 2\n[cs.SI]\narXiv:1811.07609v1\n# 1 Introduction\nNetwork embedding (a.k.a. network representation learning) has gained a tremendous amount of interest among the researchers in the last few years (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016). Most of the real life networks have some extra information within each node. For example, users in social networks such as Facebook have texts, images and other types of content. Research papers (nodes) in a citation network have scientific content in it. Typically this type of extra information is captured\n\u2217Also affiliated with Indian Institute of Science, Bangalore Copyright c\u20dd2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nM. N. Murty\nusing attribute vectors associated with each node. The attributes and the link structure of the network are highly correlated according to the sociological theories like homophily (McPherson, Smith-Lovin, and Cook 2001). But embedding attributed networks is challenging as combining attributes to generate node embeddings is not easy. Towards this end, different attributed network representation techniques such as (Yang et al. 2015; Huang, Li, and Hu 2017a; Gao and Huang 2018) have been proposed in literature. They perform reasonably well when the network is consistent in its structure and content, and nodes behave as expected. Unfortunately real world networks are noisy and there are different outliers which even affect the embeddings of normal nodes (Liu, Huang, and Hu 2017). For example, there can be research papers in a citation network with few spurious references (i.e., edges) which do not comply with the content of the papers. There are celebrities in social networks who are connected to too many other users, and generally properties like homophily are not applicable to this type of relationships. So they can also act like potential outliers in the system. Normal nodes are consistent in their respective communities both in terms of link structure and attributes. We categorize outliers in an attributed network into three categories and explain them as shown in Figure 1. One way to detect outliers in the network is to use some network embedding approach and then use algorithms like isolation forest (Liu, Ting, and Zhou 2008) on the generated embeddings. But this type of decoupled approach is not optimal as outliers adversely affect the embeddings of the normal nodes. So an integrated approach to detect outliers and minimize their effect while generating the network embedding is needed. Recently (Liang et al. 2018) proposes a semi supervised approach for detecting outliers while generating network embedding for an attributed network. But in principle, it needs some supervision to work efficiently. For real world networks, it is difficult to get such supervision or node labels. So there is a need to develop a completely unsupervised integrated approach for graph embedding and outlier detection which can be applicable to any attributed network. Contributions: Following are the contributions we make. \u2022 We propose an unsupervised algorithm called ONE (Outlier aware Network Embedding) for attributed networks. It is an iterative approach to find lower dimensional compact vector representations of the nodes, such\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0289/02896077-ab49-462a-bbfa-f2f006262a00.png\" style=\"width: 50%;\"></div>\nFigure 1: This shows different types of outliers that we consider in an attributed network. We highlight the outlier node and its associated attribute by larger circle and rectangle respectively. Different colors represent different communities. Arrows between the two nodes represent network edges and arrows between two attributes represent similarity (in some metric) between them. (a) Structural Outlier: The node has edges to nodes from different communities, i.e., its structural neighborhood is inconsistent. (b) Attribute Outlier: The attributes of the node is similar to attributes of the nodes from different communities, i.e., its attribute neighborhood is inconsistent. (c) Combined Outlier: Node belongs to a community structurally but it has a different community in terms of attribute similarity.\n# that the outliers contribute less to the overall cost function.\n\u2022 This is the first work to propose a completely unsupervised algorithm for attributed network embedding integrated with outlier detection. Also we propose a novel method to combine structure and attributes efficiently.\n We conduct a thorough experimentation on the outlier seeded versions of popularly used and publicly available network datasets to show the efficiency of our approach to detect outliers. At the same time by comparing with the state-of-the-art network embedding algorithms, we demonstrate the power of ONE as a generic embedding method which can work with different downstream machine learning tasks such as node clustering and node classification.\n# 2 Related Work\nThis section briefs the existing literature on attributed network embedding, and some outlier detection techniques in the context of networks. Network embedding has been a hot research topic in the last few years and a detailed survey can be found in (Hamilton, Ying, and Leskovec 2017b). Word embedding in natural language processing literature, such as (Mikolov et al. 2013) inspired the development of node embedding in network analysis. DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), node2vec (Grover and Leskovec 2016) and Line (Tang et al. 2015) gained popularity for network representation just by using the link structure of the network. DeepWalk and node2vec use random walk on the network to generate node sequences and feed them to language models to get the embedding of the nodes. In Line, two different objective functions have been used to capture the first and second order proximities respectively and an edge sampling strategy is proposed to solve the joint optimization for node embedding. In (Ribeiro, Saverese, and Figueiredo 2017), authors propose struc2vec where nodes\nhaving similar substructure are close in their embeddings. All the papers citeed above only consider link structure of the network for generating embeddings. Research has been conducted on attributed network representation also. TADW (Yang et al. 2015) is arguably the first attempt to successfully use text associated with nodes in the network embedding via joint matrix factorization. But their framework directly learns one embedding from content and structure together. In case when there is noise or outliers in structure or content, such a direct approach is prone to be affected more. Another attributed network embedding technique (AANE) is proposed in (Huang, Li, and Hu 2017a). The authors have used symmetric matrix factorization to get embeddings from the similarity matrix over the attributes, and use link structure of the network to ensure that the embeddings of the two connected nodes are similar. A semisupervised attributed embedding is proposed in (Huang, Li, and Hu 2017b) where the label information of some nodes are used along with structure and attributes. The idea of using convolutional neural networks for graph embedding has been proposed in (Niepert, Ahmed, and Kutzkov 2016; Kipf and Welling 2016). An extension of GCN with node attributes (GraphSage) has been proposed in (Hamilton, Ying, and Leskovec 2017a) with an inductive learning setup. These methods do not manage outliers directly, and hence are often prone to be affected heavily by them. Recently a semi-supervised deep learning based approach SEANO (Liang et al. 2018) has been proposed for outlier detection and network embedding for attributed networks. For each node, they collect its attribute and the attributes from the neighbors, and smooth out the outliers by predicting the class labels (on the supervised set) and node context. But getting labeled nodes for real world network is expensive. So we aim to design an unsupervised attributed network embedding algorithm which can detect and minimize the effect of outliers while generating the node embeddings.\n# 3 Problem Formulation\nAn information network is typically represented by a graph as G = (V, E, C), where V = {v1, v2, \u00b7 \u00b7 \u00b7 , vN} is the set of nodes (a.k.a. vertexes), each representing a data object. E \u2282{(vi, vj)|vi, vj \u2208V } is the set of edges between the vertexes. Each edge e \u2208E is an ordered pair e = (vi, vj) and is associated with a weight wvi,vj > 0, which indicates the strength of the relation. If G is undirected, we have (vi, vj) \u2261(vj, vi) and wvi,vj \u2261wvj,vi; if G is unweighted, wvi,vj = 1, \u2200(vi, vj) \u2208E. Let us denote the N \u00d7N dimensional adjacency matrix of the graph G by A = (ai,j), where ai,j = wvi,vj if (vi, vj) \u2208 E, and ai,j = 0 otherwise. So ith row of A contains the immediate neighborhood information for node i. Clearly for a large network, the matrix A is highly sparse in nature. C is a N \u00d7 D matrix with Ci\u00b7 as rows, where Ci\u00b7 \u2208RD is the attribute vector associated with the node vi \u2208V . Cid is the value of the attribute d for the node vi. For example, if there is only textual content in each node, ci can be the tf-idf vector for the content of the node vi. Our goal is to find a low dimensional representation of G which is consistent with both the structure of the network\nand the content of the nodes. More formally, for a given network G, network embedding is a technique to learn a function f : vi \ufffd\u2192yi \u2208RK, i.e., it maps every vertex to a K dimensional vector, where K < min(N, D). The representations should preserve the underlying semantics of the network. Hence the nodes which are close to each other in terms of their topographical distance or similarity in attributes should have similar representations. We also need to reduce the effect of outliers, so that the representations for the other nodes in the network are robust.\n4 Solution Approach: ONE We describe the whole algorithm in different parts.\n# 4 Solution Approach: ONE We describe the whole algorithm in different parts.\n# 4.1 Learning from the Link Structure\nGiven graph G, each node vi by default can be represented by the ith row Ai\u00b7 of the adjacency matrix. Let us assume the matrix G \u2208RN\u00d7K be the network embedding of G, only by considering the link structure. Hence row vector Gi\u00b7 is the K dimensional (K < min(N, D)) compact vector representation of node vi, \u2200vi \u2208V . Also let us introduce a K \u00d7 N matrix H to minimize the reconstruction loss: N \ufffd i=1 N \ufffd j=1 (Aij \u2212Gi\u00b7 \u00b7 H\u00b7j)2, where H\u00b7j is the jth column of H, and Gi\u00b7 \u00b7 H\u00b7j is the dot product between these two vectors1. kth row of H can be interpreted as the N dimensional description of kth feature, where k = 1, 2, \u00b7 \u00b7 \u00b7 , K. This reconstruction loss tends to preserve the original distances in the lower dimensional spaces as shown by (Cunningham and Ghahramani 2015). But if the graph has anomalous nodes, they generally affect the embedding of the other (normal) nodes. To minimize the effect of such outliers while learning embeddings from the structure, we introduce the structural outlier score O1i for node vi \u2208V , where 0 < O1i \u22641. The bigger the value of O1i, the more likely it is that node vi is an outlier, and lesser should be its contribution to the total loss. Hence we seek to minimize the following cost function w.r.t. the variables O1 (set of all structural outlier scores), G and H.\nWe also assume N \ufffd i=1 O1i = \u00b5, \u00b5 being the total outlier score of the network. Otherwise minimizing Eq. 1 amounts to assigning 1 to all the outlier scores, which makes the loss value 0. It can be readily seen that, when O1i is very high (close to 1) for a node vi, the contribution of this node N \ufffd j=1 log \ufffd 1 O1i \ufffd (Aij \u2212Gi. \u00b7 H.j)2 becomes negligible, and when O1i is small (close to 0), the corresponding contribution is high. So naturally, the optimization would concentrate more on minimizing the contributions of the outlier\n1We treat both row vector and column vector as vectors of same dimension, and hence use the dot product instead of transpose to avoid cluttering of notation\n1We treat both row vector and column vector as vectors of same dimension, and hence use the dot product instead of transpose to avoid cluttering of notation\n(w.r.t. the link structure) nodes, to the overall objective, as desired.\n# 4.2 Learning from the Attributes\nSimilar to the case of structure, here we try to learn a K dimensional vectorial representation Ui\u00b7 from the given attribute matrix C, where Ci\u00b7 is the attribute vector of node vi. Let us consider the matrices U \u2208RN\u00d7K and V \u2208RK\u00d7D, U being the network embedding just respecting the set of attributes. In the absence of outliers, one can just minimize the reconstruction loss N \ufffd i=1 D \ufffd d=1 (Cid \u2212Ui\u00b7 \u00b7 V\u00b7d)2 with respect to the matrices U and V . But as mentioned before, outliers even affect the embeddings of the normal nodes. Hence to reduce the effect of outliers while learning from the attributes, we introduce the attribute outlier score O2i for node vi \u2208V , where 0 < O2i \u22641. Larger the value of O2i, higher the chance that node vi is an attribute outlier. Hence we minimize the following cost function w.r.t. the variables O2, U and V .\n(2)\nWe again assign the constraint that N \ufffd i=1 O2i = \u00b5 for the reason mentioned before. Hence contributions from the nonoutlier (w.r.t. attributes) nodes would be bigger while minimizing Eq. 2.\n# 4.3 Connecting Structure and Attributes\nSo far, we have considered the link structure and the attribute values of the network separately. Also the optimization variables of Eq. 1 and that in Eq. 2 are completely disjoint. But optimizing them independently is not desirable as, our ultimate goal is to get a joint low dimensional representation of each node in the network. Also we intend to regularize structure with respect to attributes and vice versa. As discussed before, link structure and attributes in a network are highly correlated and they can be often noisy individually. One can see that, Gi\u00b7 and Ui\u00b7 are the representation of the same node vi with respect to structure and attributes respectively. So one can easily act as a regularizer of the other. Also as they contribute to the embedding of the same node, it makes sense to minimize N \ufffd i=1 K \ufffd k=1 (Gik \u2212Uik)2. But it is important to note that, there is no explicit guarantee that the features in Gi\u00b7 and features in Ui\u00b7 are aligned, i.e., kth feature of the structure embeddings can be very different from the kth feature of attribute embeddings. Hence before minimizing the distance of Gi\u00b7 and Ui\u00b7, it is important to align the features of the two embedding spaces. Embedding Transformation and Procrustes problem To resolve the issue above, we seek to find a linear map W \u2208 RK\u00d7K which transforms the features from the attribute embedding space to structure embedding space. More formally we want to find a matrix W which minimizes ||G\u2212WU|| .\nThis type of transformation has been used in the NLP literature, particularly for machine translation (Lample et al. 2018). If we further restrict W to be an orthogonal matrix, then a closed form solution can be obtained from the solution concept of Procrustes problem (Sch\u00a8onemann 1966) as follows:\n(3)\n \u2208O where W \u2217= XY T with X\u03a3Y T = SVD(GT U), OK is the set of all orthogonal matrices of dimension K \u00d7 K. Restricting W to be an orthogonal matrix has also several other advantages as shown in the NLP literature (Xing et al. 2015). But we cannot directly use the solution of Procrustes problem, as we have anomalies in the network. As before, we again reduce the effect of anomalies to minimize the disagreement between the structural embeddings and attribute embeddings. Let us introduce the disagreement anomaly score O3i for a node vi \u2208V , where 0 < O3i \u22641. Disagreement anomalies are required to manage the anomalous nodes which are not anomalies in either of structure or attributes individually, but they are inconsistent when considering them together. Following is the cost function we minimize.\n(4)\nN \ufffd i=1 O3i = \u00b5. We will use the solution of Procrustes problem after applying a simple trick to the cost function above, as shown in the derivation of the update rule of W later.\n# 4.4 Joint Loss Function\nHere we combine the three cost functions mentioned before, and minimize the following with respect to G, H, U, V , W and O (O contains all the variables from O1, O2 and O3).\n(5)\nL L The full set of constrains are\n \u2208O \u21d0\u21d2 I Here \u03b1, \u03b2 > 0 are weight factors. We will discuss a method to set them in the experimental evaluation. It is to be noted that, the three anomaly scores O1i, O2i and O3i for any node vi are actually connected by the cost function 5. For example, if a node is anomalous in structure, O1i would be high and its embedding Gi\u00b7 may not be optimized well. So this in turn affects its matching with transformed Ui\u00b7, and hence O3i would be given a higher value to minimize the disagreement loss.\n# 4.5 Derivations of the Update Rules\nWe will derive the necessary update rules which can be used iteratively to minimize Eq. 5. We use the alternating minimization technique, where we derive the update rule for one variable at a time, keeping all others fixed.\nWe need to take the partial derivative of L (Eq. 5) w.r.t one variable at a time and equate that to zero. For example, \u2202L \u2202Gik = 0 \u21d2 N \ufffd j=1 log \ufffd1 O1i \ufffd (Aij \u2212Gi\u00b7 \u00b7 H\u00b7j)(\u2212Hkj) + log \ufffd1 O3i \ufffd (Gik \u2212Ui\u00b7 \u00b7 (W T )\u00b7k) = 0. Solving it for Gik,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1dc/e1dcbe20-c188-4761-a420-3c504660f3d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">( Similarly we can get the following update rules.</div>\n(8)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2709/2709ff5f-18b2-46a3-b1c0-82257c8ab1ea.png\" style=\"width: 50%;\"></div>\n4.7 Updating W We use a small trick to directly apply the closed form solution of Procrustes problem as follows.\n# 4.7 Updating W\nWe use a small trick to directly apply the closed form solution of Procrustes problem as follows.\n(10)\nHere the new matrices are defined as, ( \u00afG)i,k = \ufffd log \ufffd 1 O3i \ufffd Gik and \u00afUik = \ufffd log \ufffd 1 O3i \ufffd Uik. Say, X\u03a3Y T = SVD( \u00afGT \u00afU), then W can be obtained as:\n(11)\n# 4.8 Updating O\n\u03bb \u2208R is the Lagrangian constant. Equating the partial derivative w.r.t. O1i to 0:\n(12)\n\ufffd \ufffd It is to be noted that, if we set \u00b5 = 1, the constraints 0 < Oi1 \u22641, \u2200vi \u2208V , are automatically satisfied. Even it is possible to increase the value of \u00b5 by a trick similar to (Gupta et al. 2012), but experimentally we have not seen any advantage in increasing the value of \u00b5. Hence, we set \u00b5 = 1 for all the reported experiments. A similar procedure can be followed to derive the update rules for O2 and O3.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d02/7d02091a-cf11-4301-8af0-db4f6865e607.png\" style=\"width: 50%;\"></div>\n(13) (14)\n(13)\n(14)\n# \ufffd \ufffd 4.9 Algorithm: ONE\nWith the update rules derived above, we summarize ONE in Algorithm 1. variables G, H, U and V can be initialized by any standard matrix factorization technique (A \u2248GH and C \u2248UV ) such as (Lee and Seung 2001). As our algorithm is completely unsupervised, we assume not to have any prior information about the outliers. So initially we set equal outlier scores to all the nodes in the network and normalize them accordingly. At the end of the algorithm, one can take the final embedding of a node as the average of the structural and the transformed attribute embeddings. Similarly the final outlier score of a node can be obtained as the weighted average of three outlier scores. Lemma 1 The joint cost function in Eq. 5 decreases after each iteration (steps 4 to 6) of the for loop of Algorithm 1. Proof 1 It is easy to check that the joint loss function L is convex in each of the variables G, H, U, V and O, when all other variables are fixed. Also from the Procrustes solution, update of W also minimizes the cost function. So alternating minimization guarantees decrease of cost function after every update till convergence.\nThe computational complexity of each iteration (Steps 4 to 6 in Algo. 1) takes O(N 2) time (assuming K is a constant) without using any parallel computation, as updating each variable Gik, Hkj, Vkd, O1i, O2i, O3i and W takes O(N) time. But we observe that ONE converges very fast on any of the datasets, as updating one variables amounts to reaching the global minima of the corresponding loss function when all other variables are fixed. The run time can be improved significantly by parallelizing the computation as done in (Huang, Li, and Hu 2017a).\nAlgorithm 1 ONE\nInput: The network G = (V, E, C), K: Dimension of\nthe embedding space where K < min(n, d), ratio pa-\nrameter \u03b8\nOutput: The node embeddings of the network G, Out-\nlier score of each node vinV\n1: Initialize G and H by standard matrix factorization on\nA, and U and V by that on C.\n2: Initialize the outlier scores O1, O2 and O3 uniformly.\n3: for until stopping condition satisfied do\n4:\nUpdate W by Eq. 11.\n5:\nUpdate G, H, U and V by Eq. from 6 to 9.\n6:\nUpdate outlier scores by Eq. from 12 to 14.\n7: end for\n8: Embedding for the node vi is Gi\u00b7+Ui\u00b7(W T )\n2\n, \u2200vi \u2208V .\n9: Final outlier score for the node vi is a weighted average\nof O1i, O2i and O3i, \u2200vi \u2208V .\n# 5 Experimental Evaluation\nIn this section, we evaluate the performance of the proposed algorithm on multiple attributed network datasets and compare the results with several state-of-the-art algorithms.\n# 5.1 Datasets Used and Seeding Outliers\nTo the best of our knowledge, there is no publicly available attributed networks with ground truth outliers available. So we take four publicly available attributed networks with ground truth community membership information available for each node. The datasets are WebKB, Cora, Citeseer and Pubmed2. To check the performance of the algorithms in the presence of outliers, we manually planted a total of 5% outliers (with equal numbers for each type as shown in Figure 1) in each dataset. The seeding process involves: (1) computing the probability distribution of number of nodes in each class, (2) selecting a class using these probabilities. For a structural outlier: (3) plant an outlier node in the selected class such that the node has (m \u00b1 10%) of edges connecting nodes from the remaining (unselected) classes where m is the average degree of a node in the selected class and (4) the content of the structural outlier node is made semantically consistent with the keywords sampled from the nodes of the selected class. A similar approach is employed for seeding the other two types of outliers. The statistics of these seeded datasets are given in Table 1. Outlier nodes apparently have\n2Datasets: https://linqs.soe.ucsc.edu/data\n<div style=\"text-align: center;\">Table 1: Summary of the datasets (after planting outliers).</div>\nTable 1: Summary of the datasets (after planting outliers).\nDataset\n#Nodes\n#Edges\n#Labels\n#Attributes\nWebKB\n919\n1662\n5\n1703\nCora\n2843\n6269\n7\n1433\nCiteseer\n3477\n5319\n6\n3703\nPubmed\n20701\n49523\n3\n500\nsimilar characteristics in terms of degree, number of nonzero attributes, etc., and thus we ensured that they cannot be detected trivially.\n# 5.2 Baseline Algorithms and Experimental Setup\nWe use DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), node2vec (Grover and Leskovec 2016), Line (Tang et al. 2015), TADW (Yang et al. 2015), AANE (Huang, Li, and Hu 2017a), GraphSage (Hamilton, Ying, and Leskovec 2017a) and SEANO (Liang et al. 2018) as the baseline algorithms to be compared with. The first three algorithms consider only structure of the network, the last four consider both structure and node attributes. We mostly use the default settings of the parameters values in the publicly available implementations of the respective baseline algorithms. As SEANO is semisupervised, we include 20% of the data with their true labels in the training set of SEANO to produce the embeddings. For ONE, we set the values of \u03b1 and \u03b2 in such a way that three components in the joint losss function in Eq. 5 contribute equally before the first iteration of the for loop in Algorithm 1. For all the experiments we keep embedding space dimension to be three times the number of ground truth communities. For each of the datasets, we run the for loop (Steps 4 to 6 in Alg. 1) of ONE only 5 times. We observe that ONE converges very fast on all the datasets. Convergence rate has been shown experimentally in Fig. 2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bbbe/bbbe50db-2b28-4656-853a-38e51d854950.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Values of Loss function over different iterations of ONE for Citeseer and Pubmed (seeded) datasets</div>\n# 5.3 Outlier Detection\nA very important goal of our work is to detect outliers while generating the network embedding. In this section, we see the performance of all the algorithms in detecting outliers that we planted in each dataset. SEANO and ONE give outlier scores directly as the output. For ONE, we rank the nodes in order of the higher values of a weighted average of three outlier scores (more the value of this average outlier score, more likely the vertex is an outlier). We have observed\nexperimentally that O2 is more important to determine outliers. For SEANO, we rank the nodes in order of the lower values of the weight parameter \u03bb (lower the value of \u03bb more likely the vertex is an outlier). For other embedding algorithms, as they do not explicitly output any outlier score, we use isolation forest to detect outliers from the node embeddings generated by them. We use recall to check the performance of each embedding algorithm to find outliers. As the total number of outliers in each dataset is 5%, we start with the top 5% of the nodes in the ranked list (L) of outliers, and continue till 25% of the nodes, and calculate the recall for each set with respect to the artificially planted outliers. Figure 3 shows that ONE, though completely unsupervised in nature, is able to outperform SEANO mostly on all the datasets. SEANO considers the role of predicting the class label or context of a node based on only its attributes, or the set of attributes from its neighbors, and accordingly fix the outlierness of the node. Whereas, ONE explicitly compares structure, attribute and their combination to detect outliers and then reduces their effect iteratively in the optimization process. So discriminating outliers from the normal nodes becomes somewhat easier for ONE. As expected, all the other embedding algorithms (by running isolation forest on the embedding generated by them) perform poorly on all the datasets to detect outliers, except on Cora where AANE performs good.\n# 5.4 Node Classification\nNode classification is an important application when labeling information is available only for a small subset of nodes in the network. This information can be used to enhance the accuracy of the label prediction task on the remaining/unlabeled nodes. For this task, firstly we get the embedding representations of the nodes and take them as the features to train a random forest classifier (Liaw, Wiener, and others 2002). We split the set of nodes of the graph into training set and testing set. The training set size is varied from 10% to 50% of the entire data. The remaining (test) data is used to compare the performance of different algorithms. We use two popular evaluation criteria based on F1score, i.e., Macro-F1 and Micro-F1 to measure the performance of the multi-class classification algorithms. Micro-F1 is a weighted average of F1-score over all different class labels. Macro-F1 is an arithmetic average of F1-scores of all output class labels. Normally, the higher these values are, the better the classification performance is. We repeat each experiment 10 times and report the average results. On all the datasets (Fig. 4), ONE consistently performs the best for classification, both in terms of macro and micro F1 scores. We see that conventional embedding algorithms like node2vec and TADW, which are generally good for consistent datasets, perform miserably in the presence of just 5% outliers. AANE is the second best algorithm for classification in the presence of outliers, and it is very close to ONE in terms of F1 scores on the Citeseer dataset. ONE is also able to outperform SEANO with a good margin, even though SEANO is a semi-supervised approach and uses labeled data for generating node embeddings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1bdd/1bdd7b5b-4824-4244-955b-0be12d99b93e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 3: Outlier Recall at top L% from the ranked list of outliers for all the datasets. ONE, though it is an unsupervise orithm, outperforms all the baseline algorithms in most of the cases. SEANO uses 20% labeled data for training.</div>\n<div style=\"text-align: center;\">l at top L% from the ranked list of outliers for all the datasets. ONE, though it is an unsupervised all the baseline algorithms in most of the cases. SEANO uses 20% labeled data for training.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33bb/33bbc6f7-a52a-453d-ab3f-259917f94f1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance of different embedding algorithms for Classification with Random Forest</div>\n# 5.5 Node Clustering\nNode Clustering is an unsupervised method of grouping the nodes into multiple communities or clusters. First we run all the embedding algorithms to generate the embeddings of the nodes. We use the node\u2019s embedding as the features for the node and then apply KMeans++ (Arthur and Vassilvitskii 2007). KMeans++ just divides the data into different groups. To find the test accuracy we need to assign the clusters with an appropriate label and compare with the ground truth community labels. For finding the test accuracy we use unsupervised clustering accuracy (Xie, Girshick, and Farhadi 2016) which uses different permutations of the labels and chooses the label ordering which gives best possible accuracy Acc( \u02c6C, C) = maxP N \ufffd i=1 1(P( \u02c6Ci)=Ci) N . Here C is the ground truth labeling of the dataset such that Ci gives the ground truth label of ith data point. Similarly \u02c6C is the clustering assignments discovered by some algorithm, and P is a permutation of the set of labels. We assume 1 to be a logical operator which returns 1 when the argument is true, otherwise returns 0. Clustering performance is shown and ex-\nplained in Fig. 5. It can be observed that except for ONE, all the conventional embedding algorithms fail in the presence of outliers. Our proposed unsupervised algorithm is able to outperform or remain close to SEANO, though SEANO is semi supervised in nature, on all the datasets.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1fd8/1fd8aacc-8dba-4faf-8c9e-228c9b4b4325.png\" style=\"width: 50%;\"></div>\nFigure 5: Clustering Accuracy of KMeans++ on the embeddings generated by different algorithms. ONE is always close to the best of the baseline algorithms. AANE works best for Citeseer. Though SEANO uses 20% labeled data as the extra supervision to generate the embeddings, its accuracy is always close (or less) to ONE which is completely unsupervised in nature.\n# 6 Discussion and Future Work\nWe propose ONE, an unsupervised attributed network embedding approach that jointly learns and minimizes the effect of outliers in the network. We derive the details of the algorithm to optimize the associated cost function of ONE. Through experiments on seeded real world datasets, we show the superiority of ONE for outlier detection and other downstream network mining applications. There are different ways to extend the proposed approach in the future. One interesting direction is to parallelize the algorithm and check its performance on real world large attributed networks. Most of the networks are very dynamic now-a-days. Even outliers also evolve over time. So bringing additional constraints in our framework to capture the dynamic temporal behavior of the outliers and other nodes of the network would also be interesting. As mentioned in Section 4.9, ONE converges very fast on real datasets. But\nupdating most of the variables in this framework takes O(N) time, which leads to O(N 2) runtime for ONE without any parallel processing. So, one can come up with some intelligent sampling or greedy method, perhaps based on random walks, to replace the expensive sums in various update rules.\n# References\nReferences [Arthur and Vassilvitskii 2007] Arthur, D., and Vassilvitskii, S. 2007. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 1027\u20131035. Society for Industrial and Applied Mathematics. [Cunningham and Ghahramani 2015] Cunningham, J. P., and Ghahramani, Z. 2015. Linear dimensionality reduction: Survey, insights, and generalizations. Journal of Machine Learning Research 16:2859\u20132900. [Gao and Huang 2018] Gao, H., and Huang, H. 2018. Deep attributed network embedding. In IJCAI, 3364\u20133370. [Grover and Leskovec 2016] Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 855\u2013864. ACM. [Gupta et al. 2012] Gupta, M.; Gao, J.; Sun, Y.; and Han, J. 2012. Integrating community matching and outlier detection for mining evolutionary community outliers. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, 859\u2013867. ACM. [Hamilton, Ying, and Leskovec 2017a] Hamilton, W.; Ying, Z.; and Leskovec, J. 2017a. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 1025\u20131035. [Hamilton, Ying, and Leskovec 2017b] Hamilton, W. L.; Ying, R.; and Leskovec, J. 2017b. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584. [Huang, Li, and Hu 2017a] Huang, X.; Li, J.; and Hu, X. 2017a. Accelerated attributed network embedding. In Proceedings of the 2017 SIAM International Conference on Data Mining, 633\u2013641. SIAM. [Huang, Li, and Hu 2017b] Huang, X.; Li, J.; and Hu, X. 2017b. Label informed attributed network embedding. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, 731\u2013739. ACM. [Kipf and Welling 2016] Kipf, T. N., and Welling, M. 2016. Semisupervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. [Lample et al. 2018] Lample, G.; Conneau, A.; Ranzato, M.; Denoyer, L.; and Jgou, H. 2018. Word translation without parallel data. In International Conference on Learning Representations. [Lee and Seung 2001] Lee, D. D., and Seung, H. S. 2001. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems, 556\u2013562. [Liang et al. 2018] Liang, J.; Jacobs, P.; Sun, J.; and Parthasarathy, S. 2018. Semi-supervised embedding in attributed networks with outliers. In Proceedings of the 2018 SIAM International Conference on Data Mining, 153\u2013161. SIAM. [Liaw, Wiener, and others 2002] Liaw, A.; Wiener, M.; et al. 2002. Classification and regression by randomforest. R news 2(3):18\u201322. [Liu, Huang, and Hu 2017] Liu, N.; Huang, X.; and Hu, X. 2017. Accelerated local anomaly detection via resolving attributed networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, 2337\u20132343. AAAI Press.\n[Liu, Ting, and Zhou 2008] Liu, F. T.; Ting, K. M.; and Zhou, Z.H. 2008. Isolation forest. In 2008 Eighth IEEE International Conference on Data Mining, 413\u2013422. IEEE. [McPherson, Smith-Lovin, and Cook 2001] McPherson, M.; Smith-Lovin, L.; and Cook, J. M. 2001. Birds of a feather: Homophily in social networks. Annual review of sociology 27(1):415\u2013444. [Mikolov et al. 2013] Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. [Niepert, Ahmed, and Kutzkov 2016] Niepert, M.; Ahmed, M.; and Kutzkov, K. 2016. Learning convolutional neural networks for graphs. In International conference on machine learning, 2014\u2013 2023. [Perozzi, Al-Rfou, and Skiena 2014] Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 701\u2013710. ACM. [Ribeiro, Saverese, and Figueiredo 2017] Ribeiro, L. F.; Saverese, P. H.; and Figueiredo, D. R. 2017. struc2vec: Learning node representations from structural identity. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 385\u2013394. ACM. [Sch\u00a8onemann 1966] Sch\u00a8onemann, P. H. 1966. A generalized solution of the orthogonal procrustes problem. Psychometrika 31(1):1\u2013 10. [Tang et al. 2015] Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, 1067\u20131077. International World Wide Web Conferences Steering Committee. [Xie, Girshick, and Farhadi 2016] Xie, J.; Girshick, R.; and Farhadi, A. 2016. Unsupervised deep embedding for clustering analysis. In International conference on machine learning, 478\u2013487. [Xing et al. 2015] Xing, C.; Wang, D.; Liu, C.; and Lin, Y. 2015. Normalized word embedding and orthogonal transform for bilingual word translation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1006\u20131011. [Yang et al. 2015] Yang, C.; Liu, Z.; Zhao, D.; Sun, M.; and Chang, E. Y. 2015. Network representation learning with rich text information. In IJCAI, 2111\u20132117.\n",
    "paper_type": "method",
    "attri": {
        "background": "Attributed network embedding has gained significant interest, but existing methods struggle with anomalous nodes, which adversely affect the embeddings and downstream tasks. A breakthrough is necessary to effectively detect and mitigate the impact of these outliers.",
        "problem": {
            "definition": "The issue at hand is the presence of outlier nodes in attributed networks that disrupt the consistency of node embeddings, leading to failures in various network mining tasks.",
            "key obstacle": "The main challenge is that existing network embedding methods do not account for the influence of outliers, resulting in embeddings that do not accurately represent the underlying network structure and attributes."
        },
        "idea": {
            "intuition": "The idea was inspired by the need for a method that can simultaneously detect outliers and generate robust network embeddings without supervision.",
            "opinion": "The proposed method, called ONE (Outlier aware Network Embedding), integrates outlier detection into the embedding process to minimize the effect of anomalous nodes.",
            "innovation": "This approach is unique as it is the first unsupervised method to incorporate outlier effects into attributed network embeddings, improving upon existing techniques that do not address outliers."
        },
        "method": {
            "method name": "Outlier aware Network Embedding",
            "method abbreviation": "ONE",
            "method definition": "ONE is an unsupervised algorithm that minimizes the influence of outlier nodes while learning low-dimensional representations of nodes in attributed networks.",
            "method description": "The method combines structural and attribute information to generate robust embeddings that are resilient to outliers.",
            "method steps": "Initialize embeddings, update outlier scores, and iteratively minimize the joint loss function that incorporates contributions from both structure and attributes.",
            "principle": "The effectiveness of ONE stems from its ability to reduce the contributions of outliers during the embedding process, ensuring that normal nodes are accurately represented."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on several attributed network datasets (WebKB, Cora, Citeseer, Pubmed) with manually planted outliers to assess the performance of ONE against state-of-the-art methods.",
            "evaluation method": "The performance was evaluated through recall metrics for outlier detection and F1 scores for node classification, demonstrating the superiority of ONE in handling outliers."
        },
        "conclusion": "The experimental results indicate that ONE outperforms existing methods in both outlier detection and various downstream tasks, confirming its effectiveness as a robust network embedding technique.",
        "discussion": {
            "advantage": "ONE effectively integrates outlier detection into the embedding process, leading to more accurate representations of normal nodes compared to traditional methods.",
            "limitation": "The computational complexity of ONE is O(N^2), which may be a drawback for very large networks without parallel processing.",
            "future work": "Future research could focus on parallelizing the algorithm and adapting it to dynamic networks where outliers evolve over time."
        },
        "other info": {
            "source_code": "https://github.com/sambaranban/ONE",
            "dataset_links": "https://linqs.soe.ucsc.edu/data"
        }
    },
    "mount_outline": [
        {
            "section number": "2.3",
            "key information": "The issue at hand is the presence of outlier nodes in attributed networks that disrupt the consistency of node embeddings, leading to failures in various network mining tasks."
        },
        {
            "section number": "2.1",
            "key information": "ONE (Outlier aware Network Embedding) is an unsupervised algorithm that minimizes the influence of outlier nodes while learning low-dimensional representations of nodes in attributed networks."
        },
        {
            "section number": "3.5",
            "key information": "This approach is unique as it is the first unsupervised method to incorporate outlier effects into attributed network embeddings, improving upon existing techniques that do not address outliers."
        },
        {
            "section number": "5.1",
            "key information": "ONE effectively integrates outlier detection into the embedding process, leading to more accurate representations of normal nodes compared to traditional methods."
        },
        {
            "section number": "7.1",
            "key information": "The main challenge is that existing network embedding methods do not account for the influence of outliers, resulting in embeddings that do not accurately represent the underlying network structure and attributes."
        },
        {
            "section number": "7.2",
            "key information": "Future research could focus on parallelizing the algorithm and adapting it to dynamic networks where outliers evolve over time."
        }
    ],
    "similarity_score": 0.6167987919334151,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Outlier Aware Network Embedding for Attributed Networks.json"
}