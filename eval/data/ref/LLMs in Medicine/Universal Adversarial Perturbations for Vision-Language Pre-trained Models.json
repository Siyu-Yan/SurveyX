{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.05524",
    "title": "Universal Adversarial Perturbations for Vision-Language Pre-trained Models",
    "abstract": " ABSTRACT\nABSTRACT\nVision-language pre-trained (VLP) models have been the foundation of numerous vision-language tasks. Given their prevalence, it becomes imperative to assess their adversarial robustness, especially when deploying them in security-crucial real-world applications. Traditionally, adversarial perturbations generated for this assessment target specific VLP models, datasets, and/or downstream tasks. This practice suffers from low transferability and additional computation costs when transitioning to new scenarios. In this work, we thoroughly investigate whether VLP models are commonly sensitive to imperceptible perturbations of a specific pattern for the image modality. To this end, we propose a novel black-box method to generate Universal Adversarial Perturbations (UAPs), which is so called the Effective and Transferable Universal Adversarial Attack (ETU), aiming to mislead a variety of existing VLP models in a range of downstream tasks. The ETU comprehensively takes into account the characteristics of UAPs and the intrinsic cross-modal interactions to generate effective UAPs. Under this regime, the ETU encourages both global and local utilities of UAPs. This benefits the overall utility while reducing interactions between UAP units, improving the transferability. To further enhance the effectiveness and transferability of UAPs, we also design a novel data augmentation method named ScMix. ScMix consists of self-mix and cross-mix data transformations, which can effectively increase the multi-modal data diversity while preserving the semantics of the original data. Through comprehensive experiments on various downstream tasks, VLP models, and datasets, we demonstrate that the proposed method is able to achieve effective and transferrable universal adversarial attacks.\narXiv:2405.05524v1\n# CCS CONCEPTS \u2022 Security and privacy; \u2022 Information systems;\nKEYWORDS\nVision-language Pre-training; Universal Adversarial Perturbations; Multi-modal Learning; Transferra",
    "bib_name": "zhang2024universaladversarialperturbationsvisionlanguage",
    "md_text": "# Universal Adversarial Perturbations for Vision-Language Pre-trained Models\nPeng-Fei Zhang, Zi Huang, Guangdong Bai The University of Queensland\nPeng-Fei Zhang, Zi Huang, Guangdong Bai The University of Queensland mima.zpf@gmail.com,huang@itee.uq.edu.au,g.bai@uq.edu.au\nThe University of Queensland mima.zpf@gmail.com,huang@itee.uq.edu.au,g.bai@uq.edu.au\n# ABSTRACT\nABSTRACT\nVision-language pre-trained (VLP) models have been the foundation of numerous vision-language tasks. Given their prevalence, it becomes imperative to assess their adversarial robustness, especially when deploying them in security-crucial real-world applications. Traditionally, adversarial perturbations generated for this assessment target specific VLP models, datasets, and/or downstream tasks. This practice suffers from low transferability and additional computation costs when transitioning to new scenarios. In this work, we thoroughly investigate whether VLP models are commonly sensitive to imperceptible perturbations of a specific pattern for the image modality. To this end, we propose a novel black-box method to generate Universal Adversarial Perturbations (UAPs), which is so called the Effective and Transferable Universal Adversarial Attack (ETU), aiming to mislead a variety of existing VLP models in a range of downstream tasks. The ETU comprehensively takes into account the characteristics of UAPs and the intrinsic cross-modal interactions to generate effective UAPs. Under this regime, the ETU encourages both global and local utilities of UAPs. This benefits the overall utility while reducing interactions between UAP units, improving the transferability. To further enhance the effectiveness and transferability of UAPs, we also design a novel data augmentation method named ScMix. ScMix consists of self-mix and cross-mix data transformations, which can effectively increase the multi-modal data diversity while preserving the semantics of the original data. Through comprehensive experiments on various downstream tasks, VLP models, and datasets, we demonstrate that the proposed method is able to achieve effective and transferrable universal adversarial attacks.\narXiv:2405.05524v1\n# CCS CONCEPTS \u2022 Security and privacy; \u2022 Information systems;\nKEYWORDS\nVision-language Pre-training; Universal Adversarial Perturbations; Multi-modal Learning; Transferrable Attack\n# Vision-language Pre-training; Universal Adversarial Perturbations; Multi-modal Learning; Transferrable Attack\nACM Reference Format: Peng-Fei Zhang, Zi Huang, Guangdong Bai. 2024. Universal Adversarial Perturbations for Vision-Language Pre-trained Models. In Proceedings of\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, July 2017, Washington, DC, USA \u00a9 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, July 2017, Washington, DC, USA \u00a9 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn\nACM Conference (Conference\u201917). ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn\n# 1 INTRODUCTION\nVision-language pre-trained models like CLIP [24], ALBEF [13], and TCL [33], have emerged as essential tools for understanding intricate relationships between visual and textual elements. These models are pre-trained on large-scale unlabelled datasets and finetuned for downstream tasks. Due to their promising performance, they have been widely applied to various vision-language tasks, ranging from information retrieval [17] to image captioning [41]. Despite their success, VLP models still face a notable limitation in their ability to withstand adversarial examples, which are crafted by adding imperceptible perturbations to original data. Additionally, when these models are fine-tuned for downstream tasks, the potential vulnerability would also be inherited. In response to the adversarial vulnerability, several studies have been conducted to assess the adversarial robustness of VLP models [18, 30, 35, 42]. For example, Co-Attack [35] learns adversarial examples by enlarging the gap between them with the original paired data in different modalities. SGA [18] and SA-Attack [9] adopt data augmentation strategies to increase the input diversity for better disruption of intrinsic cross-modal interactions. However, there are several limitations within previous research. Existing methods typically learn to generate specific adversarial perturbations for each data instance, which might not generalize well to unseen data with different characteristics. In such cases, adversarial perturbations have to be learned from scratch, resulting in extra computational costs. Few studies, such as AdvCLIP [42], have started investigating this issue, yet they are either specific to particular models or limited in the imperceptibility. The research problem of learning universal adversarial perturbations (UAPs) that can be applied to different unseen models, datasets and tasks without additional specialized computation remains largely open. In this paper, we investigate effective and transferrable attacks against VLP models by crafting UAPs for the modality of image. The key challenges of learning effective UAPs are at least three-fold. First, compared to learning sample-specific adversarial perturbations, learning UAPs has to be independent of the specific characteristics of individual samples. Second, conventional universal adversarial attacks in uni-modal cases only need to consider relationships between single instances and their associated labels. In contrast, in multi-modal scenarios, which VLP models focus on, the interactions between different modalities need to be engaged, and their relations are often many-to-many [9]. Data from different modalities come in different formats and describe the same object from different perspectives, containing diverse and supplementary information. Consequently, complex interactions between different modalities along with the heterogeneity issue cause significant\nchallenges to effective universal adversarial attacks. The third challenge is the transferability. During pre-training, VLP models are customized with various architectures, learning objectives, and even training datasets tailored to specific applications. They could be further fine-tuned in response to different downstream tasks [13, 24, 33]. The resulting intrinsic divergence between models makes the learning of transferrable UAPs even more challenging. To tackle these challenges, we propose a novel black-box UAP generation method, named Effective and Transferable Universal Adversarial Attack (ETU). The ETU focuses on attacking various VLP models without prior knowledge of model details such as architectures, downstream tasks and training datasets. It thoroughly considers the characteristics of UAPs and intrinsic data interactions across different modalities. Specifically, in addition to optimizing the entire space of UAPs, the ETU is designed to improve the utility of the local regions of UAPs, decreasing the interactions between different UAP units. Simultaneous global and local optimizations are expected to enhance the effectiveness of UAPs while boosting their universality. In the meanwhile, a novel data augmentation algorithm named ScMix is proposed, which performs both self-mix and cross-mix data transformations to increase the multi-modal data diversity while preserving the original semantics. Thus, it can help comprehensively exploit cross-modal interactions. UAPs are learned to maximize the dissimilarity between diverse multi-modal data pairs, disrupting cross-modal interactions. As a result, the effectiveness and transferability of UAPs are ensured. The main contributions of this work are summarized as follows: \u2022 It is the first attempt to learn UAPs in black-box settings to test the robustness of vision-language pre-trained models. It also characterizes key challenges for launching an effective universal attack in multi-modal scenarios, which can serve as the foundation for future research in this area. \u2022 A novel effective and transferrable UAP generation method is designed, which improves the utility and transferability of UAPs by comprehensively considering multi-modal interactions. A novel local UAP reinforcement technique and an ScMix data augmentation method are proposed to boost the effectiveness and transferability of adversarial attacks. \u2022 The proposed ETU is tested on a wide range of VLP models, downstream tasks and datasets, where promising results demonstrate its superiority.\n# 2 RELATED WORK\n# 2.1 Vision-Language Pre-training\nVision-language models form the cornerstone of a wide range of tasks, e.g., multi-modal retrieval [38, 40], zero-shot learning [5, 24], image captioning [4], visual question answering [1], and visual entailment [32]. To advance the capability, vision-language pretraining has been introduced. It harnesses large amounts of unlabelled multi-modal data (e.g., image-text pairs) to develop models via self-supervised learning, e.g., multi-modal contrastive learning [12, 13, 24, 33]. For instance, the groundbreaking CLIP [24] is proposed to learn aligned VLP models that generate embeddings of images and texts with corresponding unimodal encoders. Contrastive learning is utilized to train unimodal encoders by maximizing the similarity between embeddings of matched image-text pairs\nwhile simultaneously enlarging the embedding distance between unmatched pairs. BLIP [12] leverages the dataset bootstrapping method to improve the quality of the training set by synthesizing captions for web images while sieving out noisy captions. Three contrastive learning objectives, i.e., image-text contrastive learning, image-text matching, and image-conditioned language modelling, are utilized to jointly pre-train the model. ALBEF [13] first aligns unimodal representations of image-text pair and then fuses them with cross-modal attention to obtain joint representations. TCL [33] utilizes contrastive learning to perform both inter- and intra-modal alignment and preserve mutual information between global and local representations in each modality. Vision-language pre-trained models exhibit outstanding generalizability and transferability in representation learning. Consequently, they have been widely used for downstream tasks through fine-tuning.\n# 2.2 Adversarial Attack\nConventional adversarial attack. Adversarial attack aims to mislead target models to make wrong predictions by crafting imperceptible perturbations to inject into original data (i.e., adversarial perturbations)[27, 36, 39]. It is mainly utilized to test the robustness of DNNs. Thereinto, adversarial perturbations can be roughly divided into instance-specific adversarial perturbations [19] and universal adversarial perturbations (UAPs) [20]. As the name implies, instance-specific adversarial perturbations are tailored to particular instances, while UAPs refer to perturbations that are applicable across various instances. Compared to instance-specific adversarial perturbations, UAPs are more applicable for real-world applications as they can deceive different instances of a modal without additional training, even if those instances were not seen during the perturbation crafting process. Representative methods for producing adversarial perturbations include optimization-based methods [3, 26], gradient-based methods [8, 19] and generative methods [2, 23, 37]. Early attack methods generate adversarial perturbations in the white-box settings, where information regarding the victim models, tasks, and data is available. Iterative optimization methods, e.g., I-FGSM, and PGD, that apply multiple-time gradient ascent are utilized for better attack performance [11, 19]. However, in real-world applications, it is often the case that target information is often not available, which is referred to as the grey/black-box setting. While adversarial perturbations especially UAPs show certain adversarial transferability across different data and models [21], such transferability is limited as iterative optimization would incur severe overfitting problems. After some training steps, the perturbations would fit into target models excessively and can hardly transfer to another different model. To handle this case, various works have been proposed to enhance the transferability of adversarial perturbations. Taking an ensemble of networks as the target can promote transferability [16]. The drawback is that using multiple models comes with computation overhead. Momentum-based methods are designed to accumulate the previous gradients in order to avoid being trapped in poor local maxima [6]. Another line of work proposes to increase the input diversity via data augmentation to prevent overfitting,\ne.g., data mixing operations [28], data scaling [14] and random transformations (e.g., resizing, cropping and rotating) [31]. Adversarial attack against VLP models. Recently, with the proliferation of VLP models, research has started to investigate the robustness of VLP models. Attacking VLP models is different from those in DNNs. Adversarial attack in DNNs usually concentrates on the classification task. It is a unimodal task that only considers the relation between a single instance and its label, while VLP models focus on multi-modalities. One needs to consider multi-modal data relationships when implementing an attack. For example, Zhang et al. [35] propose Collaborative Multimodal Adversarial Attack (CoAttack) to attack various pre-trained models including CLIP, ALBEF, and TCL. Co-Attack generates multi-modal adversarial perturbations by enlarging the embedding distance between adversarial examples and the original data pairs. As VLP models are usually fine-tuned for downstream tasks, improving the transferability of adversarial perturbations is particularly important to comprehensively evaluate the robustness of VLP models. To this end, Set-level Guidance Attack (SGA) [18] is proposed to apply data augmentation to increase data diversity and accordingly craft multi-modal adversarial perturbations by minimizing the similarity between them and their matched data from another modality. Self-augment-based transfer attack (SA-Attack) [9] applies more data augmentations on both original data and adversarial examples to further improve transferability. Co-Attack, SGA and SA-Attack learn multi-modal adversarial perturbations one by one, i.e., first generating adversarial perturbations for one modality and then learning adversarial perturbations for the other. Wang et al. [30] propose to learn multimodal adversarial perturbations simultaneously. Despite the progress made, these methods commonly consider data-specific perturbations, suffering from limited generalization ability and transferability. Tailoring perturbations for specific instances makes it challenging to generalize to new data. In addition, it requires additional training for generating perturbations for new data, incurring huge computational costs for large-scale applications. Although AdvCLIP [42] studies to craft universal adversarial patches, it only concentrates on CLIP. Furthermore, the adversarial patch is not strictly constrained in terms of the perturbation magnitude, making it prone to be identified. To address it, we present the first attempt to learn effective universal adversarial perturbations.\n# 3 PROPOSED METHOD\n# 3.1 Preliminaries\nIn this work, we aim to learn universal adversarial perturbations that are able to transfer across different VLP models, datasets, and downstream tasks. This means that the VLP models, target datasets, and downstream tasks remain unknown or unavailable during learning. To handle this black-box setting, we train the attack model by utilizing a surrogate dataset and model. The surrogate multi-modal dataset is denoted as D\ud835\udc60= {(\ud835\udc65\ud835\udc56,\ud835\udc61\ud835\udc56)}\ud835\udc5b \ud835\udc56=1 where (\ud835\udc65\ud835\udc56,\ud835\udc61\ud835\udc56) is the image and text pair and \ud835\udc5bis the number of pairs. For the surrogate VLP model, we denote the image encoder and the text encoder as \ud835\udc53\ud835\udc65and \ud835\udc53\ud835\udc61, respectively. The goal here is to learn a universal adversarial perturbation \u2225\ud835\udeff\u2225\u221e\u2264\ud835\udf16for the modality of images, where \ud835\udf16is the magnitude of the perturbation and \ud835\udc59\u221eis the perturbation constraint. The learned UAP can mislead VLP models\nto wrongly associate images and texts in the target dataset D\ud835\udc61at the reference time, thus making wrong predictions. For example, in the image-text retrieval, it would make models return incorrect retrieved results.\n# 3.2 Overview\nTo enable successful black-box attacks, we propose a novel Effective and Transferable Universal Adversarial Attack (ETU) method. As illustrated in Figure 1, the proposed ETU learns UAPs by taking a surrogate model as the victim, which consists of an image encoder and a text encoder. During the learning procedure, the ETU increases the multi-modal input diversity using ScMix data augmentation strategy. UAPs are learned to attack the model by disrupting intra- and inter-modal relationships between original data pairs. Both global and local regions of the UAP would be optimized to improve the effectiveness and transferability.\n# 3.3 Effective and Transferable Universal Adversarial Attack\nVLP models endeavor to learn the interactions between images and texts for effective multi-modal representation. To achieve successful attacks in multi-modal scenarios, it is necessary to consider multi-modal relationships. An intuitive method is to consider the relationships between the matched pairs, and the UAP is learned by enlarging the embedding distance between the original image and its matched text: \u2211\ufe01\n(1)\nwhere \u2113is a loss function to quantify the difference between representations of two samples, e.g., the KL-divergence loss. The objective deters the model from correctly perceiving perturbed images and associating them with their paired texts. Iterative methods such as PGD [11, 19] are then used to solve the optimization problem given their promising performance. This scheme might work in white-box attack settings, where the target information is available, including target models, data and tasks. However, in real-world applications, such information is largely unknown to the attacker. A common alternative is to utilize a substitute model as the victim target. However, perturbations learned through iterative optimization methods would suffer from low transferability. During the multi-round optimization, perturbations would gradually overfit the victim model. As a result, it can hardly be used to effectively attack another model with different architectures and parameters. This situation will be exacerbated by intrinsic crossmodal interactions in multi-modal application scenarios. To alleviate these, the ETU leverages two key novel techniques, i.e., local utility reinforcement, and ScMix data augmentation. Local utility reinforcement. This technique enhances the utility of the UAP\u2019s local regions for two benefits. First, boosting the local regions can naturally help improve the utility of the entire UAP. Second, it decreases interactions between different local regions, thereby improving the transferability [29]. To achieve this, during the UAP learning process, in addition to optimizing the entire UAP, we randomly crop subregions and resize them to the same as the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0e26/0e2665eb-2902-4fc2-a5e8-59a096f54615.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 1: An illustration of the proposed ETU method. The ETU exploits the characteristics of UAPs and diverse cross-modal interactions to improve the utility and transferability of UAPs. Specifically, it generates a variety of similarity-preserving image-text pairs through the ScMix augmentation, which consists of self-mix and cross-mix operations. The ETU optimizes both the entire space and local regions of UAPs by disturbing the similarity between diverse multi-modal data pairs. In light of this, the utility and transferability of UAPs are ensured.\nsize of the original images. For brevity, we denote this transformation process as A\ud835\udc60. Similar to optimizing the entire UAP, the local regions of the UAP are learned by enlarging the embedding distance between the perturbed images and original pairs:\n(2)\n\u2211\ufe01 + \u2113(\ud835\udc53\ud835\udc65(\ud835\udc65\ud835\udc56+ A\ud835\udc60(\ud835\udeff)), \ud835\udc53\ud835\udc61(\ud835\udc61\ud835\udc56))).\nScMix augmentation. A representative method for enhancing the transferability is to leverage the data augmentation strategy to increase the input diversity. Learning with diverse inputs can effectively prevent adversarial perturbations from overfitting to specific patterns [9, 18]. In addition, in multi-modal scenarios, employing data augmentation can help further exploit the intrinsic crossmodal interactions. In light of these, we design a novel semanticpreserving data augmentation technique. Specifically, a proper data transformation for augmentation should satisfy the following criteria: diversity and semantic preservation. This means the augmentation should increase the visual difference between the original data and the augmented data. In the meantime, the augmentation should not significantly alter the semantics, which would deter the model from recognizing the data and thus harm the effectiveness of generated UAPs. Under this regime, we propose a novel semantic-preserving ScMix method. As illustrated in Figure 2, the proposed ScMix is a high-order data transformation strategy, which performs self-mix and cross-mix operations to enhance data diversity while preserving semantics. Self-mix constructs new data by mixing up the same original data of different transformations. Specifically, in the self-mix process, two subregions of the original image \ud835\udc65\ud835\udc56would be randomly cropped and resized to the same size as the original image. Denote the two rescaled subregions as \ud835\udc651 \ud835\udc56and \ud835\udc652 \ud835\udc56. They would be mixed to construct a new data instance \u02c6\ud835\udc65\ud835\udc56. Finally, the mixed data instance would be interpolated with another data \ud835\udc65\ud835\udc57instance that is randomly selected from the mini-batch to craft the final instance \u02dc\ud835\udc65\ud835\udc56. Formally, the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2e29/2e2942c5-130b-49f9-b1b3-42252da33a17.png\" style=\"width: 50%;\"></div>\nFigure 2: An illustration of the ScMix method, which consists of self-mix operation and cross-mix operations. During self-mix, two local regions of the original image would be randomly cropped and resized to the same size as the original image. Then two rescaled patches would be mixed into a new image. During cross-mix, the self-mixed image would be mixed with another image in a master-slave relation.\nwhole process can be formulated as follows:\n(3)\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd where \ud835\udefd1 > \ud835\udefd2 \u2208[0, 1) to ensure that the cross-mix would not significantly change the semantics of the original image when injecting the image from another class. \ud835\udc5dis the prediction for the data after ScMix. Beta(\u00b7, \u00b7) represents a Beta distribution. \ud835\udefc\u2208(0, \u221e).\nTable 1: Attack success rate (%) on the image-text retrieval task. The CLIP with the ViT-B/16 and Flickr30K are adopted as the source model and dataset for training. The grey background indicates the white-box attack results.\nTest Dataset\nFlickr30K\nTask\nImage-to-Text\nText-to-Image\nTarget Model\nMethod\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nETUL\n88.96\n76.43\n70.43\n93.49\n88.27\n85.32\nETUS\n93.13\n88.16\n83.84\n96.13\n93.83\n92.37\nCLIP\u2212ViT-B/16\nETU\n88.47\n78.61\n73.07\n92.69\n87.5\n84.89\nTest Dataset\nMSCOCO\nETUL\n29.21\n14.32\n9.57\n27.79\n14.35\n10.28\nETUS\n24.65\n12.2\n8.6\n25.58\n12.92\n9.22\nALBEF\nETU\n32.43\n15.76\n11.28\n29.82\n16.25\n11.83\nWith the ScMix, we can augment the original data to create diverse multi-modal data pairs. This helps effectively prevent overfitting and exploit cross-modal interactions to learn more effective and transferrable UAPs. Moreover, in datasets, e.g., Flickr30K [22] and MSCOCO [15], there are often diverse captions to describe an image for different perspectives or language styles. We capture the most matched captions for each image to further increase the multi-modal data diversity. Specifically, denote the caption set for the image \ud835\udc65\ud835\udc56is {\ud835\udc61\ud835\udc561,\ud835\udc61\ud835\udc562, ...,\ud835\udc61\ud835\udc56\ud835\udc58}. The augmented multi-modal data pairs are {( \u02dc\ud835\udc65\ud835\udc561,\ud835\udc61\ud835\udc561, \ud835\udc5d\ud835\udc561), ( \u02dc\ud835\udc65\ud835\udc562,\ud835\udc61\ud835\udc562, \ud835\udc5d\ud835\udc562), ..., ( \u02dc\ud835\udc65\ud835\udc56\ud835\udc58,\ud835\udc61\ud835\udc56\ud835\udc58, \ud835\udc5d\ud835\udc56\ud835\udc58)}, where \u02dc\ud835\udc65\ud835\udc561, \u02dc\ud835\udc65\ud835\udc562, ..., \u02dc\ud835\udc65\ud835\udc56\ud835\udc58are obtained by applying ScMix on \ud835\udc65\ud835\udc56. As a result, an enriched dataset would be obtained for learning. Cross-modal interaction disruption. The UAP learned to disrupt the cross-modal interactions by breaking the similarity between diverse paired data in the enriched dataset. Different from current data-specific attack methods that use augmented data to optimize the entire area of UAPs [9, 18], we propose to leverage them to enhance the local utility of the UAP. The reason is that UAP is learned over the whole dataset instead of on a single instance as data-specific adversarial perturbation generation. Simply increasing the number of data to optimize the entire UAP would encourage it to overfit the victim model. Instead, utilizing augmented data to optimize local regions of the UAP can further encourage the effectiveness of the UAP while relieving the overfitting problem. Table 1 lists some black-box results, where we can observe that ETUS that leverages the ScMix to optimize the entire area of the UAP achieve worse performance than that does not use the ScMix, i.e., ETUL. And the ETU that uses ScMix to optimize the local utility of the UAP can achieve better transferrable attacks. In light of this, the objective of learning UAP based on ScMix is:\n(4)\nwhere \ud835\udc5b\ud835\udc61is the caption number. The overall objective for learning UAP is defined as:\nAlgorithm 1: Universal Adversarial Perturbations for\nVision-Language Pre-trained Models\nRequire: Training data D = {(\ud835\udc65\ud835\udc56,\ud835\udc61\ud835\udc56)}\ud835\udc5b\n\ud835\udc56=1, mini-batch size \ud835\udc5a,\niteration times \ud835\udc47, parameters \ud835\udf16, \ud835\udefc, \ud835\udefd1, \ud835\udefd2;\nRequire: Randomly initialize \ud835\udeff;\n// Training;\n// Exploit diverse matching captions for each image to\naugment the dataset;\nfor \ud835\udc52\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e= 1 \u2192\ud835\udc47do\n{(\ud835\udc65\ud835\udc56,\ud835\udc61\ud835\udc56)}\ud835\udc59\n\ud835\udc56=1 \u223cD // Sample mini-batch from the dataset;\n{( \u02dc\ud835\udc65\ud835\udc56,\ud835\udc61\ud835\udc56)}\ud835\udc59\n\ud835\udc56=1 \u2190{(\ud835\udc65\ud835\udc56,\ud835\udc61\ud835\udc56)}\ud835\udc59\n\ud835\udc56=1 // Augment each image-text\npair via ScMix;\nA\ud835\udc60(\ud835\udeff) // Randomly crop and resize the UAP;\n\ud835\udeff\u2190Eq.(5) // Update the UAP by optimizing Eq.(5);\nend\nreturn \ud835\udeff;\n<div style=\"text-align: center;\">Algorithm 1: Universal Adversarial Perturbations for Vision-Language Pre-trained Models</div>\nwhere data in L1 and L2 would also be augmented with different captions for each image. To solve the optimization problem, we leverage the commonly used projected gradient descent (PGD) [19]. The detailed algorithm is summarized in Algorithm 1.\n# 4 EXPERIMENTS\n# 4.1 Settings\nDownstream tasks and datasets. To comprehensively evaluate the performance of the proposed method, we conduct experiments on three vision-language tasks with three datasets. The first is the image-text retrieval task. Two widely used datasets, i.e., Flickr30K [22] and MSCOCO [15], are selected. The Flickr30K data consists of 31,783 images, with each image accompanied by five descriptive captions. The MSCOCO dataset released in 2014 is composed of 164k images, each annotated with approximately five captions. The second is the image captioning tasks, where Flickr30K and MSCOCO are used for the training and testing in the experiments. The third is the visual grounding task. The Flickr30K is used for training. And we choose the RefCOCO+ dataset [34] for the performance evaluation, which contains 141,564 expressions corresponding to 49,856 objects found in 19,992 images. For all experiments, in line with previous research [18, 35], we use the test set in these datasets for both training and testing purposes. Models. Four widely-used VLP models are utilized to test the proposed method, i.e., CLIP [24], ALBEF [13], TCL [33] and BLIP [12]. For CLIP, different image encoders are utilized, including vision transformers (i.e., ViT-B/16, ViT-B/32, and ViT-L/14[7]) and CNNs (i.e., ResNet50, and ResNet101 [10]). The text encoder is a 6-layer transformer. For BLIP, we choose the one that consists of a ViTB/16 and a 6-layer transformer as the image and text encoder to attack. In addition, BLIP is mainly used for experiments on the image captioning task. ALBEF and TCL take ViT-B/16 as the image encoder and adopt a 6-layer transformer for both the text encoder and multimodal encoder.\n<div style=\"text-align: center;\">le 2: Attack success rate (%) on the image-text retrieval task. The CLIP with the ViT-B/16 is adopted as the source model for ining. The grey background indicates the white-box attack results. Bold indicates the best results.</div>\nTable 2: Attack success rate (%) on the image-text retrieval task. The CLIP with the ViT-B/16 is adopted  training. The grey background indicates the white-box attack results. Bold indicates the best results.\nTest Dataset\nFlickr30K\nMSCOCO\nTask\nImage-to-Text\nText-to-Image\nImage-to-Text\nText-to-Image\nTarget Model\nMethod\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nUniA\n91.9\n82.87\n78.66\n91.14\n81.65\n79.96\n95.5\n91.42\n88.82\n94.24\n90.11\n87.65\nMulA\n92.02\n82.04\n76.62\n94.85\n90.42\n86.22\n95.5\n91.6\n88.87\n96.13\n93.5\n91.76\nETUL\n88.96\n76.43\n70.43\n93.49\n88.27\n85.32\n93.78\n88.85\n85.17\n95.34\n92.21\n90.49\nETUS\n93.13\n88.16\n83.84\n96.13\n93.83\n92.37\n96.8\n94.35\n92.68\n97.25\n95.7\n94.99\nCLIP\u2212ViT-B/16\nETU\n88.47\n78.61\n73.07\n92.69\n87.5\n84.89\n93.55\n89.01\n85.88\n94.25\n91.3\n89.31\nUniA\n32.82\n19.34\n14.42\n43.77\n26.3\n19.31\n58.85\n43.32\n36.91\n64.29\n47.74\n40.59\nMulA\n42.53\n25.58\n20.19\n48.37\n29.74\n22.63\n66.73\n51.81\n44.21\n69.5\n53.13\n45.62\nETUL\n46.74\n28.01\n21.94\n50.19\n31.47\n23.99\n71.27\n56.16\n49.6\n70.89\n55.94\n48.9\nETUS\n47.25\n30.76\n24.1\n55.61\n39.13\n32.1\n72.21\n58.58\n51.71\n74.28\n60.89\n54.49\nCLIP\u2212ResNet50\nETU\n56.83\n38.9\n32.54\n61.27\n43.11\n36.59\n78.55\n66.85\n61.13\n79.37\n67.18\n60.72\nUniA\n28.35\n13.64\n9.17\n33.93\n18.17\n14.14\n50.27\n35.14\n29.22\n54.32\n39.79\n33.39\nMulA\n35.89\n18.08\n13.08\n38.46\n22.42\n17.03\n59.13\n44.03\n37.39\n60.51\n45.84\n39.52\nETUL\n37.29\n20.82\n14.01\n40.99\n24.62\n17.62\n61.59\n47.43\n40.72\n62.03\n47.91\n41.33\nETUS\n41.51\n24.0\n17.82\n48.51\n32.34\n26.25\n65.47\n52.43\n45.79\n68.71\n55.13\n49.19\nCLIP\u2212ResNet101\nETU\n52.49\n33.83\n26.57\n54.27\n37.29\n30.29\n72.01\n60.05\n53.8\n73.59\n60.79\n54.4\nUniA\n19.75\n6.85\n3.35\n29.16\n14.41\n9.27\n40.75\n23.6\n17.56\n46.39\n28.98\n23.04\nMulA\n21.84\n7.13\n4.47\n31.12\n14.97\n10.05\n41.32\n24.18\n18.67\n47.08\n30.48\n24.47\nETUL\n22.21\n7.68\n4.07\n31.83\n16.02\n10.49\n43.49\n26.74\n21.07\n49.22\n32.41\n25.98\nETUS\n20.74\n7.54\n4.78\n32.28\n15.95\n10.95\n42.92\n25.98\n20.16\n49.27\n32.04\n25.99\nCLIP\u2212ViT-B/32\nETU\n22.58\n7.89\n5.39\n33.7\n16.35\n11.4\n45.14\n27.5\n21.98\n50.51\n33.13\n26.82\nUniA\n14.48\n4.57\n2.54\n21.81\n8.99\n5.91\n35.22\n20.96\n16.61\n39.22\n24.41\n19.26\nMulA\n20.6\n8.93\n5.28\n27.93\n13.9\n10.25\n43.42\n29.49\n24.16\n45.93\n31.33\n25.89\nETUL\n16.69\n5.3\n2.7\n22.39\n8.81\n6.02\n38.23\n23.36\n18.19\n38.72\n23.99\n18.83\nETUS\n19.26\n7.48\n4.78\n33.76\n18.48\n13.96\n45.21\n29.57\n23.19\n53.38\n39.15\n33.54\nCLIP\u2212ViT-L/14\nETU\n20.86\n9.1\n5.49\n28.54\n14.34\n9.25\n43.49\n28.31\n22.77\n47.35\n32.31\n26.56\nUniA\n6.36\n2.1\n1.3\n11.3\n3.79\n2.12\n20.8\n8.7\n5.41\n21.86\n10.37\n6.95\nMulA\n8.86\n3.11\n1.9\n13.52\n4.8\n3.09\n25.93\n11.75\n7.64\n24.92\n12.36\n8.22\nETUL\n10.11\n4.01\n2.8\n15.32\n5.7\n3.36\n29.21\n14.32\n9.57\n27.79\n14.35\n10.28\nETUS\n8.76\n3.21\n1.9\n13.26\n4.12\n2.71\n24.65\n12.2\n8.6\n25.58\n12.92\n9.22\nALBEF\nETU\n13.14\n4.81\n3.3\n17.28\n6.54\n4.21\n32.43\n15.76\n11.28\n29.82\n16.25\n11.83\nUniA\n8.54\n2.31\n1.3\n13.71\n4.49\n2.7\n22.41\n9.83\n6.41\n23.05\n11.12\n7.55\nMulA\n13.38\n4.5\n2.6\n17.19\n6.02\n3.88\n27.28\n13.02\n8.42\n25.68\n13.06\n9.12\nETUL\n14.65\n6.03\n3.81\n19.48\n6.97\n4.59\n31.4\n15.37\n10.18\n27.26\n14.37\n9.87\nETUS\n12.43\n4.12\n3.01\n17.33\n6.49\n4.06\n27.14\n13.54\n9.37\n26.5\n13.49\n9.42\nTCL\nETU\n18.55\n7.94\n5.71\n21.57\n8.64\n5.91\n34.02\n17.57\n11.85\n30.28\n15.89\n11.23\nEvaluation metric. In line with prior research [18, 35], we utilize the Attack Success Rate (ASR) as a metric to quantify the effectiveness of the proposed attack and all compared baselines. ASR is calculated as the percentage of adversarial examples that successfully deceive the model, providing a reliable measure of the attackers\u2019 effectiveness. Implementation details. For fundamental experiments, the perturbation magnitude \ud835\udf16is uniformly set as 12/255. Additionally, we evaluate the proposed method under varying perturbation magnitudes. PGD is utilized to solve the optimization problem, with the number of iterations \ud835\udc47as 100 and the step size as \ud835\udf16/\ud835\udc47\u22171.25. The batch size is set as 16. \ud835\udefc= 4, \ud835\udefd1 = 0.8, \ud835\udefd2 = 0.2. For \u2113, we\nleverage the KL-divergence loss to measure the difference between two samples. Baselines. As the proposed method is the first endeavor to learn UAPs against VLP models, there are no existing benchmarks for comparison. To address this, we construct baselines based on prior works that focus on sample-specific attacks against VLP models [35], along with variants of our proposed method. These can help verify the efficacy of each component and demonstrate the superiority of our overall algorithm in learning UAPs. (1) Unimodal attack (UniA), which learns UAPs by directly enlarging the embedding distance between the adversarial images and their original counterparts;\nTable 3: Attack success rate (%) regarding the average of R@1 on the image-text retrieval task. The grey background indicate the white-box attack results. Bold indicates the best results.\nTest Dataset\nFlickr30K\nTarget Model\nCLIP\nALBEF\nTCL\nResNet50\nResNet101\nViT-B/16\nViT-B/32\nViT-L/14\nSource Model\nMethod\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nI2T\nT2I\nUniA\n94.13\n96.78\n16.86\n21.2\n8.22\n14.5\n17.79\n25.97\n7.85\n16.98\n3.86\n9.33\n8.54\n13.71\nMulA\n96.3\n97.67\n18.9\n24.36\n8.34\n13.85\n16.56\n25.04\n8.83\n16.3\n5.11\n8.49\n13.38\n17.19\nETUL\n95.79\n97.8\n27.08\n34.44\n7.48\n16.72\n14.72\n25.81\n9.45\n18.27\n4.1\n9.29\n14.65\n19.48\nETUS\n96.81\n98.66\n28.22\n34.55\n9.69\n15.82\n15.54\n24.58\n10.43\n19.46\n4.8\n9.57\n12.43\n17.33\nCLIP-ResNet50\nETU\n95.79\n98.63\n37.04\n44.94\n9.33\n17.49\n16.58\n27.74\n9.82\n18.65\n5.42\n10.59\n18.55\n21.57\nUniA\n18.65\n27.65\n12.77\n17.5\n7.36\n13.11\n17\n23.36\n9.08\n18.4\n79.35\n85.85\n20.76\n22.38\nMulA\n19.16\n28.87\n11.88\n16.5\n7.12\n12.56\n15.91\n25.26\n9.2\n17.53\n82.33\n87.35\n25.29\n27.0\nETUL\n31.33\n31.05\n15.07\n20.17\n9.71\n15.14\n17.06\n25.29\n11.78\n19.68\n79.98\n85.97\n21.75\n26.45\nETUS\n18.14\n25.28\n11.37\n17.26\n6.99\n12.79\n16.69\n23.71\n8.1\n15.59\n74.87\n72.99\n18.65\n19.62\nALBEF\nETULS\n26.56\n35.03\n20.31\n25.08\n10.06\n17.72\n17.18\n26.22\n13.01\n21.55\n83.0\n87.07\n32.98\n31.5\nUniA\n17.58\n32.08\n14.18\n21.2\n8.22\n14.2\n17.42\n26.48\n10.18\n18.46\n21.58\n22.57\n90.38\n82.26\nMulA\n17.31\n29.16\n14.3\n18.94\n7.26\n13.72\n14.85\n25.77\n10.55\n17.56\n17.52\n21.75\n93.68\n87.98\nETUL\n24.27\n35.68\n17.62\n22.74\n8.22\n16.33\n17.18\n28.16\n11.41\n19.78\n23.15\n24.58\n93.72\n88.1\nETU\n20.56\n30.15\n13.67\n19.76\n7.48\n15.17\n16.07\n26.68\n8.59\n18.07\n18.87\n20.65\n87.67\n83.64\nTCL\nETULS\n27.59\n39.69\n20.82\n26.96\n9.2\n17.94\n17.44\n28.61\n12.64\n20.59\n23.25\n25.44\n94.1\n87.6\nTest Dataset\nMSCOCO\nUniA\n93.75\n96.2\n32.28\n37.28\n21.21\n26.31\n31.61\n38.77\n24.38\n28.82\n14.43\n17.37\n17.78\n20.63\nMulA\n93.83\n95.5\n33.14\n38.92\n21.48\n25.77\n29.99\n38.13\n24.72\n28.84\n14.44\n16.77\n16.98\n20.27\nETUL\n93.87\n95.07\n46.26\n53.08\n21.79\n28.15\n30.71\n38.43\n28.58\n32.99\n16.65\n18.68\n17.46\n20.97\nETUS\n96.61\n96.92\n51.98\n55.84\n24.84\n28.39\n30.74\n39.04\n29.11\n32.34\n17.35\n19.45\n19.89\n21.77\nCLIP-ResNet50\nETU\n96.64\n97.3\n61.26\n64.95\n25.79\n31.7\n31.63\n40.27\n30.52\n33.09\n17.99\n20\n20.97\n23.1\nUniA\n37.64\n43.3\n24.52\n31.68\n18.81\n23.47\n29.91\n38.44\n27.36\n30.74\n79.71\n83.18\n37.22\n31.12\nMulA\n37.88\n44.46\n25.75\n32\n20.18\n24.28\n31.1\n38.15\n26.17\n30.89\n82.57\n84.88\n45\n37.58\nETUL\n44.34\n49.89\n31.71\n38.41\n22.87\n27.56\n31.4\n40.14\n32.51\n35.17\n79.82\n83.34\n45.56\n37.24\nETUS\n36.9\n41.82\n23.54\n30.94\n18.58\n24.37\n30.87\n38.2\n22.36\n28.2\n78.68\n75.79\n32.54\n27.62\nALBEF\nETU\n49.9\n56.11\n40.42\n45.16\n25.6\n31.95\n33.57\n41.47\n34.38\n37.32\n82.99\n85.19\n50.9\n42.04\nUniA\n41.64\n49.62\n31.19\n37.21\n21.75\n27.28\n32.62\n41.05\n30.14\n32.05\n43.57\n36.87\n92.01\n86.87\nMulA\n38.46\n47.08\n28.16\n33.58\n21.33\n26.19\n30.68\n38.09\n26.78\n31.21\n38.7\n32.01\n92.43\n87.36\nETUL\n47.32\n54.57\n36.04\n42.29\n24.8\n29.74\n33.31\n41.69\n29.11\n32.26\n45.13\n37.25\n93.89\n89\nETUS\n39.4\n47.89\n24.38\n34.33\n20.03\n21.75\n30.83\n41.35\n27.05\n30.69\n42.92\n36.91\n92.28\n88.57\nTCL\nETU\n53.25\n57.99\n42.3\n46.58\n26.4\n31.9\n33.88\n41.78\n31.48\n33.83\n45.42\n37.93\n93.28\n89.45\n(2) Multi-modal attack (MulA), which pulls adversarial images away from the original images and paired texts in the embedding space. The objective is max\ud835\udeffL1; (3) ETUL, which is a variant of the ETU that only considers local utility reinforcement. The objective is max\ud835\udeff(L1 + L2); (4) ETUS, which is a variant of the proposed method that utilizes the ScMix augmentation to increase the input diversity. The augmented data are utilized to optimize the entire area of UAPs without considering the local utility.\n# 4.2 Results on the Image-Text Retrieval\nWe assess the transferability of the proposed method by launching attacks against a range of VLP models, i.e., CLIP models with different backbones, ALBEF and TCL, in black-box settings. Given that different VLP models may accept inputs of varying sizes, the learned UAPs are resized accordingly before initiating attacks. For instance, the UAPs learned based on CLIP would be resized to 384\u00d7384 when attacking ALBEF or TCL. Conversely, when conducting transfer\nattacks from ALBEF or TCL to CLIP, the UAPs are resized from 384 \u00d7 384 to 224 \u00d7 224. Table 2 presents the attack results by taking ViT-B/16-based CLIP and the Flickr30K dataset as the source model and training set, respectively. The averages of R@1, R@5, and R@10 for both image-to-text retrieval and text-to-image retrieval are recorded. From these results, we can draw the following observations. First, all methods achieve promising white-box attack performance. However, when transferring to unseen models, methods that do not consider transferability, i.e., UniA and MulA, lose their efficacy. This means the two methods overfit the source model during training. In contrast, by considering the local utility and multi-modal data diversity, the proposed method can significantly improve black-box attack performance. Interestingly, increasing the diversity of the input to optimize the entire area of UAPs, i.e., ETUS, would improve attack performance on CLIP while harming the transferability on other models, i.e., ALBEF, and TCL. The reason may be that UAPs are learned based on the whole dataset. Increasing the number of inputs would make UAPs overfit the source\nTable 4: Performance under different attacks on the visual grounding task. The training dataset and test dataset are Flickr30K and RefCOCO+, respectively. The CLIP with the ViT-B/16 is set as the source model and ALBEF is taken as the target model. The \u201cBaseline\u201d denotes the performance of the target model on the original data. Lower values represent better adversarial transferability. Bold indicates the best results.\nTest Dataset\nRefCOCO+\nMethod\nVal\nTestA\nTestB\nBaseline\n51.2\n56.7\n44.8\nUniA\n49.6\n53.4\n42.9\nMulA\n49.6\n53.1\n42.6\nETUL\n48.7\n52.4\n42.1\nETUS\n49.2\n53.1\n42.5\nETU\n48.5\n51.5\n41.7\nTable 5: Performance under different attacks on image captioning. The training dataset and test dataset are Flickr30K and MSCOCO, respectively. The CLIP with the ViT-B/16 is adopted as the source model and BLIP is taken as the target model. \u201cBaseline\u201d denotes the performance of the target model on original data. Lower values represent better adversarial transferability. Bold indicates the best results.\nTest Dataset\nMSCOCO\nMethod\nB@4\nMETEOR\nROUGE_L\nCIDEr\nSPICE\nBaseline\n39.3\n30.7\n59.6\n131.4\n23.5\nUniA\n36.8\n29.3\n57.6\n122.3\n22.0\nMulA\n35.6\n28.7\n56.9\n118.1\n21.6\nETUL\n35.5\n28.4\n56.6\n117.1\n21.4\nETUS\n36.0\n28.8\n57.1\n119.4\n21.7\nETU\n34.8\n28.1\n56.3\n114.8\n21.0\nmodel. This might not influence the transferability across models with the same learning objective but influences performance on models with different learning objectives. Second, compared to UniA, MulA achieves better performance, which further demonstrates the necessity of considering cross-model interactions when implementing multi-modal attacks. Third, different architectures show different levels of robustness to UAPs. The models with ViT backbones show stronger robustness to the attack. ALEBF and TCL are less sensitive to the universal adversarial attack that targets CLIP. The reason may be that they adopt different learning objectives and architectures, thus introducing different representation spaces and multi-modal interactions. We further test the effectiveness of the proposed method by adopting different source models, where the results are summarized in Table 3. The promising performance achieved consistently demonstrates the superior performance of our approach in generating effective and transferable universal adversarial perturbations.\nTable 6: Comparison with different augmentation methods. Attack success rate (%) on image-text retrieval task are reported. The CLIP with the ViT-B/16 and Flickr30K are adopted as the source model and dataset for training.\nTest Dataset\nMSCOCO\nTask\nImage-to-Text\nText-to-Image\nTarget Model\nMethod\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nETUSI\n68.38\n54.72\n45.74\n71.24\n53.74\n45.08\nETUSM\n73.85\n62.14\n55.37\n76.33\n63.85\n51.17\nETUAdmix\n75.56\n64.21\n56.96\n77.24\n66.18\n56.1\nCLIP\u2212ResNet50\nETU\n78.55\n66.85\n61.13\n79.37\n67.18\n60.72\nETUSI\n39.11\n22.79\n17.39\n46.18\n28.88\n22.47\nETUSM\n42.2\n27.32\n20.71\n48.4\n31.61\n25.56\nETUAdmix\n44.14\n27.29\n21.27\n50.03\n32.66\n26.34\nCLIP\u2212ViT-B/32\nETU\n45.14\n27.5\n21.98\n50.51\n33.13\n26.82\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/322c/322cbe86-7bbe-48c4-93dc-9ff7904b0984.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Image-to-Text</div>\nFigure 3: Test accuracy on MSCOCO under different magnitudes of the UAP. The source model is ViT-B/16-based CLIP and the target model is ResNet50-based CLIP. The attack success rate in terms of the average of R@1 is reported.\n<div style=\"text-align: center;\">Figure 3: Test accuracy on MSCOCO under different magnitudes of the UAP. The source model is ViT-B/16-based CLIP and the target model is ResNet50-based CLIP. The attack success rate in terms of the average of R@1 is reported.</div>\n# 4.3 Results on the Visual Grounding and Image Captioning\n# 4.3 Results on the Visual Grounding and Image Captioning\nTo test the transferability of the proposed method across different tasks, we propose to conduct experiments on the visual grounding task and the image captioning task. For two tasks, we use the UAP generated based on Flickr30K and ViT-B/16-based CLIP in the image-text retrieval to attack target models. For the visual grounding task, we take ALBEF and RefCOCO+ as the target model and dataset. While on the image captioning, we use the learned UAP to attack BLIP on MSCOCO. The performance of target models under different attacks is reported in Table 4 and 5. From the results, we can have the consistent observation that the proposed method achieves the best universal adversarial attack. In addition, each component contributes to the effectiveness and transferability of the proposed method.\n# 4.4 Comparison with Different Augmentations\nWe compare the proposed ScMix with various data augmentation methods, including scale-invariant augmentation (ETU_SI) [18], self-mix (ETU_SM), and Admix (ETU_Admix) [28], where results on image-text retrieval are reported in Table 6. From the table, it can be observed that the proposed augmentation method is the most superior.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce17/ce17ad85-a891-47b7-8398-bdb3f3d4a599.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/815e/815e8322-7b60-4312-b2fc-c86a1ce0eaff.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/006c/006c03f6-97a9-4382-b529-703ceae17791.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/548c/548c0ef3-dae8-4058-aa5f-4f57a3457304.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Figure 4: Examples of top-5 image-text retrieval results.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/20c8/20c8f329-bc45-4a8a-9a70-195adb98825b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Figure 5: The Grad-CAM visualizations of the original images and the perturbed images by ETU.</div>\n# Figure 5: The Grad-CAM visualizations of the original images and the perturbed images by ETU.\n# 4.5 Results on Varying Perturbation Budgets\nWe test all methods under various perturbation magnitudes, with results on image-text retrieval presented in Figure 3. From the figure, we can observe that the proposed method achieves the overall best performance, further confirming its superiority. It is worth noting that as the magnitude increases, the performance of all methods\n<div style=\"text-align: center;\"></div>\nimproves. It is quite normal as larger perturbations typically lead to better attack performance. However, large perturbations would significantly influence data quality, making them easier to identify.\n# 4.6 Visualization\nIn Figure 4, we showcase some examples of image-text retrieval under the proposed attack. From the figure, it is evident that the proposed method can effectively mislead the target model to return incorrect retrieved data. Additionally, we present some Grad-CAM [25] visualization examples in Figure 5, where it can be seen that the UAP can significantly change the attention of the target model. These further confirm the effectiveness of the proposed method.\n# 5 CONCLUSIONS\nIn this paper, we investigate to learn universal adversarial perturbations that are capable of transferring across different VLP models, datasets and downstream tasks. To this end, we thoroughly study the factors that might influence the utility and transferability of the UAPs. Based on the findings, we propose a novel Effective and Transferable Universal Adversarial Attack (ETU) method. The proposed method achieves effective attacks by comprehensively considering the characteristics of UAPs and complex multi-modal interactions. Local utility enhancement of UAPs and a novel ScMix data augmentation are designed to ensure the performance of the proposed method. We test the proposed methods across different VLP models, downstream tasks and datasets, where promising results demonstrate the superiority of the proposed method.\n# 6 ACKNOWLEDGMENTS\nThis work was partially supported by Australian Research Council Discovery Project (DP230101196, CE200100025).\n# REFERENCES\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: visual question answering. In Proceedings of the IEEE International Conference on Computer Vision. 2425\u20132433. [2] Shumeet Baluja and Ian Fischer. 2018. Learning to attack: adversarial transformation networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. [3] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In Proceedings of the IEEE Symposium on Security and Privacy. 39\u201357. [4] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2015. Microsoft coco captions: data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015). [5] Zhi Chen, Pengfei Zhang, Jingjing Li, Sen Wang, and Zi Huang. 2023. Zeroshot learning by harnessing adversarial samples. In Proceedings of the ACM International Conference on Multimedia. 4138\u20134146. [6] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boosting adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 9185\u20139193. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations. [8] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014). [9] Bangyan He, Xiaojun Jia, Siyuan Liang, Tianrui Lou, Yang Liu, and Xiaochun Cao. 2023. SA-Attack: improving adversarial transferability of vision-Language pretraining models via self-augmentation. arXiv preprint arXiv:2312.04913 (2023). [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 770\u2013778. [11] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. 2016. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533 (2016). [12] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the International Conference on Machine Learning. PMLR, 12888\u201312900. [13] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: vision and language representation learning with momentum distillation. Advances in neural information processing systems 34 (2021), 9694\u20139705. [14] Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft. 2019. Nesterov accelerated gradient and scale invariance for adversarial attacks. In Proceedings of the International Conference on Learning Representations. [15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: common objects in context. In Proceedings of the European Conference on Computer Vision. 740\u2013755. [16] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving into transferable adversarial examples and black-box attacks. In Proceedings of the International Conference on Learning Representations. [17] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE International Conference on Computer Vision. 2125\u20132134. [18] Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, and Feng Zheng. 2023. Set-level guidance attack: boosting adversarial transferability of vision-language pre-training models. In Proceedings of the IEEE International Conference on Computer Vision. 102\u2013111. [19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards deep learning models resistant to adversarial attacks. In Proceedings of the International Conference on Learning Representations. [20] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1765\u20131773. [21] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277 (2016).\n[22] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE International Conference on Computer Vision. 2641\u20132649. [23] Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. 2018. Generative adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4422\u20134431. [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning. PMLR, 8748\u2013 8763. [25] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision. 618\u2013626. [26] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013). [27] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations. [28] Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He. 2021. Admix: enhancing the transferability of adversarial attacks. In Proceedings of the IEEE International Conference on Computer Vision. 16158\u201316167. [29] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang. 2020. A unified approach to interpreting and boosting adversarial transferability. In Proceedings of the International Conference on Learning Representations. [30] Youze Wang, Wenbo Hu, Yinpeng Dong, and Richang Hong. 2023. Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning. arXiv preprint arXiv:2308.12636 (2023). [31] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. 2019. Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2730\u20132739. [32] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019. Visual entailment: a novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706 (2019). [33] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. 2022. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15671\u201315680. [34] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. 2016. Modeling context in referring expressions. In Proceedings of the European Conference on Computer Vision. Springer, 69\u201385. [35] Jiaming Zhang, Qi Yi, and Jitao Sang. 2022. Towards adversarial attack on visionlanguage pre-training models. In Proceedings of the ACM International Conference on Multimedia. 5005\u20135013. [36] Peng-Fei Zhang, Guangdong Bai, Hongzhi Yin, and Zi Huang. 2023. Proactive privacy-preserving learning for cross-modal retrieval. ACM Transactions on Information Systems 41, 2 (2023), 1\u201323. [37] Peng-Fei Zhang, Zi Huang, and Xin-Shun Xu. 2021. Proactive privacy-preserving learning for retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 3369\u20133376. [38] Peng-Fei Zhang, Yang Li, Zi Huang, and Xin-Shun Xu. 2021. Aggregation-based graph convolutional hashing for unsupervised cross-modal retrieval. IEEE Transactions on Multimedia 24 (2021), 466\u2013479. [39] Peng-Fei Zhang, Yang Li, Zi Huang, and Hongzhi Yin. 2021. Privacy protection in deep multi-modal retrieval. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval. 634\u2013643. [40] Peng-Fei Zhang, Yadan Luo, Zi Huang, Xin-Shun Xu, and Jingkuan Song. 2021. High-order nonlocal Hashing for unsupervised cross-modal retrieval. World Wide Web 24 (2021), 563\u2013583. [41] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. 2020. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 13041\u201313049. [42] Ziqi Zhou, Shengshan Hu, Minghui Li, Hangtao Zhang, Yechao Zhang, and Hai Jin. 2023. Advclip: downstream-agnostic adversarial examples in multimodal contrastive learning. In Proceedings of the ACM International Conference on Multimedia. 6311\u20136320.\n",
    "paper_type": "method",
    "attri": {
        "background": "Vision-language pre-trained models have become essential in various tasks, yet they struggle with adversarial vulnerabilities. Existing methods focus on instance-specific perturbations, lacking generalization and incurring high computational costs. This paper proposes a novel method to generate Universal Adversarial Perturbations (UAPs) that are effective across different models and tasks.",
        "problem": {
            "definition": "The challenge addressed is the creation of universal adversarial perturbations that can transfer across various vision-language pre-trained models without requiring additional computation for each new instance.",
            "key obstacle": "The main difficulty lies in the need for UAPs to be independent of individual sample characteristics while effectively disrupting multi-modal interactions inherent in vision-language tasks."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that VLP models exhibit similar vulnerabilities to specific patterns of perturbations, suggesting that a universal approach could be effective.",
            "opinion": "The proposed Effective and Transferable Universal Adversarial Attack (ETU) method aims to generate UAPs that mislead VLP models across various tasks by considering both global and local utility.",
            "innovation": "The key innovation is the simultaneous optimization of both global and local utilities of UAPs, along with a novel data augmentation technique, ScMix, which enhances the diversity of training data while preserving semantics."
        },
        "method": {
            "method name": "Effective and Transferable Universal Adversarial Attack",
            "method abbreviation": "ETU",
            "method definition": "ETU generates UAPs that can mislead various VLP models by optimizing perturbations in a black-box setting without prior knowledge of model architectures or datasets.",
            "method description": "The method learns UAPs through a combination of local utility reinforcement and a novel data augmentation strategy, ScMix, to enhance transferability and effectiveness.",
            "method steps": [
                "Initialize UAP perturbation.",
                "Augment dataset using ScMix.",
                "Optimize UAP by maximizing the dissimilarity between perturbed and original paired data."
            ],
            "principle": "The effectiveness of the ETU method stems from its ability to leverage multi-modal interactions and optimize both local and global properties of UAPs, which enhances their transferability across different models."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three vision-language tasks using datasets such as Flickr30K and MSCOCO, focusing on models like CLIP, ALBEF, TCL, and BLIP.",
            "evaluation method": "The performance of the proposed method was assessed using the Attack Success Rate (ASR), measuring the percentage of adversarial examples that successfully misled the models."
        },
        "conclusion": "The ETU method demonstrates significant improvements in generating effective and transferable UAPs across various vision-language pre-trained models, contributing to the understanding of adversarial robustness in multi-modal settings.",
        "discussion": {
            "advantage": "The proposed approach stands out due to its ability to generate UAPs that are effective across multiple models and tasks without requiring extensive retraining.",
            "limitation": "The method may face challenges in scenarios where the perturbation magnitude significantly affects data quality, potentially leading to easier detection of adversarial examples.",
            "future work": "Future research could explore further enhancements in the transferability of UAPs and investigate the application of the proposed method in more complex multi-modal scenarios."
        },
        "other info": {
            "acknowledgments": "This work was partially supported by Australian Research Council Discovery Project (DP230101196, CE200100025)."
        }
    },
    "mount_outline": [
        {
            "section number": "1",
            "key information": "The paper discusses the significance of adversarial vulnerabilities in vision-language pre-trained models, which can relate to the broader impact of AI in analyzing complex data in healthcare."
        },
        {
            "section number": "6",
            "key information": "The paper addresses challenges such as the need for Universal Adversarial Perturbations (UAPs) that are effective across different models, which aligns with the discussion on limitations of AI technologies in healthcare."
        },
        {
            "section number": "6.1",
            "key information": "The discussion on ethical and privacy issues associated with adversarial attacks can be related to the ethical concerns in AI applications in healthcare."
        },
        {
            "section number": "7",
            "key information": "The paper suggests future research directions to enhance the transferability of UAPs, which can be connected to potential advancements in AI applications in healthcare."
        },
        {
            "section number": "7.1",
            "key information": "Future research could focus on improving the robustness of models against adversarial attacks, relevant to addressing bias and ethical considerations in AI healthcare applications."
        }
    ],
    "similarity_score": 0.5330550686243739,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0928_,arti/papers/Universal Adversarial Perturbations for Vision-Language Pre-trained Models.json"
}