{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2308.08361",
    "title": "KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution",
    "abstract": "Dynamic convolution learns a linear mixture of n static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by n times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of n (e.g., n > 100 instead of typical setting n < 10) to push forward the performance boundary. In this paper, we propose KernelWarehouse, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of \"kernels\" and \"assembling kernels\" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter dependencies within the same layer and across successive layers via tactful kernel partition and warehouse sharing. Specifically, KernelWarehouse sequentially divides a static kernel at any convolutional layer of a ConvNet into m disjoint kernel cells having the same dimensions first, and then computes each kernel cell as a linear mixture based on a predefined \"warehouse\" consisting of n kernel cells (e.g., n = 108) which is also shared to multiple neighboring convolutional layers, and finally replaces the static kernel by assembling its corresponding m mixtures in order, yielding a high degree of freedom to fit a desired parameter budget. To facilitate the learning of the attentions for summing up kernel cells, we also present a new attention function. We validate our method on ImageNet and MS-COCO datasets with different ConvNet architectures, and show that it attains state-of-the-art results. For instance, the ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny model trained with KernelWarehouse on ImageNet reaches 76.05%|81.05%|75.92%|82.51% top-1 accuracy. Thanks to its flexible design, KernelWarehouse can even reduce the model size of a ConvNet while improving the accuracy, e.g., our ResNet18 model with 36.45%|65.10% parameter reduction to the baseline shows 2.89%|2.29% absolute improvement to top-1 accuracy. Code and pre-trained models are available at https://github.com/OSVAI/KernelWarehouse.",
    "bib_name": "li2023kernelwarehouseparameterefficientdynamicconvolution",
    "md_text": "# KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution\nChao Li\u2217 Intel Labs China chao.li3@intel.com Anbang Yao\u2020 Intel Labs China anbang.yao@intel.com\n# Abstract\nDynamic convolution learns a linear mixture of n static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by n times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of n (e.g., n > 100 instead of typical setting n < 10) to push forward the performance boundary. In this paper, we propose KernelWarehouse, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of \"kernels\" and \"assembling kernels\" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter dependencies within the same layer and across successive layers via tactful kernel partition and warehouse sharing. Specifically, KernelWarehouse sequentially divides a static kernel at any convolutional layer of a ConvNet into m disjoint kernel cells having the same dimensions first, and then computes each kernel cell as a linear mixture based on a predefined \"warehouse\" consisting of n kernel cells (e.g., n = 108) which is also shared to multiple neighboring convolutional layers, and finally replaces the static kernel by assembling its corresponding m mixtures in order, yielding a high degree of freedom to fit a desired parameter budget. To facilitate the learning of the attentions for summing up kernel cells, we also present a new attention function. We validate our method on ImageNet and MS-COCO datasets with different ConvNet architectures, and show that it attains state-of-the-art results. For instance, the ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny model trained with KernelWarehouse on ImageNet reaches 76.05%|81.05%|75.92%|82.51% top-1 accuracy. Thanks to its flexible design, KernelWarehouse can even reduce the model size of a ConvNet while improving the accuracy, e.g., our ResNet18 model with 36.45%|65.10% parameter reduction to the baseline shows 2.89%|2.29% absolute improvement to top-1 accuracy. Code and pre-trained models are available at https://github.com/OSVAI/KernelWarehouse.\n# 1 Introduction\nConvolution is a fundamental operation in modern convolutional neural networks (ConvNets) [1\u20136 In a convolutional layer, normal convolution y = W \u2217x computes the output y by applying th same convolutional kernel W defined as a set of convolutional filters to every input sample x. Fo brevity, we refer to \"convolutional kernel\" as \"kernel\" and omit the bias term throughout this pape\nAnbang Yao\u2020 Intel Labs China anbang.yao@intel.com\nAlthough the efficacy of normal convolution is extensively validated with various types of ConvNets in numerous computer vision applications, recent progress in the efficient ConvNet architecture design shows that dynamic convolution, also known as CondConv in [7] and DY-Conv in [8], achieves large performance gains. The basic idea of dynamic convolution is to replace the static kernel in normal convolution by a linear mixture of n same dimensioned kernels W = \u03b11W1 + ... + \u03b1nWn, where \u03b11, ..., \u03b1n are scalar attentions generated by an input-dependent attention module. Benefiting from the additive property of n kernels W1, ..., Wn and compact attention module designs, dynamic convolution improves the feature learning ability with little extra multiply-add cost. However, it increases the number of convolutional parameters by n times, which leads to a huge rise in model size because the convolutional layers of a modern ConvNet occupy the vast majority of parameters. To alleviate this problem, DCD [9] directly learns a base kernel and a sparse residual to approximate dynamic convolution via matrix decomposition. This coarse approximation abandons the basic mixture learning paradigm, and cannot well retain the representation power of dynamic convolution when n becomes large. Recently, ODConv [10] presents a more powerful attention module to dynamically weight static kernels along different dimensions instead of one single dimension, which can get competitive performance with a reduced number of kernels. Under the same setting of n, ODConv has more parameters than vanilla dynamic convolution. In brief, existing dynamic convolution methods based on the mixture learning paradigm are not parameter-efficient, and they typically set n = 8 [7] or n = 4 [8, 10]. More importantly, a plain fact is that the improved capacity of a ConvNet constructed with dynamic convolution comes from increasing the number of kernels per convolutional layer facilitated by the attention mechanism. This causes an intrinsic conflict between the desired model size and capacity, which prevents researchers to explore the performance boundary of dynamic convolution with a significantly large kernel number (e.g., n > 100). In this paper, we attempt to break down this barrier, and shed new insights on research in dynamic convolution. To this end, we present KernelWarehouse (Figure 1), a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. The formulation of KernelWarehouse is inspired by two observations. On the one hand, we note that existing dynamic convolution methods treat all parameters in a regular convolutional layer as a kernel, and increase the kernel number from 1 to n, and use their attention modules to assemble n kernels into a linearly mixed kernel. Though straightforward and effective, they pay no attention to parameter dependencies within the static kernel at a convolutional layer. On the other hand, we notice that existing dynamic convolution methods allocate a different set of n kernels for every individual convolutional layer of a ConvNet, ignoring parameter dependencies across successive layers. We hypothesize that the barrier discussed above can be removed by way of flexibly leveraging these two types of convolutional parameter dependencies to reformulate dynamic convolution. Driven by this analysis, we introduce two simple strategies when formulating KernelWarehouse, namely kernel partition and warehouse sharing. By them, we redefine the basic concepts of \"kernels\" and \"assembling kernels\" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly, taking advantage of the aforementioned parameter dependencies as easy as possible. With kernel partition, the static kernel for a regular convolutional layer is sequentially divided into m disjoint kernel cells of the same dimensions, and then the linear mixture can be defined in terms of much smaller kernel cells instead of holistic kernels. Specifically, each of m kernel cells is represented as a linear mixture based on a predefined \"warehouse\" consisting of n kernel cells (e.g., n = 108), and the static kernel will be replaced by assembling its corresponding m mixtures in order. With warehouse sharing, multiple neighboring convolutional layers of a ConvNet can share the same warehouse as long as the same kernel cell size is used in the kernel partition process, further enhancing its parameter efficiency and representation power. Nevertheless, with a significantly large value of n, the optimization of KernelWarehouse becomes more challenging than existing methods. We find that popular attention functions for dynamic convolution [7, 8, 10] do not work well under this circumstance. We solve this problem by designing a new and simple attention function. These simple concept shifts provide a high degree of flexibility to balance parameter efficiency and representation power of KernelWarehouse under different parameter budgets. As a drop-in replacement of normal convolution, KernelWarehouse can be easily used to different ConvNet architectures. Extensive experiments on ImageNet and MS-COCO show that our method\n# 2 Related Work\nConvNet Architectures. In the past decade, a lot of pioneering ConvNet architectures such as AlexNet [1], VGGNet [11], GoogLeNet [2], ResNet [3], DenseNet [12], ResNeXt [13] and RegNet [14] have been presented, which significantly advance the performance on the ImageNet benchmark. Around the same time, some lightweight architectures like MobileNet [4, 5, 15], ShuffleNet [16] and EfficientNet [17] have been designed for resource-constrained applications. Recently, Liu et al. presented ConvNeXt [6] whose performance can match newly emerging vision transformers [18, 19]. As a plug-and-play design, our method could be used to improve their performance. Feature Recalibration. An effective way to enhance the capacity of a ConvNet is feature recalibration. It relies on attention mechanisms to adaptively refine the feature maps learnt by a convolutional block. Popular feature recalibration modules such as RAN [20], SE [21], BAM [22], CBAM [23], GE [24], SRM [25] and ECA [26] focus on different design aspects: using channel attention, or spatial attention, or hybrid attention to emphasize important features and suppress necessary ones. Differing from feature recalibration in multiple aspects, dynamic convolution replaces the static kernel of a convolutional layer by a linear mixture of n kernels weighted with the attention mechanism. Dynamic Weight Networks. In the field of deep learning, many research efforts have been made on developing effective methods to generate the weights for a neural network. Jaderberg et al. [27] propose a Spatial Transformer module which uses a localisation network that predicts the feature transformation parameters conditioned on the learnt feature itself. Dynamic Filter Network [28] and Kernel Prediction Networks [29, 30] introduce two filter generation frameworks which share the same idea: using a deep neural network to generate sample-adaptive filters conditioned on the input. Based on this idea, DynamoNet [31] uses dynamically generated motion filters to boost human action recognition in videos. CARAFE [32] and Involution [33] further extend this idea by designing efficient generation modules to predict the weights for extracting informative features. By connecting this idea with SE, WeightNet [34], CGC [35] and WE [36] design different attention modules to adjust the weights in convolutional layers of a ConvNet. Unlike these methods, hypernetwork [37] uses a small network to generate the weights for a larger recurrent network, and MetaNet [38] introduces a meta learning model consisting of a base learner and a meta learner, allowing the learnt network for rapid generalization across different tasks. This paper focuses on advancing dynamic convolution research, which differs from these works in formulation.\n# 3 Method\nFigure 1 shows a schematic illustration of our method called KernelWarehouse, which has three key components, namely kernel partition, warehouse sharing and a new attention function. Next, we describe the formulation of KernelWarehouse and clarify these components.\n# 3.1 Formulation of KernelWarehouse\nFor a convolutional layer, let x \u2208Rh\u00d7w\u00d7c and y \u2208Rh\u00d7w\u00d7f be the input having c feature channels with the resolution h \u00d7 w and the output having f feature channels with the same resolution to the input, respectively. Normal convolution y = W \u2217x uses a single static kernel W \u2208Rk\u00d7k\u00d7c\u00d7f consisting of f convolutional filters with the spatial size k \u00d7 k, while dynamic convolution replaces this static kernel by W = \u03b11W1 + ... + \u03b1nWn, a linear mixture of n same dimensioned static kernels weighted with their input-dependent scalar attentions \u03b11, ..., \u03b1n. Like existing dynamic convolution methods [7, 8, 10], KernelWarehouse also adopts the attentive mixture learning paradigm. Unlike them, KernelWarehouse applies the attentive mixture learning paradigm to a static kernel in dense local scales instead of a single holistic scale via kernel partition and warehouse sharing. Kernel Partition. The basic idea of kernel partition is to reduce kernel dimension and increase kernel number via explicitly enhancing parameter dependencies within the same convolutional layer. First, we sequentially divide the static kernel W at a regular convolutional layer into m disjoint parts w1,...,wm called \"kernel cells\" that have the same dimensions. For brevity, here we omit to define kernel cell dimensions. Kernel partition can be defined as W = w1 \u222a... \u222awm, and \u2200i, j \u2208{1, ..., m}, i \u0338= j, wi \u2229wj = \u2205. (1) After kernel partition, we treat kernel cells w1,...,wm as \"local kernels\", and define a \"warehouse\" containing n kernel cells E = {e1, ..., en}, where e1, ..., en have the same dimensions as w1,...,wm.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33d4/33d4d113-690c-4544-a5d3-9717a94afbd6.png\" style=\"width: 50%;\"></div>\nwhere \u03b1i1, ..., \u03b1in are the scalar attentions generated by an attention module \u03d5(x) conditioned on the input x. Finally, the static kernel W in a regular convolutional layer is replaced by assembling its corresponding m linear mixtures in order. Note that the dimensions of a kernel cell can be much smaller than the dimensions of the static kernel W (e.g., when m = 16, the number of parameters in a kernel cell is 1/16 to that of the static kernel W). Under the same parameter budget, this allows a warehouse can have a large setting of n (e.g., n > 100), in sharp contrast to existing dynamic convolution methods which define the linear mixture in terms of n \"holistic kernels\" and typically uses a much smaller setting of n (e.g., n < 10) due to its parameter-inefficient shortcoming. Warehouse Sharing. Following the simple design philosophy of kernel partition, the main goal of warehouse sharing is to further improve parameter efficiency and representation power of KernelWarehouse through explicitly enhancing parameter dependencies across successive convolutional layers. Specifically, we share a warehouse E = {e1, ..., en} to represent every kernel cell at l neighboring convolutional layers in the same-stage building blocks of a ConvNet, allowing it can use a much larger setting of n against kernel partition. This is mostly easy, as modern ConvNets such as ResNet [3], MobileNet [4, 5, 15] and ConvNeXt [6] typically adopt a modular design scheme in which static kernels in the same-stage convolutional layers usually have the same or similar dimensions. In implementation, given a desired convolutional parameter budget, we simply use common dimension divisors for l static kernels as the uniform kernel cell dimensions for kernel partition, which naturally determine the values of m and n. On the other hand, parameter dependencies across neighboring convolutional layers are also critical in strengthening the capacity of a ConvNet (see Table 5). Parameter Efficiency and Representation Power. Let n be the number of kernel cells in a warehouse shared to l convolutional layers of a ConvNet, and let mt be the total number of kernel cells in these l convolutional layers (mt = m, when l = 1). Then, b = n/mt can be server as a scaling factor to indicate the convolutional parameter budget of KernelWarehouse relative to normal convolution. Here, we do not consider the number of parameters in the attention module \u03d5(x) which generates nmt scalar attentions, as it is much smaller than the total number of parameters for normal convolution at l convolutional layers. In implementation, we use the same value of b to all convolutional layers of every ConvNet. Under such a condition, we can see that KernelWarehouse can easily scale up or scale down the model size of a ConvNet by changing b. Compared to normal convolution: (1) when b < 1, KernelWarehouse tends to get the reduced model size; (2) when b = 1, KernelWarehouse tends to get the similar model size; (3) when b > 1, KernelWarehouse tends to get the increased model size. Intriguingly, given a desired value of b, a proper and large value of n can be obtained by simply changing m, providing a representation power guarantee for KernelWarehouse. Therefore,\n(2)\nKernelWarehouse can strike a favorable trade-off between parameter efficiency and representation power, under different convolutional parameter budgets in terms of b.\nKernelWarehouse can strike a favorable trade-off between parameter efficiency and representation power, under different convolutional parameter budgets in terms of b. KernelWarehouse vs. Dynamic Convolution. According to its above formulation, KernelWarehouse will degenerate into vanilla dynamic convolution [7, 8] when uniformly setting m = 1 in kernel partition (i.e., all kernel cells in each warehouse have the same dimensions as the static kernel W in normal convolution) and l = 1 in warehouse sharing (i.e., each warehouse is limited to its specific convolutional layer). Therefore, KernelWarehouse is a more general form of dynamic convolution.\nKernelWarehouse vs. Dynamic Convolution. According to its above formulation, KernelWarehouse will degenerate into vanilla dynamic convolution [7, 8] when uniformly setting m = 1 in kernel partition (i.e., all kernel cells in each warehouse have the same dimensions as the static kernel W in normal convolution) and l = 1 in warehouse sharing (i.e., each warehouse is limited to its specific convolutional layer). Therefore, KernelWarehouse is a more general form of dynamic convolution.\n# 3.2 Attention Module of KernelWarehouse\nFollowing existing dynamic convolution methods [7, 8, 10], KernelWarehouse also adopts a compact SE-typed structure as the attention module \u03d5(x) (illustrated in Figure 1) to generate attentions for weighting kernel cells in a warehouse. For any convolutional layer with a static kernel W, it starts with a channel-wise global average pooling (GAP) operation that maps the input x into a feature vector, followed by a fully connected (FC) layer, a rectified linear unit (ReLU), another FC layer, and a new attention function. The first FC layer reduces the length of the feature vector by 16, and the second FC layer generates m sets of n feature logits in parallel which are finally normalized by the attention function set by set. Attention Function. Comparatively, the optimization of KernelWarehouse is more challenging, mainly due to its new formulation. For the linear mixture in KernelWarehouse, (1) the number of kernel cells in a warehouse is much larger (e.g., n = 108 vs. n < 10); (2) a warehouse is not only shared to represent each of m kernel cells for a specific convolutional layer of a ConvNet, but also is shared to represent every kernel cell for the other l \u22121 same-stage convolutional layers. Under this circumstance, a proper choice of the attention function is critical. An ideal attention function should have the property of easily allocating diverse attentions for all linear mixtures simultaneously, making the mixed kernel cells at l convolutional layers can learn informative features hieratically. However, we experimentally find that popular attention functions such as Sigmoid and Softmax do not work well for KernelWarehouse. Even with the temperature annealing [8], they get worse results than vanilla dynamic convolution (see Table 2 and Table 8). To address this joint optimization problem, we present a new attention function. For ith kernel cell in the static kernel W, let zi1, ..., zin be the feature logits generated by the second FC layer, then our attention function is defined as\n\ufffd  || where \u03c4 is a temperature which linearly reduces from 1 to 0 in the early training stage; zij \ufffdn p=1 |zip| is a linear normalization function which can have negative attention outputs that are essential to encourage the network to learn adversarial attention relationships among all linear mixtures sharing the same warehouse; \u03b2ij is a binary value (0 or 1) which is used for initializing the attentions. Note that the settings of \u03b2ij at l convolutional layers are very important, which assure the shared warehouse can assign (1) at least one specified kernel cell (\u03b2ij = 1) to every linear mixture, given a desired convolutional parameter budget b \u22651, and (2) at most one specific kernel cell (\u03b2ij = 1) to every linear mixture, given b < 1. In implementation, we adopt a simple strategy to assign one of the total n kernel cells in a shared warehouse to each of mt linear mixtures at l convolutional layers without repetition. When n is less than mt, we let the remaining linear mixtures always have \u03b2ij = 0 once n kernel cells are used up. In the Appendix, we provide visualization examples to illustrate this strategy, and a set of ablative experiments to compare it with other alternatives. Extensive experiments validate the effectiveness of our attention function (see Figure 3 and Table 8).\n# 4 Experiments\nIn this section, we conduct comprehensive experiments on image classification, object detection and instance segmentation to evaluate the effectiveness of our KernelWarehouse (\"KW\" for short), compare it with other attention based methods, and provide lots of ablations to study its design.\n# 4.1 Image Classification on ImageNet\nOur main experiments are conducted on ImageNet dataset [39], which consists of over 1.2 million training images and 50,000 validation images with 1,000 object categories.\n(3)\n<div style=\"text-align: center;\">Table 1: Results comparison on ImageNet with the ResNet18 backbone using the traditional training strategy. Best results are bolded.</div>\nTable 1: Results comparison on ImageNet with the ResNet18 backbone using the traditional training strategy. Best results are bolded.\nModels\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nResNet18\n11.69M\n70.25\n89.38\n+ SE [21]\n11.78M\n70.98 (\u21910.73)\n90.03 (\u21910.65)\n+ CBAM [23]\n11.78M\n71.01 (\u21910.76)\n89.85 (\u21910.47)\n+ ECA [26]\n11.69M\n70.60 (\u21910.35)\n89.68 (\u21910.30)\n+ CGC [35]\n11.69M\n71.60 (\u21911.35)\n90.35 (\u21910.97)\n+ WeightNet [35]\n11.93M\n71.56 (\u21911.31)\n90.38 (\u21911.00)\n+ DCD [9]\n14.70M\n72.33 (\u21912.08)\n90.65 (\u21911.27)\n+ CondConv (8\u00d7) [7]\n81.35M\n71.99 (\u21911.74)\n90.27 (\u21910.89)\n+ DY-Conv (4\u00d7) [8]\n45.47M\n72.76 (\u21912.51)\n90.79 (\u21911.41)\n+ ODConv (4\u00d7) [10]\n44.90M\n73.97 (\u21913.72)\n91.35 (\u21911.97)\n+ KW (1/2\u00d7)\n7.43M\n72.81 (\u21912.56)\n90.66 (\u21911.28)\n+ KW (1\u00d7)\n11.93M\n73.67 (\u21913.42)\n91.17 (\u21911.79)\n+ KW (2\u00d7)\n23.24M\n74.03 (\u21913.78)\n91.37 (\u21911.99)\n+ KW (4\u00d7)\n45.86M\n73.54 (\u21913.29)\n90.94 (\u21911.56)\nTable 2: Results comparison on ImageNet with the ResNet18, ResNet50 and ConvNeXt-Tiny backbones trained using the advanced training strategy proposed in ConvNeXt. Best results are bolded.\nModels\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nResNet18\n11.69M\n70.44\n89.72\n+ DY-Conv (4\u00d7)\n45.47M\n73.82 (\u21913.38)\n91.48 (\u21911.76)\n+ ODConv (4\u00d7)\n44.90M\n74.45 (\u21914.01)\n91.67 (\u21911.95)\n+ KW (1/4\u00d7)\n4.08M\n72.73 (\u21912.29)\n90.83 (\u21911.11)\n+ KW (1/2\u00d7)\n7.43M\n73.33 (\u21912.89)\n91.42 (\u21911.70)\n+ KW (1\u00d7)\n11.93M\n74.77 (\u21914.33)\n92.13 (\u21912.41)\n+ KW (2\u00d7)\n23.24M\n75.19 (\u21914.75)\n92.18 (\u21912.46)\n+ KW (4\u00d7)\n45.86M\n76.05 (\u21915.61)\n92.68 (\u21912.96)\nResNet50\n25.56M\n78.44\n94.24\n+ DY-Conv (4\u00d7)\n100.88M\n79.00 (\u21910.56)\n94.27 (\u21910.03)\n+ ODConv (4\u00d7)\n90.67M\n80.62 (\u21912.18)\n95.16 (\u21910.92)\n+ KW (1/2\u00d7)\n17.64M\n79.30 (\u21910.86)\n94.71 (\u21910.47)\n+ KW (1\u00d7)\n28.05M\n80.38 (\u21911.94)\n95.19 (\u21910.95)\n+ KW (4\u00d7)\n102.02M\n81.05 (\u21912.61)\n95.21 (\u21910.97)\nConvNeXt-Tiny\n28.59M\n82.07\n95.86\n+ KW (1\u00d7)\n39.37M\n82.51 (\u21910.44)\n96.07 (\u21910.21)\nTable 3:\nResu\nwith the Mobile\ntrained for 150 e\nModels\nMobileNetV2 (1.0\u00d7)\n+ DY-Conv (4\u00d7)\n+ ODConv (4\u00d7)\n+ KW (1/2\u00d7)\n+ KW (1\u00d7)\n+ KW (4\u00d7)\nMobileNetV2 (0.5\u00d7)\n+ DY-Conv (4\u00d7)\n+ ODConv (4\u00d7)\n+ KW (1/2\u00d7)\n+ KW (1\u00d7)\n+ KW (4\u00d7)\nModels\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nResNet18\n11.69M\n70.44\n89.72\n+ DY-Conv (4\u00d7)\n45.47M\n73.82 (\u21913.38)\n91.48 (\u21911.76)\n+ ODConv (4\u00d7)\n44.90M\n74.45 (\u21914.01)\n91.67 (\u21911.95)\n+ KW (1/4\u00d7)\n4.08M\n72.73 (\u21912.29)\n90.83 (\u21911.11)\n+ KW (1/2\u00d7)\n7.43M\n73.33 (\u21912.89)\n91.42 (\u21911.70)\n+ KW (1\u00d7)\n11.93M\n74.77 (\u21914.33)\n92.13 (\u21912.41)\n+ KW (2\u00d7)\n23.24M\n75.19 (\u21914.75)\n92.18 (\u21912.46)\n+ KW (4\u00d7)\n45.86M\n76.05 (\u21915.61)\n92.68 (\u21912.96)\nResNet50\n25.56M\n78.44\n94.24\n+ DY-Conv (4\u00d7)\n100.88M\n79.00 (\u21910.56)\n94.27 (\u21910.03)\n+ ODConv (4\u00d7)\n90.67M\n80.62 (\u21912.18)\n95.16 (\u21910.92)\n+ KW (1/2\u00d7)\n17.64M\n79.30 (\u21910.86)\n94.71 (\u21910.47)\n+ KW (1\u00d7)\n28.05M\n80.38 (\u21911.94)\n95.19 (\u21910.95)\n+ KW (4\u00d7)\n102.02M\n81.05 (\u21912.61)\n95.21 (\u21910.97)\nConvNeXt-Tiny\n28.59M\n82.07\n95.86\n+ KW (1\u00d7)\n39.37M\n82.51 (\u21910.44)\n96.07 (\u21910.21)\nTable 3:\nResults comparison on ImageNet\nwith the MobileNetV2 (1.0\u00d7, 0.5\u00d7) backbones\ntrained for 150 epochs. Best results are bolded.\nModels\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nMobileNetV2 (1.0\u00d7)\n3.50M\n72.02\n90.43\n+ DY-Conv (4\u00d7)\n12.40M\n74.94 (\u21912.92)\n91.83 (\u21911.40)\n+ ODConv (4\u00d7)\n11.52M\n75.42 (\u21913.40)\n92.18 (\u21911.75)\n+ KW (1/2\u00d7)\n2.65M\n72.59 (\u21910.57)\n90.71 (\u21910.28)\n+ KW (1\u00d7)\n5.17M\n74.68 (\u21912.66)\n91.90 (\u21911.47)\n+ KW (4\u00d7)\n11.38M\n75.92 (\u21913.90)\n92.22 (\u21911.79)\nMobileNetV2 (0.5\u00d7)\n1.97M\n64.30\n85.21\n+ DY-Conv (4\u00d7)\n4.57M\n69.05 (\u21914.75)\n88.37 (\u21913.16)\n+ ODConv (4\u00d7)\n4.44M\n70.01 (\u21915.71)\n89.01 (\u21913.80)\n+ KW (1/2\u00d7)\n1.47M\n65.19 (\u21910.89)\n85.98 (\u21910.77)\n+ KW (1\u00d7)\n2.85M\n68.29 (\u21913.99)\n87.93 (\u21912.72)\n+ KW (4\u00d7)\n4.65M\n70.26 (\u21915.96)\n89.19 (\u21913.98)\nConvNet Backbones. We select backbones from MobileNetV2 [5], ResNet [6] and ConvNeXt [5] families for experiments, including both lightweight networks and larger ones. Specifically, we use MobileNetV2 (1.0\u00d7), MobileNetV2 (0.5\u00d7), ResNet18, ResNet50 and ConvNeXt-Tiny. Experimental Setup. In the experiments, we make comparisons of KernelWarehouse with related methods to demonstrate its effectiveness. On the ResNet18 backbone, we compare our design with various state-of-the-art attention based methods, including: (1) SE [21], CBAM [23] and ECA [26], which focus on recalibration of feature maps; (2) CGC [35] and WeightNet [34], which focus on adjusting convolutional weights; (3) CondConv [7], DY-Conv [8], DCD [9] and ODConv [10], which focus on dynamic convolution. As DY-Conv and ODConv are state-of-the-art dynamic convolution methods which achieve top performance in our comparative experiments on the ResNet18 and are closely related to KernelWarehouse, we select them as our key reference methods. We compare our method with them on all the other ConvNet backbones except ConvNeXt-Tiny (since there is no publicly available implementation of them on ConvNeXt). To make fair comparisons, all the methods are implemented using the public codes with the same settings for training and testing. In the experiments, we use b\u00d7 to denote the convolutional parameter budget of each dynamic convolution method relative to normal convolution, the values of n and m in KernelWarehouse, and the experimental details for each network are provided in the Appendix. Results Comparison on ResNets18 with the Traditional Training Strategy. We first use the traditional training strategy adopted by lots of previous studies such as DY-Conv [8], ODConv [10], where models are trained for 100 epochs. The results are shown in Table 1. It can be observed that the dynamic convolution methods (CondConv, DY-Conv and ODConv), which introduce obviously more extra parameters, tend to have better performance compared with other methods (SE, CBAM, ECA, CGC, WeightNet and DCD). Note that our KW (1/2\u00d7), which has 36.45% parameters less than the baseline, can even outperform all the above attention based methods except ODConv (4\u00d7), including CondConv (8\u00d7) and DY-Conv (4\u00d7) which increase the model size to about 6.96|3.89 times. Our KW (2\u00d7) achieves the best results, which further surpasses ODConv (4\u00d7) by 0.06% top-1 gain\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f85/6f85a3f2-d5e3-4345-bf21-f61c5f3ed16a.png\" style=\"width: 50%;\"></div>\nFigure 2: Curves of top-1 training accuracy and validation accuracy of ResNet18 models trained for 100 epochs on ImageNet with DY-Conv (4\u00d7), ODConv (4\u00d7) and KW (1\u00d7, 2\u00d7, 4\u00d7).\nTable 3: Results comparison on ImageNet with the MobileNetV2 (1.0\u00d7, 0.5\u00d7) backbones trained for 150 epochs. Best results are bolded.\n<div style=\"text-align: center;\">Table 4: Results comparison on the MS-COCO 2017 validation set. Best results are bolded.</div>\nDetectors\nBackbone Models\nObject Detection\nInstance Segmentation\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nMask R-CNN\nResNet50\n39.6\n61.6\n43.3\n24.4\n43.7\n50.0\n36.4\n58.7\n38.6\n20.4\n40.4\n48.4\n+ DY-Conv (4\u00d7)\n39.6\n62.1\n43.1\n24.7\n43.3\n50.5\n36.6\n59.1\n38.6\n20.9\n40.2\n49.1\n+ ODConv (4\u00d7)\n42.1\n65.1\n46.1\n27.2\n46.1\n53.9\n38.6\n61.6\n41.4\n23.1\n42.3\n52.0\n+ KW (1\u00d7)\n41.8\n64.5\n45.9\n26.6\n45.5\n53.0\n38.4\n61.4\n41.2\n22.2\n42.0\n51.6\n+ KW (4\u00d7)\n42.4\n65.4\n46.3\n27.2\n46.2\n54.6\n38.9\n62.0\n41.5\n22.7\n42.6\n53.1\nMobileNetV2 (1.0\u00d7)\n33.8\n55.2\n35.8\n19.7\n36.5\n44.4\n31.7\n52.4\n33.3\n16.4\n34.4\n43.7\n+ DY-Conv (4\u00d7)\n37.0\n58.6\n40.3\n21.9\n40.1\n47.9\n34.1\n55.7\n36.1\n18.6\n37.1\n46.3\n+ ODConv (4\u00d7)\n37.2\n59.4\n39.9\n22.6\n40.0\n48.0\n34.5\n56.4\n36.3\n19.3\n37.3\n46.8\n+ KW (1\u00d7)\n36.4\n58.3\n39.2\n22.0\n39.6\n47.0\n33.7\n55.1\n35.7\n18.9\n36.7\n45.6\n+ KW (4\u00d7)\n38.0\n60.0\n40.8\n23.1\n40.7\n50.0\n34.9\n56.6\n37.0\n19.4\n37.9\n47.8\nConvNeXt-Tiny\n43.4\n65.8\n47.7\n27.6\n46.8\n55.9\n39.7\n62.6\n42.4\n23.1\n43.1\n53.7\n+ KW (1\u00d7)\n44.7\n67.5\n48.9\n29.6\n48.2\n57.2\n40.6\n64.3\n43.6\n24.6\n44.1\n54.8\nwith roughly only half of its parameters. However, when we increase KW from 2\u00d7 to 4\u00d7, it shows a decline in top-1 gain from 3.78% to 3.29%. Figure 2 illustrates the training and validation accuracy curves of ResNet18 backbone with ODConv (4\u00d7) and our KW (1\u00d7, 2\u00d7, 4\u00d7). We can see that KW (2\u00d7) already gets higher training accuracy than ODConv (4\u00d7). While compared to KW (2\u00d7), KW (4\u00d7) further brings 2.79% improvement on training set but 0.49% drop on validation set. We consider the reason of validation accuracy decline is that KW (4\u00d7) largely enhances the capacity of ResNet18 backbone, but also suffers from potential overfitting, when using the traditional training strategy. Results Comparison on ResNets and ConvNeXts with the Advanced Training Strategy. With the traditional training strategy, our KW (1/2\u00d7, 1\u00d7, 2\u00d7) have shown promising results on the ResNet18 backbone. In order to further explore the potential of KernelWarehouse on large ConvNets with a larger value of b, we next adopt the training strategy proposed in ConvNeXt [6], with a longer training schedule (300 epochs) and more aggressive augmentations for comparisons on the ResNet18, ResNet50 and ConvNeXt-Tiny. From the results shown in Table 2, we can observe: (1) with the advanced training strategy, DY-Conv (4\u00d7), ODConv (4\u00d7) and our KW (1/2\u00d7, 1\u00d7, 4\u00d7) show better results and larger top-1 gains to baseline on the ResNet18 backbone, compared to the traditional training strategy. It can be seen that the advanced training strategy successfully alleviates the overfitting of KW (4\u00d7), which gets the best results, bringing an absolute top-1 gain of 5.61%. Even with 36.45%|65.10% parameter reduction, KW (1/2\u00d7)|KW(1/4\u00d7) brings 2.89%|2.29% top-1 accuracy gain to the baseline; (2) on the larger ResNet50 backbone, while the vanilla dynamic convolution method (DY-Conv) shows much lower performance gain, KW (1/2\u00d7, 1\u00d7, 4\u00d7) still bring great performance improvement to the baseline model. With 30.99% parameter reduction, KW (1/2\u00d7) improves ResNet50 backbone by a top-1 gain of 0.86%. Our KW (4\u00d7) consistently outperforms both DY-Conv (4\u00d7) and ODConv (4\u00d7) by 2.05%|0.43% top-1 gain. Beside ResNets, we also apply KernelWarehouse to the ConvNeXt-Tiny backbone to investigate its performance on the state-of-the-art ConvNet architecture. Our method can generalize well on ConvNeXt-Tiny, brings 0.44% top-1 gain to the baseline model with KW (1\u00d7). Results Comparison on MobileNets. We further apply KernelWarehouse to MobileNetV2 (1.0\u00d7, 0.5\u00d7) to validate its effectiveness on lightweight ConvNet architectures. Since the lightweight MobileNetV2 models have lower capacity compared to ResNet and ConvNeXt models, we don\u2019t use aggressive augmentations for MobileNetV2. The results are shown in Table 3. Our KernelWarehouse can strike a favorable trade-off between parameter efficiency and representation power for lightweight ConvNets as well as larger ones. Even on the lightweight MobileNetV2 (1.0\u00d7, 0.5\u00d7) with 3.50M|1.97M parameters, our KW (1/2\u00d7) can reduce the model size by 24.29%|25.38% while bringing top-1 gain of 0.55%|0.89%. Again, our KW (4\u00d7) obtains the best results on both MobileNetV2 (1.0\u00d7) and MobileNet (0.5\u00d7).\n# 4.2 Object Detection on MS-COCO\nTo evaluate the generalization capacity of our method on different tasks, we further conduct comparative experiments for object detection and instance segmentation on the MS-COCO 2017 dataset [40], which contains 118,000 training images and 5,000 validation images with 80 object categories. Experimental Setup. We adopt Mask R-CNN [41] as the detection framework, ResNet50 and MobileNetV2 (1.0\u00d7) built with different attention methods as the backbones which are pre-trained on ImageNet dataset. All the models are trained with standard 1\u00d7 schedule on MS-COCO dataset. For a fair comparison, we adopt the same settings including data processing pipeline and hyperparameters for all the models. Experimental details are described in the Appendix.\nTable 5: Ablation of KernelWarehouse with warehouse sharing between kernel cells within each stage, within each layer and without sharing.\nModels\nSharing Strategies\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nResNet18\n-\n11.69M\n70.44\n89.72\n+ KW (1\u00d7)\nWithin each stage\n11.93M\n74.77 (\u21914.33)\n92.13 (\u21912.41)\nWithin each layer\n11.81M\n74.34 (\u21913.90)\n91.82 (\u21912.10)\nWithout sharing\n11.78M\n72.49 (\u21912.05)\n90.81 (\u21911.09)\nModels\nAttentions Initialization Strategy\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nResNet18\n-\n11.69M\n70.44\n89.72\n+ KW (1\u00d7)\n\u2713\n11.93M\n74.77 (\u21914.33)\n92.13 (\u21912.41)\n-\n11.93M\n73.39 (\u21912.95)\n91.24 (\u21911.52)\nResults Comparison. The comparison results on Mask R-CNN with different backbones are shown in Table 4. For Mask R-CNN with ResNet50 backbone models, we observe a similar trend to the main experiments on ImageNet dataset: KW (4\u00d7) outperforms DY-Conv (4\u00d7) and ODConv (4\u00d7) on both object detection and instance segmentation tasks. Our KW (1\u00d7) brings an AP improvement of 2.2%|2.0% on object detection and instance segmentation tasks, which is on par with ODConv (4\u00d7). With the MobileNetV2 (1.0\u00d7) backbone, KernelWarehouse yields consistent high accuracy improvements to the baseline. Our KW (4\u00d7) achieves the best results. With the ConvNeXt-Tiny backbone, KW (1\u00d7) brings more obvious performance gain to the baseline model on MS-COCO dataset compared to that on ImageNet dataset, showing its higher capacity and good generalization ability to the downstream tasks.\n# 4.3 Ablation Studies\nFor a deeper understanding of KernelWarehouse, we further provide a lot of ablative experiments on the ImageNet dataset. All the models are trained with the training strategy proposed in ConvNeXt [6]. Warehouse Sharing with Different Ranges. To validate the effectiveness of warehouse sharing, we first perform ablative experiments on the ResNet18 backbone with different ranges of warehouse sharing. From the results shown in Table 5, we can see that when sharing warehouse in wider range, KernelWarehouse brings larger performance improvement to the baseline model. It indicates that explicitly enhancing parameter dependencies within the same convolutional layer and across successive layers both can strengthen the network capacity. Warehouse Sharing between Kernels with Different Dimensions. In the mainstream ConvNet designs, a convolutional block mostly contains several kernels having different dimensions (k \u00d7 k \u00d7 c \u00d7 f). We next perform ablative experiments on the ResNet50 backbone to explore the effect of warehouse sharing between kernels having different dimensions in convolutional blocks. Results are summarized in Table 6, showing that warehouse sharing between kernels having different dimensions performs better compared to warehouse sharing only between kernels having the same dimensions. Combining the results in Table 5 and Table 6, we can conclude that enhancing the warehouse sharing between more kernel cells in KernelWarehouse mostly leads to better performance. Effect of Kernel Partition. Using kernel partition, KernelWarehouse can apply denser kernel assembling with more kernel cells. In Table 7, we present the comparison results on the ResNet18 backbone about kernel partition. Without kernel partition, the top-1 gain for KernelWarehouse to the baseline sharply decreases from 4.33% to 0.05%, demonstrating its great importance to KernelWarehouse. Attention Function. Recall that KernelWarehouse uses a new attention function. In the experiments, we compare the performance of KernelWarehouse with different attention functions on the ResNet18 backbone. As shown in Table 8, the attention function plays an important role in KernelWarehouse, where the performance gap between our design zij/ \ufffdn p=1 |zip| and Softmax|Sigmoid\nTable 6: Ablation of KernelWarehouse with or without warehouse sharing between kernels having different dimensions in convolutional blocks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf8a/cf8a75f3-3a82-4a4f-8ce3-92a8b079456c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse. The results are obtained from the pre-trained ResNet18 backbone with KW (1\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in. reaches 2.10%|2.68%. Our design also outperforms the function max(zij, 0)/ \ufffdn p=1 |zip| by 2.03%, validating the importance of introducing negative values for scalar attentions to encourage the network to learn adversarial attention relationships.</div>\n<div style=\"text-align: center;\">Figure 3: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse. The results are obtained from the pre-trained ResNet18 backbone with KW (1\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in.</div>\nAttentions Initialization Strategy. To help the optimization of KernelWarehouse in the early training stage, \u03b2ij with temperature \u03b3 is used for initializing the scalar attentions. In the experiments, we use ResNet18 as the backbone to study the effect of our attentions initialization strategy. As shown in Table 9, a proper initialization strategy for scalar attentions is beneficial for a network to learn relationships between linear mixtures and kernel cells, which leads to 1.38% top-1 improvement to the ResNet18 backbone based on KW (1\u00d7). Visualization. To have a better understanding of the warehouse sharing mechanism of KernelWarehouse, we further analyze the statistical mean values of \u03b1ij to study its learnt attention values. The results are obtained from the pre-trained ResNet18 backbone with KW (1\u00d7). The visualization results are shown in Figure 3, from which we can observe: (1) each linear mixture can learn its own distribution of scalar attentions for different kernel cells; (2) in each warehouse, the maximum value of \u03b1ij in each row mostly appears in the diagonal line throughout the whole warehouse. It indicates that our attentions initialization strategy can help KernelWarehouse build one-to-one relationship between linear mixtures and kernel cells according to our setting of \u03b2ij; (3) compared to linear mixtures in different layers, the attentions \u03b1ij with higher absolute values for linear mixtures in the same layer have more overlap. It indicates that parameter dependencies within the same kernel are stronger than that across layers, which can be learned by KernelWarehouse. Table 10: Results comparison of inference speed (frames per second) on the ResNet50 and MobileNetV2 (1.0\u00d7) with different dynamic convolution methods. All the models are tested on an NVIDIA TITAN X GPU (with batch size 100) and a single core of Intel E5-2683 v3 CPU (with batch size 1), separately. The input image size is 224\u00d7224.\n\u00d7\nModels\nParams\nTop-1 Acc (%)\nSpeed on GPU\nSpeed on CPU\nResNet50\n25.56M\n78.44\n647.0\n6.4\n+ DY-Conv (4\u00d7)\n100.88M\n79.00 (\u21910.56)\n322.7\n4.1\n+ ODConv (4\u00d7)\n90.67M\n80.62 (\u21912.18)\n142.3\n2.5\n+ KW (1/2\u00d7)\n17.64M\n79.30 (\u21910.86)\n201.0\n1.5\n+ KW (1\u00d7)\n28.05M\n80.38 (\u21911.94)\n246.1\n1.6\n+ KW (4\u00d7)\n102.02M\n81.05 (\u21912.61)\n178.5\n0.6\nModels\nParams\nTop-1 Acc (%)\nSpeed on GPU\nSpeed on CPU\nMobileNetV2 (1.0\u00d7)\n3.50M\n72.02\n1410.8\n17.0\n+ DY-Conv (4\u00d7)\n12.40M\n74.94 (\u21912.92)\n862.4\n11.8\n+ ODConv (4\u00d7)\n11.52M\n75.42 (\u21913.40)\n536.5\n11.0\n+ KW (1/2\u00d7)\n2.65M\n72.57 (\u21910.55)\n825.9\n11.6\n+ KW (1\u00d7)\n5.17M\n74.68 (\u21912.66)\n575.3\n10.7\n+ KW (4\u00d7)\n11.38M\n75.92 (\u21913.90)\n567.2\n8.4\nLimitations. Table 10 provides two sets of experiments for inference speed analysis, from which we can observe that the runtime speed of the models trained with KernelWarehouse is usually slower than its dynamic convolution counterparts under the similar model size budget, especially for large ConvNets. The main reason for this limitation is the dense computation of linear mixtures in KernelWarehouse. Besides, we believe KernelWarehouse could be applied to more deep and large ConvNets, yet we are unable to explore this constrained by our computational resources.\n# 5 Conclusion\nIn this paper, we present KernelWarehouse, a more general form of dynamic convolution, which could be used to improve the performance of modern ConvNets. Experiments on ImageNet and MS-COCO datasets show its great potential. We hope our work would inspire future research in dynamic convolution.\nWe thank Intel Data Center & AI group\u2019s great support of their DGX-2 and DGX-A100 servers fo training large models in this project.\n# References\n# A Appendix\n# A.1 Datasets and Implementation Details\n# A.1.1 Image Classification on ImageNet\nRecall that we use ResNet [3], MobileNetV2 [5] and ConvNeXt [6] families for the main experiments on ImageNet dataset [39], which consists of over 1.2 million training images and 50,000 validation images with 1,000 object categories. We use an input image resolution of 224\u00d7224 for both training and evaluation. All the input images are standardized with mean and standard deviation per channel. For evaluation, we report top-1 and top-5 recognition rates of a single 224\u00d7224 center crop on the ImageNet validation set. All the experiments are performed on the servers having 8 GPUs. Specifically, the models of ResNet18, MobileNetV2 (1.0\u00d7), MobileNetV2 (0.5\u00d7) are trained on the servers with 8 NVIDIA Titan X GPUs. The models of ResNet50, ConvNeXt-Tiny are trained on the servers with 8 NVIDIA Tesla V100-SXM3 or A100 GPUs. The training setups for different models are as follows. Training setup for ResNet models with the traditional training strategy. All the models are trained by the stochastic gradient descent (SGD) optimizer for 100 epochs, with a batch size of 256, a momentum of 0.9 and a weight decay of 0.0001. The initial learning rate is set to 0.1 and decayed by a factor of 10 for every 30 epoch. Horizontal flipping and random resized cropping are used for data augmentation. For KernelWarehouse, the temperature \u03c4 linearly reduces from 1 to 0 in the first 10 epochs. Training setup for ResNet and ConvNeXt models with the advanced training strategy. Following the settings of ConvNeXt [6], all the models are trained by the AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999 for 300 epochs, with a batch size of 4096, a momentum of 0.9 and a weight decay of 0.05. The initial learning rate is set to 0.004 and annealed down to zero following a cosine schedule. Randaugment [42], mixup [43], cutmix [44], random erasing [45] and label smoothing [46] are used for augmentation. For KernelWarehouse, the temperature \u03c4 linearly reduces from 1 to 0 in the first 20 epochs. Training setup for MobileNetV2 models. All the models are trained by the SGD optimizer for 150 epochs, with a batch size of 256, a momentum of 0.9 and a weight decay of 0.00004. The initial learning rate is set to 0.1 and annealed down to zero following a cosine schedule. Horizontal flipping and random resized cropping are used for data augmentation. For KernelWarehouse, the temperature \u03c4 linearly reduces from 1 to 0 in the first 10 epochs.\n# A.1.2 Object Detection and Semantic Segmentation on MS-COCO\nRecall that we conduct comparative experiments for object detection and instance segmentation on the MS-COCO 2017 dataset [40], which contains 118,000 training images and 5,000 validation images with 80 object categories. We adopt Mask R-CNN as the detection framework, ResNet50 and MobileNetV2 (1.0\u00d7) built with different dynamic convolution methods as the backbones which are pre-trained on ImageNet dataset. All the models are trained with a batch size of 16 and standard 1\u00d7 schedule on the MS-COCO dataset using multi-scale training. The learning rate is decreased by a factor of 10 at the 8th and the 11th epoch of total 12 epochs. For a fair comparison, we adopt the same settings including data processing pipeline and hyperparameters for all the models. All the experiments are performed on the servers with 8 NVIDIA Tesla V100 GPUs. The attentions initialization strategy is not used for KernelWarehouse during fine-tuning to avoid disrupting the learnt relationships of the pre-trained models between kernel cells and linear mixtures. For evaluation, we report both bounding box Average Precision (AP) and mask AP on the MS-COCO 2017 validation set, including AP50, AP75 (AP at different IoU thresholds) and APS, APM, APL (AP at different scales).\n# A.2 Visualization Examples of Attentions Initialization Strategy\nRecall that we adopt an attentions initialization strategy for KernelWarehouse using \u03c4 and \u03b2ij. It forces the scalar attentions to be one-hot in the early training stage for building one-to-one relationships between kernel cells and linear mixtures. To give a better understanding of this strategy, we provide\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c77e/c77e1c15-8f29-40f2-b971-bb40bbc03d3a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1160/11609995-245b-4c5b-8c54-d4fca824074e.png\" style=\"width: 50%;\"></div>\nFigure 4: A visualization example of attentions initialization strategy for KW (1\u00d7), where both n and mt equal to 6. It helps the ConvNet to build one-to-one relationships between kernel cells and linear mixtures in the early training stage according to our setting of \u03b2ij. ez is a kernel cell that doesn\u2019t really exist and it keeps as a zero matrix constantly. In the beginning of the training process when temperature \u03c4 is 1, a ConvNet built with KW (1\u00d7) can be roughly seen as a ConvNet with standard convolutions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/18fe/18fe0871-3814-4394-992c-29b18f187127.png\" style=\"width: 50%;\"></div>\nFigure 5: Visualization examples of attentions initialization strategies for KW (2\u00d7), where n = 4 and mt = 2. (a) our proposed strategy builds one-to-one relationships between kernel cells and linear mixtures; (b) an alternative strategy which builds two-to-one relationships between kernel cells and linear mixtures.\nvisualization examples for KW (1\u00d7), KW (2\u00d7) and KW (1/2\u00d7), respectively. We also provide a set of ablative experiments to compare our proposed strategy with other alternatives. Attentions Initialization for KW (1\u00d7). A visualization example of attentions initialization strategy for KW (1\u00d7) is shown in Figure 4. In this example, a warehouse E = {e1, . . . , e6, ez} is shared to 3 neighboring convolutional layers with kernel dimensions of 1 \u00d7 1 \u00d7 2c \u00d7 c, 1 \u00d7 3 \u00d7 c \u00d7 c and 1 \u00d7 1 \u00d7 c \u00d7 c, respectively. The kernel dimensions are selected for simple illustration. The kernel cells have the same dimensions of 1 \u00d7 1 \u00d7 c \u00d7 c. Note that the kernel cell ez doesn\u2019t really exist and it keeps as a zero matrix constantly. It is only used for attentions normalization but not assembling kernels. This kernel is mainly designed for attentions initialization when b < 1 and not counted in\nModels\nAttentions Initialization Strategies\nParams\nTop-1 Acc (%)\nTop-5 Acc (%)\nResNet18\n-\n11.69M\n70.44\n89.72\n+ KW (1\u00d7)\n1 kernel cell to 1 linear mixture\n11.93M\n74.77 (\u21914.33)\n92.13 (\u21912.41)\nall the kernel cells to 1 linear mixture\n11.93M\n73.36 (\u21912.92)\n91.41 (\u21911.69)\nwithout attentions initialization\n11.93M\n73.39 (\u21912.95)\n91.24 (\u21911.52)\n+ KW (4\u00d7)\n1 kernel cell to 1 linear mixture\n45.86M\n76.05 (\u21915.61)\n92.68 (\u21912.96)\n4 kernel cells to 1 linear mixture\n45.86M\n76.03 (\u21915.59)\n92.53 (\u21912.81)\n+ KW (1/2\u00d7)\n1 kernel cell to 1 linear mixture\n7.43M\n73.33 (\u21912.89)\n91.42 (\u21911.70)\n1 kernel cell to 2 linear mixtures\n7.43M\n72.89 (\u21912.45)\n91.34 (\u21911.62)\n# \ud835\udc866 \ud835\udc862 \ud835\udc863 \ud835\udc864 \ud835\udc865 \ud835\udc861 \ud835\udc86\ud835\udc67 \ud835\udefd\ud835\udc56\ud835\udc57\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e0a3/e0a36121-c0d7-4298-89d8-0f915ef485c5.png\" style=\"width: 50%;\"></div>\nFigure 6: Visualization examples of attentions initialization strategies for KW (1/2\u00d7), where n = 2 and mt = 4. (a) our proposed strategy builds one-to-one relationships between kernel cells and linear mixtures; (b) an alternative strategy which builds one-to-two relationships between kernel cells and linear mixtures.\nthe number of kernel cells n. In the early training stage, we adopt a strategy to explicitly force every linear mixture to build relationship with one specified kernel cell according to our setting of \u03b2ij. As shown in Figure 4, we assign one of e1, . . . , e6 in the warehouse to each of the 6 linear mixtures at the 3 convolutional layers without repetition. So that in the beginning of the training process when temperature \u03c4 is 1, a ConvNet built with KW (1\u00d7) can be roughly seen as a ConvNet with standard convolutions. The results of Table 9 in the main manuscript validate the effectiveness of our proposed attentions initialization strategy. Here, we compare it with another alternative. In this alternative strategy, we force every linear mixture to build relationships with all the kernel cells equally by setting all the \u03b2ij to be 1. The results are shown in Table 11. The all-to-one strategy demonstrates similar performance with KernelWarehouse without using any attentions initialization strategy, while our proposed strategy outperforms it by 1.41% top-1 gain. Attentions Initialization for KW (2\u00d7). For KernelWarehouse with b > 1, we adopt the same strategy for initializing attentions used in KW (1\u00d7). Figure 5(a) provides a visualization example of attentions initialization strategy for KW (2\u00d7). For building one-to-one relationships, we assign e1 to w1 and e2 to w2, respectively. When b > 1, another reasonable strategy is to assign multiple kernel cells to every linear mixture without repetition, which is shown in Figure 5(b). We use the ResNet18 backbone based on KW (4\u00d7) to compare the two strategies. From the results in Table 11, we can see that our one-to-one strategy performs better. Attentions Initialization for KW (1/2\u00d7). For KernelWarehouse with b < 1, the number of kernel cells is less than that of linear mixtures, meaning that we cannot adopt the same strategy used for b \u22651. Therefore, we only assign one of the total n kernel cells in the warehouse to n linear mixtures respectively without repetition. And we assign ez to all of the remaining linear mixtures. The visualization example for KW (1/2\u00d7) is shown in Figure 6(a). When temperature \u03c4 is 1, a ConvNet built with KW (1/2\u00d7) can be roughly seen as a ConvNet with group convolutions (groups=2). We also provide comparison results between our proposed strategy and another alternative strategy which assigns one of the n kernel cells to every 2 linear mixtures without repetition. As shown in Table 11, our one-to-one strategy achieves better result again, showing that introducing an extra kernel ez for b < 1 can help the ConvNet learn more appropriate relationships between kernel cells and linear mixtures. When assigning one kernel cell to multiple linear mixtures, a ConvNet could not balance the relationships between them well.\n# A.3 Design Details of KernelWarehouse.\nIn this section, we describe the design details of our KernelWarehouse. The corresponding values of m and n for each of our trained models are provided in the Table 12. Note that the values of m and n are naturally determined according to our setting of the dimensions of the kernel cells, the layers to share warehouses and b.\n<div style=\"text-align: center;\">Table 12: The values of m and n for the ResNet18, ResNet50, ConvNeXt-Tiny, MobileNetV2 (1.0\u00d7) and MobileNetV2 (0.5\u00d7) backbones based on KernelWarehouse.</div>\n\u00d7\nBackbones\nb\nm\nn\nResNet18\n1/4\n224, 188, 188, 108\n56, 47, 47, 27\n1/2\n224, 188, 188, 108\n112, 94, 94, 54\n1\n56, 47, 47, 27\n56, 47, 47, 27\n2\n56, 47, 47, 27\n112, 94, 94, 54\n4\n56, 47, 47, 27\n224, 188, 188, 108\nResNet50\n1/2\n348, 416, 552, 188\n174, 208, 276, 94\n1\n87, 104, 138, 47\n87, 104, 138, 47\n4\n87, 104, 138, 47\n348, 416, 552, 188\nConvNeXt-Tiny\n1\n16,4,4,4,147,24,147,24,441,72,147,24\n16,4,4,4,147,24,147,24,441,72,147,24\nMobileNetV2 (1.0\u00d7)\nMobileNetV2 (0.5\u00d7)\n1/2\n9, 36, 18, 27, 36, 27, 12, 27, 80, 40\n9, 36, 18, 27, 36, 27, 6, 27, 40, 20\n1\n9, 36, 34, 78, 18, 42, 27, 102, 36, 120, 27, 58, 27\n9, 36, 34, 78, 18, 42, 27, 102, 36, 120, 27, 58, 27\n4\n9, 36, 11, 1, 2, 18, 7, 3, 27, 4, 4, 36, 9, 3, 27, 11, 3, 27, 20\n36, 144, 44, 4, 8, 72, 28, 12, 108, 16, 16, 144, 36, 12, 108, 44, 12, 108, 80\nDesign details of KernelWarehouse on ResNet18. Recall that in KernelWarehouse, a warehouse is shared to all same-stage convolutional layers. While the layers are originally divided into different stages according to the resolutions of their input feature maps, the layers are divided into different stages according to their kernel dimensions in our KernelWarehouse. In our implementation, we usually reassign the first layer (or the first two layers) in each stage to the previous stage. An example for the ResNet18 backbone based on KW (1\u00d7) is given in Table 13. By reassigning the layers, we can avoid the condition that all the other layers have to be partitioned according to a single layer because of the greatest common dimension divisors. For the ResNet18 backbone, we apply KernelWarehouse to all the convolutional layers except the first one. In each stage, the corresponding warehouse is shared to all of its convolutional layers. For KW (1\u00d7), KW (2\u00d7) and KW (4\u00d7), we use\n<div style=\"text-align: center;\">Table 13: The example of warehouse sharing for the ResNet18 backbone based on KW (1\u00d7) according to the original stages and reassigned stages.</div>\nDimensions of Kernel Cells\nOriginal Stages\nLayers\nReassigned Stages\nDimensions of Kernel Cells\n1\u00d71\u00d764\u00d764\n1\n3\u00d73\u00d764\u00d764\n1\n1\u00d71\u00d764\u00d764\n3\u00d73\u00d764\u00d764\n3\u00d73\u00d764\u00d764\n3\u00d73\u00d764\u00d764\n1\u00d71\u00d764\u00d7128\n2\n3\u00d73\u00d764\u00d7128\n3\u00d73\u00d7128\u00d7128\n2\n1\u00d71\u00d7128\u00d7128\n3\u00d73\u00d7128\u00d7128\n3\u00d73\u00d7128\u00d7128\n1\u00d71\u00d7128\u00d7256\n3\n3\u00d73\u00d7128\u00d7256\n3\u00d73\u00d7256\u00d7256\n3\n1\u00d71\u00d7256\u00d7256\n3\u00d73\u00d7256\u00d7256\n3\u00d73\u00d7256\u00d7256\n1\u00d71\u00d7256\u00d7512\n4\n3\u00d73\u00d7256\u00d7512\n3\u00d73\u00d7512\u00d7512\n4\n1\u00d71\u00d7512\u00d7512\n3\u00d73\u00d7512\u00d7512\n3\u00d73\u00d7512\u00d7512\nthe greatest common dimension divisors for static kernels as the uniform kernel cell dimensions for kernel partition. For KW (1/2\u00d7) and KW (1/4\u00d7), we use half of the greatest common dimension divisors. Design details of KernelWarehouse on ResNet50. For the ResNet50 backbone, we apply KernelWarehouse to all the convolutional layers except the first two layers. In each stage, the corresponding warehouse is shared to all of its convolutional layers. For KW (1\u00d7) and KW (4\u00d7), we use the greatest common dimension divisors for static kernels as the uniform kernel cell dimensions for kernel partition. For KW (1/2\u00d7), we use half of the greatest common dimension divisors. Design details of KernelWarehouse on ConvNeXt-Tiny. For the ConvNeXt backbone, we apply KernelWarehouse to all the convolutional layers. In each stage, the corresponding three warehouses are shared to the point-wise convolutional layers, the depth-wise convolutional layers and the downsampling layer, respectively. We use the greatest common dimension divisors for static kernels as the uniform kernel cell dimensions for kernel partition. Design details of KernelWarehouse on MobileNetV2. For the MobileNetV2 (1.0\u00d7) and MobileNetV2 (0.5\u00d7) backbones based on KW (1\u00d7) and KW (4\u00d7), we apply KernelWarehouse to all the convolutional layers. For MobileNetV2 (1.0\u00d7, 0.5\u00d7) based on KW (1\u00d7), the corresponding two warehouses are shared to the point-wise convolutional layers and the depth-wise convolutional layers in each stage, respectively. For MobileNetV2 (1.0\u00d7, 0.5\u00d7) based on KW (4\u00d7), the corresponding three warehouses are shared to the depth-wise convolutional layers, the point-wise convolutional layers for channel expansion and the point-wise convolutional layers for channel reduction in each stage, respectively. We use the greatest common dimension divisors for static kernels as the uniform kernel cell dimensions for kernel partition. For the MobileNetV2 (1.0\u00d7) and MobileNetV2 (0.5\u00d7) backbones based on KW (1/2\u00d7), we take the parameters in the attention modules and classifier layer into consideration in order to reduce the total number of parameters. We apply KernelWarehouse to all the depth-wise convolutional layers, the point-wise convolutional layers in the last two stages and the classifier layer. We set b = 1 for the point-wise convolutional layers and b = 1/2 for the other layers. For the depth-wise convolutional layers, we use the greatest common dimension divisors for static kernels as the uniform kernel cell dimensions for kernel partition. For the point-wise convolutional layers, we use half of the greatest common dimension divisors. For the classifier layer, we use the kernel cell dimensions of 1000\u00d732.\n# A.4 More Visualization Results for Learnt Attentions of KernelWarehouse\nIn the main manuscript, we provide visualization results of learnt attention values \u03b1ij for the ResNet18 backbone based on KW (1\u00d7) (see Figure 3 in the main manuscript). For a better understanding of KernelWarehouse, we provide more visualization results in this section, covering different alternative attention functions, alternative initialization strategies and values of b. For all the results, the statistical mean values of learnt attention \u03b1ij are obtained using all of the 50,000 images on the ImageNet validation dataset.\nVisualization Results for KernelWarehouse with Different Attention Functions. The visualization results for KernelWarehouse with different attention functions are shown in Figure 7, which are corresponding to the comparison results of Table 8 in the main manuscript. From which we can\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2481/248125f9-0dcb-447a-9b91-74e107b29394.png\" style=\"width: 50%;\"></div>\nFigure 7: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse for KernelWarehouse with different attention functions. The results are obtained from the pre-trained ResNet18 backbone with KW (1\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in. The attention functions for the groups of visualization results are as follows: (a) zij/ \ufffdn p=1 |zip| (our design); (b) softmax; (c) sigmoid; (d) max(zij, 0)/ \ufffdn p=1 |zip|.\nobserve that: (1) for all of the attention functions, the maximum value of \u03b1ij in each row mostly appears in the diagonal line throughout the whole warehouse. It indicates that our proposed attentions initialization strategy also works for the other three attention functions, which helps our KernelWarehouse to build one-to-one relationships between kernel cells and linear mixtures; (2) with different attention functions, the scalar attentions learnt by KernelWarehouse are obviously different, showing that the attention function plays an important role in our design; (3) compared to the other three functions, the maximum value of \u03b1ij in each row tends to be relatively lower for our design (shown in Figure 7(a)). It indicates that the introduction of negative values for scalar attentions can help the ConvNet to enhance warehouse sharing, where each linear mixture not only focuses on the kernel cell assigned to it.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f13/7f134402-d241-407c-98e8-778790c53b8d.png\" style=\"width: 50%;\"></div>\nFigure 8: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse for KernelWarehouse with different attentions initialization strategies. The results are obtained from the pre-trained ResNet18 backbone with KW (1\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in. The attentions initialization strategies for the groups of visualization results are as follows: (a) building one-to-one relationships between kernel cells and linear mixtures; (b) building all-to-one relationships between kernel cells and linear mixtures; (c) without initialization.\n<div style=\"text-align: center;\">Figure 8: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse for KernelWarehouse with different attentions initialization strategies. The results are obtained from the pre-trained ResNet18 backbone with KW (1\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in. The attentions initialization strategies for the groups of visualization results are as follows: (a) building one-to-one relationships between kernel cells and linear mixtures; (b) building all-to-one relationships between kernel cells and linear mixtures; (c) without initialization.</div>\nVisualization Results for KernelWarehouse with Attentions Initialization Strategies. The visualization results for KernelWarehouse with different attentions initialization strategies are shown in Figure 8, Figure 9 and Figure 10, which are corresponding to the comparison results of Table 11. From which we can observe that: (1) with all-to-one strategy or without initialization strategy, the distribution of scalar attentions learnt by KernelWarehouse seems to be disordered, while our proposed\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f582/f582c65e-dbba-4fa4-b24c-9f5a8387fd87.png\" style=\"width: 50%;\"></div>\nFigure 9: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse for KernelWarehouse with different attentions initialization strategies. The results are obtained from the pre-trained ResNet18 backbone with KW (4\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in. The attentions initialization strategies for the groups of visualization results are as follows: (a) building one-to-one relationships between kernel cells and linear mixtures; (b) building four-to-one relationships between kernel cells and linear mixtures.\nstrategy can help the ConvNet learn more appropriate relationships between kernel cells and linear mixtures; (2) for KW (4\u00d7) and KW (1/2\u00d7), it\u2019s hard to directly determine which strategy is better only according to the visualization results. While the results demonstrate that the learnt attentions of KernelWarehouse are highly related to our setting of \u03b1ij; (3) for KW (1\u00d7), KW (4\u00d7) and KW (1/2\u00d7) with our proposed initialization strategy, some similar patterns of the value distributions can be found. For example, the maximum value of \u03b1ij in each row mostly appears in the diagonal line throughout the whole warehouse. It indicates that our proposed strategy can help the ConvNet learn stable relationships between kernel cells and linear mixtures.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8287/8287222e-1610-4e39-b4cf-c45c75ef8a5c.png\" style=\"width: 50%;\"></div>\nFigure 10: Visualization of statistical mean values of learnt attention \u03b1ij in each warehouse for KernelWarehouse with different attentions initialization strategies. The results are obtained from the pre-trained ResNet18 backbone with KW (1/2\u00d7) for all of the 50,000 images on the ImageNet validation dataset. Best viewed with zoom-in. The attentions initialization strategies for the groups of visualization results are as follows: (a) building one-to-one relationships between kernel cells and linear mixtures; (b) building one-to-two relationships between kernel cells and linear mixtures.\n",
    "paper_type": "method",
    "attri": {
        "background": "Dynamic convolution learns a linear mixture of n static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient, increasing the number of convolutional parameters by n times. This leads to optimization difficulties and hinders research progress in dynamic convolution. The paper introduces KernelWarehouse, a more general form of dynamic convolution that balances parameter efficiency and representation power by redefining kernels and assembling kernels to reduce kernel dimension and increase kernel number significantly.",
        "problem": {
            "definition": "The problem addressed is the inefficiency of existing dynamic convolution methods, which require a large number of parameters to achieve improved performance, thus limiting the exploration of dynamic convolution with a significantly large kernel number.",
            "key obstacle": "The main challenge is the intrinsic conflict between the desired model size and capacity, which prevents researchers from effectively exploring dynamic convolution with a large kernel number (e.g., n > 100)."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that existing dynamic convolution methods do not leverage parameter dependencies within and across convolutional layers effectively.",
            "opinion": "KernelWarehouse is proposed as a solution that redefines 'kernels' and 'assembling kernels' to enhance parameter efficiency and representation power.",
            "innovation": "The key innovation lies in the strategies of kernel partition and warehouse sharing, which allow for a significant increase in kernel number while maintaining manageable parameter budgets."
        },
        "method": {
            "method name": "KernelWarehouse",
            "method abbreviation": "KW",
            "method definition": "KernelWarehouse is a method that enhances convolutional parameter dependencies by dividing static kernels into smaller kernel cells and sharing these across layers, allowing for a flexible increase in the number of kernels used.",
            "method description": "The core of KernelWarehouse involves partitioning static kernels into smaller cells and assembling them based on a shared warehouse of kernel cells.",
            "method steps": [
                "Divide the static kernel into m disjoint kernel cells.",
                "Create a warehouse consisting of n kernel cells.",
                "Compute each kernel cell as a linear mixture based on the shared warehouse.",
                "Assemble the mixtures to replace the static kernel."
            ],
            "principle": "The method is effective because it allows for a high degree of flexibility in kernel usage, enabling the model to adaptively select and combine kernel cells to fit a desired parameter budget."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on ImageNet and MS-COCO datasets using various ConvNet architectures, including ResNet18, ResNet50, MobileNetV2, and ConvNeXt-Tiny.",
            "evaluation method": "Performance was assessed by comparing top-1 and top-5 accuracy across different models and configurations, alongside parameter counts to evaluate efficiency."
        },
        "conclusion": "KernelWarehouse demonstrates significant potential in improving the performance of modern ConvNets while being parameter-efficient, as evidenced by state-of-the-art results on the ImageNet and MS-COCO datasets.",
        "discussion": {
            "advantage": "The main advantages include improved parameter efficiency and the ability to achieve higher accuracy without a corresponding increase in model size.",
            "limitation": "A limitation of KernelWarehouse is that its runtime speed is generally slower than its dynamic convolution counterparts due to the dense computation involved.",
            "future work": "Future research could explore optimizing the computational efficiency of KernelWarehouse and applying it to deeper and larger ConvNets."
        },
        "other info": {
            "code": "Code and pre-trained models are available at https://github.com/OSVAI/KernelWarehouse."
        }
    },
    "mount_outline": [
        {
            "section number": "5.1",
            "key information": "KernelWarehouse enhances convolutional parameter dependencies by dividing static kernels into smaller kernel cells and sharing these across layers, allowing for a flexible increase in the number of kernels used."
        },
        {
            "section number": "5.4",
            "key information": "A limitation of KernelWarehouse is that its runtime speed is generally slower than its dynamic convolution counterparts due to the dense computation involved."
        },
        {
            "section number": "3.5",
            "key information": "KernelWarehouse demonstrates significant potential in improving the performance of modern ConvNets while being parameter-efficient, as evidenced by state-of-the-art results on the ImageNet and MS-COCO datasets."
        },
        {
            "section number": "4.1",
            "key information": "The main advantages of KernelWarehouse include improved parameter efficiency and the ability to achieve higher accuracy without a corresponding increase in model size."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed is the inefficiency of existing dynamic convolution methods, which require a large number of parameters to achieve improved performance, thus limiting the exploration of dynamic convolution with a significantly large kernel number."
        }
    ],
    "similarity_score": 0.5764288897111476,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/KernelWarehouse_ Towards Parameter-Efficient Dynamic Convolution.json"
}