{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.12976",
    "title": "The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis",
    "abstract": "In-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in-context learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.",
    "bib_name": "zhang2024impactdemonstrationsmultilingualincontext",
    "md_text": "# The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis\n# Miaoran Zhang1 Vagrant Gautam1 Mingyang Wang2,3,4 Jesujoba O. Alabi1 Xiaoyu Shen5\u2217Dietrich Klakow1 Marius Mosbach6\n1Saarland University, Saarland Informatic Campus 2Bosch Center for AI 3LMU Munich 4Munich Center for Machine Learning (MCML) 5Eastern Institute of Technology, Ningbo 6Mila, McGill University {mzhang,vgautam,jalabi,dietrich.klakow}@lsv.uni-saarland.de mingyang@cis.lmu.de xyshen@eitech.edu.cn marius.mosbach@mila.quebec\nAbstract\nIn-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English incontext learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.1\n 7 Jun 2024\narXiv:2402.12976v2\nAn intriguing property of large language models (LLMs) is their ability to perform in-context learning (Brown et al., 2020), i.e., solve a task conditioned on a few demonstrations at inference time, without updating the model parameters. It has been shown to be an efficient alternative to finetuning when adapting models to diverse tasks and domains (Dong et al., 2022; Min et al., 2022b; Si et al., 2023, inter alia). In light of the success of incontext learning, there has been increased interest\n\u2217Corresponding author. 1We release our code publicly at https://github.com/ uds-lsv/multilingual-icl-analysis.\nin better understanding the factors that influence its success, such as demonstration selection (Liu et al., 2022; Rubin et al., 2022; Wang et al., 2023c), prompt design (Min et al., 2022a; Wei et al., 2022), and more generally on understanding how and why in-context learning works (Xie et al., 2022; Bansal et al., 2023; Hendel et al., 2023; Pan et al., 2023; Wang et al., 2023b). However, most recent work on in-context learning predominantly focuses on English, and the exploration of multilingual in-context learning generally lags behind. This is problematic, as results that apply to English might not hold for other languages, especially those that are less represented in LLM training data. While there have been a few studies on in-context learning that go beyond English, they either focus on benchmarking LLMs on multilingual tasks without in-depth exploration, e.g., MEGA (Ahuja et al., 2023) and BUFFET (Asai et al., 2023), or zoom in on specific capabilities such as mathematical reasoning (Shi et al., 2023b), machine translation (Zhu et al., 2023; Agrawal et al., 2023), or code-switching (Zhang et al., 2023). In this work, we take a multidimensional approach (Ruder et al., 2022) that unifies these strands of research and comprehensively evaluate the multilingual in-context learning abilities of LLMs. We focus on dissecting the actual impact of in-context demonstrations, which is crucial for understanding model behaviour. Our research covers various models, tasks, and languages, and we seek to answer the following research questions: 1. Does multilingual performance benefit from demonstrations? (\u00a74)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8252/825258f2-a8a1-4b1d-aea9-a268628fe9f3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: An overview of the components of multilingual in-context learning (\u00a72) with a comparison to zero-shot learning. Sources of variation include tasks, languages, models, and the template, i.e., the task instruction, patterns for formatting inputs, and verbalized labels.</div>\nSpecifically, we address our research questions by evaluating 5 LLMs including base models that are only pre-trained on unlabeled text corpora (XGLM and Llama 2), and chat models that are further refined with instruction tuning and reinforcement learning (Llama 2-Chat, GPT-3.5, and GPT-4). We evaluate on 9 multilingual datasets that include both classification and generation tasks, covering 56 typologically different languages. Our main findings are: (1) The effectiveness of demonstrations varies widely depending on the model, task, and language used. For base models, in-context learning barely outperforms zeroshot learning on many tasks. In general, in-context learning matters more for generation tasks with loosely-specified prompts; (2) Even with sophisticated demonstration selection methods, in-context learning is not always beneficial and can sometimes be worse than using no demonstrations at all; (3) Chat models are less sensitive to seeing correctly-labeled demonstrations than base models, suggesting that for the former, demonstrations primarily help the model understand the task format, while for the latter, demonstrations also impart taskspecific knowledge; (4) Using a formatting-focused template can even eliminate the need for demonstrations with chat models. The relative significance of demonstrations versus prompt templates varies based on inherent model capabilities. In sum, we suggest that the benefits of adding demonstrations may be overestimated. Future work on in-context learning should carefully compare their results with zero-shot learning and on multiple\ntemplates to faithfully represent its effectiveness. Given the vast variance across models, tasks, and languages, it is also important to cautiously frame claims about in-context learning.\n# 2 Preliminaries\n# 2.1 In-context learning\nIn-context learning (ICL) is a popular inference strategy where models solve2 a task without any parameter updates (Brown et al., 2020). Instead, the model performs the task by conditioning on labeled demonstrations. Demonstrations are typically formatted using \u201cpattern-verbalizer pairs,\u201d as this has been shown to be effective in eliciting good task performance (Schick and Sch\u00fctze, 2021; Bach et al., 2022). Here, a pattern is used to format the input for the model, and a verbalizer maps the label to a textual representation. Additionally for instruction-tuned LLMs, a task instruction is often added to provide information about the task beyond individual demonstrations (Mishra et al., 2022b; Wang et al., 2022; Ouyang et al., 2022). Formally, given a test sample xt, k demonstrations {(xi, yi)}k i=1, a pattern P, a verbalizer V and a task instruction I, the model (parameterized by \u03b8) makes its prediction as follows:\nyt \u223cp\u03b8(y|I, {(P(xi), V(yi))}k i=1, P(xt)). (\n2The extent to which models actually \u201csolve\u201d tasks is an open question as ICL, similar to fine-tuning, has generalization issues despite its impressive results (Mosbach et al., 2023). Regardless, we use the word \u201csolve\u201d in the rest of this paper for simplicity.\nTaken together, the pattern, the verbalizer and the optional task instruction comprise the template with which demonstrations and the test sample are formatted as the input prompt for model inference. The effectiveness of demonstrations is thus linked with the template used to present them to the model.\n# 2.2 Multilingual prompting\nPrevious studies highlight that the selection of demonstrations and prompt templates can significantly influence model performance (Liu et al., 2022; Fu et al., 2023b; Sclar et al., 2024). In multilingual in-context learning, the variation in input prompts is further complicated by the language of demonstrations, templates and test samples, all of which are important design choices. For the template language, Lin et al. (2022) and Ahuja et al. (2023) found that English templates generally perform better than native language templates, possibly due to superior instructionfollowing abilities on existing LLMs on English compared to other languages. Following this, we use English templates in our study. For the language of few-shot demonstrations and test samples, there are three popular settings. Given a test sample in a certain language, the most straightforward approach is to use demonstrations in the same language (referred to as in-language demonstrations). This setting directly measures the model\u2019s inherent ability to solve problems in that language. Another choice is to use English demonstrations regardless of the language of the test sample. This is a cross-lingual transfer setup, where the goal is to transfer knowledge from a pivot language to a target language via in-context learning. As highlighted in Shi et al. (2023b) and Ahuja et al. (2023), in-language demonstrations often outperform English demonstrations on diverse multilingual tasks. Yet another option is to translate the test sample into English \u2013 an approach called translate-test, where the demonstrations are also in English. While translate-test leads to strong performance (Ahuja et al., 2023), this approach heavily relies on a translation system for data processing and centers the English proficiency of LLMs. In this work, we are interested in dissecting the intrinsic multilingual capabilities of LLMs, therefore we choose to use in-language demonstrations. All these design choices are represented visually in Figure 1, which gives an overview of multilingual in-context learning. Detailed setup information is provided in the next section.\n# 3 Experimental setup\nModels. We evaluate two types of LLMs: pretrained base models and chat models. Our base models include XGLM (Lin et al., 2022) and Llama 2 (Touvron et al., 2023). Our chat models are Llama 2-Chat, GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI et al., 2023). Specifically, we use xglm-7.5B, Llama-2-13b, and Llama-2-13b-chat on Huggingface (Wolf et al., 2020), and we access gpt-3.5-turbo-16k and gpt-4-32k APIs via Microsoft Azure.3\nModels. We evaluate two types of LLMs: pretrained base models and chat models. Our base models include XGLM (Lin et al., 2022) and Llama 2 (Touvron et al., 2023). Our chat models are Llama 2-Chat, GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI et al., 2023). Specifically, we use xglm-7.5B, Llama-2-13b, and Llama-2-13b-chat on Huggingface (Wolf et al., 2020), and we access gpt-3.5-turbo-16k and gpt-4-32k APIs via Microsoft Azure.3 Tasks and datasets. We experiment on a diverse range of multilingual classification and generation tasks, using 9 datasets covering 56 languages in total. Our dataset selection largely follows MEGA (Ahuja et al., 2023), but we add datasets for extremely under-represented African languages. Our classification tasks include natural language inference (NLI), paraphrase identification, commonsense reasoning and sentiment analysis, with the following datasets: XNLI (Conneau et al., 2018), IndicXNLI (Aggarwal et al., 2022), PAWS-X (Yang et al., 2019), XCOPA (Ponti et al., 2020), XStoryCloze (Lin et al., 2022) and AfriSenti (Muhammad et al., 2023). Our generation tasks are extractive question answering (QA) and machine translation (MT), for which we use XQuAD (Artetxe et al., 2020), TyDiQAGoldP (Clark et al., 2020), and MAFAND (Adelani et al., 2022). See Appendix A.1 for more details. In-context learning. For each test sample, we select k \u2208{0, 2, 4, 8}4 different demonstrations, which are randomly sampled unless otherwise specified. All demonstrations are in the same language as the test sample, and all templates are in English. We employ appropriate task-specific templates for different model types. All templates and data splits are shown in Appendix A.2. Metrics. For classification tasks, we report the rank classification accuracy5 for open-source base models (Muennighoff et al., 2023; Lin et al., 2022). 3We also experiment with BLOOMZ and mT0 (Muennighoff et al., 2023). Results in Appendix B.1 show that their zero-shot performance significantly surpasses few-shot performance, which we ascribe to their training scheme. 4For QA datasets, we select a maximum of 4 demonstrations due to context size limitations. 5The scoring function is the average of per-token log probabilities (ignoring the common prefix of different candidates). The candidate with the highest score is chosen as the prediction.\n# In-context learning. For each test sample, w  \nselect k \u2208{0, 2, 4, 8}4 different demonstrations, which are randomly sampled unless otherwise specified. All demonstrations are in the same language as the test sample, and all templates are in English. We employ appropriate task-specific templates for different model types. All templates and data splits are shown in Appendix A.2.\nMetrics. For classification tasks, we report the rank classification accuracy5 for open-source base models (Muennighoff et al., 2023; Lin et al., 2022).\n3We also experiment with BLOOMZ and mT0 (Muennighoff et al., 2023). Results in Appendix B.1 show that their zero-shot performance significantly surpasses few-shot performance, which we ascribe to their training scheme. 4For QA datasets, we select a maximum of 4 demonstrations due to context size limitations. 5The scoring function is the average of per-token log probabilities (ignoring the common prefix of different candidates). The candidate with the highest score is chosen as the prediction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/167a/167a9d36-9957-4c76-9bbe-fb0db7345d30.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Average performance across languages with different numbers of demonstrations. We average and report standard deviations over 3 seeds for all models except GPT-4. Note that the standard deviations are relatively small, possibly because of averaging over languages. en-xx: translating from English to another language, xx-en: translating from another language to English.</div>\nFor chat models, we measure the exact match between generated outputs6 and verbalized labels (Ahuja et al., 2023). As for generation tasks, we use the F1 score for QA datasets and ChrF++ score (Popovi\u00b4c, 2017) for MAFAND. Implementation details for our evaluation are provided in Appendix A.3.\n# 4 Do (more) demonstrations benefit multilingual performance?\nIn this section, we systematically compare ICL and zero-shot learning as this question is underexplored in previous studies of multilingual ICL (Ahuja et al., 2023; Asai et al., 2023). We examine model performance on diverse multilingual tasks while varying the number of demonstrations, and show the results for classification tasks and generation tasks in Figure 2. We begin with the overall trends across models and datasets. OpenAI\u2019s GPT-3.5 and GPT-4 models achieve the best multilingual in-context learning performance on all our datasets, which is unsurprising as they are currently the state-of-the-art on a large suite of NLP benchmarks. The next best models are Llama 2 and Llama 2-Chat, which demonstrate comparable or superior performance to the multilingual XGLM model despite being trained primarily on English corpora (Touvron et al., 2023). This indicates that their task-solving abilities can transfer across languages. Regardless of the model, however, performance on the AfriSenti\n6We extract verbalized labels from the generated outputs using regular expressions before calculating the exact match.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/db08/db087aba-ca7f-421c-ad47-74daf2c82275.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Performance difference between 4-shot and 0-shot. Each marker represents the average performance across models for each language in a given task. MT denotes the MAFAND dataset.</div>\nand MAFAND datasets, particularly when translating English to African languages, lags significantly behind other tasks, showing that language discrepancies remain even in the best models. An important pattern across datasets and models is that in-context learning does not always improve over zero-shot learning \u2013 in particular, it helps with generation tasks, but results on classification tasks are mixed. For the AfriSenti dataset, many models show noticeable improvements with ICL. However, with other tasks such as IndicXNLI, XNLI and PAWS-X, the same models, especially base models, perform much worse compared to the zero-shot setting. We also see marginal improvements in some cases, e.g., XGLM and Llama 2 on XCOPA. In comparison to chat models, the addition of demonstrations typically reduces the performance of base models across many tasks.\nModel\nXNLI\nIndicXNLI\nPAWS-X\nXCOPA\nXStoryCloze\nAfriSenti\nXQuAD\nTyDiQA\nMT (en-xx)\nMT (xx-en)\nXGLM\n4.59\n2.49\n0.24\u25bd\n0.03\n0.97\u25bd\n5.62\n1.77\n4.21\n1.31\n0.66\nLlama 2\n6.61\n4.17\n2.35\n\u22120.11\n0.33\n4.17\n1.32\n0.54\n2.15\n1.35\nLlama 2-Chat \u22120.28\n\u22121.36\n\u22121.71\u25bd\n0.32\n0.43\n2.17\n1.02\n2.42\n0.74\n0.66\nGPT-3.5\n0.18\n0.71\n\u22122.07\n0.86\n\u22120.61\n\u22120.66\u25bd\n\u22120.34\n2.98\n0.72\n0.43\nGPT-4\n0.76\n\u22120.19\n0.07\n\u22120.36\n0.05\n\u22120.68\n\u22120.77\n1.88\n1.21\n0.65\nTable 1: Performance difference of 4-shot ICL with TOP-K vs. RANDOM selection. Positive numbers show tha TOP-K is better than RANDOM (expected), and highlighted cells show where top-k is even worse than random selection. \u25bd: TOP-K performance is even worse than zero-shot learning. For RANDOM, we average over 3 seeds (except for GPT-4).\nModel\nXNLI\nIndicXNLI\nPAWS-X\nXCOPA\nXStoryCloze\nAfriSenti\nXQuAD\nTyDiQA\nMT (en-xx)\nMT (xx-en)\nXGLM\n0.46\n\u22120.05\n0.44\n0.51\n0.62\u2217\n3.78\u2217\n24.56\u2217\n26.64\u2217\n3.18\u2217\n6.73\u2217\nLlama 2\n0.96\u2217\n0.43\n1.16\n0.61\u2217\n1.12\u2217\n2.27\u2217\n26.68\u2217\n29.20\u2217\n4.79\u2217\n8.34\u2217\nLlama 2-Chat \u22120.34\n0.04\n1.48\n0.03\n\u22120.23\n0.77\u2217\n5.94\u2217\n4.37\u2217\n1.13\u2217\n1.53\u2217\nGPT-3.5\n0.39\n1.02\n0.64\n0.26\n0.58\u2217\n\u22120.62\n5.46\u2217\n5.61\u2217\n1.39\u2217\n0.48\u2217\nGPT-4\n\u22120.86\n\u22120.04\n0.57\n0.86\n1.13\n0.90\n9.60\n6.97\n1.24\n0.64\nTable 2: Performance difference of 4-shot ICL with RANDOM vs. RANDOM-CORRUPTED demonstrations. Positive numbers show that RANDOM is better than RANDOM-CORRUPTED (expected), and highlighted cells show where corrupted labels perform even better than ground-truth labels. We average over 3 seeds (except for GPT-4). \u2217: a significant difference (p = 0.05).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c1e2/c1e2c06a-e45b-49dc-b574-7e2fd54daea1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance difference between 4-shot and 0-shot for individual languages in PAWS-X. Error bars represent standard deviations calculated over 3 seeds.</div>\nWhen examining the cases where ICL improves performance, we see that improvements saturate quickly with 2 to 4 demonstrations. This aligns with Chen et al. (2023), who found that reducing the number of demonstrations to one does not significantly deteriorate chain-of-thought reasoning. Looking at the improvements over zero-shot performance (for all models and languages combined) across tasks in Figure 3, we observe that there are large fluctuations between individual languages that are not captured by the average. The PAWS-X dataset in particular shows an average degradation, but in fact some languages benefit from ICL while others degrade. For a more nuanced understanding of language-specific differences within a task, we zoom into this dataset in Figure 4 to inspect these language-specific differences.7 We see that\n7Plots for other datasets are provided in Appendix B.2.\nlanguages and models can behave very differently even on just one dataset, and a pattern which holds for one language with one model does not necessarily apply to a different language. For example, the ICL performance of Llama 2 outperforms its zeroshot performance by 2.3 points on Japanese and 1.3 points on Korean. However, demonstrations degrade performance for other languages, e.g., English performance degrades by 10.3 points. In sum, the effectiveness of demonstrations varies widely depending on the model, task, and language.\n# 5 Does demonstration quality matter?\nOur previous experiments evaluated ICL using randomly selected demonstrations. To ablate for the effects of demonstration quality, this section experiments with the choice of demonstrations as well as the importance of ground truth labels, i.e., the inputlabel mapping. Inspired by work on demonstration selection (Liu et al., 2022; Rubin et al., 2022) and input-label mapping (Min et al., 2022c; Yoo et al., 2022) in English, we compare the following three types of demonstrations:\n# \u2022 RANDOM: demonstrations are randomly selected from clean data\n\u2022 TOP-K: the k most semantically similar8 examples to a given test sample are selected (Liu\n8We quantify semantic similarity using LaBSE (Feng et al., 2022), a multilingual sentence embedding model trained on 109+ languages.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7505/7505b147-c416-4cde-8ad7-4ef872fbf604.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Performance of 4-shot ICL using different types of demonstrations for individual languages on AfriSent and XQuAD. The top row shows Llama 2 results, and the bottom row shows GPT-3.5 results.</div>\n# et al., 2022)\n\u2022 RANDOM-CORRUPTED: demonstrations are randomly selected but the labels are corrupted by replacement with random labels9 (Min et al., 2022c)\nTable 1 shows that top-k selection performs better than random selection in many cases, especially for the base models XGLM and Llama 2. For chat models, the largest improvements are on generation tasks. For example, GPT-3.5 achieves a 2.98-point improvement on TyDiQA. Nevertheless, top-k selection often degrades performance on many other tasks, e.g., GPT-3.5 is 2.07 points worse on PAWSX compared to random selection. When compared to zero-shot performance, ICL with top-k selection is even worse in some cases, such as XGLM on PAWS-X and XStoryCloze. In cases where random selection performs worse than zero-shot, even top-k selection gives only marginal improvements (see detailed numbers in Table 5 in Appendix C.1). These findings indicate that sophisticated demonstration selection methods are not always beneficial and can sometimes be worse than using no demonstrations at all. Exploring this further, in Table 2, we compare randomly selected demonstrations with ground truth labels and corrupted labels. We find that using corrupted labels does not hurt performance on multilingual classification tasks much, which is consistent with previous research on English (Min\n9For classification tasks, we randomly choose a label from the fixed label set. For generation tasks, we randomly choose a label from the label space of the entire demonstration data.\net al., 2022c). On generation tasks, however, all models perform worse with corrupted labels, but to vastly different extents. XGLM and Llama 2 perform significantly worse with corrupted labels, especially on the machine translation task, whereas chat models do not rely as much on correct labels. This might be explained by ICL helping the model understand the task format and activating prior knowledge acquired by the model, rather than the model learning the task from demonstrations. The observed model insensitivity to correct labels on certain tasks implies that random labels can serve as a strong baseline for demonstration generation before exploring more complex methods (Lyu et al., 2023; Wan et al., 2023). To investigate how these patterns split up across languages, Figure 5 shows language-specific results on AfriSenti and XQuAD with Llama 2 and GPT-3.5.10 On AfriSenti, top-k selection outperforms random selection with Llama 2 across most languages; however, in the case of Swahili and Tsonga, there is a performance drop of 3.2 and 1.2 points, respectively. With GPT-3.5, top-k selection does not help across most languages, but it does help with Mozambican Portuguese and Twi. Similarly, the impact of corrupted labels varies. Llama 2 is affected dramatically by corrupted labels on all languages in XQuAD, whereas GPT-3.5 is much less affected, although to varying degrees across different languages. We urge NLP practitioners to attend to these discrepancies when creating language-specific applications, and leave it to future work to explore where they come from.\n10See Appendix C.2 for other models and datasets.\n# 6 Better templates further reduce the benefits of demonstrations\nIn-context learning performance depends not only on the demonstrations, which we have varied so far, but also on how they are formatted using templates. Previous work (Gonen et al., 2023; Mizrahi et al., 2024) has shown that modifying the template changes task performance. This section thus seeks to examine the interplay between template choice and demonstrations.\nTemplate design. In the zero-shot setting, we observe that chat models tend to generate verbose responses (e.g., \u201cSure! I can help you with that\u201d) or explanations (e.g., \u201cThe reason is that ...\u201d) that pose a challenge for automatic evaluation. We observe a reduction in this behaviour with ICL, which leads us to question whether demonstrations are merely a means to format model responses. To see if we can achieve the same effect with minor template engineering, we augment the original templates with instructions that focus on output formatting. We call these formatting-focused templates which are shown in Table 9. In this section, we focus on XCOPA, AfriSenti, XQuAD, and TyDiQA, as these are the classification and generation tasks that seem to benefit most from in-context demonstrations (see Section 4). However, as Figure 6 shows, the performance gap between zero-shot and in-context learning diminishes with formatting-focused templates. The gap reduction is more substantial for QA datasets (i.e., the generation tasks) than for XCOPA and AfriSenti (i.e., the classification tasks). We speculate that it is simpler for the model to generate label words for classification tasks with a pre-defined label space than to answer questions in a way that is easy to evaluate automatically. In the latter case, formatting-focused templates can teach output styling, largely eliminating the benefits of demonstrations. Compared to GPT-3.5 and GPT-4, Llama 2-Chat performs worse in both zero-shot and few-shot settings, and formatting-focused templates have a less pronounced impact. On QA datasets, GPT-3.5 and GPT-4 even achieve better zero-shot performance with formatting-focused templates than ICL with original templates, a pattern that is not observed with Llama 2-Chat. This suggests that the relative significance of demonstrations and templates varies based on the inherent abilities of models at solving tasks and following instructions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0494/04947702-20ee-45ff-bf9e-266ca97830b2.png\" style=\"width: 50%;\"></div>\nFigure 6: Effect of using different templates on 0-shot and 4-shot performance. Formatting-focused templates (with hatching) improve 0-shot performance over original templates (solid colours), and reduce the gap between 0-shot and 4-shot performance. Few-shot results are averaged across 3 seeds except for GPT-4.\nModel\nDemo. Label\nXQuAD\nTyDiQA\nO\nF\nO\nF\nOriginal\n38.9\u00b10.1\n43.8\u00b10.7\n40.0\u00b10.3\n40.6\u00b10.8\nLlama 2-Chat\nCorrupted\n33.0\u00b10.4\n38.6\u00b10.3\n35.6\u00b10.1\n36.3\u00b10.5\n\u2206\n5.9\n5.2\n4.4\n4.3\nOriginal\n68.2\u00b10.4\n72.2\u00b10.4\n64.8\u00b10.5\n70.5\u00b10.5\nGPT-3.5\nCorrupted\n62.7\u00b10.2\n69.9\u00b10.2\n59.2\u00b10.3\n67.1\u00b10.7\n\u2206\n5.5\n2.3\n5.6\n3.4\nOriginal\n73.2\n79.3\n72.8\n78.3\nGPT-4\nCorrupted\n63.6\n79.8\n65.8\n77.6\n\u2206\n9.6\n-0.5\n7.0\n0.7\nTable 3: Effect of using different templates on 4-shot performance with RANDOM and RANDOM-CORRUPTED demonstrations. When using formatting-focused templates (F) over the original templates (O), the performance gap (\u2206) between original and corrupted labels decreases. We average and report standard deviations over 3 seeds for all models except GPT-4.\nWith our new formatting-focused templates, we revisit the impact of the input-label mapping discussed in Section 5. As Table 3 shows, all models perform worse with corrupted labels, but formatting-focused templates largely mitigate this degradation. Notably, GPT-4 using corrupted labels performs on par with ground truth labels. This strengthens our finding that the correct inputlabel mapping is not that important, while also highlighting the crucial role that templates play in in-context learning. Figure 7 shows the language-specific effects of formatting-focused templates on XQuAD (results for other tasks are in Appendix D.1). For Llama 2-Chat, demonstrations remain essential even with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f5c4/f5c4e4d0-11ee-4709-9703-40d8ee9a5366.png\" style=\"width: 50%;\"></div>\nFigure 7: Effect of using different templates on 0-shot and 4-shot XQuAD performance. Formatting-focused templates (with hatching) improve 0-shot performance over original templates (solid colours), and reduce the gap between 0-shot and 4-shot performance. Few-shot results are averaged across 3 seeds except for GPT-4. a formatting-focused template for most languages, but not Greek and Hindi. GPT-3.5 and GPT-4 also show variance across languages. Moreover, for most languages, zero-shot learning with minor template engineering can match and even exceed in-context learning performance, aligning with previous work on GPT-3 (Reynolds and McDonell, 2021). The fact that we can achieve the same effects through template engineering or demonstrations reinforces our hypothesis that models are not actually learning tasks on the fly. Instead, some combination of demonstrations and templates serves to activate prior knowledge of a task and encourage a consistent output format for automatic evaluation.\nFigure 7: Effect of using different templates on 0-shot and 4-shot XQuAD performance. Formatting-focused templates (with hatching) improve 0-shot performance over original templates (solid colours), and reduce the gap between 0-shot and 4-shot performance. Few-shot results are averaged across 3 seeds except for GPT-4.\na formatting-focused template for most languages, but not Greek and Hindi. GPT-3.5 and GPT-4 also show variance across languages. Moreover, for most languages, zero-shot learning with minor template engineering can match and even exceed in-context learning performance, aligning with previous work on GPT-3 (Reynolds and McDonell, 2021). The fact that we can achieve the same effects through template engineering or demonstrations reinforces our hypothesis that models are not actually learning tasks on the fly. Instead, some combination of demonstrations and templates serves to activate prior knowledge of a task and encourage a consistent output format for automatic evaluation.\n# 7 Discussion\nOur systematic study provides strong evidence that the importance of in-context demonstrations on existing multilingual datasets might be overestimated, as it highly depends on the model, task, and language used. For strong instruction-following models, the effect of demonstrations is superficial and can be eliminated with minor template engineering. These findings open up new questions, which we discuss below.\nUnderstanding the failures of ICL. There has been a surge of research interest in understanding\nthe underlying mechanisms of ICL (Xie et al., 2022; Von Oswald et al., 2023; Wang et al., 2023b; Hendel et al., 2023), motivated by its successes. Our results show that ICL is not always effective, and that its performance changes depending on multiple factors including the choice of model, task and language. The failures of ICL need as much scrutiny as its successes for a more fundamental understanding of the learning mechanisms of LLMs.\nthe underlying mechanisms of ICL (Xie et al., 2022; Von Oswald et al., 2023; Wang et al., 2023b; Hendel et al., 2023), motivated by its successes. Our results show that ICL is not always effective, and that its performance changes depending on multiple factors including the choice of model, task and language. The failures of ICL need as much scrutiny as its successes for a more fundamental understanding of the learning mechanisms of LLMs. Optimizing demonstrations or templates. With the increasing popularity of research on demonstration selection (Liu et al., 2022; Rubin et al., 2022; Li et al., 2023b) and prompt engineering (Mishra et al., 2022a; White et al., 2023; Khattab et al., 2023), it is important to understand the interplay of the two. We show that good demonstrations help base models perform better on certain tasks, but that formatting-focused prompting has a much bigger impact on chat models. These results show that the impact of demonstrations cannot be fairly evaluated in isolation from the choice of prompt. These findings have implications both for researchers interested in fairly evaluating ICL, and for practitioners to choose to spend time optimizing demonstrations, templates or both.\n# Optimizing demonstrations or templates.\nOptimizing demonstrations or templates. With the increasing popularity of research on demonstration selection (Liu et al., 2022; Rubin et al., 2022; Li et al., 2023b) and prompt engineering (Mishra et al., 2022a; White et al., 2023; Khattab et al., 2023), it is important to understand the interplay of the two. We show that good demonstrations help base models perform better on certain tasks, but that formatting-focused prompting has a much bigger impact on chat models. These results show that the impact of demonstrations cannot be fairly evaluated in isolation from the choice of prompt. These findings have implications both for researchers interested in fairly evaluating ICL, and for practitioners to choose to spend time optimizing demonstrations, templates or both.\n# Evaluating multilingual ICL. Compared to the\nextensive research on ICL in English (Zhao et al., 2021; Dong et al., 2022; Min et al., 2022b; Mosbach et al., 2023), multilingual ICL remains underexplored. There is no widely accepted setup to robustly evaluate the effectiveness of ICL across languages, since the choice of multilingual models and tasks is limited. Based on our findings, we have some recommendations for the nascent field of multilingual ICL. First, critical evaluation is important. We need to compare ICL strategies to zero-shot learning, and ablate them with multiple templates. Second, as there is so much variance across models, tasks and languages, it is important to carefully scope claims about ICL. Last but not least, every language is different, so granular per-language analysis is a must in multilingual research.\n# 8 Related Work\nMultilingual in-context learning. Most multilingual in-context learning studies focus on benchmarking LLMs on diverse tasks and comparing them with smaller fine-tuned models (Ahuja et al., 2023; Asai et al., 2023; Zhang et al., 2023; Zhu et al., 2023). As these works focus on benchmark-\ning, their analysis of the role of demonstrations is limited. Ahuja et al. (2023) explore different prompting strategies by adjusting the language of templates and demonstrations. Zhang et al. (2023) find that demonstrations sometimes do not contribute to or even degrade model performance on code-switching. Zhu et al. (2023) look at machine translation and analyze the effects of template and demonstration selection with XGLM. In the context of cross-lingual transfer, Shi et al. (2022), Tanwar et al. (2023), and Agrawal et al. (2023) investigate demonstration selection for specific applications. In contrast, we take a much broader perspective and investigate the actual impact of demonstrations across a wide range of models, tasks and languages. English-centric demonstration analysis. Most of the current demonstration analysis literature focuses on English: Lu et al. (2022) analyze the sensitivity of ICL to the order of demonstrations, Min et al. (2022c) and Yoo et al. (2022) explore whether the ground truth labels matter for classification tasks, and Wei et al. (2023) investigate the sensitivity of various model families to different input-label mappings. Similarly, Pan et al. (2023) disentangle task recognition and task learning by manipulating the label space. Beyond this, Shi et al. (2023a) and Wang et al. (2023a) modify the validity of chain-of-thought (CoT) reasoning steps in demonstrations and explore the impact of this modification on mathematical reasoning. Also focusing on CoT, Chen et al. (2023) investigate how varying the number of demonstrations affects performance.\n# 9 Conclusion\nIn this paper, we conduct an in-depth multidimensional analysis on the impact of demonstrations in multilingual in-context learning. We find that the use of demonstrations does not always provide benefits compared to zero-shot learning, and that there is a large variance in performance across models, datasets and languages. While the quality of demonstrations influences the performance of base LLMs on certain tasks, the impact is significantly reduced for LLMs tuned with alignment techniques. We also examine the interplay between demonstrations and templates, finding that a carefully crafted template can further decrease the benefits of demonstrations. Our granular analysis contributes novel insights with nuance and paves the way for a more thoughtful multilingual ICL evaluation.\n# Limitations\nData contamination. Since LLMs are trained with a vast amount of data scraped from the internet, this might result in data contamination, i.e., when the training data includes test datasets. Ahuja et al. (2023) suspect that many multilingual datasets appear in the training data of GPT-4, which might lead to an overestimation of the model\u2019s capabilities. In the context of our work, our prompt might just be reminding LLMs of a task they have already seen, whereas on an unseen task, the impact of demonstrations might be different. We do not examine the impact of potential data contamination in our paper and leave an exploration of this to future work. Other demonstration choices. In this work, we choose to use demonstrations that are in the same languages as the test sample, due to our focus on evaluating inherent multilingual abilities of LLMs, as explained in Section 2.2. However, using English demonstrations for cross-lingual transfer or translating test samples into English has its own practical value for NLP applications. Additionally, it is worth exploring selecting demonstrations from a mixture of languages. Expanding our study to more setups would provide additional insights into multilingual and cross-lingual LLM abilities.\n# Other prompting methods. In Section 6\nonly experiment with manually augmented templates to illustrate how the choice of template can reduce the effectiveness of demonstrations. There is a broad literature on prompt engineering and prompt sensitivity (White et al., 2023; Gonen et al., 2023), suggesting that it is plausible that another prompt could reduce the gap between few-shot and zero-shot performance even further. Chain-ofthought (CoT) prompting is another approach with promising multilingual abilities (Shi et al., 2023b; Huang et al., 2023) that might affect our findings. Our manually-augmented templates are intended only as a starting point for further analysis, which we leave to future work.\n# Beyond automatic evaluation.\nmodel responses, we noticed some cases where a correct answer as evaluated by a human was not fully captured by automatic evaluation metrics. Human evaluation is time-consuming, expensive, and hard to source for the wide range of languages that we explore in our work. Another option is LLM\nevaluation, which is becoming increasingly popular (Fu et al., 2023a; Chan et al., 2024), but is also an expensive approach. More importantly, we have no guarantees about LLMs\u2019 multilingual capabilities. As a trade-off between cost and evaluation quality, we stick to automatic evaluation in our work for all tasks and languages.\n# Acknowledgments\nWe thank Matan Eyal for his valuable feedback. Our use of Microsoft Azure is sponsored by the Microsoft Accelerating Foundation Models Research (AFMR) program. Miaoran Zhang and Marius Mosbach received funding from the DFG (German Research Foundation) under project 232722074, SFB 1102. Vagrant Gautam and Jesujoba O. Alabi were supported by the BMBF\u2019s (German Federal Ministry of Education and Research) SLIK project under the grant 01IS22015C.\n# References\navid Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong, Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022. A few thousand translations go a long way! leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3053\u20133070, Seattle, United States. Association for Computational Linguistics.\nDivyanshu Aggarwal, Vivek Gupta, and Anoop Kunchukuttan. 2022. IndicXNLI: Evaluating multilingual inference for Indian languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10994\u201311006, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023. Incontext examples selection for machine translation. In Findings of the Association for Computational\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4de0/4de0cf68-91cc-4553-ba57-a3677de86a93.png\" style=\"width: 50%;\"></div>\nCanada. Association for Computational Linguistics. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2023. MEGA: Multilingual evaluation of generative AI. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4232\u20134267, Singapore. Association for Computational Linguistics. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association for Computational Linguistics. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. 2023. BUFFET: Benchmarking large language models for few-shot cross-lingual transfer. arXiv.\ntephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alshaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93\u2013104, Dublin, Ireland. Association for Computational Linguistics.\nHritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, and Dan Roth. 2023. Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11833\u201311856, Toronto, Canada. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. ChatEval: Towards better LLM-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations.\nHritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, and Dan Roth. 2023. Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11833\u201311856, Toronto, Canada. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. ChatEval: Towards better LLM-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations.\nJiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023. How many demonstrations do you need for in-context learning? In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11149\u201311159, Singapore. Association for Computational Linguistics. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719\u2013730, Dublin, Ireland. Association for Computational Linguistics. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485, Brussels, Belgium. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878\u2013891, Dublin, Ireland. Association for Computational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023a. GPTScore: Evaluate as you desire. arXiv. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023b. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations. Hila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and Luke Zettlemoyer. 2023. Demystifying prompts in language models via perplexity estimation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10136\u201310148, Singapore. Association for Computational Linguistics. Roee Hendel, Mor Geva, and Amir Globerson. 2023. In-context learning creates task vectors. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9318\u20139333, Singapore. Association for Computational Linguistics.\nHaoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in LLMs: Improving multilingual capability by cross-lingual-thought prompting. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12365\u2013 12394, Singapore. Association for Computational Linguistics. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. DSPy: Compiling declarative language model calls into self-improving pipelines. arXiv. Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Nguyen. 2023. ChatGPT beyond English: Towards a comprehensive evaluation of large language models in multilingual learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13171\u201313189, Singapore. Association for Computational Linguistics. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023a. Large language models understand and can be enhanced by emotional stimuli. arXiv. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023b. Unified demonstration retriever for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4644\u20134668, Toronto, Canada. Association for Computational Linguistics. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019\u20139052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered\niachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nprompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-ICL: Zero-shot in-context learning with pseudo-demonstrations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2304\u20132317, Toronto, Canada. Association for Computational Linguistics. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022b. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022c. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Reframing instructional prompts to GPTk\u2019s language. In Findings of the Association for Computational Linguistics: ACL 2022, pages 589\u2013612, Dublin, Ireland. Association for Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022b. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2024. State of what art? A call for multi-prompt LLM evaluation. arXiv. Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. In Findings of the Association for\nXinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-ICL: Zero-shot in-context learning with pseudo-demonstrations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2304\u20132317, Toronto, Canada. Association for Computational Linguistics.\nComputational Linguistics: ACL 2023, pages 12284\u2013 12314, Toronto, Canada. Association for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991\u201316111, Toronto, Canada. Association for Computational Linguistics.\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991\u201316111, Toronto, Canada. Association for Computational Linguistics. Shamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum, David Adelani, Seid Yimam, Ibrahim Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, Oumaima Hourrane, Alipio Jorge, Pavel Brazdil, Felermino Ali, Davis David, Salomey Osei, Bello Shehu-Bello, Falalu Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Messelle, Hailu Balcha, Sisay Chala, Hagos Gebremichael, Bernard Opoku, and Stephen Arthur. 2023. AfriSenti: A Twitter sentiment analysis benchmark for African languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13968\u201313981, Singapore. Association for Computational Linguistics. Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and David I. Adelani. 2023. How good are large language models on African languages? arXiv. OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. 2023. GPT-4 technical report. arXiv. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. 2023. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8298\u20138319, Toronto, Canada. Association for Computational Linguistics. Edoardo Maria Ponti, Goran Glava\u0161, Olga Majewska, Qianchu Liu, Ivan Vuli\u00b4c, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362\u20132376, Online. Association for Computational Linguistics.\nhamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum, David Adelani, Seid Yimam, Ibrahim Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, Oumaima Hourrane, Alipio Jorge, Pavel Brazdil, Felermino Ali, Davis David, Salomey Osei, Bello Shehu-Bello, Falalu Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Messelle, Hailu Balcha, Sisay Chala, Hagos Gebremichael, Bernard Opoku, and Stephen Arthur. 2023. AfriSenti: A Twitter sentiment analysis benchmark for African languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13968\u201313981, Singapore. Association for Computational Linguistics.\nShamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum, David Adelani, Seid Yimam, Ibrahim Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, Oumaima Hourrane, Alipio Jorge, Pavel Brazdil, Felermino Ali, Davis David, Salomey Osei, Bello Shehu-Bello, Falalu Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Messelle, Hailu Balcha, Sisay Chala, Hagos Gebremichael, Bernard Opoku, and Stephen Arthur. 2023. AfriSenti: A Twitter sentiment analysis benchmark for African languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13968\u201313981, Singapore. Association for Computational Linguistics. Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and David I. Adelani. 2023. How good are large language models on African languages? arXiv. OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. 2023. GPT-4 technical report. arXiv. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. 2023. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8298\u20138319, Toronto, Canada. Association for Computational Linguistics. Edoardo Maria Ponti, Goran Glava\u0161, Olga Majewska, Qianchu Liu, Ivan Vuli\u00b4c, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362\u20132376, Online. Association for Computational Linguistics.\nMaja Popovi\u00b4c. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612\u2013618, Copenhagen, Denmark. Association for Computational Linguistics.\nMaja Popovi\u00b4c. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612\u2013618, Copenhagen, Denmark. Association for Computational Linguistics. Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u20137. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States. Association for Computational Linguistics. Sebastian Ruder, Ivan Vuli\u00b4c, and Anders S\u00f8gaard. 2022. Square one bias in NLP: Towards a multidimensional exploration of the research manifold. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2340\u20132354, Dublin, Ireland. Association for Computational Linguistics. Timo Schick and Hinrich Sch\u00fctze. 2021. It\u2019s not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u20132352, Online. Association for Computational Linguistics. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying language models\u2019 sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023a. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210\u201331227. PMLR. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023b. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations. Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022. XRICL: Cross-lingual retrieval-augmented incontext learning for cross-lingual text-to-SQL semantic parsing. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5248\u2013 5259, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 to be reliable. In The Eleventh International Conference on Learning Representations. Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy Chakraborty. 2023. Multilingual LLMs are better cross-lingual in-context learners with alignment. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6292\u20136307, Toronto, Canada. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 35151\u201335174. PMLR. Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Eisenschlos, Sercan Arik, and Tomas Pfister. 2023. Universal self-adaptive prompting. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7437\u2013 7462, Singapore. Association for Computational Linguistics. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2023a. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2717\u20132739, Toronto, Canada. Association for Computational Linguistics. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023b. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9840\u20139855, Singapore. Association for Computational Linguistics. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023c. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Curran Associates, Inc. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023. Larger language models do in-context learning differently. arXiv. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics. Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, and Yu Qiao. 2023. OpenICL: An open-source framework for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 489\u2013498, Toronto, Canada. Association for Computational Linguistics. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceed-\nAnjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nAnjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Curran Associates, Inc. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023. Larger language models do in-context learning differently. arXiv. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics. Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, and Yu Qiao. 2023. OpenICL: An open-source framework for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 489\u2013498, Toronto, Canada. Association for Computational Linguistics. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687\u20133692, Hong Kong, China. Association for Computational Linguistics. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2422\u2013 2437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Winata, and Alham Aji. 2023. Multilingual large language models are not (yet) code-switchers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12567\u201312582, Singapore. Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv.\nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687\u20133692, Hong Kong, China. Association for Computational Linguistics. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2422\u2013 2437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Winata, and Alham Aji. 2023. Multilingual large language models are not (yet) code-switchers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12567\u201312582, Singapore. Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv.\n# A Experimental details\n# A.1 Tasks and datasets\nWe conduct experiments on 9 multilingual datasets with a wide coverage of tasks and languages, as shown in Table 4. All datasets are public research datasets and our experiments are consistent with their intended use, i.e., NLP evaluation. For the machine translation dataset MAFAND, English serves as the pivot language and there are two translation directions: en-xx (i.e., translating from English to another language) and xx-en (i.e., translating from another language to English). As the black-box training data of OpenAI APIs that we used is up to September 2021, we include the dataset release date in the table which can be taken as a clue to the severity of dataset contamination.\nDataset\nTask\nLanguages\n|Lang.|\nRelease Date\nXNLI\nnatural language inference\nEnglish, German, Russian, French, Spanish, Chinese, Vietnamese,\n15\n2019.09\nTurkish, Arabic, Greek, Thai, Bulgarian, Hindi, Urdu, Swahili\nIndicXNLI\nnatural language inference\nHindi, Bengali, Tamil, Marathi, Malayalam, Telugu, Kannada, Punjabi,\n11\n2022.04\nOriya, Assamese, Gujarati\nPAWS-X\nparaphrase identification\nEnglish, German, Japanese, French, Spanish, Chinese, Korean\n7\n2019.08\nXCOPA\ncommonsense reasoning\nChinese, Italian, Vietnamese, Indonesian, Turkish, Thai, Estonian,\n11\n2020.04\nTamil, Swahili, Haitian, Quechua\nXStoryCloze\ncommonsense reasoning\nEnglish, Russian, Spanish, Chinese, Indonesian, Arabic, Hindi,\n11\n2023.05\nBasque, Telugu, Burmese, Swahili\nAfriSenti\nsentiment analysis\nSwahili, Amharic, Hausa, Kinyarwanda, Yoruba, Tigrinya, Igbo, Oromo,\n14\n2023.05\nMoroccan Arabic, Algerian Arabic, Nigerian Pidgin, Mozambican Portuguese,\nTsonga, Twi\nXQuAD\nextractive QA\nEnglish, German, Russian, Spanish, Chinese, Vietnamese, Turkish, Greek,\n12\n2019.10\nRomanian, Thai, Hindi\nTyDiQA-GoldP\nextractive QA\nEnglish, Russian, Indonesian, Korean, Arabic, Finnish, Bengali, Telugu, Swahili\n9\n2020.02\nMAFAND\nmachine translation\nAmharic, Hausa, Kinyarwanda, Luganda, Luo, Chichewa, Nigerian Pidgin,\n14\n2022.06\nShona, Swahili, Setswana, Twi, Xhosa, Yoruba, Zulu\n<div style=\"text-align: center;\">Table 4: Multilingual benchmarking datasets.</div>\nTable 4: Multilingual benchmarking datasets.\n# A.2 In-context learning\nWe sample few-shot demonstrations from the validation set and evaluate the test set. For datasets without a test data split (XStoryCloze and TyDiQA), we sample few-shots from the train set and evaluate the validation set. Since XQuAD only has a validation data split, we utilize it for both demonstration sampling and evaluation, ensuring that the test sample itself is not included in its demonstrations. For chat models (Llama 2-Chat, GPT-3.5, and GPT-4), we limit the test sample size to a maximum of 200 in order to reduce inference expenses and ensure a fair comparison. We use GPT-3 style prompting templates for XGLM and Llama 2 as shown in Table 6. The templates for BLOOMZ and mT0 are shown in Table 7. For Llama 2-Chat, GPT-3.5 and GPT-4, default templates are shown in Table 8 and task instructions are used to assign a system role to the model. Inspired by Lai et al. (2023) and Li et al. (2023a), where emotional stimuli are able to enhance LLM understanding, we design formatting-focused templates (discussed in Section 6) to reinforce LLM to generate formatted outputs that are easier to evaluate automatically, as shown in Table 9.\n# A.3 Implementation\nOur codebase is adapted from OpenICL (Wu et al., 2023). We use int8bit model quantization11 for all models except OpenAI models. Experiments are conducted using a single NVIDIA A100-80GB GPU. As models have a maximum context length, we preserve complete demonstrations that can fit within the context window. We employ greedy decoding for model generation. For chat models, the maximum new token is set to 50, while for machine translation, it is set to 100. For other models, the maximum\n11In our preliminary experiments, we found that int8 quantization led to a performance degradation of 1-2% on a few classification datasets with Llama 2 and XGLM. Since this degradation is consistent across different setups, we believe that it would not affect our overall findings.\n# new token is set to 20, while for machine translation, it is set to 50. We use three seeds (0, 33, 42) in our experiments, and the single-seed results for BLOOMZ and mT0 are obtained with the seed 0.\n# B More results for varying numbers of demonstrations\nB More results for varying numbers of demonstrati In this section, we provide supplemental results for Section 4.\n# B More results for varying numbers of demonstratio In this section, we provide supplemental results for Section 4.\n# B More results for varying numbers of demonstrations\n# B.1 Results for BLOOMZ and mT0\nIn addition to the 5 models (base models and chat models) we discussed in the main content, we also experiment with two instruction-tuned models: BLOOMZ and mT0 (Muennighoff et al., 2023). The results for varying numbers of random demonstrations are shown in Figure 8. In line with findings from Asai et al. (2023), we observe significant performance degradation when using demonstrations compared to zero-shot learning in all cases. This decline can be attributed to their training scheme, where models are",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Multilingual in-context learning remains under-explored compared to English in-context learning, resulting in a lack of understanding of the role of demonstrations in this context. Existing benchmarks primarily focus on English, which may not be applicable to less represented languages in large language model training data.",
            "purpose of benchmark": "The benchmark is intended to evaluate the multilingual in-context learning abilities of large language models across various tasks and languages."
        },
        "problem": {
            "definition": "The benchmark addresses the effectiveness of in-context demonstrations for multilingual tasks, seeking to understand whether they enhance performance compared to zero-shot learning.",
            "key obstacle": "Existing benchmarks do not adequately explore the variability of model performance across different tasks and languages, often leading to an overestimation of the benefits of demonstrations."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need to scrutinize the actual impact of in-context demonstrations on model performance across diverse languages and tasks.",
            "opinion": "The authors believe that the importance of demonstrations in multilingual in-context learning may be overstated and that careful evaluation is required to understand their true impact.",
            "innovation": "This benchmark introduces a multidimensional approach to evaluate multilingual in-context learning, systematically analyzing the effects of demonstrations across various models, tasks, and languages.",
            "benchmark abbreviation": "MICL"
        },
        "dataset": {
            "source": "The dataset was sourced from 9 publicly available multilingual datasets covering classification and generation tasks.",
            "desc": "The dataset includes 9 multilingual datasets with a total of 56 languages, focusing on various tasks such as natural language inference, sentiment analysis, and machine translation.",
            "content": "The dataset comprises text data relevant to classification and generation tasks, including examples from diverse languages.",
            "size": "9",
            "domain": "Natural Language Inference",
            "task format": "Classification"
        },
        "metrics": {
            "metric name": "Rank Classification Accuracy, F1 Score",
            "aspect": "Accuracy",
            "principle": "The metrics were chosen to evaluate the performance of models on classification and generation tasks, reflecting their ability to understand and generate language correctly.",
            "procedure": "Models were evaluated based on their performance on test sets using the specified metrics, with comparisons made against zero-shot learning results."
        },
        "experiments": {
            "model": "The benchmark tested five models: XGLM, Llama 2, Llama 2-Chat, GPT-3.5, and GPT-4.",
            "procedure": "Models were trained and evaluated using a variety of demonstrations, with careful selection of task-specific templates for different model types.",
            "result": "The results indicated that the effectiveness of demonstrations varies significantly across models, tasks, and languages, with some models performing better with zero-shot learning.",
            "variability": "Variability was accounted for by conducting multiple trials and averaging results across different languages and tasks."
        },
        "conclusion": "The findings suggest that the benefits of in-context demonstrations in multilingual tasks may be overestimated, highlighting the need for nuanced evaluation across different models and languages.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive evaluation framework for multilingual in-context learning, enabling better understanding of model performance across diverse tasks.",
            "limitation": "The benchmark may not capture all aspects of multilingual in-context learning, particularly in cases of data contamination or language-specific nuances.",
            "future work": "Future research should explore additional datasets, cross-lingual transfer settings, and further investigate the role of templates in conjunction with demonstrations."
        },
        "other info": {
            "info1": "The benchmark includes a diverse range of tasks to ensure comprehensive evaluation.",
            "info2": {
                "info2.1": "The authors emphasize the importance of granular per-language analysis in multilingual research.",
                "info2.2": "The benchmark aims to address the gap in understanding multilingual in-context learning compared to English-centric studies."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "Multilingual in-context learning remains under-explored compared to English in-context learning, resulting in a lack of understanding of the role of demonstrations in this context."
        },
        {
            "section number": "1.3",
            "key information": "The benchmark is intended to evaluate the multilingual in-context learning abilities of large language models across various tasks and languages."
        },
        {
            "section number": "2",
            "key information": "Existing benchmarks primarily focus on English, which may not be applicable to less represented languages in large language model training data."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark addresses the effectiveness of in-context demonstrations for multilingual tasks, seeking to understand whether they enhance performance compared to zero-shot learning."
        },
        {
            "section number": "3.2",
            "key information": "This benchmark introduces a multidimensional approach to evaluate multilingual in-context learning, systematically analyzing the effects of demonstrations across various models, tasks, and languages."
        },
        {
            "section number": "6.4",
            "key information": "The findings suggest that the benefits of in-context demonstrations in multilingual tasks may be overestimated, highlighting the need for nuanced evaluation across different models and languages."
        }
    ],
    "similarity_score": 0.7344530840333001,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/The Impact of Demonstrations on Multilingual In-Context Learning_ A Multidimensional Analysis.json"
}