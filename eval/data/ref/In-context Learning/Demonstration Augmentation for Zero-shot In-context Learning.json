{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.01224",
    "title": "Demonstration Augmentation for Zero-shot In-context Learning",
    "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies have highlighted that the model's performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs. In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model's reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming. To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model's previously predicted historical samples as demonstrations for subsequent ones. DAIL brings no additional inference cost and does not rely on the model's generative capabilities. Our experiments reveal that DAIL can significantly improve the model's performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.",
    "bib_name": "su2024demonstrationaugmentationzeroshotincontext",
    "md_text": "# n Augmentation for Zero-shot In-context Learn\n# nstration Augmentation for Zero-shot In-contex\nYi Su1\u2217, Yunpeng Tai1\u2217, Yixin Ji1, Juntao Li1, Bowen Yan2, Min Zhang1 1School of Computer Science and Technology, Soochow University 2Department of Computer Science and Technology, Tsinghua University yisunlp@outlook.com; yunpengtai.typ@gmail.com yanbw@mail.tsinghua.edu.cn; {ljt,minzhang}@suda.edu.cn\nAbstract\nLarge Language Models (LLMs) have demonstrated an impressive capability known as Incontext Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies have highlighted that the model\u2019s performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs. In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model\u2019s reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming. To address these issues, we propose Demonstration Augmentation for Incontext Learning (DAIL), which employs the model\u2019s previously predicted historical samples as demonstrations for subsequent ones. DAIL brings no additional inference cost and does not rely on the model\u2019s generative capabilities. Our experiments reveal that DAIL can significantly improve the model\u2019s performance over direct zero-shot inference and can even outperform few-shot ICL without any external information. Our code is available at https://github.com/yisunlp/DAIL.\n3 Jun 2024\n# 1 Introduction\nLarge Language models (LLMs) have recently gained widespread attention due to their numerous advantages, including user-friendly interactions, convenient applications, and zero-shot capabilities (Wei et al., 2021; OpenAI, 2022; Scao et al., 2022; Zhang et al., 2022a; OpenAI, 2023;\n\u2217Equal Contribution.\nTouvron et al., 2023; Baichuan, 2023). However, the expanding parameter scale of LLMs poses a significant challenge to fine-tuning, demanding considerable investments in both time and computational resources. Therefore, In-context Learning (ICL), a method enabling LLMs to acquire knowledge through textual demonstrations without the need for parameter updates, has become increasingly important in recent times (Wei et al., 2021; Dong et al., 2022). Conditioning on some input-label pairs (demonstrations), LLMs can rapidly acquire the ability to solve new tasks in a few-shot manner just by combining the demonstrations and the sample together(Radford et al., 2019; Brown et al., 2020). However, many studies indicate that the model\u2019s performance is sensitive to the choice of demonstrations (Zhang et al., 2022b; Liu et al., 2022b; Hao et al., 2022; Lu et al., 2022a). In extreme cases, inadequately chosen demonstrations can significantly degrade the model\u2019s performance, causing a drastic drop from State-of-the-Art to near-random (Liu et al., 2022b). To address this challenge, researchers have proposed various solutions, including demonstration selection, calibration, and arrangement (Rubin et al., 2022; Min et al., 2022d,b; Zhao et al., 2021a; Chen et al., 2022; Yoo et al., 2022; Min et al., 2022a). These methods can improve the model\u2019s performance and stability under ICL across many tasks. Nevertheless, these approaches are still insufficient to ensure the reliable application of ICL in real-world scenarios, where our prior knowledge of user queries is often limited. Consequently, it is a common practice to construct an extensive demonstration pool, harness external databases, and implement various strategies such as selection, calibration, and arrangement methods, as mentioned earlier, to deal with all kinds of queries from users. However, this process entails a significant investment in time and financial resources. Therefore,\nsome researchers (Zhang et al., 2022c; Kim et al., 2022; Lyu et al., 2023; Chen et al., 2023) attempt to alleviate the reliance on external information by proposing zero-shot ICL. These approaches leverage the model\u2019s generative capabilities to produce the required information for the inference process. In this context, zero-shot ICL offers a promising avenue for more efficient and cost-effective deployment for ICL. These methods can reduce the dependence on external information, but the quality of the content generated by the model cannot be guaranteed, which may pose some potential risks. Furthermore, the generation process is time-consuming, which can bring additional costs during inference. To address these problems, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model\u2019s previously predicted historical samples as demonstrations for subsequent ones1. Specifically, we only need to maintain a memory bank of a small size M and define the entry, selection, and deletion strategies. During the inference phase, the selection strategy chooses the most suitable demonstrations from the memory bank. Subsequently, we use the entry strategy to add the predicted sample to the memory bank. Upon reaching maximum capacity, we use the deletion strategy to remove some stored samples. Our experiments on different benchmarks and models demonstrate the effectiveness of DAIL. Overall, our contributions in this work include: \u2022 We point out the potential limitations of previous zero-shot methods in stability and inference time. \u2022 We introduce DAIL, an easy yet effective method to enhance zero-shot ICL. \u2022 Our experiments reveal that DAIL can significantly improve the model\u2019s performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.\n# 2 Preliminary\n# 2.1 Problem Formulation\nIn this subsection, we briefly summarize the inference process of ICL. A Large Language Model (LLM) can be formalized as a function f : X \u2212\u2192Y , mapping the input space X to the output space Y . The corresponding dataset comprises a set of labeled demonstrations {xi s, yi s}ns i=1 and a set of unlabeled queries {xi t}nt i=1. Then, a carefully crafted\n1At the beginning, we use zero-shot inference because there is no historical samples.\nMethod\nInputs\nLabels\nAUTO-COT (Zhang et al., 2022c)\nfrom training set\nno need\nZ-ICL (Lyu et al., 2023)\nfrom external corpus\nno need\nSG-ICL (Kim et al., 2022)\nno need\ngiven\nSelf-ICL (Chen et al., 2023)\nno need\nno need\nDAIL (Ours)\nno need\nno need\nTable 1: A comparison to prior attempts on zero-shot ICL. Self-ICL and DAIL do not require any external information to construct demonstrations.\ntemplate t is utilized to transform each sample into a natural language sentence that the model can process. During the inference stage for a given query, K demonstrations are selected from the demonstration pool based on a selection strategy such as TopK (Liu et al., 2022b). Subsequently, these chosen demonstrations and the query are combined to construct the input sequence for the model:\nwhere t(\u00b7) is the transformation of the template. The model processes the input sequence and generates the final output, denoted as:\n(2)\nwhere V is a mapping function that converts the model\u2019s output into a label in the label space. It can be a text-level matching function or a selection mechanism based on probability or perplexity.\n# 2.2 Potential Risks of Previous Methods\nWhile previous works have delved into zero-shot ICL, they mainly focus on reducing the reliance on labeled demonstrations and are not entirely independent of external information (Table 1 shows the resources needed for each method). We concentrate on a setting that requires no external information, aiming to minimize the extra cost. Moreover, these methods rely on the generative capabilities of the model, so there may be issues with poor generation quality and increased inference time. We take Self-ICL (Chen et al., 2023) as an example. Self-ICL performs the following three actions upon receiving a question: 1) It uses a humandesigned prompt to guide the model in generating K new, related, and diverse questions based on the original question. 2) It employs zero-shot inference to obtain the answers for the generated K questions respectively. 3) It concatenates these K questions and their answers to serve as demonstrations for ICL. In our experiments, we find that Self-ICL\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8678/86786aa3-7a60-4613-a94f-4b85bec79f29.png\" style=\"width: 50%;\"></div>\nFigure 1: A bad case for Self-ICL, the quality of the generated samples is poor, with repeated options, false labels, and too similar semantics, which leads to the decline of the model\u2019s performance. For simplicity of the figure, we omit the generated labels of demonstrations. relies heavily on the generative capability of the model. When the model generates poor demonstrations, it will hurt the performance of ICL (Figure 1). Furthermore, Self-ICL requires more queries and token consumption than direct zero-shot inference, resulting in increased inference costs. This is particularly pronounced in its generation process (Figure 2), where the expense of generating a token exceeds that of encoding a token. Consequently, this poses a challenge to computing resources during deployment for Self-ICL. To address these challenges, we need to obtain more reliable demonstrations at a lower cost. It is intuitive that text provided by humans typically have higher quality than that generated by models. With the absence of external information, the human-supplied text available to LLM is only user queries. Hence, our strategy involves leveraging previously predicted historical samples as demonstrations. This offers several advantages over other zero-shot ICL methods, including superior text quality, independence from the model\u2019s generative capabilities, and lower acquisition costs.\n# 3 Method\nFigure 3 illustrates an overview of our method. Suppose we have a LLM f and a set of user-issued queries to respond. We initialize a memory bank\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a3ff/a3fff83d-91e2-4d3d-b19d-745902c0dc6b.png\" style=\"width: 50%;\"></div>\nFigure 2: Time consumption (in seconds) for different methods and sequence lengths (batch size = 16). We use LLaMA-2-7B (Touvron et al., 2023) as the base model. Encode: cost of encoding n tokens. Generate: cost of generating n tokens. 3-shot: ICL with three demonstrations. For simplicity, we assume that all the demonstrations generated by the model have the same sequence length as the query.\nwith a maximum capacity of M. At step 0, the memory bank is empty, and we directly process the query using zero-shot inference. At step t, when a new query arrives, we employ the selection strategy to search for K samples in the memory bank. If the number of samples in the memory bank is less than K, we extract all demonstrations from the bank. Following the model\u2019s output, we use the entry strategy to add the current sample, i.e., the current query paired with the correlated model response, to the memory bank. At step t + 1, the sample from the previous step t has been in the memory bank and can be selected as a demonstration to help answer the new query. In the following subsections, we will elaborate the details of our entry, selection, and deletion strategies, which manage the dynamic data flow within our memory bank.\n# 3.1 Entry Strategy\nFollowing the processing of each query, we combine the query and the corresponding response into a sample and directly add the sample to the memory bank. Despite its simplicity, the entry strategy is a fundamental building block in our method.\n# 3.2 Selection Strategy\nOur selection strategy involves assigning a score to each sample in the memory bank and selecting those with the highest scores. Each sample\u2019s score comprises two factors: the Selection Score and the Entropy Score. Assuming the query that the model\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2baa/2baad987-5cf1-4d18-a5e9-88dbee80ec0b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Overview of our method. After each inference, we combine the current query with the model\u2019s output and add them to the memory bank. After step t, the sample is added to the memory bank and then used as a demonstration at step t+1.</div>\nneed to respond is x, and a sample in the memory bank is \u02c6x, we will explain how to compute the Selection Score and the Entropy Score for them.\n# 3.2.1 Selection Score\nThe Selection Score primarily comes from existing demonstration selection methods. We have experimented with various selection methods, including random selection, BM25 (Robertson et al., 2009), and TopK (Liu et al., 2022b).\nRandom score Under random selection, all samples are chosen with equal probability. Therefore, we assign a score of 0 to all samples.\nBM25 score The BM25 algorithm (Robertson et al., 2009) is a well-known information retrieval method based on the Okapi TF-IDF algorithm (Ramos et al., 2003). It is widely employed for ranking queries in information retrieval tasks. We can leverage it to compute a similarity score between \u02c6x and x.\n(3)\nwhere BM25(\u00b7) represents the BM25 algorithm.\nTopK score Noting that selecting demonstrations with semantics closer to the query enhances the performance of ICL (Liu et al., 2022b), we utilize Sentence-BERT (Reimers and Gurevych, 2019) to calculate the similarity between \u02c6x and x.\n(4)\nwhere emb(\u00b7) denotes the process of computing the hidden states of a sentence with Sentence-BERT.\n# 3.2.2 Entropy Score\nIntuitively, samples with lower entropy should be prioritized for selection because this suggests that the sample is simpler and the pseudo-labels are more reliable (Su et al., 2023). We can compute the entropy of a sample with the following formula:\nwhere p (yc|x) is the next token prediction probability of x provided by the model.\n# 3.2.3 Final Selection Strategy\nThe final score consists of two components: the Selection score and the Entropy Score. We normalize each score to mitigate the differences between the aforementioned two types of scores.\n(6)\nwhere N(\u00b7) stands for normalization, and \u03b1 is a manually set hyper-parameter to balance the weight of the two scores. In addition to the three selection methods above, we also utilize DPP (Kulesza and Taskar, 2011) to refine the TopK selections, aiming to enhance the diversity of the demonstrations. Specifically, we employ the TopK Score and Entropy Score to select some candidates and apply the DPP algorithm to choose K demonstrations from these candidates. In total, we consider four selection methods, and their comparison will be discussed in Section 5.1.\n# 3.3 Deletion Strategy\nUpon reaching full capacity, we delete half of the samples from the bank. We explore three different deletion strategies: Random, FIFO, and Diverse.\nModels\nMethods\nHumanities\nSTEM\nSocial Sciences\nOther\nAverage\nTime\nLLaMA-2-7B\nZero-Shot\n49.23\n34.73\n52.77\n49.12\n45.37\n-\nFew-shot\n50.54\n36.80\n54.07\n49.76\n46.75\n\u00d70.99\nSelf-ICL\n48.61\n34.86\n49.98\n48.94\n44.64\n\u00d747.28\nOurs\n51.76\n37.52\n54.12\n50.01\n47.33\n\u00d71.00\nMistral-7B\nZero-Shot\n62.93\n48.29\n66.97\n61.58\n58.83\n-\nFew-shot\n66.07\n49.55\n72.18\n65.11\n61.90\n\u00d71.00\nSelf-ICL\n62.62\n48.10\n67.16\n60.85\n58.56\n\u00d7101.68\nOurs\n66.62\n51.36\n74.30\n64.11\n62.80\n\u00d71.00\nOpenChat-7B\nZero-Shot\n67.50\n49.72\n71.67\n63.97\n61.90\n-\nFew-shot\n70.31\n51.78\n73.01\n66.33\n64.05\n\u00d71.00\nSelf-ICL\n67.74\n49.30\n70.74\n63.22\n61.44\n\u00d7134.87\nOurs\n70.67\n51.40\n74.66\n66.77\n64.47\n\u00d71.00\nTable 2: Accuracy (%) on the MMLU benchmark with different models and different methods. Time: the multiples of time spent by each method in reasoning the entire benchmark compared to DAIL. We omit the comparison with Zero-Shot in terms of time. The selection strategy of our reported result is DPP, and the deletion strategy is Diverse. Bold: the best results. We report the template in Appendix A.4 and the detailed results in Appendix A.5.\nRandom Randomly select half of the samples and delete them from the bank.\n# Random Randomly select half of the samples and delete them from the bank.\nFIFO Delete the samples that entered the bank earlier (First-In-First-Out).\nDiverse We aim to preserve the diversity of the samples in the bank after deletion. We employ the TopK Score mentioned above to calculate the similarity between each sample and the entire bank. Subsequently, we delete the samples with higher similarity, thereby maintaining a diverse set of samples in the memory bank.\n# 4 Experiments\n# 4.1 Baselines\nZero-Shot Zero-shot inference. The model directly process the query with no demonstration. CoT Chain-of-Thought (Wei et al., 2022), an easy and effective method that utilizes specific prompts to stimulate the model\u2019s own capabilities. Few-Shot Few-shot inference. For each task, meticulously crafted demonstrations are provided by humans. Note that the comparison between Few-Shot and other baselines is not entirely fair, as Few-Shot requires additional external information. Self-ICL A zero-shot ICL method allows the model to generate new samples as demonstrations based on the query (Chen et al., 2023).\n# 4.2 Datasets\nMMLU (Hendrycks et al., 2020) Commonly used to evaluate the common sense reasoning ability of LLMs, MMLU consists of multiple-choice\nquestions from various domains. It includes 57 subsets covering subjects in science, technology, humanities, and other areas. Each subset has four demonstrations. BBH (Suzgun et al., 2022) Derived from a subset of tasks within the BIG-Bench benchmark (Srivastava et al., 2022), BBH includes tasks where existing LLMs struggle to reach average humanrater performance. We focus on the multiple-choice tasks, as done in Chen et al. (2023). The demonstrations are provided in Chen et al. (2023), and each subset has three demonstrations.\n# 4.3 Models\nFor MMLU, we utilize LLaMA-2-7B (Touvron et al., 2023), Mistral-7B (Jiang et al., 2023) and OpenChat-3.5-7B (Wang et al., 2023) as our backbone. They are the latest lightweight models with powerful capabilities. For BBH, we employ gpt3.5-turbo-instruct and gpt-4-1106-preview from the GPT family (OpenAI, 2022), which are currently the most popular and influential LLMs.\n# 4.4 Implementation details\nThe number of candidates for DPP is 10. We set \u03b1 to 0.1 for MMLU. The size of the memory bank M is 2,000. We adopt the decoding strategy from Chen et al. (2023). For MMLU, we set the number of demonstrations to four. For BBH, we set the number of demonstrations to three, consistent with Chen et al. (2023), and we use the same prompt as it. In BBH, we cannot obtain the logits of gpt-3.5turbo-instruct and gpt-4-1106-preview, so we omit the Entropy Score and solely leverage the Selec-\n<div style=\"text-align: center;\">gpt-3.5-turbo-instruct</div>\nBBH Tasks\ngpt-3.5-turbo-instruct\ngpt-4-1106-preview\nZero-Shot\nCoT\nFew-Shot\nSelf-ICL\nOurs\nZero-Shot\nFew-Shot\nSelf-ICL\nOurs\nBoolean Expressions\n84.80\n85.20\n89.60\n88.40\n85.60\n67.20\n93.20\n92.80\n94.00\nCausal Judgement\n42.25\n55.08\n63.64\n12.30\n57.22\n73.80\n69.19\n69.19\n70.27\nDate Understanding\n59.20\n44.80\n52.20\n57.60\n55.20\n48.40\n73.20\n74.80\n79.20\nDisambiguation QA\n60.00\n50.40\n63.60\n63.20\n62.40\n71.20\n79.20\n80.40\n65.60\nFormal Fallacies\n52.00\n52.40\n54.80\n50.40\n52.00\n70.40\n79.60\n76.00\n80.00\nGeometric Shapes\n34.00\n34.40\n45.60\n36.40\n32.80\n28.40\n43.60\n36.69\n49.40\nHyperbaton\n82.40\n71.20\n65.60\n82.80\n80.00\n73.60\n80.80\n88.00\n87.20\nLogical Deduction (five objects)\n42.00\n36.40\n38.00\n38.40\n42.00\n44.80\n63.20\n70.40\n73.60\nLogical Deduction (seven objects)\n41.60\n28.40\n38.80\n34.80\n42.40\n45.20\n60.00\n67.20\n64.40\nLogical Deduction (three objects)\n56.00\n60.40\n60.40\n59.20\n55.20\n86.80\n88.80\n94.00\n92.40\nMovie Recommendation\n74.80\n77.20\n78.40\n76.00\n71.08\n80.40\n92.00\n80.40\n92.00\nNavigate\n42.80\n52.40\n50.80\n64.80\n53.20\n71.60\n72.80\n75.20\n75.20\nPenguins in a Table\n51.37\n59.59\n52.74\n55.48\n50.68\n74.66\n76.03\n80.82\n80.14\nReasoning about Colored Objects\n54.80\n75.20\n57.20\n56.40\n56.00\n86.80\n86.00\n82.80\n84.40\nRuin Names\n70.80\n41.20\n72.40\n64.80\n67.34\n58.63\n90.80\n88.00\n89.20\nSalient Translation Error Detection\n41.60\n46.40\n51.60\n51.20\n45.20\n68.40\n67.60\n68.40\n69.20\nSnarks\n63.48\n61.80\n58.40\n60.67\n64.61\n84.66\n86.52\n82.02\n90.12\nSports Understanding\n62.00\n63.20\n86.40\n50.00\n81.60\n84.80\n88.80\n85.20\n90.40\nTemporal Sequences\n20.80\n36.00\n38.80\n32.80\n40.00\n97.60\n100.00\n99.20\n100.00\nTracking Shuffled Objects (five objs)\n18.00\n24.80\n17.20\n16.40\n21.20\n36.40\n33.60\n28.23\n35.08\nTracking Shuffled Objects (seven objs)\n17.60\n40.90\n12.40\n12.40\n14.40\n35.60\n28.80\n28.05\n32.93\nTracking Shuffled Objects (three objs)\n32.40\n46.00\n32.40\n36.80\n33.60\n49.20\n41.20\n33.87\n38.21\nWeb of Lies\n15.20\n38.80\n50.00\n38.40\n52.00\n48.00\n77.20\n52.40\n62.80\nAll Tasks (avg)\n48.69\n50.08\n53.21\n49.55\n52.86\n64.07\n72.50\n70.81\n73.47\nTable 3: Accuracy (%) on the BBH benchmark of gpt-3.5-turbo-instruct and gpt-4-1106-preview. The selection strategy of our reported result is DPP, and the deletion strategy is Diverse. The results of Zero-Shot and Self-ICL of gpt-3.5-turbo-instruct are extracted from Chen et al. (2023). We report the cost in Appendix A.3.\ntion Score to choose the demonstrations for DAIL, which may lead to a decline in its performance. For Sentence-BERT, we use the mostly used checkpoint from Huggingface2 to get the hidden states of the queries. All the experiments are completed on NVIDIA A100-40G GPUs.\n# 4.5 Results on MMLU\nTable 2 shows the main results of each method on MMLU with different models. We have the following observations:\nSelf-ICL performs poorly on MMLU. The performance of Self-ICL on MMLU is consistently inferior to that of Zero-Shot across various models. This suggests that Self-ICL may require stronger model generative capabilities, and the models we choose may not be sufficient to generate high-quality demonstrations. Addressing this issue might necessitate the use of larger and more powerful models, which could impose certain cost concerns on model deployment. DAIL achieves State-of-the-Art (SOTA) results. DAIL stands out by significantly enhancing the model performance over Zero-Shot and can even surpass Few-Shot with no external information.\nSelf-ICL performs poorly on MMLU. The performance of Self-ICL on MMLU is consistently inferior to that of Zero-Shot across various models. This suggests that Self-ICL may require stronger model generative capabilities, and the models we choose may not be sufficient to generate high-quality demonstrations. Addressing this issue might necessitate the use of larger and more powerful models, which could impose certain cost concerns on model deployment.\nDAIL achieves State-of-the-Art (SOTA) results. DAIL stands out by significantly enhancing the model performance over Zero-Shot and can even surpass Few-Shot with no external information.\n2https://huggingface.co/sentence-transformers/all-mpnetbase-v2\n<div style=\"text-align: center;\">gpt-4-1106-preview</div>\nFurthermore, DAIL is effective across various models, indicating that it does not rely on the model\u2019s generative capabilities and possesses strong generalization ability.\nDAIL brings no additional inference time. The inference speed of Self-ICL is quite slow due to the substantial amount of time required to generate demonstrations for each query. DAIL surpasses Self-ICL in inference speed hundreds of times and is comparable to Few-Shot inference. This remarkable efficiency brings a substantial reduction in deployment costs for real-world applications.\nDAIL is a practical method for real-world applications. In scenarios with limited resources, DAIL presents a feasible solution capable of acquiring high-quality demonstrations at a minimal cost. It consistently enhances the capabilities of ICL, making it an effective and efficient approach for real-world applications.\n# 4.6 Results on BBH\nTable 3 shows the main results of each method on the BBH benchmark with different models. We have the following observations:\nSelf-ICL performs well but sometimes harms the model performance. Overall, compared to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/065b/065b8425-6ca0-44f5-8397-a0028b22f86b.png\" style=\"width: 50%;\"></div>\nFigure 4: Accuracy (%) on MMLU with different selec tion strategies. 70\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79c8/79c8c7d5-a671-4542-bac5-b80880b995df.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Accuracy (%) on MMLU with different deletion strategies.</div>\nZero-Shot, Self-ICL can achieve decent improvements. However, in some tasks, it can still cause substantial damage to the performance of the model (Causal Judgement, 42.25% \u219212.30%). Even with a powerful model, the demonstrations generated by Self-ICL may be unsatisfactory, leading to a decline in the performance of ICL. The instability makes it impossible to play a role in real-world applications. DAIL Significantly Boosts ICL Performance. For gpt-3.5-turbo-instruct, DAIL outperforms ZeroShot by 4.17% and is only 0.35% lower than Few-Shot. For gpt-4-1106-preview, DAIL surpasses Zero-Shot by 9.4% and exceeds Few-Shot by 0.97%. It demonstrates the impressive capability and generalizability of DAIL.\n# 5 Analysis\n# 5.1 Impact of Selection Strategy\nThe selection strategy are important for the success of DAIL. We propose four selection strategies and we will compare the effects of them on the performance of ICL. We conduct experiments employing different models and selection strategies on the MMLU Benchmark, Figure 4 shows the results of our experiments. The results reveal that the DPP outperforms other selection strategies, with TopK\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/713a/713a9a25-0630-47bc-aeeb-6f1903f11f28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Accuracy (%) on MMLU with different \u03b1. We use Mistral as the base model.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f2dc/f2dcea88-f73b-4d6f-a68b-9806e5979d52.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Accuracy (%) on MMLU with different M.</div>\nbeing the closest competitor. This indicates that semantic similarity plays a crucial role in DAIL. DPP can increase the diversity of the demonstrations while maintaining the semantic similarity, which may be the primary reason for its effectiveness. Although BM25 can compute the semantic similarity of the demonstrations and the test sample, its capability to represent semantic similarity is weaker than TopK. Consequently, while BM25 generally outperforms Random, it does not match the performance achieved by TopK and DPP.\n# 5.2 Impact of Deletion Strategy\nWe investigate the impact of deletion strategy using different models on the MMLU Benchmark, and the results are presented in Figure 5. Various deletion strategies show a minor impact on the model\u2019s performance, with the Diverse deletion strategy slightly outperforming others. Nevertheless, we suggest that in a more dynamic environment, where we need to process various test samples from different users, the effectiveness of the Diverse deletion strategy may become more apparent.\n# 5.3 Impact of \u03b1\n\u03b1 is a crucial hyper-parameter in DAIL, balancing the Selection and Entropy scores. To explore the impact of \u03b1, we conduct experiments using Mistral on the MMLU Benchmark, and the results are depicted in Figure 6. In the case of the Random selection strategy, a notable improvement in model performance is observed as \u03b1 increases from 0 to 0.01. Subsequently, as \u03b1 further increases, performance fluctuates slightly but consistently remains higher than the result when \u03b1 is set to 0. This reveals that incorporating the Entropy Score enhances the model\u2019s performance. When considering other selection strategies, the model\u2019s performance tends to rise initially and then decline. This is because when \u03b1 is small, the combined effect of the Selection Score and the Entropy Score leads to a better selection of demonstrations. However, when \u03b1 is too large, the excessive weight of the Entropy Score reduces the impact of the Selection Score, and make the performance of ICL decrease. Thus, choosing a suitable \u03b1 is crucial, and we find that 0.1 is a good value.\n# 5.4 Impact of M\nThe size of the Memory Bank (M) is a critical factor influencing DAIL\u2019s performance. Figure 7 displays the results of experiments with different models and M on the MMLU Benchmark. We can conclude from the figure that a too small M can harm the performance of DAIL, which may be because the limited number of samples in the memory bank makes it hard to find sufficient similar and diverse demonstrations. This limitation is alleviated as M reaches 500. Below this threshold, DAIL\u2019s performance improves with the increase of M, and above this threshold, the performance of DAIL stabilizes at a satisfactory level. Considering that a M that is too large will make the selection process slower and increase the cost of sample storage, we set M to 2,000.\n# 6 Related Work\n# 6.1 Understanding ICL\nIn recent years, much research has delved into scaling up parameters and training data for LLMs, uncovering emergent capacities such as instructionfollowing, In-context Learning (ICL), and chainof-thought. In the realm of ICL, researchers focus on optimal demonstration selection, boosting ICL\ncapabilities, and understanding underlying mechanisms. As highlighted by Zhao et al. (2021b), the instability of ICL is a critical problem, where factors like prompt format, demonstration examples, and example order significantly impact performance. Despite being a challenging problem, there have been many efforts to address optimal sample selection using heuristic (Liu et al., 2022a; Su et al., 2022) and model-based methods (Lu et al., 2022b; Wu et al., 2023; Levy et al., 2023). From another perspective, many researchers are considering enhancing the capabilities of ICL. For example, Min et al. (2022c) enhance the performance of ICL by reducing its distance from pre-training tasks, and Zhao et al. (2021a) eliminate the biases of some specific labels introduced by demonstrations in ICL, thereby making the distribution of label logits closer to the actual situation.\n# 6.2 Zero-shot ICL\nDespite the success of ICL, many studies have highlighted that the model\u2019s performance is sensitive to the choice of demonstrations. Although researchers have delved into the optimization of prompts and demonstrations (Min et al., 2022d,b; Zhao et al., 2021a; Chen et al., 2022; Min et al., 2022a), the reliance on a substantial amount of annotated data for demonstrations in ICL introduces additional data collection costs. Consequently, there is a growing interest in exploring the generative capabilities of LLMs as a method to mitigate the dependency on external information. Addressing the challenge of few-shot Chain-of-Thought (CoT) without human annotations, Zhang et al. (2022c) leverage zeroshot CoT for demonstration construction. However, their approach still depends on an existing training set as input to zero-shot CoT. Lyu et al. (2023) explore pseudo-input generation from a raw text corpus but rely on external sources to construct pseudo-inputs. Similarly, Kim et al. (2022) investigates the generation of pseudo-inputs by the LLM itself but requires access to the label set and conditioning the language model on a label provided in the prompt. Chen et al. (2023) propose a method that does not require any external information but still has the problem of relying on model generation capabilities and slow inference speed.\n# 7 Conclusion\nIn this paper, we analyze some previous zero-shot ICL methods and point out their shortcomings in\nterms of stability and inference time. To address these challenges, we propose DAIL, a simple yet effective zero-shot ICL method. Our experiments demonstrate that DAIL can significantly enhance the performance of ICL without any external information and bring no inference latency, which indicates that DAIL has substantial potential in real-world applications.\n# Limitations\nWhile DAIL has demonstrated superior accuracy and inference speed compared to all baselines, it is important to acknowledge its limitations:\n\u2022 Obtaining the Entropy Score in DAIL requires accessing logits, which can be challenging when using APIs for inference. This introduces deployment challenges for DAIL in real-world applications. \u2022 Our validation of DAIL\u2019s performance has primarily focused on MMLU and BBH, both of which involve multiple-choice tasks. Its effectiveness in open-domain text generation tasks has yet to be confirmed. \u2022 Storing previously inferred samples poses potential privacy concerns and increases the risk of privacy breaches. In scenarios prioritizing data security, DAIL may not be the most suitable solution.\n# Acknowledgements\nWe want to thank all the anonymous reviewers for their valuable comments. Juntao Li and Bowen Yan are the corresponding authors. This work was supported by the National Science Foundation of China (NSFC No. 62206194), the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20220488), Young Elite Scientists Sponsorship Program by CAST (2023QNRC001), and Supercomputing Center in Yancheng, Grant No. 20231001.\n# References\nBaichuan. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305.\n# Baichuan. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nMingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. 2022. Improving in-context few-shot learning via self-supervised training. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3558\u20133573. Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. 2023. Self-icl: Zero-shot in-context learning with self-generated demonstrations. arXiv preprint arXiv:2305.15035. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234. Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In International Conference on Learning Representations. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082. Alex Kulesza and Ben Taskar. 2011. k-dpps: Fixed-size determinantal point processes. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1193\u20131200. Itay Levy, Ben Bogin, and Jonathan Berant. 2023. Diverse demonstrations improve in-context compositional generalization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401\u2013 1422, Toronto, Canada. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022a. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. 2022b. What makes good in-context examples for gpt-3?\nIn Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022a. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-icl: Zero-shot in-context learning with pseudo-demonstrations. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022b. Metaicl: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022c. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022d. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064. OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. Open AI, blog. OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022a. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-icl: Zero-shot in-context learning with pseudo-demonstrations. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316\u20135330. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022b. Metaicl: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022c. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022d. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064. OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. Open AI, blog. OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nJuan Ramos et al. 2003. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning, volume 242, pages 29\u201348. Citeseer. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022. Selective annotation makes language models better few-shot learners. Yi Su, Yixin Ji, Juntao Li, Hai Ye, and Min Zhang. 2023. Beware of model collapse! fast and stable test-time adaptation for robust question answering. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12998\u201313011. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423\u20131436, Toronto, Canada. Association for Computational Linguistics. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2422\u2013 2437. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022c. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021a. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021b. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\n# A Appendix\n# A.1 Analyis about the order of samples\nIn our experiments, we do not shuffle the order of the samples. According to our analysis, the order of samples is not expected to have a significant impact on the results. Considering an ideal scenario where the memory bank is infinitely large, and we only use cossimilarity for demonstration selection. Additionally, each sample selects only one demonstration. Since cosine similarity is symmetric, if the optimal choice for sample A is sample B, then the optimal choice for sample B is sample A. However, either sample A or sample B must enter the memory bank first, while the other enters later. Assuming sample A enters the memory bank first, when the model infers sample A, it will select the sub-optimal sample C. If sample C is not in the memory bank, it will select an even less optimal choice. But at this point, sample B can choose its optimal solution, which is sample A, and sample C can at least choose its sub-optimal solution, which is sample A. Following this logic, if one sample selects a better choice, then another sample will inevitably have to select a worse choice. Consequently, regardless of the order in which samples enter the memory bank, the quality of demonstrations selected by all samples will be relatively consistent overall. Therefore, the performance of the entire dataset will not vary significantly. We conduct a series of experiments to investigate this, and the results is shown in Table 4.\nMethods\nHumanities\nSTEM\nSocial Sciences\nOther\nAverage\nZero-shot\n62.93\n48.29\n66.97\n61.58\n58.83\nSamples\n66.48\n51.13\n73.39\n65.11\n62.75\nSubsets\n66.58\n51.20\n72.87\n65.26\n62.75\nAll (M=2000)\n65.79\n50.24\n73.38\n64.11\n62.07\nAll (M=10000)\n65.79\n50.24\n73.38\n64.11\n62.74\nOurs\n66.62\n51.36\n74.30\n64.11\n62.80\nTable 4: Results of different orders of the sample. Samples: shuffling the order of samples within each subset. Subsets: shuffling the order in which subsets appear. All: shuffling the order of the entire dataset. M: the size of the memory bank. Base model: Mistral-7B.\nFrom the results, it can be observed that shuffling the order does not have much impact, except when shuffling the entire dataset. This is because that some useful samples are removed before a subset is complete, leading to a slight decrease in model performance. However, this issue can be easily resolved by increasing the memory size M.\nIncreasing M does not significantly increase costs, so this problem can be perfectly addressed.\n# A.2 Some results on other tasks A.3 Details of Experimental Cost\nMethods\nZero-Shot\nFew-Shot\nSelf-ICL\nOurs\ngpt-3.5-turbo-instruct\nInput Tokens\n763K\n2,499K\n4,303K\n2,765K\nOutput Tokens\n15K\n15K\n1,613K\n15K\nCost\n1.15\n3.75\n9.68\n4.14\ngpt-4-1106-preview\nInput Tokens\n940K\n2,352K\n4,992K\n2,577K\nOutput Tokens\n15K\n15K\n1,491K\n15K\nCost\n9.85\n23.97\n94.65\n26.22\nTable 5: The number of consumed tokens and the cost (in US dollars) of experiments, the results of SelfICL and Zero-Shot under gpt-3.5-turbo-instruct are estimated based on the cost reported in Chen et al. (2023).\n# A.4 Prompt\nWe present the prompt for the MMLU benchmark in Table 6.\nDemonstration:\nQuestion: A person wants to start saving money\nso that they can afford a nice vacation at the\nend of the year. After looking over their budget\nand expenses, they decide the best way\nto save money is to\nA. make more phone calls\nB. quit eating lunch out\nC. buy less with monopoly money\nD. have lunch with friends\nAnswer: B\nTest sample:\nQuestion: The complete resynthesis of phospho-\ncreatine after very high intensity exercise\nnormally takes:\nA. about 10 seconds.\nB. about 30 seconds.\nC. about 1 minute.\nD. about 4 minutes.\nAnswer:\n<div style=\"text-align: center;\">Table 6: Prompt for MMLU.</div>\n# A.5 Detailed results\nWe provide detailed results for the MMLU benchmark in Table 7.\nSubsets\nLLaMA-2-7B\nMistral-7B\nOpenchat-7B\nZS\nFS\nSelf\nDAIL\nZS\nFS\nSelf\nDAIL\nZS\nFS\nSelf\nDAIL\nabstract algebra\n25.25\n28.28\n24.24\n28.28\n28.28\n28.28\n28.28\n27.27\n31.31\n33.33\n31.31\n30.30\nanatomy\n43.28\n43.28\n41.04\n44.03\n57.46\n59.70\n59.70\n61.19\n61.19\n64.18\n55.97\n63.43\nastronomy\n45.03\n44.37\n42.38\n45.03\n56.29\n61.59\n56.29\n62.91\n65.56\n69.54\n63.58\n68.87\nbusiness ethics\n49.49\n50.51\n42.42\n44.44\n52.53\n53.54\n54.55\n57.58\n62.63\n59.60\n61.62\n61.62\nclinical knowledge\n53.03\n50.76\n51.89\n54.17\n68.18\n71.59\n66.67\n70.45\n70.08\n68.56\n72.73\n71.59\ncollege biology\n48.25\n52.45\n46.85\n52.45\n71.33\n72.73\n69.93\n72.73\n74.13\n76.22\n72.03\n77.62\ncollege chemistry\n28.28\n32.32\n28.28\n33.33\n41.41\n47.47\n43.43\n46.46\n39.39\n52.53\n47.47\n47.47\ncollege computer science\n37.37\n44.44\n35.35\n38.38\n47.47\n40.40\n48.48\n47.47\n45.45\n47.47\n43.43\n46.46\ncollege mathematics\n30.30\n31.31\n28.28\n31.31\n38.38\n40.40\n39.39\n41.41\n30.30\n30.30\n32.32\n27.27\ncollege medicine\n40.70\n40.12\n44.77\n41.28\n58.72\n63.37\n58.72\n61.63\n62.79\n65.70\n63.37\n64.53\ncollege physics\n21.78\n26.73\n21.78\n26.73\n44.55\n35.64\n43.56\n33.66\n35.64\n41.58\n35.64\n39.60\ncomputer security\n48.48\n53.54\n60.61\n59.60\n71.72\n73.74\n70.71\n78.79\n70.71\n74.75\n66.67\n74.75\nconceptual physics\n35.04\n39.32\n41.88\n38.89\n46.58\n54.27\n47.01\n54.70\n56.41\n55.13\n57.26\n55.98\neconometrics\n30.09\n34.51\n28.32\n32.74\n38.05\n46.90\n36.28\n46.02\n46.90\n46.90\n46.02\n53.98\nelectrical engineering\n40.28\n42.36\n43.75\n45.83\n52.78\n55.56\n54.17\n56.25\n50.69\n50.69\n52.08\n52.08\nelementary mathematics\n26.79\n28.65\n27.59\n26.53\n36.60\n36.34\n36.34\n36.87\n42.97\n41.11\n42.18\n44.03\nformal logic\n24.00\n24.80\n21.60\n25.60\n38.40\n33.60\n40.80\n38.40\n39.20\n43.20\n41.60\n44.00\nglobal facts\n35.35\n39.39\n41.41\n40.40\n33.33\n37.37\n34.34\n32.32\n27.27\n30.30\n31.31\n31.31\nhigh school biology\n50.16\n54.69\n50.49\n53.07\n69.26\n76.38\n68.61\n77.02\n75.40\n81.55\n76.05\n79.29\nhigh school chemistry\n35.15\n35.64\n33.17\n33.17\n45.54\n46.53\n43.07\n49.50\n44.55\n46.04\n45.54\n49.01\nhigh school computer science\n39.39\n36.36\n36.36\n45.45\n64.65\n61.62\n62.63\n63.64\n66.67\n70.71\n62.63\n69.70\nhigh school european history\n57.32\n55.49\n54.88\n54.88\n70.73\n73.17\n71.34\n75.00\n79.88\n78.05\n78.05\n80.49\nhigh school geography\n60.41\n60.91\n61.42\n64.47\n73.10\n78.68\n73.60\n83.25\n75.63\n79.70\n76.14\n82.74\nhigh school government and politics\n65.62\n65.10\n61.46\n60.94\n83.33\n88.54\n82.29\n88.02\n87.50\n89.06\n84.38\n89.06\nhigh school macroeconomics\n40.87\n41.65\n40.87\n44.47\n55.53\n64.01\n56.30\n67.61\n62.72\n62.98\n62.21\n66.32\nhigh school mathematics\n25.65\n22.68\n28.62\n25.65\n30.48\n28.62\n30.48\n28.62\n30.48\n28.25\n30.86\n28.25\nhigh school microeconomics\n43.46\n42.62\n37.97\n44.30\n60.34\n67.09\n59.92\n70.04\n67.51\n67.51\n63.71\n69.62\nhigh school physics\n26.67\n26.00\n23.33\n28.00\n37.33\n37.33\n38.67\n41.33\n38.67\n39.33\n36.00\n40.67\nhigh school psychology\n60.29\n65.07\n59.56\n65.07\n75.92\n80.51\n76.10\n83.27\n83.27\n83.82\n80.33\n83.27\nhigh school statistics\n26.98\n31.63\n27.44\n30.23\n42.33\n56.28\n44.19\n53.49\n46.98\n48.37\n43.72\n49.77\nhigh school us history\n59.11\n61.58\n55.67\n65.52\n78.33\n77.34\n75.37\n80.30\n76.85\n81.77\n78.33\n81.77\nhigh school world history\n59.75\n60.59\n61.44\n63.98\n75.00\n75.42\n69.92\n77.97\n81.78\n80.08\n80.51\n80.51\nhuman aging\n50.45\n54.50\n55.86\n54.50\n71.17\n70.27\n70.27\n70.72\n66.22\n68.92\n68.02\n71.17\nhuman sexuality\n57.69\n53.08\n49.23\n51.54\n71.54\n75.38\n70.00\n77.69\n76.15\n77.69\n76.15\n78.46\ninternational law\n55.00\n61.67\n55.83\n61.67\n69.17\n79.17\n71.67\n79.17\n78.33\n80.00\n76.67\n79.17\njurisprudence\n54.21\n54.21\n51.40\n57.94\n69.16\n72.90\n67.29\n74.77\n75.70\n79.44\n71.96\n75.70\nlogical fallacies\n53.70\n54.32\n52.47\n48.77\n71.60\n74.07\n70.99\n71.60\n72.84\n74.07\n72.22\n75.93\nmachine learning\n34.23\n31.53\n27.03\n33.33\n44.14\n38.74\n40.54\n52.25\n49.55\n45.05\n48.65\n44.14\nmanagement\n61.76\n62.75\n63.73\n64.71\n66.67\n77.45\n66.67\n75.49\n79.41\n84.31\n77.45\n85.29\nmarketing\n69.96\n71.67\n66.52\n75.11\n84.55\n87.55\n85.41\n87.12\n87.12\n88.84\n84.55\n87.55\nmedical genetics\n45.45\n51.52\n52.53\n47.47\n64.65\n72.73\n65.66\n65.66\n65.66\n76.77\n65.66\n73.74\nmiscellaneous\n64.71\n62.02\n63.43\n65.35\n76.21\n80.43\n75.83\n81.71\n81.07\n81.33\n80.05\n80.95\nmoral disputes\n46.38\n51.59\n46.96\n55.94\n66.38\n68.41\n66.38\n71.59\n71.30\n71.30\n71.88\n74.78\nmoral scenarios\n24.16\n21.36\n24.83\n24.50\n24.38\n37.47\n24.83\n24.38\n36.69\n47.20\n43.18\n46.87\nnutrition\n51.80\n54.10\n48.52\n51.80\n69.51\n73.77\n59.34\n74.43\n70.49\n73.77\n70.16\n74.75\nphilosophy\n50.00\n54.19\n54.84\n56.45\n64.84\n70.97\n63.87\n71.94\n65.81\n72.26\n67.74\n72.26\nprehistory\n55.42\n54.18\n54.49\n54.49\n68.73\n72.45\n69.35\n69.97\n70.90\n74.61\n71.21\n74.92\nprofessional accounting\n35.94\n37.01\n34.52\n35.59\n47.69\n46.26\n46.62\n47.69\n44.48\n45.91\n41.28\n49.82\nprofessional law\n35.68\n34.77\n35.75\n34.96\n43.18\n43.31\n43.38\n45.08\n45.92\n47.95\n46.05\n48.14\nprofessional medicine\n37.27\n36.53\n34.32\n34.69\n62.36\n64.21\n60.89\n61.25\n68.63\n68.27\n65.68\n69.37\nprofessional psychology\n43.70\n44.68\n42.06\n47.30\n60.56\n65.96\n61.21\n68.74\n63.18\n63.67\n63.18\n67.92\npublic relations\n51.38\n49.54\n47.71\n55.96\n57.80\n64.22\n61.47\n67.89\n62.39\n59.63\n61.47\n62.39\nsecurity studies\n45.08\n54.92\n38.93\n43.85\n62.70\n65.98\n63.93\n71.72\n68.44\n71.31\n68.44\n71.72\nsociology\n66.00\n62.00\n63.50\n65.00\n82.00\n83.00\n82.00\n82.50\n80.50\n85.00\n83.00\n83.50\nus foreign policy\n68.69\n74.75\n68.69\n73.74\n82.83\n85.86\n82.83\n84.85\n85.86\n88.89\n83.84\n86.87\nvirology\n48.48\n42.42\n44.24\n46.67\n49.09\n53.33\n47.27\n50.30\n48.48\n52.12\n47.27\n49.70\nworld religions\n65.29\n68.24\n61.76\n68.24\n78.24\n80.59\n78.82\n85.88\n82.35\n84.12\n81.18\n84.12\nAverage\n45.37\n46.75\n44.64\n47.33\n58.83\n61.90\n58.56\n62.80\n61.90\n64.05\n61.44\n64.47\nSubsets\nTable 7: Detailed Results (Accuracy%) on MMLU. ZS: Zero-Shot, FS: Few-Shot, Self: Self-ICL.\n",
    "paper_type": "method",
    "attri": {
        "background": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies have highlighted that the model\u2019s performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs. In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model\u2019s reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming.",
        "problem": {
            "definition": "The problem is the sensitivity of LLM performance to the choice of demonstrations in In-context Learning (ICL), which can lead to significant performance degradation if demonstrations are inadequately chosen.",
            "key obstacle": "The main difficulty lies in ensuring the reliability of demonstrations without external information, as existing methods often rely on generative capabilities that may produce low-quality outputs."
        },
        "idea": {
            "intuition": "The idea stems from the observation that text provided by humans typically has higher quality than that generated by models. Thus, using previously predicted historical samples as demonstrations can enhance performance.",
            "opinion": "The proposed idea, Demonstration Augmentation for In-context Learning (DAIL), utilizes historical samples predicted by the model as demonstrations for subsequent queries, aiming to improve ICL performance without external information.",
            "innovation": "DAIL differs from existing approaches by eliminating reliance on generative capabilities and external information while maintaining a memory bank of historical samples, thus bringing no additional inference cost."
        },
        "method": {
            "method name": "Demonstration Augmentation for In-context Learning",
            "method abbreviation": "DAIL",
            "method definition": "DAIL is a method that employs previously predicted samples as demonstrations for subsequent queries, optimizing the selection and management of these samples through a memory bank.",
            "method description": "DAIL enhances zero-shot ICL by utilizing historical predictions as demonstrations, thereby improving performance without external information.",
            "method steps": [
                "Initialize a memory bank with a maximum capacity.",
                "Process the initial query using zero-shot inference.",
                "For subsequent queries, select suitable demonstrations from the memory bank.",
                "Add the current query and model response to the memory bank.",
                "Apply entry, selection, and deletion strategies to manage the memory bank."
            ],
            "principle": "The effectiveness of DAIL lies in leveraging high-quality historical demonstrations which improve the reliability and performance of ICL while minimizing inference costs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using various models and benchmarks, including MMLU and BBH, comparing DAIL against zero-shot and few-shot methods.",
            "evaluation method": "The performance was assessed based on accuracy metrics across multiple-choice questions, analyzing the impact of different selection and deletion strategies on the results."
        },
        "conclusion": "DAIL significantly enhances ICL performance without any external information and does not incur additional inference latency, indicating its substantial potential in real-world applications.",
        "discussion": {
            "advantage": "DAIL provides high-quality demonstrations without reliance on generative capabilities, leading to improved performance and efficiency in ICL tasks.",
            "limitation": "DAIL requires access to logits for calculating the Entropy Score, which can pose challenges for deployment in certain environments. Additionally, its validation has focused primarily on multiple-choice tasks.",
            "future work": "Future research could explore the application of DAIL in open-domain text generation tasks and address privacy concerns related to storing historical samples."
        },
        "other info": {
            "acknowledgements": "The authors thank the anonymous reviewers for their valuable comments and acknowledge support from various foundations.",
            "code repository": "The code for DAIL is available at https://github.com/yisunlp/DAIL."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context Learning (ICL) enables Large Language Models (LLMs) to acquire knowledge from textual demonstrations without parameter updates."
        },
        {
            "section number": "1.3",
            "key information": "LLMs' performance in ICL is sensitive to the choice of demonstrations, which presents challenges for practical applications."
        },
        {
            "section number": "3.1",
            "key information": "The Demonstration Augmentation for In-context Learning (DAIL) method enhances zero-shot ICL by utilizing historical predictions as demonstrations."
        },
        {
            "section number": "3.3",
            "key information": "DAIL optimizes the selection and management of demonstrations using a memory bank, improving ICL performance without external information."
        },
        {
            "section number": "3.4",
            "key information": "DAIL leverages high-quality historical demonstrations to improve the reliability and performance of ICL while minimizing inference costs."
        },
        {
            "section number": "6.1",
            "key information": "The main difficulty in ICL lies in ensuring the reliability of demonstrations without external information, as existing methods may produce low-quality outputs."
        },
        {
            "section number": "7",
            "key information": "DAIL significantly enhances ICL performance without external information and does not incur additional inference latency, indicating its substantial potential in real-world applications."
        }
    ],
    "similarity_score": 0.7422260292641482,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Demonstration Augmentation for Zero-shot In-context Learning.json"
}