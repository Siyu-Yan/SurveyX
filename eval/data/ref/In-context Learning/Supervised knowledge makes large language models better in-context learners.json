{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.15918",
    "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
    "abstract": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users\u2019 specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific finetuned Language Models (SLMs) to improve LLMs\u2019 in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks 1. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.",
    "bib_name": "yang2024supervisedknowledgemakeslarge",
    "md_text": "# SUPERVISED KNOWLEDGE MAKES LARGE LANGUAGE MODELS BETTER IN-CONTEXT LEARNERS\nLinyi Yang1,2\u2217, Shuibai Zhang1\u2217, Zhuohao Yu3\u2217, Guangsheng Bao1, Yidong Wang3, Jindong Wang4, Ruochen Xu4, Wei Ye3, Xing Xie4, Weizhu Chen4, Yue Zhang1,2\u2020\n1School of Engineering, Westlake University, 2Westlake Institute for Advanced Study 3Peking University, 4Microsoft\n# ABSTRACT\nLarge Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users\u2019 specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific finetuned Language Models (SLMs) to improve LLMs\u2019 in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks 1. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.\n# INTRODUCTION\nTrained on extensive volumes of data with numerous parameters, large language models (LLMs) have garnered significant performance across diverse tasks. Their in-context learning (ICL) ability positions them as foundational models to adeptly address various downstream tasks, ranging from natural language understanding (Chowdhery et al., 2022; OpenAI, 2023a;b) to reasoning (Wei et al., 2022; O\u2019Brien & Lewis, 2023), and planning (Shen et al., 2023). Despite their robust performance, LLMs come with their own set of challenges; they demand substantial resources for training and deployment, demonstrate slow inference times, and are susceptible to hallucination (Li et al., 2023a). Conversely, supervised task-specific language models (SLMs) 2 offer cost-efficiency in both training and inference, despite losing general multi-task capacities. Owing to their smaller scale and reduced training cost, SLMs can swiftly adapt to distinct tasks, learning task-specific knowledge (Devlin et al., 2018). As new and tailored tasks constantly emerge in real applications, they can pose out-of-distribution (OOD) challenges to LLMs. It has been shown even with ICL, LLMs generally underperform SLMs in such natural language understanding tasks, with an increased tendency for hallucination when completing classification tasks (Sun et al., 2023b).\nTrained on extensive volumes of data with numerous parameters, large language models (LLMs have garnered significant performance across diverse tasks. Their in-context learning (ICL) ability positions them as foundational models to adeptly address various downstream tasks, ranging from natural language understanding (Chowdhery et al., 2022; OpenAI, 2023a;b) to reasoning (Wei et al. 2022; O\u2019Brien & Lewis, 2023), and planning (Shen et al., 2023).\nDespite their robust performance, LLMs come with their own set of challenges; they demand substantial resources for training and deployment, demonstrate slow inference times, and are susceptible to hallucination (Li et al., 2023a). Conversely, supervised task-specific language models (SLMs) 2 offer cost-efficiency in both training and inference, despite losing general multi-task capacities. Owing to their smaller scale and reduced training cost, SLMs can swiftly adapt to distinct tasks, learning task-specific knowledge (Devlin et al., 2018). As new and tailored tasks constantly emerge in real applications, they can pose out-of-distribution (OOD) challenges to LLMs. It has been shown even with ICL, LLMs generally underperform SLMs in such natural language understanding tasks, with an increased tendency for hallucination when completing classification tasks (Sun et al., 2023b).\n\u2217Equal Contribution. \u2020Correspondence to: zhangyue@westlake.edu.cn 1The code and data are released at: https://github.com/YangLinyi/Supervised-Knowledge-Makes-Large Language-Models-Better-In-context-Learners 2SLMs refers to cost-efficient, task-specific, pre-trained discriminative language models in this work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48c5/48c57e8f-1b9f-48e2-b0c7-fde206f6b382.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: We denote (xi, yi) as a question-answer pair and our receipt ri is inserted between the question-answer pair. Supervised knowledge plays a key role in improving OOD generalizability and factuality of LLMs. While the following two analysis tasks aim to explain why our method outperforms the traditional in-context learning method.</div>\nMost of the existing research predominantly segregates LLMs and SLMs as independent learning paradigms (Zhao et al., 2023), overlooking their potential interconnection. Given the distinct advantages and disadvantages of LLMs and SLMs, a fundamental question emerges: Can SLMs enhance LLMs\u2019 performance? Specifically, can SLMs bolster LLMs\u2019 reliability in OOD scenarios while minimizing hallucination? Prior research (Li et al., 2023b) hints at the potential for enhancing the performance of LLMs with the assistance of a smaller task-specific language model, but relatively little work addresses this research question systematically and empirically. To this end, we conduct a set of systematic empirical evaluations. Our assumption is that SLMs and LLMs have underlying complementarity in terms of knowledge \u2013 while SLMs are equipped with task knowledge thanks to supervised training data, LLMs are endowed with rich domain knowledge from large-scale pretraining. Consequently, we focus on OOD settings of various tasks in our evaluation. This paper introduces SuperContext, a versatile and straightforward in-context learning strategy to harness the strength of small models to augment LLMs, particularly focusing on OOD generalization and factuality. At the heart of SuperContext is the integration of SLM outputs representing the supervised knowledge into LLM prompts, exemplified by incorporating the predictive results and confidence of a discriminative model during the LLM\u2019s inference stage. This idea is similar in spirit to existing work on retrieving information from external knowledge bases or API tools, such as unstructured corpora, structured databases, Wikipedia, and Google API (Borgeaud et al., 2022; Larson et al., 2022; Li et al., 2023c). However, since our goal is to allow reliable task adaptation rather than knowledge acquisition, the consulting agent becomes SLMs rather than search engines. SuperContext is examined in two experiments and two perspectives of analysis. The first task is OOD natural language understanding (NLU), where LLMs are enhanced with the supervised knowledge from task-specific fine-tuned models for OOD datasets. The discriminative model is fine-tuned on task-specific data from diverse domains, and seamlessly bridges the gap between the extensive pre-trained model and task-specific data, eliminating overfitting. The second task is question answering containing unanswerable questions, where we underscore SuperContext capability to curtail hallucinations, addressing them through a discriminative-model-enhanced approach. To analyze the underlying mechanisms, an interpreter is constructed to elucidate why SuperContext transcends traditional in-context learning methods, based on a comprehensive post-hoc analysis. In addition, extensive quantitative and qualitative assessments delve into how small models facilitate LLMs in tackling the classification conundrum. We conduct experiments on both zero-shot and few-shot settings of natural language understanding and question answering (QA). SuperContext is validated on a comprehensive OOD benchmarks GLUE-X (Yang et al., 2022), and a QA dataset, SQuAD 2.0 (Rajpurkar et al., 2018). Empirical results show that our method significantly outperforms LLMs and SLMs with both zero-shot and few-shot settings on 9 distinct tasks using the OOD setting we consider. To the best of our knowledge, this work propounds SuperContext as a pioneering approach to systematically integrate SLMs into LLM inference decisions, significantly enhancing LLM performance, especially in managing OOD data and mitigating hallucinations, thereby contributing to the advancement of more generalizable and factual deployment of LLMs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d5b8/d5b8cf32-2c04-471d-8d42-f142741465bc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Illustration of prompt designs, where the supervised knowledge provided by the discriminative model is defined as ri, and the optional interpretation prompt is denoted as si.</div>\nFigure 2: Illustration of prompt designs, where the supervised knowledge provided by the discriminative model is defined as ri, and the optional interpretation prompt is denoted as si. 2 M\n# 2 METHOD\n# 2.1 IN-CONTEXT LEARNING BASELINE\nIn-context learning (ICL) has become the cornerstone of stimulating the ability of large language models (LLMs) (Dong et al., 2022). To facilitate the evaluation of the traditional in-context learning and our method, in-domain data is provided for several NLU tasks, with each task consisting of 16-shot examples. Denote (xi, yi) as a question-answer pair and Sj is the index set of in-context learning samples where n = |Sj| is the number of shots. The few-shot examples are denoted as {xi, yi}i\u2208Sj\u2282[1,N]\\{j}, where i \u2208[1..N] and N is the number of problem instances for the task. Formally, traditional in-context learning is based on the following assumption (Xu et al., 2023b): \ufffd \ufffd \ufffd \ufffd\nIn a nutshell, Eq. (1) indicates that the probability pLLM \ufffd yj | {xi, yi}i\u2208Sj , xj \ufffd of a given LLM generating the response yj when prompted with the concatenation of the few-shot examples with the discriminative model\u2019s prediction, confidence, and the special prompt si is approximately invariant to the exact choice of the few-shot examples. We consider both zero-shot and few-shot settings in this work. Notably, the choice and even the order of the examples can have a substantial impact on the test performance (Lu et al., 2021). To mitigate such impact, we employ a thrice resampling with the replacement method for computing the average results. The key to designing alternatives for ICL is to find the appropriate knowledge elsewhere to embed into the decoding process of the LLM. Recently, Li et al. (2023b) proposed the contrastive decoding approach that exploits the contrasts between the expert and amateur language models of different sizes by choosing tokens that maximize their log-likelihood difference. Their approach generates high-quality texts with the help of an amateur model. However, their approach still requires performing contrastive mapping between those two models in training, which could be tedious. In contrast to their work, the central question that we address is: \u201cCan we develop a cheap and generalized in-context learning approach that can serve more tasks?\u201d\n# 2.2 SUPERCONTEXT\nWe propose SuperContext, a simple and general approach for in-context learning that incorporates the auxiliary knowledge from a small, discriminative model with LLMs when making predictions for new tasks. This is accomplished through the integration of instruction and the prediction derived from a fine-tuned (small) discriminative language model. Specifically, our receipt ri is inserted between the question-answer pair: {xi, ri, yi}. In our work, ri plays two roles: 1) it provides the discriminative model\u2019s prediction and confidence; 2) it further explains the prediction from two aspects, questioning LLMs to answer it learns from which in-context example and which kind of rationale is important. As shown in Figure 2, we take the sentiment analysis (SST-2) task as an example to illustrate the prompt design. Throughout the process, we do not use any labels from corpus Y as demonstration\n(1)\nTable 1: Data statistics of SuperContext, which describes the source and size for OOD tests of NLU and hold-out test of QA.\nID\nSST-2\nMNLI\nQNLI\nRTE\nMRPC\nQQP\nSTS-B\nCoLA\nSQuAD 2.0\nOOD\nIMDB\nYelp\nAmazon\nFlipkart\nMNLI-mis\nSNLI\nNewsQA\nSciTail\nHANS\nQQP\nTwitter\nMRPC\nTwitter\nSICK\nTextbook\nTrain: 130,319\nDev:11,873\n\ufffd \ufffd \ufffd \ufffd where our method can be represented as {xi, ri, yi}i\u2208Sj\u2282[1,N]\\{j} of given LLM, where i \u2208[1..N] and N is the number of problem instances for the task, and si is the optional prompt defined as the instruction of the interpreter. The probability pLLM \ufffd ri, yj | {xi, ri, yi}i\u0338=j , xj, si \ufffd generating the response yj is approximately invariant to the exact choice of the few-shot examples Sj. Algorithm. Algorithm 1 summarizes the SuperContext augmentation method. The discriminative model M is trained on the in-domain dataset X and tested on the out-of-domain corpus T. For incontext learning of SuperContext, yj is prompted with the concatenation of the few-shot examples with the discriminative model\u2019s prediction, confidence, and the special prompt si. The output should be the prediction of LLMs towards the test case with interpretation if available.\nAlgorithm 1 SuperContext for Natural Language Understanding\nRequire: In-domain Corpus X, Out-of-domain Corpus Y , A discriminative language model M, A large-scale\ngenerative model L, Instruction R, Output O,\n\u25b7The Instruction R varies in per task.\nEnsure: Predicted Labels for test cases in Y\n1: M \u2032 \u2190Finetune(M, X)\n2: For each test case ei in Y\n3:\nConfidence c, Predicted Label l \u2190Predict(M \u2032, ei)\n4:\nP \u2190Concatenate(R, ei, l, c)\n5:\nO \u2190Inference(L, P)\n6:\nIf Interpretator Enabled Then\n7:\nreturn Interpretation, Predicted Label by Parser(O)\n8:\nElse\n9:\nreturn O\n# 3 EXPERIMENTS\n# 3.1 SETUP\nSource models. As reported in GLUE-X (Yang et al., 2022), ELECTRA-large (Clark et al., 2020) achieves the best performance for both ID and OOD tasks over 21 small-scale pre-trained language models (maximum 774M parameters). Hence, we select ELECTRA-large as the SLM for NLU experiments, and RoBERTa-large (Liu et al., 2019) for QA. For evaluating the performance of SLMenhanced LLMs, we select ChatGPT (OpenAI, 2023a) and Llama2-7B-chat (Touvron et al., 2023) as backbones, which are pre-trained on CommonCrawl, WebText, English Wiki, and others. Datasets. We follow the OOD generalization setting of GLUE-X (Yang et al., 2022). In particular, we consider 7 classical NLU tasks: Sentiment Analysis (SA), Natural Language Inference (NLI), Paraphrasing, Question-Answering NLI (QNLI), Textual Entailment, Textual Similarity, and Linguistic Acceptability (Grammar). We sample 3,000 examples from GLUE-X for each OOD dataset and ensure that in-context samples are extracted from different domains of test sets. In total, SuperContext contains 43,728 instances on NLU for ChatGPT and 37,438 instances for Llama2-7B-chat.\n(2)\n<div style=\"text-align: center;\">Table 2: The table vividly displays the GLUE-X metrics garnered by diverse methods across 15 unique OOD datasets. \u2018AVG\u2019 denotes the average results across these 15 OOD datasets.</div>\nTable 2: The table vividly displays the GLUE-X metrics garnered by diverse methods across 1 unique OOD datasets. \u2018AVG\u2019 denotes the average results across these 15 OOD datasets.\nModel\nSST-2\nMNLI\nQNLI\nRTE\nMRPC\nQQP\nSTS-B\nCoLA\nAvg\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nHuman Performance\n97.69\n91.80\n92.33\n91.12\n83.50\n79.13\n92.62\n66.47\n86.83\nELECTRA-large\n94.84\n87.30\n82.66\n78.45\n63.60\n78.08\n80.74\n40.29\n79.86\nChatGPT\n94.83\n41.54\n81.82\n68.56\n60.23\n43.23\n72.61\n39.05\n66.67\nChatGPT (+16-shot)\n94.72\n64.24\n74.14\n68.34\n60.91\n74.24\n64.60\n47.15\n72.28\nChatGPT (+BM25)\n94.84\n64.19\n74.00\n60.31\n64.29\n68.35\n65.22\n42.50\n71.69\nSuperContext (w/o confidence)\n94.84\n77.21\n82.66\n78.45\n63.60\n78.08\n80.74\n40.29\n78.43\nSuperContext (+interpreter)\n94.84\n80.73\n83.81\n78.60\n64.26\n77.80\n76.15\n39.47\n78.77\nSuperContext (zero-shot)\n95.19\n87.24\n82.91\n78.71\n63.87\n78.65\n78.75\n41.47\n80.05\nELECTRA-large\n95.42\n87.29\n82.69\n78.84\n37.59\n77.18\n80.74\n45.73\n76.84\nLlama2-chat\n90.56\n34.30\n66.85\n60.77\n36.20\n51.57\n37.12\n6.94\n55.92\nLlama2-chat (+16-shot)\n94.72\n48.20\n67.70\n61.62\n35.72\n59.15\n18.01\n11.52\n58.54\nLlama2-chat (+BM25)\n92.87\n48.14\n68.48\n59.40\n37.08\n58.24\n39.19\n10.57\n59.69\nSuperContext (zero-shot)\n94.95\n85.45\n81.60\n78.39\n36.70\n61.79\n45.67\n40.84\n73.89\nSuperContext (w/o confidence)\n94.29\n76.68\n82.66\n78.46\n43.41\n78.17\n80.74\n40.26\n75.68\nSuperContext (16-shot)\n95.45\n87.14\n82.17\n79.07\n54.63\n77.18\n80.74\n45.47\n79.08\nBaselines. For NLU, we consider two in-context learning methods as baselines for ChatGPT (OpenAI, 2023a) and Llama2-7B-chat (Touvron et al., 2023), namely 16-shot ICL and BM25. The 16-shot ICL indicates the method that randomly extracts few-shot examples from the in-domain dataset as the demonstration prompt. While \u201c+BM25\u201d represents the dynamic in-context examples selection method using BM25 to select the top 16 examples that are similar to the test case. We also present the ablation that leverages SuperContext with the optional interpretation prompt, shown as \u201c+interpretor\u201d. The variants of the backbone model are kept the same between ChatGPT and Llama2, namely \u201c+BM25\u201d and \u201c+16-shot\u201d. Due to the relatively low instruction following ability of Llama2-7B-chat, the \u201c+interpretor\u201d is not explored in experiments of Llama2-7B-chat. Due to the difference in the instruction-following ability between the ChatGPT and Llama2-7B-chat, we insert the 16-shot in-context examples appended with the prediction and confidence of SLMs, namely SuperContext (16-shot). Human performance is extracted from GLUE (Wang et al., 2019). Evaluations. Different from NLU, the question-answering task is evaluated by the hold-out test. The in-context examples are extracted from the training set and LLMs are evaluated on the validation set. We establish the baseline by using \u201ccluster+filter\u201d method. In particular, we adopt MiniLM (Wang et al., 2020) to encode the training examples and build a union-find set. Then, we use the cluster and filter pipeline to retrieve the most relevant examples with the test sample as in-context demonstrations for ChatGPT. For Llama2-7B-chat, we adopt two fine-tuned methods as baselines using multi-turn and single-turn tuning on 1.2 epochs, respectively. Notably, the total length of the prompt is controlled under 4,096, limited by Llama2.\n# 3.2 NLU RESULTS\nOverall Performance. The comprehensive results of natural language understanding tasks under the OOD evaluation are meticulously outlined in Table 2. Generally, SuperContext emerges as a dominant force, showcasing an elevated average result compared to both SLM (80.05% vs. 79.86%) and LLM (80.05% vs. 66.67%), underscoring the preeminent performance of SuperContext. Our experimental venture utilizing ELECTRA-large (334M Para.) to bolster Llama2-7B-chat\u2019s performance not only transcends ChatGPT (16-shot) (79.08% vs. 72.28%) but also parallels the SuperContext based on ChatGPT (79.08% vs. 80.05%), indicating its substantial capacity to markedly diminish inference costs. It is noteworthy that the data size used for ChatGPT and Llama2-7B-chat is different, leading to different results of SLMs (ELECTRA-large). With the help of 16-shot in-context learning, the performance of ChatGPT can be improved from 66.67% to 72.28%, but still much lower than SuperContext (80.05% vs. 72.28%). The comparison between the in-context learning paradigm and our method proves that our method can outperform 16-shot in-context learning with a much shorter input sequence length (\u223c30 times).\nTable 3: Results of ChatGPT and Llama2-7B-chat, and their variants on SQuAD 2.0. EM indicates the exact match and valid EM only accounts for the exact match of valid JSON. ACC No indicates the accuracy for no-answer questions and ACC accounts for the accuracy of has-answer questions.\nModel\nValid JSON\nEM\nValid EM\nACC. No.\nACC. Has.\nSuperContext (zero-shot)\n85.18\n57.68\n57.81\n54.65\n60.71\nChatGPT (cluster+filter)\n94.47\n49.31\n48.81\n24.22\n74.48\nChatGPT (16-shot)\n99.49\n44.69\n44.52\n13.22\n76.25\nChatGPT\n96.97\n55.82\n54.76\n32.35\n79.35\nSuperContext (16-shot)\n41.73\n47.91\n43.27\n63.65\n32.12\nFine-tuned multi-turn\n96.40\n25.70\n26.66\n10.47\n40.16\nFine-tuned single-turn\n97.17\n47.22\n48.60\n39.44\n55.02\nLlama2-7B-chat (16-shot)\n28.50\n37.56\n5.32\n58.99\n6.08\nLlama2-7B-chat\n40.09\n46.48\n40.13\n3.72\n31.87\nWe also present the results of SuperContext with the prompt of the interpreter, which requires LLM to recall influential in-context examples and output rationales when making the predictions, indicating as SuperContext (+interpreter). To better understand the benefits of including the model confidence in the prompt, we present the results of SuperContext (w/o confidence). By comparing the results of SuperContext w/ and w/o confidence, we observe that including model confidence can bring significant improvements in the average performance for both ChatGPT and Llama2. Meanwhile, we find that for QNLI and QQP, Llama2 without the SLM\u2019s confidence achieves the best performance among several methods. Our results also indicate that the interpreter can not bring significant benefits when compared to SuperContext in most of the tasks, except a slight improvement can be achieved on QNLI. It can be because the explain-then-predict prompt (Wang et al., 2022a) may not be suitable for incorporating with SuperContext, leading to information overload. Llama2-7B-chat. In addition to ChatGPT, we offer the comparison between SuperContext and several baselines based on the open-source model. Experimental results show that SuperContext with 16-shot in-context examples achieves the best results on seven of eight tasks included in GLUEX compared to Llama2-7B-chat under the same setting without the help of the small model (79.08% vs. 58.54%). It is interesting to see that it outperforms ELECTRA-Large in terms of the average performance (79.08 vs. 76.84). Such a huge performance increase indicates that SuperContext improves the NLU capability of both Llama2-7B-chat and ELECTRA-large simply and effectively. In addition, we find that using BM-25 to retrieve the most relevant 16-shot examples of the test case is useful for improving the in-context learning performance (59.69% vs. 58.54%). Task-level Analysis. On the task level, we observe that both ChatGPT and Llama2 show a relatively lower accuracy than the expectation on multiple tasks, including OOD evaluation on MNLI, MRPC, and QQP. For example, the original ChatGPT and Llama2-7B-chat can only achieve 41.54% and 34.30% on MNLI, respectively. With the help of SuperContext, MNLI-OOD results can be improved to 87.24% and 87.14% on ChatGPT and Llama2-chat, respectively. For STS-B which is a textual similarity task, we find that the original Llama2-chat model performs poorly with or without incontext learning and the zero-shot performance of Llama-2-chat is significantly lower than ChatGPT (37.12% vs. 72.61%). Notably, although the zero-shot performance of SuperContext based on Llama2-7B-chat is lower than ChatGPT using the same setting on all tasks, SuperContext based on 16-shot Llama2-7B-chat can even beat SuperContext based on zero-shot ChatGPT in multiple OOD tasks, including SST-2, RTE, STS-B, and CoLA, representing the efficacy of our method not only for proprietary LLMs but also for relatively small-scale models, Llama2-7B-chat.\n# 3.3 QA RESULTS\nThe fact-conflicting of LLMs is considered a core issue in LLMs because it is challenging for users to be aware of and may pose misinformation dissemination. We evaluate LLMs\u2019 ability towards minimizing the hallucination on the QA task based on SQuAD 2.0 (Rajpurkar et al., 2018), which is a suitable testbed since it can be addressed using both discriminative and generative manners. Results of ChatGPT. The results are presented in Table 3. We find that although the original ChatGPT can achieve the highest accuracy for deterministic questions (79.35%), the exact match (EM) and accuracy for open questions can be significantly improved by SuperContext. In particular, the accuracy for no answer questions can be improved from 32.35% (ChatGPT) to 54.65% (SuperCon-\nThe fact-conflicting of LLMs is considered a core issue in LLMs because it is challenging for users to be aware of and may pose misinformation dissemination. We evaluate LLMs\u2019 ability towards minimizing the hallucination on the QA task based on SQuAD 2.0 (Rajpurkar et al., 2018), which is a suitable testbed since it can be addressed using both discriminative and generative manners.\ntext), indicating the huge benefits. Besides, we find that even with the careful design of in-context learning prompts and filter methods, SuperContext still outperforms two in-context learning variants in terms of all metrics, indicating that pure in-context learning without fine-tuning LLMs brings no benefit to the QA task. Furthermore, SuperContext even outperforms the fine-tuned method in a multi-turn setting on all metrics. We believe that such a huge performance benefit (54.65% vs. 13.22%) compared to the traditional 16-shot in-context method when answering no answer questions (\u201cACC.NO.\u201d) proves that results achieved by discriminative models are effective enough to reduce the hallucination. Results of Llama2-7B-chat. We observe that the fine-tuned methods can significantly improve the rate of valid JSON. In particular, the fine-tuned single-turn method improves the valid JSON of the original Llama2-chat from 40.09% to 97.17% and achieves the best performance for valid EM (48.6%) and accuracy for has-answer questions (55.02%). Despite fine-tuned methods outperforming the original Llama2-chat and the in-context learning version, SuperContext achieves the best performance in terms of the EM and accuracy for no-answer questions. We observe that the original Llama2-7B-chat model struggled with format adherence and hallucinations, especially in answering no-answer questions. This is reflected in the notably low score of 3.72. In other words, it cannot output \u201cI don\u2019t know\u201d when the question is unanswerable. However, when applying in-context learning with a mix of no-answer and has-answer instances, we noticed an improvement in handling no-answer questions, though this came at the cost of reduced accuracy in has-answer questions.\n# 4 ANALYSIS AND DISCUSSION\n# 4.1 REVERSED PREDICTIONS\nTable 4: Statistics of reversed predictions. \u201c%Reversed\u201d denotes the percentage of LLMs\u2019 predictions that differ from the predictions of SLMs. \u201cReversed Acc.\u201d is short for the possibility of the reversed predictions that from incorrect to correct.\nAs displayed in Table 4, we study the difference between the final prediction of LLMs and the prediction of SLMs. The detailed task-level performance is shown in the Appendix. The results demonstrate that predictions of 3.02% instances have been overridden during the inference face of ChatGPT by using SuperContext. 57.88% of them have been corrected, indicating that the reference generated by SLMs brings\nMethod\n%Reversed\nReversed Acc.\nSuperContext (ChatGPT)\n3.02%\n57.88%\nSuperContext (Llama2-7B-chat)\n0.50%\n52.13%\npositive benefits for improving the NLU capability of LLMs. SuperContext on Llama2-7B-chat exhibits a relatively lower possibility of reversing the prediction of SLMs (0.5%), yet also inspires LLMs to correct SLMs\u2019 predictions in a more accurate direction than the random guess (52.13%).\nIn addition to the prediction results, we are also interested in understanding the reason behind the result that SuperContext significantly outperforms the traditional in-context learning method. We aim to answer this question from two aspects, how LLMs recall already learned concepts and rationale from pre-training (Han et al., 2023; Gu et al., 2023) and why it fails in the OOD setting.\nLearning from In-context Demonstrations. We explore how language models use long contexts. Figure 3 shows the influence of demonstrations during the inference stage, where the y-axis indicates how many times ChatGPT and InstructGPT take the ith in-context example as the emphasized one towards the prediction. The x-axis is sorted by the order of occurrence of in-context examples over 8 natural language understanding tasks. As shown in the figure, both ChatGPT and InstructGPT show a significant occurrence times difference among in-context examples. In particular, ChatGPT with 16-shot examples shows a trend of decreasing attention with the order of appearance. For example, the second in-context example has been paid attention to over 35,000 times while the last example only receives around 5,000 times attention. In terms of InstructGPT, we observe distinctive U-shaped occurrence times, which can be visualized in Figure 3(b). We find that the model tends to pay attention to the beginning or the end of the input context (in-context examples), and the attention significantly degrades in the middle of long contexts. This observation is consistent with the findings of (Liu et al., 2023) on the use of long contexts when performing downstream tasks, which suggests that model performance significantly degrades when models must access relevant\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/42a9/42a91fb4-d38d-4344-8a67-a819007566c6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Interpretation results of ChatGPT.</div>\nFigure 3: Counting the times of 16-shot in-context examples that have been considered as the influential examples over 8 NLU tasks, sorting by order of occurrence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/22f3/22f3d644-ceb9-4f11-8bdb-0781631a7f44.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) The calibration laws of ChatGPT.</div>\nFigure 4: The correlation between the SLM confidence and LLM performance evaluated on the GLUE-X benchmark. The dark green line represents the normalized performance of LLMs using SuperContext corresponding with the right y-axis while the light green bar indicates the volume of instances with the specific confidence interval corresponding with the left y-axis.\ninformation in the middle of long contexts and provide a new perspective for future long-context models. We also collect the sentence-level rationale generated by LLMs when making predictions, and count for the word frequency for each task of GLUE-X based on ChatGPT, aiming to provide the internal causes of OOD generalizability. However, the rationale is generated by LLMs and thus may contain hallucinations, which should be treated with caution and just for reference.\n# 4.3 THE EFFECT OF SLM CONFIDENCE\nSince we rely on the complementarity between SLMs and PLMs, SLMs must convey its certainly in task knowledge and uncertainly in domain knowledge to PLMs. The confidence score in the design serves a crucial role in such communication channels. We show the correlation between the confidence of SLMs and the prediction performance of LLMs. As shown in Figure 4, both ChatGPT and Llama2-7B-chat demonstrate a positive correlation between SLMs\u2019 confidence and LLM\u2019 performance, representing a high consistency between those models. The x-axis represents the confidence interval covering from 0.4-1.0, for example, 0.5 indicates the instances with the prediction confidence between 0.4-0.5. It is noteworthy that the confidence is computed by the zero-shot test based on SLMs trained on unseen domains, which indicates that high confidence requires a decent generalization ability of small models. We speculate that SuperContext shows superior performance than both SLMs and LLMs since it leverages the benefits of high consistency in discriminative models and the complementarity property of recent generative models. Besides, such a positive calibration law underscores the importance of involving both prediction and confidence in the prompt design of SuperContext. The data statistic of data quantity shows that most instances included in GLUE-X receive the highest confidence interval from 0.9 to 1.0, and this part of the data can be predicted with significantly higher accuracy than others. By comparing the experimental results of GPT-3.5 and Llama2-7B-chat, we find that when the confidence is more than 0.6, the average performance of GPT-3.5 is substantially better than Llama2-7B-chat.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d6a/0d6af24e-3712-4904-9171-984e6f6f2a76.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c462/c4626552-17f2-4146-9cbd-ff8610b42249.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) The calibration laws of Llama2-7B-chat</div>\nIn-context Learning. Scaling up pre-trained language models stimulates the in-context learning ability is first introduced by GPT-3 (Brown et al., 2020), introducing the potential to accurately comprehend instructions and complete complex tasks with no supervision (Chowdhery et al., 2022; OpenAI, 2023b; Sun et al., 2023a). As evidenced by previous work (Shwartz et al., 2020; Nye et al., 2021; Perez et al., 2021), the ICL performance can be significantly enhanced by incorporating auxiliary knowledge or reasoning instructions in a prompt, such as Chain-of-Thought (COT) (Wei et al., 2022) and Tree-of-Thoughts (TOT) (Yao et al., 2023). However, such a multi-step reasoning process could be tedious and expensive to use (assuming we perform ICL for GPT-4), whereas our method is cost-efficient since the supervised knowledge occupies only a short length in the prompt. There is a line of work for improving the in-context learning performance by either constructing demonstrations (Arora et al., 2022; Si et al., 2022; Lyu et al., 2022; Gu et al., 2023; Ye et al., 2023; Dhuliawala et al., 2023) or framing an exploration of example selection methods (Wu et al., 2023; Wang et al., 2023b; Sun et al., 2023a; Agrawal et al., 2022; Wang et al., 2022c;b; Lu et al., 2022; Wang et al., 2023c) and even order (Lu et al., 2021; Zhao et al., 2021; Liu et al., 2021; 2023). The contrastive decoding method (Li et al., 2023b) considers the assistance smaller language model but requires external computation. Differently, SuperContext demonstrates its superior performance on OOD test data in a cost-effective manner. Our work is also connected with work focusing on understanding and explaining in-context learning from different perspectives, including the implicit Bayesian Inference (Xie et al., 2021), pre-training data (Han et al., 2023; Pan et al., 2023), and information compression (Wang et al., 2023a; Wu et al., 2023). Different ways of understanding ICL in realistic NLP tasks have been proposed before (Min et al., 2022; Dong et al., 2022; Wang et al., 2023b), the interpretation part in SuperContext aims to answer how LLMs recall in-context examples and output rationale. Knowledge in Context. Using external knowledge as auxiliary information to assist LLMs in providing truthful and timely responses represents an emerging solution (Mialon et al., 2023; Xiao et al., 2023) in recent. Traditional retrieve-based methods (Rubin et al., 2021; Ni et al., 2021; King & Flanigan, 2023) require a knowledge retriever as the prior step for guiding the generation of responses. Besides, the external knowledge source could extend beyond local documents to encompass the entire Internet (Ni et al., 2021; Gao et al., 2023). In addition, LLMs can leverage special plug-ins to improve their capabilities, such as Toolformer (Schick et al., 2023) and LangChain (Chase, 2022) for calling external APIs, and HuggingGPT (Shen et al., 2023) for using models. Previous work either relies on web information and search engines for gaining external knowledge (Yu et al., 2023) or accomplishes planning tasks outside the NLP scope. (Xu et al., 2023a) evaluates the efficacy of small language models as plug-ins under an in-domain setting using GLUE and lacks an interpretation part to explain the reasons. SuperContext shares a conceptual similarity with SuperICL (Xu et al., 2023a) and HuggingGPT (Shen et al., 2023) in leveraging language model architectures. However, the key distinction lies in our approach\u2019s application and analysis under out-of-distribution (OOD) conditions, a less explored area in the existing literature.\n# 6 CONCLUSION AND FUTURE WORK\nWe constructed SuperContext, an SLM-LLM interaction framework using supervised knowledge for making LLMs better in-context learners in the OOD natural language understanding benchmark and text generation settings. Our goal is to improve the generalizability and factuality of LLMs using cost-efficient, task-specific, and generalizable SLMs. Results on 8 NLU tasks and 1 generation task show that (1) current in-context learning methods still lag much behind humans towards the OOD evaluation of NLU and hold-out test of QA; (2) the traditional in-context learning paradigm faces the forgetting problem and is limited by the input sequence length; (3) SuperContext can bring decent performance benefit compared to few-shot in-context learning and outperform original SLMs and LLMs with both zero-shot and few-shot settings. In the future, we anticipate expanding the scope of SuperContext to cover additional text generation tasks and exploring its effectiveness in various real-world applications.\nWe would like to thank the anonymous reviewers for their insightful comments and suggestions to help improve the paper. This publication has emanated from research conducted with the financial support of the Pioneer and \u201cLeading Goose\u201d R&D Program of Zhejiang under Grant Number 2022SDXHDX0003 and the National Natural Science Foundation of China Key Program under Grant Number 62336006.\n# ETHICAL STATEMEMT\nEthical Use of ChatGPT and InstructGPT. In adherence to the official guidelines provided by OpenAI, we utilized ChatGPT (gpt-3.5-turbo) and InstructGPT (text-davinci-003), setting the temperature of all tasks to zero to ensure reproducibility. For experiments conducted on the SQuAD 2.0 dataset, we employed gpt-3.5-turbo-16k to ensure the prompt length remained within the model\u2019s window length. Social Impact. The primary objective of this study is to repurpose the extensively labeled data in specific domains, which required substantial human and material resources to generate. We aim to use these data to train a task-specific model to assist LLMs in mitigating hallucinations produced during Natural Language Understanding (NLU) and Question Answering (QA) tasks, thereby enhancing the safety of the LLMs. Notably, all of datasets involved in this work belong to the publicly available detasets, and thus do not contain any personal privacy data. Potential Concerns. We acknowledge several limitations of this study and propose a series of open questions for subsequent research. We discuss the potential concerns and limitations of this work. 1. Exploration of Other Large-Scale Language Models: In this study, we delve into the examination of ChatGPT and Llama2. Nevertheless, a plethora of recently proposed models, such as GPT-4, PaLM, Falcon, and Claude, beckons for comprehensive analysis. This work does not involve any commercial competition and belongs to non-profit research. 2. Unveiling More Properties of LLMs: This work investigates the generalizability and factuality of LLMs, yet uncharted territories remain. The exploration of social bias and the reasoning capacity of LLMs promises to be an interesting avenue for further research. We respect the human rights of all people and ensure that crowdsourcing workers are adequately paid for this work. 3. In-Depth Analysis for In-Context Learning Understanding: SuperContext relies on the complementarity between SLMs and PLMs, where SLMs must convey its certainly in task knowledge and uncertainly in domain knowledge to PLMs. A pivotal question persists: can this complementary behavior be attributed to the pre-training data or a handful of few-shot demonstrations? We plan to refine the interaction mechanism between SLM and LLM to further understand in-context learning. Our current analysis does not involve any personal privacy.\nREFERENCES\nREFERENCES Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. Incontext examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022. Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. Ask me anything: A simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations, 2022. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206\u20132240. PMLR, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Harrison Chase. LangChain, October 2022. URL https://github.com/hwchase17/ langchain. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477\u201316508, 2023. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4849\u20134870, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023. acl-long.267. Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. Understanding in-context learning via supportive pretraining data. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12660\u201312673, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.708. URL https://aclanthology.org/2023. acl-long.708. Brendan King and Jeffrey Flanigan. Diverse retrieval-augmented in-context learning for dialogue state tracking. arXiv preprint arXiv:2307.01453, 2023. Stefan Larson, Gordon Lim, Yutong Ai, David Kuang, and Kevin Leach. Evaluating out-ofdistribution performance on document image classifiers. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https: //openreview.net/forum?id=uDlkiCI5N7Y.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale hallucination evaluation benchmark for large language models. arXiv e-prints, pp. arXiv\u20132305, 2023a. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In ACL, 2023b. Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya Poria. Chain of knowledge: A framework for grounding large language models with structured knowledge bases. arXiv preprint arXiv:2305.13269, 2023c. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Jinghui Lu, Linyi Yang, Brian Namee, and Yue Zhang. A rationale-centric framework for humanin-the-loop machine learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6986\u20136996, 2022. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. Z-icl: Zero-shot in-context learning with pseudo-demonstrations. arXiv preprint arXiv:2212.09865, 2022. Gr\u00b4egoire Mialon, Roberto Dess`\u0131, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozi`ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00b4andez \u00b4Abrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. Sean O\u2019Brien and Mike Lewis. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117, 2023. OpenAI. https://chat.openai.com.chat, 2023a. OpenAI. Gpt-4 technical report, 2023b. Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \u201clearns\u201d incontext: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8298\u20138319, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.527. URL https: //aclanthology.org/2023.findings-acl.527.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning \u201clearns\u201d incontext: Disentangling task recognition and task learning. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8298\u20138319, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.527. URL https: //aclanthology.org/2023.findings-acl.527.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. Advances in neural information processing systems, 34:11054\u201311070, 2021. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. arXiv preprint arXiv:2004.05483, 2020. Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. In The Eleventh International Conference on Learning Representations, 2022. Jiuding Sun, Chantal Shaib, and Byron C Wallace. Evaluating the zero-shot robustness of instruction-tuned language models. arXiv preprint arXiv:2306.11270, 2023a. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. Text classification via large language models. arXiv preprint arXiv:2305.08377, 2023b. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJ4km2R5t7. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. arXiv preprint arXiv:2305.14160, 2023a. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776\u20135788, 2020. Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916, 2023b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a. Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, et al. Usb: A unified semi-supervised learning benchmark for classification. Advances in Neural Information Processing Systems, 35:3938\u20133961, 2022b. Yidong Wang, Hao Wu, Ao Liu, Wenxin Hou, Zhen Wu, Jindong Wang, Takahiro Shinozaki, Manabu Okumura, and Yue Zhang. Exploiting unlabeled data for target-oriented opinion words extraction. arXiv preprint arXiv:2208.08280, 2022c.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1423\u20131436, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.79. URL https://aclanthology.org/2023. acl-long.79. Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, and Haobo Wang. Freeal: Towards human-free active learning in the era of large language models. arXiv preprint arXiv:2311.15614, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. Small models are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848, 2023a. Weijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993, 2023b. Linyi Yang, Eoin Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and Ruihai Dong. Generating plausible counterfactual explanations for deep transformers in financial text classification. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6150\u20136160, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.541. URL https://aclanthology.org/2020. coling-main.541. Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, and Ruihai Dong. Exploring the efficacy of automatically generated counterfactuals for sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 306\u2013316, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.26. URL https://aclanthology.org/2021.acl-long.26. Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-ofdistribution generalization perspective. arXiv preprint arXiv:2211.08073, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Qinyuan Ye, Iz Beltagy, Matthew E Peters, Xiang Ren, and Hannaneh Hajishirzi. Fid-icl: A fusionin-decoder approach for efficient in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8158\u20138185, 2023. Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296, 2023. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1423\u20131436, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.79. URL https://aclanthology.org/2023. acl-long.79. Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, and Haobo Wang. Freeal: Towards human-free active learning in the era of large language models. arXiv preprint arXiv:2311.15614, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. Small models are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848, 2023a. Weijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993, 2023b. Linyi Yang, Eoin Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and Ruihai Dong. Generating plausible counterfactual explanations for deep transformers in financial text classification. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6150\u20136160, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.541. URL https://aclanthology.org/2020. coling-main.541. Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, and Ruihai Dong. Exploring the efficacy of automatically generated counterfactuals for sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 306\u2013316, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.26. URL https://aclanthology.org/2021.acl-long.26. Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-ofdistribution generalization perspective. arXiv preprint arXiv:2211.08073, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Qinyuan Ye, Iz Beltagy, Matthew E Peters, Xiang Ren, and Hannaneh Hajishirzi. Fid-icl: A fusionin-decoder approach for efficient in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8158\u20138185, 2023. Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296, 2023. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021.\nWe provide full experimental results in Table 5. Different from Table 2, which demonstrates the average result of each task included in GLUE-X, we present a fine-grained analysis to show the efficacy of our method and differences among tasks. In particular, it is interesting to see that with the help of SuperContext, both ChatGPT and Llama2-7B-chat surpass the supervised task-specific model, ELECTRA, in terms of higher average performance, indicating that SuperContext introduces the benefits of complementarity to enhance the generalizability of LLMs\u2019 towards the NLU tasks. Such a task-level analysis also sheds light on future work to design task-specific methods. Table 5: Performance evaluation of SuperContext and several baselines based on GLUE-X dataset. The table showcases the detailed evaluation results of SLMs, LLMs, and SuperContext. C. is short for ChatGPT and L. represents Llama2-7B-chat.\nID\nOOD\nOurs (C.)\nELECTRA (C.)\nChatGPT (16)\nChatGPT\nELECTRA (L.)\nLlama2\nOurs (L.)\nSST2\nIMDB\n93.97\n94.87\n94.03\n93.63\n94.97\n91.48\n94.97\nSST2\nYELP\n97.30\n96.63\n97.00\n96.87\n97.39\n95.63\n97.53\nSST2\nAmazon\n95.87\n95.37\n94.99\n94.33\n95.84\n94.76\n95.84\nSST2\nFlipkart\n93.60\n93.50\n92.84\n94.50\n93.49\n89.59\n93.45\nCoLA\nGrammar\n41.47\n40.29\n47.15\n39.05\n45.73\n11.52\n45.47\nMRPC\nQQP\n55.06\n54.36\n66.77\n69.94\n27.66\n32.97\n57.22\nMRPC\nTwitter\n72.68\n72.83\n55.05\n50.52\n47.52\n38.46\n52.04\nQQP\nMRPC\n80.17\n79.56\n79.49\n42.04\n80.74\n17.99\n80.74\nQQP\nTwitter\n77.13\n76.59\n68.99\n44.41\n78.94\n70.35\n78.94\nMNLI\nMNLI mis\n88.67\n89.13\n64.77\n43.85\n75.42\n47.94\n75.42\nMNLI\nSNLI\n85.80\n85.47\n63.70\n39.23\n89.13\n56.30\n89.10\nRTE\nHANS\n72.87\n72.87\n58.20\n56.73\n85.45\n40.10\n85.18\nRTE\nSCITAIL\n84.55\n84.02\n62.54\n80.38\n73.68\n60.07\n74.34\nQNLI\nNewsQA\n82.91\n82.66\n74.14\n81.82\n84.00\n63.17\n83.79\nSTS-B\nSICK\n78.75\n80.74\n64.60\n72.61\n82.69\n67.69\n82.17\nGLUE-X\nAVG.\n80.05\n79.86\n72.28\n66.67\n76.84\n58.53\n79.08\nWe present the detailed analysis of predictions of ELECTRA-large reversed by ChatGPT and Llama2-7B-Chat, along with the number of test instances for each task in Table 6. In general, we observe that ChatGPT demonstrates a superior ability to reverse predictions of ELECTRA-Large compared to Llama2-7B-chat, aiming to correct errors when making OOD predictions on NLU tasks. On the other hand, ChatGPT exhibits higher accuracy in making modifications that override classification results compared to Llama2-7B-chat. The OOD testing on QNLI is perceived by both models to contain the highest proportion of data that should have the final decision overridden. Specifically, 15.11% of the test data is amended by ChatGPT, while 2.49% of the data is reversed by Llama2-7B during the inference stage. Naturally, for tasks with relatively lower error rates, such as SST-2 and MNLI, the probability of the models making modifications is also low. This underscores the significant ability of larger models to evaluate the predictions and confidence levels of task-specific fine-tuned models. In terms of the Reversed Accuracy (where the probability of random guess is 50%) as shown in Table 6, we find that ChatGPT exhibits a higher correction accuracy than random guessing on seven out of eight tasks. In contrast, Llama2-7B-chat is capable of reversing predictions in only six out of the eight tasks and surpasses random guesses in only half of the tasks.\n# C ADDITIONAL RESULTS OF CALIBRATION LAWS\nWe supplement the results of the calibration laws with two additional model groups, namely ELECTRA-large and InstrutGPT, in Figure 5. Consistent with ChatGPT and Llama2-7B-chat,\nTable 6: The detailed statistics of reversed predictions on each task of GLUE-X. \u201c%Reversed\u201d denotes the percentage of predictions of LLMs that differ from the predictions of SLMs. \u201cReversed Acc.\u201d is short for the possibility of the reversed predictions that from incorrect to correct. \u201c%Error\u201d is the error rate of the ELECTRA-large baseline. \u201c#Instances\u201d is the total number of test samples.\nModel\nMetric\nSST2\nMNLI\nQNLI\nRTE\nMRPC\nQQP\nSTS-B\nCoLA\nChatGPT\n%Error\n5.16\n12.70\n17.34\n21.55\n36.40\n21.92\n19.26\n59.71\n%Reversed\n0.81\n1.10\n15.11\n3.92\n1.40\n1.92\n8.27\n2.87\nReversed Acc.\n71.13\n42.42\n50.81\n53.93\n82.14\n68.70\n53.23\n74.42\n#Instances\n12,000\n6,000\n2,866\n4,862\n6,000\n6,000\n3,000\n3,000\nLlama2-7B-Chat\n%Error\n4.58\n12.71\n17.31\n21.16\n62.41\n22.82\n19.26\n54.27\n%Reversed\n0.04\n0.28\n2.49\n0.87\n2.05\n0\n0\n0.25\nReversed Acc.\n80.00\n23.53\n39.44\n65.85\n88.98\n0\n0\n16.77\n#Instances\n10,549\n5,996\n2,849\n4,738\n2,604\n5,326\n3,000\n2,376\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7af8/7af8e77d-357d-43ae-859b-7932eaa83ae1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) The calibration laws of ELECTRA-</div>\n<div style=\"text-align: center;\">(a) The calibration laws of ELECTRAlarge.</div>\nFigure 5: The calibration laws of ELECTRA-large and InstructGPT between the confidence and performance evaluated on the GLUE-X benchmark. The dark green line represents the LLMs\u2019 performance using SuperContext corresponding with the right y-axis while the light green bar indicates the volume of instances with the specific confidence interval corresponding with the left y-axis. both ELECTRA-large and InstructGPT exhibit a positive correlation between confidence and performance. Distinctively, the curve for InstrutGPT demonstrates more pronounced fluctuations, especially when the confidence is relatively low.\n# D DETAILED PROMPT DESIGNS\nWe present the detailed prompt designs for each task, using SST-2, CoLA, and SQuAD2.0 as demos.\nD.1 NATURAL LANGUAGE UNDERSTANDING\nSST-2: You are tasked with predicting the sentiment of a given sentence as either \u2019positive\u2019 or \u2019negative\u2019. Use the prediction from the pre-trained model (334M Parameters) fine-tuned on a sentiment analysis dataset as a reference to aid your judgment. Test Case: Sentence: \"[Input]\" Model\u2019s Prediction: Model\u2019s Confidence: Please provide your analysis using the format below and then give your final prediction: 1. Influence Degree: On a scale of 0 to 1 (in increments of 0.1), how much did the fine-tuned model\u2019s prediction influence your judgment? 2.\n1. Influence Degree: On a scale of 0 to 1 (in increments of 0.1), how much did the fine-tuned model\u2019s prediction influence your judgment? 2. Critical Features: Identify the specific word or phrase in the test case that played a pivotal role in your prediction of sentiment analysis. After analyzing, please provide your final prediction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be33/be331f8d-3319-4eef-8513-c7faebee3f54.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) The calibration laws of InstructGPT.</div>\nCoLA: You are tasked with predicting the sentiment of a sentence\u2019s grammar as either \u2018acceptable\u2019 or \u2018unacceptable\u2019. Use the prediction from the pre-trained model (334M Parameters) fine-tuned on a grammar test dataset as a reference to aid your judgment. + Test case: \\[Test case]\" Model\u2019s Prediction: Model\u2019s Confidence: Please provide your analysis using the format below and then give your final prediction: 1. Influence Degree: On a scale of 0 to 1 (in increments of 0.1), how much did the fine-tuned model\u2019s prediction influence your judgment? 2. Critical Features: Identify the specific word or phrase in the test case that played a pivotal role in your grammar-acceptable prediction. After analyzing, please provide your final prediction.\n# D.2 QUESTION ANSWERING\n# SQuAD 2.0\nSQuAD 2.0: <s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\u00b4t know the answer to a question, please don\u00b4t share false information. <</SYS>>\nExtract from the following context the minimal span word for word that best answers the question. Think step by step and explain your reasoning. Then give the answer in JSON format as follows: \u2018\u2018\u2018json \"answer\": ... \u2018\u2018\u2018 If the answer is not in the context, the answer should be exactly a string \"?\", this is very important. Context: context Question: question Here\u00b4s a potential answer to the question: \u2018\u2018\u2018json \"answer\": [\"answer\"] \u2018\u2018\u2018\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving the generalizability and factuality of Large Language Models (LLMs) in natural language understanding and question answering, highlighting the limitations of previous in-context learning methods and the necessity for a new approach that integrates task-specific knowledge from Supervised Language Models (SLMs).",
        "problem": {
            "definition": "The problem is the inadequate generalizability and factuality of LLMs when faced with out-of-distribution (OOD) tasks, which leads to suboptimal performance and increased hallucination in outputs.",
            "key obstacle": "The core obstacle is that existing methods do not effectively leverage task-specific knowledge from SLMs to enhance the inference capabilities of LLMs, particularly in OOD scenarios."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that SLMs, trained on specific tasks, can enhance the performance of LLMs by providing reliable task-specific knowledge during inference.",
            "opinion": "The proposed idea, named SuperContext, integrates the outputs of SLMs into the prompts for LLMs to improve their performance on OOD tasks while minimizing hallucinations.",
            "innovation": "The primary innovation of SuperContext is the systematic integration of supervised knowledge from SLMs into the in-context learning process of LLMs, which distinguishes it from traditional methods that treat LLMs and SLMs as independent paradigms."
        },
        "method": {
            "method name": "SuperContext",
            "method abbreviation": "SC",
            "method definition": "SuperContext is a framework that enhances LLMs by incorporating predictions and confidence scores from a fine-tuned discriminative model into the LLM's inference process.",
            "method description": "The method involves inserting the output from a discriminative model between the input question and the answer in the prompt, thereby guiding the LLM's predictions.",
            "method steps": [
                "Fine-tune a discriminative model on task-specific data.",
                "For each test case, generate predictions and confidence scores using the discriminative model.",
                "Construct the prompt by concatenating the instruction, input, prediction, and confidence score.",
                "Perform inference using the LLM with the constructed prompt."
            ],
            "principle": "The effectiveness of SuperContext lies in its ability to leverage the complementary strengths of SLMs and LLMs, where SLMs provide specific task knowledge and LLMs offer broad domain knowledge, resulting in improved performance on OOD tasks."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on various natural language understanding tasks using datasets from GLUE-X and SQuAD 2.0, comparing the performance of LLMs enhanced with SuperContext against baseline methods including traditional in-context learning.",
            "evaluation method": "Performance was assessed through quantitative metrics such as accuracy and exact match scores, with detailed comparisons across different model configurations and settings."
        },
        "conclusion": "The results demonstrate that SuperContext significantly enhances the performance of LLMs in OOD settings, outperforming both traditional in-context learning methods and the original SLMs, thus contributing to more reliable and factual language model deployments.",
        "discussion": {
            "advantage": "The key advantages of SuperContext include improved generalizability and factuality of LLMs, reduced hallucinations, and cost efficiency in training and inference compared to traditional methods.",
            "limitation": "One limitation of the method is that it may not fully address the inherent biases present in the training data of the SLMs, which can still affect the outputs of the LLMs.",
            "future work": "Future research could explore expanding SuperContext to additional tasks beyond NLU and QA, as well as investigating the integration of more diverse sources of supervised knowledge."
        },
        "other info": [
            {
                "info1": "The code and datasets used in this study are publicly available at https://github.com/YangLinyi/Supervised-Knowledge-Makes-Large-Language-Models-Better-In-context-Learners."
            },
            {
                "info2": {
                    "info2.1": "The study was supported by the Pioneer and 'Leading Goose' R&D Program of Zhejiang and the National Natural Science Foundation of China."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the foundational concepts of improving the generalizability and factuality of Large Language Models (LLMs) in natural language understanding and question answering."
        },
        {
            "section number": "1.2",
            "key information": "The significance of the paper lies in highlighting the limitations of previous in-context learning methods and the necessity for a new approach that integrates task-specific knowledge from Supervised Language Models (SLMs)."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea, named SuperContext, integrates the outputs of SLMs into the prompts for LLMs to improve their performance on out-of-distribution tasks while minimizing hallucinations."
        },
        {
            "section number": "3.1",
            "key information": "The method SuperContext enhances LLMs by incorporating predictions and confidence scores from a fine-tuned discriminative model into the LLM's inference process, which helps maintain robustness in in-context learning scenarios."
        },
        {
            "section number": "3.2",
            "key information": "SuperContext systematically integrates supervised knowledge from SLMs into the in-context learning process of LLMs, providing a theoretical framework for understanding its functionality."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts in the SuperContext method involves inserting the output from a discriminative model between the input question and the answer, which significantly influences the outcomes of in-context learning."
        },
        {
            "section number": "6.1",
            "key information": "One limitation of the SuperContext method is that it may not fully address the inherent biases present in the training data of the SLMs, which can still affect the outputs of the LLMs."
        },
        {
            "section number": "6.4",
            "key information": "Future research could explore expanding SuperContext to additional tasks beyond natural language understanding and question answering, addressing scalability challenges."
        }
    ],
    "similarity_score": 0.7706887374105618,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Supervised Knowledge Makes Large Language Models Better In-context Learners.json"
}