{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.09263",
    "title": "Auto-ICL: In-Context Learning without Human Supervision",
    "abstract": "With in-context learning ability, the performance of large language models can be significantly boosted when provided with appropriate context. However, existing in-context learning methods mainly rely on human-provided contexts, such as labeled examples and explicit instructions. Writing context by humans is labor-intensive on various tasks and limits the model to tasks manageable by humans. To overcome these limitations, we propose Automatic In-Context Learning framework that enables the model to autonomously generate examples and instructions for problem-solving. With experiments across various models and datasets, results show that model-generated contexts outperform human-annotated contexts, including Few-Shot and Few-Shot-CoT methods, and surpass existing self-generated context methods like Zero-CoT and Auto-CoT. 1",
    "bib_name": "yang2024autoiclincontextlearninghuman",
    "md_text": "# CL: In-Context Learning without Human Sup\nShuming Ma Microsoft Research shumma@microsoft.com Furu Wei Microsoft Research fuwei@microsoft.com\nJinghan Yang \u2217 The University of Hong Kong eciel@connect.hku.hk Shuming Ma Microsoft Research shumma@microsoft.com\nJinghan Yang \u2217 The University of Hong Kong eciel@connect.hku.hk Shuming Ma Microsoft Research shumma@microsoft.com Furu Wei Microsoft Research fuwei@microsoft.com\n# Abstract\nWith in-context learning ability, the performance of large language models can be significantly boosted when provided with appropriate context. However, existing in-context learning methods mainly rely on human-provided contexts, such as labeled examples and explicit instructions. Writing context by humans is labor-intensive on various tasks and limits the model to tasks manageable by humans. To overcome these limitations, we propose Automatic In-Context Learning framework that enables the model to autonomously generate examples and instructions for problem-solving. With experiments across various models and datasets, results show that model-generated contexts outperform human-annotated contexts, including Few-Shot and Few-Shot-CoT methods, and surpass existing self-generated context methods like Zero-CoT and Auto-CoT. 1\n# 1 Introduction\nIn the era of Large Language Models (LLMs), human-computer interaction has evolved towards natural language, offering unprecedented flexibility through in-context learning (Radford et al., 2019; Brown et al., 2020; Wei et al., 2022a). Given an appropriate context, the performance of LLMs can be significantly enhanced. Vanilla in-context learning relies on human-provided contexts, such as labeled examples (Few-Shot (Brown et al., 2020) and Few-Shot-CoT (Wei et al., 2022b) methods), explicit instructions, or other guiding mechanisms that shape the model\u2019s outputs. However, humanwritten contexts present two major limitations: 1. Labor-Intensive Annotation: Creating labels or instructions requires substantial human effort, especially when models perform a variety of tasks. 2. Task Limitations: The model ability is constrained to tasks within human ability. If the task is\n\u2217Work done during internship at Microsoft. 1Preprint. The code is available at https://github.com/ ecielyang/Auto-ICL\nchallenging for humans, the model\u2019s performance cannot be effectively enhanced with human written context. . To address these limitations, we propose the Automatic In-Context Learning (Auto-ICL) framework. Upon receiving a user\u2019s request, the model can independently generate examples and instructions for problem-solving. The Automatic InContext Learning framework is shown in Figure 1, which involves two steps. Firstly, a question that requires solving is presented to the model. In Step 1, the model is then prompted to generate demonstrations and/or instructions to assist in solving the question. In Step 2, the model incorporates these self-generated demonstrations and/or instructions and the original query as inputs to generate the final solution. In the experiment, the generated context results in better performance than human-annotated contexts, including Few-Shot and Few-Shot-CoT methods. It also surpasses vanilla methods that trigger models to generate context by itself, e.g., Zero-CoT and Auto-CoT. We also explored the effectiveness of different forms of context based on the availability of similar queries. Our findings reveal that retrieved queries from the dataset can enhance context generation with richer information. With retrieved queries, instruction is more beneficial than demonstration provided in context. Conversely, without retrieved queries, demonstration-plus-instruction context can be more helpful than instruction-only and demonstration-only. In summary, our contributions are as follows:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3d6d/3d6d039f-3d5b-43f5-b2fe-f5b2676a26d4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d56/9d56e8ce-9e4b-48f8-a7b2-bdc9c8f2add3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Question + Demonstrations and/or Instructions</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8e11/8e11f04d-436a-4704-afd9-87b1fad37b62.png\" style=\"width: 50%;\"></div>\nFigure 1: The problem-solving process is broken down into two steps. In Step 1, the model is prompted to generate contextual information (in the demonstrations or instruction form) that aids in answering the given query. In Step 2, the model is provided with the query and its self-generated context to produce the result. The generated demonstrations and instructions can be stored in a database and can be further refined by LLM.\n<div style=\"text-align: center;\">Figure 1: The problem-solving process is broken down into two steps. In Step 1, the model is prompted to generate contextual information (in the demonstrations or instruction form) that aids in answering the given query. In Step 2, the model is provided with the query and its self-generated context to produce the result. The generated demonstrations and instructions can be stored in a database and can be further refined by LLM.</div>\ntasks, demonstrating superior performance to human-crafted contexts and existing methods.\n3. We assess the effectiveness of different forms of context under the condition of whether retrieved queries are provided.\n# 2 Related Work\nThis section presents an overview of the background of ICL. Additionally, we highlight previous works that have explored the utilization of LLMs to generate context to assist in question-solving. In-Context Learning allows LLM to address problems by providing demonstrations or instructions. Without the need for gradient updates, ICL equips models to handle a diverse range of issues (Radford et al., 2019; Brown et al., 2020; Dong et al., 2022). Several research initiatives aim to enhance ICL\u2019s effectiveness by carefully crafting prompts. In the case of delivering demonstrations to LLMs, the selection (Liu et al., 2022) and order (Lu et al., 2022) of examples play a critical role (Zhao et al., 2021). Furthermore, significant thought goes into instructing models to resolve a problem. This involves careful design and composition of instructions (Webson and Pavlick, 2022). Chain of thought (CoT) prompting introduces a way to prompt the model to solve questions using its generative capabilities by asking model reasoning with intermediate steps . Few-Shot-CoT firstly guides the model to solve a problem step-by-step by offering reasoning pathways in each example question in a demonstration (Wei et al., 2022b).\nZero-Shot-CoT uses the phrase \"let\u2019s think step by step\" to help the model arrive at a solution by multistep reasoning (Kojima et al., 2022). Based on Zero-Shot-CoT, Auto-CoT prompts the model by \"let\u2019s think step by step\" on each example question. They concatenate the example questions and generate reasoning paths to demonstrations to assist the model in problem-solving (Zhang et al., 2022).\nSelf-generation of context for problem-solving has been progressively developed through multiple studies. Kim et al. (2022) firstly uses LLMs to generate demonstration queries in classification tasks. However, their work is limited by providing the model with the label and generating the corresponding demonstration. Further, Honovich et al. (2022); Zhou et al. (2022) leverage demonstration pairs to generate task instructions. Recently, Li et al. (2023) introduced a step-wise context generation approach, self-prompting, primarily applied in Open-Domain Question Answering. They focused solely on the QA dataset, proposing a method limited to generating demonstrations. Shao et al. (2023) use human-written demonstrations to prompt the model to generate additional demonstrations, followed by a selection process. Contrary to these prior studies, our method feeds the model with only a query, enabling it to generate demonstrations and instructions. Our research further proposes a method to convert a generated demonstration into an instruction format. Thus, our method is more general, requiring only queries, and can shown to be successful in Theory of Mind and reasoning tasks. In knowledge-intensive tasks necessitating domain or\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5802/5802e389-f8f3-4600-8795-2d3dac0c467c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1fd/e1fdd3df-870f-4e33-b6f9-fa98951705c9.png\" style=\"width: 50%;\"></div>\nFigure 2: Left: An example to generate demonstration by the model. Right: Given queries and reasoning paths, the model generates a general instruction. To generate instructions, demonstrations can from retrieved set or generated set.\nworld knowledge, Yu et al. (2022) and Sun et al. (2022) are geared towards using self-generated context to aid the model in extracting the correct answer from a document. In contrast, our method steers the model towards the right reasoning path by providing demonstrations and instructions with various generation paths rather than merely supplying the document containing the answer.\n# 3 Method\nUnder in-context learning, the context C guides the model M to produce a suitable answer a for the user\u2019s query q. We denote the context as C = {i, (q1, a1), . . . , (qn, an)}, where each (qj, aj) for j = 1, . . . , n signifies a series of demonstrations, and i represents an instruction. In the vanilla ICL framework, the context C is typically provided by a human user:\n(1)\nIn contrast, the Auto-ICL approach initially prompts the model to generate the context C autonomously. The model then uses this selfgenerated context to construct an appropriate response. Initially, given a certain prompt p, the model produces context \u02c6C, which is presented as:\n(2)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d8bb/d8bb6069-f22d-466e-8fd1-72feb5d3a857.png\" style=\"width: 50%;\"></div>\nwhere r is available context resource. We categorize Auto-ICL on whether we can retrieve2 queries from the dataset: (1) Auto-ICL (retrieving): A dataset containing q is available in this case. We use a small subset represented as Q = {q1, . . . , qn}, to generate context (i.e. r = Q). Once a context is generated, it is applied to the whole dataset. (2) Auto-ICL (generating): only the current query q is available, and context is generated using this query (i.e. r = q). Following this, the model is tasked with producing an answer \u02c6a for the query q based on the previously generated context \u02c6C:\n(3)\nIn the following subsections, we show practical ways to generate demonstration and instruction separately under retrieving and generating cases.\n# 3.1 Demonstration\nWith in-context demonstrations, we supply the model with several input-output pairs that resemble the current task. Recognizing patterns or templates within these demonstrations enhances the model\u2019s proficiency in addressing the present request (Brown et al., 2020). In traditional ICL meth-\n2Retrieving only employs queries without labels from the dataset, which is different from utilizing labeled examples in the Few-Shot method.\nQuestion 1: If 4 boys join 3 boys to play game, how many boys  are there? Answer 1: There are 4 boys + 3 boys = 7 boys. The answer is 7  boys. \u2026 Question n: If 2 teams join 3 teams to play game, how many  teams are there? Answer n: There is 1 girl and 5 boys. 1 girl + 5 boys = 6 players.  The answer is 6. Question: If 3 girls join 2 girls to play game, how many girls are  there? Answer: Few-Shot CoT\n# Instruction: Step 1: Count the number of boys/girls mentioned in the question.  Step 2: Add the numbers together.  Step 3: The result is the total number of boys/girls/teams. Auto-ICL (retrieving)\nQuestion:  If 3 girls join 2 girls to play game, how many girls are there? Solve the question step by step based on the instruction.\nFigure 3: An illustration of how to use generated context for problem-solving in Auto-ICL (retrieving) and Auto-ICL (generating). We place the Few-Shot-CoT method in the upper-left corner for comparison. In Auto-ICL (retrieving), the instruction is applied to all queries in the same dataset. In Auto-ICL (generating), the examples and instructions are tailored from each query. The generation process of demonstrations and instructions are shown in Figure 2.\nods, demonstrations are typically created by humans and supplied to the model. Under Auto-ICL, we ask the model to generate demonstration with answers automatically. Generating demonstrations based on query q. The most straightforward case involves using the question q to generate a demonstration composed of a set of queries \u02c6Q and their corresponding label \u02c6A, denoted as q \u2192( \u02c6Q, \u02c6A). For better performance, we directly generate demonstration with reasoning path toward the answer (chain-of-thought), represented as q \u2192( \u02c6Q, \u02c6 CoT). A simple example is shown in Figure 2. Retrieving queries Q from the dataset containing query q. On the other hand, when we have a dataset containing query q, we can randomly retrieve a subset Q from this dataset. The model then generates answers with reasoning path, denoted as Q \u2192(Q, \u02c6 CoT), which is exactly the Auto-CoT method (Zhang et al., 2022). Hence, Auto-CoT can be viewed as a specific case under our framework where demonstrations with reasoning processes are generated from the retrieved dataset.\nods, demonstrations are typically created by humans and supplied to the model. Under Auto-ICL, we ask the model to generate demonstration with answers automatically.\n# 3.2 Instruction generation\nIn the ICL setting, most existing methods mainly rely on human-crafted instructions (Reynolds and McDonell, 2021) or use question-answer pairs to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0fb6/0fb6a987-5828-436e-a47d-01498dd8b45b.png\" style=\"width: 50%;\"></div>\ndevise instruction (Honovich et al., 2022; Zhou et al., 2022). We propose a novel approach that enables the model to generate instructions autonomously, starting only with queries. Generating a instruction based on query q. When we only have access to the query q. We can ask the model to generate an instruction \u02c6i on q. We denote this generation path as q \u2192\u02c6i, referring to this approach as 1-to-1 instruction generation.\ndevise instruction (Honovich et al., 2022; Zhou et al., 2022). We propose a novel approach that enables the model to generate instructions autonomously, starting only with queries.\nGenerating a instruction based on query q. When we only have access to the query q. We can ask the model to generate an instruction \u02c6i on q. We denote this generation path as q \u2192\u02c6i, referring to this approach as 1-to-1 instruction generation.\nGenerating instruction based on queries. Start from demonstration generation, for each query in the randomly retrieved subset Q or generated subset \u02c6Q, we generate a set of reasoning path, denoted as \u02c6 CoT. Then we synthesize these individual paths into a single, general instruction, (Q, \u02c6 CoT) \u2192\u02c6i or ( \u02c6Q, \u02c6 CoT) \u2192\u02c6i. The instruction assures each reasoning step in the answer is consistent and applicable to all queries within the dataset. Since multiple instructions are consolidated into one general instruction, this method is referred to as N-to-1 instruction generation. The method is visualized in Figure 2, and an actual instruction generated by GPT-3.5-turbo-0301 is shown in AppendixA.2.2. As instructions can be generated upon demonstrations, we also consider incorporating both the demonstration and instruction into a context in the experiment.\nMethods\nLabeled Data\nTheory of Mind\nSymbolic Reasoning\nArithmetic\nOthers\nAvg.\nGenerating\nZero-Shot (Brown et al., 2020)\n\u2717\n48.3\n16.1\n44.4\n44.6\n38.3\nZero-Shot-CoT (Kojima et al., 2022)\n\u2717\n48.0\n80.6\n63.9\n63.1\n63.9\nDemonstration (ours)\n\u2717\n45.3\n76.3\n75.3\n57.8\n63.0\nInstruction (ours)\n\u2717\n59.5\n75.3\n72.5\n58.3\n66.3\nDemonstration + Instruction (ours)\n\u2717\n51.1\n92.4\n71.5\n64.0\n68.1\nRetrieving\nFew-Shot (Brown et al., 2020)\n\u2713\n63.5\n34.9\n45.7\n49.3\n48.3\nAPE (Zhou et al., 2022)\n\u2713\n40.2\n70.8\n67.2\n63.3\n60.4\nInduction (Honovich et al., 2022)\n\u2713\n42.8\n83.3\n73.3\n66.8\n66.6\nDemonstration (Auto-CoT)\n\u2717\n47.8\n92.3\n75.3\n57.8\n66.9\nInstruction (ours)\n\u2717\n61.5\n93.3\n75.5\n68.3\n73.4\nDemonstration + Instruction (ours)\n\u2717\n47.5\n91.5\n74.2\n66.5\n68.1\nTable 1: The table compares our method with baseline methods across various tasks and shows the average accuracy under each category. The methods are classified into \"retrieving\" and \"generating\" based on whether the data is sourced from the dataset. The usage of labeled demonstrations is indicated in the column named \"labeled data.\" We use 5-shot examples in the demonstration.\n# 3.3 Auto-ICL (retrieving) and Auto-ICL (generating)\nSince the context can be a combination of demonstrations, instructions, or each on its own, we summarize two forms under the condition of whether we have a dataset to retrieve queries based on the results from experiments: Auto-ICL (retrieving): With retrieval queries, we suggest using instructions only. Here, demonstrations are retrieved from the dataset, answers are generated with Zero-CoT, and a single instruction is created. This instruction is then applied to all queries within the same dataset. Auto-ICL (renerating): Without retrieval queries, we use both demonstrations and instructions. The process starts by generating demonstrations for the query q, followed by generating an instruction using these demonstrations. After creating the context, we combine it with the query q and input them into the model. An example is provided in Figure 3.\n# 3.4 Comparison of computational efficiency\nIn Auto-ICL (retrieving), the generated context is general and applicable to queries within the same dataset. Consequently, the decoding cost is comparable to that of the Zero-CoT and Few-Shot-CoT methods. Additionally, the generated instruction is much shorter than the demonstrations used in FewShot-CoT cases as the number of shots increases, thereby reducing encoding costs. In Auto-ICL (generating), we assume access only to the current query without similar query ex-\namples, reflecting real-world use cases where users may have only a single query. Here, the context is specifically generated for each query. Our method incurs additional decoding costs compared to the Zero-CoT method but results in higher accuracy. Compared to APE for instruction generation (Zhou et al., 2022), which requires generating several instructions and iterating them to maximize a score function, our method is computationally efficient. APE also needs access to the log probabilities of models, which our method does not require. In conclusion, Auto-ICL is a simple and costeffective method that achieves higher accuracy in the retrieving case. Although there is an accuracycost trade-off in the generating case compared to Zero-CoT, Auto-ICL does not replies on other resources compared with other methods.\n# 4 Experiment Setting\nDataset: To evaluate our method, we conduct experiments across four categories of tasks: Theory of Mind (Sap et al., 2019; Le et al., 2019; Sap et al., 2022), symbolic reasoning (Roy and Roth, 2016; Ling et al., 2017; Cobbe et al., 2021), arithmetic (Wei et al., 2022b; Kojima et al., 2022), and other reasoning tasks (Srivastava et al., 2022; Brown et al., 2020). Details of each tasks are listed below: Theory of Mind is the ability to understand and interpret others\u2019 intentions and responses during social interactions. To evaluate this, we use the dev set from the SOCIALIQA QA benchmark (Sap\nMethod\nGenerated Instruction\nInstruction Induction\n\u201cnot provided.\u201d\nAPE\n\"Find the sum of three consecutive integers whose product can be expressed as 727+728+729. The \nanswer is E) 39.\"\nAuto-ICL (retrieving)\n\"The first step is to read the question and identify what information is given and what is being asked \nfor. Next, we need to come up with a plan of attack. For example, if we are looking for the sum of \nthree consecutive integers, we can set up an equation using the information given in the question. \nOnce we have a plan, we can carry out the steps of the plan and solve the problem. Finally, we need \nto check our work to make sure that our answer makes sense.\"\nAuto-ICL (generating)\n\"First, we need to figure out what information we are given and what information we need to find.\nNext, we need to set up an equation or system of equations that we can solve.\nFinally, we need to solve the equation or system of equations to find the answer.\"\net al., 2019) and the TOMI QA dataset with English Sally-Ann-style narratives (Le et al., 2019). Both these datasets scrutinize social-emotional intelligence in common situations and understandings of others\u2019 mental states. We only consider first-order (e.g., \u201cwhere will Alan look for the object?\u201d) and second-order beliefs (e.g., \u201cwhere does Alan think that Bob will look for the object?\u201d), referred to as TOMI1 and TOMI2. We use the processed SOCIALIQA and TOMI datasets from Sap et al. (2022), adding \"unknown\" as an extra answer choice to ensure the model\u2019s choices are knowledge-based rather than by chance. Arithmetic reasoning capability pertains to proficiency in resolving issues by leveraging mathematical concepts and operations like addition, subtraction, multiplication, and division. In our study, we incorporate three datasets: MultiArith (Roy and Roth, 2016), AQUA-RAT (Ling et al., 2017), and GSM8K (Cobbe et al., 2021). These datasets demand multi-step reasoning for problem-solving. For symbolic reasoning, we utilize the Last Letter Concatenation and Coin Flip tasks (Wei et al., 2022b). The Last Letter Concatenation task requires the model to combine the last letters of each word. The Coin Flip task requires the model to determine if a coin remains heads up after a series of flips or non-flips. We use sample data from Kojima et al. (2022) For other tasks, we utilize the Tracking Shuffled Objects dataset from the BIG-bench project (Srivastava et al., 2022), and the Cycle Letter dataset (Brown et al., 2020).\nModel: We evaluate our method on the GPT family (Brown et al., 2020), including advanced models\n<div style=\"text-align: center;\">Generated Instruction</div>\nsuch as GPT-4, GPT-4-Turbo (evaluated during 2024 June), and GPT-3.5-Turbo-0301, as well as earlier models Text-Ada-001, Text-Davinci-003, Text-Davinci-002, and Text-Davinci-001. Additionally, we assess our approach on Llama-2-13B (Touvron et al., 2023), Llama-3-70B (AI@Meta, 2024), and PaLM2 text-bison (Anil et al., 2023). We set temperature as 0 for all models and tasks. Baseline: We compared our method against baseline methods: 1. Zero-Shot (Brown et al., 2020) triggers the response directly from the question without using previous examples or knowledge. 2. Zero-Shot-CoT (Kojima et al., 2022) extends the Zero-Shot method by adding the trigger \"Let\u2019s think step by step\" at the answer phase to generate a reasoning path when reaching the answer. 3. Few-Shot (Brown et al., 2020): the demonstration includes randomly selected questions and annotated labels. 3. Few-Shot-CoT (Wei et al., 2022b): the demonstration includes queries and human annotated reasoning path to the answer. We employee humanwritten demonstrations from Wei et al. (2022b). 4. Auto-CoT (Zhang et al., 2022): For each retrieved query, CoT is generated with the Zero-ShotCoT trigger. Example queries are concatenated with generated CoT to form the demonstration. 5. Instruction Induction (Honovich et al., 2022) and APE (Zhou et al., 2022) asks the model to generate instruction based on a set of question-label pairs by a fixed template. 6. A concurrent work (Yasunaga et al., 2023) also proposes using LLMs to generate demonstrations. We compare their proposed prompt to generate ex-\nMethods\nGPT-4\nGPT-4-Turbo\nGPT-3.5-Turbo-0301\nLlama-3-70B\nAvg.\n5-Shot-CoT (Brown et al., 2020)\n68.5\n74.0\n57.0\n70.5\n67.5\nSelf-generated Exemplars (Yasunaga et al., 2023)\n70.0\n68.5\n50.0\n64.5\n63.3\nAuto-ICL (generating)\n73.0\n66.0\n52.5\n66.5\n64.5\nAuto-ICL (retrieving)\n73.0\n71.4\n60.8\n71.0\n69.0\n5-Shot-CoT (Brown et al., 2020)\n88.5\n83.0\n73.0\n84.0\n82.1\nSelf-generated Exemplars (Yasunaga et al., 2023)\n92.0\n89.0\n66.0\n82.0\n82.2\nAuto-ICL (generating)\n93.5\n90.0\n78.0\n91.8\n88.3\nAuto-ICL (retrieving)\n92.0\n91.4\n79.0\n93.3\n88.9\nTable 2: The table compares our method with examplar-only methods and Few-Shot-CoT method across various models on the AQUA dataset (top) and GSM8K dataset (bottom). Auto-ICL (generating) refers to \"Demonstration + Instruction\" method and Auto-ICL (retrieving) refers to \"Instruction\" method in the Table 1. We use 5-shot examples in the demonstration.\nemplars using our method. 7. DSPy frameworks (Khattab et al., 2023) enabling few-shot examples bootstrapping. We compared our method with the DSPy framework using the GSM8K dataset on GPT-3.5-turbo and GPT4-turbo 3. For the DSPy framework, we enabled the 5-Shot-CoT demonstration selection with 50 training examples and 200 development examples on GPT-3.5-turbo, 30 training examples, and 50 development examples on GPT-4-turbo. Detailed information about the triggers and examples for our method and baselines can be found in Section A.2.\nTable 3: Comparison of Auto-ICL methods with DSPy framework on the GSM8K dataset.\nMethod\nGPT-3.5-Turbo\nGPT-4-Turbo\nDSPy (Khattab et al., 2023)\n81.0\n93.0\nAuto-ICL (generating)\n82.0\n90.0\nAuto-ICL (retrieving)\n83.0\n91.4\n# 5 Results\nWe tested demonstration-only, instruction-only, and demonstration-plue-instruction contexts on the GPT-3.5-Turbo-0301 model across various tasks and show the average accuracy of each task in Table 1 (full result is in Appendix A.1). In the retrieving case, the instruction-only context outperformed other contexts, which we denote as Auto-ICL (retrieving) from Table 2. The demonstration-plusinstruction context yielded better results in the generating case, which we denote as Auto-ICL (generating) from Table 2.\n3As GPT-3.5-turbo-0301 was deprecated when we drew attention to this work during reviewing, we used the latest model instead\n# 5.1 Comparison with baseline methods\nGenerated context outperforms human annotated context. Table 1 and Table 4 demonstrate that our method surpasses traditional methods, including Zero-CoT and Auto-CoT. It also outperforms traditional context generation methods using labeled data. Table 2 shows that our method outperforms the Few-Shot-CoT method and concurrent work that uses LLMs to generate demonstrations as context only. We also compare our method with demonstration selection with DSPy framework in Table 3, our method demonstrates performance similar to the DSPy framework. Additionally, the DSPy framework consumes a significant amount of credits for iterating through demonstrations in the training set, and it relies on demonstrations with labels. In contrast, our method substantially reduces inference costs and does not depend on human-written labels. Several factors contribute to this performance. Auto-ICL (generating) produces demonstrations and instructions tailored to the current question q. As a result, the generated contexts align closely with q from the model\u2019s perspective. These findings are consistent with (Liu et al., 2022), which assert that selecting a demonstration highly correlated with the current request improves performance. Even if the generated demonstration has an incorrect label, the correctness of the label may not be crucial for problem-solving (Min et al., 2022). Additionally, instructions are more effective and informative than demonstrations with a chain of thought, surpassing both Few-Shot-CoT and AutoCoT methods. The generated instructions are summarized from the CoT process, encapsulating the necessary steps to resolve the question. By following these instructions, the model adheres to a\nMethods\nGPT-4\nGPT-4-Turbo\nGPT-3.5-Turbo\nText-Ada-001\nLlama-3-70B\nPaLM2 Text-Bison\nAvg.\nZero-CoT (Brown et al., 2020)\n70.5\n73.0\n37.4\n18.0\n72.0\n53.0\n45.1\nAuto-CoT (Zhang et al., 2022)\n75.5\n64.0\n53.0\n18.5\n46.5\n54.8\n43.2\nAuto-ICL (generating)\n73.0\n66.0\n52.5\n33.3\n66.5\n58.5\n52.7\nAuto-ICL (retrieving)\n73.0\n71.4\n60.8\n15.0\n71.0\n52.5\n49.8\nZero-CoT (Brown et al., 2020)\n91.5\n90.7\n56.9\n0.5\n91.3\n72.0\n62.3\nAuto-CoT (Zhang et al., 2022)\n92.5\n89.3\n78.0\n1.5\n78.3\n71.0\n63.6\nAuto-ICL (generating)\n93.5\n90.0\n78.0\n2.5\n91.8\n74.0\n67.3\nAuto-ICL (retrieving)\n92.0\n91.4\n79.0\n3.0\n93.3\n72.0\n67.7\nTable 4: The table compares our method with baseline methods across various models on the AQUA dataset. AutoICL (generating) refers to \"Demonstration + Instruction\" method and Auto-ICL (retrieving) refers to \"Instruction\" method in the Table 1. We use 5-shot examples in the demonstration.\nconsistent reasoning path, ensuring that each generative step is directed toward the final answer. This structured approach helps maintain coherence and accuracy throughout the problem-solving process.\nconsistent reasoning path, ensuring that each generative step is directed toward the final answer. This structured approach helps maintain coherence and accuracy throughout the problem-solving process. Instruction Interpretability: We present a comparison of generated instructions by APE, Instruction Induction, and Auto-ICL in Figure 4 In this example, the Instruction Induction method fails to generate an effective instruction, while APE relies on a specific example. Our method produces a more general and applicable instruction. In conclusion, our method\u2019s explanations (instructions) on problems are more interpretable by human.\nInstruction Interpretability: We present a comparison of generated instructions by APE, Instruction Induction, and Auto-ICL in Figure 4 In this example, the Instruction Induction method fails to generate an effective instruction, while APE relies on a specific example. Our method produces a more general and applicable instruction. In conclusion, our method\u2019s explanations (instructions) on problems are more interpretable by human.\n# 5.2 Are retrieved queries useful?\nRetrieved data enhances context generation with richer information. In Table 1, retrieving methods consistently outperform generating methods, except in arithmetic tasks. This indicates that generating a general instruction that is adaptive and useful for various questions in a dataset benefits significantly from incorporating information from diverse examples within the dataset. By leveraging a broader context than simply q, the problemsolving process for a given question is augmented, leading to improved performance.\n# 5.3 Instruction vs. Demonstration\nIn retrieving methods, instruction approaches consistently outperform demonstration and demonstration-plus-instruction methods in Table 1. This highlights the effectiveness of instructions in fine-tuning the model\u2019s performance and guiding its problem-solving capabilities. Since the source of information remains the same (i.e., retrieved data), the repetitive usage of the same information in different formats (i.e., demonstrationplus-instruction) does not necessarily enhance the model\u2019s problem-solving ability.\nIn generating methods, demonstration-plusinstruction outperformes instruction-only and demonstration-only approaches in Table 1. Combining demonstration and instruction proved to be more effective in generating methods than the independent use of instructions or demonstrations. Given that the information from a single question, q, may be limited, it can be inferred that expressing limited information in different formats\u2014such as demonstration-plusinstruction\u2014can enrich and extend the information used to address question q.\n# 6 Discussion\nFrom task-specific to universal in-context learning. Recently, significant research has been conducted on the design of prompts to enhance the performance of LLMs on specific tasks (Li et al., 2023; Hu et al., 2023; Wang et al., 2023; Sun et al., 2023). However, these task-specific methodologies often suffer from two main limitations: first, they require a degree of pre-existing knowledge about the task to design an effective prompt, and second, they are typically not transferable across different tasks, limiting their versatility. Therefore, instead of creating task-specific prompts, our method uses a single, broad strategy applicable to various scenarios. The model can process the context of the problem and generate relevant demonstrations or instructions, enabling it to tackle many tasks. This approach represents a shift from task-specific to universal ICL. Considering resources available under an incontext setting. Our research serves as an example in assessing available in-context resources for problem-solving while also proposing and comparing multiple ways to utilize these resources. This work focused on the impact of demonstrations and\nMethods\nText-Davinci-003\nText-Davinci-002\nText-Davinci-001\nLlama2-13B\nAvg.\nZero-Shot\n10.0\n8.0\n3.5\n16.5\n9.5\nZero-CoT (Brown et al., 2020)\n52.5\n36.5\n17.5\n23.5\n32.5\nFew-Shot (Brown et al., 2020)\n13.0\n11.5\n5.5\n3.5\n8.4\nAuto-CoT (Zhang et al., 2022)\n59.3\n52.0\n13.6\n19.5\n36.1\nAuto-ICL (generating)\n56.4\n41.0\n3.5\n24.5\n31.3\nAuto-ICL (retrieving)\n50.0\n29.0\n6.0\n20.0\n26.3\nTable 5: The table compares our method with baseline methods across various models on the AQUA dataset. Auto ICL (generating) refers to \"Demonstration + Instruction\" method and Auto-ICL (retrieving) refers to \"Instruction method.\ninstructions, formats that have proven successful in ICL. However, other forms of ICL might also enhance LLM performance. Future research could delve into alternative formats such as rephrasing questions, enriching context with anecdotes or narratives, or applying different reasoning processes. Moreover, our research primarily accessed one form of resource - data retrieval from a dataset. There is room to consider other resources like realtime data from the web or integrating user feedback during the learning process. Each of these potential resources could provide additional context or information, further enhancing the problem-solving abilities of LLMs.\n# 7 Conclusion\nIn this work, we introduce a novel framework called Automatic In-Context Learning. Our method enables LLMs to automatically generate context in response to user queries. This approach demonstrates superior performance across various tasks and mdoels compared to existing methods, showing that generated context can surpass human-written context. We also evaluate the effectiveness of different contexts depending on the availability of retrieved queries, highlighting the importance of wisely utilizing available resources for context generation to enhance problem-solving.\n# 8 Limitations\nIn Table 5, we evaluate our methods on earlier OpenAI models, including Text-Davinci-001, 002, and 003, as well as the smaller Llama2-13B model. However, these methods do not perform as well compared to their performance on advanced models. This discrepancy arises because our approach heavily relies on the generation capabilities of the model. In addition, compared to GPT-3.5-Turbo\nand GPT-4, the limitations of the Davinci series, such as a lack of instruction comprehension and sensitivity to prompt variations, can negatively impact performance. As a result, Auto-ICL underperforms with less capable models compared to the Zero-CoT method.\n# References\nAI@Meta. 2024. Llama 3 model card.\n# AI@Meta. 2024. Llama 3 model card.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234. Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. 2022. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, and Yue Zhang. 2023. Chain-of-symbol prompting elicits planning in large langauge models. arXiv preprint arXiv:2305.10276. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T\nJoshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714.\nJoshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5872\u20135877, Hong Kong, China. Association for Computational Linguistics. Junlong Li, Zhuosheng Zhang, and Hai Zhao. 2023. Self-prompting large language models for zero-shot open-domain qa. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, Vancouver, Canada. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\ndeclarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5872\u20135877, Hong Kong, China. Association for Computational Linguistics. Junlong Li, Zhuosheng Zhang, and Hai Zhao. 2023. Self-prompting large language models for zero-shot open-domain qa. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, Vancouver, Canada. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\nLaria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u20137. Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413. Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463\u2013 4473, Hong Kong, China. Association for Computational Linguistics. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv preprint arXiv:2302.00618. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615. Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions over long documents. arXiv preprint arXiv:2305.14564. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. arXiv preprint arXiv:2210.01296. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. arXiv preprint arXiv:2305.04091. Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for\nAlbert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 2300\u20132344, Seattle, United States. Association for Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, and Denny Zhou. 2023. Large language models as analogical reasoners. arXiv preprint arXiv:2310.01714. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.\n# A Appendix\n# A.1 Full results\nFull results of of Table 1 are provided in the Table 6 and Table 7. For each dataset, we randomly sample 200 data points to do the evaluation and remove invalid API responses when calculating the accuracy.\n# A.2 Experimental settings A.2.1 Answer extraction\n# A.2 Experimental settings\nTable 8 presents prompts we utilize for answer extraction under the Zero-Shot setting and other settings.\n# A.2.2 Demonstration generation\nWe display an example of demonstration generation on the GSM8K dataset in Table 10.\n# 2.3 Instruction generation\nIn this section, we demonstrate the generation of instructions on the GSM8K dataset, and provide examples of the generated instructions. We present an example of the Auto-ICL generating 1-to-1 method in Table 11, and exhibit the instructions generated from each dataset in Tables 12, 13, 14, and 15. The instructions produced through the Auto-ICL N-to-1 method are displayed in Tables 16, 17, 18, and 19.\n# A.2.4 Demonstration-Plus-Instruction\nWe present an example illustrating the concatenation of demonstrations with instructions in the context in Table 20.\nA.2.5 Few-Shot Demonstration\n# A.2.5 Few-Shot Demonstration\nWe present the demonstrations used in the FewShot method as shown in Table 1. Within each dataset, we randomly selected 5 examples with labels and combined them in a random order.\nThis section shows instructions generated by the APE method (Zhou et al., 2022) in Tables 21 through 26.\n# A.2.7 Instructions from Instruction Induction\nThis section shows instructions generated by the Instruction Induction method (Honovich et al., 2022) in Tables 27 through 29.\nMethods\nLabeled Data\nTheory of Mind\nSymbolic Reasoning\nTOMI1\nTOMI2\nSocialIQA\nCoin Flip\nLast Letters\nGenerating\nZero-Shot (Brown et al., 2020)\n\u2717\n54.5\n27.5\n63.0\n30.7\n1.5\nZero-Shot-CoT (Kojima et al., 2022)\n\u2717\n52.5\n22.0\n69.5\n81.2\n80.0\nAuto-ICL (ours)\n\u2717\n64.4\n47.6\n66.5\n76.5\n85.0\nRetrieving\nFew-Shot (Brown et al., 2020)\n\u2713\n67.5\n50.5\n72.5\n61.9\n7.9\nAPE (Zhou et al., 2022)\n\u2713\n66.0\n30.0\n24.5\n54.5\n87.0\nInduction (Honovich et al., 2022)\n\u2713\n49.5\n15.0\n64.0\n92.0\n74.5\nAuto-CoT (Zhang et al., 2022)\n\u2717\n51.5\n23.0\n69.0\n99.0\n85.5\nMethods\nLabeled Data\nTheory of Mind\nSymbolic Reasoning\nMultiArith\nAQUA\nGSM8K\nCycle Letters\nShuffle Objects\nGenerating\nZero-Shot (Brown et al., 2020)\n\u2717\n82.7\n27.6\n22.7\n55.0\n34.2\nZero-Shot-CoT (Kojima et al., 2022)\n\u2717\n97.4\n37.4\n56.9\n64.0\n62.1\nAuto-ICL (ours)\n\u2717\n92.5\n60.8\n78.0\n67.0\n52.0\nRetrieving\nFew-Shot (Brown et al., 2020)\n\u2713\n82.9\n31.1\n23.1\n66.0\n32.6\nAPE (Zhou et al., 2022)\n\u2713\n92.5\n43.5\n65.5\n70.0\n56.5\nInduction (Honovich et al., 2022)\n\u2713\n94.0\n52.5\n73.5\n69.0\n64.5\nAuto-CoT (Zhang et al., 2022)\n\u2717\n98.5\n53.0\n78.0\n74.0\n57.5\nMethods\nTable 6: This table compares the accuracy across various tasks using different baseline methods and Auto-ICL. The methods are classified into two categories, \"retrieving\" and \"generating\", based on whether the data is sourced from the dataset. The usage of labeled demonstrations is indicated in the column named \"labeled data\".\nTheory of Mind\nSymbolic Reasoning\nTOMI1\nTOMI2\nSocialIQA\nCoin Flip\nLast Letters\nGenerating\nDemonstration\n58.0\n21.5\n56.3\n63.0\n89.5\nInstruction\n64.4\n47.6\n66.5\n74.6\n75.9\nDemonstration-Plus-Instruction\n67.8\n20.1\n65.5\n100.0\n84.8\nRetrieving\nDemonstration\n52.0\n23.0\n69.0\n99.0\n86.0\nInstruction\n52.0\n71.0\n61.5\n99.0\n87.5\nDemonstration-Plus-Instruction\n28.0\n67.0\n47.5\n94.5\n88.5\nArithmetic\nOthers\nMultiArith\nAQUA\nGSM8K\nCycle Letters\nShuffle Objects\nGenerating\nDemonstration\n93.0\n58.3\n74.5\n65.0\n50.5\nInstruction\n95.3\n50.3\n72.0\n71.7\n44.8\nDemonstration-Plus-Instruction\n95.4\n48.5\n70.6\n70.0\n58.1\nRetrieving\nDemonstration\n93.0\n58.0\n75.0\n65.0\n51.0\nInstruction\n95.0\n52.5\n79.0\n68.0\n68.5\nDemonstration-Plus-Instruction\n93.5\n52.6\n76.5\n72.5\n60.5\nTable 7: Comparison of instruction, demonstration, and instruction-plus-demonstration on each dataset. Th methods are classified as \u2019retrieving\u2019 or \u2019generating\u2019 depending on their utilization of dataset retrieval.\nTable 7: Comparison of instruction, demonstration, and instruction-plus-demonstration on each dataset. These methods are classified as \u2019retrieving\u2019 or \u2019generating\u2019 depending on their utilization of dataset retrieval.\n<div style=\"text-align: center;\">Table 8: Answer extraction prompts used for experiments in Table 1</div>\nNo\nTask\nZero-Shot\nOthers\n1\nTOMI1\nAmong A through C, the answer is\nTherefore, among A through C, the answer is\n2\nTOMI2\nAmong A through C, the answer is\nTherefore, among A through C, the answer is\n3\nSocialIQA\nAmong A through C, the answer is\nTherefore, among A through C, the answer is\n4\nCoin Flip\nThe answer (Yes or No) is\nTherefore, the answer (Yes or No) is\n5\nLast Letters\nThe concatenated letters is\nTherefore, the concatenated letters is\n6\nMultiArith\nThe answer (arabic numerals) is\nTherefore, the answer (arabic numerals) is\n7\nAQUA\nAmong A through E, the answer is\nTherefore, among A through E, the answer is\n8\nGSM8K\nThe answer (arabic numerals) is\nTherefore, the answer (arabic numerals) is\n9\nCycle Letters\nTherefore, the word is\nTherefore, the word is\n10\nShuffle Objects\nAmong A through C, the answer is\nTherefore, among A through C, the answer is\nTable 9: An actual example of N-to-1 instruction generation from the GSM8K dataset and GPT-3.5-Turbo-030 model. The input comprises questions and reasoning paths. Reasoning paths are summarized as a single instructio by the model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/863d/863dab8c-2fca-4e2b-bb0f-b2e0a373424b.png\" style=\"width: 50%;\"></div>\nInstruction N-to-1 generation: (Q, CoT) \u2192\u02c6i Input: Question: Gunther, the gorilla, had 48 bananas hidden under a fern branch. When Gunther wasn\u2019t looking, Arnold, the chimpanzee, stole half of the bananas from the pile. The next day, Gunther added another 25 bananas to his pile, but later that evening, Arnold stole another 12 of the bananas. On the third day, Gunther added another 6 bananas to his pile and began counting bananas. How many bananas did Gunther find were in the pile? Answer: First, we need to figure out how many bananas Arnold stole from Gunther\u2019s original pile of 48. Half of 48 is 24, so Arnold took 24 bananas. After Arnold\u2019s theft, Gunther had 48 - 24 = 24 bananas left. The next day, Gunther added 25 bananas to his pile, bringing the total to 24 + 25 = 49 bananas. But then Arnold stole another 12 bananas, leaving Gunther with 49 - 12 = 37 bananas. On the third day, Gunther added 6 more bananas, bringing the total to 37 + 6 = 43 bananas. Therefore, Gunther found 43 bananas in his pile. ... ... Demonstrations with chain-of-thought ......\nQuestion: Charlie had 10 stickers. He bought 21 stickers from a store in the mall and got 23 stickers for his birthday. Then Charlie gave 9 of the stickers to his sister and used 28 to decorate a greeting card. How many stickers does Charlie have left? Answer: Let\u2019s think step by step. First, we need to convert the number of each coin/bill into its value in dollars. - 100 pennies = 1.00 - 40 nickels = 2.00 (each nickel is worth 5 cents, so 40 nickels is 40 x 5 = 200 cents, or 2.00) - 20 dimes = 2.00 (each dime is worth 10 cents, so 20 dimes is 20 x 10 = 200 cents, or 2.00) - 40 dollar bills = 40.00 Next, we add up the values of each coin/bill: 1.00 + 2.00 + 2.00 + 40.00 = 45.00 Therefore, Brady has 45.00 in his piggy bank.\n1. Read the problem carefully and identify what information is given and what is being asked. 2. Determine what operations are needed to solve the problem (addition, subtraction, multiplication, division, etc.). 3. Use the given information to set up an equation or equations to solve for the unknown variable. 4. Solve the equation(s) using the appropriate operations. 5. Check your answer to make sure it makes sense in the context of the problem. 6. If necessary, convert units or simplify the answer to the appropriate form (fractions, decimals, percentages, etc.).\nTable 10: An example illustrating our demonstration generation process. First, we prompt the model to examples based on the current question. Then, these examples are decomposed to generate CoTs with the  \"Let\u2019s think step by step\". Finally, these examples and their CoTs serve as demonstrations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a33e/a33e201a-270b-4eae-894e-9af5e1de43af.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8489/84892bdd-1571-4ce5-ad2d-f8300c7d1453.png\" style=\"width: 50%;\"></div>\nTable 11: An example illustrating our instruction generation process. Firstly, we prompt the model to generate instruction that are based on the current question. Secondly, the generated instruction is used to aid in addressing the original question.\nnstruction that are based on the current question. Secondly, the generated instruction is used to aid in addressing he original question. Instruction 1-to-1 generation: q \u2192i Q: Generate a plan to solve this question: \"Luke is spending time at the beach building sandcastles. He eventually notices that each level of a sandcastle will have half the square footage as the level below it. If he makes a 4 leveled sandcastle where the top level has a square footage of 16, what is the average square footage of a level?\" A: 1. Determine the square footage of the bottom level of the sandcastle. 2. Use the given information that each level has half the square footage of the level below it to find the square footage of each level. 3. Add up the square footage of all four levels. 4. Divide the total square footage by the number of levels to find the average square footage per level. Q: Plan: 1. Determine the square footage of the bottom level of the sandcastle. 2. Use the given information that each level has half the square footage of the level below it to find the square footage of each level. 3. Add up the square footage of all four levels. 4. Divide the total square footage by the number of levels to find the average square footage per level. Question: Luke is spending time at the beach building sandcastles. He eventually notices that each level of a sandcastle will have half the square footage as the level below it. If he makes a 4 leveled sandcastle where the top level has a square footage of 16, what is the average square footage of a level? Solve the question step by step based on the plan. A: Step 1: Determine the square footage of the bottom level of the sandcastle. Since the top level has a square footage of 16, we can use the given information that each level has half the square footage of the level below it to work backwards and find the square footage of the bottom level. Level 4: 16 square feet Level 3: 16 x 2 = 32 square feet Level 2: 32 x 2 = 64 square feet Level 1: 64 x 2 = 128 square feet Therefore, the square footage of the bottom level is 128 square feet. Step 2: Use the given information that each level has half the square footage of the level below it to find the square footage of each level. We have already found the square footage of each level in Step 1. Step 3: Add up the square footage of all four levels. We can add up the square footage of all four levels to find the total square footage of the sandcastle. Total square footage = 16 + 32 + 64 + 128 = 240 square feet Step 4: Divide the total square footage by the number of levels to find the average square footage per level. Since there are four levels in the sandcastle, we can divide the total square footage by 4 to find the average square footage per level. Average square footage per level = 240 / 4 = 60 square feet Therefore, the average square footage of a level in the sandcastle is 60 square feet.\nInstruction 1-to-1 generation: q \u2192i\nQ: Generate a plan to solve this question: \"Luke is spending time at the beach building sandcastles. He\neventually notices that each level of a sandcastle will have half the square footage as the level below it. If\nhe makes a 4 leveled sandcastle where the top level has a square footage of 16, what is the average square\nfootage of a level?\"\nA: 1. Determine the square footage of the bottom level of the sandcastle.\n2. Use the given information that each level has half the square footage of the level below it to find the\nsquare footage of each level.\n3. Add up the square footage of all four levels.\n4. Divide the total square footage by the number of levels to find the average square footage per level.\nQ:\nPlan: 1. Determine the square footage of the bottom level of the sandcastle. 2. Use the given information\nthat each level has half the square footage of the level below it to find the square footage of each level. 3.\nAdd up the square footage of all four levels. 4. Divide the total square footage by the number of levels to\nfind the average square footage per level.\nQuestion: Luke is spending time at the beach building sandcastles. He eventually notices that each level\nof a sandcastle will have half the square footage as the level below it. If he makes a 4 leveled sandcastle\nwhere the top level has a square footage of 16, what is the average square footage of a level?\nSolve the question step by step based on the plan.\nA: Step 1: Determine the square footage of the bottom level of the sandcastle. Since the top level has a\nsquare footage of 16, we can use the given information that each level has half the square footage of the\nlevel below it to work backwards and find the square footage of the bottom level.\nLevel 4: 16 square feet Level 3: 16 x 2 = 32 square feet Level 2: 32 x 2 = 64 square feet Level 1: 64 x 2 =\n128 square feet\nTherefore, the square footage of the bottom level is 128 square feet.\nStep 2: Use the given information that each level has half the square footage of the level below it to find\nthe square footage of each level. We have already found the square footage of each level in Step 1.\nStep 3: Add up the square footage of all four levels. We can add up the square footage of all four levels to\nfind the total square footage of the sandcastle.\nTotal square footage = 16 + 32 + 64 + 128 = 240 square feet\nStep 4: Divide the total square footage by the number of levels to find the average square footage per level.\nSince there are four levels in the sandcastle, we can divide the total square footage by 4 to find the average\nsquare footage per level.\nAverage square footage per level = 240 / 4 = 60 square feet\nTherefore, the average square footage of a level in the sandcastle is 60 square feet.\nInstruction 1-to-1 generation: q \u2192i\nQ: Generate a plan to solve this question: \"Luke is spending time at the beach building sandcastles. He\neventually notices that each level of a sandcastle will have half the square footage as the level below it. If\nhe makes a 4 leveled sandcastle where the top level has a square footage of 16, what is the average square\nfootage of a level?\"\nA: 1. Determine the square footage of the bottom level of the sandcastle.\n2. Use the given information that each level has half the square footage of the level below it to find the\nsquare footage of each level.\n3. Add up the square footage of all four levels.\n4. Divide the total square footage by the number of levels to find the average square footage per level.\nQ:\nPlan: 1. Determine the square footage of the bottom level of the sandcastle. 2. Use the given information\nthat each level has half the square footage of the level below it to find the square footage of each level. 3.\nAdd up the square footage of all four levels. 4. Divide the total square footage by the number of levels to\nfind the average square footage per level.\nQuestion: Luke is spending time at the beach building sandcastles. He eventually notices that each level\nQuestion: Luke is spending time at the beach building sandcastles. He eventually notices that each leve of a sandcastle will have half the square footage as the level below it. If he makes a 4 leveled sandcastl where the top level has a square footage of 16, what is the average square footage of a level? Solve the question step by step based on the plan.\nDataset\nQuestion\nInstruction\nTOMI1\nAva entered the sunroom. Lily entered\nthe sunroom. Ava loves the skirt. The\nasparagus is in the bathtub. Ava exited\nthe sunroom. Lily moved the aspara-\ngus to the pantry. Lily dislikes the skirt.\nLily exited the sunroom. Ava entered\nthe garage. Where will Ava look for the\nasparagus? Answer Choices: (A) bath-\ntub (B) pantry (C) Unknown\n1. Read the given information carefully to un-\nderstand the sequence of events. 2. Identify\nthe key details in the passage, such as the lo-\ncation of the asparagus and the actions of Ava\nand Lily. 3. Note that Ava entered the sunroom\nand Lily moved the asparagus to the pantry af-\nter Ava left. 4. Since Ava did not witness Lily\nmoving the asparagus, she would not know\nthat it was moved to the pantry. 5. Therefore,\nAva would most likely look for the asparagus\nin its original location, which is the bathtub. 6.\nTherefore, the answer is (A) bathtub.\nTOMI2\nMason entered the laundry. Alexander\ndislikes the underpants. Alexander en-\ntered the laundry. The onion is in the\ntreasure chest. Mason exited the laun-\ndry. Alexander moved the onion to the\ncrate. Ella likes the pajamas. Alexander\nexited the laundry. Mason entered the\nlaundry. Where does Mason think that\nAlexander searches for the onion? An-\nswer Choices: (A) Unknown (B) trea-\nsure chest (C) crate\n1. Read the given information carefully to un-\nderstand the sequence of events. 2. Identify\nthe key characters and objects mentioned in the\nquestion. 3. Note down the actions performed\nby each character and the location of the ob-\njects. 4. Analyze the information to determine\nwhere Mason thinks Alexander searches for\nthe onion. 5. Eliminate answer choices that\nare not supported by the given information. 6.\nChoose the answer choice that is the most log-\nical based on the given information. 7. Verify\nthe answer by checking if it aligns with the\nsequence of events and actions performed by\nthe characters.\nSocialIQA\nRiley is the most popular student in the\nschool. What will Others want to do\nnext?\nAnswer Choices: (A) run for\nclass president (B) run for student coun-\ncil (C) join Riley\u2019s gang\n1. Identify the reasons why Riley is the most\npopular student in the school. 2. Analyze the\ncharacteristics of Riley\u2019s popularity and de-\ntermine what aspects of it others may want\nto emulate. 3. Consider the potential motiva-\ntions of the other students and what they hope\nto achieve by following in Riley\u2019s footsteps.\n4. Evaluate the answer choices provided and\ndetermine which option aligns best with the\nmotivations and goals of the other students. 5.\nChoose the answer choice that best fits the sit-\nuation.\nMason entered the laundry. Alexander dislikes the underpants. Alexander entered the laundry. The onion is in the treasure chest. Mason exited the laundry. Alexander moved the onion to the crate. Ella likes the pajamas. Alexander exited the laundry. Mason entered the laundry. Where does Mason think that Alexander searches for the onion? Answer Choices: (A) Unknown (B) treasure chest (C) crate\nTable 12: Examples of generated instructions using the q \u2192\u02c6i method for Theory of Mind tasks.\n1. Identify the reasons why Riley is the most popular student in the school. 2. Analyze the characteristics of Riley\u2019s popularity and determine what aspects of it others may want to emulate. 3. Consider the potential motivations of the other students and what they hope to achieve by following in Riley\u2019s footsteps. 4. Evaluate the answer choices provided and determine which option aligns best with the motivations and goals of the other students. 5. Choose the answer choice that best fits the situation.\nDataset\nQuestion\nInstruction\nCoin Flip\nA coin is heads up. Bill flips the coin.\nSantos does not flip the coin. Roxy flips\nthe coin. Randi flips the coin. Is the\ncoin still heads up? Note that \"flip\" here\nmeans \"reverse\".\n1. Determine the starting position of the coin\n(heads up). 2. Understand the meaning of\n\"flip\" in this context (reverse). 3. Determine\nthe order of the flips: Bill, Santos, Roxy, Randi.\n4. Apply the flip to the starting position of\nthe coin for each person in the order given.\n5. Determine the final position of the coin\nafter all flips have been applied. 6. Compare\nthe final position to the starting position to\ndetermine if the coin is still heads up.\nLast Letter\nTake the last letters of each words in\n\"Rena Devon Rosalinda Paulina\" and\nconcatenate them.\n1. Identify the given words: \"Rena Devon\nRosalinda Paulina\" 2. Determine the last letter\nof each word. 3. Write down the last letters of\neach word. 4. Concatenate the last letters in\nthe order they appear in the original phrase. 5.\nVerify that the resulting string is correct.\nnerated instructions using the q \u2192\u02c6i method for Symbolic Reasoning ta\nDataset\nQuestion\nInstruction\nMultiArith\nPaige and her friends were recycling pa-\nper for their class. For every 4 pounds\nthey recycled they earned one point.\nIf Paige recycled 14 pounds and her\nfriends recycled 2 pounds, how many\npoints did they earn?\n1. Determine the total weight of paper recy-\ncled by Paige and her friends. 2. Add the\nweight of paper recycled by Paige and her\nfriends. 3. Divide the total weight of paper re-\ncycled by 4 to determine the number of points\nearned. 4. Round the answer to the nearest\nwhole number, as points are given in whole\nnumbers. 5. Write the final answer in the form\nof a sentence.\nAQUA\nIn a row of children Neha is 12th from\nleft end and Radha is 6th from right\nend. When Radha is shifted to left by 2\nplaces and Neha is shifted to right by 2\nplaces there 6 children between Radha\nand Neha. How many children are there\nin the row? Answer Choices: (A) 23\n(B) 27 (C) 26 (D) 28 (E) 29\n1. Draw a diagram to represent the given in-\nformation. 2. Use the information given to\ndetermine the total number of children in the\nrow. 3. Determine the original positions of\nNeha and Radha in the row. 4. Determine\nthe new positions of Neha and Radha after\nthey are shifted. 5. Use the information given\nto determine the number of children between\nNeha and Radha after they are shifted. 6. Use\nthe information gathered to solve for the total\nnumber of children in the row. 7. Check the\nanswer choices and select the correct answer.\nGSM8K\nCarly is making cupcakes and brownies\nfor her art class. She makes 2 less than\nthree times as many brownies as cup-\ncakes. If Carly\u2019s class has five people\nand each person gets two treats, how\nmany cupcakes did Carly make?\n1. Use algebra to set up an equation to repre-\nsent the relationship between the number of\ncupcakes and brownies Carly made. 2. Solve\nthe equation to find the number of brownies\nCarly made. 3. Use the number of brownies\nto find the number of cupcakes Carly made. 4.\nMultiply the number of cupcakes by 2 to find\nthe total number of treats. 5. Divide the total\nnumber of treats by 10 (5 people x 2 treats per\nperson) to find the number of cupcakes each\nperson gets.\nTable 14: Examples of generated instructions using the q \u2192\u02c6i method for Arithmetic tasks.\nDataset\nQuestion\nInstruction\nCycle Letter\nPlease unscramble the letters into a\nword, and write that word: ebukkak =\n1. Identify the letters in the scrambled word:\nebukkak. 2. Rearrange the letters to form a\nword. 3. Check if the word makes sense and\nis spelled correctly. 4. Write the unscrambled\nword.\nShuffle Object\nAlice, Bob, and Claire are dancers at a\nsquare dance. At the start of a song, they\neach have a partner: Alice is dancing\nwith Sam, Bob is dancing with Helga,\nand Claire is dancing with Karl.\nThroughout the song, the dancers often\ntrade partners. First, Claire and Alice\nswitch partners. Then, Bob and Alice\nswitch partners. Finally, Claire and Bob\nswitch partners. At the end of the dance,\nAlice is dancing with Which choice is\ntrue ? Answer Choices: (A) Sam. (B)\nHelga. (C) Karl.\n1. Understand the problem: Read the prob-\nlem carefully and make sure you understand\nthe given information and what is being asked.\n2. Visualize the problem: Draw a diagram or\nuse a table to keep track of the dancers and\ntheir partners before and after each switch. 3.\nFollow the steps: Follow the given steps in\nthe problem and make sure to update the dia-\ngram or table after each switch. 4. Determine\nthe final partner: After the final switch, deter-\nmine who Alice is dancing with and compare\nit to the answer choices given. 5. Choose the\ncorrect answer: Select the answer choice that\nmatches the final partner Alice is dancing with.\nExamples of generated instructions using the q \u2192\u02c6i method for other ta\nDataset\nQuestion\nTOMI1\nAva entered the sunroom. Lily entered\nthe sunroom. Ava loves the skirt. The\nasparagus is in the bathtub. Ava exited\nthe sunroom. Lily moved the aspara-\ngus to the pantry. Lily dislikes the skirt.\nLily exited the sunroom. Ava entered\nthe garage. Where will Ava look for the\nasparagus? Answer Choices: (A) bath-\ntub (B) pantry (C) Unknown\nTOMI2\nMason entered the laundry. Alexander\ndislikes the underpants. Alexander en-\ntered the laundry. The onion is in the\ntreasure chest. Mason exited the laun-\ndry. Alexander moved the onion to the\ncrate. Ella likes the pajamas. Alexander\nexited the laundry. Mason entered the\nlaundry. Where does Mason think that\nAlexander searches for the onion? An-\nswer Choices: (A) Unknown (B) trea-\nsure chest (C) crate\nSocialIQA\nRiley is the most popular student in the\nschool. What will Others want to do\nnext?\nAnswer Choices: (A) run for\nclass president (B) run for student coun-\ncil (C) join Riley\u2019s gang\nDataset\nQuestion\nInstruction\nTOMI1\nAva entered the sunroom. Lily entered\nthe sunroom. Ava loves the skirt. The\nasparagus is in the bathtub. Ava exited\nthe sunroom. Lily moved the aspara-\ngus to the pantry. Lily dislikes the skirt.\nLily exited the sunroom. Ava entered\nthe garage. Where will Ava look for the\nasparagus? Answer Choices: (A) bath-\ntub (B) pantry (C) Unknown\n1. Identify the characters and locations men-\ntioned in the given information. 2. Look for\nany objects or items that are mentioned and\ntheir initial location. 3. Check if any charac-\nters move or interact with the objects/items\nand note their new location. 4. Determine\nwhich character is being asked about and their\nrelationship to the object/item. 5. Use the\ninformation gathered to determine where the\ncharacter would look for the object/item. If\nthere is not enough information, choose \"Un-\nknown\" as the answer.\nTOMI2\nMason entered the laundry. Alexander\ndislikes the underpants. Alexander en-\ntered the laundry. The onion is in the\ntreasure chest. Mason exited the laun-\ndry. Alexander moved the onion to the\ncrate. Ella likes the pajamas. Alexander\nexited the laundry. Mason entered the\nlaundry. Where does Mason think that\nAlexander searches for the onion? An-\nswer Choices: (A) Unknown (B) trea-\nsure chest (C) crate\n1. Identify the key information in the given\nstatements, such as the location of objects and\nthe actions of individuals. 2. Determine if\nthere is any interaction between the individu-\nals mentioned in the statements. 3. Look for\nany statements that indicate where an object\nhas been moved to. 4. Consider any personal\npreferences or dislikes mentioned in the state-\nments, but only if they are relevant to the loca-\ntion of the object. 5. If the question asks where\none individual thinks another individual will\nsearch for an object, use the information about\nwhere the object has been moved to and any\ninteractions between the individuals to make\nan educated guess. 6. If there is not enough\ninformation to determine the answer, choose\nthe option for \"Unknown\".\nSocialIQA\nRiley is the most popular student in the\nschool. What will Others want to do\nnext?\nAnswer Choices: (A) run for\nclass president (B) run for student coun-\ncil (C) join Riley\u2019s gang\nThe general chain of thought for all questions\nis to analyze the given information and deter-\nmine the most logical and likely outcome or\nnext step based on that information. This in-\nvolves considering the context of the situation,\nthe actions and behaviors of the individuals\ninvolved, and any relevant factors that may im-\npact the outcome. It also involves eliminating\nanswer choices that are irrelevant or unlikely\nbased on the information provided. Ultimately,\nthe goal is to arrive at the most accurate and\nreasonable answer based on the given informa-\ntion.\nMason entered the laundry. Alexander dislikes the underpants. Alexander entered the laundry. The onion is in the treasure chest. Mason exited the laundry. Alexander moved the onion to the crate. Ella likes the pajamas. Alexander exited the laundry. Mason entered the laundry. Where does Mason think that Alexander searches for the onion? Answer Choices: (A) Unknown (B) treasure chest (C) crate\nTable 16: Generated instructions for all data points within each Theory of Mind (ToM) dataset, achieved via the generating path Q \u2192 \u02c6 CoT \u2192\u02c6I.\n1. Identify the key information in the given statements, such as the location of objects and the actions of individuals. 2. Determine if there is any interaction between the individuals mentioned in the statements. 3. Look for any statements that indicate where an object has been moved to. 4. Consider any personal preferences or dislikes mentioned in the statements, but only if they are relevant to the location of the object. 5. If the question asks where one individual thinks another individual will search for an object, use the information about where the object has been moved to and any interactions between the individuals to make an educated guess. 6. If there is not enough information to determine the answer, choose the option for \"Unknown\".\nDataset\nQuestion\nInstruction\nCoin Flip\nA coin is heads up. Bill flips the coin.\nSantos does not flip the coin. Roxy flips\nthe coin. Randi flips the coin. Is the\ncoin still heads up? Note that \"flip\" here\nmeans \"reverse\".\n1. Identify the initial state of the coin (heads\nup or tails up). 2. For each person who flips\nthe coin, reverse the current state of the coin.\n3. For each person who does not flip the coin,\nthe state of the coin remains the same. 4. De-\ntermine the final state of the coin after all the\nflips have been completed.\nLast Letter\nTake the last letters of each words in\n\"Rena Devon Rosalinda Paulina\" and\nconcatenate them.\nTo find the answer for each question, we need\nto identify the last letter of each word in the\ngiven list. Then, we concatenate these letters\nin the order they appear to get the final answer.\nTable 17: Generated instructions for all data points within each Symbolic Reasoning dataset, achieved via the generating path Q \u2192 \u02c6 CoT \u2192\u02c6I.\nDataset\nQuestion\nI\nMultiArith\nPaige and her friends were recycling pa-\nper for their class. For every 4 pounds\nthey recycled they earned one point.\nIf Paige recycled 14 pounds and her\nfriends recycled 2 pounds, how many\npoints did they earn?\nS\nq\nb\ni\nt\nt\nc\nAQUA\nIn a row of children Neha is 12th from\nleft end and Radha is 6th from right\nend. When Radha is shifted to left by 2\nplaces and Neha is shifted to right by 2\nplaces there 6 children between Radha\nand Neha. How many children are there\nin the row? Answer Choices: (A) 23\n(B) 27 (C) 26 (D) 28 (E) 29\n1\ny\nt\n3\nw\ne\np\nm\ns\nf\na\ns\nt\nGSM8K\nCarly is making cupcakes and brownies\nfor her art class. She makes 2 less than\nthree times as many brownies as cup-\ncakes. If Carly\u2019s class has five people\nand each person gets two treats, how\nmany cupcakes did Carly make?\n1\nw\na\nn\nt\nt\ne\n4\no\ns\nl\nt\nd\nDataset\nQuestion\nInstruction\nMultiArith\nPaige and her friends were recycling pa-\nper for their class. For every 4",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of in-context learning (ICL) in large language models (LLMs), which traditionally relies on human-provided contexts that are labor-intensive and limit model capabilities to human-manageable tasks.",
        "problem": {
            "definition": "The problem is the dependence on human-generated contexts for in-context learning, which hinders the efficiency and effectiveness of large language models in solving a variety of tasks.",
            "key obstacle": "The main obstacle is the labor-intensive nature of creating human-written contexts and the limitation of tasks to those manageable by humans, which restricts the potential of LLMs."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for models to autonomously generate examples and instructions without relying on human input.",
            "opinion": "The proposed idea, Automatic In-Context Learning (Auto-ICL), allows models to independently generate necessary contexts to solve problems effectively.",
            "innovation": "The key innovation lies in the model's ability to generate its own contextual information, as opposed to relying on pre-existing human-generated contexts, thus enhancing its problem-solving capabilities."
        },
        "method": {
            "method name": "Automatic In-Context Learning",
            "method abbreviation": "Auto-ICL",
            "method definition": "Auto-ICL is a framework that enables large language models to autonomously generate examples and instructions for problem-solving based on a given query.",
            "method description": "The method allows models to generate context that aids in answering queries without human intervention.",
            "method steps": [
                "Step 1: Present the question to the model.",
                "Step 2: Prompt the model to generate demonstrations and/or instructions based on the question.",
                "Step 3: Use the self-generated context along with the original query to produce the final answer."
            ],
            "principle": "This method is effective because it allows the model to tailor the context specifically to the query, thereby enhancing relevance and accuracy in responses."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted across various tasks including Theory of Mind, symbolic reasoning, arithmetic, and others using multiple models and datasets.",
            "evaluation method": "Performance was assessed by comparing the results of Auto-ICL with baseline methods, measuring accuracy and effectiveness in generating correct answers."
        },
        "conclusion": "The Automatic In-Context Learning framework demonstrates superior performance across various tasks and models, showing that generated contexts can outperform human-written contexts and improve problem-solving capabilities.",
        "discussion": {
            "advantage": "The main advantages of Auto-ICL include reduced reliance on human effort for context creation and enhanced model performance across a broader range of tasks.",
            "limitation": "A limitation of the method is its performance on less capable models, which may not leverage the full potential of the Auto-ICL framework.",
            "future work": "Future research could explore alternative methods for context generation and the integration of real-time data to further enhance the model's problem-solving abilities."
        },
        "other info": {
            "code availability": "The code for the Automatic In-Context Learning framework is available at https://github.com/ecielyang/Auto-ICL.",
            "key findings": {
                "finding1": "Model-generated contexts outperform human-annotated contexts.",
                "finding2": "Different forms of context (retrieved vs. generated) impact performance based on task requirements."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) in large language models (LLMs) traditionally relies on human-provided contexts that are labor-intensive."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea, Automatic In-Context Learning (Auto-ICL), allows models to independently generate necessary contexts to solve problems effectively."
        },
        {
            "section number": "2",
            "key information": "The problem is the dependence on human-generated contexts for in-context learning, which hinders the efficiency and effectiveness of large language models."
        },
        {
            "section number": "3.1",
            "key information": "Auto-ICL enables large language models to autonomously generate examples and instructions for problem-solving based on a given query."
        },
        {
            "section number": "3.4",
            "key information": "The method allows models to generate context that aids in answering queries without human intervention."
        },
        {
            "section number": "6",
            "key information": "A limitation of the method is its performance on less capable models, which may not leverage the full potential of the Auto-ICL framework."
        },
        {
            "section number": "7",
            "key information": "The Automatic In-Context Learning framework demonstrates superior performance across various tasks and models, showing that generated contexts can outperform human-written contexts."
        }
    ],
    "similarity_score": 0.7260024832147487,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Auto-ICL_ In-Context Learning without Human Supervision.json"
}