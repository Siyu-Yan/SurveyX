{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.02160",
    "title": "Data Poisoning for In-context Learning",
    "abstract": "In the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL\u2019s susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT4 model, demonstrate that ICL\u2019s performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.",
    "bib_name": "he2024datapoisoningincontextlearning",
    "md_text": "# Data Poisoning for In-context Learning\nPengfei He 1 Han Xu 1 Yue Xing 1 Hui Liu 1 Makoto Yamada 2 Jiliang Tang 1\n# Abstract\nIn the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL\u2019s susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT4 model, demonstrate that ICL\u2019s performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.\narXiv:2402.02160v2\n# 1. Introduction\nIn recent years, In-Context Learning (ICL) (Brown et al., 2020; Min et al., 2022) has emerged as an important component of large language models (LLMs). Unlike traditional machine learning algorithms that require extensive retraining or fine-tuning to adapt new data (Hoi et al., 2021; Zhang & Yang, 2021; Zhuang et al., 2020), ICL enables LLMs to make predictions based on extra information in the prompt (e.g. a few examples related to a specific task), without changing the model parameters. For example, consider the task of predicting a person\u2019s nationality and the prompt con-\n*Equal contribution 1Department of Computer Science and Engineering, Michigan State University, USA 2Okinawa Institute of Science and Technology OIST, Japan. Correspondence to: Pengfei He <hepengf1@msu.edu>.\nsists of examples, e.g. \u201cAlbert Einstein was German; Isaac Newton was English;\u201d, followed by the query \u201cThomas Edison was\u201d, an LLM such as GPT-4 will predict \u201cAmerican\u201d accurately. The efficiency and flexibility of ICL have gained significant attention and revolutionized various real-world applications, ranging from reasoning (e.g. chain-of-thought (Wei et al., 2022; Wang et al., 2022)) to retrieval-augmentedgeneration (RAG) (Lewis et al., 2020). The success of ICL depends critically on the examples it utilizes. Studies have shown that the ICL performance is sensitive to certain characteristics of demonstrations, e.g., the selection of examples (Wang et al., 2023) and the order of examples in demonstration (Min et al., 2022). Therefore, it naturally raises a question: Is ICL vulnerable to potential data poisoning attacks? In reality, malicious actors may manipulate demonstration data to degrade the model performance. For example, adversaries can execute data poisoning attacks by strategically altering the examples in demonstrations used in ICL. Such attacks aim to disrupt the learning process, leading to inaccurate or biased predictions. In this paper, we aim to answer the above question by delving into data poisoning attacks in ICL and uncovering the vulnerability of ICL against these attacks. We consider the standard pipeline of ICL, where examples are randomly selected from a certain data set to tailor ICL for a downstream task. In terms of the attack, we assume that the adversary deliberately alters some examples in the data, and his goal is to ensure that the learning process is adversely affected and the overall performance decreases. This scenario is both significant and practical. For instance, in a RAG system where demonstrations are retrieved from a dataset, the attacker manipulates some data from a specific domain or a task (e.g. reviews about a specific brand) to downgrade the quality of the generated content of LLM related to that domain/task. To the best of our knowledge, we are the first to investigate the vulnerability of ICL against data poisoning attacks. Data poisoning in ICL faces both unique challenges specific to ICL and common obstacles in traditional data poisoning. First, in contrast to traditional learning algorithms with explicit training objectives, ICL enables LLMs to learn from demonstrations without explicit optimization (Brown et al., 2020; Min et al., 2022). Since traditional poisoning strategies are designed specifically to target the training process\nand exploit loss functions in conventional models (Biggio et al., 2012; Steinhardt et al., 2017; Geiping et al., 2020; He et al., 2023), the discrepancy between ICL and these strategies implies that they are not directly applicable to ICL. Conducting effective data poisoning attacks for ICL demands a thorough understanding of the unique learning mechanism of ICL. Second, similar to traditional attacking methods, data poisoning for ICL also requires creating samples that are imperceptible to humans yet disruptive. These poisoned examples must seamlessly integrate with the other data to harm the learning process in a subtle way. While both traditional data poisoning and the attack in ICL require careful designs of the attack, for ICL, one extra challenge arises from the discrete vocabulary of language models, making it hard to manipulate inputs for effective disturbance (Lei et al., 2019; Xu et al., 2023). To tackle the above challenges, we introduce a novel and versatile attacking framework, ICLPoison, to exploit the unique learning mechanism of ICL. In particular, previous research (Xie et al., 2021; Hendel et al., 2023; Liu et al., 2023b; Wang et al., 2023) has shown a strong correlation between ICL performance and the hidden states within LLMs. Our framework, ICLPoison, ingeniously distorts these hidden states through strategic text perturbations\u2014a non-trivial adaption given the attacker\u2019s limited ability to only alter the examples in the demonstration. We further design three strategies for instantiating and optimizing poisoning attacks under the ICLPoison framework. Comprehensive experiments across various LLMs and tasks demonstrate the effectiveness of our methods, highlighting the vulnerability of ICL. Notably, we have successfully degraded the performance of ICL in advanced models, including GPT-4 (a 10% decrease in ICL accuracy). Our study significantly advances the understanding of ICL\u2019s vulnerabilities to data poisoning, bolstering the security and reliability of LLMs.\n# 2. Preliminary\nIn this section, we introduce some background of ICL. We then present the preliminary results that the distortion in hidden states indeed affects the ICL performance, which inspires our framework. Key notations are also introduced.\n1Since the main focus of this paper is not on the generation of LLMs, we adopt the default generation scheme for each LLM.\ntask t, the user first randomly selects k input-output pairs from Dp t and concatenates them as a demonstration S, i.e. S = [(xp i,t, yp i,t)]k i=1. The demonstration is combined with query xquery as an input prompt, and this prompt is sent to the LLM. We define the prediction of ICL as \u02c6yquery ICL = f([S, xquery]). In this work, we consider classification tasks such as sentiment analysis and text classification. It is worth noting that when conducting ICL in practice, the demonstrations are formulated in a template such as \u201cQ:{input}, A:{output}\u201d and \u201c{input}\u2192{output}\u201d. 2.2. Understanding of ICL via Hidden States The emergence of ICL ability in LLMs has gained substantial attention, particularly in understanding its working mechanisms (Xie et al., 2021; Hendel et al., 2023; Von Oswald et al., 2023; Garg et al., 2022; Bai et al., 2023). Researchers have demonstrated that during ICL, LLMs can effectively extract \u201clatent concepts\u201d (denoted as \u03b8) from demonstrations (Xie et al., 2021; Wang et al., 2023), and the ICL predicting procedure can be decomposed as\nwhere P(\u00b7) denotes the probability distribution during the generation process of LLMs. In other words, ICL learns \u03b8 from the demonstration first and then make a prediction for xquery. The latent concept \u03b8 characterizes the intricate patterns and contextual information embedded in the data regarding the task. A key in this process is the hidden states, represented as h, which are defined as the representations of the last token of the input prompt at different layers of the model (as indicated by various studies (Hendel et al., 2023; Liu et al., 2023b; Todd et al., 2023)). The hidden states are used in encoding the latent concepts. In particular, consider a model f composed of L transformer layers, where each layer produces a vector representation hl(p, f) \u2208Rd for the last token of the input prompt p and l \u2208[L]. It has been observed that the hidden state hl at certain intermediate layers effectively works as \u03b8 (Hendel et al., 2023). Moreover, further research (Liu et al., 2023b; Todd et al., 2023) has been conducted to distill more concise representations from the set of hidden states {hl}L l=1, enhancing the concept formation in ICL.\n# 2.3. Preliminary Experiments\nAs discussed in the above, hidden states of the input prompt encode the latent concept and are closely related to the performance of ICL. Therefore, we conduct preliminary experiments to explore whether directly perturbing these hidden states can degrade ICL, and whether ICL is more vulnerable when perturbing the hidden states in all layers.\nAs discussed in the above, hidden states of the input prompt encode the latent concept and are closely related to the performance of ICL. Therefore, we conduct preliminary experiments to explore whether directly perturbing these hidden states can degrade ICL, and whether ICL is more vulnerable when perturbing the hidden states in all layers. We follow the setup of experiments in (Hendel et al., 2023). In detail, we focus on the template of demonstration: \u201c{input}\u2192{output}\\n{query}\u2192\u201d, and compute the\nrepresentation of the last token \u201c\u2192\u201d for each layer as the hidden state. Then we add Gaussian noises to these representations and conduct ICL predictions conditional on the perturbed hidden states. We consider two kinds of perturbations: adding noises to the representations of all layers, and adding noise only to the layer that achieves the best ICL performance (as indicated in (Hendel et al., 2023; Todd et al., 2023), the representation of this particular layer is shown to compactly encode latent concepts). The standard deviation of the Gaussian noise is set as a product of a strength ratio and the original deviation of representations. We experiment with different strength ratios (0.1, 0.3, 1) to evaluate the impact of varying noise intensities. We conduct experiments on open-source LLM LLaMA2-7B (Touvron et al., 2023) and a sentiment analysis dataset GLUE-SST2 (Wang et al., 2019). We conduct a 5-shot ICL following the standard pipeline discussed in Section 2.1 where examples in demonstrations are randomly selected from the training data, and report the average accuracy over 5 independent runs on the testing data as ICL accuracy. The results are shown in Figure 1. It is obvious that ICL performance is sensitive to the perturbations in these hidden states, specifically when perturbing all the layers rather than a particular layer. Our observations reveal the potential vulnerability of ICL and pave the way to design our framework based on perturbing the hidden states of LLMs during the ICL.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e72/5e723240-a795-47a5-b482-baf09dd5b827.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Results when directly perturbing the hidden states with Gaussian noises of various standard deviations (noise strength).</div>\n# 3. A Novel Framework: ICLPoison\nIn this section, we introduce a novel framework, ICLPoison, to conduct data poisoning by distorting hidden states of LLMs during the ICL process.\n# 3.1. Threat Model\nWe assume that attackers have the full access to a portion of the dataset, specifically a subset related to a particular task. However, they may lack complete knowledge of the entire prompting set. Crucially, they are unaware of the details of the ICL process, including the test data, the Large Language Models (LLMs) employed, and specific ICL configurations like the number of examples and the templates used for demonstrations. Despite these limitations, the attackers can leverage open-source LLMs to introduce poisoned data into the subset they control.\n3.2. ICLPoison Design As highlighted in Section 2.1, ICL differs from traditional learning algorithms since it lacks an explicit training objective that can be directly targeted by data poisoning attacks. To address this unique challenge, we draw inspiration from the dynamics of hidden states from Section 2.2, which indicates that the representation of the last token of the demonstration encodes the latent concept about the task. Building on this understanding, we introduce ICLPoison, a novel data poisoning framework specifically designed for the ICL process. ICLPoison strategically alters examples in demonstrations to distort the hidden states in the LLM, leveraging these internal dynamics to achieve our poisoning goals. The details of ICLPoison are as follows. We focus on a surrogate LLM f with L layers and a task-specific prompt set Dp t = {(xp i,t, yp i,t)}N i=1 from task t. Our objective is to diminish the ICL accuracy for the task t by maximizing the distortion of hidden states in samples from Dp t . We propose perturbing the input xp i,t \u223cXt using a transformation \u03b4 : Xt \u2192Xt while keeping the label yp i,t unchanged. The perturbation \u03b4 must be imperceptible to humans, hence it is constrained within a set \u2206of imperceptible mappings. Details of \u03b4 will be discussed in Section 3.3. For each example (xp i,t, yp i,t) in Dp t , we extract its hidden states. Since the attacker lacks the knowledge of the test data, we use a dummy query xquery t \u223cXt as a stand-in (as mentioned in (Hendel et al., 2023)). We then concatenate (xp i,t, yp i,t) with xquery t to create a demonstration, and denote hl(xp i,t, f) as the representation of the last token in the lth layer of model f. Since our focus is on perturbing xp i,t, we omit yp i,t in the following discussion. The representations from all L layers of model f regarding input xp i,t are denoted as H(xp i,t, f) := {hl(xp i,t, f)}L l=1, representing the hidden states for xp i,t under model f. For the perturbed input, the hidden states are H(\u03b4i(xp i,t), f), with \u03b4i being the specific perturbation for xp i,t. Our observations in Figure 1 indicate that altering the hidden state of just one layer is not optimal to significantly impact ICL performance. Therefore, we aim to maximize the minimum difference across all layers between the original and perturbed hidden states. To normalize differences across layers with varying scales, we use the normalized L2 norm to measure the distance of the hidden state between the original example and the perturbed one for each layer: ld(hl(xp i,t, f), hl(\u03b4i(xp i,t), f)) = \u2225 hl(xp i,t,f) \u2225hl(xp i,t,f)\u22252 \u2212 hl(\u03b4i(xp i,t),f) \u2225hl(\u03b4i(xp i,t),f)\u22252 \u22252. The distortion between xp i,t and \u03b4i(xp i,t) is further defined as: Ld(H(xp i,t, f), H(\u03b4i(xp i,t), f))\n(2)\n# The attacking objective of ICLPoison then becomes\nmax \u03b4i\u2208\u2206Ld(H(xp i,t, f), H(\u03b4i(xp i,t), f)).\n(3)\nIn other words, Ld(H(xp i,t, f), H(\u03b4i(xp i,t), f)) denotes the minimum changes (or lower bound of the distortion) caused by the perturbation \u03b4i across all the layers of the model. This approach ensures that the perturbation \u03b4i introduces the most substantial change to the hidden states in the LLM during the ICL process, potentially enhancing the effectiveness of the poisoning attack.\n# 3.3. Attacking Algorithms\nThe perturbation \u03b4 is a crucial component of the ICLPoison framework, designed to be imperceptible to humans while effective in manipulating the performance. These are also common requirements for NLP attacks (Ebrahimi et al., 2017; Jin et al., 2020; Xu et al., 2023; Li et al., 2018). On the other hand, the discrete nature of the objective in Eq.3 poses additional challenges in the optimization. To address these challenges as well as showcase the versatility of our framework, we introduce three representative perturbations: synonym replacement, character replacement, and adversarial suffix. These methods demonstrate the adaptability of ICLPoison across different levels of text manipulation: Synonym replacement evaluates the word-level vulnerability of ICL and subtly changes the semantics; character replacement involves minimal but precise alterations, such as changing individual letters, making it less noticeable to human reviewers (see examples in Appendix A.3); and adversarial suffix test token-level vulnerabilities in ICL, accommodating different models by adapting to their unique tokenization methods. In general, these perturbations allow for a comprehensive evaluation of the vulnerability in ICL. The optimization of these perturbations is efficiently managed through a greedy search method, proven effective in similar contexts (Lei et al., 2019; Bao et al., 2022). Synonym Replacement. This approach involves substituting words in a text with their synonyms, aiming at preserving the semantic meaning and grammatical structure (Jin et al., 2020; Xu et al., 2023). Therefore, it serves as an appropriate example of \u03b4 in Section 3.2. Within our framework, we limit the number of word replacements (denoted as k) to maintain the perturbation\u2019s imperceptibility. For a text composed of a sequence of n words x = [w1, ..., wn], \u03b4(x) is defined as [s1, ..., sn], where si is either a synonym of wi (if selected for replacement) or remains as wi (if not replaced). Our objective is to identify the optimal replacements that maximize the objective in Eq.3. This brings natural challenges: which words are to be replaced and what synonyms are suitable. Following a strategy similar to (Jin et al., 2020), we adopt a two-step optimization process. Firstly, we calculate an importance score for each word, selecting the top-k words with the highest scores for replacement. The importance score for word wi is computed as the distortion (in\nEq.2) before and after deleting wi, expressed as:\n(4)\nwhere x\\wi denotes the text after the removal of wi. In the second step, we use a greedy search method. This method involves iteratively finding the best replacement for each selected word, one at a time, while keeping the other words fixed (Yang et al., 2020; Lei et al., 2019). In each iteration, we focus on one of the selected words, replacing it with its most appropriate synonym. We retrieve synonyms using GloVe word embeddings (Pennington et al., 2014), selecting those with embeddings most similar (based on cosine similarity) to the original word. After identifying potential synonyms, each synonym is temporarily substituted into the text, and the loss function in Eq.3 is evaluated. The synonym that results in the highest loss is then chosen as the final replacement. We then proceed to the next word, repeating this replacement process until traversing all selected words. We present the detailed algorithm in Algorithm. 1. Character Replacement. This method is similar with the synonym replacement approach, but focuses on replacing individual characters instead of whole words (Ebrahimi et al., 2017; Lei et al., 2019; Xu et al., 2023). When changing only a few letters, this method can be less detectable to humans and maintain the word\u2019s pronunciation and basic structures (Ebrahimi et al., 2017). We limit the number of character replacements to k to maintain the subtlety of the perturbations. The optimization process for character replacement also follows a two-step method. Firstly, we calculate the importance score for each character, similar to the approach described in Eq.4. The primary difference is that we consider the removal of individual characters, not words. The top-k characters with the highest importance scores are earmarked for replacement. Secondly, we employ the same greedy search strategy used in synonym replacement to identify the optimal replacements for the selected characters. Note that our character set encompasses uppercase and lowercase letters, digits, punctuation marks, and whitespace, in line with the sets in (Kim et al., 2016; Ebrahimi et al., 2017). The detailed algorithm and its implementation are shown in Algorithm 2 in Appendix A.1. Adversarial Suffix. The concept of an adversarial suffix, referred to as adding additional tokens at the end of the original text, has shown considerable effectiveness in misleading LLMs (Zou et al., 2023). Thus, in addition to synonym and character replacement, we also adapt this perturbation to evaluate the token-level vulnerability of the ICL process. To ensure imperceptible to humans, we restrict the number of additional tokens when adapting to our framework. For a given text x that can be tokenized into a sequence of tokens x = [t1, ..., tn], we define \u03b4(x) as [t1, ..., tn, t\u2032 1, ..., t\u2032 k] where t\u2032 1, ..., t\u2032 k are adversarial suffices. Our goal is to identify the optimal suffixes that maximize the objective in Eq\n3. We also employ a greedy search approach, involving iteratively selecting each suffix token from t\u2032 1 to t\u2032 k one by one which results in the maximum increase in the loss. The detailed implementation and optimization process for this approach is elaborated in Algorithm 3 in Appendix A.1.\n# 4. Experiment\nWe conduct extensive experiments to validate the effectiveness of the proposed framework ICLPoison, particularly with three perturbations introduced in Section 3.3.\nDatasets. We conduct experiments on different types of classification tasks. Stanford Sentiment Treebank (SST2) dataset from the GLUE benchmark (Wang et al., 2019) is a sentiment analysis dataset consisting of sentences from movie reviews and human annotations of their sentiment in 2 classes (positive/negative); Corpus of Linguistic Acceptability (Cola) dataset from GLUE is a linguistic analysis dataset consisting of English acceptability judgments collected from linguistic books, labeled with \u201cacceptable\u201d or \u201cunacceptable\u201d; Emo dataset (Wang et al., 2023) focuses on emotion classification consisting of Twitter messages labeled in 4 classes; AG\u2019s new (AG) corpus (Zhang et al., 2015) is a topic classification dataset gathered from news sources and labeled in 4 classes (World, Sports, Business, and Science/Technology); and Poem Sentiment (Poem) (Sheng & Uthus, 2020) is a sentiment analysis dataset of poem verses from Project Gutenberg, classified into 3 classes. Models. In this work, we consider both open-source models including Llama2-7B (Touvron et al., 2023), Pythia (2.8B, 6.9B) (Biderman et al., 2023), Falcon-7B (Almazrouei et al., 2023), GPT-J-6B (Wang & Komatsuzaki, 2021), MPT-7B (Team, 2023), and API-only models GPT-3.5 and GPT-4 (Brown et al., 2020). Baselines. Since we are the first to study poisoning attacks in ICL, we compare our methods with clean ICL and random label flip (Min et al., 2022). For the baseline random label flip, we replace the true label of the example with a random label uniformly selected from the label space. Metrics. Our main focus is on the ICL accuracy. For every dataset, we generate perturbations for examples in the training data and randomly select examples from it to conduct ICL prediction for every sample in the test data. We repeat for 5 runs and report the average ICL accuracy. We also include perplexity scores, which are defined as the average negative log-likelihood of each of the tokens appearing, showing whether the perturbations are imperceptible or not. Experimental settings2. For all three perturbations, we limit the number of words/characters/tokens (also known as budget) to 5 to ensure minimal perceptibility. During the poisoning process, we apply the same template as in Section 2Code can be found in https://anonymous.4open.\n2Code can be found in https://anonymous.4open. science/r/ICLPoison-70EE\n2.3, i.e. \u201c{input}\u2192{output}\\n{query}\u2192\u201d, and extract the hidden states as the representation of the last token \u201c\u2192\u201d. For evaluation, we adopt the same template and conduct ICL predictions on 5 examples in default. The impact of templates and example numbers is explored in Section 4.6. 4.2. Main results In this subsection, we apply our framework to various LLMs and datasets to evaluate the vulnerability of ICL. Attacking open-source models. Our study initially examines open-source models. For each, we craft poisoned samples utilizing the model\u2019s own architecture and subsequently assess the ICL accuracy. Note that we will discuss the transferability i.e. examining the performance of poisoned samples crafted using Llama2-7B when tested on different models in Section 4.3. We begin by altering the entire training set to create these samples. Further studies involving only a subset of poisoned data are presented in Section 4.4. Partial results are shown in Table 1 and full results can be found in Table 5 in Appendix A.2. In the table, a lower accuracy indicates a stronger poisoning effect and the lowest performance is highlighted. It is obvious that ICL performs well on clean data, especially for the SST2 dataset and Llama2-7B model, achieving more than 88% accuracy. While random labels do interrupt the ICL process, causing a decrease in performance, LLMs are still capable of learning effectively from the demonstrations with the dropping of ICL accuracy less than 7%, which is aligned with observations by (Min et al., 2022). In contrast, our ICLPoison framework significantly reduces ICL accuracy, achieving drops to below 10% for some models and datasets such as GPT-J-6B with the Emo dataset. Notably, ICLPoison reveals that ICL is vulnerable to different levels of data poisoning attacks, ranging from subtle character-level changes to word-level manipulations. Among all three variants of ICLPoison, synonym replacement and adversarial suffix perturbations exhibit more severe threats than character replacement in reducing ICL accuracy, likely because they introduce more changes within the same perturbation budget. We present some poisoned examples from three methods in Appendix A.3 for human evaluation. We observe that the poisoning effect differs across models and datasets. For instance, Llama2-7B shows heightened sensitivity to poisoning on the Emo dataset, whereas Falcon-7B is particularly vulnerable to attacks on the Poem Sentiment dataset. These variations may stem from differences in model architectures and the inherent complexity of the datasets. Attacking API-only models. For API-only models like GPT-3.5-turbo and GPT-4, we lack direct access to their internal model representations. Therefore, we employ Llama27B as a surrogate to generate poisoned samples and assess the ICL accuracy using the provided APIs. The outcomes, detailed in Table 1, reveal that our approach using Llama27B effectively reduces the ICL accuracy of these cutting-\n# Attacking API-only models. For API-only mod\nGPT-3.5-turbo and GPT-4, we lack direct access to their internal model representations. Therefore, we employ Llama27B as a surrogate to generate poisoned samples and assess the ICL accuracy using the provided APIs. The outcomes, detailed in Table 1, reveal that our approach using Llama27B effectively reduces the ICL accuracy of these cutting-\n<div style=\"text-align: center;\">results for attacking LLMs. Average ICL accuracy on clean data and poisoned data (5 independent runs) as well as standard rted (in percentage), where lower accuracy represents a stronger poisoning effect. The lowest accuracy for each row is blue.</div>\n<div style=\"text-align: center;\">Table 1: Main results for attacking LLMs. Average ICL accuracy on clean data and poisoned data (5 independent runs) as error are reported (in percentage), where lower accuracy represents a stronger poisoning effect. The lowest accuracy highlighted in blue.</div>\nin blue.\nModel\nDataset\nClean\nRandom label\nSynonym\nCharacter\nAdv suffix\nCola\n55.2\u00b11.8\n49.3\u00b12.1\n10.4\u00b12.1\n17.6\u00b11.1\n13.8\u00b11.3\nSST2\n82.8\u00b11.4\n79.4\u00b11.9\n19.4\u00b11.8\n23.8\u00b11.9\n22.7\u00b11.6\nEmo\n70.3\u00b12.3\n39.4\u00b12.2\n12.5\u00b11.1\n14.7\u00b11.4\n10.4\u00b11.5\nPoem\n56.2\u00b11.8\n43.1\u00b12.3\n12.3\u00b11.5\n17.9\u00b11.2\n13.8\u00b11.1\nPythia-6.9B\nAG\n66.5\u00b12.1\n47.9\u00b11.7\n13.8\u00b11.3\n17.3\u00b11.5\n12.9\u00b11.7\nCola\n63.8\u00b11.9\n55.5\u00b12.0\n15.3\u00b11.7\n22.7\u00b12.1\n13.6\u00b11.4\nSST2\n88.6\u00b11.5\n82.1\u00b13.2\n18.5\u00b12.0\n26.8\u00b11.7\n20.4\u00b11.7\nEmo\n73.1\u00b11.3\n43.6\u00b11.9\n11.9\u00b11.8\n17.5\u00b11.4\n12.7\u00b11.3\nPoem\n62.9\u00b11.8\n51.4\u00b12.3\n18.1\u00b11.9\n23.3\u00b11.6\n17.2\u00b11.1\nLlama2-7B\nAG\n73.2\u00b12.0\n57\u00b12.6\n13.6\u00b12.2\n19.4\u00b11.3\n11.9\u00b11.2\nCola\n65.2\u00b11.5\n44.8\u00b11.7\n12.7\u00b11.9\n16.5\u00b11.7\n10.8\u00b11.4\nSST2\n83.8\u00b12.5\n83.1\u00b12.5\n20.1\u00b11.6\n25.8\u00b11.3\n22.7\u00b11.7\nEmo\n61.1\u00b11.7\n52.6\u00b11.9\n10.8\u00b11.5\n14.1\u00b11.9\n9.9\u00b11.1\nPoem\n55.2\u00b11.4\n42.9\u00b11.5\n10.5\u00b11.9\n17.3\u00b11.5\n13.6\u00b11.3\nFalcon-7B\nAG\n75.2\u00b11.8\n50.8\u00b11.3\n11.2\u00b12.3\n14.9\u00b11.7\n12.8\u00b11.2\nCola\n57.8\u00b11.3\n49.1\u00b12.5\n13.7\u00b11.7\n17.2\u00b11.8\n11.8\u00b10.9\nSST2\n85.4\u00b11.6\n82.8\u00b12.1\n14.8\u00b12.0\n18.9\u00b11.5\n11.4\u00b11.1\nEmo\n58.7\u00b11.1\n46.2\u00b11.7\n11.7\u00b11.8\n13.8\u00b11.3\n9.6\u00b10.7\nPoem\n57.6\u00b11.5\n46.7\u00b11.4\n12.6\u00b12.4\n14.2\u00b12.2\n10.3\u00b11.3\nGPT-J-6B\nAG\n63.2\u00b11.7\n53.4\u00b11.9\n11.9\u00b11.5\n16.8\u00b11.8\n12.5\u00b11.1\nCola\n75.6\u00b10.7\n76.3\u00b10.6\n58.1\u00b10.5\n62.6\u00b10.4\n59.7\u00b10.4\nSST2\n93.8\u00b10.3\n89.7\u00b10.5\n76.8\u00b10.2\n78.3\u00b10.5\n74.2\u00b10.9\nEmo\n73.8\u00b10.5\n72.4\u00b10.8\n65.4\u00b10.4\n63.1\u00b10.7\n61.3\u00b10.5\nPoem\n51.4\u00b10.9\n53.3\u00b10.6\n39.7\u00b10.6\n45.2\u00b10.6\n43.9\u00b10.4\nGPT-3.5-turbo\nAG\n85.6\u00b10.3\n80.7\u00b10.4\n76.2\u00b10.5\n73.8\u00b10.2\n69.4\u00b10.7\nCola\n85.8\u00b10.2\n82.1\u00b10.3\n73.1\u00b10.5\n75.8\u00b10.3\n69.6\u00b10.4\nSST2\n95.1\u00b10.4\n92.5\u00b10.5\n81.5\u00b10.2\n86.1\u00b10.2\n82.3\u00b10.5\nEmo\n84.9\u00b10.1\n81.7\u00b10.2\n80.9\u00b10.6\n78.1\u00b10.5\n78.3\u00b10.4\nPoem\n72.4\u00b10.2\n63.8\u00b10.7\n56.7\u00b10.9\n60.9\u00b10.7\n57.1\u00b10.3\nGPT-4\nAG\n90.4\u00b10.3\n87.3\u00b10.3\n83.2\u00b10.5\n83.1\u00b10.4\n84.7\u00b10.5\n<div style=\"text-align: center;\">Clean Random label Synonym Character Adv suffix</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7422/74229bb1-b41a-42a8-9b07-e6504d1060df.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Experimental results of transferring poisoned data from Llama2-7B to other models. The Y-axis represents the ICL accura smaller value represents a stronger poisoning effect), while the X-axis denotes different models.</div>\nedge models by about 10% for both GPT-3.5 and GPT-4. This not only validates the effectiveness of our method but also confirms its utility in real-world applications with advanced LLMs. Additionally, we observe that compromising such models poses greater challenges than open-source models, potentially due to their larger scale and the use of surrogate models (because of the black-box nature). Furthermore, these models display varying degrees of vulnerability to different perturbation intensities. Notably, GPT-4 exhibits particular susceptibility to character replacement, suggesting a heightened sensitivity to minor textual variations.\n4.3. Transferbility In this subsection, we consider a practical scenario when the attacker has no access to the victim LLM, and leverages a surrogate model to generate poisoned samples. Specifically, we adopt the Llama2-7B model in this study. This setup allows us to evaluate the transferability of our poisoning approach across different models, including black-box models such as GPT-3.5 and GPT-4. The initial results for the transferability study are presented in Figure 2, in which we focus on the SST2 dataset. For a more comprehensive analysis covering all five datasets used in our study, we direct\nreaders to Appendix A.2.\nFrom Figure 2, we observe a notable trend: while the efficacy of the poisoning attack diminishes when moving from the surrogate (Llama2-7B) to the victim models, the impact remains significant. The poisoned examples generated by our ICLPoison framework \u2013 in all three of its variants \u2013 substantially jeopardize the performance of ICL, leading to over a 30% decrease in accuracy for open-source models. Furthermore, our analysis sheds light on the different transferability of various perturbations. Synonym replacement and adversarial suffixes, in particular, demonstrate a stronger poisoning effect compared to character replacement. This disparity could be attributed to the more pronounced influence these methods exert on the surrogate model. We also note the influence of model size on susceptibility to poisoning. Models smaller in size than the surrogate, such as Pythia-2.8B and GPT-J-6B, appear to be more vulnerable to these poisoning examples, while larger models exhibit a degree of resistance. This pattern suggests that the effectiveness of our approach may be modulated by the size and complexity of the target model, offering valuable insights for future research in trustworthy language models.\n# 4.4. Partial poisoning\nTo consider practical scenarios where attackers can only alter a part of the data, our experiments involve perturbing a random subset of the training dataset. We allocate varying proportions (10%, 20%, 50%, and 100%) of the training data as accessible for manipulation in the Llama2-7B model and GLUE-SST2 dataset. The results, shown in Figure 3, reveal that a lower mixing rate reduces the overall poisoning effect. Nonetheless, even at a 10% rate, we observe a significant decrease in ICL performance by over 7%, and a 15% decrease at 20%, underscoring the efficacy of our ICLPoison framework. These findings highlight the vul-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f6d/6f6df2b0-9ab8-454c-97f8-a346473890c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Results for partially poisoned ICL. The X-axis represents the ratio of poisoned data while Y-axis represents the ICL accuracy.</div>\nnerability of ICL to subtle data poisoning attacks, where even a limited number of malicious inputs can significantly disrupt the ICL process. Additionally, the results show that perturbation strategies like synonym replacement and adversarial suffix, which introduce more pronounced textual changes within a fixed poisoning budget, more severely af-\n<div style=\"text-align: center;\">Table 2: Average perplexity scores for clean and poisoned data with standard error reported. Focus on model Llama2-7B.</div>\nwith standard error reported. Focus on model Llama2-7B.\nDataset\nClean\nSynonym\nCharacter\nAdv suffix\nCola\n4.66\u00b11.06\n5.22\u00b11.00\n7.37\u00b10.89\n9.58\u00b11.08\nSST2\n5.48\u00b11.16\n6.66\u00b10.73\n7.45\u00b10.70\n7.75\u00b11.13\nEmo\n5.02\u00b11.02\n5.79\u00b10.67\n7.27\u00b10.79\n7.31\u00b10.75\nPoem\n5.39\u00b10.85\n6.32\u00b10.62\n8.64\u00b10.49\n9.27\u00b11.09\nAG\n2.37\u00b10.41\n3.12\u00b10.29\n3.84\u00b10.50\n3.68\u00b10.35\n<div style=\"text-align: center;\">Table 3: The poisoned data is paraphrased by GPT-4, and ICL accuracy is reported. Original results are included in brackets.</div>\naccuracy is reported. Original results are included in brackets.\nDatasets\nClean\nRandom label\nSynonym\nCharacter\nAdv suffix\nCola\n65.2(63.8)\n53.9(55.5)\n36.5(15.3)\n50.6(22.7)\n58.5(13.6)\nSST2\n83.1(88.6)\n85.4(82.1)\n52.1(18.5)\n60.2(26.8)\n80.2(20.4)\nEmo\n75.5(73.1)\n48.2(43.6)\n40.7(14.9)\n48.3(17.5)\n66.8(12.7)\nPoem\n63.7(62.9)\n52.1(51.4)\n34.3(18.1)\n43.7(23.3)\n55.2(17.2)\nAG\n70.6(73.2)\n55.7(57)\n38.2(13.6)\n47.2(19.4)\n64.3(11.9)\nfect ICL performance compared to character replacement. This indicates that broader, coarse-grained perturbations are generally more disruptive than finer, more subtle ones.\n# 4.5. Potential defenses\nTo evaluate the robustness of our framework, we applied two representative defenses from (Jain et al., 2023): detectionbased defense perplexity filter and preprocessing defense paraphrasing. Perplexity filter. The perplexity score of a text is referred to as the average negative log-likelihood of each of the tokens appearing. The perturbation added to the original text may cause grammar mistakes, logic problems, and a reduction in fluency. This will increase the perplexity, and thus the corresponding text could be more detectable (Jain et al., 2023). We report the perplexity scores of generated poisoned data, and a higher value indicates a higher probability of being detected and lower robustness. Table 2 shows the perplexity scores for poisoned data across various models and datasets, calculated as described in Section 4.1 of (Jain et al., 2023). Among the perturbations, synonym replacement yields relatively lower perplexity most close to the clean perplexity, whereas adversarial suffixes generally result in the highest scores. This suggests that synonym replacement is less detectable than the adversarial suffix. Paraphrasing. Paraphrasing is a typical preprocessing method that utilizes a language model to rewrite the input text. This process aims to maintain original meanings while removing adversarial perturbations, making it an effective defensive strategy. In this part, we leverage the cutting-edge GPT-4 model to paraphrase the poisoned data and evaluate ICL performance on paraphrased inputs. As shown in Table 3, paraphrasing successfully neutralizes adversarial suffixes while largely preserving the impact of synonym replacements. This outcome is anticipated since adversarial suffixes typically introduce irrelevant content, whereas synonym replacements maintain the text\u2019s semantic integrity. The above findings make it evident that the implemented defenses can enhance the robustness of ICL against\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e2ad/e2adf238-e3d6-4e78-89f3-ddc31f21e083.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Clean Random label Synonym Character Adv suffix Figure 4: Experiments with different numbers of examples</div>\ndata poisoning to some extent. However, the degree of improvement is contingent upon the type of perturbation. For instance, token-level perturbations, such as adversarial suffixes, are more readily addressed by these defenses, whereas word-level perturbations like synonym replacements pose a greater threat due to their subtlety and ability to alter semantic meaning. Character-level perturbations fall between these two in terms of detectability and potential impact, with their severity likely to increase with a higher attack budget. Our results highlight the necessity to develop a robust ICL.\n# 4.6. Ablation studies\nUnder the assumption that attackers lack knowledge of the ICL process details, such as the templates and the number of examples used, we perform ablation studies to understand the impact of these settings. Specifically, we evaluate three different templates: F1, which is used in generating poisoned data; F2, formatted as \u2019input-output\u2019; and F3, structured as \u2019Q:input, A:output\u2019. Additionally, we explore the effects of using 3, 5, and 7 examples in ICL predictions. This ablation study concentrates on the Llama2-7B model and the GLUE-SST2 dataset, with comprehensive results provided in Appendix A.2. The results for different templates and varying numbers of examples are detailed in Table 4 and Figure 4, respectively. The results demonstrate that our framework is effectively adaptable to various templates and different numbers of demonstrations, highlighting its potentials in practice. Table 4: Experiments with different templates: F1, F2, F3 on model Llama2-7B and dataset GLUE-SST2.\nma2-7B and dataset GLUE-SST2.\nF1\nF2\nF3\nClean\n88.6\n92.5\n90.3\nRandom label\n82.1\n84.7\n79.2\nSynonym\n18.5\n17.9\n18.2\nCharacter\n26.8\n30.6\n28.5\nAdv suffix\n20.4\n21.7\n19.3\n# 5. Related Works 5.1. In-context Learning\nIn-context learning (ICL), since its introduction by (Brown et al., 2020), has been notable for efficiently tackling tasks with just a few examples, bypassing the need to adjust the model\u2019s parameters (Dong et al., 2022; Liu et al., 2021; Sun et al., 2022). Generally speaking, ICL allows LLMs to make\npredictions based on a few examples formatted in a template rather than changing model parameters. Existing works have shown that the performance of ICL relies on the quality of demonstrations, including the selection of examples (Liu et al., 2021; Sorensen et al., 2022; Wang et al., 2023), the order of examples (Lu et al., 2021; Min et al., 2022), the template used to format demonstrations (Liu et al., 2023a; Wei et al., 2022). The mechanisms behind ICL\u2019s success have been the subject of much interest. One line of work reveals that ICL learns latent concepts and conducts implicit Bayesian inference (Xie et al., 2021). Follow-up works (Hendel et al., 2023; Liu et al., 2023b) take a further step and point out that hidden states in LLMs encode these concepts. Another line of work focuses on the implicit learning mechanism of ICL. Garg et al. (2022) showed that Transformers (Vaswani et al., 2017) can encode effective learning algorithms to learn unseen linear functions according to demonstration samples. Von Oswald et al. (2023); Dai et al. (2022); Aky\u00a8urek et al. (2022) connect Transformers with gradient descent and claim that LLMs can implicitly perform gradient descent on examples. Bai et al. (2023) further extend the ability of implicit gradient descent to various statistical abilities including pre-ICL testing and post-ICL validation. Our work builds on the foundation of the first line, and leverages the understanding of latent concepts and hidden states to conduct effective data poisoning attacks.\n# 5.2. Data Poisoning\nData poisoning attacks (Biggio et al., 2012; Steinhardt et al., 2017) traditionally occur during the data collection phase of machine learning model training, where the training data is tampered with to induce malicious behaviors in the resulting models. These behaviors can range from degraded performance on testing data (Steinhardt et al., 2017; Huang et al., 2021) to the misclassification of specific test samples (Shafahi et al., 2018; Zhu et al., 2019) and the implantation of backdoors. In conventional models, such attacks typically exploit the training objectives (Steinhardt et al., 2017; He et al., 2023). However, applying data poisoning to in-context learning (ICL) aims to undermine the ICL\u2019s overall effectiveness, presenting unique challenges due to its implicit learning mechanism.\n# 6. Conclusion\nIn this study, we introduce ICLPoison, a novel framework devised to assess the vulnerability of in-context learning (ICL) in the face of data poisoning attacks. We use the dynamics of hidden states in Large Language Models (LLMs) to craft our attack objectives. Furthermore, we implement our framework through three distinct and practical algorithms, each employing a different method of discrete perturbation. Our research exposes previously unidentified susceptibilities of the ICL process to data poisoning. This discovery emphasizes the urgent need for enhancing the robustness of ICL implementations.\n# Broader Impact\nThis paper introduces a new framework to assess how incontext learning in large language models (LLMs) is susceptible to data poisoning attacks. It highlights the vulnerabilities in in-context learning, urging the development of more robust implementations. Additionally, the framework could aid in creating stronger attacks. Our findings underscore the pressing need to enhance LLMs\u2019 security to protect against such vulnerabilities.\n# References\nAky\u00a8urek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, \u00b4E., Hesslow, D., Launay, J., Malartic, Q., et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023. Bao, H., Han, Y., Zhou, Y., Shen, Y., and Zhang, X. Towards understanding the robustness against evasion attack on categorical inputs. In ICLR 2022-10th International Conference on Learning Representations, 2022. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397\u20132430. PMLR, 2023. Biggio, B., Nelson, B., and Laskov, P. Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389, 2012. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020. Dai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., and Wei, F. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nEbrahimi, J., Rao, A., Lowd, D., and Dou, D. Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751, 2017. Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Geiping, J., Fowl, L., Huang, W. R., Czaja, W., Taylor, G., Moeller, M., and Goldstein, T. Witches\u2019 brew: Industrial scale data poisoning via gradient matching. arXiv preprint arXiv:2009.02276, 2020. He, P., Xu, H., Ren, J., Cui, Y., Liu, H., Aggarwal, C. C., and Tang, J. Sharpness-aware data poisoning attack. arXiv preprint arXiv:2305.14851, 2023. Hendel, R., Geva, M., and Globerson, A. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023. Hoi, S. C., Sahoo, D., Lu, J., and Zhao, P. Online learning: A comprehensive survey. Neurocomputing, 459:249\u2013289, 2021. Huang, H., Ma, X., Erfani, S. M., Bailey, J., and Wang, Y. Unlearnable examples: Making personal data unexploitable. arXiv preprint arXiv:2101.04898, 2021. Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., Chiang, P.-y., Goldblum, M., Saha, A., Geiping, J., and Goldstein, T. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023. Jin, D., Jin, Z., Zhou, J. T., and Szolovits, P. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 8018\u20138025, 2020. Kim, Y., Jernite, Y., Sontag, D., and Rush, A. Characteraware neural language models. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. Lei, Q., Wu, L., Chen, P.-Y., Dimakis, A., Dhillon, I. S., and Witbrock, M. J. Discrete adversarial attacks and submodular optimization with applications to text classification. Proceedings of Machine Learning and Systems, 1:146\u2013165, 2019. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00a8uttler, H., Lewis, M., Yih, W.-t., Rockt\u00a8aschel, T., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00a8uttler, H., Lewis, M., Yih, W.-t., Rockt\u00a8aschel, T., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nLi, J., Ji, S., Du, T., Li, B., and Wang, T. Textbugger: Generating adversarial text against real-world applications. arXiv preprint arXiv:1812.05271, 2018. Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023a. Liu, S., Xing, L., and Zou, J. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668, 2023b. Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532\u20131543, 2014. Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and Goldstein, T. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31, 2018. Sheng, E. and Uthus, D. Investigating societal biases in a poetry composition system, 2020. Sorensen, T., Robinson, J., Rytting, C. M., Shaw, A. G., Rogers, K. J., Delorey, A. P., Khalil, M., Fulda, N., and Wingate, D. An information-theoretic approach to prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364, 2022. Steinhardt, J., Koh, P. W. W., and Liang, P. S. Certified defenses for data poisoning attacks. Advances in neural information processing systems, 30, 2017. Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Blackbox tuning for language-model-as-a-service. In International Conference on Machine Learning, pp. 20841\u2013 20855. PMLR, 2022.\nShafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and Goldstein, T. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31, 2018.\nSheng, E. and Uthus, D. Investigating societal biases in a poetry composition system, 2020.\nSorensen, T., Robinson, J., Rytting, C. M., Shaw, A. G., Rogers, K. J., Delorey, A. P., Khalil, M., Fulda, N., and Wingate, D. An information-theoretic approach to prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364, 2022.\nSteinhardt, J., Koh, P. W. W., and Liang, P. S. Certified defenses for data poisoning attacks. Advances in neural information processing systems, 30, 2017.\nSun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Blackbox tuning for language-model-as-a-service. In International Conference on Machine Learning, pp. 20841\u2013 20855. PMLR, 2022.\nTeam, M. N. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www. mosaicml.com/blog/mpt-7b. Accessed: 202305-05. Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151\u201335174. PMLR, 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wang, X., Zhu, W., and Wang, W. Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837, 2022. Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Xu, H., He, P., Ren, J., Wan, Y., Liu, Z., Liu, H., and Tang, J. Probabilistic categorical adversarial attack and adversarial training. In International Conference on Machine Learning, pp. 38428\u201338442. PMLR, 2023.\nYang, P., Chen, J., Hsieh, C.-J., Wang, J.-L., and Jordan, M. I. Greedy attack and gumbel attack: Generating adversarial examples for discrete data. The Journal of Machine Learning Research, 21(1):1613\u20131648, 2020. Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. Zhang, Y. and Yang, Q. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 34(12):5586\u20135609, 2021. Zhu, C., Huang, W. R., Li, H., Taylor, G., Studer, C., and Goldstein, T. Transferable clean-label poisoning attacks on deep neural nets. In International Conference on Machine Learning, pp. 7614\u20137623. PMLR, 2019. Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2020. Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.\n# A. Appendix\nA.1. Details of Algorithms\nIn this section, we present detailed algorithms, including synonym replacement in Algorithm 1, character replaceme Algorithm 2 and adversarial suffix in Algorithm 3.\nAlgorithm 1. Algorithm 1 describes the whole process of conducting ICLPoison with synonym replacement. For each example xp i,t in the accessible prompting set Dp t , it first selects words to be replaced based on an importance score (step (1)-(3)): the importance score for every word in xp i,t is computed via Eq.4 which is the distortion between the text before and after removing the word (step (2)); then the score for every word is sorted in descending order and k words with largest scores are chosen (step (3)). Secondly (step (4)-(8)), we greedily search for the optimal replacement for selected words within their synonyms: first extract m synonyms based on cosine similarity of GloVe embeddings (step (4)); then each selected word is replaced with its synonyms and evaluates the distortion with the original text via Eq. 2 (step (5)-(6)); the synonym causing the largest distortion is chosen as the final replacement. Algorithm 2. Algorithm 2 describes the whole process of conducting ICLPoison with character replacement. For each example xp i,t in the accessible prompting set Dp t , it first selects characters to be replaced based on an importance score (step (1)-(3)): the importance score for every character in xp i,t is computed via Eq.4 which is the distortion between the text before and after removing the character (step (2)); then the score for every character is sorted in descending order and k words with largest scores are chosen (step (3)). Secondly (step (4)-(7)), we greedily search for the optimal replacement for selected characters within the character set C: each selected word is replaced with characters inside C and evaluates the distortion with the original text via Eq. 2 (step (4)-(5)); the character causing the largest distortion is chosen as the final replacement. Algorithm 3. Algorithm 3 describes the whole process of conducting ICLPoison with adversarial suffix. For each example xp i,t in the accessible prompting set Dp t , it first randomly initializes the k suffices (step (2)). We greedily search for the optimal token for each suffix within the vocabulary set V : each suffix is replaced with tokens inside V and evaluates the distortion with the original text via Eq. 2 (step (3)-(4)); the character causing the largest distortion is chosen as the final replacement.\nAlgorithm 1 ICLPoison + Synonym replacement\nInput Clean prompting set Dp\nt = {(xp\ni,t, yp\ni,t)}N\ni=1, surrogate model f consisting of L layers, attacking budget k, number of\nsynonyms m.\nOutput Poisoned prompting set {(\u03b4i(xp\ni,t), yp\ni,t)}N\ni=1\nfor i = 1, ..., N do\nStep 1: Select words to replace with importance scores\n(1) Decompose input text xp\ni,t into a sequence of words [w1, ..., wn]\n(2) Compute importance score Iwj for each word wj with Eq. 4\n(3) Sort scores in descending order Iw(1) \u2265Iw(2) \u2265\u00b7 \u2265Iw(n) and select top-k words: w(1), ..., w(k)\nStep 2: Select optimal synonyms for each selected word\nfor w \u2208[w(1), ..., w(k)] do\n(4) Obtain top-m synonyms [s(1), ..., s(m)] with highest cosine similarity with w based on GloVe word embeddins.\nfor s \u2208[s(1), ..., s(m)] do\n(5) Replace w with s obtaining x\u2032\nw = [w1, ..., s, ..., wn]\n(6) Evaluate the distortion of hidden states after replacement with Eq.2: Ld(H(xp\ni,t, f), H(x\u2032\nw, f))\nend for\n(7) Select the synonym causing the largest distortion to replace w.\nend for\n(8) Obtain perturbed input \u03b4i(xp\ni,t)\nend for\nReturn poisoned prompting set {(\u03b4i(xp\ni,t), yp\ni,t)}N\ni=1\nAlgorithm 2 ICLPoison + Character replacement\nInput Clean prompting set Dp\nt = {(xp\ni,t, yp\ni,t)}N\ni=1, surrogate model f consisting of L layers, attacking budget k, charac\nset C.\nOutput Poisoned prompting set {(\u03b4i(xp\ni,t), yp\ni,t)}N\ni=1\nfor i = 1, ..., N do\nStep 1: Select characters to replace with importance scores\n(1) Decompose input text xp\ni,t into a sequence of characters [c1, ..., cn]\n(2) Compute importance score Icj for each word cj with Eq. 4\n(3) Sort scores in descending order Ic(1) \u2265Ic(2) \u2265\u00b7 \u2265Iw(n) and select top-k words: c(1), ..., c(k)\nStep 2: Select optimal character for each selected character from the whole character set.\nfor c \u2208[c(1), ..., c(k)] do\nfor c\u2032 \u2208C do\n(4) Replace c with c\u2032 obtaining x\u2032\nc = [c1, ..., c\u2032, ..., cn]\n(5) Evaluate the distortion of hidden states after replacement with Eq.2: Ld(H(xp\ni,t, f), H(x\u2032\nc, f))\nend for\n(6) Select the character causing the largest distortion to replace c.\nend for\n(7) Obtain perturbed input \u03b4i(xp\ni,t)\nend for\nReturn poisoned prompting set {(\u03b4i(xp\ni,t), yp\ni,t)}N\ni=1\nAlgorithm 3 ICLPoison + Adversarial suffix\nInput Clean prompting set Dp\nt = {(xp\ni,t, yp\ni,t)}N\ni=1, surrogate model f consisting of L layers, attacking budget k, token\nvocabulary V .\nOutput Poisoned prompting set {(\u03b4i(xp\ni,t), yp\ni,t)}N\ni=1\nfor i = 1, ..., N do\n(1) Tokenize text xp\ni,t into sequence of tokens [t1, ..., tn]\n(2) Random initialize the adversarial suffix and concatenate with the original text: \u03b4(xp\ni,t) = [t1, ..., tn, t\u2032\n1, ..., t\u2032\nk]\nfor j \u2208[k] do\nfor v \u2208V do\n(3) Replace t\u2032\nj with v obtaining x\u2032\nt = [t1, ..., tn, t\u2032\n1, ..., v, ..., t\u2032\nk]\n(4) Evaluate the distortion of hidden states after replacement with Eq.2: Ld(H(xp\ni,t, f), H(x\u2032\nt, f))\nend for\n(5) Select the token causing the largest distortion to replace t\u2032\nj.\nend for\n(6) Obtain perturbed input \u03b4i(xp\ni,t)\nend for\nReturn poisoned prompting set {(\u03b4i(xp\ni,t), yp\ni,t)}N\ni=1\n# A.2. Additional Experiments\nIn this section, we present additional experimental results, including full results on attacking open-source models in Table 5, full results of transferability in Table 7, full results of perplexity scores in Table 8, full results on various templates in Table 6 and the number of examples in Table 9. Attack open-source models. In Table 5, we include more results on additional models such as Pythia-2.8B and MPT-7B. Our observation is consistent with the analysis in Section 4.2. Transferbility. In Table 7, results on all 5 datasets are presented, and we notice that the transferability of three perturbations varies. This may be because of the capacity of models and the complexity of datasets. A detailed investigation can be an interesting future direction. Perplexity scores. Table 8 covers perplexity scores on various datasets and models. It is obvious that synonym replacement\nis more stealthy than the other 2 methods.\nImpact of templates. We test 3 different templates on various datasets and models. Our results in Table 6 reveal that our poisoned examples remain effective across templates. Impact of the number of examples. Our results about different numbers of examples in Table 9 show that more examples can improve ICL performance, while also leading to easier manipulation and stronger poisoning effect.\nImpact of templates. We test 3 different templates on various datasets and models. Our results in Table 6 reveal that our poisoned examples remain effective across templates.\n<div style=\"text-align: center;\">Table 5: Full results on attacking open-source models</div>\nModel\nDataset\nClean\nRandom label\nSynonym\nCharacter\nAdv suffix\nCola\n64.1\u00b11.6\n59.4\u00b11.6\n12.3\u00b11.2\n17.6\u00b11.4\n14.2\u00b11.3\nSST2\n76.5\u00b11.5\n70.2\u00b11.7\n17.5\u00b11.1\n24.3\u00b11.2\n18.1\u00b12.1\nEmo\n67.2\u00b11.3\n48.1\u00b12.8\n10.9\u00b11.8\n15.7\u00b11.7\n12.3\u00b11.7\nPoem\n57.1\u00b11.7\n31.8\u00b11.9\n10.5\u00b11.6\n16.4\u00b11.6\n9.7\u00b11.2\nPythia-2.8B\nAG\n59.4\u00b11.1\n46.5\u00b11.7\n15.7\u00b11.0\n20.3\u00b11.3\n14.6\u00b11.4\nCola\n55.2\u00b11.8\n49.3\u00b12.1\n10.4\u00b12.1\n17.6\u00b11.1\n13.8\u00b11.3\nSST2\n82.8\u00b11.4\n79.4\u00b11.9\n19.4\u00b11.8\n23.8\u00b11.9\n22.7\u00b11.6\nEmo\n70.3\u00b12.3\n39.4\u00b12.2\n12.5\u00b11.1\n14.7\u00b11.4\n10.4\u00b11.5\nPoem\n56.2\u00b11.8\n43.1\u00b12.3\n12.3\u00b11.5\n17.9\u00b11.2\n13.8\u00b11.1\nPythia-6.9B\nAG\n66.5\u00b12.1\n47.9\u00b11.7\n13.8\u00b11.3\n17.3\u00b11.5\n12.9\u00b11.7\nCola\n63.8\u00b11.9\n55.5\u00b12.0\n15.3\u00b11.7\n22.7\u00b12.1\n13.6\u00b11.4\nSST2\n88.6\u00b11.5\n82.1\u00b13.2\n18.5\u00b12.0\n26.8\u00b11.7\n20.4\u00b11.7\nEmo\n73.1\u00b11.3\n43.6\u00b11.9\n11.9\u00b11.8\n17.5\u00b11.4\n12.7\u00b11.3\nPoem\n62.9\u00b11.8\n51.4\u00b12.3\n18.1\u00b11.9\n23.3\u00b11.6\n17.2\u00b11.1\nLlama2-7B\nAG\n73.2\u00b12.0\n57\u00b12.6\n13.6\u00b12.2\n19.4\u00b11.3\n11.9\u00b11.2\nCola\n65.2\u00b11.5\n44.8\u00b11.7\n12.7\u00b11.9\n16.5\u00b11.7\n10.8\u00b11.4\nSST2\n83.8\u00b12.5\n83.1\u00b12.5\n20.1\u00b11.6\n25.8\u00b11.3\n22.7\u00b11.7\nEmo\n61.1\u00b11.7\n52.6\u00b11.9\n10.8\u00b11.5\n14.1\u00b11.9\n9.9\u00b11.1\nPoem\n55.2\u00b11.4\n42.9\u00b11.5\n10.5\u00b11.9\n17.3\u00b11.5\n13.6\u00b11.3\nFalcon-7B\nAG\n75.2\u00b11.8\n50.8\u00b11.3\n11.2\u00b12.3\n14.9\u00b11.7\n12.8\u00b11.2\nCola\n57.8\u00b11.3\n49.1\u00b12.5\n13.7\u00b11.7\n17.2\u00b11.8\n11.8\u00b10.9\nSST2\n85.4\u00b11.6\n82.8\u00b12.1\n14.8\u00b12.0\n18.9\u00b11.5\n11.4\u00b11.1\nEmo\n58.7\u00b11.1\n46.2\u00b11.7\n11.7\u00b11.8\n13.8\u00b11.3\n9.6\u00b10.7\nPoem\n57.6\u00b11.5\n46.7\u00b11.4\n12.6\u00b12.4\n14.2\u00b12.2\n10.3\u00b11.3\nGPT-J-6B\nAG\n63.2\u00b11.7\n53.4\u00b11.9\n11.9\u00b11.5\n16.8\u00b11.8\n12.5\u00b11.1\nCola\n53.4\u00b11.2\n45.3\u00b11.2\n15.6\u00b11.6\n17.4\u00b11.9\n14.1\u00b11.4\nSST2\n89\u00b11.5\n82.9\u00b12.3\n20.4\u00b11.9\n25.6\u00b12.5\n19.8\u00b11.3\nEmo\n59.7\u00b11.3\n41.8\u00b11.7\n9.6\u00b11.5\n11.5\u00b11.6\n10.4\u00b10.8\nPoem\n69\u00b11.8\n56.2\u00b12.5\n14.9\u00b11.4\n16.3\u00b11.5\n12.7\u00b11.2\nMPT-7B\nAG\n70.6\u00b11.6\n55.3\u00b11.9\n13.9\u00b11.7\n17.1\u00b11.9\n15.2\u00b11.6\n# A.3. Poisoned Text Examples\nIn this section, we provide some poisoned examples in Table 10 for human evaluation. The additional tokens (for adversarial suffix) and substitutions (for synonym and character replacement) are highlighted in red. It is obvious that adversarial suffixes can introduce irrelevant or non-sense content to the original text, thus can be easily detected. On the contrast, synonym and character replacements introduce more subtle changes to the text.\nIn this section, we provide some poisoned examples in Table 10 for human evaluation. The additional tokens (for adversaria suffix) and substitutions (for synonym and character replacement) are highlighted in red. It is obvious that adversaria suffixes can introduce irrelevant or non-sense content to the original text, thus can be easily detected. On the contras synonym and character replacements introduce more subtle changes to the text.\n<div style=\"text-align: center;\">Table 6: Evaluating data poisoning attacks on different ICL templates. F1, F2, F3 denote 3 different templates, an accuracy on various dataset is reported.</div>\nDataset\nICL format\nClean\nRandom label\nSynonym\nCharacter\nAdv suffix\nCola\nF1\n63.8\n55.5\n15.3\n22.7\n13.6\nF2\n55.1\n47.5\n14.6\n20.9\n13.9\nF3\n59.5\n53.3\n13.8\n19.4\n12.5\nSST2\nF1\n88.6\n82.1\n18.5\n26.8\n20.4\nF2\n92.5\n84.7\n17.9\n30.6\n21.7\nF3\n90.3\n79.2\n18.2\n28.5\n19.3\nEmo\nF1\n73.1\n43.6\n14.9\n17.5\n12.7\nF2\n66.7\n37.6\n11.6\n12.9\n9.2\nF3\n69.1\n39.8\n12.8\n15.6\n11.3\nPoem\nF1\n62.9\n51.4\n18.1\n23.3\n17.2\nF2\n57.1\n45.2\n13.7\n20.1\n12.8\nF3\n61.9\n49.5\n16.3\n21.5\n13.7\nAG\nF1\n73.2\n57.6\n13.6\n19.4\n11.9\nF2\n68.6\n54.6\n11.7\n18.2\n10.3\nF3\n75.8\n67.2\n14.1\n19.3\n12.6\n<div style=\"text-align: center;\">Table 7: Full results for testing poisoned examples generated by Llama2-7B on other models.</div>\nDataset\nClean\nRandom label\nSynonym\nCharacter\nAdv suffix\nCola\n64.1\n59.4\n31.9\n34.0\n34.8\nSST2\n76.5\n70.2\n38.5\n33.9\n36.6\nEmo\n67.2\n48.1\n26.3\n32.8\n30.3\nPoem\n57.1\n31.8\n33.3\n27.1\n24.3\nPythia-2.8B\nAG\n59.4\n46.5\n31.7\n31.6\n35.6\nCola\n55.2\n49.3\n26.7\n34.7\n34.2\nSST2\n82.8\n79.4\n39.5\n41.0\n41.3\nEmo\n70.3\n39.4\n18.9\n24.1\n20.9\nPoem\n56.2\n43.1\n29.0\n28.1\n26.2\nPythia-6.9B\nAG\n66.5\n47.9\n32.1\n33.9\n27.2\nCola\n65.2\n44.8\n23.4\n24.8\n25.0\nSST2\n83.8\n83.1\n37.7\n35.8\n34.7\nEmo\n61.1\n52.6\n28.6\n30.5\n27.1\nPoem\n55.2\n42.9\n25.1\n27.3\n25.6\nFalcon-7B\nAG\n75.2\n50.8\n24.7\n24.8\n24.2\nCola\n57.8\n49.1\n29.7\n28.5\n29.1\nSST2\n85.4\n82.8\n31.7\n31.8\n30.2\nEmo\n58.7\n46.2\n22.3\n24.1\n19.3\nPoem\n57.6\n46.7\n26.7\n28.6\n23.3\nGPT-J-6B\nAG\n63.2\n53.4\n29.4\n29.5\n29.8\nCola\n53.4\n45.3\n22.6\n23.2\n19.3\nSST2\n89\n82.9\n29.8\n29.6\n28.8\nEmo\n59.7\n41.8\n21.1\n25.4\n25.1\nPoem\n69\n56.2\n26.2\n24.5\n23.5\nMPT-7B\nAG\n70.6\n55.3\n31.0\n34.2\n27.8\nCola\n75.6\n76.3\n58.1\n62.6\n59.7\nSST2\n93.8\n89.7\n76.8\n78.3\n74.2\nEmo\n73.8\n72.4\n65.4\n63.1\n61.3\nPoem\n51.4\n53.3\n39.7\n45.2\n43.9\nGPT-3.5-turbo\nAG\n85.6\n80.7\n76.2\n73.8\n69.4\nCola\n85.8\n82.1\n73.1\n75.8\n69.6\nSST2\n95.1\n92.5\n81.5\n86.1\n82.3\nEmo\n84.9\n81.7\n80.9\n78.1\n78.3\nPoem\n72.4\n63.8\n56.7\n60.9\n57.1\nGPT-4\nAG\n90.4\n87.3\n83.2\n83.1\n84.7\nDataset\nClean\nSynonym\nCharacter\nAdv suffix\nPythia-2.8B\nCola\n4.87\n5.15\n7.38\n8.35\nSST2\n5.54\n6.37\n7.45\n8.80\nEmo\n5.46\n6.07\n7.27\n6.81\nPoem\n5.50\n6.86\n7.65\n8.09\nAG\n3.26\n4.01\n4.84\n5.98\nPythia-6.9B\nCola\n4.84\n5.43\n7.78\n7.15\nSST2\n5.56\n5.58\n7.93\n7.06\nEmo\n5.41\n5.92\n7.49\n6.45\nPoem\n5.50\n5.71\n7.94\n7.86\nAG\n3.14\n4.10\n5.08\n3.91\nLlama2-7B\nCola\n4.66\n5.22\n7.37\n9.58\nSST2\n5.48\n6.66\n7.45\n7.75\nEmo\n5.02\n5.79\n7.27\n7.31\nPoem\n5.39\n6.32\n8.64\n9.27\nAG\n2.37\n3.12\n3.84\n3.68\nFalcon-7B\nCola\n5.02\n5.43\n7.57\n7.26\nSST2\n4.76\n5.33\n6.84\n7.12\nEmo\n5.03\n5.40\n7.26\n6.30\nPoem\n5.53\n6.02\n7.80\n8.07\nAG\n2.67\n3.49\n4.50\n3.64\nMPT-7B\nCola\n4.91\n5.80\n7.73\n7.18\nSST2\n5.29\n5.45\n6.62\n6.88\nEmo\n5.12\n5.47\n6.40\n6.22\nPoem\n5.43\n5.91\n7.94\n7.72\nAG\n2.75\n3.53\n4.50\n3.68\nGPT-J-6B\nCola\n5.01\n5.37\n7.41\n7.7\nSST2\n5.06\n5.35\n6.92\n7.81\nEmo\n5.37\n5.49\n6.40\n7.33\nPoem\n5.43\n5.72\n8.32\n8.02\nAG\n3.12\n4.08\n4.06\n5.04\nDataset\nnum examples\nClean\nRandom label\nSynonym\nCharacter\nAdv suffix\nCola\n3\n63.2\n59.2\n16.5\n21.8\n12.8\n5\n63.8\n55.5\n15.3\n22.7\n13.6\n7\n63.1\n54.6\n14.8\n22.5\n13.2\nSST2\n3\n84.5\n80.2\n14.3\n29.4\n19.6\n5\n88.6\n82.1\n18.5\n26.8\n20.4\n7\n92.7\n86.1\n19.3\n32.1\n21.7\nEmo\n3\n58\n35.2\n11.5\n14.6\n7.9\n5\n73.1\n43.6\n14.9\n17.5\n12.7\n7\n79.2\n47.4\n11.7\n15.8\n11.3\nPoem\n3\n61\n48.6\n16.9\n20.4\n15.6\n5\n62.",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) has emerged as a significant method in large language models (LLMs), allowing them to adapt to new tasks without extensive retraining. However, this paper explores the vulnerability of ICL to data poisoning attacks, an area that has not been thoroughly investigated. Previous methods have not adequately addressed the potential for adversaries to manipulate data examples, leading to a decline in model performance. The introduction of a new attacking framework, ICLPoison, aims to exploit the learning mechanisms of ICL and highlights the urgent need for improved defenses.",
        "problem": {
            "definition": "The problem addressed in this paper is the vulnerability of in-context learning in large language models to data poisoning attacks, where adversaries manipulate example data to degrade model performance.",
            "key obstacle": "A primary challenge is that existing data poisoning strategies are designed for traditional machine learning models with explicit training objectives, which do not directly apply to ICL, as ICL operates without explicit optimization."
        },
        "idea": {
            "intuition": "The intuition behind ICLPoison is based on the observation that the performance of ICL is closely tied to the hidden states of LLMs, which encode latent concepts from the demonstrations.",
            "opinion": "ICLPoison is a novel framework that strategically alters examples in demonstrations to distort the hidden states of LLMs during the ICL process, thereby facilitating effective data poisoning.",
            "innovation": "The key innovation of ICLPoison lies in its ability to apply discrete text perturbations to manipulate hidden states, a method not previously explored in the context of in-context learning."
        },
        "method": {
            "method name": "ICLPoison",
            "method abbreviation": "ICL-P",
            "method definition": "ICLPoison is defined as a data poisoning framework designed to exploit the unique learning mechanisms of in-context learning by distorting the hidden states of LLMs.",
            "method description": "ICLPoison employs strategic text perturbations to manipulate the hidden states of LLMs during the ICL process, effectively executing data poisoning attacks.",
            "method steps": [
                "Identify the input-output pairs from the dataset.",
                "Select examples to perturb based on their importance scores.",
                "Apply perturbations such as synonym replacement, character replacement, or adversarial suffix.",
                "Evaluate the distortion of hidden states caused by the perturbations.",
                "Generate the poisoned examples for further testing."
            ],
            "principle": "The effectiveness of ICLPoison is rooted in its ability to maximize the distortion of hidden states across all layers of the model, thereby significantly degrading the accuracy of ICL predictions."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using various datasets including SST2, Cola, Emo, AG, and Poem, with a focus on classification tasks. Baseline methods included clean ICL and random label flip.",
            "evaluation method": "The performance of ICLPoison was assessed by measuring the ICL accuracy on poisoned examples across different models and datasets, with results averaged over multiple runs."
        },
        "conclusion": "The experiments demonstrated that ICLPoison effectively compromised the performance of ICL in various models, revealing significant vulnerabilities to data poisoning. The findings underscore the necessity for enhanced defenses in ICL applications.",
        "discussion": {
            "advantage": "The major advantage of ICLPoison is its ability to introduce subtle yet effective perturbations that significantly degrade the performance of ICL, highlighting the vulnerabilities of LLMs to such attacks.",
            "limitation": "A limitation of the method is that it requires careful design to ensure perturbations are imperceptible to humans, which may not always be feasible in practice.",
            "future work": "Future research should focus on developing robust defenses against data poisoning in ICL and exploring the transferability of poisoning attacks across different models."
        },
        "other info": [
            {
                "info1": "The paper presents three types of perturbations: synonym replacement, character replacement, and adversarial suffix."
            },
            {
                "info2": {
                    "info2.1": "ICLPoison was tested on both open-source and API-only models."
                }
            },
            {
                "info3": "The framework's effectiveness was validated through extensive experiments demonstrating its impact on model performance."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) has emerged as a significant method in large language models (LLMs), allowing them to adapt to new tasks without extensive retraining."
        },
        {
            "section number": "1.2",
            "key information": "The vulnerability of ICL to data poisoning attacks highlights its impact and relevance within the broader field of NLP."
        },
        {
            "section number": "3",
            "key information": "ICLPoison is defined as a data poisoning framework designed to exploit the unique learning mechanisms of in-context learning by distorting the hidden states of LLMs."
        },
        {
            "section number": "3.1",
            "key information": "The performance of ICL is closely tied to the hidden states of LLMs, which encode latent concepts from the demonstrations."
        },
        {
            "section number": "4.1",
            "key information": "The major advantage of ICLPoison is its ability to introduce subtle yet effective perturbations that significantly degrade the performance of ICL."
        },
        {
            "section number": "6",
            "key information": "The findings underscore the necessity for enhanced defenses in ICL applications due to significant vulnerabilities to data poisoning."
        },
        {
            "section number": "6.1",
            "key information": "A primary challenge is that existing data poisoning strategies are designed for traditional machine learning models, which do not directly apply to ICL."
        }
    ],
    "similarity_score": 0.7002876986819305,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Data Poisoning for In-context Learning.json"
}