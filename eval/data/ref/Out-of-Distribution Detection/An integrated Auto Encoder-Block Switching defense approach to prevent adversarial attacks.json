{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2203.10930",
    "title": "An integrated Auto Encoder-Block Switching defense approach to prevent adversarial attacks",
    "abstract": "According to recent studies, the vulnerability of state-of-the-art Neural Networks to adversarial input samples has increased drastically. A neural network is an intermediate path or technique by which a computer learns to perform tasks using Machine learning algorithms. Machine Learning and Artificial Intelligence model has become a fundamental aspect of life, such as self-driving cars [1], smart home devices, so any vulnerability is a significant concern. The smallest input deviations can fool these extremely literal systems and deceive their users as well as administrator into precarious situations. This article proposes a defense algorithm that utilizes the combination of an auto-encoder [3] and block-switching architecture. Auto-coder is intended to remove any perturbations found in input images whereas the block switching method is used to make it more robust against White-box attacks. The attack is planned using FGSM [9] model, and the subsequent counter-attack by the proposed architecture will take place thereby demonstrating the feasibility and security delivered by the algorithm.",
    "bib_name": "yadav2022integratedautoencoderblockswitching",
    "md_text": "# An integrated Auto Encoder-Block Switching  defense approach to prevent adversarial attacks\nAshutosh Upadhyay, Anirudh Yadav, S.Sharanya\nAbstract \u2014 According to the recent studies, the vulnerability of  state of the art Neural Networks to adversarial input samples has increased drastically. Neural network is an intermediate  path or technique by which a computer learns to perform tasks  using Machine learning algorithms. Machine Learning and  Artificial Intelligence model has become fundamental aspect of  life, such as self-driving cars [1], smart home devices, so any  vulnerability is a significant concern. The smallest input  deviations can fool these extremely literal systems and deceive  their users as well as administrator into precarious situations.  This article proposes a defense algorithm which utilizes the  combination of an auto-encoder [3] and block-switching architecture.  Auto-coder  is  intended  to  remove  any perturbations found in input images whereas block switching method is used to make it more robust against White-box attack. Attack is planned using FGSM [9] model, and the subsequent counter-attack by the proposed architecture will take place thereby demonstrating the feasibility and security delivered by the algorithm. \n# I. INTRODUCTION\nI. INTRODUCTION\nMachine Learning (ML) has spread its wings in almost all  domains from medical to industrial equipment maintenance  [1]. Adversarial examples can be interpreted as optical  illusions to ML models in layman terms. These examples are carefully perturbed as inputs to the ML models which subsequently generate erroneous outputs. While such  perturbations may seem benign to human perception, it can  elicit wrong predictions from the model with full confidence.  For example, perpetrators could target self-driving vehicles by causing perturbations using paint or stickers that will  cause vehicle to decipher the sign incorrectly. Adversarial  examples depict that many modern ML algorithms can be  deluded in incredibly simple ways.   Such failures indicate that even simple ML models can  have their behaviour manipulated. Traditional mechanisms for building robust ML models generally do not provide a  pragmatic defence against adversarial examples. There are  two effective methods that have provided a somewhat effective defence: adversarial training and defensive distillation. This article explores a much more superior  defence algorithm against such adversarial examples, thereby eliminating the possibility of white-box attacks. \n# 1.1 Definitions \nAdversarial Training: It is a brute force solution which  improves  the  model\u2019s  robustness  by  incorporating  adversarial examples into the training stage followed by an  explicit training of the model against such deceptions. In  simple terms, it is the process of creating and incorporating  adversarial examples into the training segment.  Defensive distillation: The ML model is trained to output  probabilities instead of making hard decision of classifying  input into different classes. It is a defensive strategy.   White-Box attack: It is the type of attack in which attackers  have access to the Machine Learning algorithm or model \n# 1.2 Fast Gradient Sign Method\nFast Gradient Sign Method (FGSM) [9] is the powerful attack method. It is used to attack on neural network by the means  of  gradient.  FGSM  utilises  the  back propagated gradients of loss with respect to image input. In other words, instead of dynamically modifying weights based on back propagated gradients, FGSM manipulates input image to maximize the loss with respect to same back propagated gradient. \nWhere-   o  adv_x: Adversarial Image   o  x: Original Image   o  y: Original input label  o  \u03b5: Multiplier to ensure the perturbations are small. o  \u0398: Model parameters  o  J: Loss  \n# 1.3 Generative Adversarial Networks\nGenerative  Adversarial  Networks  (GAN)  [6]  uses unsupervised learning to figure out patterns and irregularities in the input data and also use them to generate new output that has the same patterns and irregularities as the input data. The architecture is shown in fig 1.  \n1.3.1 Generator: It takes a random set of input values of fixed length, and it tries to generate a sample in the domain same as of input data. Input is drawn randomly from a Gaussian distribution. Once the model is trained, it can be used in future for generating new samples as same as input data.  \n1.3.2 Discriminator: It is a classification model which classifies the input data into real and fake classes. Here, real stands for an original data set and fake stands for data generated by generator module. It is only used for training purpose as it assists the generator in generating output samples as similar to input samples. Once training is completed discriminator is discarded.   These two models work together as discriminator provides feedback on the fake generated inputs and generator then uses these to improve output. These two are run until the\ndiscriminator model is in such a state that it can no longer distinguish between fake and real data set. \n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e7c3/e7c3ddb5-a194-4ccb-b53d-32dd44eb747f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig 1. Architecture of Generative Adversarial Networks.</div>\n# AdvGAN\nAdvGAN (Adversarial GAN) is a modified version of GAN.  However, instead of generating samples the same as of input  data, it tries to generate samples that can fool state of the art  models. The working of Adversarial GAN is similar to GAN  but it consists of 3 models:   Classifier: It checks whether the output generated belong to  the desired class (different from its correct class). It ensures  that the generated sample gets incorrectly classified. Since  discriminator ensures that samples generated by the  generator are looking similar to the input data and classifier  ensures that it gets wrongly classifies, so a similar-looking  output to a human eye easily fools state of the art neural  networks.  Auto-Encoder: These are Neural Networks that used  Unsupervised Learning to perform data compression,  denoising, dimensionality reduction, etc. The layout of auto  encoders are given in Fig 2. It consists of 3 parts:   Encoder: This part of the auto-encoder compresses the input  data into a latent space. Latent space is a compressed  representation of the input data in a reduced dimension.   Code: This part of auto-encoder is also known as a  bottleneck. It represents compressed input in a reduced  dimension and passes it to the decoder.   Decoder: It takes latent space representation (compressed  input) as an input and tries to generate the output of the same  dimension as of original input. This reconstruction is done  depending on the purpose. It can be used to remove noise  from an image or can be used for data compression. \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/983f/983ff245-a838-4c3b-929f-187014703467.png\" style=\"width: 50%;\"></div>\nFig 2. Auto-Encoder architecture. The input image is encoded to a compressed Representation and then decoded. \n<div style=\"text-align: center;\">II. LITERATURE REVIEW</div>\nTitle \nMerit \nDemerit \nOne Pixel attack for \nfooling \nDeep \nNeural Networks \n2017 [2] \nRequires \nless \nadversarial \ninformation \nFor high definition \nimage \nrequired \nmultiple \npixel \nperturbations \n \nPuVAE: \nA \nVariational \nAuto-\nencoder to Purify \nAdversarial \nExamples \n2019 [3] \n \n \nTakes only \n0.114 \nsecond to \nprocess \n \n \nOnly filter \nLimited examples \nDefensive Dropout \nFor \nHardening \nDeep \nNeural \nNetworks \nunder \nAdversarial \nAttacks- 2019 \nDecreased \nthe \nwhite box attack \nrate from 100 \npercent \nto \n13 \npercent \nUnable to defend \nall \ntypes \nof \nadversarial attacks \n<div style=\"text-align: center;\">2.1 COMPARITIVE STUDY</div>\n2.1C \nExisting \nMethods \nAdvantage \nDisadvantage \nAnomaly \nDetection \nModel \nDetects anomaly \nand \navoid \ninput \nreaching to the actual \nmodel. \nRequires additional \nTime. It can also be \ntermed as bottleneck \nfor fast models \nPruning \nMethod \nPrevents Noise Based \nPerturbations. \nNot efficient for \nBlack box attacks \nDistributed \nColour \nChannels \nBlue Channel \nPerforms \nexceptionally well to \ndetermine \nany \nadversarial attacks. \nMultiple model \nOccupy large time and \nspace while running. \nNot good for video \ninput. \nAdversarial \nDataset \nMakes model robust \nagainst those \nAdversarial \ninput \nwhich is trained. \nDifficult to train for \nall adversarial \nattack types as it \nLargely \naffects \nthe \naccuracy. \nRandom \nWeight \nswitching \nGood for White box \nattacks \nNot efficient for \nBlack box attacks \n \nThe proposed system majorly focuses on static image input  and defence architecture.  Following are the characteristics of  the proposed model:   o  Combination of two models to effectively defend  both Black box and White Box attack.   o  Randomization [8] acts as a backup for filtration  performed by auto-encoder there by increasing the  robustness of the proposed model.   o  Grad-CAM [7] allows the model to predict the  highlighted  important  region  based  on  classification. \n# 3.1 Defence Architecture\nThe defence architecture comprises of 3 major components-  Auto-encoder [3], Block switching [8] and Grad-CAM [7].  The purpose of auto-encoder is to perform filtration. Autoencoder is integrated with the Block switching model along  with the Grad-CAM. Block switching consists of multiple  processes which are selected randomly based on the  classification of images. Grad-CAM are the activation maps  which uses the gradients to produce highlights on important  region in the image. \nFig 3. Defense architecture using the combination of autoencoder and block-switching method. \nIV. IMPLEMENTATION\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bbb2/bbb27347-9ef4-4ea8-b106-3f9ee88b7a7f.png\" style=\"width: 50%;\"></div>\n# 4.1 Auto Encoder\n<div style=\"text-align: center;\">4.1 Auto Encoder</div>\nAuto-encoders can be used for filtration purpose by taking adversarial data in the form of inputs and clean data as  outputs. It is possible for them to remove adversarial noise from an input image. Thus in the proposed architecture, autoencoders can also be termed as denoisers. \n# 4.2 Block Switching and Random Weight Switching \nBlock-switching [8] model is trained on two different levels.  The first level of training comprises of individual training of  sub-models with same architecture. Due to the same \narchitecture of sub-models, every sub-model possess similar characteristics.  Block-switching  consists  of  multiple channels. Every sub-model is suitably divided in its lower body and upper body. Lower body contains of all the convolutional layer. Lower parts are again merged to form a single output by including parallel channels of blockswitching while the other parts are discarded. Every submodel has different model parameters due to random initialization and stochasticity in the training process, therefore they tend to have similar characteristics in terms of classification, accuracy and robustness. \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bd96/bd96a87c-56ca-4b6b-b81f-6b5f2c3669f4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig 4. Levels of Block-switching model training, and preview of multiple channels and sub-models. </div>\n# 4.3 Grad-CAM\nThe aim of Grad-CAM [7] is to provide a visual explanation for model decision by using gradients flowing into the final convolution layer. Further it is used to produce a map highlighting the important region in the map. \n# 4.4 Methodology\nThe flowchart (Fig 5.) depicts the entire flow of processes which are deployed in the proposed architecture. The unique  combination of Auto-encoder, Block-switching [8] and gradCAM [7] increases the accuracy and robustness of the model.  The proposed architecture follows several phases namely image classification, building of thoroughly supervised dataset of adversarial images, auto-encoder operating as a noise remover along with block-switching containing multiple sub-channels, producing highlighted maps as an output from grad-CAM and ultimately  the output of the model is classified image with highlighted important regions on it.  \nAs an initial step in the architecture, image is taken as input for two purposes specifically \u2013 classification and building of thoroughly supervised dataset of adversarial image. Image\nclassification is performed to ascertain the entire image. The  dataset which consists of two differently classified images  (original and adversarial) is provided as an input to autoencoder. Auto-encoder removes noise from the input image  and yields a denoised image as an output. The output of autoencoder is taken as an input by block-switching model.  Block-switching contains different sub-channels which are  selected randomly to render the output. Grad-CAM are  activation maps which generate highlights on the classified  image to uncover important regions in it. The output of  block-switching is taken as an input for Grad-CAM. It then  processes the image obtained from the image classification  model to provide a visual explanation for model decision by  using gradients flowing into the final convolution layer.  The  output of Grad-CAM is the highlighted image with important  regions in it. The output of Grad-CAM can be used for  anomaly detection. It also tends to notice the attack which  encompasses the above deployed tightly coupled architecture  of auto-encoder and block-switching model.  Ultimately the  output of the above explained architecture is the  classification result backed by the Grad-CAM. The proposed  architecture is tightly coupled and secured in terms of  dataflow and levels of classification deployed to prevent the  adversarial attacks. The combination of distinct phases of the  model is unique and present an overall efficient and  architecture. \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b508/b5087cb1-b2fc-4018-800f-d8e3f3621f87.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig 5. Combination of auto-encoder, block-switching and  grad-CAM </div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/059e/059e77fb-4b9b-415f-80eb-475b684682cd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig 6. Output of respective modules.</div>\nV. RESULTS AND DISCUSSION\n5.1 Auto-encoder  Input- Attack image with perturbations\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/43a6/43a6f3c3-bb6a-49bf-a5a7-26e22ce5af94.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Output- Image is formed by removing perturbations by auto-encoder. </div>\n5.2 Grad-CAM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c23f/c23f5d04-8f89-408b-aa09-b3b82afcab6a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Further the image classification and grad-CAM output can be used for verification. </div>\n# 5.3 Overall Accuracy\n \n# VI. CONCLUSION\nThe attack by the FGSM [9] model is effectively countered by the proposed defence architecture with an accuracy of 88.54%. The unique combination of auto-encoder along with randomization in the classification model ensures efficiency,  high accuracy and robustness. This work can be extended to capture images form motion videos.  \n# REFERENCES\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the vulnerability of state-of-the-art Neural Networks to adversarial input samples, highlighting the inadequacies of previous methods such as adversarial training and defensive distillation, and proposing a new defense algorithm that combines an auto-encoder and block-switching architecture.",
        "problem": {
            "definition": "The problem involves the susceptibility of machine learning models to adversarial examples, which are inputs that have been subtly modified to mislead the model into making incorrect predictions.",
            "key obstacle": "The main challenge is that traditional defense mechanisms do not adequately protect against white-box attacks, where attackers have full access to the model."
        },
        "idea": {
            "intuition": "The inspiration for the proposed idea stems from the need for a robust defense mechanism that can effectively filter out adversarial noise while maintaining high accuracy in classification.",
            "opinion": "The proposed idea involves a defense architecture that integrates auto-encoders for noise removal and block-switching techniques to enhance robustness against adversarial attacks.",
            "innovation": "The primary innovation lies in the combination of auto-encoders and block-switching methods, which together provide a more effective defense against both black-box and white-box attacks compared to existing approaches."
        },
        "method": {
            "method name": "Integrated Auto Encoder-Block Switching Defense Approach",
            "method abbreviation": "IAE-BS",
            "method definition": "A defense algorithm that utilizes an auto-encoder to filter adversarial noise and a block-switching architecture to enhance model robustness against adversarial attacks.",
            "method description": "The method combines auto-encoding for noise reduction and block-switching for improved classification accuracy.",
            "method steps": [
                "Input image is classified and used to build a supervised dataset of adversarial images.",
                "The auto-encoder processes the input to remove noise.",
                "The output from the auto-encoder is passed to the block-switching model, which randomly selects sub-channels for classification.",
                "Grad-CAM generates activation maps to highlight important regions in the image."
            ],
            "principle": "The effectiveness of this method is based on the integration of noise filtration through auto-encoders and the random selection of model parameters in block-switching, which enhances the overall robustness and accuracy of the model."
        },
        "experiments": {
            "evaluation setting": "The experimental setup includes the use of adversarial images generated through the Fast Gradient Sign Method (FGSM) for testing the proposed defense architecture.",
            "evaluation method": "Performance is assessed by measuring the accuracy of the model in correctly classifying images after applying the proposed defense mechanism."
        },
        "conclusion": "The proposed defense architecture successfully counters FGSM attacks with an accuracy of 88.54%, demonstrating its effectiveness and robustness, with potential for extension to motion video analysis.",
        "discussion": {
            "advantage": "The key advantages include improved robustness against adversarial attacks and high classification accuracy due to the effective integration of auto-encoding and block-switching methods.",
            "limitation": "One limitation is that the method may not be fully effective against all types of adversarial attacks, particularly those not represented in the training dataset.",
            "future work": "Future research may focus on enhancing the model to handle dynamic inputs such as motion video and exploring additional layers of defense against various adversarial strategies."
        },
        "other info": {
            "authors": "Ashutosh Upadhyay, Anirudh Yadav, S. Sharanya",
            "year": 2023,
            "references": [
                "Fast Gradient Sign Method [9]",
                "Generative Adversarial Networks [6]",
                "Adversarial Auto-encoders [3]",
                "Grad-CAM [7]",
                "Block-switching [8]"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "2.4",
            "key information": "The problem involves the susceptibility of machine learning models to adversarial examples, which are inputs that have been subtly modified to mislead the model into making incorrect predictions."
        },
        {
            "section number": "3.4",
            "key information": "The method is called Integrated Auto Encoder-Block Switching Defense Approach (IAE-BS), which utilizes an auto-encoder to filter adversarial noise and a block-switching architecture to enhance model robustness against adversarial attacks."
        },
        {
            "section number": "5.2",
            "key information": "The key advantages include improved robustness against adversarial attacks and high classification accuracy due to the effective integration of auto-encoding and block-switching methods."
        },
        {
            "section number": "7.1",
            "key information": "One limitation is that the method may not be fully effective against all types of adversarial attacks, particularly those not represented in the training dataset."
        },
        {
            "section number": "7.2",
            "key information": "Future research may focus on enhancing the model to handle dynamic inputs such as motion video and exploring additional layers of defense against various adversarial strategies."
        }
    ],
    "similarity_score": 0.6269136456702719,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/An integrated Auto Encoder-Block Switching defense approach to prevent adversarial attacks.json"
}