{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2011.06146",
    "title": "Learning Models for Actionable Recourse",
    "abstract": "As machine learning models are increasingly deployed in high-stakes domains such as legal and financial decision-making, there has been growing interest in post-hoc methods for generating counterfactual explanations. Such explanations provide individuals adversely impacted by predicted outcomes (e.g., an applicant denied a loan) with recourse\u2014i.e., a description of how they can change their features to obtain a positive outcome. We propose a novel algorithm that leverages adversarial training and PAC confidence sets to learn models that theoretically guarantee recourse to affected individuals with high probability. We demonstrate the efficacy of our approach with extensive experiments on real data.",
    "bib_name": "ross2022learningmodelsactionablerecourse",
    "md_text": "# Learning Models for Actionable Recourse\nAlexis Ross\u2217 Harvard University Allen Institute for Artificial Intelligence alexisr@allenai.org Himabindu Lakkaraju Harvard University hlakkaraju@seas.harvard.edu\nOsbert Bastani University of Pennsylvania obastani@seas.upenn.edu\n# Abstract\nAs machine learning models are increasingly deployed in high-stakes domains such as legal and financial decision-making, there has been growing interest in post-hoc methods for generating counterfactual explanations. Such explanations provide individuals adversely impacted by predicted outcomes (e.g., an applicant denied a loan) with recourse\u2014i.e., a description of how they can change their features to obtain a positive outcome. We propose a novel algorithm that leverages adversarial training and PAC confidence sets to learn models that theoretically guarantee recourse to affected individuals with high probability. We demonstrate the efficacy of our approach with extensive experiments on real data.\nIn recent years, there has been a growing interest in using machine learning to inform consequential decisions in legal and financial decision-making\u2014e.g., deciding whether to give an applicant a loan [Hardt et al., 2016], bail to a defendant [Lakkaraju and Rudin, 2017], or parole to a prisoner [Zeng et al., 2017]. Because these decisions have an impact on the lives of the concerned individuals, it is critical to explain why the model made its prediction. Explanations are important not only to ensure that there are no issues with the way the prediction is made (e.g., making sure the decision is free of racial/gender bias [Hardt et al., 2016] and does not suffer from causal issues [Bastani et al., 2017]), but also to give the affected individual a justification for the decision. Thus, there has been a great deal of recent interest in explainable machine learning [Lou et al., 2012, Wang and Rudin, 2015, Ribeiro et al., 2016b, Lakkaraju et al., 2019]. We focus on counterfactual explanations [Wachter et al., 2017], which specify how features can be changed to obtain a different model prediction. These explanations can be used to provide individuals who are negatively impacted by model outcomes with actionable recourses\u2014i.e., actions they can take to receive a positive outcome [Ustun et al., 2019]. For instance, for an applicant denied a loan, an actionable recourse might be: \u201cto get a loan, increase your income by $1,000.\u201d Such a recourse must satisfy two properties to be actionable: (i) it only changes features that the individual can realistically modify\u2014e.g., it cannot change race or gender, but can change income, and (ii) the magnitude of the change must be reasonable. An actionable recourse like this may be considered necessary in some settings because it provides an individual with agency over a consequential decision that affects them. Prior research has addressed the need for recourses through post-hoc algorithms for computing individual recourses corresponding to certain kinds of models\u2014e.g., using integer linear programming\nin the context of linear models [Ustun et al., 2019], or using gradient descent on the input for differentiable models [Wachter et al., 2017]. However, these approaches do not guarantee that actionable recourses exist; at best, Ustun et al. [2019] guarantee that they find one if it exists, and only for linear models. In other words, many affected individuals may not even be prescribed any actions that they can take to change their outcome. We aim to provide tools for guaranteeing the existence of actionable recourses for domains in which recourses may be necessary. We propose a novel algorithm for learning models in a way that is designed to ensure the existence of actionable recourses so that affected individuals receive recourse with a high probability. To the best of our knowledge, our approach is the first to train models for which recourse is likely to exist with a high probability. It builds on adversarial training [Goodfellow et al., 2015a], which is designed to ensure that models are robust to adversarial examples. At a high level, given a binary classification model f\u03b8 : X \u2192Y, where Y = {0, 1}, an adversarial example is a perturbation \u03b4 \u2208\u2206to an input x \u2208X such that f\u03b8(x + \u03b4) \u0338= f\u03b8(x). In a typical adversarial training setting, we choose \u2206to be \u201csmall\u201d in some sense (e.g., in terms of L\u221enorm of \u03b4 \u2208\u2206) and aim to guarantee that adversarial examples do not exist\u2014i.e., f(x + \u03b4) = f(x) for all \u03b4 \u2208\u2206. In contrast, in our setting, we intuitively want to ensure that adversarial examples do exist. In particular, given an input x for which f(x) = 0, we want there to exist recourse \u03b4 \u2208\u2206such that f(x + \u03b4) = 1, where in our case, \u2206is the given set of all permissible recourses for the application domain. Thus, we adapt existing adversarial training algorithms to ensure the existence of recourse. As an added benefit, these algorithms can compute recourse significantly faster than existing techniques for general differentiable models [Wachter et al., 2017]. While adversarial training heuristically encourages recourse to exist, it does not provide any theoretical guarantees. We build on PAC confidence sets [Park et al., 2020] to guarantee that recourse exists with high probability (assuming the test distribution is the same as the training distribution). We evaluate our approach on four real world datasets that cover lending, recidivism, bail, and credit outcomes. Our results demonstrate that our approach is very effective at improving recourse rates (i.e., the probability that individuals are given recourse) without noticeably reducing accuracy. In addition, we show that we achieve this improvement in recourse rates without noticeably harming the quality of recourses or the brittleness of the underlying model. Firstly, we empirically demonstrate that our approach improves the rate at which models provide recourses that are grounded in reality: We find that our approach encourages the existence of recourses that both obey causal constraints driven by real-world causal relationships and are in-distribution to the original data. Secondly, we show that our approach encourages the existence of robust recourses (i.e., recourses that result in positive outcomes even when changed in small ways). Lastly, we show that these improvements in recourse rates do not render the underlying classifier brittle.2 Related work. Beyond Wachter et al. [2017], Ustun et al. [2019], other approaches have been proposed for generating recourses [Zhang et al., 2018, Hendricks et al., 2018, Mothilal et al., 2020, Looveren and Klaise, 2019, Poyiadzi et al., 2020, Karimi et al., 2020a,b,c]. However, all these works focus on how to compute recourse for given predictive models; in contrast, our goal is to learn predictive models that provide recourses at high rates. Any of these methods can be used in conjunction with ours. Our work also builds on adversarial training [Szegedy et al., 2014, Goodfellow et al., 2015b, Bastani et al., 2016, Shaham et al., 2018]. While recent work in model explainability has leveraged adversarial training to improve robustness of explanations [Lakkaraju et al., 2020b], their goal is to reduce the rate of adversarial examples, whereas ours is to increase the rate of recourses.\n# 2 Problem Formulation\nPreliminaries. Consider a binary classifier f\u03b8 : X \u2192Y, where x \u2208X \u2286RnX are the features, y \u2208Y = {0, 1} are the labels, and \u03b8 \u2208\u0398 \u2286Rn\u0398 are the parameters. We assume that f\u03b8 has the form f\u03b8(x) = 1(g\u03b8(x) \u2265\u03b80), where g\u03b8 : X \u2192[0, 1] is a scoring function and \u03b80 \u2208R is a decision threshold. We assume that g\u03b8 is differentiable in x\u2014i.e., \u2207xg\u03b8(x) exists almost everywhere.3\nRecourse. We seek to ensure that individuals given negative outcomes by f\u03b8 are also given recourse. Definition 2.1. Given a classifier f\u03b8 : X \u2192Y and a set \u2206\u2286RnX, an input x \u2208X has recourse if there exists \u03b4 \u2208\u2206such that f(x + \u03b4) = 1. We use X R \u03b8 to denote the set of inputs for which recourse exists. The specific design of the set of permissible recourses \u2206\u2286RnX is domain specific. We assume that 0 \u2208\u2206; then, recourse automatically exists for positive outcomes f\u03b8(x) = 1 by taking \u03b4 = 0. In addition, we assume that \u2206 is a polytope\u2014i.e., \u2206= {\u03b4 \u2208RnX | A\u03b4 + b \u22650} for some A \u2208Rk\u00b7nX and b \u2208Rk. That is, \u2206can be expressed as a set of affine constraints. This assumption is required for computational tractability. A standard choice is \u2206= {\u03b4 | \u2225\u03b4\u2225\u221e\u2264\u03b4max}, which says that the recourse can suggest changes to any feature by a bounded amount. We can apply this constraint after an affine transformation of \u03b4 that appropriately scales different covariates. In addition, we often want to restrict features\u2014e.g., to avoid suggesting that an individual change their age, we can constrain \u03b4i = 0, or to avoid suggesting that an individual decrease their income to qualify for a loan, we can constrain \u03b4i \u22650. In principle, \u2206can also be tailored to individuals\u2014e.g., disallow suggesting increased education for individuals who cannot afford to do so. Finally, \u2206should be designed large enough so it includes a plausible recourse for every individual, yet small enough to ensure that the recourses do not overburden the individuals. All of these considerations are domain-specific; we describe our choices for datasets in our experiments in Section 4.\n# Probably Approximately has REcourse (PARE). Our goal is to ensure that recourse exists fo most individuals. Given a confidence level \u03f5 \u2208R>0, we say that \u03b8 approximately has recourse if \ufffd \ufffd\n\ufffd \ufffd i.e., recourse exists for f\u03b8 with probability at least 1 \u2212\u03f5 w.r.t. the distribution p(x) over individuals. Then, our goal is to design an algorithm for estimating the model parameters \u03b8 so that f\u03b8 approximately has recourse. To do so, our algorithm takes as input a held-out calibration dataset Z \u2286X \u00d7 Y of examples (x, y) \u223cp, where p(x, y) is the distribution over labeled examples, and outputs model parameters \u02c6\u03b8(Z). Then, as in the probably approximately correct (PAC) learning framework [Valiant, 1984], our algorithm might additionally fail due to the randomness in Z. Thus, given a second confidence level \u03b1 \u2208R>0, we say that \u02c6\u03b8 Probably Approximately has REcourse (PARE) if \ufffd \ufffd\n\ufffd \ufffd where p(Z) = \ufffd (x,y)\u2208Z p(x, y). In other words, our algorithm produces a model that approximately has recourse with probability at least 1 \u2212\u03b4 over p(Z). Constructing a PARE classifier. Note that we can trivially obtain a PARE classifier f\u03b8 by choosing the decision threshold \u03b80 = 0, in which case f\u03b8(x) = 1 for all x \u2208X. However, this approach is undesirable since it assigns a positive outcome to all individuals. Instead, we want to maximize the performance of f\u03b8 (e.g., in terms of accuracy, F1 score, etc.) subject to a constraint that f\u03b8 is PARE. Thus, we divide the problem of constructing a PARE classifier into two parts (i) increasing recourse rate: we train g\u03b8 in a way that heuristically increases the rate at which inputs x \u2208X have recourse (for any \u03b80), and (ii) guaranteeing recourse: we choose \u03b80 to guarantee that the resulting f\u03b8 is PARE.\n# 3 Our Algorithm\nWe describe our algorithm for learning models that satisfy PARE while achieving good performance. We describe Step 1 (increasing recourse rate) in Section 3.1 and Step 2 (guaranteeing recourse) in Section 3.3. We describe ways to compute recourse in Section 3.2.\n# 3.1 Step 1: Increasing Recourse Rate\nBackground: adversarial training. Consider a classifier f\u03b8 : X \u2192Y and perturbations \u2206\u2286RnX. Given x \u2208X, an adversarial example [Szegedy et al., 2014] for x is a perturbation \u03b4 \u2208\u2206such that f\u03b8(x + \u03b4) \u0338= f\u03b8(x)\u2014i.e., the perturbation \u03b4 (restricted to be small) changes the predicted label of x. Adversarial examples are undesirable because they indicate that f\u03b8 is not robust to small changes to the input that should not affect the class label (e.g., according to human predictions). Thus, there has\nbeen a great deal of interest in designing algorithms for improving robustness to adversarial examples. The basic approach is adversarial training [Goodfellow et al., 2015b], which dynamically computes adversarial examples for inputs in the training set and adds these to the objective as additional training examples, as in data augmentation. In particular, given a loss function \u2113: R \u00d7 Y \u2192R, where \u2113(g\u03b8(x), y) is the loss for training example (x, y), they seek to compute \ufffd \ufffd\n\u2208 \ufffd \ufffd In \u2113A(\u03b8), the first term is the supervised learning loss, the second is the adversarially robust loss (i.e., encourage g\u03b8 to be robust to adversarial examples x + \u03b4), and \u03bb \u2208R\u22650 is a hyperparameter. The challenge in optimizing \u2113A(\u03b8) is computing the maximum over \u03b4 \u2208\u2206. To address this challenge, existing adversarial training algorithms leverage approximations enabling efficient computation of \u03b4. Our approach. We use adversarial training to learn a model f\u03b8 for which inputs x \u2208X have recourse at higher rates compared to models trained using conventional approaches. There are two key differences compared to adversarial training. First, we want recourse to exist, which corresponds to encouraging the existence of adversarial examples. Second, we only care about changing negative labels f\u03b8(x) = 0 to positive ones f\u03b8(x + \u03b4) = 1, not vice versa. Thus, we want to solve\n\u2208 \ufffd \ufffd In \u2113A(\u03b8), the first term is the supervised learning loss, the second is the adversarially robust loss (i.e., encourage g\u03b8 to be robust to adversarial examples x + \u03b4), and \u03bb \u2208R\u22650 is a hyperparameter. The challenge in optimizing \u2113A(\u03b8) is computing the maximum over \u03b4 \u2208\u2206. To address this challenge, existing adversarial training algorithms leverage approximations enabling efficient computation of \u03b4. Our approach. We use adversarial training to learn a model f\u03b8 for which inputs x \u2208X have recourse at higher rates compared to models trained using conventional approaches. There are two key differences compared to adversarial training. First, we want recourse to exist, which corresponds to encouraging the existence of adversarial examples. Second, we only care about changing negative labels f\u03b8(x) = 0 to positive ones f\u03b8(x + \u03b4) = 1, not vice versa. Thus, we want to solve \u03b8\u2217= arg min \u03b8\u2208\u0398 \u2113R(\u03b8) where \u2113R(\u03b8) = Ep(x,y) \ufffd \u2113(g\u03b8(x), y) + \u03bb \u00b7 min \u03b4\u2208\u2206\u2113(g\u03b8(x + \u03b4), 1) \ufffd . (1) Compared to \u2113A, \u2113R has a different second term in two ways: (i) the maximum over \u03b4 with a minimum, and (ii) the label y is replaced with the label 1. We note that when \u03bb = 0, Eq. 1 is supervised learning. We optimize Eq. 1 using adversarial training [Goodfellow et al., 2015a, Shaham et al., 2018, Lakkaraju et al., 2020a], which performs stochastic gradient descent on \u2113R(\u03b8). The key challenge to computing \u2207\u03b8\u2113R(\u03b8) is computing the gradient of the second term, which can be rewritten as follows: \u2207\u03b8 min \u2208\u2113(g\u03b8(x + \u03b4), 1) = \u2207\u03b8\u2113(g\u03b8(x + \u03b4\u2217), 1),\nCompared to \u2113A, \u2113R has a different second term in two ways: (i) the maximum over \u03b4 with a minimum, and (ii) the label y is replaced with the label 1. We note that when \u03bb = 0, Eq. 1 is supervised learning. We optimize Eq. 1 using adversarial training [Goodfellow et al., 2015a, Shaham et al., 2018, Lakkaraju et al., 2020a], which performs stochastic gradient descent on \u2113R(\u03b8). The key challenge to computing \u2207\u03b8\u2113R(\u03b8) is computing the gradient of the second term, which can be rewritten as follows: \u2207\u03b8 min \u2113(g\u03b8(x + \u03b4), 1) = \u2207\u03b8\u2113(g\u03b8(x + \u03b4\u2217), 1),\nwhere \u03b4\u2217is the perturbation that maximizes the likelihood of positive outcome, or minimizes the loss between predicted and positive outcomes\u2014i.e.,\nComputing \u03b4\u2217is computationally challenging; thus, we use a Taylor approximation of the loss \u2113(g\u03b8(x + \u03b4), 1) \u2248\u2113(g\u03b8(x), 1) + \u2207x\u2113(g\u03b8(x), 1)\u22a4\u03b4. Using this approximation, Eq. 2 becomes\nwhere we have dropped the term \u2113(g\u03b8(x), 1) since it is constant with respect to \u03b4. Finally, note that the optimization problem on the last step is a linear program (LP), since we have assumed that \u03b4 \u2208\u2206 can be expressed as a set of affine constraints and since the objective is linear in \u03b4. In summary, we optimize Eq. 1 using stochastic gradient descent, where at each step we solve an LP to approximate the second term\u2014i.e., given parameters \u03b8i and example (xi, yi) on gradient step i, and step size \u03b7i \u2208R>0 on step i, we use the stochastic gradient update \u03b8i+1 = \u03b8i + \u03b7i (\u2207\u03b8\u2113(g\u03b8(x), y) + \u03bb \u00b7 \u2207\u03b8\u2113(g\u03b8(x + \u03b4\u2217 i , 1)) where \u03b4\u2217 i = arg min \u03b4\u2208\u2206 \u2207x\u2113(g\u03b8(x), 1)\u22a4\u03b4.\n# 3.2 Computing Recourse\nSo far, we have focused on how to train a model f\u03b8 that provides recourse. Once we have trained f\u03b8, we still need a way to compute recourse for a given individual x\u2014i.e., an algorithm A : X \u2192\u2206for computing \u03b4x = A(x) such that f(x + \u03b4x) = 1. We describe three such algorithms; in general, any algorithm designed to output recourses can be used [Karimi et al., 2020a, Poyiadzi et al., 2020]. Gradient descent. The approach proposed in Wachter et al. [2017] can directly be applied to compute recourse. They solve the problem \u2032 \n\u03b4x = arg min \u03b4\u2208\u2206 {\u2113(g\u03b8(x + \u03b4, 1) + \u03bb\u2032 \u00b7 \u2225\u03b4\u22252},\n(1)\n(2)\nwhere \u03bb\u2032 \u2208R\u22650 is a hyperparameter, using gradient descent on \u03b4. The term \u2225\u03b4\u22252 is designed to encourage the recourse \u03b4 to be small, which is often desirable in practice (we have excluded it from our formulation for simplicity). While this approach is generally effective, it can be very slow since we need to solve an optimization problem for each individual.\nThis approach approximates the gradient descent approach, but can be computed much more efficiently. Furthermore, since our objective in Eq. 1 is designed to encourage this specific perturbation to provide recourse, it performs nearly as well as gradient descent when f\u03b8 is trained with \u03bb > 0. Linear approximation. Finally, Ustun et al. [2019] propose an approach to compute recourse when f\u03b8(x) = 1(\u03b2\u22a4x \u2265\u03b20) is a linear model. In this case, they compute \u03b4x using an integer linear program (ILP). For nonlinear models, we can instead use the linear approximation of g\u03b8 near x. Letting \u03b4x = A(x; \u03b20, \u03b2) be the recourse generated by their algorithm, and using the Taylor expansion g\u03b8(x\u2032) \u2248g\u03b8(x) + \u2207xg\u03b8(x)\u22a4(x\u2032 \u2212x), we can use their approach to compute the recourse\n  However, this approach only works well when g\u03b8 is approximately linear as a function of x; otherwise, the Taylor approximation may be poor and \u03b4x may not satisfy the desired condition f\u03b8(x + \u03b4x) = 1.\n# 3.3 Step 2: Guaranteeing Recourse\nFinally, we describe how we choose \u03b80 to ensure f\u03b8 provides recourse with high probability. Note that \u03b80 also controls the fraction of individuals given a positive outcome without the need for recourse; thus, we choose \u03b80 to optimize the performance of f\u03b8 subject to a constraint that f\u03b8 is PARE. Background: PAC confidence sets. We build on work constructing PAC confidence sets [Park et al., 2020]. Given x \u2208X, they construct a model \u02dch\u03b8,\u03c4 : X \u2192P(Y) (where P is the power set) that returns the set of all labels y with score above threshold \u03c4 \u2208R\u2014i.e.,\nGiven \u03f5 \u2208R>0, we say \u03c4 is approximately correct if\ni.e., \u02dch\u03b8,\u03c4(x) contains the true label for x with high probability over p(x, y). Note that \u03c4 = 0 satisfies this condition, since \u02dch\u03b8,0(x) = {0, 1} for all x. The goal is to choose \u03c4 as large as possible while ensuring approximate correctness. Park et al. [2020] proposes an estimator \u02c6\u03c4 that takes as input (i) the pretrained model g\u03b8 : X \u2192[0, 1], (ii) a calibration dataset Z \u2286X \u00d7 Y of i.i.d. samples (x, y) \u223cp, and (iii) confidence levels \u03f5, \u03b1 \u2208R>0, and constructs a threshold \u02c6\u03c4(Z) \u2208[0, 1] that is probably approximately correct (PAC):\nIn other words, \u02c6\u03c4(Z) is approximately correct with probability at least 1 \u2212\u03b1 according to p(Z). Their approach leverages the fact that \u02c6\u03c4(Z) is an estimator for a single parameter; thus, they can use learning theory to obtain PAC guarantees [Kearns et al., 1994]. PARE models via PAC confidence sets. Given a model g\u03b8 : X \u2192R, an algorithm A : X \u2192\u2206for computing recourse, and a calibration set Z \u2286X \u00d7 Y, our algorithm leverages PAC confidence sets to choose a threshold \u03b80 = \u02c6\u03b80(Z) that ensures the resulting model f\u02c6\u03b8 satisfies the PARE constraint. First, our algorithm uses the PAC confidence set algorithm to construct the new calibration dataset Z\u2032 = {(x + A(x), 1) | (x, y) \u2208Z}. Intuitively, Z\u2032 says that the \u201ccorrect\u201d label for every input x + A should be 1\u2014i.e., the recourse constructed by A should satisfy f\u03b8(x + A(x)) = 1.\nMetrics\nAdult\nCompas\nBail\nGerman\nBaseline\nOurs\nBaseline\nOurs\nBaseline\nOurs\nBaseline\nOurs\nPerformance\nF1 score\n0.697\n0.636\n0.739\n0.717\n0.775\n0.760\n0.447\n0.419\nAccuracy\n0.830\n0.787\n0.667\n0.565\n0.643\n0.629\n0.600\n0.527\nPrecision\n0.621\n0.555\n0.655\n0.561\n0.646\n0.644\n0.364\n0.317\nRecall\n0.799\n0.752\n0.850\n0.991\n0.968\n0.930\n0.583\n0.638\nRecourse neg\nLinear approx.\n0.220\n0.053\n0.156\n0.068\n0.102\n0.029\n0.204\n0.086\nGradient desc.\n0.210\n0.496\n0.579\n1.000\n0.317\n1.000\n0.804\n0.864\nAdversarial train.\n0.007\n0.498\n0.000\n0.967\n0.000\n0.993\n0.127\n0.968\nRecourse all\nLinear approx.\n0.453\n0.328\n0.773\n0.982\n0.957\n0.981\n0.607\n0.593\nGradient desc.\n0.461\n0.661\n0.883\n1.000\n0.970\n1.000\n0.890\n0.920\nAdversarial train.\n0.321\n0.659\n0.722\n0.999\n0.952\n0.999\n0.517\n0.980\nTable 1: Performance and recourse for the baseline model (\u03bb = 0) and the model trained with our algorithm (\u03bb = 0.8), and for each of the three algorithms for computing recourse in Section 3.2. We show mean results across 3 random data splits and bold the higher value between the baseline and our algorithm.\nThen, our algorithm constructs \u02dch\u03b8,\u02c6\u03c4(Z) using g\u03b8, Z\u2032, and the given \u03f5, \u03b1. The PAC guarantee in Eq says that with probability at least 1 \u2212\u03b1 over p(Z), we have\n\ufffd \ufffd where (x\u2032, y\u2032) \u2208Z\u2032 is the example constructed from (x, y) \u2208Z as described above. Note that the outer probability is over p(Z) since Z\u2032 is a deterministic function of the random variable Z. Plugging in the definitions x\u2032 = x + A(x) and y\u2032 = 1, Eq. 4 becomes\n\ufffd and plugging in the definition of \u02dch\u03b8,\u03c4, it becomes\nPp(Z) \ufffd Pp(x,y)[g\u03b8(x + A(x)) \u2265\u02c6\u03c4(Z\u2032)] \u22651 \u2212\u03f5 \ufffd \u22651 \u2212\u03b1.\n\ufffd \ufffd Then, our algorithm returns \u02c6\u03b80(Z) = \u02c6\u03c4(Z\u2032) (with given parameters \u03b8 as the remaining parameters). Since Eq. 5 is equivalent to the PARE condition, we have: Theorem 3.1. \u02c6\u03b8 satisfies the PARE condition.\n# 4 Experiments\nWe evaluate our approach and show how it can effectively improve recourse rates while preserving accuracy. We also demonstrate how it can improve the correctness and robustness of recourses. In Appendix B, we provide additional results on how our approach affects fairness of models, as well as results on an NLP task with discrete covariates to demonstrate the flexibility of our framework.\n# 4.1 Experimental Setup\nDatasets. We use four real-world datasets. The first contains adult income information from the 1994 United States Census Bureau [Dua and Graff, 2017]. It includes information about adults\u2019 demographics, education, and occupations. Each adult is labeled as making below or > $50K a year, which can be thought of as a proxy for whether an individual will be able to repay a loan or not. The second contains information collected by Propublica about criminal defendants\u2019 compas recidivism scores [Angwin et al., 2016]. This dataset includes information about defendants\u2019 demographics\n(4)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f0f/3f0f1665-82ae-41a7-8807-1ca9a03749d0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: How performance and recourse vary with \u03bb; \u03bb = 0 is the baseline and \u03bb > 0 is our approach. We plot means and standard errors across 3 random data splits. The first row shows performance metrics; the second and third show recourse metrics. Performance: F1 Accuracy. Recourse Algorithm: \u201cgradient descent\u201d \u201cadversarial training\u201d.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d02e/d02e0429-7ed0-40ad-ba0d-10ee510f5f7b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2566/2566bf8e-d0ca-43d2-8c1d-84b9ff411c99.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: How performance and recourse vary with \u03b80, for our approach (\u03bb = 0.8, left) and the baseline (\u03bb = 0, right). Performance: F1 Accuracy. Recourse: Recourse neg Recourse all.</div>\nand crimes, and each defendant is labeled as having either a high or low likelihood of reoffending, as measured by the compas assessment tool. The third dataset represents bail outcomes from two different U.S. state courts from 1990-2009 [Schmidt and Witte, 1988]. It includes information about individuals\u2019 criminal histories and demographics. Each defendant is labeled as having a high or low risk of recidivism. The fourth dataset is the german credit dataset [Dua and Graff, 2017], which contains individuals\u2019 demographic, personal, and financial information. Each applicant is labeled as either having high or low credit risk. We standardize all continuous features to have mean 0 and variance 1. We randomly split each dataset into 80% train and 20% validation sets. We use 3 random data splits and report the mean across splits for the rest of this section, unless otherwise specified. For compas and adult, we hold out 500 examples from the validation set to form a test set; for german, we hold out 100 examples. For bail, we evaluate on a 500-instance subset of the test set. All results are reported on the test set.4 Models. All models are neural networks with 3 100-node hidden layers, dropout probability 0.3, and tanh activations. For evaluation, we choose the epoch achieving the highest validation F1 score. Parameters. We experimented with \u03bb values between 0.0 to 2.0 in increments of 0.2 and found that \u03bb = 0.8 provided the best tradeoff between F1 score and recourse rate across all datasets; we evaluate how our results vary with \u03bb below. For \u2206, we choose the set of actionable features (i.e., features that can be changed as part of the recourse) based on the dataset (two features for adult, bail, german; one for compas); we set \u03b4max = 0.75 after standardizing features. (See Appendix B.2 for experiments investigating the effect of varying values of \u03b4max). We also include domain-specific linear constraints in \u2206\u2014e.g., for the adult dataset, recourse can only require that hours worked increases. See Appendix A.3 for more details about our choices of \u2206.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9da/e9dab36a-417d-4fe4-abbe-5eeb7036bb04.png\" style=\"width: 50%;\"></div>\nWe choose the decision threshold \u03b80 to maximize F1 score rather than to obtain PAC guarantees, since our goal is to understand the tradeoffs between model performance and recourse rates with a fixed underlying predictive model f\u03b8. We evaluate the effects of rigorously choosing \u03b80 in Section 4.2. Baselines. To the best of our knowledge, our approach is the first to train models with the objective of providing recourse at a higher rate. Thus, we compare to a baseline that omits our recourse loss\u2014i.e., \u03bb = 0.0. For our approach and this baseline, we evaluate the performance of each of the three different algorithms for computing recourse described in Section 3.2. We use the alibi implementation [Klaise et al., 2019] of the gradient descent algorithm for computing recourse [Wachter et al., 2017] and set the initial value of the hyperparameter \u03bb\u2032 = 0.001. We use LIME [Ribeiro et al., 2016a] as our linear approximation method with the approach proposed by Ustun et al. [2019]. Metrics. We evaluate our approach and the baselines with the following metrics: (i) standard performance metrics of accuracy and F1 score, (ii) \u201crecourse neg,\u201d the proportion of instances x with negative original outcomes that receive recourse such that f(x) = 0 but f(x + \u03b4\u2217) = 1, and (iii) \u201crecourse all,\u201d the proportion of all instances x such that either f(x) = 1 or f(x) = 0 but f(x + \u03b4\u2217) = 1. Metric (iii) is most useful for measuring rate of positive outcomes, since we want to include individuals who are originally assigned a positive outcome.\n# 4.2 Efficacy of Our Framework\nIn Table 1, we show the performance and recourse rates of models trained with our approach and baseline models. Overall, our approach significantly improves recourse without sacrificing performance. Across datasets, models trained using our approach offer recourse at significantly higher rates than the baseline model, for both the \u201cadversarial training\u201d and \u201cgradient descent\u201d approaches to computing recourse. We do not observe this trend when using \u201clinear approximation\u201d to compute recourse; in this case, both the baseline and our approach perform poorly. We believe this effect can be explained by poor LIME approximations of f\u03b8, which are exacerbated by adversarial training since it increases the nonlinearity of f\u03b8. Figure 1 shows how these results vary with \u03bb: F1 scores and accuracies are relatively stable as a function of \u03bb. In Figure 2, we plot how these results vary with \u03b80 (for classifiers trained with \u03bb = 0 and \u03bb = 0.8 on a single data split), using the \u201cadversarial training\u201d method of computing recourse.5 High values of \u03b80 lead to lower recourse rates in all cases. The curves for performance are similar for the baseline and for our approach, but the decline in recourse values begins at lower thresholds in the case of the baseline. Thus, our approach improves recourse for most choices of \u03b80 without sacrificing performance. The trade-off between recourse and performance depends on the dataset. For adult, performance increases while recourse decreases since the majority label in this dataset is negative, whereas for bail and compas, performance and recourse both decrease as \u03b80 increases since the majority label is positive. Performance under PARE guarantees. Next, we show that we can often obtain PARE guarantees without significantly reducing performance. We compare the performance of choosing the decision threshold \u03b80 to maximize F1 score to that of \u03b80 chosen to obtain PARE classifiers. Specifically, for the latter, we compute \u03b80 using the approach described in Section 3.3 with parameters \u03f5 = \u03b1 = 0.05. Then, we evaluate models at thresholds in 10 equally spaced increments from 0 to the upper bound and fix \u03b80 to maximize F1 score. For these experiments, we use the \u201cadversarial training\u201d algorithm to compute recourse; we observed similar trends for the \u201cgradient descent\u201d algorithm. Results are shown in Table 2. In all cases, choosing \u03b80 to satisfy the PARE condition yields a classifier that returns recourse at a rate \u22651 \u2212\u03f5 = 0.95, which validates our theoretical guarantees. Furthermore, on all three datasets, the F1 score does not significantly decrease when imposing the PARE condition. We do see a decrease in F1 score for the baseline model on the adult dataset, but the decrease for our model is smaller, suggesting that our end-to-end framework of training models and fixing \u03b80 is successful at guaranteeing recourse without a big drop in accuracy.6 5Note that the \u201crecourse neg\u201d values are low for \u03bb = 0 because we use the \u201cadversarial training\u201d method to compute recourse, which builds on a fast linear approximation and thus does not exhaustively find recourses. We\n5Note that the \u201crecourse neg\u201d values are low for \u03bb = 0 because we use the \u201cadversarial training\u201d method to compute recourse, which builds on a fast linear approximation and thus does not exhaustively find recourses. We use this method instead of the more effective \u201cgradient descent\u201d method for efficiency, since the latter is less efficient and would require recomputing recourses for each threshold. 6We can obtain a weaker theoretical guarantee at a smaller cost in performance. For instance, applying PARE to our model in Table 2 with \u03f5 = \u03b1 = 0.25 results in an F1 score of 0.613 on the adult dataset.\nAdult\nCompas\nBail\nGerman\nF1\nRecourse\nF1\nRecourse\nF1\nRecourse\nF1\nRecourse\nBL + F1 max\n0.697\n0.321\n0.739\n0.722\n0.775\n0.952\n0.447\n0.517\nBL + PARE\n0.400\n0.981\n0.722\n0.972\n0.777\n0.996\n0.442\n0.997\nOurs + F1 max\n0.636\n0.659\n0.717\n0.999\n0.760\n0.999\n0.419\n0.980\nOurs + PARE\n0.526\n0.974\n0.721\n0.999\n0.776\n0.999\n0.457\n1.000\nTable 2: Impact of choosing decision threshold \u03b80 to satisfy the PARE constraint. We show F1 scores for the baseline model and the model trained with our algorithm using two methods of determining \u03b80: (i) maximize the F1 score, and (ii) guarantee that the model satisfies the PARE constraint (+PARE, bolded). For the baseline model (BL), \u03bb = 0; for our model (Ours), \u03bb = 0.8.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ea1/4ea14137-b935-4593-b278-b0146f227809.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: \u201cRecourse neg\u201d for the german dataset using the causal recourse computing algorithm proposed by Karimi et al. [2020b]. We show means and standard errors across 20 random data splits for varying values of \u03bb.</div>\nGroundedness of recourses. One key question is whether the recourses of models trained with our approach are grounded in reality\u2014i.e., whether they are plausible modifications of the ground truth label. For instance, if an individual is denied a loan, they should be given a recourse such that if they take the specified action, they actually increase their likelihood of paying back the loan; an individual may increase their income and be given a loan, but still fail to repay it. We want to ensure that our training approach increases the rate of recourses that obey causal relationships in the world, rather than recourses that exploit spurious correlations between features. Directly evaluating groundedness of recourses is challenging, since we do not know the ground truth labels for suggested recourses. Thus, we evaluate whether they are causally grounded and in-distribution. First, we measure the rate at which causally grounded recourses are offered by models trained with our approach. We apply the algorithm for computing causal recourses (CR) introduced by Karimi et al. [2020b] to evaluate whether our proposed training algorithm improves the rate at which causally grounded recourses are offered. Because CR requires access to an underlying structural causal model (SCM) of the world, we only experiment with the german dataset, for which Karimi et al. [2020b] provide an associated SCM. We measure the proportion of test instances x for which the CR algorithm computes a valid recourse that satisfies the constraint that perturbations be bounded by \u03b4max\u2014i.e. \u201crecourse neg\u201d.7 As shown in Figure 3, with increasing \u03bb, the rate at which recourses are by the CR algorithm increases. This finding suggests that our training algorithm encourages the existence of causally grounded recourses. Second, we evaluate whether the recourses offered by models trained with our approach are indistribution with respect to the original training data. In line with prior work [Slack et al., 2020], we train classifiers to distinguish between original data instances and recourses computed by the \u201cgradient descent\u201d algorithm for models trained with our approach (\u03bb = 0.8). In Table 3, we report the accuracies of these classifiers on a held out test set. The low classifier accuracies across all datasets indicate that these recourses are indistinguishable from original data instances. Thus, our framework encourages the existence of recourses that are in-distribution to the original data. Robustness. Another key question is whether the recourses generated using our approach are robust\u2014 i.e., whether small changes to the recourse result in valid recourses. For instance, if an individual\nAdult\nCompas\nBail\nGerman\nNeural Network\n0.54\n0.56\n0.52\n0.51\nRandom Forest\n0.53\n0.55\n0.51\n0.51\nLogistic Regression\n0.51\n0.48\n0.48\n0.47\nTable 3: Accuracies of classifiers trained to distinguish between original data instances and recourses computed using the \u201cgradient descent\u201d algorithm for computing recourse for models trained on a single random data split with our approach (\u03bb = 0.8).\nAdult\nCompas\nBail\nGerman\nRobustness Exp.\nRecourse Alg.\nBL\nOurs\nBL\nOurs\nBL\nOurs\nBL\nOurs\nRecourse\nGrad. desc.\n1.000\n0.865\n1.000\n1.000\n1.000\n1.000\n0.949\n0.898\nAdvers. train.\n1.000\n0.841\n1.000\n1.000\n1.000\n1.000\n0.857\n1.000\nModel\n\u2013\n0.976\n0.926\n0.980\n1.000\n0.994\n0.996\n0.970\n0.920\nTable 4: The first row shows the percentage of recourses found that are robust to noise. The second row shows the percentage of test inputs that are robust to noise in the recourse dimensions. We show results for models trained with varying \u03bb (\u03bb = 0 indicates the baseline, and \u03bb = 0.8 indicates our approach) on a single data split. For each model, we show results for the \u201cgradient descent\u201d and \u201cadversarial training\u201d algorithms for computing recourse in Section 3.2.\nincreases their income by more (or even slightly less) than the amount suggested in the recourse, they would expect to still be provided with a positive decision. For each computed recourse \u03b4, we compute a noisy recourse \u03b4\u2032 by adding i.i.d. Gaussian noise to each actionable feature of \u03b4\u2014i.e., \u03b4\u2032 i = \u03b4i + N(0, 0.1). We consider a recourse \u03b4 robust if its noisy recourse \u03b4\u2032 is valid\u2014i.e. if f(x + \u03b4\u2032) = 1. In the top row of Table 4, we report the percentage of recourses found that are robust for a baseline model and model trained with our adversarial approach. As shown, our training approach does not significantly reduce the robustness of recourses: There is a slight drop in robustness for the adult dataset and for the \u201cgradient descent\u201d recourse algorithm for the german dataset; however, for compas and bail, recourses remain robust. Effect on classifier brittleness. We also investigate the effect of our adversarial training approach on model brittleness. In particular, we measure how sensitive models trained with our adversarial algorithm are to noises in the recourse dimensions, as compared to baseline models. We add i.i.d. Gaussian noise, as described above, to the actionable dimensions of original inputs, and compute the proportion of test instances for which the trained model is robust to this noise. As shown in the bottom row of Table 4, our training approach does not significantly increase model brittleness\u2014we observe a small drop in model robustness in the recourse dimensions for the adult and german datasets and a small increase in robustness for the compas and bail datasets. These results suggest that our training approach effectively ensures recourse without rendering classifiers brittle.\n# 5 Conclusion\nWe have proposed a novel algorithm for training models that are guaranteed to provide recourse for reversing adverse outcomes with high probability. Our experiments show that our algorithm trains models that provide recourse at high rates without sacrificing accuracy compared to traditional learning algorithms. Future work includes extending our techniques beyond binary classification.\n# Acknowledgements\nWe would like to thank the anonymous reviewers for their insightful feedback. This work is supported in part by the NSF awards #IIS-2008461, #IIS-2040989, and #CCF-1910769, and research awards from the Harvard Data Science Institute, Amazon, Bayer, and Google. The views expressed are those of the authors and do not reflect the official policy or position of the funding agencies.\n# References\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. How we analyzed the compas recidivism algorithm. Propublica, 2016.\nOsbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and Antonio Criminisi. Measuring neural net robustness with constraints. In Advances in neura information processing systems, pages 2613\u20132621, 2016.\nArnaud Van Looveren and Janis Klaise. Interpretable counterfactual explanations guided by prototypes, 2019. Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150\u2013158, 2012. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2574\u20132582, 2016. Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 607\u2013617, 2020. Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. Pac confidence sets for deep neural networks via calibrated prediction. In ICLR, 2020. Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. Face: Feasible and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201920, page 344\u2013350, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371100. doi: 10.1145/3375627.3375850. URL https://doi.org/10.1145/3375627.3375850. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should I trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 1135\u20131144, 2016a. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016b. Peter Schmidt and Ann D. Witte. Predicting recidivism in north carolina, 1978 and 1980. Interuniversity Consortium for Political and Social Research, 1988. URL https://www.ojp.gov/ pdffiles1/Digitization/115306NCJRS.pdf. Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing local stability of supervised models through robust optimization. Neurocomputing, 307:195\u2013204, 2018. Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201920, page 180\u2013186, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371100. doi: 10.1145/3375627.3375830. URL https://doi.org/10.1145/3375627.3375830. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014. Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 10\u201319, 2019. Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142, 1984. S. Wachter, Brent D. Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the gdpr. European Economics: Microeconomics & Industrial Organization eJournal, 2017.\n# A Experiment Details\n# A.1 Training Details\nWe train our models with the ADAM optimizer using a learning rate of 0.002. For adult, compas, and bail, we train for 15 epochs using a batch size of 15. For german, train for 50 epochs and use a batch size of 30.\n# A.2 Data Processing\n# We use the following features for training our models.\n\u2022 Adult: \u201cage,\u201d \u201ceducation-num,\u201d \u201ccapital-gain,\u201d \u201ccapital-loss,\u201d \u201chours-per-week,\u201d \u201crace,\u201d \u201cnative-country,\u201d \u201cmarital-status,\u201d \u201csex.\u201d \u2022 Bail: \u201cWHITE,\u201d \u201cALCHY,\u201d \u201cJUNKY,\u201d \u201cSUPER,\u201d \u201cMARRIED,\u201d \u201cFELON,\u201d \u201cWORKREL,\u201d \u201cPROPTY,\u201d \u201cPERSON,\u201d \u201cMALE,\u201d \u201cPRIORS,\u201d \u201cSCHOOL,\u201d \u201cRULE,\u201d (i.e., the number of prison rule violations reported during the sample sentence) \u201cAGE,\u201d \u201cTSERVD,\u201d \u201cFOLLOW\u201d (i.e., length of the followup period) \u2022 Compas: \u201cage,\u201d \u201cpriors_count,\u201d \u201clength_of_stay,\u201d \u201cdays_b_screening_arrest,\u201d \u201csex,\u201d \u201crace,\u201d \u201cc_charge_degree.\u201d \u2022 German: \u201cgender,\u201d \u201cage\u201d, \u201cduration,\u201d (i.e., repayment duration of the credit), and \u201cpersonal_status_sex\u201d (i.e. credit given by the bank).\n# A.3 Choices of \u2206\nOur approach allows for customization of actionable features and constraints on their values. Here, we describe the actionable features and constraints we used in our experiments: \u2022 Adult: The actionable features are (i) education level, and (ii) number of hours worked per week. We require that education level can only increase. \u2022 Bail: The actionable features are: (i) education level and (ii) the number of prison rule violations reported during the sample sentence. We require that education level can only increase. \u2022 Compas: The actionable feature is the number of prior crimes. \u2022 German: Age and credit given by the bank. We require that age can only increase.8 Note that \u201cpast\u201d features like the number of prison rule violations and number of prior crimes can be treated as actionable if the individual can wait and re-apply for an outcome.\n# B Additional Experiments\n# B.1 Classifier Robustness to Realistic Noise\nIn addition to leveraging Gaussian noise to assess the brittleness of classifiers trained with our approach (see Section 4.2), we also experimented with other kinds of perturbations. Specifically, we experimented with: (A) the Natural Adversarial Examples (NAE) approach [Zhao et al., 2018], which employs GANs to generate adversarial examples that lie on the data manifold, (B) a variant of (A) that leverages GANs to generate small random perturbations that lie on the data manifold but does not explicitly optimize for the perturbations to have different labels than the original instances, and (C) the Deepfool approach [Moosavi-Dezfooli et al., 2016], which is an iterative gradient-based approach to generate adversarial examples for a given input sample. We add these perturbations to the actionable dimensions of original inputs and compute the proportion of test instances for which the classifiers are robust to the perturbations. We find that the difference in robustness (measured the same way as described in Section 4.2) between classifiers produced by our framework (\u03bb = 0.8) and baseline classifiers produced without our recourse loss (\u03bb = 0) is \u22640.03\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/644c/644cfec1-5b16-4e7f-88d7-9345b85e7ebb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: How performance and recourse vary with \u03b4max for the adult dataset. Performance: F1 Accuracy. Recourse Algorithm: \u201cgradient descent\u201d \u201cadversarial training\u201d.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a893/a8931482-8825-4aee-9147-8176c6bff71d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Recourse disparity vs. \u03bb, using the \u201crecourse all\u201d metric. A positive value ( ) indicates that whites receive recourse at a higher rate than non-whites\u2014i.e. a racial disparity exists; a negative value ( ) indicates the reverse. We show the baseline (BL) model (i.e., \u03bb = 0; also the black horizontal line) and for models trained using our approach with \u03bb \u2208[0.2, 0.4, ..., 2.0].</div>\nacross all datasets. Our results indicate that the classifiers produced by our framework are comparable in terms of their robustness to baseline classifiers even with these perturbation techniques.\n# B.2 Effect of \u03b4max\nWe also evaluate the sensitivity of our results to different choices of \u03b4max on the adult dataset. As shown in Figure 4, our training approach improves recourse rates without noticeably reducing performance for different values of \u03b4max.\n# B.3 Recourse Disparities for Minorities\nWe assess the effect of our adversarial training objective on recourse disparities for whites (which we consider to be the majority subpopulation) vs. non-whites (which we consider to be the minority subpopulation). In particular, we investigate whether our training objective worsens recourse disparities. For each of our three datasets, we compare the disparities in \u201crecourse all\u201d values of whites and non-whites between a baseline model (i.e., \u03bb = 0) and models trained with our approach (i.e., \u03bb > 0). We choose a threshold \u03b8 to fix precision at a value of 0.65, and compute recourse all separately for the majority and minority subpopulations. In this experiment, we use the \u201cgradient descent\u201d algorithm to compute recourse since it finds recourse at the highest rate. Results are in Figure 5. Overall, we find that our training approach does not worsen recourse disparities. For the compas dataset, we find that our training approach does not worsen the existing disparity in recourse rates offered by the baseline model to whites and non-whites. For the bail dataset, there is no disparity in recourse rates for whites and non-whites offered by the baseline model, and our training approach does not introduce one. For the adult dataset, our training approach in fact reduces disparity: The baseline models provide recourse to whites at a higher rate than to non-whites,\nMetrics\nBaseline\nAdversarial\n(\u03bb = 0)\n(\u03bb = 0.25)\nPerformance\nF1 score\n0.881\n0.864\nAccuracy\n0.875\n0.862\nPrecision\n0.926\n0.878\nRecall\n0.839\n0.850\nRecourse\nRecourse: neg\n0.239\n0.950\nRecourse: all\n0.656\n0.976\nFeature\nx\nx + \u03b4\n\u03b4\neducation level\n13.0\n14.397\n+ 1.397\nhours per week worked\n40.0\n42.921\n+ 2.921\nFeature\nx\nx + \u03b4\n\u03b4\nnumber of prior crimes\n2.0\n0.667\n\u20131.333\nourse computed using the \u201cgradient descent\u201d algorithm\nTable 6: Example recourse computed using the \u201cgradient descent\u201d algorithm [Wachter et al., 2017] for a model trained with our approach (\u03bb = 0.8) for a test instance from the adult dataset (top) and from the compas dataset (bottom).\nwhile models trained with our approach reduce the magnitude of the disparity or even reverse the disparity (green bars) for varying values of \u03bb.\n# B.4 NLP Case Study\nWe investigate whether our approach can be applied to another domain; in particular, we apply our approach to sentiment classification. We use the Stanford Sentiment Treebank dataset, which contains movie reviews labeled by sentiment [Socher et al., 2013]. We use the train/dev/tests in the dataset. We treat positive sentiment as the positive outcome. We train a model consisting of a linear layer on top of a pretrained BERT model [Devlin et al., 2019, Wolf et al., 2019], which we finetune for two epochs with a learning rate of 2e \u22125 and a linear learning rate scheduler. Since the covariates are discrete, we cannot use our choice of \u2206or our approach to solving for \u03b4\u2217in Eq. 2. Instead, we choose \u2206to be the set of perturbations obtained by replacing a single noun or adjective in the original sentence with one of its antonyms (obtained from a thesaurus). For each x, we approximate Eq. 2 as \u03b4\u2217= arg min\u03b4\u2208\u02c6\u2206\u2113(g\u03b8(x \u2295\u03b4), 1), where \u02c6\u2206= {\u03b41, ..., \u03b4n} is a set of candidates from \u2206(we choose n = 10), and where x \u2295\u03b4 is the result of replacing a word in x with its antonym encoded by \u03b4. For instance, for [There\u2019s no emotional pulse to Solaris], \u03b4\u2217is [There\u2019s no unexcited pulse to Solaris]. Results on the test set are shown in Table 5. Our approach increases the rate at which recourses are given without noticeably decreasing performance, offering preliminary evidence that our approach can be applied to non-tabular data using different techniques for computing \u03b4\u2217.\nResults on the test set are shown in Table 5. Our approach increases the rate at which recourses are given without noticeably decreasing performance, offering preliminary evidence that our approach can be applied to non-tabular data using different techniques for computing \u03b4\u2217.\n# B.5 Examples of Recourses\nWe show examples of recourses generated using our approach in Table 6. The top recourse says the individual should increase their education level by 1.397 years and increase the number of hours worked per week by 2.921 to receive a positive outcome of approval for a loan. The bottom recourse says the individual should reduce their number of prior crimes by 1.333 to receive the positive outcome of low recidivism likelihood prediction.\n",
    "paper_type": "method",
    "attri": {
        "background": "As machine learning models are increasingly deployed in high-stakes domains such as legal and financial decision-making, there has been growing interest in post-hoc methods for generating counterfactual explanations. Such explanations provide individuals adversely impacted by predicted outcomes with recourse, describing how they can change their features to obtain a positive outcome. Prior methods do not guarantee that actionable recourses exist, necessitating a new approach to ensure recourse for affected individuals.",
        "problem": {
            "definition": "The problem addressed is ensuring that individuals who receive negative outcomes from a binary classifier are provided with actionable recourse.",
            "key obstacle": "Existing methods do not guarantee the existence of actionable recourses, which may leave many individuals without any prescribed actions to alter their outcomes."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to provide guaranteed recourse to individuals adversely affected by model predictions, ensuring they have actionable steps to improve their outcomes.",
            "opinion": "The proposed method involves training models that are designed to ensure the existence of actionable recourses with high probability.",
            "innovation": "This approach builds on adversarial training and PAC confidence sets, distinguishing it from existing methods by focusing on the likelihood of recourse existence rather than merely computing it after the fact."
        },
        "method": {
            "method name": "Probably Approximately has REcourse (PARE)",
            "method abbreviation": "PARE",
            "method definition": "PARE is a method that guarantees the existence of recourse for most individuals with high probability, based on the distribution of inputs.",
            "method description": "PARE trains models to ensure that recourse exists for individuals receiving negative outcomes from a classifier.",
            "method steps": [
                "Step 1: Increase the recourse rate by adapting adversarial training to encourage the existence of recourse.",
                "Step 2: Guarantee recourse by selecting a decision threshold that satisfies the PARE condition."
            ],
            "principle": "The effectiveness of the PARE method lies in its ability to leverage adversarial training to ensure that recourse is likely to exist, backed by theoretical guarantees from PAC learning."
        },
        "experiments": {
            "evaluation setting": "The experiments involved four real-world datasets covering adult income, recidivism scores, bail outcomes, and credit risk, with a focus on measuring the performance and recourse rates of the proposed method against baseline models.",
            "evaluation method": "The performance of the models was assessed using standard metrics such as F1 score, accuracy, and measures of recourse rates, comparing the proposed method to baseline models that do not incorporate the recourse training objective."
        },
        "conclusion": "The proposed PARE method effectively trains models that provide actionable recourse at high rates without sacrificing accuracy, demonstrating the potential for broader application in high-stakes decision-making contexts.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its ability to guarantee the existence of actionable recourses, addressing a significant gap in current methods.",
            "limitation": "One limitation is that the method's effectiveness may vary depending on the complexity of the input features and the specific domain constraints.",
            "future work": "Future research could explore extending the PARE method to multi-class classification problems and other domains beyond binary outcomes."
        },
        "other info": {
            "acknowledgements": "This work is supported in part by NSF awards and research awards from various institutions, reflecting the collaborative nature of the research."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The problem addressed is ensuring that individuals who receive negative outcomes from a binary classifier are provided with actionable recourse."
        },
        {
            "section number": "1.2",
            "key information": "The proposed method involves training models that are designed to ensure the existence of actionable recourses with high probability, building on adversarial training and PAC confidence sets."
        },
        {
            "section number": "3.5",
            "key information": "The PARE method guarantees the existence of recourse for most individuals with high probability, based on the distribution of inputs."
        },
        {
            "section number": "5.2",
            "key information": "The primary advantage of the proposed approach is its ability to guarantee the existence of actionable recourses, addressing a significant gap in current methods."
        },
        {
            "section number": "7.1",
            "key information": "One limitation is that the method's effectiveness may vary depending on the complexity of the input features and the specific domain constraints."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore extending the PARE method to multi-class classification problems and other domains beyond binary outcomes."
        }
    ],
    "similarity_score": 0.6150164138793168,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Learning Models for Actionable Recourse.json"
}