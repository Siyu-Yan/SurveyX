{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.06915",
    "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
    "abstract": "For large language models (LLMs) to be effective in the financial domain \u2013 where each decision can have a significant impact \u2013 it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the fulldocument context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challenges may have a wide reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. The data and code is publicly accessible at github.com/anonymous.",
    "bib_name": "reddy2024docfinqalongcontextfinancialreasoning",
    "md_text": "# FinQA: A Long-Context Financial Reasoning D\n# Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai Michael Krumdick, Charles Lovering, Chris Tanner\nKensho Technologies varshini.bogolu@kensho.com\n# Abstract\nFor large language models (LLMs) to be effective in the financial domain \u2013 where each decision can have a significant impact \u2013 it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the fulldocument context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challenges may have a wide reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. The data and code is publicly accessible at github.com/anonymous.\narXiv:2401.06915v2 \n# 1 Introduction\nThe frequent need to reason over large volumes of textual and tabular data makes financial analysis particularly challenging for LLMs (Azzi et al., 2019). Existing work on automating financial numerical reasoning focuses on unrealistically specific document snippets (Chen et al., 2021; Zhu et al., 2021). Datasets are often limited to preselected document sections, failing to reflect the broader and more realistic scenarios faced by analysts (Masson and Montariol, 2020). Financial professionals often sift through hundreds of pages per document, requiring deep understanding of both the content and structure to effectively navigate and extract pertinent information. Current\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c8b/0c8b67c3-8745-4b55-a4d0-ef642582e0ba.png\" style=\"width: 50%;\"></div>\n(2) Average price per share is calculated using the aggregate price, excluding com We continued to repurchase shares of our common stock pursuant to our 2011 January 21, 2013, we repurchased an additional 15,790 shares of our common stock f 2011 Buyback. As a result, as of January 21, 2013, we had repurchased a total of app aggregate of $245.2 million, including commissions and fees. We expect to continue  response to general market conditions and other relevant factors.   26 We continued to repurchase shares of our c January 21, 2013, we repurchased an additional 15 2011 Buyback. As a result, as of January 21, 2013 aggregate of $245.2 million, including commissio response to general market conditions and other re   We continued to repurchase shares of January 21, 2013, we repurchased an additio 2011 Buyback. As a result, as of January 21 aggregate of $245.2 million, including comm response to general market conditions and o   Figure 1: DocFinQA extends FinQA to documents often over 150 pages long (100K+ tokens), so it is difficult to find the pertinent information. The question for the example above is: \u201cFor the quarter December 31, 2012 what was the percent of the total number of shares purchased in December?\u201d The correct answer is 16.5%.\nlong-document QA datasets such as NarrativeQA Ko\u02c7cisk\u00fd et al. (2018) do not test the quantitative reasoning skills needed in the financial domain. In this work, we introduce DocFinQA, a longdocument financial question answering task. We extend the FinQA dataset of expert annotated questions and answers (Chen et al., 2021) with full Securities and Exchange Commission (SEC) reports. This results in a significantly longer context in the DocFinQA dataset \u2013 by a factor of 175 \u2013 than the FinQA dataset. The resulting long-document QA task offers a more realistic evaluation of a model\u2019s reasoning capabilities over financial documents. In line with recent work on program synthesis for financial QA (Koncel-Kedziorski et al., 2023), the questions in DocFinQA are annotated with Python programs to generate the answers, allowing for training and evaluating program synthesis models for use in realistic financial workflows. Using this setup, we evaluate retrieval-based and long-context LLM systems. We study a typical retrieval pipeline that chunks and encodes the doc-\nument, searching for the best chunks given a question, and passing the question and top-k chunks to a generative QA model (Hsu et al., 2021). We also evaluate retrieval-free approaches using long-context LLMs (Weston and Sukhbaatar, 2023). Our results show that the successful employment of LLMs in financial settings requires further study of the specific nuances of the financial domain, such as context disambiguation. Our dataset represents a step towards better capturing these nuances.\n# 2 Related Work\nPrior studies in financial question answering focus on non-numerical reasoning (Day and Lee, 2016; J\u00f8rgensen et al., 2023; Maia et al., 2018). Short-context grounded numerical reasoning tasks were introduced with datasets such as FinQA (Chen et al., 2021) and TAT-QA (Zhu et al., 2021). Recently, understanding long documents has attracted more attention for tasks involving events (Yang et al., 2018), table of contents (Bentabet et al., 2020), and causal relations (Mariko et al., 2022). However, to the best of our knowledge, this is the first attempt to address financial numerical QA by grounded in long documents with upwards of hundreds of pages of context for each question. Long-document QA has been studied in NLP with the introduction of datasets such as SearchQA (Dunn et al., 2017), NarrativeQA (Ko\u02c7cisk\u00fd et al., 2018), QuALITY (Pang et al., 2022), and PDFTriage (Saad-Falcon et al., 2023). Due to the limited context size of LLMs, retrieval-based models are commonly used to filter irrelevant text (Izacard et al., 2022; Lewis et al., 2020). Recently, advances in attention mechanisms (Beltagy et al., 2020; Dao et al., 2022) and positional embeddings (Press et al., 2021; Su et al., 2023) allow for end-toend grounded QA with context windows of more than 100k tokens. However, these methods suffer from loss of important context (Zhang et al., 2023) and often fail to make full use of longer inputs (Liu et al., 2023). Our work studies the intersection of numerical reasoning and long-document processing, and our results demonstrate that there is still ample room for improvement in this domain.\n# 3 DocFinQA Dataset\nOur dataset is an extension of the FinQA dataset. (See Table 5 for an example of FinQA). FinQA was created by finance experts from S&P 500 companies\u2019 annual financial report (aka., 10-K). They\nDataset\n#Docs\n#QAs #Words Pag Num Tab\nNarrativeQA\n1,572 46,765\n63,000\n\u2713\n-\n-\nQuALITY\n381\n6,737\n5,159\n\u2713\n-\n-\nPDFTriage\n82\n908\n12,000\n\u2713\n\u2713\n\u2713\nTAT-QA\n2,757 16,552\n260\n-\n\u2713\n\u2713\nFinQA\n2,789\n8,281\n687\n-\n\u2713\n\u2713\nDocFinQA\n801\n7,437 123,453\n\u2713\n\u2713\n\u2713\n<div style=\"text-align: center;\">taset #Docs #QAs #Words Pag Num Tab</div>\nTable 1: Comparison of DocFinQA and existing Finance QA and Long Document QA dataset. DocFinQA includes multi-page documents rich with both numeric data and tables.\nused full documents when creating the questions (e.g., 100s of pages), but FinQA only includes the curated context necessary to produce correct answers (e.g., a paragraph and/or table of statistics). To recreate the original problem, we retrieve the full SEC filing for each question. Following Koncel-Kedziorski et al. (2023), we require the model to generate Python code to produce an answer instead of directly generating the numeric answer. The generated Python code illustrates what information from the context is used, along with what arithmetic operations are performed. Dataset Representation: Each question in FinQA is a triplet (cgolden, q, a) composed of a golden context cgolden, a question q, and an answer a written in human language. We extend the dataset in two ways: (1) context cgolden is extended to the full document context D; and (2) we added a Python program p that produces the answer a. Each final sample in DocFinQA is a quartet (D, q, p, a). See Appendix M for an example. Filings Collection: For each question of the FinQA dataset, we identify the corresponding SEC filing from which it was originally created. We retrieved the filing in HTML/XML format from SEC\u2019s EDGAR service and parsed text and table into clean markdown format text (Wang et al., 2023). The collection and parsing processes are presented in more detail in Appendix F and Appendix G, respectively. Figure 2 shows the distribution of document lengths in DocFinQA. Chunking and Alignment: To study retrievalbased QA systems, we split each document D into a set of chunks C = {c1, \u00b7 \u00b7 \u00b7 , cn}. Each chunk contains 2,750 characters (\u223c509 tokens) with a 20% overlap to avoid missing context at the edges. To compute the performance, we identify the best context chunk, c\u2217, from the chunk set C associated with each document D that includes the in-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/01d0/01d08f82-3a7b-4146-92fe-7ae89132ff1e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 2: Histogram of document length (#words) in DocFinQA dataset with dash line representing the average length of the documents. Purple line depicts the proportion of documents where the question context is within the current number of words.\nformation to answer question q. Since FinQA already provides cgolden, we compute a pair-wise score (ci, cgolden), for all chunks, ci \u2208C, including the golden chunk. We found that four-gram based similarity score offers the sharpest matching signal among tri-gram, four-gram, and fuzzy matching. The chunk with the highest score is selected as the target context chunk for retrieval. We verify that this process results in good c\u2217chunks through manual inspection and by substituting c\u2217 for cgolden in a few-shot QA evaluation with GPT3.5. The model scores slightly higher when using c\u2217rather than cgolden (67.5% vs 67.3% accuracy respectively), suggesting that little context is lost. Code Generation: The FinQA dataset provides solutions in a \u201cprogram\u201d syntax that, when executed, yields the answer (e.g., in Figure 1 the solution is divide(102400, 619314). However, this derivation does not provide meaningful context of what is being calculated. In our running example, 102400 is not semantically grounded to the document. Koncel-Kedziorski et al. (2023) augments FinQA with readable Python code (including named variables like, dec_shares = 102_400) that can be executed to derive the answer, providing a layer of interpretability. Thus, we use the code-enhanced version of FinQA to construct DocFinQA (See Appendix I). Statistics: The resultant DocFinQA dataset comprises 5,735 training, 780 development, and 922 test samples, derived from 801 unique SEC filings. Table 1 shows the statistics and characteristics of DocFinQA in comparison with other finance nu-\nmerical reasoning and long-document QA datasets. Due to the limited availability of complete SEC filings (refer Appendix F) and imperfections in the code generation process, DocFinQA encompasses a subset of FinQA questions. We analyze the distribution of the question types of FinQA and DocFinQA datasets (See Appendix C). DocFinQA remains a representation of all major question types from the original FinQA dataset.\n# 4 Retrieval-based QA Evaluation\nRetrieval Task We test three models for context retrieval: ColBERT (ColB)(Khattab and Zaharia, 2020), Sentence-BERT (SentB) (Reimers and Gurevych, 2019), and OpenAI\u2019s Ada (Greene et al., 2022). Further, we finetune the ColBERT model (FT ColB) on the training set of DocFinQA to evaluate an in-domain model. We test a matchingbased model, BM25 (Robertson et al., 1995), but observe poor performance (see Appendix K). To retrieve context for a question q over chunk set C, we encode both q and C with the encoding models mentioned above. This results in an embedding for the question vq and chunk embeddings VC = {vci|ci \u2208C}. We compute the cosine similarity between vq and each vector in VC to retrieve the top-k most similar chunks. We evaluate these models using hit rate (HR@k) on the test set of DocFinQA using the target c\u2217. Results are shown in Figure 3. The FT ColB yields the highest HR, followed by ColB. FT ColB yields an average improvement of 91% HR over SentB and obtains a 0.35 (HR@1) and 0.55 (HR@3). Question Answering Task We formulate the QA task as a few-shot in-context learning task (Brown et al., 2020). For each in-context example, we only provide the gold chunk and the correct answer. For the actual query we provide k chunks. A sample prompt template is given in Figure 10. We evaluate Falcon (Penedo et al., 2023), MPT (MosaicML, 2023), LLaMa 2 and CodeLlaMa (Touvron et al., 2023), Mistral (Jiang et al., 2023), GPT3.5 (Brown et al., 2020; OpenAI, 2023) models. We weren\u2019t able to evaluate proprietary models such as GPT3 (Brown et al., 2020), BloombergGPT (Wu et al., 2023) due to their inaccessibility. We skipped models that were not finetuned for code generation such as PIXIU (Xie et al., 2023) and FinGPT (Yang et al., 2023) due to their poor performance. We also skipped models trained for other languages such as BBT-Fin (Lu et al., 2023) and\nModel/Size\nColB SentB ADA FT ColB\nFalcon/7B\n2.3\n0.3\n1.2\n1.8\nMPT/7B\n4.6\n2.7\n3.8\n4.8\nMPT/30B\n17.3\n11.1\n12.1\n18.1\nLlama 2/7B\n13.5\n8.9\n11.1\n13.5\nLlama 2/13B\n18.7\n12.7\n14.9\n19.1\nCodeLlama/7B\n15.6\n12.2\n15.2\n16.8\nCodeLlama/13B\n19.1\n13.8\n18.8\n21.0\nMistral/7B\n23.2\n14.9\n21.5\n25.0\nLlama 2/7B+SFT\n32.9\n24.8\n34.3\n36.1\nGPT-3.5/-\n41.6\n33.8\n36.4\n42.6\nTable 2: Performance on DocFinQA test set. For each row, the best performance among all retrieval models is in bold. The fewshot setting is selected based on the best performance on the dev set (See Appendix L).\nXuanYuan 2.0 (Zhang and Yang, 2023). Table 2 reports the performance of 11 state-ofthe art models. Larger models outperform smaller models (e.g., MPT 30B vs MPT 7B). Models trained on code yield higher accuracy than noncode models (e.g., CodeLlama vs Llama). Models with additional supervised finetuning (e.g., LLama 2/7B+SFT) and instruction tuning (e.g., GPT-3.5) are among the best examined. Notably, Mistral 7B outperforms several larger models, although it lags behind Llama 2/7B+SFT and GPT-3.5. The FT ColB model is the best retrieval model in all but one setting. It yields a marginal but consistent improvement over the ColB, and a large improvement over SentB and Ada.\n# 5 Case Study w/ 100K+ Token Documents\nRecent LLMs can handle context lengths of 128K tokens, but more than 40% of the documents in DocFinQA remain unanswerable even at this content length (see Figure 2). Here, we evaluate performance on a test subsample of 2001 long documents, each of which has 100K or more tokens. We explore two retrieval-free options. System 2 Attention (S2A) extracts relevant information from each 100K-token chunk of a document before answering the question using the combined extracted information as context (Weston and Sukhbaatar, 2023). The Iterative method produces the output program iteratively as the LLM processes each 100k section of the document. A temporary answer program (initially \u201cNone\u201d) is input with each sec-\n1The test subsample is limited to 200 documents due to the monetary and temporal costs of human evaluation and GPT4.\nModel/Size + Method w/ Retrieval Test Subsample\nHuman\nNo\n41.0\nMistral/7B + Iterative\nNo\n11.5\nMistral/7B + S2A\nNo\n15.5\nMistral/7B + Retrieval\nYes\n20.0\nGPT-4 + Iterative\nNo\n20.0\nGPT-4 + S2A\nNo\n23.0\nGPT-4 + Retrieval\nYes\n47.5\n<div style=\"text-align: center;\">Model/Size + Method w/ Retrieval Test Subsample</div>\nTable 3: Retrieval-free performance on a case-study of 100K+ token documents.\nTable 3: Retrieval-free performance on a case-study of 100K+ token documents.\ntion to the LLM. We also report the performance of the best retrieval-based model (Retrieval) based on the experiment in Section 4. The human evaluation we conducted on these 200 questions highlights the challenging nature of this dataset.2 Non-expert human performance on DocFinQA is lower than FinQA (41% versus 50.7%). This can be attributed to the difficulty of also finding the golden page, compared to the golden page being given in FinQA. Notably, the expert performance reported in FinQA is 91.2%. Still, non-expert human performance is double that of retrieval-free GPT-4 on these long documents, and roughly triple that of retrieval-free Mistral. Secondly, Iterative performed worse than S2A for both GPT-4 and Mistral with a reduced accuracy of 3% and 4%, respectively. Third, with retrieval, both Mistral and GPT-4 outperform their retrievalfree counterparts, with the assisted GPT-4 now on par with the human cohort. Together, these results highlight that DocFinQA is an effective and difficult test for long-document QA, and that there is still room for significant improvement in this domain. For instance, further exploration into methods that combine information across multiple calls to a document-processing LLM is warranted.\n# 6 Conclusion\nThis paper introduces a realistic document-level question answering dataset over financial reports. Each question includes a full financial report (averaging 123K words), a far greater challenge than previous work that hones in on pre-specified content. Our findings reveal that this more realistic setting presents a significantly more difficult challenge, thereby opening new avenues for research in quantitative financial question answering.\n2Experienced but non-expert human participants. See Appendix A for details.\n# 7 Limitation\nThis work introduced an extension of the existing FinQA dataset. Due to limited human resources, we only validate the test set while the training and the development set were not fully validated. As a result, we can not make any claim of bias and question quality in the not-yet-validated data points offered in this paper. Additionally, as discussed in section 3, the code provided in this work was generated by WizardCoder LLMs. Our assumption is that the code is correct if it produces correct or approximately close to the golden answer. This method may generate both false positive codes (the code that generate correct answer with incorrect rationales) and false negative codes (the correct code that fail the approximation test).\n# 8 Broader Impact and Ethical Considerations\nWe do not foresee any considerable risks associated with our work given that it is an extension of a publicly available documents and dataset. To uphold transparency, the paper provides detailed documentation of the dataset creation process, including the sources of data and annotation details. Our dataset serves as a resource to underscore the need for more long context oriented benchmarks both within and outside the financial domain and does not intend to criticize any one or more LLMs. The annotation in this work is done automatically, so no crowd-source or contract annotators were hired throughout the process. The human evaluation in this study were done by full-time paid coworkers known to the authors.\n# References\nAbderrahim Ait Azzi, Houda Bouamor, and Sira Ferradans. 2019. The FinSBD-2019 shared task: Sentence boundary detection in PDF noisy text in the financial domain. In Proceedings of the First Workshop on Financial Technology and Natural Language Processing, pages 74\u201380, Macao, China.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. LongFormer: The long-document transformer. arXiv preprint arXiv:2004.05150.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. LongFormer: The long-document transformer. arXiv preprint arXiv:2004.05150.\nNajah-Imane Bentabet, R\u00e9mi Juge, Ismail El Maarouf, Virginie Mouilleron, Dialekti Valsamou-Stanislawski, and Mahmoud El-Haj. 2020. The financial document structure extraction shared task (FinToc 2020). In Proceedings of the 1st Joint Workshop on Financial\nNarrative Processing and MultiLing Financial Summarisation, pages 13\u201322, Barcelona, Spain (Online). COLING.\nNarrative Processing and MultiLing Financial Summarisation, pages 13\u201322, Barcelona, Spain (Online). COLING. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: A dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3697\u20133711, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. FlashAttention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359. Min-Yuh Day and Chia-Chou Lee. 2016. Deep learning for financial sentiment analysis on finance news providers. In 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pages 1127\u20131134. IEEE. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A new Q&A dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179. Ryan Greene, Ted Sanders, Lilian Weng, and Arvind Neelakantan. 2022. New and improved embedding model. Chao-Chun Hsu, Eric Lind, Luca Soldaini, and Alessandro Moschitti. 2021. Answer generation for retrievalbased question answering systems. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 4276\u20134282, Online. Association for Computational Linguistics. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Rasmus J\u00f8rgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott. 2023. MultiFin: A dataset for multilingual financial NLP.\nRasmus J\u00f8rgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott. 2023. MultiFin: A dataset for multilingual financial NLP.\nIn Findings of the Association for Computational Linguistics: EACL 2023, pages 894\u2013909, Dubrovnik, Croatia. Association for Computational Linguistics.\nCroatia. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39\u2013 48. Tom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328. Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, and Chris Tanner. 2023. Bizbench: A quantitative reasoning benchmark for business and finance. arXiv preprint arXiv:2311.06602. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in Neural Information Processing Systems, 33:9459\u2013 9474. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics. Dakuan Lu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, Hengkui Wu, and Yanghua Xiao. 2023. Bbt-fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark. arXiv preprint arXiv:2302.09432. Macedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. WWW\u201918 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pages 1941\u20131942. Dominique Mariko, Hanna Abi-Akl, Kim Trottier, and Mahmoud El-Haj. 2022. The financial causality extraction shared task (FinCausal 2022). In Proceedings of the 4th Financial Narrative Processing Workshop @LREC2022, pages 105\u2013107, Marseille, France. European Language Resources Association. Corentin Masson and Syrielle Montariol. 2020. Detecting omissions of risk factors in company annual reports. In Proceedings of the Second Workshop on Financial Technology and Natural Language Processing, pages 15\u201321, Kyoto, Japan. -. MosaicML. 2023. MPT-30B: Raising the bar for opensource foundation models.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United States. Association for Computational Linguistics. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for ccon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116. Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. In Proceedings of the 2022 International Conference on Learning Representations (ICLR). Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics. Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp, 109:109. Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, Ryan A Rossi, and Franck Dernoncourt. 2023. Pdftriage: Question answering over long, structured documents. arXiv preprint arXiv:2309.08872. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, page 127063. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv. Jilin Wang, Michael Krumdick, Baojia Tong, Hamima Halim, Maxim Sokolov, Vadym Barda, Delphine Vendryes, and Chris Tanner. 2023. A graphical approach to document layout analysis. In International Conference on Document Analysis and Recognition, pages 53\u201369. Springer. Jason Weston and Sainbayar Sukhbaatar. 2023. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829.\nXuanyu Zhang and Qing Yang. 2023. Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4435\u20134439.\nengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and TatSeng Chua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3277\u20133287, Online. Association for Computational Linguistics.\n# A Human Evaluation Setting\nWe recruited three data professionals with 4-5 years of experience working with financial documents, including but not limited to 10-K filings, to estimate human evaluation. The professionals were provided with the entire document in PDF format, maintaining the SEC\u2019s original format for ease of reading. They were allowed to use the keywordsearch feature of PDF reader applications and a simple calculator for basic arithmetic operations required for this task. On average, the professionals spent 25 minutes per question.\n# B Retrieval Performance\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5dd9/5dd93f2e-6d3b-46aa-9b51-7ba047b65960.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Hit rate of retrieval models.</div>\n# C Impact of Data Selection\nDocFinQA was subsampled from the full FinQA dataset based on the correctness of the code produced by an open-source LLM. This process may potentially filter out a collection of question types that were not answered by the LLM due to its limited capability. This section investigates the impact of this process by comparing the distribution of the question types in FinQA and DocFinQA. To do this, we show the distribution of questions grouped by their first 2 non-stop words in the Figure 4. The most important observation is that, overall, the distribution of the question set in DocFinQA and FinQA are very similar. There are no major groups being filtered out by our data selection process. The dominant questions (above 1% in FinQA) remains dominant and no major impact on the percentages of those questions is observed. The mid group (above 0.2% in FinQA) question sets see a mixed effect. A large portion of these questions see an increase in percentage while some experience significant loss (e.g., \u201cwhat percentual\u201d and \u201cwhat\n10 1\n100\n101\nPercentage (log)\nwhat interest\nwhat as\nwhat impact\nwhat unrealized\nwhat return\nwhat effective\ndid jpmorgan\nwhat market\nwas average\nwhat minimum\nwhat balance\nwhat anticipated\nwhat debt\nwhat gross\nwhat implied\nwhat company\nwhat largest\npercent total\nwhat greatest\nwhat combined\nwhat current\nwhat number\nwhat profit\nwhat maximum\nwhat range\nhow cash\nwhat mathematical\nwhat lowest\nwhat tax\nwhat rate\nwhat year\nwhat sum\nwhat yearly\nwhat cumulative\nwhat estimated\nwhat decrease\nwhat highest\nwhat approximate\nwhat annual\nwhat variation\nwhat percentual\nwhat expected\nwhat operating\nwhat amount\nwhat value\nwhat roi\nwhat be\nhow many\nwhat increase\nwhat difference\nhow much\nwhat growth\nwhat net\nwhat change\nwhat portion\nwhat ratio\nwhat average\nwhat total\nwhat percent\nwhat percentage\nFinQA\nDocFinQA\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9560/9560112a-2956-4c57-98d2-7f5ce6a759e0.png\" style=\"width: 50%;\"></div>\nFigure 4: Distribution of question grouped by question types in the original FinQA and DocFinQA. The x-axis (percentage) is presented in log-scale to magnify the differences between the two sets.\ndecrease\u201d). Lastly, the long tail group ( under 0.2% in FinQA) either remains the same (e.g., \u201cpercent total\u201d and \u201cwhat greatest\u201d) or is completely wiped out due to small population (e.g., \u201cwas average\u201d, and \u201cwhat return\u201d).\n# D Golden Chunk Position\nFigure 5 shows the distribution of the position golden chunk with the documents. We see that most of the golden chunks appear within the first 250 chunks (approximately 125K tokens which can be fed into the newest generative models). Nonetheless, there are a substantial number of questions that the golden chunk appear beyond this threshold.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ad4c/ad4c266c-34a0-472f-b6cc-8c3a135ea6d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 5: Histogram of position of the FinQA context in the original SEC filing that is split into chunks of size 2750.\n# E Model Details\nIn this work, we used the base models of Falcon, MPT, Llama 2, CodeLlama, and Mistral throughout our work. These models were not trained with supervised finetuning or reinforcement learning human feedback. The GPT-3.5 model employed in this study is gpt-3.5-turbo-0613 while the GPT-4 model used is gpt-4-1106-preview. We also included the Llama 2/7B + SFT that was finetuned on the training set of DocFinQA with golden chunk from FinQA (cgolden). The finetuning process takes 3 epochs with a batch size of 32. We use the context provided by the FinQA dataset as the input due to the limited maximum token length of the model. The maximum token length is set to 2048. The model is finetuned on 8 x Nvidia A100-80GB GPUs. We use AdamW optimizer with learning rate of 2e-6. The training process takes 4 hours to complete.\n# F SEC Filing Collection\nEach data point in the FinQA dataset consists of a document identification field as shown in Table 5. This field is made up of 3 sections separated by a forward slash. The first is a string called company ticker symbol, the second refers to the year in which this document was filed and the third is the page number in the document where the answer can be found. Downloading the right 10-K filing from SEC begins with identifying the company code from the company ticker symbol. For example, C/2017/page_328.pdf-1 in FinQA maps to the CITIGROUP INC with company code 831001. This mapping is obtained from the official file released by SEC which can be\nfound here https://www.sec.gov/file/ company-tickers. We automatically generate a URL using the company code obtained. From the SEC website, either filings are downloaded as TXT, HTML or XBRL using the generated URL. At this stage, approximately 6.5% (or 543) data points which is corresponds to approximately 9.4% (or 17) documents were dropped, either due to lack of mapping or nonavailability of older documents. Further, conversion of the downloaded files to PDF caused a loss of 117 data points (19 unique documents) due to formatting issues.\n# G Parsing SEC Filings\nSince each filing contains many tables, maintaining the structure and order during extraction is critical for numerical reasoning. We convert each HTMLformatted filing to PDF format and use a financespecific PDF extractor to parse the filing into markdown format. This process ensures that: (i) our dataset is grounded in the relevant financial documentation and (ii) all the tables in the filings are parsed with high precision into a consistent format without any HTML-tag noise. We explore different methods for parsing SEC filings consisting of HTML and XML markup into text and markdown tables for use in our QA systems. To evaluate parsing strategies, we measure HR@k when searching for the gold chunk among all document chunks for a single document using the FinQA question as the search query. Queries and document chunks are encoded with OpenAI\u2019s ADA model. We compare BeautifulSoup, a standard library for manipulating HTML and XML formatted data, and Kensho Extract, a finance specific text and table extraction model.3 Figure 6 shows the performance of these two methods. We see that the finance specific models used in Kensho Extract result in better downstream performance of this retrieval setup compared to Beautiful Soup. Qualitative analysis of the different parsers reveals that Kensho Extract is better at structuring the tables used in financial documents, resulting in better readability which seems to extend to the encodings.\n# H ColBERT Finetuning\nWe finetune the original ColBERT v1 model on the train set of DocFinQA. For each data point we\n3Passing the raw HTML/XML to the language model produces near-zero performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d87/9d873c41-7f5d-47a6-8984-96778f7c90d0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Accuracy for varying HR@ for two context extraction methods.</div>\nperform chunking and alignment to generate one golden chunk and n \u22121 negative chunks. For training, we generate a list of tuples (qid, pid+, pid-), where qid refers to the question, pid+ refers to the golden chunk and pid- refers to each of the negative chunks in that document. We train the model for a total of 3 epochs and store the checkpoints at the end of each epoch. The hit rate of the Finetuned ColBERT model after each epoch on the development set is shown in Figure 7. We observe that after the first epoch, additional finetuning does not show any performance improvement. The Finetuned ColBERT model referred to in this study thus uses the weights after the first epoch of training.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c666/c666ac33-2a2e-44a4-9f23-9846b103d190.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 7: Hit rate of different ColBERT variants on the development set of DocFinQA.\n<div style=\"text-align: center;\">Figure 7: Hit rate of different ColBERT variants on the development set of DocFinQA.</div>\n# I Code Conversion\nFigure 8 shows the steps of converting (a) derivation of the result in FinQA into (b) dummy Python code with dummy variable names , and finally transform it to (c) a meaningful Python program in DocFinQA following the work by KoncelKedziorski et al. (2023).\n(a)\nsubtract(34.8, 1.2), divide(#0, 34.8)\n(b) a = 34.8 \u22121.2 b = a/34.8 c = b \u2217100\n(c)\npayments_decrease = 34.8 \u22121.2 change = payments_decrease/34.8 answer = change \u2217100\nFigure 8: Example of code conversion. (a) Original FinQA\u2019s derivation. (b) Dummy Python Program (c) Meaningful Python Code in DocFinQA.\n# J Few-shot Settings\nDue to the limited context length of the LLMs, the number of few-shot demonstrations and the number of chunks fed into the In-Context Learning must be optimized. We explore 3 settings of number of few-shot examples and 4 settings of number of chunks used as context in the query. Figure 9 shows the performance of these settings in with retrieval and answered by LLama 13B and CodeLLaMa 13B on the development set. We see that a higher number of few-shot example (numshot=3) yield consistent better performance compared to a lower one (numshot=1).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0bfe/0bfefb63-eac4-4936-a9da-37643307956f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: A QA performance plot on the development set of DocFinQA for the Llama 2 13B and CodeLlama 2 13B models for each of the 12 configurations</div>\nFigure 10 shows a the prompt template with incontext learning that we used.\nContext:\n{golden chunk}\nQuestion:\n{question}\nPython Program:\n{program}\nAnswer:\n{answer}\nContext:\n{golden chunk}\nQuestion:\n{question}\nPython Program:\n{program}\nAnswer:\n{answer}\nContext:\n{golden chunk}\nQuestion:\n{question}\nPython Program:\n{program}\nAnswer:\n{answer}\nContext:\n{first chunk}\n{second chunk}\n{third chunk}\nQuestion:\n{question}\nPython Program:\nFigure 10: Prompt template with Top-3 context and 3-shot In-Context Learning.\n# K Detailed performance of retrieval method\nFigure 11 shows a pilot study comparing denseretrieval with OpenAI ADA and Sentence BERT versus sparse retrieval (BM 25) on the development set. We can clearly see that dense retrieval model offer a much higher hit ratio.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e06f/e06f71b5-4b91-40d2-b29e-ca8be7da4165.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Accuracy for varying HR@ for three search methods on the development set</div>\n# L Detailed performance on dev set\nTable 4 reports the full performance on the development set with four retrieval models and three few-shot settings. This results in a total of twelve unique configurations. For our analysis we employ ten state-of-the-art LLMs and a finetuned version of LLama2 7B. For both finetuned and pretrained models we use greedy decoding whenever applicable. For hyperparameter tuning, we carry out the extraction task on the development set of DocFinQA. One trend noted was that all generic LLMs showed higher accuracy with shorter context and more few-\nshot example i.e. top chunk with 3 shot. While the code based LLMs such as Starcoder and CodeLLama showed higher accuracy with longer context i.e top 3 chunks with 3 shot. This trend is also depicted in Figure 9.\n# M Prompt examples\nTable 5 and 6 shows examples of context, question, and model output in FinQA and DocFinQA.\n<div style=\"text-align: center;\">Upper Bound Original ColBERT Finetuned ColBERT Sentence-BERT OpenAI AD</div>\nModel\nSize\nUpper Bound\nOriginal ColBERT\nFinetuned ColBERT\nSentence-BERT\nOpenAI ADA\n*\n*\nTop 1\nTop 3\nTop 3\nTop 1\nTop 3\nTop 3\nTop 1\nTop 3\nTop 3\nTop 1\nTop 3\nTop 3\n1 shot\n3 shot\n3 shot\n1 shot\n3 shot\n3 shot\n1 shot\n3 shot\n3 shot\n1 shot\n3 shot\n3 shot\n1 shot\n3 shot\nFalcon\n7B\n2.0\n2.0\n1.9\n0.0\n0.0\n1.9\n1.3\n0.0\n1.2\n0.1\n1.3\n2.0\n0.1\n0.0\nMPT\n7B\n6.8\n6.6\n4.5\n0.8\n0.2\n4.9\n1.0\n1.2\n3.9\n0.6\n0.8\n4.3\n1.6\n2.0\nMPT\n30B\n27.1\n31.0\n15.3\n2.2\n1.7\n16.8\n3.2\n3.8\n1.1\n3.8\n2.7\n15.7\n10.4\n5.1\nLlama 2\n7B\n17.3\n22.0\n12.8\n5.8\n8.0\n14.0\n6.0\n10.3\n8.9\n2.7\n6.5\n11.2\n4.0\n11.0\nLlama 2 + SFT\n7B\n67.1\n69.7\n30.0\n32.6\n31.3\n32.2\n35.3\n33.9\n19.9\n24.1\n24.3\n28.7\n29.4\n27.7\nLlama 2\n13B\n30.0\n33.4\n14.4\n10.4\n14.1\n19.1\n11.9\n14.5\n14.9\n7.9\n10.2\n18.3\n9.8\n13.7\nCodeLlama\n7B\n26.9\n34.0\n12.6\n11.4\n16.1\n15.7\n12.3\n16.8\n11.9\n8.9\n13.2\n15.4\n14.2\n17.5\nCodeLlama\n13B\n32.1\n39.0\n19.5\n14.8\n21.5\n21.2\n15.7\n22.5\n13.2\n8.5\n16.0\n18.3\n14.4\n20.9\nMistral\n7B\n39.7\n48.8\n23.0\n18.8\n21.3\n25.9\n16.8\n25.2\n19.0\n13.6\n17.6\n20.9\n18.8\n22.1\nGPT 3.5\n-\n67.3\n67.5\n36.0\n39.0\n38.8\n38.8\n40.7\n40.2\n24.8\n30.1\n36.3\n35.0\n36.5\n36.9\nTable 4: Performance of the models on DocFinQA in one-shot and few-shot in-context learning settings for top 1 and top 3 retrieved chunk contexts on the development set. For each model, the best performance among all configurations is in bold. For each model, the best performance among different configurations for the same retrieval model is highlighted in violet. Top 1 and Top 3 indicate the number of retrieved chunks used as context for a configuration. *The single original context chunk from FinQA test set is used to estimate the upper bound.\n# ID: C/2017/page_328.pdf-1\n# Context:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dbb1/dbb160b2-22b5-4782-8b53-ecab6a7be1f8.png\" style=\"width: 50%;\"></div>\nPerformance graph comparison of five-year cumulative total return the following graph and table compare the cumulative total return on Citi 2019s common stock, which is listed on the NYSE under the ticker symbol 201cc 201d and held by 65691 common stockholders of record as of January 31, 2018, with the cumulative total return of the S&P 500 index and the S&P financial index over the five-year period through December 31, 2017. The graph and table assume that $ 100 was invested on December 31, 2012 in Citi 2019s common stock, the S&P 500 index and the S&P financial index, and that all dividends were reinvested . comparison of five-year cumulative total return for the years ended date Citi S&P 500 financials. | DATE | CITI | S&P 500 | S&P FINANCIALS | | :\u2014 | :\u2014 | :\u2014 | :\u2014 | | 31-Dec-2012 | 100.0 | 100.0 | 100.0 | | 31-Dec-2013 | 131.8 | 132.4 | 135.6 | | 31-Dec-2014 | 137.0 | 150.5 | 156.2 | | 31-Dec-2015 | 131.4 | 152.6 | 153.9 | | 31-Dec-2016 | 152.3 | 170.8 | 188.9 | | 31-Dec-2017 | 193.5 | 208.1 | 230.9 |\nQuestion:\nTable 5: Example from FinQA dataset. The context provided here has been formatted from the original dataset values.\n<div style=\"text-align: center;\">Sentence-BERT</div>\n<div style=\"text-align: center;\">OpenAI ADA</div>\nContext: Table of Contents UNITED STATES SECURITIES AND EXCHANGE COMMISSION # ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT of 1934 For the Fiscal Year Ended December 30, 2006 Commission file number 1-4171 # Kellogg Company (Exact Name of Registrant as Specified in its Charter) Delaware (State of Incorporation) (I.R.S. Employer Identification No.) One Kellogg Square (Address of Principal Executive Offices) Securities registered pursuant to Section 12(b) of the Securities Act: Title of each class: Name of each exchange on which registered: \u00b7 \u00b7 \u00b7 The Consolidated Financial Statements and related Notes, together with Management\u2019s Report on Internal Control over Financial Reporting, and the Report thereon of Pricewaterhouse Coopers LLP dated February 23, 2007, are included herein in Part II, Item 8. # (a) 1. Consolidated Financial Statements Consolidated Statement of Earnings for the years ended December 30, 2006, December 31, 2005 and January 1, 2005. Consolidated Statement of Shareholders\u2019 Equity for the years ended December 30, 2006, December 31, 2005 and January 1, 2005. Notes to Consolidated Financial Statements. # (a) 2. Consolidated Financial Statement Schedule All financial statement schedules are omitted because they are not applicable or the required information is shown in the financial statements or the notes thereto. # (a) 3. Exhibits required to be filed by Item 601 of Regulation S-K The information called for by this Item is incorporated herein by reference from the Exhibit Index on pages 61 through 64 of this Report. Pursuant to the requirements of Section 13 or 15(d) of the Securities Exchange Act of 1934, the Registrant has duly caused this Report to be signed on its behalf by the undersigned, thereunto duly authorized, this 23rd day of February, 2007. Pursuant to the requirements of the Securities Exchange Act of 1934, this Report has been signed below by the following persons on behalf of the Registrant and in the capacities and on the dates indicated. Electronic(E), | 10.48 | | IBRF | | :\u2014 | :\u2014 | :\u2014 | | | Commission file number 1-4171.* | | | 21.01 | Domestic and Foreign Subsidiaries of Kellogg. | E | | 23.01 | Consent of Independent Registered Public Accounting Firm. | E | | 24.01 | Powers of Attorney authorizing Gary H. Pilnick to execute our Annual Report on Form 10-K for the fiscal year ended December 30, 2006, on behalf of the Board of Directors, and each of them. | E | | 31.1 | Rule 13a-14(a)/15d-14(a) Certification by A.D. David Mackay. | E | | 31.2 | Rule 13a-14(a)/15d-14(a) Certification by John A. Bryant. | E | | 32.1 | Section 1350 Certification by A.D. David Mackay. | E | | 32.2 | Section 1350 Certification by John A. Bryant. | E |\nQuestion:\nWhat was the average cash flow from 2004 to 2006?\nnet_cash_2006 = 957.4 net_cash_2005 = 769.1 net_cash_2004 = 950.4 t o t a l _ n e t _ c a s h = net_cash_2006 + net_cash_2005 + net_cash_2004 average_net_cash = t o t a l _ n e t _ c a s h / 3 answer = average_net_cash\nnet_cash_2006 = 957.4 net_cash_2005 = 769.1 net_cash_2004 = 950.4 t o t a l _ n e t _ c a s h = net_cash_2006 + net_cash_2005 + net_cash_2004 average_net_cash = t o t a l _ n e t _ c a s h / 3 answer = average_net_cash\nTable 6: Examples from DocFinQA dataset with text and tables from entire SEC document as context (truncated for legibility), question, associated program and answer. A full report can be founded here https://www. annualreports.com/HostedData/AnnualReportArchive/k/NYSE_K_2006.pdf\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Existing work on automating financial numerical reasoning focuses on unrealistically specific document snippets, failing to reflect the broader and more realistic scenarios faced by analysts. Current financial research datasets only deal with short excerpts from documents, which do not capture the challenges of reasoning over large volumes of textual and tabular data.",
            "purpose of benchmark": "The intended use of the benchmark is to evaluate long-document financial question answering (QA) tasks, providing a more realistic evaluation of a model\u2019s reasoning capabilities over comprehensive financial documents."
        },
        "problem": {
            "definition": "The benchmark is designed to address the challenge of performing quantitative reasoning over long financial documents, specifically requiring models to extract and reason about information from extensive contexts.",
            "key obstacle": "Existing benchmarks are limited to preselected document sections, which do not adequately test the quantitative reasoning skills needed in the financial domain."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need for a more realistic evaluation framework that reflects the actual challenges faced by financial analysts when dealing with long documents.",
            "opinion": "The authors believe that this benchmark is crucial for advancing the state of the art in financial QA by addressing the specific nuances of the financial domain.",
            "innovation": "DocFinQA introduces a significant increase in context length compared to previous benchmarks, allowing for a more comprehensive assessment of models' reasoning capabilities over long documents.",
            "benchmark abbreviation": "DocFinQA"
        },
        "dataset": {
            "source": "The dataset was created by extending the existing FinQA dataset, incorporating full Securities and Exchange Commission (SEC) reports.",
            "desc": "The DocFinQA dataset comprises 801 unique SEC filings, with a total of 7,437 questions and an average document length of 123,453 words, significantly longer than the original FinQA dataset.",
            "content": "The dataset includes questions, answers, and Python programs that generate answers based on the full document context.",
            "size": "123,453",
            "domain": "Financial Analysis",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "HR@k",
            "aspect": "Model retrieval performance",
            "principle": "The metrics were chosen to evaluate the effectiveness of different retrieval models in providing relevant context for question answering.",
            "procedure": "Model performance is evaluated based on the hit rate of retrieving the correct context chunk associated with each question."
        },
        "experiments": {
            "model": "The models tested include state-of-the-art retrieval models such as ColBERT, Sentence-BERT, and OpenAI\u2019s Ada.",
            "procedure": "Models were fine-tuned on the DocFinQA training set, and performance was evaluated using various retrieval strategies and QA tasks.",
            "result": "The results indicated that the fine-tuned ColBERT model achieved the highest retrieval performance, significantly outperforming other models.",
            "variability": "Variability in results was accounted for through multiple trials and evaluation on different subsets of the dataset."
        },
        "conclusion": "The introduction of DocFinQA reveals that the more realistic long-document QA setting presents a significantly more difficult challenge for existing models, highlighting the need for further research in quantitative financial question answering.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive framework for evaluating long-document QA in the financial domain, addressing critical aspects of model performance.",
            "limitation": "The benchmark's limitations include the potential bias in the dataset due to unvalidated training and development samples.",
            "future work": "Future research may explore methods to improve model performance on long documents and address the limitations identified in this study."
        },
        "other info": [
            {
                "info1": "The dataset and code are publicly accessible at github.com/anonymous."
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark DocFinQA addresses the challenge of performing quantitative reasoning over long financial documents, integrating natural language processing techniques in financial analysis."
        },
        {
            "section number": "1.2",
            "key information": "DocFinQA reflects the need for realistic evaluation frameworks in financial QA, emphasizing the significance of NLP in enhancing analytical processes."
        },
        {
            "section number": "2.1",
            "key information": "The DocFinQA dataset comprises 801 unique SEC filings and includes questions, answers, and Python programs that generate answers based on comprehensive document contexts."
        },
        {
            "section number": "2.2",
            "key information": "Existing benchmarks in financial QA have been limited to short excerpts, failing to capture the complexity of reasoning over large volumes of textual and tabular data."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark reveals challenges in model performance when dealing with long documents, highlighting the need for further research in quantitative financial question answering."
        },
        {
            "section number": "6.3",
            "key information": "Future research may explore methods to improve model performance on long documents and address the limitations identified in the DocFinQA study."
        }
    ],
    "similarity_score": 0.5782445920534953,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1832_natur/papers/DocFinQA_ A Long-Context Financial Reasoning Dataset.json"
}