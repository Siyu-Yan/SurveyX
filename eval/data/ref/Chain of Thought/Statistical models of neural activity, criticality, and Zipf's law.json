{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1812.09123",
    "title": "Statistical models of neural activity, criticality, and Zipf's law",
    "abstract": "In this overview, we discuss the connections between the observations of critical dynamics in neuronal networks and the maximum entropy models that are often used as statistical models of neural activity, focusing in particular on the relation between \"statistical\" and \"dynamical\" criticality. We present examples of systems that are critical in one way, but not in the other, exemplifying thus the difference of the two concepts. We then discuss the emergence of Zipf laws in neural activity, verifying their presence in retinal activity under a number of different conditions. In the second part of the chapter we review connections between statistical criticality and the structure of the parameter space, as described by Fisher information. We note that the model-based signature of criticality, namely the divergence of specific heat, emerges independently of the dataset studied; we suggest this is compatible with previous theoretical findings.",
    "bib_name": "sorbaro2018statisticalmodelsneuralactivity",
    "md_text": "# Statistical models of neural activity, criticality, and Zipf\u2019s law\n# Statistical models of neural activity, criticality, and Zipf\u2019s law\nMartino Sorbaro, J. Michael Herrmann and Matthias Hennig University of Edinburgh, School of Informatics 10 Crichton St, Edinburgh, EH8 9AB, U. K.\nAbstract We discuss the connections between the observations of critical dynamics in neuronal networks and the maximum entropy models that are often used as statistical models of neural activity, focusing in particular on the relation between statistical and dynamical criticality. We present examples of systems that are critical in one way, but not in the other, exemplifying thus the difference of the two concepts. We then discuss the emergence of Zipf laws in neural activity, verifying their presence in retinal activity under a number of different conditions. In the second part of the chapter we review connections between statistical criticality and the structure of the parameter space, as described by Fisher information. We note that the model-based signature of criticality, namely the divergence of specific heat, emerges independently of the dataset studied; we suggest this is compatible with previous theoretical findings.\n# 1 Introduction\nThe debate about criticality in neural systems began with the observation of power laws in a number of experimentally measured variables related to neural activity. The first experimental observation of neuronal avalanches [6] found that their size distribution follows a power law with exponent of about \u22123/2, and their duration distribution follows one of exponent near \u22122 in cortical slices. These values are compatible with the exponents expected in critical branching processes \u2014 a well-studied topic in the field of complex systems physics [2]. Similar observations have been consistently reported in literature; moreover, the presence of power-law avalanche statistics was found to be theoretically justified by functional arguments on numerous occasions [5, 43, 42, 12, 49], and was shown to differ in different brain states [39, 15]. For an excellent high-level discussion of the topic, see [7]. An equally interesting instance of a power law is the finding that, in the population statistics of a neural network\u2019s activity, the rank of a state (the first being the\nmost frequently observed, and so on) and its frequency are inversely proportional. This phenomenon, known as Zipf\u2019s law, was first observed by Auerbach in 1913: \u201cIf one sorts individuals by a given property in a descending fashion and stops doing so at rank n1, or at n2, or generally at rank nx, where the property has gone down to values p1, p2, px, then a certain law exists between nx and px. In our case, this law is especially simple, it is expressed by the formula: nx \u00b7 px = constant\u201d [3]. This author already alluded to the possibility of more complex forms of the same law (e.g. for the distribution of wealth), but did not speculate why it assumes its simplest form in the studied data. In the mid-1930s, the American linguist George Kingsley Zipf discovered that the frequency of occurrence of words in Joyce\u2019s Ulysses and American newspapers follows the same law [52], which is today called Zipf\u2019s law: the frequency of each word decays as a power law of its frequency rank. After Auerbach\u2019s original example, city sizes [10, 22], Zipf\u2019s law was confirmed in a variety of fields, including citation counts in scientific literature [40], earthquake magnitudes, wealth, solar flare size, number of emails and phone calls received, and many others [33] . Over the years several attempt have been made to understand Zipf\u2019s law. Zipf himself explains it by the principle of least effort: If words are stored in a linear array, then the low-frequency items are optimally located in a more distant place than more often used ones. The product of distance and frequency can be considered as a measure of the effort necessary to retrieve the word which he claims to be a constant. However, the assumption of an array, where the effort needed for retrieving an item is linear, which is necessary in order to obtain an inverse relationship rather than a general power law, seems unnatural when considering how items are stored in a neural network. Another potential cause for the law can be seen in the idea of preferential attachment [4]. If, for example, the probability to move to or away from a city is assumed to be independent of its size, then Zipf\u2019s law for city sizes emerges. Other assumptions have been discussed and been shown to provide a better match for the distribution of city sizes [50], but again this may not easily carry over to states in a neural network. Li [26] demonstrated that the words of an artificial language that simply consists of randomly chosen letters including a space sign tend to obey Zipf\u2019s law. However, the \u2018space\u2019 sign, which separates words in Li\u2019s approach, plays no such role in the analysis of neural data. The authors of Ref. [1] aim at an explanation of Zipf\u2019s law by the existence of latent variables. Differently from the above attempts, this study is directly relevant for the analysis of neural activity. It also subsumes the scheme proposed by Li [26]. Although all of these attempts have their interest, there is some agreement that a deeper understanding is still lacking. In addition, there seems to be no clear justification on why criticality in the statistical sense and Zipf\u2019s law have been observed in neural data, or what brain function might benefit from it. It is interesting in this context that Zipf\u2019s law is a system property, i.e. it depends on the number of elements in the system and does not automatically apply to subset or unions of Zipfian sets. It can not be reduced to the mere presence of a particular probability distri-\nbution (such as P(x) \u223cx\u22122), but requires a conditional sampling procedure to be reproduced in a simulation [8]. The observations in neural data as well as a number of unsolved problems with this subject make it a very interesting subject of further investigation. In what follows, we will discuss the connections between the observations of critical dynamics and maximum entropy models that are often used as statistical models of neural activity, reviewing the recent literature on the matter, and debate the possible relationship between this statistical concept of criticality and the dynamical criticality related to avalanche statistics. First, we will illustrate the concept of Zipf\u2019s distribution, its origin, and its applicability to neural data. In section 2, we will introduce maximum entropy models, and show their connection to criticality and Zipf\u2019s law. In section 3, we will make three observations that emphasise the difference between statistical and dynamical criticality: (3.1) a system that shows dynamical, but not statistical criticality, (3.2) the process of fitting an energy-based model, and (3.3) the application of the theory of a large-scale corpus of biological data, where Zipf\u2019s law appears to hold, although the system is not dynamically critical. In section 4, we will show connections between statistical criticality and the structure of the parameter space, as described by Fisher information. Finally, in Section 5, we will return to the question debate whether there is a relationship between statistical and dynamical criticality and conclude with an outlook on the problem.\n# 2 Statistical description of spike trains\n# 2.1 Zipf\u2019s law in neural data\nFor the specific case of neural activity, Zipf\u2019s law refers to the rank-probability law for the occurrence of each possible pattern of activity, which has been observed to follow a power law in the same sense as for words in the English language [48]. To understand what we mean by pattern or state, we need to adopt a simplified way of representing spike trains that we can call digital: discretising time in bins of equal size \u03b4t, we can define a Boolean variable\n\u03c3n(t) = \ufffd 1 if neuron n spikes between t and t +\u03b4t 0 otherwise.\nAt any given time, then, the population activity is described by a co\n\u03c3(t) = (\u03c31(t),...,\u03c3N(t))\nwhich describes, up to a precision of \u03b4t, the spiking state of the N neurons considered (Figure 1). Modelling the system statistically, in this framework, means giving a full account of the probability of each possible codeword to appear. Note that, typically, we are not concerned with the dynamics of the system, and we disregard\n(1)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68ba/68ba3dbb-e67d-4b46-a9cb-6b39259f1bfa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1 Digitisation of spike trains from 10 neurons into a boolean matrix with bin size \u03b4t = 20 ms.</div>\ntemporal correlations on scales larger than \u03b4t: this approach is suited to describe short-time correlations across space or properties of the encoding. Needless to say, the choice of \u03b4t can have important consequences on the results: in the limit of very large bin size, the pattern where all neurons fire simultaneously will be the only one to be observed; in the opposite limit of small \u03b4t, the silent pattern will be the most common, patterns with a single active neuron arbitrarily rare, and multi-neuron patterns absent. The results we discuss hold for bin sizes of the order of 5\u201320 ms, i.e. of the same order of magnitude as the typical correlation length between neurons; this value is commonly adopted in the literature [41]. To understand why Zipf laws are considered a signature of criticality, we will now illustrate the relationship, exposed by recent literature, between them and the critical points of models that have been used to describe neural activity, and are well known in physics.\n# 2.2 Statistical modelling\nThe activity patterns of individual neurons and neural networks invariably display stochastic characteristics. A common approach, which we can call top-down, of modelling the nature of this activity is to make (simplifying) assumptions on the actual workings of neurons, synapses, and networks, in order to set up a computational model the results of which can then be compared with experimental observations. A large part, perhaps the largest, of computational neuroscience is based on this paradigm, predominantly by simulations of spiking neural networks.\nHere, on the contrary, we are concerned with what we call bottom-up modelling, which seeks to infer properties of neural activity in an entirely data-driven way. Understanding the correlation structure, the distribution of firing rates, or the repetition of identical patterns from experimental data are examples of this approach. In other words, the data is described in terms of probabilities and other statistical descriptors, instead of parameters directly implied by the biological or physical theory. In the bottom-up approach, a very broad family of models is available. We will restrict ourselves to energy-based statistical models, a number of models developed in the last decade which adopt a log-linear relation between probability and state variables. In an energy-based model probabilities are expressed in terms of an energy function E, in analogy with statistical physics:\nwhere Z is the relevant normalisation factor. Many energy-based models used in neuroscience adopt the aforementioned digital description of spike trains in terms of binary variables: we will focus on these. In this case, for N neurons, \u03c3 can take 2N different values, and determining the full population probability distribution requires specifying 2N probabilities, which is an unrealistic task even for modest population sizes. Assumptions on the analytical form of the distribution are therefore required in order to infer a complete distribution from a relatively small number of samples.\n# 2.3 Maximum entropy models\nThe first, and perhaps more elegant, strategy developed to this end is to adopt a maximum entropy approach [21], in which one first selects what features of the data should be exactly reproduced, and determines then the highest-entropy probability distribution consistent with those constraints. Schneidman et al. [41] and Shlens et al. [44] first applied this approach to neural data, using a Pairwise Maximum Entropy (PME) model, which exactly fits all \u27e8\u03c3i\u27e9and \u27e8\u03c3i\u03c3j\u27e9, i.e. firing rates and pairwise correlations. Indeed, the question behind that research was primarily related to the importance of correlations in the vertebrate retina, including the study of higher-order interactions. The PME probability distribution over all codewords has the following form:\nThe expression above is mathematically identical, in statistical physics, to that of the canonical ensemble for the Ising model with arbitrary couplings, a generalisation of the model originally used to describe ferromagnetism in solids [20]. By definition, a successful fit of a PME model correctly reproduces all firing rates and pairwise correlations present in the data from the considered neural population.\nFitting based solely on second-order statistics does not imply that third-order correlations and other statistical measures are correctly reproduced. Reports that higherorder correlations are largely irrelevant were thus very surprising [41, 44, 46], although these observation may be restricted to low activity and high pairwise correlations [51]. Assessing whether a maximum entropy model can capture additional statistics of the data provides a source of interpretability: If, say, a PME model can account for third-order correlations, then the latter are not constrained further by the data. If, conversely, third-order correlations diverge from the PME prediction, we learn that the neural activity uses higher-order statistics to encode information. Whether this is the case depends on the system and on the distance between the neurons considered [37]. Several attempts have been made at improving the quality of the fit of statistical models, using different features as known statistics. The generalisation of equation (2) \ufffd \ufffd\ncan describe any probability distribution of binary variables exactly. However, finding the values of J(n) is computationally expensive and the benefits typically do not outweigh the costs for n \u22653. As a different way to assess at least some aspects of the higher-order statistics, we can consider, for instance, the probability distribution of the number of neurons firing in a time bin, p(K), where K(t) = \u2211N i=1 \u03c3i, was used as a target. This can be introduced as a further constraint in a maximum entropy model in combination with firing rates and pairwise correlations, leading to the K-pairwise model [47, 31]. It typically produces significantly better fits than a pure PME, and is much less computationally expensive than attempting to fit higher order cumulants. Another related approach, the population tracking model, fits p(K) together with the conditional probabilities P(\u03c3i = 1|K) of each neuron firing, given the current population firing rate, providing a lightweight and interpretable model [36]. An example of an energy-based model which does not rely on the maximum entropy principle, finally, is to use a restricted or semi-restricted Boltzmann machine (RBM/sRBM). Despite not directly aiming at fitting correlations, p(K), and cumulants, as a maximum-entropy model would, RBMs were shown to perform at least comparably well in fitting all these aspects [24]. An advantage is that their complexity can be tuned, offering a choice of various degrees of accuracy and the corresponding computational costs. Additionally, contrastive divergence, the algorithm used for fitting, is an approximate but relatively fast and reliable algorithm, which lets one fit the simultaneous activity of a large number of units (up to several hundreds, whereas the exact learning algorithm for a PME model is not usable in practice over N \u224840, although more efficient methods have been studied). Finally, RBMs can be interpretable models, specifically by studying the roles taken by hidden units. Although a detailed discussion is beyond the scope of this chapter, we should at least mention the efforts to reproduce the time dynamics of the system, so that\nStatistical models of neural activity, criticality, and Zipf\u2019s law\nthe statistical model fits both the distribution of single-time bin patterns and the conditional distribution of the pattern given the pattern in the previous time bin [28, 32, 11].\n# 2.4 Phase transitions in models\nAlthough this may initially seem less relevant from the point of view of research in neuroscience, we should remind ourselves that the Ising model is one of the earliest and most commonly studied paradigms of a phase transition. To understand its behaviour, let us make the temperature dependence of equation (2) explicit:\nNote that, in the high temperature limit, this converges to a uniform probability:\nConversely, when T \u21920, only a small number of states with non-zero probability survive, the others becoming infinitely rare. If a system that obeys PT\u21920(\u03c3) is perturbed in any way, it will eventually converge to this stable set, under any reasonable dynamics. In other words, the distribution becomes, in the zero temperature limit, a finite set of stable attractors, the same as the stationary distribution of a Hopfield network [19], where the attractors play the role of memory patterns. Clearly, neither of the two limiting cases can be a realistic description of neural statistics, and the truth lays in between them, in a regime where the model is much more informative. The physics literature shows that there is sharp phase transition between a disordered phase and a spin glass phase, with the exact location of the critical point depending on the statistics of h and J [34]. It is then a natural question to ask whether the Ising model that results from a fit to neural activity is in one of the two phases, or poised near the critical point, and whether this relates to other concepts of criticality in neural systems. The divergence of specific heat, also called heat capacity, in a macroscopic system is a classic signature of discontinuity in the properties of the system upon variation of a single parameter, typically temperature (generalisations of this idea will be discussed in the next section). The most classic example is the case of a change in the state of matter \u2014 solid to liquid, liquid to gas, etc. \u2014 where an infinitesimal change in temperature through the critical point requires a finite amount of energy (the latent heat). This is equally true for spin systems of the form we examined above. Tka\u02c7cik et al. [48] fitted a model of the form (2) to binned spike trains from recordings of the salamander retina subjected to movies of naturalistic stimuli. They varied the temperature of the model around T = 1 (this value corresponding to the\n(3)\nfit to neural data), and studied the specific heat as a function of T for an increasing number of neurons. Their result clearly showed a peak in the specific heat of their models, with the peak temperature approaching T=1 as N is increased. This is evidence that T=1 coincides with the critical point, and therefore, the model is poised at criticality for parameter values exactly corresponding to those that fit the neural data. Similar observations were independently repeated, e.g. in [31, 35, 15], generating a debate on the nature of this observation and its biological interpretation, as will be discussed in later sections.\n# 2.5 Model criticality and Zipf\u2019s law\nZipf laws can be related to statistical criticality in the sense of models, as shown in [48] (supplementary information), as follows. Call p1,..., pk,..., p2N the probability of occurrence for each of the 2N possible codewords. In statistical physics, microcanonical entropy can be defined as S = log\u2126, where \u2126= \u2126(E) is the number of states with energy lower than E. On the other hand, the energy level associated with a pattern is a function of its probability:\nNow, Zipf\u2019s law states that, for every pattern, its rank rk \u221d1/pk. In the notation used above, note that rk = \u2126(Ek). Therefore, Zipf\u2019s law implies\nIf the above linear relation holds, then d2S/dE2=0. Since both specific heat and the variance of energy are inversely proportional to d2S/dE2, these thermodynamic quantities diverge. This is the classic signature of a second order phase transition. The rank-probability relation defined by Zipf\u2019s law, therefore, is a model-independent way of showing criticality in this statistical sense. Its appearance guarantees the divergence of the specific heat of a PME model fit to the same data, but does not require complex and computationally expensive fitting procedures, and relies only on the statistical properties of the data.\n# 3 Statistical and dynamical criticality\nAs we have mentioned, most energy-based models do not account for dynamics, as they are concerned only with fitting a single-time bin distribution. The formulation of the Ising model in physics describes a stationary distribution and does not include any dynamics. Transition probabilities from a state to another can be added\n(4)\nthrough additional assumptions about the dynamics of the system. For instance, Glauber dynamics [13] generates a Markov chain whose stationary distribution coincides with the distribution (2). This is useful, for example, when sampling states from that probability distribution. Avalanches can be observed in high-dimensional Ising systems when they are driven out of stationarity by a change in temperature or applied magnetic field. Therefore, it is not expected that maximum entropy models reproduce any aspect related to avalanche dynamics. It is not clear a priori, then, whether the observation of Zipf laws and diverging specific heats should be related to power-laws in the dynamics. In fact, finding a connection between the two concepts seems challenging. In the next sections, we will provide examples of how the two might be entirely distinct, which prompts questions on the nature, meaning, and relevance of statistical criticality.\n# 3.1 The Eurich model is dynamically, but not statistically c\nFor a discussion of the relationship between the two concepts of criticality, it is interesting to consider the Eurich model for neural avalanches as a \u201ctestbed\u201d [9]. It is mathematically well-understood and can be conveniently tuned, because the parameter values for (quasi-)critical as well as sub- or super-critical behaviour are known analytically (even for finite systems). The very definition of the model is such that all neurons have identical properties, and the same for pairs, triplets, etc., of neurons. As a consequence, all N patterns with exactly one active neuron appear with the same frequency; all N(N \u22121)/2 patterns with exactly two active neurons appear with the same frequency, and so on, giving the rank-probability plot a steplike appearance, which cannot follow a Zipf law (Figure 2). It is tempting to consider the tail of the rank distribution. Although the number of states increases with the activity (for neurally plausible activity levels), their probability decreases strongly if the firing rate (per time bin) is low. Therefore, steps will disappear for higher ranks, which may or may not produce a power-law-like behaviour. However, in the statistical approach, typically small values of N\u03b4t (see equation 1) are used, such that the potentially Zipf-like tail (figure 2, right) will be statistically irrelevant. It has also been claimed that the statistical approach is most likely restricted to low-activity patterns [51]. Note that the rank curve would be less step-like if some form of heterogeneity is introduced, as opposed to the complete symmetry between neurons that characterises the Eurich model. However, this would amount to an additional assumption that is, as the Eurich model shows, not necessary for criticality. In particular, we would need to assume that the patterns with a single active neuron already follow Zipf\u2019s law. This is a particular, but not unreasonable assumption, as this can be expected, for example, in a scale-free neural network. In fact, we cannot rule out that a Zipf profile, or at least an approximation, could be found just by tuning the distribution of firing rates, even in the absence of correlations. Such a finding would entirely rule out any relation to dynamical criticality, which appears exclusively as\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f1de/f1de9e7b-1228-4d03-8165-4f3db2b8196a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2 Avalanche statistics compared to Zipf law for the Eurich model in different regimes [9]. Here N = 100, and the critical point is at \u03b1c = 0.90: in this case, the avalanche size and duration distributions most closely approach a power law with exponent \u22123/2, except for a cutoff due to the finite size. The subcritical case (blue), on the other hand, shows short-tailed distributions, and the supercritical cases (green and red) exhibit respectively one or many peaks at large values. In all cases, the Zipf plot does not show power-law dependence. Note that the smoothing of the step-like function is due to the finite sample size.</div>\nFig. 2 Avalanche statistics compared to Zipf law for the Eurich model in different regimes [9]. Here N = 100, and the critical point is at \u03b1c = 0.90: in this case, the avalanche size and duration distributions most closely approach a power law with exponent \u22123/2, except for a cutoff due to the finite size. The subcritical case (blue), on the other hand, shows short-tailed distributions, and the supercritical cases (green and red) exhibit respectively one or many peaks at large values. In all cases, the Zipf plot does not show power-law dependence. Note that the smoothing of the step-like function is due to the finite sample size.\na consequence of emergent phenomena deriving from complex interactions. However, it would still require a specific distribution of firing rates among the neurons, an assumption that in itself would prompt questions about its functional reasons. Firing rate distributions have been studied extensively [30], and found to be highly skewed, with a small fraction of neurons responsible for the majority of emitted spikes. A clear theory on why this is an advantage for the encoding is still missing. In section 3.3, we will consider a case in which the Zipf relation holds even when correlations are destroyed, which suggests the long-tailed firing rate distribution is sufficient for it to hold.\n# 3.2 Fitting energy-based models to critical activity\nA natural way of checking if dynamical and statistical criticality are related could involve fitting a statistical model to neural models that exhibit various kinds of dynamics, and can be tuned to a supercritical (noisy), subcritical or critical regime. This was one of the goals of in Ref. [15]. The authors identified five different dynamical states of the cat and monkey cortex, studied their avalanche statistics, and evaluated the temperatures corresponding to peak specific heats of Ising models fitted to each dataset. The results did show a small but significant relationship between avalanche dynamics and specific heat peak location. However, it should be noted that some of the results in this work were obtained with small datasets of six neurons only, which may not offer insight on what happens in the thermodynamic limit. We attempted a similar task fitting Bernoulli RBMs to the activity generated by a tunable model of a neural network, similar to the binary, non-leaky, integrate and fire model used by [12]. In this model, the strength of inhibition can be tuned,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f054/f0545a8b-1161-4178-bcb0-ba283352ff14.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3 RBM specific heats peak when fitted to a variety of datasets, with the peak approaching the temperature of the fit as the model size increases. RBMs were fitted to simulated data with different avalanche statistics: supercritical (left), critical (centre) and subcritical (right).</div>\nleading to a network with low, random-looking activity and a short-tailed avalanche distribution (high inhibition); or a network generating activity in large bursts (low or no inhibition). The critical regime lies in between the two. Details about the implementation of the model are given below. We found that although the absolute value of the peak does depend on the correlations of the data, its location is always at a temperature near T = 1 (which is the value corresponding to the original fit), and further approaches this temperature as the number of units increases, i.e. in the thermodynamic limit. These results are compatible with what was shown by [35], using a different dataset, for the Kpairwise model. It seems, then, not only that the statistical model that we fitted does not accurately detect criticality in the dynamical sense, but it also exhibits statistical criticality no matter the dataset it was fitted to. This implies, on the one hand, that the dynamical criticality of a dataset and the statistical criticality of a model fitted to it are unrelated, and, on the other hand, that a model fitted to datasets of very different nature all tend to exhibit statistical criticality. This is compatible with an argument that was put forward by theoreticians [29], as will be discussed in section 4.2.\n# Methods: network model\nThe binary neuron model used for the simulations is similar to the one presented by [12]. In this model, each neuron has a probability of firing given by a weighted sum of its inputs, divided by a factor dependent on its own firing history. A fifth of all neurons are inhibitory, while the rest are excitatory; the value of inhibition was tuned to 0.0, 1.0, or 2.0, to enforce different regimes, corresponding to different correlations and avalanche statistics. While in the original work the connectivity was all-to-all, with weights drawn from a uniform distribution, we modified it to identical couplings, but set on a network with scale-free degree distribution. This enforced larger variability of firing rates between neurons; both experimental evidence and the theory in [25] suggest this choice does not affect the location of the\ncritical point. We simulated 1000 neurons, from which we took subsets of the sizes required for analysis, for 1 million time steps (conventionally taken to equal 1 ms). The resulting activity was re-binned in 5 ms bins, to reduce sparseness.\nMethods: RBM specific heats\nAs we will argue in section 4.2, the direction that best indicates the critical point coincides with the fist eigenvector (the one corresponding to the largest eigenvalue) of the Fisher information tensor. However, in practice, this is never orthogonal to the direction of increasing/decreasing temperature: thus, varying temperature is an acceptable way to look for a phase transition. In statistical physics, the general expression for the probability of a pattern in an energy-based model at temperature T is\nThe expression for the energy in the case of RBMs is\nE(v,h) = \u2212a\u22bav\u2212b\u22bah\u2212v\u22baJh.\nWhere v and h are vectors of visible and hidden binary variables respectively. Since this expression is linear in the parameters ai,bj and Jij for all i, j, changing the temperature of a model coincides with rescaling these parameters by a linear factor \u03b2 = 1/T. In the following, we have adopted the standard strategy of fitting an RBM to neural data, obtaining values for its parameters, and then rescaling them \u2014 this means T = 1 (no rescaling) coincides with the parameters as they were fitted, the values corresponding to a model that correctly reproduces the given data. Fits were obtained by 1-step persistent contrastive divergence. We can then compute the specific heats at different temperatures. The marginal probability of v is\nDisregarding an additive constant, the energy of a visible pattern can be expressed as the logarithm:\nIn accordance with statistical physics, we can define\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8a0a/8a0ad53f-8fc5-4a4e-8cb9-9c403f133d84.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ed1/1ed119ca-17e1-44b1-bba5-cfc57723bd71.png\" style=\"width: 50%;\"></div>\nFig. 4 Avalanche statistics compared to Zipf law in the neural activity of a healthy, adult (postnatal day 91) mouse retina stimulated by projection of a white noise checkerboard pattern. The detection of avalanches and the pattern count were repeated over 30 sets of 100 neighbouring neurons, each of which was recorded for 20 minutes. The sets may overlap. The solid lines are medians over these sets; the shaded area is delimited by the first and third quartiles. The grey line in the rightmost plot is for comparison with Zipf\u2019s law. The data were made available by G. Hilgen and E. Sernagor, University of Newcastle. We refer to [18] for experimental and data analysis methods.\nThis quantity can be computed from a sample. For each temperature value, in each dataset, we extracted 20 chains of 2000 samples, taken every 10 steps in order to reduce spurious correlations, and computed c(T) by the expression above.\n# 3.3 Retinal activity obeys Zipf\u2019s law, but is not dynamically critical\nThe mammalian retina, a system that is often chosen when studying the statistics of neural activity, and whose encoding and dynamical properties are well known, is an example of the opposite case: It was the first system in which statistical criticality was observed, but it does not exhibit dynamical criticality. Avalanches arise in the mammalian retina only during the period of development: for mice, in the first few days after birth, before eye opening, when the retina does not respond to light and the network activates spontaneously. During this stage, the activity of the retina consists of the so-called retinal waves, which are effectively power-law distributed avalanches. Direct comparison with a computational model showed that these are indeed the signature of a critical state between locally and globally connected activity [16]. However, these disappear in a functional retina: Figure 4 shows the statistics of a 20-minute recording of an untreated, adult mouse retina under an uncorrelated black-and-white checkerboard stimulation. It is evident that the avalanche statistics is short-tailed, and, at the same time, the probabilityrank plot of pattern frequencies is well compatible with a Zipf law. Note that correlations between the activities of retinal ganglion cells change significantly with the statistics of the stimulus, and the avalanche statistics will consequently appear different. The example of adult retinas is complementary to Section 3.1 in the sense that, here, a system that does not show dynamical criticality can well obey Zipf\u2019s law.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/392e/392e6194-bb14-4d5d-8bdc-7b407d2491a8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5 Examples of stimulation frames. Correlations increase from left to right (dataset ID 0 to 3): the frequency spectra follow f \u2212a with a = 0.5,1.0,1.5,2.0, i.e. from noise to the statistics of natural images. The correlation statistics extend to time.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3dd8/3dd8366d-257b-4b4e-9068-1244e608c508.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6 Left: Zipf plots, before and after treatment with bicuculline. 30 groups of 100 neurons, selected as explained in the Methods paragraph. Centre: Zipf plots for a unique group of 100 neurons under stimuli of different statistics; the difference between datasets 0-3 consist in the different spatial frequency \u2014 from near-white noise to natural stimulus statistics. Right: the same data as in the left panel (control), and its shuffled version, where correlations have been destroyed, while keeping the same firing rates. The red dashed lines correspond to 1/x laws.</div>\nFig. 6 Left: Zipf plots, before and after treatment with bicuculline. 30 groups of 100 neurons, selected as explained in the Methods paragraph. Centre: Zipf plots for a unique group of 100 neurons under stimuli of different statistics; the difference between datasets 0-3 consist in the different spatial frequency \u2014 from near-white noise to natural stimulus statistics. Right: the same data as in the left panel (control), and its shuffled version, where correlations have been destroyed, while keeping the same firing rates. The red dashed lines correspond to 1/x laws.\nIt is worth mentioning that the observation of Zipf law in retinas is very robust to a number of external factors. We found no significant differences in the rankfrequency plots of patterns observed when the retina was treated with bicuculline (a GABA blocker) compared to a control; analogously for retinas under stimuli characterised by very different level of spatial correlations. If Zipf\u2019s laws have a functional role, there is no expectation this phenomenon would survive in a non functioning neural system, such as a retina that has been pharmacologically treated in a way that breaks its normal operative mode. Here, we took data from the same mouse retina, before and after treatment with a 20 \u00b5M solution of bicuculline, which is a GABAA antagonist. The results are shown in figure 6 (left): as it is evident, there is no clear difference between the two rankprobability plots. Of course, the only strong argument against the functional role of Zipf laws would be finding a functional retina in which this law is broken, which is not the case here. However, we can notice that even an intervention that significantly disrupts the retina\u2019s activity, by blocking inhibitory interactions, doesn\u2019t prevent this phenomenon to arise. This is despite the large change in the correlation between neurons induced by bicuculline.\nLikewise, one may expect a dependence of pattern frequency-rank statistics on stimulus statistics. The retina, after all, is a neural system design to encode a stimulus \u2014 and the correlation structure of its neurons\u2019 activity strongly depends on the correlations in the stimulus. However, we found no significant difference in Zipf laws under different stimulations. Figure 6 (centre) shows a single group of 100 neurons selected in a retina that was stimulated with light patterns of different kinds. All stimulus presentations consisted of black-and-white random checkerboards, which are binarised versions of random noise of given frequency spectrum f \u2212a with a = 0.5,1.0,1.5,2.0 in space and time: from near-white noise to the statistics of natural images (figure 5). The independence from correlations is evident in the right panel of figure 6: here, the \u201ccontrol\u201d curve is the same as in the left panel, and is compared with the rank-probability plot for a \u201cshuffled\u201d version of the same data, where the firing rates were kept the same, but spikes were moved in time in order to cancel neuron-neuron correlations. The difference between the two curves is clearly not significant. This demonstrates how a firing rate distribution which is long-tailed (approximately lognormal) can in itself produce a Zipf-like plot. More research is needed to show whether this holds in general.\n# Methods\nThe handling of the retinas, experimental apparatus, and the first part of the data analysis pipeline were performed as illustrated in [18]. Starting from detected and sorted spikes, we removed those with very low amplitudes, by selecting a threshold corresponding roughly to the lowest 10%. This was to ensure only good-quality events were left. Then, we selected, for each Zipf plot, N = 100 clusters all pertaining to the same area of the retina (figure 7). At this stage, spikes were binarised into a N \u00d7 T matrix S of boolean variables, with S(n,t) = 1 if neuron n spiked between times t and t +\u03b4t and S(n,t) = 0 otherwise. When multiple spikes from the same neuron occurred in a single time bin, the extra spikes were disregarded. For recordings shown in this chapter, T = 120000 or more, and \u03b4t = 10 ms, implying at least 20 minutes of neural activity were recorded.\n# 4 Parametric sensitivity\nThe basic fitting procedure of a maximum entropy model minimises the quadratic difference between the data moments and the moments predicted by the model. During fitting, any model is updated by exploring the parameter space, following a direction given by the loss function. When a model admits a phase transition, the parameter space is characterised by (at least) two regions, corresponding to the phases, separated by a critical surface. From a theoretical point of view, asking why a model is poised at criticality coincides with asking why the fitting process tends to\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3360/3360029e-f4d1-43db-8d3d-2a11770a4a2c.png\" style=\"width: 50%;\"></div>\nFig. 7 All spike clusters in a dataset (P91 mouse retina under white noise checkerboard stimulation), arranged spatially. For Zipf analysis, a random cluster was selected, and the 100 nearest ones picked along it (coloured patches on the figure are examples) to form a 100-neuron group. The process was repeated 30 times to study error intervals. The size of the dot scales with the number of spikes in the cluster. Even if this image only represents detected spikes, the optic disc is noticeable at the bottom end; other inactive areas corresponds to cuts in the retina, unavoidable when placing it on a flat surface.\nlead towards the critical surface in the parameter space. This has been discussed by [29]; before introducing their argument, we provide some theoretical background.\n# 4.1 Model distance\nIntuitively, a phase transition occurs at a location (the critical point or critical surface) where an arbitrarily small change in the parameters yields a sharp, qualitative change in the behaviour of the model. In this section, we will formalise this idea, and link the notion of model distance to the statistical physics framework that we have introduced above. A common measure of the distance (in model space) of a probability distribution p from a given one q, both defined on a set S, is the Kullback-Leibler divergence\nIt measures the amount of information that is lost when approximating p by q. The name divergence stresses that this quantity does not have the mathematical proper-\nties of a distance, namely not being symmetric. If the model space is parametrised by \u03b8, and p and q are close to each other in this space, so that q = P\u03b8 and p = P\u03b8+\u03b4\u03b8, at second order in \u03b4\u03b8, the divergence can be approximated as\nwhere F is called Fisher information tensor (FIT) which is given by\nFisher information is here expressed as a statistical quantity, but it has an important relation to the physics of statistical models. Consider a Hamiltonian model, where the probability distribution is given by\nwhich is an obvious generalisation of maximum entropy models. Calculating the Fisher information for this form of P (5), we retrieve the direct connection between the covariance (with respect to P\u03b8) of the physical quantities f and the FIT that is characteristic for probability distributions of the exponential type:\nThis means that the FIT characterises the variances and correlations of the functions f which are now considered as stochastic variables and depend on the state x of the system. Additionally, note that changing the temperature in the traditional canonical ensemble corresponds to scaling the Hamiltonian by a factor \u03b2 = 1/T, similarly to equation (3). In the formulation above (5), this is equivalent to scaling all the \u03b8i by \u03b2. Given a point \u03b8 on the parameter manifold, the direction \u2202/\u2202\u03b2 can be expressed\nas\nwhich is just a linear combination. The specific heat is given by\nThus, we can arrange for the specific heat to be one of the entries of the Fisher information matrix, with a change of basis, which includes the \u03b2 direction as a base vector together with other n \u22121 linearly independent ones. Analogous considerations can be made for the magnetic field and magnetic susceptibility. In this sense, the Fisher information tensor is a generalisation of specific heats and susceptibilities.\n(5)\n# 4.2 Fisher information and criticality\nWe can now look at the relationship between statistical criticality and the model\u2019s parameter space. Suppose any generalised susceptibility (i.e. a component of the Fisher tensor) diverges at a point \u03b80. Then an eigenvalue of the Fisher information, say \u03bbk, diverges at \u03b80. Call vk the corresponding normalised eigenvector. For small \u03b1,\nand the r.h.s. diverges. This means that, moving from \u03b80 in the vk direction by an arbitrarily small step yields a model P\u03b80+\u03b1vk that is completely different from P\u03b80, as indicated by an infinite KL divergence. We introduced this description in terms of Fisher information in order to give an interpretation of criticality from the point of view of modelling. A model is at a critical point whenever there is a direction in parameter space that leads to an infinitely different model by a finite change in parameters. This, incidentally, shows that the best way of measuring the distance from a critical point is not to vary temperature, but to use the first eigenvalue of the FIT and move in the direction of the corresponding eigenvector. Temperature is not always the most relevant control parameter. Mastromatteo and Marsili [29] have argued that, because of this special property critical points have in the parameter space, they are particularly favoured by model fitting. In particular, they show that distinguishable models accumulate near critical points, whereas models farther away from these are largely indistinguishable. Their argument, in brief, goes a follows. Two models are considered indistinguishable if their Kullback-Leibler divergence is less than a given value \u03b5. For small \u03b5, DKL is approximated by Fisher information, and the volume of parameter space occupied by models indistinguishable from \u03b80 turns out to be proportional to (detF(\u03b80))\u22121 2 . This quantity diverges at critical points due to the first eigenvalue diverging as explained above. Thus, most models actually are poised near a critical point, according to this metric. They conclude that criticality may be a feature induced by the inference process, rather than one intrinsic to the real system being studied by the model. This may be the reason why statistical models seem to be poised at a critical point, for a variety of training datasets, as we showed in section 3.2. However, it does not affect Zipf laws, which are directly observed in the data.\n# 4.3 Criticality and parameter \u2018sloppiness\u2019\nIt is well known that the parameter spaces of many models often show only a small number of directions (linear combinations of parameter changes) along which the overall properties of the model strongly change (\u201cstiff\u201d), and a large number of directions which have little influence on the model (\u201csloppy\u201d). This phenomenon,\ntermed \u201cmodel sloppiness\u201d, has been observed in a wide number of cases in systems science [14, 27]. For the specific case of neuronal networks, in Ref. [38], although for small numbers of neurons, \u201cstiff\u201d dimensions corresponding to large FIT eigenvalues were identified. The remaining \u201csloppy\u201d dimensions, on the other hand, can change without much effect on the goodness of fit of the model. A further development of this approach has been reported in Ref. [17], where it was shown that about half of the dimensions in the data manifold are irrelevant for the modelling. As shown in paragraph 4.2, near a critical point, the direction pointing towards the critical surface has a diverging FIT eigenvalue, while the others are smaller. This hints there may be a connection between sloppiness and criticality, which, at the moment, we can only leave at the level of speculation. Additionally, however, sloppiness indicates that a fitting algorithm for the data may be improved if different dimensions are differently weighted during the optimisation process. We can then ask whether using a natural gradient in the fitting procedure would lead to a different result while evaluating model criticality. In natural gradient optimisation, the components of the gradient are compensated by the inverse Fisher information, i.e. the divergence near a critical point of the model would disappear, at least theoretically when the Fisher information is exactly known. As a result, the fitting procedure is not homogeneous with respect to the set of the parameters, but with respect to the space of the parameters, taking into account its geometrical properties, and parameters can be identified equally well in all regions. In this way, the problem discussed in Ref. [29] may disappear \u2014 more research will be needed in order to verify this.\n# 5 Discussion\nNeuronal avalanches are an experimentally well-studied phenomenon, that can be explained as a consequence of the optimisation of information processing in the brain. It should be noted that an understanding of how the potential functional benefits of this \u201cdynamical\u201d criticality are realised is missing [42] \u2014 however, it has been shown that the maximisation of the dynamical range happens at criticality [23]. Statistical criticality is an equally complex phenomenon to explain theoretically. Like dynamical criticality, it can be taken to indicate the complexity of the neural data and the relevance of higher-order correlations or latent variables, but its functional implications are less clear. In this chapter, we have reviewed the concept, both in the context of fitted statistical models, and as a direct observation of Zipf laws in neural population data. Through experiments on restricted Boltzmann machines, we suggested that the divergence of model specific heat is not a reliable way to infer properties of the data. We mentioned how Fisher information provides the correct description of the parameter space and the critical surfaces, and reviewed a possible explanation of why statistical models tend to poise themselves at a critical point. Then, we tried to describe the connection between statistical and dynamical critical-\nity, and argued there is no clear connection, by showing examples where one of the two was present without the other. Further insight on this matter might come from models that are capable of both, provided they can reproduce not only the equilibrium distribution of the data, but also the dynamics. A multi-time maximum entropy model might provide a starting point for this work. Of course, it may well be that the observation of Zipf laws is simply a consequence of problems related to how we describe the data \u2014 these include the typically small sets of observables, the choice of binning size, failure to account for the real dynamics, and biases introduced by sampling. However, the ubiquity of Zipf laws in complex systems means its emergence in biological neural networks should not surprise us, and it could be explained in terms of mechanisms such as the one described by [1], or perhaps with preferential attachment. Conversely, an important open problem is an explanation on whether statistical criticality is something that is actively sought by the system because of some functional relevance. On this matter, we tried to analyse the Zipf profile of retinal activity under various conditions (various stimulus statistics, pharmacological treatment), but we found no significant differences in the cases examined. Interestingly, it seems to be possible to generate a Zipf profile simply by enforcing a long-tailed firing rate distribution, despite the absence of correlations. Even if this observation were confirmed, the question would simply shift towards finding a reason for such a skewed distribution of firing rates, which has not yet found a justification in terms of function. Notably, recent research has started showing how Zipf laws appear in different kinds of parametric models, including \u201cdeep\u201d ones, as soon as learning occurs. It has been shown that the Zipf property arises to different degrees in different layers of a deep network, and is maximal in the layers that attain an optimal trade-off between resolution and accuracy in generating samples [45]. This is a starting point in linking statistical criticality to function. It is not known whether similar principles are relevant in the case of biological neural networks, and finding such a link could be an interesting direction of future research.\n# References\n1. Aitchison, L., Corradi, N., Latham, P.E.: Zipfs law arises naturally when there are underlying, unobserved variables. PLoS Computational Biology 12(12), 1\u201332 (2016) 2. Athreya, K.B., Jagers, P.: Classical and Modern Branching Processes, IMA, vol. 84. Springer (1997) 3. Auerbach, F.: Das Gesetz der Bev\u00a8olkerungskonzentration. Petermanns Geographische Mitteilungen 59, 74\u201376 (1913). (Quote translated by J.M.H.) 4. Barab\u00b4asi, A.L., Albert, R.: Emergence of scaling in random networks. Science 286, 509\u2013512 (1999) 5. Beggs, J.M.: The criticality hypothesis: how local cortical networks might optimize information processing. Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences 366(1864), 329\u2013343 (2008) 6. Beggs, J.M., Plenz, D.: Neuronal avalanches in neocortical circuits. Journal of Neuroscience 23(35), 11,167\u201311,177 (2003)\n7. Beggs, J.M., Timme, N.: Being critical of criticality in the brain. Frontiers in Physiology 3, 163 (2012) 8. Cristelli, M., Batty, M., Pietronero, L.: There is more than power law in Zipf. Scientific Reports 2, 812(7) (2012) 9. Eurich, C.W., Herrmann, J.M., Ernst, U.A.: Finite-size effects of avalanche dynamics. Physical Review E 66(6), 066,137 (2002) 10. Gabaix, X.: Zipf\u2019s law and the growth of cities. American Economic Review 89(2), 129\u2013132 (1999) 11. Gardella, C., Marre, O., Mora, T.: Blindfold learning of an accurate neural metric. Proceedings of the National Academy of Sciences p. 201718710 (2018) 12. Gautam, S.H., Hoang, T.T., McClanahan, K., Grady, S.K., Shew, W.L.: Maximizing sensory dynamic range by tuning the cortical state to criticality. PLoS Computational Biology 11(12), e1004,576 (2015) 13. Glauber, R.J.: Time-dependent statistics of the Ising model. Journal of Mathematical Physics 4(2), 294\u2013307 (1963) 14. Gutenkunst, R.N., Waterfall, J.J., Casey, F.P., Brown, K.S., Myers, C.R., Sethna, J.P.: Universally sloppy parameter sensitivities in systems biology models. PLoS Computational Biology 3(10), e189 (2007) 15. Hahn, G., Ponce-Alvarez, A., Monier, C., Benvenuti, G., Kumar, A., Chavane, F., Deco, G., Fr\u00b4egnac, Y.: Spontaneous cortical activity is transiently poised close to criticality. PLoS Computational Biology 13(5), 1\u201329 (2017) 16. Hennig, M.H., Adams, C., Willshaw, D., Sernagor, E.: Early-stage waves in the retinal network emerge close to a critical state transition between local and global functional connectivity. The Journal of Neuroscience 29(4), 1077\u20131086 (2009) 17. Herzog, R., Escobar, M.J., Cofre, R., Palacios, A.G., Cessac, B.: Dimensionality reduction on spatio-temporal maximum entropy models on spiking networks. Preprint bioRxiv:278606 (2018) 18. Hilgen, G., Sorbaro, M., Pirmoradian, S., Muthmann, J.O., Kepiro, I.E., Ullo, S., Ramirez, C.J., Encinas, A.P., Maccione, A., Berdondini, L., et al.: Unsupervised spike sorting for largescale, high-density multielectrode arrays. Cell reports 18(10), 2521\u20132532 (2017) 19. Hopfield, J.J.: Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences 79(8), 2554\u20132558 (1982) 20. Ising, E.: Beitrag zur Theorie des Ferromagnetismus. Zeitschrift f\u00a8ur Physik 31(1), 253\u2013258 (1925) 21. Jaynes, E.T.: Information theory and statistical mechanics. Physical Review 106(4), 620\u2013630 (1957) 22. Jiang, B., Jia, T.: Zipf\u2019s law for all the natural cities in the united states: a geospatial perspective. International Journal of Geographical Information Science 25(8), 1269\u20131281 (2011) 23. Kinouchi, O., Copelli., M.: Optimal dynamical range of excitable networks at criticality. Nat. Phys. 2, 348\u2013352 (2006) 24. K\u00a8oster, U., Sohl-Dickstein, J., Gray, C.M., Olshausen, B.A.: Modeling higher-order correlations within cortical microcolumns. PLoS Computational Biology 10(7), e1003,684 (2014) 25. Larremore, D.B., Shew, W.L., Restrepo, J.G.: Predicting criticality and dynamic range in complex networks: effects of topology. Physical Review Letters 106(5), 058,101 (2011) 26. Li, W.: Random texts exhibit Zipf\u2019s-law-like word frequency distribution. IEEE Transactions on Information Theory 38(6), 1842\u20131845 (1992) 27. Machta, B.B., Chachra, R., Transtrum, M.K., Sethna, J.P.: Parameter space compression underlies emergent theories and predictive models. Science 342(6158), 604\u2013607 (2013) 28. Marre, O., El Boustani, S., Fr\u00b4egnac, Y., Destexhe, A.: Prediction of spatiotemporal patterns of neural activity from pairwise correlations. Physical Review Letters 102(13), 138,101 (2009) 29. Mastromatteo, I., Marsili, M.: On the criticality of inferred models. Journal of Statistical Mechanics: Theory and Experiment 2011(10), P10,012 (2011) 30. Mizuseki, K., Buzs\u00b4aki, G.: Preconfigured, skewed distribution of firing rates in the hippocampus and entorhinal cortex. Cell reports 4(5), 1010\u20131021 (2013)\n31. Mora, T., Deny, S., Marre, O.: Dynamical criticality in the collective activity of a population of retinal neurons. Physical Review Letters 114(7), 078,105 (2015) 32. Nasser, H., Marre, O., Cessac, B.: Spatio-temporal spike train analysis for large scale networks using the maximum entropy principle and monte carlo method. Journal of Statistical Mechanics: Theory and Experiment 2013(03), P03,006 (2013) 33. Newman, M.E.: Power laws, Pareto distributions and Zipf\u2019s law. Contemporary physics 46(5), 323\u2013351 (2005) 34. Nishimori, H.: Statistical physics of spin glasses and information processing: an introduction, vol. 111. Clarendon Press (2001) 35. Nonnenmacher, M., Behrens, C., Berens, P., Bethge, M., Macke, J.H.: Signatures of criticality arise from random subsampling in simple population models. PLoS Computational Biology 13(10), e1005,718 (2017) 36. O\u2019Donnell, C., Gonc\u00b8alves, J.T., Whiteley, N., Portera-Cailliau, C., Sejnowski, T.J.: The population tracking model: A simple, scalable statistical model for neural population data. Neural Computation 29(1), 50\u201393 (2016) 37. Ohiorhenuan, I.E., Mechler, F., Purpura, K.P., Schmid, A.M., Hu, Q., Victor, J.D.: Sparse coding and high-order correlations in fine-scale cortical networks. Nature 466(7306), 617\u2013 621 (2010) 38. Panas, D., Amin, H., Maccione, A., Muthmann, O., van Rossum, M., Berdondini, L., Hennig, M.H.: Sloppiness in spontaneously active neuronal networks. Journal of Neuroscience 35(22), 8480\u20138492 (2015) 39. Priesemann, V., Valderrama, M., Wibral, M., Le Van Quyen, M.: Neuronal avalanches differ from wakefulness to deep sleep\u2013evidence from intracranial depth recordings in humans. PLoS Computational Biology 9(3), e1002,985 (2013) 40. Redner, S.: How popular is your paper? an empirical study of the citation distribution. The European Physical Journal B-Condensed Matter and Complex Systems 4(2), 131\u2013134 (1998) 41. Schneidman, E., Berry, M.J., Segev, R., Bialek, W.: Weak pairwise correlations imply strongly correlated network states in a neural population. Nature 440(7087), 1007\u20131012 (2006) 42. Shew, W.L., Plenz, D.: The functional benefits of criticality in the cortex. The Neuroscientist 19(1), 88\u2013100 (2013) 43. Shew, W.L., Yang, H., Petermann, T., Roy, R., Plenz, D.: Neuronal avalanches imply maximum dynamic range in cortical networks at criticality. Journal of Neuroscience 29(49), 15,595\u201315,600 (2009) 44. Shlens, J., Field, G.D., Gauthier, J.L., Grivich, M.I., Petrusca, D., Sher, A., Litke, A.M., Chichilnisky, E.: The structure of multi-neuron firing patterns in primate retina. The Journal of Neuroscience 26(32), 8254\u20138266 (2006) 45. Song, J., Marsili, M., Jo, J.: Emergence and relevance of criticality in deep learning. arXiv preprint arXiv:1710.11324 (2017) 46. Tang, A., Jackson, D., Hobbs, J., Chen, W., Smith, J.L., Patel, H., Prieto, A., Petrusca, D., Grivich, M.I., Sher, A., Hottowy, P., Dabrowski, W., Litke, A.M., Beggs, J.M.: A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro. Journal of Neuroscience 28, 505518 (2008) 47. Tka\u02c7cik, G., Marre, O., Amodei, D., Schneidman, E., Bialek, W., Berry II, M.J.: Searching for collective behavior in a large network of sensory neurons. PLoS Computational Biology 10(1), e1003,408 (2014) 48. Tka\u02c7cik, G., Mora, T., Marre, O., Amodei, D., Palmer, S.E., Berry, M.J., Bialek, W.: Thermodynamics and signatures of criticality in a network of neurons. Proceedings of the National Academy of Sciences 112(37), 11,508\u201311,513 (2015) 49. V\u00b4azquez-Rodr\u00b4\u0131guez, B., Avena-Koenigsberger, A., Sporns, O., Griffa, A., Hagmann, P., Larralde, H.: Stochastic resonance at criticality in a network model of the human cortex. Scientific Reports 7(1), 13,020 (2017) 50. Vitanov, N.K., Ausloos, M.: Test of two hypotheses explaining the size of populations in a system of cities. Journal of Applied Statistics 42(12), 2686\u20132693 (2015) 51. Yu, S., Yang, H., Nakahara, H., Santos, G.S., Nikoli\u00b4c, D., Plenz, D.: Higher-order interactions characterized in cortical activity. Journal of Neuroscience 30(48), 17,514\u201317,526 (2011)\n52. Zipf, G.K.: Human behavior and the principle of least effort. Addison-Wesley, Cambridge (1949)\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to explore the connections between critical dynamics in neuronal networks and maximum entropy models used as statistical models of neural activity, addressing gaps in understanding the relationship between statistical and dynamical criticality.",
            "scope": "The survey focuses on critical dynamics, maximum entropy models, and the emergence of Zipf's law in neural activity. It excludes unrelated areas of neuroscience that do not pertain to criticality or statistical modeling."
        },
        "problem": {
            "definition": "The core issue explored is the relationship between statistical and dynamical criticality in neural systems, particularly how these concepts manifest in neuronal activity and their implications.",
            "key obstacle": "Researchers face challenges in clearly distinguishing between statistical and dynamical criticality, and understanding the functional relevance of observed phenomena like Zipf's law."
        },
        "architecture": {
            "perspective": "The survey introduces a framework that categorizes existing research into statistical models of neural activity and criticality, emphasizing the differences between statistical and dynamical perspectives.",
            "fields": "The survey organizes current methods into fields such as statistical modeling, energy-based models, and criticality analysis, using criteria based on the nature of the neural data and the modeling approaches."
        },
        "conclusion": {
            "comparisions": "The survey compares various studies on neural activity, highlighting differences in effectiveness and outcomes between models that exhibit statistical criticality and those that do not.",
            "results": "Key takeaways include the observation that Zipf's law can emerge in neural data independent of dynamical criticality, suggesting a complex interplay between statistical properties and neural activity."
        },
        "discussion": {
            "advantage": "Existing research has successfully identified critical dynamics and statistical modeling approaches that enhance our understanding of neural activity patterns.",
            "limitation": "Current studies often lack a clear functional interpretation of criticality and Zipf's law, and there are limitations in the methodologies used to analyze neural data.",
            "gaps": "There remain unanswered questions regarding the functional implications of statistical criticality and the reasons behind the emergence of Zipf's law in neural networks.",
            "future work": "Future research should focus on developing models that integrate both statistical and dynamical aspects of neural activity, potentially exploring the implications of long-tailed firing rate distributions."
        },
        "other info": {
            "additional_info": {
                "notable_studies": "The survey references significant studies that have contributed to the understanding of criticality in neural systems.",
                "data_sources": "Data analyzed includes spike train recordings from various neural systems, particularly the mammalian retina."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The survey aims to explore the connections between critical dynamics in neuronal networks and maximum entropy models used as statistical models of neural activity."
        },
        {
            "section number": "2.2",
            "key information": "The core issue explored is the relationship between statistical and dynamical criticality in neural systems, particularly how these concepts manifest in neuronal activity."
        },
        {
            "section number": "2.3",
            "key information": "The survey introduces a framework that categorizes existing research into statistical models of neural activity and criticality, emphasizing the differences between statistical and dynamical perspectives."
        },
        {
            "section number": "6.3",
            "key information": "Future research should focus on developing models that integrate both statistical and dynamical aspects of neural activity, potentially exploring the implications of long-tailed firing rate distributions."
        },
        {
            "section number": "7.1",
            "key information": "The survey references significant studies that have contributed to the understanding of criticality in neural systems."
        },
        {
            "section number": "7.2",
            "key information": "Data analyzed includes spike train recordings from various neural systems, particularly the mammalian retina."
        }
    ],
    "similarity_score": 0.5053900811167988,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/Statistical models of neural activity, criticality, and Zipf's law.json"
}