{
    "from": "google",
    "scholar_id": "NiOEIKKBcb8J",
    "detail_id": null,
    "title": "Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning",
    "abstract": " Abstract\n\nDemonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require high computational costs to introduce the priori knowledge. In this paper, inspired by the human learning process, we propose a simple but effective demonstration ordering method for ICL, named the few-shot In-Context Curriculum Learning (ICCL). The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process. The difficulty can be assessed by human experts or LLMs-driven metrics, such as perplexity. Then we design extensive experiments to discuss the effectiveness of the ICCL at both corpus-level and instance-level. Moreover, we also investigate the formation mechanism of LLM\u2019s ICCL capability. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for representative open-source LLMs. To facilitate further research and applications by other scholars, we make the code publicly available 1.\n\n# 1 Introduction\n\nHuman education is methodical and incremental, building upon previously accumulated knowledge, which inspires curriculum  based algorithm designs in machine learning. Curriculum learning, introduced by Bengio et al. (2009), is originally a method that progressively raises the difficulty of the data samples utilized in the training process. Many studies have demonstrated the efficacy of curriculum learning applied in different models (Portelas et al., 2020; Nagatsuka et al., 2021) and different tasks (Wang et al., 2022; Xu et al., 2020). Since instruction-tuned Large Language Models (LLMs) exhibited remarkable proficiency in under\n\n1 https://github.com/61peng/curri_learning\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/77cf/77cf7cac-4e4d-4b2e-8061-cfd60bc35fae.png\" style=\"width: 50%;\"></di",
    "bib_name": "liu2024let",
    "md_text": "# Let\u2019s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning\n\nYinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, Yong Huang and Wei Lu School of Information Management, Wuhan University, China Information Retrieval and Knowledge Mining Laboratory, Wuhan University, China inpengliu, laujames2017, coding, chengqikai, yonghuang1991, weilu}@whu.edu.c\n\n# Abstract\n\nDemonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require high computational costs to introduce the priori knowledge. In this paper, inspired by the human learning process, we propose a simple but effective demonstration ordering method for ICL, named the few-shot In-Context Curriculum Learning (ICCL). The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process. The difficulty can be assessed by human experts or LLMs-driven metrics, such as perplexity. Then we design extensive experiments to discuss the effectiveness of the ICCL at both corpus-level and instance-level. Moreover, we also investigate the formation mechanism of LLM\u2019s ICCL capability. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for representative open-source LLMs. To facilitate further research and applications by other scholars, we make the code publicly available 1.\n\n# 1 Introduction\n\nHuman education is methodical and incremental, building upon previously accumulated knowledge, which inspires curriculum  based algorithm designs in machine learning. Curriculum learning, introduced by Bengio et al. (2009), is originally a method that progressively raises the difficulty of the data samples utilized in the training process. Many studies have demonstrated the efficacy of curriculum learning applied in different models (Portelas et al., 2020; Nagatsuka et al., 2021) and different tasks (Wang et al., 2022; Xu et al., 2020). Since instruction-tuned Large Language Models (LLMs) exhibited remarkable proficiency in under\n\n1 https://github.com/61peng/curri_learning\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/77cf/77cf7cac-4e4d-4b2e-8061-cfd60bc35fae.png\" style=\"width: 50%;\"></div>\nFigure 1: Illustration of In-Context Curriculum Learning (ICCL). The curriculum schedule can be designed by both human and LLMs, schedule constructor sort demonstrations from easy to hard based on their understanding.\n\n<div style=\"text-align: center;\">Figure 1: Illustration of In-Context Curriculum Learning (ICCL). The curriculum schedule can be designed by both human and LLMs, schedule constructor sort demonstrations from easy to hard based on their understanding.\n</div>\nstanding human intentions and generating humanlike text (Ouyang et al., 2022), researchers have initiated the integration of curriculum learning during instruction tuning (Feng et al., 2023; Lee et al., 2023). Aforementioned works demonstrate that curriculum learning facilitates accelerated convergence and the identification of better local minima during the parameter updating process. However, research on the effectiveness of curriculum learning within In-Context Learning (ICL) remains limited. Some methods that gradually prompt LLMs within instruction, such as Chain of Thought (CoT) (Wei et al., 2022), have significantly enhanced the ability of model to perform complex reasoning. This inspires us to apply curriculum learning for ICL. LLMs with varying performance are treated as students with varying learning abilities, and a human educator plays the role of a facilitator, guiding the learners through the curriculum. Under\n\nthe human-led curriculum, the models are gradually prompted to solve complex tasks. Is such a curriculum schedule effective, particularly when compared with many superior demonstration ordering algorithms? If this is the case, at what point is the model\u2019s capacity to learn from a curriculum curriculum established? To answer those questions, we propose the In-Context Curriculum Learning (ICCL), as illurstrated in Figure 1. The ICCL framework encompasses two roles: curriculum constructor and curriculum learner. The curriculum constructor, which could be either human experts or LLMs, ranks the demonstrations based on their comprehension of difficulty. Subsequently, the learner is guided in progressively solving tasks. Our main contributions are as follows: (1) We propose the ICCL, a straightforward and effective demonstration ordering method, and validate the effectiveness of ICCL for open-source LLMs. (2) We adopt perplexity as the metric to assess the difficulty of demonstration, which outperformed many superior demonstration ordering methods. (3) Comparative analysis indicates that the ICCL capability of LLMs is developed during the instruction-tuning stage.\n\n# 2 Related Work\n\nDemonstrations Organization Numerous studies (Dong et al., 2022; Wan et al., 2023) show that the performance of LLMs is heavily influenced by the selection and ordering of demonstrations, indicating that different organizational approaches lead to the assimilation of distinct semantic information.\nLu et al. (2022) find that the performance of pretrained language models can vary from nearly stateof-the-art to random guess performance depending on how samples are ordered. This implies there exist multiple strategies for arranging prompt orders to enhance performance. They identify outstanding demonstration organizations based on entropy statistics. Liu et al. (2022) retrieve demonstrations that are semantically-similar to test source and order them by increasing cosine similarity. Wu et al. (2023) propose a ranking algorithm inspired by the compression viewpoint, which considers the codelength required to compress and transmit testing label. The codelength can be calculated using Shannon-Huffman code. Since the existing research has substantiated that demonstration organization can significantly af\n\n# Demonstrations Organization\n\nfects performance, we aim to delve into the optimization of prompt orders in ICL. To this end, we introduce curriculum learning strategies successfully employed in machine learning into ICL.\n\nCurriculum Learning The concept of curriculum learning (Bengio et al., 2009) has inspired numerous research to address various natural language processing tasks. Wang et al. (2022) proposes a novel framework for Abstract Meaning Representation (AMR) parsing using hierarchical curriculum learning, achieving significant improvements on AMR2.0 and AMR3.0 benchmarks. Jia et al. (2023) introduces an approach applying curriculum learning to natural language generation (NLG) tasks. The authors propose a strategy that starts by training models to generate the final few words of a sequence, progressively extending to generate the entire sequence. However, the aforementioned studies all require adjustments to model parameters. There is currently a lack of research exploring curriculum learning in context.\n\n# 3 Methodology\n\nInspired by curriculum learning employed in training process, we investigate a novel few-shot InContext Curriculum learning (ICCL), which is essentially a strategy for ordering demonstrations: sort the demonstration examples in order of increasing difficulty. This ordering strategy prompt LLM to absorb many skills and tasks within the parameters gradually.\n\n# 3.1 Problem Formulation\n\nGiven a LLM \u03b8, there are n demonstrations {(x i, y i)} n i =0 selected to instruct \u03b8 to solve specific task T. While trying to adapt \u03b8 for T, different demonstration orders D have different efficiency in utilizing parameters \u03b8, the parameter-efficiency E p is measured by performance metrics. We hypothesizes that when demonstrations are arranged from simple to difficult, it will increase the model\u2019s E p ad much as possible. Therefore, the objective of ICCL is to acquire an order D curriculum that:\n\n(1)\n\nICCL remain parameters \u03b8 fixed throughout the process and merely modifies the ordering of D to progress from simple to complex. Consequently, the crux lies in the method of measuring the complexity of demonstrations.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f131/f131c2e0-ff97-4c5e-bfdc-ec8877961aba.png\" style=\"width: 50%;\"></div>\nMethod\nSciCite\nSciNLI\nSciERC\nOverall\nMacro P\nMacro F1\nAccuracy\nMacro F1\nMicro F1\nAvg F1\nMIXTRAL-8X7B-INSTRUCT-V0.1\nRandom\n69.74\u00b13.76\n62.57\u00b10.94\n42.38\u00b10.11\n37.21\u00b10.10\n23.91\u00b10.32\n41.23\nVoteK\n68.82\u00b12.11\n49.88\u00b11.81\n38.89\u00b10.08\n31.66\u00b10.44\n30.24\u00b11.70\n37.26\nICCL(Ours)\n71.32\u00b11.58\n66.76\u00b12.65\n52.21\u00b10.28\n49.87\u00b10.26\n24.90\u00b10.74\n47.18\nLLAMA 2-70B-CHAT\nRandom\n64.99\u00b10.45\n59.37\u00b10.18\n39.68\u00b10.45\n34.31\u00b10.38\n24.40\u00b15.28\n39.36\nVoteK\n61.27\u00b11.09\n63.11\u00b10.25\n37.53\u00b10.20\n27.46\u00b10.20\n30.54\u00b10.60\n40.37\nICCL(Ours)\n67.58\u00b12.84\n62.56\u00b11.28\n41.13\u00b10.39\n35.59\u00b10.52\n31.45\u00b10.90\n43.20\nQWEN1.5-72B-CHAT\nRandom\n75.19\u00b10.75\n74.70\u00b10.38\n48.38\u00b10.24\n45.85\u00b10.31\n19.51\u00b10.32\n46.59\nVoteK\n77.82\u00b10.22\n75.62\u00b10.22\n49.88\u00b10.40\n47.55\u00b10.51\n30.19\u00b10.63\n51.12\nICCL(Ours)\n76.98\u00b10.45\n75.02\u00b10.87\n50.83\u00b10.31\n49.09\u00b10.30\n26.68\u00b10.15\n50.26\nTable 1: Evaluation result of mainstream LLMs applying ICCL on three scientific datasets at corpus level. We adopt F 1 score as the core metric and perform averaging to get overall F 1 score. All the results are calculated based on 3 different random seeds over test set of each task. standard deviation are in small font. Dark and light blue colored cells stand for decline> 1% and <1% compared to Random baseline, respectively. Dark and light orange colored cells stand for improvement> 1% and <1%, respectively.\n\n# 3.2 Curriculum Schedule Construction\n\nWe firstly rely on human experts to construct a curriculum-based context for LLMs at corpus level. Specifically, we engaged five human experts, ranging from undergraduates to professors, to rank the demonstrations based on their perceived difficulty, The final ordering was determined by averaging the rankings provided by each expert. To ensure the reliability of the final order, we employed Kendall\u2019s coefficient of concordance as the agreement scores among experts. At instance level, appropriate samples can be selected for each test target using demonstrations retrieval algorithms (such as TopK (Liu et al., 2022)). The ranker shifts from humans to LLMs. While human experts can judge the demonstrations difficulty based on their understanding, LLMs may not perceive it the same way. An intuitive approach is to use perplexity to quantify the LLMs\u2019 understanding of complexity. We retrieve n candidate samples that are most similar to the test target. Then, we calculate the complexity of each sample using:\n\nComp (x i, y i) = exp {\u2212 log p (y i |I \u03b8 (x i))}\n\nwhere (x i, y i) represents a sample in the candidate set, and I \u03b8 (x i) denotes the instruction template of LLM \u03b8 with input x i. We measure the complexity of a sample by calculating the perplexity on the label y i given the specified instruction, and order the demonstrations {(x i, y i)} with lower perplexity first.\n\n# 4 Experiments\n\n# 4.1 Setup\n\nDatasets Scientific papers are relatively complex discourse in the structured educational journey of human. The comprehension of scientific text requires domain expertise and logical reasoning capability. Therefore, corpus composed of scientific texts is selected as the benchmark evaluation set for estimating the effectiveness of curriculum learning applied during the inference stage of LLMs. We evaluate ICCL on three scientific dataset: SciCite (Cohan et al., 2019), SciNLI (Sadat and Caragea, 2022) and SciERC (Luan et al., 2018), encompassing tasks in text classification, natural language inference and information extraction.\nModels We utilize a range of open-source performant LLMs, including LlaMA2 (Touvron et al., 2023), Mixtral-8x7B (Jiang et al., 2024), Qwen1.5 (Bai et al., 2023), exploring their applications within the ICCL framework.\n\nBaseline For corpus-level methods, we consider VoteK (Su et al., 2022) and a Random baseline. For instance-level methods, we select KATE (Liu et al., 2022), TopK+LocalE (Lu et al., 2022), TopK+MDL (Wu et al., 2023) and a TopK baseline that select 5 demonstrations that are semantically closest to testing samples and rank them randomly. More experiment details are present in Appendix.\n\nLLM\nMethod\nSciCite\nSciNLI\nSciERC\nMixtral-8x7B\n-Instruct-v0.1\nTopK\n64.85\n34.69\n32.98\n+ KATE\n64.19\n35.20\n32.48\n+ LocalE\n67.13\n33.16\n-\n+ MDL\n67.06\n35.22\n-\n+ ICCL(Ours)\n67.92\n37.58\n33.58\nLlama2\n-70B-Chat\nTopK\n60.86\n39.70\n38.54\n+ KATE\n57.11\n39.81\n38.71\n+ LocalE\n59.30\n39.95\n-\n+ MDL\n64.11\n40.02\n-\n+ ICCL(Ours)\n63.30\n40.48\n39.03\nTable 2: Evaluation result of LLMs applying InstanceLevel ICCL on three scientific datasets. Numbers in bold indicate the highest F 1 among all methods. All the results are calculated based on 3 different random seeds over test set of each task.\n\n# 4.2 Main Result\n\nA comparison between 3 mainstream open-source LLMs across 3 NLP tasks shows the superiority of ICCL over other corpus-level methods, depicted in Table 1. The demonstrations for both ICCL and the random baseline were selected by human experts, while VoteK, built upon TopK, selected diverse yet representative examples using voting mechanism. ICCL maintains an ordering from simple to complex for all test samples, whereas the random and VoteK baseline employs a random order. Despite VoteK achieving commendable performance in certain settings, it exhibits a large standard deviation, indicating instability in its improvements. For Mixtral-8x7B, the performance of VoteK is approximately 10% lower than random baseline. While ICCL consistently achieves stable improvements across all LLMs and NLP tasks compared to random baseline, and shows an average improvement of 9% compared to random baseline, and 7.8% compared to VoteK. This demonstrates\n\n# that heuristic curriculum learning methods are\n\nAt instance level, we employ the topK algorithm for demonstrations retrieval to construct candidate set. Subsequently, we utilize ordering algorithms such as KATE, LocalE, and MDL to generate performant prompts from this candidate set as baselines. As shown in Table 2, ICCL still registers decent improvements on most tasks compared with instance-level baselines. Our method achieves an overall improvement of 4.96% compared to TopK with random order and 5.48% compared to KATE for Mixtral. We also notice that LocalE and MDL are competitive on SciCite. However, these two methods are only suitable for classification tasks\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68f5/68f5072d-2407-45ea-a0ff-179de230b1c1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: F 1 scores improvement (or decline) rates for both Based LLMs and Instruction-Tuned LLMs using ICCL compared with random order.\n</div>\nwith a limited search space and are difficult to apply to complex generative tasks. This highlights the versatility of our method, indicating that it can be applied to a wide range of NLP tasks.\n\n# 4.3 Formation Mechanism of ICCL Capability\n\nTo explore the formation mechanism of ICCL, i.e., whether the model\u2019s ability to learn the curriculum from context is established during pre-training or instruction-tuning, we conduct ICCL experiments on both the base models and the instruction-tuned models separately. Figure 2 shows that the performance of ICCL on the base model is unstable, with an average decrement of 7.16%. Even in instances where an improvement is observed, it is weaker compared to the enhancement effect on the corresponding instruction-tuned models. This evidences the base models\u2019 lack of sensitivity to the curriculum-based demonstration order. It further intimates that the competency for ICCL is most likely acquired during the instruction-tuning stage.\n\n# 5 Conclusion\n\nIn this work, we argue that gradually increase the complexity of the demonstrations in prompt can achieve better performance. To substantiate this claim, we propose In-Context Curriculum Learning (ICCL), a straightforward yet effective demonstration ordering method for both corpus-level and instance-level. We design three sets of experiments to exploring the validity and mechanism of curriculum learning within context. The experimental results affirm the effectiveness of ICCL on opensource LLMs.\n\nDue to the limited timeframe of the experiment, we were unable to utilize the latest LLMs, such as Meta Llama 3 (AI@Meta, 2024). However, our experiments with the current mainstream models have demonstrated the effectiveness of the heuristic method of in-context curriculum learning. In future research, we will incorporate more recent LLMs to ensure the robustness of our method across different models.\n\n# References\n\n# AI@Meta. 2024. Llama 3 model card.\n\nAI@Meta. 2024. Llama 3 model card.\n\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348.\nArman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019.  Structural scaffolds for citation intent classification in scientific publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3586\u20133596, Minneapolis, Minnesota. Association for Computational Linguistics.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234.\nTao Feng, Zifeng Wang, and Jimeng Sun. 2023. Citing: Large language models create curriculum for instruction tuning. arXiv preprint arXiv:2310.02527.\nQi Jia, Yizhu Liu, Haifeng Tang, and Kenny Zhu. 2023.\nIn-sample curriculum learning by sequence completion for natural language generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11937\u201311950, Toronto, Canada. Association for Computational Linguistics.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.\nBruce W Lee, Hyunsoo Cho, and Kang Min Yoo. 2023. Instruction tuning with human curriculum. arXiv preprint arXiv:2310.09518.\n\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In  Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219\u20133232, Brussels, Belgium. Association for Computational Linguistics.\nKoichi Nagatsuka, Clifford Broni-Bediako, and Masayasu Atsumi. 2021. Pre-training a BERT with curriculum learning by increasing block-size of input text. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 989\u2013996, Held Online. INCOMA Ltd.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744.\nR\u00e9my Portelas, C\u00e9dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. 2020. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. In Conference on Robot Learning, pages 835\u2013853. PMLR.\nMobashir Sadat and Cornelia Caragea. 2022. SciNLI: A corpus for natural language inference on scientific text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7399\u20137409, Dublin, Ireland. Association for Computational Linguistics.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. arXiv preprint arXiv:2209.01975.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\n\nZhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, et al. 2023. Efficient large language models: A survey. arXiv preprint arXiv:2312.03863, 1.\nPeiyi Wang, Liang Chen, Tianyu Liu, Damai Dai, Yunbo Cao, Baobao Chang, and Zhifang Sui. 2022.  Hierarchical curriculum learning for AMR parsing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 333\u2013339, Dublin, Ireland. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023.  Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423\u20131436, Toronto, Canada. Association for Computational Linguistics.\nBenfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. 2020. Curriculum learning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6095\u20136104, Online. Association for Computational Linguistics.\n\n# A Datasets\n\nDataset information is detailed in Table 3. Here are examples of each datasets:\n\n# \u2022 SciCite\n\n\u2013 Sentence: A direct consequence is that overheads for address translation have grown to dominate run time for many important workloads with large memory footprint [46, 113, 241, 302, 303, 375].\n\u2013 Label: background\n\n# \u2022 SciERC\n\n\u2013 Sentence:  This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information.\n\u2013 Label:  [[\u2019approach\u2019, \u2019Generic\u2019], [\u2019unsupervised learning of parts of speech\u2019, \u2019Task\u2019], [\u2019morphological and syntactic information\u2019, \u2019OtherScientificTerm\u2019]]\n\nDataset\nTask\n# Test\n# Labels\n# Demos\nSciCite\nCitation Intent Classification\n1861\n3\n5\nSciERC\nScientific Information Recognition\n551\n7\n5\nSciNLI\nScientific Language Inference\n4000\n4\n4\n\u2013 Sentence1: Without L alignment , we observe a reduction in both accuracy and BLEU on Yelp.\n\u2013 Sentence2: this tendency is inconsistent on Amazon (i.e., -2.2 accuracy and +0.56 BLEU).\n\u2013 Label: contrasting\n\n# B Model\n\nWe use both publicly available and proprietary LLMs with the different model size as follow:\n\n\u2022 Qwen1.5 (Bai et al., 2023) is the LLM family built by Alibaba Cloud. We select  Qwen1.572B-Chat, which is the largest version of Qwen series.\n\nThe prompt template of each LLM is shown in Table 4.\n\n# C Evaluation Detail\n\nAt Corpus level, we arbitrarily select 4-5 demonstration examples from training set, which remain consistent across all test samples to eliminate the influence of demonstration selection. Table 5 show some example of demonstrations selected by different methods. For each ordering setting, the sequence of demonstrations in the prompt for all models is identical. To equitably evaluate the curriculum learning capabilities of diverse LLMs, we adopt the test set and evaluation metric inherent to each dataset. Concretely, we report macro-averaged values for\n\nLLM\nIn-context Curriculum Learning Prompts\nMixtral\nTemplate: [INST]{Task Description} + {Sentence}[/INST]{Label}</s>\n[INST]{Test Input}[/INST]\nTask Description: You are a scientific literature analyst. Extract scientific entities from\nsentences. The scientific entity category includes [\u2019Method\u2019, \u2019Task\u2019, \u2019Metric\u2019, \u2019Material\u2019,\n\u2019Generic\u2019, \u2019OtherScientificTerm\u2019, \u2019Generic\u2019].\nDemonstration: Sentence: text xi\nLabel: label yi\nTest Input: Sentence: text x\nLlama2\nTemplate: <s>[INST] \u00abSYS\u00bb {System Message}\u00ab/SYS\u00bb\n{Task Description} + {Sentence}[/INST]{Label}</s>\n<s>[INST]{Test Input}[/INST]\nTask Description: Identify the intent of a citation in scientific papers. Choose the citation\nintention of the following sentence from [\u2019method\u2019, \u2019background\u2019, \u2019result\u2019].\nDemonstration: Sentence: text xi\nLabel: label yi\nTest Input: Sentence: text x\nQwen1.5\nTemplate: <|im_start|>system {System Message} <|im_end|>\n<|im_start|>user{Task Description} + {Sentence}<|im_end|>\n<|im_start|>assistant {Label}<|im_end|>\n<|im_start|>user{Test Input}<|im_end|>\nTask Description: Identify the semantic relationship between the following pair of\nsentences. The semantic relationship includes [\u2019reasoning\u2019, \u2019entailment\u2019, \u2019contrasting\u2019,\n\u2019neutral\u2019].\nDemonstration: Sentence1: text xi1\nSentence2: text xi2\nLabel: label yi\nTest Input: Sentence1: text x1\nSentence2: text x2\nTable 4: The prompt template of In-Context Curriculum Learning for each LLM.\n\nSciCite, micro-averaged values for SciERC, and accuracy and macro F 1 for SciNLI. We select F 1 score as the core metric for all tasks to ascertain the efficacy of ICCL. To guarantee the reproducibility of our experimental results, We run every experiment with 3 different random seed, and calculate average metric to report in paper.\n\nManual Select\nTask\nCitation Intent Class\nW\n0.968\nDemos\nSentence: This result is consistent with the conclu-\nsions of the aforementioned recent study of (34) and\nreinforces them from a significantly broader perspec-\ntive. Label: result\nSentence: To determine the cell velocity, Darcy\u2019s\nlaw may be used as the constitutive assump-\ntion[21],[18],[22],[13]. Label: method\nSentence: This is clearly in contrast to the results of\nearlier investigations (Laprise&Peltier1989a, Pierre-\nhumbert&Wyman1985, Clark&Peltier1977), where it\nwas found that the criteria for static and dynamic insta-\nbilities are simultaneously satisfied. Label: result\nSentence: nest burrows in close proximity of one an-\nother appears to be well founded as previously shown\nby several studies that measured distances between kin\nvs. non-kin nest burrows, including in long-term data\nsets(King 1989b; Viblanc et al. 2010; Arnaud, Dobson\n& Murie 2012; Dobson et al. 2012). (5) Label: back-\nground\nSentence: We employed three modelling approaches\nthat have successfully been applied in previous stud-\nies on species distribution (Guisan and Zimmermann\n2000): generalised linear models (GLM, i.e. logistic\nregression in this case), generalised additive models\n(GAM), and classification and regression trees. Label:\nmethod\nSent\nas d\nMo\u02d80\nSent\n1978\nGorm\nMes\nbel: \nSent\npress\nthe I\nSQO\nmeth\nSent\nPoli,\nEll e\nMitc\n2012\net al.\nSent\njimu\nrema\nstudi\nlast t\nlist f\nTask\nScientific Information R\n0.936\nTask\nCitation Intent Classification\nW\n0.968\n-\nDemos\nSentence: This result is consistent with the conclu-\nsions of the aforementioned recent study of (34) and\nreinforces them from a significantly broader perspec-\ntive. Label: result\nSentence: To determine the cell velocity, Darcy\u2019s\nlaw may be used as the constitutive assump-\ntion[21],[18],[22],[13]. Label: method\nSentence: This is clearly in contrast to the results of\nearlier investigations (Laprise&Peltier1989a, Pierre-\nhumbert&Wyman1985, Clark&Peltier1977), where it\nwas found that the criteria for static and dynamic insta-\nbilities are simultaneously satisfied. Label: result\nSentence: nest burrows in close proximity of one an-\nother appears to be well founded as previously shown\nby several studies that measured distances between kin\nvs. non-kin nest burrows, including in long-term data\nsets(King 1989b; Viblanc et al. 2010; Arnaud, Dobson\n& Murie 2012; Dobson et al. 2012). (5) Label: back-\nground\nSentence: We employed three modelling approaches\nthat have successfully been applied in previous stud-\nies on species distribution (Guisan and Zimmermann\n2000): generalised linear models (GLM, i.e. logistic\nregression in this case), generalised additive models\n(GAM), and classification and regression trees. Label:\nmethod\nSentence: Genotyping of the SNPs wa\nas described previously (Gutknecht et\nMo\u02d80308ssner et al. 2006a, b). Label: me\nSentence: 1991) and empirical studies \n1978; Kruuk and Parish 1982; Mills 198\nGorman 1997; Geffen et al.\n1992; P\nMessier 2001; Valenzuela and Macdonal\nbel: background\nSentence: Furthermore, the hospital an\npression scale (HAD) (Snaith and Zigmon\nthe IBS special scale for quality of life\nSQOL) (Drossman et al., 2000) were u\nmethod\nSentence: (Massie and Holland, 1984; Ci\nPoli, 2001; Uchitomi et al., 2003; Katz\nEll et al., 2005; Boyd et al., 2012; Kim\nMitchell et al., 2012; Palmer et al., 201\n2012; Tada et al., 2012; Warmenhoven et \net al., 2012). Label: background\nSentence:were retrospective (Okada et a\njimura et al., 2002; Ramasamy et al., 2005\nremaining studies were pseudo-randomiz\nstudies (Colpi et al., 2009; Ghalayini et al.\nlast two studies were randomized based o\nlist for the operative theatre. Label: back\nTask\nScientific Information Recognition\n<div style=\"text-align: center;\">Scientific Information Recognition\n</div>\nTable 5: Examples of Demonstrations Selection by Corpus-level Method, where W is Kendall\u2019s coefficient of\n\nTable 5: Examples of Demonstrations Selection by Corpus-level Method, where W is Kendall\u2019s coefficient of concordance.\n\nTable 5: Examples of Demonstrations Selection by Corpus-level Method, whe concordance.\n\ntent Classification\n-\nu-\nnd\nc-\n\u2019s\np-\nof\ne-\nit\na-\nn-\nwn\nin\nta\non\nk-\nes\nd-\nnn\nic\nls\nel:\nSentence: Genotyping of the SNPs was performed\nas described previously (Gutknecht et al.\n2007;\nMo\u02d80308ssner et al. 2006a, b). Label: method\nSentence: 1991) and empirical studies (e.g. Kruuk\n1978; Kruuk and Parish 1982; Mills 1989; Mills and\nGorman 1997; Geffen et al.\n1992; Patterson and\nMessier 2001; Valenzuela and Macdonald 2002). La-\nbel: background\nSentence: Furthermore, the hospital anxiety and de-\npression scale (HAD) (Snaith and Zigmond, 1986) and\nthe IBS special scale for quality of life (QOL) (IB-\nSQOL) (Drossman et al., 2000) were used. Label:\nmethod\nSentence: (Massie and Holland, 1984; Ciaramella and\nPoli, 2001; Uchitomi et al., 2003; Katz et al., 2004;\nEll et al., 2005; Boyd et al., 2012; Kim et al., 2012;\nMitchell et al., 2012; Palmer et al., 2012; Pirl et al.,\n2012; Tada et al., 2012; Warmenhoven et al., 2012; Yu\net al., 2012). Label: background\nSentence:were retrospective (Okada et al., 2002; Tsu-\njimura et al., 2002; Ramasamy et al., 2005) and the two\nremaining studies were pseudo-randomized controlled\nstudies (Colpi et al., 2009; Ghalayini et al., 2011); these\nlast two studies were randomized based on the waiting\nlist for the operative theatre. Label: background\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of demonstration ordering in in-context learning (ICL) for large language models (LLMs), highlighting the limitations of existing methods that require high computational costs and emphasizing the need for a simpler, more effective approach.",
        "problem": {
            "definition": "The problem is the inefficiency of current demonstration ordering methods in ICL, which often involve high computational costs and do not effectively utilize the complexity of tasks.",
            "key obstacle": "The main challenge is the lack of effective strategies to order demonstrations from simple to complex, which limits the models' ability to learn efficiently."
        },
        "idea": {
            "intuition": "Inspired by human educational practices, the idea is to enhance learning by gradually increasing the complexity of demonstrations presented to LLMs during inference.",
            "opinion": "The proposed idea, In-Context Curriculum Learning (ICCL), entails a method for ordering prompt demonstrations based on their difficulty to improve model performance.",
            "innovation": "ICCL differentiates itself from existing methods by using a curriculum-based approach that does not alter model parameters but focuses solely on the ordering of demonstrations."
        },
        "method": {
            "method name": "In-Context Curriculum Learning",
            "method abbreviation": "ICCL",
            "method definition": "ICCL is a demonstration ordering strategy that sorts examples by increasing difficulty to enhance the learning process of LLMs.",
            "method description": "The method involves ranking demonstrations from simple to complex, allowing models to absorb skills progressively.",
            "method steps": [
                "Select demonstrations for a specific task.",
                "Rank the demonstrations based on perceived difficulty by human experts or LLM-driven metrics.",
                "Present the ranked demonstrations to the model in order of increasing complexity."
            ],
            "principle": "ICCL is effective because it leverages the natural learning process of gradually increasing task complexity, which enhances the model's ability to learn from the provided demonstrations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on three scientific datasets: SciCite, SciNLI, and SciERC, using various open-source LLMs.",
            "evaluation method": "Performance was assessed using F1 scores as the core metric, comparing ICCL against baseline methods, including random ordering and VoteK."
        },
        "conclusion": "The results confirm that ICCL significantly improves performance in LLMs by utilizing a structured approach to demonstration ordering, affirming the effectiveness of curriculum learning in ICL.",
        "discussion": {
            "advantage": "ICCL stands out due to its stability and consistent performance improvements across different tasks and models, unlike existing methods that may exhibit variability.",
            "limitation": "The method may not perform optimally with the latest LLMs, as the experiments were limited to mainstream models available at the time.",
            "future work": "Future research will focus on incorporating more recent LLMs to validate the robustness of ICCL across a broader range of models and tasks."
        },
        "other info": {
            "repository": "The code for ICCL is publicly available at https://github.com/61peng/curri_learning."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of demonstration ordering in in-context learning for large language models, highlighting the limitations of existing methods."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge is the lack of effective strategies to order demonstrations from simple to complex, which limits the models' ability to learn efficiently."
        },
        {
            "section number": "3.4",
            "key information": "In-Context Curriculum Learning (ICCL) is proposed as a demonstration ordering strategy that sorts examples by increasing difficulty to enhance the learning process of LLMs."
        },
        {
            "section number": "4.1",
            "key information": "ICCL involves ranking demonstrations based on perceived difficulty to allow models to absorb skills progressively."
        },
        {
            "section number": "5.1",
            "key information": "The experiments were conducted on three scientific datasets: SciCite, SciNLI, and SciERC, using various open-source LLMs to evaluate ICCL."
        },
        {
            "section number": "6.4",
            "key information": "ICCL may not perform optimally with the latest LLMs, indicating potential scalability and applicability challenges."
        },
        {
            "section number": "7",
            "key information": "The results confirm that ICCL significantly improves performance in LLMs by utilizing a structured approach to demonstration ordering."
        }
    ],
    "similarity_score": 0.73450701513441,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/77cf/77cf7cac-4e4d-4b2e-8061-cfd60bc35fae.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f131/f131c2e0-ff97-4c5e-bfdc-ec8877961aba.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68f5/68f5072d-2407-45ea-a0ff-179de230b1c1.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Let's Learn Step by Step_ Enhancing In-Context Learning Ability with Curriculum Learning.json"
}