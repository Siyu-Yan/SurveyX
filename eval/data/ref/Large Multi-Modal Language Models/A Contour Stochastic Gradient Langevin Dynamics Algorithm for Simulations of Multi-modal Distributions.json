{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2010.09800",
    "title": "A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions",
    "abstract": "We propose an adaptively weighted stochastic gradient Langevin dynamics algorithm (SGLD), so-called contour stochastic gradient Langevin dynamics (CSGLD), for Bayesian learning in big data statistics. The proposed algorithm is essentially a scalable dynamic importance sampler, which automatically flattens the target distribution such that the simulation for a multi-modal distribution can be greatly facilitated. Theoretically, we prove a stability condition and establish the asymptotic convergence of the self-adapting parameter to a unique fixed-point, regardless of the non-convexity of the original energy function; we also present an error analysis for the weighted averaging estimators. Empirically, the CSGLD algorithm is tested on multiple benchmark datasets including CIFAR10 and CIFAR100. The numerical results indicate its superiority to avoid the local trap problem in training deep neural networks.",
    "bib_name": "deng2022contourstochasticgradientlangevin",
    "md_text": "# A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions\nWei Deng Department of Mathematics Purdue University West Lafayette, IN, USA weideng056@gmail.com Guang Lin Departments of Mathematics & School of Mechanical Engineering Purdue University West Lafayette, IN, USA guanglin@purdue.edu Faming Liang \u2217 Departments of Statistics Purdue University West Lafayette, IN, USA fmliang@purdue.edu\n# Abstract\nWe propose an adaptively weighted stochastic gradient Langevin dynamics algorithm (SGLD), so-called contour stochastic gradient Langevin dynamics (CSGLD), for Bayesian learning in big data statistics. The proposed algorithm is essentially a scalable dynamic importance sampler, which automatically flattens the target distribution such that the simulation for a multi-modal distribution can be greatly facilitated. Theoretically, we prove a stability condition and establish the asymptotic convergence of the self-adapting parameter to a unique fixed-point, regardless of the non-convexity of the original energy function; we also present an error analysis for the weighted averaging estimators. Empirically, the CSGLD algorithm is tested on multiple benchmark datasets including CIFAR10 and CIFAR100. The numerical results indicate its superiority to avoid the local trap problem in training deep neural networks.\nAI safety has long been an important issue in the deep learning community. A promising solution to the problem is Markov chain Monte Carlo (MCMC), which leads to asymptotically correct uncertainty quantification for deep neural network (DNN) models. However, traditional MCMC algorithms [Metropolis et al., 1953, Hastings, 1970] are not scalable to big datasets that deep learning models rely on, although they have achieved significant successes in many scientific areas such as statistical physics and bioinformatics. It was not until the study of stochastic gradient Langevin dynamics (SGLD) [Welling and Teh, 2011] that resolves the scalability issue encountered in Monte Carlo computing for big data problems. Ever since, a variety of scalable stochastic gradient Markov chain Monte Carlo (SGMCMC) algorithms have been developed based on strategies such as Hamiltonian dynamics [Chen et al., 2014, Ma et al., 2015, Ding et al., 2014], Hessian approximation [Ahn et al., 2012, Li et al., 2016, \u00b8Sim\u00b8sekli et al., 2016], and higher-order numerical schemes [Chen et al., 2015, Li et al., 2019]. Despite their theoretical guarantees in statistical inference [Chen et al., 2015, Teh et al., 2016, Vollmer et al., 2016] and non-convex optimization [Zhang et al., 2017, Raginsky et al., 2017, Xu et al., 2018], these algorithms often converge slowly, which makes them hard to be used for efficient uncertainty quantification for many AI safety problems. To develop more efficient SGMCMC algorithms, we seek inspirations from traditional MCMC algorithms, such as simulated annealing [Kirkpatrick et al., 1983], parallel tempering [Swendsen and Wang, 1986, Geyer, 1991], and flat histogram algorithms [Berg and Neuhaus, 1991, Wang and Landau,\n2001]. In particular, simulated annealing proposes to decay temperatures to increase the hitting probability to the global optima [Mangoubi and Vishnoi, 2018], which, however, often gets stuck into a local optimum with a fast cooling schedule. Parallel tempering proposes to swap positions of neighboring Markov chains according to an acceptance-rejection rule. However, under the mini-batch setting, it often requires a large correction which is known to deteriorate its performance [Deng et al., 2020]. The flat histogram algorithms, such as the multicanonical [Berg and Neuhaus, 1991] and Wang-Landau [Wang and Landau, 2001] algorithms, were first proposed to sample discrete states of Ising models by yielding a flat histogram in the energy space, and then extended as a general dynamic importance sampling algorithm, the so-called stochastic approximation Monte Carlo (SAMC) algorithm [Liang, 2005, Liang et al., 2007, Liang, 2009]. Theoretical studies [Leli\u00e8vre et al., 2008, Liang, 2010, Fort et al., 2015] support the efficiency of the flat histogram algorithms in Monte Carlo computing for small data problems. However, it is still unclear how to adapt the flat histogram idea to accelerate the convergence of SGMCMC, ensuring efficient uncertainty quantification for AI safety problems. This paper proposes the so-called contour stochastic gradient Langevin dynamics (CSGLD) algorithm, which successfully extends the flat histogram idea to SGMCMC. Like the SAMC algorithm [Liang, 2005, Liang et al., 2007, Liang, 2009], CSGLD works as a dynamic importance sampling algorithm, which adaptively adjusts the target measure at each iteration and accounts for the bias introduced thereby by importance weights. However, theoretical analysis for the two types of dynamic importance sampling algorithms can be quite different due to the fundamental difference in their transition kernels. We proceed by justifying the stability condition for CSGLD based on the perturbation theory, and establishing ergodicity of CSGLD based on newly developed theory for the convergence of adaptive SGLD. Empirically, we test the performance of CSGLD through a few experiments. It achieves remarkable performance on some synthetic data, UCI datasets, and computer vision datasets such as CIFAR10 and CIFAR100.\n# 2 Contour stochastic gradient Langevin dynamics\n \u221d\u2212  \u2208X where X denotes the sample space, U(x) is the energy function, and \u03c4 is the temperature. It is known that when U(x) is highly non-convex, SGLD can mix very slowly [Raginsky et al., 2017]. To accelerate the convergence, we exploit the flat histogram idea in SGLD. Suppose that we have partitioned the sample space X into m subregions based on the energy function U(x): X1 = {x : U(x) \u2264u1}, X2 = {x : u1 < U(x) \u2264u2}, . . ., Xm\u22121 = {x : um\u22122 < U(x) \u2264 um\u22121}, and Xm = {x : U(x) > um\u22121}, where \u2212\u221e< u1 < u2 < \u00b7 \u00b7 \u00b7 < um\u22121 < \u221eare specified by the user. For convenience, we set u0 = \u2212\u221eand um = \u221e. Without loss of generality, we assume ui+1 \u2212ui = \u2206u for i = 1, . . . , m \u22122. We propose to simulate from a flattened density\n \u221d\u2212  \u2208X where X denotes the sample space, U(x) is the energy function, and \u03c4 is the temperature. It is known that when U(x) is highly non-convex, SGLD can mix very slowly [Raginsky et al., 2017]. To accelerate the convergence, we exploit the flat histogram idea in SGLD.\nSuppose that we have partitioned the sample space X into m subregions based on the energy function U(x): X1 = {x : U(x) \u2264u1}, X2 = {x : u1 < U(x) \u2264u2}, . . ., Xm\u22121 = {x : um\u22122 < U(x) \u2264 um\u22121}, and Xm = {x : U(x) > um\u22121}, where \u2212\u221e< u1 < u2 < \u00b7 \u00b7 \u00b7 < um\u22121 < \u221eare specified by the user. For convenience, we set u0 = \u2212\u221eand um = \u221e. Without loss of generality, we assume ui+1 \u2212ui = \u2206u for i = 1, . . . , m \u22122. We propose to simulate from a flattened density\nhere \u03b6 > 0 is a hyperparameter controlling the geometric property of the flatted density (see Figure a) for illustration), and \u03b8 = (\u03b8(1), \u03b8(2), . . . , \u03b8(m)) is an unknown vector taking values in the space:\n\u0398 = \ufffd (\u03b8(1), \u03b8(2), \u00b7 \u00b7 \u00b7 , \u03b8(m)) \ufffd\ufffd0 < \u03b8(1), \u03b8(2), \u00b7 \u00b7 \u00b7 , \u03b8(m) < 1 and m \ufffd i=1 \u03b8(i) = 1 \ufffd .\n# 2.1 A na\u00efve contour SGLD It is known if we set \u2020\nIt is known if we set \u2020\n(1)\n(2)\n(3)\n(4)\nthe algorithm will act like the SAMC algorithm [Liang et al., 2007], yielding a flat histogram in the space of energy (see the pink curve in Figure 1(b)). Theoretically, such a density flattening strategy enables a sharper logarithmic Sobolev inequality and accelerates the convergence of simulations [Leli\u00e8vre et al., 2008, Fort et al., 2015]. However, such a density flattening setting only works under the framework of the Metropolis algorithm [Metropolis et al., 1953]. A na\u00efve application of the step function in formula (4(i)) to SGLD results in \u2202log \u03a8\u03b8(u) \u2202u = 1 \u03a8\u03b8(u) \u2202\u03a8\u03b8(u) \u2202u = 0 almost everywhere, which leads to the vanishing-gradient problem for SGLD. Calculating the gradient for the na\u00efve contour SGLD, we have \ufffd \ufffd\n# 2.2 How to resolve the vanishing gradient\nTo tackle this issue, we propose to set \u03a8\u03b8(u) as a piecewise continuous function:\n\ufffd \ufffd \ufffd where \u03b8(0) is fixed to \u03b8(1) for simplicity. A direct calculation shows that \ufffd \ufffd\nwhere J(x) \u2208{1, 2, \u00b7 \u00b7 \u00b7 , m} denotes the index that x belongs to, i.e., uJ(x)\u22121 < U(x) \u2264uJ(x)\n# 2.3 Estimation via stochastic approximation\nSince \u03b8\u22c6is unknown, we propose to estimate it on the fly under the framework of stochastic approximation [Robbins and Monro, 1951]. Provided that a scalable transition kernel \u03a0\u03b8k(xk, \u00b7) is available and the energy function U(x) on the full data can be efficiently evaluated, the weighted density \u03d6\u03a8\u03b8(x) can be simulated by iterating between the following steps:\napproximation [Robbins and Monro, 1951]. Provided that a scalable transition kernel \u03a0\u03b8k(xk, \u00b7) is available and the energy function U(x) on the full data can be efficiently evaluated, the weighted density \u03d6\u03a8\u03b8(x) can be simulated by iterating between the following steps: (i) Simulate xk+1 from \u03a0\u03b8k(xk, \u00b7), which admits \u03d6\u03b8k(x) as the invariant distribution, (ii) \u03b8k+1(i) = \u03b8k(i) + \u03c9k+1\u03b8\u03b6 k(J(xk+1)) \ufffd 1i=J(xk+1) \u2212\u03b8k(i) \ufffd for i \u2208{1, 2, \u00b7 \u00b7 \u00b7 , m}. (7) where \u03b8k denotes a working estimate of \u03b8 at the k-th iteration. We expect that in a long run, such an algorithm can achieve an optimization-sampling equilibrium such that \u03b8k converges to the fixed point \u03b8\u22c6and the random vector xk converges weakly to the distribution \u03d6\u03a8\u03b8\u22c6(x). To make the algorithm scalable to big data, we adopt the Langevin transition kernel for drawing samples at each iteration, for which a mini-batch of data can be used to accelerate computation. In addition, evaluating the energy U(x) on the full data can be quite expensive, while it is free to obtain the stochastic energy \ufffdU(x) in evaluating the stochastic gradient \u2207x \ufffdU(x) due to the nature of auto-differentiation [Paszke et al., 2017]. For this reason, we propose a stochastic index J \ufffdU(x)\n\ufffd  \u2212 \ufffd  \u2208{ \u00b7 \u00b7 \u00b7} where \u03b8k denotes a working estimate of \u03b8 at the k-th iteration. We expect that in a long run, such an algorithm can achieve an optimization-sampling equilibrium such that \u03b8k converges to the fixed point \u03b8\u22c6and the random vector xk converges weakly to the distribution \u03d6\u03a8\u03b8\u22c6(x).\n\ufffd  \u2212 \ufffd  \u2208{ \u00b7 \u00b7 \u00b7} where \u03b8k denotes a working estimate of \u03b8 at the k-th iteration. We expect that in a long run, such an algorithm can achieve an optimization-sampling equilibrium such that \u03b8k converges to the fixed point \u03b8\u22c6and the random vector xk converges weakly to the distribution \u03d6\u03a8\u03b8\u22c6(x). To make the algorithm scalable to big data, we adopt the Langevin transition kernel for drawing samples at each iteration, for which a mini-batch of data can be used to accelerate computation. In addition, evaluating the energy U(x) on the full data can be quite expensive, while it is free to obtain the stochastic energy \ufffdU(x) in evaluating the stochastic gradient \u2207x \ufffdU(x) due to the nature of auto-differentiation [Paszke et al., 2017]. For this reason, we propose a stochastic index J \ufffdU(x)\nTo make the algorithm scalable to big data, we adopt the Langevin transition kernel for drawing samples at each iteration, for which a mini-batch of data can be used to accelerate computation. In addition, evaluating the energy U(x) on the full data can be quite expensive, while it is free to obtain the stochastic energy \ufffdU(x) in evaluating the stochastic gradient \u2207x \ufffdU(x) due to the nature of auto-differentiation [Paszke et al., 2017]. For this reason, we propose a stochastic index J \ufffdU(x)\n \ufffd \ufffd  \ufffd where N is the sample size of the full dataset and n is the mini-batch size. Let {\u03f5k}\u221e k=1 and {\u03c9k}\u221e k=1 denote the learning rates and step sizes for SGLD and stochastic approximation, respectively. Given the above notations, the proposed algorithm can be presented in Algorithm 1, which can be viewed as a scalable Wang-Landau algorithm for deep learning and big data problems.\n# 2.4 Related work\nCompared to the existing MCMC algorithms, the proposed algorithm has a few innovations: \u00a7Eq.(6) shows a practical numerical scheme. An alternative is presented in the supplementary material\n(5)\n(6)\n(7)\n(8)\nAlgorithm 1 Contour SGLD Algorithm. One can conduct a resampling step from the pool of mportance samples according to the importance weights to obtain the original distribution. For more calable updates, one can adopt the stochastic approximation scheme Eq.(15) in Deng et al. [2022]. [1.] (Data subsampling) Simulate a mini-batch of data of size n from the whole dataset of size N; Compute the stochastic gradient \u2207x \ufffdU(xk) and stochastic energy \ufffdU(xk). [2.] (Simulation step) Sample xk+1 using the SGLD algorithm based on xk and \u03b8k, i.e.,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1cc4/1cc4553e-4e28-43e6-9038-3184606baa7e.png\" style=\"width: 50%;\"></div>\nFirst, CSGLD is an adaptive MCMC algorithm based on the Langevin transition kernel instead of the Metropolis transition kernel [Liang et al., 2007, Fort et al., 2015]. As a result, the existing convergence theory for the Wang-Landau algorithm does not apply. To resolve this issue, we first prove a stability condition for CSGLD based on the perturbation theory, and then verify regularity conditions for the solution of the Poisson equation so that the fluctuations of the mean-field system induced by CSGLD get controlled, which eventually ensures convergence of CSGLD. Second, the use of the stochastic index J \ufffdU(x) in Eq.(8) avoids the evaluation of U(x) on the full data and thus significantly accelerates the computation of the algorithm, although it leads to a small bias, depending on the variance of the energy estimators, in parameter estimation. Compared to other methods, such as using a fixed sub-dataset to estimate U(x), the implementation is much simpler. Moreover, combining the variance reduction of the noisy energy estimators [Deng et al., 2021], the bias also decreases to zero asymptotically as \u03f5 \u21920. Third, unlike the existing SGMCMC algorithms [Welling and Teh, 2011, Chen et al., 2014, Ma et al., 2015], CSGLD works as a dynamic importance sampler which flattens the target distribution and reduces the energy barriers for the sampler to traverse between different regions of the energy landscape (see Figure 1(a) for illustration). The sampling bias introduced thereby is accounted for by the importance weight \u03b8\u03b6(J \ufffdU(\u00b7)). Interestingly, CSGLD possesses a self-adjusting mechanism to ease escapes from local traps, which is similar to the self-repulsive dynamics [Ye et al., 2020] and can be explained in Figure 2. That is, in order to escape from local traps, CSGLD is sometimes forced to move toward higher energy regions by adopting negative learning rates. This is a very attractive feature for simulations of multi-modal distributions.\n# 3 Theoretical study of the CSGLD algorithm\nIn this section, we study the convergence of CSGLD algorithm under the framework of stochastic approximation and show the ergodicity property based on weighted averaging estimators.\n# 3.1 Convergence analysis\n\ufffd \ufffd \ufffd  \ufffd \ufffd Notably, \ufffdH(\u03b8, x) works under an empirical measure \u03d6\u03b8(x) which approximates the invariant measure \u03d6\u03a8\u03b8(x) \u221d \u03c0(x) \u03a8\u03b6 \u03b8(U(x)) asymptotically as \u03f5 \u21920 and n \u2192N. As shown in Lemma 1, we have\n(9)\n(10)\n(11)\n(12)\n\ufffd X \ufffd where \u03b8\u22c6= ( \ufffd X1 \u03c0(x)dx, \ufffd X2 \u03c0(x)dx, . . . , \ufffd Xm \u03c0(x)dx), Z\u03b8 is the normalizing constant, \u03b2(\u03b8) is a perturbation term, \u03b5 is a small error depending on \u03f5, n and m. The mean-field equation implies that for any \u03b6 > 0, \u03b8k converges to a small neighbourhood of \u03b8\u22c6. By applying perturbation theory and setting the Lyapunov function V(\u03b8) = 1 2\u2225\u03b8\u22c6\u2212\u03b8\u22252, we can establish the stability condition: Lemma 1 (Stability, informal version of Lemma 3). Given a small enough \u03f5 (learning rate), a large enough n (batch size) and m (partition number), there is a constant \u03c6 = inf\u03b8 Z\u22121 \u03b8 > 0 such that the mean-field h(\u03b8) satisfies \ufffd \ufffd\nwhere Var(\u03ben(x)) denotes the variance of the noise of the stochastic energy estimator \u03ben(\u00b7) of batch size n and the variance decays to 0 as n \u2192N. Together with the tool of Poisson equation [Benveniste et al., 1990, Andrieu et al., 2005], which controls the fluctuation of \ufffdH(\u03b8, x) \u2212h(\u03b8), we can establish convergence of \u03b8k in Theorem 1, whose proof is given in the supplementary material. Theorem 1 (L2 convergence rate, informal version of Theorem 3). Given standard smoothness and dissipativity assumptions, a small enough learning rate \u03f5k, a large partition number m and a large batch size n, \u03b8k converges to \u03b8\u22c6such that \ufffd \ufffd\n\ufffd \ufffd where k0 is some large enough integer and \u03b8\u22c6= ( \ufffd X1 \u03c0(x)dx, \ufffd X2 \u03c0(x)dx, . . . , \ufffd Xm \u03c0(x)dx).\n# \ufffd 3.2 Ergodicity and dynamic importance sampler\nCSGLD belongs to the class of adaptive MCMC algorithms, but its transition kernel is based on SGLD instead of the Metropolis algorithm. As such, the ergodicity theory for traditional adaptive MCMC algorithms [Roberts and Rosenthal, 2007, Andrieu and \u00c9ric Moulines, 2006, Fort et al., 2011 Liang, 2010] is not directly applicable. To tackle this issue, we conduct the following theoretical study. First, rewrite (9) as \ufffd \ufffd\n(14)\nFinally, we consider the problem of estimating the quantity \ufffd X f(x)\u03c0(x)dx. Recall that \u03c0(x) is the target distribution that we would like to make inference for. To estimate this quantity, we naturally consider the weighted averaging estimator \ufffdk i=1 \u03b8\u03b6 i (J \ufffd U(xi))f(xi) \ufffdk i=1 \u03b8\u03b6 i (J \ufffd U(xi)) by treating \u03b8\u03b6(J \ufffdU(xi)) as the dynamic importance weight of the sample xi for i = 1, 2, . . . , k. The convergence of this estimator is established in Theorem 2, which can be proved by repeated applying Theorem 1 and Lemma 2 with the details given in the supplementary material. Theorem 2 (Convergence of the Weighted Averaging Estimators, informal version of Theorem 4). Given the smoothness, dissipativity and other mild assumptions, for any bounded function f, we have \ufffd\ufffd\ufffd\ufffd\ufffdE \ufffd\ufffdk i=1 \u03b8\u03b6 i (J \ufffd U(xi))f(xi) \ufffdk i=1 \u03b8\u03b6 i (J \ufffd U(xi)) \ufffd \u2212 \ufffd \u03c7 f(x)\u03c0(dx) \ufffd\ufffd\ufffd\ufffd\ufffd= O \uf8eb \uf8ed1 k\u03f5 + \u221a\u03f5 + \ufffd\ufffdk i=1 \u03c9k k + 1 \u221am + sup x \ufffd Var(\u03ben(x The bias of the weighted averaging estimator decreases if one applies a larger batch size, a finer sample space partition, a smaller learning rate \u03f5, and smaller step sizes {\u03c9k}k\u22650. Admittedly, the order of this bias is slightly larger than O \ufffd1 k\u03f5 + \u03f5 \ufffd achieved by the standard SGLD. We note that this is necessary as simulating from the flattened distribution \u03d6\u03a8\u03b8\u22c6often leads to a much faster convergence, see e.g. the green curve v.s. the purple curve in Figure 1(c). Discussions on more scalable updates The stochastic approximation update (7)(ii) yields a global stability property but may not be scalable enough in some big data problems. For more scalable updates, one can adopt an elegant stochastic approximation update proposed in Deng et al. [2022] \ufffd \ufffd\n\ufffd \ufffd where \u03b8(i) converges to a smoother estimate of \ufffd\ufffd Xi \u03c0(x)dx) \ufffd1 \u03b6 instead of the original target \ufffd Xi \u03c0(x)dx. Since high energy region often yields exponentially decreasing probability mass, the exponent 1 \u03b6 given \u03b6 \u226b1 greatly facilitates the estimation tasks.\n# 4 Numerical studies\n# 4.1 Simulations of multi-modal distributions\nA Gaussian mixture distribution The first numerical study is to test the performance of CSGLD on a Gaussian mixture distribution \u03c0(x) = 0.4N(\u22126, 1) + 0.6N(4, 1). In each experiment, the algorithm was run for 107 iterations. We fix the temperature \u03c4 = 1 and the learning rate \u03f5 = 0.1. The step size for stochastic approximation follows \u03c9k = 1 k0.6+100. The sample space is partitioned into 50 subregions with \u2206u = 1. The stochastic gradients are simulated by injecting additional random noises following N(0, 0.01) to the exact gradients. For comparison, SGLD is chosen as the baseline algorithm and implemented with the same setup as CSGLD. We repeat the experiments 10 times and report the average and the associated standard deviation. We first assume that \u03b8\u22c6is known and plot the energy functions for both \u03c0(x) and \u03d6\u03a8\u03b8\u22c6with different values of \u03b6. Figure 1(a) shows that the original energy function has a rather large energy barrier which strongly affects the communication between two modes of the distribution. In contrast, CSGLD samples from a modified energy function, which yields a flattened landscape and reduced energy barriers. For example, with \u03b6 = 0.75, the energy barrier for this example is greatly reduced from 12 to as small as 2. Consequently, the local trap problem can be greatly alleviated. Regarding the bizarre peaks around x = 4, we leave the study in the supplementary material. Figure 1(b) summarizes the estimates of \u03b8\u22c6with \u03b6 = 0.75, which matches the ground truth value of \u03b8\u22c6 very well. Notably, we see that \u03b8\u22c6(i) decays exponentially fast as the partition index i increases, which indicates the exponentially decreasing probability of visiting high energy regions and a severe local trap problem. CSGLD tackles this issue by adaptively updating the transition kernel or, equivalently, the invariant distribution such that the sampler moves like a \u201crandom walk\u201d in the space of energy. In particular, setting \u03b6 = 1 leads to a flat histogram of energy (for the samples produced by CSGLD). To explore the performance of CSGLD in quantity estimation with the weighed averaging estimator, we compare CSGLD (\u03b6 = 0.75) with SGLD and KSGLD in estimating the posterior mean\n(15)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6e5c/6e5c722d-aa57-4ecf-9a9e-00108c633722.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Original v.s. trial energies (b) \u03b8\u2019s estimates and histograms</div>\n<div style=\"text-align: center;\">Figure 1: Comparison between SGLD and CSGLD: figure (a) presents only the first 12 partitions for an illustrative purpose; KSGLD in figure (c) is implemented by assuming \u03b8\u22c6is known.</div>\n\ufffd X x\u03c0(x)dx, where KSGLD was implemented by assuming \u03b8\u22c6is known and sampling from \u03d6\u03a8\u03b8\u22c6 directly. Each algorithm was run for 10 times, and we recorded the mean absolute estimation error along with iterations. As shown in Figure 1(c), the estimation error of SGLD decays quite slow and rarely converges due to the high energy barrier. On the contrary, KSGLD converges much faster, which shows the advantage of sampling from a flattened distribution \u03d6\u03a8\u03b8\u22c6. Admittedly, \u03b8\u22c6is unknown in practice. CSGLD instead adaptively updates its invariant distribution while optimizing the parameter \u03b8 until an optimization-sampling equilibrium is reached. In the early period of the run, CSGLD converges slightly slower than KSGLD, but soon it becomes as efficient as KSGLD. Finally, we compare the sample path and learning rate for CSGLD and SGLD. As shown in Figure 2(a), SGLD tends to be trapped in a deep local optimum for an exponentially long time. CSGLD, in contrast, possesses a self-adjusting mechanism for escaping from local traps. In the early period of a run, CSGLD might suffer from a similar local-trap problem as SGLD (see Figure 2(b)). In this case, the components of \u03b8 corresponding to the current subregion will increase very fast, eventually rendering a smaller or even negative gradient multiplier which bounces the sampler back to high energy regions. To illustrate the process, we plot a bouncy zone and an absorbing zone in Figure 2(c). The bouncy zone enables the sampler to \u201cjump\u201d over large energy barriers to explore other modes. As the run continues, \u03b8k converges to \u03b8\u22c6. Figure 2(d) shows that larger bouncy \u201cjumps\u201d (in red lines) can potentially be induced in the bouncy zone, which occurs in both local and global optima. Due to the self-adjusting mechanism, CSGLD has the local trap problem much alleviated.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14e4/14e46d95-8b61-46bb-a521-18806619f8bd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/45c8/45c87ed5-3192-4080-b9c2-0aa1348fac57.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) SGLD paths</div>\n<div style=\"text-align: center;\">Figure 2: Sample trajectories of SGLD and CSGLD: figures (a) and (c) are implemented by 100,000 iterations with a thinning factor 100 and \u03b6 = 0.75, while figure (b) utilizes a thinning factor 10.</div>\nA synthetic multi-modal distribution We next simulate from a distribution \u03c0(x) \u221de\u2212U(x), where U(x) = \ufffd2 i=1 x(i)2\u221210 cos(1.2\u03c0x(i)) 3 and x = (x(1), x(2)). We compare CSGLD with SGLD, replica exchange SGLD (reSGLD) [Deng et al., 2020], and SGLD with cyclic learning rates (cycSGLD) [Zhang et al., 2020] and detail the setups in the supplementary material. Figure 3(a) shows that the distribution contains nine important modes, where the center mode has the largest probability mass and the four modes on the corners have the smallest mass. We see in Figure 3(b) that SGLD spends too much time in local regions and only identifies three modes. cycSGLD has a better ability to explore the distribution by leveraging large learning rates cyclically. However, as illustrated in Figure 3(c), such a mechanism is still not efficient enough to resolve the local trap issue for this problem. reSGLD proposes to include a high-temperature process to encourage exploration and allows interactions between the two processes via appropriate swaps. We observe in Figure 3(d)\nthat reSGLD obtains both the exploration and exploitation abilities and yields a much better result. However, the noisy energy estimator may hinder the swapping efficiency and it becomes difficult to estimate a few modes on the corners. As to our algorithm, CSGLD first simulates the importance samples and recovers the original distribution according to the importance weights. We notice that the samples from CSGLD can traverse freely in the parameter space and eventually achieve a remarkable performance, as shown in Figure 3(e).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8778/8778150a-8f02-4b16-8145-70595d3ff945.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Simulations of a multi-modal distribution. A resampling scheme is used for CSGLD.</div>\n# 4.2 UCI data\nWe tested the performance of CSGLD on the UCI regression datasets. For each dataset, we normalized all features and randomly selected 10% of the observations for testing. Following [Hernandez-Lobato and Adams, 2015], we modeled the data using a Multi-Layer Perception (MLP) with a single hidden layer of 50 hidden units. We set the mini-batch size n = 50 and trained the model for 5,000 epochs. The learning rate was set to 5e-6 and the default L2-regularization coefficient is 1e-4. For all the datasets, we used the stochastic energy N n \ufffdU(x) to evaluate the partition index. We set the energy bandwidth \u2206u = 100. We fine-tuned the temperature \u03c4 and the hyperparameter \u03b6. For a fair comparison, each algorithm was run 10 times with fixed seeds for each dataset. In each run, the performance of the algorithm was evaluated by averaging over 50 models, where the averaging estimator was used for SGD and SGLD and the weighted averaging estimator was used for CSGLD. As shown in Table 1, SGLD outperforms the stochastic gradient descent (SGD) algorithm for most datasets due to the advantage of a sampling algorithm in obtaining more informative modes. Since all these datasets are small, there is only very limited potential for improvement. Nevertheless, CSGLD still consistently outperforms all the baselines including SGD and SGLD. The contour strategy proposed in the paper can be naturally extended to SGHMC [Chen et al., 2014, Ma et al., 2015] without affecting the theoretical results. In what follows, we adopted a numerical method proposed by Saatci and Wilson [2017] to avoid extra hyperparameter tuning. We set the momentum term to 0.9 and simply inherited all the other parameter settings used in the above experiments. In such a case, we compare the contour SGHMC (CSGHMC) with the baselines, including M-SGD (Momentum SGD) and SGHMC. The comparison indicates that some improvements can be achieved by including the momentum.\n<div style=\"text-align: center;\">Table 1: Algorithm evaluation using average root-mean-square error and its standard deviation</div>\nDataset\nEnergy\nConcrete\nYacht\nWine\nHyperparameters (\u03c4/\u03b6)\n1/1\n5/1\n1/2.5\n5/10\nSGD\n1.13\u00b10.07\n4.60\u00b10.14\n0.81\u00b10.08\n0.65\u00b10.01\nSGLD\n1.08\u00b10.07\n4.12\u00b10.10\n0.72\u00b10.07\n0.63\u00b10.01\nCSGLD\n1.02\u00b10.06\n3.98\u00b10.11\n0.69\u00b10.06\n0.62\u00b10.01\nM-SGD\n0.95\u00b10.07\n4.32\u00b10.27\n0.73\u00b10.08\n0.71\u00b10.02\nSGHMC\n0.77\u00b10.06\n4.25\u00b10.19\n0.66\u00b10.07\n0.67\u00b10.02\nCSGHMC\n0.76\u00b10.06\n4.15\u00b10.20\n0.72\u00b10.09\n0.65\u00b10.01\nThis section compares only CSGHMC with M-SGD and SGHMC due to the popularity of momentum in accelerating computation for computer vision datasets. We keep partitioning the sample space according to the stochastic energy N n \ufffdU(x), where a mini-batch data of size n is randomly chosen from the full dataset of size N at each iteration. Notably, such a strategy significantly accelerates the computation of CSGHMC. As a result, CSGHMC has almost the same computational cost as\nSGHMC and SGD. To reduce the bias associated with the stochastic energy, we choose a large batch size n = 1, 000. For more discussions on the hyperparameter settings, we refer readers to section D.4 in the supplementary material. CIFAR10 is a standard computer vision dataset with 10 classes and 60,000 images, for which 50,000 images were used for training and the rest for testing. We modeled the data using a Resnet of 20 layers (Resnet20) [He et al., 2016]. In particular, for CSGHMC, we considered a partition of the energy space in 200 subregions, where the energy bandwidth was set to \u2206u = 1000. We trained the model for a total of 1000 epochs and evaluated the model every ten epochs based on two criteria, namely, best point estimate (BPE) and Bayesian model average (BMA). We repeated each experiment 10 times and reported in Table 2 the average prediction accuracy and the standard deviation. In the first set of experiments, all the algorithms utilized a fixed learning rate \u03f5 = 2e \u22127 and a fixed temperature \u03c4 = 0.01 under the Bayesian setting.SGHMC performs quite similarly to M-SGD, both obtaining around 90% accuracy in BPE and 92% in BMA. Notably, in this case, simulated annealing is not applied to any of the algorithms and achieving the state-of-the-art is quite difficult. However, BMA still consistently outperforms BPE, implying the great potential of advanced MCMC techniques in deep learning. Instead of simulating from \u03c0(x) directly, CSGHMC adaptively simulates from a flattened distribution \u03d6\u03b8\u22c6and adjusts the sampling bias by dynamic importance weights. As a result, the weighted averaging estimators obtain an improvement by as large as 0.8% on BMA. In addition, the flattened distribution facilitates optimization and the increase in BPE is quite significant. In the second set of experiments, we employed a decaying schedule on both learning rates and temperatures (if applicable) to obtain simulated annealing effects. For the learning rate, we fix it at 2\u00d7 10\u22126 in the first 400 epochs and then decayed it by a factor of 1.01 at each epoch. For the temperature, we consistently decayed it by a factor of 1.01 at each epoch. We call the resulting algorithms by saM-SGD, saSGHMC, and saCSGHMC, respectively. Table 2 shows that the performances of all algorithms are increased quite significantly, where the fine-tuned baselines already obtained the state-of-the-art results. Nevertheless, saCSGHMC further improves BPE by 0.25% and slightly improve the highly optimized BMA by nearly 0.1%. CIFAR100 dataset has 100 classes, each of which contains 500 training images and 100 testing images. We follow a similar setup as CIFAR10, except that \u2206u is set to 5000. For M-SGD, BMA can be better than BPE by as large as 5.6%. CSGHMC has led to an improvement of 3.5% on BPE and 2% on BMA, which further demonstrates the superiority of advanced MCMC techniques. Table 2 also shows that with the help of both simulated annealing and importance sampling, saCSGHMC can outperform the highly optimized baselines by almost 1% accuracy on BPE and 0.7% on BMA. The significant improvements show the advantage of the proposed method in training DNNs. Table 2: Experiments on CIFAR10 & 100 using Resnet20, where BPE and BMA are short for best point estimate and Bayesian model average, respectively.\nAlgorithms\nCIFAR10\nCIFAR100\nBPE\nBMA\nBPE\nBMA\nM-SGD\n90.02\u00b10.06\n92.03\u00b10.08\n61.41\u00b10.15\n67.04\u00b10.12\nSGHMC\n90.01\u00b10.07\n91.98\u00b10.05\n61.46\u00b10.14\n66.43\u00b10.11\nCSGHMC\n90.87\u00b10.04\n92.85\u00b10.05\n63.97\u00b10.21\n68.94\u00b10.23\nsaM-SGD\n93.83\u00b10.07\n94.25\u00b10.04\n69.18\u00b10.13\n71.83\u00b10.12\nsaSGHMC\n93.80\u00b10.06\n94.24\u00b10.06\n69.24\u00b10.11\n71.98\u00b10.10\nsaCSGHMC\n94.06\u00b10.07\n94.33\u00b10.07\n70.18\u00b10.15\n72.67\u00b10.15\n# 5 Conclusion\nWe have proposed CSGLD as a general scalable Monte Carlo algorithm for both simulation and optimization tasks. CSGLD automatically adjusts the invariant distribution during simulations to facilitate escaping from local traps and traversing over the entire energy landscape. The sampling bias introduced thereby is accounted for by dynamic importance weights. We proved a stability condition for the mean-field system induced by CSGLD together with the convergence of its self-adapting parameter \u03b8 to a unique fixed point \u03b8\u22c6. We established the convergence of a weighted averaging estimator for CSGLD. The bias of the estimator decreases as we employ a finer partition, a larger mini-batch size, and smaller learning rates and step sizes. We tested CSGLD and its variants on a few examples, which show their great potential in deep learning and big data computing.\nOur algorithm shows a potential to achieve free mode explorations in complex systems such as deep neural networks and greatly avoid the local trap problem. It is an extension of the flat histogram algorithms from the Metropolis kernel to the Langevin kernel and paves the way for future research in various dynamic importance samplers and adaptive biasing force (ABF) techniques for big data problems. The Bayesian community and the researchers in the area of Monte Carlo methods will enjoy the benefit of our work.\n# Acknowledgment\nLiang\u2019s research was supported in part by the grants DMS-2015498, R01-GM117597 and R01GM126089. Lin acknowledges the support from NSF (DMS-1555072, DMS-1736364), BNL Subcontract 382247, W911NF-15-1-0562, and DE-SC0021142.\n# References\n# Supplimentary Material for \u201cA Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions\u201d\nThe supplementary material is organized as follows: Section A provides a review for the related methodologies, Section B proves the stability condition and convergence of the self-adapting parameter, Section C establishes the ergodicity of the contour stochastic gradient Langevin dynamics (CSGLD) algorithm, and Section D provides more discussions for the algorithm.\n# A Background on stochastic approximation and Poisson equation\n# A.1 Stochastic approximation\nStochastic approximation [Benveniste et al., 1990] provides a standard framework for the development of adaptive algorithms. Given a random field function \ufffdH(\u03b8, x), the goal of the stochastic approximation algorithm is to find the solution to the mean-field equation h(\u03b8) = 0, i.e., solving \ufffd\n\ufffd X \ufffd where x \u2208X \u2282Rd, \u03b8 \u2208\u0398 \u2282Rm, \ufffdH(\u03b8, x) is a random field function and \u03d6\u03b8(x) is a distribution function of x depending on the parameter \u03b8. The stochastic approximation algorithm works by repeating the following iterations (1) Draw xk+1 \u223c\u03a0\u03b8k(xk, \u00b7), where \u03a0\u03b8k(xk, \u00b7) is a transition kernel that admits \u03d6\u03b8k(x) as the invariant distribution, (2) Update \u03b8k+1 = \u03b8k + \u03c9k+1 \ufffdH(\u03b8k, xk+1) + \u03c92 k+1\u03c1(\u03b8k, xk+1), where \u03c1(\u00b7, \u00b7) denotes a bias term.\n\ufffd where x \u2208X \u2282Rd, \u03b8 \u2208\u0398 \u2282Rm, \ufffdH(\u03b8, x) is a random field function and \u03d6\u03b8(x) is a distribution function of x depending on the parameter \u03b8. The stochastic approximation algorithm works by repeating the following iterations\n(1) Draw xk+1 \u223c\u03a0\u03b8k(xk, \u00b7), where \u03a0\u03b8k(xk, \u00b7) is a transition kernel that admits \u03d6\u03b8k(x) as the invariant distribution, (2) Update \u03b8k+1 = \u03b8k + \u03c9k+1 \ufffdH(\u03b8k, xk+1) + \u03c92 k+1\u03c1(\u03b8k, xk+1), where \u03c1(\u00b7, \u00b7) denotes a bias term.\n \ufffd The algorithm differs from the Robbins\u2013Monro algorithm [Robbins and Monro, 1951] in that x is simulated from a transition kernel \u03a0\u03b8k(\u00b7, \u00b7) instead of the exact distribution \u03d6\u03b8k(\u00b7). As a result, a Markov state-dependent noise \ufffdH(\u03b8k, xk+1) \u2212h(\u03b8k) is generated, which requires some regularity conditions to control the fluctuation \ufffd k \u03a0k \u03b8( \ufffdH(\u03b8, x) \u2212h(\u03b8)). Moreover, it supports a more general form where a bounded bias term \u03c1(\u00b7, \u00b7) is allowed without affecting the theoretical properties of the algorithm.\n# A.2 Poisson equation\nStochastic approximation generates a nonhomogeneous Markov chain {(xk, \u03b8k)}\u221e k=1, for which the convergence theory can be studied based on the Poisson equation\n \u2212 \ufffd \u2212 where \u03a0\u03b8(x, A) is the transition kernel for any Borel subset A \u2282X and \u00b5\u03b8(\u00b7) is a function on X The solution to the Poisson equation exists when the following series converges: \ufffd\n\ufffd  \ufffd That is, the consistency of the estimator \u03b8 can be established by controlling the perturbations of \ufffd k\u22650 \u03a0k \u03b8( \ufffdH(\u03b8, x) \u2212h(\u03b8)) via imposing some regularity conditions on \u00b5\u03b8(\u00b7). Towards this goal, Benveniste et al. [1990] gave the following regularity conditions on \u00b5\u03b8(\u00b7) to ensure the convergence of the adaptive algorithm: There exist a function V : X \u2192[1, \u221e), and a constant C such that for all \u03b8, \u03b8\u2032 \u2208\u0398, \u2225\u03a0\u03b8\u00b5\u03b8(x)\u2225\u2264CV (x), \u2225\u03a0\u03b8\u00b5\u03b8(x) \u2212\u03a0\u03b8\u2032\u00b5\u03b8\u2032(x)\u2225\u2264C\u2225\u03b8 \u2212\u03b8\u2032\u2225V (x), E[V (x)] \u2264\u221e, which requires only the first order smoothness. In contrast, the ergodicity theory by Mattingly et al. [2010] and Vollmer et al. [2016] relies on the much stronger 4th order smoothness.\n# B.1 CSGLD algorithm\nTo make the theory more general, we slightly extend CSGLD by allowing a higher order bias term The resulting algorithm works by iterating between the following two steps:\n\ufffd \ufffd \ufffd  \ufffd for some constant \u03b6 > 0, and \u03c1(\u03b8k, xk+1) is a bias term.\n# B.2 Convergence of parameter estimation\nTo establish the convergence of \u03b8k, we make the following assumptions: Assumption A1 (Compactness). The space \u0398 is compact such that inf\u0398 \u03b8(i) > 0 for any i \u2208 {1, 2, . . . , m}; the perturbation term \u03c1(\u03b8, x) is also uniformly bounded for any x \u2208X and \u03b8 \u2208\u0398.\nAssumption A1 (Compactness). The space \u0398 is compact such that inf\u0398 \u03b8(i) > 0 for any i \u2208 {1, 2, . . . , m}; the perturbation term \u03c1(\u03b8, x) is also uniformly bounded for any x \u2208X and \u03b8 \u2208\u0398.\nTo simplify the proof, we consider a slightly stronger assumption such that inf\u0398 \u03b8(i) > 0 holds for any i \u2208{1, 2, . . . , m}. To relax this assumption, we refer interested readers to Fort et al. [2015] where the recurrence property was proved for the sequence {\u03b8k}k\u22651 of a similar algorithm. Such a property guarantees \u03b8k to visit often enough to a desired compact space, rendering the convergence of the sequence. By Assumption A1 and Eq.(17), it is easy to conclude that there exists a constant Q > 0 such that for any \u03b8 \u2208\u0398 and x \u2208X,\nAssumption A2 (Smoothness). U(x) is M-smooth; that is, there exists a constant M > 0 such that for any x, x\u2032 \u2208X, \u2225\u2207U(x) \u2212\u2207U(x\u2032)\u2225\u2264M\u2225x \u2212x\u2032\u2225. (19)\nAssumption A2 (Smoothness). U(x) is M-smooth; that is, there exists a constant M > 0 such that for any x, x\u2032 \u2208X,\nSmoothness is a standard assumption in the study of convergence of SGLD, see e.g. Raginsky et al. [2017], Xu et al. [2018]. Assumption A3 (Dissipativity). There exist constants \u02dcm > 0 and \u02dcb \u22650 such that for any x \u2208X and \u03b8 \u2208\u0398,\n(S1) (S2)\n(17)\n(18)\n(19)\nLemma 3 establishes a stability condition for CSGLD, which implies potential convergence of \u03b8k. Lemma 3 (Stability, restatement of Lemma 1). Suppose that Assumptions A1-A4 hold. For any \u03b8 \u2208\u0398, \u27e8h(\u03b8), \u03b8\u2212\u03b8\u22c6\u27e9\u2264\u2212\u03c6\u2225\u03b8\u2212\u03b8\u22c6\u22252+O \ufffd supx Var(\u03ben(x)) + \u03f5 + 1 m \ufffd , where \u03c6 = inf\u03b8 Z\u22121 \u03b8 > 0, \u03b8\u22c6= ( \ufffd X1 \u03c0(x)dx, \ufffd X2 \u03c0(x)dx, . . . , \ufffd Xm \u03c0(x)dx), Var(\u03ben(\u00b7)) denotes the variance of the noise of the stochastic energy estimator \u03ben(\u00b7) of batch size n and it decays to 0 as n \u2192N.\n \u221d \u03a8 \u03b8(U(x))  a fixed piecewise continuous function given by\n \u221d a fixed piecewise continuous function given by\n\ufffd the full data is used in determining the indexes of subregions, and the learning rate converges to zero. In addition, we define a piece-wise constant function\n(i) For the term I1, we have\nwhere Z\u03b8 = \ufffdm i=1 \ufffd Xi \u03c0(x)dx \u03b8(i)\u03b6 denotes the normalizing constant of \u03d6\ufffd\u03a8\u03b8(x). (ii) As to the integral I2. By Lemma 6 and the boundedness of H(\u03b8, x), we have \ufffd X Hi(\u03b8, x)(\u2212\u03d6\ufffd\u03a8\u03b8(x) + \u03d6\u03a8\u03b8(x))dx = O \ufffd1 m \ufffd . (iii) Regarding the term I3, we have for any fixed \u03b8, \ufffd X Hi(\u03b8, x) (\u2212\u03d6\u03a8\u03b8(x) + \u03d6\u03b8(x)) dx = O(\u03f5),\n(21)\n(22)\n(23)\n(24)\n# where the order of O(\u03f5) follows from Theorem 6 of Sato and Nakagawa [2014]. Plugging (23), (24) and (25) into (22), we have\nwhere \u03c6 = inf\u03b8 Z\u22121 \u03b8 > 0 by the compactness assumption A1. This concludes the proof. The following is a restatement of Lemma 3.2 [Raginsky et al., 2017], which holds for any \u03b8 in the compact space \u0398. Similar results have been shown in Deng et al. [2019]. Lemma 4 (Uniform L2 bounds). Suppose Assumptions A1, A3 and A4 hold. Given a small enough learning rate, then supk\u22651 E[\u2225xk\u22252] < \u221e. The ensure the perturbations of the stochastic approximation process decays sufficiently fast, we present the following result on the regularity properties. Lemma 5 (Solution of Poisson equation). Suppose that Assumptions A1-A4 hold. There is a solution \u00b5\u03b8(\u00b7) on X to the Poisson equation \u00b5\u03b8(x) \u2212\u03a0\u03b8\u00b5\u03b8(x) = \ufffdH(\u03b8, x) \u2212h(\u03b8). (28) In addition, for all \u03b8, \u03b8\u2032 \u2208\u0398, there exists a constant C such that E[\u2225\u03a0\u03b8\u00b5\u03b8(x)\u2225] \u2264C, E[\u2225\u03a0\u03b8\u00b5\u03b8(x) \u2212\u03a0\u03b8\u2032\u00b5\u03b8\u2032(x)\u2225] \u2264C\u2225\u03b8 \u2212\u03b8\u2032\u2225. (29)\nProof The proof hinges on verifying drift conditions proposed in Section 6 of [Andrieu et al., 2005] and the details have been given in Lemma 6 of Deng et al. [2022]. Now we are ready to prove the first main result on the convergence of \u03b8k. The technique lemmas are listed in Section B.3. Assumption A5 (Learning rate and step size). The learning rate {\u03f5k}k\u2208N is a positive non-increasing sequence of real numbers satisfying the conditions\nThe step size {\u03c9k}k\u2208N is a positive decreasing sequence of real numbers such that\nAccording to Benveniste et al. [1990], we can choose \u03c9k := A k\u03b1+B for some \u03b1 \u2208( 1 2, 1] and some suitable constants A > 0 and B > 0.\n(26)\n(27)\n(28)\n(29)\n(30)\nTheorem 3 (L2 convergence rate, restatement of Theorem 1). Suppose Assumptions A1-A5 hold. For a sufficiently large value of m, a sufficiently small learning rate sequence {\u03f5k}\u221e k=1, and a sufficiently small step size sequence {\u03c9k}\u221e k=1, {\u03b8k}\u221e k=0 converges to \u03b8\u22c6in L2-norm such that \ufffd \ufffd\n\ufffd \ufffd where k0 is a sufficiently large constant, \u03ben is the noisy energy estimator of batch size n, and Var denotes the variance.\n\ufffd \ufffd\ufffd \ufffd \ufffd For the term D1, by Lemma 3, we have\nFor convenience, in the following context, we denote O(supx Var(\u03ben(x)) + \u03f5k + 1 m) by \u2206k. To deal with the term D2, we make the following decomposition\n\u27e8Tk, \u03a0\u03b8k\u22121\u00b5\u03b8k\u22121(xk) \u2212\u03a0\u03b8k\u00b5\u03b8k(xk+1)\u27e9 = \ufffd \u27e8Tk, \u03a0\u03b8k\u22121\u00b5\u03b8k\u22121(xk)\u27e9\u2212\u27e8Tk+1, \u03a0\u03b8k\u00b5\u03b8k(xk+1)\u27e9 \ufffd + (\u27e8Tk+1, \u03a0\u03b8k\u00b5\u03b8k(xk+1)\u27e9\u2212\u27e8Tk, \u03a0\u03b8k\u00b5\u03b8k(xk+1)\u27e9) =(zk \u2212zk+1) + \u27e8Tk+1 \u2212Tk, \u03a0\u03b8k\u00b5\u03b8k(xk+1)\u27e9,\n(31)\n(D21)\n(32)\nwhich satisfies the conditions (45) and (46) of Lemma 11. Applying Lemma 11 leads to\n\uf8f0 \uf8fb where \u03c8k = \u03bb0\u03c9k + 1 \u03c6 supi\u2265k0 \u2206i for all k > k0. Based on (34) and the increasing condition of \u039b in Lemma 10, we have\nGiven \u03c8k = \u03bb0\u03c9k + 1 \u03c6 supi\u2265k0 \u2206i which satisfies the conditions (45) and (46) of Lemma 11, it follows from (35) and (36) that the following inequality holds for any k > k0,\nwhere \u03bb = \u03bb0 + 12QC, \u03bb0 = 2G supi\u2265k0 \u2206i+2C0\u03c6 C1\u03c6 , C1 = lim inf 2\u03c6 \u03c9k \u03c9k+1 + \u03c9k+1 \u2212\u03c9k \u03c92 k+1 > 0, C0 = G + 5Q2C + 2QC + 2Q2 and G = 4Q2(1 + Q2).\n# B.3 Technical lemmas\nLemma 6. Suppose Assumption A1 holds, and u1 and um\u22121 are fixed such that \u03a8(u1) > \u03bd and \u03a8(um\u22121) > 1 \u2212\u03bd for some small constant \u03bd > 0. For any bounded function f(x), we have\n(D3)\n(34)\n(35)\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd The proof can then be concluded by combining the orders of I1 and I2. Lemma 7. Given sup{\u03c9k}\u221e k=1 \u22641, there exists a constant G = 4Q2(1 + Q2) such that \u2225\ufffdH(\u03b8k, xk+1) + \u03c9k+1\u03c1(\u03b8k, xk+1)\u22252 \u2264G(1 + \u2225\u03b8k \u2212\u03b8\u22c6\u22252).\nLemma 8. Given sup{\u03c9k}\u221e k=1 \u22641, we have that \u2225\u03b8k \u2212\u03b8k\u22121\u2225\u22642\u03c9kQ\nProof Following the update \u03b8k \u2212\u03b8k\u22121 = \u03c9k \ufffdH(\u03b8k\u22121, xk) + \u03c92 k\u03c1(\u03b8k\u22121, xk), we have that \u2225\u03b8k \u2212\u03b8k\u22121\u2225= \u2225\u03c9k \ufffdH(\u03b8k\u22121, xk) + \u03c92 k\u03c1(\u03b8k\u22121, xk)\u2225\u2264\u03c9k\u2225\ufffdH(\u03b8k\u22121, xk)\u2225+ \u03c92 k\u2225\u03c1(\u03b8k\u22121, xk) By the compactness condition in Assumption A1 and sup{\u03c9k}\u221e k=1 \u22641, (41) can be derived.\n(39)\n(41)\nIt is clear that for a large enough k0 and \u03bb0 such that \u03c9k0 \u2264C1 2G, \u03bb0 = 2G supi\u2265k0 \u2206i+2C0\u03c6 C1\u03c6 , the desired conclusion (43) holds for all such k \u2265k0 and \u03bb \u2265\u03bb0. The following lemma is a restatement of Lemma 25 (page 247) from Benveniste et al. [1990]. Lemma 10. Suppose k0 is an integer satisfying infk>k0 \u03c9k+1 \u2212\u03c9k \u03c9k\u03c9k+1 + 2\u03c6 \u2212G\u03c9k+1 > 0 for some constant G. Then for any k > k0, the sequence {\u039bK k }k=k0,...,K defined below is increasing and uppered bounded by 2\u03c9k \uf8f1 \ufffd\n\uf8f3 Lemma 11. Let {\u03c8k}k>k0 be a series that satisfies the following inequality for all k > k0 \u03c8k+1 \u2265\u03c8k \ufffd 1 \u22122\u03c9k+1\u03c6 + G\u03c92 k+1 \ufffd + C0\u03c92 k+1 + 2\u2206k\u03c9k+1, and assume there exists such that\nThen for all k > k0, we have\n(43)\n(45)\n(46)\n(47)\n# C Ergodicity and dynamic importance sampler\nOur interest is to analyze the deviation between the weighted averaging estimator 1 k \ufffdk i=1 \u03b8\u03b6 i (J \ufffdU(xi))f(xi) and posterior expectation \ufffd X f(x)\u03c0(dx) for a bounded function f. To accomplish this analysis, we first study the convergence of the posterior sample mean 1 k \ufffdk i=1 f(xi) to the posterior expectation \u00aff = \ufffd X f(x)\u03d6\u03a8\u03b8\u22c6(x)(dx) and then extend it to \ufffd X f(x)\u03d6\ufffd\u03a8\u03b8\u22c6(x)(dx). The key tool for establishing the ergodic theory is still the Poisson equation which is used to characterize the fluctuation between f(x) and \u00aff:\nL \u2212 where g(x) is the solution of the Poisson equation, and L is the infinitesimal generator of the Langevin diffusion Lg := \u27e8\u2207g, \u2207L(\u00b7, \u03b8\u22c6)\u27e9+ \u03c4\u22072g.\nL \u2212 where g(x) is the solution of the Poisson equation, and L is the infinitesimal generator of the Langevin diffusion Lg := \u27e8\u2207g, \u2207L(\u00b7, \u03b8)\u27e9+ \u03c4\u22072g.\nL \u2212 where g(x) is the solution of the Poisson equation, and L is the infinitesima diffusion\nLg := \u27e8\u2207g, \u2207L(\u00b7, \u03b8\u22c6)\u27e9+ \u03c4\u22072g.\nRegularity Given a sufficiently smooth function g(x) and a function V(x) such that \u2225Dkg\u2225\u2272 Vpk(x) for some constants pk > 0, where k \u2208{0, 1, 2, 3}. In addition, Vp has a bounded expectation, i.e., supx E[Vp(x)] < \u221e; and V is smooth, i.e. sups\u2208{0,1} Vp(sx + (1 \u2212s)y) \u2272Vp(x) + Vp(y) for all x, y \u2208X and p \u22642 maxk{pk}. For stronger but verifiable conditions, we refer readers to Vollmer et al. [2016]. For further verifications, Erdogdu et al. [2018] showed that the 0th, 1st and 2nd order of the regularity can be verified given standard smoothness and dissipative assumptions. In what follows, we present a lemma, which is majorly adapted from Theorem 2 of Chen et al. [2015] with a fixed learning rate \u03f5. Lemma 12 (Convergence of the Averaging Estimators, restatement of Lemma 2). Suppose Assumptions A1-A5 hold. For any bounded function f, \ufffd\ufffd\ufffd\ufffd\ufffdE \ufffd\ufffdk i=1 f(xi) k \ufffd \u2212 \ufffd X f(x)\u03d6\ufffd\u03a8\u03b8\u22c6(x)dx \ufffd\ufffd\ufffd\ufffd\ufffd= O \uf8eb \uf8ed1 k\u03f5 + \u221a\u03f5 + \ufffd\ufffdk i=1 \u03c9i k + 1 \u221am + \ufffd sup x Var(\u03ben(x)) \uf8f6 \uf8f8, where k0 is a sufficiently large constant, \u03d6\ufffd\u03a8\u03b8\u22c6(x) \u221d \u03c0(x) \u03b8\u03b6 \u22c6(J(x)), and \ufffdk i=1 \u03c9i k = o( 1 \u221a k) as implied by Assumption A5. Proof We rewrite the CSGLD algorithm as follows:\n\ufffd \ufffd \ufffd where \u2207x\ufffdL(x, \u03b8) = N n \ufffd 1 + \u03b6\u03c4 \u2206u (log \u03b8(J(x)) \u2212log \u03b8((J(x) \u22121) \u22281)) \ufffd \u2207x \ufffdU(x), \u2207x\ufffdL(x, \u03b8) is as defined in Section B.1, and the bias term is given by \u03a5(xk, \u03b8k, \u03b8\u22c6) = \u2207x\ufffdL(xk, \u03b8k) \u2212 \u2207x\ufffdL(xk, \u03b8\u22c6). By Assumption A2, we have \u2225\u2207xU(x)\u2225= \u2225\u2207xU(x)\u2212\u2207xU(x\u22c6)\u2225\u2272\u2225x\u2212x\u22c6\u2225\u2264\u2225x\u2225+\u2225x\u22c6\u2225for some optimum. Then the L2 upper bound in Lemma 4 implies that \u2207xU(x) has a bounded second moment. Combining Assumption A4, we have E \ufffd \u2225\u2207x \ufffdU(x)\u22252\ufffd < \u221e. Further by Eve\u2019s law (i.e., the variance decomposition formula), it is easy to derive that E \ufffd \u2225\u2207x \ufffdU(x)\u2225 \ufffd < \u221e. Then, by the triangle inequality and Jensen\u2019s inequality, \u2225E[\u03a5(xk, \u03b8k, \u03b8\u22c6)]\u2225\u2264E[\u2225\u2207x\ufffdL(xk, \u03b8k) \u2212\u2207x\ufffdL(xk, \u03b8\u22c6)\u2225] + E[\u2225\u2207x\ufffdL(xk, \u03b8\u22c6) \u2212\u2207x\ufffdL(xk, \u03b8\u22c6)\u2225] \u2272E[\u2225\u03b8k \u2212\u03b8\u22c6\u2225] + O(sup x Var(\u03ben(x))) \u2264 \ufffd E[\u2225\u03b8k \u2212\u03b8\u22c6\u22252] + O(sup x Var(\u03ben(x)))\n(48)\nwhere Assumption A1 and Theorem 3 are used to derive the smoothness of \u2207x \u02dcL(x, \u03b8) with respect to \u03b8, and supx Var(\u03ben(x)) is the bias caused by the mini-batch evaluation of U(x). The ergodic average based on biased gradients and a fixed learning rate is a direct result of Theorem 2 of Chen et al. [2015]. By simulating from \u03d6\u03a8\u03b8\u22c6(x) \u221d \u03c0(x) \u03a8\u03b6 \u03b8\u22c6(U(x)) and combining (49) and Theorem 3, we have\n# \ufffd Proof\nApplying triangle inequality and |E[x]| \u2264E[|x|], we have\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5fc6/5fc6565b-80ef-407b-951c-ece16cd6f18f.png\" style=\"width: 50%;\"></div>\nFor the term I1, consider the bias E[ \ufffdH(\u03b8, x) \u2212H(\u03b8, x)] = O (supx Var(\u03ben(x))) as defined in proof of Lemma 3. By applying mean-value theorem, we have \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c8f/1c8fad47-cd92-4ecd-b260-0a9c4afecbf2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/041e/041ed438-498c-4d8c-aa03-357753b4b2de.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">\uf8f0 \ufffd \ufffd \uf8fb \uf8f0 \uf8fb where the compactness of \u0398 has been used in deriving the second inequality. For I22, considering the following relation</div>\n\uf8f0 \ufffd \ufffd \uf8fb \uf8f0 \uf8fb where the compactness of \u0398 has been used in deriving the second inequality. For I22, considering the following relation\n\ufffd then we have\n\uf8ed \uf8f8 where the last equality follows from Lemma 12 as the step function \ufffdm i=1 \u03b8\u03b6 \u22c6(i)1J(x)=i is bounded. For I3, by the boundedness of f, the mean value theorem and Cauchy-Schwarz inequality, we have \ufffd\n\ufffd (51)\n(52)\n\ufffd (53)\nFor the last term I4, we first decompose \ufffd X f(x)\u03c0(dx) into m disjoint regions to facilitate t analysis\nPlugging (54) into the last term I4, we have\n\ufffd\ufffd\ufffd \ufffd  \ufffd which concludes the proof of the theorem.\n# D More discussions on the algorithm\n# D.1 An alternative numerical scheme\nIn addition to the numerical scheme used in (6) and (8) in the main body, we can also consider t following numerical scheme\n  Such a scheme leads to a similar theoretical result and a better treatment of \u03a8\u03b8(\u00b7) for the subregion that contains stationary points.\n# D.2 Bizarre peaks in the Gaussian mixture distribution\nA bizarre peak always indicates that there is a stationary point of the same energy in somewhere o the sample space, as the sample space is partitioned according to the energy function in CSGLD. Fo example, we study a mixture distribution with asymmetric modes \u03c0(x) = 1/6N(\u22126, 1)+5/6N(4, 1) Figure 4 shows a bizarre peak at x. Although x is not a local minimum, it has the same energy as \u201c-6 which is a local minimum. Note that in CSGLD, x and \u201c-6\u201d belongs to the same subregion.\n# D.3 Simulations of multi-modal distributions\nWe run all the algorithms with 200,000 iterations and assume the energy and gradient follow the Gaussian distribution with a variance of 0.1. We include an additional quadratic regularizer (\u2225x\u22252 \u2212 7)1\u2225x\u22252>7 to limit the samples to the center region. We use a constant learning rate 0.001 for SGLD,\n(54)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6244/6244b6d1-6bd8-41b2-a809-b064233b359b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Explanation of bizarre peaks.</div>\nreSGLD, and CSGLD; We adopt the cyclic cosine learning rates with initial learning rate 0.005 and 20 cycles for cycSGLD. The temperature is fixed at 1 for all the algorithms, excluding the high-temperature process of reSGLD, which employs a temperature of 3. In particular for CSGLD, we choose the step size \u03c9k = min{0.003, 10/(k0.8 + 100)} for learning the latent vector. We fix 100 partitions and each energy bandwidth is set to 0.25. We choose \u03b6 = 0.75.\n# D.4 Extension to the scenarios with high-\u03b6\nIn some complex big data experiments, the first subregion may dominate the probability mass and estimating \u03b8(i)\u2019s for the high energy subregions can be quite difficult due to the limitation of floating points. If a small value of \u03b6 is used, the gradient multiplier 1 + \u03b6\u03c4 log \u03b8\u22c6(i)\u2212log \u03b8\u22c6((i\u22121)\u22281) \u2206u is close to 1 for any i and the algorithm will perform similarly to SGLD, except with different weights. When a large value of \u03b6 is used, the stochastic approximation update in Eq.(17) is slow since \u03b8\u03b6 is close to 0. To tackle this issue, one solution is to include a high-order bias item in the stochastic approximation: \u03b8k+1(i) = \u03b8k(i) + \u03c9k+1 \ufffd \u03b8\u03b6 k(J \ufffdU(xk+1) + \u03c9k+11i\u2265J \ufffd U(xk+1)\u03c1) \ufffd\ufffd 1i=J \ufffd U(xk+1) \u2212\u03b8k(i) \ufffd , (57) for i = 1, 2, . . . , m, where \u03c1 is a constant. As shown early, our convergence theory allows inclusion of such a high-order bias term. In simulations, the high-order bias term \u03c92 k+11i\u2265J \ufffd U(xk+1)\u03c1 penalized more on the higher energy regions, and thus accelerates the convergence in the early period. In the computer vision examples, we set the momentum coefficient to 0.9 and the weight decay to 25. For CSGHMC and saCSGHMC, we set \u03c9k = 10 k0.75+1000 and \u03c1 = 1 in (57) for both CIFAR10 and CIFAR100, and set \u03b6 = 1 \u00d7 106 for CIFAR10 and 3 \u00d7 106 for CIFAR100. Can we make the algorithm more scalable: The additional term \u03c1 makes the algorithm less appealing in practice. To tackle that issue and make it more scalable to big datasets, we can adopt a more scalable and elegant stochastic approximation scheme proposed in Deng et al. [2022] \ufffd \ufffd\n \ufffd \ufffd  \ufffd \ufffd Compared to Eq.(57), the dependence of \u03c1 is no longer required. Moreover, since \u03b8(i) < 1 for any i \u2208{1, 2, \u00b7 \u00b7 \u00b7 , m}, \u03b8(i)\u03b6 \u22480 when \u03b6 \u226b1, which makes Eq.(57) hard to update. By contrast, the new scheme doesn\u2019t have this issue. Theoretically, a local stability shows that \u03b8k(i) converges to a much smoother estimate ( \ufffd Xi \u03c0(x)dx) 1 \u03b6 instead of the original \ufffd Xi \u03c0(x)dx for i \u2208{1, 2, \u00b7 \u00b7 \u00b7 , m} and \u03b6 > 1. Notably, the new target is proven to be easier to estimate for high-energy regions.\n# D.5 Number of partitions\nA fine partition will lead to a smaller discretization error, but it may increase the risk in stability. In particular, it leads to large bouncy jumps around optima (a large negative learning rate, i.e., log \u03b8(2)\u2212log \u03b8(1) \u2206u \u226a0 in formula (8) may be caused there). Empirically, we suggest to partition the sample space into a moderate number of subregions, e.g. 10-1000, to balance between stability and discretization error.\n(57)\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of efficiently simulating multi-modal distributions in the context of Bayesian learning for big data statistics. Traditional MCMC methods struggle with scalability, particularly when applied to deep learning models. The introduction of stochastic gradient Langevin dynamics (SGLD) marked a significant advancement, yet existing SGMCMC algorithms often converge slowly and fail to effectively quantify uncertainty in AI safety problems. The proposed contour stochastic gradient Langevin dynamics (CSGLD) algorithm aims to overcome these limitations by adapting the flat histogram idea to enhance convergence and exploration in complex energy landscapes.",
        "problem": {
            "definition": "The problem is the slow convergence of existing SGMCMC algorithms when applied to multi-modal distributions, which hampers their effectiveness in uncertainty quantification for deep learning models.",
            "key obstacle": "The primary challenge is the presence of high energy barriers in the target distribution, which leads to local traps that existing methods struggle to escape."
        },
        "idea": {
            "intuition": "The idea behind CSGLD is inspired by traditional MCMC methods, particularly simulated annealing and flat histogram algorithms, which suggest that flattening the energy landscape can facilitate better exploration of the sample space.",
            "opinion": "CSGLD is proposed as a dynamic importance sampling algorithm that adaptively adjusts the target distribution to improve sampling efficiency and convergence rates.",
            "innovation": "The key innovation of CSGLD lies in its self-adjusting mechanism that flattens the target distribution, thereby reducing energy barriers and enhancing the sampler's ability to traverse between different modes of the distribution."
        },
        "method": {
            "method name": "Contour Stochastic Gradient Langevin Dynamics",
            "method abbreviation": "CSGLD",
            "method definition": "CSGLD is an adaptive MCMC algorithm that utilizes a Langevin transition kernel to sample from a flattened distribution, improving the exploration of multi-modal distributions.",
            "method description": "CSGLD enhances sampling efficiency by dynamically adjusting the target distribution to flatten the energy landscape during the simulation process.",
            "method steps": [
                "Simulate a mini-batch of data from the entire dataset.",
                "Compute the stochastic gradient and energy based on the sampled data.",
                "Update the parameter estimates using the stochastic approximation framework.",
                "Iterate the process to refine the sampling and parameter estimates."
            ],
            "principle": "The effectiveness of CSGLD is rooted in its ability to dynamically adjust the sampling distribution, allowing it to escape local traps and efficiently explore the energy landscape."
        },
        "experiments": {
            "evaluation setting": "CSGLD was tested on benchmark datasets including CIFAR10 and CIFAR100, comparing its performance against traditional SGLD and other SGMCMC methods.",
            "evaluation method": "The performance of CSGLD was assessed through numerical experiments measuring convergence rates, estimation accuracy, and the ability to avoid local traps in the energy landscape."
        },
        "conclusion": "CSGLD demonstrates significant improvements in sampling efficiency and convergence rates compared to traditional methods. The algorithm effectively addresses the local trap problem, enabling better exploration of multi-modal distributions, which is crucial for uncertainty quantification in deep learning and big data applications.",
        "discussion": {
            "advantage": "The main advantages of CSGLD include its adaptive mechanism for flattening the target distribution, improved convergence rates, and enhanced ability to escape local traps, making it suitable for complex multi-modal distributions.",
            "limitation": "One limitation of CSGLD is the potential introduction of bias due to the dynamic adjustment of the sampling distribution, which may affect parameter estimation in certain scenarios.",
            "future work": "Future research could focus on refining the algorithm to further reduce bias, exploring its applications in other areas of machine learning, and integrating it with other adaptive sampling techniques."
        },
        "other info": {
            "acknowledgment": "Liang\u2019s research was supported in part by grants DMS-2015498, R01-GM117597, and R01GM126089. Lin acknowledges support from NSF (DMS-1555072, DMS-1736364), BNL Subcontract 382247, W911NF-15-1-0562, and DE-SC0021142."
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper introduces the concept of Contour Stochastic Gradient Langevin Dynamics (CSGLD) as an adaptive MCMC algorithm that utilizes a Langevin transition kernel to sample from a flattened distribution."
        },
        {
            "section number": "2.2",
            "key information": "CSGLD aims to improve convergence rates and sampling efficiency by dynamically adjusting the target distribution to flatten the energy landscape during the simulation process."
        },
        {
            "section number": "5.1",
            "key information": "The paper highlights that CSGLD addresses the slow convergence of existing SGMCMC algorithms, which hampers their effectiveness in uncertainty quantification for deep learning models."
        },
        {
            "section number": "5.4",
            "key information": "One limitation of CSGLD is the potential introduction of bias due to the dynamic adjustment of the sampling distribution, which may affect parameter estimation in certain scenarios."
        },
        {
            "section number": "7.1",
            "key information": "The primary challenge addressed in the paper is the presence of high energy barriers in the target distribution, leading to local traps that existing methods struggle to escape."
        },
        {
            "section number": "7.3",
            "key information": "Future research could focus on refining the CSGLD algorithm to reduce bias, exploring its applications in other areas of machine learning, and integrating it with other adaptive sampling techniques."
        }
    ],
    "similarity_score": 0.5855180868747827,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions.json"
}