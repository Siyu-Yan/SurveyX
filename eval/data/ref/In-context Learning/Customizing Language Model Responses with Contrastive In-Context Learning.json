{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.17390",
    "title": "Customizing Language Model Responses with Contrastive In-Context Learning",
    "abstract": "Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user\u2019s need and guides it towards generting a better answer. We tested our approach on both synthesized and real-world datasets, including StackExchange and Reddit, and found that it significantly improves performance compared to standard few-shot prompting.",
    "bib_name": "gao2024customizinglanguagemodelresponses",
    "md_text": "# Customizing Language Model Responses with Contrastive In-Context Learning\n# Xiang Gao, Kamalika Das\nIntuit AI Research, 2700 Coast Avenue, Mountain View, CA 94043 {xiang gao, kamalika das}@intuit.com\n# Abstract\nLarge language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user\u2019s need and guides it towards generting a better answer. We tested our approach on both synthesized and real-world datasets, including StackExchange and Reddit, and found that it significantly improves performance compared to standard few-shot prompting.\narXiv:2401.17390v2\n# Introduction\nIn recent years, large language models like GPT, Llama, and PaLM series have made significant progress in natural language processing (Bommasani et al. 2021; Brown et al. 2020; Touvron et al. 2023), enabling them to generate coherent and contextually relevant responses. However, despite their impressive capabilities, these models can still struggle to align with our intent, particularly when it comes to generating content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. As a result, there has been a growing interest in developing techniques that can help us better steer the output of LLMs towards our desired goals. In this work, we propose an approach that leverages contrastive examples to better describe our intent, which can significantly improve the performance of LLMs in generating desirable responses. In light of the challenges faced by current LLMs in aligning with user intent, it is crucial to develop novel techniques that can effectively guide these models towards generating more desirable responses. Prior research has demonstrated\nCopyright \u00a9 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/96c3/96c358cd-ac3a-481a-b87f-7007cfd3d515.png\" style=\"width: 50%;\"></div>\nFigure 1: Contrastive examples provide positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid.\nthe benefits of few-shot learning (Brown et al. 2020), finetuning with smaller models (Gao, Fisch, and Chen 2020), selective annotation frameworks (Su et al. 2022), and visual language modeling (Alayrac et al. 2022) for enhancing LLM performance. However, these approaches do not explicitly address the challenge of guiding LLMs to generate content that adheres to specific preferences, styles, or tones. Additionally, although contrastive learning techniques have shown promise in areas such as image representation (Radford et al. 2021), dialogue response ranking (Gao et al. 2020), and self-supervised learning (Meng et al. 2021), their application to content generation in LLMs remains underexplored. Furthermore, while recent work on prompt optimization (Honovich et al. 2022; Zhou et al. 2022; Sun et al. 2023) has highlighted the importance of effective prompts in steering LLMs, there remains a need for more robust methods that can better capture user intent through diverse and contrastive examples. In this paper, we propose a novel approach that addresses these gaps by leveraging contrastive examples, including positive and negative instances, to more\naccurately define user intent and guide LLMs in generating responses that are better aligned with desired outcomes. By incorporating this contrastive reasoning step, our method aims to overcome the limitations of existing techniques and substantially enhance the performance of LLMs in generating preferable content. Our proposed approach involves providing the LLM with both positive and negative examples to better understand our intent (Figure 1). The positive examples showcase the desired outcomes, while the negative examples highlight what characteristics the LLM should avoid. By analyzing both types of examples before generating an answer, the model can reason about our intent and make a more informed decision about what to generate. It may be challenging for LLMs to understand negative instruction, but we observed that using negative examples helps aligning our preference with LLMs. Moreover, the negative examples can be retrieved from labeled data, written by a human or generated by the model itself, making it a flexible and scalable technique. Our experiments show that this approach can significantly improve the performance of LLMs in generating desirable responses, making them more useful for a wide range of natural language processing applications. In summary, our proposed approach of using contrastive examples to better describe our intent significantly improves the performance of LLMs in generating desirable responses. This approach can help address the challenge of aligning LLMs with our intended goals and make them more useful for a wide range of natural language processing applications. The key contributions of this work are: i) Providing a novel approach that leverages contrastive examples to improve the performance of LLMs in generating desirable responses. ii) Demonstrating the effectiveness of this approach on both synthesized and real-world datasets, including StackExchange and Reddit. iii) Highlighting the potential of previously discarded negative examples to make LLMs more useful for a wide range of applications by better aligning them with our intended goals.\n# Related Work\nRecent advancements in natural language processing have led to the development of large language models (LLMs) that are capable of few-shot learning, where they can learn new tasks with only a small number of annotated examples. Significant works in this area include Brown et al. (2020), who demonstrated the impressive few-shot performance of GPT-3, Gao, Fisch, and Chen (2020), who proposed LM-BFF for fine-tuning smaller language models, Su et al. (2022), who presented a selective annotation framework, and Alayrac et al. (2022), who introduced Flamingo, a family of visual language models. These works highlight the importance of few-shot learning in LLMs and the need for efficient annotation and prompt generation techniques. The use of contrastive learning for LLMs has also gained attention, with Radford et al. (2021) proposing Contrastive Language-Image Pre-training (CLIP) for learning image\nrepresentations, Gao et al. (2020) leveraging social media feedback data for training a dialogue response ranking model, and Meng et al. (2021) presenting COCO-LM, a self-supervised learning framework that pretrains LLMs by correcting and contrasting corrupted text sequences. These works emphasize the importance of providing diverse and contrastive examples to LLMs and suggest that contrastive learning is a promising direction for LLM research. Additionally, recent works have focused on improving the performance of LLMs by optimizing the prompts used to steer them towards a desired task or outcome. Honovich et al. (2022) introduce the instruction induction challenge, Zhou et al. (2022) propose Automatic Prompt Engineer (APE) for instruction generation and selection, and Sun et al. (2023) present AutoHint, a framework for automatic prompt engineering and optimization. To apply these approaches for preference alignment, however, is challenging when the characteristics are hard to describe in instruction or measured by automated metrics. Our work builds upon these previous efforts by proposing an approach that uses contrastive examples to better describe our intent, which we tested on both synthesized and realworld datasets. This approach significantly improves performance compared to standard few-shot prompting, emphasizing the potential for LLMs to become more human-like in their ability to generate natural language instructions.\nOur method comprises two main components: (1) obtaining paired positive and negative examples and (2) forming the prompt. See Figure 2 for an illustration.\n# Obtaining Paired Contrastive Examples\nObtaining Paired Contrastive Examples There are several ways to obtain contrastive examples.\nObtaining Paired Contrastive Examples There are several ways to obtain contrastive examples.\nThere are several ways to obtain contrastive examples.\nUsing Labeled Feedback In some tasks, there exist multiple natural outputs for a single input, and feedback for these outputs is available. For example, Reddit and StackExchange posts typically receive multiple responses, and users provide feedback through upvotes and downvotes. Similarly, in business applications like email and copywriting, multiple versions might be generated given the same constraints, and feedback (e.g., click-through rate) can be obtained through A/B testing or other experiments. For these tasks, we can use the response with the highest feedback as the \u201cpostitive\u201d example and the one with the lowest feedback as the \u201cnegative\u201d example. It is important to note that a \u201cnegative\u201d example here does not necessarily mean it is incorrect or unacceptable, but rather less preferred given the specific audience and scenario. It is also important to note, that the highest versus lowest votes is an indication of popularity which may or may not represent personal preference, but we are using this popularity (or lack thereof) signal as a surrogate for general preference of a particular response over others, ignoring confounding factors such as time of response etc. Using LLM-Generated Responses In cases where labeled feedback is unavailable or negative examples do not capture the characteristics we want LLMs to avoid, we propose a second method. We let the target LLM generate a\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e28c/e28c932f-dc22-4138-9588-d27acef73e4b.png\" style=\"width: 50%;\"></div>\nFigure 2: Contrastive in-context learning prompting utilize contrastive few-shot examples, which consists of a \u201cpostitive\u201d example and a \u201cnegative\u201d example for each demonstration input, and a prompt to elicit analysis of the characteristics of the positive/negative examples, before generating an output for the new input. This strategy can be applied to both conversational and non-conversational LLMs. For non-conversational LLMs, the labels and instructions are provided as system messages (the gear symbols).\nresponse and use it as the negative example. LLMs generated responses often appears mechanical, lacking emotion, and impersonal. Pairing this with a highly preferred labeled response as the positive example guides the LLM to generate a response more aligned with the positive example. Using Automated Evaluator For some tasks, it is possible to define automated evaluation rules or apply classifiers to measure the quality. The evaluation score can be used to select \u201cpositive\u201d and \u201cnegative\u201d examples. We demonstrate this with a words-constrained generation task, with accuracy determined by whether all given words were included (scored as 1) or not (scored as 0) in the generated sentences.\n# Forming the Prompt\nOnce we have obtained paired positive and negative examples, we consider two prompting strategies:\nContrastive Examples as Few-Shot Examples In this strategy, we provide the contrastive example pairs as fewshot examples. Labels are included in the prompt, such as \u201cpreferred answer\u201d for the positive example and \u201cless preferred answer\u201d for the negative example, to indicate the role\nof each part in the prompt. The LLM is then asked to generate a \u201cpreferred answer\u201d for the new input.\nReasoning and Analysis In addition to the first strategy, we ask the LLM to analyze the reasons for preference and the characteristics of the examples before generating a \u201cpreferred answer\u201d for the new input. This reasoning step is inspired by the Chain-of-Thought prompting (Wei et al. 2022) and is designed to summarize the characteristics from the contrastive examples, allowing the LLM to automatically generate instructions for itself. Coupled with contrastive examples, this guidance helps the LLM to better align with the preferred intent.\n# Experiments\nIn this section, we describe the experiments conducted to evaluate the effectiveness of our proposed approach using contrastive examples. We focus on text generation tasks where user preferences play a significant role, rather than tasks with objective right or wrong answers. We first outline the datasets used in our experiments, which include both real-world and synthetic datasets.\n# We consider the following datasets to investigate the impact of user preferences on text generation tasks:\nHuman Preference Datasets User preference is expressed via different forms of feedback publicly available at a large scale on many platforms.\nStackExchange People post questions on StackExchange, and other users provide answers, with upvotes and downvotes used to rate the responses. Our focus is on subjective preferences, so we created a dataset using data from cooking.stackexchange.com, which includes how-to type cooking-related questions. Cooking often does not have right or wrong answers, and different people have different preferences or ways to make a dish they consider good.\nReddit Users post questions or topics on Reddit to trigger discussions, and other users can comment (comments can have nested comments). In our study, we only considered the comments of the posts as they share a similar context. Redditors can provide upvotes or downvotes as feedback for each post and comment. We experimented a subreddit called \u2018NoStupidQuestions\u2018. \u2018NoStupidQuestions\u2018 encourages people to ask any question without fear of being judged by conventional social norms. The subjects cover a wide range of topics, including objective subjects and social or personal experiences. These characteristics make this subreddit ideal for our study, as personal (or group collective) preference is an essential factor, rather than objective right or wrong. For both datasets, We filtered the posts to include those with multiple answers (to have answers with varying \u201cquality\u201d or level of preference). For the remaining posts, we used the top-rated answer as the \u201cpositive\u201d example, and bottomrated answer as the \u201cnegative\u201d example. We randomly selected 500 samples for evaluation.\n# Synthetic Stylistic Datasets\nHuman preferences play a crucial role in the aforementioned real datasets. However, it is challenging to interpret and analyze the preferences, as it is often difficult even for humans to articulate what exactly makes an answer highly voted. Therefore, we also considered a few simplified synthetic datasets. Using the cooking.stackexchange.com dataset, we leveraged ChatGPT to generate the following datasets: i) Funny vs. Serious, ii) Concise vs. Detailed, and iii) British vs. American We then used the generated responses as contrastive fewshot examples. It is essential to note that by \u201cpostitive\u201d and \u201cnegative\u201d, we do not mean right or wrong, and the choice of \u201cpostitive\u201d is arbitrary in this case for synthesized datasets. The synthetic datasets aim to illustrate the ability of the proposed method to guide LLMs to generate output in a given direction.\n# Constrained Generation Dataset\nThe datasets mentioned above focus on controlling\nimplicit stylistic aspects of text generation, whether matching a specific attribute such as level of conciseness (as in the synthetic datasets) or an overall holistic style (as in the human perference datasets). In addition to these datasets, we further experiment with a dataset focused on the control of lexical content, constrained generation. This also allows us to test the ability of our method to generalize to tasks with explicit constraints. Following Zhou et al. (2023), the language model is prompted to craft sentences using specific seed words as constraints. We randomly selected 500 questions from the StackExchange dataset and randomly chose 5 words from each question to serve as lexical constraints. We use the wrong results generated by the LLM as the negative examples. The model\u2019s performance is measured by the success rate, defined as the percentage of sentences generated that contain all 5 given seed words. Success requires an exact uncased match between the generated sentence and constraint words.\n# Prompt Settings\nIn our experiments, we consider two types of large language models (LLMs): non-conversational LLMs, such as GPT3, and conversational LLMs, including ChatGPT (GPT-3.5turbo) and GPT-4. We evaluate these LLMs under two different settings: zero-shot and few-shot.\nZero-Shot For non-conversational LLMs, we use a prompt consisting of the post with the prefix \u201cQuestion:\u201d followed by the second line \u201cAnswer:\u201d. For conversational LLMs, we use a system prompt stating, \u201cYou are a good StackExchange/Reddit user,\u201d followed by the user prompt containing the post. In both cases, we input the post title. We exclude the post body, as it often includes edits after the original poster has read the replies, potentially leaking preference information. Few-Shot For each query, we randomly select k labeled examples as few-shot examples 1. For non-conversational LLMs, we include the obtained few-shot examples in the prompt, using the \u201cQuestion:\u201d and \u201cAnswer:\u201d prefixes. For conversational LLMs, we use the few-shot examples to create a conversation history between the user (post) and the bot (top-rated reply). We experimented with k = 1 to k = 4 and stops when performance does not increase significantly as k increases.\n# Contrastive Examples Based Prompts\nFor the few-shot setting, we compare the standard approach, which only includes positive examples, with three settings involving contrastive examples:\nContrastive Examples Only For each few-shot example, we provide a positive and a negative example. For nonconversational LLMs, we use the prefixes \u201ctop-rated\u201d and\n1We experimented with retrieving few-shot examples based on query embedding similarity but did\u2019t observe significant performance difference compared to randomly selected few-shot examples\n1We experimented with retrieving few-shot examples based on query embedding similarity but did\u2019t observe significant performance difference compared to randomly selected few-shot examples\n\u201clow-rated\u201d to indicate positive and negative examples, respectively. For conversational LLMs, we use system messages (\u201cprovide a top-rated answer\u201d and \u201cprovide a lowrated answer\u201d) to inform the LLMs of the preference. Contrastive Instruction Only Given the contrastive examples (with labels defined in method 1), we ask LLMs to automatically generate an instruction based on the preference revealed by the positive and negative examples. A sample prompt could be, \u201cSummarize the characteristics of the preferred and not preferred asnwer.\u201d We then use the generated analysis, followed by \u201cgenerate a top-rated answer for input,\u201d as the instruction for the test input. The contrastive examples are only used to generate the analysis and are not included in the final prompt. Contrastive Examples + Instruction This method combines the previous two approaches. We first provide the contrastive examples, followed by the instruction asking LLMs to perform an analysis. Then, the LLM generates a new response. When generating the final answer, both the contrastive examples and the generated analysis are available. These three settings involving contrastive examples allow us to conduct an ablation study to examine the contributions of specific contrastive examples and more general instructions.\n# Evaluation Method\nIn order to evaluate the performance of our approach, we consider two distinct methods: reference-based and reference-free evaluation.\nReference-Based Evaluation The reference-based evaluation method measures the similarity between the generated responses and the top-rated reference answer. We employ two metrics, BERT Score (Zhang et al. 2019) and Emb. Similarity. The latter is the cosine similarity of the sentence embeddings obtained using the Sentence-BERT model (Reimers and Gurevych 2019).\nReference-Free Evaluation An answer that deviates from the reference could still be preferred by many readers. Therefore, we also consider two reference-free evaluation method. i) We leverage DialogRPT (Gao et al. 2020), a pretrained dialog response ranking model. This model is trained on Reddit data and shows a high correlation with human upvotes (Gao et al. 2020), so we apply it to the Reddit dataset only. ii) In addition, we employ LLM as an evaluator following Liu et al. (2023). We compute a GPT Score by prompting GPT4 to score the generated results using the positive and negative examples as in-context few shot examples. The LLM generated score and human evaluation labels exhibit positive correlation (StackExchange: 0.65, Reddit: 0.58).\n# Results\nWe evaluated the performance of the baselines and contrastive in-context prompting strategies on the synthetic and two real datasets. The results are shown in Table 1 and Table 2. The \u201ccontrastive-combined\u201d approach (which combines the contrastive examples and the derived instruction)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1884/18840bfd-20e1-43f0-bbaa-b4366003dd91.png\" style=\"width: 50%;\"></div>\nFigure 3: The dependence of the performance on prompt length. For standard few-shot, we experimented with k = 1, 2, 3, 4, shown by the four black dots from left to right in each subfigure. For contrastive prompts, empty circles denotes prompts using generated answers as negative examples, and filled symbols for prompts using low-rated human answers as negative examples.\nachieved the best performance for most cases. Using similar number of tokens in prompt, this contrastive in-context learning approach performs significantly better than standard few-shot approach, as shown in Figure 3. For Reddit dataset, we observe the most obvious improvement with ChatGPT model. Standard two-shot prompt only performs better than zero-shot approach for about 64 % test cases, while the \u201ccontrastive-combined\u201d approach wins 76 % test cases. For StackExchange, standard few-shot approaches do not show obvious improvement compared to zero-shot, while \u201ccontrastive-combined\u201d improve BERT Score by approximately 0.01. For synthetic datasets, the improvement of is more obvious, with BERT Score increased by 0.02 to 0.03, and much higher than the improvement achieved with standard few-shot prompts. As an ablation study, we also evaluated the results for \u201ccontrastive - examples only\u201d and \u201ccontrastive - instruction only\u201d. Both approaches improve the performance compared to zero shot, and the \u201ccontrastive - examples only\u201d performs better than \u201ccontrastive - instruction only\u201d for many cases. Combining them together further improves the results, as discussed above.\n# The Impact of Contrastive Examples\nabove, using contrastive in-context learning by introducing negative examples improve the performance compared to standard few-shot approaches. We futher investigate different ways to obtain negative examples. The first method, \u201chuman\u201d, uses human-written answers, where low-rated replies serve as \u201cnegative\u201d examples. The second method, \u201cgenerated\u201d, uses zero-shot generated replies as \u201cnegative\u201d examples. Interestingly, we observed that the second method performs on par with, and sometimes even better than, the first method (see Tables 1 and 2). This finding demonstrates that the proposed contrastive method is not limited to cases\nDataset\nFunny vs. Serious\nConcise vs. Detailed\nBritish vs. American\nMethod\nEmb. Similarity\nBERT Score\nEmb. Similarity\nBERT Score\nEmb. Similarity\nBERT Score\nGPT-3\nZero-shot\n0.606\n0.858\n0.849\n0.903\n0.785\n0.883\nFew-shot, k=1\n0.611\n0.863\n0.887\n0.916\n0.813\n0.892\nFew-shot, k=2\n0.629\n0.864\n0.886\n0.920\n0.808\n0.887\nContrastive, k=1\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\n- Examples only\n0.614\n0.620\n0.857\n0.859\n0.817\n0.865\n0.895\n0.910\n0.827\n0.807\n0.891\n0.885\n- Instruction only\n0.645\n0.595\n0.865\n0.861\n0.834\n0.845\n0.895\n0.902\n0.824\n0.784\n0.888\n0.879\n- Combined\n0.655\n0.617\n0.864\n0.863\n0.869\n0.897\n0.910\n0.922\n0.826\n0.838\n0.896\n0.891\nChatGPT\nZero-shot\n0.604\n0.841\n0.826\n0.878\n0.783\n0.867\nFew-shot, k=1\n0.613\n0.842\n0.829\n0.880\n0.807\n0.870\nFew-shot, k=2\n0.610\n0.846\n0.834\n0.884\n0.806\n0.874\nContrastive, k=1\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\n- Examples only\n0.608\n0.599\n0.847\n0.844\n0.850\n0.856\n0.881\n0.891\n0.825\n0.818\n0.882\n0.880\n- Instruction only\n0.617\n0.617\n0.849\n0.850\n0.831\n0.835\n0.882\n0.888\n0.819\n0.821\n0.876\n0.876\n- Combined\n0.638\n0.630\n0.862\n0.862\n0.873\n0.883\n0.914\n0.915\n0.830\n0.827\n0.891\n0.891\nGPT-4\nZero-shot\n0.596\n0.838\n0.803\n0.873\n0.786\n0.864\nFew-shot, k=1\n0.607\n0.843\n0.805\n0.877\n0.792\n0.867\nFew-shot, k=2\n0.624\n0.846\n0.809\n0.880\n0.807\n0.868\nContrastive, k=1\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\nhuman\ngen.\n- Examples only\n0.609\n0.611\n0.844\n0.843\n0.828\n0.855\n0.888\n0.893\n0.817\n0.821\n0.870\n0.871\n- Instruction only\n0.624\n0.612\n0.843\n0.841\n0.853\n0.852\n0.893\n0.895\n0.821\n0.820\n0.868\n0.870\n- Combined\n0.616\n0.630\n0.855\n0.857\n0.875\n0.853\n0.919\n0.917\n0.813\n0.824\n0.886\n0.884\nTable 1: Results on the synthetic dataset. k is the number of demonstration inputs. The \u201cnegative\u201d exam two methods, \u201chuman\u201d and \u201cgen.\u201d (generated by the LLM itself).\nwhere developers have to provide human-written pairs of positive and negative examples. Instead, it requires the same inputs as standard few-shot settings, which only need a few positive examples since we can generate the negative examples. This observation also supports our assumption that negative examples obtained from human-written data may not capture all characteristics we want LLMs to avoid. For instance, in the Reddit dataset, a human-written reply may receive downvotes because the content is rude, disrespectful, or violates some rules of the subreddit, such as selfpromotion. However, recent LLMs like ChatGPT are generally trained to follow social norms and sometimes even apologize too frequently. As a result, it is highly unlikely for these LLMs to generate offensive language. Providing human-written negative examples, therefore, may not offer much helpful signal in such cases. In contrast, providing generated responses as negative examples can supply the signals of certain characteristics we want the LLM to avoid. For example, although LLM-generated responses are typically fluent and relevant to the question, they often lack emotion, details, examples, or elements that trigger readers\u2019 personal feelings. These characteristics, however, are essential factors that make users prefer certain answers. By providing zero-shot generated responses, we can guide LLMs to move away from the machine-generated style and toward a more human-preferred style.\nDistilling Contrastive Examples as an Instruction Although the primary focus of this work is not to automat-\nically generate instructions for LLMs, we were interested in understanding what instructions LLMs could derive from the contrastive examples. Therefore, we asked the LLMs to summarize the characteristics of the provided \u201cpostitive\u201d and \u201cnegative\u201d examples. We then used this analysis in the prompt, instead of the actual contrastive examples, to offer the LLMs insight into user preferences. This compress the contrastive examples into a shorter instruction, and reduce prompt length and cost. Our experimental results showed that, with this analysis, LLMs performed better than in the zero-shot setting (see Tables 1 and 2). However, the improvement was not significantly better than the standard few-shot setting. This could be partially explained by the fact that the preference was not consistent across the examples chosen based on upvotes. There is room for enhancement in the way the preference is summarized through the use of automated prompt generation approaches (Sun et al. 2023). By refining the way LLMs derive instructions from contrastive examples, we can potentially achieve more significant improvements in performance compared to standard few-shot prompting.\n# Combining the Complementary Parts We ob\nthat when combining the contrastive examples and the instruction (analysis of the characteristics of \u201cpostitive\u201d and \u201cnegative\u201d examples automatically generated based on the contrastive examples) together, an even better performance was achieved (see Tables 1 and 2). This improved performance can be attributed to the fact that the analysis and the contrastive examples provide complementary information to\nDataset\nStackExchange\nReddit\nConstrained Gen.\nMethod\nBERT Score\nGPT Score\nDialogRPT score\nGPT Score\nSuccess rate\nGPT-3\nZero-shot\n0.840\n0.550\n0.605\n0.533\n0.750\nFew-shot, k=1\n0.841\n0.555\n0.596\n0.531\n0.760\nFew-shot, k=2\n0.841\n0.561\n0.610\n0.540\n0.772\nContrastive, k=1\nhuman\ngenerated\nhuman\ngenerated\nhuman\ngenerated\nhuman\ngenerated\nExamples only\n0.846\n0.845\n0.605\n0.600\n0.630\n0.632\n0.634\n0.638\n0.801\nInstruction only\n0.845\n0.846\n0.605\n0.601\n0.606\n0.610\n0.621\n0.630\n0.790\nCombined\n0.847\n0.846\n0.608\n0.606\n0.631\n0.632\n0.635\n0.640\n0.830\np-value\n(0.047)\n(0.049)\n(0.018)\n(0.023)\n(0.030)\n(0.030)\n(0.020)\n(0.019)\n(0.021)\nChatGPT\nZero-shot\n0.839\n0.550\n0.602\n0.531\n0.780\nFew-shot, k=1\n0.839\n0.561\n0.636\n0.578\n0.800\nFew-shot, k=2\n0.839\n0.570\n0.645\n0.583\n0.810\nContrastive, k=1\nhuman\ngenerated\nhuman\ngenerated\nhuman\ngenerated d\nhuman\ngenerated\nExamples only\n0.844\n0.842\n0.592\n0.589\n0.654\n0.663\n0.654\n0.659\n0.866\nInstruction only\n0.844\n0.843\n0.598\n0.597\n0.640\n0.645\n0.640\n0.642\n0.840\nCombined\n0.845\n0.846\n0.601\n0.603\n0.656\n0.663\n0.690\n0.701\n0.872\n(p-value)\n(0.048)\n(0.044)\n(0.033)\n(0.031)\n(0.048)\n(0.041)\n(0.023)\n(0.021)\n(0.045)\nGPT-4\nZero-shot\n0.839\n0.575\n0.657\n0.580\n0.850\nFew-shot, k=1\n0.840\n0.578\n0.655\n0.585\n0.870\nFew-shot, k=2\n0.841\n0.582\n0.656\n0.590\n0.900\nContrastive, k=1\nhuman\ngenerated\nhuman\ngenerated\nhuman\ngenerated\nhuman\ngenerated\nExamples only\n0.844\n0.842\n0.595\n0.590\n0.658\n0.665\n0.660\n0.671\n0.923\nInstruction only\n0.842\n0.842\n0.590\n0.591\n0.658\n0.660\n0.661\n0.672\n0.903\nCombined\n0.847\n0.845\n0.610\n0.607\n0.660\n0.665\n0.665\n0.670\n0.943\n(p-value)\n(0.047)\n(0.049)\n(0.023)\n(0.025)\n(0.043)\n(0.040)\n(0.034)\n(0.032)\n(0.045)\nTable 2: Results on real-world datasets. k is the number of demonstration inputs. The \u201cnegative\u201d examples are obtained via two methods, \u201chuman\u201d and \u201cgenerated\u201d. p-value indicate the statistical significance of the comparison between the \u201cCombined\u201d contrastive prompting and the standard few-shot (k = 2) prompting.\nguide the LLM about user preferences. The generated instruction exhibits better generalization but may lack the necessary clarity and specificity. This is because the instructions can be vague or provide only general descriptions, which can be challenging for LLMs to interpret. In contrast, the actual contrastive examples offer more detailed information by providing demonstrations of the desired and undesired characteristics. In summary, our approach of using contrastive examples along with an analysis of the characteristics of positive and negative examples has proven to be a valuable method for aligning LLMs with user prference. This approach not only enhances performance but also provides LLMs with a better understanding of the user\u2019s preferences, resulting in more accurate and satisfactory responses.\n# Prompt Token Efficiency\nPrompt Token Efficiency In real-world and industrial applications, the number of prompt tokens plays a crucial role as it is directly related to latency and monetary cost. Ideally, a good prompt should be short yet lead to highquality output. Therefore, we measure the token efficiency to understand the effectiveness of our approach. As shown in Figure 3, the standard few-shot strategies do exhibit an increase in performance as we increase the number of examples. However, the performance, as measured by BERT score, is still lower than the contrastive in-context learning\nstrategies using a similar number of tokens. Furthermore, the contrastive instruction strategy, which summarizes the contrastive examples as an instruction, utilizes fewer tokens but achieves better performance than few-shot examples. This indicates that, given the same budget of prompt tokens, contrastive in-context learning strategies outperform the standard few-shot approaches, thereby demonstrating higher efficiency.\n# Conclusion\nThis research introduced an approach to enhance the alignment of large language models (LLMs) with user preference using contrastive examples. By integrating positive examples showcasing desired outputs and negative ones emphasizing unwanted LLM traits, we tested our methodology on both synthesized and real-world datasets, including StackExchange and Reddit. The results confirmed the superiority of contrastive examples over the standard few-shot prompting, particularly in terms of performance and prompt token efficiency. Notably, negative examples generated from zero-shot outputs were as effective as those from human-written data. Future endeavors might delve into refining LLM instructions based on these examples and innovating automatic prompt generation techniques.\n# References\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.; et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716\u201323736. Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.; Brunskill, E.; et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u2013 1901. Gao, T.; Fisch, A.; and Chen, D. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723. Gao, X.; Zhang, Y.; Galley, M.; Brockett, C.; and Dolan, B. 2020. Dialogue response ranking training with large-scale human feedback data. arXiv preprint arXiv:2009.06978. Honovich, O.; Shaham, U.; Bowman, S. R.; and Levy, O. 2022. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782. Liu, Y.; Iter, D.; Xu, Y.; Wang, S.; Xu, R.; and Zhu, C. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Meng, Y.; Xiong, C.; Bajaj, P.; Bennett, P.; Han, J.; Song, X.; et al. 2021. Coco-lm: Correcting and contrasting text sequences for language model pretraining. Advances in Neural Information Processing Systems, 34: 23102\u201323114. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748\u20138763. PMLR. Reimers, N.; and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Su, H.; Kasai, J.; Wu, C. H.; Shi, W.; Wang, T.; Xin, J.; Zhang, R.; Ostendorf, M.; Zettlemoyer, L.; Smith, N. A.; et al. 2022. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975. Sun, H.; Li, X.; Xu, Y.; Homma, Y.; Cao, Q.; Wu, M.; Jiao, J.; and Charles, D. 2023. AutoHint: Automatic Prompt Optimization with Hint Generation. arXiv preprint arXiv:2307.07415. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837.\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.; et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716\u201323736. Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.; Brunskill, E.; et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u2013 1901. Gao, T.; Fisch, A.; and Chen, D. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723. Gao, X.; Zhang, Y.; Galley, M.; Brockett, C.; and Dolan, B. 2020. Dialogue response ranking training with large-scale human feedback data. arXiv preprint arXiv:2009.06978. Honovich, O.; Shaham, U.; Bowman, S. R.; and Levy, O. 2022. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782. Liu, Y.; Iter, D.; Xu, Y.; Wang, S.; Xu, R.; and Zhu, C. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Meng, Y.; Xiong, C.; Bajaj, P.; Bennett, P.; Han, J.; Song, X.; et al. 2021. Coco-lm: Correcting and contrasting text sequences for language model pretraining. Advances in Neural Information Processing Systems, 34: 23102\u201323114. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748\u20138763. PMLR. Reimers, N.; and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Su, H.; Kasai, J.; Wu, C. H.; Shi, W.; Wang, T.; Xin, J.; Zhang, R.; Ostendorf, M.; Zettlemoyer, L.; Smith, N. A.; et al. 2022. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975. Sun, H.; Li, X.; Xu, Y.; Homma, Y.; Cao, Q.; Wu, M.; Jiao, J.; and Charles, D. 2023. AutoHint: Automatic Prompt Optimization with Hint Generation. arXiv preprint arXiv:2307.07415. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824\u201324837.\nZhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675. Zhou, Y.; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis, S.; Chan, H.; and Ba, J. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of aligning large language models (LLMs) with user intent in generating content that is preferable or adheres to specific styles or tones. Previous methods have struggled to effectively guide LLM outputs, necessitating a novel approach that leverages contrastive examples.",
        "problem": {
            "definition": "The problem focuses on the challenge of ensuring that LLMs generate outputs that align with user preferences regarding content style and tone.",
            "key obstacle": "Existing methods do not effectively capture the nuances of user intent, particularly when preferences are subjective and hard to articulate."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that providing both positive and negative examples can better inform LLMs about user intent and preferences.",
            "opinion": "The proposed approach involves using contrastive examples\u2014positive examples that illustrate desired outputs and negative examples that highlight what to avoid\u2014to guide LLMs in generating preferable content.",
            "innovation": "The key innovation lies in the incorporation of negative examples, which have not been effectively utilized in previous methods, thereby enhancing the ability of LLMs to generate user-aligned responses."
        },
        "method": {
            "method name": "Contrastive In-Context Learning",
            "method abbreviation": "CICL",
            "method definition": "CICL is a method that uses contrastive examples to improve the alignment of LLM outputs with user intent by providing both positive and negative examples.",
            "method description": "The method consists of presenting LLMs with paired examples to analyze before generating responses, thereby enhancing output alignment with user preferences.",
            "method steps": [
                "Obtain paired positive and negative examples.",
                "Formulate prompts that include these examples.",
                "Request the LLM to analyze the examples before generating a response."
            ],
            "principle": "The effectiveness of this method is rooted in the contrastive reasoning it facilitates, helping LLMs discern between preferred and less preferred characteristics in generated content."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using both synthesized and real-world datasets, including StackExchange and Reddit, to evaluate the performance of the proposed method against standard few-shot prompting.",
            "evaluation method": "Performance was assessed using reference-based methods like BERT Score and reference-free methods like DialogRPT, measuring how well the generated responses aligned with user preferences."
        },
        "conclusion": "The proposed method demonstrates a significant improvement in aligning LLM outputs with user preferences compared to standard few-shot prompting, particularly through the use of contrastive examples.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to effectively guide LLMs towards generating content that aligns better with user intent by leveraging both positive and negative examples.",
            "limitation": "One limitation is that the method may still struggle with subjective preferences that are difficult to quantify or articulate, even with the use of contrastive examples.",
            "future work": "Future research could focus on refining the generation of instructions based on contrastive examples and exploring automatic prompt generation techniques to further enhance performance."
        },
        "other info": {
            "authors": [
                {
                    "name": "Xiang Gao",
                    "affiliation": "Intuit AI Research"
                },
                {
                    "name": "Kamalika Das",
                    "affiliation": "Intuit AI Research"
                }
            ],
            "datasets used": [
                "StackExchange",
                "Reddit",
                "Synthetic datasets"
            ],
            "key contributions": [
                "Novel approach leveraging contrastive examples.",
                "Demonstrated effectiveness on synthesized and real-world datasets.",
                "Highlighted the potential of negative examples for preference alignment."
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The foundational concept of in-context learning is enhanced by the use of contrastive examples, which provide both positive and negative guidance to align LLM outputs with user intent."
        },
        {
            "section number": "1.3",
            "key information": "Large language models facilitate in-context learning by utilizing methods like Contrastive In-Context Learning (CICL), which incorporates contrastive examples to improve output alignment with user preferences."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, CICL, allows LLMs to adapt to user preferences by analyzing paired positive and negative examples before generating responses, thus enhancing robustness in in-context learning."
        },
        {
            "section number": "4.1",
            "key information": "Effective prompt design in CICL involves formulating prompts that include both positive and negative examples, significantly influencing the outcomes of in-context learning by guiding LLMs towards user-aligned content."
        },
        {
            "section number": "6.1",
            "key information": "The method may still struggle with subjective preferences that are difficult to quantify or articulate, highlighting challenges related to model bias in the context of in-context learning."
        },
        {
            "section number": "6.4",
            "key information": "The scalability of the proposed method, which utilizes contrastive examples, may be challenged by the complexity of user preferences and the need for effective instruction generation."
        }
    ],
    "similarity_score": 0.7376973841489367,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Customizing Language Model Responses with Contrastive In-Context Learning.json"
}