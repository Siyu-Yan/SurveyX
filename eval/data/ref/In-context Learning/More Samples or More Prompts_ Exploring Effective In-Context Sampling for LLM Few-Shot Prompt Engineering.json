{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2311.09782",
    "title": "More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering",
    "abstract": "While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM's performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs' performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM's performance, which sheds light on a new yet promising future research direction.",
    "bib_name": "yao2024samplespromptsexploringeffective",
    "md_text": "# More Samples or More Prompts? Exploring Effective Few-Shot In-Context Learning for LLMs with In-Context Sampling\nBingsheng Yao Rensselaer Polytechnic Institute Northeastern University Guiming Chen The Chinese University of Hong Kong, Shenzhe\nYisi Sang\nMichigan State University Rensselaer Polytechnic Institute\nNortheastern University\nDakuo Wang \u2217 Northeastern University\nAbstract\nWhile most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM\u2019s performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs\u2019 performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM\u2019s performance, which sheds light on a new yet promising future research direction.\narXiv:2311.09782v2\n# 1 Introduction\nLarge Language Models (LLMs) with billions of parameters, such as FLAN-T5 (Chung et al., 2022), LLaMA (Touvron et al., 2023b,c), and Mistral (Jiang et al., 2023), have demonstrated exceptional natural language interpretation capability in terms of understanding versatile prompt inputs1. In comparison with much smaller language models like BERT (Devlin et al., 2018) and GPT (Radford et al., 2018), such LLMs can understand not only\n\u2217Corresponding Author: d.wang@northeastern.edu. This work was done while Guiming, Ruishi, and Shao were visiting students at Northeastern University. 1We use \u201cprompt input\u201d to refer to the composition of prompt structures, including the task narrative instructions, plus in-context examples, and the targeting data for inference.\nShanghai Jiao Tong University\nRensselaer Polytechnic Institute\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6025/6025a101-6362-4ce1-ba14-9750982bb944.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4f19/4f192ff6-c253-4376-9490-13db926f6e0a.png\" style=\"width: 50%;\"></div>\nFigure 1: Our proposed ICS paradigm comprises three steps: 1) sample representative ICL demonstration candidates, 2) augment different ICL prompt inputs from the sampled candidates and acquire LLM\u2019s prediction for each input correspondingly, and 3) determine and vote LLM\u2019s most confident prediction. more complex and detailed task narratives but also a few task examples with annotations within the prompt inputs, namely few-shot In-Context Learning (ICL) (Brown et al., 2020; Shin et al., 2022). As a prominent prompting strategy to exploit LLMs\u2019 task-solving capabilities, especially for un-\nseen tasks, ICL inserts a few data examples as well as their corresponding annotations into the prompt input. The data examples, along with their annotations, serve as demonstrations2 for the targeting task. The demonstrations are expected to facilitate LLMs\u2019 better understanding of the task narrative, the expected outputs, and potentially the underlying rationales needed for solving the task. Several recent works investigate the influence of different ICL setups, including the number, ordering, and combinations of demonstrations (Wang et al., 2022; Lu et al., 2022; Yoo et al., 2022). However, there is no common ground for the best ICL strategy yet. Moreover, despite LLMs\u2019 superb natural language interpretation and generation capability, realworld tasks requiring extensive domain expertise remain challenging for LLMs (e.g., children\u2019s education and mental issue detection (Chen et al., 2023a; Xu et al., 2024; Zhang et al., 2023)), and thus, how to exploit LLMs\u2019 ability with ICL for solving these tasks is an under-explored topic but holds great promise. We hypothesize that different ICL demonstrations provide LLMs with distinct knowledge about the task, leading to disparate understanding and predictions for the same data. Consequently, a research question emerges: Can we augment multiple ICL prompt inputs efficiently to facilitate more accurate and confident LLM predictions? To address this question, we propose In-Context Sampling (ICS), a low-resource methodology inspired by the query-by-committee strategy (Seung et al., 1992; Liere and Tadepalli, 1997) and the fewshot In-Context Learning approach. ICS follows a three-step pipeline as shown in Figure 1: 1. Sample demonstration candidates; 2. Augment ICL prompt inputs and predictions; 3. Vote the most confident label. We also propose three data similarity-based ICS strategies inspired by established data sampling strategies for Active Learning (Settles, 2009). We believe ICS can be a more reliable prompting paradigm than the traditional ICL, better squeezing LLM\u2019s task-solving capabilities and seamlessly supporting \u201cplug-and-play\u201d customizations. Our evaluation of the ICS paradigm comprises bi-fold. First, we benchmark the effectiveness of a baseline ICS strategy with the traditional ICL approach on three open-source LLMs (FLAN-T52We use \u201cexamples\u201d and \u201cdemonstrations\u201d interchange-\n2We use \u201cexamples\u201d and \u201cdemonstrations\u201d interchangeably to refer to the few-shot data examples within the prompts.\nXL (Chung et al., 2022), Mistral-7B (Jiang et al., 2023), and Mixtral-8x7B (Jiang et al., 2024))3 over five datasets, including four natural language inference (NLI) (Bowman et al., 2015) datasets as well as the CommonsenseQA (CQA) dataset (Talmor et al., 2018). Among four NLI datasets, three are general-domain NLI tasks of increasing difficulty (e-SNLI (Camburu et al., 2018), MultiNLI (Williams et al., 2017), and ANLI (Nie et al., 2019)), and the last one is Contract-NLI (Koreeda and Manning, 2021), a domain-specific NLI dataset for the real-world contract review task. We also investigate how different sample sizes and the number of ICL prompt inputs affect model reliability in terms of performance enhancement. Results indicate that ICS can consistently improve prediction accuracy and robustness despite LLMs demonstrating different levels of ICL capabilities. We further investigate the additional advantages provided by three different ICS strategies through simulations with the best-performing setting from the previous experiment, compared with the random ICS and traditional ICL approaches on the aforementioned four datasets. Despite being conceptually straightforward, all three types of databased strategies can effectively and consistently improve LLM performance, leading to a broader research scope to exploit ICS in the future.\n# 2 Related Work\n# 2.1 Large Language Models\nLarge Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023a,c; OpenAI, 2023) show impressive capability in understanding free-form instructions and generating high-quality content in a variety of tasks (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022; Mahmood et al., 2023; Yao et al., 2023b; Yang et al., 2024). For instance, Wei et al. (2021) proposed FLAN-T5, a model trained to follow natural language instruction on over 60 NLP tasks. Ouyang et al. (2022) proposed a pipeline to instruction-finetune LLM with Reinforcement Learning from Human Feedback. In addition, various prompting methods such as Chainof-Thoughts (Wei et al., 2023; Chung et al., 2022) and In-Context Learning (ICL) (Brown et al., 2020) have been developed to exploit LLMs\u2019 potential, where the former technique asks models to generate a sequence of rationales, and the latter methodol3We also experiment with Llama2 (Touvron et al., 2023c) and discussed its limited performance in Appendix D.\n3We also experiment with Llama2 (Touvron et al., 2023c) and discussed its limited performance in Appendix D.\nogy allows LLMs to learn from few-shot examples in the input context. Our ICS paradigm extends the traditional ICL approach to improve the performance and confidentiality of LLM predictions.\n# 2.2 In-Context Learning Optimization\nOptimizing ICL performance has garnered significant attention recently. Dong et al. (2023) summarized three categories for different ICL optimization approaches: fine-tuning with ICL, ICL sample selection, and analyzing order sensitivity. Finetuning with ICL generally requires a significant amount of computing resources and effort to tune model parameters, such that Wei et al. (2021) proposed an instruction tuning method that improves both zero-shot and few-shot In-Context Learning performance. Sample selection in ICL has been demonstrated to have a considerable impact on model performance (Zhang et al., 2022b; Rubin et al., 2022; Li et al., 2023). Zhang et al. (2022b) initiated a reinforcement learning technique to select more advantageous samples for in-context demonstration. Rubin et al. (2022) proposed a two-staged method with an unsupervised retriever followed by a supervised model. Some work focused on reducing LLM\u2019s ICL order sensitivity issue. Lu et al. (2022) proposed multiple sample sorting methods, while Liu et al. (2022) introduced a method for arranging examples based on their semantic similarity. A few other works attempted to exploit the benefits of the ICL pipeline to improve model performance, better alignment, and minimize reliance on external demonstrations (Yu et al., 2023; Lin et al., 2023; Kim et al., 2022).\n# 2.3 Sampling Strategies\nThe data sampling strategy is a key element of many low-resource learning paradigms that attempt to select the most representative examples, such as Active Learning (AL) (Settles, 2009). Following established works, the data sampling strategies have been mainly categorized into three categories: model-based, data-based, and hybrid (Settles, 2009; Olsson, 2009; Fu et al., 2013; Schr\u00f6der and Niekler, 2020; Ren et al., 2021; Zhang et al., 2022c; Schr\u00f6der et al., 2022; Lu et al., 2023). Model-based strategies aim to find the data with the most model uncertainty (Wang et al., 2017; Zeng et al., 2019). For instance, Margatina et al. (2021) and Zhang et al. (2022a) explored using the divergence of a model\u2019s prediction as a measurement of model uncertainty. Data-based strategies,\non the other hand, aim to find the most diverse or representative data in the data space (Erdmann et al., 2019; Prabhu et al., 2019; Karamcheti et al., 2021). Such that Deng et al. (2018); Sinha et al. (2019) leveraged adversarial learning to select the most representative data. In contrast to modelbased strategies, data-based strategies are generally model-agnostic and demand fewer computational resources but necessitate the analysis of unlabeled samples. Hybrid or ensemble Sampling Strategies integrate various strategy types in unison (Krogh and Vedelsby, 1994; Tang et al., 2002; Melville and Mooney, 2004; Donmez et al., 2007; Zhu et al., 2008; Ambati et al., 2011). For instance, Qian et al. (2020) proposed a combined approach of a diversity-based and an uncertainty-based tactic to benefit from both strategies.\n# 3 ICS Prompting Paradigm\nGiven a natural language task instruction I and a datum to predict x \u2208D, LLMs can take the InContext Learning (ICL) input format, denoted as:\n(1)\nwhere (xicl m , yicl m ) denotes an oracle-annotated incontext demonstration. We believe in-context demonstrations can provide LLMs with two types of knowledge: 1) explicit insights to interpret the task instruction I and expected outputs through (yicl 1 , ..., yicl m ) and 2) implicit guidance for how to solve the task via demonstrations (xicl m \u2192yicl m ). We hypothesize that different sets of ICL demonstrations provide LLMs with disparate implicit knowledge about the task; thus, LLMs may alter their predictions for the same data x given different ICL prompt inputs, but the predictions will eventually converge to a most confident result. Our hypothesis stands on the shoulder of the query-by-committee (Seung et al., 1992; Liere and Tadepalli, 1997) strategy that has been around for a long time. The original concept is to ask a committee of models to vote on whether the unlabeled data needs to be annotated, where the voting models focus on competing hypotheses. However, most existing works focused on measuring the disagreements among committee models (Engelson and Dagan, 1996; McCallum et al., 1998) and creating different committees with probabilistic and nonprobabilistic models (Dagan and Engelson, 1995; Freund and Schapire, 1997; Abe and Mamitsuka, 1998; Melville and Mooney, 2004; Tomanek and Hahn, 2009; Sarawagi and Bhamidipaty, 2002).\nIn this work, we present In-Context Sampling (ICS), a low-resource paradigm for LLMs through effectively augmenting ICL prompt inputs, as shown in Figure 1. We view the ICS strategy as exploring efficient approaches to create committee ICL prompt inputs and query LLMs for the most confident prediction. ICS consists of three steps: 1. Sample demonstration candidates and acquiring oracle annotations, 2. Augment prompt inputs and label predictions with different ICL combinations, and 3. Vote the most confident label as the final prediction from augmented labels. Before diving deep into the details of each step in ICS, we want to emphasize that our prototyped ICS strategies in this work are model-agnostic. We will demonstrate the consistent effectiveness of a random baseline ICS strategy over the traditional ICL approach across five datasets and three LLMs in Section 4.1. More importantly, our ICS supports \u201cplug-and-play\u201d customizations by switching to different sampling, augmenting, and voting strategies with minimum effort. In addition to justifying the effectiveness of our proposed ICS pipeline and investigating the influence of different factors on performance improvement and robustness, we propose three types of model-agnostic ICS strategies and demonstrate their further improvements over the random ICS pipeline in Section 4.2. The following sections illustrate each ICS step in detail as well as our proposed three data similarity-based ICS strategies: diversity, similarity, and hybrid. We also leave a broad research area to explore strategy variations in future work.\n# 3.1 Demonstration Candidate Sampling\nHow to effectively select unlabeled examples to benefit model performance shares the same spirit as the Active Learning (AL) data sampling strategy (Settles, 2009), where an AL strategy iteratively samples few examples for annotation and fine-tuning the model. The AL strategies are often categorized into three types, as illustrated above in Section 2: data diversity-based, model probability-based, and hybrid strategies. Existing work stated that the effectiveness of model-based strategies might differ from model to model (Yao et al., 2023a), which could introduce irreverent factors when we benchmark our ICS versus the traditional ICL approach. In this work, we implement three different data similarity-based, modelagnostic strategies for ICS and evaluate their effec-\ntiveness in Section 4.2, in addition to the baseline Random strategy where we demonstrate the effectiveness compared with traditional ICL approach in Section 4.1. The mathematical notations of our proposed strategies are illustrated in Algorithm 1.\nDiversity This strategy adheres to established cluster-based strategies (i.e., core-set) (Sener and Savarese, 2017; Yao et al., 2023a), aiming to identify examples representative of all unlabeled data while maximizing the diversity among these selected instances. The concept of ensuring data diversity derives from the established density-weighted sampling strategies (Settles and Craven, 2008; Shen et al., 2004). They assume the instances that can provide the most helpfulness should be the ones that are representative of the input space (He et al., 2023). In other words, the diversity among selected data should be maximized. Specifically, our strategy calculates the cosine similarity for each data xi, encoded with sentencetransformer (Reimers and Gurevych, 2019), with the following formula, where embed represents sentence-transformer embedding:\n\ufffd Subsequently, we rank the data by similarity score and retrieve n examples with the same interval, ensuring the sampling diversity. for instance, to sample 4 demonstrations from 10 ranked unlabeled data, we choose the 1st, 4th, 7th, and 10th data.\nSimilarity The similarity strategy shares the same procedure as the diversity strategy of calculating the averaged similarity score for each unlabeled data. Nevertheless, the similarity strategy aims to find examples that are of the highest averaged similarity to the whole unlabeled training data space so that the sampled data will most likely be similar to the actual testing data. The underlying concept of this strategy is analogous to a family of density-weighted sampling strategies that look for the ones that appear most in the unlabeled data space or are most similar to unlabeled data (Fujii et al., 1999; Xu et al., 2003; Haffari and Sarkar, 2009). We follow the same mathematical procedure 2 above to calculate and rank the unlabeled data by the averaged similarity score. Then, differing from the diversity strategy, we retrieve n highest-ranked examples from the ranked list.\nAlgorithm 1 Proposed Data-based ICS Strategies 1: function ICS_STRATEGY(D, n, strategy) \u25b7 D : array of data content; n : sample size; strategy : strategy type 2: A \u2190(s(Di, D))i\u2208[1,|D|] \u25b7Average score 3: S \u2190argsort(A) \u25b7Descending order 4: if strategy = \u201cdiversity\u201d then 5: t = \ufffd |D| n \ufffd \u25b7Step 6: Return (Si)i\u22610(mod t) 1\u2264i\u2264|D| 7: else if strategy = \u201csimilarity\u201d then 8: Return (Si)i\u2208[1,n) 9: else if strategy = \u201chybrid\u201d then 10: t = \ufffd |D| (n/2) \ufffd 11: Rdiv = (Si)i\u22610(mod t) 1\u2264i\u2264|D| 12: S\u2032 = S \u2296Rdiv \u25b7Array subtract. 13: Rsim = (S\u2032 i)i\u2208[1,n/2) 14: Return Rdiv \u2295Rsim \u25b7Array concat. 15: end if 16: end function\nHybrid Similar to the aforementioned line of ensemble strategies that incorporate different strategies altogether in Section 2, our hybrid strategy expects to benefit from both above-mentioned strategies, which aims to locate examples that are either representative of the sampling space or of the highest similarity to the whole space. Subsequently, this hybrid strategy comprises two steps: first, sample n/2 examples following the diversity strategy, then sample n/2 examples following the similarity strategy from the remaining list.\n# 3.2 ICL Prompt Inputs Augmentation\nAs described in Section 3 and shown in Figure 1 above, ICS augments label predictions for the same data by constructing multiple disparate ICL combinations from the demonstration candidates sampled in the previous step. Many recent works (Chen et al., 2023b; Levy et al., 2023; Zhang et al., 2022b; Rubin et al., 2022; Nguyen and Wong, 2023; Lu et al., 2022; Liu et al., 2022) attempted different ICL constructions by altering the demonstrations\u2019 numbers, orderings, prompts, or sampling strategies. Nevertheless, there is no commonly recognized best strategy yet, and we believe models will learn disparate implicit guidance for solving the task via different demonstrations. In this work, we utilize four NLI datasets of varying difficulties and fix three as the number of demonstrations per\nprompt input, consistent with the number of NLI categories. This setting also applies to the CQA task in our evaluation. Still, the computation could be massive if we permutate every combination of the candidates. for example, 50 demonstration candidates can result in 19, 600 3-demonstration ICL combinations. We believe, however, that ICS does not need every ICL combination to find the model\u2019s most confident label. Analogous to the query-by-committee concept, where a few representative committee models vote for the best prediction, we plan to investigate a reasonable amount of \u201ccommittees\u201d (i.e., prompt inputs) that balance between establishing robust and reliable predictions and minimizing costs (i.e., computational resources, time, annotation efforts. The task of augmenting ICL prompt inputs can be naturally viewed as a variation of the candidate sampling task for the previous step, where the underlying concept for both steps attempts to sample a few examples that could be potentially helpful to LLMs. Despite that, the optimal strategy for candidate sampling may not be optimal for augmenting prompt inputs in terms of effectiveness and helpfulness. The demonstrations in each prompt input are ordered in the same order as they are sampled. In this work, we benchmark ICS over traditional ICL with a random strategy for augmenting prompt inputs in Section 4.1. Analogous to the sampling step, we implement and evaluate three similaritybased, model-agnostic strategies proposed in Section 4.2 to select demonstrations for each prompt input. Specifically, for each data to be predicted, we iteratively sample three demonstrations from the candidate list with a certain strategy for k times, remove them from the list, construct k different prompt inputs, and thus, acquire k predicted labels. For ICS strategy evaluation in Section 4.2, we leverage the best-performing parameters from the benchmark experiment, where n=100 and k=10.\n# 3.3 Confident Prediction Voting\nOnce we acquire a set of predicted labels from the abovementioned ICS steps for each datum to be predicted, we can apply different voting algorithms to find LLM\u2019s most confident prediction. A straightforward design could be a majority vote algorithm to select the prediction with the most appearances among all the predictions for the current data, which is analogous to finding the mode value mathematically: yfinal = mode(yics 1 , ..., yics k ), where yics k denotes the prediction for each augmented\nprompt input of data x. In this work, we leverage the majority vote algorithm in our prototyped ICS pipelines. We can further consider the model\u2019s different prediction confidences for a more complex algorithm design. Additionally, we can envision ICS to provide reliable unsupervised labels to iteratively fine-tune LLM and compact models in resource-deficient scenarios where expert annotations are difficult and expensive to access.\n# 4 Evaluations\nThe evaluation of our proposed ICS paradigm comprises bi-fold. First, in Section 4.1, we execute a benchmark experiment between the random ICS strategy and traditional ICL approach on five datasets with two LLMs to demonstrate the paradigm effectiveness. Additionally, we attempt to identify a sample size and the amount of augmented ICL combinations that strike a balance across three perspectives: 1) encompass sufficient diversity to represent the underlying data adequately, 2) possess robustness toward confident predictions, and 3) minimize annotation costs. Subsequently, in Section 4.2, we pick the best-performing parameters from the first experiment to compare the additional advantages of the three proposed ICS strategies described above in Section 3.1.\n# 4.1 Benchmark Evaluation: ICS vs. ICL 4.1.1 Setup\nWe conduct benchmark experiments to demonstrate the effectiveness of our ICS pipeline with a random sampling strategy for both sampling demonstration candidates and augmenting ICL prompt inputs. The baseline setting is a traditional ICL approach with the same amount of demonstrations in each prompt input. Specifically, we employ three open-source LLMs (FLAN-T5-XL (Chung et al., 2022), Mistral7B (Jiang et al., 2023)), and Mixtral-8x7B (Jiang et al., 2024), which is a Mixture-of-Experts (Jacobs et al., 1991; Shazeer et al., 2017) LLM. We experiment on three generic NLI tasks of increasing difficulties: e-SNLI (Camburu et al., 2018), Multi-NLI (Williams et al., 2017), ANLI (Nie et al., 2019), a domain-specific Contract-NLI (Koreeda and Manning, 2021) dataset, and the CommonsenseQA (Talmor et al., 2018) dataset (dataset statistics in Appendix B). We originally considered Llama2 (Touvron et al., 2023c) but eventually excluded it because our preliminary experiment, discussed in Appendix D,\nshows that Llama2 tends to output \u201cneutral\u201d regardless of the inputs on ANLI. We also conduct a small-scale ablation study with OpenAI\u2019s closedomain GPT-3.5 in Appendix E. We intended to manipulate and investigate two controlled variables of ICS: the size of sampled demonstration candidates n, where n \u2208 {50, 100, 250, 500}, and the number of augmented prompt inputs k for each data to be predicted, where k \u2208{3, 5, 10, 20}. We fix the number of demonstrations in each prompt input as three across all methodologies and experiments. The baseline is the vanilla ICL approach with randomly chosen three examples, denoted as baseline in Figure 2 and ICL in diagrams from Appendix C. We consider 500 annotations a reasonable budget cap for various real-world, low-resource scenarios. Each setting is repeated and averaged over 10 trials to counter the sampling randomness. All the detailed experiment settings, including the task instruction narrative, are reported in Appendix A.\n# 4.1.2 Results\nThe complete evaluation results for every setting are reported in Appendix C. We notice that the accuracy improvement becomes insignificant once n goes beyond 100. This observation implies that a sample size over 100 can be considered diverse and representative enough for the tasks we experimented with, and selecting more data would have only a marginal effect on representativeness. In Figure 2, we present the prediction accuracy of baseline ICL and our ICS strategy for every model and dataset when n = 100. We report the prediction accuracy as colored bars, where the green bars denote FLAN-T5-XL, the blue bars denote Mistral7B, and the orange bars denote Mixtral-8x7B. By comparing the accuracy differences in every diagram between the baseline ICL approach and our ICS strategy for each model, we can observe that ICS can consistently improve both LLMs\u2019 prediction performance in every (n, k) combination. It justifies the validity of our proposed ICS paradigm. It is not difficult to observe that the accuracy improvement provided by the ICS strategy for FLAN-T5-XL is much less than that for Mistral-7B and Mixtral-8x7B, where the latter two models illustrate more than 5% average improvement across all datasets with our ICS strategy. Additionally, we observe that FLAN-T5-XL results in extremely poor performance on Contract-NLI, implying that the model lacks domain knowledge to solve this\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bd3d/bd3da516-7612-4ae4-803b-8939dbb3f0d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) e-SNLI (Camburu et al., 2018)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65b7/65b745c2-c1dd-4e2e-b802-0d350d4f9cee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) CQA (Talmor et al., 2018)</div>\nFigure 2: Benchmark experiment of FLAN-T5-XL, Mistral-7B, and Mixtral-8x7B on five datasets with 100 sample demonstration candidates (n=100) for random ICS strategy compared with the baseline ICL approach.\n<div style=\"text-align: center;\">: Benchmark experiment of FLAN-T5-XL, Mistral-7B, and Mixtral-8x7B on five datasets with 100 sampled ration candidates (n=100) for random ICS strategy compared with the baseline ICL approach.</div>\ntask. Our discussion about the potential reasons for the disparate performance between different models is detailed in Section 5.\n# 4.2 ICS Strategy Evaluation\nGiven the observations from the previous benchmark experiment, the best-performing ICS setting in terms of the candidate sampling size and the size of augmented prompt inputs is when n=100 and k=10. In this ICS strategy evaluation experiment, we utilize this set of parameters and further investigate the effectiveness of different ICS strategies we introduced in Section 3.1 over the random ICS and baseline ICL strategies. We implement different ICS strategy combinations to conduct an in-depth analysis of the sampling strategies at each ICS step:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/33c3/33c3a297-eb8f-4d18-bd4a-bb2d60ecbcf9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Multi-NLI (Williams et al., 2017)</div>\n<div style=\"text-align: center;\">(d) Contract-NLI (Koreeda and Manning, 2021)</div>\nsampling demonstration candidates and augmenting the prompt inputs. We determine Mistral-7B as the backbone because it performs higher effectiveness toward ICL and more robust performance on the domain-specific dataset from the benchmark experiment, compared with FLAN-T5-XL. Compared with Mixtral-8x7B, inferencing with Mistral7B is faster and more cost-efficient. Because of the massive size of e-SNLI and MultiNLI (540k and 390k in train splits, correspondingly), we borrow the concept from Active Learning simulations (Yao et al., 2023a) to efficiently evaluate the strategies with a reasonable amount of data and acquire the averaged score over multiple trials. Specifically, for each trial, we randomly sample 3, 000 and 1, 000 data from the train and test split correspondingly as the actual train and\nSampling\nStrategy\nPrompting\nStrategy\ne-SNLI\n(Camburu et al., 2018)\nMulti-NLI\n(Williams et al., 2017)\nANLI\n(Nie et al., 2019)\nContract-NLI\n(Koreeda and Manning, 2021)\nDiversity\nDiversity\n73.28 (\u21918.54)\n62.10 (\u21915.20)\n42.78 (\u21912.36)\n87.66 (\u21918.83)\nDiversity\nRandom\n73.68 (\u21918.94)\n62.27 (\u21915.37)\n42.77 (\u21912.35)\n89.42 (\u219110.59)\nRandom\nDiversity\n73.47 (\u21918.73)\n61.21 (\u21914.31)\n42.33 (\u21911.91)\n87.53 (\u21918.70)\nSimilarity\nSimilarity\n73.63 (\u21918.89)\n61.79 (\u21914.89)\n42.47 (\u21912.05)\n90.44 (\u219111.61)\nSimilarity\nRandom\n74.11 (\u21919.37)\n62.09 (\u21915.19)\n42.60 (\u21912.18)\n90.48 (\u219111.65)\nRandom\nSimilarity\n73.74 (\u21919.00)\n62.17 (\u21915.27)\n42.63 (\u21912.21)\n88.88 (\u219110.05)\nHybrid\nHybrid\n73.86 (\u21919.12)\n62.52 (\u21915.62)\n42.59 (\u21912.17)\n88.85 (\u219110.02)\nHybrid\nRandom\n73.96 (\u21919.22)\n62.41 (\u21915.51)\n42.56 (\u21912.14)\n89.73 (\u219111.90)\nRandom\nHybrid\n73.95 (\u21919.21)\n62.39 (\u21915.49)\n42.45 (\u21912.03)\n89.06 (\u219110.23)\nRandom\nRandom\n72.57 (\u21917.83)\n61.17 (\u21914.27)\n42.22 (\u21911.80)\n86.69 (\u21917.86)\nICL (Baseline)\n64.742\n56.905\n40.420\n78.83\nTable 1: Comparison of different ICS strategies versus the ICL baseline on four datasets with Mistral-7B (Jiang et al., 2023). We implement different strategy combinations and average each score over 40 trials. The change in prediction accuracy compared with the traditional ICL approach is reported in the parenthesis.\ntest data for the current trial. We then conduct each setting 40 trials to minimize the randomness provided by subsampling training and testing data and report the averaged prediction accuracy in Table 1.\n# 4.2.2 Results\nIn addition to the prediction accuracy of different ICS strategy combinations, we also report the change in prediction accuracy compared with the baseline ICL approach in the parenthesis, where green denotes improvement. We can easily observe that all three ICS sampling strategies (diversity, similarity, and hybrid) can consistently and significantly improve the prediction accuracy of Mistral-7B compared with the baseline setting, with more than 9% improvement on e-SNLI and two-digits elevation on Contract-NLI. It is worth noticing that all the ICS settings with non-random strategies in at least one ICS step can outperform the benchmark ICS setting that utilizes the random strategy for both sampling and prompt augmentation. From the results, we can also observe that no single best strategy exists, even for the same NLI task. This observation is aligned with our motivation and the aforementioned existing works that different ICL demonstrations provide distinct knowledge about the task, and there\u2019s no single best ICL strategy yet. Specifically, the diversity strategy stands out on ANLI, whereas the hybrid strategy outperforms the other strategies on MultiNLI, and the similarity strategy surpasses the others on e-SNLI as well as Contract-NLI. Additionally, we observe that non-random strategies do not lead to consistent performance improve-\nment for augmenting ICL prompt inputs by comparing them with the random strategy. For example, leveraging the random strategy for augmenting prompt inputs outperforms the similarity strategy on all four datasets, implying that high similarity among the demonstrations within each prompt input is not preferred. On the other hand, we can observe a significant performance improvement in leveraging non-random strategies demonstration candidate sampling compared to the random strategy. This observation leads to the conclusion that all three strategies demonstrate more contributions during demonstration candidate sampling compared with augmenting ICL prompt inputs. We also hypothesize that more carefully curated strategies are needed to sample ICL combinations effectively, leaving a broader avenue for future research. Furthermore, we notice the improvement provided by ICS sampling strategies is inversely proportional to the difficulty of the tasks. If we consider the model\u2019s baseline ICL performance from Section 4.1 as a faithful indicator of dataset difficulty, we can conclude that the dataset ordering in ascending order of task difficulty will be e-SNLI, Multi-NLI, and ANLI, where the performance improvement provided by ICS strategies is the smallest on ANLI and the largest on e-SNLI. Our evaluation of different ICS strategies illustrates promising results that fundamental similaritybased algorithms can effectively increase ICS enhancement, leading to broader future research avenues in exploiting the benefits of more carefully curated ICS strategies with LLMs.\n# 5 Discussion\nLimited Performance with FLAN-T5 FLANT5 models have been fine-tuned on various downstream tasks, including NLI. This fine-tuning could indeed influence the models\u2019 performance in incontext learning scenarios, potentially skewing the effectiveness of ICS. Additionally, we observe FLAN-T5-XL results in poor performance on Contract-NLI from Figure 2, despite it can perform adequately well on the other three generic-domain NLI datasets. We conduct an ablation study with FLAN-T5-XL for ICL to investigate the potential reasons and report in Appendix F. Given the ablation study results, we hypothesize several possible reasons: 1) FLAN-T5-XL falls short of properly interpreting long text sequences; 2) FLAN-T5-XL was not fine-tuned to elevate the ability to interpret ICL demonstrations, and 3) FLAN-T5-XL lacks the necessary domain knowledge to solve the Contract-NLI task.\nICS-Related Work A very recent work attempts multiple ICL methodologies to investigate whether LLMs can beat domain-specific fine-tuned models in the medical domain (Nori et al., 2023). The Choice Shuffling Ensemble technique in their proposed ensemble methodology shares a similar concept with our proposed ICS paradigm, but the authors only focus on shuffling the answer choices for selecting robust predictions. Nevertheless, we believe that ICS depicts vast prospects and potential to exploit the capabilities of LLMs.\n# 6 Conclusion\nThis work presents In-context Sampling (ICS), a novel In-Context Learning paradigm for probing confident predictions by sampling demonstration candidates and augmenting different ICL prompt inputs. Our experiments show that even ICS with the random strategy can lead to consistent accuracy improvement compared with the traditional ICL approach, and further illustrate the additional helpfulness provided by three fundamental but effective data similarity-based sampling strategies with ICS. Our work lays the foundation for implementing ICL-based applications to support non-expert users in the real world, as they do not know how to write a single perfect prompt to get their work done but often write multiple prompt inputs (ZamfirescuPereira et al., 2023). Our method aligns well with such user scenarios.\nThe primary focus of this paper is to propose and demonstrate the effectiveness of our ICS pipeline compared with the traditional ICL approach. Thus, we do not compare with other prompting strategies that do not focus on in-context demonstrations, such as Chain-of-Thoughts. Our experiments showed that ICS can improve the model\u2019s performance (in prediction accuracy) even with a random strategy. We further illustrate the potential of three proposed similarity-based ICS strategies, which, despite fundamental, can further exploit LLM\u2019s capability and boost the prediction performance. However, despite extensive experiments with different n and k combinations, several potential variables require further analysis. For instance, we considered five datasets of different difficulties and each ICL combination is arbitrary, where four of the datasets are NLI tasks and the other one is a commonsense QA task. The generalizability of the ICS paradigm to other types of tasks goes beyond the scope of this paper, and we are working on this interesting and substantial research question as a follow-up work, especially in real-world scenarios. We only implement and evaluate the same three strategies for both steps of sampling demonstration candidates and augmenting prompt inputs in ICS because the data similarity-based strategies are model agnostic and generally require fewer computing resources than model-based strategies. We are also aware that the optimal strategy for demonstration candidate sampling may not be optimal for prompt input augmentations, and we leave the analysis of strategy optimization for future work. In addition, we do not perform an in-depth analysis of optimizing time consumption and reducing computing resources in this work, though we are aware that ICS may require more time than the traditional ICL approach. Lastly, our experiment comprises four open-source LLMs as the original plan but excludes Llama2 due to its over inclination to predict the \u201cneutral\u201d category (Appendix D). We identify that there are still a variety of other instructional-finetuned LLMs we do not include in this work, such as InstructGPT (Ouyang et al., 2022). We do not focus on close-sourced and commercial-oriented LLMs such as GPT-4 (OpenAI, 2023) in this work. However, we report a small-scale ablation study with GPT-3.5 in Appendix E that further illustrates the generalizability of our proposed ICS strategy.\nNaoki Abe and Hiroshi Mamitsuka. 1998. Query learning strategies using boosting and bagging. In Proceedings of the Fifteenth International Conference on Machine Learning, ICML \u201998, page 1\u20139, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Vamshi Ambati, Stephan Vogel, and Jaime Carbonell. 2011. Multi-strategy approaches to active learning for statistical machine translation. In Proceedings of Machine Translation Summit XIII: Papers, Xiamen, China.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.\nom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, pages 1877\u20131901, Red Hook, NY, USA. Curran Associates Inc.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31.\nJiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, and Yuling Sun. 2023a. Fairytalecqa: Integrating a commonsense knowledge graph into children\u2019s storybook narratives. arXiv preprint arXiv:2311.09756.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. Pinar Donmez, Jaime G Carbonell, and Paul N Bennett. 2007. Dual strategy active learning. In Machine Learning: ECML 2007: 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007. Proceedings 18, pages 116\u2013127. Springer. Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In 34th Annual Meeting of the Association for Computational Linguistics, pages 319\u2013326, Santa Cruz, California, USA. Association for Computational Linguistics.\nAtsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1999. Selective sampling for example-based word sense disambiguation. arXiv preprint cs/9910020.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3063/30638914-efce-4141-8ba9-07cf2bed0a2b.png\" style=\"width: 50%;\"></div>\nProceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\nProceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming FewShot Prompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Yuxuan Lu, Bingsheng Yao, Shao Zhang, Yun Wang, Peng Zhang, Tun Lu, Toby Jia-Jun Li, and Dakuo Wang. 2023. Human still wins over llm: An empirical study of active learning on domain-specific annotation tasks. arXiv preprint arXiv:2311.09825. Amama Mahmood, Junxiang Wang, Bingsheng Yao, Dakuo Wang, and Chien-Ming Huang. 2023. Llmpowered conversational voice assistants: Interaction patterns, opportunities, challenges, and design guidelines. Katerina Margatina, Giorgos Vernikos, Lo\u00efc Barrault, and Nikolaos Aletras. 2021. Active learning by acquiring contrastive examples. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650\u2013663, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Andrew McCallum, Kamal Nigam, et al. 1998. Employing em and pool-based active learning for text classification. In ICML, volume 98, pages 350\u2013358. Citeseer. Prem Melville and Raymond J Mooney. 2004. Diverse ensembles for active learning. In Proceedings of the twenty-first international conference on Machine learning, page 74. Tai Nguyen and Eric Wong. 2023. In-context example selection with influences. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2019. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. 2023. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452. Fredrik Olsson. 2009. A literature survey of active machine learning in the context of natural language processing. OpenAI. 2023. Gpt-4 technical report.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. 2023. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452.\nFredrik Olsson. 2009. A literature survey of active machine learning in the context of natural language processing. OpenAI. 2023. Gpt-4 technical report.\nChristopher Schr\u00f6der, Andreas Niekler, and Martin Potthast. 2022. Revisiting uncertainty-based query strategies for active learning with transformers. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2194\u20132203, Dublin, Ireland. Association for Computational Linguistics.\nFindings of the Association for Computational Linguistics: ACL 2022, pages 2194\u20132203, Dublin, Ireland. Association for Computational Linguistics. Ozan Sener and Silvio Savarese. 2017. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489. Burr Settles. 2009. Active learning literature survey. Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In proceedings of the 2008 conference on empirical methods in natural language processing, pages 1070\u2013 1079. H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. 1992. Query by committee. In Proceedings of the fifth annual workshop on Computational learning theory, pages 287\u2013294. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 589\u2013 596, Barcelona, Spain. Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, et al. 2022. On the effect of pretraining corpora on in-context learning by a large-scale language model. arXiv preprint arXiv:2204.13509. Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. 2019. Variational adversarial active learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5972\u20135981. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937. Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 120\u2013127, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Katrin Tomanek and Udo Hahn. 2009. Reducing class imbalance during active learning for named entity annotation. In Proceedings of the fifth international conference on Knowledge capture, pages 105\u2013112.\nOzan Sener and Silvio Savarese. 2017. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489.\nBurr Settles. 2009. Active learning literature survey.\nMin Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 120\u2013127, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\nKatrin Tomanek and Udo Hahn. 2009. Reducing class imbalance during active learning for named entity annotation. In Proceedings of the fifth international conference on Knowledge capture, pages 105\u2013112.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023b. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023c. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Boshi Wang, Xiang Deng, and Huan Sun. 2022. Iteratively prompt pre-trained language models for chain of thought. arXiv preprint arXiv:2203.08383. Chenguang Wang, Laura Chiticariu, and Yunyao Li. 2017. Active learning for black-box semantic role labeling with neural factors. In IJCAI, pages 2908\u2013 2914. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind K. Dey, and Dakuo Wang. 2024. Mentalllm: Leveraging large language models for mental health prediction via online text data. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 8(1). Zhao Xu, Kai Yu, Volker Tresp, Xiaowei Xu, and Jizhi Wang. 2003. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14\u201316, 2003. Proceedings 25, pages 393\u2013407. Springer. Ziqi Yang, Xuhai Xu, Bingsheng Yao, Shao Zhang, Ethan Rogers, Stephen Intille, Nawar Shara, Guodong Gordon Gao, and Dakuo Wang. 2024. Talk2care: Facilitating asynchronous patientprovider communication with large-language-model.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank Srivastava, Yunyao Li, James Hendler, and Dakuo Wang. 2023a. Beyond labels: Empowering human annotators with natural language explanations through a novel active-learning architecture. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11629\u201311643, Singapore. Association for Computational Linguistics. Bingsheng Yao, Prithviraj Sen, Lucian Popa, James Hendler, and Dakuo Wang. 2023b. Are human explanations always helpful? towards objective evaluation of human natural language explanations. arXiv preprint arXiv:2305.03117. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. arXiv preprint arXiv:2205.12685. Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, and Chao Zhang. 2023. Cold-start data selection for better few-shot language model fine-tuning: A prompt-based uncertainty propagation approach. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2499\u20132521, Toronto, Canada. Association for Computational Linguistics. J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why johnny can\u2019t prompt: How non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI \u201923, New York, NY, USA. Association for Computing Machinery. Xiangkai Zeng, Sarthak Garg, Rajen Chatterjee, Udhyakumar Nallasamy, and Matthias Paulik. 2019. Empirical evaluation of active learning techniques for neural MT. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 84\u201393, Hong Kong, China. Association for Computational Linguistics. Shao Zhang, Jianing Yu, Xuhai Xu, Changchang Yin, Yuxuan Lu, Bingsheng Yao, Melanie Tory, Lace M Padilla, Jeffrey Caterino, Ping Zhang, et al. 2023. Rethinking human-ai collaboration in complex medical decision making: A case study in sepsis diagnosis. arXiv preprint arXiv:2309.12368. Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. 2022a. ALLSH: Active learning guided by local sensitivity and hardness. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1328\u20131342, Seattle, United States. Association for Computational Linguistics. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active Example Selection for In-Context Learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n9134\u20139148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nZhisong Zhang, Emma Strubell, and Eduard Hovy. 2022c. A survey of active learning for natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6166\u20136190, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K Tsou. 2008. Active learning with sampling by uncertainty and density for word sense disambiguation and text classification. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1137\u20131144, Manchester, UK. Coling 2008 Organizing Committee.\nZhisong Zhang, Emma Strubell, and Eduard Hovy. 2022c. A survey of active learning for natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6166\u20136190, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\nDataset\nTrain\nValidation\nTest\ne-SNLI\nCamburu et al. (2018)\n549, 367\n9, 842\n9, 824\nMulti-NLI\nWilliams et al. (2017)\n392, 702\n9, 815\n9, 832\nANLI\nNie et al. (2019)\n16, 946\n1, 000\n1, 000\nContract-NLI\nKoreeda and Manning\n(2021)\n3, 999\n555\n1, 113\nCommonsenseQA\nTalmor et al. (2018)\n9, 741\n1, 221\n1, 140\nTable 2: Datasets involved in our experiment. ContractNLI only comprises annotations of \u201centailment\u201d and \u201ccontradiction\u201d categories.\n# A Experiment Setup\nWe incorporate four natural language inference datasets (e-SNLI, Multi-NLI, ANLI, and ContractNLI) in our evaluation. Thus, we leverage the same instruction narrative across all the experiments for these datasets: Determine whether a hypothesis is entailment, neutral, contradiction giving a premise. For Contract-NLI, the original dataset only consists of annotations for the \u201centailment\u201d and \u201ccontradiction\u201d categories. Thus, we only evaluate the performance of those data. For CommonsenseQA, we design the prompt to be: Answer this commonsense question from the given choices. All the experiments are computed on one of two resources: 1) an NVIDIA A100 40G graphic card or 2) an NVIDIA 3090 24G graphic card. To fit the models in both graphic cards, we load both Llama2 and Mistral-7B in fp16 precision, load Mixtral8x7B in 4-bit precision, and limit to generate a maximum of 10 tokens.\n# B Dataset Statistics\n# C Complete Evaluation Results\nHere, we report the complete results of our evaluation (Section 4) in Figure 3, 4, 5, 6, 7 on e-SNLI, Multi-NLI, ANLI, Contract-NLI, and CQA, correspondingly. We acquire an average prediction accuracy score over 10 trials of each setting. n denotes the amount of demonstration candidate data we sampled, and k denotes the number of ICL combinations for each test data. We can observe that the ICS strategy can consistently improve LLMs\u2019 performance compared with the traditional ICL baseline; in addition, FLAN-T5XL is much less sensitive than Mistral and Mixtral toward the improvement provided by the ICS strat-\nLlama2\nInst. 1\nInst. 2\nInst. 3\nGround-truth\nentailment\n75\n202\n151\n334\nneutral\n808\n668\n785\n333\ncontradiction\n117\n130\n64\n333\nSetting\ne-SNLI\nMulti-NLI\nANLI\nCommonsenseQA\nICL\n0.57\n0.55\n0.55\n0.78\nICS\n0.59\n0.6\n0.58\n0.81\nTable 4: Ablation study with GPT-3.5 on four datasets.\negy. From the diagrams, k = 10 and n = 100 are the best-performing parameters that maximize the performance improvement and minimize the standard deviations.\n# D Analysis on Llama2\nWe conduct an initial inference experiment with Llama2 (Touvron et al., 2023c) on ANLI utilizing three different natural language instructions: i Determine whether a hypothesis is entailment, neutral, contradiction giving a premise. ii Classifying a pair of premise and hypothesis sentences into three classes: entailment, neutral, contradiction iii Predict the relationship between the premise and hypothesis by entailment, neutral, contradiction\nThe results are reported in Table 3. We can easily observe that Llama2 tends to overly predict \u201cneutral\u201d over the other two categories despite changing instruction narratives, whereas the ground-truth distribution is even across categories. Thus, we omit Llama2 in our work. There could be different reasons contributing to this issue; for example, Llama2 was overfitted to the NLI task or similar tasks that share the same set of targeting categories: \u201centailment\u201d, \u201cneutral\u201d, and \u201ccontradiction\u201d.\n# E Ablation Study with GPT-3.5\nWe extend the scope of our work by conducting ablation experiments with OpenAI\u2019s close-domain GPT-3.5 on four datasets. For each dataset, we randomly sample 200 examples from the test split and report the averaged accuracy on three trials, due to budget limit. Our result in Table 4 shows that the proposed ICS strategy can consistently improve the performance of close-domain LLMs as well, strengthening the generalizability of our strategy.\nSetting\nzero-shot\n1-shot\n2-shot\n3-shot\nICL\n2.48\n19.39\n23.80\n22.88\nICS\n/\n20.03\n24.54\n23.34\nTable 5: ICL ablation experiment of FLAN-T5-XL on Contract-NLI.\n# F Ablation on FLAN-T5-XL with Contract-NLI\nWe design and conduct an ablation study with FLAN-T5-XL for ICL to verify our hypothesis. The experiment is conducted on the Contract-NLI dataset. Specifically, we start with the zero-shot setting to examine whether FLAN-T5-XL can properly solve the task without demonstrations. Then, we experiment with both ICS and ICL approaches and gradually increase the number of demonstrations from 1 to 3. The demonstrations are randomly selected from the training split, and each ICL setting is repeated 3 times to acquire the average score. From table 5, we can observe that FLAN-T5-XL can hardly interpret the dataset and solve it with a zero-shot setting. Since we leverage the same prompt narrative as the one for the other NLI tasks that FLAN-T5-XL performs relatively well, we can imply that the lack of domain knowledge might be the primary reason for such low performance. Nevertheless, we can observe that the 1-shot setting can significantly improve the model performance, although the overall accuracy is still very low. It is worth noticing that the improvement becomes relatively trivial once we add more demonstrations to the prompt inputs, which implies that FLANT5-XL falls short of interpreting longer and more complex ICL format, possibly due to its relatively short training input length limit. Moreover, our random ICS strategy can still outperform the ICL baseline across all settings.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/065c/065ce69c-91e9-49a9-a0e1-5277a56d3bde.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fdd1/fdd19796-db8c-43f3-9f61-413330f865ce.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Evaluation results with FlanT5-XL, Mistral, and Mixtral on Multi-NLI (Williams et al., 2017) dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e76/1e76dbf2-3c8f-4b36-ba35-54dd6ad81ae1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) n=100</div>\n<div style=\"text-align: center;\">(d) n=500</div>\n<div style=\"text-align: center;\">(b) n=100</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/86b2/86b2a96e-2597-473c-a4f6-2275cb8dcc52.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a9f4/a9f4eda4-f920-47aa-812e-d4dc31daccfd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) n=250</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2447/24479d4d-05ce-4b91-8e3c-b42f558d42b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) n=500</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8701/87014bba-5470-40e8-ba01-328caa154806.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Evaluation results with FlanT5-XL, Mistral, and Mixtral on CQA (Talmor et al., 2018) dataset.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d846/d8463dc8-e1bc-4827-9cea-bdcb1f1b6ea3.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of improving the performance of Large Language Models (LLMs) through a new prompting technique called In-Context Sampling (ICS), which optimizes the construction of multiple prompt inputs to enhance prediction confidence.",
        "problem": {
            "definition": "Existing methods for In-Context Learning (ICL) focus on selecting a better set of data samples within a single prompt input but fail to leverage multiple prompts together for improved predictions.",
            "key obstacle": "The main challenge is that current methods do not effectively utilize the diverse knowledge provided by different ICL demonstrations, leading to suboptimal model performance on complex tasks."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that different ICL demonstrations provide distinct knowledge about the task, which can lead to varying predictions for the same data.",
            "opinion": "The proposed method, In-Context Sampling (ICS), involves augmenting multiple ICL prompt inputs to facilitate more accurate and confident predictions from LLMs.",
            "innovation": "ICS differs from traditional ICL by introducing a committee-based approach that samples multiple demonstrations and aggregates their predictions, thereby enhancing the model's performance."
        },
        "method": {
            "method name": "In-Context Sampling",
            "method abbreviation": "ICS",
            "method definition": "ICS is a low-resource prompting technique that constructs multiple ICL prompt inputs by sampling demonstration candidates and voting on the most confident predictions.",
            "method description": "ICS augments ICL prompt inputs by sampling representative candidates and combining them to improve prediction accuracy.",
            "method steps": [
                "Sample demonstration candidates from the dataset.",
                "Augment ICL prompt inputs by combining different candidates.",
                "Vote on the most confident prediction based on the augmented inputs."
            ],
            "principle": "ICS is effective because it leverages the diversity of multiple ICL demonstrations, allowing the model to capture a broader range of task-related knowledge."
        },
        "experiments": {
            "evaluation setting": "The evaluation involves three open-source LLMs (FLAN-T5-XL, Mistral-7B, and Mixtral-8x7B) tested on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA).",
            "evaluation method": "Performance is assessed by comparing prediction accuracy between the ICS approach and traditional ICL methods across various settings, with results averaged over multiple trials."
        },
        "conclusion": "The results demonstrate that ICS consistently improves prediction accuracy compared to traditional ICL methods, highlighting its effectiveness in enhancing LLM performance through diverse prompt inputs.",
        "discussion": {
            "advantage": "ICS provides significant improvements in prediction accuracy by effectively utilizing multiple ICL demonstrations, which traditional methods fail to do.",
            "limitation": "The method may require more computational resources compared to traditional ICL approaches, and its optimal performance may vary depending on the task complexity.",
            "future work": "Future research should explore optimizing the ICS strategies for different types of tasks and investigate the potential for further performance enhancements."
        },
        "other info": {
            "dataset details": {
                "e-SNLI": {
                    "train": 549367,
                    "validation": 9842,
                    "test": 9824
                },
                "Multi-NLI": {
                    "train": 392702,
                    "validation": 9815,
                    "test": 9832
                },
                "ANLI": {
                    "train": 16946,
                    "validation": 1000,
                    "test": 1000
                },
                "Contract-NLI": {
                    "train": 3999,
                    "validation": 555,
                    "test": 1113
                },
                "CommonsenseQA": {
                    "train": 9741,
                    "validation": 1221,
                    "test": 1140
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of improving the performance of Large Language Models (LLMs) through a new prompting technique called In-Context Sampling (ICS), which optimizes the construction of multiple prompt inputs to enhance prediction confidence."
        },
        {
            "section number": "1.3",
            "key information": "ICS differs from traditional ICL by introducing a committee-based approach that samples multiple demonstrations and aggregates their predictions, thereby enhancing the model's performance."
        },
        {
            "section number": "3.3",
            "key information": "ICS augments ICL prompt inputs by sampling representative candidates and combining them to improve prediction accuracy."
        },
        {
            "section number": "4.1",
            "key information": "The proposed method, In-Context Sampling (ICS), involves augmenting multiple ICL prompt inputs to facilitate more accurate and confident predictions from LLMs."
        },
        {
            "section number": "6.2",
            "key information": "The method may require more computational resources compared to traditional ICL approaches, and its optimal performance may vary depending on the task complexity."
        },
        {
            "section number": "6.4",
            "key information": "ICS provides significant improvements in prediction accuracy by effectively utilizing multiple ICL demonstrations, which traditional methods fail to do."
        }
    ],
    "similarity_score": 0.7533855958378468,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/More Samples or More Prompts_ Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering.json"
}