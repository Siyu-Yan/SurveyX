{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2412.10960",
    "title": "Can LLMs Help Create Grammar?: Automating Grammar Creation for Endangered Languages with In-Context Learning",
    "abstract": "Yes! In the present-day documenting and preserving endangered languages, the application of Large Language Models (LLMs) presents a promising approach. This paper explores how LLMs, particularly through in-context learning, can assist in generating grammatical information for low-resource languages with limited amount of data. We takes Moklen as a case study to evaluate the efficacy of LLMs in producing coherent grammatical rules and lexical entries using only bilingual dictionaries and parallel sentences of the unknown language without building the model from scratch. Our methodology involves organising the existing linguistic data and prompting to efficiently enable to generate formal XLE grammar. Our results demonstrate that LLMs can successfully capture key grammatical structures and lexical information, although challenges such as the potential for English grammatical biases remain. This study highlights the potential of LLMs to enhance language documentation efforts, providing a cost-effective solution for generating linguistic data and contributing to the preservation of endangered languages.",
    "bib_name": "spencer2024llmshelpcreategrammar",
    "md_text": "# Can LLMs Help Create Grammar?: Automating Grammar Creation for Endangered Languages with In-Context Learning\n\nPiyapath T Spencer 1,2 and Nanthipat Kongborrirak 1\nLinguistic Analysis\nn Process\n(e.g. Grammar Induction)\ngDoc)\n1 Language and Information Technology Programme, Faculty of Arts, CU, Thailand 2 Center for Information and Language Processing (CIS), LMU Munich, Germany linguistics@piyapath.uk prompt.k10@gmail.com\n\n# Abstract\n\nWordlist (Dictionary)\n\u0e21\u0e32\u0e25\u0e31\u0e14 (v) to love \u0e2e\u0e30\u0e2b\u0e4c (prt) general negator \u0e0d\u0e32 (pron) third person singular pronoun \u0e40\u0e08\u0e47\u0e22 (pron) first person singular pronoun\nGenerati\n\u22ee\nFormatte\nGrammar R\n& Lexical En\nYes! In the present-day documenting and preserving endangered languages, the application of Large Language Models (LLMs) presents a promising approach. This paper explores how LLMs, particularly through in-context learning, can assist in generating grammatical information for low-resource languages with limited amount of data. We takes Moklen as a case study to evaluate the efficacy of LLMs in producing coherent grammatical rules and lexical entries using only bilingual dictionaries and parallel sentences of the unknown language without building the model from scratch. Our methodology involves organising the existing linguistic data and prompting to efficiently enable to generate formal XLE grammar. Our results demonstrate that LLMs can successfully capture key grammatical structures and lexical information, although challenges such as the potential for English grammatical biases remain. This study highlights the potential of LLMs to enhance language documentation efforts, providing a cost-effective solution for generating linguistic data and contributing to the preservation of endangered languages.\n\n# 1 Introduction\n\nIn linguisitics, the quest for universality (Chomsky and Berwick, 2016), despite its criticism, motivates the comparison of languages to identify common patterns and structures. Many linguists investigate how different languages handle concepts like tense, number, or syntactic structure to identify universal features or constraints. Within this broader discourse, LexicalFunctional Grammar (LFG) offers a model for understanding the complexities of different information within and across languages. In its different structures, it assumes both variability and universality. For instance, the analysis of a language with a free word order sequence is valid within the LFG framework (Simpson, 1991).\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5322/53228af4-2a55-4edc-bb53-516ed0279ea0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The \u03d5 mapping (Dalrymple, 2023) from different c(onstituent)-structures of Moklen and English to the same f(unctional)-structure\n</div>\nAccording to Butt et al. (1999), \"a common set of linguistic principles and a commonly agreed upon set of grammatical analyses and features\" unify linguistic insights across languages whilst acknowledge the uniqueness of individual languages. The XLE (Xerox Linguistic Environment), developed on the basis of LFG (Maxwell and Kaplan; Crouch et al., 2011), serves as a powerful tool for the computational implementation of multilingual grammars (grammars across a wide range of languages). However, the \u2019grammars\u2019 are usually manually constructed since it requires the understanding of nuances within a language, which is expensive and time-consuming to produce. This fact makes it difficult to start developing deep grammar for low-resource languages, especially endangered languages. The recent development of large language models (LLM) has shown notable capabilities for \u2019adaptation\u2019 (Brown et al., 2020; Wei et al., 2022) to various tasks and understanding natural language instructions through in-context learning and understanding natural language instructions with incontext learning (Brown et al., 2020; OpenAI, 2024; Team, 2024). Their linguistic performances\n\nand metalinguistic abilities have also been recognised (Begu\u0161 et al., 2023). This presents an opportunity to use their capabilities in generating grammar and linguistic information for endangered languages, potentially overcoming the need for exhaustive resources and the limitations of manual grammar construction. This paper proposes a novel approach that exploits LLMs\u2019 linguistic competences in English language to generate such coherent formal language as XLE grammar for natural languages that have not been encountered during the model\u2019s pre-training. This approach relies solely on bilingual dictionaries and few pairs of parallel sentences, which reflect the amount of data linguists typically has at hand during the process of documenting endangered languages (Spencer, 2024).\n\n# 2 Background and Related Work\n\nDespite the significant advancements, most existing studies focus on high-resource languages, with limited exploration of how LLMs can be effectively applied to lower-resource or truly low-resource languages. This gap is particularly concerning given the vast number of languages spoken worldwide, many of which lack sufficient digital representation and the risk of extinction. On Large Multilingual Grammar Development Large multilingual grammar development has seen significant advancements through theoretical grammar models such as Head-Driven Phrase Structure Grammar (HPSG) and Lexical-Functional Grammar (LFG). Such projects include LinGO (Oepen et al., 2002), Matrix (Zamaraeva et al., 2022), and DELPH-IN (Copestake, 2002) for HPSG, and the ParGram project (Butt et al., 2002) for LFG. These frameworks often rely on rule-based systems to capture the syntactic and semantic nuances of multiple languages. Their common objective is to create comprehensive grammatical analyses that can be applied across diverse linguistic contexts. However, the manual construction of these grammars demands extensive linguistic expertise and resources, which can be considered as a barrier for low-resource languages. On \u2019 Low-Resource \u2019 In the fields of artificial intelligence (AI), natural language processing (NLP), and LLMs, the term\"low-resource\" is surprisingly is a broad. This classification even includes languages like German, Filipino, and some institutional languages that have\n\ntheir own pretrained models, machine translation (MT) systems, and various NLP applications. This disparity arises from the uneven distribution of linguistic resources, with only 14 languages comprising over 90% of internet content, and English alone accounting for half of all data (W3Techs). In contrast, there are over 7,000 known languages, with approximately half classified as endangered according to Ethnologue. Many of these endangered languages are likely to disappear by the end of this century. When considering endangered languages alongside the NLP concept of low-resource languages, it becomes evident that many of these languages even lacks native speakers in their own society, not to mentioned its minimal or no digital presence. Often, they are primarily spoken languages without a writing system, meaning there may be no written documentation or corpus available. Despite these challenges, linguists have made efforts to document these sorts of languages by, firstly, collecting vocabulary, creating grammar books and dictionaries to preserve their existence of language in the world. On Competence and Performance Psycholinguistically, a distinction exists between linguistic competence and performance (Pritchett,\n1992). Competence refers to the innate ability to control all aspects of a language\u2019s structure, ranging from the intricate array of grammatical rules to pragmatic nuances of usage. Performance, on the other hand, pertains to the actual production of language in real-world contexts. This distinction differentiates a critical difference between human linguistic abilities and those of AI, as LLMs possess extensive knowledge of a language due to their rigorous pretraining on large datasets. When discussing generative AI, the focus is often on how language is created or generated, as suggested by the term itself. Generative AI, including LLMs, thus simulates human linguistic performance (Miranda-Saavedra, 2024). In the context of linguistics, this raises questions about how language is produced and understood. Over time, benchmarks for LLMs have shown saturation, indicating that models are becoming increasingly powerful and capable of performing complex tasks, as exemplified by models like GPT-4. This leads to comparisons between AI capabilities and human language abilities, prompting inquiries into whether LLMs truly understand language. Generally, the performance of LLMs is largely determined by both quality and, but much more\n\nimportant, quantity of the data on which they are trained. Whilst LLMs may demonstrate impressive capabilities across various languages, this does not necessarily guarantee a true understanding of those languages. This distinction is particularly evident in the case of English, where LLMs benefit from extensive training on vast and diverse datasets. As a result, their performance in English is often more robust and nuanced compared to their performance in lower-resource languages, which may not have the same level of data availability and linguistic representation. On In-Context Learning Recent advancements in LLMs have improved their performance not by training from scratch or requiring extensive datasets, but through prompt engineering that enables in-context learning. This approach allows models to adopt information presented within the context of specific inputs to generate relevant and coherent responses. Various techniques, such as zero-shot, few-shot, and chainof-thought prompting, have emerged as effective strategies for enhancing LLM capabilities. In the context of low-resource languages, incontext learning has been employed to refine LLM performance, particularly in machine translation. For instance, Tanzer et al. (2024) utilised dictionaries and grammar books to translate endangered languages with LLMs, establishing benchmarks for evaluation. Zhang et al. (2024), whose work this paper aims to build upon, extended this approach to cover multiple NLP tasks, evaluating their LINGOLLM methodology across different endangered languages. In contrast to these studies, we seek to advance the application of LLMs in a more radical linguistic task: analysing the grammar of languages under extremely limited conditions where prior data is not applicable.\n\n# 3 Language under Study: Moklen Language\n\nMoklen (ISO 639-3: mkm) is an endangered language estimated to be spoken by fewer than 1,000 people, which constitutes only one-fourth of the total population, most of whom are over 50 years old. The speakers are predominantly scattered across Phang Nga and Phuket in Southern Thailand. Moklen has been influenced by Malay and Thai, the latter being the national language of Thailand. It is classified as one of the Austronesian languages.\n\nDespite attempts by the community to use the Thai alphabet to record and teach Moklen to children, the language remains primarily oral, with no formal written tradition or administrative use. Consequently, there is little to no evidence of the language being used on the internet. Moklen is analysed to have a subject-verb-object (SVO) word order and a nominative-accusative alignment. While this word order is prominent, Moklen also features topicalisation to emphasise certain parts of an utterance. Additionally, it lacks inflectional morphology, meaning that words can be combined in various ways to convey meaning. Grammatical features such as tense, aspect, and number are expressed through content words. Thorough documentation of the language began in late 2017, leading to the development of a pilot version of a dictionary (Pittayaporn et al., 2022), following the establishment of a Thai-based orthography system (Pittayaporn and Choemprayong, Forthcoming). Nevertheless, there are only a few samples of Moklen sentences, primarily derived from field notes, and limited work has been done on the language. Considering all these aspects, Moklen is wellsuited for the current study. Firstly, it meets the requirements of the paper\u2019s approach, which necessitates bitext and a dictionary with the source language (Moklen) and the target language (English). Secondly, the dictionary is relatively comprehensive, containing words that can be used to express a wide range of concepts. Finally, the isolating nature of Moklen syntax allows for the application of LLMs without the complications of inflectional morphology and complex tokenisation.\n\n# 4 Methodology\n\nOur methodology reflects the approach linguists take when analysing grammar and the cognitive aspects of human language. The process comprises five major steps, as shown in Figure 2: (1) Given the bitext, a dictionary-based tokeniser is used to segment the Moklen text into individual words. (2) For every word in a sentence, we search for the closest match from the dictionary to map the English meaning to the corresponding words in the source language. (3) We concatenate the meanings of each word in each sentence and pair them side-by-side with the original sentences.\n\nOur methodology reflects the approach linguists take when analysing grammar and the cognitive aspects of human language. The process comprises five major steps, as shown in Figure 2: (1) Given the bitext, a dictionary-based tokeniser is used to segment the Moklen text into individual words. (2) For every word in a sentence, we search for the closest match from the dictionary to map the English meaning to the corresponding words in the source language. (3) We concatenate the meanings of each word in each sentence and pair them side-by-side with the original sentences.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab52/ab52e5d4-3eac-49af-bf5b-dd25154d00f6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">T am making a white sea bass curry this evening\n</div>\n<div style=\"text-align: center;\">2: The approach requires each pair of sentences from bitext going through the process of tokenisation, sen ng, and sentence concatenation prior to include into the prompt to do the task\n</div>\n(4) We prompt a large language model (LLM) with the mapped, tokenised bitext and materials related to creating grammar in XLE format. (5) We then use the generated grammar rules to guide the LLM in generating lexical entries based on the words from the dictionary. The primary data sources for this study include bilingual dictionaries and parallel sentences that pair Moklen with English. The dictionary in this paper developed from (Pittayaporn et al., 2022) serves as a foundational resource, containing approximately 1,000 basic vocabulary items in Moklen along with their English translations. Additionally, a collection of parallel sentences, derived from field notes and existing documentation, will be used to provide contextual examples of grammatical structures in both languages.\n\n# 4.1 Tokenisation\n\nIn this step, we create a dictionary-based tokeniser to segment the Moklen text into individual words. This approach is feasible since all words in the Moklen sentences from the bitext exist in the dictionary. However, tokenising Moklen presents unique challenges due to its high frequency of compound words. To address this, a longest-match strategy is employed where the tokeniser first looks for the closest and largest item in the dictionary before considering smaller units. This prevents overtokenisation and ensures that the true meaning of each sentence is accurately represented. For example, the Moklen word maklaw, meaning\n\n\u2019to speak\u2019, is a compound form derived from the root word klaw. If tokenised incorrectly, it could be split into two different words: ma (meaning \u2019horse\u2019) and klaw  (meaning \u2019to speak\u2019). This misinterpretation could lead to a sentence being incorrectly understood as \"The horse is speaking.\" Accordingly, the tokeniser will recognise maklaw as a single unit, thereby preserving the correct meaning and enhancing the overall accuracy of the analysis.\n\n# 4.2 Sense Mapping\n\nOnce the text is tokenised, we perform sense mapping by searching for the corresponding meaning of each token in the Moklen sentences. This step is crucial for ensuring that the LLM does not attempt to match each token independently, which often results in inaccuracies. Instead, we aim to provide the LLM with contextually relevant mappings. Each word in Moklen may have multiple meanings, and it is essential for the LLM to select the appropriate meaning based on the context provided. This process assumes that the LLM can consider the various meanings of words and choose the one that best fits the context of the sentence. For instance, in the sentence tichum boh pong, which translates to \u2019The bird is building the nest\u2019, the word pong  has two different senses: \u2019nest\u2019 and \u2019father\u2019. The LLM should correctly interpret the sentence as \u2019The bird is building the nest\u2019 rather than \u2019The bird is building/making the father\u2019, demonstrating its ability to disambiguate based on context.\n\nAfter mapping the meanings, we concatenate the meanings of each word and prepare the data for input into the LLM. The underlying idea is that even though the LLM may have no prior knowledge of Moklen, it can employ its logical reasoning capabilities to analyse the provided data. By comparing the shared structures of Moklen and English, the LLM can induce grammatical rules from the context and information given. For the model generation, there are two approaches to creating the grammar of Moklen. The first approach involves describing the language using natural language, detailing information such as word order, grammatical features, and functions. The second approach is to generate the grammar in a formal language, specifically XLE, by creating rules and lexical entries using a specific template and syntax 1. Whilst we favour the second approach as the end product of this paper, the natural language description can be a good indicator to understand how the LLM interprets and analyses the language data.\n\n# 5 Experiment\n\n# 5.1 Experimental Setup\n\nWe ran all experiments on OpenAI\u2019s API-based model gpt-4o-mini-2024-07-18 and keep a rather low temperature at 0.1 across tasks. The benchmark data, code, and model generations can be found in the supplementary material. Despite being a single task, we also perform the task under different experimental settings, varying the kind of retrieved context that are provided in the prompt. See?? for full details. The types of context include: No context (-): Apart from Moklen sentences, the model is told only that Moklen is a language spoken around Southern Thailand and given no reference material. This measures the base model\u2019s zero-shot capabilities and validates that they have effectively learned zero Moklen during pretraining. Bitext Context (B): For each sentence, we provide an English translation into the prompt. Tokenised Context (T): We experiment with two different ways. First, for each sentence, we provide tokens (T 0). Second, for each word in each Moklen sentence, we map them with the English definition\n\n1 Henceforth, grammar refers to formal rules and lexical entries in XLE.\n\nno matter how many senses they are into the prompt (T D). Concatenated Sentence Context (C): For each sentence, we take the translation of each word to concatenate as one string, though they may be nonsensical or ungrammatical (since we need the incorrect ones on purpose!). It is assumed that this type of string will reflect how word order in Moklen works as opposed to English. Example Context (E): The model is provided with an example of how the English language is implemented with XLE. Self-Explanation Context (S):  Prior to the generation, the model is given the relevant data to analyse and describe in natural language, and take it as a guideline when generating the grammar. For all contexts, the model is retrievalaugmented by providing with the XLE documentation that describes how to create a grammar and a Moklen dictionary except for no context condition, storing in a separate datastore (Asai et al., 2024).\n\n# 5.2 Evaluation Measures\n\nEvaluating the quality of generated grammars is inherently an complex task, as there is no single correct approach to grammar construction. Consequently, the evaluation must consider both quantitative and qualitative aspects of the model\u2019s performance and the output. Translation. We follow Tanzer et al. (2024) and Zhang et al. (2024) in that translation performance is used as an indirect indicator of the model\u2019s understanding of linguistic structures, albeit not explicitly. Thus, this task is the starting point to selecting the best combination of contexts to proceed to the more sophisticated step. We will evaluate the model\u2019s performance on Moklen-to-English translation tasks, using an additional set of 40 parallel sentences. The quality of translations will be assessed using several metrics including a traditional BLEU (Papineni et al., 2002), ROUGE (Ganesan, 2018), METEOR (Banerjee and Lavie, 2005), chrF (Popovi\u00b4c, 2015), and BERTScore (Zhang et al., 2020). This very task will serve as an experimental ablation to determine the most effective context settings for further steps. Apart from this, Englishto-Moklen translation will be conducted both before and after generating the grammar to determine the impact of the generated grammar on translation performance. Grammar Rule Accuracy. Never before had Moklen been developed its grammar using XLE.\n\nHence, there is no XLE grammar for Moklen; we attempt to create one based on Spencer (2022) and served as a gold standard. The model will be prompted to generate the generated grammar rules which will be qualitatively evaluated. Ideally, the rules must demonstrate the capacity to accurately model all possible sentence structures in Moklen, including the full range of parts-of-speech categories. Specifically, the grammar should cover syntactic constructions, word order, and morphological agreement, ensuring that all core grammatical elements of Moklen are represented. Lexical Entry and Schemata Accuracy. The model will generate lexical entries for 100 words from the Moklen dictionary that do not appear in the bitext, primarily based on the grammar rules produced by the model and the information provided in the dictionary. For each lexical entry, there are differences between how different element differently exhibits their functional schemata. This accuracy is also assumed to be resulted by the accuracy of the grammar rules, as well as the understanding of both grammar of the language and XLE architecture. We will manually assess the accuracy of these lexical entries by comparing them to existing dictionary definitions and evaluating their coherence across the generation. we will consider the completeness of the entries, examining whether the model captures various meanings and usages of each word. Ultimately, these lexical entries will be integrated into the XLE system.\n\n# 6 Results\n\n# 6.1 Translation\n\nAblation. We explored 48 possible context combinations (see??) and identified 10 particularly noteworthy ones based on representativeness, not necessarily the best performance. These combinations are:, B, T 0, B + T 0 + S, B + T D, B + T D + C, T D + C + S, B + T D + C + E, T D + C + E + S, and B + T D + C + E + S. Further translation tasks were conducted on these combinations. Surprisingly, the combination with all contexts did not yield the best performance. Instead, the T D context alone achieved the highest score, with a BERTScore F1 of 0.4904, only slightly better than the second-best. This suggests that T D is more effective when combined with other contexts, outperforming T 0 in all scenarios. The impact of contexts E and S is relatively equal when combined with other contexts.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/72f1/72f1abf5-e2d5-4f74-900d-746d917753c6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: When instructing with grammar, the model performs better across various combinations of context\n</div>\nEnglish-to-Moklen with Grammar.  When incorporating the XLE gold standard grammar into the prompt across all models, we observed an overall improvement in translation performance. This suggests that the inclusion of grammar plays a crucial role in enhancing model output quality.\nThis finding is consistent with our previous observations. Specifically, the context combination T D + C + S emerged as the most effective, achieving the highest score in nearly all benchmarks with an estimated BERTScore F1 of 0.7110. This result highlights that certain combinations of contexts can significantly enhance translation accuracy.\nEnglish-to-Moklen with Grammar. Interestingly, translating from Moklen to English demonstrated better performance compared to the reverse translation (English-to-Moklen). This suggests that the model\u2019s ability to generate English sentences from Moklen inputs is more refined or effective than generating Moklen sentences from English. The implication behind this will be discussed later in the next section.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ae04/ae04e6f9-bd63-4660-9164-22fc66f347f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: The improvement in scores across contexts leading to the suspicion of the reliability of the metric\n</div>\n# 6.2 Grammar Rules: Accuracy and Completeness\n\nTo assess the completeness of the grammar rules, we evaluated whether they could accurately capture all parts of speech in Moklen without overly relying on English examples. Using the context T D + C + S, the model effectively identified common parts of speech such as nouns, verbs, adjectives, and adverbs. However, some unique English parts of speech, particularly determiners, were incorrectly applied to Moklen grammar. Despite this, the XLE implementation accurately captures most grammatical structures in the language. This discrepancy raises questions that will be explored in the following discussion.\n\n# 6.3 Lexical Entries: Accuracy and Coherence\n\nWe generated lexical entries for 100 Moklen words not present in the bitext and assessed their accuracy by comparing them to existing dictionary definitions. Out of these, 86 entries were deemed accurate and coherent. However, some incomplete lexical entries captured only the more prominent senses of words. For example, the word data was defined as a preposition meaning \u2018on\u2019 and a noun meaning \u2018the top\u2019, but the latter sense was not captured by the model.\n\n# 7 Analysis & Discussion\n\nThere are several critical aspects emerging from the experimental results worth for further discussion. Dictionary Integration.  The inclusion of a dictionary in the prompt significantly impacts model performance, even without further contextual modifications. This simple integration yields results that are comparable to or better than combining the dictionary with other contexts. Specifically, the dictionary enhances the model\u2019s lexical understanding by grounding it in a structured reference, which is essential when working with languages with minimal resources. For Moklen, a language that lacks morphological inflections and instead uses content words to express grammatical relations (e.g., tense or aspect), dictionary-based lexical grounding proves effective. The performance improvements can be seen in the BERTscores, where the model achieved translation quality approaching that of more morphologically complex languages, despite Moklen\u2019s isolating structure. This supports the idea that even minimal data, when correctly utilised, can provide significant gains in language processing\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9be7/9be73eb9-9f04-4812-8a62-2094d135ce08.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: The clear improvement after intergrating dictionary into the prompt in Moklen-to-English translation tasks\n</div>\nEvaluation of Translation Metrics.  A key challenge with existing translation metrics like BLEU, ROUGE-L, METEOR, and chrF is their heavy reliance on word forms and syntactic similarity, which aligns poorly with Moklen\u2019s structural characteristics. For instance, these metrics work effectively in English, where grammatical markers such as tense are explicitly coded into verb forms. However, Moklen\u2019s lack of grammatical tense markers presents a problem, as a single word can express multiple temporal aspects. This reduces the validity of word-form-based metrics for this language, leading to discrepancies between numerical scores and actual translation quality. As an alternative, BERTScore, which prioritizes semantic similarity over word forms, emerges as a better choice for evaluating translations of languages like Moklen. However, even BERTScore falls short in fully capturing the subtleties of Moklen\u2019s grammar. To address this, future work should explore the development of a new evaluation metric tailored to languages with similar grammatical features, focusing on meaning representation rather than form. Model Size Considerations. One critical design decision in this study was the intentional use of a small model. The choice to work with a smaller model was made to demonstrate the potential for generating usable grammar and lexical information with limited computational resources. The success of the small model implies that larger models could yield even better performance, especially in capturing more subtle linguistic nuances or handling rare linguistic phenomena. Given the scalability of LLMs, this opens a pathway for applying similar\n\nmethods to larger models for endangered languages with more complex syntactic structures or larger\n\nmethods to larger models for endangered languages with more complex syntactic structures or larger datasets. Hallucinations and Model Inaccuracies.  In lowresource settings, model hallucinations, particularly in zero-shot translation tasks, were evident. Hallucinations primarily arose when the model was forced to translate or generate sentences with little or no exposure to similar data during training. This was especially pronounced with Moklen, where the use of the Thai script caused confusion. Due to overlap in the lexicons of Thai and Moklen, the model occasionally produced hybridized translations, mistaking Thai words for Moklen ones. Although these inaccuracies were rare, they underscore the importance of refining training data and prompt construction to mitigate such issues. Moving forward, the inclusion of more diverse Moklen data or deliberate disambiguation in the training process may reduce hallucinations and improve model robustness. However, while these deviations can be problematic, they can also offer unexpected insights for linguists. In some cases, the model\u2019s introduction of \u2019non-Moklen\u2019 parts of speech or grammar elements not traditionally associated with the language might suggest an overlooked pattern or nuance. These hallucinations, though initially appearing as errors, can prompt linguists to reconsider their assumptions and explore whether the model has captured subtle relationships or features that were not immediately apparent from a human perspective. For instance, the model\u2019s introduction of categories that do not exist formally in Moklen may highlight potential areas of linguistic overlap or underlying structures that deserve further investigation. This kind of speculative output, while not always accurate, provides a thought-provoking dimension to the analysis, encouraging a holistic re-examination of linguistic data. Potential for XLE Parsing and Beyond.  this approach should not be overlooked. XLE\u2019s highly sophisticated nature for parsing natural language makes it an ideal framework for documenting endangered languages. However, given the success of this methodology, it is reasonable to assume that the generated grammar could be adapted to other formal frameworks, such as HPSG or dependency grammars, making it widely applicable across different linguistic projects. This flexibility suggests that LLMs, when properly guided, can not only assist in XLE parsing but also contribute to\n\nthe broader field of computational linguistics by providing insights into the underlying structure of under-documented languages.\n\n# 8 Conclusion\n\nIt is evident that an LLM, if sufficient information is provided, can assist linguists in generating linguistic data for such complex and, moreover, tedious tasks. This methodology with careful prompting techniques offers a cost- and time-effective means of obtaining grammatical information for the endangered Moklen language by means of minimal linguistic resources and analogy from English language. By using as less as only bilingual dictionaries and in-context learning, we successfully extracted coherent grammatical rules and lexical entries, thereby contributing to the documentation and preservation of Moklen or, at least, isolating languages in general.\n\n# Limitation\n\nWhile this study demonstrates promising results, it is important to acknowledge the limitations of this study. We experimented with only a single language, Moklen, which is an isolating language without grammatical inflections. This typological characteristic may have made it easier for the LLM to decipher the language from a negligible amount of information compared to other languages in the world. We plan to extend this methodology to inclusively experiment with various languages from different typologies and morphologies to assess its broader applicability. Additionally, the potential for English grammatical biases in the model\u2019s outputs suggests that further research is needed to develop techniques for minimising cross-linguistic interference in low-resource language modelling.\n\n# References\n\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen tau Yih. 2024. Reliable, adaptable, and attributable language models with retrieval. Preprint, arXiv:2403.03187.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, Michigan. Association for Computational Linguistics.\n\nGa\u0161per Begu\u0161, Maksymilian D \u02dbabkowski, and Ryan\nRhodes. 2023.\nLarge linguistic models: Analyz-\ning theoretical linguistic abilities of llms. Preprint,\narXiv:2305.00948.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. Preprint, arXiv:2005.14165.\nMiriam Butt, Helge Dyvik, Tracy Holloway King, Hi-\nroshi Masuichi, and Christian Rohrer. 2002. The\nparallel grammar project. In COLING-02: Grammar\nEngineering and Evaluation.\nMiriam Butt, Tracy Holloway King, Maria-Eugenia\nNi\u00f1o, and Frederique Segond. 1999. A Grammar\nWriter\u2019s Cookbook. Stanford University Press.\nNoam Chomsky and Robert Berwick. 2016. Why Only\nUs. MIT Press.\nAnn Copestake. 2002. Definitions of typed feature struc-\ntures. In Stephan Oepen, Dan Flickinger, Jun-ichi\nTsujii, and Hans Uszkoreit, editors, Collaborative\nLanguage Engineering, pages 227\u2013230. CSLI Publi-\ncations, Stanford, CA.\nRichard Crouch, Mary Dalrymple, Ronald M. Kaplan,\nTracy Holloway King, John T. III Maxwell, and\nPaula S. Newman. 2011. XLE Documentation.\nMary Dalrymple, editor. 2023. Handbook of Lexical\nFunctional Grammar. Number 13 in Empirically Ori-\nented Theoretical Morphology and Syntax. Language\nScience Press, Berlin.\nEthnologue. 2024. How many languages are endan-\ngered?\nKavita Ganesan. 2018. Rouge 2.0: Updated and im-\nproved measures for evaluation of summarization\ntasks. Preprint, arXiv:1803.01937.\nJohn T. Maxwell, III and Ronald M. Kaplan. The in-\nterface between phrasal and functional constraints.\nComputational Linguistics.\nDiego Miranda-Saavedra. 2024. Generative ai models\nand the quest for human-level artificial intelligence.\nReal World Data Science.\nStephan Oepen, Kristina Toutanova, Stuart Shieber,\nChristopher Manning, Dan Flickinger, and Thorsten\nBrants. 2002.\nThe LinGO redwoods treebank:\nMotivation and preliminary applications.\nIn\nCOLING 2002: The 17th International Conference\non Computational Linguistics: Project Notes.\narXiv:2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002.\nBleu: a method for automatic\nevaluation of machine translation. In Proceedings\nof the 40th Annual Meeting of the Association for\nComputational Linguistics, pages 311\u2013318, Philadel-\nphia, Pennsylvania, USA. Association for Computa-\ntional Linguistics.\nPittayawat Pittayaporn and Songphan Choempray-\nong. Forthcoming.\nA proposal for a thai-based\nmoklen orthography. Language Documentation and\nConservation.\nPittayawat Pittayaporn, Warunsiri Pornpottanamas,\nDaniel Loss, Songphan Choemprayong, Chengnan\nZhang, Piyangkoon Thaweepol, Pongporn Pray-\nongsap, Pornsuda Nawarak, Peerasak Sirisawasdi,\nand Leena Maluleem. 2022. Moklen-Thai-English\nDictionary: A Pilot Version.\nMaja Popovi\u00b4c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392\u2013395, Lisbon, Portugal. Association for\nComputational Linguistics.\nBradley L. Pritchett. 1992. Grammatical Competence\nand Parsing Performance. The University of Chicago\nPress.\nJane Simpson. 1991.\nWarlpiri Morpho-Syntax: A\nLexicalist Approach. Kluwer Academic Publisher.\nPiyapath Spencer. 2024. Documenting endangered lan-\nguages with LangDoc: A wordlist-based system and\na case study on Moklen. In Proceedings of the 3rd\nWorkshop on NLP Applications to Field Linguistics\n(Field Matters 2024), pages 28\u201336, Bangkok, Thai-\nland. Association for Computational Linguistics.\nPiyapath T Spencer. 2022.\nGrammatical sketch of\nmoklen verb phrase. Unpublished Manuscipt.\nGarrett Tanzer, Mirac Suzgun, Eline Visser, Dan Juraf-\nsky, and Luke Melas-Kyriazi. 2024. A benchmark\nfor learning to translate a new language from one\ngrammar book.\nGemini Team. 2024. Gemini 1.5: Unlocking multi-\nmodal understanding across millions of tokens of\ncontext. Preprint, arXiv:2403.05530.\nW3Techs. 2024. Usage statistics of content languages\nfor websites.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022.\nFinetuned\nlanguage models are zero-shot learners. Preprint,\narXiv:2109.01652.\nOlga Zamaraeva, Chris Curtis, Guy Emerson, Antske Fokkens, Michael Goodman, Kristen Howell, T.J. Trimble, and Emily M. Bender. 2022. 20 years of the grammar matrix: cross-linguistic hypothesis testing of increasingly complex interactions. Journal of Language Modelling, 10(1):49\u2013137.\nKexun Zhang, Yee Choi, Zhenqiao Song, Taiqi He, William Yang Wang, and Lei Li. 2024.  Hire a linguist!: Learning endangered languages in LLMs with in-context linguistic descriptions. In Findings of\nthe Association for Computational Linguistics ACL 2024, pages 15654\u201315669, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. Preprint, arXiv:1904.09675.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/58d6/58d61f61-2957-4b93-9cf6-c5e5a7636110.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of generating grammatical information for endangered languages, particularly focusing on low-resource languages like Moklen. Previous methods have relied heavily on manual grammar construction, which is resource-intensive and time-consuming. The need for a new approach arises from the increasing risk of extinction for many languages and the limited digital representation they have.",
        "problem": {
            "definition": "The problem this paper aims to solve is the lack of accessible grammatical resources for endangered languages, which hinders their documentation and preservation.",
            "key obstacle": "The core obstacle is the extensive linguistic expertise and resources required for manual grammar construction, which is often unfeasible for low-resource languages."
        },
        "idea": {
            "intuition": "The idea was inspired by the capabilities of large language models (LLMs) in adapting to various linguistic tasks through in-context learning, which could potentially automate grammar generation.",
            "opinion": "The proposed idea involves using LLMs to generate coherent grammatical rules and lexical entries for low-resource languages using minimal data, specifically bilingual dictionaries and parallel sentences.",
            "innovation": "The key innovation is the use of LLMs to create formal XLE grammar for languages that the models have not encountered during pre-training, relying solely on limited linguistic resources."
        },
        "method": {
            "method name": "Grammar Generation with LLMs",
            "method abbreviation": "GGL",
            "method definition": "GGL is a method that utilizes large language models to automate the generation of grammatical rules and lexical entries for endangered languages based on minimal existing data.",
            "method description": "The method involves segmenting text, mapping meanings from a dictionary, and prompting an LLM to generate grammar and lexical entries.",
            "method steps": [
                "Segment Moklen text into individual words using a dictionary-based tokeniser.",
                "Map the meanings of each word in Moklen to their English equivalents.",
                "Concatenate the meanings and original sentences to prepare data for the LLM.",
                "Prompt the LLM with the prepared data and relevant materials for grammar generation.",
                "Generate lexical entries based on the rules produced by the LLM."
            ],
            "principle": "The method is effective because it leverages the LLM's ability to understand and generate language based on contextual prompts, allowing it to induce grammatical rules from limited data."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using OpenAI\u2019s API-based model gpt-4o-mini-2024-07-18, with a focus on varying the context provided to the model during prompting.",
            "evaluation method": "The evaluation included translation tasks, grammar rule accuracy assessments, and lexical entry coherence checks, utilizing metrics like BLEU, ROUGE, METEOR, and BERTScore."
        },
        "conclusion": "The study demonstrates that LLMs can assist in generating linguistic data for endangered languages by extracting coherent grammatical rules and lexical entries from minimal resources, contributing to their documentation and preservation.",
        "discussion": {
            "advantage": "The main advantage of the proposed approach is its ability to generate usable grammar and lexical information with limited computational resources, making it accessible for low-resource languages.",
            "limitation": "A significant limitation is the reliance on a single language, Moklen, which may not represent the complexities of other languages with different typologies and morphologies.",
            "future work": "Future research should extend this methodology to include a wider range of languages and develop techniques to minimize potential English grammatical biases in model outputs."
        },
        "other info": {
            "additional details": {
                "language studied": "Moklen, an endangered language spoken in Southern Thailand with no formal written tradition.",
                "data sources": "The primary data sources include bilingual dictionaries and parallel sentences from field notes."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of generating grammatical information for endangered languages, focusing on low-resource languages like Moklen, highlighting the foundational concepts of in-context learning as an approach to automate grammar generation."
        },
        {
            "section number": "1.2",
            "key information": "The significance of in-context learning is underscored by its potential to aid in the documentation and preservation of endangered languages, which is critical in the broader field of natural language processing."
        },
        {
            "section number": "3.1",
            "key information": "The method 'Grammar Generation with LLMs' demonstrates how LLMs adapt to various linguistic tasks through in-context learning, showcasing adaptation to the context of low-resource languages."
        },
        {
            "section number": "3.4",
            "key information": "The method involves segmenting text and mapping meanings, illustrating how LLMs utilize memory and context to facilitate in-context learning for grammar generation."
        },
        {
            "section number": "4.1",
            "key information": "The paper discusses how the design of prompts can significantly influence the outcomes of in-context learning, specifically in generating grammatical rules and lexical entries for endangered languages."
        },
        {
            "section number": "6.1",
            "key information": "A significant limitation identified in the paper is the reliance on a single language, Moklen, which raises concerns about model bias and context sensitivity in in-context learning applications."
        },
        {
            "section number": "7",
            "key information": "The conclusion emphasizes that LLMs can assist in generating linguistic data for endangered languages, contributing to their documentation and preservation, and reflects on future directions for extending this methodology."
        }
    ],
    "similarity_score": 0.7087858295241596,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Can LLMs Help Create Grammar__ Automating Grammar Creation for Endangered Languages with In-Context Learning.json"
}